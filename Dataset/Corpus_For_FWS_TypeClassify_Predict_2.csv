id,chapter,year,text
2020.acl-demos.1.txt,7 Conclusion,2020,"one such important direction for future improvement is the expansion of areas that it can work in, which can be achieved through a promising approach of adopting model based technologies together with rule/template based ones."
2020.acl-demos.10.txt,6 Conclusion,2020,"in particular, we plan to incorporate human performance as a reference metric, integrating psycholinguistic experimental results and supporting easy experimental design starting from the test suite format."
2020.acl-demos.10.txt,6 Conclusion,2020,"syntaxgym is continually evolving: we plan to add new features to the site, and to develop further in response to user feedback."
2020.acl-demos.10.txt,6 Conclusion,2020,"we also plan to further incorporate language models into the lm-zoo tool, allowing broader access to state-of-the-art language models in general."
2020.acl-demos.14.txt,5 Conclusion and Future Work,2020,"for future work, we consider the following areas of improvement in the near term: • models downloadable in sta n z a are largely trained on a single dataset."
2020.acl-demos.14.txt,5 Conclusion and Future Work,2020,"to make models robust to many different genres of text, we would like to investigate the possibility of pooling various sources of compatible data to train “default” models for each language; • the amount of computation and resources available to us is limited."
2020.acl-demos.14.txt,5 Conclusion and Future Work,2020,"we would like to further investigate reducing model sizes and speeding up computation in the toolkit, while still maintaining the same level of accuracy.• we would also like to expand sta n z a ’s functionality by adding other processors such as neural coreference resolution or relation extraction for richer text analytics."
2020.acl-demos.16.txt,4 Conclusion,2020,"we will extend mt-dnn to support natural language generation tasks, e.g. question generation, and incorporate more pre-trained encoders, e.g. t5 (raffel et al., 2019) in future."
2020.acl-demos.17.txt,6 Conclusion and Future Work,2020,"an interesting direction to explore is re-ranking corrective suggestions, so that the suggestion more relevant to the original sentence goes to the top."
2020.acl-demos.17.txt,6 Conclusion and Future Work,2020,many avenues exist for future research and improvement of our system.
2020.acl-demos.17.txt,6 Conclusion and Future Work,2020,yet another direction of research would be to detect fine-grained error types.
2020.acl-demos.18.txt,5 Conclusions,2020,"our case study on the wmt2019 metrics shared task further highlights the potential of clir as a proxy task for mt evaluation, and we hope that clireval can facilitate future research in this area."
2020.acl-demos.19.txt,5 Conclusion,2020,we hope that convlab-2 is instrumental in promoting the research on task-oriented dialogue.
2020.acl-demos.2.txt,6 Conclusion and Future Work,2020,"in the future, we aim to integrate neural architecture search into the toolkit to automate the searching for model structures."
2020.acl-demos.20.txt,4 Conclusions and Future Work,2020,"additionally, we would like to explore opusfilter’s use in different scenarios and for other language pairs."
2020.acl-demos.20.txt,4 Conclusions and Future Work,2020,"in future work, we would like to extend the toolbox with additional filters and classification options."
2020.acl-demos.24.txt,6 Conclusion and Future Work,2020,a comprehensive evaluation will also be conducted among the users of our system.
2020.acl-demos.24.txt,6 Conclusion and Future Work,2020,"we also plan to improve the performance of core models in photon, such as semantic parsing (text-to-sql), response generation (table-to-text) and context-aware user interaction (text-to-text)."
2020.acl-demos.24.txt,6 Conclusion and Future Work,2020,"we will continue to add more features to photon, such as voice input, spelling checking, and visualizing the output when appropriate to inspect the translation process."
2020.acl-demos.25.txt,5 Discussion and Future Work,2020,"another promising approach to enable more robust natural language understanding is to leverage the pre-trained generalpurpose language models (e.g., bert (devlin et al., 2018)) to encode the user instructions and the information extracted from app guis.5.3 extracting task semantics from guis an interesting future direction is to better extract semantics from app guis so that the user can focus on high-level task specifications and personal preferences without dealing with low-level mundane details (e.g., “buy 2 burgers” means setting the value of the textbox below the text “quantity” and next to the text “burger” to be “2”)."
2020.acl-demos.25.txt,5 Discussion and Future Work,2020,"for itl, an interesting future challenge is to combine these user-independent domain-agnostic machine-learned models with the user’s personalized instructions for a specific task."
2020.acl-demos.25.txt,5 Discussion and Future Work,2020,"this could be supported by improved background knowledge and task models, and more flexible dialog frameworks that can handle the continuous refinement and uncertainty inherent in natural language interaction, and the variations in user goals."
2020.acl-demos.25.txt,5 Discussion and Future Work,2020,"this will likely require a new kind of mixedinitiative instruction (horvitz, 1999) where the agent is more proactive in guiding the user and takes more initiative in the dialog."
2020.acl-demos.25.txt,5 Discussion and Future Work,2020,we are currently exploring other ways of using multi-modal interactions to supplement natural language instructions in itl.
2020.acl-demos.26.txt,7 Conclusion,2020,"for future work, we plan to keep adding the state-of-the-art algorithms, reduce latency and fine-tune the implemented models on larger and/or more comprehensive corpus to improve performance."
2020.acl-demos.27.txt,6 Conclusions and Future Work,2020,"all of the data and interactive visualizations associated with this work are freely available through the project homepage.13 future work will provide additional functionalities such as search within abstracts and whole texts, document clustering, and automatically identifying related papers."
2020.acl-demos.28.txt,6 Conclusion and Future Work,2020,"in future, we would like to extend the funlines data collection setup to a more general crowdsourcing framework, for example, to collect style transfer data."
2020.acl-demos.29.txt,6 Discussion,2020,interactive fiction is an art form with an uncertain future that is connected in no small way to its proximity to games and the social norms separating games and fine art.
2020.acl-demos.29.txt,6 Discussion,2020,"we hope foremost that authors will enjoy using our tool to create something they care about, and that readers will enjoy their creations."
2020.acl-demos.3.txt,9 Conclusions,2020,"we intend to release the code as open source, as well as providing hosted open access to a pubmed-based corpus."
2020.acl-demos.30.txt,7 Conclusion,2020,detection and control of toxic output will be a major focus of future investigation.
2020.acl-demos.30.txt,7 Conclusion,2020,we will investigate leveraging reinforcement learning to further improve the relevance of the generated responses and prevent the model from generating egregious responses.
2020.acl-demos.32.txt,5 Conclusion and Future Work,2020,"in future work, we plan to add more media sources, especially from non-english media and regions."
2020.acl-demos.32.txt,5 Conclusion and Future Work,2020,"we further want to extend the tool to support other propaganda techniques such as cherrypicking and omission, among others, which would require analysis beyond the text of a single article."
2020.acl-demos.33.txt,5 Conclusion,2020,we will try to utilize external resources to solve the few-shot and zero-shot problem in the future.
2020.acl-demos.34.txt,5 Conclusion,2020,"in the future, we will support more corpora and implement novel techniques to bridge the gap between end-to-end and cascaded approaches."
2020.acl-demos.35.txt,6 Conclusion,2020,"transformations defined at both the graph and tree level make it applicable for pre- and postprocessing steps for corpus creation, evaluation, machine learning projects, and more."
2020.acl-demos.36.txt,5 Conclusion,2020,"in future work, we will focus on expanding the database to include additional domains and article sources."
2020.acl-demos.36.txt,5 Conclusion,2020,"we will also seek to improve discovery performance by testing more recent text embedding methods (e.g., bert (devlin et al., 2018)) and by optimizing the search for different input text lengths, such as a whole document, a paragraph, or even a single sentence."
2020.acl-demos.36.txt,5 Conclusion,2020,"we will investigate whether query expansion techniques (azad and deepak, 2019) could mitigate this issue by suggesting or automatically appending semantically related keywords to the boolean filters."
2020.acl-demos.36.txt,5 Conclusion,2020,"we will work on augmenting the workflow with automated tasks, such as suggesting references as the author writes a manuscript, or notifying users about the latest publications relevant to their work."
2020.acl-demos.37.txt,4 Conclusion,2020,"as a next step, we will improve the prototype based on the participants’ valuable feedback."
2020.acl-demos.37.txt,4 Conclusion,2020,"last, we will investigate whether using the different modalities has an impact on cognitive load during pe (herbig et al., 2019b)."
2020.acl-demos.38.txt,7 Conclusion and Future Work,2020,"finally, we hope to explore further optimizations to make core algorithms competitive with highly-optimized neural network components."
2020.acl-demos.38.txt,7 Conclusion and Future Work,2020,"in the future, we hope to support research and production applications employing structured models."
2020.acl-demos.38.txt,7 Conclusion and Future Work,2020,these approaches provide a benchmark for improving autodifferentiation systems and extending their functionality to higher-order properties.
2020.acl-demos.4.txt,4 Conclusion,2020,"in addition to improving the banned words selection process, future work on tabouid includes generating specific lists of cards based on school programs to use the game as an educational tool, using the category system of wikipedia to let users select more or less specific categories to play with, and adapting the algorithms to leverage the wide variety of languages wikipedia is available in beyond english and french."
2020.acl-demos.40.txt,6 Conclusion,2020,"there are many open questions which we intend to research, such as whether autoregressivity in neural sentence compression can be exploited and how to compose themes over longer time periods."
2020.acl-demos.42.txt,7 Conclusion,2020,we hope that our work on lean-life will allow for researches and practitioners alike to more easily obtain useful labeled datasets and models for the various nlp tasks they face.
2020.acl-demos.5.txt,6 Conclusion,2020,future work include (1) expanding the database to more papers (2) improving the qa model using the collected data to better handle question answering in the context of research domain.
2020.acl-demos.9.txt,5 Conclusions,2020,"moving forward, we hope to refine the linking of extracted snippets to structured vocabularies to run a more comprehensive user-study to evaluate the use of the system in practice by different types of users."
2020.acl-main.1.txt,7 Discussion,2020,"in future work, we intend to curate a test set with data from separate sources, which can serve as a benchmark for the models we study."
2020.acl-main.1.txt,7 Discussion,2020,"the combined test set scores are more directly comparable, but ideally, we would like to compare the generalization of both models on an independent test set."
2020.acl-main.1.txt,7 Discussion,2020,we also intend to use tools for interpreting the knowledge encoded in neural networks (such as diagnostic classifiers and representational similarity analysis) to investigate the emergent representation of linguistic units such as phonemes and words.
2020.acl-main.1.txt,7 Discussion,2020,we intend to explore how a curriculum of cds followed by ads affects learning trajectories and outcomes.
2020.acl-main.100.txt,5 Conclusion,2020,"in the future, we are interested in injecting knowledge into text representation learning (cao et al., 2017, 2018b) for deeply understanding expert language, and will help to generate knowledgeenhanced questions (pan et al., 2019) for laymen."
2020.acl-main.102.txt,5 Conclusion,2020,"since dynamic memory can be a learning mechanism more general than what we have used here for fewshot learning, we will investigate this type of models in other learning problems."
2020.acl-main.103.txt,7 Conclusion and Future Work,2020,one interesting future direction is to explore whether the beam search is helpful to our model.
2020.acl-main.104.txt,6 Conclusion,2020,"furthermore, our framework is extended into a parallel variant based on multi-label attention and a serial variant of text feature propagation."
2020.acl-main.105.txt,4 Conclusion,2020,other retrieval tasks may also benefit from using keyphrase information and we expect our results to serve as a basis for further improvements.
2020.acl-main.108.txt,6 Summary,2020,all the resources developed in this paper are freely available.3
2020.acl-main.11.txt,5 Conclusion and Future Work,2020,"in future work, we plan to experiment with multi-domain span extraction architectures."
2020.acl-main.111.txt,6 Conclusion,2020,we leave it as future work.
2020.acl-main.111.txt,6 Conclusion,2020,we plan to continue the work on herbert and use the klej benchmark to guide its development.
2020.acl-main.112.txt,7 Conclusion,2020,"a promising direction would be to combine a multilingual sense inventory such as babelnet (navigli and ponzetto, 2012) with sense embeddings (camacho-collados and pilehvar, 2018)."
2020.acl-main.112.txt,7 Conclusion,2020,future work will have to deepen the way we deal with word sense ambiguity by way of exchanging the simplifying type-level approach our current work is based on with a semantically more informed sense-level approach.
2020.acl-main.112.txt,7 Conclusion,2020,our study is “large-scale” in many respects.
2020.acl-main.113.txt,7 Conclusions,2020,"finally, we would like to test this approach for comparing different mt systems."
2020.acl-main.113.txt,7 Conclusions,2020,"first, we plan to test whether similar observations will hold for more language pairs and text domains."
2020.acl-main.116.txt,8 Conclusion,2020,a natural next step is to combine the datasets in a multi-task setting to investigate to what extent models can profit from combining the information annotated in the respective datasets.
2020.acl-main.116.txt,8 Conclusion,2020,"further research will investigate the joint modeling of entity extraction, typing and experiment frame recognition."
2020.acl-main.117.txt,7 Discussion and Future Work,2020,techqa is a challenging dataset for models developed for existing open-domain mrc systems.
2020.acl-main.117.txt,7 Discussion and Future Work,2020,we consider techqa to be a stepping stone on which to build future data collections and leaderboards.
2020.acl-main.117.txt,7 Discussion and Future Work,2020,"we envision a roadmap where future releases of techqa will require synergy between multiple ai disciplines, from deep-learning based mrc to reasoning, knowledge base acquisition, and causality detection."
2020.acl-main.117.txt,7 Discussion and Future Work,2020,we plan on releasing questions with answers in a broader and more diverse collection that will include documents with a less formulaic structure than the technotes.
2020.acl-main.117.txt,7 Discussion and Future Work,2020,"we will also relax the length limitations to include questions rich in details, and answers that include complex procedures; in the same spirit, we will allow answers consisting of multiple spans from a single document."
2020.acl-main.118.txt,7 Conclusion and Future Work,2020,"we aim to promote research in sarcasm detection, and to encourage future investigations into sarcasm in general and how it is perceived across cultures."
2020.acl-main.118.txt,7 Conclusion and Future Work,2020,we believe this dataset will allow future work in sarcasm detection to progress in a setting free of the noise found in existing datasets.
2020.acl-main.119.txt,7 Conclusion,2020,an interesting future work is to make the number of inference steps adaptive to input sentences.
2020.acl-main.120.txt,6 Conclusion,2020,important challenges for future work include how to scale deep learning methods to such large amounts of source documents and how to close the gap to the oracle methods.
2020.acl-main.120.txt,6 Conclusion,2020,"we conducted extensive experiments to establish baseline results, and we hope that future work on mds will use this dataset as a benchmark."
2020.acl-main.121.txt,6 Conclusion and Future Work,2020,"besides, we will also explore the influence of probabilistic bilingual lexicon obtained by learning only from monolingual data on our method."
2020.acl-main.121.txt,6 Conclusion and Future Work,2020,"furthermore, our method has two advantages over the state-of-the-art: (1) our model requires only an additional probabilistic bilingual lexicon rather than largescale parallel datasets of other tasks, thus reducing the model’s dependence on data and making it easier for the model to migrate to other domains or other language pairs."
2020.acl-main.121.txt,6 Conclusion and Future Work,2020,"in our future work, we consider incorporating our method into the multi-task method."
2020.acl-main.122.txt,7 Conclusion,2020,"potential future directions include a more principled use of our proposed heuristic for detecting content relevant to specific dates, the use of abstractive techniques, a more effective treatment of the redundancy challenge, and extending the new dataset with multiple sources."
2020.acl-main.123.txt,7 Conclusion and future work,2020,"in the future, we explore a more sophisticated method to improve the relevance and truthfulness of generated headlines, for example, removing only deviated spans in untruthful headlines rather than removing untruthful headlines entirely from the supervision data."
2020.acl-main.123.txt,7 Conclusion and future work,2020,"we conjectured that one of the reasons lies in the defect in the task setting and data set, where generating a headline from the source document is impossible because of the insufficiency of the source information."
2020.acl-main.124.txt,7 Conclusion,2020,"we explored unsupervised multi-document summary evaluation methods, which require neither reference summaries nor human annotations."
2020.acl-main.125.txt,6 Conclusion,2020,"for future work, we intend to apply our method to other transformer-based summarization models."
2020.acl-main.126.txt,7 Conclusion,2020,"however, our analysis indicates that self-reported dialog ratings are skewed (j-shape), noisy and insensitive due to bias and variance among different users."
2020.acl-main.127.txt,5 Conclusions,2020,"in the future, we will adapt the method to more neural models especially the generation-based methods for the dialog system."
2020.acl-main.129.txt,7 Conclusions and Future Work,2020,"in future, we want to continue to investigate the possibility of using even weaker demonstrations."
2020.acl-main.13.txt,4 Conclusion,2020,we will explore cross-lingual transfer learning for supporting more languages.
2020.acl-main.130.txt,5 Conclusion,2020,we hope that this dataset facilitates future research on multi-turn conversation reasoning problem.
2020.acl-main.131.txt,7 Conclusion & Future Work,2020,"for future work, we would like to extend receiver to conversational recommender systems."
2020.acl-main.132.txt,7 Conclusions,2020,"finally, we will release our experimental qa datasets (in the squad json format) for bridging anaphora resolution on isnotes and bashi."
2020.acl-main.133.txt,5 Conclusions,2020,"for future work, we would like to deeply study the impacts of our perturbations on the coherence of the examined dialogues."
2020.acl-main.133.txt,5 Conclusions,2020,we will also investigate to what extent the rankings of dialogues obtained by our model correlate with human-provided rankings.
2020.acl-main.135.txt,5 Conclusion and Future Works,2020,there are at least two potential future directions.
2020.acl-main.138.txt,6 Conclusions,2020,future work will involve improvements in the proposed noise model to study the importance of fidelity to real-world error patterns.
2020.acl-main.138.txt,6 Conclusions,2020,"moreover, we plan to evaluate nat on other real noise distributions (e.g., from asr) and other sequence labeling tasks to support our claims further."
2020.acl-main.139.txt,5 Conclusion,2020,"future work will investigate how to take into account potential correlations between labelling functions in the aggregation model, as done in e.g.(bach et al., 2017)."
2020.acl-main.139.txt,5 Conclusion,2020,"of specific linguistic interest is the contribution of document-level labelling functions, which take advantage of the internal coherence and narrative structure of the texts."
2020.acl-main.139.txt,5 Conclusion,2020,we also wish to evaluate the approach on other types of sequence labelling tasks beyond named entity recognition.
2020.acl-main.140.txt,6 Conclusion,2020,"in future work, we want to extend the probing tasks to also cover specific linguistic patterns such as appositions, and also investigate a model’s ability of generalizing to specific entity types, e.g.company and person names."
2020.acl-main.141.txt,5 Conclusion,2020,one possible direction is to extend the scope of structure induction for constructions of nodes without relying on an external parser.
2020.acl-main.141.txt,5 Conclusion,2020,there are multiple avenues for future work.
2020.acl-main.142.txt,7 Conclusion and Future Work,2020,"to improve the evaluation accuracy and reliability of future re methods, we provide a revised, extensively relabeled tacred."
2020.acl-main.144.txt,6 Conclusions and Further Work,2020,"as regards distributed representations, we plan to study alternative networks to more accurately model the identification and incorporation of additional context."
2020.acl-main.144.txt,6 Conclusions and Further Work,2020,"in our future work, we plan to optimise the thresholds used with the retrieval algorithms in order to more intelligently select those translations providing richest information to the nmt model and generalize the use of edit distance on the target side."
2020.acl-main.144.txt,6 Conclusions and Further Work,2020,"we would also like to explore better techniques to inject information of small-size n-grams with possible convergence with terminology injection techniques, unifying framework where target clues are mixed with source sentence during translation."
2020.acl-main.145.txt,6 Conclusion,2020,"in future work, we will extend our analysis to include additional source and target languages from different language families, such as more asian languages."
2020.acl-main.145.txt,6 Conclusion,2020,training on multiple input languages is also effective and leads to improvements across all languages when the source and target languages are similar.
2020.acl-main.145.txt,6 Conclusion,2020,"we will also work towards improving the training efficiency of character-level models, which is one of their main bottlenecks, as well as towards improving their effectiveness in multilingual training."
2020.acl-main.148.txt,7 Conclusion and Future Work,2020,"in the future, we will develop lightweight alternatives to lalt to reduce the number of model parameters."
2020.acl-main.148.txt,7 Conclusion and Future Work,2020,"we release opus-100, a multilingual dataset from opus including 100 languages with around 55m sentence pairs for future study."
2020.acl-main.149.txt,6 Conclusion,2020,"in future work, we plan to extend this analysis across more translation pairs, more diverse languages and multiple domains, as well as investigating the effect of translationese or source-side grammatical errors (anastasopoulos, 2019)."
2020.acl-main.153.txt,6 Conclusions and Future Work,2020,"in the future, we plan to extend the cross-lingual position encoding to non-autoregressive mt (gu et al., 2018) and unsupervised nmt (lample et al., 2018)."
2020.acl-main.155.txt,6 Conclusion,2020,"an exploration of multi-modal measuring approaches (herbig et al., 2019b) shows the feasibility of this, so we will try to combine explicit multi-modal input, as done in this work, with implicit multi-modal sensor input to better model and support the user during pe."
2020.acl-main.155.txt,6 Conclusion,2020,"as a next step, we will integrate the participants’ valuable feedback to improve the prototype."
2020.acl-main.155.txt,6 Conclusion,2020,"while the presented study provided interesting first insights regarding participants’ use of and preferences for the implemented modalities, it did not allow us to see how they would use the modalities over a longer time period in day-to-day work, which we also want to investigate in the future."
2020.acl-main.157.txt,4 Conclusions,2020,"also, the multi-domain nature of the dataset enables future research in cross-target and cross-domain adaptation, a clear weak point of current models according to our evaluations."
2020.acl-main.157.txt,4 Conclusions,2020,"future research directions might explore the usage of transformer-based models, as well as of models which exploit not only linguistic but also network features, which have been proven to work well for existing stance detection datasets (aldayel and magdy, 2019)."
2020.acl-main.160.txt,5 Discussion,2020,one possibility would be to include infelicitous “colorless green ideas” sentences with grammatical syntax (cf.
2020.acl-main.161.txt,7 Conclusions,2020,autoregressive models that generate step by step alternatives for future continuations are computationally impractical for longer rollouts and are not cognitively plausible.
2020.acl-main.161.txt,7 Conclusions,2020,"however, generating plausible future continuations is an essential part of the model."
2020.acl-main.161.txt,7 Conclusions,2020,"strong psycholinguistic claims about suspense are difficult to make due to several weaknesses in our approach, which highlight directions for future research: the proposed model does not have a higher-level understanding of event structure; most likely it picks up the textual cues that accompany dramatic changes in the text."
2020.acl-main.161.txt,7 Conclusions,2020,"they also differ from the ely et al.(2015) conception of suspense, which is in terms of bayesian beliefs over a longer-term future state, not step by step."
2020.acl-main.162.txt,6 Conclusion,2020,we hope this resource can benefit future research into developing techniques to model and understand human responses to document sized text.
2020.acl-main.163.txt,6 Conclusion,2020,"in future, we aim to refine our generative model to better emphasise this difference of the two tasks."
2020.acl-main.164.txt,8 Conclusion,2020,identifying ways to improve the language models and decoding strategies we use in order to generate text that is both exciting (ie.unlikely) and semantically plausible.2.
2020.acl-main.164.txt,8 Conclusion,2020,this variance in likelihood is what makes human-written text interesting and exciting to read.
2020.acl-main.164.txt,8 Conclusion,2020,we therefore suggest three prongs for future research: 1.
2020.acl-main.166.txt,5 Conclusion,2020,"in the future, we will investigate how to extend the cg to support hierarchical topic management in conversational systems."
2020.acl-main.167.txt,5 Conclusions,2020,future work will focus on incorporating better encoding of the amr graph into the current system and exploring data augmentation techniques leveraging the proposed approach.
2020.acl-main.170.txt,8 Conclusions,2020,future research directions include adaptive dropout rates for different merges and an in-depth analysis of other pathologies in learned token embeddings for different segmentations.
2020.acl-main.171.txt,6 Discussion,2020,there are several open questions to investigate: are the benefits of monolingual data orthogonal to other techniques like iterative refinement?
2020.acl-main.171.txt,6 Discussion,2020,we will consider these research directions in future work.
2020.acl-main.175.txt,6 Conclusions,2020,"in the future, we would like to model aspects and sentiment more explicitly as well as apply some of the techniques presented here to unsupervised single-document summarization."
2020.acl-main.177.txt,9 Discussion and Conclusion,2020,our analyses contain two ideas that may be useful for future studies of systematicity.
2020.acl-main.177.txt,9 Discussion and Conclusion,2020,we believe these are a novel ideas that can be employed in future studies.
2020.acl-main.178.txt,6 Conclusion,2020,we hope these findings bring attention to the feasibility of employing statistical natural language processing machinery as tools for exploring human cognition.
2020.acl-main.179.txt,8 Discussion,2020,future work needs to be done to understand more fully what biases are present in the data and learned by language models.
2020.acl-main.180.txt,5 Discussion,2020,conversational context may make a speaker’s intended message easier or harder to recover from ambiguous acoustics.
2020.acl-main.180.txt,5 Discussion,2020,the study suggests several directions for future work.
2020.acl-main.180.txt,5 Discussion,2020,"third, a more sophisticated channel model would allow for insertions and deletions, and better capture transitional coarticulatory cues (wright 2004)."
2020.acl-main.180.txt,5 Discussion,2020,"this supports the hypothesis that variation and structure in natural languages are shaped not only by pressures for eﬃcient signals, but also pressures for eﬀective communication of the speaker’s intended message in the face of noise and uncertainty (lindblom 1990, lindblom et al.1995, hall et al.2018)."
2020.acl-main.181.txt,6 Conclusion,2020,such studies will be crucial to achieve a complete computational understanding of natural language syntax.
2020.acl-main.182.txt,7 Conclusions,2020,we hope the dataset we release will be used to benchmark future dialog system uncertainty research.
2020.acl-main.183.txt,5 Discussion and Conclusion,2020,"one natural extension would be to generalize these findings to other skills than the three addressed here, such as humor/wit, eloquence, image commenting, etc."
2020.acl-main.184.txt,6 Conclusion and Future Work,2020,"conceptflow models conversation structure explicitly as transitions in the latent concept space, in order to generate more informative and meaningful responses."
2020.acl-main.184.txt,6 Conclusion and Future Work,2020,"in future, we plan to explore how to combine knowledge with pre-trained language models, e.g."
2020.acl-main.186.txt,5 Conclusion and Future Work:,2020,"as future work, we are extending the proposed approach and test its efficacy on real human conversations."
2020.acl-main.186.txt,5 Conclusion and Future Work:,2020,"more broadly, we continue to explore strategies that combine semantic parsing and neural networks for frame generation."
2020.acl-main.187.txt,7 Conclusions and Future Work,2020,"future work can explore improving the correction models, leveraging logs of natural language feedback to improve text-to-sql parsers, and expanding the dataset to include multiple turns of correction."
2020.acl-main.190.txt,6 Discussion,2020,extending expbert to other natural language tasks where this relationship might not hold is an open problem that would entail finding different ways of interpreting an explanation with respect to the input.
2020.acl-main.190.txt,6 Discussion,2020,we outline two such avenues of future work.
2020.acl-main.191.txt,5 Conclusion,2020,"from a linguistic perspective, it is worth investigating what the generator encodes in the produced representations."
2020.acl-main.191.txt,5 Conclusion,2020,"moreover, we will investigate the potential impact of the adversarial training directly in the bert pre-training."
2020.acl-main.192.txt,6 Conclusion,2020,future directions include (1) devising hierarchical span representations that can handle spans of different length and diverse content more effectively and efficiently; (2) robust multitask learning or meta-learning algorithms that can reconcile very different tasks.
2020.acl-main.192.txt,6 Conclusion,2020,we merge 8 datasets into our glad benchmark for evaluating future models for natural language analysis.
2020.acl-main.194.txt,6 Conclusion,2020,"for future direction, we plan to explore the effectiveness of mixtext in other nlp tasks such as sequential labeling tasks and other real-world scenarios with limited labeled data."
2020.acl-main.196.txt,5 Conclusion,2020,"thus, we encourage future research into obtaining tighter bounds on latent lm perplexity, possibly by using more powerful proposal distributions that consider entire documents as context, or by considering methods such as annealed importance sampling."
2020.acl-main.196.txt,5 Conclusion,2020,we investigate the application of importance sampling to evaluating latent language models.
2020.acl-main.197.txt,6 Conclusion,2020,we will explore this direction as future work.
2020.acl-main.198.txt,6 Conclusion,2020,alternative architectures that can overcome the stolen probability effect are an item of future work.
2020.acl-main.199.txt,5 Conclusion,2020,"in the future, we would like to figure out different strategies to merge individual gains, obtained by separate application of the dag constraint, into a setup that can take the best of both precision and recall improvements, and put forth a better performing system."
2020.acl-main.199.txt,5 Conclusion,2020,we also plan on looking into strategies to improve recall of the constructed taxonomy.
2020.acl-main.2.txt,6 Conclusion,2020,future work can further investigate temporal patterns in how language used by depressed people evolves over the course of an interaction.
2020.acl-main.20.txt,5 Conclusion,2020,"as future work, we plan to extend our qag model to a meta-learning framework, for generalization over diverse datasets."
2020.acl-main.201.txt,6 Conclusion and Discussion,2020,"therefore, future work should focus on downstream tasks instead of bli."
2020.acl-main.201.txt,6 Conclusion and Discussion,2020,we leave the exploration of non-linear projections to future work.
2020.acl-main.203.txt,6 Conclusion,2020,our findings point to future research opportunities to build stealthy authorship obfuscation methods.
2020.acl-main.204.txt,5 Conclusions and Future Work,2020,"there are a few interesting questions left unanswered in this paper, which would provide interesting future research directions: (1) deebert’s training method, while maintaining good quality in the last off-ramp, reduces model capacity available for intermediate off-ramps; it would be important to look for a method that achieves a better balance between all off-ramps.(2) the reasons why some transformer layers appear redundant2 and why dee-bert considers some samples easier than others remain unknown; it would be interesting to further explore relationships between pre-training and layer redundancy, sample complexity and exit layer, and related characteristics."
2020.acl-main.206.txt,6 Conclusion,2020,"additional experiments indicated that the removal of filled pauses from the transcriptions was beneficial to the scoring models, and that scoring performance is best for the middle grades of the scoring range."
2020.acl-main.206.txt,6 Conclusion,2020,"therefore future work could include different sampling methods, generation of synthetic data, or training objectives which reward models which are less conservatively drawn to the middle of the scoring scale."
2020.acl-main.207.txt,8 Conclusions and Future Work,2020,another item of future work is to develop better multitask approaches to leverage multiple signals of relatedness information during training.
2020.acl-main.207.txt,8 Conclusions and Future Work,2020,including other information such as outgoing citations as additional input to the model would be yet another area to explore in future.
2020.acl-main.207.txt,8 Conclusions and Future Work,2020,it would be interesting to initialize our model weights from more recent transformer models to investigate if additional gains are possible.
2020.acl-main.208.txt,7.3 Code Piece Error Analysis,2020,"to help the readers understand the bottleneck for code piece generation and point out important future directions, we randomly sampled 200 “hard” lines and manually analyzed why the generation fails by looking at the top 1 candidate of the model."
2020.acl-main.21.txt,7 Conclusion,2020,"finally, performing sqg on other types of inputs, e.g., images and knowledge graphs, is an interesting topic."
2020.acl-main.21.txt,7 Conclusion,2020,"for future works, the major challenge is generating more meaningful, informative but concise questions."
2020.acl-main.211.txt,6 Discussion and Future Work,2020,"however, a host of other options could be considered in future work."
2020.acl-main.213.txt,5 Conclusions,2020,"we improve the generalization of autoregressive predictive coding by multi-target training of future prediction lf and past memory reconstruction lr, where the latter serves as a regularization."
2020.acl-main.216.txt,5 Conclusions,2020,"finally, we will evaluate the multiresolution loss on larger datasets to analyze it’s regularizing effects."
2020.acl-main.216.txt,5 Conclusions,2020,"furthermore, we will experiment with more ellaborate, attention-based fusion mechanisms."
2020.acl-main.216.txt,5 Conclusions,2020,"in the future, we plan to alleviate this by incorporating ideas from sparse transformer variants (kitaev et al., 2020; child et al., 2019)."
2020.acl-main.218.txt,7 Conclusion,2020,we plan to continue our data-driven approach for grounded conversations by expanding our dataset through our iterative data collection process with other larger text-based open-domain dialogue corpora and extend our work to model and collect longer conversations exhibiting more complex improv-backed turns.
2020.acl-main.219.txt,6 Conclusion,2020,"the next challenge will also be to combine this engagingness with other skills, such as world knowledge (antol et al., 2015) relation to personal interests (zhang et al., 2018), and task proficiency."
2020.acl-main.219.txt,6 Conclusion,2020,"while our human evaluations were on short conversations, initial investigations indicate the model as is can extend to longer chats, see appendix g, which should be studied in future work."
2020.acl-main.220.txt,5 Conclusion,2020,"since it provides immediate continuous rewards and at the singlestep level, maude can be also be used to optimize and train better dialogue generation models, which we want to pursue as future work."
2020.acl-main.222.txt,6 Discussion,2020,"future work should consider adding these tasks to the ones used here, while continuing the quest for improved models."
2020.acl-main.223.txt,5 Conclusion,2020,"and finally, we would like to adapt the model for automatic poetry translation—as we feel that the constraint-based approach lends itself perfectly to a poetry translation model that is able to adhere to an original poem in both form and meaning."
2020.acl-main.223.txt,5 Conclusion,2020,"first of all, we would like to experiment with different neural network architectures."
2020.acl-main.223.txt,5 Conclusion,2020,"secondly, we would like to incorporate further poetic devices, especially those based on meaning."
2020.acl-main.223.txt,5 Conclusion,2020,we conclude with a number of future research avenues.
2020.acl-main.224.txt,6 Conclusion,2020,future work will validate the effectiveness of this method on more varied data-to-text generation tasks.
2020.acl-main.224.txt,6 Conclusion,2020,"this serialized plan is more compatible with the output sequence, making the information alignment between the input and output easier."
2020.acl-main.225.txt,8 Conclusion,2020,"in future work, we plan to incorporate these features into co-creation systems which assist humans in the writing process."
2020.acl-main.225.txt,8 Conclusion,2020,"we hope that our work encourages more investigation of infilling, which may be a key missing element of current writing assistance tools."
2020.acl-main.226.txt,5 Conclusions and Outlook,2020,"we propose a framework called inset to decouple three aspects of the task (understanding, planning, and generation) and address them in a unified manner."
2020.acl-main.227.txt,7 Conclusions,2020,the guider network provides a plan-ahead mechanism for next-word selection.
2020.acl-main.228.txt,6 Conclusion and Future Work,2020,"by performing analysis on gigaword, we find that there exists room to improve summarization performance with better post-ranking algorithms, a promising direction for future research."
2020.acl-main.228.txt,6 Conclusion and Future Work,2020,"moving forward, we would like to apply this framework to other retrieve-and-edit based generation scenarios such as dialogue, conversation, and code generation."
2020.acl-main.229.txt,6 Discussion,2020,"for instance, developing agents that are robust to variations in both visual appearance and instruction descriptions is an important next step."
2020.acl-main.229.txt,6 Discussion,2020,there are a few future directions to pursue.
2020.acl-main.230.txt,6 Conclusions and Future Work,2020,"in the future, we aim to extend our framework to extract events from videos, and make it scalable to new event types."
2020.acl-main.230.txt,6 Conclusions and Future Work,2020,"we will also apply our extraction results to downstream applications including cross-media event inference, timeline generation, etc."
2020.acl-main.231.txt,8 Discussion,2020,"future work might explore methods for incorporating richer learned representations both of the diverse visual observations in videos, and the narration that describes them, into such models."
2020.acl-main.231.txt,8 Discussion,2020,we hope that future work will continue to evaluate broadly.
2020.acl-main.232.txt,7 Conclusion and Future Work,2020,"in the future, richer representations of the dialogue history (e.g.by using bert (devlin et al., 2019) or of past builder actions) combined with de-noising of the human data and perhaps more exhaustive data augmentation should produce better output sequences."
2020.acl-main.234.txt,6 Conclusion and Related Work,2020,"studying this type of difference between expressive models and their less expressive, restricted variants remains an important direction for future work."
2020.acl-main.235.txt,6 Conclusions and Future Work,2020,further study in this direction may be interesting.
2020.acl-main.236.txt,5 Conclusion,2020,"we hope this work inspires future research on better understanding the differences between embedding methods, and on designing simpler and more efficient models."
2020.acl-main.237.txt,8 Conclusion,2020,a potential future research direction is to bridge the gap between this simple bootstrapping paradigm and the incorporation of user free-form responses to allow the system to handle free-text responses.
2020.acl-main.238.txt,6 Conclusion,2020,"in the future, we would like to jointly learn discrete representations of entities as well as relations."
2020.acl-main.238.txt,6 Conclusion,2020,our approaches learn to represent entities in a kg as a vector of discrete codes in an end-to-end fashion.
2020.acl-main.240.txt,6 Conclusion,2020,"future work could find additional modular uses of mlms, simplify maskless pll computations, and use plls to devise better sentence- or document-level scoring metrics."
2020.acl-main.242.txt,5 Conclusion and Future Directions,2020,"the theoretical underpinnings of our poscal idea are not explored in detail here, but developing formal statistical support for these ideas constitutes interesting future work."
2020.acl-main.243.txt,10 Conclusion,2020,another direction is to improve the sources of weak supervision and such as interactive new constraints provided by users.
2020.acl-main.243.txt,10 Conclusion,2020,"finally, it would be interesting to explore alternative training methods for these models, such as reducing reliance on hard sampling through better relaxations of structured models."
2020.acl-main.243.txt,10 Conclusion,2020,induction of grounded control states opens up many possible future directions for this work.
2020.acl-main.245.txt,6 Discussion,2020,additional related work.
2020.acl-main.245.txt,6 Discussion,2020,"gong et al.(2019) apply a spellcorrector to correct typos chosen to create ambiguity as to the original word, but these typos are not adversarially chosen to fool a model."
2020.acl-main.245.txt,6 Discussion,2020,"other attack surfaces involving insertion of sentences (jia and liang, 2017) or syntactic rearrangements (iyyer et al., 2018) are harder to pair with roben, and are interesting directions for future work."
2020.acl-main.245.txt,6 Discussion,2020,"using context is not fundamentally at odds with the idea of robust encodings, and making contextual encodings stable is an interesting technical challenge and a promising direction for future work."
2020.acl-main.245.txt,6 Discussion,2020,we hope our work inspires new task-agnostic robust encodings that lead to more robust and more accurate models.
2020.acl-main.247.txt,7 Conclusion and Future Work,2020,"in future work, we will address end-to-end question answering with pre-training for both the answer selection and retrieval components."
2020.acl-main.248.txt,4 Conclusion and Future Work,2020,our future work is to include the paragraph representation in the constraint prediction model.
2020.acl-main.249.txt,7 Conclusion,2020,"we hope that this work makes clear the necessity for asserting the genuineness of pre-trained weights, just like there exist similar mechanisms for establishing the veracity of other pieces of software."
2020.acl-main.251.txt,6 Conclusion,2020,"in the future, we seek to expand upon energy-based translation using our method."
2020.acl-main.252.txt,7 Conclusion and Future Directions,2020,"future work should explore techniques like iterative back-translation (hoang et al., 2018) for further improvement and scaling to larger model capacities and more languages (arivazhagan et al., 2019b; huang et al., 2019) to maximize transfer across languages and across data sources."
2020.acl-main.253.txt,6 Conclusions,2020,"in the future, we plan to investigate more thoroughly the use of language models for evaluating fluency, the effect of domain mismatch in the choice of monolingual data, and ways to generalize this study to other applications beyond mt."
2020.acl-main.255.txt,6 Conclusion,2020,we leave it as future work to explore ways to raise accuracy on unseen synsets without harming performances on frequent synsets.
2020.acl-main.257.txt,7 Conclusion and Future Work,2020,in future work we will investigate more sophisticated neural (sub-)networks within the proposed framework.
2020.acl-main.257.txt,7 Conclusion and Future Work,2020,"we will also apply the idea of functionspecific training to other interrelated linguistic phenomena and other languages, probe the usefulness of function-specific vectors in other language tasks, and explore how to integrate the methodology with sequential models."
2020.acl-main.259.txt,7 Conclusions and Future Work,2020,"directionality of edges did not result in improvement in our models in this work, however for future, we plan to develop gcns that incorporate edge typing, which would enable us to differentiate between different mwe types and dependency relations while comparing them against the current models."
2020.acl-main.259.txt,7 Conclusions and Future Work,2020,"for future work, we plan to add vmwe annotations to the vu amsterdam corpus (steen, 2010) which is the largest metaphor dataset and extend our experiments using that resource."
2020.acl-main.260.txt,5 Conclusion,2020,recently bias in embeddings has attracted much attention.
2020.acl-main.260.txt,5 Conclusion,2020,we hope this study can work as a foundation to motivate future research about the analysis and mitigation of bias in multilingual embeddings.
2020.acl-main.261.txt,5 Moving Forward,2020,looking to other scientific disciplines that have faced similar issues in the past may provide some guidance for our future.
2020.acl-main.261.txt,5 Moving Forward,2020,"one could consider this as part of an extension of data statements, in requiring that all code/model releases associated with acl papers be accompanied with a structured risk assessment of some description, and if risk is found to exist, some management plan be put in place."
2020.acl-main.264.txt,7 Conclusion,2020,"however, a comprehensive study is required to prove the conjecture and we leave this as future work."
2020.acl-main.264.txt,7 Conclusion,2020,"however, our analysis and the mitigation framework is general and can be adopted to other applications and other types of bias."
2020.acl-main.265.txt,6 Conclusion,2020,"additionally, future work should further probe the source of gender bias in the model’s predictions, perhaps by visualizing attention or looking more closely at the model’s outputs."
2020.acl-main.265.txt,6 Conclusion,2020,"in our study, we create and publicly release wiki-genderbias: the first dataset aimed at evaluating bias in nre models."
2020.acl-main.265.txt,6 Conclusion,2020,we encourage future work to dive deeper into this problem.
2020.acl-main.265.txt,6 Conclusion,2020,"we only consider binary gender, but future work should consider non-binary genders."
2020.acl-main.265.txt,6 Conclusion,2020,"while these findings will help future work avoid gender biases, this study is preliminary."
2020.acl-main.267.txt,6 Conclusion and Future Work,2020,"first, we plan to explore why the model prefers “soft” attentions rather than “hard” ones, which is different from the findings in several prior works based on hard attention."
2020.acl-main.267.txt,6 Conclusion and Future Work,2020,"in our future work, we will explore several potential directions."
2020.acl-main.267.txt,6 Conclusion and Future Work,2020,"second, we plan to study how to model the differences on the characteristics of different samples and use different pooling norms, which may have the potential to further improve our approach."
2020.acl-main.267.txt,6 Conclusion and Future Work,2020,"third, we will explore how to generalize our approach to other modalities, such as images, audios and videos, to see whether it can facilitate more attention-based methods."
2020.acl-main.268.txt,6 Conclusion,2020,"in future work, apart from improving the similarity measures, it could be examined to predict mtl scores or estimate the right amount of auxiliary data or shared parameters in the neural network."
2020.acl-main.269.txt,6 Conclusion,2020,"future directions include validating our findings on other san architectures (e.g., bert (devlin et al., 2019)) and more general attention models (bahdanau et al., 2015; luong et al., 2015)."
2020.acl-main.27.txt,8 Conclusion,2020,"as future work, we will extend our framework to more complex contexts by devising efficient learning algorithms."
2020.acl-main.270.txt,8 Conclusion,2020,"by showing that sublayer ordering can improve models at no extra cost, we hope that future research continues this line of work by looking into optimal sublayer ordering for other tasks, such as translation, question answering, and classification."
2020.acl-main.273.txt,5 Conclusion,2020,"finally, we will apply our model into other multi-modal tasks such as multi-modal sentiment analysis."
2020.acl-main.273.txt,5 Conclusion,2020,"in the future, we plan to incorporate attributes of visual objects and dependency trees to enrich the multi-modal graphs."
2020.acl-main.278.txt,7 Conclusion,2020,"as well-calibrated confidence estimation is more likely to establish trustworthiness with users, we plan to apply our work to interactive machine translation scenarios in the future."
2020.acl-main.279.txt,4 Conclusion,2020,"in the future, we plan to enable the glyph and phonetic variation detection by integrating the variation graph representation learning, which may improve signal’s performance."
2020.acl-main.28.txt,5 Conclusion and Future Work,2020,"in the future, we plan to apply the sa framework on syntactic parse trees in hopes of generating more syntactically different sentences (motivated by our case study)."
2020.acl-main.280.txt,6 Conclusion,2020,"in the future, we plan to study complicated situations such as a law case with multiple defendants and charges."
2020.acl-main.283.txt,7 Conclusion,2020,"as recent works explore the superiority of hyperbolic space to euclidean space for serval natural language processing tasks, we intend to couple with the hyperbolic neural networks (ganea et al., 2018b) and the hyperbolic word embedding method such as poincare´glove (tifrea et al., 2019) in the future."
2020.acl-main.284.txt,7 Conclusion,2020,"our future research direction includes a thorough study of differences in this dataset with actual tickets, and potential for transfer."
2020.acl-main.285.txt,6 Conclusion and Future Work,2020,promising future directions include: 1) utilize more types of data from mooccube to facilitate existing topics; 2) employ advanced models in existing tasks; 3) more innovative nlp application tasks in online education domain.
2020.acl-main.289.txt,6 Conclusion and Future Work,2020,"finally, it would be interesting to study the semantic roles of emotion (bostan et al., 2020), which considers the full structure of an emotion expression and is more challenging."
2020.acl-main.289.txt,6 Conclusion and Future Work,2020,"in future work, we shall explore the following directions."
2020.acl-main.289.txt,6 Conclusion and Future Work,2020,"our approach effectively models inter-clause relationships to learn clause representations, and integrates relative position enhanced clause pair ranking into a unified neural network to extract emotioncause pairs in an end-to-end fashion."
2020.acl-main.29.txt,6 Conclusion and Future Work,2020,"s-lstm is agnostic as to the sentence encoder used, so we would like to investigate the potential usefulness of transformer-based language models as sentence encoders."
2020.acl-main.29.txt,6 Conclusion and Future Work,2020,we would also like to investigate the usefulness of an unconsidered source of document structure: the hierarchical nature of sections and subsections.
2020.acl-main.291.txt,6 Conclusion,2020,"third, we extend the standard bilstm classifier to fully integrate the external knowledge by adding a novel knowledge-aware memory unit to the bilstm cell."
2020.acl-main.292.txt,7 Conclusion,2020,"we introduced a domain-adversarial framework called kingdom, which relies on an external commonsense kb (conceptnet) to perform unsupervised domain adaptation."
2020.acl-main.293.txt,7 Conclusion and Future work,2020,the proposed srd allows the aspect sentiment classifier to focus on critical sentiment words which modify the target aspect term through dependency-based structure.
2020.acl-main.293.txt,7 Conclusion and Future work,2020,the substantial improvements highlight the under-performance of recent contextualized embedding models in “understanding” syntactical features and suggests future directions in developing more syntax-learning contextualized embeddings.
2020.acl-main.296.txt,5 Conclusion,2020,"for future works, we will explore pair-wise at and ot extraction together with aspect category and sentiment polarity classification."
2020.acl-main.298.txt,8 Conclusion,2020,"for future work, we aim to develop a universal model to handle both tree and non-tree arguments."
2020.acl-main.30.txt,8 Conclusions and Future Work,2020,"currently, we perform coarse- and fine-grained classifications separately."
2020.acl-main.30.txt,8 Conclusions and Future Work,2020,"in the future, we are interested in generalizing contextualized weak supervision to hierarchical text classification problems."
2020.acl-main.300.txt,4 Discussion,2020,we make the following recommendations for future experiments on unsupervised constituency parsing.
2020.acl-main.302.txt,6 Conclusions,2020,we propose to batchify the inside algorithm to accommodate gpus.
2020.acl-main.303.txt,7 Discussion,2020,"finally, future work should investigate whether data augmentation can fully bridge the gap between low-bias learners and structured tree lstms, and whether our conclusions apply to other syntactic phenomena besides agreement."
2020.acl-main.303.txt,7 Discussion,2020,future work should further explore both of these approaches.
2020.acl-main.303.txt,7 Discussion,2020,"in future work, these syntactic limitations may be overcome by giving the model typed dependencies (which would distinguish between a subject-verb dependency and a verb-object dependency)."
2020.acl-main.303.txt,7 Discussion,2020,it seems particularly promising to explore alternative formulations of the dependency lstm (as mentioned above) and the effect of learning embeddings of non-terminal symbols for the constituency lstm.
2020.acl-main.303.txt,7 Discussion,2020,"one might expect the head-lexicalized model to perform the best, since it can leverage both syntactic formalisms."
2020.acl-main.303.txt,7 Discussion,2020,our conclusions about the importance of explicit mechanisms for representing syntactic structure can be strengthened by developing different formulations of the tree lstms.
2020.acl-main.303.txt,7 Discussion,2020,the first approach is to use models that have explicit mechanisms for representing syntactic structure.
2020.acl-main.303.txt,7 Discussion,2020,"with this approach, it is possible to bring the syntactic performance of less-structured models closer to that of models with explicit tree structure, even with an augmentation set generated simply and easily using a pcfg."
2020.acl-main.306.txt,5 Conclusion,2020,there are several future directions for this work.
2020.acl-main.306.txt,5 Conclusion,2020,"therefore, our next step is to enhance umt so as to dynamically filter out the potential noise from images."
2020.acl-main.307.txt,8 Conclusion,2020,"we trained stock embeddings for the task of binary classification of stock price movements on two different datasets, the wsj and r&b."
2020.acl-main.308.txt,5 Conclusion and Future Work,2020,"finally, we plan to experiment with other languages."
2020.acl-main.308.txt,5 Conclusion and Future Work,2020,"in future work, we plan to perform user profiling with respect to polarizing topics such as gun control (darwish et al., 2020), which can then be propagated from users to media (atanasov et al., 2019; stefanov et al., 2020)."
2020.acl-main.308.txt,5 Conclusion and Future Work,2020,"we further want to model the network structure, e.g., using graph embeddings (darwish et al., 2020)."
2020.acl-main.309.txt,7 Discussion and Conclusion,2020,alleviating this restriction is an important future direction.
2020.acl-main.310.txt,8 Conclusion,2020,"in this paper, we conducted a thorough study to evaluate the robustness of language encoders against grammatical errors."
2020.acl-main.310.txt,8 Conclusion,2020,this study shed light on understanding the behaviors of language encoders against grammatical errors and encouraged future work to enhance the robustness of these models.
2020.acl-main.312.txt,6 Conclusions,2020,there are some future directions that are worth exploring.
2020.acl-main.313.txt,5 Conclusion,2020,"in future work, we plan to extend r-men for multi-hop knowledge graph reasoning."
2020.acl-main.315.txt,4 Conclusion,2020,"furthermore, the word-aligned attention can also be applied to english plms to bridge the semantic gap between the whole word and the segmented word-piece tokens, which we leave for future work."
2020.acl-main.317.txt,5 Conclusion,2020,"in further work, we will explore more efficient ways for constructing the perturbation set."
2020.acl-main.317.txt,5 Conclusion,2020,"we also plan to generalize our approach to achieve certified robustness against other types of adversarial attacks in nlp, such as the out-of-list attack."
2020.acl-main.318.txt,7 Conclusion,2020,"in the future, we will consider combining our method with graph neural networks to update the word graphs we build."
2020.acl-main.319.txt,7 Conclusion,2020,"furthermore, we notice some exceptional cases which we call as “reinforced samples”, which we leave as the future work."
2020.acl-main.32.txt,5 Conclusion,2020,"in the future, we would like to devise a nonparametric neural topic model based on adversarial training."
2020.acl-main.321.txt,5 Conclusions,2020,"in this work, we explore the solutions to improve the uni-encoder structures for document-level machine translation."
2020.acl-main.324.txt,8 Conclusion and Future Work,2020,"in the future, we intend to extend the work to include language types such as asian languages."
2020.acl-main.324.txt,8 Conclusion and Future Work,2020,we will also introduce other effective methods to improve zero-shot translation quality.
2020.acl-main.326.txt,5 Conclusions,2020,"furthermore, we believe that a better understanding of the links between exposure bias and well-known translation problems will help practitioners decide when sequence-level training objectives are especially promising, for example in settings where the test domain is unknown, or where hallucinations are a common problem."
2020.acl-main.326.txt,5 Conclusions,2020,"our findings are pertinent to the academic debate how big of a problem exposure bias is in practice – we find that this can vary substantially depending on the dataset –, and they provide a new justification for sequence-level training objectives that reduce or eliminate exposure bias."
2020.acl-main.327.txt,6 Conclusion,2020,"in future work, we will work around the problem of evaluation errors in the low da range."
2020.acl-main.327.txt,6 Conclusion,2020,we also investigated why our proposed method worked poorly in the other conditions and found the importance of tlm training.
2020.acl-main.328.txt,9 Conclusions,2020,a practical line of future work is embedding our plotting agent in interactive environments such as jupyter lab.
2020.acl-main.328.txt,9 Conclusions,2020,future work includes methods that get closer to human performance on the dataset.
2020.acl-main.329.txt,6 Conclusion,2020,"all the datasets used in the gluecos benchmark are publicly available, and we plan to make the nli dataset available for research use."
2020.acl-main.329.txt,6 Conclusion,2020,"in future work, we would like to experiment with a multi-task setup wherein tasks with less training data can significantly benefit from those having abundant labelled data, since most code-switched datasets are often small and difficult to annotate."
2020.acl-main.329.txt,6 Conclusion,2020,we would like to add more diverse tasks and language pairs to the gluecos benchmark in a future version.
2020.acl-main.33.txt,5 Conclusion,2020,"in future work, we will examine semantic relations between class labels in the auxiliary task."
2020.acl-main.33.txt,5 Conclusion,2020,"moreover, we will adapt our model to text generation tasks."
2020.acl-main.33.txt,5 Conclusion,2020,"we expect that our model will encourage a generation model to generate texts with different labels, such as styles, have distinct representations, which will result in class specific expressions."
2020.acl-main.331.txt,6 Conclusion and Discussion,2020,"besides news recommendation, the mind dataset can also be used in other natural language processing tasks such as topic classification, text summarization and news headline generation."
2020.acl-main.331.txt,6 Conclusion and Discussion,2020,"in addition, besides the click behaviors, we plan to incorporate other user behaviors such as read and engagement to support more accurate user modeling and performance evaluation."
2020.acl-main.331.txt,6 Conclusion and Discussion,2020,"in the future, we plan to extend the mind dataset by incorporating image and video information in news as well as news in different languages, which can support the research of multi-modal and multi-lingual news recommendation."
2020.acl-main.332.txt,8 Conclusions and Future Work,2020,"in future work, we plan to extend this work to more datasets and to more languages."
2020.acl-main.332.txt,8 Conclusions and Future Work,2020,"we further want to go beyond textual claims, and to take claimimage and claim-video pairs as an input."
2020.acl-main.333.txt,6 Conclusion,2020,it is our hope the proposed holistic metrics may pave the way towards the comparability of open-domain dialogue models.
2020.acl-main.334.txt,5 Conclusion and Future Work,2020,"future work includes i) improving projection learning to model complicated linguistic properties of hypernymy; ii) extending our model to address other tasks, such as graded lexical entailment (vulic et al., 2017) and cross-lingual graded lexical entailment (vulic et al., 2019); and iii) exploring how deep neural language models (such as bert (devlin et al., 2019), transformer-xl (dai et al., 2019), xlnet (yang et al., 2019)) can improve the performance of hypernymy detection."
2020.acl-main.335.txt,7 Conclusion,2020,"for future work, an extrinsic evaluation of our methods is needed to prove the effectiveness of learned biomedical entity representations and to prove the quality of the entity normalization in downstream tasks."
2020.acl-main.337.txt,6 Conclusion and Future Work,2020,"however, several studies have expanded the method for acquiring a word vector to account for the uncertainty of word meanings and word polysemy (e.g., athiwaratkun et al.2018)."
2020.acl-main.337.txt,6 Conclusion and Future Work,2020,"thus, in the future, we will take the uncertainty, polysemy, and context sensitivity of the word meanings and the frequency of words into account and explore better ways of modeling the word-class distributions in semantic vector spaces."
2020.acl-main.338.txt,6 Conclusion,2020,"in our future work, we would like to improve the performance of the asc task by using unlabeled data since our graph-based neural network approach is easy to add unlabeled data."
2020.acl-main.338.txt,6 Conclusion,2020,"moreover, we would like to apply our approach to other sentiment analysis tasks, e.g., aspect-oriented opinion summarization and multi-label emotion detection."
2020.acl-main.339.txt,6 Conclusion,2020,"in the future, we will develop a syntax-based multi-scale graph convolutional network to deal with both short and long aspects."
2020.acl-main.34.txt,6 Conclusion and Future Works,2020,"in future work, we will investigate the impact of fine-grained word categories (such as nouns, verbs, and adjectives) on the translation performance and design specific methods according to these categories."
2020.acl-main.341.txt,6 Conclusion,2020,"for future work, we will extend sentibert to other applications involving phrase-level annotations."
2020.acl-main.342.txt,7 Conclusion,2020,"in the future, one possible direction is creating complete graphs with their nodes being input clauses to achieve full coverage."
2020.acl-main.343.txt,7 Conclusion,2020,"in the future, we will further explore the connection between multimodal analysis and multi-task learning and incorporate more fusion strategy, including early- and middle-fusion."
2020.acl-main.343.txt,7 Conclusion,2020,we hope that the introduction of ch-sims will provide a new perspective for researches on multi-modal analysis.
2020.acl-main.344.txt,5 Conclusion and Future Work,2020,"besides, we expect the idea of curriculum pre-training can be adopted on other nlp tasks."
2020.acl-main.344.txt,5 Conclusion and Future Work,2020,"in the future, we will explore how to leverage unlabeled speech data and large bilingual text data to further improve the performance."
2020.acl-main.345.txt,8 Summary,2020,we will investigate this direction in future work.
2020.acl-main.346.txt,6 Conclusion,2020,"in future work, we intend to explore the idea of self-training for parsing written texts."
2020.acl-main.347.txt,4 Conclusion,2020,"the experiments show that our proposed framework is capable of providing high-quality representations of lattices, yielding consistent improvement on slu tasks."
2020.acl-main.348.txt,6 Conclusion,2020,"finally, we will explore further the generability of our meta-transfer learning approach to more downstream multilingual tasks in our future work."
2020.acl-main.350.txt,7 Conclusion,2020,"for future work, we will design more flexible policies to achieve better translation quality and lower delay in simultaneous spoken language translation."
2020.acl-main.350.txt,7 Conclusion,2020,"in this work, we developed simulspeech, an end-to-end simultaneous speech to text translation system that directly translates source speech into target text concurrently."
2020.acl-main.350.txt,7 Conclusion,2020,we further introduced several techniques including data-level and attention-level knowledge distillation to boost the accuracy of simulspeech.
2020.acl-main.350.txt,7 Conclusion,2020,we will also investigate simultaneous translation from the speech in a source language to the speech in a target.
2020.acl-main.351.txt,7 Future work,2020,"we plan to do a detailed analysis along two lines: 1) comparing if the proposed modeling technique can help bridge gap between predicted and human annotations, and 2) effect of environment variables e.g., background noise, speaker features, different languages etc."
2020.acl-main.353.txt,7 Conclusion,2020,an exciting synthesis would incorporate deception and language generation into an agent’s policy; our data would help train such agents.
2020.acl-main.355.txt,6 Conclusions and Future Work,2020,"as a potential direction for future work, it would be interesting to investigate the use of the ema technique on transformer models as well and conduct similar studies to examine needless architectural complexity in other nlp tasks."
2020.acl-main.356.txt,5 Conclusion and Future Work,2020,"in the future, we plan to extend the applicability of the presented model to other linguistics tasks as well as recommendations and medical inference tasks."
2020.acl-main.357.txt,6 Conclusions,2020,future works include applying such scoring method on broader classification tasks like natural language inference and sentiment analysis.
2020.acl-main.358.txt,5 Conclusion and Future Work,2020,"in the future, we plan to extend the key insights of segmenting features and facilitating interactions to other representation learning problems."
2020.acl-main.359.txt,7 Conclusions and Future Work,2020,"finally, we intend to analyse the effect of these measures in a wider range of language pairs and settings, in order to propose a more general solution."
2020.acl-main.359.txt,7 Conclusions and Future Work,2020,"in the future, we plan to investigate ways to directly incorporate the rescoring metrics into the data selection process itself, so that penalising similar sentences can also be taken into account."
2020.acl-main.359.txt,7 Conclusions and Future Work,2020,we also aim to conduct a human evaluation of the translated sentences in order to obtain a better understanding of the effects of data selection and backtranslation on the overall quality.
2020.acl-main.36.txt,6 Conclusion,2020,"in the future, we will extend the investigation on the functionalities of the encoder and decoder to other sequence-to-sequence tasks such as text summarization and text style transfer to explore more applications of our model."
2020.acl-main.360.txt,6 Conclusion,2020,"in future work, we suggest performing a hyperparameter search over possible values for t in slt pruning (i.e., the number of training steps that are not discarded during model reset), and over si for the switch from slt to mp in slt-mp."
2020.acl-main.360.txt,6 Conclusion,2020,"in slt-mp, slt pruning first discards 60% of all parameters, so mp can focus on fine-tuning the model for maximum accuracy."
2020.acl-main.361.txt,5 Conclusion and Future Work,2020,"as future work, we plan to extend our method to other nlp tasks which rely on evidence finding, such as natural language inference."
2020.acl-main.362.txt,6 Conclusion,2020,"for future work, we aim to consider more complex relationships among the quantities and other attributes to enrich quantity representations further."
2020.acl-main.362.txt,6 Conclusion,2020,we will also explore adding heuristic in the tree-based decoder to guide and improve the generation of solution expression.
2020.acl-main.363.txt,6 Conclusions,2020,code to compute cem will be available at github.com/evallteam/evalltoolkit.
2020.acl-main.363.txt,6 Conclusions,2020,future work includes the application of cem at scales other than the ordinal.
2020.acl-main.364.txt,6 Conclusion,2020,we leave this as an avenue for future work.
2020.acl-main.365.txt,7 Conclusion,2020,"in the future, we plan to investigate whether usage representations can provide an even finer grained account of lexical meaning and its dynamics, e.g., to automatically discriminate between different types of meaning change."
2020.acl-main.366.txt,6 Conclusion,2020,"thus, it is not limited to resource-rich languages and can be used by any applications that operate on text."
2020.acl-main.368.txt,6 Conclusion,2020,"in future work, we want to investigate bertram’s potential benefits for such frequent words."
2020.acl-main.368.txt,6 Conclusion,2020,"we have introduced bertram, a novel architecture for inducing high-quality representations for rare words in bert’s and roberta’s embedding spaces."
2020.acl-main.369.txt,7 Conclusions,2020,"as future work, we plan to refine our approach by exploiting other strategies for weighting the words in the clusters and to leverage them for automatically building multilingual sense-tagged corpora."
2020.acl-main.370.txt,5 Conclusion and Future Work,2020,"in the future, we would like to investigate the application of our theory in these domain adaptation tasks."
2020.acl-main.371.txt,5 Conclusion,2020,"finally, detecting the more implicit relations between the argument and the key point, as seen in our error analysis, is another intriguing direction for future work."
2020.acl-main.371.txt,5 Conclusion,2020,"in addition, we plan to apply the methods presented in this work also to automatically-mined arguments."
2020.acl-main.371.txt,5 Conclusion,2020,the natural next step for this work is the challenging task of automatic key point generation.
2020.acl-main.371.txt,5 Conclusion,2020,we proposed to represent such summaries as a set of key points scored according to their relative salience.
2020.acl-main.372.txt,7 Conclusion,2020,"future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
2020.acl-main.373.txt,7 Conclusion,2020,"in the future, we plan to develop more complex models to be added in the next stages of the cascade classifier as well as automatically identify irony, gender stereotypes and sexist vocabulary."
2020.acl-main.374.txt,7 Conclusion,2020,"in the future, we hope to apply skep on more sentiment analysis tasks, to further see the generalization of skep, and we are also interested in exploiting more types of sentiment knowledge and more fine-grained sentiment mining methods."
2020.acl-main.374.txt,7 Conclusion,2020,sentiment masking and three sentiment pre-training objectives are designed to incorporate various types of knowledge for pre-training model.
2020.acl-main.375.txt,7 Conclusion and Future Work,2020,"for future work, besides seeking a deeper understanding of the interplay of linguistic factors and tree shape, we want to explore probes that combine the distance and depth assumptions into a single transformation, rather than learning separate probes and combining them post-hoc, as well as methods for alleviating treebank supervision altogether."
2020.acl-main.381.txt,5 Conclusion,2020,in future we will sample target models with a larger number of plausible combinations of factors.
2020.acl-main.381.txt,5 Conclusion,2020,in future work we hope to further disentangle these differences.
2020.acl-main.381.txt,5 Conclusion,2020,in this systematic study of analysis methods for neural models of spoken language we offered some suggestions on best practices in this endeavor.
2020.acl-main.382.txt,5 Summary and Outlook,2020,"future work will focus on developing more advanced procedures for detecting inconsistencies, and on building robust models that do not generate inconsistencies."
2020.acl-main.383.txt,8 Discussion & Conclusion,2020,"we leave it for future work to use our technique to test other linguistic properties (e.g., coreference) and to extend our study to more downstream tasks and systems."
2020.acl-main.384.txt,5 Conclusion,2020,"future work should investigate where these primitive referential abilities stem from and how they can be fostered in future architectures and training setups for language modeling, and neural models more generally."
2020.acl-main.384.txt,5 Conclusion,2020,"we find that the two models behave similarly, but the transformer performs consistently better (around 10% higher accuracy in the probe tasks).8 future work should test other architectures, like cnn-based lms and lstms with attention, to provide additional insights into the linguistic capabilities of language models."
2020.acl-main.385.txt,5 Conclusion,2020,"since in transformer decoder, future tokens are masked, naturally there is more attention toward initial tokens in the input sequence, and both attention rollout and attention flow will be biased toward these tokens."
2020.acl-main.387.txt,7 Conclusion & Future work,2020,"as future work, we would like to extend our analysis and proposed techniques to more complex models and downstream tasks."
2020.acl-main.388.txt,6 Conclusion,2020,"in the future, it would be fruitful to develop a novel weighting strategy for the tchebycheff procedure."
2020.acl-main.39.txt,8 Discussion,2020,"despite promising initial results, our model is still unable to extrapolate perfectly for harder tasks."
2020.acl-main.39.txt,8 Discussion,2020,"this is a bottleneck for extrapolation, suggesting that removing this heuristic is key to reaching perfect extrapolation and should be investigated in future work."
2020.acl-main.39.txt,8 Discussion,2020,we hope that this paper acts as a reminder that extrapolation is a hard setting that has not been much investigated by the machine learning community.
2020.acl-main.390.txt,7 Conclusion,2020,additional use cases like the training of personalized recommendation models as well as the use of reinforcement learning to find a good trade-off between system and user objective remain to be investigated in future work.
2020.acl-main.391.txt,7 Conclusion,2020,"in future work, we will investigate whether bert-init can be used effectively by using methods to deal with catastrophic forgetting."
2020.acl-main.393.txt,7 Conclusion,2020,we hope to address this problem with a completely semantic-based approach in the future.
2020.acl-main.394.txt,7 Conclusion,2020,"for instance, we expect that abuse detection may also benefit from joint learning with complex semantic tasks, such as figurative language processing and inference."
2020.acl-main.395.txt,5 Conclusion,2020,future adoptions to fuse will include the integration of a dialog component.
2020.acl-main.395.txt,5 Conclusion,2020,"it will be interesting to see, if we can reuse (or transfer) the machine learning models as well as the rest of the approach."
2020.acl-main.395.txt,5 Conclusion,2020,we plan to evaluate fuse in other domains.
2020.acl-main.396.txt,4 Conclusions and Future Work,2020,"a limitation of our work is that we considered a narrow contextual context, comprising only the previous comment and the discussion title.11 it would be interesting to investigate in future work ways to improve the annotation quality when more comments in the discussion thread are provided, and also if our findings hold when broader context is considered (e.g., all previous comments in the thread, or the topic of the thread as represented by a topic model)."
2020.acl-main.396.txt,4 Conclusions and Future Work,2020,our experiments and datasets provide an initial foundation to investigate these important directions.
2020.acl-main.398.txt,7 Conclusion,2020,"in future work we aim to extend the model to represent a database with multiple tables as context, and to effectively handle large tables."
2020.acl-main.399.txt,7 Conclusion,2020,"combining target inference with stance classification in future work, we can already generate basic conclusions, say, “raising the school leaving age is good”."
2020.acl-main.40.txt,6 Conclusion and Future Work,2020,"and the deeper msc model results in high computational overhead, to address this issue, we would like to apply the average attention network (zhang et al., 2018a) to our deep msc models."
2020.acl-main.40.txt,6 Conclusion and Future Work,2020,"in the future, we would like to extend our model to extremely large datasets, such as wmt’14 english-to-french with about 36m sentence-pairs."
2020.acl-main.400.txt,4 Conclusion,2020,"in future work, we would like to investigate the effectiveness of our model in these tasks."
2020.acl-main.401.txt,6 Conclusion,2020,"along with investigating new techniques, we hope that assembling a bigger curated dataset with quality annotations will help in better performance."
2020.acl-main.402.txt,6 Conclusion and Future Work,2020,"in future, conversation history, speaker information, fine-grained modality encodings can be incorporated to predict da with more accuracy and precision."
2020.acl-main.403.txt,7 Conclusion,2020,"in the future, we plan to study more in depth the stylistic and figurative devices used for parody, extend the data set beyond the political case study and explore human behavior regarding parody, including how this is detected and diffused through social media."
2020.acl-main.405.txt,6 Conclusion,2020,"more generally, stereotypes exist along a network of beliefs (freeman and ambady, 2011) reflecting unwarranted correlations between many dimensions (ridgeway, 2011); we must therefore be careful not to expect that removing meaning along one dimension will expel social biases from our models."
2020.acl-main.406.txt,7 Conclusion and Future Work,2020,"from the application perspective, it is clear that the graph contains more information than we have exploited so far."
2020.acl-main.407.txt,7 Discussion,2020,"as we argued that compositionality has, after all, desirable properties, future work could adapt methods for learning disentangled representations (e.g., higgins et al., 2017; kim and mnih, 2018) to let (more) compositional languages emerge."
2020.acl-main.407.txt,7 Discussion,2020,"it is thus worth exploring the link between the area of language emergence and that of representation learning (bengio et al., 2013)."
2020.acl-main.408.txt,7 Conclusions and Future Directions,2020,eraser is intended to facilitate progress on explainable models for nlp.
2020.acl-main.408.txt,7 Conclusions and Future Directions,2020,"our hope is that eraser enables future work on designing more interpretable nlp models, and comparing their relative strengths across a variety of tasks, datasets, and desired criteria."
2020.acl-main.409.txt,9 Conclusions,2020,we view these as interesting directions for future work.
2020.acl-main.414.txt,6 Conclusion,2020,"in addition of improving qa, we hypothesize that these simple unsupervised components of air will benefit future work on supervised neural iterative retrieval approaches by improving their query reformulation algorithms and termination criteria."
2020.acl-main.417.txt,11 Conclusions,2020,we are especially interested in further extending this work into low resource languages where resources tend to be noisier and underlying models to support data mining less reliable.
2020.acl-main.418.txt,6 Discussion and Moving Forward,2020,"in particular, the gender taxonomy we presented, while not novel, is (to our knowledge) previously unattested in discussions around gender bias in nlp systems; we hope future work in this area can draw on these ideas."
2020.acl-main.418.txt,6 Discussion and Moving Forward,2020,"more broadly, we found that trans-exclusionary assumptions around gender in nlp papers is made commonly (and implicitly), a practice that we hope to see change in the future because it fundamentally limits the applicability of nlp systems."
2020.acl-main.418.txt,6 Discussion and Moving Forward,2020,"we also emphasize that while we endeavored to be inclusive, our own positionality has undoubtedly led to other biases."
2020.acl-main.418.txt,6 Discussion and Moving Forward,2020,we hope this paper can serve as a roadmap for future studies.
2020.acl-main.419.txt,8 Conclusion,2020,"our research not only results in insights into significant similarities between bidirectional rnns and human attention, but also opens the avenue for promising future research directions."
2020.acl-main.422.txt,7 Conclusion,2020,"an exciting direction for future work is to combine the two approaches in order to identify which linguistic properties are captured in model components that are similar to one another, or explicate how localization of information contributes to the learnability of particular properties."
2020.acl-main.422.txt,7 Conclusion,2020,"finally, the similarity analysis may also help improve model efficiency, for instance by pointing to components that do not change much during fine-tuning and can thus be pruned."
2020.acl-main.424.txt,7 Conclusion,2020,"finally, we hope that asset’s multi-transformation features will motivate the development of ss models that benefit a variety of target audiences according to their specific needs such as people with low literacy or cognitive disabilities."
2020.acl-main.425.txt,6 Conclusions,2020,"furthermore, we presented a methodology to extend the resource by fine-tuning vlp, a state-of-the-art pre-trained language-vision architecture."
2020.acl-main.425.txt,6 Conclusions,2020,"in this work we introduced babelpic, a new resource for language-vision tasks, built by validating the existing image-to-synset associations in the babelnet resource."
2020.acl-main.428.txt,5 Conclusion,2020,"future work could apply this same technique with other supervised data, e.g.correcting causal or commonsense reasoning errors (zellers et al., 2019; qin et al., 2019)."
2020.acl-main.430.txt,5 Conclusions,2020,"we speculate that the primary path’s attribution diminishes with the length of the context, which would suggest that at some context size, the handling of number will devolve to be mostly heuristic-like with no significant primary paths."
2020.acl-main.433.txt,6 Conclusion,2020,"future work will explore other measures and alternative game settings for the emergence of compositionality, as well as more subtle psychological effects (categeorical perception) of continuous biological systems exhibiting discrete structure, like the auditory system."
2020.acl-main.434.txt,10 Conclusion,2020,"overall, these results help to clarify the patterns of distribution of context information within contextual embeddings— future work can further clarify the impact of more diverse syntactic relations between words, and of additional types of word features."
2020.acl-main.439.txt,5 Discussion,2020,future work should explore the extent to which our model could further benefit from initializing with stronger models and what computational challenges may arise.
2020.acl-main.439.txt,5 Discussion,2020,"in future work, we hope to better understand how a discourse model can also learn fine-grained relationship types between sentences from unlabeled data."
2020.acl-main.440.txt,8 Conclusion,2020,we also envision extending this work by including audio and video features to enhance the quality of our alignment algorithm.
2020.acl-main.441.txt,7 Discussion & Conclusion,2020,future work could explore a detailed cost and time trade-off between adversarial and static collection.
2020.acl-main.441.txt,7 Discussion & Conclusion,2020,"luckily, if the benchmark does turn out to saturate quickly, we will always be able to collect a new round."
2020.acl-main.441.txt,7 Discussion & Conclusion,2020,"we provided inference labels for the development set, opening up possibilities for interesting more fine-grained studies of nli model performance."
2020.acl-main.443.txt,7 Conclusion,2020,"in this work, we investigated the task of named entity recognition in the social computer programming domain."
2020.acl-main.444.txt,7 Conclusions,2020,"in the future, we are interested in investigating the generality of our defined schema for other comedies and different conversational registers, identifying the temporal intervals when relations are valid (surdeanu, 2013) in a dialogue, and joint dialogue-based information extraction as well as its potential combinations with multimodal signals from images, speech, and videos."
2020.acl-main.445.txt,6 Conclusion and Future Work,2020,"in the future, we will investigate multi-document summarization datasets such as duc (paul and james, 2004) and tac (dang and owczarzak, 2008) to see whether our findings coincide when multiple references are provided."
2020.acl-main.448.txt,6 Conclusion,2020,"thus, future evaluations should also measure correlations after removing outlier systems."
2020.acl-main.449.txt,5 Conclusion,2020,"in our future work, we want to study the effective incorporation of code structure into the transformer and apply the techniques in other software engineering sequence generation tasks (e.g., commit message generation for source code changes)."
2020.acl-main.450.txt,9 Conclusion,2020,"additionally, incorporating a content selection mechanism to focus the generated questions on salient facts is a promising direction."
2020.acl-main.450.txt,9 Conclusion,2020,"the framework we present is general, and extending it to other conditional text generation tasks such as image captioning or machine translation is a promising directions."
2020.acl-main.451.txt,6 Conclusion,2020,"for future work, we will explore better graph encoding methods, and apply discourse graphs to other tasks that require long document encoding."
2020.acl-main.453.txt,6 Discussion and Conclusion,2020,"in future work, we plan to experiment more with this, examining how we can combine constituents to make fluent sentences without including potentially irrelevant context."
2020.acl-main.453.txt,6 Discussion and Conclusion,2020,"we would also like to further experiment with abstractive summarization to re-examine whether large, pre-trained language models (liu and lapata, 2019) can be improved for our domain."
2020.acl-main.455.txt,7 Conclusions and Future Work,2020,"in the future, we intend to investigate different meaning representation formalisms, such as amr (banarescu et al., 2013) and dynamic syntax (kempson et al., 2001) and extend to other datasets (e.g.multiplereference summarization) and tasks (e.g.response generation in dialogue)."
2020.acl-main.458.txt,8 Conclusion,2020,we hope that our work draws the community’s attention to the factual correctness issue of abstractive summarization models and inspires future work in this direction.
2020.acl-main.459.txt,6 Conclusion and Future Work,2020,"we also hope that the dataset can be added to in the future with multi-modal extractions, more granular annotations, and deeper mining of the wiki."
2020.acl-main.46.txt,5 Conclusion and Future Work,2020,the second direction is towards extending the approach to morphologically rich languages.
2020.acl-main.46.txt,5 Conclusion and Future Work,2020,two orientations can be identified for future work.
2020.acl-main.464.txt,6 Conclusions,2020,a crucial direction of future work is to develop richer ways of capturing scholarly impact.
2020.acl-main.466.txt,5 Conclusion,2020,"in the future, for these existing tasks, researchers can focus on solving the three most pressing challenges of legalai combining embedding-based and symbol-based methods."
2020.acl-main.466.txt,5 Conclusion,2020,"in this paper, we describe the development status of various legalai tasks and discuss what we can do in the future."
2020.acl-main.467.txt,6 Conclusion and Future Work,2020,future work in this area would benefit greatly from improvements to both the breadth and depth of available probing tasks.
2020.acl-main.467.txt,6 Conclusion and Future Work,2020,"intermediate-task training may help improve the handling of syntax, but there is little to no correlation between target-task and probing-task performance for these skills."
2020.acl-main.467.txt,6 Conclusion and Future Work,2020,our results therefore suggest a need for further work on efficient transfer learning mechanisms.
2020.acl-main.469.txt,5 Conclusion and Future Work,2020,"for future work, it would be interesting to see if more linguistically-inspired phenomena can be systematically found in cross-modal models."
2020.acl-main.47.txt,7 Conclusion and Future work,2020,"since lms are language-agnostic, analyzing word order in another language with the lm-based method would also be an interesting direction to investigate."
2020.acl-main.47.txt,7 Conclusion and Future work,2020,"we plan to further explore the capability of lms on other linguistic phenomena related to word order, such as “given new ordering” (nakagawa, 2016; asahara et al., 2018)."
2020.acl-main.470.txt,6 Discussion and Future Work,2020,future work could bolster the measure’s usefulness in several ways.
2020.acl-main.470.txt,6 Discussion and Future Work,2020,"however, the method’s efficacy in the present setting is likely boosted by the relative uniformity of crisis counseling conversations; and future work could aim to better accomodate settings with less structure and more linguistic variability."
2020.acl-main.470.txt,6 Discussion and Future Work,2020,"the preliminary explorations in section 5.4 could also be extended to gauge the causal effects of counselors’ behaviors (kazdin, 2007)."
2020.acl-main.470.txt,6 Discussion and Future Work,2020,"we expect balancing problems to recur in conversational settings beyond crisis counseling, such as court proceedings, interviews, debates and other mental health contexts like long-term therapy."
2020.acl-main.470.txt,6 Discussion and Future Work,2020,"with such improvements, it would be interesting to study other domains where interlocutors are faced with conversational challenges."
2020.acl-main.473.txt,7 Conclusion,2020,"for example, we observe that skilled forecasters are more open-minded and exhibit a higher level of uncertainty about future events."
2020.acl-main.477.txt,4 Conclusion,2020,it is designed to be challenging where state-of-the-art nlp models still struggle at ≈ 60%.
2020.acl-main.478.txt,8 Conclusion,2020,"in the future, we will further improve the performance of news discourse profiling by investigating subgenres of news articles, and extensively explore its usage for various other nlp tasks and applications."
2020.acl-main.479.txt,7 Conclusion and future work,2020,"considering the importance of context in drawing both scalar and other inferences in communication (grice, 1975; clark, 1992; bonnefon et al., 2009; zondervan, 2010; bergen and grodner, 2012; goodman and stuhlmu¨ller, 2013; degen et al., 2015), the development of appropriate representations of larger context is an exciting avenue for future research."
2020.acl-main.48.txt,6 Conclusion,2020,we will explore model generalization in the future work.
2020.acl-main.480.txt,5 Conclusion,2020,"we discussed several future directions, including data augmentation for downstream transferability, applicability of pretrained encoders to discourse, and utilizing larger discourse contexts."
2020.acl-main.481.txt,6 Conclusion and Future Work,2020,"in future work, we plan to extend this work to longer documents such as the recently released dataset of bamman et al.(2019)."
2020.acl-main.483.txt,7 Conclusion & Future Work,2020,"for example, this post from the ghc requires background information and reasoning across sentences in order to classify as offensive or prejudiced: “donald trump received much criticism for referring to haiti, el salvador and africa as ‘shitholes’."
2020.acl-main.483.txt,7 Conclusion & Future Work,2020,"future work includes direct extension and validation of this technique with other language models such as gpt-2 (radford et al., 2019); experimenting with other hate speech or offensive language datasets; and experimenting with these and other sets of identity terms."
2020.acl-main.483.txt,7 Conclusion & Future Work,2020,"in this work, we effectively applied this technique to hate speech classifiers biased towards group identifiers; future work can determine the effectiveness and further potential for this technique in other tasks and contexts."
2020.acl-main.483.txt,7 Conclusion & Future Work,2020,regularizing soc explanations of group identifiers tunes hate speech classifiers to be more context-sensitive and less reliant on high-frequency words in imbalanced training sets.
2020.acl-main.483.txt,7 Conclusion & Future Work,2020,we hope that the present work can motivate more attempts to inject more structure into hate speech classification.
2020.acl-main.484.txt,6 Conclusion,2020,"while we have shown that this method significantly reduces gender bias while preserving quality, we hope that this work encourages further research into debiasing along other dimensions of word embeddings in the future."
2020.acl-main.487.txt,7 Discussion and Conclusion,2020,future work is required to study if our ﬁndings carry over to other languages and cultural contexts.
2020.acl-main.490.txt,7 Conclusions,2020,"in future work, we intend to expand the coverage of clams by incorporating language-specific and non-binary phenomena (e.g., french subjunctive vs. indicative and different person/number combinations, respectively), and by expanding the typological diversity of our languages."
2020.acl-main.490.txt,7 Conclusions,2020,"this issue could be mitigated in the future with architectural changes to neural lms (such as better handling of morphology), more principled combinations of languages (as in dhar and bisazza 2020), or through explicit separation between languages during training (e.g., using explicit language ids)."
2020.acl-main.492.txt,7 Conclusion,2020,"finally, we are interested in exploring how these types of explanations are actually interpreted by users, and whether providing them actually establishes trust in predictive systems."
2020.acl-main.492.txt,7 Conclusion,2020,"future work might explore how rankings induced over training instances by influence functions can be systematically analyzed in a stand-alone manner (rather than in comparison with interpretations from other methods), and how these might be used to improve model performance."
2020.acl-main.493.txt,7 Discussion,2020,future work could extend this analysis to include quantitative results on the extent of agreement with ud.
2020.acl-main.493.txt,7 Discussion,2020,"future work should explore other multilingual models like xlm and xlm-roberta (lample and conneau, 2019) and attempt to come to an understanding of the extent to which the properties we’ve discovered have causal implications for the decisions made by the model, a claim our methods cannot support."
2020.acl-main.495.txt,7 Conclusion,2020,"we encourage future work to judge model interpretability using the proposed evaluation and publicly published annotations, and explore techniques for improving faithfulness and interpretability in compositional models."
2020.acl-main.496.txt,8 Conclusion,2020,"furthermore, our method is agnostic to the underlying nature of the two objects being aligned and can therefore align disparate objects such as images and captions, enabling a wide range of future applications within nlp and beyond."
2020.acl-main.497.txt,5 Conclusion,2020,we believe that in future they can be used more directly to yield better performance gains.
2020.acl-main.5.txt,6 Conclusion,2020,annotations complement for multiwoz dataset in the future might enable dst-sc to handle the related-slot problem more effectively and further improve the joint accuracy.
2020.acl-main.50.txt,7 Conclusion and Future Work,2020,"ideally, we would like to automatically identify such polarizing topics."
2020.acl-main.50.txt,7 Conclusion and Future Work,2020,"in future work, we plan to increase the number of topics that we use to characterize media."
2020.acl-main.500.txt,5 Conclusion,2020,"possible future directions include a systematic study of different aspects of qg diversity (e.g., lexical and factual) and controlled diversification of individual aspects in generation."
2020.acl-main.501.txt,7 Conclusions,2020,"a future direction is to extend this work to question answering tasks that require reasoning over multiple documents, e.g., open-domain qa."
2020.acl-main.503.txt,6 Discussion,2020,"while we focus on question answering, our framework is general and extends to any prediction task for which graceful handling of out-of-domain inputs is necessary."
2020.acl-main.504.txt,6 Conclusions and Future Work,2020,"in future work, we plan to explore techniques to automatically learn where to place intermediate classifiers, and what drop ratio to use for each one of them."
2020.acl-main.506.txt,6 Conclusion,2020,we hope that this work provides a complementary picture of hypothesis assessment techniques for the field and encourages more rigorous reporting trends.
2020.acl-main.507.txt,6 Discussion,2020,"our results demonstrate the promise of our annotation framework and dataset in supporting a wide range of reading behavior analyses, as well as the feasibility of developing automated question validation tools for reading comprehension examinations for humans as exciting directions for future work."
2020.acl-main.512.txt,5 Conclusion,2020,"apart from that, we also plan to design novel models to perform the related tasks of entity extraction and aspect extraction from comparative sentences."
2020.acl-main.512.txt,5 Conclusion,2020,our future work aims to improve the cpc performance further.
2020.acl-main.514.txt,6 Conclusions,2020,exploring the space of subsets of our preprocessing factors might yield more interesting combinations; we leave this for future work.
2020.acl-main.514.txt,6 Conclusions,2020,"interestingly, incorporating preprocessing into word representations appears to be far more beneficial than applying it in a downstream task to classification datasets."
2020.acl-main.515.txt,5 Conclusion and Future Work,2020,"although conkadi has achieved a notable performance, there is still much room to improve.1) while ats2smmi is behind our conkadi, we find mmi can effectively enhance the ats2s; hence, in the future, we plan to verify the feasibility of the re-ranking technique for knowledge-aware models.2) we will continue to promote the integration of high-quality knowledge, including more types of knowledge and a more natural integration method."
2020.acl-main.516.txt,5 Conclusion and Future Work,2020,"in the future, we plan to extend our approach to improve the consistency of multi-turn dialogues."
2020.acl-main.517.txt,6 Conclusion,2020,"we propose cmaml, which is able to customize unique dialogue models for different tasks."
2020.acl-main.518.txt,5 Conclusions,2020,"despite using gpt-2 models, our framework can be extended with other language models and similarly adopted to improve other multi-modal dialogues."
2020.acl-main.519.txt,6 Conclusion,2020,"in the future, we would like to explore variants of the model architecture."
2020.acl-main.52.txt,6 Conclusion,2020,"besides, with the support of curriculum learning, it can be more efficient."
2020.acl-main.521.txt,8 Conclusions and Discussion,2020,"this general observation may be of independent interest beyond openie, such as in text summarization."
2020.acl-main.522.txt,5 Conclusion,2020,ekd forces the student model to learn open-domain trigger knowledge from teacher model by mimicking the predicted results of the teacher model.
2020.acl-main.523.txt,4 Conclusion,2020,"using a larger amount of general domain texts to build pre-trained representations (peters et al., 2018; radford et al., 2018; devlin et al., 2019; clark et al., 2020) can complement with our model and is one of the directions that we plan to take in future work."
2020.acl-main.524.txt,5 Conclusion,2020,we have investigated a multi-cell compositional lstm structure for cross-domain ner under the multi-task learning strategy.
2020.acl-main.53.txt,7 Conclusion,2020,"from this result, we propose that tackling dst with our proposed problem definition is a promising future research direction."
2020.acl-main.530.txt,5 Conclusions,2020,"in future work, we plan to investigate translation of other discourse phenomena that may benefit from the use of future context."
2020.acl-main.530.txt,5 Conclusions,2020,"in this paper, we have investigated the use of future context for nmt and particularly for pronoun translation."
2020.acl-main.532.txt,4 Discussions,2020,"for future work, following the work on automatic identification of translationese (rabinovich and wintner, 2015; rubino et al., 2016), we plan to investigate the impact of tagging translationese texts inside parallel training data, such as parallel sentences collected from the web."
2020.acl-main.535.txt, 5 Conclusion ,2020,"in future work, we intend to adapt our editor module for other learning tasks with both the structured input and structured output."
2020.acl-main.537.txt, 6 Future work ,2020,"these promising results point to future works in (1) linearizing the speed-speedup curve; (2) extending this approach to other pre-training architectures such as xlnet (yang et al., 2019) and elmo (peters et al., 2018); (3) applying fastbert on a wider range of nlp tasks, such as named entity recognition and machine translation."
2020.acl-main.538.txt, 4 Conclusion and Future Work ,2020,"in the future, evaluation by automatically executing generated code with test cases could be a better way to assess code generation results."
2020.acl-main.54.txt,7 Conclusion,2020,this also demonstrates the power of large-scale pre-trained language models to be adopted for building end-to-end goal-oriented dialogue systems.
2020.acl-main.540.txt, 6 Conclusion and Future Work ,2020,"in the future, we will try to increase the robustness gains of adversarial training and consider utilizing sememes in adversarial defense model."
2020.acl-main.542.txt, 6 Conclusion ,2020,"in the future, we look forward to extend cl strategy to the pretraining stage, and guide deep models like transformer from a language beginner to a language expert."
2020.acl-main.543.txt, 6 Conclusion ,2020,we hope that our work will be useful in future research for realizing more advanced models that are capable of appropriately performing arbitrary inferences.
2020.acl-main.545.txt, 6 Conclusion and Future Work ,2020,"in the future, we will explore more diverse and advanced paraphrase expanding methods for both sentence and paragraph level qg."
2020.acl-main.545.txt, 6 Conclusion and Future Work ,2020,"moreover, we will apply our methods to other similar tasks, such as sentence simplification."
2020.acl-main.546.txt, 6 Conclusions ,2020,"for future works, to further improve the method, we will explore the introduction of additional information, such as rules and external texts."
2020.acl-main.549.txt,7 Conclusion,2020,"a potential solution is to jointly learn evidence selection and claim verification model, which we leave as a future work."
2020.acl-main.55.txt,5 Conclusion,2020,"in the future, we will provide labels that indicate “why this candidate is false” for false candidates in our test set, so that one can easily detect weak points of systems through error analysis."
2020.acl-main.550.txt, 8 Conclusion and Future Work ,2020,"in future work, we will consider introducing more information like the citation texts to the cited paper in other papers to help the generation."
2020.acl-main.553.txt, 6 Conclusion ,2020,"furthermore, our models have achieved the best results on cnn/dailymail compared with non-bert-based models, and we will take the pretrained language models into account for better encoding representations of nodes in the future."
2020.acl-main.555.txt, 5 Conclusion ,2020,"in the future we would like to explore other more informative graph representations such as knowledge graphs, and apply them to further improve the summary quality."
2020.acl-main.556.txt, 5 Conclusion and Future Work ,2020,"in the future, we will introduce more tasks like document ranking to supervise the learning of the multi-granularity representations for further improvement."
2020.acl-main.557.txt, 5 Conclusion,2020,"we hope that this formulation can be useful as a simple and low-overhead way of integrating syntax into any neural nlp model, including for multi-task training and to predict syntactic annotations during inference."
2020.acl-main.558.txt, 5 Conclusions and future work ,2020,"this dataset, named mlqe, has been released to the research community3 and will be used for the wmt20 shared task on quality estimation.4 in future work, we will test the partial input hypothesis on this data."
2020.acl-main.559.txt, 8 Conclusions,2020,we hope that the paradigm presented here will help provide coherence to such efforts.
2020.acl-main.560.txt, 6 Conclusion,2020,pertinent questions should be posed to authors of future publications about whether their proposed language technologies extend to other languages.
2020.acl-main.561.txt, 6 Conclusions,2020,this analysis suggests that an important next step in deep learning architectures for natural language understanding will be the induction of entities.
2020.acl-main.562.txt, 7 The Final Frontier – An Outlook ,2020,"however, many questions regarding the future of corpus querying still remain, two of which we consider of particular importance and will discuss in the following sections.7.1 one language to query them all?"
2020.acl-main.562.txt, 7 The Final Frontier – An Outlook ,2020,navigating this ocean in order to find the right tool for the job and then learn to use it can already be as much effort as manually investigating the data at hand.
2020.acl-main.562.txt, 7 The Final Frontier – An Outlook ,2020,"while this effort is still in an early stage, we are looking forward to having catalogs available in the not too distant future, allowing us to browse for query languages based on our individual information needs."
2020.acl-main.562.txt, 7 The Final Frontier – An Outlook ,2020,"with several dozens of systems contributing their individual variations, the pool of available corpus query tools and languages has become quite large."
2020.acl-main.563.txt, 6 Conclusion,2020,we will explore it in the future.
2020.acl-main.564.txt, 5 Conclusion ,2020,"future work will investigate other data manipulation techniques (e.g., data synthesis), which can be further integrated to improve the performance."
2020.acl-main.565.txt, 5 Conclusion,2020,"besides, our model can quickly adapt to a new domain with little annotated data."
2020.acl-main.567.txt, 8 Conclusions and Future Work,2020,designing a new model to address these problems may be our future work.
2020.acl-main.568.txt, 8 Conclusion and Future Work,2020,"first, we can make ssrem more robust on adversarial attacks."
2020.acl-main.568.txt, 8 Conclusion and Future Work,2020,there are several future directions to improve ssrem.
2020.acl-main.568.txt, 8 Conclusion and Future Work,2020,"third, we can extend using ssrem to various conversation corpora such as task-oriented dialogues."
2020.acl-main.568.txt, 8 Conclusion and Future Work,2020,we will apply ssrem to various conversation tasks for evaluating the generated text automatically.
2020.acl-main.568.txt, 8 Conclusion and Future Work,2020,we will explore these directions in our future work.
2020.acl-main.568.txt, 8 Conclusion and Future Work,2020,we will make ssrem more robust on the attacks.
2020.acl-main.569.txt, 5 Conclusion,2020,"in the future work, we will focus on more effective discourse parsing with additional carefully designed features and joint learning with edu segmentation."
2020.acl-main.57.txt,5 Conclusion and Future Work,2020,"in the future, we plan to use more powerful encoders and evaluate our methods on real dialog data."
2020.acl-main.570.txt, 7 Conclusion and Future Work ,2020,future work aims at enhancing sequence feature extraction methods to improve the classification performance as those suffer from low accuracy.
2020.acl-main.572.txt, 5 Conclusion and Future Work ,2020,our modeling method is general and should apply to other typeoriented tasks.
2020.acl-main.573.txt, 5 Conclusion and Future Work ,2020,"for future work, how to combine open relation learning and continual relation learning together to complete the pipeline for emerging relations still remains a problem, and we will continue to work on it."
2020.acl-main.576.txt, 6 Conclusion and Future Work ,2020,"in the future, we should further leverage the internal relations in the candidate end, and try to introduce rich medical background knowledge into our work."
2020.acl-main.58.txt,5 Conclusion,2020,we hope to provide new guidance for the future slot tagging work.
2020.acl-main.580.txt, 8 Conclusion and Future Work ,2020,"in future work, we hope to tackle repeated fields and learn domainspecific candidate generators."
2020.acl-main.580.txt, 8 Conclusion and Future Work ,2020,"we are also actively investigating how our learned candidate representations can be used for transfer learning to a new domain and, ultimately, in a few-shot setting."
2020.acl-main.581.txt, 5 Conclusion,2020,"we also propose a language similarity measuring method based on language identification, to better weight different teacher models."
2020.acl-main.583.txt, 7 Conclusions and Future Work ,2020,future work should study this additional relation in the context of caption annotation and generation.
2020.acl-main.583.txt, 7 Conclusions and Future Work ,2020,"however, by studying the examples in the other category, we discovered an additional coherence relation that exists between an image and caption, in which the caption identifies an object or entity in the image–identification."
2020.acl-main.583.txt, 7 Conclusions and Future Work ,2020,the presented work has limitations that can be addressed in future research.
2020.acl-main.584.txt, 5 Discussion and Conclusion ,2020,"future work should look at more complex fusion strategies, possibly coupled with bottom-up recalibration mechanisms (zarrieß and schlangen, 2016; mojsilovic, 2005) to further enhance colour classification under difficult illumination conditions."
2020.acl-main.586.txt, 6 Conclusion,2020,we hope that ref-hard and ref-adv will foster more research in this area.
2020.acl-main.588.txt, 6 Conclusion,2020,"in future work, we can further improve our method in the following aspects."
2020.acl-main.588.txt, 6 Conclusion,2020,we plan to employ an edgeaware graph neural network considering the edge labels.
2020.acl-main.589.txt, 6 Conclusion,2020,"in the future, we would like to extend our work to make a syntactically-aware window that can automatically learn tree (or phrase) structures."
2020.acl-main.59.txt,7 Conclusion,2020,"as future work, we will apply madpl in the more complex dialogs and verify the role-aware reward decomposition in other dialog scenarios."
2020.acl-main.594.txt, 7 Conclusion,2020,"the pattern of training robust systems on data that has been augmented by the knowledge captured in symbolic systems could be applied to areas outside of morphological analysis, and is a promising avenue of future exploration."
2020.acl-main.594.txt, 7 Conclusion,2020,this work represents a successful first iteration of a process whereby the morphological model can be continually improved.
2020.acl-main.596.txt, 7 Conclusion and Future Work ,2020,"combined with the ability to process infixation and reduplication, our system improves access for geographically diverse low-resource languages."
2020.acl-main.596.txt, 7 Conclusion and Future Work ,2020,future work will aim to extend the current model to capture particularly challenging morphological patterns such as templatic non-concatenative morphology and polysynthetic composition.
2020.acl-main.596.txt, 7 Conclusion and Future Work ,2020,"our next step will be to attempt to automate the determination of language typology, yielding somewhat better performance with a system requiring no human intervention per language at all."
2020.acl-main.598.txt, 6 Conclusion,2020,"by substituting the current transducers in our pipeline, we expect that we will be able to improve the overall performance of our system."
2020.acl-main.598.txt, 6 Conclusion,2020,"in the future, we will explore the following directions: (i) a difficult challenge for our proposed system is to correctly determine the paradigm size."
2020.acl-main.598.txt, 6 Conclusion,2020,"since transfer across related languages has shown to be beneficial for morphological tasks (jin and kann, 2017; mccarthy et al., 2019; anastasopoulos and neubig, 2019, inter alia), future work could use typologically aware priors to guide the number of paradigm slots based on the relationships between languages.(ii) we plan to explore other methods, like word embeddings, to incorporate context information into our feature function.(iii) we aim at developing better performing string transduction models for the morphological inflection step."
2020.acl-main.599.txt, 6 Conclusion,2020,improving our graph structure of representing the document as well as the document-level pretraining tasks is our future research goals.
2020.acl-main.6.txt,5 Conclusion,2020,"in future work, we plan to analyze each turn of dialogue with reinforcement learning architecture, and to enhance the diversity of the whole dialogue by avoiding knowledge reuse."
2020.acl-main.601.txt, 6 Conclusions and Future Works,2020,we will investigate the robustness and scalability of the model.
2020.acl-main.602.txt, 6 Conclusions,2020,"for modeling, we plan to explore recent advances in conditional language models for jointly modeling qa with generating their derivations."
2020.acl-main.602.txt, 6 Conclusions,2020,one immediate future work is to evaluate state-of-the-art rc systems’ internal reasoning on our dataset.
2020.acl-main.606.txt, 7 Conclusion and Future Work ,2020,"for intended meaning, we investigate how grammatical errors affect the understanding of sentences as well as how grammatical error correction (gec) can contribute to the parsing."
2020.acl-main.606.txt, 7 Conclusion and Future Work ,2020,"future research may involve tailoring existing parsers to learner data, combining literal and intended meanings in a unified framework, evaluating gec models in terms of speakers’ intention and parsing for other languages."
2020.acl-main.606.txt, 7 Conclusion and Future Work ,2020,we establish parallel meaning representations by combining the complementary strengths of knowledge-intensive erg-licensed analysis and dependency tree annotations through a new reranking model.
2020.acl-main.607.txt, 6 Conclusion,2020,this points to future directions of applying our model to low-resource languages and cross-domain settings.
2020.acl-main.611.txt, 6 Conclusion and Future Work ,2020,we leave adjusting our model to different kinds of lattice or graph as our future work.
2020.acl-main.612.txt, 6 Conclusion,2020,"fgs2ee first uses the word embeddings of semantic type words to generate semantic embeddings, and then combines them with existing entity embeddings through linear aggregation."
2020.acl-main.612.txt, 6 Conclusion,2020,"for the future work, we are planning to extract fine-grained semantic types from unlabelled documents and use the relatedness between the finegrained types and contexts as distant supervision for entity linking."
2020.acl-main.616.txt, 6 Conclusion and Future Work ,2020,"in the future, it is necessary to interpret the semantics that transformer layers in different depths can convey, which is beneficial for the computing-efficiency."
2020.acl-main.617.txt, 6 Conclusion,2020,"future directions for this work include exploring other tasks that might benefit from hyperbolic geometry, such as hypernym detection."
2020.acl-main.618.txt, 4 Conclusion and Future Work ,2020,"one particularly exciting direction is the application of our classification-based self-learning framework on top of the most recent methods that induce bilingual spaces via non-linear alignments (glavasˇ and vulic´, 2020; mohiuddin and joty, 2020)."
2020.acl-main.618.txt, 4 Conclusion and Future Work ,2020,"this proof-of-concept work opens up a wide spectrum of interesting avenues for future research, including the use of more powerful classifiers, more sophisticated features (e.g., character-level transformers), and fine-grained linguistic analyses on the importance of disparate features over different language pairs."
2020.acl-main.62.txt,6 Conclusions,2020,"for future work, we will explore the scenarios that annotations are absent for all expert dialogues."
2020.acl-main.622.txt, 6 Conclusion,2020,"in future work, we will explore novel approaches to generate the questions based on each mention, and evaluate the influence of different question generation methods on the coreference resolution task."
2020.acl-main.623.txt, 8 Conclusions and Future Work ,2020,"future work would include a comparison with other, more complex, methods for uncertainty estimation, incorporating uncertainty to affect model decisions over time, and further investigating links between uncertainty values and linguistic features of the input."
2020.acl-main.624.txt, 6 Conclusion,2020,"in the future, we want to investigate more powerful recommenders, combine interactive entity linking with knowledge base completion and use online learning to leverage deep models, despite their long training time."
2020.acl-main.626.txt, 6 Conclusion,2020,"applying our proposed controlled crowdsourcing protocol to qa-srl successfully attains truly scalable high-quality annotation by laymen, facilitating future research of this paradigm."
2020.acl-main.626.txt, 6 Conclusion,2020,"we release our data, software and protocol, enabling easy future dataset production and evaluation for qa-srl, as well as possible extensions of the qa-based semantic annotation paradigm."
2020.acl-main.628.txt, 5 Conclusion,2020,"inspired by the success of word meta-embeddings, we have shown how to apply different metaembedding techniques to ensembles of sentence encoders."
2020.acl-main.630.txt, 6 Conclusion,2020,future work may focus on how to directly induce topic information into bert without corrupting pretrained information and whether combining topics with other pretrained contextual models can lead to similar gains.
2020.acl-main.632.txt, 6 Conclusion,2020,"in the future, we plan to consider the ethos mode of persuasion by exploring how debaters strengthen their credibility in debates."
2020.acl-main.633.txt, 7 Conclusions,2020,"future research may focus on the motivation we described, but may also utilize the large speeches corpus we release as part of this work to a variety of additional different endeavors."
2020.acl-main.634.txt, 7 Conclusion and Future Work ,2020,"the model can be potentially improved by filtering the corpus according to different domains, or augmenting with a retrieve-and-rewrite mechanism, which we leave for future work."
2020.acl-main.635.txt, 5 Conclusion and Future Work ,2020,"extensive experiments demonstrate that these models can be enhanced by introducing knowledge, whereas there is still much room in knowledge-grounded conversation modeling for future work."
2020.acl-main.636.txt, 6 Conclusion,2020,"in future work, we intend to explore more with the combination of rl and dst on the basis of reward designing, trying to explore more in the internal mechanism."
2020.acl-main.639.txt,5 Conclusion,2020,"in the future, we plan to adapt variational neural network to refine our style transfer model, which has shown effectiveness in other conditional text generation tasks, such as machine translation (zhang et al., 2016; su et al., 2018)."
2020.acl-main.640.txt, 6 Conclusion,2020,"on the other hand, we would also like to investigate how to make use of our proposed model to solve sequence-to-sequence tasks."
2020.acl-main.640.txt, 6 Conclusion,2020,one is to investigate how the other graph models can benefit from our proposed heterogeneous mechanism.
2020.acl-main.640.txt, 6 Conclusion,2020,there are two directions for future works.
2020.acl-main.642.txt, 5 Conclusion,2020,we will explore more complicated object relation modeling in future work.
2020.acl-main.643.txt, 6 Conclusions,2020,a future research direction is to combine rgcs with distant supervision by an external knowledge base to answer the visual questions that need external knowledge; for example which animal in this photo can climb a tree?
2020.acl-main.645.txt, 8 Conclusion,2020,the question of knowing whether pretraining on small domain specific content will be a better option than transfer learning techniques such as fine-tuning remains open and we leave it for future work.
2020.acl-main.646.txt, 8 Discussion,2020,"we hope this work, i.e.the organised review it contributes and the techniques it introduces, will pave the way to deeper—in statistical hierarchy—generative models of language."
2020.acl-main.647.txt, 8 Conclusion,2020,we aim to explore those directions in a future work.
2020.acl-main.648.txt, 5 Takeaways and Open Questions ,2020,we leave some open questions to explore: • how can we exploit subword variations to reduce skewness in the nlu tasks?• would subword-segmentation-transfer be helpful for other nmt-nlu task pairs like we did for 2kenize (script conversion) to 1kenize (classification)?
2020.acl-main.649.txt, 8 Conclusion,2020,"in future work, we intend to further fine-tune our methodological apparatus for tackling mfep."
2020.acl-main.65.txt,7 Conclusion,2020,"in future work, we plan to seek better ways to guide the learning of latent variables, such as using dynamic routing (sabour et al., 2017) method to align the latent variables and sememes, and learn more explainable latent codes."
2020.acl-main.652.txt,8 Conclusion and Future Work,2020,"for the future, we would like to exploit the abstractive answers in our dataset, explore more sophisticated systems in both scenarios and perform user studies to study how real users interact with a conversational qa system when accessing faqs."
2020.acl-main.654.txt, 6 Conclusion,2020,"in future work, we explore to extend this approach for other low resource tasks in nlp."
2020.acl-main.655.txt, 7 Conclusion,2020,"in the future, we will further study this properties of kernel-based attentions in neural networks, both in the effectiveness front and also the explainability front."
2020.acl-main.656.txt, 7 Conclusions,2020,"for future work, an obvious next step is to investigate the possibility of generating veracity explanations from evidence pages crawled from the web."
2020.acl-main.657.txt, 7 Conclusion & Future Work ,2020,"as future work, we will explore different heuristics for navigating in the premises graph, as researched before for textual entailment (silva et al., 2019, 2018) and selective reasoning (freitas et al., 2014)."
2020.acl-main.661.txt, 7 Conclusion,2020,we exposed a significant space of both modeling ideas and application-specific requirements left to be addressed in future research.
2020.acl-main.662.txt, 5 A Call to Action ,2020,"appreciate ambiguity if your intended qa application has to handle ambiguous questions, do justice to the ambiguity by making it part of your task—for example, recognize the original ambiguity and resolve it (“did you mean...”) instead of giving credit for happening to ‘fit the data’."
2020.acl-main.662.txt, 5 A Call to Action ,2020,here are our recommendations if you want to have an effective leaderboard.
2020.acl-main.662.txt, 5 A Call to Action ,2020,"if your task is realistic, fun, and challenging, you will find experts to play against your computer."
2020.acl-main.662.txt, 5 A Call to Action ,2020,the first is that single numbers have some variance; it’s better to communicate estimates with error bars.
2020.acl-main.662.txt, 5 A Call to Action ,2020,these skills are exactly those we want computers to develop.
2020.acl-main.662.txt, 5 A Call to Action ,2020,"trivia is not just the accumulation of information but also connecting disparate facts (jennings, 2006)."
2020.acl-main.663.txt, 6 Conclusion,2020,i hope that this survey paper will help other researchers to develop the field in a way that keeps long-term goals in mind.
2020.acl-main.664.txt, 5 Conclusions,2020,in the near future we plan to extend the proposed approach to several other language-vision modeling tasks.
2020.acl-main.664.txt, 5 Conclusions,2020,the representation is further enhanced with text and visual features of neighbouring nodes.
2020.acl-main.665.txt,4 Conclusion,2020,another avenue of research would be to investigate the role of synthetic data in surface realization in other languages.
2020.acl-main.665.txt,4 Conclusion,2020,"assuming the use of synthetic data, more needs to be investigated in order to fully maximize its benefit on performance."
2020.acl-main.665.txt,4 Conclusion,2020,"future work will look more closely at the choice of corpus, construction details of the synthetic dataset, as well as the tradeoff between training time and accuracy that comes with larger vocabularies."
2020.acl-main.666.txt, 5 Conclusions,2020,"as future work, we plan to further evaluate the impact of different sequential architectures, longer contexts, alternative sentence embeddings, and cleverer selection of distractors."
2020.acl-main.666.txt, 5 Conclusions,2020,this work introduces a sentence-level language model which takes a sequence of sentences as context and predicts a distribution over a finite set of candidate next sentences.
2020.acl-main.667.txt, 4 Conclusion,2020,we hope that this work can shed some light and inspire future work at this line of research.
2020.acl-main.668.txt, 9 Conclusion,2020,future work will explore the use of domain adaptation techniques to enhance performance where the domains of the ci and event text differ substantially.
2020.acl-main.669.txt, 5 Conclusion,2020,"we also plan to use different types of labelled data, e.g., domain specific data sets, to ascertain whether entity type information is more discriminative in sub-languages."
2020.acl-main.67.txt,7 Conclusion and Future Work,2020,"also, we plan to use large-scale unlabeled data to improve the performance further."
2020.acl-main.67.txt,7 Conclusion and Future Work,2020,in future work we would like to do several experiments on other related tasks to test the versatility of our framework.
2020.acl-main.671.txt, 6 Conclusion,2020,"therefore, future work will aim at relaxing the prior of winograd-structured twin-question pairs."
2020.acl-main.672.txt, 6 Discussion,2020,however these models maintain the use of uniform memory capacity for each layer.
2020.acl-main.672.txt, 6 Discussion,2020,we believe the solution of maintaining a small number of long-range memories is a step towards tractable lifelong memory.
2020.acl-main.673.txt, 6 Conclusions,2020,a sample-based mutual information upper bound is derived to help reduce the dependence between embedding spaces.
2020.acl-main.673.txt, 6 Conclusions,2020,"for future work, our model can be extended to disentangled representation learning with non-categorical style labels, and applied to zero-shot style transfer with newly-coming unseen styles."
2020.acl-main.675.txt, 4 Conclusion,2020,"first, we will explore mechanisms for instance-specific translation that are more sophisticated than the aggregation of translation vectors of nearest dictionary neighbours."
2020.acl-main.675.txt, 4 Conclusion,2020,"second, we plan to couple instance-based mapping with other informative features (e.g., character-level features) in classification-based bli frameworks (heyman et al., 2017; karan et al., 2020)."
2020.acl-main.675.txt, 4 Conclusion,2020,we plan to extend this work in two directions.
2020.acl-main.676.txt, 7 Discussion,2020,"the procedure detailed in this paper relies on exact string matching to identify common context; future work might take advantage of learned representations of spans and their environments (mikolov et al., 2013; peters et al., 2018)."
2020.acl-main.677.txt, 6 Conclusion,2020,"this representation learning will be beneficial in tasks beyond text-to-sql, as long as the input has some predefined structure."
2020.acl-main.678.txt, 5 Conclusion,2020,the proposed method may be an important module for future applications related to time.
2020.acl-main.680.txt, 7 Conclusions,2020,"we then use current state-of-the-art approaches for named entity recognition and demonstrated that, through modeling of temporal information, performance can be improved when testing on future data."
2020.acl-main.681.txt, 5 Conclusion,2020,"future directions to explore include incorporating noise-robust training procedures (goldberger and ben-reuven, 2017) and example weighting (dehghani et al., 2018) during self-training, and exploring lexical alignment methods from literature on learning cross-lingual embeddings."
2020.acl-main.682.txt, 7 Conclusions & Future Work ,2020,"in addition, we will use the compguesswhat?!image annotations to design a visual grounding evaluation to assess the ability of the model to attend to the correct objects during the turns of the dialogue."
2020.acl-main.682.txt, 7 Conclusions & Future Work ,2020,we hope that grolla and the compguesswhat?!data will encourage the implementation of learning mechanisms that fuse taskspecific representations with more abstract representations to encode attributes in a more compositional manner.
2020.acl-main.683.txt, 6 Conclusion,2020,we propose an end-to-end cross-modality relevance framework that is tailored for language and vision reasoning.
2020.acl-main.684.txt, 6 Conclusion,2020,"future work can also explore curriculum learning in this domain, by first learning simpler tasks, which can be compositionally invoked in explanations for complex tasks."
2020.acl-main.684.txt, 6 Conclusion,2020,"in future work, the possibility of learning from a mix of explanations, exploration and a limited budget of interaction with the environment can be explored."
2020.acl-main.686.txt, 6 Conclusion,2020,we hope hat can open up an avenue towards efficient transformer deployments for real-world applications.
2020.acl-main.687.txt, 8 Conclusion,2020,our work provides a foundation for future work into simpler and more computationally efficient neural machine translation.
2020.acl-main.688.txt, 7 Conclusion,2020,"in transfer learning, we can also transfer the alignment."
2020.acl-main.688.txt, 7 Conclusion,2020,"therefore, a promising research direction to investigate would involve the development and assessment of improved initialisation methods that would more efficiently yield the benefits of the model transfer."
2020.acl-main.689.txt, 6 Conclusion,2020,existing curriculum learning research in nmt focuses on a single domain.
2020.acl-main.69.txt,5 Discussion,2020,the larger goal of qg is currently far from being solved.
2020.acl-main.692.txt, 6 Conclusions and Future Work ,2020,"this work just scratches the surface with what can be done on the subject; possible avenues for future work include extending this with multilingual data selection and multilingual lms (conneau and lample, 2019; conneau et al., 2019; wu et al., 2019; hu et al., 2020), using such selection methods with domain-curriculum training (zhang et al., 2019; wang et al., 2019b), applying them on noisy, web-crawled data (junczys-dowmunt, 2018) or for additional tasks (gururangan et al., 2020)."
2020.acl-main.693.txt, 4 Conclusions and future work ,2020,"while the scope of this work does not extend to sampling sentences given document context, this would be an interesting direction for future work."
2020.acl-main.694.txt, 4 Discussions and Conclusions ,2020,"also, we plan to consider more structured latent variables beyond modeling the sentence-level variation as well as to apply our vnmt model to more language pairs."
2020.acl-main.694.txt, 4 Discussions and Conclusions ,2020,we plan to conduct a more in-depth investigation into actual multimodality condition with high-coverage sets of plausible translations.
2020.acl-main.698.txt, 6 Conclusion,2020,"implications for future work on pretrained language models.(i) both factual knowledge and logic are discrete phenomena in the sense that sentences with similar representations in current pretrained language models differ sharply in factuality and truth value (e.g., “newton was born in 1641” vs. “newton was born in 1642”)."
2020.acl-main.699.txt, 4 Conclusions,2020,future work includes a deeper qualitative analysis of which (type of) papers are being cited; a more fine-grained analysis of different research topics in nlp to determine whether changes are more prevalent within certain areas than others; or extending the analysis to a larger set of the papers in the acl anthology.
2020.acl-main.703.txt, 8 Conclusions,2020,"bart performs comparably to roberta on discriminative tasks, and achieves new state-of-the-art results on several text generation tasks."
2020.acl-main.703.txt, 8 Conclusions,2020,"future work should explore new methods for corrupting documents for pretraining, perhaps tailoring them to specific end tasks."
2020.acl-main.704.txt, 7 Conclusion,2020,"future research directions include multilingual nlg evaluation, and hybrid methods involving both humans and classifiers."
2020.acl-main.705.txt, 5 Conclusion,2020,"for future work, we will explore the extension of conditional mlm to multimodal input such as image captioning."
2020.acl-main.706.txt, 7 Conclusions and Future Directions ,2020,an rl agent that leverages natural language descriptions of physical events to reason about the solution for a given goal (similar to zhong et al.(2020)) or for reward shaping (similar to goyal et al.(2019)) could be a compelling line of future research.
2020.acl-main.707.txt, 5 Conclusion,2020,"in future work, we plan to add paraphrase generation to generate diverse simple sentences."
2020.acl-main.708.txt, 8 Conclusion,2020,"to promote the research in this direction, we host a logicnlg challenge2 to help better benchmark the current progress."
2020.acl-main.710.txt, 7 Conclusion and Future Work ,2020,"in future work, we plan to investigate how target phrase order affects the generation behavior, and further explore set generation in an order invariant fashion."
2020.acl-main.711.txt, 8 Conclusion,2020,"language models conflate the two, so developing methods that are nuanced enough to recognize this difference is key to future progress."
2020.acl-main.713.txt, 6 Conclusions and Future Work ,2020,"in the future, we plan to incorporate more comprehensive event schemas that are automatically induced from multilingual multimedia data and external knowledge to further improve the quality of ie."
2020.acl-main.713.txt, 6 Conclusions and Future Work ,2020,we also plan to extend our framework to more ie subtasks such as document-level entity coreference resolution and event coreference resolution.
2020.acl-main.714.txt, 6 Conclusion and Future Work ,2020,"in the future work, it would be interesting to further explore how the model can be adapted to jointly extract role fillers, tackles coreferential mentions and constructing event templates."
2020.acl-main.715.txt, 5 Conclusion,2020,"in the future, we plan to apply ceon-lstm to other related nlp tasks (e.g., event extraction, semantic role labeling) (nguyen et al., 2016a; nguyen and grishman, 2018a)."
2020.acl-main.718.txt, 7 Conclusion,2020,we hope that rams will stimulate further work on multi-sentence argument linking.
2020.acl-main.72.txt,6 Conclusion,2020,our systematic study will pave the way to future research about the effective construction of dictionaries for text analytics.
2020.acl-main.720.txt, 6 Conclusions,2020,"in future work, we will explore whether the observed trends hold in much larger polyglot settings, e.g.the wikiann ner corpus (pan et al., 2017b)."
2020.acl-main.720.txt, 6 Conclusions,2020,"on the other hand, when the objective is to maximize performance on a single target language it may be possible to improve the proposed fine-tuning approach further using methods such as elastic weight consolidation (kirkpatrick et al., 2016)."
2020.acl-main.721.txt, 8 Conclusion,2020,future extensions of this work involve a more general pre-training objective allowing for the learned representations to be useful in many tasks as well as distantly or semi-supervised approaches to benefit from more data.
2020.acl-main.722.txt, 6 Conclusion,2020,possible future directions include using more sophisticated feature design and combinations of candidate retrieval methods.
2020.acl-main.723.txt, 7 Conclusions and Future Work ,2020,"in our experiment, a “relevant item” is a person classified by experts as being at risk of attempting suicide in the near future."
2020.acl-main.724.txt, 6 Conclusion,2020,"as future work, we intend to apply cluhtm in other representative applications on the web, such as hierarchical classification by devising a supervised version of cluhtm."
2020.acl-main.724.txt, 6 Conclusion,2020,we also intend to incorporate some type of attention mechanism into our methods to better understand which cluwords are more important to define certain topics.
2020.acl-main.725.txt, 6 Conclusions,2020,"in the future, we plan to expand the method scope from expanding concrete entity sets to more abstract concept sets."
2020.acl-main.726.txt, 5 Conclusion,2020,"in our future work, we will consider extending it to graph-based methods such as gcn for graph data, and to generation-based methods such as gan for adversarial learning."
2020.acl-main.727.txt, 7 Conclusion,2020,future visual grounding methods should be tested with a more comprehensive experimental setup and datasets for proper evaluation.
2020.acl-main.729.txt, 7 Conclusion,2020,reinforcement learning that has been applied in previous grounding work may help improve out-of-sample prediction for grounding in uis and improve direct grounding from hidden state representations.
2020.acl-main.735.txt, 6 Conclusion,2020,"for future work, we plan to apply the same methodology to other nlp tasks."
2020.acl-main.737.txt, 7 Conclusion,2020,"while these mappings provide a convenient way to avoid formalizing the complex notions of the phonetic and visual similarity, they are restrictive and do not capture all the diverse aspects of similarity that idiosyncratic romanization uses, so designing more suitable priors via operationalizing the concept of character similarity could be a promising direction for future work."
2020.acl-main.740.txt, 7 Conclusion ,2020,"our work points to numerous future directions, such as better data selection for tapt, efficient adaptation large pretrained language models to distant domains, and building reusable language models after adaptation."
2020.acl-main.742.txt, 7 Discussion,2020,"future work could also make the stronger assumption that a small number of in-domain training examples are available, and train and evaluate in a few-shot setting."
2020.acl-main.743.txt, 6 Conclusions,2020,"negation is generally understood to carry positive meaning, or in other words, to suggest affirmative alternatives."
2020.acl-main.743.txt, 6 Conclusions,2020,"predicting the focus of negation (i.e., pinpointing the usually few tokens that are actually negated) is key to revealing affirmative alternatives."
2020.acl-main.744.txt, 5 Discussion & Conclusion ,2020,"final words in this work, we have presented a framework that seeks to predict structurally consistent outputs without extensive model redesign, or any expensive decoding at prediction time."
2020.acl-main.744.txt, 5 Discussion & Conclusion ,2020,there are some recent works on the design of models and loss functions by relaxing boolean formulas.
2020.acl-main.745.txt, 7 Conclusion and Future Work ,2020,"finally, to extend tabert to cross-lingual settings with utterances in foreign languages and structured schemas defined in english, we plan to apply more advanced semantic similarity metrics for creating content snapshots."
2020.acl-main.745.txt, 7 Conclusion and Future Work ,2020,"first, we plan to evaluate tabert on other related tasks involving joint reasoning over textual and tabular data (e.g., table retrieval and table-to-text generation)."
2020.acl-main.745.txt, 7 Conclusion and Future Work ,2020,this work also opens up several avenues for future work.
2020.acl-main.746.txt, 8 Conclusion,2020,we envision future efforts exploring the interactions between improving the underlying graphstructure prediction and ever-better correlations to human judgements on individual properties.
2020.acl-main.748.txt, 8 Conclusion,2020,our proposed semantic type regularizer allows the ranker to incorporate semantic type information into its predictions without requiring semantic types at prediction time.
2020.acl-main.75.txt,5 Conclusions,2020,"moreover, we would love to apply the proposed model to other problems, such as general humor recognition, irony discovery, and sarcasm detection, as the future work."
2020.acl-main.750.txt, 8 Conclusions,2020,future work will focus on domain adaptation at the embedding layer.
2020.acl-main.751.txt, 6 Conclusions and Future Work ,2020,"interesting future work includes applying our techniques to different taxonomies (e.g., biomedical) and training a model for different attributes."
2020.acl-main.754.txt, 8 Conclusion,2020,"notably, multidds is not limited to nmt, and future work may consider applications to other multilingual tasks."
2020.acl-main.756.txt, 5 Conclusions ,2020,"because it is artificial to use synthetic data for training a filter classifier, future work can focus on a better objective that models parallelism more smoothly."
2020.acl-main.756.txt, 5 Conclusions ,2020,future work also includes extending the method to low-resource languages not covered by multilingual bert.
2020.acl-main.757.txt, 4 Conclusions,2020,"in the future, we believe more work on alleviating context control problem has the potential to improve translation performance as quantified in table 3."
2020.acl-main.759.txt, 6 Conclusion and Future Work ,2020,"one of our next steps is to investigate the impact of tc extraction methods on a corresponding awe system (zhang et al., 2019), which uses the feature values produced by aesrubric to generate formative feedback to guide essay revision."
2020.acl-main.759.txt, 6 Conclusion and Future Work ,2020,"this leads to an interesting future investigation direction, which is training the aesneural using the gold standard that can be extracted automatically."
2020.acl-main.760.txt, 6 Discussion,2020,future work will consider additional methods for integrating ontology structure into representation learning.
2020.acl-main.760.txt, 6 Discussion,2020,"in contrast, a more complex linker, such as ours, maybe a better option for specific subsets of notes that require better accuracy (e.g., the results of specific clinical studies)."
2020.acl-main.760.txt, 6 Discussion,2020,"our results demonstrate the advantages of using contextualized embeddings for ranking tasks, and that using information from the knowledge base for training is an essential direction for learning concept representations for sparse kb domains."
2020.acl-main.761.txt, 7 Conclusion,2020,"future work in multi-hop reasoning could represent the relation between consecutive pieces of evidence and future work in temporal reasoning could incorporate numerical operations with bert (andor et al., 2019)."
2020.acl-main.761.txt, 7 Conclusion,2020,"propositions with causal relations (hidey and mckeown, 2016), which are event-based rather than attribute-based as in fever, are also challenging."
2020.acl-main.762.txt, 6 Conclusion,2020,"the current approach covers a fixed number of fonts, but it can be extended to support a larger set of fonts."
2020.acl-main.764.txt, 8 Conclusion and Future Work ,2020,we believe that modeling domain shift is a promising future direction to improve performance prediction.
2020.acl-main.764.txt, 8 Conclusion and Future Work ,2020,"while investigating the systematic implications of model structures or hyperparameters is practically infeasible in this study, we may use additional information such as textual model descriptions for modeling nlp models and training procedures more elaborately in the future."
2020.acl-main.764.txt, 8 Conclusion and Future Work ,2020,"while this discovery is a promising start, there are still several avenues on improvement in future work."
2020.acl-main.766.txt, 6 Conclusion,2020,"in the future, we will investigate a hub language ranking/selection model a la lin et al.(2019)."
2020.acl-main.767.txt, 6 Conclusions,2020,there are several directions for future work including better architecture design for utilizing structured meta-data and replacing the two-stage framework with a multi-task generation model that can jointly identify helpful context for the task and perform corresponding text generation.
2020.acl-main.768.txt, 8 General Discussion & Conclusion ,2020,maybe this induces patterns in the data that make the nature of those assumptions recoverable from the data itself.
2020.acl-main.769.txt, 7 Conclusion,2020,future work may include developing debiasing strategies that do not require prior knowledge of bias patterns and can automatically identify them.
2020.acl-main.77.txt,6 Conclusion and Future Work,2020,"in the future, we will do more tests and surveys on the improvement of business objectives such as user experience, user engagement and service revenue."
2020.acl-main.770.txt, 7 Conclusion,2020,several challenges in this direction of research may include extending the debiasing methods to overcome multiple biases at once or to automatically identify the format of those biases which simulate a setting where the prior knowledge is unavailable.
2020.acl-main.773.txt, 7 Conclusion,2020,"finally, we would like to point out that our methods and results here do not mean to belittle the importance of collecting clean/unbiased data."
2020.acl-main.773.txt, 7 Conclusion,2020,joint efforts are needed for promoting unbiased models that learn true semantics; and we hope our paper can encourage more work towards this important direction.
2020.acl-main.775.txt, 6 Conclusion and Further Directions ,2020,"an interesting additional use-case for our joint decoder is when a downstream task, e.g., relation extraction, requires output structures from both a parser and a tagger."
2020.acl-main.775.txt, 6 Conclusion and Further Directions ,2020,future community efforts on a unified representation of flat structures for all languages would facilitate further research on linguistically-motivated treatments of headless structures in “headful” dependency treebanks.
2020.acl-main.775.txt, 6 Conclusion and Further Directions ,2020,"to handle non-projectivity, one possible solution is pseudo-projective parsing (nivre and nilsson, 2005)."
2020.acl-main.775.txt, 6 Conclusion and Further Directions ,2020,we leave it to future work to design a non-projective decoder for joint parsing and headless structure extraction.
2020.acl-main.778.txt, 7 Conclusion,2020,another area for future work is to explore what information treebank vectors encode.
2020.acl-main.778.txt, 7 Conclusion,2020,"future work should also test even simpler strategies which do not use the las of previous parses to gauge the best treebank vector, e. g. always picking the largest treebank."
2020.acl-main.778.txt, 7 Conclusion,2020,"in experiments with czech, english and french, we investigated treebank embedding vectors, exploring the ideas of interpolated vectors and vector weight prediction."
2020.acl-main.778.txt, 7 Conclusion,2020,"interpolating treebank vectors adds a layer of opacity, and, in future work, it would be interesting to carry out experiments with synthetic data, e. g. varying the number of unknown words, to get a better understanding of what they may be capturing."
2020.acl-main.778.txt, 7 Conclusion,2020,"the type of information will depend on the selection of tree-banks, e. g. in a polyglot setting the vector may simply encode the language, and in a monolingual setting such as ours it may encode annotation or domain differences between the treebanks."
2020.acl-main.79.txt,5 Conclusion,2020,"in this work, we presented a framework for explaining the gnn-based models by extending the influence function to estimate the effect of samples in graph data."
2020.acl-main.79.txt,5 Conclusion,2020,"some interesting observations include the effects of regions and the sensitivity of gnn-based models, which open potentials for further improvements that we plan to address in our future work."
2020.acl-main.8.txt,7 Conclusions,2020,"in the future, we aim to leverage these pre-trained models to advance sota on downstream conversational tasks, such as knowledge-grounded conversations or question answering."
2020.acl-main.8.txt,7 Conclusions,2020,"when a large conversational model is trained on a diverse collection of multi-turn conversations, it is able to generate quality conversations that are engaging, coherent, and plausibly human."
2020.acl-main.80.txt,6 Conclusion,2020,"it can be further extended for l1 → l1l2, or l2 → l1l2 for code switch sentence generation, and l1 → l2, or l2 → l1 for machine translation."
2020.acl-main.81.txt,5 Conclusions,2020,we leave this direction to future work.
2020.acl-main.81.txt,5 Conclusions,2020,we proposed spellgcn for csc to incorporate both phonological and visual similarities into language models.
2020.acl-main.82.txt,5 Conclusion,2020,"as future work, we plan to extend soft-masked bert to other problems like grammatical error correction and explore other possibilities of implementing the detection network."
2020.acl-main.84.txt,8 Conclusion,2020,"based on these results, we will explore ways to leverage the token assignment to domain adaption and few-shot learning."
2020.acl-main.84.txt,8 Conclusion,2020,"this baseline achieves scores of up to 48% precision, which are already reasonable while also leaving large potential for improvement in future research."
2020.acl-main.84.txt,8 Conclusion,2020,we also plan to enhance the annotation process by automatically generating proposals for the nl questions and token assignments and letting the annotators only perform corrections.
2020.acl-main.84.txt,8 Conclusion,2020,we hope that this increases annotation efficiency even more.
2020.acl-main.86.txt,4 Conclusions,2020,we also show that interleaving instances from different tasks within each epoch and forming heterogeneous batches is crucial for optimizing multi-task performance.
2020.acl-main.86.txt,4 Conclusions,2020,"we hope that future work experiments further with dynamic sampling such as by modifying the metric (e.g., using bleu or rouge score if applicable) and/or modifying other values like number of instances per epoch based on performance metrics (not only does this effectively change learning rate, but it would also allow the model to update the sampling distribution more or less frequently)."
2020.acl-main.9.txt,5 Conclusion,2020,"in the future, we will also explore to boost the latent selection policy with reinforcement learning and extend our pre-training to support dialogue generation in other languages."
2020.acl-main.90.txt,8 Conclusion,2020,incorporating our three-way attentive pooling network into open domain conversational qa systems will be interesting future work.
2020.acl-main.95.txt,8 Conclusion,2020,"another direction for future work would improve few-shot approaches to wsd, which is both important for moving wsd into new domains and for modeling rare senses that naturally have less support in wsd data."
2020.acl-main.95.txt,8 Conclusion,2020,"potential directions include finding ways to obtain more informative training signal from uncommon senses, such as with different approaches to loss reweighting, and exploring the effectiveness of other model architectures on lfs examples."
2020.acl-main.95.txt,8 Conclusion,2020,this approach leads to a 31.1% error reduction over prior work on the less frequent sense examples.
2020.acl-main.95.txt,8 Conclusion,2020,this leaves better disambiguation of less common senses as the main avenue for future work on wsd.
2020.acl-main.96.txt,6 Conclusion,2020,further we plan to investigate other nlp applications that can benefit from the simple linguistic features introduced here.
2020.acl-main.96.txt,6 Conclusion,2020,"in future, we would like to extend this work for other language pairs."
2020.acl-main.97.txt,5 Conclusion,2020,"in the future, we will extend the proposed framework by considering more context (meta data) information, such as time, storylines, and comment sentiment, to further enrich our explainability."
2020.acl-main.98.txt,6 Conclusion,2020,the study of these tasks will be left as the future work.
2020.acl-main.98.txt,6 Conclusion,2020,we demonstrate usability of this dataset and provide results of state of the art models for future studies.
2020.acl-main.99.txt,6 Conclusion,2020,"in future work, we plan to conduct more empirical studies on seg and further improve its performance on new intent identification."
2020.acl-main.99.txt,6 Conclusion,2020,we also plan to conduct more case studies in applying seg to boost the performance of current zero-shot intent classification methods.
2020.acl-srw.1.txt, 6 Conclusion,2020,"while the empirical results are encouraging, important future work includes explorations of higher efficient adaptive and sparse mechanisms that can significantly cause flops and parameter reduction with minimal loss in performance."
2020.acl-srw.10.txt, 6 Conclusion ,2020,"future extensions to this work may include applying it to character-level instead of subword representations, and using it for morphologically richer languages, especially low-resourced agglutinative ones, where our approach, together with the incorporation of linguistic information, may provide larger improvements in translation quality."
2020.acl-srw.11.txt, 7 Conclusions and Future Work ,2020,"in the future, we intend to use the english translation data of north korean news articles to create an evaluation dataset that considers differences in words, and attempt to develop a translation method using a language model with context, such as bert (devlin et al., 2019)."
2020.acl-srw.14.txt, 5 Conclusion,2020,"in the future, we will strengthen tags that contain semantic information to extract keywords for more accurate information, such as disease information, location, and size."
2020.acl-srw.15.txt, 6 Conclusions,2020,"in future work, we plan to utilize other word segmentation methods for model training."
2020.acl-srw.15.txt, 6 Conclusions,2020,we also plan to combine the proposed multi-task neural model with back-translation method to enhance the ability of the nmt model on target-side language modeling.
2020.acl-srw.16.txt, 5 Conclusion,2020,"in the future, we want to extend this method to language features other than words."
2020.acl-srw.18.txt, 6 Conclusion,2020,"furthermore, it can be expected that the methodological insights gained in the simulation experiments can inform other approaches investigating non-transparent embedding representations and yield important insights about the behavior of distributional models."
2020.acl-srw.19.txt, 7 Conclusion and Future Work ,2020,"future work could include finding a way to incorporate other linguistic features like case-markers, gender, number, person, tense, aspect and verb agreement information into the parser."
2020.acl-srw.2.txt, 4 Summary,2020,we plan to focus mostly on studying the possible application of gcn in this task.
2020.acl-srw.2.txt, 4 Summary,2020,we will perform extensive experiments and report results in future work.
2020.acl-srw.20.txt, 6 Conclusion,2020,"this suggests future work to reconsider how to match the training and evaluation to the actual objective of downstream applications, and thus create more reliable evaluation metrics and benchmarks."
2020.acl-srw.22.txt, 5 Conclusion & Future Work ,2020,"in future, we would like to work on effective techniques to exploit monolingual data and parallel data from other languages together to improve the translation of low-resource languages."
2020.acl-srw.23.txt, 5 Conclusions,2020,"further, we plan to use this decoder in an iterative, semi-supervised learning scenario akin to co-training (blum and mitchell, 1998)."
2020.acl-srw.23.txt, 5 Conclusions,2020,"in the longer term, we envision a decoder with constraints, which enforces that the generated rules follow correct odin syntax."
2020.acl-srw.23.txt, 5 Conclusions,2020,we plan to include constraints as part of decoding to aid in rule synthesis.
2020.acl-srw.24.txt, 5 Conclusion,2020,future qualitative work could also suggest further variables whose inclusion would enhance our knowledge of humor perception.
2020.acl-srw.24.txt, 5 Conclusion,2020,"we hope that the inclusion of demographic information will shift the state of the art away from objective classification, towards a more subjective approach."
2020.acl-srw.26.txt, 6 Conclusion,2020,"our findings shed light on the importance of modeling points of correspondence, suggesting important future directions for sentence fusion."
2020.acl-srw.28.txt, 5 Conclusion,2020,"future work would entail analysing and implementing more detailed underlying morphonological rules, and investigating the cross-over from fsts to neural models."
2020.acl-srw.3.txt, 5 Conclusion,2020,our future work will target demonstrating the method on other languages.
2020.acl-srw.3.txt, 5 Conclusion,2020,"we also hope to address semantic paraphasia in future work and create, deploy aac systems building on the method proposed in this paper."
2020.acl-srw.30.txt, 5 Conclusion,2020,"also, we plan to extend the simple label embedding calculation methods to more sophisticated ones."
2020.acl-srw.30.txt, 5 Conclusion,2020,"for future work, we envision to apply our method to other tasks and datasets and investigate the effectiveness."
2020.acl-srw.31.txt, 7 Conclusion ,2020,"while the focus of this paper was on data construction, developing a higher-quality typo correction system is the future direction to pursue."
2020.acl-srw.32.txt, 6 Conclusion and Future Work ,2020,"moreover, we must develop a method for more accurately estimating the confidence scores, which is our primary focus in the next step."
2020.acl-srw.33.txt, 7 Conclusion,2020,"from the cognitive neuroscience perspective, it would be interesting to investigate if the proposed decay rnn can capture some aspects of actual neuronal behaviour and language cognition."
2020.acl-srw.34.txt, 5 Conclusion,2020,for the future we would like to apply our model on other cross-lingual nlp tasks such as xnli or cross-lingual semantic textual similarity.
2020.acl-srw.35.txt, 4 Conclusion,2020,"in future work, we will extend our analysis to cover the more complex constructions mentioned in section 3."
2020.acl-srw.36.txt, 6 Conclusion and Future Work ,2020,"in the future, we plan to introduce constraints for asymmetric relations as well as extend our proposed method to leverage them."
2020.acl-srw.36.txt, 6 Conclusion and Future Work ,2020,"moreover, we improved a state-of-the-art post-specialization method by incorporating adversarial losses with the wasserstein distance."
2020.acl-srw.36.txt, 6 Conclusion and Future Work ,2020,"moreover, we plan to experiment with adapting our model to a multilingual scenario, to be able to use it in a neural machine translation task."
2020.acl-srw.37.txt,6 Conclusion,2020,"in the future, we plan to experiment with even more challenging language pairs such as japanese–russian and attempt to leverage monolingual corpora belonging to diverse language families.we might be able to identify subtle relationships among languages and approaches to better leverage assisting languages for several nlp tasks."
2020.acl-srw.38.txt,6 Conclusions and Future Work,2020,we also plan to explore the work by c¸aglar gu¨lc¸ehre et al.(2017) and c¸aglar gu¨lc¸ehre et al.(2015) that introduces language models into the existing neural architecture with methods such as shallow fusion and deep fusion.
2020.acl-srw.39.txt,6 Conclusion,2020,"in the future, it would be interesting to explore weak suervision and other data augmentation techniques to improve models’ robustness further."
2020.acl-srw.40.txt,4 Discussion and Conclusion,2020,"for future work, we consider conducting the same experiments on cola, a dataset for judging the grammatical acceptability of a sentence (warstadt et al., 2019)."
2020.acl-srw.42.txt,5 Conclusion,2020,"we believe this factorization prevents the model from memorizing spurious correlations in the data, and note that similar ideas may be useful in other natural language tasks."
2020.acl-srw.43.txt,6 Conclusion and Future Work,2020,our future agenda includes further bifurcating and exploring the specific types of victim blaming and the efficacy of the proposed approach on such a multi label classification task.
2020.acl-srw.43.txt,6 Conclusion and Future Work,2020,we plan to explore the different weighting factors for the language modelling loss and classification loss described in section 4 to determine if weighting factors can help customize the auxiliary loss for different tasks.
2020.acl-srw.8.txt, 7 Conclusion,2020,"the proposed method transfers binary word attributes using reflection-based mappings and keeps non-attribute words unchanged, without attribute knowledge in inference time."
2020.emnlp-main.1.txt,"
7 Conclusion
",2020,"further, argumentation structure (support relations between sentences or lack thereof) might provide useful information about each sentences attackability."
2020.emnlp-main.1.txt,"
7 Conclusion
",2020,we leave this analysis to future work.
2020.emnlp-main.10.txt,"
6.5 Future Directions
",2020,a future extension could explore more robust techniques for identifying abstract chains which do not make such assumptions.
2020.emnlp-main.10.txt,"
6.5 Future Directions
",2020,extending the proposed approaches for longer chains is an important future direction.
2020.emnlp-main.10.txt,"
6.5 Future Directions
",2020,"nonetheless, a useful future direction is exploring answer prediction and explanation prediction as joint goals, and perhaps they can benefit each other."
2020.emnlp-main.101.txt,"
6 Concluding Remarks
",2020,thus it will be interesting to investigate the performance of rrt in other applications of vae beyond topic modelling.
2020.emnlp-main.103.txt,"
7 Conclusion
",2020,"future work includes using these approaches to induce model structure, develop accurate models with better interpretability, and to apply these approaches in lower data regimes."
2020.emnlp-main.105.txt,"
6 Conclusion
",2020,"stateof-the-art performance on zest is 12%, leaving much room for future improvement."
2020.emnlp-main.105.txt,"
6 Conclusion
",2020,"this is an interesting avenue for future work, for which zest should also be useful."
2020.emnlp-main.105.txt,"
6 Conclusion
",2020,"to facilitate future work, we make our models, code, and data available at https://allenai.org/data/ zest."
2020.emnlp-main.109.txt,"
7 Discussion
",2020,(2) focusing on one of the most important crises of the future: water;
2020.emnlp-main.109.txt,"
7 Discussion
",2020,this merits a deeper exploration with a holdout attribution set we aim to investigate in future.• flint water crisis: we were curious to know how our model performs in the wild on a data set of a different water crisis.
2020.emnlp-main.110.txt,"
6 Discussion
",2020,future semeval challenges should consider this when constructing test datasets and mention the hashtags and keywords they use for data collection.
2020.emnlp-main.110.txt,"
6 Discussion
",2020,"in the future, we hope to explore abstract topics like ‘immigration’ where differentiating between direct and indirect stance is non-trivial and ensemble models that combine the strengths of multiple methods."
2020.emnlp-main.110.txt,"
6 Discussion
",2020,"this suggests that future research should explore approaches like coreference resolution (for pronouns), word sense disambiguation (for epithets), and background knowledge (relationships to other entities)."
2020.emnlp-main.111.txt,"
6 Conclusion
",2020,"in the future, we will explore how to apply our model to more domains, and enhance the interpretability of the reasoning path when the model answers questions."
2020.emnlp-main.116.txt,"
6 Conclusions and Future Work
",2020,"in the future, we plan to focus on how to improve the performance of medical entity normalization when resources are limited."
2020.emnlp-main.116.txt,"
6 Conclusions and Future Work
",2020,"notwithstanding, considering the complexity of the domain specificity and the scarcity of training data, this challenging task is far from being solved."
2020.emnlp-main.117.txt,"
7 Conclusion
",2020,"third, we find that chexbert outperforms models which do not use backtranslation."
2020.emnlp-main.118.txt,"
7 Conclusion
",2020,our findings have clear implications for future work.
2020.emnlp-main.12.txt,"
6 Conclusion
",2020,"while this is not a dataset paper (since our focus is on more on the value of natural perturbations for robust model design), we provide the natural perturbations resource for boolq constructed during the course of this study.4 this work suggests a number of interesting lines of future investigation."
2020.emnlp-main.12.txt,"
6 Conclusion
",2020,"while we leave a detailed study to future work, we expect general trends regarding the value of perturbations to hold broadly."
2020.emnlp-main.120.txt,"
7 Conclusions
",2020,we are excited about future work that could extend our motivation and further aim at incorporating stronger hierarchy into the language model architectures and the pre-training tasks.
2020.emnlp-main.122.txt,"
6 Conclusion
",2020,"in future work, we will explore generalizing this approach to the multilingual setting, or applying it to the pre-train and fine-tune paradigm used widely in other models such as bert."
2020.emnlp-main.123.txt,"
5 Conclusion
",2020,"furthermore, this aligner may help to build or increase semantic resources, using a promising approach as back-translation (sobrevilla cabezudo et al., 2019)."
2020.emnlp-main.123.txt,"
5 Conclusion
",2020,"future work includes adopting multilingual word embeddings (lample et al., 2018) to produce alignments for other languages."
2020.emnlp-main.124.txt,"
5 Conclusions
",2020,"in the future, we want to explore semi-supervised methods for sentence embedding and its transferability across domains."
2020.emnlp-main.125.txt,"
7 Discussion and Future Work
",2020,"further, as our method does not restrict input to syntactic trees but only assumes tree structures with arbitrary numbering (e.g.leftto-right post-order numbering) as input, we intend to try alignments of chunk-based trees, which is desirable for applications that process text fragments, e.g.those that perform information extraction."
2020.emnlp-main.125.txt,"
7 Discussion and Future Work
",2020,"we intend to expand our method to conduct forest alignments for making it robust against parsing errors, which are inevitable in handling large corpora."
2020.emnlp-main.125.txt,"
7 Discussion and Future Work
",2020,"we plan to apply it to a comparable corpus of partial paraphrases and investigate the performance, with the aim of creating a large-scale syntactic and phrasal paraphrase dataset."
2020.emnlp-main.126.txt,"
5 Conclusion
",2020,there still exists plenty of potentials that require future studies in this direction.
2020.emnlp-main.128.txt,"
6 Conclusion and Future Work
",2020,"in the future, we would adapt our method to other ie tasks to study its application scope."
2020.emnlp-main.129.txt,"
7 Conclusion and Future work
",2020,"in the future, we will extend maven to more event-related tasks like event argument extraction, event sequencing, etc."
2020.emnlp-main.13.txt,"
6 Conclusion
",2020,"an investigation of how, whether, and why formalisms and their implementations affect probing results for tasks beyond role labeling and for frameworks beyond edge probing constitutes an exciting avenue for future research."
2020.emnlp-main.13.txt,"
6 Conclusion
",2020,"second, if possible, results on multiple formalisations of the same task should be reported and validated for several languages."
2020.emnlp-main.133.txt,"
7 Conclusion
",2020,"in the future, we would like to investigate how the table representation may be applied to other tasks."
2020.emnlp-main.136.txt,"
6 Conclusion
",2020,"in the future, we will investigate the feasibility of incorporating classical mds guidance to abstractive models with large-scale pre-training (gu et al., 2020) and more challenging settings where each document set may contain hundreds or even thousands of documents."
2020.emnlp-main.137.txt,"
7 Conclusions and Future Work
",2020,another intriguing direction is exploring the connection between our methods and neural network interpretability.
2020.emnlp-main.137.txt,"
7 Conclusions and Future Work
",2020,"finally, although we are motivated primarily by the widespread use of topic models for identifying interpretable topics (boyd-graber et al., 2017, ch.3), we plan to explore the ideas presented here further in the context of downstream applications like document classification."
2020.emnlp-main.137.txt,"
7 Conclusions and Future Work
",2020,"in future work, we also hope to explore the effects of the pretraining corpus (gururangan et al., 2020) and teachers (besides bert) on the generated topics."
2020.emnlp-main.137.txt,"
7 Conclusions and Future Work
",2020,"the use of knowledge distillation to facilitate interpretability has also been previously explored, for example, in liu et al.(2018a) to learn interpretable decision trees from neural networks."
2020.emnlp-main.138.txt,"
6 Conclusion
",2020,future works could focus on employing the proposed model in more downstream tasks.
2020.emnlp-main.140.txt,"
6 Conclusions and Future Work
",2020,we also plan to add more types of meta-features to generate richer multimodal representations.
2020.emnlp-main.141.txt,"
5 Conclusion
",2020,a state-of-the-art model was trained to establish strong baselines for future studies.
2020.emnlp-main.142.txt,"
7 Conclusion
",2020,"in this work, we explore unsupervised disfluency detection by combining self-training and selfsupervised learning."
2020.emnlp-main.147.txt,"
5 Conclusion
",2020,we will explore this direction in future work.
2020.emnlp-main.148.txt,"
6 Conclusion
",2020,"in the future, we will further explore how to explicitly incorporate linguistics information, such as named entities into the latent states."
2020.emnlp-main.149.txt,"
6 Conclusion and Outlook
",2020,"in future work, we will further investigate othercontent generation problems by leveraging multi-granularity copying mechanism."
2020.emnlp-main.15.txt,"
8 Conclusion
",2020,future work will be separated into two strands.
2020.emnlp-main.15.txt,"
8 Conclusion
",2020,"the first will focus on how to better model the distribution of embeddings given a morphosyntactic attribute; as mentioned above, this should yield a better probe overall."
2020.emnlp-main.150.txt,"
6 Conclusion
",2020,"in the future, we will move on to develop a more general dialogue dependency parser and better incorporate dependency information into dialogue context modeling tasks."
2020.emnlp-main.152.txt,"
4 Conclusion
",2020,"in the future, we plan to extend our nonautoregressive refiner to other natural language understanding (nlu) tasks, e.g., named entity recognition (tjong kim sang and de meulder, 2003), semantic role labeling (he et al., 2018), and natural language generation (nlg) tasks, e.g., machine translation (vaswani et al., 2017), summarization (liu and lapata, 2019)."
2020.emnlp-main.153.txt,"
7 Conclusion
",2020,"in future work, we would like to apply our approach on document-level and multi-document nlu tasks."
2020.emnlp-main.154.txt,"
8 Conclusion
",2020,"another possible avenue for future work is to use crows-pairs to help directly debias lms, by in some way minimizing a metric like ours."
2020.emnlp-main.154.txt,"
8 Conclusion
",2020,"this highlights the danger of deploying systems built around mlms like these, and we expect crows-pairs to serve as a metric for stereotyping in future work on model debiasing."
2020.emnlp-main.154.txt,"
8 Conclusion
",2020,"while our evaluation is limited to mlms, we were limited by our metric, a clear next step of this work is to develop metrics that would allow one to test autoregressive language models on crowspairs."
2020.emnlp-main.156.txt,"
9 Discussion and conclusion
",2020,we hope that answers to these questions will not just demystify the empirical success of rnns but ultimately drive new methodological improvements as well.
2020.emnlp-main.158.txt,"
7 Conclusion
",2020,future works include looking into models and continual learning algorithms to better address the challenge.
2020.emnlp-main.158.txt,"
7 Conclusion
",2020,we benchmark our proposed datasets with extensive analysis using state-of-theart continual learning methods.
2020.emnlp-main.158.txt,"
7 Conclusion
",2020,"we propose viscoll, a novel continual learning setup for visually grounded language acquisition."
2020.emnlp-main.159.txt,"
5 Conclusions
",2020,detailed analysis is also provided to help future works investigate other critical feature enrichment and alignment methods for this task.
2020.emnlp-main.16.txt,"
7 Future Work & Conclusion
",2020,future work might use msgs as a diagnostic tool to measure how effectively new model architectures and selfsupervised pretraining tasks can more efficiently equip neural networks with better inductive biases.
2020.emnlp-main.16.txt,"
7 Future Work & Conclusion
",2020,these models could prove to be a helpful resource for future studies looking to study learning curves of various kinds with respect to the quantity of pretraining data.
2020.emnlp-main.161.txt,"
5 Conclusion
",2020,"we consider extension of our model to other videoand-language tasks as future work, as well as developing more well-designed pre-training tasks."
2020.emnlp-main.163.txt,"
7 Summary Of Exploitable Weaknesses and Defense Directions
",2020,other interesting avenues for future research is to understand the importance of metadata in this multimodal setting and investigating counter-attacks to improved generators that incorporate image-text consistency.
2020.emnlp-main.163.txt,"
7 Summary Of Exploitable Weaknesses and Defense Directions
",2020,"we hope future work will address any potential limitations of this work, such as expanding the dataset to evaluate generalization across different news sources, and a larger variety of neural generators."
2020.emnlp-main.165.txt,"
5 Conclusion and Future Work
",2020,"as to future work, we plan to explore how to jointly extract entities and relations in federated settings."
2020.emnlp-main.167.txt,"
7 Conclusions and Future Work
",2020,"there are many potential directions for future work on oia, including 1) more labeled data; 2) better learning algorithm; 3) becoming crosslingual by adding support for more natural languages; 4) porting existing oie strategies on oia and evaluating the performance compared with the original ones."
2020.emnlp-main.169.txt,"
7 Conclusion
",2020,"in future work, we would like to investigate methods to stabilize the training of weight tied models and apply our model on other tasks in natural language generation."
2020.emnlp-main.171.txt,"
8 Conclusions
",2020,"we hope that our insights, including some of our negative results, may encourage future research on learning with latent structures."
2020.emnlp-main.172.txt,"
7 Conclusions
",2020,"this method provides a principled way to perform statistical model comparison using k-fold crossvalidation, a data-efficient evaluation technique."
2020.emnlp-main.173.txt,"
6 Conclusion
",2020,"in the future, we are interested in social science topics, such as modeling the causal effect between mental health and the suicide decisions reflected through social media, which may help predict and stop the final decisions."
2020.emnlp-main.174.txt,"
7 Conclusion
",2020,"future work may explore the possibility of applying masking to the pretrained multilingual encoders like mbert (devlin et al., 2019) and xlm (conneau and lample, 2019)."
2020.emnlp-main.175.txt,"
7 Conclusion and Future Work
",2020,"in the future, we will select context sentences in larger candidate space, and explore more effective ways to extend our approach to select target-side context sentences."
2020.emnlp-main.176.txt,"
6 Conclusion
",2020,"future directions include exploring advanced identification and rejuvenation models that can better reflect the learning abilities of nmt models, as well as validating on other nlp tasks such as dialogue and summarization."
2020.emnlp-main.177.txt,"
7 Conclusions and Future Work
",2020,"in future work, we will explore other such applications of our proposed methods."
2020.emnlp-main.179.txt,"
7 Conclusion and Future Works
",2020,"in the future, the proposed mgl method can potentially applied to more cross-lingual natural language understanding (xlu) tasks (conneau et al., 2018b; wang et al., 2019; lewis et al., 2019; karthikeyan et al., 2020), and be generalized to learn to learn for domain adaptation (blitzer et al., 2007), representation learning (shen et al., 2018), multi-task learning (shen et al., 2019) problems, etc. universal syntactic interpretations are valuable language interpretations, which have been developed in years of study."
2020.emnlp-main.182.txt,"
8 Conclusion
",2020,our attack experiments on dependency parsing and pos tagging show that our proposed framework can produce high-quality sentences that can effectively attack current state-of-the-art models.
2020.emnlp-main.183.txt,"
6 Conclusion
",2020,future work includes finding applications of our novel tagging scheme in other tasks involving extracting triplets as well as extending our approach to support other tasks within sentiment analysis.
2020.emnlp-main.184.txt,"
6 Conclusion
",2020,"we hope that future research continues this line of work, especially by finding novel ways to devise adaptive policies – such as reinforcement learning models with the visual modality."
2020.emnlp-main.186.txt,"
6 Further Discussion and Conclusion
",2020,in future research we also plan an in-depth study of these factors and their relation to our spectral analysis.
2020.emnlp-main.186.txt,"
6 Further Discussion and Conclusion
",2020,"our findings, suggesting that it is possible to effectively combine these two types of language distance measures, call for further research that will advance our understanding of: 1) what knowledge is captured in monolingual and cross-lingual embedding spaces (gerz et al., 2018; pires et al., 2019; artetxe et al., 2020); 2) how that knowledge complements or overlaps with linguistic knowledge compiled into lexical-semantic and typological databases (dryer and haspelmath, 2013; wichmann et al., 2018; ponti et al., 2019); and 3) how to use the combined knowledge for more effective transfer in cross-lingual nlp applications (ponti et al., 2018; eisenschlos et al., 2019)."
2020.emnlp-main.186.txt,"
6 Further Discussion and Conclusion
",2020,we believe that the main insights from this study will inform and guide different cross-lingual transfer learning methods and scenarios in future work.
2020.emnlp-main.187.txt,"
10 Conclusion
",2020,"we plan to study how to deeply incorporate our typologically-enriched embeddings in multilingual nmt, where there are promising avenues in parameter selection (sachan and neubig, 2018) and generation (platanios et al., 2018)."
2020.emnlp-main.189.txt,"
6 Conclusion
",2020,"we showed that blanc increases reading comprehension performance, and we verify that the performance gain increases for complex examples (i.e., when the answer occurs two or more times in the passage)."
2020.emnlp-main.19.txt,"
5 Conclusions
",2020,"as future work, we would like to investigate complementary attention mechanisms like those of reformer (kitaev et al., 2020) or routing transformer (roy et al., 2020), push scalability with ideas like those from revnet (gomez et al., 2017), and study the performance of etc in datasets with even richer structure."
2020.emnlp-main.191.txt,"
5 Conclusion
",2020,"in future, we plan to explore how to incorporate discourse parsing into the current decision making model for end-to-end learning."
2020.emnlp-main.192.txt,"
7 Conclusion
",2020,"in the future, we will make a model learn commonsense with the obtained dataset and consider applying it to semantic tasks, such as anaphora resolution and discourse parsing."
2020.emnlp-main.195.txt,"
6 Conclusion
",2020,we explored transfer learning techniques to enable high performance cross-lingual amr parsing.
2020.emnlp-main.197.txt,"
7 Conclusion and Future Work
",2020,"as a future research, semantically challenging cases at fine-grained level with respect to complexities of abusive/offensive (targeted) and profane (untargeted) language demand further investigation."
2020.emnlp-main.198.txt,"
4 Discussion and Conclusions
",2020,"we also plan to add domain-specific features to our model, collect more data, integrate existing suicidal risk datasets with various languages to improve performance."
2020.emnlp-main.199.txt,"
6 Conclusion
",2020,"as unpreventable as selection bias in social data can be, we believe there is a way to mitigate it by incorporating evaluation as a step which directs the construction of a new dataset or when combining existing corpora."
2020.emnlp-main.200.txt,"
6 Conclusion
",2020,"in addition, it is worthwhile to further model information propagation and temporal correlation of comments in the future."
2020.emnlp-main.200.txt,"
6 Conclusion
",2020,such results can encourage future studies to develop advanced graph neural networks in better representing the interactions between heterogeneous information.
2020.emnlp-main.202.txt,"
5 Summary and Conclusions
",2020,"this raises the question of how ssnmt will perform on really distant languages (less homographs) or when using smaller bpe sizes (more homographs), which is something that we will examine in our future work."
2020.emnlp-main.204.txt,"
8 Conclusions
",2020,we hope that our new datasets and our reflections on assumptions in low-resource settings help to foster future research in this area.
2020.emnlp-main.206.txt,"
6 Conclusions
",2020,"in terms of future work, there are many ways of improving the segmenter system that has been presented here."
2020.emnlp-main.206.txt,"
6 Conclusions
",2020,the proposed model improves the results of previous works on the europarl-st test set when evaluated with two training data setups.
2020.emnlp-main.206.txt,"
6 Conclusions
",2020,we plan to look into additional acoustic features as well as possible ways to incorporate asr information to the segmentation process.
2020.emnlp-main.207.txt,"
8 Conclusion and Future Works
",2020,"in future, we plan to design segmentation-agnostic aligners or aligners that can jointly segment and align sentences."
2020.emnlp-main.207.txt,"
8 Conclusion and Future Works
",2020,"we want to experiment more with the laser toolkit: we used laser out-of-the-box, we want to train it with our data, and modify the model architecture to improve it further."
2020.emnlp-main.207.txt,"
8 Conclusion and Future Works
",2020,"we would also like to experiment with bert (devlin et al., 2019) embeddings for similarity search."
2020.emnlp-main.208.txt,"
6 Conclusions and Future work
",2020,"firstly, we are interested in applying csp to other related nlp areas for code-switching problems."
2020.emnlp-main.208.txt,"
6 Conclusions and Future work
",2020,"secondly, we plan to investigate the pre-training objectives which are more effective in utilizing the cross-lingual alignment information for nmt."
2020.emnlp-main.208.txt,"
6 Conclusions and Future work
",2020,there are two promising directions for the future work.
2020.emnlp-main.209.txt,"
7 Conclusion
",2020,"the abc dataset focuses on a specific linguistic phenomenon that does not occur in english but is found in languages with type b reflexivization: namely, anti-reflexive gendered pronouns."
2020.emnlp-main.210.txt,"
6 Conclusion
",2020,"in future work, we will pre-train on larger corpus to further boost the performance."
2020.emnlp-main.210.txt,"
6 Conclusion
",2020,we leave different alignment approaches to be explored in the future.
2020.emnlp-main.212.txt,"
5 Conclusion
",2020,"in the future, we will continue investigate the learning method for effectively utilizing self-generated samples and expand to other text generation tasks."
2020.emnlp-main.212.txt,"
5 Conclusion
",2020,"our work can employ on different text generation tasks, e.g., text summarization and dialogue, to enhance the key phrases (or terms) generation."
2020.emnlp-main.213.txt,"
8 Conclusions and Future Work
",2020,"a primary avenue for future work on comet will look at the impact of more compact solutions such as distilbert (sanh et al., 2019)."
2020.emnlp-main.213.txt,"
8 Conclusions and Future Work
",2020,future work will investigate the optimality of this formulation and further examine the interdependence of the different inputs.
2020.emnlp-main.214.txt,"
6 Conclusions
",2020,"in future work, we will apply our method to languages with corpora from diverse domains and also to other languages."
2020.emnlp-main.218.txt,"
5 Conclusion
",2020,"in the future work, we will extend our method by replacing the simple role matching score with grammatical or semantic similaritybased measures to improve the alignment accuracy."
2020.emnlp-main.219.txt,"
5 Conclusion
",2020,future research could explore neural architectures and training losses tailored to our approach.
2020.emnlp-main.22.txt,"
6 Discussions and Future Work
",2020,adapting the current methods to be robust to an active eavesdropper who may alter the cover text is another interesting direction.
2020.emnlp-main.22.txt,"
6 Discussions and Future Work
",2020,"second, we will study whether this current method is still effective when a small-scale neural lm (e.g., distilgpt-2) is applied."
2020.emnlp-main.22.txt,"
6 Discussions and Future Work
",2020,there are several directions we will further explore in the future.
2020.emnlp-main.220.txt,"
6 Discussion and Conclusion
",2020,we hope this is a factor that designers of future syntactic treebanks will take into account.
2020.emnlp-main.222.txt,"
4 Conclusion
",2020,our future work will include conducting experiments on dependency trees and more nlp tasks.
2020.emnlp-main.224.txt,"
8 Conclusion
",2020,"in future work, we plan to extend the annotation process to also cover inter-sentential relations."
2020.emnlp-main.225.txt,"
7 Conclusion
",2020,"in future, we plan to evaluate disa on other discourse analysis tasks."
2020.emnlp-main.226.txt,"
5 Future Work
",2020,one way to mitigate this issue is to leverage longer context information to identify better keywords which is subject of the future work.
2020.emnlp-main.226.txt,"
5 Future Work
",2020,other interesting direction may include incorporating the structure-level controllability by adding it as either an extra input for the conditional generator or a multitask learning supervision for each sequence.
2020.emnlp-main.226.txt,"
5 Future Work
",2020,"we also observed that in some cases during the generation, our model simply mentions the given word in the sentence, and talks about things that are not strictly related to or restricted by the given word."
2020.emnlp-main.226.txt,"
5 Future Work
",2020,"writingprompts (fan et al., 2018)) is a subject for future work."
2020.emnlp-main.227.txt,"
6 Conclusion & Future Work
",2020,"in the future, we will investigate on extending our approach to more areas."
2020.emnlp-main.229.txt,"
4 Conclusion
",2020,"however,as discussion in error analysis, there are several challenges to solve in the future."
2020.emnlp-main.230.txt,"
6 Discussion
",2020,"an alternate approach, which we did not compare with, is automatic template generation (biran et al., 2016; wiseman et al., 2018)."
2020.emnlp-main.233.txt,"
6 Conclusion
",2020,"this paper presents lifelong language knowledge distillation (l2kd), a simple method that effectively help lifelong language learning models to maintain good performance comparable to its multitask upper bounds."
2020.emnlp-main.234.txt,"
Conclusion
",2020,we hope these contributions enable wider use of bayesian nonparametrics for large collections of text.
2020.emnlp-main.235.txt,"
5 Conclusion
",2020,"as future work, we will further study our method’s ability of extreme multi-label learning (bhatia et al., 2016) and different document encoders."
2020.emnlp-main.237.txt,"
5 Conclusion and Future Work
",2020,"in the future, we will explore to extend the idea of disentanglement in the continual learning of other nlp tasks."
2020.emnlp-main.239.txt,"
5 Conclusion and Future Work
",2020,"in the future, we plan to study different regularizers in the asymmetrical text matching task, for further exploring their effectiveness in bridging the gap between asymmetrical domains."
2020.emnlp-main.24.txt,"
8 Discussion and Conclusions
",2020,"first, the word clouds may reveal sensitive contents in the training data to human debuggers."
2020.emnlp-main.24.txt,"
8 Discussion and Conclusions
",2020,"for future work, it would be interesting to extend find to other nlp tasks, e.g., question answering and natural language inference."
2020.emnlp-main.24.txt,"
8 Discussion and Conclusions
",2020,"in order to generalize the framework beyond cnns, there are two questions to consider."
2020.emnlp-main.24.txt,"
8 Discussion and Conclusions
",2020,"we exemplified this with two word clouds representing each bilstm feature in appendix c, and we plan to experiment with advanced visualizations such as lstmvis (strobelt et al., 2018) in the future."
2020.emnlp-main.241.txt,"
5 Conclusion
",2020,"in the future, we will focus our attention on the topic of generalization in the presence of domain differences such as novel objects; and given goal statements in test games that were not seen by the agent during training."
2020.emnlp-main.243.txt,"
7 Conclusion
",2020,"in future work, the supporting span annotation can be added to the datasets of a task-oriented dialog system, for the reason that supporting span serves as a bridge between diverse descriptions of users and the normative values in the system."
2020.emnlp-main.244.txt,"
6 Discussions and Future Works
",2020,"on gpus we cannot expect a reduction in the number of operations to translate 1:1 to lower execution times, since they are highly optimised for parallelism.3 we leave the parallelism enhancements of skylinebuilder for future work."
2020.emnlp-main.245.txt,"
6 Conclusion
",2020,we have additionally presented some analysis of ropes that should inform future work on this dataset.
2020.emnlp-main.245.txt,"
6 Conclusion
",2020,"while our model is not a neural module network, as our model uses a single fixed layout instead of different layouts per question, we believe there are enough similarities that future work could explore combining our modules with those used in other neural module networks over text, leading to a single model that could perform the necessary reasoning for multiple different datasets."
2020.emnlp-main.246.txt,"
7 Discussion and Future Work
",2020,we assume for the purposes of this study that questions have an answer span contained in a single document and leave an extension to multi-hop questions and unanswerable questions to future research.
2020.emnlp-main.246.txt,"
7 Discussion and Future Work
",2020,we deliberately choose to leave experiments including real human annotators to future research for the following reason.
2020.emnlp-main.249.txt,"
5 Conclusion & Future Work
",2020,"additional future investigations may include a deeper analysis of the mathematical and statistical properties of the weighted coefficients ρw, τw, as well as a rigorous derivation of the optimal values for the parameters of the data collection approach."
2020.emnlp-main.249.txt,"
5 Conclusion & Future Work
",2020,"as future work, we plan to collect human annotations (i) to test the proposed data collection approach on real data and (ii) to assess the validity and estimate the parameters of the proposed stochastic transitivity model."
2020.emnlp-main.25.txt,"
5 Conclusion and Future Work
",2020,"other potential document types that could be considered are pdfs, doc etc.and urls without content (e.g.login, tracking)."
2020.emnlp-main.25.txt,"
5 Conclusion and Future Work
",2020,we plan to address these challenges in future work.
2020.emnlp-main.250.txt,"
5 Conclusion and Future Work
",2020,"in the future, we plan to apply mft to other language models (e.g., transformerxl (dai et al., 2019) and albert (lan et al., 2019)) and for other nlp tasks."
2020.emnlp-main.251.txt,"
6 Conclusions
",2020,"in future work, we will focus on producing novel continuations of the user’s search intent, extending the approach to other domains, and automating the design of behavioral hypotheses."
2020.emnlp-main.251.txt,"
6 Conclusions
",2020,qualitative evaluation for open-ended generation is also an interesting topic on the roadmap.
2020.emnlp-main.252.txt,"
7 Conclusion and Future Work
",2020,there remain many important and interesting problems ahead of us.
2020.emnlp-main.253.txt,"
6 Conclusion
",2020,"nonetheless, the missing performance of 28-46% (depending on the evaluation scenario) encourages future research on this area to take this corpus as a challenging yet reliable evaluation benchmark for further development of models specific to this domain."
2020.emnlp-main.258.txt,"
8 Conclusion and Future Work
",2020,"in the future, we plan to apply fa-rnn to other tasks and explore other variants of fa-rnn."
2020.emnlp-main.26.txt,"
7 Discussion and conclusion
",2020,"finally, we believe that using attention mechanisms to study the grounding of the edits, similarly to the ideas in kohn ¨ (2018), can be an important step towards understanding how the preliminary representations are built and decoded; we want to test this as well in future work."
2020.emnlp-main.26.txt,"
7 Discussion and conclusion
",2020,we show that bidirectional encoders can be adapted to work under an incremental interface without a too drastic impact on their performance.
2020.emnlp-main.264.txt,"
6 Conclusion and Future Works
",2020,we discussed the current line of cqa work and proposed future directions by outlining the limitations of the current datasets and models.
2020.emnlp-main.269.txt,"
6 Conclusion
",2020,"besides, it can either rank or generate answers seamlessly."
2020.emnlp-main.270.txt,"
6 Conclusion
",2020,"in light of these results, it would be interesting to explore the use of unsupervised tokenisers that work well for languages without spaces (e.g."
2020.emnlp-main.271.txt,"
7 Conclusion
",2020,we believe that the idea of subinstruction module and a sub-instruction annotated dataset can benefit future studies in the vln task as well as other vision-and-language problems.
2020.emnlp-main.273.txt,"
5 Conclusion
",2020,"in future work, we plan to explore taskoriented dialogues domain-adaptive pre-training methods (wu et al., 2020; peng et al., 2020) to enhance our language model backbones, and extend the framework for mixed chit-chat and taskoriented dialogue agents (madotto et al., 2020a)."
2020.emnlp-main.274.txt,"
5 Conclusion
",2020,"we would also like to explore different implementations in line with recent advances in dialog models, especially using large-scale pre-trained language models."
2020.emnlp-main.275.txt,"
7 Conclusion
",2020,"in the future, we would explore three aspects: (1) more efficient posterior information representation and corresponding prediction module, (2) the interpretability of knowledge selection and (3) knowledge selection without knowledge label."
2020.emnlp-main.278.txt,"
5 Conclusions
",2020,this might be an interesting topic for future work.
2020.emnlp-main.28.txt,"
6 Conclusion
",2020,"for the future work, we suggest to integrate the ranking models and generation model, e.g., in beam search stage or reinforcement learning using ranking score as reward signal."
2020.emnlp-main.283.txt,"
6 Conclusion
",2020,for future work it would be interesting to test these sense embeddings in a wider range of applications outside wsd.
2020.emnlp-main.285.txt,"
10 Conclusion
",2020,"as future work, we plan to exploit the information brought by our embeddings to other downstream tasks, such as multilingual semantic role labeling (di fabio et al., 2019; conia et al., 2020) and cross-lingual semantic parsing (blloshmi et al., 2020)."
2020.emnlp-main.287.txt,"
5 Conclusion
",2020,ac-mimlln predicts the sentiment of an aspect category mentioned in a sentence by aggregating the sentiments of the words indicating the aspect category in the sentence.
2020.emnlp-main.287.txt,"
5 Conclusion
",2020,"in some sentences, phrases or clauses rather than words indicate the given aspect category, future work could consider multi-grained instances, including words, phrases and clauses."
2020.emnlp-main.287.txt,"
5 Conclusion
",2020,"since ac-mimlln finds the key instances for the given aspect category and predicts the sentiments of the key instances, it is more interpretable."
2020.emnlp-main.287.txt,"
5 Conclusion
",2020,"since directly finding the key instances for some aspect categories is ineffective, we will try to first recognize all opinion snippets in a sentence, then assign these snippets to the aspect categories mentioned in the sentence."
2020.emnlp-main.288.txt,"
5 Conclusion
",2020,"one future direction is to investigate how to integrate the two different attention mechanisms, namely the standard attention and structured attention for nlp applications."
2020.emnlp-main.289.txt,"
4 Conclusion
",2020,"in the future, we will explore the extension of this approach to achieve full coverage."
2020.emnlp-main.29.txt,"
8 Discussion and Conclusion
",2020,"automatic constraint induction from database content and schema descriptions might also be possible, which is on its own an open research problem."
2020.emnlp-main.29.txt,"
8 Discussion and Conclusion
",2020,"however, it is necessary to revisit and verify this hypothesis some time later due to goodhardt’s law, since researchers will optimize over our metric."
2020.emnlp-main.29.txt,"
8 Discussion and Conclusion
",2020,"nevertheless, to completely solve this issue, we recommend future dataset builders to explicitly define the database generation procedure."
2020.emnlp-main.29.txt,"
8 Discussion and Conclusion
",2020,our test suites will be released for eleven datasets so that future works can conveniently evaluate test suite accuracy.
2020.emnlp-main.291.txt,"
6 Conclusion
",2020,"furthermore, we would like to investigate other approaches (e.g., graph-based neural network) to better model the modality and label dependence in multi-modal multi-label emotion detection."
2020.emnlp-main.291.txt,"
6 Conclusion
",2020,"in our future work, we will extend our approach to more multi-modal multi-label scenarios, such as intention detection in video conversations and aspect analysis in multi-modal reviews."
2020.emnlp-main.296.txt,"
6 Conclusions
",2020,"in the future, we would like to generate abstractive summaries following an unsupervised approach (baziotis et al., 2019; chu and liu, 2019) and investigate how recent advances in open domain qa (wang et al., 2019; qi et al., 2019) can be adapted for query focused summarization."
2020.emnlp-main.297.txt,"
6 Conclusion
",2020,"in the future, we would like to investigate other objectives to pre-train seq2seq models for abstractive summarization."
2020.emnlp-main.297.txt,"
6 Conclusion
",2020,"we proposed three sequence-to-sequence pretraining objectives, including sentence reordering, next sentence generation, and masked document generation."
2020.emnlp-main.298.txt,"
6 Conclusion
",2020,"in the future, we will continue to explore better re pre-training techniques, especially with a focus on open relation extraction and relation discovery."
2020.emnlp-main.3.txt,"
6 Conclusion
",2020,"in future work, we would like to improve comment matching, e.g., by making it stance-aware."
2020.emnlp-main.3.txt,"
6 Conclusion
",2020,we also plan to experiment with sequence-tosequence neural models for generating key point candidates from comments.
2020.emnlp-main.300.txt,"
5 Conclusion
",2020,"in the future, we will explore how to improve the efficiency of our pre-training."
2020.emnlp-main.301.txt,"
7 Conclusion
",2020,and we call for a unified end-to-end re evaluation setting to prevent future mistakes and enable more meaningful cross-domain comparisons.
2020.emnlp-main.302.txt,"
7 Discussion and Conclusion
",2020,"we suggest the following recommendation for future re data collection: evaluation sets should be exhaustive, and contain all relevant entity pairs."
2020.emnlp-main.303.txt,"
5 Conclusion
",2020,"in future work, we plan to integrate knowledge graphs and explore other document graph modeling ways (e.g., hierarchical graphs) to improve the performance."
2020.emnlp-main.304.txt,"
6 Conclusion
",2020,"as future work, we intend to explore its application in those fields."
2020.emnlp-main.306.txt,"
9 Conclusion
",2020,we plan to explore the utility of this architecture in other nlp problems.
2020.emnlp-main.308.txt,"
6 Conclusion
",2020,"as future work, we plan to generalize the ept to other datasets, including non-english word problems or non-algebraic domains in math, to extend our model."
2020.emnlp-main.31.txt,"
7 Discussion
",2020,"future work is also needed to handle attributes containing long free-form text, as autoqa currently only supports database operations without reading comprehension."
2020.emnlp-main.310.txt,"
4 Conclusion
",2020,"in the future, we would like to extend gtm to corpora with explicit doc-doc interactions, e.g., scientific documents with citations or social media posts with user relationships."
2020.emnlp-main.310.txt,"
4 Conclusion
",2020,replacing gcn in gtm with more advanced graph neural networks is another promising research direction.
2020.emnlp-main.311.txt,"
5 Conclusion and Future Work
",2020,"for example, the clustering ability of routing algorithm can be used to control the style of generated texts."
2020.emnlp-main.311.txt,"
5 Conclusion and Future Work
",2020,there are several directions to explore in the future.
2020.emnlp-main.312.txt,"
6 Conclusion
",2020,"in the future, we would like to explore if the learner-like agent can be extended to materials and data beyond the example sentences for near-synonyms."
2020.emnlp-main.313.txt,"
7 Conclusion and Future Work
",2020,"moreover, we will conduct further exploration of the multiproduct ad post form, including more vivid multimedia information, such as pictures and videos."
2020.emnlp-main.318.txt,"
5 Conclusion
",2020,"in the future, we will continue studying the efficiency of the neural architecture, and pay attention to improving the speed of both training and testing steps on an ever-increasing dataset."
2020.emnlp-main.319.txt,"
5 Conclusions
",2020,"we also plan to extend our framework to semi-supervised learning, where a small number of annotations might also be available in the target language."
2020.emnlp-main.32.txt,"
5 Conclusion
",2020,extending our method to an abstractive setting is meaningful future work.
2020.emnlp-main.321.txt,"
5 Conclusions
",2020,future work should address the application of our method to more and typologically more divergent languages.
2020.emnlp-main.322.txt,"
7 Conclusions
",2020,"given that gcns over dependency and constituency structure have access to very different information, it would be interesting to see in future work if combining two types of representations can lead to further improvements."
2020.emnlp-main.323.txt,"
7 Conclusion
",2020,"in future work, one could make the a* parser more accurate by extending it to non-projective dependency trees, especially on dm, eds and amr."
2020.emnlp-main.323.txt,"
7 Conclusion
",2020,it would also be interesting to see if our method for avoiding dead ends can be applied to other formalisms with complex symbolic restrictions.
2020.emnlp-main.324.txt,"
5 Final Remarks
",2020,"in image classification problems, for instance, word graphs related to visual words could be computed."
2020.emnlp-main.327.txt,"
7 Discussion
",2020,future work should investigate more rewards for training an open-domain dialog model such as long term conversation rewards that may need to be computed over many conversation turns.
2020.emnlp-main.333.txt,"
6 Conclusion
",2020,"in the future, we plan to apply the transformed representations on more lexical semantics tasks such as word sense disambiguation within an application (navigli and vannella, 2013)."
2020.emnlp-main.334.txt,"
8 Conclusions
",2020,"to support future work that can help model individuals and demographics, our code is available at http://lit.eecs.umich.edu."
2020.emnlp-main.336.txt,"
6 Conclusion
",2020,"in the future, we plan to annotate some of the data, explore supervised segmentation models (li et al., 2018) and introduce more conversation structures like dialogue acts (oya and carenini, 2014; joty and hoque, 2016) into abstractive dialogue summarization."
2020.emnlp-main.336.txt,"
6 Conclusion
",2020,"via thorough error analyses, we concluded a set of challenges that current models struggled with, which can further facilitate future research on conversation summarization."
2020.emnlp-main.338.txt,"
4 Conclusion
",2020,future work may explore the use of points of correspondence and sentence fusion in the standard setting of document summarization.
2020.emnlp-main.339.txt,"
8 Conclusion
",2020,"future work will focus on extending our models to generate extractive plans for better abstractive summarization of long or multiple documents (liu et al., 2018)."
2020.emnlp-main.340.txt,"
5 Conclusion and future work
",2020,"for future work, we think it will be interesting to look at: 1. zero-shot clir models for low-resource languages, 2. comparison of end-to-end neural rankers with traditional translation+ir pipelines in terms of both scalability, cost, and retrieval accuracy, 3. advanced neural architectures and training algorithms that can exploit our large training data, 4. building universal models for multilingual ir."
2020.emnlp-main.341.txt,"
6 Conclusion
",2020,we hope that techniques like sledge-z can help overcome the global covid-19 crisis.
2020.emnlp-main.342.txt,"
7 Conclusion
",2020,these findings provide opportunities for future work towards more efficient and interpretable neural ir.
2020.emnlp-main.342.txt,"
7 Conclusion
",2020,"we further discovered two types of interaction: further query understanding based on the document, and the query to document tokens matching for relevance."
2020.emnlp-main.343.txt,"
5 Conclusions and Future work
",2020,"as a future work, we plan to utilize automatic summarization for missing abstracts, instead of taking the first 512 content tokens."
2020.emnlp-main.345.txt,"
6 Conclusion and Future Work
",2020,one avenue of future work is to learn explainable rules that domain experts can interact with on top of such embeddings.
2020.emnlp-main.345.txt,"
6 Conclusion and Future Work
",2020,we show that it is possible to learn human-interpretable models by designing neural networks keeping explainability in mind.
2020.emnlp-main.345.txt,"
6 Conclusion and Future Work
",2020,"while recent embeddings (devlin et al., 2019) may lead to improved accuracy, these remain poorly understood (moradshahi et al., 2019)."
2020.emnlp-main.35.txt,"
6 Conclusion and future work
",2020,"in the future, we will study the effectiveness of ta on other nlp tasks, such as the document-level translation, and investigate whether ta is useful for transformer pre-training."
2020.emnlp-main.350.txt,"
7 Conclusion
",2020,a promising research direction is to investigate the root cause behind memorization.
2020.emnlp-main.350.txt,"
7 Conclusion
",2020,"to learn to infer more global features, we explored alternative architectures based on bag-ofword assumptions on the encoder or decoder side, as well as a pretraining procedure."
2020.emnlp-main.351.txt,"
8 Conclusion and Future Work
",2020,our findings also suggest future work on additional ways to incorporate story principles into plot generation.
2020.emnlp-main.353.txt,"
7 Conclusion
",2020,"we believe that the resulting dataset of referring utterance chains can be a useful resource to analyse and model other dialogue phenomena, such as saliency or partner specificity, both on language alone or on the interaction of language and vision."
2020.emnlp-main.355.txt,"
7 Conclusion
",2020,"in the future, we plan to examine the hierarchical classification architecture’s potential for reducing computational runtime."
2020.emnlp-main.355.txt,"
7 Conclusion
",2020,we extend the concept of active learning to class-based active learning for choosing the most informative query pair.
2020.emnlp-main.356.txt,"
6 Conclusion
",2020,this is obtained as a by-product of significant work on the annotation tooling itself and designing the process to be more natural for guides.
2020.emnlp-main.357.txt,"
5 Conclusion and Future Work
",2020,"for the real world, both visual and linguistic will be more complicated."
2020.emnlp-main.358.txt,"
7 Conclusion
",2020,in future work we plan to incorporate crosslingual signals as vulic et al.´ (2019) argue that a fully unsupervised setting is hard to motivate.
2020.emnlp-main.36.txt,"
5 Conclusion
",2020,"for future work, we plan to investigate the use of a more powerful language model, such as megatron-lm (shoeybi et al., 2019), as the teacher; and different strategies for choosing hard negatives to further boost the performance."
2020.emnlp-main.361.txt,"
7 Conclusion
",2020,"future work will investigate the compositional capability of these adapters, and combine domain and monolingual adapters for nmt."
2020.emnlp-main.362.txt,"
5 Discussion
",2020,"additionally, no alignment methods improve xlmr and larger xlmrlarge performs much better, and raw text is easier to obtain than bitext."
2020.emnlp-main.362.txt,"
5 Discussion
",2020,"therefore, scaling models to more raw text and larger capacity models may be more beneficial for producing better cross-lingual models."
2020.emnlp-main.362.txt,"
5 Discussion
",2020,"therefore, we make the following recommendations for future work on cross-lingual alignment or multilingual representations: 1) evaluations should consider average quality data, not exclusively high-quality bitext.2) evaluation must consider multiple nlp tasks or datasets.3) evaluation should report mean and variance over multiple seeds, not a single run."
2020.emnlp-main.363.txt,"
5 Conclusion
",2020,"this finding provides a strong incentive for intensifying future research efforts that focus on cheap or naturally occurring supervision (vulic et al.´ , 2019; artetxe et al., 2020c; marchisio et al., 2020), quick and simple annotation procedure, and the more effective few-shot transfer learning setups."
2020.emnlp-main.364.txt,"
8 Conclusions
",2020,"for future work, we plan to expand the automatic domain induction methods and test the mdkd framework on generic mt with data exhibiting varying degrees of heterogeneity: as mdkd distills domain-specific models to create multiple simpler data distributions, we want to investigate if inducing train-time specializations and using them for distillation through mdkd can lead to better quality."
2020.emnlp-main.368.txt,"
7 Conclusion
",2020,future studies will extend this work to other crosslingual nlp tasks and more languages.
2020.emnlp-main.373.txt,"
7 Discussion and Conclusion
",2020,"an interesting future direction is to generate each clarification in response to the previous ones, in a dialogue setup (saeidi et al., 2018)."
2020.emnlp-main.373.txt,"
7 Discussion and Conclusion
",2020,we hope that our framework will facilitate future research in this area.
2020.emnlp-main.375.txt,"
5 Discussion
",2020,scaling these carefully-controlled methods to the larger data setting will be an important next step.
2020.emnlp-main.376.txt,"
6 Conclusions
",2020,"another line of future research is to compare the incremental predictions of neural models to finergrained eye-tracking evidence during sentence processing of double-object sentences (e.g.filik et al., 2004)."
2020.emnlp-main.377.txt,"
9 Conclusions
",2020,"in the future, our approach and new evaluation measure could be applied to larger eyetracking datasets, such as the english dataset by he et al.(2019)."
2020.emnlp-main.377.txt,"
9 Conclusions
",2020,we leave testing whether the reported pattern of results holds across different languages to future work.
2020.emnlp-main.378.txt,"
6 Discussion
",2020,one future direction is to consider more sophisticated mechanisms to gain stronger controlability over longer sentences while maintaining the compactness of latent representations.
2020.emnlp-main.378.txt,"
6 Discussion
",2020,we hope that this paper will help renew interest in dgms for this purpose.
2020.emnlp-main.38.txt,"
7 Conclusion
",2020,"this opens up the possibility of exploring large-scale meta-learning in nlp for various meta problems, including neural architecture search, continual learning, hyperparameter learning, and more."
2020.emnlp-main.380.txt,"
7 Conclusion
",2020,"in future work, we plan to further investigate how different techniques apply to the problem of text segmentation, including data augmentation (wei and zou, 2019; lukasik et al., 2020b) and methods for regularization and mitigating labeling noise (jiang et al., 2020; lukasik et al., 2020a)."
2020.emnlp-main.383.txt,"
6 Discussions and future work
",2020,"the data size for yahoo finance and news, while being one of the largest in the context of hatespeech classification, is nevertheless small in the context of language modeling."
2020.emnlp-main.383.txt,"
6 Discussions and future work
",2020,we plan to perform large-scale pre-training and evaluation on glue datasets for the comprehensive analysis.
2020.emnlp-main.383.txt,"
6 Discussions and future work
",2020,we will continue to explore this line in the future.
2020.emnlp-main.384.txt,"
6 Conclusion
",2020,"in our future work, we would like to extend this idea to unseen numbers in vocabulary as a function of seen ones."
2020.emnlp-main.385.txt,"
6 Conclusion
",2020,future work might explore combining more expressive flows with discrete latent variables.
2020.emnlp-main.387.txt,"
8.1 Future Work
",2020,"the next step towards this goal would be to recognize when characters refer to one another, and how this contributes to the movie-level risk behavior rating."
2020.emnlp-main.387.txt,"
8.1 Future Work
",2020,"we hope this leads to tools that can be helpful during the creative process, rather than after the fact."
2020.emnlp-main.389.txt,"
9 Conclusion
",2020,"furthermore, we used the interpretability of constituency tests to highlight and explain the parser’s strengths and shortcomings, like the “[ subject verb ]” and “adverb [ adjective noun ]” misbracketings, revealing potential next steps for improvement."
2020.emnlp-main.39.txt,"
5 Conclusion
",2020,"in this work, we identify three principles underlying different lifelong language learning methods and show how to unify them in a meta-lifelong framework."
2020.emnlp-main.39.txt,"
5 Conclusion
",2020,"our analysis also shows that negative transfer is an overlooked factor that could cause sub-optimal performance, and we highlight the importance of balancing the trade-off tween catastrophic forgetting and negative transfer for future work."
2020.emnlp-main.390.txt,"
4 Conclusion
",2020,our hope is that this paper will remind future research in dependency parsing to please mind the root.
2020.emnlp-main.391.txt,"
5 Conclusion and Future Work
",2020,"in addition, stem and morpheme information can be utilized as additional signals in training."
2020.emnlp-main.391.txt,"
5 Conclusion and Future Work
",2020,"in the future, we plan to enhance the system for handling morphologically complex languages trough unsupervised morphological segmentation."
2020.emnlp-main.391.txt,"
5 Conclusion and Future Work
",2020,one approach is to perform the alignment and projection on the stem and morpheme levels.
2020.emnlp-main.394.txt,"
9 Conclusion
",2020,"in the future work wish to explore more data independent methods such as lrc, for both speed and lack of data dependency, as well as manipulation of the decay w.r.t.what we have discovered from our layer-wise analysis."
2020.emnlp-main.396.txt,"
8 Conclusion
",2020,"first, the approach could apply the same meta-learning approach to other classes of tasks beyond span id."
2020.emnlp-main.397.txt,"
9 Conclusions and future directions
",2020,"future work can apply these tests to a broader range of models, and continue to develop controlled tests that target encoding of complex compositional meanings, both for two-word phrases and for larger meaning units."
2020.emnlp-main.397.txt,"
9 Conclusions and future directions
",2020,we hope that our findings will stimulate further work on leveraging the power of these generalized transformers while improving their capacity to capture compositional meaning.
2020.emnlp-main.4.txt,"
6 Conclusion and Future Work
",2020,"therefore, we plan to complement this work with approaches for other frequently applied schemes such as arguments by expert opinion and arguments by example."
2020.emnlp-main.40.txt,"
7 Recommendations and discussion
",2020,"to avoid unexpected variation in future cross-lingual publications, we recommend that authors report oracle accuracies alongside their zero-shot results."
2020.emnlp-main.400.txt,"
8 Conclusion
",2020,"future work can explore representations for rare or unseen entities, as well as developing less memory-intensive ways to learn and integrate entity representations."
2020.emnlp-main.402.txt,"
5 Summary and Outlook
",2020,future work may investigate whether these results translate to other language models besides roberta as well as other training datasets besides winogrande.
2020.emnlp-main.403.txt,"
6 Conclusion
",2020,we hope the insights provided here will help guide the development of better language models in the future.
2020.emnlp-main.404.txt,"
6 Conclusion and Future Work
",2020,"in future work, we plan to explore topic-level bias prediction as well as going beyond left-centerright bias."
2020.emnlp-main.404.txt,"
6 Conclusion and Future Work
",2020,"in particular, we created a new large dataset for this task, which features article-level annotations and is well-balanced across topics and media."
2020.emnlp-main.404.txt,"
6 Conclusion and Future Work
",2020,"last but not least, we plan to experiment with other languages, and to explore to what extent a model for one language is transferable to another one given that the left-center-right division is not universal and does not align perfectly across countries and cultures, even when staying within the western political world."
2020.emnlp-main.405.txt,"
5 Conclusion
",2020,"in future work, we plan to apply our semantic label smoothing technique to various sequence to sequence problems, including text summarization (zhang et al., 2019) and text segmentation (lukasik et al., 2020b)."
2020.emnlp-main.405.txt,"
5 Conclusion
",2020,we also plan to study the relation between pretraining and data augmentation techniques.
2020.emnlp-main.408.txt,"
6 Conclusions
",2020,"further, to advance session-based task-oriented semantic parsing, we release to the public a new dataset of roughly 20k sessions (over 60k utterances)."
2020.emnlp-main.409.txt,"
6 Conclusion
",2020,"from the ranking results of two different probings, we show a list of interesting observations to provide model selection guidelines and shed light on future research towards a more advanced language modeling learning for dialogue applications."
2020.emnlp-main.41.txt,"
6 Conclusion
",2020,"future works include using other multilingual pretraining models such as xlm-roberta (conneau et al., 2019) for a more accurate model and distilmbert (sanh et al., 2019) for a more compact model."
2020.emnlp-main.41.txt,"
6 Conclusion
",2020,"we have already applied it to bilingual sentence alignment (chousa et al., 2020) and we plan to extend it to other related problems."
2020.emnlp-main.410.txt,"
7 Conclusion
",2020,we release our multiatis++ corpus to facilitate future research on cross-lingual nlu to bridge the gap between cross-lingual transfer and supervised methods.
2020.emnlp-main.411.txt,"
7 Conclusion
",2020,future work includes its cross-lingual transfer and cross-dataset (or cross-task) generalization.
2020.emnlp-main.412.txt,"
4 Conclusion
",2020,"in the future, we plan to explore maskaugment for other tasks in nlp domain."
2020.emnlp-main.413.txt,"
7 Conclusion
",2020,"in particular, we focus on the 25 spis setting to investigate whether a model can effectively adapt to new domains with a very limited amount of training data."
2020.emnlp-main.415.txt,"
6 Conclusions
",2020,"in the future, we plan to explore this approach with other language pairs and other generation tasks."
2020.emnlp-main.416.txt,"
6 Discussion
",2020,future work can aim to propose more general formulations that encapsulate more properties of the circumstance.forms of assistance.
2020.emnlp-main.416.txt,"
6 Discussion
",2020,"future work can consider adapting experiment designs from prior work (gao et al., 2015; hohenstein and jung, 2018) to establish the impact of offering such intention-preserving paraphrases in real conversations, potentially by considering downstream outcomes."
2020.emnlp-main.416.txt,"
6 Discussion
",2020,"future work may consider more complex stylistic aspects and strategies that are more tied to the content, such as switching from active to passive voice."
2020.emnlp-main.416.txt,"
6 Discussion
",2020,"future work may consider more comprehensive modeling of how people form politeness perceptions or obtain more reliable causal estimates for strategy strength (wang and culotta, 2019).task formulation."
2020.emnlp-main.416.txt,"
6 Discussion
",2020,the results and limitations of our method open up several natural directions for future work.modeling politeness perceptions.
2020.emnlp-main.417.txt,"
5 Conclusion and Discussion
",2020,"as for future directions, one natural extension is how we can automatically identify those attributes."
2020.emnlp-main.418.txt,"
5 Discussion
",2020,"another line of future work is to extend our model to sequence rewriting tasks, such as machine translation post-editing, that do not have existing error-tag dictionaries."
2020.emnlp-main.418.txt,"
5 Discussion
",2020,"even though our approach is open-vocabulary, future work will explore task specific restrictions."
2020.emnlp-main.418.txt,"
5 Discussion
",2020,"for example, in a model for dialog applications, we may want to restrict the set of response tokens to a predefined list."
2020.emnlp-main.419.txt,"
10 Conclusion
",2020,"additionally, we identify limitations to this ability, specifically in the small data, random permutation setting, and will focus on this going forward."
2020.emnlp-main.42.txt,"
6 Conclusion
",2020,we leave it for future work to extend our study to more downstream tasks and systems.
2020.emnlp-main.420.txt,"
5 Conclusion
",2020,"blm has plenty of future applications, including template filling, information fusion, assisting human writing, etc."
2020.emnlp-main.420.txt,"
5 Conclusion
",2020,"while we proposed blm for language generation, it would also be interesting to compare the representations learned by blm with those produced by other pre-training methods."
2020.emnlp-main.421.txt,"
5 Conclusion
",2020,"cod3s leads to more diverse outputs in a multi-target generation task in a controllable and interpretable manner, suggesting the potential of semantically guided diverse decoding for a variety of text generation tasks in the future."
2020.emnlp-main.422.txt,"
7 Future Work
",2020,"we also plan to expand our methodology for extracting grammar rules from raw text to other aspects of morphosyntax, such as argument structure and word order phenomena."
2020.emnlp-main.422.txt,"
7 Future Work
",2020,we leave a more expressive model and evaluation on more languages as future work.
2020.emnlp-main.423.txt,"
7 Conclusion
",2020,"however, we find a large gap between the emulated and the real low-resource scenarios: while accuracy is above 50% for all high-resource languages even with reduced amounts of training data, for popoluca and tepehua, our best model only obtains 37.4% and 28.4% accuracy, respectively."
2020.emnlp-main.424.txt,"
6 Conclusion
",2020,another approach would be to compare improvements between manual-only cleaning and cleaning done by a linguist working with someone who can write scripts to automatically correct repeated patterns of noise.
2020.emnlp-main.424.txt,"
6 Conclusion
",2020,better techniques for further cleaning might be useful since accuracy seems to have close related to data quality.
2020.emnlp-main.424.txt,"
6 Conclusion
",2020,"for example, the generated inflected forms could be used for automated glossing of raw text."
2020.emnlp-main.424.txt,"
6 Conclusion
",2020,"however, at some point more cleaning will return less improvement."
2020.emnlp-main.424.txt,"
6 Conclusion
",2020,in languages with the noisiest data performance is improved even further by data augmentation techniques.
2020.emnlp-main.424.txt,"
6 Conclusion
",2020,there is room for future improvement.
2020.emnlp-main.424.txt,"
6 Conclusion
",2020,we investigated the effect of manual cleaning on model performance and showed that even very limited cleaning effort (2-7 hours) drastically improves results.
2020.emnlp-main.426.txt,"
6 Conclusion
",2020,"in this work, we focused only on the protagonist, but future works can explore modeling motivations, goals, achievements, and emotional trajectory of all characters."
2020.emnlp-main.426.txt,"
6 Conclusion
",2020,this paper is a step towards future research directions on planning emotional trajectory while generating stories.
2020.emnlp-main.427.txt,"
8 Conclusion
",2020,future work needs to look into how question and reply context can improve automatic identification of advice.
2020.emnlp-main.43.txt,"
6 Conclusion and Future Work
",2020,our future work involves converting the monolingual data to parallel and collecting more data from the news domain.
2020.emnlp-main.43.txt,"
6 Conclusion and Future Work
",2020,we hope these diverse baselines will serve as useful strong starting points for future work by the community.
2020.emnlp-main.430.txt,"
9 Conclusions
",2020,"in the future, we would like to explore uses of the subevent knowledge base for other eventoriented applications such as event tracking."
2020.emnlp-main.433.txt,"
7 Conclusion
",2020,"in the future, we plan to extend our annotation to include event arguments and other properties of events."
2020.emnlp-main.435.txt,"
5 Conclusion
",2020,"in the future, we plan to apply the proposed model for the related tasks and other settings of ed, including new type extension (nguyen et al., 2016b; lai and nguyen, 2019), and few-shot learning (lai et al., 2020a,b)."
2020.emnlp-main.436.txt,"
5 Conclusions and Future Work
",2020,"for the future, instead of using the roberta baseline model for the self-training experiments, we could run several iterations by retraining on the data produced by our best self-trained model(s); this could be a good avenue for further improvements."
2020.emnlp-main.436.txt,"
5 Conclusions and Future Work
",2020,"in addition we plan to extend our work by moving to other languages beyond english (we currently have not tried this due to lack of data) using cross-lingual models, (subburathinam et al., 2019), applying other architectures like cnns (nguyen and grishman, 2015), incorporating tree structure in our models (miwa and bansal, 2016) and/or by handling jointly performing event recognition and temporal ordering (li and ji, 2014; katiyar and cardie, 2017)."
2020.emnlp-main.437.txt,"
4 Conclusion
",2020,"third, the maximum-likelihood objective used to train our model provides no guarantees as to whether a model will learn a fact or not."
2020.emnlp-main.437.txt,"
4 Conclusion
",2020,"this model size can be prohibitively expensive in resource-constrained settings, prompting future work on more efficient language models."
2020.emnlp-main.437.txt,"
4 Conclusion
",2020,"this suggests a fundamentally different approach to designing question answering systems, motivating many threads for future work: first, we obtained state-of-the-art results only with the largest models which had around 11 billion parameters."
2020.emnlp-main.438.txt,"
7 Conclusion and Future Work
",2020,"in future work, we plan to extend the dataset with more questions, more subjects, and more languages."
2020.emnlp-main.438.txt,"
7 Conclusion and Future Work
",2020,we further plan to develop new models to address the specific challenges we identified.
2020.emnlp-main.439.txt,"
5 Conclusions
",2020,improving lm-score-based filtering is a future direction of our work.
2020.emnlp-main.439.txt,"
5 Conclusions
",2020,it would be interesting to explore how one can adapt the generative models to the type of target domain questions.
2020.emnlp-main.44.txt,"
8 Limitations and Future Work
",2020,there are additional avenues for future work beyond our proposed framework.
2020.emnlp-main.44.txt,"
8 Limitations and Future Work
",2020,"while our work serves as an initial approach toward unsupervised detection of comment-level gender bias, we identify several limitations and areas for future work."
2020.emnlp-main.440.txt,"
6 Conclusion
",2020,"in future work, we aim to extend our approach to more domains and explore more generalizable approaches for unsupervised domain adaptation."
2020.emnlp-main.446.txt,"
6 Conclusion
",2020,"moving forward, we hope to improve and deploy defenses against adversarial attacks in nlp, and more broadly, we hope to make security and privacy a more prominent focus of nlp research."
2020.emnlp-main.446.txt,"
6 Conclusion
",2020,"to do so, we first explore new model vulnerabilities (i.e., threat modeling in computer security)."
2020.emnlp-main.447.txt,"
6 Conclusion
",2020,"seqmix is efficient and easy to implement, and as a secondary contribution, we provide a framework that unifies several data augmentation strategies for compositionality, which naturally suggests avenue for future research (e.g., a relaxed variant of geca)."
2020.emnlp-main.450.txt,"
7 Conclusion
",2020,"finally, snoek et al.(2019) find that deep ensembles can significantly improve outof-domain performance over single models, and we are interested in exploring whether our distillation techniques retain these benefits."
2020.emnlp-main.450.txt,"
7 Conclusion
",2020,"in future work, we are interested in exploring nat using distilled ensembles with truncated distributions, and assessing how improved calibration impacts non-sequential decoding performance."
2020.emnlp-main.456.txt,"
8 Conclusion
",2020,"we note that we could further extend our measures to fuzzy partitions, which remain less explored in community detection, but are a promising avenue for future work."
2020.emnlp-main.457.txt,"
5 Discussion
",2020,we first verify that the label consistency of words and word length have a more consistent impact on cws performance.
2020.emnlp-main.459.txt,"
6 Conclusion
",2020,"in future work, we plan to improve the quality of the additional actions."
2020.emnlp-main.460.txt,"
6 Conclusion and Future Work
",2020,"for future work, we plan to incorporate hyperbolic rnns (ganea et al., 2018) to encode auxiliary information for zero-shot entity and concept representations."
2020.emnlp-main.461.txt,"
9 Conclusion
",2020,"we plan to apply the proposed framework on various event reasoning tasks and construct novel distributional constraints that could leverage domain knowledge beyond corpus statistics, such as the larger unlabeled data and rich information contained in knowledge bases."
2020.emnlp-main.462.txt,"
5 Conclusion
",2020,future work involves exploring the generalization of temp to continuous tkgc and better imputation techniques to induce representations for infrequent and inactive entities.
2020.emnlp-main.463.txt,"
7 Conclusion
",2020,"it leads to many interesting future works, including generalizing theorem 2 to other models, designing new algorithms to automatically adapt deep networks to different training configurations, upgrading the transformer architecture, and applying our proposed admin to conduct training in a larger scale."
2020.emnlp-main.464.txt,"
6 Conclusion
",2020,"this opens a wide range of possibilities for generation tasks where monotonic orderings are not the most natural choice, and we would be excited to explore some of these areas in future work."
2020.emnlp-main.465.txt,"
5 Conclusion
",2020,for future work we would like to explore if their success transfers to other generation tasks with mlms where inference efficiency is a concern.
2020.emnlp-main.465.txt,"
5 Conclusion
",2020,we investigated inference strategies for machine translation based on cmlm with a focus on the trade-off between generation speed and quality.
2020.emnlp-main.466.txt,"
7 Conclusion & Future Work
",2020,"furthermore, future work may build on the ambigqa task with more open-ended approaches such as (1) applying the approach to qa over structured data (such as ambiguous questions that require returning tables), (2) handling questions with no answer or ill-formed questions that require inferring and satisfying more complex ambiguous information needs, and (3) more carefully evaluating usefulness to end users."
2020.emnlp-main.466.txt,"
7 Conclusion & Future Work
",2020,"future research developing on ambigqa models may include explicitly modeling ambiguity over events and entities or in the retrieval step, as well as improving performance on the difficult problems of answer recall and question disambiguation."
2020.emnlp-main.467.txt,"
5 Conclusion
",2020,"under the guidance of a pre-trained mrc model, the original question is revised in a continuous embedding space with gradient-based optimization and then decoded back to the discrete space as a new question data sample."
2020.emnlp-main.468.txt,"
8 Conclusions and Future Work
",2020,"doing so required us to scale model size for our answer generators, question generators, and filtration models."
2020.emnlp-main.468.txt,"
8 Conclusions and Future Work
",2020,extension of this work to unanswerable and boolean questions is also a future work direction.
2020.emnlp-main.468.txt,"
8 Conclusions and Future Work
",2020,"finally, we generate synthetic text from a wikipedia-finetuned gpt-2 model, generate answer candidates and synthetic questions based on those answers, and then train a bert-large model and achieve similar question answering accuracy."
2020.emnlp-main.468.txt,"
8 Conclusions and Future Work
",2020,"more generally application of this work to multi dataset question generation with datasets such as multiqa (talmor and berant, 2019) is a promising avenue for future work."
2020.emnlp-main.468.txt,"
8 Conclusions and Future Work
",2020,of particular interest for future work is handling low-resource question answering domains.
2020.emnlp-main.469.txt,"
5 Conclusion
",2020,"in the future, we plan to improve mrl-cqa by designing a retriever that could be optimized jointly with the programmer under the meta-learning paradigm, instead of manually pre-defining a static relevance function."
2020.emnlp-main.469.txt,"
5 Conclusion
",2020,"to effectively create the support sets, we propose an unsupervised retriever to find the questions that are structurally and semantically similar to the new questions from the training dataset."
2020.emnlp-main.47.txt,"
6 Conclusion
",2020,"in developing this pipeline to examine how authors depict the transmission of information within narrative texts, we hope to drive a variety of future research in this space, including not only such narratological questions as how “gossip impels plots” (spacks, 1985), but also questions pertaining to issues of bias in representation, the flow of information, and factuality."
2020.emnlp-main.470.txt,"
6 Conclusion
",2020,"finally, we would also like to apply our models to languages with even less resources available to help coping with the problem of offensive language in social media."
2020.emnlp-main.470.txt,"
6 Conclusion
",2020,"in future work, we would like to further evaluate our models using solid, a novel large english dataset with over 9 million tweets (rosenthal et al., 2020), along with datasets in four other languages (arabic, danish, greek, and turkish) that were made available for the second edition of offenseval (zampieri et al., 2020)."
2020.emnlp-main.470.txt,"
6 Conclusion
",2020,"this opens exciting new avenues for future research considering the multitude of phenomena (e.g.hate speech, aggression, cyberbulling), annotation schemes and guidelines used in offensive language datasets."
2020.emnlp-main.472.txt,"
11 Conclusion
",2020,"ultimately, we hope our work can open up new horizons for studying mds in various languages."
2020.emnlp-main.473.txt,"
5 Conclusion
",2020,we hope our findings can pave the way for further inclusion of diverse language in future nlg models.
2020.emnlp-main.474.txt,"
5 Conclusion
",2020,"in the future, we would like to extend our method to enhance the back-translation method in multidomain settings."
2020.emnlp-main.476.txt,"
6 Conclusion
",2020,"in this study, we re-evaluate the m2 model and suggest it as an appropriate choice for multilingual translation in industries."
2020.emnlp-main.477.txt,"
6 Conclusion
",2020,"it is an interesting question for future work whether strong alignment always comes at a cost, or if better training techniques will lead to models that can improve on all these measures simultaneously."
2020.emnlp-main.478.txt,"
8 Conclusion
",2020,"as future work, we plan to investigate the effect of using other available data for the three languages (for example, word lists collected by documentary linguists or the additional griko folk tales collected by anastasopoulos et al.(2018))."
2020.emnlp-main.478.txt,"
8 Conclusion
",2020,future work that focuses on overcoming the challenges of applying language-specific models to endangered language texts would be needed to confirm our method’s applicability to post-correcting the first pass transcriptions from different ocr systems.
2020.emnlp-main.478.txt,"
8 Conclusion
",2020,"future work will focus on large-scale digitization of scanned documents, aiming to expand our ocr benchmark on as many endangered languages as possible, in the hope of both easing linguistic documentation and preservation efforts and collecting enough data for nlp system development in under-represented languages."
2020.emnlp-main.479.txt,"
8 Conclusion
",2020,future directions include other pre-training or fine-tuning methods to improve retrieval performance and methods that encourage the lm to predict entities of the right types.
2020.emnlp-main.48.txt,"
8 Conclusion
",2020,comprehensive modeling of social norms presents a promising challenge for nlp work in the future.
2020.emnlp-main.482.txt,"
6 Conclusion and Future Work
",2020,another line of future work is to investigate alternative user interfaces.
2020.emnlp-main.482.txt,"
6 Conclusion and Future Work
",2020,"in the future, we plan to train a policy that dynamically combines the two interactions with reinforcement learning (fang et al., 2017)."
2020.emnlp-main.482.txt,"
6 Conclusion and Future Work
",2020,"therefore, future advances in these areas may also improve clime."
2020.emnlp-main.483.txt,"
5 Conclusion
",2020,"our method uses multilingual sentence embeddings and explicitly models the order of sentences in documents, in both candidate generation and candidate re-scoring."
2020.emnlp-main.486.txt,"
5 Conclusions
",2020,"for future work, we will apply hit to other languages, and further explore potential cases of overlapping entities in nested ner task."
2020.emnlp-main.489.txt,"
7 Discussion
",2020,"this is just a first step towards the goal of fully-automated interpretable evaluation, and applications to new attributes and tasks beyond ner are promising future directions."
2020.emnlp-main.49.txt,"
6 Conclusion
",2020,"for future work, it would be interesting to try incorporating broader context (e.g., paragraph/document-level context (ji and grishman, 2008; huang and riloff, 2011; du and cardie, 2020) in our methods to improve the accuracy of the predictions."
2020.emnlp-main.49.txt,"
6 Conclusion
",2020,"we investigate how the question generation strategies affect the performance of our framework on both trigger detection and argument span extraction, and find that more natural questions lead to better performance."
2020.emnlp-main.490.txt,"
4 Conclusion
",2020,we hope to provide new guidance for the future slot filling work.
2020.emnlp-main.491.txt,"
5 Conclusion
",2020,"since our framework can be used with any pretrained autoencoder, it will benefit from large-scale pretraining in future research."
2020.emnlp-main.494.txt,"
7 Conclusion
",2020,"finally, although our experiments in this paper focused on sequenceto-sequence settings, we are interested in exploring the use of imitkd for compressing large language models aimed at transfer learning."
2020.emnlp-main.494.txt,"
7 Conclusion
",2020,we are excited about several possible avenues for future work.
2020.emnlp-main.499.txt,"
5 Conclusion
",2020,"in the future, we would like to explore data-free distillation on more complex tasks."
2020.emnlp-main.5.txt,"
8 Conclusions
",2020,"future work, can investigate whether finer ratings could correct the bias in favor of lower effort ratings, and how this may interact with document-level evaluation."
2020.emnlp-main.50.txt,"
7 Conclusions and Future Work
",2020,"in the future, we aim to extend graph schemas to encode hierarchical and temporal relations, as well as rich ontologies in open domain."
2020.emnlp-main.50.txt,"
7 Conclusions and Future Work
",2020,"we will also assemble our graph schemas to represent more complex scenarios involving multiple events, so they can be applied to more downstream applications including event graph completion and event prediction."
2020.emnlp-main.500.txt,"
6 Conclusion
",2020,"thus, enhancing language models to generate more semantically related perturbations can be one possible solution to perfect bert-attack in the future."
2020.emnlp-main.502.txt,"
7 Conclusion and Future Work
",2020,"in the future, we will extend the similar approach to multilingual (yu et al., 2020a) or crosslingual (upadhyay et al., 2018) lexical entailment tasks."
2020.emnlp-main.502.txt,"
7 Conclusion and Future Work
",2020,"moreover, one interesting direction is to use hyperbolic embeddings (le et al., 2019; balazevic et al., 2019) for pattern-based models due to their inherent modeling ability of hierarchies."
2020.emnlp-main.504.txt,"
7 Conclusion
",2020,"for future work, we intend to scale srefkb to a multilingual version and explore the possibilities of using the multilingual wordnet so that abundant knowledge regarding english can be transferred to other languages."
2020.emnlp-main.508.txt,"
6 Conclusion
",2020,we believe this approach can power future analyses of pre-trained text generation systems.
2020.emnlp-main.509.txt,"
5 Conclusion
",2020,"the method can be extended to other text genres such as public policies to aid reader comprehension, which will be our future work to explore."
2020.emnlp-main.51.txt,"
5 Conclusion
",2020,"for future work, we plan to extend the framework towards an end-to-end system with event extraction."
2020.emnlp-main.510.txt,"
5 Conclusions
",2020,"the promising empirical results motivate us to explore further the integration of more external knowledge and other rich forms of supervisions (e.g., constraints, interactions, auxiliary models, adversaries) (hu and xing, 2020; ziegler et al., 2019) in learning."
2020.emnlp-main.510.txt,"
5 Conclusions
",2020,"we are also interested in extending the aspect-based summarization in more application scenarios (e.g., summarizing a document corpus)."
2020.emnlp-main.512.txt,"
5 Conclusion
",2020,there are some possible future directions from our work.
2020.emnlp-main.513.txt,"
6 Conclusion
",2020,for future work we are interested in exploring how definition modeling could be adapted to a multilingual or cross-lingual setting.
2020.emnlp-main.515.txt,"
5 Conclusion and Future Work
",2020,"in the future, we are interested in replacing bert with knowledge enhanced and number sensitive text representations models (cao et al., 2017; geva et al., 2020)."
2020.emnlp-main.516.txt,"
7 Conclusion
",2020,"in the future, we want to extend our system to other few-shot sequence tagging problems such as part-of-speech tagging and slot filling."
2020.emnlp-main.517.txt,"
4 Concluding Remarks
",2020,one immediate future work is to generate explanations for model predictions using structured vector.
2020.emnlp-main.519.txt,"
7 Conclusion
",2020,future work includes: • enriching entity representations by adding entity type and entity graph information; • modeling coherence by jointly resolving mentions in a document; • extending our work to other languages and other domains; • joint models for mention detection and entity linking.
2020.emnlp-main.520.txt,"
7 Conclusion
",2020,an exciting direction is to leverage visuals of each step to deal with unmentioned entities and indirect effects.
2020.emnlp-main.520.txt,"
7 Conclusion
",2020,"as future work, we will explore more sophisticated models that can address the highlighted shortcomings of the current model."
2020.emnlp-main.521.txt,"
7 Conclusion
",2020,"given that our experiments show a 25% increase in the candidate generation, one future research direction is to improve candidate ranking in lrl by incorporating coherence statistics and entity types."
2020.emnlp-main.523.txt,"
6 Conclusions
",2020,"future work involves applying luke to domain-specific tasks, such as those in biomedical and legal domains."
2020.emnlp-main.524.txt,"
8 Conclusion
",2020,"future directions include exploration of other knowledge bases to help the inference process and applying our simile generation approach to different creative nlg tasks such as pun (he et al., 2019), sarcasm (chakrabarty et al., 2020), and hyperbole (troiano et al., 2018)."
2020.emnlp-main.525.txt,"
7 Conclusion
",2020,"we devise a metric on top of their edits that correlates strongly with judgments of the relevance of the generated text, which user interviews suggest is the most important area for improvement moving forward."
2020.emnlp-main.526.txt,"
7 Conclusion
",2020,"for example, one could rewrite technical documentation by constraining on the target operating system, rewrite lesson plans by constraining on the target grade level, or rewrite furniture assembly instructions by constraining on the tools used."
2020.emnlp-main.527.txt,"
8 Conclusion and Future Work
",2020,"another interesting line of future work is to investigate the use of t2g2 for generating user utterances, which could be useful for dialogue data augmentation and user simulation."
2020.emnlp-main.527.txt,"
8 Conclusion and Future Work
",2020,we also hope to apply t2g2 to languages other than english.
2020.emnlp-main.527.txt,"
8 Conclusion and Future Work
",2020,"while in this paper we use standard pre-trained models, designing pre-training tasks tailored to sentence fusion is an interesting line of future work."
2020.emnlp-main.528.txt,"
6 Conclusion
",2020,future work involves collecting data to addresses weaknesses of lerc.
2020.emnlp-main.529.txt,"
6 Conclusion
",2020,combining the heterogeneous planning systems will be a crucial step towards developing a human-like language generation.
2020.emnlp-main.529.txt,"
6 Conclusion
",2020,"for example, one can study the generation quality with respect to the position of the target sentences (beginning, middle, end), the comparison of plan keywords predicted by human and system, the effect of data augmentation by their positions (e.g., masking the only middle), the generation quality with respect to the ratio between masked and unmasked sentences, and more."
2020.emnlp-main.529.txt,"
6 Conclusion
",2020,"our results suggest several promising directions: although our ablation tests show the effect of each self-supervision module, types of plan keywords, and the amount of keywords with respect to generation quality, there are more spaces to explore in self-supervised text planning."
2020.emnlp-main.529.txt,"
6 Conclusion
",2020,predicting such structural plans from context and imposing them into the generator would be a potential direction for future work.
2020.emnlp-main.529.txt,"
6 Conclusion
",2020,"second, we can extend the set of plan keywords to be more structured like a discourse tree."
2020.emnlp-main.529.txt,"
6 Conclusion
",2020,"to generate more human-like utterances, different planning stages should be simultaneously combined together (kang, 2020), such as abstractive planning, strategic planning, coherence planning, and diversity planning."
2020.emnlp-main.529.txt,"
6 Conclusion
",2020,"to solve the task, we propose a text planner ssplanner that explicitly predicts topical content keywords, and then guides the surface generator using the predicted plan keywords."
2020.emnlp-main.53.txt,"
5 Conclusion and Future Work
",2020,"in the future, we will extend this approach to argument role induction to discover complete event schemas."
2020.emnlp-main.53.txt,"
5 Conclusion and Future Work
",2020,we have designed a semi-supervised vector quantized variational autoencoder approach which automatically learns a discrete representations for each seen and unseen type and predict a type for each candidate trigger.
2020.emnlp-main.532.txt,"
6 Conclusions and Future Work
",2020,"in the future, we will explore fullfledged solutions to address the privacy concerns of both humans and dialogue systems."
2020.emnlp-main.532.txt,"
6 Conclusions and Future Work
",2020,we hope this work and the dataset will pave the way for the research on privacy leakage in conversations.
2020.emnlp-main.534.txt,"
5 Conclusion
",2020,"for future work, we will incorporate pre-trained models into our framework (e.g., bert as a teacher and gpt as a student) to further unlock the performance improvement and explore how to balance diverse prior knowledge from multiple teachers."
2020.emnlp-main.534.txt,"
5 Conclusion
",2020,"in this work, we introduce the future conversation with the corresponding dialogue history to learn the implicit conversation scenario, which entails latent context knowledge and specifies how people interact in the real world."
2020.emnlp-main.534.txt,"
5 Conclusion
",2020,the scenario-based teacher model first learns to generate responses with access to both the future conversation and the dialogue history and then a conventional student model is trained to imitate the teacher by hierarchical supervisory signals.
2020.emnlp-main.534.txt,"
5 Conclusion
",2020,"to incorporate such scenario knowledge without requiring future conversation in inference, we propose an imitation learning framework."
2020.emnlp-main.539.txt,"
7 Conclusion and Discussion
",2020,we hope that the data will aid training dialogue agents to be more consistent.
2020.emnlp-main.541.txt,"
6 Conclusion
",2020,"interesting future work includes developing a fast and efficient version of re-net, and modeling lasting events and performing inference on the long-lasting graph structures."
2020.emnlp-main.543.txt,"
7 Conclusion & Future Work
",2020,"we hope future research to explore scenarios where human intuition is not working as well as text classification, such as graph attention (velickovic et al., 2017)."
2020.emnlp-main.544.txt,"
5 Conclusion
",2020,"in future work, we plan to validate its effectiveness for aspect-level sentiment classification."
2020.emnlp-main.545.txt,"
4 Conclusion
",2020,"in future work, we will try other types of single networks (e.g., (lai et al., 2015; yang et al., 2016; shimura et al., 2019))."
2020.emnlp-main.546.txt,"
5 Conclusion
",2020,we plan to investigate these in future.
2020.emnlp-main.549.txt,"
5 Conclusion
",2020,"in the future, we plan to extend our model to learn the heterogeneous graph automatically, which assures more flexibility for numerical reasoning."
2020.emnlp-main.55.txt,"
8 Conclusion
",2020,"possible future work includes (1) exploring other applications of diverse paraphrasing, such as data augmentation; (2) performing style transfer at a paragraph level; (3) performing style transfer for styles unseen during training, using few exemplars provided during inference."
2020.emnlp-main.552.txt,"
7 Conclusion
",2020,"the high specificity of our method opens doors to more insights from future work, which may include investigating how lsl results vary with probe architecture, developing intrinsic quality measures on latent ontologies, or applying the technique to discover new patterns in settings where gold annotations are not present."
2020.emnlp-main.553.txt,"
7 Conclusion
",2020,we hope our work can bring more insights into what makes a pretrained language model a pretrained language model.
2020.emnlp-main.554.txt,"
7 Discussion
",2020,"a first step in future work would be to test if the results of this paper hold on transformer architectures, or if instead transformers result in different patterns of structural encoding transfer."
2020.emnlp-main.554.txt,"
7 Discussion
",2020,"future work expanding on our results could focus on ablating specific structural features by creating hypothetical languages that differ in single grammatical features from the l2, in the style of galactic dependencies (wang and eisner, 2016), and testing the effect of structured data that’s completely unrelated to language, such as images."
2020.emnlp-main.555.txt,"
6 Conclusion
",2020,"as a result, it is believed that this study will benefit future work about choosing suitable positional encoding functions or designing other modeling methods for position information in the target nlp tasks based on their properties."
2020.emnlp-main.555.txt,"
6 Conclusion
",2020,the empirical experiments on the pre-trained position embeddings validate our hypothesis.
2020.emnlp-main.556.txt,"
7 Ethical Considerations and Conclusion
",2020,"we hope future work may better address this limitation, as in the work of cao and daume iii ´ (2019)."
2020.emnlp-main.557.txt,"
7 Conclusion
",2020,we hope our findings and probing dataset will provide a basis for improving pre-trained masked language models’ numerical and other concrete types of commonsense knowledge.
2020.emnlp-main.558.txt,"
5 Conclusion and Future work
",2020,"in future work, we will consider how to interpret environment specifications to facilitate grounded adaptation in these other areas."
2020.emnlp-main.559.txt,"
7 Conclusion and Future Work
",2020,"in the future, we will look into more accurate confidence measure via neural network calibration (guo et al., 2017) or using machine learning components (e.g., answer triggering (zhao et al., 2017) or a reinforced active selector (fang et al., 2017))."
2020.emnlp-main.559.txt,"
7 Conclusion and Future Work
",2020,one important future work is thus to conduct large-scale user studies and train parsers from real user interaction.
2020.emnlp-main.559.txt,"
7 Conclusion and Future Work
",2020,"we also plan to derive a more realistic formulation of user/expert annotation costs by analyzing real user statistics (e.g., average time spent on each question)."
2020.emnlp-main.56.txt,"
7 Conclusion and Future Work
",2020,"based on the ac-nlg method, in the future, we can explore the following directions: (1) improve the accuracy of judgment on a claim-level.(2) add external knowledge (e.g.a logic graph) to the predictor for the interpretability of the model."
2020.emnlp-main.560.txt,"
7 Conclusion and Future work
",2020,"for future work, we will explore methods attempting to solve hard and extra hard questions."
2020.emnlp-main.561.txt,"
8 Conclusion and Future Work
",2020,"in the future, we are interested in distilling and reusing the common knowledge from users’ selections."
2020.emnlp-main.562.txt,"
7 Conclusion
",2020,"for future work, we will continually improve the scale and quality of our dataset, to facilitate future research and to meet the need of database-oriented applications."
2020.emnlp-main.563.txt,"
5 Conclusion and Future Work
",2020,"since the current automatic-generated annotations are still noisy, it is useful to further improve the automatic annotation procedure."
2020.emnlp-main.563.txt,"
5 Conclusion and Future Work
",2020,we also plan to extend our approach to cope with multitable text-to-sql task spider.
2020.emnlp-main.564.txt,"
6 Conclusion
",2020,"our experiments show that schema linking, often overlooked as simple preprocessing, is actually a requisite for good sql parsing performance, providing an intriguing perspective for future improvements on this task."
2020.emnlp-main.564.txt,"
6 Conclusion
",2020,"our study sheds light on the characteristics of text-tosql parsing for future efforts including advanced modeling, problem identification, dataset construction and model evaluation."
2020.emnlp-main.566.txt,"
4 Conclusion
",2020,"besides, there are still two important directions for future work: (1) how to apply task-guided pre-training to general domain data when the indomain data is limited.(2) how to design more effective strategies to capture domain-specific and task-specific patterns for selective masking."
2020.emnlp-main.568.txt,"
6 Conclusion
",2020,"in the future, we plan to adapt our methods to more general applications that are not restricted to the field of sentiment analysis, such as doing multiple-dimension classification (e.g., topic, location) on general text corpus."
2020.emnlp-main.569.txt,"
7 Conclusions and Future Work
",2020,"in the future, we will explore the latent information between peer reviews and author responses to improve argument pair extraction."
2020.emnlp-main.569.txt,"
7 Conclusions and Future Work
",2020,we will also explore related useful research tasks using extra collected information related to scientific work submissions in rr.
2020.emnlp-main.570.txt,"
5 Conclusion
",2020,"in the future, we plan to further improve d-miln with aspect-level annotations and find appropriate way to combine d-miln with pre-training methods (tian et al., 2020)."
2020.emnlp-main.571.txt,"
6 Conclusion
",2020,"to stimulate research on this topic, we make hypo-cn publicly available.8 in future work, we plan to use hypo and hypo-cn to conduct a cross-lingual study on whether there are differences in the way exaggeration is expressed in english and chinese."
2020.emnlp-main.574.txt,"
7 Conclusions and future work
",2020,"in future work, we plan to apply our norm-based analysis to attention in other models, such as finetuned bert, roberta (liu et al., 2019), and albert (lan et al., 2020)."
2020.emnlp-main.574.txt,"
7 Conclusions and future work
",2020,"using our norm-based method, we provided a more detailed interpretation of the inner workings of transformers, compared to the studies using the weight-based analysis."
2020.emnlp-main.574.txt,"
7 Conclusions and future work
",2020,"we proposed the incorporation of another factor, the transformed input vectors."
2020.emnlp-main.576.txt,"
8 Discussion
",2020,"although our results have some implications on them, we leave a detailed study on context-free languages for future work."
2020.emnlp-main.576.txt,"
8 Discussion
",2020,another interesting direction would be to understand whether certain modifications or recently proposed variants of transformers improve their performance on formal languages.
2020.emnlp-main.576.txt,"
8 Discussion
",2020,we showed that transformers can easily generalize on certain counter languages such as shuffle-dyck and boolean expressions in a manner similar to our proposed construction.
2020.emnlp-main.58.txt,"
7 Conclusion
",2020,"we presented delorean, an unsupervised lmbased approach to generate text conditioned on past context as well as future constraints, through forward and backward passes considering each condition."
2020.emnlp-main.581.txt,"
6 Conclusion and Future Work
",2020,"it is inspiring and promising to be generalized to more rewriting tasks, which will be studied as our future work."
2020.emnlp-main.582.txt,"
6 Conclusion and Future Work
",2020,"in the future, there are several prospective research directions: (1) we introduce a distant supervision (ds) assumption in our mrp training task."
2020.emnlp-main.583.txt,"
4 Conclusions
",2020,our results suggest that future works introducing graph structure into nlp tasks should explain their necessity and superiority.
2020.emnlp-main.584.txt,"
7 Conclusions
",2020,"as for future work, we plan to investigate using languages other than english for training (e.g., our larger french and german training sets) in our cross-lingual transfer experiments, since english may not always be the optimal source language (anastasopoulos and neubig, 2020)."
2020.emnlp-main.586.txt,"
5 Further Discussion and Conclusion
",2020,"given the current large gaps between monolingual and multilingual lms, we will also focus on lightweight methods to enrich lexical content in multilingual lms (wang et al., 2020; pfeiffer et al., 2020)."
2020.emnlp-main.586.txt,"
5 Further Discussion and Conclusion
",2020,"in future work, we plan to investigate how domains of external corpora affect aoc configurations, and how to sample representative contexts from the corpora."
2020.emnlp-main.586.txt,"
5 Further Discussion and Conclusion
",2020,"in particular, some universal choices of configuration can be recommended: i) choosing monolingual lms; ii) encoding words with multiple contexts; iii) excluding special tokens; iv) averaging over lower layers."
2020.emnlp-main.586.txt,"
5 Further Discussion and Conclusion
",2020,"in-depth analyses of these factors are out of the scope of this work, but they warrant further investigations.opening future research avenues."
2020.emnlp-main.588.txt,"
8 Conclusion
",2020,"in future work, we hope that slurp will be a valuable resource for developing e2e-slu systems, as well as more traditional pipeline approaches to slu."
2020.emnlp-main.588.txt,"
8 Conclusion
",2020,"the next step is to extend slurp with spontaneous speech, which would again increase its complexity, but also move it one step closer to real-life applications."
2020.emnlp-main.593.txt,"
6 Conclusion
",2020,"to capture the temporal evolution of temporal kgs, we use velocity vectors defined in tangent spaces to learn time-dependent entity representations."
2020.emnlp-main.594.txt,"
7 Conclusions
",2020,"also, given the recent success of models such as elmo and bert, it would be interesting to explore extensions of graphglove to the class of contextualized embeddings."
2020.emnlp-main.594.txt,"
7 Conclusions
",2020,"possible directions for future work include using graphglove for unsupervised hypernymy detection, analyzing undesirable word associations, comparing learned graph topologies for different languages, and downstream applications such as sequence classification."
2020.emnlp-main.596.txt,"
7 Conclusion
",2020,"in the future, we aim at applying stare for node and graph classification tasks as well as extend our approach to large-scale kgs."
2020.emnlp-main.596.txt,"
7 Conclusion
",2020,"in the future, we plan to enrich wd50k entities with class labels and probe it against node classification tasks."
2020.emnlp-main.597.txt,"
6 Conclusion
",2020,"in future studies, we plan to increase the number of dimensions of the relational position encodings, since a scalar value may not be able to express positional information adequately."
2020.emnlp-main.598.txt,"
8 Conclusion
",2020,"in future work, we plan to extend our methodology to new languages, and experiment with multilingual and language specific bert models."
2020.emnlp-main.6.txt,"
5 Conclusion
",2020,this results in our first recommendation in future mt evaluations to avoid the use of source side test data that was created via human translation from another language.
2020.emnlp-main.6.txt,"
5 Conclusion
",2020,"we provided guidance in relation to sample size and statistical power to help planning future human evaluations of mt, particularly relevant to document-level human-parity investigations."
2020.emnlp-main.60.txt,"
6 Conclusion
",2020,we plan to investigate other automatic tools in curating more accurate denotation graphs with a complex composition of fine-grained concepts for future directions.
2020.emnlp-main.600.txt,"
5 Conclusion
",2020,"in future work, we plan to extend our approach to further improve the reward and policy functions, and to reduce the human-labeling factor."
2020.emnlp-main.601.txt,"
7 Conclusion
",2020,there are exciting avenues for multilingual work to account for language and cultural differences.
2020.emnlp-main.603.txt,"
5 Conclusions and Future Work
",2020,"this work can be extended in several ways: (i) we plan to investigate into further functions for τ to enhance the exploration-exploitation tradeoff.(ii) additional strategies to assign nuclearity should be explored, considering the excessive n-nclassification shown in our evaluation.(iii) we plan to apply our approach to more sentiment datasets (e.g., diao et al.(2014)), creating even larger treebanks.(iv) our new and scalable solution can be extended to also predict discourse relations besides structure and nuclearity.(v) we also plan to use a neural discourse parser (e.g."
2020.emnlp-main.603.txt,"
5 Conclusions and Future Work
",2020,"yu et al.(2018)) in combination with our large-scale treebank to fully leverage the potential of data-driven discourse parsing approaches.(vi) taking advantage of the new mega-dt corpus, we want to revisit the potential of discourse-guided sentiment analysis, to enhance current systems, especially for long documents.(vii) finally, more long term, we intend to explore other auxiliary tasks for distant supervision of discourse, like summarization, question answering and machine translation, for which plenty of annotated data exists (e.g., nallapati et al.(2016); cohan et al.(2018); rajpurkar et al.(2016, 2018))."
2020.emnlp-main.604.txt,"
5 Conclusions
",2020,"interestingly, we find statistical differences of trees generated from texts of different quality."
2020.emnlp-main.604.txt,"
5 Conclusions
",2020,"our model identifies the hierarchy of discourse segments without human annotations, and incorporates structural information into the model."
2020.emnlp-main.605.txt,"
7 Conclusion and Future Work
",2020,"moreover, we intend to instantiate our proposed framework to other domains such as teacher/student conversations and other types of discourse such as social media narratives."
2020.emnlp-main.605.txt,"
7 Conclusion and Future Work
",2020,we believe that our work may be extended to language generation in chatbots for producing more polite language to mediate face threats.
2020.emnlp-main.607.txt,"
6 Conclusions
",2020,"finally, we would like to combine plts with bert, similarly to attention-xml, but the computational cost of fine-tuning multiple bert encoders, one for each plt node, would be massive, surpassing the training cost of very large transformerbased models, like t5-3b (raffel et al., 2019) and megatron-lm (shoeybi et al., 2019) with billions of parameters (30-100x the size of bert-base)."
2020.emnlp-main.607.txt,"
6 Conclusions
",2020,"in future work, we would like to further investigate few and zero-shot learning in lmtc, especially in bert models that are currently unable to cope with zero-shot labels."
2020.emnlp-main.607.txt,"
6 Conclusions
",2020,pretraining bert from scratch on discharge summaries with a new bpe vocabulary is a possible solution.
2020.emnlp-main.608.txt,"
9 Conclusions
",2020,we hope this work provides some assistance to both those entering the nlp community and those already using contextualized encoders in looking beyond sota (and twitter) to make more educated choices.
2020.emnlp-main.609.txt,"
8 Conclusion and future work
",2020,"this last challenge will be especially crucial for future work that seeks to verify scientific claims against sources other than the research literature – for instance, social media and the news."
2020.emnlp-main.609.txt,"
8 Conclusion and future work
",2020,"we hope that the resources presented in this paper encourage future research on these important challenges, and help facilitate progress toward the broader goal of scientific document understanding."
2020.emnlp-main.61.txt,"
8 Conclusion
",2020,"our evaluation verifies the effectiveness of our method, while also indicating a scope for further study, enhancement, and extensions in the future."
2020.emnlp-main.610.txt,"
6 Conclusion
",2020,linguistic theories assume a close relationship between the realization of semantic arguments and syntactic configurations.
2020.emnlp-main.610.txt,"
6 Conclusion
",2020,"we invite future research into further integration of syntactic methods into shallow semantic analysis in other languages and other formulations, such as frame-semantic parsing, and other semantically oriented tasks."
2020.emnlp-main.611.txt,"
6 Conclusion and Future Work
",2020,"in the future, we will continue to investigate effective ways to obtain domain knowledge and incorporate it into enhanced models for paraphrase identification."
2020.emnlp-main.612.txt,"
6 Conclusions and Future Work
",2020,"a causal definition is in no way limited to this pairwise case, and future work may generalize it to the sequential case or to event representations that are compositional."
2020.emnlp-main.612.txt,"
6 Conclusions and Future Work
",2020,"having a causal model shines a light on the assumptions made here, and indeed, future work may further refine or overhaul them, a process which may further shine a light on the nature of the knowledge we are after."
2020.emnlp-main.613.txt,"
7 Conclusion
",2020,"based on our analysis, future work in the direction of automatic bias mitigation may include identifying potentially biased examples in an online fashion and discouraging models from exploiting them throughout the training."
2020.emnlp-main.614.txt,"
5 Discussion
",2020,"we suggest that one possibility for future work is to focus on fully unsupervised criteria, such as language model perplexity (shen et al., 2018a, 2019; kim et al., 2019b; peng et al., 2019; li et al., 2020) and model stability across different random seeds (shi et al., 2019), for model selection, as discussed in unsupervised learning work (smith and eisner, 2005, 2006; spitkovsky et al., 2010a,b, inter alia)."
2020.emnlp-main.615.txt,"
7 Conclusions
",2020,"also, we would like to explore how to overcome the obstacles that prevent us from fully exploiting large pretrained lms (e.g., gpt-2) in low-resource settings."
2020.emnlp-main.615.txt,"
7 Conclusions
",2020,"in future work, we intend to experiment with the lm-prior under more challenging conditions, such as when there is domain discrepancy between the parallel and monolingual data."
2020.emnlp-main.616.txt,"
6 Conclusion
",2020,extending model-agnostic attack strategies to incorporate other types of dataset biases and to target natural language processing tasks other than machine translation is likewise a promising avenue for future research.
2020.emnlp-main.617.txt,"
8 Conclusion
",2020,"in future work, we will apply mad-x to other pre-trained models, and employ adapters that are particularly suited for languages with certain properties (e.g.with different scripts)."
2020.emnlp-main.617.txt,"
8 Conclusion
",2020,"we will also evaluate on additional tasks, and investigate leveraging pre-trained language adapters from related languages for improved transfer to truly low-resource languages with limited monolingual data."
2020.emnlp-main.618.txt,"
7 Conclusions
",2020,"so as to facilitate similar studies in the future, we release our nli dataset,13 which, unlike previous benchmarks, was annotated in a non-english language and human translated into english."
2020.emnlp-main.619.txt,"
7 Conclusion
",2020,"additionally, in the future, we would also want to quantify the impact of varying degrees of granularity of learning emotional features from tweets on statenet’s performance."
2020.emnlp-main.619.txt,"
7 Conclusion
",2020,"priority-based suicide risk assessment for ranking tweets for suicidal risk, rather than classifying them forms our future direction."
2020.emnlp-main.619.txt,"
7 Conclusion
",2020,we plan to explore the impact of varying amounts of historical context for a user in our future work.
2020.emnlp-main.62.txt,"
7 Conclusion and Future Work
",2020,future work would be well-suited to explore 1) methods for better understanding which datasets (and individual instances) can be rebalanced and which cannot; and 2) the non-trivial task of estimating additive human baselines to compare against.• hypothesis 2: modeling feature interactions can be data-hungry.
2020.emnlp-main.62.txt,"
7 Conclusion and Future Work
",2020,"our hope is that future work on multimodal classification tasks report not only the predictive performance of their best model + baselines, but also the emap of that model."
2020.emnlp-main.62.txt,"
7 Conclusion and Future Work
",2020,"we postulate the following potential explanations, pointing towards future work: • hypothesis 1: these unbalanced tasks don’t require complex cross-modal reasoning."
2020.emnlp-main.622.txt,"
4 Conclusion
",2020,"future work can explore how to enhance a sub-optimal model using the teacher-student setup for tasks that change across domains or over time, or in scenarios where the original model and data are restricted for privacy reasons."
2020.emnlp-main.623.txt,"
6 Conclusion and Future work
",2020,"furthermore, we hope to explore the quality of fact-checking explanations with respect to properties other than coherence, e.g., actionability and impartiality.lastly, we plan to explore congruity between veracity prediction and explanation generation tasks, i.e., generating explanations which are compatible with the predicted label and vice versa."
2020.emnlp-main.623.txt,"
6 Conclusion and Future work
",2020,"in order to do this, we hope to explore other subjects, in addition to public health, for which factchecking requires a level of expertise in the subject area."
2020.emnlp-main.623.txt,"
6 Conclusion and Future work
",2020,we hope to explore the topics of explainable fact-checking and specialist fact-checking further.
2020.emnlp-main.624.txt,"
5 Conclusion
",2020,"our formulation also bridges broader nlu/rc techniques to address other critical challenges in if games for future work, e.g., common-sense reasoning, noveltydriven exploration, and multi-hop inference."
2020.emnlp-main.628.txt,"
6 Conclusions
",2020,"in the future, we will investigate the properties of our proposed method on verifying statements with more complicated operations and explore the explainability of the model."
2020.emnlp-main.630.txt,"
6 Conclusion
",2020,future work could investigate the use of non-expert human raters to improve the dataset quality further.
2020.emnlp-main.630.txt,"
6 Conclusion
",2020,"in pursuit of improved entity representations, future work could explore the joint use of complementary multi-language descriptions per entity, methods to update representations in a light-weight fashion when descriptions change, and incorporate relational information stored in the kb."
2020.emnlp-main.631.txt,"
5 Conclusions
",2020,"while our experiments are limited to cjk languages on bert, we believe the methods proposed are generic and simple to implement and expect the performance gains to also apply to different languages and models."
2020.emnlp-main.633.txt,"
6 Discussion
",2020,"although our model has achieved good performance compressing bert, it would be interesting to explore its possible applications in other neural models."
2020.emnlp-main.633.txt,"
6 Discussion
",2020,"for future work, we would like to explore the possibility of applying theseus compression on heterogeneous network modules."
2020.emnlp-main.633.txt,"
6 Discussion
",2020,"therefore, it is potential to apply theseus compression to other large models (e.g., resnet (he et al., 2016) in computer vision)."
2020.emnlp-main.633.txt,"
6 Discussion
",2020,"we will also investigate the combination of our compression-based approach with recently proposed dynamic acceleration method (zhou et al., 2020b) to further improve the efficiency of pretrained language models."
2020.emnlp-main.636.txt,"
5 Conclusion & Future Work
",2020,"as part of future work, we will explore cvt on other sequence-labeling tasks such as chunking, elementary discourse unit segmentation and argumentative discourse unit segmentation, thus moving beyond entity-level spans."
2020.emnlp-main.636.txt,"
5 Conclusion & Future Work
",2020,"furthermore, we intend to implement cvt as a training strategy over transformers (bert) and compare it with adaptivelypretrained bert."
2020.emnlp-main.637.txt,"
8 Conclusion
",2020,future work may focus on finding representations that encode the most important information for al.
2020.emnlp-main.638.txt,"
6 Conclusions
",2020,a natural future direction is to conduct a similar empirical investigation of al over bert in the context of multi-class classification and regression tasks.
2020.emnlp-main.638.txt,"
6 Conclusions
",2020,"however, while the random al baseline is limited in its ability to help bert emerge from this poor initial model, al strategies turn out to be very helpful."
2020.emnlp-main.638.txt,"
6 Conclusions
",2020,"it would also be interesting to investigate the realm of larger annotation budgets, and more recent bert variants (liu et al., 2019; lan et al., 2019)."
2020.emnlp-main.638.txt,"
6 Conclusions
",2020,"the development of novel al methods, that are tailored for pre-trained models such as bert, seems like an important direction for future work."
2020.emnlp-main.638.txt,"
6 Conclusions
",2020,"we hope that the experimental results and analyses reported here, as well as the release of the research framework we developed, would be instrumental for these and other future studies."
2020.emnlp-main.639.txt,"
6 Conclusion
",2020,we hope that this work can help inform researchers of considerations to make when using lpx models in the presence of domain shift.
2020.emnlp-main.64.txt,"
5 Conclusion
",2020,"in the future, we will investigate debiasing retrieval-based dialogue models and more complicated pipeline-based dialogue systems."
2020.emnlp-main.640.txt,"
6 Conclusion and Future Work
",2020,"in future work, we plan to optimize the lowlevel code and to develop new hardware to deploy vvmas in real-world applications."
2020.emnlp-main.640.txt,"
6 Conclusion and Future Work
",2020,we hope that this work would bring the novel concept of ai co-design (between software and hardware) to the domain of nlp applications.
2020.emnlp-main.641.txt,"
4 Conclusion
",2020,"besides, we propose and compare various token representation and pre-processing strategies in order to integrate fillers."
2020.emnlp-main.641.txt,"
4 Conclusion
",2020,"we plan to extend these results by studying the mixing of such textual filler-oriented representations with acoustic representations, and further investigate the representation of fillers learnt during pre-training."
2020.emnlp-main.643.txt,"
8 Conclusion and Future Work
",2020,"analyzing the demographic, cultural, and gender bias in research pertaining to financial disclosures, particularly earnings calls, forms a future direction of research."
2020.emnlp-main.643.txt,"
8 Conclusion and Future Work
",2020,"experimenting with other sets of commonly used acoustic features such as mfcc coefficients, opensmile features and audeep features for representing audio utterances also form a future direction for audio feature extraction."
2020.emnlp-main.643.txt,"
8 Conclusion and Future Work
",2020,"second, we want to expand the analysis presented in this paper beyond the s&p 500 index and us-based companies."
2020.emnlp-main.643.txt,"
8 Conclusion and Future Work
",2020,there are several promising directions of future work that we wish to explore.
2020.emnlp-main.645.txt,"
4 Conclusions and Future work
",2020,"future direction also include alternate architectures, reward schemes, and evaluation using human judges."
2020.emnlp-main.645.txt,"
4 Conclusions and Future work
",2020,"recent works (lu et al., 2018; d’autume et al., 2019) have proposed some solutions to address these challenges and we plan to explore them."
2020.emnlp-main.646.txt,"
7 Conclusion and future work
",2020,another important direction is to investigate how to integrate the ability to aggregate entities derived from training on tesa into an abstractive summarizer.
2020.emnlp-main.646.txt,"
7 Conclusion and future work
",2020,"in future work, we would like to expand the domains covered by our dataset, which is biased towards topics found in the source corpus, such as politics."
2020.emnlp-main.647.txt,"
7 Conclusion
",2020,"in future works we plan to add other languages including arabic and hindi, and to investigate the adaptation of neural metrics to multilingual summarization."
2020.emnlp-main.648.txt,"
6 Conclusion
",2020,"we introduce multi-xscience, a largescale dataset for mds using scientific articles."
2020.emnlp-main.649.txt,"
9 Conclusion
",2020,our findings suggest that more intentional and deliberate decisions should be made in selecting summarization datasets for downstream modelling research and that further scrutiny should be placed upon summarization datasets released in the future.
2020.emnlp-main.65.txt,"
6 Conclusion
",2020,an important future direction will be generating the distractors and learning the rationality coefficients.
2020.emnlp-main.654.txt,"
7 Conclusion and Future Work
",2020,"finally, another interesting exploration is to extend the model with a jointly trainable movie recommendation and movie information modules."
2020.emnlp-main.654.txt,"
7 Conclusion and Future Work
",2020,"then, we plan to investigate the strategy patterns for people with different personalities and movie preferences to make dialog system more personalized."
2020.emnlp-main.654.txt,"
7 Conclusion and Future Work
",2020,"this work opens up several directions for future studies in building sociable and personalized recommendation dialog systems as follows: first, we will explore more ways of utilizing the strategies, including dynamic strategy selection after decoding."
2020.emnlp-main.655.txt,"
7 Future Work and Conclusion
",2020,we hope that our dataset will encourage further interest in curiosity-driven dialog.
2020.emnlp-main.655.txt,"
7 Future Work and Conclusion
",2020,we see two immediate directions for future work.
2020.emnlp-main.657.txt,"
8 Conclusions and Future Work
",2020,"future work may explore generating of diverse sets of hypotheses for a given premise and label, with the goal of performing data augmentation."
2020.emnlp-main.657.txt,"
8 Conclusions and Future Work
",2020,other future work will be to measure the performance of gennli on adversarial and similarly challenging nli datasets.
2020.emnlp-main.658.txt,"
5 Conclusion
",2020,bias mitigation in models and datasets remains a crucial direction for future work if systems based on datasets like the ones we study are to be widely deployed.
2020.emnlp-main.658.txt,"
5 Conclusion
",2020,"while these interventions may be helpful for future evaluation data, it appears that the type of creativity induced by our relatively open-ended base prompt works well for pretraining, and the resulting artifacts are a tolerable side-effect of that creativity."
2020.emnlp-main.659.txt,"
6 Conclusions
",2020,"finally, we give suggestions on future research directions and on better analysis variance reporting."
2020.emnlp-main.659.txt,"
6 Conclusions
",2020,"however, large instability of current models on some of these analysis sets undermine such benefits and bring non-ignorable obstacles for future research."
2020.emnlp-main.659.txt,"
6 Conclusions
",2020,we hope this paper will guide researchers on how to handle instability and inspire future work in this direction.
2020.emnlp-main.661.txt,"
7 Conclusion
",2020,"however, we also show limitations of our proposed methods, thereby encouraging future work on conjnli for better understanding of conjunctive semantics."
2020.emnlp-main.662.txt,"
5 Conclusion
",2020,"though language-dependent tasks like dependency parsing are challenging to translate, mt can efficiently transfer large and expensive-to-create labeled datasets from english to other languages in many nlp tasks, including text classification, question answering, and text summarization."
2020.emnlp-main.664.txt,"
5 Conclusion
",2020,we also encourage future reports of system performances to use the same task formalization whenever possible.
2020.emnlp-main.666.txt,"
7 Conclusions
",2020,"in the future, we plan to study how we can apply synsetexpan at the entity mention level for conducting contextualized synonym discovery and set expansion."
2020.emnlp-main.669.txt,"
7 Conclusion and outlook
",2020,"overall, we hope that codex will provide a boost to research in kgc, which will in turn impact many other fields of artificial intelligence."
2020.emnlp-main.669.txt,"
7 Conclusion and outlook
",2020,"some promising future directions on codex include: • better model understanding codex can be used to analyze the impact of hyperparameters, training strategies, and model architectures in kgc tasks.• revival of triple classification we encourage the use of triple classification on codex in addition to link prediction because it directly tests discriminative power.• fusing text and structure including text in both the link prediction and triple classification tasks should substantially improve performance (toutanova et al., 2015)."
2020.emnlp-main.67.txt,"
6 Conclusion
",2020,"we also report the benchmark models and results of five evaluation tasks on risawoz, indicating that the dataset is a challenging testbed for future work."
2020.emnlp-main.670.txt,"
6 Conclusion and Future Work
",2020,"in the future, we are interested in effectively integrating different forms of supervision including annotated documents."
2020.emnlp-main.672.txt,"
6 Conclusion and Future Work
",2020,potential future work includes combining the neural and cut-based approaches into a stronger method.
2020.emnlp-main.673.txt,"
10 Conclusion
",2020,"notable findings from our empirical evaluation include: (1) not all neural text generation methods generate high-quality human-mimicking texts–in particular, gpt2, grover, and fair generated better-quality texts and (2) using specific linguistic features and simple neural architectures, we can solve three problems reasonably well, except gpt2 and fair in p2 and grover in p3."
2020.emnlp-main.675.txt,"
6 Conclusions
",2020,"as next steps, we plan to address further types of revisions and extend our experiments to document-level settings."
2020.emnlp-main.676.txt,"
8 Conclusion and Future Work
",2020,"another interesting direction of future research is to explore the cold start problem, where man-sf could be leveraged to predict stock movements for new stocks."
2020.emnlp-main.676.txt,"
8 Conclusion and Future Work
",2020,"we plan to further use news articles, earnings calls, and other data sources to capture market dynamics better."
2020.emnlp-main.677.txt,"
7 Conclusions and Future Work
",2020,"future work will investigate ways to improve performance (and especially precision scores) on our data, in particular on low-support labels."
2020.emnlp-main.677.txt,"
7 Conclusions and Future Work
",2020,"in this study, we provided a first stepping stone towards future research at the intersection of nlp and sustainable development (sd)."
2020.emnlp-main.68.txt,"
10 Conclusion
",2020,we hope that this study will facilitate discussions in the dialogue response generation community regarding the issue of filtering noisy corpora.
2020.emnlp-main.681.txt,"
6 Conclusion
",2020,we believe a robust understanding of these algorithms is a prerequisite for theoretically motivated development of deeper models.
2020.emnlp-main.682.txt,"
6 Conclusion and Future Work
",2020,future work can use anomaly detection approaches operating on our model’s predicted word vectors to detect anomalies in a word’s representation across time.
2020.emnlp-main.682.txt,"
6 Conclusion and Future Work
",2020,"we also plan to investigate different architectures, such as variational autoencoders (kingma and welling, 2014), and incorporate contextual representations (devlin et al., 2019; hu et al., 2019) to detect new senses of words."
2020.emnlp-main.684.txt,"
5 Conclusion
",2020,the extension of the scope will be the future work.
2020.emnlp-main.685.txt,"
7 Conclusion and Future Work
",2020,"in future work we plan to apply our model to longer, book length documents, and plan to add more structure to the memory."
2020.emnlp-main.687.txt,"
8 Conclusion
",2020,we believe there is much potential for additional selfsupervision tasks and leave those for future work.
2020.emnlp-main.688.txt,"
5 Conclusions
",2020,"in future work, we would like to study how to introduce acyclic rules to the walk-based systems."
2020.emnlp-main.689.txt,"
6 Conclusion and Future Work
",2020,"as discussed in section 5, we also plan to develop a more flexible scoring function which can handle equivalent trees."
2020.emnlp-main.689.txt,"
6 Conclusion and Future Work
",2020,"finally, we plan to evaluate bertft on other temporal relation datasets as part of a larger pipeline, which will include a mapping between tdts and other temporal relation annotation schemas such as the tempeval-3 dataset (uzzaman et al., 2013)."
2020.emnlp-main.689.txt,"
6 Conclusion and Future Work
",2020,"for future research, we plan to explore other types of deep neural lms such as transformerxl (dai et al., 2019) and xlnet (yang et al., 2019)."
2020.emnlp-main.69.txt,"
6 Conclusions
",2020,"future directions include the incorporation of phonological and morphosyntactic features, application to other languages, and most importantly, a model extension to infer temporal ordering."
2020.emnlp-main.691.txt,"
6 Conclusion
",2020,"for future research, it is interesting to enhance seqmix with language models during the mixup process, and harness external knowledge for further improving diversity and plausibility."
2020.emnlp-main.692.txt,"
8 Conclusions
",2020,"future work may want to build on our approach for more comprehensive extraction tasks, focussing on more types of result, as well as other information contained in papers such as architectural details and hyperparameters."
2020.emnlp-main.693.txt,"
6 Conclusion and discussion
",2020,we leave this as our future work.
2020.emnlp-main.694.txt,"
6 Conclusion
",2020,"future work can focus on expanding the capabilities to generating whole paragraphs of text from graphs in kb, as well as converting large parts of text into coherent graph structures."
2020.emnlp-main.697.txt,"
5.8 Conclusion
",2020,such a framework provides a plausible solution to greatly reduce human annotation costs in future nlg applications.
2020.emnlp-main.698.txt,"
5 Conclusion
",2020,"in future work, we hope to leverage sentence structure, such as the use of constituency parsing, to further enhance the design of the progressive hierarchy."
2020.emnlp-main.7.txt,"
7 Conclusion
",2020,"as paraphrasing continues to improve and cover more languages, we are optimistic smrt will provide larger improvements across the board—including for higher-resource mt and for additional target languages beyond english."
2020.emnlp-main.70.txt,"
5 Conclusions
",2020,future work will be required to assess the extent to which these effects do in fact reflect the acquisition of a latent form of discourse modeling ability.
2020.emnlp-main.70.txt,"
5 Conclusions
",2020,"that having been said, we consider the identification of alternative language model architectures that are capable of capturing the requisite discourse modeling capability for this task to be an interesting challenge problem for future work."
2020.emnlp-main.70.txt,"
5 Conclusions
",2020,we hope that this short paper will inspire further research that takes next steps in this and a variety of other directions.
2020.emnlp-main.700.txt,"
5 Conclusions
",2020,our future work will explore the potential of training palm for longer on much more unlabeled text data.
2020.emnlp-main.701.txt,"
5 Conclusion & Future Works
",2020,future research works can be conducted to make the generation process more robust and interpretable.
2020.emnlp-main.702.txt,"
5 Conclusion
",2020,"in this work, we introduce a new technique for sequence generation models called teacher-forcing with n-grams (teaforn), which (a) addresses exposure bias, (b) allows the decoder to better take into account future decisions, and (c) requires no curriculum training."
2020.emnlp-main.702.txt,"
5 Conclusion
",2020,"overall, teaforn is a promising approach for improving quality and/or reducing inference costs in sequence generation models."
2020.emnlp-main.703.txt,"
7 Conclusions
",2020,"as with many who have analyzed the history of nlp, its trends (church, 2007), its maturation toward a science (steedman, 2008), and its major challenges (hirschberg and manning, 2015; mcclelland et al., 2019), we hope to provide momentum for a direction many are already heading."
2020.emnlp-main.703.txt,"
7 Conclusions
",2020,"we call for and embrace the incremental, but purposeful, contextualization of language in human experience."
2020.emnlp-main.704.txt,"
6 Conclusion
",2020,by releasing our dataset and code we hope to provide a solid foundation to accelerate work in this direction.
2020.emnlp-main.704.txt,"
6 Conclusion
",2020,"in the future, we believe that strong linguistic priors will continue to be a key ingredient for building nextlevel learning agents in these games."
2020.emnlp-main.706.txt,"
6 Conclusion
",2020,"we introduce a new task, video-and-language event prediction (vlep) - given a video with aligned dialogue, and two future events, machines is required to predict which event is more likely to happen."
2020.emnlp-main.708.txt,"
6 Conclusion
",2020,"we study the sample variance in visually-grounded language generation, in terms of reference sample variance within datasets, effects of training or testing sample variance on metric scores, and the trade-off between the visual instance number and the parallel reference number per visual."
2020.emnlp-main.710.txt,"
5 Conclusion
",2020,future work includes investigating the interaction and joint training between hgn and paragraph retriever for performance improvement.
2020.emnlp-main.712.txt,"
6 Conclusions
",2020,"however, our transformation alleviates this issue: a model that connects information will have an edge in determining the sufficiency of the given context."
2020.emnlp-main.712.txt,"
6 Conclusions
",2020,we leave further exploration to future work.
2020.emnlp-main.713.txt,"
7 Conclusion
",2020,"overall, this work opens up exciting avenues for leveraging methods in unsupervised learning and natural language generation to improve the interpretability and generalization of machine learning systems."
2020.emnlp-main.715.txt,"
9 Conclusion and Future Work
",2020,"finally, we will carry out a thorough investigation into emotion-cause pairs (xia and ding, 2019)."
2020.emnlp-main.715.txt,"
9 Conclusion and Future Work
",2020,"in the future, we plan to study how contextual information (i.e., different aspects of people’s interactions captured through contiguous posts in a discussion thread) affects the perceived emotions."
2020.emnlp-main.715.txt,"
9 Conclusion and Future Work
",2020,we also plan to perform a cross-corpus analysis to investigate if emotions are expressed differently in the health domain compared to other domains.
2020.emnlp-main.715.txt,"
9 Conclusion and Future Work
",2020,"we believe that these characteristics add interestingness and challenges to our dataset and we hope that our work will spur future research in emotion detection from health data, especially in the context of life-threatening diseases such as cancer."
2020.emnlp-main.717.txt,"
6 Conclusion
",2020,"in addition, we will study more explicitly how to decouple stance models from sentiment, and how to improve performance further on difficult phenomena."
2020.emnlp-main.717.txt,"
6 Conclusion
",2020,"in future work we plan to investigate additional methods to represent and use generalized topic information, such as topic modeling."
2020.emnlp-main.720.txt,"
5 Conclusion
",2020,an important avenue of future work will be to assess to what extent there may be cultural differences in these associations (see discussion in section 3.1).
2020.emnlp-main.721.txt,"
6 Conclusion
",2020,"also, stochasticity was applied to the emotion mixture for varied response generation."
2020.emnlp-main.722.txt,"
6 Conclusion
",2020,"in the future, instead of pre-training on sentences, we will leverage raw text at passage or document level to alleviate the performance degeneration brought by short context."
2020.emnlp-main.722.txt,"
6 Conclusion
",2020,"in this work, we aim at equipping pre-trained lms with structured knowledge via self-supervised tasks."
2020.emnlp-main.724.txt,"
6 Conclusions
",2020,we also point out several directions for future work by generalizing our methods to other tasks or combining with other techniques.
2020.emnlp-main.725.txt,"
5 Conclusion
",2020,"in the future, we plan to extend our model to cope with external word or document semantics."
2020.emnlp-main.725.txt,"
5 Conclusion
",2020,it would also be interesting to explore alternative architectures other than cyclegan under our formulation of topic modeling.
2020.emnlp-main.726.txt,"
8 Conclusion
",2020,"in the future, we plan to implement a more sophisticated guidance for the augmentation by adding syntactic and position features to the reward function, to enable augmentation of more diverse types of text data."
2020.emnlp-main.727.txt,"
5 Conclusion and Future Work
",2020,"therefore, one direction for future work is to explore an online state detection algorithm and perform it for each event, but at the same time ensure that the state of each event is globally defined."
2020.emnlp-main.728.txt,"
5 Conclusion
",2020,"looking forward, we plan to leverage pymt5 for various downstream automated software engineering tasks—including code documentation and method generation from natural language statements—and develop more model evaluation criteria to leverage the unique properties of source codes."
2020.emnlp-main.729.txt,"
7 Conclusion and Future Work
",2020,"first, we would like to explore more explainable reasoning method for question generation, such as symbolic-based models."
2020.emnlp-main.729.txt,"
7 Conclusion and Future Work
",2020,"in the future, there can be two research directions."
2020.emnlp-main.73.txt,"
8 Conclusion
",2020,"while we showed that iterative inference with a learned score function is effective for spherical gaussian priors, more work is required to investigate if such an approach will also be successful for more sophisticated priors, such as gaussian mixtures or normalizing flows."
2020.emnlp-main.730.txt,"
7 Conclusions and Future Work
",2020,future work includes applying time inference models to question answering and other nlp systems.
2020.emnlp-main.733.txt,"
5 Conclusion and Future Work
",2020,"in the future, we are looking forward to diving in representation learning with flow-based generative models from a broader perspective."
2020.emnlp-main.734.txt,"
6 Discussion & Conclusion
",2020,"in conclusion, we hope our data and analysis inspire future directions such as explicit modeling of collective human opinions; providing theoretical supports for the connection between human disagreement and the difficulty of acquiring language understanding in general; exploring potential usage of these human agreements; and studying the source of the human disagreements and its relations to different linguistic phenomena."
2020.emnlp-main.736.txt,"
5 Conclusion
",2020,"as future work, we will explore the similar idea of designing unreferenced metrics for dialog generation."
2020.emnlp-main.737.txt,"
5 Conclusion
",2020,"thus, future work involves extending the method to other related tasks, such as machine translation and text summarization, and investigating the potential gains from transfer learning."
2020.emnlp-main.739.txt,"
7 Conclusion
",2020,"as future work, we would like extend the prior network to sample more than one persona sentences by expanding the sample space of the discrete random variable to generate more interesting responses."
2020.emnlp-main.739.txt,"
7 Conclusion
",2020,"while our expansions are limited by the performance of comet or paraphrase systems, we envision future work to train the dialog model endto-end along with the expansion generation."
2020.emnlp-main.74.txt,"
5 Conclusion and Future Work
",2020,"in our future work, i) we are interested in distilling from deep nmt models into extremely small students with ckd, in the hope of achieving the same results of large models with much smaller counterparts.ii) we also try to improve the combination module and find a better alternative than concatenation.iii) finally, we plan to evaluate ckd in other tasks such as language modeling."
2020.emnlp-main.740.txt,"
7 Conclusion and Future Work
",2020,there are some interesting directions for future work.
2020.emnlp-main.742.txt,"
5 Conclusion and Discussion
",2020,a limitation of grade is the inconsistency between the training objective (relative ranking) and the expected behavior (absolute scoring).
2020.emnlp-main.742.txt,"
5 Conclusion and Discussion
",2020,"besides, we also release a new large-scale human evaluation bench-mark to facilitate future research on automatic metrics."
2020.emnlp-main.743.txt,"
6 Conclusions and Future Works
",2020,"for future work, we will annotate medical entities in our datasets."
2020.emnlp-main.743.txt,"
6 Conclusions and Future Works
",2020,such annotations can facilitate the development of goal-oriented medical dialog systems.
2020.emnlp-main.746.txt,"
8 Conclusion
",2020,our work shows the effectiveness of simple training dynamics measures based on mean and standard deviation; exploration of more sophisticated measures to build data maps is an exciting future direction.
2020.emnlp-main.748.txt,"
5 Conclusion
",2020,it would be very interesting to see what kind of performance larger models could achieve.
2020.emnlp-main.749.txt,"
5 Conclusion
",2020,"for future work, we plan to apply our method for other type of spans, such as noun phrases, verbs, and clauses."
2020.emnlp-main.75.txt,"
6 Conclusion
",2020,"for future work, we are interested in investigating the proposed approach in a scaled setting with more languages and a larger amount of monolingual data."
2020.emnlp-main.75.txt,"
6 Conclusion
",2020,"furthermore, we would also like to explore the most sample efficient strategy to add a new language to a trained mnmt system."
2020.emnlp-main.750.txt,"
6 Conclusions
",2020,shortcomings of our approach explained in section 5.2 can serve as guidelines for future work.
2020.emnlp-main.751.txt,"
6 Implications and Future Directions
",2020,future works on meta-evaluation should investigate the effect of these settings on the performance of metrics.(2) metrics easily overfit on limited datasets.
2020.emnlp-main.751.txt,"
6 Implications and Future Directions
",2020,"in closing, we highlight some potential future directions: (1) the choice of metrics depends not only on different tasks (e.g, summarization, translation) but also on different datasets (e.g., tac, cnndm) and application scenarios (e.g, system-level, summary-level)."
2020.emnlp-main.752.txt,"
7 Conclusion
",2020,"in near future, we aim to incorporate the video script information in the multimodal summarization process."
2020.emnlp-main.78.txt,"
5 Conclusion
",2020,"future directions include continuing the exploration of this research topic for large sequenceto-sequence pre-training models (liu et al., 2020) and multi-domain translation models (wang et al., 2019b)."
2020.emnlp-main.78.txt,"
5 Conclusion
",2020,"we also analyze the gains from perspectives of learning dynamics and linguistic probing, which give insightful research directions for future work."
2020.emnlp-main.78.txt,"
5 Conclusion
",2020,"we will employ recent analysis methods to better understand the behaviors of rejuvenated models (he et al., 2019; yang et al., 2020)."
2020.emnlp-main.8.txt,"
7 Conclusion and Future Work
",2020,"in future work, we would like to extend prism to paragraph- or document-level evaluation by training a paragraph- or document-level multilingual nmt system, as there is growing evidence that mt evaluation would be better conducted at the document level, rather than the sentence level (laubli et al., 2018)."
2020.emnlp-main.80.txt,"
5 Conclusion
",2020,"it is interesting to combine with other techniques (li et al., 2018; hao et al., 2019) to further improve nmt."
2020.emnlp-main.82.txt,"
8 Conclusion
",2020,"in future work, firstly, since our model is randomly sampled from model distribution to generate diverse translation, it is meaningful to explore better algorithms and training strategies to represent model distribution and search for the most distinguishable results in model distribution."
2020.emnlp-main.84.txt,"
6 Conclusion
",2020,our findings also generalize to different positions and different datasets.
2020.emnlp-main.85.txt,"
7 Conclusion
",2020,"in addition to the elements of this task which are appealing from a common sense modeling perspective, the inherent appeal of this task to humans opens a number of possibilities for future data collection and evaluation."
2020.emnlp-main.87.txt,"
13 Discussion and Future Work
",2020,how to most efficiently and effectively adapt transformer-based qa systems remains an important topic for future research.
2020.emnlp-main.88.txt,"
8 Conclusion
",2020,"results show that even a state-of-the-art language model, roberta-large, falls behind human performance by a large margin, necessitating more investigation on improving mrc on temporal relationships in the future."
2020.emnlp-main.88.txt,"
8 Conclusion
",2020,"torque has 3.2k news snippets, 9.5k hard-coded questions asking which events had happened, were ongoing, or were still in the future, and 21.2k human-generated questions querying more complex phenomena."
2020.emnlp-main.9.txt,"
5 Discussion and Future Work
",2020,"e.g., if there is a disjunction rule “if x or y then z” instead of a conjunction rule “if x and y then z”, only the shape of the graph changes."
2020.emnlp-main.9.txt,"
5 Discussion and Future Work
",2020,inferences over modals like “might” and disjunction rules like “if x then y or z” will mean that both the answer and the proof will be probabilistic.
2020.emnlp-main.9.txt,"
5 Discussion and Future Work
",2020,we hope that prover will encourage further work towards developing interpretable nlp models with structured explanations.
2020.emnlp-main.90.txt,"
7 Conclusions and Future Work
",2020,"in the future, we will explore more informative generation and consider applying mgcn to other nlp tasks for better information extraction and aggregation."
2020.emnlp-main.91.txt,"
7 Conclusion and Future Work
",2020,"moreover, future work should inspect the effect of split and rephrase on downstream tasks such as machine translation or information retrieval, and examine if models’ performance on these tasks correlate with that on our benchmarks."
2020.emnlp-main.93.txt,"
6 Conclusions and Future Work
",2020,"in future, we plan to explore the following two directions: (1) interpolating the contexts between consecutive steps by introducing a new infilled image, and (2) addressing the underspecification problem by controlling the content in infilled image with explicit guidance."
2020.emnlp-main.94.txt,"
6 Conclusion
",2020,our neural poet is available at https://nala-cub.github.io/resources as a baseline for future research on the task.
2020.emnlp-main.96.txt,"
6 Conclusion
",2020,"in future work, it would be interesting to investigate to what extent pretrained language models benefit from groc on such zero-resource or lowresource adaptation settings."
2020.emnlp-main.96.txt,"
6 Conclusion
",2020,"this work indicates several other future directions for language modeling in low-resource domains: extension to other languages, scaling training to even larger vocabularies, and applying groc in a large pretraining setting to expand its zero-shot generalization."
2020.emnlp-main.97.txt,"
8 Conclusion
",2020,"future work will explore applying ssmba to the target side manifold in structured prediction tasks, as well as other natural language tasks and settings where data augmentation is difficult."
2020.emnlp-main.98.txt,"
5 Conclusion
",2020,we leave it for future work.
P16-1002,"
6 Discussion
",2016,"related work has also explored the idea of training on altered or out-of-domain data, often interpreting it as a form of regularization."
P16-1004,"
5 Conclusions
",2016,"beyond semantic parsing, we would also like to apply our seq2tree model to related structured prediction tasks such as constituency parsing."
P16-1005,"
8 Conclusions and Future Work
",2016,in the future we aim to label slot types based on contextual information as well as sentence structures instead of trigger gazetteers only.
P16-1006,"
9 Conclusions and Future Work
",2016,"furthermore, we will exploit edge labels while walking through a knowledge base to retrieve more relevant entities."
P16-1006,"
9 Conclusions and Future Work
",2016,"in the future we will apply visual pattern recognition and concept detection techniques to perform deep content analysis of the retrieved images, so we can do matching and inference on concept/entity level instead of shallow visual similarity."
P16-1006,"
9 Conclusions and Future Work
",2016,our long-term goal is to extend this framework to other knowledge extraction and population tasks such as event extraction and slot filling to construct multimedia knowledge bases effectively from multiple languages with low cost.
P16-1006,"
9 Conclusions and Future Work
",2016,we will also extend anchor image retrieval from documentlevel into phrase-level or sentence-level to obtain richer background information.
P16-1007,"
9 Conclusion
",2016,the complementary strengths of both systems suggest future work in combining these techniques.
P16-1009,"
6 Conclusion
",2016,future work will explore the effectiveness of our approach in more settings.
P16-1010,"
7 Conclusion
",2016,"in experiments, this model further improves our system."
P16-1010,"
7 Conclusion
",2016,"in the future, we will extend this model to allow discontinuity on target sides and explore the possibility of directly encoding reordering information in translation rules."
P16-1011,"
9 Conclusion
",2016,"more importantly, as our approach is based on incremental learning, it can be potentially integrated in a dialogue system to support life-long learning from humans."
P16-1011,"
9 Conclusion
",2016,our future work will extend the current approach with dialogue modeling to learn more reliable hypothesis spaces of resulting states for verb semantics.
P16-1012,"
6 Conclusion
",2016,as future work we aim to make our approach completely knowledgefree by eliminating this dependency.
P16-1012,"
6 Conclusion
",2016,"we will explore unsupervised acquisition of relational similarity (mikolov et al., 2013b) for this task."
P16-1015,"
8 Conclusion
",2016,"finally, we would like to highlight two insights that the experiments provide."
P16-1015,"
8 Conclusion
",2016,"second, non-projective trees can be parsed by allowing a larger arc-distance which is a simple way to allow for non-projective edges."
P16-1015,"
8 Conclusion
",2016,we think that the transition systems with more active tokens or the combination with edges that span over more words provide very attractive transition systems for possible future parsers.
P16-1016,"
6 Conclusion
",2016,"this can be a useful starting point for several lines of research: implementing more advanced transitionbased techniques (beam search, dynamic oracles, deep learning); extending other classical transition systems like arc-eager and hybrid as well as handling non-projectivity."
P16-1018,"
7 Conclusion
",2016,"finally, it would be interesting to investigate modeling metaphorical mappings as nonlinear mappings within the deep learning framework."
P16-1018,"
7 Conclusion
",2016,"in the cdsm framework we apply, verbs would be represented as third-order tensors."
P16-1018,"
7 Conclusion
",2016,we have shown that modeling metaphor explicitly within a cdsm can improve the resulting vector representations.
P16-1019,"
6 Conclusions and Future Work
",2016,"in addition, we also plan to compare our work to the method of sporleder et al.(2010) as well apply our work on the idx corpus (sporleder et al., 2010) and to other languages."
P16-1019,"
6 Conclusions and Future Work
",2016,in future work we plan to investigate the use of sent2vec to encode larger samples of text - not only the sentence containing idioms.
P16-1019,"
6 Conclusions and Future Work
",2016,the focus of these future experiments will be to test how our approach which is relatively less dependent on nlp resources compares with these other methods for idiom token classification.
P16-1019,"
6 Conclusions and Future Work
",2016,we also plan to further analyse the errors made by our “general” model and investigate the “general” approach on the skewed part of the vnc-tokens dataset.
P16-1019,"
6 Conclusions and Future Work
",2016,we also plan to investigate an end-to-end approach based on deep learning-based representations to classify literal and idiomatic language use.
P16-1019,"
6 Conclusions and Future Work
",2016,we tested this approach with different machine learning (ml) algorithms (k-nearest neighbours and support vector machines) and compared our work against a topic model representation that include the full paragraph or the surrounding paragraphs where the potential idiom is inserted.
P16-1020,"
8 Conclusion and Future Work
",2016,"in future work, we will apply our method to other kinds of phrases and tasks."
P16-1023,"
6 Conclusion and future work
",2016,"based on this paper, there are serveral lines of investigation we plan to conduct in the future.(i) we will attempt to support our results on artificially generated corpora by conducting experiments on real natural language data.(ii) we will study the coverage of our four criteria in evaluating word representations.(iii) we modeled the four criteria using separate pcfgs, but they could also be modeled by one single unified pcfg."
P16-1023,"
6 Conclusion and future work
",2016,but the validity of the assumption that embedding spaces can be decomposed into “linear” subspaces should be investigated in the future.
P16-1024,"
5 Conclusions and Future Work
",2016,"encouraged by the excellent results, we also plan to test the portability of the approach to more language pairs, and other tasks and applications."
P16-1024,"
5 Conclusions and Future Work
",2016,"in future work, we plan to investigate other methods for seed pairs selection, settings with scarce resources (agic et al., 2015; zhang et al., 2016), ′ other context types inspired by recent work in the monolingual settings (levy and goldberg, 2014a; melamud et al., 2016), as well as model adaptations that can work with multi-word expressions."
P16-1025,"
5 Conclusions and Future Work
",2016,"in the future, we will extend this framework to other information extraction tasks."
P16-1026,"
5 Conclusions
",2016,"in future work, we will investigate scalable and parallel model learning to explore the performance of our model for large-scale real-time event extraction and visualization."
P16-1028,"
7 Conclusion
",2016,"in future work, we plan to apply semlms to other semantic related nlp tasks e.g.machine translation and question answering."
P16-1030,"
6 Conclusion
",2016,all the learned connotation frames and annotations will be shared at http://homes.cs.washington.edu/?hrashkin/connframe.html.
P16-1031,"
5 Conclusions and Future Work
",2016,"first, since deep learning may obtain better generalization on large-scale data sets (bengio, 2009), a straightforward path of the future research is to apply the proposed btdnns for domain adaptation on a much larger industrial-strength data set of 22 domains (glorot et al., 2011)."
P16-1031,"
5 Conclusions and Future Work
",2016,"second, we will try to investigate the use of the proposed approach for other kinds of data set, such as 20 newsgroups and reuters21578 (li et al., 2012; zhuang et al., 2013)."
P16-1032,"
8 Conclusion
",2016,"experiments demonstrated that the approach can infer implied sentiment and point toward potential directions for future work, including joint entity detection and incorporation of more varied types of factual relationships."
P16-1033,"
7 Conclusions
",2016,"for future work, we would like to advance this study in the following directions."
P16-1033,"
7 Conclusions
",2016,"our plan is to study which syntactic structures are more suitable for human annotation, and balance informativeness of a candidate task and its suitability for human annotation."
P16-1036,"
9 Conclusions

",2016,"also, we would like to experiment with other deep neural architectures such as recurrent neural networks, long short term memory networks, etc.to form the sub-networks."
P16-1036,"
9 Conclusions

",2016,"as part of future work, we would like to enhance scqa with the meta-data information like categories, user votes, ratings, user reputation of the questions and answer pairs."
P16-1038,"
10 Conclusion
",2016,"4 future directions for this work include further improving the number and quality of g2p models, as well as performing external evaluations of the models in speech- and text-processing tasks."
P16-1038,"
10 Conclusion
",2016,we plan to use the presented data and methods for other areas of multilingual natural language processing.
P16-1043,"
7 Conclusion
",2016,"curriculum learning is inspired by the way humans acquire knowledge and skills: by mastering simple concepts first, and progressing through information with increasing difficulty to grasp more complex topics."
P16-1043,"
7 Conclusion
",2016,the approach is quite general and we hope that this paper will encourage more nlp researchers to explore curriculum learning in their own works.
P16-1044,"
6 Conclusion
",2016,"all proposed models are departed from a basic architecture, built on bidirectional lstms."
P16-1044,"
6 Conclusion
",2016,"potential future work include: 1) evaluating the proposed approaches for different tasks, such as community qa and textual entailment; 2) including the sentential attention mechanism; 3) integrating the hybrid and the attentive mechanisms into a single framework."
P16-1044,"
6 Conclusion
",2016,"second, we introduce a simple oneway attention mechanism, in order to generate answer embeddings influenced by the question context."
P16-1044,"
6 Conclusion
",2016,we propose three independent models in two directions.
P16-1045,"
7 Conclusions
",2016,"we believe it offers interesting challenges that go beyond the scope of this paper – such as question parsing, or textual entailment – and are exciting avenues for future research."
P16-1046,"
7 Conclusions
",2016,"a third direction would be to adopt an information theoretic perspective and devise a purely unsupervised approach that selects summary sentences and words so as to minimize information loss, a task possibly achievable with the dataset created in this work."
P16-1047,"
6 Conclusion and Future Work
",2016,can we transfer our model to other languages?
P16-1047,"
6 Conclusion and Future Work
",2016,future work can be directed towards answering two main questions: can we improve the performance of our classifier?
P16-1049,"
8 Conclusion
",2016,we leave better triggering component and multiple rounds of conversation handling to be addressed in our future work.
P16-1052,"
5 Conclusion
",2016,"despite of the overall presence of dialogues with such mixed motives in everyday life, little attention has been given to this topic in text planning in contrast to scrutinized dialogue systems that support dialogues fully aligned with anticipated user interests."
P16-1053,"
5 Conclusion and Future Work
",2016,"so, we are going to extend sin to tree-based sin for sentence modeling as future work."
P16-1056,"
6 Conclusion
",2016,"finally, we use our best performing neural network model to generate a corpus of 30m question and answer pairs, which we hope will enable future researchers to improve their question answering systems."
P16-1056,"
6 Conclusion
",2016,"when evaluated by untrained human subjects, the question and answer pairs produced by our best performing neural network appears to be comparable in quality to real human-generated questions."
P16-1058,"
5 Discussion and Conclusion
",2016,"an obvious direction for future work is to automatically induce such a strategy, based on confidence measures that automatically predict the trust-worthiness of a word for an object."
P16-1058,"
5 Discussion and Conclusion
",2016,"another extension that we have planned for future work is to implement relational expressions, similar to (kennington and schlangen, 2015)."
P16-1058,"
5 Discussion and Conclusion
",2016,"based on relational expressions, we will be able to generate reformulations and installments tailored to the interaction with the user."
P16-1058,"
5 Discussion and Conclusion
",2016,"for instance, a very natural option for installments is to relate the wrong target object clicked on by the user to the intended target, e.g.something like to the left of that one, the bigger object."
P16-1059,"
7 Conclusion
",2016,"finally, one may consider a more elaborate model in which attention depends on the current state of the system; for example, the state can summarize the mention context."
P16-1060,"
8 Conclusions
",2016,averaging unreliable scores does not result in a reliable one.
P16-1060,"
8 Conclusions
",2016,"we first report the mention identification effect on b 3 , ceaf and blanc which causes these metrics to report misleading values."
P16-1062,"
7 Conclusions and Future Work
",2016,"a pilot effort to use word embeddings to alter the variety of vocabulary in a dataset has so far not succeeded, but future experiments altering vocabulary width or modularity of a dataset and finding that the modified dataset behaved like natural datasets with the same properties could increase confidence in causality."
P16-1062,"
7 Conclusions and Future Work
",2016,"future work can also explore finer clusters within these datasets, such as clustering clue by word sense of the answers and toon by joke sense."
P16-1062,"
7 Conclusions and Future Work
",2016,future work can manipulate datasets’ text properties to confirm that a specific property is the cause of observed differences in clustering.
P16-1062,"
7 Conclusions and Future Work
",2016,"future work will explore further how the goals of short text authors translate into measurable properties of the texts they write, and how measuring those properties can help predict which similarity metrics and clustering methods will combine to provide the best performance."
P16-1065,"
7 Conclusions and Future Work
",2016,"as next steps, we plan to explore model variations to support a wider range of use cases."
P16-1065,"
7 Conclusions and Future Work
",2016,"in the spirit of treating links probabilistically, we plan to explore application of the model in suggesting links that do not exist but should, for example in discovering missed citations, marking social dynamics (nguyen et al., 2014), and identifying topically related content in multilingual networks of documents (hu et al., 2014)."
P16-1066,"
7 Conclusion

",2016,this is a possible future research direction.
P16-1066,"
7 Conclusion

",2016,"we also plan to make the classification multi-way: positive, negative, neutral and mixed."
P16-1067,"
5 Conclusions
",2016,an application of the proposed approach on authorship attribution has also achieved perfect results of 100% accuracies together with confidence measurement for the first time.
P16-1071,"
8 Conclusion
",2016,"while our approach may not be readily applicable for developing nlp models today, we believe that the presented results may inspire nlp researchers to consider learning models from combinations of linguistic resources and auxiliary, behavioral data that reflects human cognition."
P16-1073,"
6 Conclusions
",2016,"in future work, we will explore more advanced sentence rewriting methods."
P16-1075,"
9 Discussion and Conclusion
",2016,"future work will aim to study different dimensions of the prompt (e.g.genre, topic) using multitask learning at a finer level."
P16-1075,"
9 Discussion and Conclusion
",2016,we also aim to further study the characteristics of the multi-task model in order to determine which features transfer well across tasks.
P16-1076,"
7 Conclusion
",2016,future work could be extending the proposed method to handle more complex questions.
P16-1080,"
8 Conclusions
",2016,"another avenue of future research can look at the annotators’ own traits and how these relate to perception (flekova et al., 2015)."
P16-1081,"
5 Conclusions
",2016,the idea of shared model adaptation is general and can be further extended.
P16-1082,"
7 Conclusions
",2016,we are releasing human annotations of concept nodes and possible dependency edges learned from the acl anthology as well as implementations of the methods described in this paper to enable future research on modeling scientific corpora.
P16-1082,"
7 Conclusions
",2016,"we have proposed the concept graph framework, which gives weighted links from documents to the concepts they discuss and links concepts to one another."
P16-1083,"
7 Conclusion
",2016,"in future work, we plan to improve performace of feature weight tuning and investigate more general features."
P16-1085,"
7 Conclusions
",2016,"as future work, we plan to investigate the possibility of designing word representations that best suit the wsd framework."
P16-1087,"
8 Conclusion
",2016,"in future work, we plan to explore the effects of pre-training (bengio et al., 2009) and scheduled sampling (bengio et al., 2015) for training our lstm network."
P16-1087,"
8 Conclusion
",2016,we would also like to explore re-ranking methods for our problem.
P16-1087,"
8 Conclusion
",2016,"with respect to the fine-grained opinion mining task, a potential future direction to be able to model overlapping and embedded entities and relations and also to extend this model to handle cross-sentential relations."
P16-1088,"
7 Conclusion

",2016,"in future work, we will develop lexical features which are captured by nonlocal dependencies."
P16-1089,"
6 Conclusion
",2016,"however, it would be interesting to see how siamese cbow embeddings would affect results in supervised tasks."
P16-1089,"
6 Conclusion
",2016,"it would be interesting to see how embeddings for larger pieces of texts, such as documents, would perform in document clustering or filtering tasks."
P16-1090,"
6 Discussion and related work
",2016,"a natural next step is to explore our framework with additional modeling improvements—especially in dealing with context, structure, and noise."
P16-1091,"
5 Conclusions

",2016,"furthering this work, there would be still much room for improvement in future."
P16-1091,"
5 Conclusions

",2016,the other direction of our future work is to investigate joint models for tracking dialogue topics and states simultaneously.
P16-1093,"
6 Conclusions
",2016,"in future work, we plan to conduct larger-scale evaluations to further validate these results, and to apply these methods on other common learner errors."
P16-1094,"
7 Conclusions
",2016,"there are many other dimensions of speaker behavior, such as mood and emotion, that are beyond the scope of the current paper and must be left to future work."
P16-1095,"
7 Conclusion
",2016,the future work has two main directions.one is semi-supervised learning.
P16-1097,"
5 Conclusion
",2016,"in the future, we plan to apply our approach to real-world non-parallel corpora to further verify its effectiveness."
P16-1097,"
5 Conclusion
",2016,"it is also interesting to extend the phrase translation model to more sophisticated models such as ibm models 2-5 (brown et al., 1993) and hmm (vogel and ney, 1996)."
P16-1098,"
8 Conclusion and Future Work
",2016,"in future work, we would like to investigate our model on more text matching tasks."
P16-1099,"
6 Conclusion
",2016,this is left for future work.
P16-1100,"
7 Conclusion
",2016,"for future work, we hope to be able to improve the memory usage and speed of purely character-based models."
P16-1101,"
6 Conclusion
",2016,another interesting direction is to apply our model to data from other domains such as social media (twitter and weibo).
P16-1101,6 Conclusion,2016,there are several potential directions for future work.
P16-1102,"
6 Conclusion and Future Work
",2016,further exploration of different topic vector representations and their combinations is necessary in future work.
P16-1104,"
7 Conclusion
",2016,we propose to augment this work in future by exploring deeper graph and gaze features.
P16-1106,"
7 Conclusions and outlook

",2016,"at some stage the shifted path must intersect with the path of z from py to ?px, before the entanglement of the two paths is broken."
P16-1106,"
7 Conclusions and outlook

",2016,"our assumptions imply that the final parts of the paths of x, y and z from ?px, ?py and ?pz, respectively, to p0 should not intersect with this surface."
P16-1107,"
7 Conclusions and Future Work
",2016,"also, we will apply our contextaware argumentative relation mining to different argument mining corpora to further evaluate its generality."
P16-1107,"
7 Conclusions and Future Work
",2016,our next step will investigate uses of topic segmentation to identify context sentences and compare this linguistically-motivated approach to our current window-size heuristic.
P16-1107,"
7 Conclusions and Future Work
",2016,we plan to follow prior research on graph optimization to refine the argumentation structure and improve argumentative relation prediction.
P16-1108,"
7 Conclusion

",2016,"in the future, we plan to expand our method to predict morphological analyses, as well as to incorporate other information such as parts-of-speech."
P16-1111,"
8 Conclusion
",2016,"nonetheless, we hope this work offers another step towards using computational tools to better understand the ‘rhetorical structure of science’ (latour, 1987)."
P16-1111,"
8 Conclusion
",2016,"this result has important implications for research funding and public policy: the most promising topics—in terms of potential for future growth—are not those that are currently generating the most results, but are instead those that are active areas of methodological inquiry."
P16-1112,"
9 Conclusion
",2016,"as part of future work, it would be beneficial to investigate the effect of automatically generated training data for error detection (e.g., rozovskaya and roth (2010))."
P16-1114,"
7 Conclusion
",2016,the future work includes using those prediction models in a real service to take targeted actions to users who are likely to stop using intelligent assistants.
P16-1115,"
7 Conclusions
",2016,all this is future work.
P16-1115,"
7 Conclusions
",2016,"in its current state— besides, we believe, strongly motivating this future work—, we hope that the model can also serve as a strong baseline to other future approaches to reference resolution, as it is conceptually simple and easy to implement."
P16-1115,"
7 Conclusions
",2016,"the design of the model, as mentioned in the introduction, makes it amenable for use in interactive systems that learn; we are currently exploring this avenue."
P16-1116,"
7 Conclusion
",2016,"future work may be done to integrate our method into a joint approach, use some global feature, which may improve our performance."
P16-1116,"
7 Conclusion
",2016,then we propose a regularization method to make full use of the argument relationship.
P16-1117,"
6 Conclusion
",2016,"in the future, we plan to extend our model to incorporate coreference resolution and intersentential zero anaphora resolution."
P16-1119,"
7 Conclusions and Future Work
",2016,"future work can use our annotated corpus to develop classifiers that deal better with prepositional and adjectival modifiers, which require deeper semantic analysis."
P16-1120,"
8 Conclusions
",2016,"as future work, we would like to verify the effectiveness of the proposed models for other datasets or other cross-lingual tasks, such as cross-lingual document classification (ni et al., 2009; platt et al., 2010; ni et al., 2011; smet et al., 2011) and cross-lingual information retrieval (vulic et al., ′ 2013)."
P16-1122,"
7 Conclusion and Future Work
",2016,in the future we plan to apply our inner-attention intuition to other neural networks such as cnn or multi-layer perceptron.
P16-1123,"
5 Conclusion
",2016,"we expect this sort of architecture to be of interest also beyond the specific task of relation classification, which we intend to explore in future work."
P16-1124,"
6 Conclusion
",2016,it will be interesting to study whether one can merge the clustering step and the coupling step so as to have a richer inter-task dependent structure.
P16-1124,"
6 Conclusion
",2016,there are still many interesting topics to study.
P16-1124,"
6 Conclusion
",2016,we have tested cpra on benchmark data created from freebase.
P16-1124,"
6 Conclusion
",2016,we will investigate such topics in our future work.
P16-1124,"
6 Conclusion
",2016,"we would like to design new mechanisms to discover loosely correlated relations, and investigate whether coupling such relations still provides benefits."
P16-1127,"
6 Conclusion and Future Work
",2016,"but there are other possible choices that might make the encoding even more easily learnable by the lstm, and we would like to explore those in future work."
P16-1127,"
6 Conclusion and Future Work
",2016,"in order to improve performance, other promising directions would involve adding re-reranking techniques and extending our neural networks with attention models in the spirit of (bahdanau et al., 2015)."
P16-1127,"
6 Conclusion and Future Work
",2016,we propose a sequence-based approach for the task of semantic parsing.
P16-1129,"
8 Conclusion and Future Work

",2016,another important direction is to focus on the construction of datasets in larger scale.
P16-1129,"
8 Conclusion and Future Work

",2016,we would like to extend our system to produce sports news beyond pure sentence extraction.
P16-1130,"
6 Conclusion
",2016,"for future work, we will explore more sophisticated features for the csrs model, such as syntactic dependency relationships and head words, since only simple lexical features are used in the current incarnation."
P16-1133,"
5 Conclusion and Future Work
",2016,"in addition, we will also explore the possibility of using more complex neural network models such as convolutional neural network and recurrent neural network to build bilingual document representation system."
P16-1133,"
5 Conclusion and Future Work
",2016,our future work will focus on extending the bilingual document representation model into the multilingual scenario.
P16-1133,"
5 Conclusion and Future Work
",2016,we will try to learn a single embedding space for a source language and multiple target languages simultaneously.
P16-1134,"
6 Conclusions
",2016,"in future work, we are interested in exploring better ways of utilizing vast unlabelled data to improve grsemi-crfs, e.g., to learn phrase embeddings from unlabelled data or designing a semisupervised version of grsemi-crfs."
P16-1135,"
7 Conclusion
",2016,"in the future, we may improve this step using a machine learning approach."
P16-1135,"
7 Conclusion
",2016,"linguistic expressions of causality in other languages is another avenue for future research, and it would be interesting to note if other languages have the same variety of expression."
P16-1136,"
5 Conclusions
",2016,"in the future, we would like to study the impact of relation paths for additional basic kb embedding models and knowledge domains."
P16-1137,"
8 Conclusion
",2016,"in future work, we will explore how to use our model to improve downstream nlp tasks, and consider applying our methods to other knowledge bases."
P16-1141,"
5 Discussion
",2016,the two statistical laws we propose have strong implications for future work in historical semantics.
P16-1141,"
5 Discussion
",2016,we show how distributional methods can reveal statistical laws of semantic change and offer a robust methodology for future work in this area.
P16-1143,"
8 Discussion and Future Work
",2016,"in conclusion, we have created extensive resources for future work in nlp and related disciplines."
P16-1143,"
8 Discussion and Future Work
",2016,"in particular, we intend to expand lexsemtm by applying hca-wsi across the vocabularies of languages other than english, and also to multiword lemmas."
P16-1143,"
8 Discussion and Future Work
",2016,the most immediate extension of our work would be to apply our sense learning method to a broader range of data.
P16-1143,"
8 Discussion and Future Work
",2016,"we currently use a simple approach, and we believe this process could be improved, e.g.by using word embeddings."
P16-1145,"
6 Conclusion
",2016,"in light of this finding, we suggest some focus areas for future research."
P16-1145,"
6 Conclusion
",2016,we have demonstrated the complexity of the wikireading task and its suitability as a benchmark to guide future development of dnn models for natural language understanding.
P16-1147,"
6 Conclusions
",2016,"in future work, we will extend this idea beyond sequence modeling to improve models in nlp that utilize parse trees as features."
P16-1147,"
6 Conclusions
",2016,"through a simple learning method we call “stack-propagation,” our model learns effective intermediate representations for parsing by using pos tags as regularization of implicit representations."
P16-1150,"
5 Conclusion and future work
",2016,"we believe that the presence of 44k reasons (550k tokens) is another important asset of the newly created corpus, which deserves future investigation."
P16-1152,"
5 Conclusion

",2016,"given the advantages of faster convergence and the fact that only relative feedback in terms of comparative evaluations is required, bandit pairwise preference learning is a promising framework for future real-world interactive learning."
P16-1152,"
5 Conclusion

",2016,"we investigated the performance of all algorithms by test set performance on different tasks, however, the main interest of this paper was a comparison of convergence speed across different objectives by early stopping on a convergence criterion based on heldout data performance."
P16-1153,"
5 Conclusion
",2016,"future work includes: (i) adding an attention model to robustly analyze which part of state/actions text correspond to strategic planning, and (ii) applying the proposed methods to more complex text games or other tasks with actions defined through natural language."
P16-1154,"
7 Conclusion and Future Work
",2016,"for future work, we will extend this idea to the task where the source and target are in heterogeneous types, for example, machine translation."
P16-1156,"
9 Conclusion and Future Work
",2016,future work will consider the role of derivational morphology in embeddings as well as noncompositional cases of inflectional morphology.
P16-1159,"
6 Conclusion
",2016,"in the future, we plan to test our approach on more language pairs and more end-to-end neural mt systems."
P16-1159,"
6 Conclusion
",2016,it is also interesting to extend minimum risk training to minimum risk annealing following smith and eisner (2006).
P16-1160,"
8 Conclusion
",2016,"however, this has allowed us a more fine-grained analysis, but in the future, a setting where the source side is also represented as a character sequence must be investigated."
P16-1162,"
6 Conclusion
",2016,"one avenue of future research is to learn the optimal vocabulary size for a translation task, which we expect to depend on the language pair and amount of training data, automatically."
P16-1165,"
6 Conclusions and Future Work
",2016,"in the future, we would like to combine crfs with lstms for doing the two steps jointly, so that the lstms can learn the embeddings using the global thread-level feedback."
P16-1165,"
6 Conclusions and Future Work
",2016,"we would also like to apply our models to conversations, where the graph structure is extractable using the meta data or other clues, e.g., the fragment quotation graphs for email threads (carenini et al., 2008)."
P16-1166,"
8 Conclusion
",2016,"a next major step in our research agenda is to integrate se type information into various applications, including argument mining, temporal reasoning, and summarization."
P16-1166,"
8 Conclusion
",2016,"among others, we plan to create subtypes of the state label, which currently subsumes clauses stativized by negation, modals, lexical information or other aspectual operators."
P16-1166,"
8 Conclusion
",2016,"as we have reported before (friedrich et al., 2015), the most difficult case is the identification of generic sentences, which are defined as making a statement about a kind or class."
P16-1166,"
8 Conclusion
",2016,"here we focus on the automatic identification of se types, leaving the identification of discourse modes to future work."
P16-1166,"
8 Conclusion
",2016,"therefore the automatic labeling of clauses with their se type is a prerequisite for automatically identifying a passage’s discourse mode, which in turn has promising applications in many areas of nlp, as the mode of a text passage has implications for the linguistic phenomena to be found in the passage."
P16-1167,"
8 Conclusion
",2016,"finally, we provide a dataset depicting 12 scenarios with ～1.5 m images for future research."
P16-1167,"
8 Conclusion
",2016,future directions could include exploring nuances in the type of temporal knowledge that can be learned across different scenarios.
P16-1171,"
7 Conclusion
",2016,"in the future, we plan to build a resource for modeling physical causality for action verbs."
P16-1172,"
Future Work
",2016,"for example, we plan to incorporate lexicalsemantic information in the feature representation and leverage large-scale unsupervised pretraining."
P16-1172,"
Future Work
",2016,the promising results we obtained for summarization with a basic learner (see section 4.3) encourage future work on plugging in more sophisticated supervised learners in our framework.
P16-1174,"
6 Conclusion
",2016,"finally, while we conducted a cursory analysis of model weights in §4.2, an interesting next step would be to study such weights for even deeper insight.(note that using lexeme tag component features, as suggested above, should make this anaysis more robust since features would be less sparse.)"
P16-1174,"
6 Conclusion
",2016,"instead of the sparse indicator variables used here, it may be better to decompose lexeme tags into denser and more generic features of tag components9 (e.g., part of speech, tense, gender, case), and also use corpus frequency, word length, etc."
P16-1175,"
7 Conclusion
",2016,this opens the door to a number of future directions with applications to language acquisition using personalized content and learners’ knowledge.
P16-1175,"
7 Conclusion
",2016,"we also leave as future work the integration of this model into an adaptive system that tracks learner understanding and creates scaffolded content that falls in their zone of proximal development, keeping them engaged while stretching their understanding."
P16-1175,"
7 Conclusion
",2016,we plan a deeper investigation into how learners detect and combine cues for incidental comprehension.
P16-1176,"
7 Conclusion
",2016,we also plan to investigate how the results vary when limited to specific l1s.
P16-1176,"
7 Conclusion
",2016,"we are interested in expanding the preliminary results of this work: we intend to replicate the experiments with more languages and more domains, investigate additional varieties of constrained language and employ more complex lexical, syntactic and discourse features."
P16-1178,"
7 Conclusions
",2016,"as future work, we plan to investigate other linguistic representations that can improve the automated extraction of the proposed aspects to better predict the article’s perceived quality."
P16-1179,"
9 Conclusions and Future Work
",2016,13 the analysis of the acquired information and the error analysis show several avenues for future work.
P16-1180,"
9 Conclusion and Future Work
",2016,another task for future work is semantic alignment.
P16-1180,"
9 Conclusion and Future Work
",2016,"since we already discover all the entity types involved, all that is missing is the proposition (or frame, or set of propositions); this seems to be a straightforward, though not necessarily easy, task to tackle in the near future."
P16-1180,"
9 Conclusion and Future Work
",2016,we leave these to future work.
P16-1183,"
6 Conclusion
",2016,we intend to explore these possibilities in future work.
P16-1184,"
7 Conclusion
",2016,our results provide a strong baseline for future work in weakly supervised morphological tagging.
P16-1185,"
5 Conclusion

",2016,"another interesting direction is to enhance the connection between source-to-target and target-tosource models (e.g., letting the two models share the same word embeddings) to help them benefit more from interacting with each other."
P16-1185,"
5 Conclusion

",2016,"as our method is sensitive to the oovs present in monolingual corpora, we plan to integrate jean et al.(2015)’s technique on using very large vocabulary into our approach."
P16-1186,"
6 Conclusions
",2016,"an interesting future direction is to combine complementary approaches, either through combined parameterization (e.g.hierarchical softmax with differentiated capacity per word) or through a curriculum (e.g.transitioning from target sampling to regular softmax as training progresses)."
P16-1187,"
5 Conclusions
",2016,"as future work, we plan to examine the use of a voting scheme for combining the output of complementary dsms."
P16-1187,"
5 Conclusions
",2016,"moreover, we also plan to combine additional sources of information for building the models, such as multilingual resources or translation data, to improve even further the compositionality prediction."
P16-1187,"
5 Conclusions
",2016,we would also like to propose and evaluate more sophisticated compositionality functions that take into account the unbalanced contribution of individual words to the global meaning of a compound.
P16-1189,"
6 Conclusions
",2016,"it thus allows for the fast deployment of a crucial component in comparable corpora alignment, which opens the path for an increase in the amount of such corpora that can be exploited in the future."
P16-1192,"
8 Conclusion
",2016,it would also be interesting to combine the strengths of the condensed and sibling-finder algorithms for further efficiency gains.
P16-1193,"
6 Conclusion
",2016,this future work on compositional distributional semantics should further demonstrate the full power of the proposed framework for modelling entailment in a vector space.
P16-1194,"
5 Conclusions
",2016,this may further improve the model’s performance.
P16-1195,"
8 Conclusion
",2016,"in future work, we plan to develop better models for capturing the structure of the input, as well as extend the use of our system to other applications such as automatic documentation of source code."
P16-1196,"
4 Conclusion and Future Work
",2016,"in addition we plan to further investigate how to fine-tune some of the hyper parameters of the cpm such as spline scaling, single global scaling factor, convergence tolerance, and initialization of the latent trace with a centroid."
P16-1196,"
4 Conclusion and Future Work
",2016,"in addition, to aid cpm convergence to a good local optimum, in future work we will investigate dimensionality reduction approaches that are reversible such as principal component analysis (pearson, 1901) and other pre-processing approaches similar to (listgarten, 2007), where the training data set is coarsely pre-aligned and pre-scaled based on the center of mass of the time series."
P16-1196,"
4 Conclusion and Future Work
",2016,"while this work used the latent trace as the basis for animation, in future work, we also plan to explore methods for sampling from the model to produce variations in face and head movement."
P16-1197,"
5 Conclusion
",2016,"deriving sentiment labels for supervised training is an important topic for future study, as is inferring the sentiment of published news from stock price fluctuations instead of the reverse."
P16-1198,"
6 Discussion
",2016,in extensive multilingual experiments our model performs very similarly to a standard directed firstorder parser.
P16-1198,"
6 Discussion
",2016,it may also be utilized as an inference/initialization subroutine as a part of more complex approximation frameworks such as belief propagation (e.g.
P16-1198,"
6 Discussion
",2016,the potential embodied in this work extends to a number of promising research directions: ?
P16-1198,"
6 Discussion
",2016,we believe this contribution has the potential to affect future research on additional nlp problems.
P16-1198,"
6 Discussion
",2016,we intend to investigate all of these directions in future work.
P16-1199,"
6 Conclusion and Future Works

",2016,"in the next step, we plan to exploit fine-grained discourse structures, e.g., dialogue acts (ritter et al., 2010), and propose a unified model that jointly inferring discourse roles and topics of posts in context of conversation tree structures."
P16-1200,"
5 Conclusion and Future Works
",2016,"in the future, we will explore the following directions: ?"
P16-1200,"
5 Conclusion and Future Works
",2016,"in the future, we will incorporate our instance-level selective attention technique with those models for relation extraction."
P16-1200,"
5 Conclusion and Future Works
",2016,we will explore our model in other area such as text categorization.?
P16-1201,"
6 Conclusions and Future Work
",2016,"in the future, we will extend this work to the complete event extraction task."
P16-1201,"
6 Conclusions and Future Work
",2016,we plan to refine the event schemas by the finer-grained frames defined in fn (i.e.
P16-1202,"
6 Conclusion
",2016,our future plan is to apply this model to general arithmetic problems which require multiple applications of formulas.
P16-1203,"
6 Conclusions and Future Work
",2016,our future research will test the correlation between the polarity and the name of a fictional character beyond the movie domain.
P16-1207,"
7 Conclusion
",2016,"as a consequence, inferring 2https://www.ukp.tu-darmstadt.de/data /timeline-generation/temporal-anchoring -of-events/ from tlinks when an event happened is less precise as temporal information that is more than one sentence away can often not be taken into account."
P16-1208,"
6 Discussion and Conclusions
",2016,"for future work, we intend to study the problem in the context of other languages."
P16-1210,"
7 Discussion
",2016,"in the future, we would like to extend this work in several ways."
P16-1210,"
7 Discussion
",2016,"second, since the svd basis-shift seems to be the source of much of the gains, we would like to explore replacing the svd with other algorithms, such as independent component analysis."
P16-1210,"
7 Discussion
",2016,we proposed several extensions to the popular structural correspondence learning (scl) algorithm for domain adaptation to make it more amenable to tasks like authorship attribution.
P16-1212,"
5 Conclusion and Future Work

",2016,"in the future, we will conduct experiments on large corpus in different domains."
P16-1214,"
4 Conclusion
",2016,our future research includes improving the search performance for embeddings with heavy-tailed distributions and creating embeddings that can keep both task quality and search performance high.
P16-1215,"
6 Conclusion
",2016,"analyzing the internal mechanism of lstm, gru, and gac, we plan to explore an alternative architecture of neural networks that is optimal for relational patterns."
P16-1215,"
6 Conclusion
",2016,"we expect that several further studies will use the new dataset not only for distributed representations of relational patterns but also for other nlp tasks (e.g., paraphrasing)."
P16-1216,"
8 Conclusion
",2016,"in future work, we look to incorporate methods that incur less cost, possibly tolerating some error in the formation of sentences without significantly degrading performance."
P16-1216,"
8 Conclusion
",2016,section 4 gives some insight into the problem but a much deeper investigation is necessary to devise a detection method.
P16-1216,"
8 Conclusion
",2016,we look to devote future work to handling such cases.
P16-1217,"
6 Conclusions and Future Work
",2016,"in future work, we will explore to make use of all the three kinds of labels together to improve the users’ experience when they want to browse, understand and leverage the topics."
P16-1217,"
6 Conclusions and Future Work
",2016,"in future work, we will try to make the summary label more coherent by considering the discourse structure of the summary and leveraging sentence ordering techniques."
P16-1220,"
7 Conclusion and Future Work
",2016,our future work involves exploring other alternatives such as treating structured and unstructured data as two independent resources in order to overcome the knowledge gaps in either of the two resources.
P16-1223,"
7 Conclusion
",2016,"as future work, we need to consider how we can utilize these datasets (and the models trained upon them) to help solve more complex rc reasoning tasks (with less annotated data)."
P16-1224,"
6 Related Work and Discussion
",2016,"looking forward, we believe that the illg setting is worth studying and has important implications for natural language interfaces."
P16-1225,"
6 Conclusion
",2016,"finally, we would like to test our model using other representations of word-form and wordmeaning."
P16-1225,"
6 Conclusion
",2016,future work could also test whether a more interpretable meaningspace representation such as that provided by binary wordnet feature vectors reveals patterns of systematicity not found using a distributional semantic space.
P16-1225,"
6 Conclusion
",2016,future work may investigate to what extent the smlkr model can predict human intuitions about form-meaning systematicity in language.
P16-1225,"
6 Conclusion
",2016,"however, it would be interesting to verify our results in a phonological setting, perhaps using a monodialectal corpus."
P16-1225,"
6 Conclusion
",2016,our algorithm offers improved global predictions of word-meaning given word-form at the lexicon-wide level.
P16-1225,"
6 Conclusion
",2016,we would also like to investigate the degree to which our statistical model predicts the behavioral effects of phonosemantic systematicity during human semantic processing that have been reported in the psycholinguistics literature.
P16-1226,"
8 Conclusion
",2016,"finally, our architecture seems straightforwardly applicable for multi-class classification, which, in future work, could be used to classify term-pairs to multiple semantic relations."
P16-1227,"
5 Conclusions and Further Work
",2016,a further avenue of future research is improving models such as that presented in elliott et al.(2015) by crucial components of neural mt such as “attention mechanisms”.
P16-1227,"
5 Conclusions and Further Work
",2016,combining these two types of attention mechanisms in a neural caption translation model is a natural next step in caption translation.
P16-1227,"
5 Conclusions and Further Work
",2016,"in future work, we plan to evaluate our approach in more naturalistic settings, such machine translation for captions in online multimedia repositories such as wikimedia commons16 and digitized art catalogues, as well as e-commerce localization."
P16-1228,"
6 Discussion and Future Work
",2016,"finally, we also would like to generalize our framework to automatically learn the confidence of different rules, and derive new rules from data."
P16-1228,"
6 Discussion and Future Work
",2016,"the encouraging empirical results indicate a strong potential of our approach for improving other application domains such as vision tasks, which we plan to explore in the future."
P16-1228,"
6 Discussion and Future Work
",2016,we plan to explore these diverse knowledge representations to guide the dnn learning.
P16-1230,"
5 Conclusion
",2016,this may be too simplistic for many commercial applications and further work will be needed in conjunction with human interaction experts to identify and incorporate the extra dimensions of dialogue quality that will be needed to achieve the highest levels of user satisfaction.
p17-1002,"
6 Conclusion
",2017,"its suitability for the end-to-end learning task is scope for future work, but its adequacy for component classification and relation identification has been investigated in potash et al.(2016)."
p17-1002,"
6 Conclusion
",2017,"we experimented with different framings, such as encoding am as a dependency parsing problem, as a sequence tagging problem with particular label set, as a multi-task sequence tagging problem, and as a problem with both sequential and tree structure information."
p17-1004,"
5 Conclusion
",2017,"in future, we will extend mnre to more languages and explore its significance."
p17-1004,"
5 Conclusion
",2017,"we will explore the effectiveness of word-level multilingual attention for relation extraction as our future work.(2) mnre can be flexibly implemented in the scenario of multiple languages, and this paper focuses on two languages of english and chinese."
p17-1004,"
5 Conclusion
",2017,"we will explore the following directions as future work: (1) in this paper, we only consider sentence-level multi-lingual attention for relation extraction."
p17-1005,"
5 Discussion
",2017,"aside from relaxing strict isomorphism, we would also like to perform crossdomain semantic parsing where the first stage of the semantic parser is shared across domains."
p17-1005,"
5 Discussion
",2017,"while it would be challenging to handle drastically non-isomorphic structures in the current model, it is possible to perform local structure matching, i.e., when the mapping between natural language and domainspecific predicates is many-to-one or one-to-many."
p17-1006,"
7 Conclusion and Future Work
",2017,"future work will focus on other potential sources of morphological knowledge, porting the framework to other morphologically rich languages and downstream tasks, and on further refinements of the post-processing specialisation algorithm and the constraint selection."
p17-1008,"
7 Conclusion
",2017,"semantic schemes diverge in whether they are anchored in the words and phrases of the text (e.g., all types of semantic dependencies and ucca) or not (e.g., amr and logic-based representations)."
p17-1010,"
6 Conclusion
",2017,"second, we will develop other neural network architecture to make it more appropriate for zero pronoun resolution task."
p17-1010,"
6 Conclusion
",2017,"the future work will be carried out on two main aspects: first, as experimental results show that the unknown words processing is a critical part in comprehending context, we will explore more effective way to handle the unk issue."
p17-1011,"
6 Conclusion
",2017,"considering these characteristics, we proposed a neural sequence labeling approach for identifying discourse modes."
p17-1011,"
6 Conclusion
",2017,"in future, we plan to exploit discourse mode identification for providing novel features for more downstream nlp applications."
p17-1012,"
6 Conclusion
",2017,"also, we plan to investigate the effectiveness of our architecture on other sequence-tosequence tasks, e.g.summarization, constituency parsing, dialog modeling."
p17-1012,"
6 Conclusion
",2017,future work includes better training to enable faster convergence with the convolutional encoder to better leverage the higher processing speed.
p17-1012,"
6 Conclusion
",2017,we introduced a simple encoder model for neural machine translation based on convolutional networks.
p17-1014,"
9 Conclusions
",2017,"for future work, we would like to extend our work to different meaning representations such as the minimal recursion semantics (mrs; copestake et al.(2005))."
p17-1014,"
9 Conclusions
",2017,"taking a step further, we would like to apply our models on semantics-based machine translation using mrs as an intermediate representation between pairs of languages, and investigate the added benefit compared to directly translating the surface strings, especially in the case of distant language pairs such as english and japanese (siegel, 2000)."
p17-1015,"
8 Conclusion
",2017,"in this work, we addressed the problem of generating rationales for math problems, where the task is to not only obtain the correct answer of the problem, but also generate a description of the method used to solve the problem."
p17-1016,"
6 Conclusions
",2017,"in future work, it would be useful to investigate models based on morphemes, rather than characters, which offers potentially superior performance for complex and rare words (luong et al., 2013), which are common in poetry."
p17-1016,"
6 Conclusions
",2017,"this approach offers greater control over the style of the generated poetry than the earlier method, and facilitates themes and poetic devices."
p17-1017,"
5 Conclusion
",2017,we hope that the webnlg dataset which we have made available for the webnlg shared task will drive the deep learning community to take up this new challenge and work on the development of neural generators that can handle the generation of kb verbalisers and of linguistically rich texts.
p17-1018,"
7 Conclusion
",2017,"as for future work, we are applying the gated self-matching networks to other reading comprehension and question answering datasets, such as the ms marco dataset (nguyen et al., 2016)."
p17-1019,"
6 Conclusion and Future Work
",2017,"and the future work includes: a) lots of questions cannot be answered directly by facts in a kb (e.g.鈥淲ho is jet li鈥檚 father-in-law?鈥), we plan to learn qa system with latent knowledge (e.g."
p17-1019,"
6 Conclusion and Future Work
",2017,"kb embedding (bordes et al., 2013)); b) we plan to adopt memory networks (sukhbaatar et al., 2015) to encode the temporary kb for each question."
p17-1019,"
6 Conclusion and Future Work
",2017,"specifically, the sequences of sus in the generated answer may be predicted from the vocabulary, copied from the given question and retrieved from the corresponding kb."
p17-1020,"
8 Conclusion
",2017,"in future work we would like to deepen the use of structural clues and answer questions over multiple documents, using paragraph structure, titles, sections and more."
p17-1020,"
8 Conclusion
",2017,incorporating coreference resolution would be another important direction for future work.
p17-1021,"
6 Conclusion
",2017,"specifically, we employ the focus of the answer aspects to each question word and the attention weights of the question towards the answer aspects."
p17-1022,"
9 Conclusion
",2017,"the current work has focused on learning a purely categorical model of the translation process, supported by an unstructured inventory of translation candidates, and future work could explore the compositional structure of messages, and attempt to synthesize novel natural language or neuralese messages from scratch."
p17-1023,"
7 Discussion and Conclusion
",2017,"as we have tested these approaches on a rather small vocabulary, which may limit generality of conclusions, future work will be devoted to scaling up these findings to larger test sets, as e.g.recently collected through conversational agents (das et al., 2016) that circumvent the need for human-human interaction data."
p17-1024,"
6 Conclusion
",2017,"our hypothesis is that systems which, like humans, deeply integrate the language and vision modalities, should spot foil captions quite easily."
p17-1024,"
6 Conclusion
",2017,"to complete the analysis of these results, we plan to carry out a further task, namely ask the system to detect in the image the area that produces the mismatch with the foil word (the red box around the bird in figure 1.)"
p17-1024,"
6 Conclusion
",2017,"we note that the addition of this extra step will move this work closer to the textual/visual explanation research (e.g., (park et al., 2016; selvaraju et al., 2016))."
p17-1027,"
7 Conclusion
",2017,"while we used a perceptron classifier for our experiments, our oracle could also be used in neuralnetwork implementations of greedy transitionbased parsing (chen and manning, 2014; dyer et al., 2015), providing an interesting avenue for future work."
p17-1028,"
6 Conclusions and Future Work
",2017,"we also plan to investigate extension of the crowd component in our hmm method with hierarchical models, as well as a fully-bayesian approach."
p17-1028,"
6 Conclusions and Future Work
",2017,"we expect our methods to be applicable to and similarly benefit other sequence labeling tasks, such as pos tagging and chunking (hovy et al., 2014)."
p17-1029,"
7 Conclusion and Future Work
",2017,"future work will adapt this framework to other sequence transduction scenarios such as machine translation, dialogue generation, question answering, where continuous and discrete latent variables can be abstracted to guide sequence generation."
p17-1031,"
7 Conclusion and Future Work
",2017,reranking the predictions with a language model could be one possible way to improve on this.
p17-1032,"
5 Discussion and Conclusions
",2017,"finally, the optimization of the kda methodology through the suitable parallelization on multicore architectures, as well as the exploration of mechanisms for the dynamic reconstruction of kernel spaces (e.g., operating over hn y) also constitute interesting future research directions on this topic."
p17-1032,"
5 Discussion and Conclusions
",2017,"future work will address experimentations with larger scale datasets; moreover, it is interesting to experiment with more landmarks in order to better understand the trade-off between the representation capacity of the nystrom approximation 篓 of the kernel functions and the over-fitting that can be introduced in a neural network architecture."
p17-1032,"
5 Discussion and Conclusions
",2017,"notice that other low-dimensional approximations of kernel functions have been studied, as for example the randomized feature mappings proposed in rahimi and recht (2008)."
p17-1034,"
6 Conclusion and Future Work
",2017,we are going to explore more effective models in future.
p17-1035,"
9 Conclusion and Future Directions
",2017,"our future agenda includes: (a) optimizing the cnn framework hyper-parameters (e.g., filter width, dropout, embedding dimensions, etc.)to obtain better results, (b) exploring the applicability of our technique for documentlevel sentiment analysis and (c) applying our framework to related problems, such as emotion analysis, text summarization, and questionanswering, where considering textual clues alone may not prove to be sufficient."
p17-1037,"
6 Conclusion
",2017,"for our immediate future work, we plan to embed the topic and user vectors to create a crosstopic stance detector."
p17-1037,"
6 Conclusion
",2017,"it is possible to generalize our work to model heterogeneous signals, such as interests and behaviors of people, for example, 鈥渢hose who are interested in a also support b,鈥 and 鈥渢hose who favor a also vote for b鈥."
p17-1037,"
6 Conclusion
",2017,"therefore, we believe that our work will bring about new applications in the field of nlp and other disciplines."
p17-1038,"
7 Conclusion and Future Work
",2017,"in the future, we will use the proposed automatically data labeling method to more event types and explore more models to extract events by using automatically labeled data."
p17-1038,"
7 Conclusion and Future Work
",2017,"in this paper, we present an approach to automatically label training data for ee."
p17-1039,"
6 Conclusion and future work
",2017,in the future we will try our analytical method on other parts of language.
p17-1040,"
8 Conclusions
",2017,"one of our key innovations is to exploit a curriculum learning based training method to gradually learn to model the underlying noise pattern without direct guidance, and to provide the flexibility to exploit any prior knowledge of the data quality to further improve the effectiveness of the transition matrix."
p17-1042,"
7 Conclusions and future work
",2017,"finally, we would like to apply our model in the decipherment scenario (dou et al., 2015)."
p17-1042,"
7 Conclusions and future work
",2017,"in addition to that, we would like to explore non-linear transformations (lu et al., 2015) and alternative dictionary induction methods (dinu et al., 2015; smith et al., 2017)."
p17-1042,"
7 Conclusions and future work
",2017,"in the future, we would like to delve deeper into this direction and fine-tune our method so it can reliably learn high quality bilingual word embeddings without any bilingual evidence at all."
p17-1045,"
7 Conclusions and Discussion
",2017,"effective implementation of this, however, requires the e2e agent to learn quickly and this is the research direction we plan to focus on in the future."
p17-1046,"
6 Conclusion and Future Work
",2017,"in the future, we shall study how to model logical consistency of responses and improve candidate retrieval."
p17-1047,"
6 Conclusions and Future Work
",2017,the future directions in which this research could be taken are incredibly fertile.
p17-1048,"
5 Summary and discussion
",2017,"future work will apply this technique to the other languages including english, where we have to solve an issue of long sequence lengths, which requires heavy computation cost and makes it difficult to train a decoder network."
p17-1049,"
7 Conclusion
",2017,we are presently trying to extend these results to translations in a different domain (literary texts) into a very different language (hebrew).
p17-1049,"
7 Conclusion
",2017,we leave this as a direction for future research.
p17-1050,"
6 Conclusion and Outlook
",2017,"in future work, we also plan to explore the role of native and second language writing system characteristics in second language reading."
p17-1051,"
7 Conclusions and Future Work
",2017,"for future work, we plan to address the limitations of morse: minimal supervision, greedy inference, and concatenative orthographic model."
p17-1051,"
7 Conclusions and Future Work
",2017,"moreover, we plan to computationally optimize the training stage for the sake of wider adoption by the community."
p17-1052,"
4 Conclusion
",2017,"we proposed a deep pyramid cnn model which has low computational complexity, and can efficiently represent long-range associations in text and so more global information."
p17-1053,"
7 Conclusion
",2017,"for future work, we will investigate the integration of our hr-bilstm into end-to-end systems."
p17-1053,"
7 Conclusion
",2017,"we will also investigate new emerging datasets like graphquestions (su et al., 2016) and complexquestions (bao et al., 2016) to handle more characteristics of general qa."
p17-1054,"
7 Conclusions and Future Work
",2017,"in the future, we are interested in comparing the model to human annotators and using human judges to evaluate the quality of predicted phrases.鈥 our current model does not fully consider correlation among target keyphrases."
p17-1054,"
7 Conclusions and Future Work
",2017,it would also be interesting to explore the multiple-output optimization aspects of our model.
p17-1054,"
7 Conclusions and Future Work
",2017,"our future work may include the following two directions.鈥 in this work, we only evaluated the performance of the proposed model by conducting off-line experiments."
p17-1055,"
8 Conclusion
",2017,"in this context, we are planning to investigate the problems that need comprehensive reasoning over several sentences."
p17-1055,"
8 Conclusion
",2017,the future work will be carried out in the following aspects.
p17-1058,"
8 Conclusions
",2017,"we hope that by comparing and combining our methodology with other approaches of studying dialogue, we can reach a more comprehensive and holistic understanding of this common yet mysterious human practice."
p17-1059,"
6 Conclusions and Future Work
",2017,"for future work, we wish to extend this model by investigating language generation conditioned on other modalities such as facial images and speech, and to applications such as dialogue generation for virtual agents."
p17-1060,"
6 Conclusion
",2017,future work can include an extension of domain experts to take into account dialog history aiming for a holistic framework that can handle contextual interpretation as well.
p17-1061,"
6 Conclusion and Future Work
",2017,all of the above suggest a promising research direction.
p17-1061,"
6 Conclusion and Future Work
",2017,"in addition to dialog acts, we plan to apply our kgcvae model to capture other different linguistic phenomena including sentiment, named entities,etc."
p17-1062,"
7 Conclusion
",2017,"in future work, we plan to extend hcns by incorporating lines of existing work, such as integrating the entity extraction step into the neural network (dhingra et al., 2017), adding richer utterance embeddings (socher et al., 2013), and supporting text generation (sordoni et al., 2015)."
p17-1062,"
7 Conclusion
",2017,"more broadly, hcns are a general model for stateful control, and we would be interested to explore applications beyond dialog systems 鈥 for example, in nlp medical settings or humanrobot nl interaction tasks, providing domain constraints are important for safety; and in resourcepoor settings, providing domain knowledge can amplify limited data."
p17-1062,"
7 Conclusion
",2017,"of course, we also plan to deploy the model in a live dialog system."
p17-1062,"
7 Conclusion
",2017,"we will also explore using hcns with automatic speech recognition (asr) input, for example by forming features from n-grams of the asr n-best results (henderson et al., 2014b)."
p17-1063,"
8 Conclusion
",2017,"finally, it would be interesting to combine our algorithm with a speech synthesis system."
p17-1063,"
8 Conclusion
",2017,"in future work, we will complement the overhearer experiment presented here with an end-toend evaluation in an interactive nlg setting."
p17-1064,"
7 Conclusion
",2017,"in our future work, we expect several developments that will shed more light on utilizing source syntax, e.g., designing novel syntactic features (e.g., features showing the syntactic role that a word is playing) for nmt, and employing the source syntax to constrain and guild the attention models."
p17-1065,"
6 Conclusion and Future Work
",2017,"in addition, we will apply our method to other sequenceto-sequence tasks, such as text summarization, to verify the effectiveness."
p17-1065,"
6 Conclusion and Future Work
",2017,"in future work, along this research direction, we will try to integrate other prior knowledge, such as semantic information, into nmt systems."
p17-1066,"
6 Conclusion and Future Work
",2017,"in the future, we will focus on improving the rumor detection task by exploring network representation learning framework."
p17-1066,"
6 Conclusion and Future Work
",2017,"moreover, we plan to investigate unsupervised models considering massive unlabeled rumorous data from social media."
p17-1068,"
7 Conclusions
",2017,"another direction of future study will look at political ideology prediction in other countries and cultures, where ideology has different or multiple dimensions."
p17-1069,"
7 Conclusion
",2017,"finally, our global psl models can be applied to other domains, such as politics in other countries, simply by changing the initial unigram keywords to reflect the politics of those countries."
p17-1071,"
5 Conclusion
",2017,"we also intend to test other variants around the same concept, including considering the matched words and sequences to have a negative weight to balance further the weight of missing words."
p17-1072,"
6 Concluding Discussion
",2017,"similarly, new ideas show up and even the same idea may change over time and be represented by different words."
p17-1072,"
6 Concluding Discussion
",2017,there are many potential directions to improve our method to account for complex relations between ideas.
p17-1073,"
5 Conclusions
",2017,"however, it is very likely that the annotation framework will work for other slavic languages (e.g."
p17-1075,"
7 Conclusion
",2017,"in future work, we plan to use the analysis from the present study in constructing a system that can be applied to multiple datasets."
p17-1079,"
5 Conclusion
",2017,one interesting avenue of future work is to automatically learn encodings and error correcting codes that are well-suited for the type of binary code prediction we are performing here.
p17-1080,"
7 Conclusion
",2017,"another area for future work is to extend the analysis to other word representations (e.g.byte-pair encoding), deeper networks, and more semantically-oriented tasks such as semantic rolelabeling or semantic parsing."
p17-1081,"
5 Conclusion
",2017,"as future work, we plan to develop a lstmbased attention model to determine the importance of each utterance and its specific contribution to each modality for sentiment classification."
p17-1082,"
8 Conclusion
",2017,our hope is that these stance dimensions will be valuable as a convenient building block for future research on interactional meaning.
p17-1084,"
8 Conclusion
",2017,"as a future work, we would like to explore the feasibility of marrying our semantic and neural models to exploit the benefits that each of them has to offer."
p17-1085,"
8 Conclusion
",2017,"in future, we plan to explore pretraining methods for our model which were shown to improve recall on entity and relation performance by miwa and bansal (2016)."
p17-1085,"
8 Conclusion
",2017,we also plan to extend the identification of entities to full entity mention span instead of only the head phrase as in lu and roth (2015).
p17-1085,"
8 Conclusion
",2017,"we also plan to use sparsemax (martins and astudillo, 2016) instead of softmax for multiple relations, as the former is more suitable for multi-label classification for sparse labels."
p17-1086,"
7 Related work and discussion
",2017,"in contrast, the action space is considered constant in the naturalizing pl approach, and the language adapts to be more natural while accommodating the action space."
p17-1086,"
7 Related work and discussion
",2017,"in the future, we wish to test these ideas in more domains, naturalize a real pl, and handle paraphrasing and implicit arguments."
p17-1086,"
7 Related work and discussion
",2017,"like jia et al.(2017), azaria et al.(2016) starts with an ad-hoc set of initial slot-filling commands in natural language as the basis of further instructions鈥攐ur approach starts with a more expressive core pl designed to interpolate with natural language."
p17-1087,"
9 Conclusion
",2017,"finally, we plan to extend the hard signed clustering presented here to probabilistic soft clustering."
p17-1088,"
7 Conclusion and Future Work
",2017,"in the future, we plan to enable itransf to perform multi-step inference, and extend the sharing mechanism to entity and relation embeddings, further enhancing the statistical binding across parameters."
p17-1093,"
5 Discussions
",2017,we have developed an adversarial neural framework that facilitates an implicit relation network to extract highly discriminative features by mimicking a connective-augmented network.
p17-1094,"
7 Conclusions
",2017,"given the scale of our investigation, we limited our study to lsp, which is anyway considered state of the art."
p17-1094,"
7 Conclusions
",2017,"our study opens several future directions, ranging from defining algorithms based on automatically learned loss functions to learning more effective measures from expert examples."
p17-1095,"
6 Conclusion
",2017,"it would also be interesting to explore more expressive parameterizations, such recurrent neural networks for hy."
p17-1095,"
6 Conclusion
",2017,there are many potential avenues for future work.
p17-1096,"
6 Conclusions
",2017,"in the future, we plan to apply our approach to more question answering datasets in different domains."
p17-1098,"
8 Conclusion
",2017,the diversification model proposed is general enough to apply to other nlg tasks with suitable modifications and we are currently working on extending this to dialog systems and general summarization.
p17-1101,"
7 Conclusion
",2017,"with the proposed selective mechanism, we build an end-to-end neural network summarization model which consists of three phases: encoding, selection, and decoding."
p17-1102,"
5 Conclusion and Future Work
",2017,"in the future, it would be interesting to explore the performance of positionrank on other types of documents, e.g., web pages and emails."
p17-1103,"
7 Discussion
",2017,an important direction for future work is modifying adem such that it is not subject to this bias.
p17-1103,"
7 Discussion
",2017,an important direction of future research is building models that can evaluate the capability of a dialogue system to have an engaging and meaningful interaction with a human.
p17-1103,"
7 Discussion
",2017,we leave investigating the domain transfer ability of adem for future work.
p17-1104,"
7 Conclusion
",2017,"future work will evaluate tupa in a multilingual setting, assessing ucca鈥檚 cross-linguistic applicability."
p17-1104,"
7 Conclusion
",2017,"in addition, we will explore different conversion procedures (kong et al., 2015) to compare different representations, suggesting ways for a data-driven design of semantic annotation."
p17-1104,"
7 Conclusion
",2017,"we believe ucca鈥檚 merits in providing a cross-linguistically applicable, broadcoverage annotation will support ongoing efforts to incorporate deeper semantic structures into various applications, such as sentence simplification (narayan and gardent, 2014) and summarization (liu et al., 2015)."
p17-1104,"
7 Conclusion
",2017,"we will also apply the tupa transition scheme to different target representations, including amr and sdp, exploring the limits of its generality."
p17-1105,"
5 Conclusion
",2017,"our results demonstrate their promise for tree prediction tasks, and we believe their application to more general output structures is an interesting avenue for future work."
p17-1106,"
6 Conclusion
",2017,"in the future, we plan to apply our approach to more nmt approaches (sutskever et al., 2014; shen et al., 2016; tu et al., 2016; wu et al., 2016) on more language pairs to further verify its effectiveness."
p17-1107,"
6 Conclusions
",2017,"our system architecture is generic and can be applied to any classification task, and we expect it to be of use in many annotation projects, especially when dealing with non-standard data or in out-of-domain settings."
p17-1108,"
5 Conclusion and Future Work
",2017,further endeavor may be needed.
p17-1108,"
5 Conclusion and Future Work
",2017,there is lots of future work we can do.
p17-1112,"
7 Conclusion
",2017,"we believe that there are many future avenues to explore to further increase the accuracy of such parsers, including different training objectives, more structured architectures and semisupervised learning."
p17-1113,"
6 Conclusion
",2017,"in the future work, we will replace the softmax function in the output layer with multiple classifier, so that a word can has multiple tags."
p17-1115,"
7 Conclusions and Future Work
",2017,future work may involve testing prewin on an ner task to see if and how it can generalise to a different classification task and how the results compare to the sota and similar methods such as that of collobert et al.(2011) using the conll 2003 ner datasets.
p17-1115,"
7 Conclusions and Future Work
",2017,"if it does perform better, this will be of considerable interest to classification research (and beyond) in nlp."
p17-1116,"
7 Conclusion
",2017,"additionally, we plan to apply the proposed model to other social media analyses such as gender analysis and age analysis."
p17-1116,"
7 Conclusion
",2017,"as future works of this study, we are planning to expand the proposed model to handle multiple locations and a temporal state to capture location changes and states like traveling."
p17-1117,"
6 Conclusion
",2017,"in future work, we are applying our entailment-based multi-task paradigm to other directed language generation tasks such as image captioning and document summarization."
p17-1118,"
6 Conclusions and Future Work
",2017,"in future work, we intend to explore other methods to enrich cn, such as the recurrent language model, and use other metrics to characterize an adjacency network."
p17-1120,"
6 Future Work
",2017,"an important future work is to develop a sophisticated dialogue manager to handle such utterances, for example, by making clarification questions (schloder and fernandez, 2015).篓 we manually investigated the dialogue acts in the chat detection dataset (c.f., section 3.2)."
p17-1120,"
6 Future Work
",2017,further efforts on improving nontask-oriented dialogue systems is an important future work.
p17-1120,"
6 Future Work
",2017,incorporating these techniques into our methods is also an important future work.
p17-1120,"
6 Future Work
",2017,it is an interesting research topic to use such contextual information beyond text.
p17-1120,"
7 Conclusion
",2017,"to facilitate future research, we are going to release the dataset together with the feature values derived from the tweets and web search queries."
p17-1121,"
7 Conclusion and Future Work
",2017,"in future, we would like to include other sources of information in our model."
p17-1121,"
7 Conclusion and Future Work
",2017,"our initial plan is to include rhetorical relations, which has been shown to benefit existing grid models (feng et al., 2014)."
p17-1121,"
7 Conclusion and Future Work
",2017,"we would also like to extend our model to other forms of discourse, especially, asynchronous conversations, where participants communicate with each other at different times (e.g., forum, email)."
p17-1122,"
8 Conclusion
",2017,"finally, we believe that future work should be evaluated in situ, to examine if, and to what extent, the generated responses participate in and affect the discourse (feed) in social media."
p17-1122,"
8 Conclusion
",2017,"some future avenues for investigation include improving the relevance and human-likeness results by improving the automatic parses quality, acquiring more complex templates via abstract grammars, and experimenting with more sophisticated scoring functions for reranking."
p17-1122,"
8 Conclusion
",2017,"with the emergence of deep learning, we further embrace the opportunity to combine the sequence-to-sequence modeling view explored so far with conditioning generation on speakers agendas and user profiles, pushing the envelope of opinionated generation further."
p17-1123,"
7 Conclusion and Future Work
",2017,"besides this, it would also be interesting to consider to incorporate mechanisms for other language generation tasks (e.g., copy mechanism for dialogue generation) in our model to further improve the quality of generated questions."
p17-1123,"
7 Conclusion and Future Work
",2017,here we point out several interesting future research directions.
p17-1123,"
7 Conclusion and Future Work
",2017,we would like to explore how to better use the paragraph-level information to improve the performance of qg system regarding questions of all categories.
p17-1124,"
6 Conclusion and Future Work
",2017,"as future work, we plan to investigate more sophisticated sampling strategies based on active learning and concept graphs to incorporate lexicalsemantic information for concept selection."
p17-1124,"
6 Conclusion and Future Work
",2017,"we also plan to look into ways to propagate feedback to similar and related concepts with partial feedback, to reduce the total amount of feedback."
p17-1125,"
6 Conclusions
",2017,future work involves investigating a better memory selection scheme.
p17-1126,"
5 Conclusion and Future Work
",2017,"in future work, we plan to apply our model to descriptions of time-series data in various domains such as weather forecasts and sports, which share the above writing-style characteristics."
p17-1126,"
5 Conclusion and Future Work
",2017,we also plan to use multiple time-series as input such as multiple brands of stock.
p17-1128,"
5 Conclusion
",2017,"in the future, we will study how to construct a taxonomy from texts in chinese."
p17-1129,"
5 Conclusions and Future Work
",2017,"for future work, we will investigate the wider applicability of chain-of-trees lstm as a general text encoder that can simultaneously capture local syntactic structure and long-range semantic dependency."
p17-1129,"
5 Conclusions and Future Work
",2017,"we will also apply the tree-guided attention mechanism to nlp tasks that need syntaxaware attention, such as machine translation, sentence summarization, textual entailment, etc."
p17-1131,"
8 Conclusions
",2017,"in the future, we plan to build upon the acquired knowledge and the developed classifiers to create automatic tools that provide accurate evaluative feedback of counseling practice."
p17-1133,"
6 Conclusions and Future Work
",2017,"promising future directions would be to investigate how to utilize user interaction in moocs for better prerequisite learning, as well as how deep learning models can be used to automatically learn useful features to help infer prerequisite relations."
p17-1134,"
6 Conclusion and Future Work
",2017,a more sophisticated approach would be to incorporate features into the unsupervised model.
p17-1134,"
6 Conclusion and Future Work
",2017,the incorporation of these features for this segmentation task could be a potentially fruitful avenue for future work.
p17-1136,"
4 Conclusion
",2017,"apart from it, we intend to propose a neural architecture that accomplishes the joint learning of lemmas with other morphological attributes."
p17-1137,"
8 Conclusion
",2017,we will investigate a model which can marginalise word segmentation as latent variables in the future work.
p17-1138,"
6 Conclusion
",2017,"in future work, we would like to apply the presented non-linear bandit learners to other structured prediction tasks."
p17-1140,"
6 Conclusions
",2017,"in the future, we plan to validate the effectiveness of our model on more language pairs."
p17-1141,"
6 Conclusion
",2017,"by using a domain-specific terminology to generate target-side constraints, we have shown that a general domain model can be adapted to a new domain without any retraining."
p17-1141,"
6 Conclusion
",2017,"in future work, we hope to evaluate gbs with models outside of mt, such as automatic summarization, image captioning or dialog generation."
p17-1142,"
7 Conclusion and Future Work
",2017,"another direction involves working to understand text in images, which can provide more information about the subjects of the images."
p17-1142,"
7 Conclusion and Future Work
",2017,future direction involves using graphical modeling to understand interactions in the scene.
p17-1142,"
7 Conclusion and Future Work
",2017,we compared the performance of the htdn to various models that use language and vision alone.
p17-1143,"
6 Conclusion
",2017,we hope that this paper and the accompanying database serve as a first step towards nlp being applied in cybersecurity and that other researchers will be inspired to contribute to the database and to construct their own datasets and implementations.
p17-1144,"
5 Conclusion and Future Work
",2017,we also plan to augment the corpus to support additional types of research on revision analysis.
p17-1144,"
5 Conclusion and Future Work
",2017,"while in this paper we explored language as one factor influencing rewriting behavior, our corpus also contains information about other potential factors such as gender and education level which we plan to investigate in the future."
p17-1146,"
7 Conclusion
",2017,"because our neural models are applicable to srl, applying our models for multilingual srl tasks presents an interesting future research direction."
p17-1146,"
7 Conclusion
",2017,"in future work, we plan to explore effective methods for exploiting large-scale unlabeled data to learn the neural models."
p17-1147,"
8 Conclusion and Future Work
",2017,results from current state-of-the-art baselines indicate that triviaqa is a challenging testbed that deserves significant future study.
p17-1148,"
7 Future Work
",2017,"other document-level features, such as example input-output pairs, unit tests, might be useful in this endeavor."
p17-1148,"
7 Future Work
",2017,"while our experiments look at learning rudimentary translational correspondences between text and code, a next step might be learning to synthesize executable programs via these translations, along the lines of (desai et al., 2016; raza et al., 2015)."
p17-1149,"
7 Conclusions and Future Work
",2017,"in the future, we will improve the scalability of our model and learn multi-prototype embeddings for the mentions without reference entities in a knowledge base, and introduce compositional approaches to model the internal structures of multiword mentions."
p17-1150,"
6 Conclusion
",2017,future work can explore deep neural network to alleviate feature engineering.
p17-1150,"
6 Conclusion
",2017,one of the important questions to address in the future is how to learn new predicates through interaction with humans.
p17-1150,"
6 Conclusion
",2017,our future work will incorporate interactive learning of verb semantics with task learning to enable autonomy that can learn by communicating with humans.
p17-1150,"
6 Conclusion
",2017,"recent years have seen an increasing amount of work on task learning from human partners (saunders et al., 2006; chernova and veloso, 2008; cantrell et al., 2012; mohan et al., 2013; asada et al., 2009; mohseni-kabir et al., 2015; nejati et al., 2006; liu et al., 2016)."
p17-1150,"
6 Conclusion
",2017,the interaction strategies are learned through reinforcement learning.
p17-1151,"
5 Discussion
",2017,"in the future, multimodal word distributions could open the doors to a new suite of applications in language modelling, where whole word distributions are used as inputs to new probabilistic lstms, or in decision functions where uncertainty matters."
p17-1152,"
6 Conclusions and Future Work
",2017,"future work interesting to us includes exploring the usefulness of external resources such as wordnet and contrasting-meaning embedding (chen et al., 2015) to help increase the coverage of wordlevel inference relations."
p17-1153,"
6 Conclusion
",2017,future work includes expanding the analyses to non-english movies and combining the linguistic metrics with character networks.
p17-1154,"
6 Conclusion and Future Work
",2017,"as future work, we plan to apply the linguistic regularizers to tree-lstm to address the scope issue since the parsing tree is easier to indicate the modification scope explicitly."
p17-1155,"
8 Discussion and Future Work
",2017,designing automatic measures is hence left for future research.
p17-1155,"
8 Discussion and Future Work
",2017,future research directions rise from cases in which the sign models left the tweet unchanged.
p17-1155,"
8 Discussion and Future Work
",2017,several challenges are still to be addressed in future research so that sarcasm interpretation can be performed in a fully automatic manner.
p17-1155,"
8 Discussion and Future Work
",2017,we hope this new resource will help researchers make further progress on this new task.
p17-1156,"
5 Conclusion
",2017,we also propose a unified model to incorporate different types of sentiment information to train sentiment classifier for target domain.
p17-1157,"
7 Conclusion
",2017,"we traced this to the size of the available data in each sector, and show that there are still benefits in considering sectors, which could be further explored in the future as more data becomes available."
p17-1158,"
6 Conclusion and Future Work
",2017,"in future, we will strive to implement cane on a wider variety of information networks with multi-modal data, such as labels, images and so on.(2) cane encodes latent relations between vertices into their context-aware embeddings."
p17-1158,"
6 Conclusion and Future Work
",2017,"thus, we want to explore how to incorporate and predict these explicit relations between vertices in ne."
p17-1158,"
6 Conclusion and Future Work
",2017,we will explore the following directions in future: (1) we have investigated the effectiveness of cane on text-based information networks.
p17-1159,"
7 Conclusion
",2017,possible future work include expanding the investigation to other regional languages such as malay and indonesian.
p17-1160,"
Future work
",2017,an interesting avenue for future work is to explore higher order factorizations for noncrossing digraphs and the related inference.
p17-1160,"
Future work
",2017,"we are planning to extend the coverage of the approach by exploring 1-endpointcrossing and mhk trees (pitler et al., 2013; gomez-rodr 麓 麓谋guez, 2016), and related digraphs 鈥 see (yli-jyra篓, 2004; gomez-rodr 麓 麓谋guez et al., 2011)."
p17-1163,"
7 Conclusion
",2017,"in future work, we intend to explore applications of the nbt for multi-domain dialogue systems, as well as in languages other than english that require handling of complex morphological variation."
p17-1165,"
5 Discussion
",2017,"in the future, we plan on relying on other inference approaches, based for example on variational bayes known to yield better estimates for perplexity (asuncion et al., 2009); it is however not certain that the gain in perplexity one can expect from the use of variational bayes approaches will necessarily result in a gain in, say, topic coherence."
p17-1166,"
5 Conclusion and Future Works
",2017,"in the future, we will focus on two aspects: (1) our method in this paper considers pairwise intersections between labels, so to better exploit class ties, we will extend our method to exploit all other labels鈥 influences on each relation for relation extraction, transferring second-order to high-order (zhang and zhou, 2014); (2) we will focus on other problems by leveraging class ties between labels, specially on multi-label learning problems (zhou et al., 2012) such as multi-category text categorization (rousu et al., 2005) and multi-label image categorization (zha et al., 2008)."
p17-1167,"
6 Conclusion & Future Work
",2017,"because of the dynamic nature of our framework, it is not trivial to leverage the computational capabilities of gpus using minibatched training; we plan to investigate ways to take full advantage of modern computing machinery in the near future."
p17-1167,"
6 Conclusion & Future Work
",2017,"in the future, we plan to investigate several interesting research questions triggered by this work."
p17-1168,"
5 Conclusion
",2017,"we also showed empirically that multiplicative gating is superior to addition and concatenation operations for implementing gated-attentions, though a theoretical justification remains part of future research goals."
p17-1169,"
6 Conclusions and Future Work
",2017,it would also be interesting to investigate several possible extensions to the current clustering work.
p17-1170,"
7 Conclusion and Future Work
",2017,"also, given the promising results observed for supersenses, we plan to investigate taskspecific coarsening of sense inventories, particularly wikipedia, or the use of sentiwordnet (baccianella et al., 2010), which could be more suitable for polarity detection."
p17-1170,"
7 Conclusion and Future Work
",2017,"as future work, we plan to investigate the extension of the approach to other languages and applications."
p17-1170,"
7 Conclusion and Future Work
",2017,we hope that our work will foster future research on the integration of senselevel knowledge into downstream applications.
p17-1171,"
6 Conclusion
",2017,future work should aim to improve over our drqa system.
p17-1174,"
6 Conclusion
",2017,"in addition, we plan to combine our decoder with other encoders that capture language structure, such as a hierarchical rnn (luong and manning, 2016), a tree-lstm (eriguchi et al., 2016b), or an order-free encoder, such as a cnn (kalchbrenner and blunsom, 2013)."
p17-1174,"
6 Conclusion
",2017,"in future work, we will explore the optimal structures of chunk-based decoder for other free word-order languages such as czech, german, and turkish."
p17-1175,"
7 Conclusions and Future Work
",2017,"in the future, we will incorporate coverage into our model and study how to apply it to other natural language processing tasks."
p17-1176,"
6 Conclusion
",2017,"in the future, we plan to test our approach on more diverse language pairs, e.g., zero-resource uyghur-english translation using chinese as a pivot."
p17-1176,"
6 Conclusion
",2017,it is also interesting to extend the teacherstudent framework to other cross-lingual nlp applications as our method is transparent to architectures.
p17-1177,"
6 Conclusion
",2017,"for future work, it will be interesting to make use of node labels from the tree, or to use syntactic information on the target side, as well."
p17-1178,"
6 Conclusions and Future Work
",2017,"in the future, we will explore the topological structure of related languages and exploit cross-lingual knowledge transfer to enhance the quality of extraction and linking."
p17-1179,"
6 Conclusion
",2017,"future work also includes investigating other divergences that adversarial training can minimize (nowozin et al., 2016), and broader mathematical tools that match distributions (mohamed and lakshminarayanan, 2016)."
p17-1180,"
8 Conclusion and Future Work
",2017,"as future directions, we plan to extend gwld to several other languages and conduct similar sociolinguistic studies on cs patterns including not only more languages and geographies, but also other aspects like topic and sentiment."
p17-1181,"
5 Conclusions
",2017,cognates detection is an interesting and challenging task.
p17-1182,"
8 Future Work
",2017,"in the future, we want to develop methods to make better use of languages with different alphabets or morphosyntactic features, in order to increase the applicability of our knowledge transfer method."
p17-1183,"
7 Conclusion
",2017,"future work may include applying our model to other nearly-monotonic alignand-transduce tasks like abstractive summarization, transliteration or machine translation."
p17-1184,"
6 Conclusion
",2017,we plan to explore these effects in future work.
p17-1185,"
7 Conclusions
",2017,possible direction of future work is to apply more advanced optimization techniques to the step 1 of the scheme proposed in section 1 and to explore the step 2 鈥 obtaining embeddings with a given low-rank matrix.
p17-1186,"
6 Conclusion
",2017,in future work we hope to explore cross-task scoring and inference for tasks where parallel annotations are not available.
p17-1187,"
5 Conclusion and Future Work
",2017,"specifically, we utilize sememe information to represent various senses of each word and propose sememe attention to select appropriate senses in contexts automatically."
p17-1187,"
5 Conclusion and Future Work
",2017,we will explore the effectiveness of sememe information for wrl in other languages.
p17-1187,"
5 Conclusion and Future Work
",2017,"we will explore the following research directions in future: (1) the sememe information in hownet is annotated with hierarchical structure and relations, which have not been considered in our framework."
p17-1187,"
5 Conclusion and Future Work
",2017,we will explore to utilize these annotations for better wrl.(2) we believe the idea of sememes is universal and could be wellfunctioned beyond languages.
p17-1188,"
7 Conclusion and Future Work
",2017,"in the future, we hope to further generalize this method to other tasks such as pronunciation estimation, which can take advantage of the fact that pronunciation information is encoded in parts of the characters as demonstrated in fig.1, or machine translation, which could benefit from a wholistic view that considers both semantics and pronunciation."
p17-1189,"
7 Conclusion and Future Work
",2017,"so in the future, we might try to combine more heterogeneous semantic data for other tasks like event extraction and relation classification, etc."
p17-1189,"
7 Conclusion and Future Work
",2017,we hope that it will be helpful in providing common benchmarks for future work on chinese srl tasks.
p17-1190,"
6 Conclusion
",2017,"future work will explore additional data sources, including from aligning different translations of novels (barzilay and mckeown, 2001), aligning new articles of the same topic (dolan et al., 2004), or even possibly using machine translation systems to translate bilingual text into paraphrastic sentence pairs."
p17-1193,"
6 Conclusion and Future Work
",2017,the arcfactored model proposed in this paper may be enhanced with higher-order features too.
p17-1193,"
6 Conclusion and Future Work
",2017,we leave this for future investigation.
p17-1194,"
9 Conclusion
",2017,future work could investigate the extension of this architecture to additional unannotated resources.
p17-1194,"
9 Conclusion
",2017,we proposed a novel sequence labeling framework with a secondary objective 鈥 learning to predict surrounding words for each word in the dataset.
p17-1195,"
10 Conclusion
",2017,our future work includes the expansion of the lexicon with the aid of the semantic parser and the development of a disambiguation model for the binding and scoping structures.
p18-1001,"
6 Conclusion and Future Work
",2018,"future work includes an investigation into the trade-off between learning full covariance matrices for each word distribution, computational complexity, and performance."
p18-1001,"
6 Conclusion and Future Work
",2018,other future work involves co-training pft on many languages.
p18-1004,"
6 Conclusion
",2018,"in future work, we will investigate explicit retrofitting methods for asymmetric relations like hypernymy and meronymy."
p18-1004,"
6 Conclusion
",2018,we also intend to apply the method to other downstream tasks and to investigate the zero-shot language transfer of the specialization function for more language pairs.
p18-1005,"
5 Conclusion and Future work
",2018,"in the future, we would like to investigate how to utilize the monolingual data more effectively, such as incorporating the language model and syntactic information into unsupervised nmt."
p18-1005,"
5 Conclusion and Future work
",2018,unsupervised nmt opens exciting opportunities for the future research.
p18-1006,"
5 Conclusion
",2018,"in the future, we may extend our architecture to other scenarios, such as totally unsupervised training with no bilingual data for the rare language."
p18-1007,"
6 Conclusion
",2018,"additionally, we would like to explore the application of subword regularization for machine learning, including denoising auto encoder (vincent et al., 2008) and adversarial training (goodfellow et al., 2015)"
p18-1007,"
6 Conclusion
",2018,"promising avenues for future work are to apply subword regularization to other nlp tasks based on encoder-decoder architectures, e.g., dialog generation (vinyals and le, 2015) and automatic summarization (rush et al., 2015)."
p18-1009,"
8 Conclusion
",2018,"these results set the first performance levels for our evaluation dataset, and suggest that the data will support significant future work."
p18-1010,"
8 Conclusion
",2018,"while this work already demonstrates considerable improvement over non-hierarchical modeling, future work will explore techniques such as box embeddings (vilnis et al., 2018) and poincare麓 embeddings (nickel and kiela, 2017) to represent the hierarchical embedding space, as well as methods to improve recall in the candidate generation process for entity linking."
p18-1015,"
5 Conclusion and Future Work
",2018,"on the one hand, since the candidate templates are far inferior to the optimal ones, we intend to improve the retrieve module, e.g., by indexing both the sentence and summary fields."
p18-1015,"
5 Conclusion and Future Work
",2018,"on the other hand, we plan to test our system on the other tasks such as document-level summarization and short text conversation."
p18-1016,"
8 Conclusion
",2018,future work will leverage ucca鈥檚 cross-linguistic applicability to support multi-lingual ts and ts pre-processing for mt.
p18-1016,"
8 Conclusion
",2018,"the two works, which apply this assumption to different ends (ts system construction, and ts evaluation), confirm its validity."
p18-1018,"
7 Conclusion
",2018,"we expect that future work will develop methods to scale the annotation process beyond requiring highly trained experts; bring this scheme to bear on other languages; and investigate the relationship of our scheme to more structured semantic representations, which could lead to more robust models."
p18-1020,"
6 Conclusions
",2018,the significant gains demonstrated suggests easl as a promising approach for future dataset curation and system evaluation in the community.
p18-1023,"
7 Conclusion
",2018,"we provide a corpus to train respective approaches, but leave the according research to future work."
p18-1023,"
7 Conclusion
",2018,"while the model can be considered open-topic, a next step will be to study counterargument retrieval open-source."
p18-1024,"
7 Concluding Remarks and Future Work
",2018,"for future work, we would like to extend the current evaluation of our work from a two-graph setting to multiple graphs."
p18-1024,"
7 Concluding Remarks and Future Work
",2018,"however, we point out that our method is not restricted to such use cases鈥攐ne can readily apply our method to directly make inference over multiple graphs to support applications like question answering and conversations."
p18-1025,"
6 Conclusion and Future Work
",2018,an exciting direction is the incorporation of multi-relational data for general knowledge representation and inference.
p18-1026,"
7 Discussion and Conclusion
",2018,incorporating both ideas to our architecture is an research direction we plan for future work.
p18-1028,"
9 Conclusion
",2018,"as a simple version of an rnn, which is more expressive than one-layer cnns, we hope that sopa will encourage future research on the bridge between these two mechanisms."
p18-1028,"
9 Conclusion
",2018,"it naturally models flexible-length spans with insertion and deletion, and it can be easily customized by swapping in different semirings."
p18-1029,"
6 Discussion and Future Work
",2018,"future work can model how these parameters can be adapted in a task specific way (e.g., cases such as cancer prediction where base rates are small), and provide better models of quantifier semantics.e.g., as distributions, rather than point values."
p18-1029,"
6 Discussion and Future Work
",2018,"this is an exciting direction, which contrasts with the predominant theme of using statistical learning methods to advance the field of nlp."
p18-1029,"
6 Discussion and Future Work
",2018,"we believe that language may have as much to help learning, as statistical learning has helped nlp."
p18-1030,"
5 Conclusion
",2018,"next directions also include the investigation of s-lstm to more nlp tasks, such as machine translation."
p18-1030,"
5 Conclusion
",2018,"we have investigated s-lstm, a recurrent neural network for encoding sentences, which offers richer contextual information exchange with more parallelism compared to bilstms."
p18-1030,"
5 Conclusion
",2018,we leave such investigation to future work.
p18-1031,"
6 Discussion and future directions
",2018,another direction is to apply the method to novel tasks and models.
p18-1031,"
6 Discussion and future directions
",2018,"given that transfer learning and particularly fine-tuning for nlp is under-explored, many future directions are possible."
p18-1031,"
6 Discussion and future directions
",2018,"language modeling can also be augmented with additional tasks in a multi-task learning fashion (caruana, 1993) or enriched with additional supervision, e.g.syntax-sensitive dependencies (linzen et al., 2016) to create a model that is more general or better suited for certain downstream tasks, ideally in a weakly-supervised manner to retain its universal properties."
p18-1033,"
6 Conclusion
",2018,developers of future large-scale datasets should incorporate joins and nesting to create more human-like data.
p18-1033,"
6 Conclusion
",2018,our analysis has clear implications for future work.
p18-1034,"
6 Conclusion and Future Work
",2018,"in the future, we plan to improve the accuracy of the column prediction component."
p18-1034,"
6 Conclusion and Future Work
",2018,we also plan to build a large-scale dataset that considers more sophisticated sql queries.
p18-1034,"
6 Conclusion and Future Work
",2018,"we also plan to extend the approach to low-resource scenarios (feng et al., 2018)."
p18-1035,"
10 Conclusion
",2018,"future work will investigate whether a single algorithm and architecture can be competitive on all of these parsing tasks, an important step towards a joint many-task model for semantic parsing."
p18-1036,"
7 Conclusion
",2018,"however, relatively less improvement is expected when model complexity is increased, 鈥 they generally perform better than models that only have access to predicted/silver morphological tags."
p18-1039,"
8 Conclusion
",2018,"in addition, we plan to expand the coverage of our meaning representation to support more mathematic concepts."
p18-1039,"
8 Conclusion
",2018,"in the future, we will focus on tackling this challenge."
p18-1040,"
8 Conclusions
",2018,"in the future, we plan to model document-level representations which are more in line with drt and the gmb annotations."
p18-1042,"
8 Conclusion
",2018,"there are hundreds of millions of parallel sentence pairs, and more are being generated continually."
p18-1044,"
6 Conclusion
",2018,"in the future, we will apply this semi-supervised training method to other nlp tasks."
p18-1045,"
6 Conclusions and the Future Work
",2018,"in the future, we will explore the use of additional discourse structures that correlate highly with event coreference chains."
p18-1047,"
Conclusions and Future Work
",2018,another future work is test our model in other nlp tasks like event extraction.
p18-1047,"
Conclusions and Future Work
",2018,our future work will concentrate on how to improve the performance further.
p18-1048,"
7 Conclusion
",2018,"therefore, in the future, we will encode the global information by neural networks and use the self-regulation strategy to reduce the negative influence of noises."
p18-1048,"
7 Conclusion
",2018,we use a self-regulated learning approach to improve event detection.
p18-1050,"
7 Conclusions
",2018,"for the future work, we plan to expand event temporal knowledge acquisition by dealing with event sense disambiguation and event synonym identification (e.g., drag, pull and haul)."
p18-1051,"
5 Conclusion
",2018,"in future work, we intend to thoroughly study the impact of tds given morphosyntactic information."
p18-1052,"
7 Conclusion
",2018,our future goal is to use the coherence model to generate new conversations.
p18-1053,"
5 Conclusion
",2018,"in the future, we plan to explore neural network models for efficaciously resolving anaphoric zero pronoun documents and research on some specific components which might influence the performance of the model, such as the embedding."
p18-1053,"
5 Conclusion
",2018,"meanwhile, we plan to research on the possibility of applying adversarial learning (goodfellow et al., 2014) to generate better rewards than the human-defined reward functions."
p18-1056,"
5 Conclusion
",2018,"however, we want to stress the difference between the priming-induced alignment at lower linguistic levels and the intentional accommodation that is caused by higher-level perception of social power."
p18-1056,"
5 Conclusion
",2018,"in particular, our findings suggest that the probability change of liwc categories is more likely to be a case of automatic alignment, rather than an intentional accommodation, because it is better explained by lowerlevel linguistic features (utterance length)."
p18-1056,"
5 Conclusion
",2018,"therefore, we suggest that future work on social power and language use should consider other (maybe higher-level) linguistic elements."
p18-1056,"
5 Conclusion
",2018,"we call for the inclusion of a wider range of factors in future studies of social influences on language use, especially low-level but interpretable cognitive factors."
p18-1057,"
7 Conclusion and Future Work
",2018,"additionally, the corpus promotes research on tasks not limited to pedagogical function classification, topic modeling and prerequisite relation labelling."
p18-1057,"
7 Conclusion and Future Work
",2018,for future work we plan to continue the collection and annotation of resources and to separately explore each of the above research tasks.
p18-1057,"
7 Conclusion and Future Work
",2018,"to this point, we believe that this dataset, with its multiple layers of annotation and usable interface, will be an invaluable tool to the students, educators and researchers of nlp."
p18-1059,"
5 Conclusion
",2018,"indeed, if outputs all similarly under-correct, correlation studies will not be affected by whether an rbm is sensitive to undercorrection."
p18-1059,"
5 Conclusion
",2018,"we argue that using low-coverage reference sets has adverse effects on the reliability of referencebased evaluation, with gec and ts as a test case, and consequently on the incentives offered to systems."
p18-1060,"
7 Discussion
",2018,"as a result, the evaluation is systematically biased against genuine system improvements that would lead to higher human evaluation scores but not improve automatic metrics."
p18-1060,"
7 Discussion
",2018,"certainly, we can try to improve the automatic metric (which is potentially as difficult as solving the task) and brainstorming alternative ways of soliciting evaluation (which has been less explored)."
p18-1060,"
7 Discussion
",2018,we hope our work provides some clarity on to how to make it more cost effective.
p18-1062,"
7 Conclusion and Next Steps
",2018,"future efforts should be dedicated to improving the community detection phase and generating more abstractive sentences, probably by harnessing deep learning."
p18-1065,"
5 Conclusions and Recommendations
",2018,"however, there have been exciting developments in helpfulness prediction: systems that have attempted to exploit user and reviewer information, along with those based on sophisticated models (e.g., probabilistic matrix factorization, hmm-lda) and neural network architectures, are promising prospects for future work."
p18-1065,"
5 Conclusions and Recommendations
",2018,"this is why we consider that user-specific helpfulness prediction, first presented in moghaddam et al.(2012) and tang et al.(2013), should be the goal of future work, as it allows systems to tailor their predictions to users鈥 preferences and needs (much like a recommender system)."
p18-1065,"
5 Conclusions and Recommendations
",2018,we conclude our survey with several recommendations for future work on computational modeling and prediction of review helpfulness.
p18-1066,"
7 Conclusion
",2018,"future directions include: 1) mining cross-cultural differences in general concepts other than names and slang, 2) merging the mined knowledge into existing knowledge bases, and 3) applying the socvec in downstream tasks like machine translation."
p18-1067,"
7 Conclusion
",2018,"in future works, we will exploit the interesting connections between moral foundations and frames for the analysis of more detailed ideological leanings and stance prediction."
p18-1068,"
6 Conclusions
",2018,"in the future, we would like to apply the framework in a weakly supervised setting, i.e., to learn semantic parsers from question-answer pairs and to explore alternative ways of defining meaning sketches."
p18-1069,7 Conclusions,2018,directions for future work are many and varied.
p18-1070,"
6 Conclusion
",2018,"we propose structvae, a deep generative model with tree-structured latent variables for semi-supervised semantic parsing."
p18-1071,"
6 Conclusions
",2018,"for future work, to solve the problem of the lack of training data, we want to design weakly supervised learning algorithm using denotations (qa pairs) as supervision."
p18-1071,"
6 Conclusions
",2018,"furthermore, structure and semantic constraints can be easily incorporated in decoding to enhance semantic parsing."
p18-1073,"
6 Conclusions
",2018,"in the future, we would like to extend the method from the bilingual to the multilingual scenario, and go beyond the word level by incorporating embeddings of longer phrases."
p18-1074,"
5 Conclusions and Future Work
",2018,"the next step of this research is to apply this architecture to other types of tasks, such as event extract and semantic role labeling that involve structure prediction."
p18-1074,"
5 Conclusions and Future Work
",2018,we also plan to explore the possibility of integrating incremental learning into this architecture to adapt a trained model for new tasks rapidly.
p18-1076,"
7 Conclusion and Future Work
",2018,"in future work, we will investigate even tighter integration of the attended knowledge and stronger reasoning methods."
p18-1076,"
7 Conclusion and Future Work
",2018,"this opens up for deeper investigation and future improvement of rc models in a targeted way, allowing us to investigate what knowledge sources are required for different data sets and domains."
p18-1079,"
7 Limitations and Future Work
",2018,"developing more effective ways of leveraging the expert鈥檚 time to close the loop, and facilitating more interactive collaboration between humans and sears are exciting areas for future work."
p18-1079,"
7 Limitations and Future Work
",2018,"having demonstrated the usefulness of seas and sears in a variety of domains, we now describe their limitations and opportunities for future work."
p18-1079,"
8 Conclusion
",2018,"we demonstrated that seas and sears can be an invaluable tool for debugging nlp models, while indicating their current limitations and avenues for future work."
p18-1080,"
7 Conclusion
",2018,in future work we intend to apply this technique to debiasing sentences and anonymization of author traits such as gender and age.
p18-1080,"
7 Conclusion
",2018,"in particular, it would be interesting to back-translate through multiple target languages with a single source language (johnson et al., 2016)."
p18-1080,"
7 Conclusion
",2018,"in the future work, we will also explore whether an enhanced back-translation by pivoting through several languages will learn better grounded latent meaning representations."
p18-1081,"
5 Conclusion
",2018,"in future work, we hope to explore the potential of this architecture on further kinds of data, including multimodal data (long et al., 2018), from which one can extract structured signals."
p18-1084,"
8 Conclusion and Future Work
",2018,in future work we plan to improve our methods by exploiting the internal structure of images and sentences as well as by effectively integrating signals from more than two languages.
p18-1085,"
5 Conclusion
",2018,"in future work, we would like to explore other aspects of search engines for language grounding as well as the effect these embeddings may have on learning generic sentence representations (kiros et al., 2015b; hill et al., 2016; conneau et al., 2017a; logeswaran and lee, 2018)."
p18-1086,"
6 Discussion and Conclusion
",2018,"nevertheless, we hope this work can motivate more research in this area, enabling physical action-effect reasoning, towards agents which can perceive, act, and communicate with humans in the physical world."
p18-1086,"
6 Discussion and Conclusion
",2018,"they need to understand the current state, to map their goals to the world state, and to plan for actions that can lead to the goals."
p18-1086,"
6 Discussion and Conclusion
",2018,"we also plan to incorporate action-effect prediction to humanrobot collaboration, for example, to bridge the gap of commonsense knowledge about the physical world between humans and robots."
p18-1086,"
6 Discussion and Conclusion
",2018,"we plan to apply more advanced approaches in the future, for example, attention models that jointly capture actions, image states, and effect descriptions."
p18-1088,"
7 Conclusion and Future Work
",2018,we believe that there will be more effective solutions coming in the near future.
p18-1090,"
5 Conclusions and Future Work
",2018,"for future work, we would like to explore a fine-grained version of sentiment-to-sentiment translation that not only reverses sentiment, but also changes the strength of sentiment."
p18-1091,"
7 Conclusion
",2018,future works involve the choice of discourse markers and some other transfer learning sources.
p18-1092,"
5 Conclusion
",2018,"while other models have focused on different parts of these components, we think that is important to find ways to combine these different mechanisms if we want to build models capable of complex reasoning."
p18-1095,"
6 Conclusions
",2018,"in the future we want to apply our marginal utility based framework to other metrics, such as mean average precision (map)."
p18-1098,"
6 Conclusions and Discussions
",2018,a full study is ongoing.
p18-1098,"
6 Conclusions and Discussions
",2018,"for example, in tweets, we can use the method to map an informally written mention 鈥榥bcbightlynews鈥 to a canonical entity 鈥楴bc nightly news鈥 in the knowledge base."
p18-1098,"
6 Conclusions and Discussions
",2018,"we will address these two issues in future by investigating diversity-promoting regularization (xie et al., 2017) and leveraging an external knowledge base that maps medical abbreviations into their full names."
p18-1100,"
6 Conclusions & Future Work
",2018,further study of the uses of transfer learning algorithms on promptindependent aes needs to be undertaken.
p18-1101,"
5 Conclusion and Future Work
",2018,"our findings also suggest promising future research directions, including learning better context-based latent actions and using reinforcement learning to adapt policy networks."
p18-1103,"
6 Conclusion
",2018,we would like to explore in depth how attention can help improve neural dialogue modeling for both chatbots and taskoriented dialogue systems in our future work.
p18-1104,"
6 Conclusion and Future Work
",2018,"due to the nature of social media text, some emotions, such as fear and disgust, are underrepresented in the dataset, and the distribution of emojis is unbalanced to some extent."
p18-1105,"
6 Conclusion
",2018,another direction will be to apply fluctuation analysis in formulating a statistical test to evaluate the structural complexity underlying a sequence.
p18-1105,"
6 Conclusion
",2018,"our future work will include an analysis using other kinds of data, such as twitter data and adult utterances, and a study of how taylor鈥檚 law relates to grammatical complexity for different sequences."
p18-1107,"
8 Conclusion and Future Work
",2018,we further plan to empirically evaluate our lexicalization on an alignment task and to offer a comparison against the lexicalization due to zhang and gildea (2005).
p18-1107,"
8 Conclusion and Future Work
",2018,"we plan to verify whether rank-k pl-rstag is more powerful than rank-k scfg in future work, and to reduce the rank of the transformed grammar if possible."
p18-1111,"
7 Conclusion
",2018,"in the future, we plan to take generalization one step further, and explore the possibility to use the bilstm for generating completely new paraphrase templates unseen during training."
p18-1113,"
7 Conclusion
",2018,future work will introduce weighted cbow and skip-gram to learn positional information within sentences.
p18-1114,"
6 Conclusion
",2018,"in the future, we intend to evaluate our models for some morpheme-rich languages like russian, german and so on."
p18-1115,"
7 Conclusion and Future Work
",2018,"as this is the first work that systematically considers word-level variation in nmt, there are lots of research ideas to explore in the future."
p18-1116,"
6 Conclusion and future work
",2018,"as future work, we plan to design some more elaborate structures to incorporate the score layer into the encoder."
p18-1116,"
6 Conclusion and future work
",2018,we will also apply the proposed linearization method to other tasks.
p18-1117,"
7 Conclusions
",2018,"future work could also investigate whether context-aware nmt systems learn other discourse phenomena, for example whether they improve the translation of elliptical constructions, and markers of discourse relations and information structure."
p18-1118,"
7 Conclusion
",2018,"for future work, we intend to investigate models which incorporate specific discourse-level phenomena."
p18-1119,"
6 Conclusions and Future Work
",2018,future work may see our methods applied to document geolocation to assess the effectiveness of scaling geodesic vectors from paragraphs to entire documents.
p18-1120,"
5 Conclusions and Future Work
",2018,"in future work, we hope to see if we can take advantage of more contextual information as well as other external knowledge to improve the recognition of goalacts."
p18-1120,"
5 Conclusions and Future Work
",2018,"nevertheless, our work represents a first step toward learning goal knowledge about locations, and we believe that learning knowledge about plans and goals is an important direction for natural language understanding research."
p18-1122,"
7 Conclusion
",2018,"given the fact that existing schemes suffer from low iaas and lack of data, we hope that the findings in this work would provide a good start towards understanding more sophisticated semantic phenomena in this area."
p18-1123,"
6 Conclusions
",2018,in our future work we plan to extend the proposed method to these other applications.
p18-1124,"
6 Conclusion
",2018,the model learns from only simulated data which makes it easy to adapt to new domains.
p18-1125,"
7 Conclusions and Future Work
",2018,a promising line of future work could consider the complementary problem of identifying pragmatic strategies that can help bring uncivil conversations back on track.
p18-1125,"
7 Conclusions and Future Work
",2018,"in this work, we started to examine the intriguing phenomenon of conversational derailment, studying how the use of pragmatic and rhetorical devices relates to future conversational failure."
p18-1125,"
7 Conclusions and Future Work
",2018,our approach has several limitations which open avenues for future work.
p18-1125,"
7 Conclusions and Future Work
",2018,the human accuracy on predicting future attacks in this setting (72%) suggests it is feasible at least at the level of human intuition.
p18-1125,"
7 Conclusions and Future Work
",2018,"to this end, we develop a computational framework for analyzing how general politeness strategies and domain-specific rhetorical prompts deployed in the initial stages of a conversation are tied to its future trajectory."
p18-1125,"
7 Conclusions and Future Work
",2018,"we show that our computational framework can recover some of that intuition, hinting at the potential of automated methods to identify signals of the future trajectories of online conversations."
p18-1125,"
7 Conclusions and Future Work
",2018,"while our analysis focused on the very first exchange in a conversation for the sake of generality, more complex modeling could extend its scope to account for conversational features that more comprehensively span the interaction."
p18-1127,"
9 Conclusion
",2018,"the difficulties in basing validation on system outputs may be applicable to other text-to-text generation tasks, a question we will explore in future work."
p18-1128,"
6 Conclusions
",2018,"we hope this paper will serve as a guide for nlp researchers and, not less importantly, that it will encourage discussions and collaborations that will contribute to the soundness and correctness of our research."
p18-1130,"
5 Conclusion
",2018,"another interesting direction is to further improve our model by exploring reinforcement learning approaches to learn an optimal order for the children of head words, instead of using a predefined fixed order."
p18-1130,"
5 Conclusion
",2018,"first, we intend to consider how to conduct experiments to improve the analysis of parsing errors qualitatively and quantitatively."
p18-1132,"
5 Conclusion
",2018,we explore the possibility that how the structure is built affects number agreement performance.
p18-1133,"
6 Conclusion
",2018,"for our future work, we will consider advanced instantiations for sequicity, and extend sequicity to handle unsupervised cases where information and requested slots values are not annotated."
p18-1137,"
6 Conclusion
",2018,"for the specific-requirement scenario, such as customer service, which requires specific and high quality responses, maximum generated likelihood is used as the objective function instead of the averaged one."
p18-1137,"
6 Conclusion
",2018,"in future work, we plan to further investigate the impact of risksensitive objective functions, including the relations between model robustness and diverse generations."
p18-1137,"
6 Conclusion
",2018,"while for the diverse-requirement, such as chatbot, which requires diverse and high quality responses even if for the same post, cvar is used as the objective function for worst case optimization."
p18-1138,"
5 Conclusion
",2018,"in future work, we plan to introduce reinforcement learning and knowledge base reasoning mechanisms to improve the performance."
p18-1139,"
5 Conclusion
",2018,"as for future work, we will investigate how to apply the technique to multi-turn conversational systems, provided that the most proper sentence function can be predicted under a given conversation context."
p18-1142,"
7 Conclusions and Future Work
",2018,future work will look into automating the tree processing procedure.
p18-1143,"
7 Conclusion
",2018,"further, we would like to study sampling techniques motivated by natural distributions of linguistic structures."
p18-1143,"
7 Conclusion
",2018,"hence, as a future work, we would like to compare the usefulness of different linguistic theories and different constraints within each theory in our proposed lm framework."
p18-1146,"
5 Conclusion
",2018,we are hopeful that the backoff-based factorization idea exploited in tfba will be useful in other sparse factorization settings.
p18-1147,"
8 Conclusions
",2018,"first of all, we plan to explore the use of more advanced forms of entity detection and linking, including propagating features from the edl system forward for both unary and binary deep models."
p18-1147,"
8 Conclusions
",2018,"in addition we plan to exploit unary and binary relations as source of evidence to bootstrap a probabilistic reasoning approach, with the goal of leveraging constraints from the kb schema such as domain, range and taxonomies."
p18-1148,"
5 Conclusion and Future work
",2018,"in future work, we would like to use syntactic and discourse structures (e.g., syntactic dependency paths between mentions) to encourage the models to discover a richer set of relations."
p18-1148,"
5 Conclusion and Future work
",2018,"in this way we also hope it will lead to interesting follow-up work, as individual relations can be informed by injecting prior knowledge (e.g., by training jointly with relation extraction models)."
p18-1148,"
5 Conclusion and Future work
",2018,we also would like to combine ment-norm and relnorm.
p18-1151,"
5 Conclusions
",2018,the proposed model maintains the structure of input rdf triples as a small knowledge graph to optimize the amount of information preserved in the input of the model.
p18-1153,"
5 Conclusion and Future Work
",2018,"for future work, we hope to improve the results by using the pun data and design a more proper way to select candidates from associative words."
p18-1153,"
5 Conclusion and Future Work
",2018,"joint model makes use of conditional language model and the joint beam search algorithm, which can assure the assigned senses of target words suitable in one sentence."
p18-1154,"
6 Conclusions
",2018,an interesting point to explore is whether such pragmatically trained game state representations can be leveraged for the task of game commentary generation.
p18-1154,"
6 Conclusions
",2018,generating commentary for such multi-moves is a potential direction for future work.
p18-1155,"
7 Discussion
",2018,"given the numerous potential applications of such an oracle, we believe improving its accuracy will be a promising future direction."
p18-1157,"
7 Conclusion
",2018,"further, we also would like to explore san on other tasks, such as text classification and natural language inference for its generalization in the future."
p18-1161,"
5 Conclusion and Future Work
",2018,"in the future, we will explore the following directions: (1) an additional answer re-ranking step can further improve our model."
p18-1161,"
5 Conclusion and Future Work
",2018,we will incorporate external knowledge bases into our ds-qa model to improve its performance.
p18-1162,"
8 Conclusion and Future Work
",2018,"in future work, we will try to incorporate more hand-crafted features in our model."
p18-1164,"
6 Conclusion
",2018,"additionally, we also intend to apply these methods to other sequence-to-sequence tasks, including natural language conversation."
p18-1164,"
6 Conclusion
",2018,"in future work, we will explore further strategies to bridge the source and target side for sequence-to-sequence and tree-based nmt."
p18-1166,"
6 Conclusion and Future Work
",2018,"in the future, we plan to apply our model on other sequence to sequence learning tasks."
p18-1166,"
6 Conclusion and Future Work
",2018,the model is further enhanced with a masking trick and a dynamic programming method to accelerate the transformer鈥檚 decoder.
p18-1168,"
8 Discussion
",2018,in future work we plan to extend this work and automatically learn such a lexicon.
p18-1170,"
8 Conclusion
",2018,"in future work, we will speed it up through the use of pruning techniques."
p18-1172,"
6 Conclusion
",2018,"in future, we will extend our proposed all-sample learning scheme to deep learning methods, which are more expressive than the shallow embedding model."
p18-1172,"
6 Conclusion
",2018,"lastly, we are interested in exploring the recent adversarial learning techniques to enhance the robustness of word embeddings."
p18-1174,"
6 Conclusion
",2018,"we formalize pool-based active learning as a markov decision process, in which active learning corresponds to the selection decision of the most informative data points from the pool."
p18-1174,"
6 Conclusion
",2018,"we show that the algorithmic expert allows direct policy learning, while at the same time, the learned policies transfer successfully between domains and languages, demonstrating improvement over previous heuristic and reinforcement learning approaches."
p18-1176,"
7 Conclusion
",2018,our attribution-based methods can be directly used to gauge the extent of such problems.
p18-1177,"
7 Conclusion
",2018,it would also be interesting to apply our approach to incorporating coreference knowledge to other text generation tasks.
p18-1184,"
6 Conclusions and Future Work
",2018,"in our future work, we plan to integrate other types of information such as user properties into the structured neural models to further enhance representation learning and detect rumor spreaders at the same time."
p18-1184,"
6 Conclusions and Future Work
",2018,we also plan to use unsupervised models for the task by exploiting structural information.
p18-1185,"
6 Conclusions and Future Work
",2018,we hope this work will encourage more research on multimodal social media in the future and we plan on making our benchmark available upon request.
p18-1185,"
6 Conclusions and Future Work
",2018,we plan to expand our model to tasks such as fine-grained name tagging or entity liking in the future.
p18-1187,"
6 Conclusion
",2018,"in future work, we are interested in modelling the extent to which a social interaction is caused by geographical proximity (e.g.using user鈥搖ser gates)."
p18-1193,"
9 Discussion
",2018,"future modeling work may include using intermediate world states from previous turns in the interaction, which is required for some of the most complex references in the data."
p18-1193,"
9 Discussion
",2018,"one possible direction for future work is to use an estimator to predict rewards for all actions, rather than observing them."
p18-1196,"
7 Conclusion
",2018,"our approaches in modelling and evaluation can be used in future work in tasks such as approximate information extraction, knowledge base completion, numerical fact checking, numerical question answering, and fraud detection."
p18-1198,"
6 Conclusion
",2018,"in future work, we would like to extend the probing tasks to other languages (which should be relatively easy, given that they are automatically generated), investigate how multi-task training affects probing task performance and leverage our probing tasks to find more linguistically-aware universal encoders."
p18-1201,"
8 Conclusions and Future Work
",2018,"in the future, we will extend this framework to other information extraction problems."
p18-1203,"
4 Conclusion
",2018,one interesting topic for future research is exploration in planning.
p18-1203,"
4 Conclusion
",2018,"to this end, we want the agent to explore in the environment, but not so much that the performance would be greatly degraded."
p18-1205,"
6 Conclusion & Discussion
",2018,we believe persona-chat will be a useful resource for training components of future dialogue systems.
p18-1205,"
6 Conclusion & Discussion
",2018,"we hope that the data will aid training agents that can ask questions about users鈥 profiles, remember the answers, and use them naturally in conversation."
p18-1206,"
6 Conclusions
",2018,"in future work, we plan to incorporate various types of context (e.g.anaphora, device-specific capabilities) and dialogue history into a large-scale nlu system."
p18-1209,"
6 Conclusion
",2018,"future work on similar topics could explore the applications of using low-rank tensors for attention models over tensor representations, as they can be even more memory and computationally intensive."
p18-1211,"
5 Conclusion
",2018,"more generally, similar approaches can explore a wider range of scenarios involving sequences of text."
p18-1212,"
6 Conclusion
",2018,"we hope that this notable improvement can foster more interest in jointly studying multiple aspects of events (e.g., event sequencing, coreference, parent-child relations) towards the goal of understanding events in natural language."
p18-1214,"
5 Conclusion
",2018,"in the future, we plan to enrich the architecture of dazer to allow few-shot document filtering by incorporating several labeled examples."
p18-1217,"
5 Conclusion
",2018,"for future work, we are interested in various extensions, including combining stc with autoencoding variational bayes (avb)."
p18-1219,"
7 Conclusion and Future Work
",2018,"in future, we plan to use use approaches, like multi-task learning (mishra et al., 2018), in estimating gaze features and using those estimated features for text quality prediction."
p18-1223,"
7 Conclusions
",2018,"though mainly fouced on search, we hope our findings shed some lights on a potential path towards more intelligent neural systems and will motivate more explorations in this direction."
p18-1224,"
6 Conclusions
",2018,"the model is equipped with external knowledge in its main components, specifically, in calculating coattention, collecting local inference, and composing inference."
p18-1226,"
6 Conclusion and Discussions
",2018,"generally, informal korean text contains intentional typos (鈥橂鞛囯嫟鈥榙elicious鈥 with typo鈥), stand-alone jamo as a character, (鈥樸厠銋媗ol鈥) and segmentation errors.(鈥橁皺 鞚搓皜雼も榞o together鈥 without space鈥)."
p18-1226,"
6 Conclusion and Discussions
",2018,"prior to evaluating the performance of the vectors, we developed test set for word similarity and word analogy tasks for korean."
p18-1226,"
6 Conclusion and Discussions
",2018,"since korean words are divisible once more into grapheme level, resulting in longer sequence of jamos for a given word, we plan to explore potential applicability of deeper level of subword information in korean."
p18-1226,"
6 Conclusion and Discussions
",2018,"since these errors occur frequently, it is important to apply the vectors in training nlp models over real-word data."
p18-1226,"
6 Conclusion and Discussions
",2018,"we plan to apply these vectors for various neural network based nlp models, such as conversation modeling."
p18-1227,"
6 Conclusion and Future Work
",2018,"in the future, we will explore methods of exploiting internal information in other languages.(4) we believe that sememes are universal for all human languages."
p18-1227,"
6 Conclusion and Future Work
",2018,"in the future, we will take structured annotations into account.(2) it would be meaningful to take more information into account for blending external and internal information and design more sophisticated methods.(3) besides chinese, many other languages have rich subword-level information."
p18-1227,"
6 Conclusion and Future Work
",2018,we evaluated our csp framework on the classical manually annotated sememe kb hownet.
p18-1227,"
6 Conclusion and Future Work
",2018,we proposed a character-enhanced sememe prediction (csp) framework which integrates both internal and external information for lexical sememe prediction and proposed two methods for utilizing internal information.
p18-1227,"
6 Conclusion and Future Work
",2018,we will explore a general framework to recommend and utilize sememes for other nlp tasks.
p18-1227,"
6 Conclusion and Future Work
",2018,"we will explore the following research directions in the future: (1) concepts in hownet are annotated with hierarchical structures of senses and sememes, but those are not considered in this paper."
p18-1228,"
6 Discussion and Conclusion
",2018,our study may facilitate further investigations on context-dependent text analysis techniques and applications.
p18-1228,"
6 Discussion and Conclusion
",2018,we hope that semaxis can facilitate research on other semantic axes so that we will have labeled datasets for other axes as well.
p18-1229,"
7 Conclusion and Future Work
",2018,"in the future, we will explore more strategies towards term pair selection (e.g., allow the rl agent to remove terms from the taxonomy) and reward function design."
p18-1230,"
5 Conclusions and Future Work
",2018,"in the next step, we will consider integrating the rich structural information into the neural network for word sense disambiguation."
p18-1230,"
5 Conclusions and Future Work
",2018,there is still one challenge left for the future.
p18-1231,"
8 Conclusion
",2018,"in the future, we will extend our model so that it can project multi-word phrases, as well as single words, which could help with negations and modifiers."
p18-1234,"
8 Conclusions and Future Work
",2018,how to leverage large-scale sentiment lexicons in neural networks would be our future work.
p18-1237,"
5 Discussion and Conclusion
",2018,"also, our model helps analyzing the influence of user interaction and behavior on the effectiveness of discussion decisions."
p18-1237,"
5 Discussion and Conclusion
",2018,"in future work, we plan to study how to distinguish effective from ineffective discussions based on our model as well as how to learn from the strategies used in successful discussions, in order to predict the best next deliberative move in an ongoing discussion."
p18-1237,"
5 Discussion and Conclusion
",2018,"many categories in our model will apply to deliberative discussions in general, particularly the discourse acts and argumentative relations."
p18-1237,"
5 Discussion and Conclusion
",2018,"we expect the general derivation steps to be the same, whereas the techniques applied within each step may differ depending on the types, frequency, and quality of metadata."
p18-1241,"
5 Conclusion
",2018,our method stands out from the well-studied adversarial learning on image classifiers and cnn models.
p18-1243,"
6 Discussion
",2018,"in the current work, we have designed and used a computer game (synthetic task with synthetic language) for training the agent."
p18-1243,"
6 Discussion
",2018,we plan to investigate the generalization and application of the proposed approach to more realistic environments with more diverse tasks in future work.
p18-1244,"
6 Conclusions
",2018,"future work includes data collection and evaluation in a real world scenario since the data used in our experiments are simulated mixed speech, which is already extremely challenging but still leaves some acoustic aspects, such as lombard effects and real room impulse responses, that need to be alleviated for further performance improvement."
p18-1245,"
8 Conclusion
",2018,"as the model鈥檚 rich parameterization prevents tractable inference, we craft a variational inference procedure, based on the wake-sleep algorithm, to marginalize out the latent variables."
p18-1247,"
7 Conclusion and Future Work
",2018,"due to the robustness of the model across languages, we believe it can also be scaled to perform morphological tagging for multiple languages together."
p18-1250,"
6 Conclusion
",2018,it should be a well-justified solution to identify empty categories as well as to integrate empty categories into syntactic analysis.
p18-1251,"
5 Future Work
",2018,"for future work, other potential bottlenecks could be addressed."
p18-1252,"
6 Conclusions and Future Work
",2018,"in future, we would like to advance this work in two directions: 1) proposing more effective conversion approaches, especially by exploring the potential of treelstms; 2) constructing bi-tree aligned data for other treebanks and exploiting all available single-tree and bi-tree labeled data for better conversion."
p18-1253,"
8 Conclusion
",2018,"oonp is neural netbased, but equipped with sophisticated architecture and mechanism for document understanding, therefore nicely combining interpretability and learnability."
p18-1255,"
7 Conclusion
",2018,"in order to move to a full system that can help users like terry write better posts, there are three interesting lines of future work."
p18-1255,"
7 Conclusion
",2018,"such pragmatic principles have recently been shown to be useful in other tasks as well (golland et al., 2010; smith et al., 2013; orita et al., 2015; andreas and klein, 2016)."
p18-1256,"
8 Conclusion
",2018,"in future work, we would like to focus more on designing models that can deal with and be optimized for scenarios with severe data imbalance."
p18-1256,"
8 Conclusion
",2018,"we would like to also explore various applications of presupposition trigger prediction in language generation applications, as well as additional attention-based neural network architectures."
P19-1001,"
7 Conclusions and Future Work
",2019,"in the future, we plan to integrate our ioi model with models like elmo (peters et al., 2018) and bert (devlin et al., 2018) to study if the performance of ioi can be further improved."
P19-1002,"
5 Conclusion and Future Work

",2019,"in the future, we plan to apply reinforcement learning to further improve the performance."
P19-1003,"
6 Conclusion
",2019,we hope the collected dataset and proposed model can benefit future related research.
P19-1006,"
4 Conclusion and Future Work
",2019,"in the future, we would like to explore the effectiveness of various attention methods to solve indefinite choices task with interpretive features."
P19-1007,"
6 Conclusions and Future Work
",2019,"in the future, we want to incorporate this framework with much refined primal and dual models, and design more informative reward signals to make the training more efficient."
P19-1009,"
8 Conclusion
",2019,"for future work, we would like to extend our model to other semantic parsing tasks (oepen et al., 2014; abend and rappoport, 2013)."
P19-1010,"
6 Conclusions
",2019,"for future direction, we are interested in exploring constrained decoding, better incorporating pre-trained language representations within our architecture, conditioning on additional relations between entities, and different gnn formulations."
P19-1012,"
7 Discussion and Conclusion
",2019,"evidently, the employment of bilstm feature extractors blurs the difference between the two architectures."
P19-1012,"
7 Discussion and Conclusion
",2019,we leave this question for future work.
P19-1014,"
8 Conclusions
",2019,"in future work, we’d like to improve this integration in order to gain from training on examples from different domains for tags like ‘name’ and ‘location’."
P19-1016,"
5 Conclusions and Future Work
",2019,"our future work includes integrating advanced word representation methods (e.g., elmo and bert) and extending the proposed model to other tasks, such as event extraction and co-reference resolution."
P19-1016,"
5 Conclusions and Future Work
",2019,we also plan to incorporate external knowledge and common sense as additional signals into our architecture as they are important for human readers to recognize names but still absent from the current model.
P19-1018,"
6 Conclusions
",2019,"on analyzing the model errors, we find that a large fraction of them arise due to polysemy and antonymy (an interested reader can find the details in appendix (§a.2)."
P19-1018,"
6 Conclusions
",2019,"we also find that translating in a common embedding space, as opposed to the target embedding space, obtains orthogonal gains for bli, and plan on investigating this in the semi-supervised setting in future work."
P19-1019,"
6 Conclusions and future work
",2019,"finally, we would like to adapt our approach to more relaxed scenarios with multiple languages and/or small parallel corpora."
P19-1019,"
6 Conclusions and future work
",2019,"in addition to that, we would like to incorporate a language modeling loss during nmt training similar to he et al.(2016)."
P19-1019,"
6 Conclusions and future work
",2019,"in the future, we would like to explore learnable similarity functions like the one proposed by (mccallum et al., 2005) to compute the characterlevel scores in our initial phrase-table."
P19-1021,"
6 Conclusions
",2019,"this has practical relevance for languages where large amounts of monolingual data, or multilingual data involving related languages, are not available."
P19-1023,"
5 Conclusions
",2019,"in the future, we plan to explore contextbased similarity to complement the lexical similarity to improve the overall performance."
P19-1024,"
5 Conclusion
",2019,there are multiple venues for future work.
P19-1027,"
4 Limitations and Conclusions
",2019,"our evaluation did not include other subword representations, most notably elmo (peters et al., 2018) and contextual string embeddings (akbik et al., 2018), since, even though they are languageagnostic in principle, pretrained models are only available in a few languages.conclusions."
P19-1032,"
4 Conclusion
",2019,"this mechanism allows for models with longer context, and thus with the capability to catch longer dependencies."
P19-1033,"
6 Conclusion
",2019,"besides, we propose two methods to fuse long- and short-term user representations, i.e., using long-term user representation to initialize the hidden state of the gru network in short-term user representation, or concatenating both longand short-term user representations as a unified user vector."
P19-1034,"
6 Conclusion
",2019,another interesting topic for future research is the comparison of domain adaptation based on our domain-specific word embeddings vs. based on word embeddings trained on much larger corpora.
P19-1034,"
6 Conclusion
",2019,"in the future, we plan to investigate whether there are changes over time that significantly impact the linguistic characteristics of the data, in the simplest case changes in the meaning of a word."
P19-1035,"
8 Conclusion
",2019,future manipulation strategies that take the competencies into account have the potential to train particular skills and to better control the competencies required for a placement test.
P19-1035,"
8 Conclusion
",2019,our error analysis points out important directions for future work on detecting ambiguous gaps and modeling gap interdependencies for c-tests deviating from the default generation scheme.
P19-1036,"
6 Conclusion
",2019,this is certainly an avenue that we seek to explore.
P19-1037,"
9 Conclusion + Future Work
",2019,"currently, we are only using frequency as an indicator of lexical complexity, however other factors such as word length, etymology, etc.may be used."
P19-1037,"
9 Conclusion + Future Work
",2019,doctors and patients speak a different language and we hope that our work will help them communicate.
P19-1037,"
9 Conclusion + Future Work
",2019,"finally, we will explore adaptations of our methodology for general (non-medical) domains, e.g., simplified search interfaces (ananiadou et al., 2013) for semantically annotated news (thompson et al., 2017)."
P19-1037,"
9 Conclusion + Future Work
",2019,one clear avenue of future work is to apply this system in a clinical setting and to test the results with actual patients.
P19-1040,5 Conclusions and Future Work,2019,we leave this for future work.
P19-1043,"
7 Conclusions and Future Work
",2019,"in the future, we would like to generalize it to multiple domains and datasets."
P19-1044,"
7 Conclusions and future work
",2019,"in the future, we plan to evaluate temporal referencing against the related dynamic embedding models on an annotated empirical lexical change dataset with multiple languages."
P19-1044,"
7 Conclusions and future work
",2019,"we also plan on testing how well temporal referencing deals with corpora that are too small for alignment-based methods, hopefully opening new avenues of quantitative research."
P19-1045,"
5 Conclusion
",2019,aan takes the advantages from both adversarial learning and attention mechanism by conducting adversarial learning between two attention layers in order to learn better weighted information in a given text.
P19-1045,"
5 Conclusion
",2019,"in our future work, we would like to improve the model structure and the adversarial learning algorithm."
P19-1045,"
5 Conclusion
",2019,"last but not least, we would like to apply our approach to other heterogeneous texts-concerned nlp tasks."
P19-1045,"
5 Conclusion
",2019,"moreover, we would like to seek a stable and controllable way to conduct adversarial learning among more than two objects."
P19-1046,"
5 Conclusion
",2019,"in future work, we intend to explore multiple local fusion methods within our framework."
P19-1047,"
6 Conclusions and future work
",2019,"promising directions for future research include examining additional features and feature representations: pragmatic features such as formality (pavlick and tetreault, 2016) or politeness (danescu-niculescu-mizil et al., 2013); acousticprosodic features from earnings call audio; more sophisticated semantic representations such as claims (lim et al., 2016), automatically induced entity-relation graphs (bansal et al., 2017) or question-answer motifs (zhang et al., 2017) (these representations are non-trivial to construct because a single turn may contain many questions or answers); or even discourse structures."
P19-1047,"
6 Conclusions and future work
",2019,"the models used in this work aim to be just complex enough to determine whether useful signals exist for this task; future modeling work could include training a complete end-to-end system such as a hierarchical attention network (yang et al., 2016), or building industry-specific models."
P19-1049,"
5 Conclusion
",2019,it would also be nice to explore about more fine-grained functional components and grammatical entities in the future works.
P19-1050,"
7 Conclusion
",2019,"building upon this dataset, future research can explore the design of efficient multimodal fusion algorithms, novel erc frameworks, as well as the extraction of new features from the audio, visual, and textual modalities."
P19-1050,"
7 Conclusion
",2019,we believe this dataset will also be useful as a training corpus for both conversational emotion recognition and multimodal empathetic response generation.
P19-1053,"
6 Conclusion and Future Work
",2019,"thus, we plan to extend our approach to other neural nlp tasks with attention mechanisms, such as neural document classification (yang et al., 2016) and neural machine translation (zhang et al., 2018)."
P19-1054,"
6 Conclusion
",2019,future work should thus study the overlapping nature of argument clustering.
P19-1055,"
7 Conclusions
",2019,"the modular approach also provides interesting directions for future research, focusing on alleviating the supervision bottleneck by using large amount of partially labeled data that are cheaper and easy to obtain, together with only a handful amount of annotated data, a scenario especially suitable for low-resource languages."
P19-1058,"
6 Conclusion
",2019,"in the future work, we will focus on how to mine different representations for different discourse relation types and apply the topic information to other languages."
P19-1059,"
5 Conclusion
",2019,future work should provide further exploration of the data regime in which pragmatic learning is most beneficial and its correspondence to realworld language use.
P19-1060,"
6 Conclusion
",2019,"as part of future work, we would like to investigate the use of contextualized embeddings (e.g., bert, devlin et al.(2018)) for coherence assessment – as such representations have been shown to carry syntactic information of words (tenney et al., 2019) – and whether they allow multi-task learning frameworks to learn complementary aspects of language."
P19-1061,"
6 Conclusions and Future Work
",2019,"in future work, we will enrich our weak supervision system by giving the lfs access to more sophisticated contexts that take into account global structuring constraints in order to see how they compare to exogenous decoding constraints applied in (muller et al., 2012; perret et al., 2016)."
P19-1063,"
6 Conclusion
",2019,"while some aspects of the experimental setting are, admittedly, simplified (e.g.compilation of an artificial test set, uncertainty estimation), we believe that this is an encouraging result for scaling models in computational pragmatics to realworld conversation and its complexities."
P19-1064,"
5 Conclusion
",2019,our model transforms the supervised higher order coreference model to a policy gradient model that can directly optimizes coreference evaluation metrics.
P19-1064,"
5 Conclusion
",2019,"there are several potential improvements to our model as future work, such as incorporating mention detection result as a part of the reward."
P19-1065,"
6 Conclusion and Future Work
",2019,"since implicit discourse relation identification is a key task for dialogue systems, there are still many approaches worth investigating in future work."
P19-1066,"
8 Conclusion
",2019,in the future we plan to further enrich these representations by considering information from across the document.
P19-1068,"
7 Conclusion
",2019,"in future work, we aim to determine if the same level of accuracy can be obtained when single sentences will be used as samples for training and testing."
P19-1068,"
7 Conclusion
",2019,we leave these ideas for future exploration.
P19-1068,"
7 Conclusion
",2019,we would like to stress out that the methods presented in this paper are only provided as baselines in order to enable comparisons in future work.
P19-1069,"
6 Conclusions
",2019,"as future work we plan to exploit a subset of the wikipedia categories as coarse-grained sense inventory and enrich our dataset with coarser labels, hence enabling wsd at different granularities."
P19-1070,"
5 Conclusion
",2019,"by systematically evaluating cle models for many language pairs on bli and three downstream tasks, we shed new light on the ability of current cutting-edge cle models to support cross-lingual nlp."
P19-1070,"
5 Conclusion
",2019,we hope that this study will encourage future work on cle evaluation and analysis and help guide the development of new cle models.
P19-1075,"
6 Conclusion
",2019,all these findings indicate that the corpus may be a proper benchmark for chinese cloze test and worth further research.
P19-1076,"
7 Conclusion
",2019,"we propose that this new metric, which we colloquially refer to as consistency, be adopted alongside evaluations of global topic quality for future work with topic model comparison."
P19-1077,"
9 Discussion and Conclusions
",2019,"about 5,000 sentences were annotated by regular (i.e., not paid) players in this initial development phase, but we expect the game will be able to collect a comparable amount of judgments as for phrase detectives once it’s fully operational and properly advertised."
P19-1078,"
7 Conclusion
",2019,"in future work, transferring knowledge from other resources can be applied to further improve zero-shot performance, and collecting a dataset with a large number of domains is able to facilitate the application and study of metalearning techniques within multi-domain dst."
P19-1079,"
7 Conclusions
",2019,we further explored learning these features in parallel and serial mtl architectures.
P19-1085,"
6 Conclusion
",2019,"in the future, we would like to design a multi-step evidence extractor and incorporate external knowledge into our framework."
P19-1086,"
8 Conclusion
",2019,the distributional similarity of both positive and negative examples makes sherliic a promising benchmark to track future nli models’ ability to go beyond shallow semantics relying primarily on distributional evidence.
P19-1086,"
8 Conclusion
",2019,we hope that sherliic will foster better modeling of lexical inference in context as well as progress in nli in general.
P19-1087,"
7 Conclusions
",2019,we plan to investigate the impact of combining the two models.
P19-1088,"
8 Conclusions
",2019,"in the future, we plan to build upon the acquired knowledge and the developed classifiers to generate systems able to provide actionable feedback on how to achieve high-quality counseling."
P19-1089,"
7 Discussion and Future Work
",2019,"for instance, increased diversification may point to flexibility and specialization that are beneficial, but might also signal detrimental deviations from good counseling practices."
P19-1089,"
7 Discussion and Future Work
",2019,future work could adapt our framework to examine more complex forms of conversational development.
P19-1089,"
7 Discussion and Future Work
",2019,"future work could more directly model how counselors respond to texter behaviors, hence gauging the extent to which counselors evolve in their interactional practices."
P19-1089,"
7 Discussion and Future Work
",2019,"our methodology could also be extended to examine other conversational contexts such as academic advising or business interactions, where individuals are expected to learn from experience."
P19-1090,"
6 Conclusion and Future Work
",2019,"a next step would be comprehensive error analysis for a better understanding of where the models succeed or fail in capturing semantic information, particularly for the low-resource languages."
P19-1090,"
6 Conclusion and Future Work
",2019,"such a model can serve in a semiautomated answer selection process, with a human in the loop to choose the final answer."
P19-1090,"
6 Conclusion and Future Work
",2019,"this feedback can help improve the automated response service, and assist future research tasks."
P19-1090,"
6 Conclusion and Future Work
",2019,"we further intend to explore transfer learning techniques (zhang et al., 2017) as well as deep architectures designed specifically for answer selection (lai et al., 2018)."
P19-1091,"
5 Conclusion
",2019,"for future work, we plan to investigate the model on other related tasks such as relation extraction, normalization as well as the use of advanced conditional models."
P19-1093,"
7 Discussion and future work

",2019,"in the future we aim to test and adapt other improvements in the learning to rank field to our task, hoping for further improvement by those models (burges, 2010; severyn and moschitti, 2015)."
P19-1094,"
8 Conclusion
",2019,we plan to pursue these research directions in future work.
P19-1096,"
6 Conclusions and Future Work
",2019,"in the future work, we will try to build a one-step model that directly extract the emotion-cause pairs in an end-to-end fashion."
P19-1101,"
4 Conclusion and Future Work
",2019,a summarizer intends to extract or generate a summary maximizing θi .
P19-1101,"
4 Conclusion and Future Work
",2019,"by aggregating over different people but in one domain, one can uncover a domain-specific k. similarly, by aggregating over many topics for one person, one would find a personalized k. these consistute promising research directions for future works."
P19-1101,"
4 Conclusion and Future Work
",2019,"then, interesting research questions arise like which granularity offers a good approximation of semantic units?"
P19-1102,"
8 Conclusion
",2019,in the future we plan to explore interactions among documents beyond concatenation and experiment with summarizing longer input documents.
P19-1103,"
6 Conclusion
",2019,"in the future, we would like to evaluate the attacking effectiveness and efficiency of our methods on more datasets and models, and do elaborate human evaluation on the similarity between clean texts and the corresponding adversarial examples."
P19-1105,"
5 Conclusion
",2019,"future work will include: (i) incorporating lexical semantics such as named entities for further improvement, (ii) comparing our model to other deep contextualized word representation such as elmo and bert, and (iii) applying the method to other domains for quantitative evaluation."
P19-1106,"
7 Conclusion
",2019,we aim to include review reliability prediction in the pipeline of our future work.
P19-1106,"
7 Conclusion
",2019,we intend to work upon those and also explore more sophisticated techniques for sentiment polarity encoding.
P19-1108,"
8 Conclusions
",2019,"other ways of combining the two modules, more sophisticated classifiers for both phm detection and figurative usage detection, are possible directions of future work."
P19-1109,"
5 Conclusions
",2019,"finally, we plan to investigate alternative methods to modelling phrase and multi-word expression complexity."
P19-1109,"
5 Conclusions
",2019,our future research will focus on the relative nature of complexity judgements and will use the seq model to predict complexity on a scale.
P19-1109,"
5 Conclusions
",2019,we will also investigate whether the seq model may benefit from sources of information other than word embeddings and character-level morphology.
P19-1111,"
4 Conclusion
",2019,"from a pedagogical perspective, it will be beneficial for learners of the language to look into the prose of the verses for an easier comprehension of the concepts discussed in the verse."
P19-1112,"
7 Conclusion
",2019,"as future work, we plan to investigate emphasis selection on a larger and more diverse dataset."
P19-1112,"
7 Conclusion
",2019,"we also plan to investigate the role of word sentiment and emotion intensity as well as more advanced language models such as bert (devlin et al., 2018) in modeling emphasis."
P19-1114,"
6 Conclusions and Future Work

",2019,"given that, in our future work, we will be investigating those false positive cases with our collaborators to assess what the correct label for these ads should be."
P19-1114,"
6 Conclusions and Future Work

",2019,"moreover, since the proposed full feature set involves hundreds of features we plan to increase our sample size to have a better estimation of the performance of our final predictor."
P19-1115,"
7 Conclusion
",2019,promising future work includes extension to tree-structured inputs and application to other tasks.
P19-1120,"
7 Conclusion
",2019,"5 as for future work, we will test our methods in the nmt transfer where the target language is switched."
P19-1120,"
7 Conclusion
",2019,"we also plan to compare different algorithms for learning the cross-lingual mapping (artetxe et al., 2018a; xu et al., 2018; joulin et al., 2018) to optimize the transfer performance."
P19-1122,"
7 Conclusions & Future Work
",2019,"finally, we hope that future work in this area will follow our lead in using carefully-controlled experiments to enable meaningful comparisons."
P19-1122,"
7 Conclusions & Future Work
",2019,"while our method is currently restricted to languages that have reliable constituency parsers, an exciting future direction is to explore unsupervised tree induction methods for low-resource target languages (drozdov et al., 2019)."
P19-1123,"
6 Conclusion
",2019,"in future, we would like to extend the method to handle more than two curricula objectives."
P19-1124,"
6 Conclusions and Future Work
",2019,"in the future, we believe more work on improving cfs alignment is potential to improve translation quality, and we will investigate on using source context and target history context in a more robust manner for better predicting cfs and cft words."
P19-1125,"
7 Conclusion
",2019,another direction is to apply the proposed imitation learning framework to similar scenarios such as simultaneous interpretation.
P19-1125,"
7 Conclusion
",2019,"as a future work, we can try to improve the performance of the nmt by introducing more powerful demonstrator with different structure (e.g.right to left)."
P19-1129,"
7 Conclusion
",2019,we also construct a new entity-relation extraction dataset that requires hierarchical relation reasoning and the proposed model achieves the best performance.
P19-1132,"
5 Conclusion
",2019,"in the future work, we will explore the usage of this method with other applications."
P19-1133,"
5 Conclusion
",2019,"future work will investigate more complex and recent neural network models such as devlin et al.(2018), as well as alternative losses."
P19-1133,"
5 Conclusion
",2019,we demonstrated the effectiveness of our reldist losses on three datasets and showcased its effect on cluster purity.
P19-1134,"
7 Conclusion
",2019,"in future work, we want to further investigate the extent of syntactic structure captured in deep language language representations."
P19-1134,"
7 Conclusion
",2019,"this allows for an increased domain and language independence, and an additional error reduction because pre-processing can be omitted."
P19-1135,"
6 Conclusion
",2019,"in the future, we hope to improve our work by the utilization of better model-based pattern extractor, and resorting to latent variable model (kim et al., 2018) for jointly modeling instance selector."
P19-1135,"
6 Conclusion
",2019,"what is more, we also hope to verify the effectiveness of our method on more tasks, including open information extraction and event extraction, and also overlapping relation extraction models (dai et al., 2019)."
P19-1135,"
6 Conclusion
",2019,"with a more interpretable model, we then conduct noise reduction by evaluating how well the model explains the relation of an instance."
P19-1137,"
5 Conclusion and Future Work
",2019,"for the future work, we plan to extend diagnre to other ds-based applications, such as question answering (lin et al., 2018), event extraction (chen et al., 2017), etc."
P19-1138,"
5 Conclusions

",2019,mgner is framework with high modularity and each component in mgner can adopt a wide range of neural networks.
P19-1139,"
5 Conclusion
",2019,"there are three important directions remain for future research: (1) inject knowledge into feature-based pre-training models such as elmo (peters et al., 2018); (2) introduce diverse structured knowledge into language representation models such as conceptnet (speer and havasi, 2012) which is different from the world knowledge database wikidata; (3) annotate more real-world corpora heuristically for building larger pre-training data."
P19-1140,"
7 Conclusions
",2019,"in future, we are interested in introducing text information of entities for alignment by considering word ambiguity (cao et al., 2017b); and meanwhile, through cross-kg entity proximity (cao et al., 2015)."
P19-1141,"
4 Conclusion and Future Work
",2019,"although we specifically investigated the ner task for chinese in this work, we believe the proposed model can be extended and applied to other languages, for which we leave as future work."
P19-1142,"
8 Conclusion
",2019,"also, it will be worthwhile to ascertain the efficacy of pdr for language models using transformers and in combination with frage embeddings."
P19-1142,"
8 Conclusion
",2019,future work includes exploring the application of pdr to other seq2seq models that have a similar input-output symmetry.
P19-1142,"
8 Conclusion
",2019,we propose a new past decode regularization (pdr) method for language modeling that exploits the input-output symmetry in each step to decode the last token in the context from the predicted next token distribution.
P19-1143,"
4 Conclusion
",2019,"we introduce a simple technique to do so, allowing to apply dynamic programming for learning and inference."
P19-1144,"
7 Conclusion
",2019,"in future work, we aim to capture better structural information and possible connections to unsupervised grammar induction."
P19-1146,"
6 Conclusion and Future Work
",2019,"a natural next step is to apply entmax to self-attention (vaswani et al.,2017)."
P19-1147,"
9 Conclusions
",2019,future work includes developing a adversarial training scheme as well as devising a more robust architecture based on our findings.
P19-1149,"
7 Conclusion and Future Work
",2019,"in the future, we are interested in testing lowlevel optimizations of lrn, which are orthogonal to this work, such as dedicated cudnn kernels."
P19-1150,"
5 Conclusion
",2019,"in the future, we plan to apply capsule networks to even more challenging nlp problems such as language modeling and text generation."
P19-1153,"
4 Conclusion & Future Work
",2019,"finally, we would like to learn our sentence embeddings’ latent space, similarly to subramanian et al.(2018)’s method, so as to leverage our autoencoder’s strong reconstruction ability and generate very long sequences of text."
P19-1158,"
7 Conclusion
",2019,we expect our model contributes to improved performance of other complex state-of-the-art encoding architectures for text classification.
P19-1159,"
5 Conclusion and Future Directions
",2019,"below, we identify a few future directions."
P19-1159,"
5 Conclusion and Future Directions
",2019,future work can look to apply existing methods or devise new techniques towards mitigating gender bias in other languages as well.
P19-1159,"
5 Conclusion and Future Directions
",2019,"non-binary genders (richards et al., 2016) as well as racial biases have largely been ignored in nlp and should be considered in future work."
P19-1160,"
5 Conclusion

",2019,"in future, we plan to extend the proposed method to debias other types of demographic biases such as ethnic, age or religious biases."
P19-1161,"
7 Conclusion

",2019,"finally, we also identified avenues for future work, such as the inclusion of co-reference information."
P19-1161,"
7 Conclusion

",2019,"to the best of our knowledge, this task has not been studied previously."
P19-1165,"
6 Conclusions
",2019,trying more complex networks is also within our scope and is left as future work.
P19-1167,"
5 Conclusion and Limitations
",2019,"in future work, we intend to conduct a diachronic analysis in english using the same corpus, in addition to a cross-linguistic study of gendered language."
P19-1168,"
5 Conclusion
",2019,it may be of interest to extend these techniques to embed knowledge graph elements.
P19-1169,"
5 Conclusion and Future Work

",2019,"in the future, we will improve our model to deal with datasets containing a relatively large number of lexical relation types and random term pairs."
P19-1170,"
5 Conclusion
",2019,"finally, the preliminary results we have shown on aligning more than two languages at the same time provide an exciting path for future research."
P19-1171,"
6 Conclusion
",2019,"an avenue for future work is connecting our discovered phonesthemes to putative meanings, as done by abramova et al.(2013) and abramova and fernandez ′ (2016)."
P19-1173,"
8 Conclusions and Future Work
",2019,"future work includes joint and cross-dialectal lemmatization models, in addition to further extension to other dialects."
P19-1174,"
7 Conclusion and Future Work
",2019,"in future work, we will further explore the effectiveness of the reordering mechanism and apply it to other natural language processing tasks, such dependency parsing (zhang et al., 2016; li et al., 2018), and semantic role labeling (he et al., 2018; li et al., 2019)."
P19-1175,"
7 Conclusion
",2019,"in a next step, we plan to compare the performance of nfr to other approaches to tm-nmt integration, for example by carrying out evaluations on the jrc-acquis corpus (gu et al., 2018; koehn and senellart, 2010a; zhang et al., 2018)."
P19-1175,"
7 Conclusion
",2019,"we also intend to carry out further tests to potentially improve the quality of the output, for example by testing different match metrics and retrieval methods, nmt architectures (e.g.transformer), ways to include alignment information and by applying additional morphological preprocessing."
P19-1178,"
5 Conclusions and Future Work
",2019,"as future work, we will apply our methodology to domain adaptation."
P19-1179,"
6 Conclusion
",2019,"our method does not introduce additional parameters: we hope to motivate future work on learning speech representations, with continued performance on lowerresource settings if additional parameters are introduced."
P19-1180,"
5 Discussion
",2019,"finally, it may be possible to extend our approach to other linguistic tasks such as dependency parsing (christie et al., 2016b), coreference resolution (kottur et al., 2018), and learning pragmatics beyond semantics (andreas and klein, 2016)."
P19-1180,"
5 Discussion
",2019,the results suggest multiple future research directions.
P19-1181,"
6 Conclusion
",2019,"for example, future extensions of vln will likely involve games (baldridge et al., 2018) where the instructions being given take the agent around a trap or help it avoid opponents."
P19-1181,"
6 Conclusion
",2019,"furthermore, our findings suggests ways that future datasets and metrics for judging agents should be constructed and set up for evaluation."
P19-1181,"
6 Conclusion
",2019,future agents will need to make effective use of language and its connection to the environment to both drive cls up and bring ne down in r4r.
P19-1181,"
6 Conclusion
",2019,"we expect path fidelity to not only be interesting with respect to grounding language, but to be crucial for many vln-based problems."
P19-1182,"
6 Conclusion
",2019,"for future work, we are going to further explore the possibility to merge the three datasets by either learning a joint image representation or by transferring domain-specific knowledge."
P19-1184,"
8 Conclusion
",2019,"in future work, the data can be used to further investigate common ground and conceptual pacts; be extended through manual annotations for a more thorough linguistic analysis of co-reference chains; exploit the combination of vision and language to develop computational models for referring expression generation; or use the photobook task in the parlai framework for turing-test-like evaluation of dialogue agents."
P19-1187,"
6 Conclusions
",2019,in the future work we would like to consider setups where human-annotated data is combined with naturally occurring one (i.e.distantly-supervised one).
P19-1188,"
9 Conclusion
",2019,"as a result, much of ai and nlp community has focused on making larger and larger datasets, but we believe it is equally important to go the other direction and explore methods that help performance with little data."
P19-1190,"
6 Conclusion
",2019,"as future work, we will consider integrating more kinds of syntactic features from linguistic analysis such as dependency parsing."
P19-1191,"
5 Conclusions and Future Work
",2019,"in the future, we plan to develop techniques for extracting entities of more fine-grained entity types, and extend paperrobot to write related work, predict authors, their affiliations and publication venues."
P19-1192,"
5 Conclusion and Future work
",2019,"in the future, we will investigate the possibility of incorporating additional forms of rhetoric, such as parallelism and exaggeration, to further enhance the model and generate more diverse poems."
P19-1195,"
8 Conclusions
",2019,"however, we have only scratched the surface; future improvements involve integrating content planning with entity modeling, placing more emphasis on play-by-play, and exploiting dependencies across input tables."
P19-1196,"
5 Conclusion and Future Work
",2019,"aside from such syntax templates, in the future, we aim to explore how semantic templates contribute to type description generation."
P19-1198,"
7 Conclusion
",2019,"in future, we would like to improve the system further by incorporating better architectural designs and training schemes to tackle complex simplification operations."
P19-1201,"
6 Conclusion
",2019,"in the future, we will further apply dim to learn semantic parser and nl generator from the noisy datasets."
P19-1203,"
6 Conclusion
",2019,"in the future, we would like to explore how to better select the rationale for each question."
P19-1204,"
5 Conclusion
",2019,"in the future, we plan to study the effect of other video modalities on the alignment algorithm."
P19-1204,"
5 Conclusion
",2019,we hope our method and dataset will unlock new opportunities for scientific paper summarization.
P19-1206,"
6 Conclusion
",2019,"in future work, we will make comparisons with those of a humanannotated dataset."
P19-1208,"
7 Conclusion and Future Work
",2019,"another interesting direction is to apply our rl approach on the microblog hashtag annotation problem (wang et al., 2019; gong and zhang, 2016; zhang et al., 2018b)."
P19-1208,"
7 Conclusion and Future Work
",2019,"one potential future direction is to investigate the performance of other encoder-decoder architectures on keyphrase generation such as transformer (vaswani et al., 2017) with multi-head attention module (li et al., 2018; zhang et al., 2018a)."
P19-1210,"
5 Conclusions and Future Work
",2019,"in the future, we plan to further integrate higher level participant interactions, such as gestures, face expressions, etc."
P19-1210,"
5 Conclusions and Future Work
",2019,"we also plan to construct a larger multimedia meeting summarization corpus to cover more diverse scenarios, building on our previous work (bhattacharya et al., 2019)."
P19-1211,"
6 Summary
",2019,"in the future, we would like to understand the usefulness of artificial titles for training the decoder relative to other factors that may impact performance, e.g., how similar the true titles or summaries are in the different domains."
P19-1211,"
6 Summary
",2019,we investigated unsupervised domain adaptation methods for an encoder-decoder model.
P19-1212,"
6 Conclusion
",2019,bigpatent can enable future research to build robust systems that generate abstractive and coherent summaries.
P19-1213,"
6 Conclusions
",2019,our proposed task and collected data can therefore be a valuable resource for future extrinsic evaluations of nli models.
P19-1215,"
6 Conclusion
",2019,"in future work, we intend to examine multitask approaches combining question summarization and question understanding."
P19-1215,"
6 Conclusion
",2019,we also explored data augmentation methods and studied the behavior of abstractive models on this task.
P19-1216,"
6 Conclusion
",2019,"in the future, we will consider extending the current approach to the single document or multiple document summarization."
P19-1217,"
5 Conclusions and Future Works
",2019,"in the future, we will expand it to support the questions on text span selection by using the relation type rather than the option as the terminated condition."
P19-1219,"
7 Conclusion
",2019,"in the future, we plan to use some larger knowledge bases, such as conceptnet and freebase, to improve the quality and scope of the general knowledge."
P19-1220,"
7 Conclusion
",2019,our future work will involve exploring the potential of our multi-style learning towards natural language understanding.
P19-1221,"
6 Conclusion
",2019,"future work will concentrate on designing a fast neural pruner to replace the ir-based pruning component, developing better end-to-end training strategies, and adapting our approach to other datasets such as natural questions (kwiatkowski et al., 2019)."
P19-1222,"
6 Concluding Remarks
",2019,an interesting improvement to our approach would be to allow the retriever to automatically determine whether or not more retrieval iterations are needed.
P19-1222,"
6 Concluding Remarks
",2019,we hope to tackle this problem in future work to allow learning more than two retrieval iterations.
P19-1224,"
8 Conclusion
",2019,"we believe squash is a challenging text generation task and we hope the community finds it useful to benchmark systems built for document understanding, question generation and question answering."
P19-1226,"
6 Conclusion
",2019,"this work demonstrates the feasibility of further enhancing advanced lms with knowledge from kbs, which indicates a potential direction for future research."
P19-1227,"
7 Conclusion
",2019,we hope our work could contribute to the development of cross-lingual openqa systems and further promote the research of overall cross-lingual language understanding.
P19-1228,"
7 Conclusion
",2019,learning deep generative models which exhibit such conditional markov properties is an interesting direction for future work.
P19-1233,"
7 Conclusion
",2019,"in the future, we would like to extend gcdt to other analogous sequence labeling tasks and explore its effectiveness on other languages."
P19-1239,"
7 Conclusion and Future Work
",2019,"in future work, we will incorporate other modality such as audio into the sarcasm detection task and we will also investigate to make use of common sense knowledge in our model."
P19-1240,"
6 Conclusion and Future Work
",2019,"also, our topic-aware neural keyphrase generation model can be investigated in a broader range of text generation tasks."
P19-1240,"
6 Conclusion and Future Work
",2019,"in the future, we will explore how to explicitly leverage the topic-word distribution to further improve the performance."
P19-1241,"
8 Conclusion and Future Work

",2019,our future agenda includes exploring the applicability of our analysis and system for identifying patterns and potential prevention.
P19-1241,"
8 Conclusion and Future Work

",2019,we also plan to use this model to solve other downstream medium-specific tasks pertaining to mental health and welfare.
P19-1242,"
8 Conclusion
",2019,"although we focused on english hashtags, our pairwise ranking approach is language-independent and we intend to extend our toolkit to languages other than english as future work."
P19-1243,"
7 Conclusions and Future Work
",2019,"we leave alternative solutions for future work, including training embeddings from scratch or fine-tuning on the target corpus (however, these ideas are only feasible with a large target corpus, and the need for fine-tuning reduces the usefulness of pre-trained embeddings)."
P19-1244,"
6 Conclusions and Future Work
",2019,"for the future work, beyond what we have mentioned, we plan to examine our model on different information sources."
P19-1244,"
6 Conclusions and Future Work
",2019,"we will also try to incorporate relevant metadata into it, e.g., author profile, website credibility, etc."
P19-1246,"
9 Conclusion
",2019,"nevertheless, there are limitations of distant labelling and social media data — with issues related specifically to the language of food (askalidis and malthouse, 2016) — that we will take into account in future work."
P19-1247,"
7 Conclusion

",2019,"we intend to study fine-grained political perspectives, capturing how different events are framed."
P19-1248,"
6 Conclusions and Future Work
",2019,"our new dataset, analysis of spoiler language, and positive results facilitate several directions for future work."
P19-1249,"
5 Conclusion
",2019,"in future work, we plan on improving the corpus by incorporating verified accounts from other social networks, and, by inferring new labels for as of yet unlabeled celebrities through link prediction."
P19-1250,"

5 Conclusion
",2019,our future work will include efficiently labeling promising comments via active learning.
P19-1250,"

5 Conclusion
",2019,we created a new labeled dataset for ranking constructive comments.
P19-1252,"
5 Conclusion and Future Work

",2019,"directions of future research include adaptation of our methods to a large scale, sparsely connected social network."
P19-1252,"
5 Conclusion and Future Work

",2019,"one might also want to investigate the inductive settings of gcn (hamilton et al., 2017) to predict demographic information of a user from outside the black network."
P19-1253,"
7 Conclusion and Future Work
",2019,we also plan to adapt daml to multi-domain dialog tasks.
P19-1255,"
8 Conclusion
",2019,"an argument generation component then employs a text planning decoder to conduct content selection and specify a suitable language style at sentence-level, followed by a content realization decoder to produce the final argument."
P19-1258,"
4 Conclusion
",2019,"finally, the improved performance on two task-oriented datasets demonstrates the contributions from the separated storage and the reasoning ability of working memory."
P19-1258,"
4 Conclusion
",2019,our future work will focus on how to transfer the long-term memory across different tasks.
P19-1259,"
6 Discussion and Conclusion
",2019,"benefiting from the explicit structure in the cognitive graph, system 2 in cogqa has potential to leverage neural logic techniques to improve reliability."
P19-1259,"
6 Discussion and Conclusion
",2019,"moreover, we expect that prospective architectures combining attention and recurrent mechanisms will largely improve the capacity of system 1 by optimizing the interaction between systems."
P19-1259,"
6 Discussion and Conclusion
",2019,multiple future research directions may be envisioned.
P19-1260,"
5 Conclusion
",2019,"in the future, we would like to investigate explainable gnn for this task, such as explicit reasoning path in (kundu et al., 2018), and work on other data sets such as hotpotqa."
P19-1262,"
8 Conclusion
",2019,"overall, we hope that these insights and initial improvements will motivate the development of new models that combine explicit compositional reasoning with adversarial training."
P19-1265,"
6 Conclusion
",2019,"we expect our work to have a large impact on future work requiring expert annotations, in particular regarding new tasks with no or little available data, for example for legal (nazarenko et al., 2018), chemical (guo et al., 2014), or psychiatric (mieskes and stiegelmayr, 2018) text processing."
P19-1271,"
5 Conclusion
",2019,"as future work, we intend to continue collecting more data for islam and to include other hate targets such as migrants or lgbt+, in order to put the dataset at the service of other organizations and further research."
P19-1271,"
5 Conclusion
",2019,"moreover, as a future direction, we want to utilize conan dataset to develop a counter-narrative generation tool that can support ngos in fighting hate speech online, considering counter-narrative type as an input feature."
P19-1272,"
8 Conclusions
",2019,"future work will look deeper into using the similarity between the content of the text and image (leong and mihalcea, 2011), as the text task results showed room for improvements."
P19-1272,"
8 Conclusions
",2019,"we envision that our data, task and classifiers will be useful as a preprocessing step in collecting data for training large scale models for image captioning (feng and lapata, 2010) or tagging (mahajan et al., 2018) or for improving recommendations (chen et al., 2016) by filtering out tweets where the text and image have no semantic overlap or can enable new tasks such as identifying tweets that contain creative descriptions for images."
P19-1273,"
7 Conclusion
",2019,"finally, an interleaved cross-disciplinary collaboration may support the future research process further: the claim ontology for a new field of debate could be constructed in a bootstrapping process, combining the political scientists’ analytical insights with (preliminary) predictions of computational seed models from partially overlapping fields."
P19-1274,"
7 Conclusions
",2019,"future work could study other types of accounts with similar posting behaviors such as organizational accounts, explore other sources for ground truth tweet identity information (robinson, 2016) or study the effects of user traits such as gender or political affiliation in tweeting signed content."
P19-1275,"
6 Conclusion
",2019,we suggest a future research direction in sarcasm detection where the two types of sarcasm are treated as separate phenomena and socio-cultural differences are taken into account.
P19-1277,"
6 Conclusions
",2019,studying few-shot relation classification with data generated by distant supervision and extending our mlman model to zero-shot learning will be the tasks of our future work.
P19-1278,"
11 Conclusion and Future Work
",2019,"we note that there are a wide range of future directions: (1) human prior knowledge could be incorporated into the similarity quantification; (2) similarity between relations could also be considered in multi-modal settings, e.g., extracting relations from images, videos, or even from audios; (3) by analyzing the distributions corresponding to different relations, one can also find some “meta-relations” between relations, such as hypernymy and hyponymy."
P19-1279,"
6 Conclusion and Future Work
",2019,"in future work, we plan to work on relation discovery by clustering relation statements that have similar representations according to bertem+mtb."
P19-1279,"
6 Conclusion and Future Work
",2019,we will also study representations of relations and entities that can be used to store relation triples in a distributed knowledge base.
P19-1281,"
6 Conclusions
",2019,"future research directions include making selection routines more robust to evaluation outliers, relaxing our gaussian assumptions and developing more effective batch strategies."
P19-1282,"
7 Related and Future Work
",2019,"finally, another direction for future work would be to extend the importance-ranking comparisons that we deploy here for evaluation purposes into a method for deriving better, more informative rankings, which in turn could be useful for the development of new, more interpretable models."
P19-1283,"
6 Conclusion
",2019,we plan to explore these options in future work.
P19-1284,"
8 Conclusions
",2019,we leave further explorations for future work.
P19-1287,"
6 Conclusion and Future Work
",2019,"in the future, we would like to investigate the feasibility of our methods on non-recurrent nmt models such as transformer (vaswani et al., 2017)."
P19-1287,"
6 Conclusion and Future Work
",2019,"moreover, we are also interested in incorporating discourse-level relations into our models."
P19-1288,"
6 Conclusion
",2019,"in the future, we plan to investigate better methods to leverage the sequential information."
P19-1289,"
8 Conclusions
",2019,"we leave many open questions to future work, e.g., adaptive policy using a single model (zheng et al., 2019)."
P19-1292,"
5 Conclusion and Future Work
",2019,"in future work, we would like to do an extensive analysis on the capabilities of bert and transfer learning in general for different domains and language pairs in ape."
P19-1292,"
5 Conclusion and Future Work
",2019,we explored various ways for coupling bert in the decoder for language generation.
P19-1295,"
6 Conclusion
",2019,the proposed method gives highly effective improvements in their integration.
P19-1296,"
5 Conclusion
",2019,"in future work, we intend to apply these methods to other natural language tasks."
P19-1297,"
7 Conclusion
",2019,"in future, we would like to explore other languages with diverse linguistic characteristics."
P19-1299,"
5 Conclusion
",2019,"for future work, we plan to apply man-moe to more challenging languages for tasks such as syntactic parsing, where multilingual data exists (nivre et al., 2017)."
P19-1299,"
5 Conclusion
",2019,"furthermore, we would like to experiment with multilingual contextualized embeddings such as the multilingual bert (devlin et al., 2018)."
P19-1300,"
6 Conclusion
",2019,it would be also interesting to investigate how our approach compares to the baselines given a large amount of data such as wikipedia.
P19-1300,"
6 Conclusion
",2019,our future work is to exploit character and subword information in our model and see how those information affect the performance in each language pair.
P19-1301,"
8 Conclusion
",2019,"through analyzing the learned ranking models, we also gain some insights on the types of features that are most influential in selecting transfer languages for each of the nlp tasks, which may inform future ad hoc selection even without useing our method."
P19-1302,"
7 Conclusions
",2019,"the resource has been made available online, together with a graphical webbased tool for the exploration of cognate data, our hope being to attract both linguists and computer scientists as potential users."
P19-1304,"
5 Conclusions
",2019,"in the future, we will explore more applications of the proposed idea of attentive graph matching."
P19-1307,"
6 Conclusion
",2019,"in the future, we will investigate whether our method helps other downstream tasks."
P19-1312,"
4 Conclusion and future work
",2019,"as a future work, we would like to study, for training bwe, the impact of the use of synthetic parallel data generated by unsupervised nmt, or of a different nature, such as translation pairs extracted from monolingual corpora without supervision."
P19-1312,"
4 Conclusion and future work
",2019,"our approach has, however, a higher computational cost due to the need of generating synthetic parallel data, while generating more data would also improve the vocabulary coverage."
P19-1312,"
4 Conclusion and future work
",2019,our experiments also highlight the robustness of joint training that can take advantage of bilingual contexts even from very noisy synthetic parallel data.
P19-1312,"
4 Conclusion and future work
",2019,"since our approach works on top of unsupervised mapping for bwe and uses synthetic data generated by unsupervised mt, it will directly benefit from any future advances in these two types of techniques."
P19-1316,"
6 Conclusion
",2019,4 two directions for future work are (i) to extend our approach to other languages by using multilingual resources or translation data; and (ii) to explore various compositionality functions to combine the words’ representation on the basis of their grammatical function within a phrase.
P19-1318,"
7 Conclusions
",2019,"for future work, we would be interested in further exploring the behavior of neural architectures for nlp tasks which intuitively would benefit from having access to relational information, e.g., text classification (espinosa anke and schockaert, 2018; camacho-collados et al., 2019) and other language understanding tasks such as natural language inference or reading comprehension, in the line of joshi et al.(2019)."
P19-1318,"
7 Conclusions
",2019,"our model is intended to capture knowledge which is complementary to that of standard similarity-centric embeddings, and can thus be used in combination."
P19-1322,"
5 Conclusion
",2019,it is an interesting future work and will make learning word embeddings more like human learning a language.
P19-1323,"
6 Conclusion and future work
",2019,"future work will offer a more principled account of aspectual classification for specific verb classes, among them speech act and communication verbs (e.g., promise or call) that occur frequently in corpora but have hitherto been neglected in aspectual analyses."
P19-1323,"
6 Conclusion and future work
",2019,"on a more general scale, we envisage examining the interplay of verb class (e.g., the classes of levin 1993), verb sense, and aspectual class, with the purpose of estimating the influence of the sentential context on the aspectual value of the predicate."
P19-1323,"
6 Conclusion and future work
",2019,"we also intend to develop a more principled treatment for the aspectual classification of metaphors, which are frequent in other corpora."
P19-1323,"
6 Conclusion and future work
",2019,"we report substantial interannotator agreement, and validate our resource by training automatic aspectual classifiers, permitting favourable comparisons to prior work."
P19-1324,"
4 Future work
",2019,"though we focused on lstm lms for english, this method can be applied to other architectures, objective tasks, and languages; possibilities to explore in future work."
P19-1324,"
4 Future work
",2019,"we also plan to carry out further analyses aimed at individuating factors that challenge the resolution of lexical ambiguity (e.g., morphosyntactic vs. semantic ambiguity, frequency of a word or sense, figurative uses), as well as clarifying the interaction between prediction and processing of words within neural lms."
P19-1326,"
6 Conclusion and Future Work
",2019,future research directions include a theoretical explanation of kg2vec and applications to downstream nlp tasks.
P19-1329,"
6 Conclusion
",2019,this work also raises important questions about other categories of word-like tokens that need to be treated like special cases.
P19-1330,"
7 Conclusion and Future Work
",2019,"also, we will explore ways of automating parts of the process, e.g.the highlight annotation."
P19-1330,"
7 Conclusion and Future Work
",2019,"in future work, we would like to extend our framework to other variants of summarization e.g.multi-document."
P19-1332,"
5 Conclusion
",2019,"in the future, we plan to investigate more efficient methods of unsupervised domain adaptation with decomposition mechanism on other nlp tasks."
P19-1332,"
5 Conclusion
",2019,we further propose a fast and incremental method for unsupervised domain adaptation.
P19-1333,"
7 Conclusion
",2019,"in the future, we plan to investigate the constituency type classification and rhetorical relation identification steps and port this approach to languages other than english."
P19-1335,"
8 Conclusion
",2019,future variations of the task could incorporate nil recognition and mention detection (instead of mention boundaries being provided).
P19-1335,"
8 Conclusion
",2019,we also expect models that jointly resolve mentions in a document would perform better than resolving them in isolation.
P19-1338,"
5 Conclusion
",2019,"in future work, we would like to combine more potential parsers—including chartstyle parsing and shift-reduce parsing—and transfer knowledge from one to another in a co-training setting."
P19-1340,"
6 Conclusion
",2019,the remarkable effectiveness of unsupervised pretraining of vector representations of language suggests that future advances in this area can continue improving the ability of machine learning methods to model syntax (as well as other aspects of language).
P19-1344,"
5 Conclusion and Future Work
",2019,"in our future work, we plan to apply our approach to other sequence labeling tasks, such as named entity recognition, word segmentation and so on."
P19-1345,"
7 Conclusion
",2019,"furthermore, we would like to explore the effectiveness of our approach to asc-qa in other languages."
P19-1345,"
7 Conclusion
",2019,"in our future work, we would like to solve other challenges in asc-qa such as data imbalance and negation detection to improve the performance."
P19-1345,"
7 Conclusion
",2019,"specifically, we first build a high-quality human annotated benchmark corpus."
P19-1346,"
7 Conclusion
",2019,"we hope eli5 will inspire future work in all aspects of long-form qa, from the information extraction problem of obtaining information from long, multi-document input to generating more coherent and accurate paragraph-length answers."
P19-1350,"
6 Conclusion
",2019,we reserve a deeper investigation of this aspect to future research.
P19-1351,"
6 Conclusion
",2019,"in future work, we are investigating whether the vtqa model can be jointly trained with the paragraph captioning model."
P19-1353,"
7 Conclusion
",2019,we hope this initial application inspires further research by literary scholars and computational humanists in the future.
P19-1355,"
5 Conclusions
",2019,an effort can also be made in terms of software.
P19-1356,"
8 Conclusion
",2019,it would be interesting to see if our results transfer to other domains with higher variability in syntactic structures (such as noisy user generated content) and with higher word order flexibility as experienced in some morphologically-rich languages.
P19-1358,"
6 Future Work
",2019,"in this way, a dialogue agent could both improve its dialogue ability and its potential to improve further."
P19-1358,"
6 Future Work
",2019,we leave exploration of this metalearning theme to future work.
P19-1360,"
7 Conclusion and Future Work
",2019,"in the future, we intend to infer the dialog acts from the annotated responses and use such noisy data to guide the response generation."
P19-1362,"
5 Conclusion
",2019,"in future work, we plan to further investigate the proposed recosa model."
P19-1363,"
6 Conclusion
",2019,"future work may also incorporate contradiction information into the dialogue model itself, and extend to generic contradictions."
P19-1364,"
4 Conclusion
",2019,another interesting direction is to design a trainable budget scheduler.
P19-1364,"
4 Conclusion
",2019,"in future, we plan to investigate the effectiveness of our method on more complex task-oriented dialogue datasets."
P19-1366,"
6 Conclusion and Future Work
",2019,"in addition, we will also explore how to integrate external knowledge in other formats, like the knowledge graph, into adversarial training so that the quality could be further improved."
P19-1366,"
6 Conclusion and Future Work
",2019,"in future research, we will further investigate how to better leverage larger training data to improve the reat method."
P19-1367,"
6 Conclusion and Future Work
",2019,"in the future, there are some promising explorations in vocabulary pyramid networks.1) we will further study how to obtain multi-level vocabularies, such as employing other clustering methods and incorporating semantic lexicons like wordnet; 2) we also plan to design deep-pass encoding and decoding for vpn; 3) we will investigate how to apply vpn to other natural language generation tasks such as machine translation and generative text summarization."
P19-1368,"
6 Conclusion
",2019,we proposed embedding-free on-device neural network that uses joint structured and context partitioned projections for short text classification.
P19-1369,"
7 Conclusion
",2019,experimental results show that dialogue models that plan over knowledge graph can make more full use of related knowledge to generate more diverse conversations.
P19-1369,"
7 Conclusion
",2019,"our dataset and proposed models are publicly available, which can be used as benchmarks for future research on constructing knowledge-driven proactive dialogue systems."
P19-1372,"
6 Conclusion and future work
",2019,directions of future work may be pursuing betterdefined features and easier training strategies.
P19-1373,"
7 Conclusion and Future Work
",2019,"in this paper, unsupervised pretraining has been shown to learn effective representations of dialog context, making this an important research direction for future dialog systems."
P19-1373,"
7 Conclusion and Future Work
",2019,these results open three future research directions.
P19-1376,"
6 General discussion and conclusions
",2019,"although it learns some of the relevant features anyway, it would be interesting to see whether its behaviour becomes more human-like if the correct features are provided in the input."
P19-1376,"
6 General discussion and conclusions
",2019,"as noted by seidenberg and plaut (2014), models’ failures as well as successes can be informative, and we hope that our detailed exploration of the ed model’s behaviour will inspire future developments in these models, both for cognitive modelling and nlp."
P19-1376,"
6 General discussion and conclusions
",2019,"in future, a stricter test might use nonce words that are intentionally less similar to real words (e.g., the example from prasada and pinker (1993): to out-gorbachev)."
P19-1376,"
6 General discussion and conclusions
",2019,"this was not true for earlier models: plunkett and juola’s model often chose the wrong regular suffix (with incorrect voicing in the final phoneme), and rumelhart and mcclelland’s (1986) model failed to produce regular endings for nonce verbs (prasada and pinker, 1993; marcus, 1998)."
P19-1378,"
6 Conclusion
",2019,"another interesting direction is to explore combining different semantic similarity measures (lin et al., 2015) for our task."
P19-1378,"
6 Conclusion
",2019,"in future work, we will explore ensemble learning."
P19-1379,"
5 Conclusion and Future Work
",2019,"in addition to tracking the language evolvement in the history, we believe it is promising future work to use deep contextual embeddings in predicting the future change or trend, as well as detecting novel senses that are not included in existing dictionaries."
P19-1381,"
5 Conclusion
",2019,"in future work, we would like to further our insights on the cnn aspects that are crucial for the task, our preliminary analyses of kernel width and attention."
P19-1381,"
5 Conclusion
",2019,we leave a proper formulation of a tighter comparison to future work.
P19-1384,"
4 Conclusions
",2019,"in future work, we will extend our set of languages, aiming at more typological variety (indoeuropean languages are greatly over-represented in our current data)."
P19-1385,"
6 Conclusions & Future work
",2019,"in the future, we aim to incorporate more elaborate linguistic resources (e.g.knowledge bases) and to investigate the performance of our methods on more complex nlp tasks, such as named entity recognition and sequence labelling, where prior knowledge integration is an active area of research."
P19-1386,"
5 Conclusion
",2019,"in the future, we wish to study the use of knowref to improve performance on general coreference resolution tasks (e.g., the conll 2012 shared tasks)."
P19-1386,"
5 Conclusion
",2019,we also plan to develop new models on knowref and transfer them to difficult common sense reasoning tasks.
P19-1388,"
6 Conclusion and Discussion
",2019,"below, we discuss a few interesting issues brought up by the data collection process that should be addressed in future work."
P19-1390,"
5 Conclusion
",2019,we believe our annotations will be a valuable resource to the nlp community.
P19-1390,"
5 Conclusion
",2019,we chose to annotate the essays in icle that have previously been scored along multiple dimensions in order to facilitate future developments of joint models that can capture the interactions among different dimensions.
P19-1392,"
5 Conclusions and Further Work
",2019,in further work we plan to carry out a multilingual alignment of the collocations in each language.
P19-1392,"
5 Conclusions and Further Work
",2019,"this dataset can serve as a basis to evaluate systems designed to automatically extract collocations and identify their lexical functions, which in turn may be useful for different nlp and corpus linguistics tasks."
P19-1394,"
6 Conclusion and Future Work
",2019,"in the future, we aim to analyze how changes in the training procedure and hyperparameters of ulmfit affect resulting model performance."
P19-1394,"
6 Conclusion and Future Work
",2019,"on top of that, we hope to improve model generalization by augmenting negative examples with a split of jokes into setups and punchlines, as they should not be funny by themselves."
P19-1394,"
6 Conclusion and Future Work
",2019,we also plan to reproduce the experiment on english data.
P19-1396,"
5 Conclusion
",2019,"for future study, we would like to investigate how to automatically learn the number of latent clusters with nonparametric bayesian methods."
P19-1397,"
6 Conclusion
",2019,future work could potentially expand our work into an end-to-end invertible model that is able to produce high-quality representations by omnidirectional computations.
P19-1399,"
6 Conclusion
",2019,"future works include applying the method to more language pairs and more domain-specific lexicon induction, e.g., terminologies."
P19-1400,"
7 Conclusions
",2019,"in future work we will aim to improve candidate selection (including different strategies to select candidate lists e+, e?)."
P19-1401,"
7 Conclusion
",2019,"our approach is the first study to interleave (i) the wake phase, where the al policy is exploited to improve the student learner and (ii) the dream phase, where the student learner in turn acts as an imperfect annotator to enhance the al policy."
P19-1404,"
6 Conclusion & Future Work
",2019,"on the contrary, both the source specific and common parts of the representation come along for efficient performance in the source domain task."
P19-1405,"
6 Conclusions 
",2019,"in the future, we will refine the bayes test of p, r, and f1 in an m × 2 bcv and provide accurate interval estimation of other evaluation metrics on the basis of the confusion matrix."
P19-1407,"
7 Conclusion
",2019,"by letting the decoder ’keep notes’ on the encoder, or said another way, re-encode the input at every decoding step, the scratchpad mechanism effectively guides future generation."
P19-1408,"
8 Conclusions
",2019,a future direction is to investigate the effect of using mina spans not only in evaluation but also for training existing coreference resolvers.
P19-1408,"
8 Conclusions
",2019,"in addition to coreference evaluation, automatically extracted minimum spans can benefit the annotation process of new corpora."
P19-1408,"
8 Conclusions
",2019,"investigating the use of mina in other nlp areas, e.g., evaluating spans in named entity recognition or reading comprehension, is another future line of work."
P19-1408,"
8 Conclusions
",2019,the incorporation of automatically extracted minimum spans reduces the effect of maximum boundary detection errors in coreference evaluation and results in a fairer comparison.
P19-1409,"
8 Conclusion
",2019,"future directions include investigating ways to minimize the pipeline errors from the extraction of predicate-argument structures, and incorporating a mention prediction component, rather than relying on gold mentions."
P19-1410,"
6 Conclusions
",2019,"also, in the future, we would like to investigate how our approach can be generalized to other discourse frameworks such as the penn discourse treebank (pdtb)."
P19-1410,"
6 Conclusions
",2019,we also train the segmenter and the parser jointly through the encoder-decoder architecture and improve the results further.
P19-1411,"
5 Conclusion
",2019,"in the future work, we plan to extend the idea of multi-task learning/transfer learning with label embeddings to the problems in information extraction (e.g., event detection, relation extraction, entity mention detection) (nguyen and grishman, 2015a,b, 2016d; nguyen et al., 2016a,b,c; nguyen and nguyen, 2018b, 2019)."
P19-1413,"
5 Summary
",2019,"in the future we would like to expand this direction, and find ways to connect event and relation representation, learning and inference in a unified framework."
P19-1414,"
6 Conclusion and Future Work
",2019,"as a future work, we plan to introduce a wider range of background knowledge including another type of event causality (hashimoto et al., 2012, 2014, 2015; kruengkrai et al., 2017)."
P19-1415,"
5 Conclusions
",2019,"as for future work, we would like to enhance the ability to utilize antonyms for unanswerable question generation by leveraging external resources."
P19-1416,"
6 Conclusions
",2019,"in this context, we suggest that future work can explore better retrieval methods for multi-hop questions."
P19-1416,"
6 Conclusions
",2019,"in this end, future multi-hop rc datasets can develop improved methods for distractor collection."
P19-1417,"
5 Conclusion
",2019,"in future work, we will extend the proposed idea to other qa tasks with evidence of multimodality, e.g.combining with symbolic approaches for visual qa (gan et al., 2017; mao et al., 2019; hu et al., 2019)."
P19-1421,"
6 Conclusions and Future Work
",2019,"promising future directions would be to investigate how to utilize user interaction in moocs more adequately, as well as how attributes of course concepts can help expanding."
P19-1423,"
6 Conclusion
",2019,"as future work, we plan to incorporate joint named entity recognition training as well as sub-word embeddings in order to further improve the performance of the proposed model."
P19-1424,"
7 Limitations and Future Work
",2019,"finally, we plan to adapt bespoke models proposed for the chinese criminal court (luo et al., 2017; zhong et al., 2018; hu et al., 2018) to data from other courts and explore multitask learning."
P19-1424,"
7 Limitations and Future Work
",2019,"providing valid justifications is an important priority for future work and an emerging topic in the nlp community.8 in this direction, we plan to expand the scope of this study by exploring the automated analysis of additional resources (e.g., relevant case law, dockets, prior judgments) that could be then utilized in a multi-input fashion to further improve performance and justify system decisions."
P19-1424,"
7 Limitations and Future Work
",2019,"we also plan to apply neural methods to data from other courts, e.g., the european court of justice, the us supreme court, and multiple languages, to gain a broader perspective of their potential in legal justice prediction."
P19-1425,"
6 Conclusion
",2019,"in future work, we plan to explore the direction to generate more natural adversarial examples dispensing with word replacements and more advanced defense approaches such as curriculum learning (jiang et al., 2018, 2015)."
P19-1428,"
7 Conclusions and future work
",2019,"as future work, we plan to study the introduction of high-level knowledge to deal with the issue of invalid pipelines and improve the performance of the optimization process."
P19-1428,"
7 Conclusions and future work
",2019,"therefore, we plan to explore this line of research in the future, to compare our proposal with other automl frameworks in standard benchmarks."
P19-1429,"
6 Conclusions
",2019,"for future work, we plan to investigate new auxiliary ?-learning algorithms using our ?-learning framework."
P19-1430,"
5 Conclusion and Future Work
",2019,"from coarse to fine, sememe-level information can be intuitively valuable."
P19-1430,"
5 Conclusion and Future Work
",2019,"here, sememe is the minimum semantic unit of word sense, whose information may potentially assist the model to explore deeper semantic features."
P19-1430,"
5 Conclusion and Future Work
",2019,"in the future, we will attempt to improve the ability of the mg lattice to utilize multi-grained information."
P19-1431,"
5 Conclusion
",2019,"future research will look into applying such methods to reason jointly about text and kg, by attending to textual mentions of entities in addition to graph (verga et al., 2016)."
P19-1432,"
5 Conclusion & Future Work
",2019,"consequently, in the future work, we plan to develop methods that can automatically induce the sentence structures for efp."
P19-1434,5 Conclusion,2019,further qualitative analysis of memory contents after learning confirms that such good performance comes from its ability to retain important instances for future qa tasks.
P19-1434,5 Conclusion,2019,"to handle this problem, we proposed episodic memory reader (emr), which is basically a memory-augmented network with rl-based memory-scheduler, that learns the relative importance among memory entries and replaces the entries with the lowest importance to maximize the qa performance for future tasks."
P19-1435,7 Conclusion,2019,"we suggest for future nlsm datasets, the providers should pay more attention to this problem."
P19-1436,7 Conclusion,2019,future work includes better phrase representation learning to close its accuracy gap with qa models with query-dependent document encoding.
P19-1436,7 Conclusion,2019,utilizing the phrase index as an external memory for an interaction with text-based knowledge is also an interesting direction.
P19-1437,7 Conclusion,2019,"the parser encodes grammar knowledge in natural language, which helps language model quickly land on new corpus."
P19-1438,7 Conclusion,2019,promising future directions include experimenting with our zero-shot adaptation methods in the context of neural semantic parsing (after increasing the number of examples per domain) and extending the dataset to include more complicated applications and multi-utterance instructions.
P19-1439,8 Conclusions,2019,these trends suggest that improving on lm pretraining with current techniques will be challenging.
P19-1439,8 Conclusions,2019,"while further work on language modeling seems straightforward and worthwhile, we believe that the future of this line of work will require a better understanding of the settings in which target task models can effectively utilize outside knowledge and data, and new methods for pretraining and transfer learning to do so."
P19-1441,5 Conclusion,2019,"at last, we also would like to verify whether mt-dnn is resilience against adversarial attacks (glockner et al., 2018; talman and chatzikyriakidis, 2018; liu et al., 2019)."
P19-1441,5 Conclusion,2019,"there are many future areas to explore to improve mt-dnn, including a deeper understanding of model structure sharing in mtl, a more effective training method that leverages relatedness among multiple tasks, for both fine-tuning and pre-training (dong et al., 2019), and ways of incorporating the linguistic structure of text in a more explicit and controllable manner."
P19-1442,9 Conclusion,2019,we fine-tune larger models on this task and achieve state-of-the-art on the pdtb implicit discourse relation prediction.
P19-1447,4 Conclusion,2019,in the future we plan to apply the reranker to other parsers and more benchmark datasets.
P19-1449,5 Conclusion,2019,"given these results, and the continued difficulty neural methods have with the winograd schema challenge, we argue that future work on glue-style sentence understanding tasks might benefit from a focus on learning from smaller training sets."
P19-1450,5 Conclusion,2019,"in the future, we would like to extend our approach to sembanks which are annotated with different types of semantic representation, e.g."
P19-1450,5 Conclusion,2019,we will explore latent-variable models to learn the dependency trees automatically.
P19-1455,7 Conclusion and Future Work,2019,"future work could investigate advanced spatiotemporal fusion strategies (e.g., tensor-fusion (zadeh et al., 2017), cca (hotelling, 1936)) to better encode the correspondence between modalities."
P19-1455,7 Conclusion and Future Work,2019,future work should try to leverage these factors to improve the baseline scores reported in this paper.
P19-1455,7 Conclusion and Future Work,2019,"future work should try to overcome this issue with solutions involving pre-training, transfer learning, domain adaption, or low-parameter models."
P19-1455,7 Conclusion and Future Work,2019,"moreover, while conducting this research, we identified several challenges that we believe are important to address in future research work on multimodal sarcasm detection."
P19-1456,6 Conclusion,2019,"besides, developing techniques to incorporate the claim stance and specificity detection models in argument generation to generate more coherent and consistent arguments is another interesting research direction to be explored."
P19-1456,6 Conclusion,2019,"for future work, it may be interesting to understand which other models would be effective in claim specificity and stance detection tasks."
P19-1458,"
9 Discussion and Future Considerations
",2019,"in the future, we will investigate other nlp tasks such as named entity recognition (ner), question answering (qa) and aspect-based sentiment analysis (absa) (pontiki et al., 2016) to see whether results we saw in sentiment analysis is consistent across these tasks."
P19-1458,"
9 Discussion and Future Considerations
",2019,we hope that our experimental results inspire future research dedicated to japanese.
P19-1459,8 Conclusion,2019,the adversarial dataset should be adopted as the standard in future work on arct.
P19-1459,8 Conclusion,2019,we hope that providing a more robust evaluation will help to spur more productive research on this problem.
P19-1460,5 Conclusion and Future Work,2019,"in the future, this work can be progressed in several ways."
P19-1462,4 Conclusion,2019,"in future works, we will explore the extension of this approach for other tasks."
P19-1463,5 Conclusion,2019,"for future work, we plan to i) automatically predict relations between argument components in the uselecdeb60to16 dataset, and ii) propose a new task, i.e., fallacy detection so that common fallacies in political argumentation (zurloni and anolli, 2010) can be automatically identified, in line with the work of (habernal et al., 2018)."
P19-1464,5 Conclusion and Future work,2019,one interesting line of our future work is to investigate the performance of our model in an endto-end setting (including ac segmentation).
P19-1466,5 Conclusion and Future Work,2019,"in the future, we intend to extend our method to better perform on hierarchical graphs and capture higher-order relations between entities (like motifs) in our graph attention model."
P19-1467,7 Conclusion,2019,"in future work, we intend to refine these alignment boundaries and to optimize the alignment procedure for speed."
P19-1467,"
7 Conclusion
",2019,we hope that this work will raise more interest in developing alignment systems for longer paraphras.
P19-1469,"
6 Conclusion
",2019,"for future work, we plan to focus on generating more realistic text and use the generated text in other tasks e.g.data augmentation, addressing adversarial attack."
P19-1469,"
6 Conclusion
",2019,we expect the model could perform more natural generation via applying recent advancements on deep generative models.
P19-1470,"
7 Conclusion
",2019,"these positive results point to future work in extending the approach to a variety of other types of knowledge bases, as well as investigating whether comet can learn to produce openie-style knowledge tuples for arbitrary knowledge seeds."
P19-1473,"
7 Conclusions and Future Work
",2019,we also plan to investigate the possibility of augmenting the parallel corpus by bootstrapping from shared templates across domains.
P19-1473,"
7 Conclusions and Future Work
",2019,we plan to explore if further fine-tuning using denotations based training on the distilled model can lead to improvements in the unified parser.
P19-1473,"
7 Conclusions and Future Work
",2019,we would also like to explore if guiding the decoder through syntactical and domain-specific constraints helps in reducing the search space for the weakly supervised unified parser.
P19-1474,"
6 Conclusion
",2019,a prominent direction for future work is using the hyperbolic embeddings as the sole signal for taxonomy extraction.
P19-1474,"
6 Conclusion
",2019,"since distributional and hyperbolic embeddings cover different relations between terms, it may be interesting to combine them."
P19-1474,"
6 Conclusion
",2019,"this observation confirms the theoretical capability of poincare embeddings to learn ′ hierarchical relations, which enables their future use in a wide range of semantic tasks."
P19-1476,"
4 Conclusion
",2019,"we next plan to evaluate glen on multilingual and cross-lingual graded le datasets (vulic et al.′ , 2019) and release a large multilingual repository of le-specialized embeddings."
P19-1477,"
5 Conclusion
",2019,"future work will entail adaption of the attentions, to further improve the performance."
P19-1478,"
6 Summary and Outlook
",2019,"in future work, other uses and the statistical significance of maskedwiki’s impact and its applications to different tasks will be investigated."
P19-1478,"
6 Summary and Outlook
",2019,the consistent improvement of several language models indicates the robustness of this method.
P19-1479,"
5 Conclusion
",2019,"in the future, we would like to explore how to introduce external knowledge into the graph to make the generated comments more logical."
P19-1480,"
7 Conclusion and Future Work
",2019,there are several future directions for this setting.
P19-1481,"
7 Conclusion
",2019,"in future work, we will explore the use of cross-lingual embeddings to further improve performance on this task."
P19-1486,"
5 Conclusion
",2019,we proposed curriculum learning based pointergenerator networks for reading long narratives.
P19-1487,"
7 Conclusion and Future Work
",2019,"with deferral of explanation to neural models, it will be crucial in the future to study the ethical implications of biases that are accumulated during pretraining or fine-tuning."
P19-1489,"
7 Discussion: What Modularity Can and
Cannot Do
",2019,"future work should investigate how to combine techniques that use both word meaning and nearest neighbors for a more robust, semisupervised cross-lingual evaluation."
P19-1490,"
5 Conclusion and Future Work
",2019,"in the future, we will work on cross-lingual extensions of monolingual hyperbolic embedding models (nickel and kiela, 2017; ganea et al., 2018)."
P19-1490,"
5 Conclusion and Future Work
",2019,"we will also experiment with other sources of bilingual information (e.g., cross-lingual word embeddings) and port the transfer approach to more language pairs, with a particular focus on resource-poor languages."
P19-1493,"
6 Conclusion
",2019,it is our hope that these kinds of probing experiments will help steer researchers toward the most promising lines of inquiry by encouraging them to focus on the places where current contextualized word representation approaches fall short.
P19-1493,"
6 Conclusion
",2019,"the model handles transfer across scripts and to code-switching fairly well, but effective transfer to typologically divergent and transliterated targets will likely require the model to incorporate an explicit multilingual training objective, such as that used by lample and conneau (2019) or artetxe and schwenk (2018)."
P19-1494,"
6 Conclusions and future work
",2019,"in the future, we would like to further improve our method by incorporating additional ideas from unsupervised machine translation such as joint refinement and neural hybridization (artetxe et al., 2019)."
P19-1494,"
6 Conclusions and future work
",2019,"we propose a new approach to bli which, instead of directly inducing bilingual dictionaries from cross-lingual embedding mappings, uses them to build an unsupervised machine translation system, which is then used to generate a synthetic parallel corpus from which to extract bilingual lexica."
P19-1494,"
6 Conclusions and future work
",2019,"we thus conclude that, contrary to recent trend, future work in bli should not focus exclusively in direct retrieval approaches, nor should bli be the only evaluation task for cross-lingual embeddings."
P19-1495,"
7 Conclusions & Future Work

",2019,"in the future, we plan to identify the target of the complaint in a similar way to aspect-based sentiment analysis (pontiki et al., 2016)."
P19-1495,"
7 Conclusions & Future Work

",2019,this would allow them to improve efficiency in customer service or to more cheaply gauge popular opinion in a timely manner in order to identify common issues around a product launch or policy proposal.
P19-1495,"
7 Conclusions & Future Work

",2019,we plan to use additional context and conversational structure to improve performance and identify the sociodemographic covariates of expressing and phrasing complaints.
P19-1497,"
5 Conclusion and Future Work
",2019,"besides, how to better leverage prior knowledge during openqg (like human often do) is also interesting."
P19-1497,"
5 Conclusion and Future Work
",2019,"first, we will explore more powerful qg structure to deal with the huge difference between the length of input and output texts."
P19-1497,"
5 Conclusion and Future Work
",2019,there are many future works to be done.
P19-1499,"
5 Conclusions

",2019,"in the future, we plan to apply models to other tasks that also require hierarchical document encodings (e.g., document question answering)."
P19-1499,"
5 Conclusions

",2019,we are also interested in improving the architectures of hierarchical document encoders and designing other objectives to train hierarchical transformers.
P19-1500,"
6 Conclusions
",2019,in the future we would like to apply our hierarchical transformer to question answering and related textual inference tasks.
P19-1503,"
5 Conclusion
",2019,future work could be comparing language models of different types and scales in this direction.
P19-1505,"
6 Conclusion
",2019,"more generally, we observe that our wug-test techniques provides a general way of studying regularity and predictability within languages and may prove useful for attacking other difficult problems in the literature, such as detecting inflectional classes."
P19-1505,"
6 Conclusion
",2019,perhaps of greater interest than this positive result is the difference in the strength of the correlation between the level of individual forms and the level of lexemes.
P19-1509,"
5 Discussion
",2019,how to incorporate “effort”-based pressures in neural networks is an exciting direction for future work.
P19-1513,"
8 Conclusions
",2019,"in the future, more effort is needed to extract the best score."
P19-1516,"
7 Conclusion
",2019,"in future, we would like to assist the model with multiple linguistic aspects of social media text like figurative languages."
P19-1517,"
4 Conclusion and Future Work
",2019,"despite its simplicity, it yields better performance."
P19-1517,"
4 Conclusion and Future Work
",2019,"in the future, we would also like to investigate better models that are capable to address general arithmetic word problems, including addition, subtraction, multiplication and division."
P19-1518,"
6 Conclusion
",2019,"the proposed model not only captures high-order correlations between labels, but also reduces the dependence on the order of output labels."
P19-1520,"
5 Conclusion and Future Work
",2019,"for future work, we plan to apply the main idea of our approach to other tasks."
P19-1522,"
6 Conclusion and Discussion
",2019,"therefore, for the future work, we will incorporate relation between events and relation between arguments into pre-trained language models, and take effective measures to overcome the deviation problem of roles in the generation."
P19-1523,"
5 Conclusion
",2019,an error analysis is performed to shed light on possible future directions.
P19-1524,"
4 Discussion
",2019,"also, we would like to further explore the possibility to use domain-specific gazetteers or dictionaries to boost the performance of ner in various domains (shang et al., 2018), beyond the standard corpora."
P19-1524,"
4 Discussion
",2019,"future directions will include trying similarly enhanced modules on other different types of segmental models (kong et al., 2016; liu et al., 2016; zhuo et al., 2016; zhai et al., 2017; sato et al., 2017), along with richer representations for further gain."
P19-1529,"
6 Conclusion and Future Work
",2019,in future work we will explore how external information is best used in ensembles of models for srl and other tasks.
P19-1529,"
6 Conclusion and Future Work
",2019,we will also investigate whether the choice of method for injecting external information has the same impact on other nlp tasks as it does on srl.
P19-1533,"
7 Conclusion

",2019,future work will consider problems where more challenging forms of neighbor manipulation are necessary for prediction.
P19-1567,"
7 Conclusion
",2019,we hope that this work will raise more interest in developing alignment systems for longer paraphrase.
P19-1568,"
7 Conclusion and Future Work

",2019,another potential future work would be to explore other ways of providing rich supervision from textual descriptions as targets.
P19-1569,"
7 Future Work
",2019,in future work we plan to use multilingual resources (i.e.embeddings and glosses) for improving our sense embeddings and evaluating on multilingual wsd.
P19-1570,"
8 Conclusion and future work
",2019,this will make our model more comprehensive by enabling the embedding of words and short texts in the same space.
P19-1571,"
6 Conclusion and Future Work
",2019,"in the future, we will explore the following directions: (1) context information is also essential to mwe representation learning, and we will try to combine both internal information and external context information to learn better mwe representations; (2) many mwes lack sememe annotation and we will seek to calculate an mwe’s scd when we only know the sememes of the mwe’s constituents; (3) our proposed models are also applicable to the mwes with more than two constituents and we will extend our models to longer mwes; (4) sememe is universal linguistic knowledge and we will explore to generalize our methods to other languages."
P19-1572,"
6 Conclusion
",2019,"finally, we plan to further extend and use the humour dataset to investigate open questions on the linguistics of humour, such as what relationships hold between a pun’s phonology and its “successfulness” or humorousness (lagerquist, 1980; hempelmann and miller, 2017)."
P19-1572,"
6 Conclusion
",2019,"given that our model achieves good results with rudimentary, task-agnostic linguistic features, in future work we plan to investigate the use of humourand metaphor-specific features, including some of those used in past work (see §2) as well as those inspired by the prevailing linguistic theories of humour (attardo, 1994) and metaphor (black, 1955; lakoff and johnson, 1980)."
P19-1573,"
7 Conclusion
",2019,"aside from combo embeddings, linguistic information is retained most exactly in the recently proposed laser sentence embeddings, provided by an encoder designed with a relatively simple bilstm architecture, but estimated on tremendous multilingual data."
P19-1575,"
6 Conclusions
",2019,future work can improve this method further by examining the effects of different dimensionality reduction methods with varying properties on extracting the most informative pathways from the activations.
P19-1576,"
5 Conclusions and Future Work
",2019,"in the future, we would like to experiment with more data, so that enough training data can be obtained for less frequent lfs."
P19-1577,"
5 Conclusion 
",2019,"in future, we plan to develop an automatic procedure of finding thesaurus regularities in dbag of problematic words, which can make more evident what kind of relations or senses are missed in the thesaurus."
P19-1578,"
4 Discussion and Future Work
",2019,"for the future work, we hope to extend this idea proposed in this paper to train a model capable of handling different types of errors through the generative model since it can generate different lengths of results."
P19-1579,"
7 Conclusion
",2019,"in future work, we will explore methods for controlling the induced dictionary quality to improve word substitution as well as m-umt."
P19-1580,"
8 Conclusions
",2019,"in future work, we would like to investigate how our pruning method compares to alternative methods of model compression in nmt."
P19-1581,"
4 Conclusions
",2019,using this noisy synthetic parallel data we fine-tune the initial nmt system.
P19-1584,"
10 Conclusions & Future Work
",2019,future work: the automatic pseudonymization approach could serve as a data augmentation scheme to be used as a regularizer for deidentification models.
P19-1584,"
10 Conclusions & Future Work
",2019,"when more training data from multiple sources become available in the future, it will be possible to evaluate our adversarially learned representation against unseen data."
P19-1586,"
7 Conclusion
",2019,our frameworks of transfer and active learning for deep learning models are potentially applicable to low-resource settings beyond entity resolution.
P19-1587,"
5 Conclusion
",2019,the proposed model offers promising future extensions in terms of directly optimizing other metrics such as recall and fβ.
P19-1588,"
6 Conclusion
",2019,"in the future, first, we try to utilize more eyetracking corpus and estimate more features of reading behavior."
P19-1588,"
6 Conclusion
",2019,"moreover, human attention is also effective on unsupervised models."
P19-1588,"
6 Conclusion
",2019,"then, we will attempt to analyze real human reading behavior on social media and thereby explore more specific human attention features on social media."
P19-1589,"
5 Conclusion and Future Work
",2019,"in future work, we want to extend this approach to other natural language processing tasks."
P19-1591,"
6 Conclusions
",2019,"in future work we would like to develop more sophisticated trl algorithms, for both in-domain and domain adaptation nlp setups."
P19-1591,"
6 Conclusions
",2019,the resulting pblm-cnn model improves both the accuracy and the stability of the original pblm-cnn model where pblm is trained without trl.
P19-1592,"
6 Conclusion
",2019,"in future work, we hope to further study the connections between our optimal transport-based alignment method and methods based on attention."
P19-1593,"
5 Conclusion
",2019,"a key question for future work is the performance on longer texts, such as the full-length news articles encountered in ontonotes."
P19-1596,"
6 Conclusions
",2019,"for future work, we plan on exploring other models for nlg, and on providing models with a more detailed input representation in order to help preserve more dependency information, as well as to encode more information on syntactic structures we want to realize in the output."
P19-1597,"
5 Conclusion and Future Work
",2019,another interesting direction is to extend our models to multimove commentary generation tasks.
P19-1598,"
7 Conclusions and Future Work
",2019,"in this work, we proposed the knowledge graph language model (kglm), a neural language model that can access an external source of facts, encoded as a knowledge graph, in order to generate text."
P19-1598,"
7 Conclusions and Future Work
",2019,this work lays the groundwork for future research into knowledge-aware language modeling.
P19-1600,"
6 Conclusion and Future Work
",2019,"in the future, we will explore the representation for the implicit information like whether a man is retired or not or how long a sportsman’s career is given starting and ending years, in the table by including some inference strategies."
P19-1601,"
5 Conclusions and Future W
",2019,"especially, because our proposed approach doesn’t assume a disentangled latent representation for manipulating the sentence style, our model can get better content preservation on both of two datasets."
P19-1601,"
5 Conclusions and Future W
",2019,"in the future, we are planning to adapt our style transformer to the multiple-attribute setting like lample et al.(2019)."
P19-1603,"
6 Conclusion and Future Work
",2019,"future work can combine the analyzer and generator via joint training, hopefully to achieve better results."
P19-1606,"
6 Conclusions

",2019,we plan on exploring backpropable variants as a scaffold for structure and also extend the techniques to other how-to domains in future.
P19-1606,"
6 Conclusions

",2019,we plan to explore explicit evaluation of the latent structure learnt.
P19-1610,"
6 Conclusion
",2019,our experiments highlight the need for separate adversarial testing and the importance of improving the robustness of qa models to question paraphrasing for better reliability when tested on future unseen test questions.
P19-1610,"
6 Conclusion
",2019,there are several possible future directions stemming from this work.
P19-1611,"
6 Conclusion
",2019,"altogether, rankqa provides a new, strong baseline for future research on neural qa."
P19-1615,"
6 Conclusion
",2019,we have provided two new ways of performing knowledge extraction over a knowledge base for qa and evaluated three ways to perform abductive inference over natural language.
P19-1616,"
7 Conclusion
",2019,"similar problems may exist in other nlp tasks, which will be interesting to investigate in the future."
P19-1617,"
5 Conclusion
",2019,"in the future, we may incorporate new advances in building entity graphs from texts, and solve more difficult reasoning problems, e.g.the cases of comparison query type in hotpotqa."
P19-1617,"
5 Conclusion
",2019,we evaluate dfgn on hotpotqa and achieve leading results.
P19-1618,"
6 Discussion and Future Work
",2019,"additionally, it would be interesting to study the behavior of nlprolog in the presence of multiple wikihop query predicates."
P19-1618,"
6 Discussion and Future Work
",2019,"we are also interested in incorporating future improvements of symbolic provers, triple extraction systems and pretrained sentence representations to further enhance the performance of nlprolog."
P19-1620,"
5 Conclusion
",2019,"we additionally proposed a possible direction for formal grounding of this method, which we hope to develop more thoroughly in future work."
P19-1621,"
5 Discussion
",2019,"we hope that our work persuades others to consider the importance of consistency, and initiates a body of work in qa models that achieve real understanding by design."
P19-1622,"
4 Conclusion
",2019,we will study further the capability of our approaches on other datasets and tasks in the future work.
P19-1626,"
6 Conclusion
",2019,3 future work includes the fusion of additional operations in neural models.
P19-1627,"
5 Conclusion
",2019,"here, we believe that the new framework can provide considerable help to future research, specifically also, because it does not not require the technical support of high-end clusters."
P19-1627,"
5 Conclusion
",2019,our methods are best used for the purpose of exploratory analysis on larger datasets which have so far not yet been thoroughly studied.
P19-1628,"
6 Conclusions
",2019,"in the future, we would like to investigate whether some of the ideas introduced in this paper can improve the performance of supervised systems as well as sentence selection in multi-document summarization."
P19-1629,"
8 Conclusions
",2019,"in the future, we would like to more faithfully capture the semantics of documents by explicitly modeling entities and their linking."
P19-1630,"
6 Conclusions
",2019,"the aspects considered in this work, as well as the creation process of synthetic data by interleaving documents which are maximally distinct with respect to the target aspects leave room for refinement."
P19-1630,"
6 Conclusions
",2019,"we believe that training models on heuristic, but inexpensive data sets is a valuable approach which opens up exciting opportunities for future research."
P19-1631,"
6 Conclusion and Future Work
",2019,there are several avenues we can explore as future research.
P19-1633,"
5 Conclusions and Future Work
",2019,"as future work, we will investigate approaches to hyperparameter tuning to find better model architectures for hierarchical multi-label text classification tasks."
P19-1634,"
6 Conclusion
",2019,"fortunately, this will come naturally to researchers in the field as they are already trained to avoid features that encode topic rather than style."
P19-1634,"
6 Conclusion
",2019,"in particular, we strongly suggest that future evaluations should adopt a stateless one-case-at-a-time test policy."
P19-1634,"
6 Conclusion
",2019,inadvertent properties of the data act as confounders that a learning algorithm will gladly fit onto if they are not controlled.
P19-1635,"
6 Conclusion
",2019,"in future work, we plan to extend our work to further applications such as detecting exaggerated statements by investors in social media data."
P19-1636,"
6 Limitations and Future Work
",2019,"finally, we plan to investigate generalized zero-shot learning (liu et al., 2018)."
P19-1636,"
6 Limitations and Future Work
",2019,we also plan to experiment with hierarchical flavors of bert to surpass its length limitations.
P19-1636,"
6 Limitations and Future Work
",2019,we leave the investigation of methods for extremely large label sets for future work.
P19-1636,"
6 Limitations and Future Work
",2019,"we plan to investigate more computationally efficient methods, e.g., dilated cnns (kalchbrenner et al., 2017) and transformers (vaswani et al., 2017; dai et al., 2019)."
P19-1637,"
6 Conclusion
",2019,"an important question for future models, especially interactive ones, is how to signal to the user when their desires do not comport with reality."
P19-1637,"
6 Conclusion
",2019,"while we simulate user behavior for good and random users, future work should compare these systems with end users, as well as compare end user ratings of control with our proposed automated metrics."
P19-1638,"
5 Conclusions and Future work
",2019,"future work can investigate other embedding methods with a richer set of probe tasks, or explore a wider range of downstream tasks."
P19-1640,"
7 Conclusion and Future Work
",2019,a future direction is to improve the gan-based training of w-lda.
P19-1640,"
7 Conclusion and Future Work
",2019,another future direction is to experiment with more complex priors than the dirichlet prior.
P19-1640,"
7 Conclusion and Future Work
",2019,"unlike existing neural network based models, w-lda can directly enforce dirichlet prior, which plays a central role in the sparse mixed membership model of lda."
P19-1642,"
5 Conclusions and Future work
",2019,"in future work we will explore other generative models for multi-modal mt, as well as different ways to directly incorporate images into these models."
P19-1643,"
7 Conclusion
",2019,"in future work, we plan to explore additional representations and architectures to improve the accuracy of our model, and to identify finer-grained alignments between visual actions and their verbal descriptions."
P19-1647,"
6 Conclusion
",2019,"limitations and future work our current model does not include an explicit speech-to-text decoder, which limits the types of analyses we can perform."
P19-1647,"
6 Conclusion
",2019,we are also planning to investigate how sensitive our approach is to amount of data for the auxiliary task.
P19-1651,"
7 Conclusion
",2019,"we hope that the grounded, goal-driven communication setting that codraw is a testbed for can lead to future progress in building agents that can speak more naturally and better maintain coherency over a long dialog, while being grounded in perception and actions."
P19-1652,"
5 Conclusion and Future Work
",2019,"in future, we plan to study more effective ways to construct the image-grounded vocabulary."
P19-1656,"
5 Discussion
",2019,"we believe the results of mult on unaligned human multimodal language sequences suggest many exciting possibilities for its future applications (e.g., visual question answering tasks, where the input signals is a mixture of static and time-evolving signals)."
P19-1656,"
5 Discussion
",2019,"we hope the emergence of mult could encourage further explorations on tasks where alignment used to be considered necessary, but where crossmodal attention might be an equally (if not more) competitive alternative."
P19-1659,"
6 Conclusions

",2019,"in the future, we would like to extend this work to generate multidocument (multi-video) summaries and also build end-to-end models directly from audio in the video instead of text-based output from pretrained asr."
P19-1660,"
6 Conclusion
",2019,"an orthogonal line of future work might involve using a visual question answering (vqa) task (such as in krishna et al.(2017)), either on its own replacing the captioning task, or in conjunction with the captioning task with a multi-task learning objective."
P19-1660,"
6 Conclusion
",2019,"one possible line of work involves removing the requirement of ground truth bounding boxes altogether by leveraging a recent line of work that does weakly-supervised object detection (such as (oquab et al., 2015; bilen and vedaldi, 2016; zhang et al., 2018; bai and liu, 2017; arun et al., 2018))."
P19-1660,"
6 Conclusion
",2019,there are several interesting avenues for future work.
