id,text,label,year
2020.acl-demos.1.txt,another direction for improvement is to further enhance the ability to interact with users via a conversation interface.,1,2020
2020.acl-demos.1.txt,"one such important direction for future improvement is the expansion of areas that it can work in, which can be achieved through a promising approach of adopting model based technologies together with rule/template based ones.",1,2020
2020.acl-demos.10.txt,"in particular, we plan to incorporate human performance as a reference metric, integrating psycholinguistic experimental results and supporting easy experimental design starting from the test suite format.",3,2020
2020.acl-demos.10.txt,"syntaxgym is continually evolving: we plan to add new features to the site, and to develop further in response to user feedback.",1,2020
2020.acl-demos.10.txt,"we also plan to further incorporate language models into the lm-zoo tool, allowing broader access to state-of-the-art language models in general.",1,2020
2020.acl-demos.14.txt,"for future work, we consider the following areas of improvement in the near term: • models downloadable in sta n z a are largely trained on a single dataset.",1,2020
2020.acl-demos.14.txt,the amount of computation and resources available to us is limited.,2,2020
2020.acl-demos.14.txt,"to make models robust to many different genres of text, we would like to investigate the possibility of pooling various sources of compatible data to train “default” models for each language",1,2020
2020.acl-demos.14.txt,"we would like to further investigate reducing model sizes and speeding up computation in the toolkit, while still maintaining the same level of accuracy.• we would also like to expand sta n z a ’s functionality by adding other processors such as neural coreference resolution or relation extraction for richer text analytics.",1,2020
2020.acl-demos.16.txt,"we will extend mt-dnn to support natural language generation tasks, e.g. question generation, and incorporate more pre-trained encoders, e.g. t5 (raffel et al., 2019) in future.",1,2020
2020.acl-demos.17.txt,"an interesting direction to explore is re-ranking corrective suggestions, so that the suggestion more relevant to the original sentence goes to the top.",1,2020
2020.acl-demos.17.txt,many avenues exist for future research and improvement of our system.,1,2020
2020.acl-demos.17.txt,yet another direction of research would be to detect fine-grained error types.,1,2020
2020.acl-demos.18.txt,"our case study on the wmt2019 metrics shared task further highlights the potential of clir as a proxy task for mt evaluation, and we hope that clireval can facilitate future research in this area.",3,2020
2020.acl-demos.19.txt,we hope that convlab-2 is instrumental in promoting the research on task-oriented dialogue.,4,2020
2020.acl-demos.2.txt,"for example, its usability in generation tasks such as machine translation has not been tested.",3,2020
2020.acl-demos.2.txt,"in the future, we aim to integrate neural architecture search into the toolkit to automate the searching for model structures.",1,2020
2020.acl-demos.2.txt,we will keep adding more examples and tests to expand textbrewer’s scope of application.,2,2020
2020.acl-demos.20.txt,"additionally, we would like to explore opusfilter’s use in different scenarios and for other language pairs.",4,2020
2020.acl-demos.20.txt,especially interesting would be the application in low-resource settings and various levels of noise in the original data.,2,2020
2020.acl-demos.20.txt,"furthermore, the use for domain adaptation and data selection should be further explored.",2,2020
2020.acl-demos.20.txt,"in future work, we would like to extend the toolbox with additional filters and classification options.",1,2020
2020.acl-demos.24.txt,"we also plan to improve the performance of core models in photon, such as semantic parsing (text-to-sql), response generation (table-to-text) and context-aware user interaction (text-to-text).",1,2020
2020.acl-demos.24.txt,"we will continue to add more features to photon, such as voice input, spelling checking, and visualizing the output when appropriate to inspect the translation process.",1,2020
2020.acl-demos.25.txt,"a promising direction is to use gui references to help with repairing conversational breakdowns (beneteau et al., 2019; ashktorab et al., 2019; myers et al., 2018) caused by incorrect semantic parsing, intent classification, or entity recognition.",1,2020
2020.acl-demos.25.txt,"another promising approach to enable more robust natural language understanding is to leverage the pre-trained generalpurpose language models (e.g., bert (devlin et al., 2018)) to encode the user instructions and the information extracted from app guis.5.3 extracting task semantics from guis an interesting future direction is to better extract semantics from app guis so that the user can focus on high-level task specifications and personal preferences without dealing with low-level mundane details (e.g., “buy 2 burgers” means setting the value of the textbox below the text “quantity” and next to the text “burger” to be “2”).",1,2020
2020.acl-demos.25.txt,"for itl, an interesting future challenge is to combine these user-independent domain-agnostic machine-learned models with the user’s personalized instructions for a specific task.",1,2020
2020.acl-demos.25.txt,we are currently exploring other ways of using multi-modal interactions to supplement natural language instructions in itl.,1,2020
2020.acl-demos.26.txt,"for future work, we plan to keep adding the state-of-the-art algorithms, reduce latency and fine-tune the implemented models on larger and/or more comprehensive corpus to improve performance.",1,2020
2020.acl-demos.28.txt,"in future, we would like to extend the funlines data collection setup to a more general crowdsourcing framework, for example, to collect style transfer data.",2,2020
2020.acl-demos.3.txt,"we intend to release the code as open source, as well as providing hosted open access to a pubmed-based corpus.",2,2020
2020.acl-demos.30.txt,detection and control of toxic output will be a major focus of future investigation.,1,2020
2020.acl-demos.30.txt,"dialogpt is fully opensourced and easy to deploy, allowing users to extend the pre-trained conversational system to bootstrap training using various datasets.",2,2020
2020.acl-demos.30.txt,we will investigate leveraging reinforcement learning to further improve the relevance of the generated responses and prevent the model from generating egregious responses.,1,2020
2020.acl-demos.32.txt,"in future work, we plan to add more media sources, especially from non-english media and regions.",2,2020
2020.acl-demos.32.txt,"we further want to extend the tool to support other propaganda techniques such as cherrypicking and omission, among others, which would require analysis beyond the text of a single article.",1,2020
2020.acl-demos.33.txt,we will try to utilize external resources to solve the few-shot and zero-shot problem in the future.,2,2020
2020.acl-demos.34.txt,"in the future, we will support more corpora and implement novel techniques to bridge the gap between end-to-end and cascaded approaches.",2,2020
2020.acl-demos.36.txt,"in future work, we will focus on expanding the database to include additional domains and article sources.",2,2020
2020.acl-demos.36.txt,"we will also seek to improve discovery performance by testing more recent text embedding methods (e.g., bert (devlin et al., 2018)) and by optimizing the search for different input text lengths, such as a whole document, a paragraph, or even a single sentence.",1,2020
2020.acl-demos.36.txt,"we will investigate whether query expansion techniques (azad and deepak, 2019) could mitigate this issue by suggesting or automatically appending semantically related keywords to the boolean filters.",1,2020
2020.acl-demos.36.txt,"we will work on augmenting the workflow with automated tasks, such as suggesting references as the author writes a manuscript, or notifying users about the latest publications relevant to their work.",1,2020
2020.acl-demos.37.txt,"as a next step, we will improve the prototype based on the participants’ valuable feedback.",1,2020
2020.acl-demos.37.txt,"last, we will investigate whether using the different modalities has an impact on cognitive load during pe (herbig et al., 2019b).",3,2020
2020.acl-demos.38.txt,"finally, we hope to explore further optimizations to make core algorithms competitive with highly-optimized neural network components.",1,2020
2020.acl-demos.38.txt,"in addition to the problems discussed so far, torch-struct also includes several other example implementations including supervised dependency parsing with bert, unsupervised tagging, structured attention, and connectionist temporal classification (ctc) for speech.",5,2020
2020.acl-demos.38.txt,"in the future, we hope to support research and production applications employing structured models.",1,2020
2020.acl-demos.39.txt,"using the cl machine teaching ui, the dialog author can provide corrections to the logged user-system dialogs and further improve the cl’s dm performance.",1,2020
2020.acl-demos.39.txt,"we are planning to extend this work by looking into following problems: 1) investigating effectiveness of different ranking algorithms for log correction recommendation, 2) optimizing number of training samples and action masks generated from the rule-based dm, and 3) improving predictions of hcn-based dm by looking into alternative network architectures.",5,2020
2020.acl-demos.4.txt,"in addition to improving the banned words selection process, future work on tabouid includes generating specific lists of cards based on school programs to use the game as an educational tool, using the category system of wikipedia to let users select more or less specific categories to play with, and adapting the algorithms to leverage the wide variety of languages wikipedia is available in beyond english and french.",1,2020
2020.acl-demos.40.txt,"there are many open questions which we intend to research, such as whether autoregressivity in neural sentence compression can be exploited and how to compose themes over longer time periods.",5,2020
2020.acl-demos.41.txt,we hope to encourage additional research to improve the safety and benefits of dietary supplements for their consumers.,6,2020
2020.acl-demos.42.txt,better training methods also allow us to fight the potential generation of noisy data due to inaccurate annotation recommendations.,1,2020
2020.acl-demos.42.txt,we hope that our work on lean-life will allow for researches and practitioners alike to more easily obtain useful labeled datasets and models for the various nlp tasks they face.,2,2020
2020.acl-demos.5.txt,future work include (1) expanding the database to more papers (2) improving the qa model using the collected data to better handle question answering in the context of research domain.,2,2020
2020.acl-demos.9.txt,"moving forward, we hope to refine the linking of extracted snippets to structured vocabularies to run a more comprehensive user-study to evaluate the use of the system in practice by different types of users.",3,2020
2020.acl-main.1.txt,"in future work, we intend to curate a test set with data from separate sources, which can serve as a benchmark for the models we study.",2,2020
2020.acl-main.1.txt,"the combined test set scores are more directly comparable, but ideally, we would like to compare the generalization of both models on an independent test set.",3,2020
2020.acl-main.1.txt,we also intend to use tools for interpreting the knowledge encoded in neural networks (such as diagnostic classifiers and representational similarity analysis) to investigate the emergent representation of linguistic units such as phonemes and words.,1,2020
2020.acl-main.1.txt,we intend to explore how a curriculum of cds followed by ads affects learning trajectories and outcomes.,5,2020
2020.acl-main.100.txt,"in the future, we are interested in injecting knowledge into text representation learning (cao et al., 2017, 2018b) for deeply understanding expert language, and will help to generate knowledgeenhanced questions (pan et al., 2019) for laymen.",1,2020
2020.acl-main.102.txt,"since dynamic memory can be a learning mechanism more general than what we have used here for fewshot learning, we will investigate this type of models in other learning problems.",1,2020
2020.acl-main.103.txt,"besides, we also propose a soft and a hard exclusion mechanisms to enhance the diversity of the generated keyphrases.",1,2020
2020.acl-main.103.txt,one interesting future direction is to explore whether the beam search is helpful to our model.,1,2020
2020.acl-main.105.txt,other retrieval tasks may also benefit from using keyphrase information and we expect our results to serve as a basis for further improvements.,1,2020
2020.acl-main.11.txt,"in future work, we plan to experiment with multi-domain span extraction architectures.",3,2020
2020.acl-main.110.txt,"in this scenario, automation strategies, such as natural language generation, are necessary to help ngo operators in their countering effort.",1,2020
2020.acl-main.111.txt,"its goal is to drive the development of better nlu models, so careful selection of tasks was crucial.",1,2020
2020.acl-main.111.txt,we leave it as future work.,6,2020
2020.acl-main.111.txt,we plan to continue the work on herbert and use the klej benchmark to guide its development.,3,2020
2020.acl-main.112.txt,"a promising direction would be to combine a multilingual sense inventory such as babelnet (navigli and ponzetto, 2012) with sense embeddings (camacho-collados and pilehvar, 2018).",2,2020
2020.acl-main.112.txt,future work will have to deepen the way we deal with word sense ambiguity by way of exchanging the simplifying type-level approach our current work is based on with a semantically more informed sense-level approach.,1,2020
2020.acl-main.113.txt,"finally, we would like to test this approach for comparing different mt systems.",3,2020
2020.acl-main.113.txt,"first, we plan to test whether similar observations will hold for more language pairs and text domains.",3,2020
2020.acl-main.113.txt,"second, the score combination strategies could be improved by learning weights for each component.",1,2020
2020.acl-main.113.txt,this work can be extended in numerous ways.,6,2020
2020.acl-main.116.txt,a natural next step is to combine the datasets in a multi-task setting to investigate to what extent models can profit from combining the information annotated in the respective datasets.,2,2020
2020.acl-main.116.txt,"further research will investigate the joint modeling of entity extraction, typing and experiment frame recognition.",1,2020
2020.acl-main.116.txt,"in addition, there are also further natural language processing tasks that can be researched using our dataset.",2,2020
2020.acl-main.117.txt,we consider techqa to be a stepping stone on which to build future data collections and leaderboards.,2,2020
2020.acl-main.117.txt,"we envision a roadmap where future releases of techqa will require synergy between multiple ai disciplines, from deep-learning based mrc to reasoning, knowledge base acquisition, and causality detection.",4,2020
2020.acl-main.117.txt,we plan on releasing questions with answers in a broader and more diverse collection that will include documents with a less formulaic structure than the technotes.,2,2020
2020.acl-main.118.txt,we believe this dataset will allow future work in sarcasm detection to progress in a setting free of the noise found in existing datasets.,2,2020
2020.acl-main.119.txt,an interesting future work is to make the number of inference steps adaptive to input sentences.,1,2020
2020.acl-main.120.txt,important challenges for future work include how to scale deep learning methods to such large amounts of source documents and how to close the gap to the oracle methods.,5,2020
2020.acl-main.120.txt,"we conducted extensive experiments to establish baseline results, and we hope that future work on mds will use this dataset as a benchmark.",3,2020
2020.acl-main.120.txt,we hope this dataset will facilitate the creation of real-world mds systems for use cases such as summarizing news clusters or search results.,2,2020
2020.acl-main.121.txt,"besides, we will also explore the influence of probabilistic bilingual lexicon obtained by learning only from monolingual data on our method.",1,2020
2020.acl-main.121.txt,"in our future work, we consider incorporating our method into the multi-task method.",1,2020
2020.acl-main.122.txt,"potential future directions include a more principled use of our proposed heuristic for detecting content relevant to specific dates, the use of abstractive techniques, a more effective treatment of the redundancy challenge, and extending the new dataset with multiple sources.",1,2020
2020.acl-main.123.txt,"in the future, we explore a more sophisticated method to improve the relevance and truthfulness of generated headlines, for example, removing only deviated spans in untruthful headlines rather than removing untruthful headlines entirely from the supervision data.",1,2020
2020.acl-main.123.txt,"moreover, it will be also interesting to see whether the same issue occurs in other related tasks such as data-to-text generation.",4,2020
2020.acl-main.125.txt,"for future work, we intend to apply our method to other transformer-based summarization models.",4,2020
2020.acl-main.127.txt,"in the future, we will adapt the method to more neural models especially the generation-based methods for the dialog system.",1,2020
2020.acl-main.129.txt,"in future, we want to continue to investigate the possibility of using even weaker demonstrations.",3,2020
2020.acl-main.13.txt,we will explore cross-lingual transfer learning for supporting more languages.,2,2020
2020.acl-main.130.txt,we hope that this dataset facilitates future research on multi-turn conversation reasoning problem.,2,2020
2020.acl-main.131.txt,"for future work, we would like to extend receiver to conversational recommender systems.",1,2020
2020.acl-main.133.txt,"for future work, we would like to deeply study the impacts of our perturbations on the coherence of the examined dialogues.",3,2020
2020.acl-main.133.txt,we will also investigate to what extent the rankings of dialogues obtained by our model correlate with human-provided rankings.,1,2020
2020.acl-main.135.txt,"although dp-based and srl-based semantic parsing are widely used, more advanced semantic representations could also be explored, such as discourse structure representation (van noord et al., 2018; liu et al., 2019b) and knowledge graph-enhanced text representations (cao et al., 2017; yang et al., 2019).",1,2020
2020.acl-main.135.txt,there are at least two potential future directions.,6,2020
2020.acl-main.138.txt,future work will involve improvements in the proposed noise model to study the importance of fidelity to real-world error patterns.,1,2020
2020.acl-main.138.txt,"moreover, we plan to evaluate nat on other real noise distributions (e.g., from asr) and other sequence labeling tasks to support our claims further.",3,2020
2020.acl-main.139.txt,"future work will investigate how to take into account potential correlations between labelling functions in the aggregation model, as done in e.g.(bach et al., 2017).",1,2020
2020.acl-main.139.txt,we also wish to evaluate the approach on other types of sequence labelling tasks beyond named entity recognition.,3,2020
2020.acl-main.140.txt,"in future work, we want to extend the probing tasks to also cover specific linguistic patterns such as appositions, and also investigate a model’s ability of generalizing to specific entity types, e.g.company and person names.",1,2020
2020.acl-main.141.txt,there are multiple avenues for future work.,6,2020
2020.acl-main.142.txt,"to improve the evaluation accuracy and reliability of future re methods, we provide a revised, extensively relabeled tacred.",3,2020
2020.acl-main.144.txt,"as regards distributed representations, we plan to study alternative networks to more accurately model the identification and incorporation of additional context.",1,2020
2020.acl-main.144.txt,"in our future work, we plan to optimise the thresholds used with the retrieval algorithms in order to more intelligently select those translations providing richest information to the nmt model and generalize the use of edit distance on the target side.",1,2020
2020.acl-main.144.txt,"we would also like to explore better techniques to inject information of small-size n-grams with possible convergence with terminology injection techniques, unifying framework where target clues are mixed with source sentence during translation.",1,2020
2020.acl-main.145.txt,"in future work, we will extend our analysis to include additional source and target languages from different language families, such as more asian languages.",2,2020
2020.acl-main.145.txt,"we will also work towards improving the training efficiency of character-level models, which is one of their main bottlenecks, as well as towards improving their effectiveness in multilingual training.",1,2020
2020.acl-main.148.txt,"in the future, we will develop lightweight alternatives to lalt to reduce the number of model parameters.",1,2020
2020.acl-main.148.txt,"we release opus-100, a multilingual dataset from opus including 100 languages with around 55m sentence pairs for future study.",2,2020
2020.acl-main.149.txt,"in future work, we plan to extend this analysis across more translation pairs, more diverse languages and multiple domains, as well as investigating the effect of translationese or source-side grammatical errors (anastasopoulos, 2019).",2,2020
2020.acl-main.151.txt,"first, they portray the viability of referencefree mt evaluation and warrant wider research efforts in this direction.",3,2020
2020.acl-main.152.txt,more research is needed on this problem given the prevalent usage of nmt.,6,2020
2020.acl-main.153.txt,"in the future, we plan to extend the cross-lingual position encoding to non-autoregressive mt (gu et al., 2018) and unsupervised nmt (lample et al., 2018).",1,2020
2020.acl-main.155.txt,"as a next step, we will integrate the participants’ valuable feedback to improve the prototype.",2,2020
2020.acl-main.155.txt,"while the presented study provided interesting first insights regarding participants’ use of and preferences for the implemented modalities, it did not allow us to see how they would use the modalities over a longer time period in day-to-day work, which we also want to investigate in the future.",5,2020
2020.acl-main.157.txt,"also, the multi-domain nature of the dataset enables future research in cross-target and cross-domain adaptation, a clear weak point of current models according to our evaluations.",3,2020
2020.acl-main.157.txt,"future research directions might explore the usage of transformer-based models, as well as of models which exploit not only linguistic but also network features, which have been proven to work well for existing stance detection datasets (aldayel and magdy, 2019).",1,2020
2020.acl-main.158.txt,this work addresses multiple open questions about syntactic evaluations and their relationship to other language model assessments.,5,2020
2020.acl-main.160.txt,one possibility would be to include infelicitous “colorless green ideas” sentences with grammatical syntax (cf.,1,2020
2020.acl-main.161.txt,a more sophisticated model would incorporate similar ideas.,1,2020
2020.acl-main.161.txt,"additional annotations, such as how certain readers are about the outcome of the story, may also be helpful in better understanding the relationship between suspense and uncertainty.",2,2020
2020.acl-main.161.txt,"automated interpretability methods as proposed by sundararajan et al.(2017), could shed further light on models’ predictions.",1,2020
2020.acl-main.161.txt,it provides a springboard to further interesting applications and research on suspense in storytelling.,4,2020
2020.acl-main.162.txt,we hope this resource can benefit future research into developing techniques to model and understand human responses to document sized text.,1,2020
2020.acl-main.163.txt,"in future, we aim to refine our generative model to better emphasise this difference of the two tasks.",1,2020
2020.acl-main.164.txt,we therefore suggest three prongs for future research: 1.,6,2020
2020.acl-main.165.txt,"moreover, we remark that our approach can be extended to other multi-domain or multi-task nlp problems.",4,2020
2020.acl-main.166.txt,"in the future, we will investigate how to extend the cg to support hierarchical topic management in conversational systems.",1,2020
2020.acl-main.167.txt,future work will focus on incorporating better encoding of the amr graph into the current system and exploring data augmentation techniques leveraging the proposed approach.,1,2020
2020.acl-main.170.txt,future research directions include adaptive dropout rates for different merges and an in-depth analysis of other pathologies in learned token embeddings for different segmentations.,1,2020
2020.acl-main.171.txt,"other work in nmt has examined this issue in the context of backtranslation (e.g., edunov et al.(2018)), and we expect the conclusions to be similar in the nar-mt case.",1,2020
2020.acl-main.171.txt,there are several open questions to investigate: are the benefits of monolingual data orthogonal to other techniques like iterative refinement?,5,2020
2020.acl-main.171.txt,we will consider these research directions in future work.,6,2020
2020.acl-main.174.txt,"although currently our approach relies solely on textual information, it would be interesting to incorporate additional modalities such as video or audio.",2,2020
2020.acl-main.174.txt,"besides narrative structure, we would also like to examine the role of emotional arcs (vonnegut, 1981; reagan et al., 2016) in a screenplay.",3,2020
2020.acl-main.175.txt,"in the future, we would like to model aspects and sentiment more explicitly as well as apply some of the techniques presented here to unsupervised single-document summarization.",1,2020
2020.acl-main.175.txt,our key insight is to enable the use of supervised techniques by creating synthetic review-summary pairs using noise generation methods.,1,2020
2020.acl-main.177.txt,our analyses contain two ideas that may be useful for future studies of systematicity.,1,2020
2020.acl-main.179.txt,future work needs to be done to understand more fully what biases are present in the data and learned by language models.,1,2020
2020.acl-main.180.txt,the study suggests several directions for future work.,6,2020
2020.acl-main.181.txt,it is an open question how to implement ig for these postor mixed-placement adjectives; one possibility is to measure the information gained when the set of adjectives associated to a noun an is partitioned by an adjective a.,5,2020
2020.acl-main.182.txt,we hope the dataset we release will be used to benchmark future dialog system uncertainty research.,2,2020
2020.acl-main.183.txt,"one natural extension would be to generalize these findings to other skills than the three addressed here, such as humor/wit, eloquence, image commenting, etc.",4,2020
2020.acl-main.184.txt,"in future, we plan to explore how to combine knowledge with pre-trained language models, e.g.",1,2020
2020.acl-main.186.txt,"as future work, we are extending the proposed approach and test its efficacy on real human conversations.",3,2020
2020.acl-main.186.txt,"more broadly, we continue to explore strategies that combine semantic parsing and neural networks for frame generation.",1,2020
2020.acl-main.187.txt,"future work can explore improving the correction models, leveraging logs of natural language feedback to improve text-to-sql parsers, and expanding the dataset to include multiple turns of correction.",1,2020
2020.acl-main.190.txt,extending expbert to other natural language tasks where this relationship might not hold is an open problem that would entail finding different ways of interpreting an explanation with respect to the input.,4,2020
2020.acl-main.190.txt,"however, more work will need to be done to make this approach more broadly applicable.",4,2020
2020.acl-main.190.txt,recent progress in general-purpose language representation models like bert open up new opportunities to incorporate language into learning.,1,2020
2020.acl-main.190.txt,we outline two such avenues of future work.,6,2020
2020.acl-main.191.txt,"moreover, we will investigate the potential impact of the adversarial training directly in the bert pre-training.",3,2020
2020.acl-main.191.txt,"this first investigation paves the way to several extensions including adopting other architectures, such as gpt-2 (radford et al., 2019) or distilbert (sanh et al., 2019) or other tasks, e.g., sequence labeling or question answering.",1,2020
2020.acl-main.192.txt,future directions include (1) devising hierarchical span representations that can handle spans of different length and diverse content more effectively and efficiently; (2) robust multitask learning or meta-learning algorithms that can reconcile very different tasks.,1,2020
2020.acl-main.193.txt,the proposed learning framework also shows promising results on other nlp tasks like text classification.,4,2020
2020.acl-main.194.txt,"for future direction, we plan to explore the effectiveness of mixtext in other nlp tasks such as sequential labeling tasks and other real-world scenarios with limited labeled data.",4,2020
2020.acl-main.195.txt,we believe our findings are generic and can be applied to other model compression problems.,4,2020
2020.acl-main.196.txt,"thus, we encourage future research into obtaining tighter bounds on latent lm perplexity, possibly by using more powerful proposal distributions that consider entire documents as context, or by considering methods such as annealed importance sampling.",1,2020
2020.acl-main.196.txt,we investigate the application of importance sampling to evaluating latent language models.,3,2020
2020.acl-main.197.txt,our proposed fine-tuning framework can be generalized to solve other transfer learning problems.,4,2020
2020.acl-main.197.txt,we will explore this direction as future work.,6,2020
2020.acl-main.199.txt,"in the future, we would like to figure out different strategies to merge individual gains, obtained by separate application of the dag constraint, into a setup that can take the best of both precision and recall improvements, and put forth a better performing system.",1,2020
2020.acl-main.199.txt,we also plan on looking into strategies to improve recall of the constructed taxonomy.,1,2020
2020.acl-main.2.txt,future work can further investigate temporal patterns in how language used by depressed people evolves over the course of an interaction.,1,2020
2020.acl-main.2.txt,we hope that this combination will encourage the research community to make more progress in this direction.,6,2020
2020.acl-main.20.txt,"as future work, we plan to extend our qag model to a meta-learning framework, for generalization over diverse datasets.",1,2020
2020.acl-main.200.txt,"for industrial applications where there is a trade-off typically between accuracy and latency, our findings suggest it might be feasible to gain accuracy for faster models by collecting more training examples.",1,2020
2020.acl-main.201.txt,"there are other ways to fit the dictionary better; e.g., using a non-linear projection such as a neural network.",1,2020
2020.acl-main.201.txt,"therefore, future work should focus on downstream tasks instead of bli.",6,2020
2020.acl-main.201.txt,we leave the exploration of non-linear projections to future work.,1,2020
2020.acl-main.202.txt,"we perform extensive study of several distillation dimensions like the impact of unlabeled transfer set, embeddings and student architectures, and make interesting observations outlined in summary.",1,2020
2020.acl-main.203.txt,our findings point to future research opportunities to build stealthy authorship obfuscation methods.,1,2020
2020.acl-main.204.txt,"there are a few interesting questions left unanswered in this paper, which would provide interesting future research directions: (1) deebert’s training method, while maintaining good quality in the last off-ramp, reduces model capacity available for intermediate off-ramps; it would be important to look for a method that achieves a better balance between all off-ramps.(2) the reasons why some transformer layers appear redundant2 and why dee-bert considers some samples easier than others remain unknown; it would be interesting to further explore relationships between pre-training and layer redundancy, sample complexity and exit layer, and related characteristics.",5,2020
2020.acl-main.205.txt,"finally, we also open a path to study integration of knowledge into the decoding phase, which can benefit other tasks such as neural machine translation.",4,2020
2020.acl-main.206.txt,"therefore future work could include different sampling methods, generation of synthetic data, or training objectives which reward models which are less conservatively drawn to the middle of the scoring scale.",1,2020
2020.acl-main.207.txt,another item of future work is to develop better multitask approaches to leverage multiple signals of relatedness information during training.,1,2020
2020.acl-main.207.txt,including other information such as outgoing citations as additional input to the model would be yet another area to explore in future.,1,2020
2020.acl-main.207.txt,it would be interesting to initialize our model weights from more recent transformer models to investigate if additional gains are possible.,1,2020
2020.acl-main.208.txt,"to help the readers understand the bottleneck for code piece generation and point out important future directions, we randomly sampled 200 “hard” lines and manually analyzed why the generation fails by looking at the top 1 candidate of the model.",1,2020
2020.acl-main.21.txt,"besides, more powerful question clustering and coarse-to-fine generation scenarios are also worth exploration.",1,2020
2020.acl-main.21.txt,"finally, performing sqg on other types of inputs, e.g., images and knowledge graphs, is an interesting topic.",2,2020
2020.acl-main.21.txt,"for future works, the major challenge is generating more meaningful, informative but concise questions.",5,2020
2020.acl-main.211.txt,"however, a host of other options could be considered in future work.",6,2020
2020.acl-main.212.txt,"an alternative would be to create training sets that adequately represent a diverse range of linguistic phenomena; crowdworkers’ (rational) preferences for using the simplest generation strategies possible could be counteracted by approaches such as adversarial filtering (nie et al., 2019).",2,2020
2020.acl-main.216.txt,"finally, we will evaluate the multiresolution loss on larger datasets to analyze it’s regularizing effects.",3,2020
2020.acl-main.216.txt,"furthermore, we will experiment with more ellaborate, attention-based fusion mechanisms.",1,2020
2020.acl-main.216.txt,"in the future, we plan to alleviate this by incorporating ideas from sparse transformer variants (kitaev et al., 2020; child et al., 2019).",1,2020
2020.acl-main.218.txt,we plan to continue our data-driven approach for grounded conversations by expanding our dataset through our iterative data collection process with other larger text-based open-domain dialogue corpora and extend our work to model and collect longer conversations exhibiting more complex improv-backed turns.,2,2020
2020.acl-main.219.txt,"the next challenge will also be to combine this engagingness with other skills, such as world knowledge (antol et al., 2015) relation to personal interests (zhang et al., 2018), and task proficiency.",5,2020
2020.acl-main.219.txt,"while our human evaluations were on short conversations, initial investigations indicate the model as is can extend to longer chats, see appendix g, which should be studied in future work.",3,2020
2020.acl-main.220.txt,"since it provides immediate continuous rewards and at the singlestep level, maude can be also be used to optimize and train better dialogue generation models, which we want to pursue as future work.",1,2020
2020.acl-main.221.txt,"with the advent of commercial sds systems that attempt to engage users over extended multi-turn interactions (e.g.(zhou et al., 2018)) generating realistic response behaviors is a potentially desirable addition to the overall experience.",1,2020
2020.acl-main.222.txt,"for example, longer conversations involving memory (moon et al., 2019), or mixing open-domain conversation with task oriented goals.",2,2020
2020.acl-main.222.txt,"future work should consider adding these tasks to the ones used here, while continuing the quest for improved models.",1,2020
2020.acl-main.223.txt,"first of all, we would like to experiment with different neural network architectures.",1,2020
2020.acl-main.223.txt,"secondly, we would like to incorporate further poetic devices, especially those based on meaning.",1,2020
2020.acl-main.223.txt,we conclude with a number of future research avenues.,6,2020
2020.acl-main.224.txt,future work will validate the effectiveness of this method on more varied data-to-text generation tasks.,3,2020
2020.acl-main.225.txt,"in future work, we plan to incorporate these features into co-creation systems which assist humans in the writing process.",1,2020
2020.acl-main.226.txt,"one can try to use a variational autoencoder (kingma and welling, 2014) instead.",1,2020
2020.acl-main.226.txt,"since sentence infilling is analogous to masked language modeling, we expect that it can also be used as a pre-training task.",1,2020
2020.acl-main.228.txt,"by performing analysis on gigaword, we find that there exists room to improve summarization performance with better post-ranking algorithms, a promising direction for future research.",1,2020
2020.acl-main.228.txt,"moving forward, we would like to apply this framework to other retrieve-and-edit based generation scenarios such as dialogue, conversation, and code generation.",4,2020
2020.acl-main.229.txt,"for instance, developing agents that are robust to variations in both visual appearance and instruction descriptions is an important next step.",1,2020
2020.acl-main.229.txt,there are a few future directions to pursue.,6,2020
2020.acl-main.230.txt,"in the future, we aim to extend our framework to extract events from videos, and make it scalable to new event types.",1,2020
2020.acl-main.230.txt,we also develop a novel multimedia structured common space construction method to take advantage of the existing image-caption pairs and singlemodal annotated data for weakly supervised training.,1,2020
2020.acl-main.230.txt,"we plan to expand our annotations by including event types from other text event ontologies, as well as new event types not in existing text ontologies.",2,2020
2020.acl-main.230.txt,"we will also apply our extraction results to downstream applications including cross-media event inference, timeline generation, etc.",4,2020
2020.acl-main.231.txt,"future work might explore methods for incorporating richer learned representations both of the diverse visual observations in videos, and the narration that describes them, into such models.",1,2020
2020.acl-main.231.txt,we hope that future work will continue to evaluate broadly.,3,2020
2020.acl-main.234.txt,"our empirical analysis is related to htut et al.(2018), who methodologically, and successfully replicate the results of shen et al.(2018a) to study their performance.",3,2020
2020.acl-main.234.txt,"studying this type of difference between expressive models and their less expressive, restricted variants remains an important direction for future work.",1,2020
2020.acl-main.235.txt,further study in this direction may be interesting.,6,2020
2020.acl-main.236.txt,"we hope this work inspires future research on better understanding the differences between embedding methods, and on designing simpler and more efficient models.",1,2020
2020.acl-main.237.txt,a potential future research direction is to bridge the gap between this simple bootstrapping paradigm and the incorporation of user free-form responses to allow the system to handle free-text responses.,5,2020
2020.acl-main.237.txt,our modeling choices enable the system to perform zero-shot generalization to unseen classification targets and questions.,1,2020
2020.acl-main.237.txt,"we hope our work will encourage more research on different possibilities of building interactive systems that do not necessarily require handling full-fledged dialogue, but still benefit from user interaction.",1,2020
2020.acl-main.238.txt,"in the future, we would like to jointly learn discrete representations of entities as well as relations.",1,2020
2020.acl-main.238.txt,our approaches learn to represent entities in a kg as a vector of discrete codes in an end-to-end fashion.,1,2020
2020.acl-main.240.txt,"future work could find additional modular uses of mlms, simplify maskless pll computations, and use plls to devise better sentence- or document-level scoring metrics.",1,2020
2020.acl-main.242.txt,"the theoretical underpinnings of our poscal idea are not explored in detail here, but developing formal statistical support for these ideas constitutes interesting future work.",1,2020
2020.acl-main.243.txt,another direction is to improve the sources of weak supervision and such as interactive new constraints provided by users.,1,2020
2020.acl-main.243.txt,"finally, it would be interesting to explore alternative training methods for these models, such as reducing reliance on hard sampling through better relaxations of structured models.",1,2020
2020.acl-main.243.txt,induction of grounded control states opens up many possible future directions for this work.,1,2020
2020.acl-main.245.txt,"edizel et al.(2019) attempt to learn typo-resistant word embeddings, but focus on common typos, rather than worst-case typos.",1,2020
2020.acl-main.245.txt,"in computer vision, chen et al.(2019) discretizes pixels to compute exact robust accuracy on mnist, but their approach generalizes poorly to other tasks like cifar-10.",4,2020
2020.acl-main.245.txt,"other attack surfaces involving insertion of sentences (jia and liang, 2017) or syntactic rearrangements (iyyer et al., 2018) are harder to pair with roben, and are interesting directions for future work.",5,2020
2020.acl-main.245.txt,"using context is not fundamentally at odds with the idea of robust encodings, and making contextual encodings stable is an interesting technical challenge and a promising direction for future work.",5,2020
2020.acl-main.247.txt,"in future work, we will address end-to-end question answering with pre-training for both the answer selection and retrieval components.",1,2020
2020.acl-main.248.txt,our future work is to include the paragraph representation in the constraint prediction model.,1,2020
2020.acl-main.248.txt,this will help our methodology to have the benefit of making informed decision while also solving constraints.,1,2020
2020.acl-main.251.txt,"in the future, we seek to expand upon energy-based translation using our method.",1,2020
2020.acl-main.252.txt,"future work should explore techniques like iterative back-translation (hoang et al., 2018) for further improvement and scaling to larger model capacities and more languages (arivazhagan et al., 2019b; huang et al., 2019) to maximize transfer across languages and across data sources.",1,2020
2020.acl-main.253.txt,"in the future, we plan to investigate more thoroughly the use of language models for evaluating fluency, the effect of domain mismatch in the choice of monolingual data, and ways to generalize this study to other applications beyond mt.",3,2020
2020.acl-main.255.txt,we leave it as future work to explore ways to raise accuracy on unseen synsets without harming performances on frequent synsets.,1,2020
2020.acl-main.257.txt,in future work we will investigate more sophisticated neural (sub-)networks within the proposed framework.,1,2020
2020.acl-main.257.txt,"we will also apply the idea of functionspecific training to other interrelated linguistic phenomena and other languages, probe the usefulness of function-specific vectors in other language tasks, and explore how to integrate the methodology with sequential models.",4,2020
2020.acl-main.259.txt,"directionality of edges did not result in improvement in our models in this work, however for future, we plan to develop gcns that incorporate edge typing, which would enable us to differentiate between different mwe types and dependency relations while comparing them against the current models.",1,2020
2020.acl-main.259.txt,"for future work, we plan to add vmwe annotations to the vu amsterdam corpus (steen, 2010) which is the largest metaphor dataset and extend our experiments using that resource.",2,2020
2020.acl-main.260.txt,we encourage researchers to look at languages with different grammatical gender (such as czech and slovak) and propose new methods to reduce the bias in multilingual embeddings as well as in cross-lingual transfer learning.,2,2020
2020.acl-main.260.txt,we hope this study can work as a foundation to motivate future research about the analysis and mitigation of bias in multilingual embeddings.,2,2020
2020.acl-main.261.txt,looking to other scientific disciplines that have faced similar issues in the past may provide some guidance for our future.,6,2020
2020.acl-main.264.txt,"however, a comprehensive study is required to prove the conjecture and we leave this as future work.",6,2020
2020.acl-main.265.txt,"additionally, future work should further probe the source of gender bias in the model’s predictions, perhaps by visualizing attention or looking more closely at the model’s outputs.",1,2020
2020.acl-main.265.txt,we encourage future work to dive deeper into this problem.,6,2020
2020.acl-main.265.txt,"we only consider binary gender, but future work should consider non-binary genders.",1,2020
2020.acl-main.265.txt,"while these findings will help future work avoid gender biases, this study is preliminary.",6,2020
2020.acl-main.267.txt,"first, we plan to explore why the model prefers “soft” attentions rather than “hard” ones, which is different from the findings in several prior works based on hard attention.",1,2020
2020.acl-main.267.txt,"in our future work, we will explore several potential directions.",6,2020
2020.acl-main.267.txt,"instead of using a fixed pooling norm for universal text representation learning, we propose to learn the norm in an end-to-end framework to automatically find the optimal ones for learning text representations in different tasks.",1,2020
2020.acl-main.267.txt,"second, we plan to study how to model the differences on the characteristics of different samples and use different pooling norms, which may have the potential to further improve our approach.",1,2020
2020.acl-main.267.txt,"third, we will explore how to generalize our approach to other modalities, such as images, audios and videos, to see whether it can facilitate more attention-based methods.",4,2020
2020.acl-main.268.txt,"in future work, apart from improving the similarity measures, it could be examined to predict mtl scores or estimate the right amount of auxiliary data or shared parameters in the neural network.",1,2020
2020.acl-main.269.txt,"future directions include validating our findings on other san architectures (e.g., bert (devlin et al., 2019)) and more general attention models (bahdanau et al., 2015; luong et al., 2015).",3,2020
2020.acl-main.27.txt,"as future work, we will extend our framework to more complex contexts by devising efficient learning algorithms.",1,2020
2020.acl-main.270.txt,"by showing that sublayer ordering can improve models at no extra cost, we hope that future research continues this line of work by looking into optimal sublayer ordering for other tasks, such as translation, question answering, and classification.",1,2020
2020.acl-main.271.txt,"the proposed method is not limited to the two aforementioned tasks, but can be applied to any nlp as well as other tasks such as machine translation and image recognition.",4,2020
2020.acl-main.273.txt,"besides, how to introduce scene graphs into multi-modal nmt is a worthy problem to explore.",5,2020
2020.acl-main.273.txt,"finally, we will apply our model into other multi-modal tasks such as multi-modal sentiment analysis.",4,2020
2020.acl-main.273.txt,"in the future, we plan to incorporate attributes of visual objects and dependency trees to enrich the multi-modal graphs.",1,2020
2020.acl-main.276.txt,the riemannian framework allows to exploit the geometry of the doubly stochastic manifold.,1,2020
2020.acl-main.278.txt,"as well-calibrated confidence estimation is more likely to establish trustworthiness with users, we plan to apply our work to interactive machine translation scenarios in the future.",4,2020
2020.acl-main.278.txt,"through a series of in-depth analyses, we report several interesting findings which may help to analyze, understand and improve nmt models.",1,2020
2020.acl-main.279.txt,"in the future, we plan to enable the glyph and phonetic variation detection by integrating the variation graph representation learning, which may improve signal’s performance.",1,2020
2020.acl-main.28.txt,"in the future, we plan to apply the sa framework on syntactic parse trees in hopes of generating more syntactically different sentences (motivated by our case study).",4,2020
2020.acl-main.280.txt,"in the future, we plan to study complicated situations such as a law case with multiple defendants and charges.",5,2020
2020.acl-main.282.txt,"we believe our method can also be applied to other tasks that need to exploit hierarchical label structure and label co-occurrence, such as fine-grained entity typing and hierarchical multi-label classification.",4,2020
2020.acl-main.283.txt,"as recent works explore the superiority of hyperbolic space to euclidean space for serval natural language processing tasks, we intend to couple with the hyperbolic neural networks (ganea et al., 2018b) and the hyperbolic word embedding method such as poincare´glove (tifrea et al., 2019) in the future.",1,2020
2020.acl-main.284.txt,"our future research direction includes a thorough study of differences in this dataset with actual tickets, and potential for transfer.",2,2020
2020.acl-main.284.txt,"we also study the performance of the most recent recurrent neural network-based approaches to sequence labelling, on this task.",1,2020
2020.acl-main.285.txt,promising future directions include: 1) utilize more types of data from mooccube to facilitate existing topics; 2) employ advanced models in existing tasks; 3) more innovative nlp application tasks in online education domain.,2,2020
2020.acl-main.288.txt,"we also develop two kinds of 2d transformers, i.e., window-constrained and cross-road 2d transformers, to further model the interaction of different emotion-cause pairs.",1,2020
2020.acl-main.289.txt,"finally, it would be interesting to study the semantic roles of emotion (bostan et al., 2020), which considers the full structure of an emotion expression and is more challenging.",1,2020
2020.acl-main.289.txt,"in future work, we shall explore the following directions.",6,2020
2020.acl-main.289.txt,"second, designing effective methods to inject appropriate linguistic knowledge into neural models is valuable to emotion analysis tasks (ke et al., 2019; zhong et al., 2019).",1,2020
2020.acl-main.29.txt,"having shown that segment bounds contain useful supervisory signal, it would be interesting to examine if segment hierarchies might also contain useful signal.",1,2020
2020.acl-main.29.txt,"s-lstm is agnostic as to the sentence encoder used, so we would like to investigate the potential usefulness of transformer-based language models as sentence encoders.",1,2020
2020.acl-main.29.txt,"there are additional engineering challenges associated with using models such as bert as sentence encoders, since encoding entire documents can be too expensive to fit on a gpu without model parallelism.",5,2020
2020.acl-main.29.txt,we would also like to investigate the usefulness of an unconsidered source of document structure: the hierarchical nature of sections and subsections.,3,2020
2020.acl-main.293.txt,"one can try to adapt our proposed csae architecture for an integrated approach by applying the unified tagging scheme; thereby, aspect extraction and sentiment classification can be achieved simultaneously.",1,2020
2020.acl-main.296.txt,"for future works, we will explore pair-wise at and ot extraction together with aspect category and sentiment polarity classification.",1,2020
2020.acl-main.298.txt,"for future work, we aim to develop a universal model to handle both tree and non-tree arguments.",1,2020
2020.acl-main.30.txt,"also, extending our method for other types of textual data, such as short texts, multi-lingual data, and code-switched data is a potential direction.",2,2020
2020.acl-main.30.txt,"in the future, we are interested in generalizing contextualized weak supervision to hierarchical text classification problems.",4,2020
2020.acl-main.303.txt,"finally, future work should investigate whether data augmentation can fully bridge the gap between low-bias learners and structured tree lstms, and whether our conclusions apply to other syntactic phenomena besides agreement.",5,2020
2020.acl-main.303.txt,future work should further explore both of these approaches.,1,2020
2020.acl-main.303.txt,it seems particularly promising to explore alternative formulations of the dependency lstm (as mentioned above) and the effect of learning embeddings of non-terminal symbols for the constituency lstm.,1,2020
2020.acl-main.305.txt,we also analyze our model’s outputs to get more insights into user interest dynamics.,1,2020
2020.acl-main.306.txt,there are several future directions for this work.,6,2020
2020.acl-main.306.txt,"therefore, our next step is to enhance umt so as to dynamically filter out the potential noise from images.",1,2020
2020.acl-main.308.txt,"finally, we plan to experiment with other languages.",2,2020
2020.acl-main.308.txt,"in future work, we plan to perform user profiling with respect to polarizing topics such as gun control (darwish et al., 2020), which can then be propagated from users to media (atanasov et al., 2019; stefanov et al., 2020).",1,2020
2020.acl-main.308.txt,"we further want to model the network structure, e.g., using graph embeddings (darwish et al., 2020).",1,2020
2020.acl-main.309.txt,alleviating this restriction is an important future direction.,6,2020
2020.acl-main.310.txt,this study shed light on understanding the behaviors of language encoders against grammatical errors and encouraged future work to enhance the robustness of these models.,1,2020
2020.acl-main.311.txt,immediate attention should be paid to the investigation of how heat maps would vary during the extensive pre-training so that we have a better understanding of the dynamics of the learning processes.,1,2020
2020.acl-main.312.txt,"moreover, we can also examine the influence of using deep contextualized input encoders such as elmo (peters et al., 2018) or bert (devlin et al., 2018).",3,2020
2020.acl-main.312.txt,there are some future directions that are worth exploring.,6,2020
2020.acl-main.313.txt,"in future work, we plan to extend r-men for multi-hop knowledge graph reasoning.",1,2020
2020.acl-main.315.txt,"furthermore, the word-aligned attention can also be applied to english plms to bridge the semantic gap between the whole word and the segmented word-piece tokens, which we leave for future work.",4,2020
2020.acl-main.317.txt,"in further work, we will explore more efficient ways for constructing the perturbation set.",1,2020
2020.acl-main.317.txt,"we also plan to generalize our approach to achieve certified robustness against other types of adversarial attacks in nlp, such as the out-of-list attack.",1,2020
2020.acl-main.318.txt,"in the future, we will consider combining our method with graph neural networks to update the word graphs we build.",1,2020
2020.acl-main.319.txt,"furthermore, we notice some exceptional cases which we call as “reinforced samples”, which we leave as the future work.",6,2020
2020.acl-main.32.txt,"besides, developing correlated topic modelsis another promising direction.",1,2020
2020.acl-main.32.txt,"in the future, we would like to devise a nonparametric neural topic model based on adversarial training.",1,2020
2020.acl-main.324.txt,"in the future, we intend to extend the work to include language types such as asian languages.",2,2020
2020.acl-main.324.txt,we will also introduce other effective methods to improve zero-shot translation quality.,1,2020
2020.acl-main.327.txt,"in future work, we will work around the problem of evaluation errors in the low da range.",3,2020
2020.acl-main.328.txt,a practical line of future work is embedding our plotting agent in interactive environments such as jupyter lab.,1,2020
2020.acl-main.328.txt,future work includes methods that get closer to human performance on the dataset.,1,2020
2020.acl-main.329.txt,"in future work, we would like to experiment with a multi-task setup wherein tasks with less training data can significantly benefit from those having abundant labelled data, since most code-switched datasets are often small and difficult to annotate.",2,2020
2020.acl-main.329.txt,"we hope that this will encourage researchers to test multilingual, cross-lingual and code-switched embedding techniques and models on this benchmark.",3,2020
2020.acl-main.329.txt,we would like to add more diverse tasks and language pairs to the gluecos benchmark in a future version.,2,2020
2020.acl-main.33.txt,"in future work, we will examine semantic relations between class labels in the auxiliary task.",1,2020
2020.acl-main.33.txt,"moreover, we will adapt our model to text generation tasks.",4,2020
2020.acl-main.331.txt,"besides news recommendation, the mind dataset can also be used in other natural language processing tasks such as topic classification, text summarization and news headline generation.",4,2020
2020.acl-main.331.txt,"in addition, besides the click behaviors, we plan to incorporate other user behaviors such as read and engagement to support more accurate user modeling and performance evaluation.",3,2020
2020.acl-main.331.txt,"in the future, we plan to extend the mind dataset by incorporating image and video information in news as well as news in different languages, which can support the research of multi-modal and multi-lingual news recommendation.",2,2020
2020.acl-main.331.txt,"many interesting researches can be conducted on the mind dataset, such as designing better news and user modeling methods, improving the diversity, fairness and explainability of news recommendation results, and exploring privacy-preserving news recommendation.",1,2020
2020.acl-main.332.txt,"in future work, we plan to extend this work to more datasets and to more languages.",2,2020
2020.acl-main.332.txt,"we further want to go beyond textual claims, and to take claimimage and claim-video pairs as an input.",2,2020
2020.acl-main.333.txt,it is our hope the proposed holistic metrics may pave the way towards the comparability of open-domain dialogue models.,3,2020
2020.acl-main.334.txt,"future work includes i) improving projection learning to model complicated linguistic properties of hypernymy; ii) extending our model to address other tasks, such as graded lexical entailment (vulic et al., 2017) and cross-lingual graded lexical entailment (vulic et al., 2019); and iii) exploring how deep neural language models (such as bert (devlin et al., 2019), transformer-xl (dai et al., 2019), xlnet (yang et al., 2019)) can improve the performance of hypernymy detection.",1,2020
2020.acl-main.335.txt,"for future work, an extrinsic evaluation of our methods is needed to prove the effectiveness of learned biomedical entity representations and to prove the quality of the entity normalization in downstream tasks.",3,2020
2020.acl-main.337.txt,"thus, in the future, we will take the uncertainty, polysemy, and context sensitivity of the word meanings and the frequency of words into account and explore better ways of modeling the word-class distributions in semantic vector spaces.",1,2020
2020.acl-main.338.txt,"in our future work, we would like to improve the performance of the asc task by using unlabeled data since our graph-based neural network approach is easy to add unlabeled data.",1,2020
2020.acl-main.338.txt,"moreover, we would like to apply our approach to other sentiment analysis tasks, e.g., aspect-oriented opinion summarization and multi-label emotion detection.",4,2020
2020.acl-main.339.txt,"in the future, we will develop a syntax-based multi-scale graph convolutional network to deal with both short and long aspects.",1,2020
2020.acl-main.34.txt,"in future work, we will investigate the impact of fine-grained word categories (such as nouns, verbs, and adjectives) on the translation performance and design specific methods according to these categories.",1,2020
2020.acl-main.341.txt,"for future work, we will extend sentibert to other applications involving phrase-level annotations.",4,2020
2020.acl-main.342.txt,"in the future, one possible direction is creating complete graphs with their nodes being input clauses to achieve full coverage.",1,2020
2020.acl-main.343.txt,"in the future, we will further explore the connection between multimodal analysis and multi-task learning and incorporate more fusion strategy, including early- and middle-fusion.",1,2020
2020.acl-main.344.txt,"besides, we expect the idea of curriculum pre-training can be adopted on other nlp tasks.",4,2020
2020.acl-main.344.txt,"in the future, we will explore how to leverage unlabeled speech data and large bilingual text data to further improve the performance.",2,2020
2020.acl-main.345.txt,we will investigate this direction in future work.,6,2020
2020.acl-main.346.txt,"in future work, we intend to explore the idea of self-training for parsing written texts.",2,2020
2020.acl-main.346.txt,"the first step is to develop parsing models that parse asr output, rather than speech transcripts.",1,2020
2020.acl-main.346.txt,we also aim at integrating syntactic parsing and self-training more closely with automatic speech recognition.,1,2020
2020.acl-main.348.txt,"finally, we will explore further the generability of our meta-transfer learning approach to more downstream multilingual tasks in our future work.",4,2020
2020.acl-main.350.txt,"for future work, we will design more flexible policies to achieve better translation quality and lower delay in simultaneous spoken language translation.",1,2020
2020.acl-main.351.txt,"we plan to do a detailed analysis along two lines: 1) comparing if the proposed modeling technique can help bridge gap between predicted and human annotations, and 2) effect of environment variables e.g., background noise, speaker features, different languages etc.",3,2020
2020.acl-main.353.txt,an exciting synthesis would incorporate deception and language generation into an agent’s policy; our data would help train such agents.,2,2020
2020.acl-main.354.txt,we believe this work opens a new competitive avenue in the area of implicit generative models for sequential data.,1,2020
2020.acl-main.355.txt,"as a potential direction for future work, it would be interesting to investigate the use of the ema technique on transformer models as well and conduct similar studies to examine needless architectural complexity in other nlp tasks.",1,2020
2020.acl-main.356.txt,"in the future, we plan to extend the applicability of the presented model to other linguistics tasks as well as recommendations and medical inference tasks.",4,2020
2020.acl-main.357.txt,future works include applying such scoring method on broader classification tasks like natural language inference and sentiment analysis.,4,2020
2020.acl-main.357.txt,"we also think that our token-level scoring method could be used during the self-supervised pretraining phase to extend traditional next sentence prediction and sequence ordering tasks, bringing more commonsense knowledge in the model.",4,2020
2020.acl-main.358.txt,"in the future, we plan to extend the key insights of segmenting features and facilitating interactions to other representation learning problems.",1,2020
2020.acl-main.359.txt,"in the future, we plan to investigate ways to directly incorporate the rescoring metrics into the data selection process itself, so that penalising similar sentences can also be taken into account.",1,2020
2020.acl-main.359.txt,we also aim to conduct a human evaluation of the translated sentences in order to obtain a better understanding of the effects of data selection and backtranslation on the overall quality.,3,2020
2020.acl-main.36.txt,"in the future, we will extend the investigation on the functionalities of the encoder and decoder to other sequence-to-sequence tasks such as text summarization and text style transfer to explore more applications of our model.",4,2020
2020.acl-main.360.txt,"in future work, we suggest performing a hyperparameter search over possible values for t in slt pruning (i.e., the number of training steps that are not discarded during model reset), and over si for the switch from slt to mp in slt-mp.",1,2020
2020.acl-main.361.txt,"as future work, we plan to extend our method to other nlp tasks which rely on evidence finding, such as natural language inference.",4,2020
2020.acl-main.362.txt,"for future work, we aim to consider more complex relationships among the quantities and other attributes to enrich quantity representations further.",1,2020
2020.acl-main.362.txt,we will also explore adding heuristic in the tree-based decoder to guide and improve the generation of solution expression.,1,2020
2020.acl-main.363.txt,future work includes the application of cem at scales other than the ordinal.,3,2020
2020.acl-main.364.txt,"we believe that our method can benefit simultaneously from other compression techniques, such as pruning (han et al., 2016) and low-precision representation (ling et al., 2016).",1,2020
2020.acl-main.364.txt,we leave this as an avenue for future work.,6,2020
2020.acl-main.365.txt,"in the future, we plan to investigate whether usage representations can provide an even finer grained account of lexical meaning and its dynamics, e.g., to automatically discriminate between different types of meaning change.",1,2020
2020.acl-main.365.txt,we expect our work to inspire further analyses of variation and change which exploit the expressiveness of contextualised word representations.,1,2020
2020.acl-main.368.txt,"furthermore, it would be interesting to explore more complex ways of incorporating surface-form information – e.g., by using a character-level cnn similar to the one of kim et al.(2016) – to balance out the potency of bertram’s form and context parts.",1,2020
2020.acl-main.368.txt,"in future work, we want to investigate bertram’s potential benefits for such frequent words.",1,2020
2020.acl-main.369.txt,"as future work, we plan to refine our approach by exploiting other strategies for weighting the words in the clusters and to leverage them for automatically building multilingual sense-tagged corpora.",1,2020
2020.acl-main.370.txt,"in the future, we would like to investigate the application of our theory in these domain adaptation tasks.",4,2020
2020.acl-main.370.txt,our purpose is to inject the target domain knowledge to bert and encourage bert to be domain-aware.,2,2020
2020.acl-main.370.txt,"the proposed post-training procedure could also be applied to other domain adaptation scenarios such as named entity recognition, question answering, and reading comprehension.",4,2020
2020.acl-main.371.txt,"finally, detecting the more implicit relations between the argument and the key point, as seen in our error analysis, is another intriguing direction for future work.",1,2020
2020.acl-main.371.txt,"in addition, we plan to apply the methods presented in this work also to automatically-mined arguments.",4,2020
2020.acl-main.371.txt,the natural next step for this work is the challenging task of automatic key point generation.,5,2020
2020.acl-main.372.txt,"future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains.",4,2020
2020.acl-main.373.txt,"in the future, we plan to develop more complex models to be added in the next stages of the cascade classifier as well as automatically identify irony, gender stereotypes and sexist vocabulary.",1,2020
2020.acl-main.374.txt,"in the future, we hope to apply skep on more sentiment analysis tasks, to further see the generalization of skep, and we are also interested in exploiting more types of sentiment knowledge and more fine-grained sentiment mining methods.",4,2020
2020.acl-main.375.txt,"lastly, given recent criticisms of probing approaches in nlp, it will be vital to revisit the insights produced here within a non-probing framework, for example, using representational similarity analysis (rsa) (chrupała and alishahi, 2019) over symbolic representations from treebanks and their encoded representations.",1,2020
2020.acl-main.376.txt,"in addition, any advances in seq2seq neural architectures or pretrained transformer-based language models (devlin et al., 2019) can be directly used to enhance our approach.",1,2020
2020.acl-main.38.txt,we also study the effects of deep decoders in addition to deep encoders extending previous works.,1,2020
2020.acl-main.38.txt,"we first investigate convergence differences between the published transformer (vaswani et al., 2017) and its official implementation (vaswani et al., 2018), and compare the differences of computation orders between them.",3,2020
2020.acl-main.381.txt,in future we will sample target models with a larger number of plausible combinations of factors.,1,2020
2020.acl-main.381.txt,in future work we hope to further disentangle these differences.,3,2020
2020.acl-main.382.txt,"future work will focus on developing more advanced procedures for detecting inconsistencies, and on building robust models that do not generate inconsistencies.",1,2020
2020.acl-main.383.txt,"we also extend our method to probe document structure, which sheds lights on bert’s effectiveness in modeling long sequences.",1,2020
2020.acl-main.383.txt,"we leave it for future work to use our technique to test other linguistic properties (e.g., coreference) and to extend our study to more downstream tasks and systems.",4,2020
2020.acl-main.384.txt,"future work should investigate where these primitive referential abilities stem from and how they can be fostered in future architectures and training setups for language modeling, and neural models more generally.",1,2020
2020.acl-main.384.txt,"we find that the two models behave similarly, but the transformer performs consistently better (around 10% higher accuracy in the probe tasks).8 future work should test other architectures, like cnn-based lms and lstms with attention, to provide additional insights into the linguistic capabilities of language models.",1,2020
2020.acl-main.385.txt,"following this work, we can build the attention graph with effective attention weights (brunner et al., 2020) instead of raw attentions.",1,2020
2020.acl-main.387.txt,"as future work, we would like to extend our analysis and proposed techniques to more complex models and downstream tasks.",1,2020
2020.acl-main.388.txt,"in the future, it would be fruitful to develop a novel weighting strategy for the tchebycheff procedure.",1,2020
2020.acl-main.39.txt,"this is a bottleneck for extrapolation, suggesting that removing this heuristic is key to reaching perfect extrapolation and should be investigated in future work.",5,2020
2020.acl-main.390.txt,additional use cases like the training of personalized recommendation models as well as the use of reinforcement learning to find a good trade-off between system and user objective remain to be investigated in future work.,1,2020
2020.acl-main.391.txt,"in future work, we will investigate whether bert-init can be used effectively by using methods to deal with catastrophic forgetting.",1,2020
2020.acl-main.393.txt,we hope to address this problem with a completely semantic-based approach in the future.,1,2020
2020.acl-main.394.txt,"for instance, we expect that abuse detection may also benefit from joint learning with complex semantic tasks, such as figurative language processing and inference.",1,2020
2020.acl-main.394.txt,"the mutually beneficial relationship that exists between these two tasks opens new research avenues for improvement of abuse detection systems in other domains as well, where emotion would equally play a role.",1,2020
2020.acl-main.395.txt,future adoptions to fuse will include the integration of a dialog component.,1,2020
2020.acl-main.395.txt,"it will be interesting to see, if we can reuse (or transfer) the machine learning models as well as the rest of the approach.",1,2020
2020.acl-main.395.txt,we plan to evaluate fuse in other domains.,3,2020
2020.acl-main.395.txt,we will also implement a sanity check that considers feasibility and meaningfulness of the sequence of actions in the method body.,3,2020
2020.acl-main.396.txt,"a limitation of our work is that we considered a narrow contextual context, comprising only the previous comment and the discussion title.11 it would be interesting to investigate in future work ways to improve the annotation quality when more comments in the discussion thread are provided, and also if our findings hold when broader context is considered (e.g., all previous comments in the thread, or the topic of the thread as represented by a topic model).",2,2020
2020.acl-main.396.txt,our experiments and datasets provide an initial foundation to investigate these important directions.,2,2020
2020.acl-main.397.txt,"we also propose to incorporate the latent graph into other multi-task learning problems (chen et al., 2019; kurita and søgaard, 2019).",1,2020
2020.acl-main.398.txt,"in future work we aim to extend the model to represent a database with multiple tables as context, and to effectively handle large tables.",1,2020
2020.acl-main.399.txt,"combining target inference with stance classification in future work, we can already generate basic conclusions, say, “raising the school leaving age is good”.",1,2020
2020.acl-main.40.txt,"in the future, we would like to extend our model to extremely large datasets, such as wmt’14 english-to-french with about 36m sentence-pairs.",2,2020
2020.acl-main.400.txt,"in future work, we would like to investigate the effectiveness of our model in these tasks.",1,2020
2020.acl-main.401.txt,"along with investigating new techniques, we hope that assembling a bigger curated dataset with quality annotations will help in better performance.",2,2020
2020.acl-main.402.txt,"in future, conversation history, speaker information, fine-grained modality encodings can be incorporated to predict da with more accuracy and precision.",1,2020
2020.acl-main.403.txt,"in the future, we plan to study more in depth the stylistic and figurative devices used for parody, extend the data set beyond the political case study and explore human behavior regarding parody, including how this is detected and diffused through social media.",2,2020
2020.acl-main.404.txt,"to remove these biases, however, presumably more sophisticated methods will be necessarily in the general case.",1,2020
2020.acl-main.404.txt,"while we only evaluated the strategy on one model, we believe its benefits carry over to other model architectures and similar tasks.",3,2020
2020.acl-main.406.txt,the framework introduces a range of important questions both from the inference and the application perspectives.,5,2020
2020.acl-main.407.txt,"as we argued that compositionality has, after all, desirable properties, future work could adapt methods for learning disentangled representations (e.g., higgins et al., 2017; kim and mnih, 2018) to let (more) compositional languages emerge.",1,2020
2020.acl-main.408.txt,"it also serves as an ideal starting point for several future directions such as better evaluation metrics for interpretability, causal analysis of nlp models and datasets of rationales in other languages.",3,2020
2020.acl-main.408.txt,"our hope is that eraser enables future work on designing more interpretable nlp models, and comparing their relative strengths across a variety of tasks, datasets, and desired criteria.",3,2020
2020.acl-main.408.txt,"we believe these metrics provide reasonable means of comparison of specific aspects of interpretability, but we view the problem of measuring faithfulness, in particular, a topic ripe for additional research (which eraser can facilitate).",3,2020
2020.acl-main.409.txt,we view these as interesting directions for future work.,6,2020
2020.acl-main.413.txt,"we aim to experiment with other datasets and other domains, incorporate our synthetic data in a semi-supervised setting and test the feasibility of our framework in a multi-lingual setting.",2,2020
2020.acl-main.415.txt,we hope this corpus will motivate and enable further developments in both phonetic typology and methodology for working with cross-linguistic speech corpora.,2,2020
2020.acl-main.417.txt,we are especially interested in further extending this work into low resource languages where resources tend to be noisier and underlying models to support data mining less reliable.,2,2020
2020.acl-main.418.txt,"it may be the case that to truly build gender inclusive datasets and systems, we need to hire or consult experiential experts (patton et al., 2019; young et al., 2019).",2,2020
2020.acl-main.418.txt,we also hope that developers of datasets or systems can use some of our analysis as inspiration for how one can attempt to measure—and then root out—different forms of bias in coreference resolution systems and nlp systems more broadly.,2,2020
2020.acl-main.418.txt,we hope this paper can serve as a roadmap for future studies.,6,2020
2020.acl-main.419.txt,"our research not only results in insights into significant similarities between bidirectional rnns and human attention, but also opens the avenue for promising future research directions.",1,2020
2020.acl-main.421.txt,"to provide a more comprehensive benchmark to evaluate cross-lingual models, we also released the cross-lingual question answering dataset (xquad).",3,2020
2020.acl-main.422.txt,"an exciting direction for future work is to combine the two approaches in order to identify which linguistic properties are captured in model components that are similar to one another, or explicate how localization of information contributes to the learnability of particular properties.",1,2020
2020.acl-main.422.txt,"it may be insightful to compare the results of our analysis to the loss surfaces of the same models, especially before and after fine-tuning (hao et al., 2019).",3,2020
2020.acl-main.422.txt,one could also study whether a high similarity entail that two models converged to a similar solution.,1,2020
2020.acl-main.427.txt,"we believe this data will be useful to researchers studying semantic parsing, especially interactive semantic parsing, human-robot interaction, and even imitation and reinforcement learning.",4,2020
2020.acl-main.428.txt,"future work could apply this same technique with other supervised data, e.g.correcting causal or commonsense reasoning errors (zellers et al., 2019; qin et al., 2019).",2,2020
2020.acl-main.431.txt,"we further study the extent to which various social biases (gender, race, religion) are encoded, employing several different quantification schemas.",3,2020
2020.acl-main.433.txt,"future work will explore other measures and alternative game settings for the emergence of compositionality, as well as more subtle psychological effects (categeorical perception) of continuous biological systems exhibiting discrete structure, like the auditory system.",1,2020
2020.acl-main.434.txt,"overall, these results help to clarify the patterns of distribution of context information within contextual embeddings— future work can further clarify the impact of more diverse syntactic relations between words, and of additional types of word features.",1,2020
2020.acl-main.434.txt,"we apply these tests to examine the distribution of contextual information across sentence tokens for popular contextual encoders bert, elmo, and gpt.",3,2020
2020.acl-main.438.txt,"the learned constraints can be used for structured prediction problems in two ways: (1) combining them with an existing model to improve prediction performance, or (2) incorporating them into the training process to train a better model.",1,2020
2020.acl-main.439.txt,future work should explore the extent to which our model could further benefit from initializing with stronger models and what computational challenges may arise.,1,2020
2020.acl-main.439.txt,"in future work, we hope to better understand how a discourse model can also learn fine-grained relationship types between sentences from unlabeled data.",1,2020
2020.acl-main.44.txt,"a more general form, f ∝ ∏k(r + γk)−βk , can be considered for further investigation.",1,2020
2020.acl-main.440.txt,"diy (do-it-yourself) videos and websites, for instance, are an obvious next target.",1,2020
2020.acl-main.440.txt,"ultimately, we believe this work will further the goal of building agents that can work with human collaborators to carry out complex tasks in the real world.",1,2020
2020.acl-main.440.txt,we also envision extending this work by including audio and video features to enhance the quality of our alignment algorithm.,1,2020
2020.acl-main.441.txt,future work could explore a detailed cost and time trade-off between adversarial and static collection.,3,2020
2020.acl-main.443.txt,"we believe our corpus, stackoverflow-specific bert embeddings and named entity tagger will be useful for various language-and-code tasks, such as code retrieval, software knowledge base extraction and automated question-answering.",4,2020
2020.acl-main.444.txt,"in the future, we are interested in investigating the generality of our defined schema for other comedies and different conversational registers, identifying the temporal intervals when relations are valid (surdeanu, 2013) in a dialogue, and joint dialogue-based information extraction as well as its potential combinations with multimodal signals from images, speech, and videos.",1,2020
2020.acl-main.445.txt,"in the future, we will investigate multi-document summarization datasets such as duc (paul and james, 2004) and tac (dang and owczarzak, 2008) to see whether our findings coincide when multiple references are provided.",2,2020
2020.acl-main.449.txt,"in our future work, we want to study the effective incorporation of code structure into the transformer and apply the techniques in other software engineering sequence generation tasks (e.g., commit message generation for source code changes).",4,2020
2020.acl-main.450.txt,"the framework we present is general, and extending it to other conditional text generation tasks such as image captioning or machine translation is a promising directions.",4,2020
2020.acl-main.451.txt,"for future work, we will explore better graph encoding methods, and apply discourse graphs to other tasks that require long document encoding.",4,2020
2020.acl-main.453.txt,"in future work, we plan to experiment more with this, examining how we can combine constituents to make fluent sentences without including potentially irrelevant context.",1,2020
2020.acl-main.453.txt,"we would also like to further experiment with abstractive summarization to re-examine whether large, pre-trained language models (liu and lapata, 2019) can be improved for our domain.",1,2020
2020.acl-main.455.txt,"in the future, we intend to investigate different meaning representation formalisms, such as amr (banarescu et al., 2013) and dynamic syntax (kempson et al., 2001) and extend to other datasets (e.g.multiplereference summarization) and tasks (e.g.response generation in dialogue).",1,2020
2020.acl-main.458.txt,we hope that our work draws the community’s attention to the factual correctness issue of abstractive summarization models and inspires future work in this direction.,1,2020
2020.acl-main.459.txt,"we also hope that the dataset can be added to in the future with multi-modal extractions, more granular annotations, and deeper mining of the wiki.",2,2020
2020.acl-main.459.txt,"we hope crd3 offers useful, unique data for the community to further explore dialogue modeling and summarization.",2,2020
2020.acl-main.46.txt,the adaptation to other types of morphological markers will necessitate more elaborate linguistic reflection.,1,2020
2020.acl-main.46.txt,the second direction is towards extending the approach to morphologically rich languages.,2,2020
2020.acl-main.46.txt,two orientations can be identified for future work.,6,2020
2020.acl-main.462.txt,we hope that a deeper appreciation of the role of construal in language use will spur progress toward systems that more closely approximate human linguistic intelligence.,1,2020
2020.acl-main.463.txt,with this we hope to encourage a top-down perspective on our field which we think will help us select the right hill to climb towards human-analogous nlu.,6,2020
2020.acl-main.464.txt,a crucial direction of future work is to develop richer ways of capturing scholarly impact.,1,2020
2020.acl-main.464.txt,"we used the citation counts of a subset (∼27k papers) to examine patterns of citation across paper types, venues, over time, and across areas of research within nlp.",2,2020
2020.acl-main.466.txt,"furthermore, we need to take the ethical issues of legalai seriously.",5,2020
2020.acl-main.466.txt,"in addition to these applications and tasks we have mentioned, there are many other tasks in legalai like legal text summarization and information extraction from legal contracts.",4,2020
2020.acl-main.466.txt,"in the future, for these existing tasks, researchers can focus on solving the three most pressing challenges of legalai combining embedding-based and symbol-based methods.",5,2020
2020.acl-main.467.txt,future work in this area would benefit greatly from improvements to both the breadth and depth of available probing tasks.,1,2020
2020.acl-main.467.txt,our results therefore suggest a need for further work on efficient transfer learning mechanisms.,1,2020
2020.acl-main.469.txt,"for future work, it would be interesting to see if more linguistically-inspired phenomena can be systematically found in cross-modal models.",1,2020
2020.acl-main.47.txt,"furthermore, we would like to extend a comparison between machine and human language processing beyond the perspective of word order.",3,2020
2020.acl-main.47.txt,"since lms are language-agnostic, analyzing word order in another language with the lm-based method would also be an interesting direction to investigate.",1,2020
2020.acl-main.47.txt,"we plan to further explore the capability of lms on other linguistic phenomena related to word order, such as “given new ordering” (nakagawa, 2016; asahara et al., 2018).",1,2020
2020.acl-main.470.txt,future work could bolster the measure’s usefulness in several ways.,1,2020
2020.acl-main.470.txt,"however, the method’s efficacy in the present setting is likely boosted by the relative uniformity of crisis counseling conversations; and future work could aim to better accomodate settings with less structure and more linguistic variability.",1,2020
2020.acl-main.470.txt,technical improvements like richer utterance representations could improve the measure’s fidelity; more sophisticated analyses could better capture the dynamic ways in which the balance of objectives is negotiated across many turns.,1,2020
2020.acl-main.470.txt,"the preliminary explorations in section 5.4 could also be extended to gauge the causal effects of counselors’ behaviors (kazdin, 2007).",3,2020
2020.acl-main.470.txt,"we expect balancing problems to recur in conversational settings beyond crisis counseling, such as court proceedings, interviews, debates and other mental health contexts like long-term therapy.",5,2020
2020.acl-main.470.txt,"with such improvements, it would be interesting to study other domains where interlocutors are faced with conversational challenges.",5,2020
2020.acl-main.478.txt,"in the future, we will further improve the performance of news discourse profiling by investigating subgenres of news articles, and extensively explore its usage for various other nlp tasks and applications.",4,2020
2020.acl-main.479.txt,"considering the importance of context in drawing both scalar and other inferences in communication (grice, 1975; clark, 1992; bonnefon et al., 2009; zondervan, 2010; bergen and grodner, 2012; goodman and stuhlmu¨ller, 2013; degen et al., 2015), the development of appropriate representations of larger context is an exciting avenue for future research.",1,2020
2020.acl-main.479.txt,"it would be interesting to investigate how much supervision is necessary and, for example, to what extent a model trained to perform another task such as predicting natural language inferences is able to predict scalar inferences (see jiang and de marneffe (2019b) for such an evaluation of predicting speaker commitment, and jereticˇ et al.(2020) for an evaluation of different nli models for predicting lexically triggered scalar inferences).",1,2020
2020.acl-main.479.txt,it would be straightforward to train similar models for other types of inferences.,1,2020
2020.acl-main.479.txt,one further interesting line of research would be to extend this work to other pragmatic inferences.,4,2020
2020.acl-main.48.txt,"besides, while fake news usually targets at some events, we will also extend gcan to study how to remove eventspecific features to further boost the performance and explainability.",1,2020
2020.acl-main.48.txt,we will explore model generalization in the future work.,1,2020
2020.acl-main.480.txt,"we discussed several future directions, including data augmentation for downstream transferability, applicability of pretrained encoders to discourse, and utilizing larger discourse contexts.",2,2020
2020.acl-main.481.txt,"in future work, we plan to extend this work to longer documents such as the recently released dataset of bamman et al.(2019).",2,2020
2020.acl-main.483.txt,"future work includes direct extension and validation of this technique with other language models such as gpt-2 (radford et al., 2019); experimenting with other hate speech or offensive language datasets; and experimenting with these and other sets of identity terms.",3,2020
2020.acl-main.483.txt,"in this work, we effectively applied this technique to hate speech classifiers biased towards group identifiers; future work can determine the effectiveness and further potential for this technique in other tasks and contexts.",4,2020
2020.acl-main.487.txt,future work is required to study if our ﬁndings carry over to other languages and cultural contexts.,4,2020
2020.acl-main.490.txt,"in future work, we intend to expand the coverage of clams by incorporating language-specific and non-binary phenomena (e.g., french subjunctive vs. indicative and different person/number combinations, respectively), and by expanding the typological diversity of our languages.",2,2020
2020.acl-main.490.txt,"this issue could be mitigated in the future with architectural changes to neural lms (such as better handling of morphology), more principled combinations of languages (as in dhar and bisazza 2020), or through explicit separation between languages during training (e.g., using explicit language ids).",2,2020
2020.acl-main.492.txt,"finally, we are interested in exploring how these types of explanations are actually interpreted by users, and whether providing them actually establishes trust in predictive systems.",5,2020
2020.acl-main.492.txt,"future work might explore how rankings induced over training instances by influence functions can be systematically analyzed in a stand-alone manner (rather than in comparison with interpretations from other methods), and how these might be used to improve model performance.",1,2020
2020.acl-main.493.txt,future work could extend this analysis to include quantitative results on the extent of agreement with ud.,3,2020
2020.acl-main.493.txt,"future work should explore other multilingual models like xlm and xlm-roberta (lample and conneau, 2019) and attempt to come to an understanding of the extent to which the properties we’ve discovered have causal implications for the decisions made by the model, a claim our methods cannot support.",1,2020
2020.acl-main.495.txt,"we encourage future work to judge model interpretability using the proposed evaluation and publicly published annotations, and explore techniques for improving faithfulness and interpretability in compositional models.",1,2020
2020.acl-main.496.txt,"furthermore, our method is agnostic to the underlying nature of the two objects being aligned and can therefore align disparate objects such as images and captions, enabling a wide range of future applications within nlp and beyond.",4,2020
2020.acl-main.5.txt,annotations complement for multiwoz dataset in the future might enable dst-sc to handle the related-slot problem more effectively and further improve the joint accuracy.,2,2020
2020.acl-main.50.txt,"ideally, we would like to automatically identify such polarizing topics.",1,2020
2020.acl-main.50.txt,"in future work, we plan to increase the number of topics that we use to characterize media.",2,2020
2020.acl-main.500.txt,"possible future directions include a systematic study of different aspects of qg diversity (e.g., lexical and factual) and controlled diversification of individual aspects in generation.",1,2020
2020.acl-main.500.txt,we hope that our work will encourage further exploration of diversity-promoting qg and its evaluation.,3,2020
2020.acl-main.501.txt,"a future direction is to extend this work to question answering tasks that require reasoning over multiple documents, e.g., open-domain qa.",4,2020
2020.acl-main.501.txt,"in addition, the findings may generalize to other tasks, e.g., corpus-level distantly-supervised relation extraction.",4,2020
2020.acl-main.502.txt,"through scde, we aim to encourage the development of more advanced language understanding models.",1,2020
2020.acl-main.503.txt,"models trained on standard natural language inference datasets (bowman et al., 2015) generalize poorly to other distributions (thorne et al., 2018; naik et al., 2018).",1,2020
2020.acl-main.504.txt,"in future work, we plan to explore techniques to automatically learn where to place intermediate classifiers, and what drop ratio to use for each one of them.",1,2020
2020.acl-main.505.txt,we will evaluate our approach on other machine comprehension tasks using dialogues as evidence documents to further verify the generalizability of this work.,3,2020
2020.acl-main.506.txt,we hope that this work provides a complementary picture of hypothesis assessment techniques for the field and encourages more rigorous reporting trends.,3,2020
2020.acl-main.511.txt,"a second field of application is debate systems, where a dataset can be of use for training a system to formulate new arguments.",4,2020
2020.acl-main.512.txt,"apart from that, we also plan to design novel models to perform the related tasks of entity extraction and aspect extraction from comparative sentences.",1,2020
2020.acl-main.512.txt,our future work aims to improve the cpc performance further.,1,2020
2020.acl-main.514.txt,exploring the space of subsets of our preprocessing factors might yield more interesting combinations; we leave this for future work.,1,2020
2020.acl-main.515.txt,"although conkadi has achieved a notable performance, there is still much room to improve.1) while ats2smmi is behind our conkadi, we find mmi can effectively enhance the ats2s; hence, in the future, we plan to verify the feasibility of the re-ranking technique for knowledge-aware models.2) we will continue to promote the integration of high-quality knowledge, including more types of knowledge and a more natural integration method.",1,2020
2020.acl-main.516.txt,"in the future, we plan to extend our approach to improve the consistency of multi-turn dialogues.",1,2020
2020.acl-main.519.txt,"in the future, we would like to explore variants of the model architecture.",1,2020
2020.acl-main.523.txt,"using a larger amount of general domain texts to build pre-trained representations (peters et al., 2018; radford et al., 2018; devlin et al., 2019; clark et al., 2020) can complement with our model and is one of the directions that we plan to take in future work.",2,2020
2020.acl-main.53.txt,"from this result, we propose that tackling dst with our proposed problem definition is a promising future research direction.",5,2020
2020.acl-main.530.txt,"in future work, we plan to investigate translation of other discourse phenomena that may benefit from the use of future context.",2,2020
2020.acl-main.532.txt,"for future work, following the work on automatic identification of translationese (rabinovich and wintner, 2015; rubino et al., 2016), we plan to investigate the impact of tagging translationese texts inside parallel training data, such as parallel sentences collected from the web.",1,2020
2020.acl-main.534.txt,"in this paper, we aim to evaluate document influence from a fine-grained level by additionally considering word semantic shifts.",3,2020
2020.acl-main.535.txt,"in future work, we intend to adapt our editor module for other learning tasks with both the structured input and structured output.",4,2020
2020.acl-main.535.txt,this editor learns how to extract edits from a paraphrase pair and also when and how to apply these edits to a new input sentence.,1,2020
2020.acl-main.537.txt,"these promising results point to future works in (1) linearizing the speed-speedup curve; (2) extending this approach to other pre-training architectures such as xlnet (yang et al., 2019) and elmo (peters et al., 2018); (3) applying fastbert on a wider range of nlp tasks, such as named entity recognition and machine translation.",1,2020
2020.acl-main.538.txt,"in the future, evaluation by automatically executing generated code with test cases could be a better way to assess code generation results.",3,2020
2020.acl-main.538.txt,"it will also likely be useful to generalize our re-sampling procedures to zero-shot scenarios, where a programmer writes a library and documents it, but nobody has used it yet.",1,2020
2020.acl-main.540.txt,"in the future, we will try to increase the robustness gains of adversarial training and consider utilizing sememes in adversarial defense model.",1,2020
2020.acl-main.542.txt,"in the future, we look forward to extend cl strategy to the pretraining stage, and guide deep models like transformer from a language beginner to a language expert.",1,2020
2020.acl-main.543.txt,we hope that our work will be useful in future research for realizing more advanced models that are capable of appropriately performing arbitrary inferences.,1,2020
2020.acl-main.545.txt,"in the future, we will explore more diverse and advanced paraphrase expanding methods for both sentence and paragraph level qg.",1,2020
2020.acl-main.545.txt,"moreover, we will apply our methods to other similar tasks, such as sentence simplification.",4,2020
2020.acl-main.546.txt,"for future works, to further improve the method, we will explore the introduction of additional information, such as rules and external texts.",1,2020
2020.acl-main.549.txt,"a potential solution is to jointly learn evidence selection and claim verification model, which we leave as a future work.",1,2020
2020.acl-main.549.txt,evidence selection is an important component of fact checking as finding irrelevant evidence may lead to different predictions.,1,2020
2020.acl-main.55.txt,"in the future, we will provide labels that indicate “why this candidate is false” for false candidates in our test set, so that one can easily detect weak points of systems through error analysis.",1,2020
2020.acl-main.550.txt,"in future work, we will consider introducing more information like the citation texts to the cited paper in other papers to help the generation.",2,2020
2020.acl-main.553.txt,it is also convenient to adapt our singledocument graph to multi-document with document nodes.,1,2020
2020.acl-main.553.txt,the introduction of more fine-grained semantic units in the summarization graph helps our model to build more complex relationships between sentences .,1,2020
2020.acl-main.555.txt,"in the future we would like to explore other more informative graph representations such as knowledge graphs, and apply them to further improve the summary quality.",1,2020
2020.acl-main.556.txt,"in the future, we will introduce more tasks like document ranking to supervise the learning of the multi-granularity representations for further improvement.",1,2020
2020.acl-main.558.txt,"this dataset, named mlqe, has been released to the research community3 and will be used for the wmt20 shared task on quality estimation.4 in future work, we will test the partial input hypothesis on this data.",2,2020
2020.acl-main.558.txt,we hope it will be useful for general research in qe towards more reliable models.,1,2020
2020.acl-main.559.txt,we hope that the paradigm presented here will help provide coherence to such efforts.,6,2020
2020.acl-main.560.txt,pertinent questions should be posed to authors of future publications about whether their proposed language technologies extend to other languages.,5,2020
2020.acl-main.561.txt,"but of one thing we can be certain: the immense success of adapting deep learning architectures to fit with our computational-linguistic understanding of the nature of language will doubtless continue, with greater insights for both natural language processing and machine learning.",1,2020
2020.acl-main.561.txt,this analysis suggests that an important next step in deep learning architectures for natural language understanding will be the induction of entities.,1,2020
2020.acl-main.562.txt,"we would like to see them become true enablers instead, allowing queries to go far beyond of what a corpus has to offer with its bare annotations alone and for example include the following extensions to create more informed search solutions: • use knowledge bases and similar external resources to allow more generalized queries, e.g.“find verbal constructions containing a preposition in combination with some sort of furniture”.• add (semantic) similarity measures (e.g.word embeddings) and other approaches for increased fuzziness to improve example-based search.• offer true scripting support for users to extent or customize the ability provided by a system.",2,2020
2020.acl-main.563.txt,we will explore it in the future.,6,2020
2020.acl-main.564.txt,"future work will investigate other data manipulation techniques (e.g., data synthesis), which can be further integrated to improve the performance.",1,2020
2020.acl-main.565.txt,"besides, our model can quickly adapt to a new domain with little annotated data.",1,2020
2020.acl-main.567.txt,designing a new model to address these problems may be our future work.,1,2020
2020.acl-main.567.txt,"we believe that sas provides promising potential extensions, such as adapting our model on other tasks where are troubled by excessive information.",1,2020
2020.acl-main.568.txt,"recently, zhang et al.(2020) uses bert (devlin et al., 2019) to evaluate generated candidate sentences by comparing reference sentence.",3,2020
2020.acl-main.568.txt,there are several future directions to improve ssrem.,1,2020
2020.acl-main.568.txt,we will apply ssrem to various conversation tasks for evaluating the generated text automatically.,3,2020
2020.acl-main.568.txt,we will explore these directions in our future work.,6,2020
2020.acl-main.568.txt,"we will use ranking loss (wang et al., 2014; schroff et al., 2015) to learn the difference among samples.",1,2020
2020.acl-main.568.txt,we will use the contextual embedding to represent utterances.,1,2020
2020.acl-main.569.txt,"in the future work, we will focus on more effective discourse parsing with additional carefully designed features and joint learning with edu segmentation.",1,2020
2020.acl-main.57.txt,"in the future, we plan to use more powerful encoders and evaluate our methods on real dialog data.",3,2020
2020.acl-main.570.txt,future work aims at enhancing sequence feature extraction methods to improve the classification performance as those suffer from low accuracy.,1,2020
2020.acl-main.572.txt,"next, we are considering to use this framework to conduct kg entity type noise detection.",1,2020
2020.acl-main.572.txt,our modeling method is general and should apply to other typeoriented tasks.,4,2020
2020.acl-main.573.txt,"for future work, how to combine open relation learning and continual relation learning together to complete the pipeline for emerging relations still remains a problem, and we will continue to work on it.",5,2020
2020.acl-main.576.txt,"in the future, we should further leverage the internal relations in the candidate end, and try to introduce rich medical background knowledge into our work.",2,2020
2020.acl-main.58.txt,we hope to provide new guidance for the future slot tagging work.,6,2020
2020.acl-main.580.txt,"in future work, we hope to tackle repeated fields and learn domainspecific candidate generators.",5,2020
2020.acl-main.580.txt,"we are also actively investigating how our learned candidate representations can be used for transfer learning to a new domain and, ultimately, in a few-shot setting.",1,2020
2020.acl-main.583.txt,future work should study this additional relation in the context of caption annotation and generation.,6,2020
2020.acl-main.583.txt,the presented work has limitations that can be addressed in future research.,6,2020
2020.acl-main.584.txt,"future work should look at more complex fusion strategies, possibly coupled with bottom-up recalibration mechanisms (zarrieß and schlangen, 2016; mojsilovic, 2005) to further enhance colour classification under difficult illumination conditions.",1,2020
2020.acl-main.585.txt,the effectiveness of vslnet (and even vslbase) suggest that it is promising to explore span-based qa framework to address nlvl problems.,1,2020
2020.acl-main.588.txt,"in future work, we can further improve our method in the following aspects.",1,2020
2020.acl-main.588.txt,"second and last, domain-specific knowledge can be incorporated into our method as an external learning source.",2,2020
2020.acl-main.588.txt,we plan to employ an edgeaware graph neural network considering the edge labels.,1,2020
2020.acl-main.589.txt,"in the future, we would like to extend our work to make a syntactically-aware window that can automatically learn tree (or phrase) structures.",1,2020
2020.acl-main.59.txt,"as future work, we will apply madpl in the more complex dialogs and verify the role-aware reward decomposition in other dialog scenarios.",4,2020
2020.acl-main.594.txt,"the pattern of training robust systems on data that has been augmented by the knowledge captured in symbolic systems could be applied to areas outside of morphological analysis, and is a promising avenue of future exploration.",4,2020
2020.acl-main.596.txt,future work will aim to extend the current model to capture particularly challenging morphological patterns such as templatic non-concatenative morphology and polysynthetic composition.,1,2020
2020.acl-main.596.txt,"our next step will be to attempt to automate the determination of language typology, yielding somewhat better performance with a system requiring no human intervention per language at all.",1,2020
2020.acl-main.598.txt,"by substituting the current transducers in our pipeline, we expect that we will be able to improve the overall performance of our system.",1,2020
2020.acl-main.598.txt,"in the future, we will explore the following directions: (i) a difficult challenge for our proposed system is to correctly determine the paradigm size.",5,2020
2020.acl-main.598.txt,"since transfer across related languages has shown to be beneficial for morphological tasks (jin and kann, 2017; mccarthy et al., 2019; anastasopoulos and neubig, 2019, inter alia), future work could use typologically aware priors to guide the number of paradigm slots based on the relationships between languages.(ii) we plan to explore other methods, like word embeddings, to incorporate context information into our feature function.(iii) we aim at developing better performing string transduction models for the morphological inflection step.",4,2020
2020.acl-main.599.txt,improving our graph structure of representing the document as well as the document-level pretraining tasks is our future research goals.,1,2020
2020.acl-main.6.txt,"in future work, we plan to analyze each turn of dialogue with reinforcement learning architecture, and to enhance the diversity of the whole dialogue by avoiding knowledge reuse.",1,2020
2020.acl-main.601.txt,we will investigate the robustness and scalability of the model.,1,2020
2020.acl-main.602.txt,"for modeling, we plan to explore recent advances in conditional language models for jointly modeling qa with generating their derivations.",1,2020
2020.acl-main.602.txt,one immediate future work is to evaluate state-of-the-art rc systems’ internal reasoning on our dataset.,3,2020
2020.acl-main.603.txt,we also add a recurrent mechanism to allow the information to flow across segments so that the model could have knowledge beyond the current segment when selecting answers.,1,2020
2020.acl-main.606.txt,"future research may involve tailoring existing parsers to learner data, combining literal and intended meanings in a unified framework, evaluating gec models in terms of speakers’ intention and parsing for other languages.",3,2020
2020.acl-main.607.txt,this points to future directions of applying our model to low-resource languages and cross-domain settings.,4,2020
2020.acl-main.611.txt,we leave adjusting our model to different kinds of lattice or graph as our future work.,4,2020
2020.acl-main.612.txt,"for the future work, we are planning to extract fine-grained semantic types from unlabelled documents and use the relatedness between the finegrained types and contexts as distant supervision for entity linking.",1,2020
2020.acl-main.616.txt,"in the future, it is necessary to interpret the semantics that transformer layers in different depths can convey, which is beneficial for the computing-efficiency.",1,2020
2020.acl-main.617.txt,"atth learns embeddings with trainable hyperbolic curvatures, allowing it to learn the right geometry for each relationship and generalize across multiple embedding dimensions.",1,2020
2020.acl-main.617.txt,"future directions for this work include exploring other tasks that might benefit from hyperbolic geometry, such as hypernym detection.",4,2020
2020.acl-main.617.txt,the proposed attention-based transformations can also be extended to other geometric operations.,4,2020
2020.acl-main.618.txt,"one particularly exciting direction is the application of our classification-based self-learning framework on top of the most recent methods that induce bilingual spaces via non-linear alignments (glavasˇ and vulic´, 2020; mohiuddin and joty, 2020).",4,2020
2020.acl-main.618.txt,"this proof-of-concept work opens up a wide spectrum of interesting avenues for future research, including the use of more powerful classifiers, more sophisticated features (e.g., character-level transformers), and fine-grained linguistic analyses on the importance of disparate features over different language pairs.",1,2020
2020.acl-main.62.txt,"for future work, we will explore the scenarios that annotations are absent for all expert dialogues.",5,2020
2020.acl-main.620.txt,another promising direction is to design more powerful training strategies to replace the baby step.,1,2020
2020.acl-main.620.txt,"as our model is not limited to machine translation, it is interesting to validate the proposed framework into other nlp tasks that need to exploit cl.",4,2020
2020.acl-main.622.txt,"in future work, we will explore novel approaches to generate the questions based on each mention, and evaluate the influence of different question generation methods on the coreference resolution task.",3,2020
2020.acl-main.623.txt,"future work would include a comparison with other, more complex, methods for uncertainty estimation, incorporating uncertainty to affect model decisions over time, and further investigating links between uncertainty values and linguistic features of the input.",1,2020
2020.acl-main.624.txt,"in the future, we want to investigate more powerful recommenders, combine interactive entity linking with knowledge base completion and use online learning to leverage deep models, despite their long training time.",1,2020
2020.acl-main.629.txt,"despite the promising results, the accuracy of our approach could probably be boosted further by experimenting with new feature information and specifically tuning hyper-parameters for the sdp task, as well as using different enhancements such as implementing the hierarchical decoding recently presented by liu et al.(2019), including contextual string embeddings (akbik et al., 2018) like he and choi (2019), or applying multi-task learning across the three formalisms like peng et al.(2017).",1,2020
2020.acl-main.630.txt,"another research direction is to investigate if introducing more sophisticated topic models, such as named entity promoting topic models (krasnashchok and jouili, 2018) into the proposed framework can further improve results.",1,2020
2020.acl-main.630.txt,future work may focus on how to directly induce topic information into bert without corrupting pretrained information and whether combining topics with other pretrained contextual models can lead to similar gains.,1,2020
2020.acl-main.631.txt,"moreover, the proposed augmentation method tends not to be unique to the current task and could be applied to other low-resource sequence labeling tasks such as chunking and named entity recognition.",4,2020
2020.acl-main.632.txt,"in the future, we plan to consider the ethos mode of persuasion by exploring how debaters strengthen their credibility in debates.",5,2020
2020.acl-main.633.txt,"future research may focus on the motivation we described, but may also utilize the large speeches corpus we release as part of this work to a variety of additional different endeavors.",2,2020
2020.acl-main.634.txt,"the model can be potentially improved by filtering the corpus according to different domains, or augmenting with a retrieve-and-rewrite mechanism, which we leave for future work.",1,2020
2020.acl-main.636.txt,"in future work, we intend to explore more with the combination of rl and dst on the basis of reward designing, trying to explore more in the internal mechanism.",1,2020
2020.acl-main.636.txt,"in the long run, we are interested in combing many tasks into one learning process with meta-learning.",1,2020
2020.acl-main.639.txt,"in the future, we plan to adapt variational neural network to refine our style transfer model, which has shown effectiveness in other conditional text generation tasks, such as machine translation (zhang et al., 2016; su et al., 2018).",4,2020
2020.acl-main.640.txt,"on the other hand, we would also like to investigate how to make use of our proposed model to solve sequence-to-sequence tasks.",1,2020
2020.acl-main.640.txt,one is to investigate how the other graph models can benefit from our proposed heterogeneous mechanism.,1,2020
2020.acl-main.640.txt,there are two directions for future works.,6,2020
2020.acl-main.642.txt,we will explore more complicated object relation modeling in future work.,1,2020
2020.acl-main.643.txt,a future research direction is to combine rgcs with distant supervision by an external knowledge base to answer the visual questions that need external knowledge; for example which animal in this photo can climb a tree?,1,2020
2020.acl-main.645.txt,the question of knowing whether pretraining on small domain specific content will be a better option than transfer learning techniques such as fine-tuning remains open and we leave it for future work.,5,2020
2020.acl-main.645.txt,this paves the way for the rise of monolingual contextual pre-trained language-models for under-resourced languages.,1,2020
2020.acl-main.647.txt,we aim to explore those directions in a future work.,6,2020
2020.acl-main.648.txt,we leave some open questions to explore: • how can we exploit subword variations to reduce skewness in the nlu tasks?• would subword-segmentation-transfer be helpful for other nmt-nlu task pairs like we did for 2kenize (script conversion) to 1kenize (classification)?,5,2020
2020.acl-main.649.txt,"in future work, we intend to further fine-tune our methodological apparatus for tackling mfep.",1,2020
2020.acl-main.65.txt,"in future work, we plan to seek better ways to guide the learning of latent variables, such as using dynamic routing (sabour et al., 2017) method to align the latent variables and sememes, and learn more explainable latent codes.",1,2020
2020.acl-main.650.txt,"the techniques developed here readily apply to other types of normalization data (e.g.informal, dialectal).",4,2020
2020.acl-main.651.txt,"we hope that this dataset will encourage research into clarification question generation and, in the long run, enhance dialog and question-answering systems.",2,2020
2020.acl-main.652.txt,"for the future, we would like to exploit the abstractive answers in our dataset, explore more sophisticated systems in both scenarios and perform user studies to study how real users interact with a conversational qa system when accessing faqs.",1,2020
2020.acl-main.654.txt,"in future work, we explore to extend this approach for other low resource tasks in nlp.",4,2020
2020.acl-main.655.txt,"in the future, we will further study this properties of kernel-based attentions in neural networks, both in the effectiveness front and also the explainability front.",1,2020
2020.acl-main.656.txt,"for future work, an obvious next step is to investigate the possibility of generating veracity explanations from evidence pages crawled from the web.",1,2020
2020.acl-main.657.txt,"as future work, we will explore different heuristics for navigating in the premises graph, as researched before for textual entailment (silva et al., 2019, 2018) and selective reasoning (freitas et al., 2014).",1,2020
2020.acl-main.660.txt,"this type of research needs to be extended to the investigation of multiple tasks, multiple languages, and multiple possible pre-training regimes (words, chars, morphemes, lattices) in order to investigate whether this trend extends to other languages and tasks.",4,2020
2020.acl-main.661.txt,we exposed a significant space of both modeling ideas and application-specific requirements left to be addressed in future research.,1,2020
2020.acl-main.662.txt,"finally, if you want to make human–computer comparisons, pick the right humans.",6,2020
2020.acl-main.662.txt,"moreover, another lesson the qa community could learn from trivia games is to turn it into a spectacle: exciting games with a telegenic host.",5,2020
2020.acl-main.662.txt,these skills are exactly those we want computers to develop.,1,2020
2020.acl-main.663.txt,"however, there is a trade-off between expressiveness and learnability: the more structure we add, the more difficult it can be to work with our representations.",5,2020
2020.acl-main.663.txt,"my own recent work in this direction has been to develop the pixie autoencoder (emerson, 2020a), and i look forward to seeing alternative approaches from other authors, as the field of distributional semantics continues to grow.",1,2020
2020.acl-main.663.txt,"to this end, there are promising neural architectures for working with structured data, such dependency graphs (for example: marcheggiani and titov, 2017) or logical propositions (for example: rockta¨schel and riedel, 2017; minervini et al., 2018).",1,2020
2020.acl-main.664.txt,in the near future we plan to extend the proposed approach to several other language-vision modeling tasks.,4,2020
2020.acl-main.665.txt,another avenue of research would be to investigate the role of synthetic data in surface realization in other languages.,2,2020
2020.acl-main.666.txt,"as future work, we plan to further evaluate the impact of different sequential architectures, longer contexts, alternative sentence embeddings, and cleverer selection of distractors.",3,2020
2020.acl-main.666.txt,"inspired by deliberation networks and automatic post editing methods (xia et al., 2017; freitag et al., 2019), we ultimately want to apply our model to two-step generation, first selecting a sentence from a large set before refining it to fit the context.",4,2020
2020.acl-main.667.txt,we hope that this work can shed some light and inspire future work at this line of research.,6,2020
2020.acl-main.668.txt,future work will explore the use of domain adaptation techniques to enhance performance where the domains of the ci and event text differ substantially.,1,2020
2020.acl-main.669.txt,"we also plan to use different types of labelled data, e.g., domain specific data sets, to ascertain whether entity type information is more discriminative in sub-languages.",2,2020
2020.acl-main.67.txt,"also, we plan to use large-scale unlabeled data to improve the performance further.",2,2020
2020.acl-main.67.txt,"furthermore, our framework can be efficiently applied to other graph-to-sequence tasks such as webnlg (gardent et al., 2017) and syntax-based neural machine translation (bastings et al., 2017).",4,2020
2020.acl-main.67.txt,in future work we would like to do several experiments on other related tasks to test the versatility of our framework.,3,2020
2020.acl-main.671.txt,"furthermore, we seek to investigate the transferability of the obtained inductive bias to other commonsense-demanding downstream tasks, which are distinct from the winograd-structure.",4,2020
2020.acl-main.671.txt,"therefore, future work will aim at relaxing the prior of winograd-structured twin-question pairs.",1,2020
2020.acl-main.671.txt,"we believe in order to solve commonsense reasoning truly, algorithms should refrain from using labeled data, instead exploit the structure of the task itself.",1,2020
2020.acl-main.673.txt,"for future work, our model can be extended to disentangled representation learning with non-categorical style labels, and applied to zero-shot style transfer with newly-coming unseen styles.",4,2020
2020.acl-main.675.txt,"first, we will explore mechanisms for instance-specific translation that are more sophisticated than the aggregation of translation vectors of nearest dictionary neighbours.",1,2020
2020.acl-main.675.txt,"second, we plan to couple instance-based mapping with other informative features (e.g., character-level features) in classification-based bli frameworks (heyman et al., 2017; karan et al., 2020).",1,2020
2020.acl-main.675.txt,we plan to extend this work in two directions.,6,2020
2020.acl-main.676.txt,"the procedure detailed in this paper relies on exact string matching to identify common context; future work might take advantage of learned representations of spans and their environments (mikolov et al., 2013; peters et al., 2018).",1,2020
2020.acl-main.678.txt,the proposed method may be an important module for future applications related to time.,4,2020
2020.acl-main.680.txt,"we expect our data, results, and error analysis to inform the design of similar experimental setups for other nlp tasks beyond ner, such as part-of-speech tagging or relation extraction.",3,2020
2020.acl-main.681.txt,"future directions to explore include incorporating noise-robust training procedures (goldberger and ben-reuven, 2017) and example weighting (dehghani et al., 2018) during self-training, and exploring lexical alignment methods from literature on learning cross-lingual embeddings.",1,2020
2020.acl-main.682.txt,"however, the same framework could be applied to other multi-modal tasks.",4,2020
2020.acl-main.682.txt,we hope that grolla and the compguesswhat?!data will encourage the implementation of learning mechanisms that fuse taskspecific representations with more abstract representations to encode attributes in a more compositional manner.,1,2020
2020.acl-main.684.txt,"future work can also explore curriculum learning in this domain, by first learning simpler tasks, which can be compositionally invoked in explanations for complex tasks.",1,2020
2020.acl-main.684.txt,"here, we posed the learning of web-based tasks as similar to instruction-following problem, with no aspect of interactivity or exploration of the environment.",5,2020
2020.acl-main.684.txt,"in future work, the possibility of learning from a mix of explanations, exploration and a limited budget of interaction with the environment can be explored.",1,2020
2020.acl-main.687.txt,our work provides a foundation for future work into simpler and more computationally efficient neural machine translation.,1,2020
2020.acl-main.688.txt,"therefore, a promising research direction to investigate would involve the development and assessment of improved initialisation methods that would more efficiently yield the benefits of the model transfer.",3,2020
2020.acl-main.692.txt,"another interesting avenue is applying this to unsupervised nmt, which is highly sensitive to domain mismatch (marchisio et al., 2020; kim et al., 2020).",4,2020
2020.acl-main.692.txt,"this work just scratches the surface with what can be done on the subject; possible avenues for future work include extending this with multilingual data selection and multilingual lms (conneau and lample, 2019; conneau et al., 2019; wu et al., 2019; hu et al., 2020), using such selection methods with domain-curriculum training (zhang et al., 2019; wang et al., 2019b), applying them on noisy, web-crawled data (junczys-dowmunt, 2018) or for additional tasks (gururangan et al., 2020).",2,2020
2020.acl-main.692.txt,"we hope this work will encourage more research on finding the right data for the task, towards more efficient and robust nlp.",2,2020
2020.acl-main.693.txt,"while the scope of this work does not extend to sampling sentences given document context, this would be an interesting direction for future work.",6,2020
2020.acl-main.694.txt,"also, we plan to consider more structured latent variables beyond modeling the sentence-level variation as well as to apply our vnmt model to more language pairs.",1,2020
2020.acl-main.694.txt,we plan to conduct a more in-depth investigation into actual multimodality condition with high-coverage sets of plausible translations.,2,2020
2020.acl-main.695.txt,"we believe our benchmark system represents a reasonable approach to solving the problem based on past work and highlights many directions for improvement, e.g.joint modeling and making better use of distributional semantic information.",1,2020
2020.acl-main.699.txt,future work includes a deeper qualitative analysis of which (type of) papers are being cited; a more fine-grained analysis of different research topics in nlp to determine whether changes are more prevalent within certain areas than others; or extending the analysis to a larger set of the papers in the acl anthology.,1,2020
2020.acl-main.70.txt,"more importantly, osdm tried to incorporate semantic information in the proposed graphical representation model to remove the term ambiguity problem in short-text clustering.",1,2020
2020.acl-main.703.txt,"future work should explore new methods for corrupting documents for pretraining, perhaps tailoring them to specific end tasks.",1,2020
2020.acl-main.704.txt,"future research directions include multilingual nlg evaluation, and hybrid methods involving both humans and classifiers.",3,2020
2020.acl-main.705.txt,"for future work, we will explore the extension of conditional mlm to multimodal input such as image captioning.",1,2020
2020.acl-main.706.txt,we hope that the dataset we collected will facilitate research in using natural language for physical reasoning.,2,2020
2020.acl-main.707.txt,"in future work, we plan to add paraphrase generation to generate diverse simple sentences.",1,2020
2020.acl-main.708.txt,"there are still some unsolved problems for logical nlg, e.g.how to improve the quality of automatic metrics to better help human automatically judge models’ performances.",5,2020
2020.acl-main.708.txt,"to promote the research in this direction, we host a logicnlg challenge2 to help better benchmark the current progress.",1,2020
2020.acl-main.710.txt,"in future work, we plan to investigate how target phrase order affects the generation behavior, and further explore set generation in an order invariant fashion.",1,2020
2020.acl-main.711.txt,"language models conflate the two, so developing methods that are nuanced enough to recognize this difference is key to future progress.",1,2020
2020.acl-main.713.txt,"in the future, we plan to incorporate more comprehensive event schemas that are automatically induced from multilingual multimedia data and external knowledge to further improve the quality of ie.",2,2020
2020.acl-main.713.txt,we also plan to extend our framework to more ie subtasks such as document-level entity coreference resolution and event coreference resolution.,4,2020
2020.acl-main.714.txt,"in the future work, it would be interesting to further explore how the model can be adapted to jointly extract role fillers, tackles coreferential mentions and constructing event templates.",1,2020
2020.acl-main.715.txt,"in the future, we plan to apply ceon-lstm to other related nlp tasks (e.g., event extraction, semantic role labeling) (nguyen et al., 2016a; nguyen and grishman, 2018a).",4,2020
2020.acl-main.716.txt,"in our ongoing research, we are investigating the expansion of this technique to language pairs where english may not be involved.",2,2020
2020.acl-main.718.txt,we hope that rams will stimulate further work on multi-sentence argument linking.,5,2020
2020.acl-main.72.txt,our systematic study will pave the way to future research about the effective construction of dictionaries for text analytics.,2,2020
2020.acl-main.720.txt,"in future work, we will explore whether the observed trends hold in much larger polyglot settings, e.g.the wikiann ner corpus (pan et al., 2017b).",2,2020
2020.acl-main.720.txt,"with this in mind, exploring different training strategies, such as multi-objective optimization, may prove beneficial (sener and koltun, 2018).",1,2020
2020.acl-main.721.txt,future extensions of this work involve a more general pre-training objective allowing for the learned representations to be useful in many tasks as well as distantly or semi-supervised approaches to benefit from more data.,4,2020
2020.acl-main.722.txt,possible future directions include using more sophisticated feature design and combinations of candidate retrieval methods.,1,2020
2020.acl-main.724.txt,"as future work, we intend to apply cluhtm in other representative applications on the web, such as hierarchical classification by devising a supervised version of cluhtm.",4,2020
2020.acl-main.724.txt,we also intend to incorporate some type of attention mechanism into our methods to better understand which cluwords are more important to define certain topics.,1,2020
2020.acl-main.725.txt,another interesting direction is to generate a class name hierarchy via language model probing.,1,2020
2020.acl-main.725.txt,"for example, we may expand the set {“machine translation”, “information extraction”, “syntactic parsing”} to acquire more nlp task concepts.",2,2020
2020.acl-main.725.txt,"in the future, we plan to expand the method scope from expanding concrete entity sets to more abstract concept sets.",1,2020
2020.acl-main.726.txt,"in our future work, we will consider extending it to graph-based methods such as gcn for graph data, and to generation-based methods such as gan for adversarial learning.",1,2020
2020.acl-main.728.txt,"vilbert (lu et al., 2019), lxmert (tan and bansal, 2019), uniter (chen et al., 2019) etc.",1,2020
2020.acl-main.735.txt,"for future work, we plan to apply the same methodology to other nlp tasks.",4,2020
2020.acl-main.737.txt,"another research avenue that could be explored is modeling specific user preferences: since each user likely favors a certain set of character substitutions, allowing user-specific parameters could improve decoding and be useful for authorship attribution.",1,2020
2020.acl-main.740.txt,"our work points to numerous future directions, such as better data selection for tapt, efficient adaptation large pretrained language models to distant domains, and building reusable language models after adaptation.",1,2020
2020.acl-main.744.txt,"introducing extra regularization terms to a downstream task have been shown to be beneficial in terms of both output structure consistency and prediction accuracy (e.g., minervini and riedel, 2018; hsu et al., 2018; mehta et al., 2018; du et al., 2019; li et al., 2019).",1,2020
2020.acl-main.745.txt,"finally, to extend tabert to cross-lingual settings with utterances in foreign languages and structured schemas defined in english, we plan to apply more advanced semantic similarity metrics for creating content snapshots.",1,2020
2020.acl-main.745.txt,"first, we plan to evaluate tabert on other related tasks involving joint reasoning over textual and tabular data (e.g., table retrieval and table-to-text generation).",3,2020
2020.acl-main.745.txt,"second, following the discussions in § 5, we will explore other table linearization strategies with transformers, improving the quality of pretraining corpora, as well as novel unsupervised objectives.",1,2020
2020.acl-main.745.txt,this work also opens up several avenues for future work.,6,2020
2020.acl-main.746.txt,we envision future efforts exploring the interactions between improving the underlying graphstructure prediction and ever-better correlations to human judgements on individual properties.,1,2020
2020.acl-main.75.txt,"moreover, we would love to apply the proposed model to other problems, such as general humor recognition, irony discovery, and sarcasm detection, as the future work.",4,2020
2020.acl-main.750.txt,future work will focus on domain adaptation at the embedding layer.,1,2020
2020.acl-main.751.txt,"interesting future work includes applying our techniques to different taxonomies (e.g., biomedical) and training a model for different attributes.",4,2020
2020.acl-main.754.txt,"notably, multidds is not limited to nmt, and future work may consider applications to other multilingual tasks.",4,2020
2020.acl-main.756.txt,"because it is artificial to use synthetic data for training a filter classifier, future work can focus on a better objective that models parallelism more smoothly.",1,2020
2020.acl-main.756.txt,future work also includes extending the method to low-resource languages not covered by multilingual bert.,2,2020
2020.acl-main.757.txt,"in the future, we believe more work on alleviating context control problem has the potential to improve translation performance as quantified in table 3.",1,2020
2020.acl-main.759.txt,"one of our next steps is to investigate the impact of tc extraction methods on a corresponding awe system (zhang et al., 2019), which uses the feature values produced by aesrubric to generate formative feedback to guide essay revision.",1,2020
2020.acl-main.759.txt,"this leads to an interesting future investigation direction, which is training the aesneural using the gold standard that can be extracted automatically.",3,2020
2020.acl-main.760.txt,future work will consider additional methods for integrating ontology structure into representation learning.,1,2020
2020.acl-main.761.txt,"future work in multi-hop reasoning could represent the relation between consecutive pieces of evidence and future work in temporal reasoning could incorporate numerical operations with bert (andor et al., 2019).",1,2020
2020.acl-main.764.txt,we believe that modeling domain shift is a promising future direction to improve performance prediction.,1,2020
2020.acl-main.764.txt,"while investigating the systematic implications of model structures or hyperparameters is practically infeasible in this study, we may use additional information such as textual model descriptions for modeling nlp models and training procedures more elaborately in the future.",1,2020
2020.acl-main.764.txt,"while this discovery is a promising start, there are still several avenues on improvement in future work.",6,2020
2020.acl-main.765.txt,further investigations are thus required to fully understand how narratives can be effectively used in dialogue generation.,5,2020
2020.acl-main.766.txt,"in the future, we will investigate a hub language ranking/selection model a la lin et al.(2019).",1,2020
2020.acl-main.767.txt,there are several directions for future work including better architecture design for utilizing structured meta-data and replacing the two-stage framework with a multi-task generation model that can jointly identify helpful context for the task and perform corresponding text generation.,1,2020
2020.acl-main.769.txt,"additionally, we extend our methods to combat multiple bias patterns simultaneously.",1,2020
2020.acl-main.769.txt,future work may include developing debiasing strategies that do not require prior knowledge of bias patterns and can automatically identify them.,1,2020
2020.acl-main.77.txt,"in the future, we will do more tests and surveys on the improvement of business objectives such as user experience, user engagement and service revenue.",3,2020
2020.acl-main.770.txt,several challenges in this direction of research may include extending the debiasing methods to overcome multiple biases at once or to automatically identify the format of those biases which simulate a setting where the prior knowledge is unavailable.,5,2020
2020.acl-main.773.txt,joint efforts are needed for promoting unbiased models that learn true semantics; and we hope our paper can encourage more work towards this important direction.,1,2020
2020.acl-main.773.txt,"since none of our methods is biastype specific, we believe these results can also be generalized to other similar lexical biases.",4,2020
2020.acl-main.775.txt,we leave it to future work to design a non-projective decoder for joint parsing and headless structure extraction.,1,2020
2020.acl-main.776.txt,"another interesting line of research would be to evaluate the contribution of higher-order features in a cross-lingual setting, leveraging structure learned from larger treebanks to underresourced languages.",3,2020
2020.acl-main.776.txt,our results indicate that even a powerful encoder as bert can still benefit from explicit output structure modelling; this would be interesting to explore in other nlp tasks as well.,4,2020
2020.acl-main.778.txt,another area for future work is to explore what information treebank vectors encode.,2,2020
2020.acl-main.778.txt,"interpolating treebank vectors adds a layer of opacity, and, in future work, it would be interesting to carry out experiments with synthetic data, e. g. varying the number of unknown words, to get a better understanding of what they may be capturing.",2,2020
2020.acl-main.778.txt,"we plan to explore other methods to predict tree-bank vectors, e. g. neural sequence modelling, and to apply our ideas to the related task of language embedding prediction for zero-shot learning.",4,2020
2020.acl-main.79.txt,"some interesting observations include the effects of regions and the sensitivity of gnn-based models, which open potentials for further improvements that we plan to address in our future work.",1,2020
2020.acl-main.8.txt,"in the future, we aim to leverage these pre-trained models to advance sota on downstream conversational tasks, such as knowledge-grounded conversations or question answering.",1,2020
2020.acl-main.81.txt,we leave this direction to future work.,6,2020
2020.acl-main.82.txt,"as future work, we plan to extend soft-masked bert to other problems like grammatical error correction and explore other possibilities of implementing the detection network.",1,2020
2020.acl-main.82.txt,the technique of soft-masking is general and potentially useful in other detection-correction tasks.,4,2020
2020.acl-main.84.txt,"based on these results, we will explore ways to leverage the token assignment to domain adaption and few-shot learning.",1,2020
2020.acl-main.84.txt,we also plan to enhance the annotation process by automatically generating proposals for the nl questions and token assignments and letting the annotators only perform corrections.,2,2020
2020.acl-main.84.txt,we hope that this increases annotation efficiency even more.,2,2020
2020.acl-main.9.txt,"in the future, we will also explore to boost the latent selection policy with reinforcement learning and extend our pre-training to support dialogue generation in other languages.",1,2020
2020.acl-main.9.txt,our work can be potentially improved with more fine-grained latent variables.,1,2020
2020.acl-main.90.txt,incorporating our three-way attentive pooling network into open domain conversational qa systems will be interesting future work.,1,2020
2020.acl-main.95.txt,"another direction for future work would improve few-shot approaches to wsd, which is both important for moving wsd into new domains and for modeling rare senses that naturally have less support in wsd data.",1,2020
2020.acl-main.95.txt,"potential directions include finding ways to obtain more informative training signal from uncommon senses, such as with different approaches to loss reweighting, and exploring the effectiveness of other model architectures on lfs examples.",1,2020
2020.acl-main.95.txt,this leaves better disambiguation of less common senses as the main avenue for future work on wsd.,1,2020
2020.acl-main.96.txt,further we plan to investigate other nlp applications that can benefit from the simple linguistic features introduced here.,4,2020
2020.acl-main.96.txt,"in future, we would like to extend this work for other language pairs.",2,2020
2020.acl-main.97.txt,"in the future, we will extend the proposed framework by considering more context (meta data) information, such as time, storylines, and comment sentiment, to further enrich our explainability.",1,2020
2020.acl-main.98.txt,"the complexity in durecdial makes it a great testbed for more tasks such as knowledge grounded conversation (ghazvininejad et al., 2018), domain transfer for dialog modeling, target-guided conversation (tang et al., 2019a) and multi-type dialog modeling (yu et al., 2017).",4,2020
2020.acl-main.98.txt,the study of these tasks will be left as the future work.,6,2020
2020.acl-main.99.txt,"in future work, we plan to conduct more empirical studies on seg and further improve its performance on new intent identification.",3,2020
2020.acl-main.99.txt,we also plan to conduct more case studies in applying seg to boost the performance of current zero-shot intent classification methods.,3,2020
2020.acl-srw.1.txt,"in this work, we extend adaptive approaches to visiolinguistic tasks to understand more about attention and adaptive mechanisms.",4,2020
2020.acl-srw.1.txt,"while the empirical results are encouraging, important future work includes explorations of higher efficient adaptive and sparse mechanisms that can significantly cause flops and parameter reduction with minimal loss in performance.",1,2020
2020.acl-srw.10.txt,"further extensions may include studying the behavior of more powerful subword combination strategies (e.g.convolutions, self-attention) and the application of subword merging to the target side.",1,2020
2020.acl-srw.10.txt,"future extensions to this work may include applying it to character-level instead of subword representations, and using it for morphologically richer languages, especially low-resourced agglutinative ones, where our approach, together with the incorporation of linguistic information, may provide larger improvements in translation quality.",2,2020
2020.acl-srw.11.txt,"in the future, we intend to use the english translation data of north korean news articles to create an evaluation dataset that considers differences in words, and attempt to develop a translation method using a language model with context, such as bert (devlin et al., 2019).",3,2020
2020.acl-srw.14.txt,"in the future, we will strengthen tags that contain semantic information to extract keywords for more accurate information, such as disease information, location, and size.",1,2020
2020.acl-srw.15.txt,"in future work, we plan to utilize other word segmentation methods for model training.",1,2020
2020.acl-srw.15.txt,we also plan to combine the proposed multi-task neural model with back-translation method to enhance the ability of the nmt model on target-side language modeling.,1,2020
2020.acl-srw.16.txt,"in the future, we want to extend this method to language features other than words.",1,2020
2020.acl-srw.17.txt,"next, we aim to develop an actionable bayesian game-theoretic model for social talk, focusing on decomposing its utility function.",1,2020
2020.acl-srw.19.txt,"future work could include finding a way to incorporate other linguistic features like case-markers, gender, number, person, tense, aspect and verb agreement information into the parser.",1,2020
2020.acl-srw.2.txt,we plan to focus mostly on studying the possible application of gcn in this task.,4,2020
2020.acl-srw.2.txt,we will perform extensive experiments and report results in future work.,3,2020
2020.acl-srw.20.txt,"this suggests future work to reconsider how to match the training and evaluation to the actual objective of downstream applications, and thus create more reliable evaluation metrics and benchmarks.",3,2020
2020.acl-srw.22.txt,"in future, we would like to work on effective techniques to exploit monolingual data and parallel data from other languages together to improve the translation of low-resource languages.",2,2020
2020.acl-srw.23.txt,"further, we plan to use this decoder in an iterative, semi-supervised learning scenario akin to co-training (blum and mitchell, 1998).",1,2020
2020.acl-srw.23.txt,we plan to include constraints as part of decoding to aid in rule synthesis.,1,2020
2020.acl-srw.23.txt,we suspect that including such validity constraints will further improve the quality of the decoded rules.,1,2020
2020.acl-srw.24.txt,future qualitative work could also suggest further variables whose inclusion would enhance our knowledge of humor perception.,1,2020
2020.acl-srw.24.txt,"this could set a new standard for shared tasks which aim to model humor in future, and could outline a methodology that can be replicated with other cultures and languages.",1,2020
2020.acl-srw.26.txt,"our findings shed light on the importance of modeling points of correspondence, suggesting important future directions for sentence fusion.",1,2020
2020.acl-srw.28.txt,"future work would entail analysing and implementing more detailed underlying morphonological rules, and investigating the cross-over from fsts to neural models.",1,2020
2020.acl-srw.3.txt,our future work will target demonstrating the method on other languages.,2,2020
2020.acl-srw.3.txt,"we also hope to address semantic paraphasia in future work and create, deploy aac systems building on the method proposed in this paper.",1,2020
2020.acl-srw.30.txt,"also, we plan to extend the simple label embedding calculation methods to more sophisticated ones.",1,2020
2020.acl-srw.30.txt,"for future work, we envision to apply our method to other tasks and datasets and investigate the effectiveness.",4,2020
2020.acl-srw.31.txt,"it is also interesting to apply our methods to other languages requiring word segmentation, most notably, chinese.",4,2020
2020.acl-srw.31.txt,"while the focus of this paper was on data construction, developing a higher-quality typo correction system is the future direction to pursue.",2,2020
2020.acl-srw.32.txt,"moreover, we must develop a method for more accurately estimating the confidence scores, which is our primary focus in the next step.",1,2020
2020.acl-srw.34.txt,for the future we would like to apply our model on other cross-lingual nlp tasks such as xnli or cross-lingual semantic textual similarity.,4,2020
2020.acl-srw.35.txt,"in future work, we will extend our analysis to cover the more complex constructions mentioned in section 3.",1,2020
2020.acl-srw.35.txt,"we are also considering combining our system with an abduction mechanism that uses large knowledge bases (yoshikawa et al., 2019) for handling commonsense reasoning with external knowledge.",1,2020
2020.acl-srw.36.txt,"in the future, we plan to introduce constraints for asymmetric relations as well as extend our proposed method to leverage them.",1,2020
2020.acl-srw.36.txt,"moreover, we plan to experiment with adapting our model to a multilingual scenario, to be able to use it in a neural machine translation task.",4,2020
2020.acl-srw.37.txt,"in the future, we plan to experiment with even more challenging language pairs such as japanese–russian and attempt to leverage monolingual corpora belonging to diverse language families.we might be able to identify subtle relationships among languages and approaches to better leverage assisting languages for several nlp tasks.",2,2020
2020.acl-srw.38.txt,it is another promising area to be looked upon for reranking.,1,2020
2020.acl-srw.38.txt,one can investigate our approach with varying beam sizes and analyzing the effect of length penalty wu et al.(2016) and comparing it with methods such as yang et al.(2018).,3,2020
2020.acl-srw.38.txt,we also plan to explore the work by c¸aglar gu¨lc¸ehre et al.(2017) and c¸aglar gu¨lc¸ehre et al.(2015) that introduces language models into the existing neural architecture with methods such as shallow fusion and deep fusion.,1,2020
2020.acl-srw.39.txt,"in the future, it would be interesting to explore weak suervision and other data augmentation techniques to improve models’ robustness further.",1,2020
2020.acl-srw.39.txt,we also motivate the use of manifold mixup for further improvement.,1,2020
2020.acl-srw.40.txt,"for future work, we consider conducting the same experiments on cola, a dataset for judging the grammatical acceptability of a sentence (warstadt et al., 2019).",3,2020
2020.acl-srw.43.txt,our future agenda includes further bifurcating and exploring the specific types of victim blaming and the efficacy of the proposed approach on such a multi label classification task.,1,2020
2020.acl-srw.43.txt,we anticipate that this study encourages further research on how victims of sexual assault are portrayed on social media.,5,2020
2020.acl-srw.43.txt,we plan to explore the different weighting factors for the language modelling loss and classification loss described in section 4 to determine if weighting factors can help customize the auxiliary loss for different tasks.,1,2020
2020.acl-srw.6.txt,our research aims to apply the current work on transfer learning to new tasks and also find novel methods to obtain better multi-task learning models.,4,2020
2020.acl-srw.6.txt,transfer learning is a promising area of research for deep neural network based machine learning models.,1,2020
2020.acl-srw.8.txt,"this research aims to transfer word binary attributes (e.g., gender) for applications such as data augmentation of a sentence.",2,2020
2020.emnlp-main.1.txt,"our work could be improved also by including discourse properties (coherence, cohesiveness).",1,2020
2020.emnlp-main.1.txt,we leave this analysis to future work.,6,2020
2020.emnlp-main.10.txt,a future extension could explore more robust techniques for identifying abstract chains which do not make such assumptions.,1,2020
2020.emnlp-main.10.txt,extending the proposed approaches for longer chains is an important future direction.,1,2020
2020.emnlp-main.10.txt,"nonetheless, a useful future direction is exploring answer prediction and explanation prediction as joint goals, and perhaps they can benefit each other.",1,2020
2020.emnlp-main.101.txt,thus it will be interesting to investigate the performance of rrt in other applications of vae beyond topic modelling.,4,2020
2020.emnlp-main.103.txt,"future work includes using these approaches to induce model structure, develop accurate models with better interpretability, and to apply these approaches in lower data regimes.",1,2020
2020.emnlp-main.105.txt,"this is an interesting avenue for future work, for which zest should also be useful.",6,2020
2020.emnlp-main.105.txt,"to facilitate future work, we make our models, code, and data available at https://allenai.org/data/ zest.",1,2020
2020.emnlp-main.109.txt,(2) focusing on one of the most important crises of the future: water;,5,2020
2020.emnlp-main.109.txt,• unseen attribution factor: our model can generalize to unseen attributions factors.,1,2020
2020.emnlp-main.109.txt,this merits a deeper exploration with a holdout attribution set we aim to investigate in future.• flint water crisis: we were curious to know how our model performs in the wild on a data set of a different water crisis.,1,2020
2020.emnlp-main.110.txt,future semeval challenges should consider this when constructing test datasets and mention the hashtags and keywords they use for data collection.,5,2020
2020.emnlp-main.110.txt,"in the future, we hope to explore abstract topics like ‘immigration’ where differentiating between direct and indirect stance is non-trivial and ensemble models that combine the strengths of multiple methods.",1,2020
2020.emnlp-main.110.txt,"this suggests that future research should explore approaches like coreference resolution (for pronouns), word sense disambiguation (for epithets), and background knowledge (relationships to other entities).",1,2020
2020.emnlp-main.111.txt,"in the future, we will explore how to apply our model to more domains, and enhance the interpretability of the reasoning path when the model answers questions.",4,2020
2020.emnlp-main.116.txt,"in the future, we plan to focus on how to improve the performance of medical entity normalization when resources are limited.",5,2020
2020.emnlp-main.12.txt,"while we leave a detailed study to future work, we expect general trends regarding the value of perturbations to hold broadly.",5,2020
2020.emnlp-main.120.txt,we are excited about future work that could extend our motivation and further aim at incorporating stronger hierarchy into the language model architectures and the pre-training tasks.,1,2020
2020.emnlp-main.122.txt,"in future work, we will explore generalizing this approach to the multilingual setting, or applying it to the pre-train and fine-tune paradigm used widely in other models such as bert.",4,2020
2020.emnlp-main.123.txt,"furthermore, this aligner may help to build or increase semantic resources, using a promising approach as back-translation (sobrevilla cabezudo et al., 2019).",2,2020
2020.emnlp-main.123.txt,"future work includes adopting multilingual word embeddings (lample et al., 2018) to produce alignments for other languages.",2,2020
2020.emnlp-main.123.txt,"this simple approach may be adopted for other languages with few resources, aiming to get tools for natural language understanding tasks.",4,2020
2020.emnlp-main.124.txt,"in the future, we want to explore semi-supervised methods for sentence embedding and its transferability across domains.",1,2020
2020.emnlp-main.125.txt,"we intend to expand our method to conduct forest alignments for making it robust against parsing errors, which are inevitable in handling large corpora.",1,2020
2020.emnlp-main.125.txt,"we plan to apply it to a comparable corpus of partial paraphrases and investigate the performance, with the aim of creating a large-scale syntactic and phrasal paraphrase dataset.",2,2020
2020.emnlp-main.126.txt,"the proposed method can further contribute to other semi-structured data (table, graph, etc.)related tasks, e.g.",4,2020
2020.emnlp-main.126.txt,there still exists plenty of potentials that require future studies in this direction.,6,2020
2020.emnlp-main.126.txt,"wikitablequestions (pasupat and liang, 2015) and commonsenseqa (talmor et al., 2019).",5,2020
2020.emnlp-main.128.txt,"in the future, we would adapt our method to other ie tasks to study its application scope.",4,2020
2020.emnlp-main.129.txt,"in the future, we will extend maven to more event-related tasks like event argument extraction, event sequencing, etc.",4,2020
2020.emnlp-main.129.txt,"we also explore some promising directions with analytic experiments, including modeling multiple event correlations (section 5.3), utilizing the hierarchical event schema to distinguish close types (section 5.6), and improving other ed tasks with transfer learning (section 5.5).",3,2020
2020.emnlp-main.13.txt,"an investigation of how, whether, and why formalisms and their implementations affect probing results for tasks beyond role labeling and for frameworks beyond edge probing constitutes an exciting avenue for future research.",5,2020
2020.emnlp-main.133.txt,another direction is to generalize the way in which the table and sequence interact to other types of representations.,1,2020
2020.emnlp-main.133.txt,"in the future, we would like to investigate how the table representation may be applied to other tasks.",4,2020
2020.emnlp-main.136.txt,"in the future, we will investigate the feasibility of incorporating classical mds guidance to abstractive models with large-scale pre-training (gu et al., 2020) and more challenging settings where each document set may contain hundreds or even thousands of documents.",1,2020
2020.emnlp-main.137.txt,another intriguing direction is exploring the connection between our methods and neural network interpretability.,1,2020
2020.emnlp-main.137.txt,"finally, although we are motivated primarily by the widespread use of topic models for identifying interpretable topics (boyd-graber et al., 2017, ch.3), we plan to explore the ideas presented here further in the context of downstream applications like document classification.",1,2020
2020.emnlp-main.137.txt,"in future work, we also hope to explore the effects of the pretraining corpus (gururangan et al., 2020) and teachers (besides bert) on the generated topics.",2,2020
2020.emnlp-main.137.txt,"we believe mining this connection can open up further research avenues; for instance, by investigating the differences in such teacher-topics conditioned on the pre-training corpus.",2,2020
2020.emnlp-main.138.txt,future works could focus on employing the proposed model in more downstream tasks.,4,2020
2020.emnlp-main.140.txt,"as a first attempt to introduce macro-level meta-features for strategy selection, we believe there is much potential to refine and improve our approach.",1,2020
2020.emnlp-main.140.txt,"furthermore, the smart-kpe framework can be easily adapted to other nlp tasks, and we believe there is much potential in combining smart-kpe with different models to further boost performance on opendomain kpe and other web-related tasks.",4,2020
2020.emnlp-main.140.txt,we also plan to add more types of meta-features to generate richer multimodal representations.,1,2020
2020.emnlp-main.142.txt,"in this work, we explore unsupervised disfluency detection by combining self-training and selfsupervised learning.",1,2020
2020.emnlp-main.147.txt,we will explore this direction in future work.,6,2020
2020.emnlp-main.148.txt,"in the future, we will further explore how to explicitly incorporate linguistics information, such as named entities into the latent states.",1,2020
2020.emnlp-main.149.txt,"in future work, we will further investigate othercontent generation problems by leveraging multi-granularity copying mechanism.",5,2020
2020.emnlp-main.15.txt,future work will be separated into two strands.,6,2020
2020.emnlp-main.15.txt,"the first will focus on how to better model the distribution of embeddings given a morphosyntactic attribute; as mentioned above, this should yield a better probe overall.",1,2020
2020.emnlp-main.150.txt,"in the future, we will move on to develop a more general dialogue dependency parser and better incorporate dependency information into dialogue context modeling tasks.",1,2020
2020.emnlp-main.152.txt,"in the future, we plan to extend our nonautoregressive refiner to other natural language understanding (nlu) tasks, e.g., named entity recognition (tjong kim sang and de meulder, 2003), semantic role labeling (he et al., 2018), and natural language generation (nlg) tasks, e.g., machine translation (vaswani et al., 2017), summarization (liu and lapata, 2019).",4,2020
2020.emnlp-main.153.txt,"in future work, we would like to apply our approach on document-level and multi-document nlu tasks.",4,2020
2020.emnlp-main.154.txt,"another possible avenue for future work is to use crows-pairs to help directly debias lms, by in some way minimizing a metric like ours.",1,2020
2020.emnlp-main.16.txt,future work might use msgs as a diagnostic tool to measure how effectively new model architectures and selfsupervised pretraining tasks can more efficiently equip neural networks with better inductive biases.,1,2020
2020.emnlp-main.16.txt,these models could prove to be a helpful resource for future studies looking to study learning curves of various kinds with respect to the quantity of pretraining data.,2,2020
2020.emnlp-main.160.txt,one area for future work would be to better identify and model words that either don’t have a visual grounding or whose identified visual grounding doesn’t align with human expectation.,1,2020
2020.emnlp-main.161.txt,"we consider extension of our model to other videoand-language tasks as future work, as well as developing more well-designed pre-training tasks.",4,2020
2020.emnlp-main.163.txt,"last but not least, didan and neuralnews may be leveraged to supplement fact verification in detecting humanwritten misinformation in general by evaluating visual-semantic consistency.",3,2020
2020.emnlp-main.163.txt,other interesting avenues for future research is to understand the importance of metadata in this multimodal setting and investigating counter-attacks to improved generators that incorporate image-text consistency.,1,2020
2020.emnlp-main.163.txt,"we hope future work will address any potential limitations of this work, such as expanding the dataset to evaluate generalization across different news sources, and a larger variety of neural generators.",2,2020
2020.emnlp-main.164.txt,the performance of softproto can be further improved after introducing the large-scale external unlabeled data like yelp and amazon reviews.,2,2020
2020.emnlp-main.165.txt,"as to future work, we plan to explore how to jointly extract entities and relations in federated settings.",1,2020
2020.emnlp-main.167.txt,"there are many potential directions for future work on oia, including 1) more labeled data; 2) better learning algorithm; 3) becoming crosslingual by adding support for more natural languages; 4) porting existing oie strategies on oia and evaluating the performance compared with the original ones.",2,2020
2020.emnlp-main.169.txt,"in future work, we would like to investigate methods to stabilize the training of weight tied models and apply our model on other tasks in natural language generation.",4,2020
2020.emnlp-main.171.txt,"we hope that our insights, including some of our negative results, may encourage future research on learning with latent structures.",1,2020
2020.emnlp-main.172.txt,"the results provide new insights into the strengths and weaknesses of english part-of-speech tagging models, complementing other approaches to model comparison and interpretation.",1,2020
2020.emnlp-main.173.txt,"in the future, we are interested in social science topics, such as modeling the causal effect between mental health and the suicide decisions reflected through social media, which may help predict and stop the final decisions.",1,2020
2020.emnlp-main.174.txt,developing methods improving both memory and inference efficiency without sacrificing task performance can open the possibility of widely deploying the powerful pretrained language models to more nlp applications.,4,2020
2020.emnlp-main.174.txt,"future work may explore the possibility of applying masking to the pretrained multilingual encoders like mbert (devlin et al., 2019) and xlm (conneau and lample, 2019).",4,2020
2020.emnlp-main.175.txt,"in the future, we will select context sentences in larger candidate space, and explore more effective ways to extend our approach to select target-side context sentences.",1,2020
2020.emnlp-main.176.txt,"future directions include exploring advanced identification and rejuvenation models that can better reflect the learning abilities of nmt models, as well as validating on other nlp tasks such as dialogue and summarization.",1,2020
2020.emnlp-main.177.txt,"in future work, we will explore other such applications of our proposed methods.",4,2020
2020.emnlp-main.179.txt,"in the future, the proposed mgl method can potentially applied to more cross-lingual natural language understanding (xlu) tasks (conneau et al., 2018b; wang et al., 2019; lewis et al., 2019; karthikeyan et al., 2020), and be generalized to learn to learn for domain adaptation (blitzer et al., 2007), representation learning (shen et al., 2018), multi-task learning (shen et al., 2019) problems, etc. universal syntactic interpretations are valuable language interpretations, which have been developed in years of study.",4,2020
2020.emnlp-main.182.txt,"we believe that our framework is general and can be applied to many other structured prediction tasks in nlp, such as neural machine translation, semantic parsing and so on.",4,2020
2020.emnlp-main.183.txt,future work includes finding applications of our novel tagging scheme in other tasks involving extracting triplets as well as extending our approach to support other tasks within sentiment analysis.,4,2020
2020.emnlp-main.184.txt,"we hope that future research continues this line of work, especially by finding novel ways to devise adaptive policies – such as reinforcement learning models with the visual modality.",1,2020
2020.emnlp-main.185.txt,we hope that this new challenging evaluation set will foster further research in multilingual commonsense reasoning and cross-lingual transfer.,3,2020
2020.emnlp-main.186.txt,in future research we also plan an in-depth study of these factors and their relation to our spectral analysis.,1,2020
2020.emnlp-main.186.txt,"our findings, suggesting that it is possible to effectively combine these two types of language distance measures, call for further research that will advance our understanding of: 1) what knowledge is captured in monolingual and cross-lingual embedding spaces (gerz et al., 2018; pires et al., 2019; artetxe et al., 2020); 2) how that knowledge complements or overlaps with linguistic knowledge compiled into lexical-semantic and typological databases (dryer and haspelmath, 2013; wichmann et al., 2018; ponti et al., 2019); and 3) how to use the combined knowledge for more effective transfer in cross-lingual nlp applications (ponti et al., 2018; eisenschlos et al., 2019).",1,2020
2020.emnlp-main.186.txt,we believe that the main insights from this study will inform and guide different cross-lingual transfer learning methods and scenarios in future work.,1,2020
2020.emnlp-main.187.txt,"the benefits are noticeable in multilingual nmt tasks, like language clustering and ranking related languages for multilingual transfer.",4,2020
2020.emnlp-main.187.txt,"we plan to study how to deeply incorporate our typologically-enriched embeddings in multilingual nmt, where there are promising avenues in parameter selection (sachan and neubig, 2018) and generation (platanios et al., 2018).",1,2020
2020.emnlp-main.19.txt,"as future work, we would like to investigate complementary attention mechanisms like those of reformer (kitaev et al., 2020) or routing transformer (roy et al., 2020), push scalability with ideas like those from revnet (gomez et al., 2017), and study the performance of etc in datasets with even richer structure.",1,2020
2020.emnlp-main.191.txt,"in future, we plan to explore how to incorporate discourse parsing into the current decision making model for end-to-end learning.",1,2020
2020.emnlp-main.191.txt,one possibility would be to frame them as multi-task learning with a common (shared) encoder.,1,2020
2020.emnlp-main.191.txt,we also conduct comprehensive analyses to unveil the limitations of discern and challenges for sharc.,5,2020
2020.emnlp-main.192.txt,"in the future, we will make a model learn commonsense with the obtained dataset and consider applying it to semantic tasks, such as anaphora resolution and discourse parsing.",4,2020
2020.emnlp-main.192.txt,"to acquire a wider range of commonsense, it is possible to combine our method with other methods based on physical world resources, such as video captions used in swag.",1,2020
2020.emnlp-main.194.txt,researchers could also use our 140 domain-specific adapters and investigate further combination techniques to make them even more broadly applicable.,1,2020
2020.emnlp-main.195.txt,"it would therefore be promising to extend this line of our research to exploit larger multilingual semantic resources, in order to further improve the parsing quality.",2,2020
2020.emnlp-main.195.txt,these amr representations could then be integrated into downstream crosslingual tasks to investigate their added value.,1,2020
2020.emnlp-main.195.txt,we explored transfer learning techniques to enable high performance cross-lingual amr parsing.,1,2020
2020.emnlp-main.197.txt,"as a future research, semantically challenging cases at fine-grained level with respect to complexities of abusive/offensive (targeted) and profane (untargeted) language demand further investigation.",5,2020
2020.emnlp-main.198.txt,"we also plan to add domain-specific features to our model, collect more data, integrate existing suicidal risk datasets with various languages to improve performance.",1,2020
2020.emnlp-main.200.txt,"in addition, it is worthwhile to further model information propagation and temporal correlation of comments in the future.",1,2020
2020.emnlp-main.200.txt,it is also crucial to understand why a media session is detected as cyberbullying.,5,2020
2020.emnlp-main.200.txt,such results can encourage future studies to develop advanced graph neural networks in better representing the interactions between heterogeneous information.,1,2020
2020.emnlp-main.202.txt,"this raises the question of how ssnmt will perform on really distant languages (less homographs) or when using smaller bpe sizes (more homographs), which is something that we will examine in our future work.",5,2020
2020.emnlp-main.206.txt,the segmenter model itself could also benefit from the incorporation of additional text data as well as pre-training procedures.,1,2020
2020.emnlp-main.206.txt,we plan to look into additional acoustic features as well as possible ways to incorporate asr information to the segmentation process.,1,2020
2020.emnlp-main.207.txt,"furthermore, we wish to explore semi-supervised and unsupervised approaches to leverage monolingual data and explore multilingual machine translation for low-resource indic languages.",1,2020
2020.emnlp-main.207.txt,"in future, we plan to design segmentation-agnostic aligners or aligners that can jointly segment and align sentences.",1,2020
2020.emnlp-main.207.txt,"laser fails to identify one-tomany/many-to-one sentence alignments, we want to address this.",5,2020
2020.emnlp-main.207.txt,"we would also like to experiment with bert (devlin et al., 2019) embeddings for similarity search.",3,2020
2020.emnlp-main.208.txt,"firstly, we are interested in applying csp to other related nlp areas for code-switching problems.",4,2020
2020.emnlp-main.208.txt,"secondly, we plan to investigate the pre-training objectives which are more effective in utilizing the cross-lingual alignment information for nmt.",1,2020
2020.emnlp-main.208.txt,there are two promising directions for the future work.,6,2020
2020.emnlp-main.210.txt,"in future work, we will pre-train on larger corpus to further boost the performance.",2,2020
2020.emnlp-main.210.txt,we leave different alignment approaches to be explored in the future.,1,2020
2020.emnlp-main.212.txt,"in the future, we will continue investigate the learning method for effectively utilizing self-generated samples and expand to other text generation tasks.",1,2020
2020.emnlp-main.212.txt,"our work can employ on different text generation tasks, e.g., text summarization and dialogue, to enhance the key phrases (or terms) generation.",4,2020
2020.emnlp-main.213.txt,"a primary avenue for future work on comet will look at the impact of more compact solutions such as distilbert (sanh et al., 2019).",1,2020
2020.emnlp-main.213.txt,future work will investigate the optimality of this formulation and further examine the interdependence of the different inputs.,1,2020
2020.emnlp-main.214.txt,"in future work, we will apply our method to languages with corpora from diverse domains and also to other languages.",4,2020
2020.emnlp-main.216.txt,"while we showed that uncertainty-aware semantic augmentation with gaussian priors is effective, more work is required to investigate if such an approach will also be successful for more sophisticated priors.",1,2020
2020.emnlp-main.218.txt,"in the future work, we will extend our method by replacing the simple role matching score with grammatical or semantic similaritybased measures to improve the alignment accuracy.",1,2020
2020.emnlp-main.219.txt,future research could explore neural architectures and training losses tailored to our approach.,1,2020
2020.emnlp-main.22.txt,there are several directions we will further explore in the future.,6,2020
2020.emnlp-main.220.txt,we hope this is a factor that designers of future syntactic treebanks will take into account.,1,2020
2020.emnlp-main.222.txt,our future work will include conducting experiments on dependency trees and more nlp tasks.,3,2020
2020.emnlp-main.223.txt,"moreover, we display the ability of ted-cdb to help address the issue of insufficient or unbalanced data on other corpora and improve the performance of models for other languages.",1,2020
2020.emnlp-main.224.txt,"in future work, we plan to extend the annotation process to also cover inter-sentential relations.",2,2020
2020.emnlp-main.225.txt,"in future, we plan to evaluate disa on other discourse analysis tasks.",3,2020
2020.emnlp-main.226.txt,one way to mitigate this issue is to leverage longer context information to identify better keywords which is subject of the future work.,1,2020
2020.emnlp-main.226.txt,"writingprompts (fan et al., 2018)) is a subject for future work.",6,2020
2020.emnlp-main.227.txt,"in the future, we will investigate on extending our approach to more areas.",4,2020
2020.emnlp-main.229.txt,"however,as discussion in error analysis, there are several challenges to solve in the future.",5,2020
2020.emnlp-main.235.txt,"as future work, we will further study our method’s ability of extreme multi-label learning (bhatia et al., 2016) and different document encoders.",1,2020
2020.emnlp-main.237.txt,"in the future, we will explore to extend the idea of disentanglement in the continual learning of other nlp tasks.",4,2020
2020.emnlp-main.239.txt,"in the future, we plan to study different regularizers in the asymmetrical text matching task, for further exploring their effectiveness in bridging the gap between asymmetrical domains.",1,2020
2020.emnlp-main.24.txt,"first, the word clouds may reveal sensitive contents in the training data to human debuggers.",2,2020
2020.emnlp-main.24.txt,"for example, using relu as activation functions in lstm cells (instead of tanh) renders the features non-negative.",1,2020
2020.emnlp-main.24.txt,"for future work, it would be interesting to extend find to other nlp tasks, e.g., question answering and natural language inference.",4,2020
2020.emnlp-main.24.txt,"in order to generalize the framework beyond cnns, there are two questions to consider.",5,2020
2020.emnlp-main.24.txt,it is also convenient to use since only the trained model and the training data are required as input.,1,2020
2020.emnlp-main.24.txt,"third, it is possible that one feature detects several patterns (jacovi et al., 2018) and it will be difficult to disable the feature if some of the detected patterns are useful while the others are harmful.",1,2020
2020.emnlp-main.24.txt,this will require some modifications to understand how the features capture relationships between two input texts.,1,2020
2020.emnlp-main.24.txt,"we exemplified this with two word clouds representing each bilstm feature in appendix c, and we plan to experiment with advanced visualizations such as lstmvis (strobelt et al., 2018) in the future.",1,2020
2020.emnlp-main.240.txt,"we propose to solve each of them with existing techniques (artetxe et al., 2018b; alvarezmelis and jaakkola, 2018; jawanpuria et al., 2019).",1,2020
2020.emnlp-main.244.txt,"on gpus we cannot expect a reduction in the number of operations to translate 1:1 to lower execution times, since they are highly optimised for parallelism.3 we leave the parallelism enhancements of skylinebuilder for future work.",1,2020
2020.emnlp-main.245.txt,"while our model is not a neural module network, as our model uses a single fixed layout instead of different layouts per question, we believe there are enough similarities that future work could explore combining our modules with those used in other neural module networks over text, leading to a single model that could perform the necessary reasoning for multiple different datasets.",1,2020
2020.emnlp-main.246.txt,we deliberately choose to leave experiments including real human annotators to future research for the following reason.,3,2020
2020.emnlp-main.249.txt,"additional future investigations may include a deeper analysis of the mathematical and statistical properties of the weighted coefficients ρw, τw, as well as a rigorous derivation of the optimal values for the parameters of the data collection approach.",1,2020
2020.emnlp-main.249.txt,"as future work, we plan to collect human annotations (i) to test the proposed data collection approach on real data and (ii) to assess the validity and estimate the parameters of the proposed stochastic transitivity model.",3,2020
2020.emnlp-main.25.txt,we plan to address these challenges in future work.,6,2020
2020.emnlp-main.250.txt,"in the future, we plan to apply mft to other language models (e.g., transformerxl (dai et al., 2019) and albert (lan et al., 2019)) and for other nlp tasks.",4,2020
2020.emnlp-main.251.txt,"in future work, we will focus on producing novel continuations of the user’s search intent, extending the approach to other domains, and automating the design of behavioral hypotheses.",4,2020
2020.emnlp-main.252.txt,"besides, how to enable the existing emotion-cause pair extraction models to consider the effect of context is also a meaningful task.",1,2020
2020.emnlp-main.255.txt,"regarding the model-agnostic and task-agnostic properties of our method, they are applicable to any types of nlp model for various tasks, such as neural machine translation and visual question answering.",4,2020
2020.emnlp-main.256.txt,"as fc is only one test bed for adversarial attacks, it would be interesting to test this method on other nlp tasks requiring semantic understanding such as question answering to better understand shortcomings of models.",3,2020
2020.emnlp-main.257.txt,"the study suggests that besides improving our alignment algorithms for distant languages (vulic et al.´ , 2019), we should also focus on improving monolingual word vector spaces, and monolingual training conditions to unlock a true potential of cross-lingual learning.",1,2020
2020.emnlp-main.258.txt,"in the future, we plan to apply fa-rnn to other tasks and explore other variants of fa-rnn.",4,2020
2020.emnlp-main.26.txt,"finally, we believe that using attention mechanisms to study the grounding of the edits, similarly to the ideas in kohn ¨ (2018), can be an important step towards understanding how the preliminary representations are built and decoded; we want to test this as well in future work.",1,2020
2020.emnlp-main.261.txt,we hope that this work will help the research community interpret bert for other complex tasks and explore the above open-ended questions.,5,2020
2020.emnlp-main.266.txt,we constrain types to reduce the solution space and add negative relationships to leverage negative training samples.,1,2020
2020.emnlp-main.270.txt,"examples of more sophisticated game scenarios are bidirectional conversations where multi-symbol messages are challenging to analyse (kottur et al., 2017; bouchacourt and baroni, 2019) or games with image sequences as input (santamaría-pang et al., 2019).",5,2020
2020.emnlp-main.270.txt,"in light of these results, it would be interesting to explore the use of unsupervised tokenisers that work well for languages without spaces (e.g.",2,2020
2020.emnlp-main.270.txt,"sentencepiece kudo and richardson, 2018) prior to our approach and to try other word embedding models for diora, such as the character-based elmo embeddings8 (peters et al., 2018) or the more recent bert (devlin et al., 2019).",1,2020
2020.emnlp-main.271.txt,we believe that the idea of subinstruction module and a sub-instruction annotated dataset can benefit future studies in the vln task as well as other vision-and-language problems.,2,2020
2020.emnlp-main.273.txt,"in addition, two pre-trained seq2seq language models: t5 (raffel et al., 2019) and bart (lewis et al., 2019) are incorporated in our framework.",1,2020
2020.emnlp-main.273.txt,"in future work, we plan to explore taskoriented dialogues domain-adaptive pre-training methods (wu et al., 2020; peng et al., 2020) to enhance our language model backbones, and extend the framework for mixed chit-chat and taskoriented dialogue agents (madotto et al., 2020a).",1,2020
2020.emnlp-main.274.txt,"we would also like to explore different implementations in line with recent advances in dialog models, especially using large-scale pre-trained language models.",1,2020
2020.emnlp-main.275.txt,"in the future, we would explore three aspects: (1) more efficient posterior information representation and corresponding prediction module, (2) the interpretability of knowledge selection and (3) knowledge selection without knowledge label.",1,2020
2020.emnlp-main.277.txt,our method may inspire other research in low-resource nlp tasks.,4,2020
2020.emnlp-main.278.txt,"in this paper, one main focus is to demonstrate the differences between background and decisiontime planning.",3,2020
2020.emnlp-main.278.txt,this might be an interesting topic for future work.,6,2020
2020.emnlp-main.28.txt,"for the future work, we suggest to integrate the ranking models and generation model, e.g., in beam search stage or reinforcement learning using ranking score as reward signal.",1,2020
2020.emnlp-main.283.txt,for future work it would be interesting to test these sense embeddings in a wider range of applications outside wsd.,3,2020
2020.emnlp-main.285.txt,"as future work, we plan to exploit the information brought by our embeddings to other downstream tasks, such as multilingual semantic role labeling (di fabio et al., 2019; conia et al., 2020) and cross-lingual semantic parsing (blloshmi et al., 2020).",4,2020
2020.emnlp-main.287.txt,"in some sentences, phrases or clauses rather than words indicate the given aspect category, future work could consider multi-grained instances, including words, phrases and clauses.",1,2020
2020.emnlp-main.287.txt,"since directly finding the key instances for some aspect categories is ineffective, we will try to first recognize all opinion snippets in a sentence, then assign these snippets to the aspect categories mentioned in the sentence.",1,2020
2020.emnlp-main.288.txt,"one future direction is to investigate how to integrate the two different attention mechanisms, namely the standard attention and structured attention for nlp applications.",1,2020
2020.emnlp-main.289.txt,"in the future, we will explore the extension of this approach to achieve full coverage.",1,2020
2020.emnlp-main.29.txt,collecting multiple gold sql query references for evaluation (like machine translation) might be a potential solution.,3,2020
2020.emnlp-main.29.txt,our test suites will be released for eleven datasets so that future works can conveniently evaluate test suite accuracy.,3,2020
2020.emnlp-main.291.txt,"furthermore, we would like to investigate other approaches (e.g., graph-based neural network) to better model the modality and label dependence in multi-modal multi-label emotion detection.",1,2020
2020.emnlp-main.291.txt,"in our future work, we will extend our approach to more multi-modal multi-label scenarios, such as intention detection in video conversations and aspect analysis in multi-modal reviews.",4,2020
2020.emnlp-main.296.txt,"in the future, we would like to generate abstractive summaries following an unsupervised approach (baziotis et al., 2019; chu and liu, 2019) and investigate how recent advances in open domain qa (wang et al., 2019; qi et al., 2019) can be adapted for query focused summarization.",1,2020
2020.emnlp-main.297.txt,"in the future, we would like to investigate other objectives to pre-train seq2seq models for abstractive summarization.",1,2020
2020.emnlp-main.298.txt,"in the future, we will continue to explore better re pre-training techniques, especially with a focus on open relation extraction and relation discovery.",1,2020
2020.emnlp-main.3.txt,"in future work, we would like to improve comment matching, e.g., by making it stance-aware.",1,2020
2020.emnlp-main.3.txt,we also plan to experiment with sequence-tosequence neural models for generating key point candidates from comments.,1,2020
2020.emnlp-main.300.txt,"in the future, we will explore how to improve the efficiency of our pre-training.",1,2020
2020.emnlp-main.303.txt,"in future work, we plan to integrate knowledge graphs and explore other document graph modeling ways (e.g., hierarchical graphs) to improve the performance.",1,2020
2020.emnlp-main.304.txt,"as future work, we intend to explore its application in those fields.",4,2020
2020.emnlp-main.306.txt,we plan to explore the utility of this architecture in other nlp problems.,4,2020
2020.emnlp-main.308.txt,"as future work, we plan to generalize the ept to other datasets, including non-english word problems or non-algebraic domains in math, to extend our model.",2,2020
2020.emnlp-main.31.txt,"future work is also needed to handle attributes containing long free-form text, as autoqa currently only supports database operations without reading comprehension.",2,2020
2020.emnlp-main.310.txt,"in the future, we would like to extend gtm to corpora with explicit doc-doc interactions, e.g., scientific documents with citations or social media posts with user relationships.",2,2020
2020.emnlp-main.310.txt,replacing gcn in gtm with more advanced graph neural networks is another promising research direction.,1,2020
2020.emnlp-main.311.txt,there are several directions to explore in the future.,6,2020
2020.emnlp-main.312.txt,"in the future, we would like to explore if the learner-like agent can be extended to materials and data beyond the example sentences for near-synonyms.",2,2020
2020.emnlp-main.313.txt,"moreover, we will conduct further exploration of the multiproduct ad post form, including more vivid multimedia information, such as pictures and videos.",2,2020
2020.emnlp-main.314.txt,we are also releasing a part of our forms dataset to aid further research in this direction.,2,2020
2020.emnlp-main.318.txt,"in particular, we will enhance the practicability of chinese word segmentation to improve the effectiveness of other downstream chinese nlp tasks.",4,2020
2020.emnlp-main.318.txt,"in the future, we will continue studying the efficiency of the neural architecture, and pay attention to improving the speed of both training and testing steps on an ever-increasing dataset.",1,2020
2020.emnlp-main.319.txt,"we also plan to extend our framework to semi-supervised learning, where a small number of annotations might also be available in the target language.",4,2020
2020.emnlp-main.319.txt,we have also contributed two quality controlled datasets (compatible with propbank-style guidelines) which we hope will be useful for the development of crosslingual models.,2,2020
2020.emnlp-main.32.txt,extending our method to an abstractive setting is meaningful future work.,1,2020
2020.emnlp-main.321.txt,future work should address the application of our method to more and typologically more divergent languages.,4,2020
2020.emnlp-main.322.txt,"given that gcns over dependency and constituency structure have access to very different information, it would be interesting to see in future work if combining two types of representations can lead to further improvements.",1,2020
2020.emnlp-main.322.txt,"while we experimented only with constituency syntax, spangcn may be able to encode any kind of span structure, for example, co-reference graphs, and can be used to produce linguistically-informed encoders for other nlp tasks rather than only srl.",1,2020
2020.emnlp-main.323.txt,"in future work, one could make the a* parser more accurate by extending it to non-projective dependency trees, especially on dm, eds and amr.",1,2020
2020.emnlp-main.323.txt,it would also be interesting to see if our method for avoiding dead ends can be applied to other formalisms with complex symbolic restrictions.,4,2020
2020.emnlp-main.327.txt,future work should investigate more rewards for training an open-domain dialog model such as long term conversation rewards that may need to be computed over many conversation turns.,1,2020
2020.emnlp-main.333.txt,"in the future, we plan to apply the transformed representations on more lexical semantics tasks such as word sense disambiguation within an application (navigli and vannella, 2013).",4,2020
2020.emnlp-main.334.txt,we hope this will support work on solutions for nlp applications and resources that can better serve minorities and underrepresented groups.,5,2020
2020.emnlp-main.336.txt,"in the future, we plan to annotate some of the data, explore supervised segmentation models (li et al., 2018) and introduce more conversation structures like dialogue acts (oya and carenini, 2014; joty and hoque, 2016) into abstractive dialogue summarization.",2,2020
2020.emnlp-main.338.txt,future work may explore the use of points of correspondence and sentence fusion in the standard setting of document summarization.,1,2020
2020.emnlp-main.339.txt,"future work will focus on extending our models to generate extractive plans for better abstractive summarization of long or multiple documents (liu et al., 2018).",1,2020
2020.emnlp-main.34.txt,our work paves the way for further research on bridging q-learning and unsupervised text summarization.,1,2020
2020.emnlp-main.340.txt,"for future work, we think it will be interesting to look at: 1. zero-shot clir models for low-resource languages, 2. comparison of end-to-end neural rankers with traditional translation+ir pipelines in terms of both scalability, cost, and retrieval accuracy, 3. advanced neural architectures and training algorithms that can exploit our large training data, 4. building universal models for multilingual ir.",1,2020
2020.emnlp-main.340.txt,"the large number of supported language directions allows the research community to explore and build new models for many more languages, especially the low-resource ones.",1,2020
2020.emnlp-main.342.txt,these findings provide opportunities for future work towards more efficient and interpretable neural ir.,1,2020
2020.emnlp-main.343.txt,"as a future work, we plan to utilize automatic summarization for missing abstracts, instead of taking the first 512 content tokens.",1,2020
2020.emnlp-main.345.txt,"as an extension, it may even be possible to determine the number of les k from the data using recurrent neural networks (yang et al., 2017).",1,2020
2020.emnlp-main.345.txt,one avenue of future work is to learn explainable rules that domain experts can interact with on top of such embeddings.,1,2020
2020.emnlp-main.345.txt,"while recent embeddings (devlin et al., 2019) may lead to improved accuracy, these remain poorly understood (moradshahi et al., 2019).",5,2020
2020.emnlp-main.35.txt,"in the future, we will study the effectiveness of ta on other nlp tasks, such as the document-level translation, and investigate whether ta is useful for transformer pre-training.",4,2020
2020.emnlp-main.350.txt,a promising research direction is to investigate the root cause behind memorization.,5,2020
2020.emnlp-main.350.txt,"therefore, another research avenue would be to blend the two frameworks (im et al., 2017).",1,2020
2020.emnlp-main.351.txt,our findings also suggest future work on additional ways to incorporate story principles into plot generation.,1,2020
2020.emnlp-main.351.txt,"there is also further work to be done in investigating what models are best able to incorporate plots, which would enable plot improvements to be even more effective.",1,2020
2020.emnlp-main.353.txt,generating subsequent references with such properties has the potential to enhance user adaptation and successful communication in dialogue systems.,1,2020
2020.emnlp-main.355.txt,"in the future, we plan to examine the hierarchical classification architecture’s potential for reducing computational runtime.",3,2020
2020.emnlp-main.356.txt,we have only begun to explore the possibilities opened up by pose traces.,5,2020
2020.emnlp-main.358.txt,in future work we plan to incorporate crosslingual signals as vulic et al.´ (2019) argue that a fully unsupervised setting is hard to motivate.,1,2020
2020.emnlp-main.36.txt,"for future work, we plan to investigate the use of a more powerful language model, such as megatron-lm (shoeybi et al., 2019), as the teacher; and different strategies for choosing hard negatives to further boost the performance.",1,2020
2020.emnlp-main.361.txt,"future work will investigate the compositional capability of these adapters, and combine domain and monolingual adapters for nmt.",1,2020
2020.emnlp-main.362.txt,"therefore, scaling models to more raw text and larger capacity models may be more beneficial for producing better cross-lingual models.",1,2020
2020.emnlp-main.362.txt,"therefore, we make the following recommendations for future work on cross-lingual alignment or multilingual representations: 1) evaluations should consider average quality data, not exclusively high-quality bitext.2) evaluation must consider multiple nlp tasks or datasets.3) evaluation should report mean and variance over multiple seeds, not a single run.",3,2020
2020.emnlp-main.363.txt,"this finding provides a strong incentive for intensifying future research efforts that focus on cheap or naturally occurring supervision (vulic et al.´ , 2019; artetxe et al., 2020c; marchisio et al., 2020), quick and simple annotation procedure, and the more effective few-shot transfer learning setups.",1,2020
2020.emnlp-main.364.txt,"for future work, we plan to expand the automatic domain induction methods and test the mdkd framework on generic mt with data exhibiting varying degrees of heterogeneity: as mdkd distills domain-specific models to create multiple simpler data distributions, we want to investigate if inducing train-time specializations and using them for distillation through mdkd can lead to better quality.",3,2020
2020.emnlp-main.365.txt,we extensively tested the approach for various languages from different language families.,3,2020
2020.emnlp-main.366.txt,"we also apply our method to both semantic and syntactic parsing, demonstrating our method’s broader applicability to tasks that process variable-output-length data in a sequential manner.",4,2020
2020.emnlp-main.368.txt,future studies will extend this work to other crosslingual nlp tasks and more languages.,4,2020
2020.emnlp-main.369.txt,"with these contributions, we hope that this corpus will be an important resource to the research community.",2,2020
2020.emnlp-main.370.txt,"together with this paper, we release our dataset16 and models17, which we hope will enable the ai research community to explore effective approaches to incorporate commonsense reasoning capabilities into various downstream tasks.",1,2020
2020.emnlp-main.373.txt,"an interesting future direction is to generate each clarification in response to the previous ones, in a dialogue setup (saeidi et al., 2018).",1,2020
2020.emnlp-main.373.txt,we hope that our framework will facilitate future research in this area.,6,2020
2020.emnlp-main.375.txt,scaling these carefully-controlled methods to the larger data setting will be an important next step.,2,2020
2020.emnlp-main.376.txt,"another line of future research is to compare the incremental predictions of neural models to finergrained eye-tracking evidence during sentence processing of double-object sentences (e.g.filik et al., 2004).",3,2020
2020.emnlp-main.377.txt,"in the future, our approach and new evaluation measure could be applied to larger eyetracking datasets, such as the english dataset by he et al.(2019).",2,2020
2020.emnlp-main.377.txt,"since different eye-tracking datasets tend to make use of different gaze encodings and formats, the amount of pre-processing and analysis steps required to apply our method to other resources was beyond the scope of this paper.",2,2020
2020.emnlp-main.377.txt,we leave testing whether the reported pattern of results holds across different languages to future work.,3,2020
2020.emnlp-main.378.txt,one future direction is to consider more sophisticated mechanisms to gain stronger controlability over longer sentences while maintaining the compactness of latent representations.,1,2020
2020.emnlp-main.378.txt,we hope that this paper will help renew interest in dgms for this purpose.,6,2020
2020.emnlp-main.38.txt,"this opens up the possibility of exploring large-scale meta-learning in nlp for various meta problems, including neural architecture search, continual learning, hyperparameter learning, and more.",1,2020
2020.emnlp-main.380.txt,"in future work, we plan to further investigate how different techniques apply to the problem of text segmentation, including data augmentation (wei and zou, 2019; lukasik et al., 2020b) and methods for regularization and mitigating labeling noise (jiang et al., 2020; lukasik et al., 2020a).",1,2020
2020.emnlp-main.383.txt,we plan to perform large-scale pre-training and evaluation on glue datasets for the comprehensive analysis.,3,2020
2020.emnlp-main.383.txt,we will continue to explore this line in the future.,6,2020
2020.emnlp-main.384.txt,"in our future work, we would like to extend this idea to unseen numbers in vocabulary as a function of seen ones.",2,2020
2020.emnlp-main.385.txt,future work might explore combining more expressive flows with discrete latent variables.,1,2020
2020.emnlp-main.386.txt,"finally, the models to generate cross-lingual annotations should be thoroughly evaluated in downstream music retrieval and recommendation tasks.",3,2020
2020.emnlp-main.386.txt,"hence, the effectiveness of language-specific concept representations to model the culturally diverse perception could be further probed.",1,2020
2020.emnlp-main.386.txt,our work provides a methodological framework to study the annotation behavior across languagebound cultures in other domains too.,2,2020
2020.emnlp-main.387.txt,"the next step towards this goal would be to recognize when characters refer to one another, and how this contributes to the movie-level risk behavior rating.",1,2020
2020.emnlp-main.389.txt,"furthermore, we used the interpretability of constituency tests to highlight and explain the parser’s strengths and shortcomings, like the “[ subject verb ]” and “adverb [ adjective noun ]” misbracketings, revealing potential next steps for improvement.",1,2020
2020.emnlp-main.391.txt,"in the future, we plan to enhance the system for handling morphologically complex languages trough unsupervised morphological segmentation.",1,2020
2020.emnlp-main.394.txt,"in the future work wish to explore more data independent methods such as lrc, for both speed and lack of data dependency, as well as manipulation of the decay w.r.t.what we have discovered from our layer-wise analysis.",1,2020
2020.emnlp-main.396.txt,"first, the approach could apply the same meta-learning approach to other classes of tasks beyond span id.",4,2020
2020.emnlp-main.396.txt,our current study could be extended in various directions.,6,2020
2020.emnlp-main.397.txt,"future work can apply these tests to a broader range of models, and continue to develop controlled tests that target encoding of complex compositional meanings, both for two-word phrases and for larger meaning units.",3,2020
2020.emnlp-main.4.txt,"besides the future extensions of this approach that we mentioned in our results discussion and error analysis, this work opens several interesting research paths.",6,2020
2020.emnlp-main.4.txt,"therefore, we plan to complement this work with approaches for other frequently applied schemes such as arguments by expert opinion and arguments by example.",1,2020
2020.emnlp-main.400.txt,"furthermore, integrating information from knowledge-bases can further improve the quality of entity representation.",1,2020
2020.emnlp-main.400.txt,"future work can explore representations for rare or unseen entities, as well as developing less memory-intensive ways to learn and integrate entity representations.",1,2020
2020.emnlp-main.402.txt,future work may investigate whether these results translate to other language models besides roberta as well as other training datasets besides winogrande.,2,2020
2020.emnlp-main.403.txt,we hope the insights provided here will help guide the development of better language models in the future.,1,2020
2020.emnlp-main.404.txt,"in future work, we plan to explore topic-level bias prediction as well as going beyond left-centerright bias.",5,2020
2020.emnlp-main.404.txt,"last but not least, we plan to experiment with other languages, and to explore to what extent a model for one language is transferable to another one given that the left-center-right division is not universal and does not align perfectly across countries and cultures, even when staying within the western political world.",2,2020
2020.emnlp-main.404.txt,"we further want to develop models that would be able to detect specific fragments in an article where the bias occurs, thus enabling explainability.",1,2020
2020.emnlp-main.405.txt,"in future work, we plan to apply our semantic label smoothing technique to various sequence to sequence problems, including text summarization (zhang et al., 2019) and text segmentation (lukasik et al., 2020b).",4,2020
2020.emnlp-main.405.txt,we also plan to study the relation between pretraining and data augmentation techniques.,1,2020
2020.emnlp-main.409.txt,"we investigate representations from pre-trained language models for task-oriented dialogue tasks, including domain identification, intent detection, slot tagging, and dialogue act prediction.",1,2020
2020.emnlp-main.41.txt,"future works include using other multilingual pretraining models such as xlm-roberta (conneau et al., 2019) for a more accurate model and distilmbert (sanh et al., 2019) for a more compact model.",1,2020
2020.emnlp-main.41.txt,"we have already applied it to bilingual sentence alignment (chousa et al., 2020) and we plan to extend it to other related problems.",4,2020
2020.emnlp-main.410.txt,we release our multiatis++ corpus to facilitate future research on cross-lingual nlu to bridge the gap between cross-lingual transfer and supervised methods.,2,2020
2020.emnlp-main.411.txt,future work includes its cross-lingual transfer and cross-dataset (or cross-task) generalization.,2,2020
2020.emnlp-main.412.txt,"in the future, we plan to explore maskaugment for other tasks in nlp domain.",4,2020
2020.emnlp-main.413.txt,"last but not least, we collect the topv2 dataset, a large-scale multi-domain task-oriented semantic parsing dataset with 8 domains and more than 180k annotated samples to evaluate our models, which we release to the research community.",2,2020
2020.emnlp-main.415.txt,"in the future, we plan to explore this approach with other language pairs and other generation tasks.",4,2020
2020.emnlp-main.416.txt,"furthermore, to move towards more practical applications, we would also need to conduct communication-based evaluation (newman et al., 2020) in addition to annotating individual utterances.",3,2020
2020.emnlp-main.416.txt,future work can aim to propose more general formulations that encapsulate more properties of the circumstance.forms of assistance.,1,2020
2020.emnlp-main.416.txt,"future work can consider adapting experiment designs from prior work (gao et al., 2015; hohenstein and jung, 2018) to establish the impact of offering such intention-preserving paraphrases in real conversations, potentially by considering downstream outcomes.",3,2020
2020.emnlp-main.416.txt,"future work may consider more complex stylistic aspects and strategies that are more tied to the content, such as switching from active to passive voice.",1,2020
2020.emnlp-main.416.txt,"future work may consider more comprehensive modeling of how people form politeness perceptions or obtain more reliable causal estimates for strategy strength (wang and culotta, 2019).task formulation.",1,2020
2020.emnlp-main.416.txt,"hence, while we work towards providing fully automated suggestions, we might also want to utilize the language ability humans possess and consider assistance approaches in the form of interpretable (partial) suggestions.evaluation.",1,2020
2020.emnlp-main.416.txt,the results and limitations of our method open up several natural directions for future work.modeling politeness perceptions.,1,2020
2020.emnlp-main.417.txt,"as for future directions, one natural extension is how we can automatically identify those attributes.",1,2020
2020.emnlp-main.418.txt,"alternatively, it may be useful to explore generation in a non left-to-right order to improve the efficiency of inference.",1,2020
2020.emnlp-main.418.txt,"another line of future work is to extend our model to sequence rewriting tasks, such as machine translation post-editing, that do not have existing error-tag dictionaries.",4,2020
2020.emnlp-main.418.txt,"even though our approach is open-vocabulary, future work will explore task specific restrictions.",1,2020
2020.emnlp-main.42.txt,we leave it for future work to extend our study to more downstream tasks and systems.,4,2020
2020.emnlp-main.420.txt,"blm has plenty of future applications, including template filling, information fusion, assisting human writing, etc.",4,2020
2020.emnlp-main.420.txt,"such models can be used in machine translation to support editing and refining translation, as well as in dialogue systems to compose a complete sentence with given elements.",4,2020
2020.emnlp-main.420.txt,"while we proposed blm for language generation, it would also be interesting to compare the representations learned by blm with those produced by other pre-training methods.",3,2020
2020.emnlp-main.421.txt,"cod3s leads to more diverse outputs in a multi-target generation task in a controllable and interpretable manner, suggesting the potential of semantically guided diverse decoding for a variety of text generation tasks in the future.",1,2020
2020.emnlp-main.422.txt,"we also plan to expand our methodology for extracting grammar rules from raw text to other aspects of morphosyntax, such as argument structure and word order phenomena.",4,2020
2020.emnlp-main.422.txt,we leave a more expressive model and evaluation on more languages as future work.,3,2020
2020.emnlp-main.424.txt,another approach would be to compare improvements between manual-only cleaning and cleaning done by a linguist working with someone who can write scripts to automatically correct repeated patterns of noise.,3,2020
2020.emnlp-main.424.txt,it could be integrated into linguists’ workflow in order to improve the study of inflection and increase igt data.,2,2020
2020.emnlp-main.424.txt,it might also be integrated into linguistic software such as flex.,1,2020
2020.emnlp-main.424.txt,there is room for future improvement.,6,2020
2020.emnlp-main.427.txt,future work needs to look into how question and reply context can improve automatic identification of advice.,5,2020
2020.emnlp-main.43.txt,our future work involves converting the monolingual data to parallel and collecting more data from the news domain.,2,2020
2020.emnlp-main.43.txt,we hope these diverse baselines will serve as useful strong starting points for future work by the community.,6,2020
2020.emnlp-main.430.txt,"in the future, we would like to explore uses of the subevent knowledge base for other eventoriented applications such as event tracking.",4,2020
2020.emnlp-main.431.txt,beesl is broadly applicable to event extraction and other tasks that can be recast as sequence labeling.,4,2020
2020.emnlp-main.431.txt,"we release the code freely, to foster research on using beesl for other nlp tasks as well, e.g., enhanced dependency parsing, fine-grained named entity recognition, and semantic parsing.",4,2020
2020.emnlp-main.433.txt,"in the future, we plan to extend our annotation to include event arguments and other properties of events.",2,2020
2020.emnlp-main.435.txt,"in the future, we plan to apply the proposed model for the related tasks and other settings of ed, including new type extension (nguyen et al., 2016b; lai and nguyen, 2019), and few-shot learning (lai et al., 2020a,b).",4,2020
2020.emnlp-main.436.txt,"for the future, instead of using the roberta baseline model for the self-training experiments, we could run several iterations by retraining on the data produced by our best self-trained model(s); this could be a good avenue for further improvements.",1,2020
2020.emnlp-main.436.txt,"in addition we plan to extend our work by moving to other languages beyond english (we currently have not tried this due to lack of data) using cross-lingual models, (subburathinam et al., 2019), applying other architectures like cnns (nguyen and grishman, 2015), incorporating tree structure in our models (miwa and bansal, 2016) and/or by handling jointly performing event recognition and temporal ordering (li and ji, 2014; katiyar and cardie, 2017).",2,2020
2020.emnlp-main.437.txt,"this model size can be prohibitively expensive in resource-constrained settings, prompting future work on more efficient language models.",1,2020
2020.emnlp-main.437.txt,"we are therefore interested in measuring performance on question answering tasks that require reasoning capabilities such as drop (dua et al., 2019).",3,2020
2020.emnlp-main.438.txt,"in future work, we plan to extend the dataset with more questions, more subjects, and more languages.",2,2020
2020.emnlp-main.438.txt,we further plan to develop new models to address the specific challenges we identified.,1,2020
2020.emnlp-main.439.txt,improving lm-score-based filtering is a future direction of our work.,1,2020
2020.emnlp-main.439.txt,it would be interesting to explore how one can adapt the generative models to the type of target domain questions.,1,2020
2020.emnlp-main.44.txt,there are additional avenues for future work beyond our proposed framework.,6,2020
2020.emnlp-main.440.txt,"in future work, we aim to extend our approach to more domains and explore more generalizable approaches for unsupervised domain adaptation.",4,2020
2020.emnlp-main.446.txt,addressing potential ethical concerns the goal of our work is to help to make nlp models more robust.,1,2020
2020.emnlp-main.446.txt,"moving forward, we hope to improve and deploy defenses against adversarial attacks in nlp, and more broadly, we hope to make security and privacy a more prominent focus of nlp research.",1,2020
2020.emnlp-main.450.txt,"in future work, we are interested in exploring nat using distilled ensembles with truncated distributions, and assessing how improved calibration impacts non-sequential decoding performance.",3,2020
2020.emnlp-main.450.txt,"non-autoregressive translation (nat) is an active area of research for nmt (gu et al., 2017; stern et al., 2019; ghazvininejad et al., 2019).",1,2020
2020.emnlp-main.455.txt,"hence, we guide the data-driven tokenizer by incorporating linguistic information to learn a more efficient vocabulary and generate symbol sequences that increase the network’s robustness to inflectional variation.",1,2020
2020.emnlp-main.456.txt,"we note that we could further extend our measures to fuzzy partitions, which remain less explored in community detection, but are a promising avenue for future work.",1,2020
2020.emnlp-main.458.txt,"in the presence of one-to-many mappings between pinyin and characters, the mapping accuracy is severely downgraded, leaving open an opportunity to design more robust unsupervised vector mapping systems.",1,2020
2020.emnlp-main.459.txt,"in future work, we plan to improve the quality of the additional actions.",6,2020
2020.emnlp-main.46.txt,"possible extensions of this work could investigate enriching trump vectors by incorporating other sources of text, such as white house press releases and speeches.",2,2020
2020.emnlp-main.460.txt,"another meaningful direction is to use hyperka to infer the associations between snapshots in temporally dynamic kgs (xu et al., 2020).",1,2020
2020.emnlp-main.460.txt,"for future work, we plan to incorporate hyperbolic rnns (ganea et al., 2018) to encode auxiliary information for zero-shot entity and concept representations.",1,2020
2020.emnlp-main.460.txt,"we also seek to investigate the use of hyperka for cross-domain representations of biological and medical knowledge (hao et al., 2020).",1,2020
2020.emnlp-main.461.txt,"we plan to apply the proposed framework on various event reasoning tasks and construct novel distributional constraints that could leverage domain knowledge beyond corpus statistics, such as the larger unlabeled data and rich information contained in knowledge bases.",4,2020
2020.emnlp-main.462.txt,future work involves exploring the generalization of temp to continuous tkgc and better imputation techniques to induce representations for infrequent and inactive entities.,1,2020
2020.emnlp-main.463.txt,"it leads to many interesting future works, including generalizing theorem 2 to other models, designing new algorithms to automatically adapt deep networks to different training configurations, upgrading the transformer architecture, and applying our proposed admin to conduct training in a larger scale.",1,2020
2020.emnlp-main.464.txt,"this opens a wide range of possibilities for generation tasks where monotonic orderings are not the most natural choice, and we would be excited to explore some of these areas in future work.",5,2020
2020.emnlp-main.465.txt,for future work we would like to explore if their success transfers to other generation tasks with mlms where inference efficiency is a concern.,4,2020
2020.emnlp-main.466.txt,"furthermore, future work may build on the ambigqa task with more open-ended approaches such as (1) applying the approach to qa over structured data (such as ambiguous questions that require returning tables), (2) handling questions with no answer or ill-formed questions that require inferring and satisfying more complex ambiguous information needs, and (3) more carefully evaluating usefulness to end users.",1,2020
2020.emnlp-main.466.txt,"future research developing on ambigqa models may include explicitly modeling ambiguity over events and entities or in the retrieval step, as well as improving performance on the difficult problems of answer recall and question disambiguation.",1,2020
2020.emnlp-main.468.txt,extension of this work to unanswerable and boolean questions is also a future work direction.,6,2020
2020.emnlp-main.468.txt,"more generally application of this work to multi dataset question generation with datasets such as multiqa (talmor and berant, 2019) is a promising avenue for future work.",4,2020
2020.emnlp-main.468.txt,of particular interest for future work is handling low-resource question answering domains.,5,2020
2020.emnlp-main.468.txt,we build upon existing work in large scale language modeling and question generation to push the quality of synthetic question generation.,1,2020
2020.emnlp-main.468.txt,we hope that better synthetic questions will enable new breakthroughs in question answering systems and related natural language tasks.,5,2020
2020.emnlp-main.469.txt,"in the future, we plan to improve mrl-cqa by designing a retriever that could be optimized jointly with the programmer under the meta-learning paradigm, instead of manually pre-defining a static relevance function.",1,2020
2020.emnlp-main.469.txt,"other potential directions of research could be toward learning to cluster questions into fine-grained groups and assign each group a set of specific initial parameters, making the model finetune the parameters more precisely.",1,2020
2020.emnlp-main.47.txt,"in developing this pipeline to examine how authors depict the transmission of information within narrative texts, we hope to drive a variety of future research in this space, including not only such narratological questions as how “gossip impels plots” (spacks, 1985), but also questions pertaining to issues of bias in representation, the flow of information, and factuality.",1,2020
2020.emnlp-main.470.txt,"finally, we would also like to apply our models to languages with even less resources available to help coping with the problem of offensive language in social media.",2,2020
2020.emnlp-main.470.txt,"in future work, we would like to further evaluate our models using solid, a novel large english dataset with over 9 million tweets (rosenthal et al., 2020), along with datasets in four other languages (arabic, danish, greek, and turkish) that were made available for the second edition of offenseval (zampieri et al., 2020).",3,2020
2020.emnlp-main.470.txt,"this opens exciting new avenues for future research considering the multitude of phenomena (e.g.hate speech, aggression, cyberbulling), annotation schemes and guidelines used in offensive language datasets.",5,2020
2020.emnlp-main.472.txt,"ultimately, we hope our work can open up new horizons for studying mds in various languages.",2,2020
2020.emnlp-main.473.txt,we hope our findings can pave the way for further inclusion of diverse language in future nlg models.,1,2020
2020.emnlp-main.474.txt,"in the future, we would like to extend our method to enhance the back-translation method in multidomain settings.",1,2020
2020.emnlp-main.477.txt,"it is an interesting question for future work whether strong alignment always comes at a cost, or if better training techniques will lead to models that can improve on all these measures simultaneously.",5,2020
2020.emnlp-main.478.txt,"additionally, it would be valuable to examine whether our method can improve the ocr on highresource languages, which typically have much better recognition rates in the first pass transcription than the endangered languages in our dataset.",3,2020
2020.emnlp-main.478.txt,"as future work, we plan to investigate the effect of using other available data for the three languages (for example, word lists collected by documentary linguists or the additional griko folk tales collected by anastasopoulos et al.(2018)).",2,2020
2020.emnlp-main.478.txt,"future work will focus on large-scale digitization of scanned documents, aiming to expand our ocr benchmark on as many endangered languages as possible, in the hope of both easing linguistic documentation and preservation efforts and collecting enough data for nlp system development in under-represented languages.",2,2020
2020.emnlp-main.479.txt,future directions include other pre-training or fine-tuning methods to improve retrieval performance and methods that encourage the lm to predict entities of the right types.,1,2020
2020.emnlp-main.48.txt,comprehensive modeling of social norms presents a promising challenge for nlp work in the future.,5,2020
2020.emnlp-main.480.txt,additional work could also leverage aligned documents as supervision to learn better cross-lingual document representations.,2,2020
2020.emnlp-main.480.txt,one natural followup to this work is to develop techniques to better mine parallel sentences from these aligned documents – especially for low-resource language pairs.,1,2020
2020.emnlp-main.482.txt,another line of future work is to investigate alternative user interfaces.,1,2020
2020.emnlp-main.482.txt,"in the future, we plan to train a policy that dynamically combines the two interactions with reinforcement learning (fang et al., 2017).",1,2020
2020.emnlp-main.482.txt,"the keyword ranking and the embedding refinement modules build upon existing methods for interpreting neural networks (li et al., 2016) and fine-tuning word embeddings (mrkšic´ et al., 2017).",1,2020
2020.emnlp-main.482.txt,"therefore, future advances in these areas may also improve clime.",6,2020
2020.emnlp-main.482.txt,we also explore a simple combination of active learning and clime.,1,2020
2020.emnlp-main.486.txt,"for future work, we will apply hit to other languages, and further explore potential cases of overlapping entities in nested ner task.",2,2020
2020.emnlp-main.489.txt,"this is just a first step towards the goal of fully-automated interpretable evaluation, and applications to new attributes and tasks beyond ner are promising future directions.",3,2020
2020.emnlp-main.49.txt,"for future work, it would be interesting to try incorporating broader context (e.g., paragraph/document-level context (ji and grishman, 2008; huang and riloff, 2011; du and cardie, 2020) in our methods to improve the accuracy of the predictions.",2,2020
2020.emnlp-main.490.txt,we hope to provide new guidance for the future slot filling work.,6,2020
2020.emnlp-main.491.txt,"since our framework can be used with any pretrained autoencoder, it will benefit from large-scale pretraining in future research.",4,2020
2020.emnlp-main.494.txt,"one branch of ideas involves incorporating more advanced il algorithms beyond dagger, such as lols (chang et al., 2015), to further improve the distillation process.",1,2020
2020.emnlp-main.494.txt,we are excited about several possible avenues for future work.,6,2020
2020.emnlp-main.499.txt,"in the future, we would like to explore data-free distillation on more complex tasks.",2,2020
2020.emnlp-main.499.txt,"to dynamically adjust synthetic samples according to students’ situations, we involve an adversarial self-supervised module to quantify students’ abilities.",1,2020
2020.emnlp-main.5.txt,"future work, can investigate whether finer ratings could correct the bias in favor of lower effort ratings, and how this may interact with document-level evaluation.",3,2020
2020.emnlp-main.50.txt,"in the future, we aim to extend graph schemas to encode hierarchical and temporal relations, as well as rich ontologies in open domain.",1,2020
2020.emnlp-main.50.txt,"we will also assemble our graph schemas to represent more complex scenarios involving multiple events, so they can be applied to more downstream applications including event graph completion and event prediction.",4,2020
2020.emnlp-main.500.txt,"thus, enhancing language models to generate more semantically related perturbations can be one possible solution to perfect bert-attack in the future.",1,2020
2020.emnlp-main.502.txt,"in the future, we will extend the similar approach to multilingual (yu et al., 2020a) or crosslingual (upadhyay et al., 2018) lexical entailment tasks.",2,2020
2020.emnlp-main.502.txt,"moreover, one interesting direction is to use hyperbolic embeddings (le et al., 2019; balazevic et al., 2019) for pattern-based models due to their inherent modeling ability of hierarchies.",1,2020
2020.emnlp-main.504.txt,"for future work, we intend to scale srefkb to a multilingual version and explore the possibilities of using the multilingual wordnet so that abundant knowledge regarding english can be transferred to other languages.",2,2020
2020.emnlp-main.504.txt,it is also worth investigating regarding how to better incorporate sense embedding into other downstream tasks.,1,2020
2020.emnlp-main.508.txt,we believe this approach can power future analyses of pre-trained text generation systems.,1,2020
2020.emnlp-main.509.txt,"the method can be extended to other text genres such as public policies to aid reader comprehension, which will be our future work to explore.",4,2020
2020.emnlp-main.51.txt,"for future work, we plan to extend the framework towards an end-to-end system with event extraction.",1,2020
2020.emnlp-main.51.txt,we also seek to extend the conjunctive constraints along with event argument relations.,1,2020
2020.emnlp-main.510.txt,"the promising empirical results motivate us to explore further the integration of more external knowledge and other rich forms of supervisions (e.g., constraints, interactions, auxiliary models, adversaries) (hu and xing, 2020; ziegler et al., 2019) in learning.",1,2020
2020.emnlp-main.510.txt,"we are also interested in extending the aspect-based summarization in more application scenarios (e.g., summarizing a document corpus).",4,2020
2020.emnlp-main.512.txt,"from the decoding side, a promising direction would be to make global inference in a more efficient way.",1,2020
2020.emnlp-main.512.txt,there are some possible future directions from our work.,6,2020
2020.emnlp-main.513.txt,for future work we are interested in exploring how definition modeling could be adapted to a multilingual or cross-lingual setting.,4,2020
2020.emnlp-main.515.txt,"in the future, we are interested in replacing bert with knowledge enhanced and number sensitive text representations models (cao et al., 2017; geva et al., 2020).",1,2020
2020.emnlp-main.516.txt,"in the future, we want to extend our system to other few-shot sequence tagging problems such as part-of-speech tagging and slot filling.",4,2020
2020.emnlp-main.517.txt,one immediate future work is to generate explanations for model predictions using structured vector.,1,2020
2020.emnlp-main.519.txt,future work includes: • enriching entity representations by adding entity type and entity graph information; • modeling coherence by jointly resolving mentions in a document; • extending our work to other languages and other domains; • joint models for mention detection and entity linking.,4,2020
2020.emnlp-main.520.txt,an exciting direction is to leverage visuals of each step to deal with unmentioned entities and indirect effects.,1,2020
2020.emnlp-main.520.txt,"as future work, we will explore more sophisticated models that can address the highlighted shortcomings of the current model.",1,2020
2020.emnlp-main.521.txt,"given that our experiments show a 25% increase in the candidate generation, one future research direction is to improve candidate ranking in lrl by incorporating coherence statistics and entity types.",1,2020
2020.emnlp-main.521.txt,"moreover, given the effectiveness of query logs, we believe it can be applied to other cross-lingual tasks like relation extraction and knowledge base completion.",4,2020
2020.emnlp-main.523.txt,"future work involves applying luke to domain-specific tasks, such as those in biomedical and legal domains.",4,2020
2020.emnlp-main.524.txt,"future directions include exploration of other knowledge bases to help the inference process and applying our simile generation approach to different creative nlg tasks such as pun (he et al., 2019), sarcasm (chakrabarty et al., 2020), and hyperbole (troiano et al., 2018).",4,2020
2020.emnlp-main.527.txt,"another interesting line of future work is to investigate the use of t2g2 for generating user utterances, which could be useful for dialogue data augmentation and user simulation.",1,2020
2020.emnlp-main.527.txt,we also hope to apply t2g2 to languages other than english.,2,2020
2020.emnlp-main.527.txt,"while in this paper we use standard pre-trained models, designing pre-training tasks tailored to sentence fusion is an interesting line of future work.",1,2020
2020.emnlp-main.528.txt,future work involves collecting data to addresses weaknesses of lerc.,2,2020
2020.emnlp-main.529.txt,"our results suggest several promising directions: although our ablation tests show the effect of each self-supervision module, types of plan keywords, and the amount of keywords with respect to generation quality, there are more spaces to explore in self-supervised text planning.",3,2020
2020.emnlp-main.529.txt,"our selfsupervised planning, in addition to other types of planning (e.g., discourse, goals, coreference, tenses) can be an important step toward modeling a long-term coherence in text generation.",1,2020
2020.emnlp-main.529.txt,predicting such structural plans from context and imposing them into the generator would be a potential direction for future work.,1,2020
2020.emnlp-main.529.txt,"second, we can extend the set of plan keywords to be more structured like a discourse tree.",1,2020
2020.emnlp-main.53.txt,"in the future, we will extend this approach to argument role induction to discover complete event schemas.",1,2020
2020.emnlp-main.532.txt,"in the future, we will explore fullfledged solutions to address the privacy concerns of both humans and dialogue systems.",1,2020
2020.emnlp-main.532.txt,we hope this work and the dataset will pave the way for the research on privacy leakage in conversations.,2,2020
2020.emnlp-main.534.txt,"for future work, we will incorporate pre-trained models into our framework (e.g., bert as a teacher and gpt as a student) to further unlock the performance improvement and explore how to balance diverse prior knowledge from multiple teachers.",1,2020
2020.emnlp-main.539.txt,we further test the proposed method on two downstream tasks.,3,2020
2020.emnlp-main.541.txt,"interesting future work includes developing a fast and efficient version of re-net, and modeling lasting events and performing inference on the long-lasting graph structures.",1,2020
2020.emnlp-main.543.txt,"we hope future research to explore scenarios where human intuition is not working as well as text classification, such as graph attention (velickovic et al., 2017).",5,2020
2020.emnlp-main.544.txt,"in future work, we plan to validate its effectiveness for aspect-level sentiment classification.",3,2020
2020.emnlp-main.545.txt,"in future work, we will try other types of single networks (e.g., (lai et al., 2015; yang et al., 2016; shimura et al., 2019)).",1,2020
2020.emnlp-main.546.txt,larger pre-training dataset with supervised labels or self-supervised learning strategies could be explored.,2,2020
2020.emnlp-main.546.txt,we plan to investigate these in future.,6,2020
2020.emnlp-main.548.txt,"meanwhile, extending these models to a larger scope of question types or more complex scenarios is still a challenge, and we will further investigate the trade-off between explainability and scalability.",1,2020
2020.emnlp-main.549.txt,"in the future, we plan to extend our model to learn the heterogeneous graph automatically, which assures more flexibility for numerical reasoning.",1,2020
2020.emnlp-main.55.txt,"possible future work includes (1) exploring other applications of diverse paraphrasing, such as data augmentation; (2) performing style transfer at a paragraph level; (3) performing style transfer for styles unseen during training, using few exemplars provided during inference.",4,2020
2020.emnlp-main.554.txt,"a first step in future work would be to test if the results of this paper hold on transformer architectures, or if instead transformers result in different patterns of structural encoding transfer.",3,2020
2020.emnlp-main.554.txt,"future work expanding on our results could focus on ablating specific structural features by creating hypothetical languages that differ in single grammatical features from the l2, in the style of galactic dependencies (wang and eisner, 2016), and testing the effect of structured data that’s completely unrelated to language, such as images.",1,2020
2020.emnlp-main.556.txt,our methodology relies on a number of limitations that should be considered in understanding the scope of our conclusions.,5,2020
2020.emnlp-main.556.txt,"we hope future work may better address this limitation, as in the work of cao and daume iii ´ (2019).",6,2020
2020.emnlp-main.557.txt,we hope our findings and probing dataset will provide a basis for improving pre-trained masked language models’ numerical and other concrete types of commonsense knowledge.,1,2020
2020.emnlp-main.558.txt,"in future work, we will consider how to interpret environment specifications to facilitate grounded adaptation in these other areas.",1,2020
2020.emnlp-main.559.txt,"in the future, we will look into more accurate confidence measure via neural network calibration (guo et al., 2017) or using machine learning components (e.g., answer triggering (zhao et al., 2017) or a reinforced active selector (fang et al., 2017)).",1,2020
2020.emnlp-main.559.txt,one important future work is thus to conduct large-scale user studies and train parsers from real user interaction.,2,2020
2020.emnlp-main.56.txt,"based on the ac-nlg method, in the future, we can explore the following directions: (1) improve the accuracy of judgment on a claim-level.(2) add external knowledge (e.g.a logic graph) to the predictor for the interpretability of the model.",1,2020
2020.emnlp-main.560.txt,"for future work, we will explore methods attempting to solve hard and extra hard questions.",1,2020
2020.emnlp-main.561.txt,"in the future, we are interested in distilling and reusing the common knowledge from users’ selections.",2,2020
2020.emnlp-main.562.txt,"for future work, we will continually improve the scale and quality of our dataset, to facilitate future research and to meet the need of database-oriented applications.",2,2020
2020.emnlp-main.563.txt,we also plan to extend our approach to cope with multitable text-to-sql task spider.,4,2020
2020.emnlp-main.564.txt,"our study sheds light on the characteristics of text-tosql parsing for future efforts including advanced modeling, problem identification, dataset construction and model evaluation.",1,2020
2020.emnlp-main.564.txt,we critically examine the role of schema linking for the text-to-sql task.,3,2020
2020.emnlp-main.565.txt,further research may be concerned with zero-shot learning on new categories.,1,2020
2020.emnlp-main.565.txt,"in this paper, in order to make multi-task learning feasible for incremental learning, we proposed cne-net with different attention mechanisms.",1,2020
2020.emnlp-main.566.txt,"besides, there are still two important directions for future work: (1) how to apply task-guided pre-training to general domain data when the indomain data is limited.(2) how to design more effective strategies to capture domain-specific and task-specific patterns for selective masking.",2,2020
2020.emnlp-main.568.txt,"another promising direction is to leverage taxonomy construction algorithms (huang et al., 2020) to capture more fine-grained aspects, such as “smell” and “taste” for “food”.",1,2020
2020.emnlp-main.568.txt,"in the future, we plan to adapt our methods to more general applications that are not restricted to the field of sentiment analysis, such as doing multiple-dimension classification (e.g., topic, location) on general text corpus.",4,2020
2020.emnlp-main.569.txt,"in the future, we will explore the latent information between peer reviews and author responses to improve argument pair extraction.",1,2020
2020.emnlp-main.570.txt,"in the future, we plan to further improve d-miln with aspect-level annotations and find appropriate way to combine d-miln with pre-training methods (tian et al., 2020).",1,2020
2020.emnlp-main.571.txt,"to stimulate research on this topic, we make hypo-cn publicly available.8 in future work, we plan to use hypo and hypo-cn to conduct a cross-lingual study on whether there are differences in the way exaggeration is expressed in english and chinese.",2,2020
2020.emnlp-main.574.txt,"furthermore, we expect to extend the scope of analysis from the attention to an entire transformer architecture to better understand the inner workings and linguistic capabilities of the current powerful systems in nlp.",1,2020
2020.emnlp-main.574.txt,"in future work, we plan to apply our norm-based analysis to attention in other models, such as finetuned bert, roberta (liu et al., 2019), and albert (lan et al., 2020).",4,2020
2020.emnlp-main.574.txt,we hope that this paper will inspire researchers to have a broader view of the possible methodological choices for analyzing the behavior of transformer-based models.,1,2020
2020.emnlp-main.576.txt,"although our results have some implications on them, we leave a detailed study on context-free languages for future work.",2,2020
2020.emnlp-main.576.txt,regular and counter languages model some aspects of natural language while contextfree languages model other aspects such as hierarchical dependencies.,1,2020
2020.emnlp-main.58.txt,"we presented delorean, an unsupervised lmbased approach to generate text conditioned on past context as well as future constraints, through forward and backward passes considering each condition.",1,2020
2020.emnlp-main.581.txt,"it is inspiring and promising to be generalized to more rewriting tasks, which will be studied as our future work.",4,2020
2020.emnlp-main.582.txt,"in the future, there are several prospective research directions: (1) we introduce a distant supervision (ds) assumption in our mrp training task.",1,2020
2020.emnlp-main.583.txt,our results suggest that future works introducing graph structure into nlp tasks should explain their necessity and superiority.,1,2020
2020.emnlp-main.583.txt,this study set out to investigate whether graph structure is necessary for multi-hop qa and what role it plays.,5,2020
2020.emnlp-main.584.txt,"as for future work, we plan to investigate using languages other than english for training (e.g., our larger french and german training sets) in our cross-lingual transfer experiments, since english may not always be the optimal source language (anastasopoulos and neubig, 2020).",2,2020
2020.emnlp-main.586.txt,"given the current large gaps between monolingual and multilingual lms, we will also focus on lightweight methods to enrich lexical content in multilingual lms (wang et al., 2020; pfeiffer et al., 2020).",1,2020
2020.emnlp-main.586.txt,"in future work, we plan to investigate how domains of external corpora affect aoc configurations, and how to sample representative contexts from the corpora.",2,2020
2020.emnlp-main.586.txt,"in-depth analyses of these factors are out of the scope of this work, but they warrant further investigations.opening future research avenues.",6,2020
2020.emnlp-main.586.txt,"we will also extend the study to more languages, more lexical semantic probes, and other larger underlying lms.",2,2020
2020.emnlp-main.588.txt,"in future work, we hope that slurp will be a valuable resource for developing e2e-slu systems, as well as more traditional pipeline approaches to slu.",2,2020
2020.emnlp-main.588.txt,"the next step is to extend slurp with spontaneous speech, which would again increase its complexity, but also move it one step closer to real-life applications.",4,2020
2020.emnlp-main.589.txt,for model creators: (1) model probing and experimenting with perturbed inputs can give deep insights about how a model is reasoning (2) experimenting with adversarial inputs early on in the design process can help build better models.,1,2020
2020.emnlp-main.594.txt,"also, given the recent success of models such as elmo and bert, it would be interesting to explore extensions of graphglove to the class of contextualized embeddings.",1,2020
2020.emnlp-main.594.txt,"possible directions for future work include using graphglove for unsupervised hypernymy detection, analyzing undesirable word associations, comparing learned graph topologies for different languages, and downstream applications such as sequence classification.",1,2020
2020.emnlp-main.596.txt,"in the future, we aim at applying stare for node and graph classification tasks as well as extend our approach to large-scale kgs.",4,2020
2020.emnlp-main.596.txt,"in the future, we plan to enrich wd50k entities with class labels and probe it against node classification tasks.",2,2020
2020.emnlp-main.597.txt,"in future studies, we plan to increase the number of dimensions of the relational position encodings, since a scalar value may not be able to express positional information adequately.",1,2020
2020.emnlp-main.598.txt,"in future work, we plan to extend our methodology to new languages, and experiment with multilingual and language specific bert models.",2,2020
2020.emnlp-main.60.txt,we plan to investigate other automatic tools in curating more accurate denotation graphs with a complex composition of fine-grained concepts for future directions.,1,2020
2020.emnlp-main.600.txt,"in future work, we plan to extend our approach to further improve the reward and policy functions, and to reduce the human-labeling factor.",1,2020
2020.emnlp-main.601.txt,there are exciting avenues for multilingual work to account for language and cultural differences.,2,2020
2020.emnlp-main.603.txt,"this work can be extended in several ways: (i) we plan to investigate into further functions for τ to enhance the exploration-exploitation tradeoff.(ii) additional strategies to assign nuclearity should be explored, considering the excessive n-nclassification shown in our evaluation.(iii) we plan to apply our approach to more sentiment datasets (e.g., diao et al.(2014)), creating even larger treebanks.(iv) our new and scalable solution can be extended to also predict discourse relations besides structure and nuclearity.(v) we also plan to use a neural discourse parser (e.g.",1,2020
2020.emnlp-main.603.txt,"yu et al.(2018)) in combination with our large-scale treebank to fully leverage the potential of data-driven discourse parsing approaches.(vi) taking advantage of the new mega-dt corpus, we want to revisit the potential of discourse-guided sentiment analysis, to enhance current systems, especially for long documents.(vii) finally, more long term, we intend to explore other auxiliary tasks for distant supervision of discourse, like summarization, question answering and machine translation, for which plenty of annotated data exists (e.g., nallapati et al.(2016); cohan et al.(2018); rajpurkar et al.(2016, 2018)).",1,2020
2020.emnlp-main.605.txt,"moreover, we intend to instantiate our proposed framework to other domains such as teacher/student conversations and other types of discourse such as social media narratives.",4,2020
2020.emnlp-main.605.txt,we also wish to expand the current face framework to a more comprehensive politeness framework that incorporates notions of power and social distance between the interlocutors.,1,2020
2020.emnlp-main.607.txt,"in future work, we would like to further investigate few and zero-shot learning in lmtc, especially in bert models that are currently unable to cope with zero-shot labels.",1,2020
2020.emnlp-main.608.txt,"we cover background on contextualized encoders, pretraining objectives, efficiency, data, approaches in model interpretability, and research in multilingual systems.",1,2020
2020.emnlp-main.609.txt,"this last challenge will be especially crucial for future work that seeks to verify scientific claims against sources other than the research literature – for instance, social media and the news.",5,2020
2020.emnlp-main.609.txt,"we hope that the resources presented in this paper encourage future research on these important challenges, and help facilitate progress toward the broader goal of scientific document understanding.",2,2020
2020.emnlp-main.61.txt,"our evaluation verifies the effectiveness of our method, while also indicating a scope for further study, enhancement, and extensions in the future.",3,2020
2020.emnlp-main.610.txt,"we invite future research into further integration of syntactic methods into shallow semantic analysis in other languages and other formulations, such as frame-semantic parsing, and other semantically oriented tasks.",1,2020
2020.emnlp-main.611.txt,"in addition, since parade provides entities like “machine code” for definitions, this new dataset could also be useful for other tasks like entity linking (shen et al., 2014), entity retrieval (petkova and croft, 2007) and entity or word sense disambiguation (navigli, 2009).",4,2020
2020.emnlp-main.611.txt,"in the future, we will continue to investigate effective ways to obtain domain knowledge and incorporate it into enhanced models for paraphrase identification.",1,2020
2020.emnlp-main.612.txt,"a causal definition is in no way limited to this pairwise case, and future work may generalize it to the sequential case or to event representations that are compositional.",1,2020
2020.emnlp-main.612.txt,"having a causal model shines a light on the assumptions made here, and indeed, future work may further refine or overhaul them, a process which may further shine a light on the nature of the knowledge we are after.",1,2020
2020.emnlp-main.613.txt,"based on our analysis, future work in the direction of automatic bias mitigation may include identifying potentially biased examples in an online fashion and discouraging models from exploiting them throughout the training.",1,2020
2020.emnlp-main.615.txt,"also, we would like to explore how to overcome the obstacles that prevent us from fully exploiting large pretrained lms (e.g., gpt-2) in low-resource settings.",1,2020
2020.emnlp-main.615.txt,"in future work, we intend to experiment with the lm-prior under more challenging conditions, such as when there is domain discrepancy between the parallel and monolingual data.",3,2020
2020.emnlp-main.616.txt,"as a continuation to this work, we intend to evaluate whether multilingual translation models are more resilient to lexical disambiguation biases and, as a consequence, are less susceptible to adversarial attacks that exploit source-side homography.",3,2020
2020.emnlp-main.616.txt,extending model-agnostic attack strategies to incorporate other types of dataset biases and to target natural language processing tasks other than machine translation is likewise a promising avenue for future research.,1,2020
2020.emnlp-main.617.txt,"in future work, we will apply mad-x to other pre-trained models, and employ adapters that are particularly suited for languages with certain properties (e.g.with different scripts).",4,2020
2020.emnlp-main.617.txt,"we will also evaluate on additional tasks, and investigate leveraging pre-trained language adapters from related languages for improved transfer to truly low-resource languages with limited monolingual data.",3,2020
2020.emnlp-main.618.txt,"so as to facilitate similar studies in the future, we release our nli dataset,13 which, unlike previous benchmarks, was annotated in a non-english language and human translated into english.",2,2020
2020.emnlp-main.619.txt,"additionally, in the future, we would also want to quantify the impact of varying degrees of granularity of learning emotional features from tweets on statenet’s performance.",3,2020
2020.emnlp-main.619.txt,"priority-based suicide risk assessment for ranking tweets for suicidal risk, rather than classifying them forms our future direction.",3,2020
2020.emnlp-main.619.txt,"through this work, we aim to form a component in a larger human-in-the-loop infrastructure for analyzing potentially concerning suicide-related social media posts.",2,2020
2020.emnlp-main.619.txt,we plan to explore the impact of varying amounts of historical context for a user in our future work.,2,2020
2020.emnlp-main.62.txt,future work would be well-suited to explore 1) methods for better understanding which datasets (and individual instances) can be rebalanced and which cannot; and 2) the non-trivial task of estimating additive human baselines to compare against.• hypothesis 2: modeling feature interactions can be data-hungry.,5,2020
2020.emnlp-main.62.txt,"so, we may need models with different inductive biases and/or much more training data.",1,2020
2020.emnlp-main.62.txt,"we postulate the following potential explanations, pointing towards future work: • hypothesis 1: these unbalanced tasks don’t require complex cross-modal reasoning.",1,2020
2020.emnlp-main.623.txt,"furthermore, we hope to explore the quality of fact-checking explanations with respect to properties other than coherence, e.g., actionability and impartiality.lastly, we plan to explore congruity between veracity prediction and explanation generation tasks, i.e., generating explanations which are compatible with the predicted label and vice versa.",3,2020
2020.emnlp-main.623.txt,"in order to do this, we hope to explore other subjects, in addition to public health, for which factchecking requires a level of expertise in the subject area.",5,2020
2020.emnlp-main.623.txt,we hope to explore the topics of explainable fact-checking and specialist fact-checking further.,5,2020
2020.emnlp-main.624.txt,"our formulation also bridges broader nlu/rc techniques to address other critical challenges in if games for future work, e.g., common-sense reasoning, noveltydriven exploration, and multi-hop inference.",1,2020
2020.emnlp-main.626.txt,we believe that the contributions made in this work would also generalize to other kinds of expert-lay dialogue like customer-service chats.,4,2020
2020.emnlp-main.628.txt,"in the future, we will investigate the properties of our proposed method on verifying statements with more complicated operations and explore the explainability of the model.",1,2020
2020.emnlp-main.630.txt,future work could investigate the use of non-expert human raters to improve the dataset quality further.,2,2020
2020.emnlp-main.630.txt,"in pursuit of improved entity representations, future work could explore the joint use of complementary multi-language descriptions per entity, methods to update representations in a light-weight fashion when descriptions change, and incorporate relational information stored in the kb.",1,2020
2020.emnlp-main.632.txt,"these preliminary results pave the way for further experiments with other language models, various architectures and new downstream tasks.",3,2020
2020.emnlp-main.633.txt,"although our model has achieved good performance compressing bert, it would be interesting to explore its possible applications in other neural models.",4,2020
2020.emnlp-main.633.txt,"for future work, we would like to explore the possibility of applying theseus compression on heterogeneous network modules.",4,2020
2020.emnlp-main.633.txt,"in addition, we would like to conduct theseus compression on more types of neural networks including convolutional neural networks and graph neural networks.",1,2020
2020.emnlp-main.633.txt,"therefore, it is potential to apply theseus compression to other large models (e.g., resnet (he et al., 2016) in computer vision).",4,2020
2020.emnlp-main.633.txt,"we will also investigate the combination of our compression-based approach with recently proposed dynamic acceleration method (zhou et al., 2020b) to further improve the efficiency of pretrained language models.",1,2020
2020.emnlp-main.635.txt,these task embeddings allow us to predict source tasks that will likely improve target task performance.,1,2020
2020.emnlp-main.636.txt,"as part of future work, we will explore cvt on other sequence-labeling tasks such as chunking, elementary discourse unit segmentation and argumentative discourse unit segmentation, thus moving beyond entity-level spans.",4,2020
2020.emnlp-main.636.txt,"furthermore, we intend to implement cvt as a training strategy over transformers (bert) and compare it with adaptivelypretrained bert.",1,2020
2020.emnlp-main.636.txt,"moreover, other supervised tasks such as classification could also be studied in this context.",4,2020
2020.emnlp-main.637.txt,future work may focus on finding representations that encode the most important information for al.,1,2020
2020.emnlp-main.638.txt,a natural future direction is to conduct a similar empirical investigation of al over bert in the context of multi-class classification and regression tasks.,3,2020
2020.emnlp-main.638.txt,"it would also be interesting to investigate the realm of larger annotation budgets, and more recent bert variants (liu et al., 2019; lan et al., 2019).",2,2020
2020.emnlp-main.638.txt,"the development of novel al methods, that are tailored for pre-trained models such as bert, seems like an important direction for future work.",1,2020
2020.emnlp-main.638.txt,"we hope that the experimental results and analyses reported here, as well as the release of the research framework we developed, would be instrumental for these and other future studies.",3,2020
2020.emnlp-main.639.txt,we hope that this work can help inform researchers of considerations to make when using lpx models in the presence of domain shift.,1,2020
2020.emnlp-main.64.txt,"in the future, we will investigate debiasing retrieval-based dialogue models and more complicated pipeline-based dialogue systems.",1,2020
2020.emnlp-main.640.txt,"distilling models to their vvma counterparts would be an interesting experiment, and potentially an orthogonal enhancement to pre-existing frameworks (sanh et al., 2019).",1,2020
2020.emnlp-main.640.txt,"in future work, we plan to optimize the lowlevel code and to develop new hardware to deploy vvmas in real-world applications.",1,2020
2020.emnlp-main.641.txt,"we plan to extend these results by studying the mixing of such textual filler-oriented representations with acoustic representations, and further investigate the representation of fillers learnt during pre-training.",1,2020
2020.emnlp-main.643.txt,"analyzing the demographic, cultural, and gender bias in research pertaining to financial disclosures, particularly earnings calls, forms a future direction of research.",5,2020
2020.emnlp-main.643.txt,"experimenting with other sets of commonly used acoustic features such as mfcc coefficients, opensmile features and audeep features for representing audio utterances also form a future direction for audio feature extraction.",1,2020
2020.emnlp-main.643.txt,"first, we want to improve upon the audio feature extraction.",1,2020
2020.emnlp-main.643.txt,there are several promising directions of future work that we wish to explore.,6,2020
2020.emnlp-main.643.txt,"we would also want to work on studying a wider set of earnings calls and companies spanning multiple languages, demographics, speakers and gender.",2,2020
2020.emnlp-main.645.txt,"future direction also include alternate architectures, reward schemes, and evaluation using human judges.",3,2020
2020.emnlp-main.645.txt,"recent works (lu et al., 2018; d’autume et al., 2019) have proposed some solutions to address these challenges and we plan to explore them.",1,2020
2020.emnlp-main.646.txt,another important direction is to investigate how to integrate the ability to aggregate entities derived from training on tesa into an abstractive summarizer.,1,2020
2020.emnlp-main.646.txt,"in future work, we would like to expand the domains covered by our dataset, which is biased towards topics found in the source corpus, such as politics.",2,2020
2020.emnlp-main.646.txt,this would require models to tackle another challenging issue which we have not addressed: which set of entities should a model aggregate in the first place?,5,2020
2020.emnlp-main.647.txt,"in future works we plan to add other languages including arabic and hindi, and to investigate the adaptation of neural metrics to multilingual summarization.",2,2020
2020.emnlp-main.649.txt,our findings suggest that more intentional and deliberate decisions should be made in selecting summarization datasets for downstream modelling research and that further scrutiny should be placed upon summarization datasets released in the future.,2,2020
2020.emnlp-main.65.txt,an important future direction will be generating the distractors and learning the rationality coefficients.,5,2020
2020.emnlp-main.65.txt,"our self-conscious agents improved the base agents on the dialogue nli (welleck et al., 2019) and personachat (zhang et al., 2018) dataset, without consistency labels and nli models.",1,2020
2020.emnlp-main.652.txt,we hope this work will inspire and assist both dialogue and document modeling for tackling more real-life dialogue tasks.,1,2020
2020.emnlp-main.654.txt,"finally, another interesting exploration is to extend the model with a jointly trainable movie recommendation and movie information modules.",1,2020
2020.emnlp-main.654.txt,"then, we plan to investigate the strategy patterns for people with different personalities and movie preferences to make dialog system more personalized.",1,2020
2020.emnlp-main.654.txt,"this work opens up several directions for future studies in building sociable and personalized recommendation dialog systems as follows: first, we will explore more ways of utilizing the strategies, including dynamic strategy selection after decoding.",1,2020
2020.emnlp-main.655.txt,we hope that our dataset will encourage further interest in curiosity-driven dialog.,2,2020
2020.emnlp-main.655.txt,we see two immediate directions for future work.,6,2020
2020.emnlp-main.656.txt,newly collected or constructed datasets should consider how to carefully craft the collection to mitigate bias issues from the very start.,2,2020
2020.emnlp-main.657.txt,"future work may explore generating of diverse sets of hypotheses for a given premise and label, with the goal of performing data augmentation.",2,2020
2020.emnlp-main.657.txt,other future work will be to measure the performance of gennli on adversarial and similarly challenging nli datasets.,3,2020
2020.emnlp-main.658.txt,bias mitigation in models and datasets remains a crucial direction for future work if systems based on datasets like the ones we study are to be widely deployed.,2,2020
2020.emnlp-main.658.txt,machine learning methods work on transfer learning could help to better understand and exploit the effects that drive the successes we have seen with nli data so far.,1,2020
2020.emnlp-main.659.txt,"finally, we give suggestions on future research directions and on better analysis variance reporting.",1,2020
2020.emnlp-main.659.txt,"however, large instability of current models on some of these analysis sets undermine such benefits and bring non-ignorable obstacles for future research.",1,2020
2020.emnlp-main.659.txt,we hope this paper will guide researchers on how to handle instability and inspire future work in this direction.,5,2020
2020.emnlp-main.66.txt,"tod-bert is easy-to-deploy and will be open-sourced, allowing the nlp research community to apply or fine-tune any task-oriented conversational problem.",4,2020
2020.emnlp-main.660.txt,our goal is to push forward the research and practical use of textual entailment in a broader vision of natural language processing.,4,2020
2020.emnlp-main.660.txt,the final entailment system ufo-entail generalizes well to open domain entailment benchmarks and downstream nlp tasks including question answering and coreference resolution.,4,2020
2020.emnlp-main.661.txt,"however, we also show limitations of our proposed methods, thereby encouraging future work on conjnli for better understanding of conjunctive semantics.",1,2020
2020.emnlp-main.662.txt,"first, we used nli-tr to analyze the effects of in-language pretraining.",3,2020
2020.emnlp-main.662.txt,we also used nli-tr to investigate central issues in turkish nli.,1,2020
2020.emnlp-main.663.txt,we enhance the target semantic model by incorporating syntax in a multitask learning framework.,1,2020
2020.emnlp-main.664.txt,this echoes the strong results seen with t5 and offers further motivation to explore these kinds of design decisions in other tasks.,4,2020
2020.emnlp-main.666.txt,"in the future, we plan to study how we can apply synsetexpan at the entity mention level for conducting contextualized synonym discovery and set expansion.",4,2020
2020.emnlp-main.670.txt,"in the future, we are interested in effectively integrating different forms of supervision including annotated documents.",2,2020
2020.emnlp-main.670.txt,this is another potential direction for the extension of our method.,1,2020
2020.emnlp-main.672.txt,"finally, it would be interesting to do a deeper dive into variations of author strategies in chapterization, focusing more intently on books with large numbers of short chapters as being more reflective of episode boundaries.",1,2020
2020.emnlp-main.672.txt,"our work opens up avenues for further research in text segmentation, with potential applications in summarization and discourse analysis.",4,2020
2020.emnlp-main.672.txt,potential future work includes combining the neural and cut-based approaches into a stronger method.,1,2020
2020.emnlp-main.674.txt,we believe our work opens up the necessity of further investigation pertaining to careful information fusion techniques for downstream tasks.,1,2020
2020.emnlp-main.675.txt,"as next steps, we plan to address further types of revisions and extend our experiments to document-level settings.",3,2020
2020.emnlp-main.676.txt,"another interesting direction of future research is to explore the cold start problem, where man-sf could be leveraged to predict stock movements for new stocks.",5,2020
2020.emnlp-main.676.txt,"lastly, we would also like to extend man-sf’s architecture to not be limited to model all stocks together (because of its gat component) to increase scalability to cross-market scenarios.",1,2020
2020.emnlp-main.676.txt,"we plan to further use news articles, earnings calls, and other data sources to capture market dynamics better.",2,2020
2020.emnlp-main.677.txt,"future work will investigate ways to improve performance (and especially precision scores) on our data, in particular on low-support labels.",1,2020
2020.emnlp-main.68.txt,we hope that this study will facilitate discussions in the dialogue response generation community regarding the issue of filtering noisy corpora.,5,2020
2020.emnlp-main.680.txt,we hope that the dataset shall broaden the target domain of gec beyond learner and/or exam writing and facilitate the development of robust gec models in the open-domain setting.,1,2020
2020.emnlp-main.682.txt,future work can use anomaly detection approaches operating on our model’s predicted word vectors to detect anomalies in a word’s representation across time.,1,2020
2020.emnlp-main.682.txt,"we also plan to investigate different architectures, such as variational autoencoders (kingma and welling, 2014), and incorporate contextual representations (devlin et al., 2019; hu et al., 2019) to detect new senses of words.",1,2020
2020.emnlp-main.684.txt,the extension of the scope will be the future work.,6,2020
2020.emnlp-main.685.txt,"in future work we plan to apply our model to longer, book length documents, and plan to add more structure to the memory.",4,2020
2020.emnlp-main.687.txt,we believe there is much potential for additional selfsupervision tasks and leave those for future work.,6,2020
2020.emnlp-main.688.txt,"in future work, we would like to study how to introduce acyclic rules to the walk-based systems.",1,2020
2020.emnlp-main.689.txt,"as discussed in section 5, we also plan to develop a more flexible scoring function which can handle equivalent trees.",1,2020
2020.emnlp-main.689.txt,"finally, we plan to evaluate bertft on other temporal relation datasets as part of a larger pipeline, which will include a mapping between tdts and other temporal relation annotation schemas such as the tempeval-3 dataset (uzzaman et al., 2013).",3,2020
2020.emnlp-main.689.txt,"for future research, we plan to explore other types of deep neural lms such as transformerxl (dai et al., 2019) and xlnet (yang et al., 2019).",1,2020
2020.emnlp-main.69.txt,"future directions include the incorporation of phonological and morphosyntactic features, application to other languages, and most importantly, a model extension to infer temporal ordering.",1,2020
2020.emnlp-main.691.txt,"for future research, it is interesting to enhance seqmix with language models during the mixup process, and harness external knowledge for further improving diversity and plausibility.",1,2020
2020.emnlp-main.692.txt,"future work may want to build on our approach for more comprehensive extraction tasks, focussing on more types of result, as well as other information contained in papers such as architectural details and hyperparameters.",1,2020
2020.emnlp-main.693.txt,our solution for attribute value extraction can be extended to other nlp tasks.,4,2020
2020.emnlp-main.693.txt,we leave this as our future work.,6,2020
2020.emnlp-main.694.txt,"future work can focus on expanding the capabilities to generating whole paragraphs of text from graphs in kb, as well as converting large parts of text into coherent graph structures.",1,2020
2020.emnlp-main.696.txt,our work also suggests that standard model components like embedding tying should be retested as we continue to explore the space of language modeling.,3,2020
2020.emnlp-main.698.txt,"in future work, we hope to leverage sentence structure, such as the use of constituency parsing, to further enhance the design of the progressive hierarchy.",1,2020
2020.emnlp-main.7.txt,"as paraphrasing continues to improve and cover more languages, we are optimistic smrt will provide larger improvements across the board—including for higher-resource mt and for additional target languages beyond english.",2,2020
2020.emnlp-main.70.txt,examples would include variants that employ longer and more realistic contexts.,2,2020
2020.emnlp-main.70.txt,future work will be required to assess the extent to which these effects do in fact reflect the acquisition of a latent form of discourse modeling ability.,3,2020
2020.emnlp-main.70.txt,"since an experiment that collects data on this scale would require a substantial annotation effort, a more careful comparison of this sort must be left for future work.",3,2020
2020.emnlp-main.70.txt,we hope that this short paper will inspire further research that takes next steps in this and a variety of other directions.,6,2020
2020.emnlp-main.700.txt,our future work will explore the potential of training palm for longer on much more unlabeled text data.,2,2020
2020.emnlp-main.702.txt,"overall, teaforn is a promising approach for improving quality and/or reducing inference costs in sequence generation models.",1,2020
2020.emnlp-main.704.txt,"in the future, we believe that strong linguistic priors will continue to be a key ingredient for building nextlevel learning agents in these games.",1,2020
2020.emnlp-main.705.txt,we hope this work will motivate the image captioning field to learn to anticipate and provide for the information needs of specific user communities.,5,2020
2020.emnlp-main.710.txt,future work includes investigating the interaction and joint training between hgn and paragraph retriever for performance improvement.,1,2020
2020.emnlp-main.712.txt,we leave further exploration to future work.,6,2020
2020.emnlp-main.713.txt,"overall, this work opens up exciting avenues for leveraging methods in unsupervised learning and natural language generation to improve the interpretability and generalization of machine learning systems.",1,2020
2020.emnlp-main.715.txt,"finally, we will carry out a thorough investigation into emotion-cause pairs (xia and ding, 2019).",3,2020
2020.emnlp-main.715.txt,"in the future, we plan to study how contextual information (i.e., different aspects of people’s interactions captured through contiguous posts in a discussion thread) affects the perceived emotions.",1,2020
2020.emnlp-main.715.txt,we also plan to perform a cross-corpus analysis to investigate if emotions are expressed differently in the health domain compared to other domains.,3,2020
2020.emnlp-main.717.txt,"in addition, we will study more explicitly how to decouple stance models from sentiment, and how to improve performance further on difficult phenomena.",1,2020
2020.emnlp-main.717.txt,"in future work we plan to investigate additional methods to represent and use generalized topic information, such as topic modeling.",1,2020
2020.emnlp-main.720.txt,an important avenue of future work will be to assess to what extent there may be cultural differences in these associations (see discussion in section 3.1).,3,2020
2020.emnlp-main.720.txt,"similarly, variation with respect to age and other variables merits further study as well.",3,2020
2020.emnlp-main.720.txt,"temporal aspects could be considered in diachronic studies, to account for the fact that emoji use has been evolving.",1,2020
2020.emnlp-main.722.txt,"in the future, instead of pre-training on sentences, we will leverage raw text at passage or document level to alleviate the performance degeneration brought by short context.",2,2020
2020.emnlp-main.722.txt,"in this work, we aim at equipping pre-trained lms with structured knowledge via self-supervised tasks.",2,2020
2020.emnlp-main.722.txt,it masks informative mentions and facilitates learning structured knowledge in free-form text.,1,2020
2020.emnlp-main.722.txt,"moreover, we will use a combination of commonsense and ontological kgs, and large-scale corpora (e.g., common crawl) to pre-train an mlm from scratch, which we expect to benefit a wide range of tasks.",2,2020
2020.emnlp-main.724.txt,we also point out several directions for future work by generalizing our methods to other tasks or combining with other techniques.,4,2020
2020.emnlp-main.725.txt,"in the future, we plan to extend our model to cope with external word or document semantics.",1,2020
2020.emnlp-main.725.txt,it would also be interesting to explore alternative architectures other than cyclegan under our formulation of topic modeling.,1,2020
2020.emnlp-main.726.txt,"in the future, we plan to implement a more sophisticated guidance for the augmentation by adding syntactic and position features to the reward function, to enable augmentation of more diverse types of text data.",1,2020
2020.emnlp-main.727.txt,it would be even better if the state detection and segmentation step can be integrated with subsequent state-independent feature extraction and rumor detection in an end-to-end framework.,1,2020
2020.emnlp-main.727.txt,"therefore, one direction for future work is to explore an online state detection algorithm and perform it for each event, but at the same time ensure that the state of each event is globally defined.",1,2020
2020.emnlp-main.728.txt,"looking forward, we plan to leverage pymt5 for various downstream automated software engineering tasks—including code documentation and method generation from natural language statements—and develop more model evaluation criteria to leverage the unique properties of source codes.",3,2020
2020.emnlp-main.729.txt,"first, we would like to explore more explainable reasoning method for question generation, such as symbolic-based models.",1,2020
2020.emnlp-main.729.txt,"in the future, there can be two research directions.",6,2020
2020.emnlp-main.73.txt,"while we showed that iterative inference with a learned score function is effective for spherical gaussian priors, more work is required to investigate if such an approach will also be successful for more sophisticated priors, such as gaussian mixtures or normalizing flows.",1,2020
2020.emnlp-main.730.txt,future work includes applying time inference models to question answering and other nlp systems.,4,2020
2020.emnlp-main.730.txt,we also seek to annotate information about dates and seasons.,2,2020
2020.emnlp-main.731.txt,"for structural generalization cases, the results of bowman et al.(2015); evans et al.(2018) and mccoy et al.(2019) suggest that treestructured models may provide a better inductive bias.",1,2020
2020.emnlp-main.731.txt,what architecture would be needed to solve cogs?,5,2020
2020.emnlp-main.733.txt,"in the future, we are looking forward to diving in representation learning with flow-based generative models from a broader perspective.",1,2020
2020.emnlp-main.736.txt,"as future work, we will explore the similar idea of designing unreferenced metrics for dialog generation.",3,2020
2020.emnlp-main.737.txt,"thus, future work involves extending the method to other related tasks, such as machine translation and text summarization, and investigating the potential gains from transfer learning.",4,2020
2020.emnlp-main.739.txt,"as future work, we would like extend the prior network to sample more than one persona sentences by expanding the sample space of the discrete random variable to generate more interesting responses.",2,2020
2020.emnlp-main.74.txt,"in our future work, i) we are interested in distilling from deep nmt models into extremely small students with ckd, in the hope of achieving the same results of large models with much smaller counterparts.ii) we also try to improve the combination module and find a better alternative than concatenation.iii) finally, we plan to evaluate ckd in other tasks such as language modeling.",1,2020
2020.emnlp-main.740.txt,there are some interesting directions for future work.,6,2020
2020.emnlp-main.742.txt,"besides, we also release a new large-scale human evaluation bench-mark to facilitate future research on automatic metrics.",3,2020
2020.emnlp-main.743.txt,"for future work, we will annotate medical entities in our datasets.",2,2020
2020.emnlp-main.743.txt,such annotations can facilitate the development of goal-oriented medical dialog systems.,1,2020
2020.emnlp-main.743.txt,we use transfer learning to apply these pretrained models for low-resource dialogue generation.,4,2020
2020.emnlp-main.747.txt,"the explainable ai community hopes to use them as a guide for evaluating model explanations and, possibly, for teaching models to make robust and wellreasoned decisions.",3,2020
2020.emnlp-main.748.txt,it would be very interesting to see what kind of performance larger models could achieve.,1,2020
2020.emnlp-main.749.txt,"for future work, we plan to apply our method for other type of spans, such as noun phrases, verbs, and clauses.",4,2020
2020.emnlp-main.75.txt,"for future work, we are interested in investigating the proposed approach in a scaled setting with more languages and a larger amount of monolingual data.",2,2020
2020.emnlp-main.75.txt,"furthermore, we would also like to explore the most sample efficient strategy to add a new language to a trained mnmt system.",1,2020
2020.emnlp-main.75.txt,scheduling the different tasks and different types of data would be an interesting problem.,5,2020
2020.emnlp-main.750.txt,we hope that this work will encourage continued research into factual consistency checking of abstractive summarization models.,1,2020
2020.emnlp-main.751.txt,future works on meta-evaluation should investigate the effect of these settings on the performance of metrics.(2) metrics easily overfit on limited datasets.,3,2020
2020.emnlp-main.752.txt,"in near future, we aim to incorporate the video script information in the multimodal summarization process.",1,2020
2020.emnlp-main.78.txt,"future directions include continuing the exploration of this research topic for large sequenceto-sequence pre-training models (liu et al., 2020) and multi-domain translation models (wang et al., 2019b).",1,2020
2020.emnlp-main.78.txt,"we also analyze the gains from perspectives of learning dynamics and linguistic probing, which give insightful research directions for future work.",1,2020
2020.emnlp-main.78.txt,"we will employ recent analysis methods to better understand the behaviors of rejuvenated models (he et al., 2019; yang et al., 2020).",1,2020
2020.emnlp-main.8.txt,"in future work, we would like to extend prism to paragraph- or document-level evaluation by training a paragraph- or document-level multilingual nmt system, as there is growing evidence that mt evaluation would be better conducted at the document level, rather than the sentence level (laubli et al., 2018).",3,2020
2020.emnlp-main.8.txt,we are optimistic our method will improve further as stronger multilingual nmt models become publicly available.,1,2020
2020.emnlp-main.80.txt,"besides, as this idea is not limited to machine translation, it is also interesting to validate our model in other nlp tasks, such as low-resource nmt model training (lample et al., 2018; wan et al., 2020) and neural architecture search (guo et al., 2020).",4,2020
2020.emnlp-main.80.txt,"it is interesting to combine with other techniques (li et al., 2018; hao et al., 2019) to further improve nmt.",1,2020
2020.emnlp-main.82.txt,"also, we’ll try to extend our methods in a wider range of nlp tasks.",4,2020
2020.emnlp-main.82.txt,"in future work, firstly, since our model is randomly sampled from model distribution to generate diverse translation, it is meaningful to explore better algorithms and training strategies to represent model distribution and search for the most distinguishable results in model distribution.",1,2020
2020.emnlp-main.83.txt,applying these latent alignment models for parallel translation of long documents can be an interesting research direction.,1,2020
2020.emnlp-main.84.txt,our findings also generalize to different positions and different datasets.,2,2020
2020.emnlp-main.85.txt,the inclusion of counts over clusters of answers provides a very rich dataset for training and evaluation.,2,2020
2020.emnlp-main.87.txt,how to most efficiently and effectively adapt transformer-based qa systems remains an important topic for future research.,1,2020
2020.emnlp-main.88.txt,"torque has 3.2k news snippets, 9.5k hard-coded questions asking which events had happened, were ongoing, or were still in the future, and 21.2k human-generated questions querying more complex phenomena.",2,2020
2020.emnlp-main.9.txt,we hope that prover will encourage further work towards developing interpretable nlp models with structured explanations.,1,2020
2020.emnlp-main.90.txt,"in the future, we will explore more informative generation and consider applying mgcn to other nlp tasks for better information extraction and aggregation.",4,2020
2020.emnlp-main.91.txt,"moreover, future work should inspect the effect of split and rephrase on downstream tasks such as machine translation or information retrieval, and examine if models’ performance on these tasks correlate with that on our benchmarks.",3,2020
2020.emnlp-main.93.txt,"in future, we plan to explore the following two directions: (1) interpolating the contexts between consecutive steps by introducing a new infilled image, and (2) addressing the underspecification problem by controlling the content in infilled image with explicit guidance.",1,2020
2020.emnlp-main.96.txt,"in future work, it would be interesting to investigate to what extent pretrained language models benefit from groc on such zero-resource or lowresource adaptation settings.",1,2020
2020.emnlp-main.96.txt,"this work indicates several other future directions for language modeling in low-resource domains: extension to other languages, scaling training to even larger vocabularies, and applying groc in a large pretraining setting to expand its zero-shot generalization.",2,2020
2020.emnlp-main.97.txt,"future work will explore applying ssmba to the target side manifold in structured prediction tasks, as well as other natural language tasks and settings where data augmentation is difficult.",4,2020
2020.emnlp-main.98.txt,combining sparse deep learning techniques with setconv is a potential solution to this issue.,1,2020
2020.emnlp-main.98.txt,we leave it for future work.,6,2020
D00-1302,we hope that other learning algorithms can benefit from the ideas presented here and that the idea of learning rres can be generalized to allow other learners to incorporate more powerful features as well.,1,2000
D00-1302,we plan to use argumentative zoning as a first step for ir and shallow document understanding tasks like summarization.,1,2000
D00-1303,"of course, we have to be careful in introducing such constraints, and they should be learned from existing corpus.",1,2000
D00-1303,integration with other simple models.,1,2000
D00-1303,error-driven data selection.,1,2000
D00-1303,"then, by analyzing held-out training data and selecting the features that affect the parsing accuracy.",1,2000
D00-1303,we can use the system to output some redundant parsing results and use only those results for the positive and negative examples.,2,2000
D00-1303,we can start with a small size of training data with a small size of feature set.,3,2000
D00-1303,"for future research, to reduce the computational overhead, we will work on methods for sample selection as follows: introduction of constraints on nondependency.",1,2000
D00-1304,we also intend to investigate the effects that other decision tree growth and smoothing techniques may have on continued refinement of the converted rule list.,3,2000
D00-1304,"future research will include testing the behavior of the system under adaboost (freund and schapire, 1997).",3,2000
D00-1306,the current approach uses uncertainty-based evaluation functions; we hope to consider other factors such as confidence about the parameters of the grammars and domain knowledge.,1,2000
D00-1306,we also plan to focus on the constituent units within a sentence as training examples.,1,2000
D00-1306,"first, since the ideas behind the proposed evaluation fimctions are general and independent of formalisms, we would like to empirically determine their effect on other parsers.",3,2000
D00-1306,"next, we shall explore alternative formulations of evaluation functions for the single-learner system.",3,2000
D00-1306,another area of interest is to experiment with committee-based sample selection using multiple learners.,3,2000
D00-1306,"finally, we are interested in applying sample selection to other natural language learning algorithms that have been limited by the sparsity of annotated data.",4,2000
D00-1307,"For future work, we plan to use derivation trees to train LTAG parsers directly and use LexTract to add semantic information to the Penn Treebank. ",1,2000
D00-1308,in the future we hope to explore automatically discovering information sources that can be profitably incorporated into maximum entropy part-of-speech prediction.,5,2000
D00-1309,"for future work, we will explore the effectivessness of considering even more contextual information on approximation of p(t~""ig ~) by using the forward-backward algorithm (rabiner 1989) while currently we only consider the contextual information of current location and previous location.",1,2000
D00-1312,we would like to extend the model to allow phrase generation in the query generation process.,1,2000
D00-1312,future improvements in cross-lingual ir will come by attacking the incompleteness of bilingual dictionaries and by improved query expansion and context-dependent translation.,5,2000
D00-1312,we also wish to explore techniques to extend bilingual lexicons.,5,2000
D00-1313,we are currently exploring an algorithm to generate the matrices more efficiently and the selection of coefficients in formula (6) also needs further research.,1,2000
D00-1314,we should extend our method to the large corpus of other domains without lost much accuracy.,2,2000
D00-1320,"although it has been very useful to work with the bbn model, we are currently implementing and hope to augment a more state-of-the-art model, vzz., models 2 and 3 of (collins, 1997).",1,2000
D00-1320,"we would also like to explore the use of a more radical model, where nonterminals only have synsets as their heads, and words are generated strictly at the leaves.",1,2000
D00-1320,"we would also like to incorporate long-distance context in the model as an aid to wsd, a demonstrably effective feature in virtually all the recent, statistical wsd work.",1,2000
D00-1320,"also, as mentioned earlier, we believe there are several features that would allow significant parsing improvement.",1,2000
D00-1320,"finally, we would like to investigate the incorporation of unsupervised methods for wsd, such as the heuristically-based methods of (stetina and nagao, 1997) and (stetina et al., 1998), and the theoretically purer bootstrapping method of (yarowsky, 1995).",1,2000
D00-1320,"bolstered by the success of (stetina and nagao, 1997), (lin, 1997) and especially (stetina et al., 1998), we believe there is great promise the incorporation of word-sense into a probabilistic parsing model.",1,2000
D00-1321,"first, studies on recovery mechanisms for unsafe segmentation before parsing seem necessary since ungafe segmentation may cause parsing failures.",5,2000
D00-1321,"second, parsing control mechanisms should be studied that exploit the characteristics of segmentation positions and the parallelism among segments.",5,2000
D00-1322,"since most of the knowledge learned from a domain is not useful when changing to a new domain, further investigation is needed on tuning strategies, specially on those using non-supervised algorithms.",1,2000
D00-1322,"further work on web-based ne acquisition could take advantage of machine learning techniques as used for wrapper induction (kushmerick et al., 1997).",1,2000
D00-1322,extensively evaluate lazyboosting on the wsd task.,3,2000
D00-1324,"another task for the near future is, as mentioned in the previous section, to add an ordering mechanism on binary conjuncts in order to ensure that the more restrictive node pairs are searched for first.",1,2000
D00-1324,"first, the set of queries the tool can process needs to be extended to all queries allowed in the query language.",2,2000
D00-1324,"further, the design of a graphical user-interface to enter the queries is planned, allowing to specify queries by drawing partial trees instead of typing in the expressions in the query language.",6,2000
D00-1324,"finally, we also want to implement a web-based userinterface for the query tool.",6,2000
D00-1325,"however, it seems from our experiments that it would be better to avoid hypothesis tests that make use of the unconditional distribution.",1,2000
D00-1325,"one possibility is to put more effort into the estimation of pe, and to avoid use of the unconditional distribution for this.",1,2000
D00-1325,further work on handling low frequency data in scf acquisition is warranted.,2,2000
D00-1326,"we plan to devise ways to integrate genre/topic parameters into the word sense disambiguation models, and to apply them on a system to acquire training examples automatically.",1,2000
D00-1326,"further work will focus on evaluating the separate weight of genre and topic in word sense disambiguation performance, and on studying the behavior of each particular word and features through genre and topic variations.",3,2000
D00-1327,"if filtering based on relative frequencies still achieves better results, it would be worth investigating ways of handling the low frequency data for integration with this method.",1,2000
D00-1327,"this will involve (i) defining the set of semantic verb classes across the lexicon, (ii) obtaining back-off estimates for each verb class, and (iii) implementing a method capable of automatically classifying verbs to semantic classes.",1,2000
D00-1327,"while this approach proved satisfactory, our future work will include investigating ways of addressing the problem of polysemy better.",1,2000
D00-1327,"in addition to refining the filtering method, our future work will focus on integrating this approach with large-scale scf acquisition.",1,2000
D01-0502,"better understanding of methods for thresholding the probability distributions that the classifiers output, as well as principled ways to order them are also among the future directions of this research.",1,2001
D01-0503,and then using the model for apractical application such as n-best list rescoring.,3,2001
D01-0503,more work remains to be done to evaluate now well the semantic coherence measure improves the baseline language model.,3,2001
D01-0503,this evaluation can be performed by incorporating the semantic coherence features into an exponential language model,3,2001
D01-0504,the heart of the problem still seems to befinding the overall best translation for all thewords rather than advanced word sense disambiguation task of finding the right translationin a given context.,5,2001
D01-0504,this is even more true forlow density languages.where less resources areavailable.,6,2001
D01-0505,we expect that the alignment can be more accurate and efficient by combining the structural features with translation axicon in the future.,1,2001
D01-0506,"finally, it would be interesting to compare the performance of the stacked generalization approach to other multiclassifier methods, such as boosting (schapire & singer, 2000).",3,2001
D01-0506,"furthermore, we would like to evaluate other classifiers in the role of the president.",3,2001
D01-0506,"a larger variety of  classifiers is expected to lead the president to  more informed decisions, resulting in further  improvement of the filters performance.",1,2001
D01-0506,"in particular, we are  interested in combining more classifiers, such as  decision trees (quinlan, 1993) and support  vector machines (drucker, et al.1999), within  the stacking framework.",1,2001
D01-0508,"we hope to use query expansion, analogous to the way it was used with bining to improve information retrieval in(umemura and church.2000).to boost results further.",1,2001
D01-0508,even without conducting full experiments examination of term weights can determine which features are important.,1,2001
D01-0508,"also we believe that it might be better to compute term weights for individual words when possible, and so back off to the bin only when necessary to her words.",1,2001
D01-0508,"in the future, we hope to test more binning features.",1,2001
D01-0510,"as for using the slm as the language understanding component of a speech driven application, such as mipad, it would be interesting to evaluate the impact of incorporating the semantic constraints on the word-level accuracy of the system.",3,2001
D01-0510,another possible research direction is to modify the framework such that it finds the most likely semantic parse given the acoustics -- thus treating the word sequence as a hidden variable.,1,2001
D01-0511,we also plan to tackle the problem of noun compounds containing more than two terms.,5,2001
D01-0511,in future we plan to train the algorithm to allow different levels for each noun in the compound.,1,2001
D01-0511,"we also plan to compare the results to the tree cut algorithm reported in (li and abe, 1998), which allows different levels to be identified for different subtrees.",1,2001
D01-0514,"finally, a lsa procedure for computing document specihc similarity values will be evaluated.",3,2001
D01-0514,future work will focus on document specifica and the terminations algorithm.,5,2001
D01-0514,wethe threshold selection method has to be modified.,1,2001
D01-0514,"in terms of clustering, dynamic programming approaches (ponte and croft, 1997:utiyama and isahara, 2001, for example)will be examined.",1,2001
D01-0515,"we will again start by investigating data, in particular the distribution of symbol tuples.",2,2001
D01-0515,"however, we need to compare our system with other methods and the whole issue of evaluation of large scale language processing tools needs much more attention.",3,2001
D01-0516,"examples are : 	mechanisms for processing these abbreviations, which tend to occur in informal text such as email, chat rooms, or customer service call records, are the subject of ongoing research in our project.",6,2001
D01-0517,"in the future, we have to automatically construct dialog style tobi labeled corpus to verify our prediction model more thoroughly since current mbcnewsdb has biased tone labels because it is a reading corpus for broadcast newsscripts.",2,2001
D01-0520,we are shus assessing the ability of our grammars to map between surface string and some sort of meaning representation which is exactly what grammar is generally assumed to do.,3,2001
D01-0521,cross corpus experiments could reveal whether these clusters uncover generally applicable semantic categories for the parser's use.,2,2001
D01-0521,"an area for future work is investigation of the degree to which such features apply across corpora, or, on the other hand, further tune the parser to the peculiarities of the wall street journal.",4,2001
D01-0521,of particular interest are the automatic clustering of lexical cooccurrences used in charniak (1997) and magerman (1995).,1,2001
D02-1003,"notice however that the chief advantage of decision lists over linear models is their compact size and understandability, and our techniques simultaneously improve those aspects; adding additional splits will almost certainly lead to larger models, not smaller.",1,2002
D02-1003,"it would also be interesting to try more sophisticated smoothing techniques, such as those of yarowsky.",1,2002
D02-1005,"finally, since the mixture model and its improvements performed well on two major tasks and several multilingual data sets, we believe that they can be productively applied to other related high-dimensionality lexical classification problems, including named-entity classification, topic classification, and lexical choice in machine translation.",1,2002
D02-1006,we will investigate the effect of more elaborate feature selection schemes on the performance of different learning algorithms for wsd in future work.,1,2002
D02-1008,"in addition, although we use ripper as the underlying learning algorithm in our coreference system, we expect that the techniques described in this paper can be used in conjunction with other learning algorithms.",4,2002
D02-1008,we plan to explore this possibility in future work.,6,2002
D02-1009,we expect to further pursue transformation models (and simpler variants that are easier to estimate) within this flexible finite state framework.,1,2002
D02-1011,"hence, we believe that an important next step is the identification of features indicating whether sentences are on topic (which is a kind of coreference problem); we look forward to addressing this challenge in future work.",5,2002
D02-1014,"we are currently working on a version of the lr parser for a subclass of tags, the tree insertion grammars (schabes and waters, 1995), for which efficient true lr parsers can be obtained.",1,2002
D02-1017,"in future work, we hope to investigate other types of syntactic structures that may be used to identify semantically related terms, and other types of heuristics that can reveal specific semantic relationships.",1,2002
D02-1019,"our ultimate goal, towards which this work is the first step, is to construct loss functions that take advantage of linguistic structures such as syntactic dependencies found through monolingual analysis of the sentences to be aligned.",1,2002
D02-1019,in future work we will investigate loss functions that incorporate french and english parse tree information into the alignment decoding process.,1,2002
D02-1020,we feel that the idea of creating explicit user models to guide the behaviour of interactive systems is likely to have applications in areas of nlp apart from translators’ tools.,5,2002
D02-1020,other possibilities include virtually any application where a human and a machine communicate through a language rich interface.,1,2002
D02-1024,"in particular, we plan to investigate semiautomatic methods for extracting ontological knowledge from existing webpages and databases.",1,2002
D02-1024,"in our current work, we are focusing on expanding system coverage to other domains.",4,2002
D02-1025,future work would include experiments using larger scale test collections in various domains.,3,2002
D02-1026,"(iii) evaluating the manipulating data approach using automatically generating hierarchies(sanderson and croft, 1999).",3,2002
D02-1026,"future work includes (i) extracting features which discriminate between categories within the same top level category,",1,2002
D02-1026,"(ii) investigating other machine learning techniques to obtain further advantages in efficiency in the manipulating data approach, and",1,2002
D02-1029,this includes extending these experiments to an even larger corpus with the hope of establishing the cross over point for thesaurus extraction.,2,2002
D02-1029,"finally, although wider machine learning research uses large ensembles, many nlp ensembles use only a handful of classifiers.",5,2002
D02-1029,"we would like to further investigate the relationship between contextual complexity, data sparseness, noise and learner bias on very large corpora.",5,2002
D02-1029,it would be very interesting to experiment with a large number of classifiers using bagging and boosting techniques on very large corpora.,1,2002
D02-1030,"if web counts correlate reliable with smoothed counts, then this provides further evidence for our claim that the web can be used to overcome data sparseness.",5,2002
D02-1030,"in future work, we plan to compare web counts for unseen bigrams with counts recreated using standard smoothing algorithms, such as similarity based smoothing (dagan et al., 1999) or class based smoothing (resnik, 1993).",1,2002
D02-1031,we are currently extending the almost parsing superarv lm to a full parser based lm.,1,2002
D02-1032,other areas include the application of the proposed model to a wider variety of test corpora and to related tasks.,4,2002
D02-1032,one area of future work is therefore to reduce the model size.,1,2002
D02-1039,we are interested in examining different language pairs as the opportunity arises.,3,2002
D02-1039,"in the future, we plan to explore this hypothesis in an actual translation system.",5,2002
D02-1040,we are curious to investigate further cheap features and compare them to what could be obtained when taking domain or world knowledge into account.,3,2002
D02-1041,an investigation of how to combine these sources of information is left for future research.,2,2002
D03-1003,"we are developing maximum entropy models to more effectively combine the multiple information sources we have used in our experiments, and expect to report the results in the near future.",1,2003
D03-1007,"in future work, we intend to apply the lessons learned here to the problem of frame element identification.",4,2003
D03-1007,gildea and jurafsky have shown that improvements in identification can be had by more closely integrating the task with classification (they report an f-score of .719 using an integrated model).,6,2003
D03-1007,initial results show that significant improvements can be had using techniques similar to those described above.,1,2003
D03-1007,we are currently exploring a me approach which integrates these two tasks under a tagging framework.,1,2003
D03-1008,we would expect a higher performance for the ccg-based system if the analyses in ccgbank resembled more closely those in propbank.,1,2003
D03-1009,finding applications of these results is the most important direction for future research.,1,2003
D03-1021,there are still many interesting problems in applying the neural network enhenced slm to real applications.,5,2003
D03-1021,"in particular, if we use separate mapping matrices for word/nt/pos at different positions in the context, we may be able to learn very different representations of the same word/nt/pos.",1,2003
D03-1021,"interpreting the word representations learned in this framework: for example, word clustering, context clustering, etc.",1,2003
D03-1021,"among those, we think the following are of most of interest: speeding up the stochastic gradient descent algorithm for neural network training: since training the neural network models is very time-consuming, it is essential to speed up the training in order to carry out many more interesting experiments.",1,2003
D03-1027,the combined approach may yield better results with a small number of labeled examples.,1,2003
D03-1027,"in the future, it would be interesting to employ virtual examples with methods to use both labeled and unlabeled examples (e.g., (blum and mitchell, 1998; nigam et al., 1998; joachim's, 1999)).",1,2003
D03-1027,we believe we can use prior knowledge on these tasks to create effective virtual examples.,1,2003
D03-1027,"another interesting direction would be to develop methods to create virtual examples for the other tasks (e.g., named entity recognition, pos tagging, and parsing) in nlp.",5,2003
D03-1028,future work will examine alternative approaches to evaluation.,3,2003
D03-1028,"one possibility for a more liberal evaluation could be to use human evaluators with real information needs, as done by turney (2000).",3,2003
D03-1028,"future work should also go in the direction of generating (as opposed to extracting) keywords, by for example exploring potential knowledge provided by a thesaurus.",5,2003
D03-1028,"this would hopefully lead to a better precision, while recall probably would be affected negatively; the importance of recall would then need to be reconsidered.",5,2003
D03-1028,"another possibility would be to let several persons index each document, thus getting a larger set of acceptable terms to choose from.",1,2003
D04-3201,"this trade-off between the complexity, accuracy and efficiency of a parsing model is an important area of future research.",5,2004
D04-3203,we plan to add a beam search to explore the speed-accuracy tradeoff.,1,2004
D04-3203,"improvements in the state representation are possible, particularly along the lines of linguistically-motivated treebank transformations, as in klein and manning (2003).",1,2004
D04-3203,adding a lexical component to the model is another extension we intend to investigate.,1,2004
D04-3204,"in addition, we will give preference to multiwords that contain the target word when choosing the relatives.",2,2004
D04-3204,"now that the monosemous corpus is available for all nouns, we would also like to test the system on the all-words task.",3,2004
D04-3204,"finally, more sophisticated methods to acquire examples are now available, like exretriever (fernandez et al., 2004), and they could open the way to better examples and performance.",1,2004
D04-3204,"for the future we also want to test the performance of more powerful machine learning methods, explore feature selection methods for each individual word, and more sophisticated ways to combine the examples from the web corpus with those of semcor or senseval.",1,2004
D04-3205,"we hope to open the way to inferring implied, but not stated assertions and to benefit applications such as question answering, information retrieval, and summarization.",5,2004
D04-3205,further work may refine extraction methods and further process the mined semantics to derive other relations such as entailment.,1,2004
D04-3205,"another possibility would be to use more relaxed patterns when the part of speech confusion is not likely (e.g. ""eat” is a common verb which does not have a noun sense, and patterns need not protect against noun senses when testing such verbs).our approach can potentially be extended to multiword paths.",1,2004
D04-3205,"one avenue would be to automatically learn or manually craft more patterns and to extend the pattern vocabulary (when developing the system, we have noticed that different registers and verb types require different patterns).",1,2004
D04-3205,there are several ways to improve the accuracy of the current algorithm and to detect relations between low frequency verb pairs.,1,2004
D04-3206,in future work we also plan to find the valid contexts for entailment relations.,2,2004
D04-3206,in future work we aim to improve the yield by increasing the size of the sample-corpus in a qualitative way.,2,2004
D04-3206,"like (lin and pantel, 2001), learning the context for which entailment relations are valid is beyond the scope of this paper.",6,2004
D04-3206,"in future work we aim to improve the yield by increasing the size of the sample-corpus in a qualitative way, as well as precision, using statistical methods such as supervised learning for better anchor set identification and cross-correlation between different pivots.",1,2004
D04-3206,"we also plan to support noun phrases as input, in addition to verb phrases.",1,2004
D04-3206,"finally, we would like to extend the learning task to discover the correct entailment direction between acquired templates, completing the knowledge required by practical applications.",1,2004
D04-3208,"finally, we would like to suggest that bootstrapping can in the future be used in conjunction with other sentence or word alignment learning methods to provide better mining results.",1,2004
D04-3209,"in future work we hope to determine how the individual qualitative differences of the two models (estimation methods, model structure, etc.)contribute to the observed differences in results.",1,2004
D04-3209,we also plan to investigate using a conditional random field (crf) model.,1,2004
D04-3209,"to improve results overall, we plan to explore features that combine multiple knowledge sources, as well as approaches that model recognition uncertainty in order to mitigate the effects of word errors.",1,2004
D04-3210,"in the future, we plan to apply our model to new domains (e.g., broadcast news or scientific papers), to non-indo-european languages such as arabic and chinese, and to machine generated texts.",4,2004
D04-3211,another area for future research involves the estimation of class probabilities.,3,2004
D04-3211,further research is also required to determine how rfs generalize to new genres.,4,2004
D04-3213,"we also hope to integrate some processing of adjunct roles, rather than limiting ourselves to the specified arguments.",6,2004
D04-3215,we will extend the question corpus to other question types.,2,2004
D04-3215,"we are also continuing to develop the super tagger, which we have demonstrated is central to efficient portable wide-coverage ccg parsing.",1,2004
D04-3216,"finally, we are looking to incorporate the results of this model into a real system.",1,2004
D04-3217,results from this study will be instrumental in shaping the future of the plot analysis system in story station and the expansion of the current system into a general purpose plot analysis system for other writing tasks.,1,2004
D04-3219,we also plan to pursue better (automated) metrics for paraphrase evaluation.,3,2004
D04-3219,"this will we hope, eventually allow us to address such issues as paraphrase identification for ir.",1,2004
D04-3219,"although we have not attempted to address the issue of paraphrase identification here, we are currently exploring machine learning techniques, based in part on features of document structure and other linguistic features that should allow us to bootstrap initial alignments to develop more data.",1,2004
D04-3219,we will be experimenting with more sophisticated decoder models designed to handle reordering and mappings to discontinuous elements.,1,2004
D04-3219,"to exploit richer data sets, we will also seek to address the monotone limitation of our decoder that further limits the complexity of our paraphrase output.",1,2004
D04-3219,"while the alternations our system produces are currently limited in character, the field of smt offers a host of possible enhancements—including reordering models—affording a natural path for future improvements.",1,2004
D04-3220,"most importantly, more specific use could be made of scf information besides modeling its joint distribution with sense, for example conditioning on headwords of (perceived) arguments, especially particles and prepositions.",1,2004
D04-3220,"one way this could be done would be to use a parser only to estimate the probability of the sequence of word tags (i.e., parts of speech) in the sentence, then to use a sense-specific lexicon to estimate the probability of finding the words under the tags.",1,2004
D04-3220,"it may also be possible to improve parsing accuracy on verb phrases or other phrases, by simultaneously resolving word sense ambiguities, as attempted unsuccessfully by bikel (2000).",1,2004
D04-3220,"third, we could hope to get some improvement from changing our model structure to address the issue of double generation of words discussed in section 3.",1,2004
D04-3220,"second, although we made some attempt at extracting the “underlying” scf of verbs by analyzing passive constructions separately, similar analysis of other types of movement such as relative clauses may also be useful.",1,2004
D04-3221,we plan to extend this work both by refining our notion of attribute and by using more sophisticated patterns working off the output of a parser.,1,2004
D04-3222,we also plan to explore annotation with more features from hpsg signs.,1,2004
D04-3222,"in future work, we aim to explore the definition of new string kernels that are more suitable for this particular application and apply these ideas to penn treebank parse trees.",1,2004
D04-3223,further improvements of efficiency of grafting are possible by applying zhou et al.’s (2003) technique of restricting feature selection in each step to the top ranked features from previous stages.,1,2004
D04-3224,"of course, there is still much more analysis, hypothesizing, testing and extrapolation to be done.",5,2004
D04-3224,a thorough study of the highest-entropy distributions should reveal new ways in which to use grammar transforms or develop features to reduce the entropy and increase parse accuracy.,1,2004
D04-3224,"a closer look at the low-entropy distributions may reveal additional reductions in the size of the model, and, perhaps, a way to incorporate hard constraints without disturbing the more ambiguous parts of the model more suited to machine learning than human engineering.",1,2004
D04-3227,"our future work is to explore alternatives such as the reranking work in (collins, 2002) and include more knowledge such as syntax information in rescoring the phrase translation pairs.",1,2004
D04-3229,"finally, we would like to extend our work to slavic languages for which there are even fewer available resources than russian, such as belarusian, since this was the original motivation for undertaking the work in the first place.",2,2004
D04-3229,"we are seeking the right way to operationalize this intuition in our system, bearing in mind that we want a sufficiently general algorithm to make the method portable to other languages, for which we assume we have neither the time nor the expertise to undertake knowledge-intensive work.",1,2004
D04-3230,"for this challenge, mccallum proposes an interesting research avenue to explore (mccallum, 2003).",5,2004
D04-3230,there exist some phenomena which cannot be analyzed only with bi-gram features in japanese morphological analysis.,5,2004
D04-3230,"however, the numbers of features and nodes in the lattice increase exponentially as longer contexts are captured.",6,2004
D04-3230,"to deal with longer contexts, we need a practical feature selection which effectively trades between accuracy and efficiency.",1,2004
D04-3230,"to improve accuracy, tri-gram or more general n-gram features would be useful.",1,2004
D04-3230,crfs have capability of handling such features.,1,2004
D04-3233,"while the reranking is very efficient in the classification phase, training a support vector reranking system is computationally very expensive.",5,2004
D04-3235,are there situations for which we must expect a significance difference between the two decision rules?,5,2004
D04-3235,can we come up with a training criterion tailored to the symbol error rate?,5,2004
D04-3235,is that an experimental coincidence?,5,2004
D04-3235,"we speculate that the two decision rules could always have similar performance if the error rates are small.2) ideally, the training criterion should be closely related to the error measure used in the decision rule.",5,2004
D04-3235,is it possible to derive closed-form bayes decision rules (or suitable analytic approximations) for these error measures?,5,2004
D04-3235,1) the error rates for the two decision rules are comparable.,6,2004
D04-3235,"right now, we have used the training criteria that had been developed in the past and that had been (more or less) designed for the string error rate as error measure.",6,2004
D04-3237,"another interesting research direction is to explore the usefulness of the map adaptation of maxent models for other problems among which we wish to include language modeling, part-of-speech tagging, parsing, machine translation, information extraction, text routing.",5,2004
D04-3237,"as future work we plan to investigate the best way to blend increasing amounts of less-specific background training data with specific, indomain data for this and other problems.",5,2004
D04-3238,"a direction that we plan to investigate is the adaptation of such a technique to the general purpose spelling correction, by using statistics from both query-logs and large office document collections.",2,2004
D04-3239,"we would like to apply our method to other applications where instances are represented in a tree and their subtrees play an important role in classifications (e.g., parse re-ranking (collins and duffy, 2002) and information extraction).",4,2004
D04-3240,this suggests that useful task-tracking tools could be constructed based on automatic classifiers—a potentially important practical application.,4,2004
D04-3240,"we showed that entity extraction and part of speech tagging improves classifier performance, but leave open the question of whether other types of linguistic analysis would be useful.",5,2004
D04-3241,"another important prediction of the entropy rate principle remains to be evaluated in future work: for out-of-context sentences, there should be a correlation between sentence position and processing effort.",3,2004
D04-3241,this prediction can be tested by obtaining reading times for sentences sampled from a corpus and read by experimental subjects in isolation.,3,2004
D04-3242,it will be very interesting to see how our approach performs in a longer history than the trigram.,5,2004
D04-3242,"since our current rf models uses kn smoothing exclusively in lower order probabilities, 3for the * -test, we used the standard sclite’s statistical system comparison program from nist with the option “maps we”, which means the test is the matched pairs sentence segment word error test.it may not be adequate when we apply it to higher order  -gram models.",6,2004
D04-3242,one possible solution is to use rf models for lower order probabilities as well.,1,2004
D04-3242,higher order rfs will be grown based on lower order rfs which can be recursively grown.,1,2004
D04-3244,"second, as noted in section 5.2.1, the dimensionality reduction required for linguistic data may constrain the performance of the metric distance.",5,2004
D04-3244,"the latter obviates the problem of dimensionality, while it restricts the usage to a situation where the kernel-based approach is available.",6,2004
D04-3244,"to alleviate this problem, simultaneous dimensionality reduction and metric induction may be necessary, or the same idea in a kernel-based approach is worth considering.",1,2004
D04-3244,"first, as we stated in section 4.3, the effect of a cluster weighted generalized metric must be investigated and optimal weighting must be induced.",1,2004
D04-3245,as well as some improvements in the search algorithms to reduce the computational cost of finding a path in the word graph with the minimum edit cost.,1,2004
D04-3245,"finally, the introduction of morpho-syntactic information or bilingual categories in finite-state transducers, are topics that leave an open door to future research.",1,2004
D04-3246,we intend to investigate this problem for hebrew in the future.,2,2004
D04-3246,"another example is the case of morphological disambiguation in languages with non-trivial morphology, which can be viewed as a pos tagging problem with a large number of tags on which structure can be imposed using the various morphological and morphosyntactic features that morphological analyzers produce.",1,2004
D04-3248,we also notice that ner performance over the source language can be improved using bilingual knowledge.,2,2004
D04-3248,we may need some manually-generated rules to fix this.,1,2004
D04-3249,"as far as the evaluation of dre is concerned, for the moment we have tested its usefulness in the context of a wsd task, but we are going deeper, considering a pure tc framework.",3,2004
D04-3251,more effort is required in order to better integrate the cluster-specific models.,1,2004
D04-3251,strategy overlap analysis and refinement of local optimization criteria has the potential to improve overall performance under time constraints.,1,2004
D04-3252,"an important aspect of textrank is that it does not require deep linguistic knowledge, nor domain or language specific annotated corpora, which makes it highly portable to other domains, genres, or languages.",4,2004
D04-3253,the approach presented here is flexible and suggests promising avenues of further investigation.,6,2004
D04-3254,"we also plan to expand our data sets to more texts, in order to investigate the presence and distribution of factoids, types of factoids and relations between factoids in summaries and summary collections.",2,2004
D04-3254,we now plan to elicit the help of new annotators to increase our data pool.,2,2004
D04-3254,another pressing line of investigation is reducing the cost of factoid analysis.,5,2004
D04-3254,the flrst reason why this analysis is currently expensive is the need for large summary bases for consensus summaries.,5,2004
D04-3254,"all in all, the use of factoid analysis and weighted factoid score, even though initially expensive to set up, provides a promising alternative which could well bring us closer to a solution to several problems in summarization evaluation.",1,2004
D04-3255,we can apply this method to each submodel of machine translation.,4,2004
D05-1001,"further work will include, in addition to extending the set of documents and testing the system with other collections, evaluating the improvement to be achieved by adding a proper noun resolution algorithm to guitar.",1,2005
D05-1002,we also want to benefit from our experience with the czech data in order to create an english corpus annotated with information structure.,2,2005
D05-1002,"issues for further research include, on the one hand, a deeper investigation of the topic-focus articulation in the prague dependency treebank of czech, by improving the feature set, considering also the distinction between contrastive and noncontrastive t items and, most importantly, by investigating how we can use the t/f annotation in pdt (and respectively our results) in order to detect the topic/focus partitioning of the whole sentence.",1,2005
D05-1003,"this approach could be applied more broadly, to different nlp tasks, and also more deeply, going beyond the simple one-and-a-halfiteration procedure we present here.",4,2005
D05-1003,we also intend to extend our method both to cross-document relation detection and to event detection.,4,2005
D05-1005,this will result in a more generative and less extractive approach to summarization - indeed the case for generative approaches to summarization is more convincing when the input is noisy.,1,2005
D05-1005,"in the future, we plan to consider other types of constituents, such as correcting errors in verb groups, and in the argument structure of verbs.",1,2005
D05-1007,"in future work, we plan to run our experiments on other datasets when they become available to us.",2,2005
D05-1007,"in particular, we want to experiment with multi-topic audio documents where we expect more marked advantages for windowing and alternative aggregation schemes like max and 3max.",2,2005
D05-1007,"some named entities could have high semantic similarity with the text if they are frequently mentioned in the same contexts in the web corpus, but some names could be common to many contexts.",2,2005
D05-1007,"a final direction for research is to conduct experiments with human subjects, to evaluate the degree to which filtered transcripts are better than unfiltered ones for tasks like browsing, gisting and searching audio clips.",3,2005
D05-1007,we plan to explore ways to scale up other corpusbased semantic similarity measures to large terabyte corpora.,4,2005
D05-1007,"we plan to explore more approaches to detecting semantic outliers, for example clustering or lexical chains (hirst and st-onge, 1997).",1,2005
D05-1007,another future direction will be to actually correct the errors instead of just filtering them out.,1,2005
D05-1007,"the most promising direction is to combine our method with confidence measures that use internal information from the asr system (although the internal information is hard to obtain when using an asr as a black box, and it could be recognizer specific).",1,2005
D05-1007,to increase recall we can also identify named entities and not filter them out.,1,2005
D05-1007,"a combination is likely to improve the performance, with the pmi-based measure contributing at the high-precision end and the internal asr measure contributing to the high-recall end of the spectrum.",1,2005
D05-1007,"for example, we might look at the top n speech recognizer hypotheses (for a fairly large n like 1000) and choose the one that maximizes semantic cohesion.",1,2005
D05-1009,we are also planning to investigate whether neuralign helps when the individual aligners are trained using more data.,2,2005
D05-1009,we also intend to evaluate the effectiveness of our improved alignment approach in the context of machine translation and cross-language projection of resources.,3,2005
D05-1009,we will do additional experiments to observe the effects of varying the size of the annotated data while learning neural nets.,3,2005
D05-1009,"we will extend our combination approach to combine word alignment systems based on different models, and investigate the effectiveness of our technique on other language pairs.",1,2005
D05-1013,"we intend to explore more complex features for resolving pronouns, and to incorporate these features into our current model.",1,2005
D05-1013,"we also intend to explore more complex models for automatically extracting knowledge from data that can help with this task and applying this technique to a real application, such as summarization.",1,2005
D05-1016,in the future ned can also be extended to other interesting domains like scientific literature to detect the emerge of new topics and interests.,4,2005
D05-1016,the reason for this superior performance over other kernels needs to be investigated.,5,2005
D05-1016,a classifier with rbf kernel with γ set to one exhibited the best performance.,6,2005
D05-1016,engineering of better features is also a definite priority.,1,2005
D05-1017,"furthermore, it may be very interesting to explore optimal combinations of intensional and extensional supervision, provided by the user in the forms of seed features and labeled examples.",1,2005
D05-1017,future work is needed to investigate optimal procedures for collecting seed features and to find out whether additional seeds might still contribute to better performance.,1,2005
D05-1019,we are now planning to apply our method to an evaluation of machine translation.,4,2005
D05-1019,we believe that our method will also be useful for other natural language generation tasks.,4,2005
D05-1020,"of course, using only few test cases (topics sets and collections) is a limitation of this current study, which we are going to address in our future research.",2,2005
D05-1020,"our approach can be also used as an explorative tool in order to identify important relevance-indicating features, which can be later modeled analytically.",4,2005
D05-1020,"we believe that our work and the ones referred in this paper may bring many of the achievements made in a more general area of classification and machine learning closer to the task of rank ordered information retrieval, thus making retrieval engines more helpful in reducing the information overload and meeting people’s needs.",4,2005
D05-1021,future work will build on these simple structures to produce more powerful models of word and phrase movement in translation.,1,2005
D05-1024,"finally, we will evaluate the improved alignments in the context of an end-to-end application, such as machine translation.",3,2005
D05-1024,"whether alp improves the statistical alignment systems when they are trained on more data is an interesting research problem, which we plan to tackle in future.",5,2005
D05-1026,"this is similar to boosting techniques (freund, 1995) which build sequentially classifiers that focus on examples wrongly classified by the preceding one.",1,2005
D05-1026,one could also imagine to associate a probability to each training example and to use these probabilities to weight the random sampling.,1,2005
D05-1026,several extensions of the learning algorithm itself are promising.,1,2005
D05-1026,these probabilities would be updated after each epoch.,1,2005
D05-1026,we are in particular interested in smarter ways to select different subsets from the large corpus at each epoch (instead of a random choice).,1,2005
D05-1026,"one possibility would be to use active learning, i.e. focusing on examples that are most useful to decrease the perplexity.",1,2005
D05-1027,"however, due to the lack of theoretical underpinnings, we are unable to prove that msr will always succeed.",5,2005
D05-1028,our future work will specifically investigate how to combine information from multiple sources in salience modeling and how to apply the salience models in different early stages of processing.,1,2005
D05-1029,our next efforts will focus on using reinforcement learning to automatically derive the error recovery policies.,1,2005
D05-1030,"finally, it may also be useful to integrate the prosodic events directly into the pcfg, in addition to their use in reranking.",1,2005
D05-1030,"one could combine ip and prosodic break features (so far explored separately), find new combinations of prosody and syntactic structure, and/or incorporate other prosodic events.",1,2005
D05-1030,"in addition to assessing the impact of prosody in a fully automatic system, other avenues for future work include improving feature extraction.",1,2005
D05-1033,"an important future direction lies in extending our model to the document-level and the assignment of rhetorical relations, thus going beyond the basic nucleus-satellite distinction.",1,2005
D05-1033,"our results indicate that a modular approach to discourse processing (i.e., treating segmentation as separate from labelling) could increase performance.",1,2005
D05-1033,"in the future, we plan to investigate how to combine our chunker with models like spade for improved prediction on both local and global levels.",1,2005
D05-1034,"one direction of future research is to apply this technique to an incremental learning scenario, i.e., to incrementally build models using incoming data for adaptation, taking all previously available data as background corpus.",4,2005
D05-1036,"it produces code that is slower than hand-crafted code but acceptably fast for our nlp research, where it has been extremely helpful.",6,2005
D05-1036,"we hope it will facilitate emnlp research, just as fs toolkits have done for the fs case.",1,2005
D05-1039,"to improve the performance on “what-is” questions, we could divide “what-is” questions into finer classes such as organization, location, disease, and general substance, and process them specifically.",1,2005
D05-1039,"another possible improvement is to generalize using automatically derived word clusters, which provide semantic information.",2,2005
D05-1041,we have already pointed out the need to improve the positive precision of the training examples.,5,2005
D05-1041,"finally, our method can be improved by including attributes for the layout and authority of web pages.",1,2005
D05-1041,one way may be to combine our similarity method with cui et al.’s centroids.,1,2005
D05-1041,we also plan to study the effect of including more automatically acquired patterns and using more training target terms.,1,2005
D05-1042,"an interesting question is how to integrate our component into a generation pipeline, using feedback from other components to guide collective content selection.",5,2005
D05-1042,"ideally, we would like to express more complex relations between items.",5,2005
D05-1042,"for instance, we may want to represent disjunctive constraints, such as “at least one of the defense players should be mentioned in the summary.”",5,2005
D05-1042,another promising approach is the combination of our automatically acquired cross-entity links with domain knowledge.,1,2005
D05-1042,"for instance, we may want to represent disjunctive constraints, such as “at least one of the defense players should be mentioned in the summary.” such dependencies can be efficiently handled in a collective classification framework by using approximate probabilistic inference (taskar et al., 2002).",6,2005
D05-1042,"currently, we consider a limited set of contextual dependencies based on attribute similarity.",1,2005
D05-1042,"in the future, we plan to explore how to integrate more refined discourse models in the content selection process.",1,2005
D05-1045,the probability or the strength of an opinion expression may also play a useful role in encouraging or suppressing source extraction.,1,2005
D05-1045,"for example, the fact that a coreferring noun phrase was marked as a source in one sentence could be a useful clue for extracting the source from another sentence.",1,2005
D05-1045,"directions for future work include trying to increase recall by identifying relationships between opinions and sources that cross sentence boundaries, and relationships between multiple opinion expressions by the same source.",1,2005
D05-1049,we believe that the methods employed in this work show much potential for improving the state-of-the-art in computational semantic inference.,1,2005
D05-1051,the challenge lies in producing pseudowords that better model real words.,5,2005
D05-1051,"however, the size of modern ir test collections dictates that future studies will need to rely more heavily on simulation.",1,2005
D05-1051,"therefore, until such time that a significant manually disambiguated ir collection exists pseudowords remain an interesting way to explore the effects of ambiguity within a large collection.",1,2005
D05-1053,we will do further work to ascertain the best method for quantifying “substantial change”.,1,2005
D05-1053,we also intend to exploit the automatic ranking to obtain information on sense frequency distributions (rather than just predominant senses) given the genre as well as the domain of the text.,1,2005
D05-1053,"we plan to combine this with local context, using collocates of neighbors in the thesaurus, for contextual wsd.",1,2005
D05-1054,"so in the future work, we will be focusing more on recognizing abbreviated ons.",1,2005
D05-1056,"as future work, experiments should be expanded to include additional entity types and other types of informal text, such as blogs and forum postings.",2,2005
D05-1057,our methods also generalize well across languages since there are no language specific techniques employed.,4,2005
D05-1057,"we can improve and apply our methods to other domains like switchboard data (godfrey , 1992).",4,2005
D05-1059,"a natural extension of this work is to replace the maximum entropy modeling, which was used as the local classifiers, with other machine learning algorithms.",1,2005
D05-1059,support vector machines with appropriate kernels is a good candidate because they have good generalization performance as a single classifier.,1,2005
D05-1059,"although svms do not output probabilities, the easiest-first method would be easily applied by considering the margins output by svms as the confidence of local classification.",1,2005
D05-1061,"we also would like to test our approach on more standard test sets, and compare the performance with other systems.",3,2005
D05-1061,"in the future we plan to try more flexible translation candidate selection methods, and apply them to other language pairs.",4,2005
D05-1063,"our future plans are to overcome some of the limitations in this study, specifically using more than a single (although standard and very diverse) collection and study other experimental setups, such as document retrieval, text categorization, or topic detection and tracking.",1,2005
D05-1064,"another area for future research is to investigate the use of unlabeled data within the approach, for example by making use of clusters derived from large amounts of unlabeled data (e.g., see miller (2004)).",2,2005
D05-1064,"finally, future work may apply the models to nlp tasks other than parsing.",4,2005
D05-1064,"future work may consider the use of hidden– value domains with mixed contents, such as a domain that contains 3 refinement–oriented lexical values and 3 clustering–oriented part–of–speech values.",1,2005
D05-1064,these mixed values would allow the hidden– variable model to exploit interactions between clustering and refinement at the level of words and dependencies.,1,2005
D05-1065,more data should be annotated to create a treebank of morphological analyses.,2,2005
D05-1065,guidelines should be developed for the manual annotation of data in order to make it less dependent on the annotators intuitions.,2,2005
D05-1065,"given such a treebank, the parser could be trained on labeled data or on a combination of labeled and unlabeled data, which presumably would further increase the parsing accuracy.",1,2005
D05-1067,"given that automatic humor-recognition is a rather understudied problem, we believe that this is an important result, as it provides insights into potentially productive directions for future work.",5,2005
D05-1067,we plan to address these aspects in future work.,5,2005
D05-1067,"the flattened shape of the curves toward the end of the learning process suggests that rather than focusing on gathering more data, future work should concentrate on identifying more sophisticated humor-specific features, e.g. semantic oppositions, ambiguity, and others.",1,2005
D05-1069,"secondly, we could try this approach on other language pairs, japanese-english, for example.",2,2005
D05-1069,there are a number of future directions that we could investigate.,6,2005
D05-1069,"in other words, when a chinese translation of an english sense is still ambiguous, we could try to collect sense examples using translation in a third language, japanese, for instance.",1,2005
D05-1069,"thirdly, it would be interesting to try to tackle the problem of chinese wsd using sense examples built using english, the reverse process to the one described in this paper.",1,2005
D05-1069,this is also a possible solution to the problem that ambiguity may be preserved between chinese and english.,1,2005
D05-1069,"firstly, instead of using a bilingual dictionary to translate chinese text snippets back to english, we could use machine translation software.",1,2005
D05-1071,"while our experiments have used the web corpus, our approach transfers readily to other large corpora; experimentation with other corpora is another topic for future work.",2,2005
D05-1074,"further investigation is needed to look at data not clicked, which is a critical step to see whether the improvement on prediction accuracy of user preference will help the system serve the user better in a real system.",3,2005
D05-1075,We plan to investigate these problems in the future.,6,2005
D05-1078,work to extend this technique to propbank annotation is underway.,4,2005
D05-1078,"since function labels describe dependence relations between the predicative head and its complements, whether they be arguments or adjuncts, this paper suggests that a left-corner parser and its probabilistic model, which are defined entirely on configurational criteria, can be used to produce a dependency output.",1,2005
D05-1078,consequences of this observation will be explored in future work.,6,2005
D05-1079,"thus, although test suites establish a controlled way of assessing textual entailment detection systems, the importance of being able to predict textual entailment in nlp might be better justified using task-based evaluation.",3,2005
D05-1079,this can be achieved by incorporating them in qa or summarization systems.,1,2005
D05-1080,"in the future, we plan to enhance the system to also deal with verbs, adjectives, and adverbs, as well as compound nouns.",1,2005
D05-1084,another interesting research direction that our study suggests is the combination of syntactic and semantic models in co-training.,1,2005
D05-1084,the next step would then be to construct a combined meta-model that describes the behavior of systems with both syntactic and semantic features.,1,2005
D05-1084,"co-training can be sensibly applied only when conditional independence holds for the two target functions and the distribution (blum and mitchell, 1998), i.e. when it uses two independent views on the instance set.",1,2005
D05-1086,"preliminary results suggest that sentence retrieval can be used to improve document retrieval, but we plan a more extensive investigation of evaluating document similarity and relevance based on sentence-level similarity.",3,2005
D05-1086,"we used this model because it had been shown effective in document retrieval, and was easily incorporated in the query-likelihood framework, but we intend to explore more sophisticated translation models, and better alignment mechanisms.",1,2005
D05-1087,"in the absence of methods that work well for a wide range of operating points, we need training procedures that can be made sensitive to rare cases depending on the particular demands of the application.",1,2005
D05-1088,"the accuracy ratio for this latter task is already fairly acceptable (86.26%), but it still needs to be enhanced in order to guarantee an optimal detection of subordinating intensional contexts (recall examples 1-2).",5,2005
D05-1088,both lines of work will involve the exploration and use of word sense disambiguation techniques.,1,2005
D05-1088,"further work on evita will be focused on two main areas: (1) improving the sense disambiguation of candidates to event nominals by experimenting with additional learning techniques, and",1,2005
D05-1088,(2) improving event classification.,1,2005
D05-1090,"beyond new-information detection, the idea of tracking context with a surface means like the focus variable is worth exploring in other tasks, including summarization and question-answering.",4,2005
D05-1090,"in addition, the work here suggests three directions for future work: adapt the features used here to some of the newer probabilistic formalisms, like conditional random fields.",4,2005
D05-1090,"high precision is very difficult to obtain, and every point in precision costs too much in recall.",5,2005
D05-1090,"further exploration is needed to determine whether linguistic knowledge will help, and whether state-of-the-art tools are powerful enough to improve performance.",5,2005
D05-1090,try to identify all nominal references to canonical forms.,1,2005
D05-1090,try full segmentation of the input documents rather than treat the sentences as a sequence.,1,2005
D05-1091,"recent research (roth and yih, 2004) indicates that integrating entity recognition with relation extraction in a global model that captures the mutual influences between the two tasks can lead to significant improvements in accuracy.",1,2005
D05-1091,a natural extension is to automatically extract both the entities and their relationships.,1,2005
D05-1093,"we plan to optimize the blanc parameters for different criteria in addition to incorporating syntactic and semantic features (e.g. ngrams, word classes, part of speech).",1,2005
D05-1093,"in the future, we plan to investigate the stability and performance of blanc and also apply it to automatic summarization evaluation.",4,2005
D05-1094,"our results suggest that off-the-shelf nlp tools will need not only to provide a single-best prediction, but also to be engineered so that they can easily communicate distributions over predictions to models for higher-level tasks.",1,2005
D05-1095,this is the line of research that we intend to pursue in the near future.,6,2005
D05-1095,"in other words, it’s not how many bi-phrases you have, it’s how good they are.",1,2005
D05-1099,"a third possibility is to optimize the definition of the shallow-parse phrase types themselves, for use in other applications.",4,2005
D05-1099,future work will also include continued exploration of possible features that can be of use for either shallow parsing models or context-free parsing models.,1,2005
D05-1099,"in addition, we intend to investigate ways in which to encode approximations to context-free parser derived features that can be used within finite state models, thus perhaps preserving finite-state efficiency while capturing at least some of the accuracy gain that was observed in this paper.",1,2005
D05-1099,we intend to explore including features from the context-free parser output in our perceptron model to improve shallow parsing accuracy.,1,2005
D05-1099,another possibility is to look at improving context-free parsing accuracy.,1,2005
D05-1100,"some preliminary investigation of this suggests that we see much higher gains when using generic features than these more specific ones, but we leave a thorough investigation of this to future work.",5,2005
D05-1100,we are curious to know the extent to which a close analysis of the dependency errors made by the baseline parser can be corrected by the development of features tailored to addressing these problems.,5,2005
D05-1100,"finally, we would like to use the spanish parser in an application such as machine translation.",1,2005
D05-1100,"another avenue for future investigation is to try using a more sophisticated baseline model such as collins’ model 2, which incorporates both subcategorization and complement/adjunct information.",1,2005
D05-1101,further experimental evaluation needs to be carried out on multilingual corpora in order to asses the practical impact of these findings.,3,2005
D05-1103,extending our experiments to the question types that we have not yet assessed is an important next step.,3,2005
D05-1103,"finally, we need to assess questions generated on word lists with different characteristics.",3,2005
D05-1103,"in addition, we want to assess questions individually, evaluating their use of distractors.",3,2005
D05-1103,"another is using other resources such as text collections to enable us to generate more questions per word, especially for the cloze questions.",5,2005
D05-1103,"in addition, we are looking at ways to predict word knowledge using confidence ratings and morphological and semantic cohorts in situations where we cannot perform a standard assessment or cannot test all the vocabulary words we would like to.",5,2005
D05-1103,one is the creation of new question types to test other aspects of word knowledge.,5,2005
D05-1103,there are also a number of ongoing extensions to this project.,6,2005
D05-1104,"we leave it to future work to explain why adaptation is much stronger in co-ordination: is co-ordination special because of extra constrains (i.e., some kind of expected contrast/comparison between co-ordinate sisters) or because of fewer constraints (i.e., both co-ordinate sisters have a similar grammatical role in the sentence)?",5,2005
D05-1105,in future work we intend to explore better-motivated evidence combination algorithms and to apply the approach to other nlp problems.,1,2005
D05-1106,"finally, since we only require shallow syntactic analysis (in terms of np chunking), our approach might be well suited to be easily portable to other domains.",1,2005
D05-1107,one is to explore ways of applying the proposed approach to other learning models.,4,2005
D05-1107,"finally, we will investigate whether the proposed approach can be adapted to more complex tasks in which the output is not a class label but a structure (e.g.parsing).",5,2005
D05-1107,another is to compare against other methods of combining evidences from multiple learners.,1,2005
D05-1108,"an important direction for future work lies in the assessment of more shallow syntactic information (i.e., chunks) which can be obtained more easily for new languages, and generally in the integration of more linguistic knowledge to guide projection.",3,2005
D05-1108,"finally, we will incorporate into our projection approach automatic semantic role annotations for the source language and investigate the potential of the projected annotations for training semantic parsers for the target language.",1,2005
D05-1109,post-processing can be used to reduce the training time and improve recognition accuracy by aiding generation of more training data once basic recognition capability is in place.,2,2005
D05-1113,"in future, we will evaluate the effectiveness of the techniques developed in this paper for applications like machine translation.",3,2005
D05-1113,we will also extend our approach to other types of mwes and to the mwes of other languages (work on hindi is in progress).,4,2005
D05-1114,"in the future, we may extend our work by using more datasets to empirically evaluate this feature clustering algorithm.",2,2005
D05-1114,"this semi-supervised feature clustering framework is quite general, which can be applied to other nlp tasks, for example, text categorization.",4,2005
D05-1117,a much shorter experimental cycle will allow researchers to explore different techniques and receive immediate feedback on their effectiveness.,1,2005
D05-1123,"it would be an interesting challenge to apply the cp framework further for other tasks, possibly with more practical flavor, such as comparing and detecting commonalities between commercial products and firms, identifying equivalencies and precedents in legal cases and so on.",4,2005
D05-1124,"a broader area of investigation are other problems in language processing that can benefit from structured multilabel classification, e.g., ambiguities in language often result in multiple acceptable parses for sentences.",5,2005
D05-1124,"in further work, the classification threshold should also be learned to achieve the desired balance between precision and recall.",6,2005
D05-1124,it would also be useful to investigate methods for combining these models with standard sequential tagging models to get top performance on simple segmentations as well as on overlapping or non-contiguous ones.,1,2005
D05-1124,it may be possible to extend the algorithms presented here to learn to distinguish all acceptable parses from unacceptable ones instead of just finding a single parse when many are valid.,1,2005
D05-1125,"we plan next to further improve our system by evaluating a number of novel pattern classification techniques to increase accuracy and user-independence, and to introduce additional vocal characteristics (possibilities include vibrato, degree of nasality, rate of change of any of the above as an independent parameter) to increase the available simultaneous degrees of freedom controllable via the voice.",3,2005
D05-1125,"moreover, we plan to develop algorithms to decouple unintended user correlations of these parameters, and to further advance both our adaptation and acceleration algorithms.",1,2005
D05-1126,"the methodology proposed here is not so dependent on the domains, thus applicable to many other tasks of this category.",4,2005
D05-1127,even with the lack of convergence our approach could be applied to more complicated domains in order to learn an effective dialog policy.,4,2005
D05-1127,our approach would be especially useful in situations where there are no existing corpora of human-human interactions for the domain or as a way to provide a check against a policy based on human intuition.,4,2005
D06-1601,"more work needs to be done to resolve particular classes of errors; for example, the one reported above for the verb work.",6,2006
D06-1602,"finally, we would like to investigate the extent to which existing nlp systems (such as open-domain qa systems) can benefit from a detailed analysis of superlatives.",3,2006
D06-1602,"also, only one of the possible types of superlative was considered, namely the attributive case.",5,2006
D06-1602,"another aspect which we have neglected in this study but want to consider in future work is the interaction between superlatives and focus (heim, 1999; gawron, 1995).",5,2006
D06-1602,"one obvious improvement is to amend ccgbank in order to avoid the need for postprocessing rules, thereby also allowing the creation of more accurate language models.",1,2006
D06-1602,"in future work we will consider the interpretation of predicative and adverbial superlatives, as well as comparative expressions.",1,2006
D06-1603,unpaired tuples that are semantically redundant should also be regarded as insignificant.,5,2006
D06-1603,another direction is to detect semantic redundancy in a sentence.,1,2006
D06-1603,"while we continue to explore more suitable representation of unpaired predicate argument tuples, we plan to augment the similarity measure for phrasal units to reduce the error rate in the first component.",1,2006
D06-1604,"in future work, we hope to further validate this paradigm by constructing additional semantic filters that detect other types of errors.",3,2006
D06-1604,such a corpus could be used to re-train a statistical parser to improve its performance.,5,2006
D06-1604,"if semantic filters become sufficiently accurate, they could rule out enough erroneous parses that the parser is left with just the correct one.",1,2006
D06-1604,"beyond that, we plan to embed semantic filtering into the parser itself.",1,2006
D06-1604,we also plan to use semantic filters such as woodward to build a large-scale corpus of automatically-parsed sentences that has higher accuracy than can be achieved today.,1,2006
D06-1605,part of our future work will be to try an intermediate degree of coarseness (still much coarser than wordnet) by using the paragraph subdivisions of the thesaurus instead of its categories to see if this gives even better results.,1,2006
D06-1606,this correlation suggests that we can continue to use the bleu metric to further improve our models and systems.,1,2006
D06-1608,we challenge others who are conducting research on syntactically-informed smt to verify whether or to what extent their systems are sensitive to parse quality.,5,2006
D06-1609,"as further research, we would like to add extra features to the smr system, and study new types of classes for the reordering task.",1,2006
D06-1610,"from examining the paraphrase extraction process, it is unclear how to relate translation probabilities and confidences with semantic closeness.",5,2006
D06-1610,we plan to explore the parallels between the two to enable a weighted implementation of paraeval.,5,2006
D06-1611,"in the future, we plan to test the generality of our paradise model on other corpora and to compare models built using our interaction parameters against models based on parameters commonly used in previous work (moller, 2005a).",3,2006
D06-1611,"we also want to see if our results hold for performance metrics based on user satisfaction questionnaires; in the new itspoke corpus we are currently annotating, each student also completed a user satisfaction survey (forbes-riley and litman, 2006) similar to the one used in the darpa communicator multi-site evaluation (walker , 2002).",3,2006
D06-1611,testing if our results generalize to a human annotation of the discourse structure and automated models of certainty and correctness is also of importance.,5,2006
D06-1612,"for example, an entity that is linked in wordnet (within a given depth) and/or framenet to a previously introduced one is more likely to be mediated than new.",5,2006
D06-1612,"additionally, we will attempt to exploit dialogue turns, since knowing which speaker said what is clearly very valuable information.",1,2006
D06-1612,"in particular, we plan to use additional lexical and relational features derived from knowledge sources such as wordnet (fellbaum, 1998) and framenet (baker et al., 1998) which should be especially helpful in distinguishing mediated from new entities, the most difficult decision to make.",1,2006
D06-1612,we also plan to run experiments on the automatic classification of old and mediated subtypes (the finer-grained classification) that is included in the corpus but that we did not consider for the present study (see section 2.1).,1,2006
D06-1612,"in a similar vein, we will experiment with distance measures, in terms of turns, sentences, or even time, for determining when an introduced entity might stop to be available.",1,2006
D06-1613,"we are currently performing an experiment to see if citation processing can increase performance in a large-scale, real-world information retrieval task, by creating a test collection of researchers queries and relevant documents for these (ritchie et al., 2006a).",1,2006
D06-1614,"with this additional information, we expect considerable improvement in grammatical function assignment for the functions subject, accusative object, and dative object, which are marked by nominative, accusative, and dative case, respectively.",1,2006
D06-1614,additional experiments with the tuba-d/z treebank are planned in future work.,1,2006
D06-1615,one of our next goals is to apply scl directly to parsing.,4,2006
D06-1615,"we are also focusing on other potential applications, including chunking (sha and pereira, 2003), named entity recognition (florian, 2004; ando and zhang, 2005b; daume iii and marcu, 2006), and speaker adaptation (kuhn, 1998).",5,2006
D06-1615,"finally, we are investigating more direct ways of applying structural correspondence learning when we have labeled data from both source and target domains.",1,2006
D06-1618,"finally, our training curves suggest that  future research in this area should focus primarily on identifying more discriminative features.",1,2006
D06-1619,we hope that this research provides a novel approach to deterministic parsing in which only lexical selection and little phrasal information without packed representations dominates the parsing strategy.,1,2006
D06-1621,"in the long run, we believe that the availability of such datasets will facilitate improved models that consider the various sub-cases of lexical reference, as well as applying supervised learning to optimize model combination and performance.",1,2006
D06-1622,"future work will investigate using more features,  new heuristics and/or other ml approaches to  improve the performance of instance-based  learning algorithms at the srl task.",1,2006
D06-1623,"we will build on the existing formal framework (fikes , 2003) for the verification of ordering consistency.",3,2006
D06-1623,we are also interested in expanding our framework for global inference to other temporal annotation schemes.,4,2006
D06-1623,"in the future, we will explore a richer set of constraints on the topology on the ordering graph.",1,2006
D06-1623,"we will build on the existing formal framework (fikes et al., 2003) for the verification of ordering consistency.",3,2006
D06-1623,"given a richer set of temporal relations, the benefits from global inference can be even more significant.",1,2006
D06-1624,"then, high level knowledges, such as the dialog context, can also be included as the features of topic and semantic classifiers.",2,2006
D06-1624,the future work includes further evaluation of our approach in other application domains and languages.,3,2006
D06-1624,we also plan to integrate this understanding system into a whole dialog system.,1,2006
D06-1624,"then, it is worthwhile to investigate how to appropriately define topics and the probability of exploiting the sentence clustering techniques to  facilitate the topic (frame) designment.",1,2006
D06-1625,we also like to investigate if our findings generalize to other types of corpora besides tv-show dialogs.,2,2006
D06-1625,"in the future, we plan to explore more sophisticated semantic and pragmatic features such as incongruity, ambiguity, expectation-violation etc.",1,2006
D06-1626,we are planning to integrate the distributed lm in the statistical machine translation decoder in the near future.,4,2006
D06-1626,we will investigate different relevance weighting schemes to better combine n-gram statistics from different data sources.,1,2006
D06-1628,"for example, we might consider aeps that include larger chunks of phrase structure, or we might consider aeps that contain more detailed information about the relative ordering of modifiers.",6,2006
D06-1628,future work may also consider expanded definitions of aeps.,6,2006
D06-1628,there is certainly room for improvement in the accuracy with which aeps are predicted in our data; the feature-driven approach allows a wide range of features to be tested.,6,2006
D06-1629,"future work will include collecting impression keywords automatically, and adapting the language model to the category of source words.",1,2006
D06-1630,one particular area that we will continue to work on is phonetic distance. the work we report is ongoing and is part of a larger project on multilingual named entity recognition and transliteration.,1,2006
D06-1631,we are presently exploring algorithms to normalize foreign words in arabic text.,1,2006
D06-1631,this will allow us to identify normalized forms for foreign words and use a single consistent version for indexing and retrieval.,1,2006
D06-1632,"in the future, we also want to experiment with a larger data set for determining whether discourse cues really do not correlate with paragraph boundaries.",2,2006
D06-1632,"then, we will move on towards multi-document summarization, the application which motivates the research described here.",4,2006
D06-1635,"for longer sequences, we will take advantage of the fact that cky is easily parallelizable, since any operation which combines the entries of two cells chart and chart is completely independent of other parts of the chart.",1,2006
D06-1635,"we also plan to adapt this technique to other, more realistic, representations of proteins, and to longer sequences.",1,2006
D06-1638,"when modeling the lexical rule p(x[]  w), we could use features that consider the spelling of the word w in conjunction with the value of .",1,2006
D06-1639,"while such functionality is well beyond the scope of our current study, we are optimistic that we can develop methods to exploit additional types of relationships in future work.",1,2006
D06-1640,we plan to investigate algorithms that can directly optimize for complex measures (such as b3)for the problem of partially supervised clustering.,1,2006
D06-1641,"our model can be easily extended to opinion retrieval, if the opinion retrieval is defined as retrieving sentences or documents that contain either positive or negative sentiments.",4,2006
D06-1641,this issue is worth pursuing in future work.,6,2006
D06-1641,"approaches considering polarity strength or continuous values for the polarity specification, rather than using {-1,1} can also be considered in future work.",1,2006
D06-1644,"furthermore, we hope to improve the convergence properties of the dy- namic adaptation scheme at the start of lectures and across topic transitions.",5,2006
D06-1644,"lastly, we would like to extend the lda framework to support speaker-specific adaptation and apply the resulting topic distributions to lecture segmentation.",1,2006
D06-1645,sentence selection can then be improved by considering the composite entropy for all the models.,1,2006
D06-1645,we are exploring the use of ranking to reorder the data such that the sequential selection process gives better results.,1,2006
D06-1645,another idea we are currently investigating is to use multiple instances of the selection algorithm with different initial language models pinit generated by bagging.,1,2006
D06-1647,future work includes a more detailed analysis of transductive learning in this domain and possible solutions to alleviating error propagation.,2,2006
D06-1648,"also, determining the size of a sufficiently large corpus to generate a correction dictionary and to train a language model is desirable.",2,2006
D06-1648,"finally, word prediction might prove useful for cases where ocr grossly misrecognized words.",5,2006
D06-1648,"for future work, a factor language model  might prove beneficial to incorporate morphological information and other factors such as part  of speech tags while overcoming training data  sparseness problems.",1,2006
D06-1650,"it is our hope through this work to shed some light onto what people find helpful in usersupplied reviews and, by automatically ranking them, to ultimately enhance user experience.",4,2006
D06-1653,this implies that a large amount of work in the field of ir could be imported into cf.,4,2006
D06-1653,this would be interesting to investigate in future work.,6,2006
D06-1654,"an interesting direction would be the development of algorithms that allowed the incremental application of weights, perhaps by re-weighting vectors when a new context is learned.",1,2006
D06-1657,"furthermore, more sophisticated character sequence kernels can be evaluated, such as mismatch string kernels used in bioinformatics, where mutations in the sequences are allowed (leslie , 2004).",3,2006
D06-1657,"in future work it would be useful to appraise composite kernels (joachims , 2001) in order to combine character and word sequence kernels.",3,2006
D06-1657,"if the two kernel types use (partly) complementary information, better performance could be achieved.",1,2006
D06-1659,"in our future work, we plan to focus on generalizing the approach for targeting more nlp problems.",1,2006
D06-1660,"in the future, wed like to focus on further exploring more effective methods to adapt ner model to a new domain with much less efforts, time and performance degrading.",1,2006
D06-1662,"in future, we may explore the role of term-level or word-level features, e.g., proper nouns, in the ordering of summary sentences.",4,2006
D06-1662,one specific task is how to generate coreference among sentences in summaries.,5,2006
D06-1662,"to make summaries more coherent and  readable, we may also need to discover how to  detect and control topic movement automatic summaries.",1,2006
D06-1662,"in  addition, we will also try other semi-supervised  classification methods, and other evaluation  metrics, etc.",1,2006
D06-1665,"these relations can be applied in many other tasks, such as machine translation, word sense disambiguation / discrimination, and so on.",4,2006
D06-1665,these are some interesting research work in the future.,6,2006
D06-1667,"moreover, instead of cosine similarity measure to calculate the distance between context vectors, we will try other distributional similarity measures to see whether the performance of relation extraction can be improved.",1,2006
D06-1667,in the future we will further explore other semantic information to assist the relation extraction problem.,1,2006
D06-1667,"in addition, if we can find an effective unsupervised way to filter out unrelated entity pairs in advance, it would make our proposed method more practical.",1,2006
D06-1669,"we would also like to integrate different kinds of information, specially the local or syntactic features so successfully used by supervised systems, but also more heterogeneous information from knowledge bases.",1,2006
D06-1669,"for the future, we would like to look more closely the micro-senses induced by hyperlex, and see if we can group them into coarser clusters.",1,2006
D06-1671,"one shortcoming of this approach is that fields are not allowed to belong to multiple records, because the partitioning algorithm returns nonoverlapping clusters.",5,2006
D06-1671,another avenue of future research is to consider syntactic information in the compatibility function.,1,2006
D06-1671,exploring overlapping clustering techniques is an area of future work.,1,2006
D06-1672,we plan to further investigate the relationship between the local and global approaches to complex learning problems in natural  language.,1,2006
D06-1673,we are proposing a general method to deal with all multi-stage algorithms.,1,2006
D07-1001,"in general, we will assess the impact of discourse information more systematically by incorporating it into generative and discriminative modelling paradigms.",3,2007
D07-1001,"in the future, we will interface our compression model with sentence extraction.",1,2007
D07-1001,we also plan to study the effect of global discourse structure (daume iii and marcu 2002) on the compression task.,3,2007
D07-1002,"important future directions lie in evaluating the contribution of alternative semantic role frameworks (e.g., propbank) to the answer extraction task and developing models that learn semantic roles directly from unannotated text without the support of framenet annotations (grenager and manning, 2006).",3,2007
D07-1002,"beyond question answering, we also plan to investigate the potential of our model for shallow semantic parsing since our experience so far has shown that it achieves good recall.",4,2007
D07-1004,"moreover, applying the u-svm to qa systems in other languages, like english and japanese, will also be included in our future work.",2,2007
D07-1005,"in future work, we will consider several extensions to this framework that lead to more powerful system combination strategies using multiple bridge languages.",1,2007
D07-1007,"it would also be interesting to assess whether a more grammatically structured statistical mt model that is less reliant on an n-gram language model, such as the syntactic itg based grammatical channel translation model of (wu and wong, 1998), could make more effective use of wsd predictions.",3,2007
D07-1008,"secondly, the present paper used a relatively simple feature set.",2,2007
D07-1008,future research will follow three directions.,6,2007
D07-1008,"first, we will extend the framework to incorporate position dependent loss functions.",1,2007
D07-1008,examples include the hamming distance or more sophisticated functions that take the tree structure of the source and target sentences into account.,1,2007
D07-1008,such functions can be supported by augmenting our generation algorithm with a beam search.,1,2007
D07-1008,our intention was to examine our models performance without extensive feature engineering.,1,2007
D07-1008,"nevertheless, improvements should be possible by incorporating features defined over n-grams and dependencies (mcdonald, 2006).",1,2007
D07-1009,we are also interested in further developing our system for automatic update of wikipedia pages.,1,2007
D07-1010,"finally, jointly modeling propbank and the pdtb is another interesting area we plan toinvestigate, something to which the head-basedapproach and dependency parse representation we advocate here would be well-suited.",1,2007
D07-1011,"another possible limitation of iapart is that, despite strong evidence for overspecification, complex domains could yield very lengthy outputs.",5,2007
D07-1011,"strategies to avoid them include the utilisation of other boolean operators like negation (the desks which are not red) (horacek, 2004).",5,2007
D07-1011,these issues are open to future empirical research.,1,2007
D07-1012,"our plan is to investigate why all methods perform poorly on missing word errors, to extend the error creation procedure so that it includes a wider range of errors, to try the deep approach with other parsers, to integrate additional features from stateof-the-art shallow techniques and to repeat the experiments for languages other than english.",5,2007
D07-1016,future work will also involve determining the english inclusion classifiers merit when applied to rule-based parsing.,4,2007
D07-1016,this provides an upper bound on the performance we can expect from a parser that uses inclusion detection.,6,2007
D07-1016,"finally, our results indicate that future work could improve parsing performance for inclusions further: we found that parsing the inclusion set is still harder than parsing a randomly sampled test set, even for our best-performing model.",1,2007
D07-1017,it is our hope that methods such as the one proposed in this paper may one day be used to harness the richness of automatically created inference rule resources within large-scale nlp applications.,4,2007
D07-1018,"third, the application of the acquired information to broader nlp tasks.",4,2007
D07-1018,future work involves three main lines of research.,6,2007
D07-1018,"first, the refinement of the classification itself, based on the results of the experiments presented.",1,2007
D07-1018,"second, the use of additional linguistic evidence that contributes to the semantic class distinctions (e.g., selectional restrictions).",1,2007
D07-1019,"for example, we can work on page ranking information of returning pages, because trusted or well-known sites with high page rank generally contain few wrong spellings.",4,2007
D07-1019,there is still further potential useful information that should be studied in this direction.,4,2007
D07-1019,"in addition, the term cooccurrence statistics on the returned snippet text  are also worth deep investigation.",1,2007
D07-1025,"we believe this versatility will lead to other successful applications of the idea, both within computational linguistics and in other fields involving sequential learning.",4,2007
D07-1026,"finally, we are going to explore more elaborated kernel functions to recognize lexical entailment and more efficient learning strategies to apply our method to web-size corpora.",4,2007
D07-1026,"for the future, we plan to apply our instance based approach to a wide variety of tasks, e.g., lexical substitution, word sense disambiguation and information retrieval.",4,2007
D07-1026,"in addition, we plan to exploit our lexical entailment as a subcomponent of a more complex system to recognize textual entailment.",1,2007
D07-1027,"we also plan to adapt other nld recovery methods (jijkoun and rijke, 2004; schmid, 2006) to chinese and compare them with the current results.",1,2007
D07-1027,"we will investigate ways of closing the gap between the performance of gold-standard and parser output trees, including improving parsing result for chinese.",1,2007
D07-1027,"in future work, we will refine and extend the conditioning features in our models to discriminate subcat frames and explore the possibilities to use the chinese propbank and hownet to supplement our automatically acquired subcat frames.",1,2007
D07-1028,in the future we will experiment with increasing conditioning context further and using more sophisticated smoothing techniques to avoid sparse data problems when conditioning is increased.,1,2007
D07-1030,"for example, we will investigate  to train chinese-to-english smt system based on  natural english and rbmt-generated synthetic  chinese.",1,2007
D07-1030,"in the future work, we will investigate the possibility of training a reverse smt system with the  rbmt systems.",1,2007
D07-1032,"our future work involves incorporating ellipsis resolution to develop an integrated model for syntactic, case, and ellipsis analysis.",1,2007
D07-1033,we would like to investigate various types of new nonlocal features using the proposed algorithm in future work.,1,2007
D07-1035,"we hope to explore our future work in several areas, such as further consolidating the new ground-truth from different points of view and verifying the effectiveness of low-quality review detection with other applications.",4,2007
D07-1036,introducing language model optimization  into our system might further improve translation  performance.,1,2007
D07-1036,it might work better by trying other sophisticated similarity measure models or using some  optimization algorithms to determine submodels  weights.,1,2007
D07-1037,we intend to extend the hybrid indexing approach by considering more vocabulary subsets.,2,2007
D07-1037,"syntactic similarity is more appropriate for verbs, for example, than co-occurrence.",1,2007
D07-1037,"as a next step, we intend to embed verbs using syntactic similarity.",1,2007
D07-1037,it would also be interesting to use lexical chains for proper names and learn the weights for different similarity scores.,1,2007
D07-1039,"we hope to use these models in tasks such as diathesis alternation detection (mccarthy, 2000; tsang and stevenson, 2004) and contrast with wordnet models previously used for this purpose.",4,2007
D07-1040,"given the increasingly large number of books available in electronic format, and correspondingly the growing need for tools for book summarization, we believe that the topic of automatic book summarization will become increasingly important.",5,2007
D07-1040,we hope that this paper will encourage and facilitate the development of an active line of research concerned with book summarization.,5,2007
D07-1042,how much noise this approximation introduces when finer role sets are used is an open research question.,5,2007
D07-1042,"excluding the direct use of role-annotated corpora like framenet for coverage reasons, the most promising strategy is to extend our present scheme of approximating semantic relations by grammatical realizations.",1,2007
D07-1045,"in particular, the problem of generalization to new translations seems to be promising to us.",5,2007
D07-1045,"despite these encouraging results, we believe that additional research on improved estimation of probabilities in n-gram- or phrase-based statistical machine translation systems is needed.",1,2007
D07-1046,"finally, we are puzzled by the differences between hebrew and arabic (for which the baseline and the current state of the art are significantly higher) on this task.",5,2007
D07-1046,we intend to investigate the linguistic sources for this puzzle in the future.,5,2007
D07-1046,"we also believe that further linguistic exploration, based on deeper error analysis, will result in more hard constraints which can reduce the error rate of the combination module.",1,2007
D07-1049,we are currently implementing kneser-ney smoothing within the proposed framework.,5,2007
D07-1049,"we hope the present work will, together with talbot and osborne (2007), establish the bloom filter as a practical alternative to conventional associative data structures used in computational linguistics.",1,2007
D07-1051,further research is needed to investigate whether the problem class (classification with a fixed and moderate number of classes vs. ranking large numbers of possible candidates) is responsible for limited data reusability.,5,2007
D07-1051,in future work we will directly compare qbc and uncertainty sampling with respect to data reusability.,1,2007
D07-1053,this is one of the main aims of the recently started esac_imc project.,6,2007
D07-1053,for  this reason we will perform an extended qualitative analysis of the presented methods with persons  who use our aac system sibylle.,1,2007
D07-1055,whether this pays off in terms of translation quality is left open for future work.,5,2007
D07-1055,we think that the n-gram level computation has certain advantages: the n-gram posterior probabilities could be computed from a word graph which would result in more reliable estimates.,1,2007
D07-1056,"in the future work, we plan to improve the reordering model by introducing n-best syntax trees  and exploiting richer syntactic knowledge.",1,2007
D07-1057,"in future, we plan to apply our model directly on machine-generated parse trees.",4,2007
D07-1057,we also plan to classify non-coreferential zero pronouns into the six categories.,1,2007
D07-1058,"the exact relation of p-dop to other dop models, including s-dop (bod, 2003), backoff-dop (simaan and buratto, 2003), dop* (zollmann and simaan, 2005) and ml-dop (bod, 2006; based on expectation maximization) and not dissimilar automatic enrichment models such as (petrov et al., 2006), remains a topic for future work.",1,2007
D07-1060,"cross-lingual dpcs also have tremendous potential in tasks inherently involving more than one language, such as machine translation and multilanguage multi-document summarization.",4,2007
D07-1060,our future work will explore other tasks such as information retrieval and text categorization.,5,2007
D07-1061,"in future work, we hope to integrate other lexical resources such as wikipedia into the walk.",2,2007
D07-1061,incorporating more types of links from more resources will underline the importance of determining appropriate relative weights for all of the types of edges in the walks matrix.,2,2007
D07-1061,"even for wordnet, we believe that certain link types, such as antonyms, may be more or less appropriate for certain tasks and should weighted accordingly.",4,2007
D07-1063,"for example, (zero-) anaphora resolution is considered as a good candidate task for application.",4,2007
D07-1063,we are also planning to apply the proposed method to other tasks which need to construct tree structures.,4,2007
D07-1063,the features used by sassano (2004) are promising as well.,1,2007
D07-1063,"by extracting new features that are more suitable for the ancestor-descendant relation, we can further improve our method.",1,2007
D07-1064,the similarity of two words derived from an external knowledge base can be assigned to a substitute node at a corresponding location in the state space in a straightforward manner.,1,2007
D07-1064,we are also planning to reimplement our algorithms using crfs instead of the averaged perceptron algorithm.,1,2007
D07-1064,This is a topic we are currently working on.,6,2007
D07-1065,"in addition, we will investigate methods for automatically inferring patterns from a treebank corpus to support fast porting of our approach to other languages with treebanks.",1,2007
D07-1065,"in future work, we will investigate methods for adding lexical information to our model in order to improve the performance on whadvps and whpps.",1,2007
D07-1067,it would be interesting to compare this method to latent semantic analysis approaches for text segmentation as studied for example in bestgen (2006) and the references thereof.,3,2007
D07-1067,"by defining appropriate features, we can use our method immediately for text and discourse segmentation.",4,2007
D07-1067,"as future work, we plan on implementing 3 features in order to perform an accuracy/time analysis.",1,2007
D07-1068,"for nlp applications such as qa, ne dictionary with finegrained label sets will be a useful resource.",2,2007
D07-1068,"in future research, we plan to explore ne categorization with more fine-grained label set.",2,2007
D07-1068,"however, generally, classification with statistical methods becomes difficult in case that the label set is large, because of the insufficient positive examples.",5,2007
D07-1068,it is an issue to be resolved in the future.,6,2007
D07-1069,we are currently working on applying our methods to the us house of representatives and other records of parliamentary speech from the united kingdom and australia.,4,2007
D07-1069,"we are interested in dynamic mavenrank to go further with the idea of tracking how ideas get propagated through a network of debates, including congressional records, blogs, and newsgroups.",5,2007
D07-1073,exploiting wikipedia structures such as disambiguation pages and link structures will be the key in that case as well.,1,2007
D07-1073,we thus would like to incorporate a disambiguation technique into our method in future work.,1,2007
D07-1075,"first, it would be interesting to see if the use of richer features can improve classifier performance, and if that in turn improves the performance of the ie system.",5,2007
D07-1075,"finally, other techniques for learning semantically appropriate extraction patterns could be investigated.",1,2007
D07-1076,"finally, we will study how to resolve the data imbalance and sparseness issues from the learning algorithm viewpoint.",5,2007
D07-1076,our preliminary work of including the entity type information significantly improves the performance.,6,2007
D07-1076,"moreover,  we will explore more entity-related information in the  parse tree.",1,2007
D07-1076,"for the future work, we will focus on improving  the context-sensitive convolution tree kernel by exploring more useful context information.",1,2007
D07-1077,"we plan to output reordered lattices in the future, so that the approach would be more robust to errors made during parsing/reordering.",1,2007
D07-1080,future work involves scaling up to larger data and more features.,2,2007
D07-1082,"in the future work, we will study how to exactly identify these borderline samples thus they are not firstly selected in active learning procedure.",5,2007
D07-1082,the borderline instances can be detected using the  concept of tomek links (tomek 1976).,1,2007
D07-1082,"it is also  worth studying cost-sensitive learning for active  learning with imbalanced data, and using such  techniques for wsd.",1,2007
D07-1083,"in future we intend to investigate more appropriate model and feature design for unlabeled data, which may further improve the performance achieved in our experiments.",1,2007
D07-1085,"for future work, we plan to test our proposed method on english speech corpora, and with largerscale retrieval tasks involving more queries and more documents.",3,2007
D07-1085,"we would like to extend our method to other speech processing tasks, such as spoken document classification and example-based spoken document retrieval as well.",4,2007
D07-1086,the next step in this research is to directly investigate how query segmentation affects search performance.,1,2007
D07-1088,more effort will also  be put into the sentence-level analysis to reduce  error propagations.,1,2007
D07-1088,"in addition, ontology based  knowledge inference strategies might be useful to  validate attributes in single record and in turn help  data record extraction.",1,2007
D07-1088,"for the future, we plan to explore additional feature types and feature selection strategies to determine what is good for unstructured record templates to improve our results.",1,2007
D07-1088,the last thing under our  direction is to explore new models if applicable.,1,2007
D07-1091,"moreover, we expect to overcome the constraints of the currently implemented synchronous factored models by developing a more general asynchronous framework, where multiple translation steps may operate on different phrase segmentations (for instance a part-of-speech model for large scale reordering).",1,2007
D07-1091,"we are currently exploring these possibilities, for instance use of syntactic information in reordering and models with augmented input information.",1,2007
D07-1092,"first, we are investigating why our approach remains silent for some words or phrases.",5,2007
D07-1092,"second, we are investigating how a systematic enrichment of a phrase-transfer table will impact a phrase-based statistical machine translation engine.",5,2007
D07-1092,"last, we want to investigate the training of a model that can learn regularities from the analogies we are making.",1,2007
D07-1092,this will allow us to better characterize the limitations of analog and will hopefully lead us to design a better strategy for identifying the stems of a given word or phrase.,1,2007
D07-1092,this work is currently being developed in several directions.,6,2007
D07-1093,"we believe that the probabilistic framework we have introduced for diachronic phonology is promising, and scaling it up to richer phylogenetic may indeed reveal something insightful about language change.",4,2007
D07-1096,"thus, finding ways of reusing already invested development efforts by adapting the outputs of existing systems to new requirements, without substantial loss in accuracy, seems to be another line of research that may be worth pursuing.",1,2007
D07-1100,improving dependency relation labeling is left for future work.,5,2007
D07-1101,"thus, a promising line of research is the investigation of methods to efficiently incorporate higher-order relations in discriminative parsing.",5,2007
D07-1103,"for example, skip-ngrams (n-grams that allow for gaps of fixed or variable size) may be studied better using this approach leading to insight about methods that weakly approximate patterns.",4,2007
D07-1103,"the code base for taking a list of n, m-grams and computing the required frequencies for signifance evaluation can be applied to related problems.",4,2007
D07-1103,there are a number of important ways that this work can and will be continued.,6,2007
D07-1104,"this model would bring statistical machine translation closer to convergence with so-called example-based translation, following current trends (marcu, 2001; och, 2002).",4,2007
D07-1104,we intend to explore these ideas in future work.,6,2007
D07-1106,"first, the  method explored here can be extended as an alterative way to support such mt subtasks as back  transliteration (knight and graehl 1998) and noun  phrase translation (koehn and knight 2003).",1,2007
D07-1106,many opportunities exist for future research and improvement of the proposed approach.,6,2007
D07-1107,we hope these new lexical resources will be useful for nlp applications that require a coarser-grained sense hierarchy than that already found in wordnet.,5,2007
D07-1108,it would be interesting to see how the feature can help on wsd of other languages and other natural language processing tasks such as named-entity recognition.,4,2007
D07-1109,"similarly, any topic based information retrieval scheme could employ topics that include semantically relevant (but perhaps unobserved) terms.",1,2007
D07-1109,"incorporating this model in a larger syntactically aware model, which could benefit from the local context as well as the document level context, is an important component of future research.",1,2007
D07-1110,the statistical measures are currently only used in a preprocessing step to filter the non-mwes for the lexical type predictor.,6,2007
D07-1110,"alternatively, the statistical outcomes can be incorporated more tightly, i.e.to combine with the lexical type predictor and give confidence scores on the resulting lexical entries.",1,2007
D07-1110,These possibilities will be explored in future work.,6,2007
D07-1111,"as mentioned in section 5, the addition of a backward svm model did improve accuracy on the turkish set significantly, and it is likely that improvements would also be obtained in other languages.",2,2007
D07-1111,"of course, the use of different approaches used by different groups in the conll 2006 and 2007 shared tasks represents great opportunity for parser ensembles.",6,2007
D07-1111,one of the simplest improvements to our approach is simply to train more models with no other changes to our set-up.,1,2007
D07-1111,"in addition, other  learning approaches, such as memory-based language processing (daelemans and van den bosch,  2005), could be used.",1,2007
D07-1111,"for example, using mstparser  (mcdonald and pereira, 2005), a large-margin all pairs parser, in our domain adaptation procedure  results in significantly improved accuracy (83.2  las).",1,2007
D07-1111,"a  similar idea that may be more effective, but requires more effort, is to add parsers based on different approaches.",1,2007
D07-1111,"a drawback of adding more  models that became obvious in our experiments  was the increased cost of both training (for example, the svm parsers we used required significantly longer to train than the maxent parsers) and  run-time (parsing with mbl models can be several  times slower than with maxent, or even svm).",5,2007
D07-1112,"in the future, we would like to model predictive opinions in other domains such as the real estate market and the stock market which would require further exploration of system design and data collection.",4,2007
D07-1117,"in future efforts, we plan to extract additional reranking features utilizing more explicitly the characteristics of mandarin.",1,2007
D07-1117,"we also plan to extend our work to speech transcripts for broadcast news and broadcast conversation corpora, and explore semisupervised training methods for reranking.",4,2007
D07-1119,"since the technique is applicable to any parser,  we plan to test it also with more accurate english  parsers.",1,2007
D07-1121,"future work will focus on tuning the many parameters our system has, as well as on experimenting with different types of constraints to supplement or replace one or more of the three types used in this study.",1,2007
D07-1123,"while the parser is outperformed by a system based on local classifiers, we still hope that the parsing and training combination described here opens new ways in parser design and eventually leads to the improvement of parsing performance.",1,2007
D07-1126,future work will also be focused on extending our method to a version of using semi-supervised learning that can efficiently be learnt by using labeled and unlabeled data.,4,2007
D07-1126,we hope that the application of the pa algorithm to other nlp problems such as semantic parsing will be explored in future work.,4,2007
D07-1129,"future work includes use of more sophisticated features such as pos and other morphological features, possibly a joint domain adaptation of pos tagging and dependency parsing for unlabeled data as well as re-examination of pivot features.",1,2007
D07-1131,"in this way, the parse reranking  algorithm can be adopted to further improve the  performance.",1,2007
D07-1131,"also, we are investigating how to convert  the shift-reduce parser into approximate n-best  parser efficiently.",1,2007
D07-1131,"in the future, we plan to report the actual performance with replacing the mfn-svm by the  polynomial kernel svm.",1,2007
D08-1001,"finally, the framework presented in this paper opens up exciting possibilities for future work.",6,2008
D08-1001,such tasks are greatly facilitated by the explicit knowledge gained during structure recognition.,6,2008
D08-1001,"in particular, we aim at automatically transforming report dictations into properly formatted and rephrased reports that conform to the requirements of the relevant domain.",1,2008
D08-1002,"we believe that these lessons are broadly applicable, but verification of this claim is a topic for future work.",3,2008
D08-1004,"our new method, being essentially bayesian inference, is potentially extensible to many other situations—other tasks, classifier architectures, and more complex features.",4,2008
D08-1004,"in future, we are interested in new domains that can adaptively solicit rationales for some or all training examples.",5,2008
D08-1006,"therefore, in future work we plan to experiment with classifiers whose decision function is cheaper to compute, such as neural networks and decision trees.",1,2008
D08-1006,"another possible direction would be using the recently proposed deep belief network formalism (hinton et al., 2006).",1,2008
D08-1006,"in future work we plan to experiment with richer representations, e.g. including long-range n-grams (rosenfeld, 1996), class n-grams (brown et al., 1992), grammatical features (amaya and benedy, 2001), etc.",1,2008
D08-1007,"it would be interesting to expand our cooccurrence features, including co-occurrence counts across more grammatical relations and using counts from external, unparsed corpora like the world wide web.",4,2008
D08-1007,"also, like other models of sp, our technique can also be used for sense disambiguations: the weightings on our semantic class features indicate, for a particular noun, which of its senses (classes) is most compatible with each verb.",4,2008
D08-1007,"it would be interesting to expand our cooccurrence features, including co-occurrence counts across more grammatical relations and using counts from external, unparsed corpora like the world wide web. also, like other models of sp, our technique can also be used for sense disambiguation: the weightings on our semantic class features indicate, for a particular noun, which of its senses (classes) is most compatible with each verb.",4,2008
D08-1009,"in the future, we plan more extensive tests to characterize when holmes’s inference is helpful.",3,2008
D08-1009,we also hope to examine in what cases jointly performing extraction and inference (as opposed to performing them separately) is feasible at scale.,5,2008
D08-1009,"finally, we plan to examine methods for holmes to learn both rule weights and new inference rules.",1,2008
D08-1010,"moreover, we will test the performance of the mers model on large scale corpus.",3,2008
D08-1010,"in future, we will explore more sophisticated features for the mers model.",1,2008
D08-1013,"we would expand the idea in this paper into other models, such as semi-crfs and hierarchical-crfs.",4,2008
D08-1013,"for our future work, we will explore other hierarchical models for sentimental strength rating because the experiments presented in this paper prove this hierarchical frame is effective for ordinal regression.",1,2008
D08-1014,"in the future, we plan to explore additional language-specific clues, and integrate them into the subjectivity classifiers.",1,2008
D08-1015,we plan to combine these two methods together.,1,2008
D08-1015,we would also like to integrate emotion ranking into information retrieval.,1,2008
D08-1015,"an observation is that plm exploits pairwise order information, whereas edr exploits emotional distribution information.",1,2008
D08-1015,another research direction is to improve edr by finding better features.,1,2008
D08-1016,"we are interested in extending these ideas to phrase-structure and lattice parsing, and in trying other higher-order features, such as those used in parse reranking (charniak and johnson, 2005; huang, 2008) and history-based parsing (nivre and mcdonald, 2008).",1,2008
D08-1018,another future work will be to apply our work to chart parsing.,4,2008
D08-1018,"it is known that binarization is also essential for an o(n3) complexity of chart parsing, where dotted rules are used to binarize the grammar implicitly from left.",4,2008
D08-1018,therefore chart parsing can use multiple binarizations.,4,2008
D08-1018,we expect that a better binarization will also help improve the efficiency of chart parsing.,4,2008
D08-1018,"as shown in charniak et al.(1998), we can binarize explicitly and use intermediate symbols to replace dotted rules in chart parsing.",4,2008
D08-1018,one future work will be relaxing the assumption and finding a better approach.,1,2008
D08-1019,to generate coherent texts we plan to move beyond sentence generation and add discourse constraints to our system.,1,2008
D08-1021,"syntactic constraints significantly improve the quality of this paraphrasing method, and their use opens the question about whether analogous constraints can be usefully applied to paraphrases generated from purely monolingual corpora.",5,2008
D08-1022,"for future work we would like to apply this approach to other types of syntax-based translation systems, namely the string-to-tree systems (galley , 2006) and tree-to-tree systems.",4,2008
D08-1023,"with further work on scaling these models to large data sets, and engineering high performance features, we believe this research has the potential to provide significant increases in translation quality.",2,2008
D08-1025,"although the noisy channel has been in use for many years in spelling correction, our model could be used more generally for grammar corrections, including insertions, deletions, and (with new noise functions) potentially changes in word order.",4,2008
D08-1025,"finally, we note that the model here could potentially find practical application in grammar correction.",4,2008
D08-1026,incorporating eye gaze with automatic word acquisition provides another potential approach to improve the robustness of human machine conversation.,1,2008
D08-1029,this suggests both that a document-level ie system operating over a large corpus text can improve its accuracy with information that it learns from the corpus; and also that integrating an ie system more closely with a source of world knowledge (e.g.a knowledge base) could improve extraction accuracy.,2,2008
D08-1030,"for future work, we intend to investigate the use of automatic feature selection methods on the same data.",1,2008
D08-1032,"in future, we have the plan to decompose the complex questions into several simple questions before measuring the similarity between the document sentence and the query sentence.",5,2008
D08-1032,"we expect that by decomposing complex questions into the sets of subquestions that they entail, systems can improve the average quality of answers returned and achieve better coverage for the question as a whole.",5,2008
D08-1034,what if we could extend the idea of hierarchical architecture to the single semantic role level?,5,2008
D08-1034,would that help the improvement of src?,5,2008
D08-1035,"in the future, we hope to explore the use of similar bayesian techniques for hierarchical segmentation, and to incorporate additional features such as prosody and speaker change information.",1,2008
D08-1036,"inspired by this, one can devise hybrid strategies that interleave blocked and pointwise sampling; these might perform better than both the blocked and pointwise samplers described here.",5,2008
D08-1037,"we plan to investigate using more constraints within this framework, such as soft constraints which can penalize unlikely local decisions while not completely eliminating the entire solution.",1,2008
D08-1039,"further experiments will be conducted, especially on large tasks such as the nist chinese-english and arabicenglish task.",2,2008
D08-1039,training on these huge databases will only be possible with an appropriate selection of promising triplets.,2,2008
D08-1039,"for the inverse model, p(e|f), an integration into the search is directly possible.",1,2008
D08-1039,future work will address an integration into the decoder since the performance of the current rescoring framework is limited by the quality of the nbest lists.,1,2008
D08-1040,by opening the systems code and giving others the opportunity of adding their own modules and changes we hope to solve remaining problems.,5,2008
D08-1040,we assume that the affect recognition mentioned above will help us to achieve this goal in near future and this is our next step.,1,2008
D08-1042,"the dependency-based word subsequence kernel could be tested on other tasks which require computing similarity between sentences or texts, like text classification, paraphrasing, summarization etc.",4,2008
D08-1042,this will generalize the kernel and make it more robust to data sparsity.,6,2008
D08-1042,we believe this kernel will help improve performance on those tasks.,6,2008
D08-1042,"in future, the dependency-based word subsequence kernel could be extended to incorporate word classes like the kernels presented in (bunescu and mooney, 2005a; zelenko et al., 2003).",1,2008
D08-1042,"it should be possible to achieve this by incorporating matches between word classes in addition to the exact word matches in the kernel computations similar to the way in which the word subsequence kernel was extended to incorporate word classes in (bunescu and mooney, 2005b).",1,2008
D08-1043,future work will focus on testing the effectiveness of the proposed method on a larger set of qa collections with broader domains.,3,2008
D08-1043,"since the proposed approach cannot handle many-to-one or one to-many word transformations, we also plan to investigate the effectiveness of phrase-based translation models in closing gaps between queries and questions for further enhancement of qa retrieval.",1,2008
D08-1045,these phenomena are not observed in newspaper articles but cannot be ignored in web texts.,5,2008
D08-1045,"in the future, we will work on these phenomena. hence in the future, we aim to use the proposed method to improve the quality of these applications.",1,2008
D08-1047,"a natural extension of this study is to handle multiple regions of changes for morphologically rich languages (e.g. german) and to handle changes at the phrase/term level (e.g., “estrogen receptor” and “receptor of oestrogen”).",1,2008
D08-1047,another direction would be to incorporate the methodologies for semisupervised machine learning to accommodate situations in which positive instances and/or unlabeled strings are insufficient.,1,2008
D08-1048,"as future work, we will evaluate new types of spaces (e.g.dimensionality reduction methods) to improve the generalization capabilities of the space models.",3,2008
D08-1048,"we will also address the data sparseness issue, by testing smoothing techniques to better model low frequency lus.",1,2008
D08-1048,"finally, we will implement the presented models in a complex architecture for semi-supervised framenets development, both for specializing the existing english framenet in specific domains, and for creating new framenets in other languages.",1,2008
D08-1050,"the utility of measures such as unknown word rate (which can be performed with unlabelled data) and unknown pos n-gram rate (which can be performed with only pos tags) is not yet sufficiently clear to rely on them as predictive measures, but it seems a fruitful avenue for future work to investigate the importance of such measures for parser domain adaptation.",5,2008
D08-1054,"as future work, we plan to compare the htm model with other existing models, to develop learning and inference methods for handling extremely large-scale data sets, and to combine the current method with a keyphrase extraction method for extracting keyphrases from web pages.",1,2008
D08-1055,"in future, we will use richer constraints and research better ways of distinguishing whether or not cases are obligatory.",1,2008
D08-1056,one interesting question for future research is whether different game architectures might be better suited to certain kinds of data.,5,2008
D08-1056,"there are two significant difficulties: keeping the game fun, and making sure the collected data is not too noisy.",5,2008
D08-1056,probably the most interesting direction for future work is trying to increase the complexity of the data collected from a game.,1,2008
D08-1057,"in future work, we intend to explore other string matches corresponding to variations due to paraphrases and synonymy.",5,2008
D08-1057,we would also like to study the effects of corpus size when learning schematic patterns.,5,2008
D08-1057,"finally, we are currently investigating the use of machine learning methods to combine the best of the salience and schemata models in order to provide a single model for use in decoding.",1,2008
D08-1058,"in future work, more additional english resources will be used to further improve the results.",2,2008
D08-1058,we will also apply the idea to supervised chinese sentiment analysis.,4,2008
D08-1059,"the idea of combining different approaches to the same problem using beam-search and a global model could be applied to other parsing tasks, such as constituent parsing, and possibly other nlp tasks.",4,2008
D08-1060,"in future, we plan to incorporate features from target-side syntactic information, and connect them with the source information explored in this paper, to model long-distance reordering for better translation quality.",1,2008
D08-1061,combine information from more data sources to answer the question of whether more data or diverse sources are more effective in increasing precision and coverage.,2,2008
D08-1061,apply similar ideas to other information extraction tasks such as relation extraction.,4,2008
D08-1061,"1.encode richer relationships between nodes, for example instance-instance associations and other types of nodes.2.",1,2008
D08-1062,"of the parameters not considered in section 4.2, we would like to further investigate the benefits of chunking entities on the resulting base relations, experimenting with different measures of collocation.",3,2008
D08-1062,"first, we would like to generalize the scope of our discovery pipeline beyond binary relations and with richer considerations of context, even across sentences.",1,2008
D08-1062,"in future work, we seek to expand upon our rd methods in three directions.",1,2008
D08-1062,"finally, we intend to induce entire concept maps from text using the discovered relations to bootstrap an re phase, where the underlying problem is not just of inferring multiple types of relations, but to have sufficient co-ordination among the discovered relations to ensure connectedness among the resulting concepts.",1,2008
D08-1062,"second, we hope to achieve greater tunability of performance, to account for additional discovery metrics besides precision.",1,2008
D08-1063,"the experiments are conducted on clearly specified partitions of the ace 2007 data set, so future comparisons against the presented work can be correctly and accurately made.",3,2008
D08-1063,"as future work, we plan to extend this work to use semisupervised and unsupervised approaches that can make use of cross-language information propagation.",1,2008
D08-1065,"we hope that our approach will provide some insight into the design of lattice-based search procedures along with the use of non-linear, global loss functions such as bleu.",4,2008
D08-1066,"firstly, we plan to explore our estimator on other language pairs in order to obtain more evidence on its behavior.",3,2008
D08-1066,"finally, it would be interesting to study properties of the penalized deleted estimation used in this paper.",5,2008
D08-1066,"apart from a new decoder, it will be worthwhile adapting the prior probability in our model to allow for consistent estimation.",1,2008
D08-1066,"secondly, as (blunsom et al., 2008) show, marginalizing out the different segmentations during decoding leads to improved performance.",1,2008
D08-1066,we plan to build our own decoder (based on itg) where different ideas can be tested including tractable ways for achieving a marginalization effect.,1,2008
D08-1068,"future directions include incorporating additional knowledge, conducting joint entity detection and coreference resolution, and combining coreference resolution with other nlp tasks.",4,2008
D08-1070,"in future work we plan to empirically evaluate ner with an approximate version of the full model m2 which, while more demanding in terms of time complexity, could lead to even more significant gains in accuracy.",3,2008
D08-1070,we also intend to comprehensively evaluate the proposed scheme for computing probabilities by experimenting with alternative normalization functions.,3,2008
D08-1071,"in addition to an analysis of the theoretical properties of the algorithm presented, the most compelling avenue for future work is to apply this framework to other task pairs.",4,2008
D08-1071,"with a little thought, one can imagine formulating compatibility functions between tasks like discourse parsing and summarization (marcu, 2000), parsing and word alignment, or summarization and information extraction.",1,2008
D08-1072,"additionally, how can prior knowledge about domain similarity be included into the combination methods?",5,2008
D08-1072,this work also raises some questions about learning on large numbers of disparate domains: can a hierarchical online clustering yield a better representation than just selecting between k shared parameters?,5,2008
D08-1073,we expect such an improved ordering classifier to be used to improve the performance of tasks such as summarization and question answering about the temporal nature of events.,4,2008
D08-1073,"further progress in improving global constraints will require new methods to more accurately identify unknown events, as well as new approaches to create implicit constraints over the ordering.",1,2008
D08-1074,in the future we will attempt to solve this problem along these lines and work toward a system that can be used in practical applications.,4,2008
D08-1075,"secondly, the system should be tested in different types of biomedical texts, like full papers or medical reports to check its robustness.",3,2008
D08-1075,further research is possible in several directions.,6,2008
D08-1075,"in the first place, other machine learning algorithms could be integrated in the system in order to optimize performance.",1,2008
D08-1075,"finally, the postprocessing algorithm could be improved by using more sophisticated sequence classification techniques (dietterich, 2002) .",1,2008
D08-1076,our future work will therefore focus on how much system combination and syntax-augmented machine translation can benefit from lattice mert and to what extent feature function weights can robustly be estimated using the suggested method.,5,2008
D08-1077,"in future, we hope to study translations from other languages into english to study the role of deletions in such cases.",2,2008
D08-1078,"however, we suspect that the conclusions would be similar for most statistical machine translation models because of their dependence on automatic alignments.",5,2008
D08-1079,"in future work, we will exploit this kind of subtopic-level information to further improve the summarization performance.",1,2008
D08-1080,future work includes testing the spreading activation and page ranking method in the context of the update summarization task and exploring methods of extracting related concepts from the full text of wikipedia articles.,1,2008
D08-1081,"we are also investigating domain adaptation techniques; for example, we hypothesize that the relatively well-resourced domain of meetings can be leveraged to improve email results, and preliminary findings are encouraging.",2,2008
D08-1081,"we are currently working on extending our system to other conversation domains such as chats, blogs and telephone speech.",4,2008
D08-1082,we are also interested in investigating ways to apply the generative model to the inverse task: generation of a nl sentence that explains a given mr structure.,4,2008
D08-1082,"in future, we would like to extend the current model to have a wider range of support of mr formalisms, such as the one with lambda-calculus support.",1,2008
D08-1083,future research includes an approach that learns the compositional inference rules from data.,1,2008
D08-1085,"methods such as these should also be useful for natural language decipherment problems such as character code conversion, phonetic decipherment, and word substitution ciphers with applications in machine translation (knight , 2006).",4,2008
D08-1085,"obtaining optimal keys according to such models will permit the automatic decipherment of shorter ciphers, but this requires more specialized search than what is provided by general integer programming solvers.",1,2008
D08-1087,"thus, for future work, we plan to investigate the use of the initial recognition hypotheses as the development set, as well as manually transcribing a subset of the test set utterances.",1,2008
D08-1093,there are many avenues for future work opened up by the work presented here.,6,2008
D08-1093,the accuracy of the predictor can be further improved by incorporating more complex syntax-based features and multiple agreement features.,1,2008
D08-1093,"moreover, rather than predicting an intrinsic metric such as the parseval fscore, the metric that the predictor learns to predict can be chosen to better fit the final metric on which an end to-end system is measured, in the style of (och, 2003).",1,2008
D08-1094,"we will explore the usability of vector space models of word meaning in nlp applications, formulated as the question of how to perform inferences on them in the context of the textual entailment task (dagan et al., 2006).",1,2008
D08-1095,we are also interested in exploring a possible relation between the path-constrained walk approach and reinforcement learning.,1,2008
D08-1096,"future work will examine how to best treat this challenge, e.g., by using an estimation of density instead of the simplistic “1 nearest neighbor” distance used here.",5,2008
D08-1096,we also want to address that the model as it currently stands is trained under the false assumption that the training input is grammatical.,5,2008
D08-1096,"in future work, we intend to investigate the influence of noise and ambiguity on the quality of the representations in order to characterize when higher order representations improve generalization and exemplar-theoretic inference.",1,2008
D08-1096,the most important future work concerns class based language models.,1,2008
D08-1098,we will also investigate other applications of our cotraining framework to tasks such as sentiment analysis in community question answering and similar social media content.,4,2008
D08-1098,we also plan to explore related variants of semi-supervised learning such as co-boosting methods to further improve classification performance.,1,2008
D08-1098,"in the future we plan to explore more sophisticated features such semantic concepts and relationships (e.g., derived from wordnet or wikipedia), and richer syntactic and linguistic information.",1,2008
D08-1099,"in future experiments, we will investigate its performance on question answering tasks in languages such as chinese and japanese.",2,2008
D08-1099,"in future experiments, we will investigate other methods of merging answer candidates, such as taking the union of answers from both systems.",2,2008
D08-1099,"we would also like to emphasize that the se approach is entirely language independent, and thus can be readily applied to answer candidates in other languages.",4,2008
D08-1099,"we expect further improvements from adding candidates that are found only by the qa system, but it is unclear how the confidence measures from the two systems can be combined effectively.",5,2008
D08-1099,"in the future, we will investigate how to utilize more answer candidates from the qa system and determine the minimal quality of those candidates required for se approach to make an improvement. in future experiments, we will investigate other methods of merging answer candidates, such as taking the union of answers from both systems.",1,2008
D08-1101,future work could focus on applying ranking statistics to techniques for mining and tracking temporal and time-changing parameters in conjunction with techniques like (agrawal and srikant 1995; pratt 2001; last et al 2001).,4,2008
D08-1101,"our approach is also suited for the analysis of large streams of real time conversations, and this is a very important area of focus as presently more and more conversational data gets generated through channels like chat, mobile telephony, voip etc.",4,2008
D08-1101,another area of possible future work is the detection and separation of multiple underlying trends in dialogs.,1,2008
D08-1102,we also want to investigate the integration of our approach to multilingual language models and move beyond cs to address other deeper linguistic phenomena.,4,2008
D08-1102,some directions for future work include: exploring the extent to which our results can be improved by including a multi-word expression recognition system.,5,2008
D08-1102,"lastly, we would like to explore similar approaches in other popular language combinations.",1,2008
D08-1103,our future goals include porting this approach to a cross-lingual framework in order to determine antonymy in a resource-poor language by combining its text with a thesaurus from a resource-rich language.,4,2008
D08-1103,"we also intend to use the approach proposed here in tasks where keyword matching is especially problematic, for example, separating paraphrases from contradictions.",4,2008
D08-1103,we will use antonym pairs to identify contrast relations between sentences to in turn improve automatic summarization.,1,2008
D08-1104,"at any rate, our idiom corpus will play an important role in the development of unsupervised or semi-supervised methods, and the experimental results obtained in this study will be a good reference point to evaluate those methods.",4,2008
D08-1107,we believe that query grammar can be further exploited to increase query understanding and that this understanding can improve the overall search experience.,1,2008
D08-1110,an interesting line of future work would be to explore if the method presented here can be adapted to different language combinations.,4,2008
D08-1110,"moreover, multilingual communities will code-switch among more than two codes and this poses fascinating research challenges as well.",5,2008
D08-1111,"moreover, we will try to relabel sighan corpus on our three labels, and do experiments on them, which will be more convenient to compare with other segmentation methods.",2,2008
D08-1111,"besides, we will carry out more experiments to search the effectiveness of our segmentation method to cir.",1,2008
D08-1111,"as future work, we would search another more encouraging method to make a segmentation decision from the ranking result.",1,2008
D08-1113,"in future work, we would like to identify a set of features, latent variables, and training methods that port well across languages and string-transduction tasks.",1,2008
D08-1114,more empirical comparative analysis between the two algorithms of this sort will appear in future work.,5,2008
D08-1114,"we see, moreover, qualitative differences in the solutions as well – e.g., ams solution for the pendant node 5 is less confident than is lps solution.",5,2008
D09-1001,"directions for future work include: better handling of antonyms, subsumption relations among expressions, quantifier scoping, more complex lambda forms, etc.; use of context and discourse to aid expression clustering and semantic parsing; more efficient learning and inference; application to larger corpora; etc.",1,2009
D09-1002,an interesting question is whether the incorporation of wordnet-based similarity would lead to similar improvements in our case.,5,2009
D09-1002,an obvious direction for future work concerns improving our scoring function.,1,2009
D09-1003,"the current model uses the context in a very straightforward way, i.e. the two words left and right of the current word, but in the future we would like to explore more advanced methods to improve the similarity estimates.",1,2009
D09-1003,"in the future we would like to improve the described automatic expansion methods, since we feel that their full potential has not yet been reached.",1,2009
D09-1003,more specifically we plan to experiment with more advanced methods to decide whether some automatically generated examples should be added to the training set.,1,2009
D09-1008,we will employ a gaussian model to unify various linguistic and contextual features.,1,2009
D09-1008,we will also improve the dependency-to dependency method with a better bi-lingual parser.,1,2009
D09-1008,"in future, we will continue this work in two directions.",6,2009
D09-1010,we are investigating if this is a valid algorithm for two general directed acyclic graphs.,1,2009
D09-1011,"our modeling approach is potentially applicable to a wide range of other tasks, including transliteration, phonology, cognate modeling, multiplesequence alignment and system combination.",4,2009
D09-1011,our work ties into a broader vision of using algorithms like belief propagation to coordinate the work of several nlp models and algorithms.,1,2009
D09-1012,"in our future work, we plan to widen the list of covered tasks and to extend our algorithm to cope with different kernel families, such as the partial tree kernel and kernels defined over pairs of trees, e.g. the ones used for textual entailment in (moschitti and zanzotto, 2007).",1,2009
D09-1012,"we also plan to move from mining fragments to mining classes of fragments, i.e.to identify prototypical fragments in the fragment space that generalize topological sub-classes of the most relevant fragments.",1,2009
D09-1013,"for further improvement, we need to analyze the cause of these false negatives more deeply, and design a more discriminative feature space.",1,2009
D09-1014,"in the future, we plan to surpass supervised accuracy by applying our method to millions of parallel record-text pairs collected automatically using matching.",4,2009
D09-1014,we also want to explore the addition of markov dependencies into our alignment model and other constraints such as monotonicity and one-to-one correspondence.,1,2009
D09-1015,"while most ner corpus designers have defenestrated embedded entities, we hope that in the future this will not continue, as large amounts of information are lost due to this design decision.",5,2009
D09-1017,the future work focuses in two directions: (1) building a relational database from the summaries and ratings and using it to enhance users’ experiences in a multimodal spoken dialogue system; and,2,2009
D09-1017,(2) applying our techniques to other domains to demonstrate generality.,4,2009
D09-1018,the implementation and comparison of the two methods under full automation is the focus of our future work.,1,2009
D09-1019,"although there are some special conditional sentences that do not use easily recognizable conditional connectives and identifying them are useful, such sentences are very rare and spending time and effort on them may not be cost-effective at the moment.",5,2009
D09-1019,"in our future work, we will further improve the classification accuracy and study related problems, e.g., identifying topics/features.",5,2009
D09-1020,"we will manually annotate a moderate number of strategically chosen words, namely frequent ones which are highly ambiguous.",2,2009
D09-1020,we plan several future directions which promise to further increase the impact of swsd on subjectivity and sentiment analysis.,5,2009
D09-1020,"in addition, we will add features to the swsd system reflecting the subjectivity of the surrounding context.",1,2009
D09-1020,"finally, there are more sophisticated strategies to explore for improving subjectivity and sentiment analysis via swsd than the simple, intuitive rules we began with in this paper.",1,2009
D09-1021,a key area for future work will be further development of the discriminative dependency model (section 4.1).,1,2009
D09-1022,"for the trigger-based model, we plan to investigate more model variants.",1,2009
D09-1022,"in future work, we plan to extend the discriminative word lexicon model in two directions: extending context to the document level and feature engineering.",1,2009
D09-1022,"it might be interesting to look at cross-lingual trigger models such as p(f|e, f&apos;) or constrained variants like p(f|e, e&apos;) with pos(e&apos;) &lt; pos(e), i.e. the second trigger coming from the left context within a sentence which has already been generated.",1,2009
D09-1023,we believe one cause for this performance gap is the generation of the lattice and plan to address this in future work by allowing the phrase table to inform lattice generation.,1,2009
D09-1025,"on entity extraction, exploring more knowledge extractors from different sources (such as the html tables and query log sources used for our features) is promising.",2,2009
D09-1025,"other feature types may potentially capture other aspects of the semantics of entities, such as wordnet and search engine click logs.",5,2009
D09-1025,There is ample directions for future work.,6,2009
D09-1026,"because labeled lda is a graphical model in the lda family, it enables a range of natural extensions for future investigation.",1,2009
D09-1027,"investigate the performance of the method onother types of documents, such as long articles, product reviews and news;",3,2009
D09-1027,investigate the feasibility of combining cooccurrence-based and wikipedia-based relatedness for clustering; 3.,1,2009
D09-1027,"the solution of using frequent word list for filtering out too common single-word keyphrases is undoubtedly simple, and we plan to make a better combination of the clustering-based method with traditional frequency-based methods for keyphrase extraction.",1,2009
D09-1028,"automated acquisition of spatial data can significantly help many nlp tasks, e.g., question answering.",4,2009
D09-1028,"we would also like to incorporate some patterns based on (egenhofer and shariff, 1998), such as ‘crosses’, ‘goes through’ or ‘runs into’, which may allow automated acquisition of complex spatial relationships.",1,2009
D09-1028,"finally, we would like to incorporate in our framework mod-ules which may allow recognition of structured data, like those developed by (schockaert et al., 2008).",1,2009
D09-1029,"anyway, the english version has proved to be more precise, while the resource for new languages would require a more accurate revision.",2,2009
D09-1029,the resource can be produced and made available with a reduced effort for every language in wikipedia.,2,2009
D09-1029,the retrieved sentences will be made available as training or annotation material.,2,2009
D09-1029,"in the next research step, we plan to carry out an extended evaluation process in order to compute inter-annotator agreement and eventually point out validation problems.",3,2009
D09-1029,"then, we want to extend the mapping and the data extraction process to all (f, l) pairs in framenet (about 10,000).",4,2009
D09-1029,"besides, we want to create an online resource where the links between (f, l) pairs and wikipages are made explicit and where users can browse the retrieved sentences.",6,2009
D09-1033,"while our initial studies in this direction were negative, careful feature engineering might lead to better results.",1,2009
D09-1033,"while this is already pretty good, a more sophisticated model might lead to further improvements.",1,2009
D09-1033,"for example, one could experiment with linguistically more informed features.",1,2009
D09-1033,"future work should look at improving the supervised classifier, which so far has an accuracy of 90%.",1,2009
D09-1037,"a second avenue is to develop better means of inference under the grammar, in order to ensure faster mixing and a means to escape from local optima.",1,2009
D09-1037,with these extensions we expect that our model of grammar induction has the potential to greatly improve translation output.,1,2009
D09-1037,"finally, we wish to develop a method for decoding under the full bayesian model, instead of the current beam search.",1,2009
D09-1037,"one avenue is to develop a more sophisticated prior over rules, e.g., one that recognizes common types of rule via the shape of the tree and ordering pattern in the target.",1,2009
D09-1038,we think that the cost of a binary rule can be better estimated by taking the rules with different source-sides into account.,1,2009
D09-1043,"finally, the perceptron model paves the way for exploring the utility of richer feature spaces in statistical realization, including the use of linguistically-motivated and non-local features, a topic which we plan to investigate in future work.",1,2009
D09-1045,we also plan to investigate more sophisticated composition models that take syntactic structure into account.,1,2009
D09-1045,"an interesting future direction would be to optimize the vector components of the probabilistic model over a suitable training corpus, in order to derive a vector model of semantics adapted specifically to the task of composition.",1,2009
D09-1046,"second, the vector space model we used was very simple; it might be worthwhile to test more sophisticated one-class classifiers (marsland, 2003; schoellkopf et al., 2000).",1,2009
D09-1046,in the future we plan to investigate features that are more informative for making graded judgments.,1,2009
D09-1048,it would be interesting to test our algorithm on other domains and other languages to conclusively establish the effectiveness of parameter projection for multilingual wsd.,3,2009
D09-1048,it would also be interesting to analyze the contribution of corpus and wordnet parameters independently.,5,2009
D09-1049,"methods for gathering enough positive instances of such mwes will be useful for testing the methods proposed here, as well as for general mwe research.",3,2009
D09-1049,another direction is a more sophisticated corpus sampling algorithm.,1,2009
D09-1049,"future research should experiment with nonverbal mwes, since our features are not specific to verbal mwe types.",1,2009
D09-1052,we plan to extend these ideas to structured problems with exponentially many labels and develop methods that efficiently model label correlations.,1,2009
D09-1053,robustness deserves more investigation and forms one area of our future work.,1,2009
D09-1053,"another family of model adaptation methods that we have not studied in this paper is transfer learning, which has been well-studied in the machine learning community (e.g., caruana, 1997; marx et al., 2008).",1,2009
D09-1054,"our future work includes: (a) to summarize threads and represent the forum threads in question-context-answer triple, which will change the organization of online forums; and",1,2009
D09-1054,"(b) to enhance qa services (e.g., yahoo!answers) by the contents extracted from online forums.",1,2009
D09-1055,we are currently extending our work to a variety of exact match features and different sources of clickthrough logs.,4,2009
D09-1061,"second, since none of the steps in our method is specifically designed for sentiment classification, we plan to apply it to other non-topic-based text classification tasks.",4,2009
D09-1061,"for instance, instead of having the user construct a relevant feature space from scratch, she can simply extend the set of informative features identified for the user-selected dimension.",1,2009
D09-1061,"first, we plan to use our user-feedback method in combination with existing methods (e.g., bekkerman et al.(2007)) for improving its performance.",1,2009
D09-1061,"in future work, we plan to explore several extensions to our proposed method.",1,2009
D09-1062,the positive results from our experiments encourage further research for lexical resource adaptation techniques.,1,2009
D09-1063,"theoretically, a much larger turney–littman lexicon can be created even though it may be computationally intensive when working with 100 billion words.we are also developing methods to leverage the information in an english thesaurus to create semantic orientation lexicons for a low-resource language through the use of a bilingual lexicon and a translation disambiguation algorithm.",2,2009
D09-1065,"and as the discriminative accuracy of crosstraining techniques improves, further insights into the relative validity of corpus representations will be attainable.",2,2009
D09-1065,"in future research, we plan to extract richer models from larger corpora.",2,2009
D09-1066,"in our own future work, we are especially interested in using higher-order windowless association measures for retrieving paradigmatic relations as well as exploring their use in various nlp applications.",4,2009
D09-1067,"finally, we plan to conduct a bigger experiment with a larger number of verbs, and conduct evaluation in the context of practical application tasks.",3,2009
D09-1067,"in addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring sps for verb classification.",1,2009
D09-1067,"the number and type (and combination) of grs for which sps can be reliably acquired, especially when the data is sparse, requires also further investigation.",1,2009
D09-1067,"in addition, we plan to investigate other potentially useful features for verb classification (e.g. named entities and preposition classes) and explore semi-automatic ml technology and active learning for guiding the classification.",1,2009
D09-1068,"we are currently investigating document-side analysis to complement the queryside work, and believe that this will further boost the retrieval accuracy; we hope to report on this in a follow-up study.",1,2009
D09-1069,we hope that our answer promotes research on phoneme-based english-to-chinese transliteration.,4,2009
D09-1070,"in particular, we will move from the use of document boundaries to a flexible notion of textual distance to estimate likelihood of morphological relatedness.",1,2009
D09-1071,"on the implementation side, it would be interesting to see how our methods scale in a distributed map-reduce architecture where network communication overhead becomes an issue.",4,2009
D09-1071,we will need new breakthroughs to unleash the full potential of unsupervised learning for nlp.,1,2009
D09-1072,how the system can be adapted to work for other languages and 2) how to automatically obtain the knowledge of functional elements.,5,2009
D09-1072,we are certainly aware that our work does not yet address two problems: 1).,6,2009
D09-1073,it is able to work with any long phrase pairs with gap of any length in-between.,1,2009
D09-1073,we would also like to use one individual tree kernel for one partition in a structured feature space.,1,2009
D09-1074,pushing the weight estimation at the alignment link level will alleviate this problem and will make the discriminative training more targeted.,1,2009
D09-1075,"we also used the most simple word-alignment model, but more complex word alignment models could be incorporated into our bilingual model.",1,2009
D09-1077,future work will explore ways to apply additional features of these systems or other sources of information to account for the remainder of the performance gap.,4,2009
D09-1078,"as our next steps, first, we need to verify that the results obtained on a moderate-sized training corpus are repeatable on much larger corpora.",3,2009
D09-1078,"second, we plan to extend this work to incorporate language model size reduction by word clustering, which has been shown by goodman and gao (2000) to produce additional gains when combined with previous methods of language model pruning.",1,2009
D09-1079,"for example, can we use small randomised representations called sketches to compactly represent side-information on the stream telling us which aspects of it we should insert into our data?",5,2009
D09-1079,how can we efficiently deal with smoothing in this setting?,5,2009
D09-1080,future investigators should evaluate the gains possible by integrating this information into the features and ideas presented here.,3,2009
D09-1081,"we are now interested in improving semantic distance measures for verb–verb, adjective– adjective, and cross-part-of-speech pairs, by exploiting specific information pertaining to these parts of speech in lexical resources in addition to purely co-occurrence information.",1,2009
D09-1082,"furthermore, since the pas is usually a bag of unconnected graphs, we could find a way to joint them together, in order to consider both inter- and intra- sentential inferences based on it.",2,2009
D09-1082,"even more, we also plan to compare/combine it with other methods which are not based on overlapping information between t and h.",1,2009
D09-1082,"for future work, we would like to see whether the pas can help the second-stage classification as well, e.g. the semantic dependency of negation (am-neg) could be helpful for the contraction recognition.",1,2009
D09-1083,"in the future, we plan to explore more vector operations other than the inner-product (i.e., cosine) as well as different functional forms of the termweighting function (e.g.log-linear instead of linear).",4,2009
D09-1083,"in terms of applications, we would like to apply tweak in other problems such as paraphrase recognition and nearduplicate detection.",4,2009
D09-1086,we are exploring methods that incorporate a packed parse forest on the source side and similar representations of uncertainty about alignments.,1,2009
D09-1087,"in future work, we plan to scale up the training process with more unlabeled training data (e.g., gigaword) and investigate automatic selection of materials that are most suitable for self-training.",2,2009
D09-1087,"finally, it is also important to explore other ways to exploit the use of unlabeled data.",1,2009
D09-1087,we also plan to investigate domain adaptation and apply the model to other languages with modest treebank resources.,4,2009
D09-1088,"in particular we wish to examine whether parsing different languages should be pursued by different models, or whether the rr strategy can effectively cope with different languages types.",1,2009
D09-1088,in the future we plan to investigate how the different models fare against one another in parsing different languages.,1,2009
D09-1088,"finally, we wish to explore the implications of rr modeling for applications that consider the form of expression in multiple languages, for instance statistical machine translation (smt).",1,2009
D09-1093,"furthermore, future work will involve evaluating the performance of the system for these language on real typed data.",3,2009
D09-1093,it may be beneficial to perform more tuning in future.,6,2009
D09-1095,"since selectional preferences acquired from sense-untagged corpora have worked well for the metonymy resolution task, we plan to push further towards unsupervised metonymy resolution, putting to use the lessons learned from unsupervised wsd.",1,2009
D09-1095,"we plan to expand on this, and find methods to extract more such features automatically, without manually provided clues.",1,2009
D09-1096,"in future work, we plan to determine the effectiveness of using a sequential learning algorithm like conditional random fields (crf).",1,2009
D09-1097,future work will be to filter out difficult hypernyms for hyponymy extraction process to achieve higher precision.,1,2009
D09-1100,"in the future we hope to explore ways to interleave semantic analysis with exploration of the learning domain, by using the environment as a supervision signal for linguistic analysis.",1,2009
D09-1103,"in particular, we will further employ the centering theory in pronoun resolution from both grammatical and semantic perspectives on more corpora.",1,2009
D09-1103,"for future work, we will explore more kinds of semantic information and structured syntactic information in pronoun resolution.",1,2009
D09-1105,"it would be interesting to compare the speed and accuracy of our dynamic-programming localsearch method with an exact algorithm for solving the lop, such as integer linear programming with branch and bound (cf.charon and hudry (2006)).",3,2009
D09-1105,"that possibility remains future work, but it is likely to lead to further improvements, because it allows the translation system to consider multiple possible reorderings under the model, as well as to tune the weight of the model relative to the other parts of the system during mert.",1,2009
D09-1106,we expect that these syntax-based systems could benefit more from our approach.,4,2009
D09-1106,"another interesting direction is applying our approach to extracting translation rules with hierarchical structures such as hierarchical phrases (chiang, 2007) and tree-to-string rules (galley , 2006; liu , 2006).",4,2009
D09-1106,we believe that better estimation of alignment distributions will result in more significant improvements.,1,2009
D09-1109,another issue which is worth a deeper investigation is the application of fourier transform methods which offer tools for studying the periodic structure of the temporal sequences.,4,2009
D09-1109,this agrees with the intuition that time provides a fundamental semantic dimension possibly orthogonal to broad topical classification.,6,2009
D09-1109,this issue however deserves further investigation.,6,2009
D09-1110,"our efficient representation of the consequent search space opens the way to future investigation of the benefit of larger-scale rule chaining, and to the development of efficient search strategies required to support such inferences.",4,2009
D09-1111,"next, we plan to combine our models with a classifier that makes such a decision, allowing us to integrate transliteration into smt for other language pairs.",1,2009
D09-1112,"additionally, term similarity kernels, e.g.(basili et al., 2005; bloehdorn et al., 2006), will be likely improve our models, especially when combined syntactic and semantic kernels are used, i.e.(bloehdorn and moschitti, 2007a; bloehdorn and moschitti, 2007b).",1,2009
D09-1112,"kernel methods show that combinations of feature vectors, sequence kernels and other structural kernels, e.g. on shallow or deep syntactic parse trees, appear to be a promising future research line3.",1,2009
D09-1112,"in the future we would like to extend this research by focusing on advanced shallow semantic approaches such as predicate argument structures, e.g.(giuglea and moschitti, 2004; moschitti and cosmin, 2004; moschitti et al., 2008).",1,2009
D09-1113,"therefore, an interesting topic is how to further reduce the inconsistency between skipabove pairs and human labeling so that such data may also be useful for task-specific ranking.",5,2009
D09-1114,"in the future, we will explore more possibilities for feature subspaces selection and experiment with our method in a word-level system combination model.",4,2009
D09-1116,"finally, we plan to apply the technique to other languages with treebanks, such as chinese and arabic.",2,2009
D09-1116,"we also plan on evaluating the models performance on other genres of speech, as well as in other tasks such as machine translation.",3,2009
D09-1116,we intend to release the source code of our model within several months of this publication.,6,2009
D09-1116,"we plan to investigate how parser accuracy and data selection strategies, e.g., based on parser confidence scores, impact the performance of our model.",1,2009
D09-1116,we are also working on scaling our model further to accommodate amounts of data typical for modern large-scale ngram models.,1,2009
D09-1117,in the future we would like to explore the possibilities created by more tightly coupling the forward and reverse components of the bidirectional decoder.,1,2009
D09-1117,"scores from partial hypotheses of both processes could be combined and used at each step of the decoding, making the search more informed.",1,2009
D09-1117,"furthermore, forward partial hypotheses and reverse hypotheses would ‘meet’ during decoding (when one decoding direction has covered words in the source that the other has yet to cover), and provide paths for each other to a final state in the search.",1,2009
D09-1118,we will also experiment with using sequential models in order to try to exploit any sequential ordering patterns in the occurrence of the ddas.,1,2009
D09-1118,"in future work, we plan to extend the decision discussion annotation scheme and try to extract supporting arguments for decisions.",1,2009
D09-1120,"indeed, we suspect that further improving the syntactic and semantic modules in our system may produce greater error reductions than any other route forward.",1,2009
D09-1120,"of course, a system which is rich in all axes will find some advantage over any simplified approach.",1,2009
D09-1121,"with the obtained robust information, we will explore rewarding ways for parser improvements.",1,2009
D09-1121,"in our future work, we will improve the performance of our approaches by adding more patterns for the descriptive approach and by handling uncorrectable errors for the empirical approach.",1,2009
D09-1123,future work will attempt further extensions of our ddtm system to allow for the exploitation of long-range aspects of the dependency structure.,1,2009
D09-1123,we will work on expanding the features set of ddtm system to leverage features from the constructed dependency structure itself.,1,2009
D09-1123,"finally, we will work on enabling the deployment of source side dependency structures to influence the construction of the target dependency structure based on a bilingually enabled dependency parsing mechanism using the discriminative modeling capabilities.",1,2009
D09-1127,"furthermore, we believe this bilingual-monolingual approach can easily transfer to shift-reduce constituency parsing (sagae and lavie, 2006).",4,2009
D09-1127,"so we will engineer more such features, especially with lexicalization and soft alignments (liang et al., 2006), and study the impact of alignment quality on parsing improvement.",1,2009
D09-1127,"from a linguistics point of view, we would like to see how linguistics distance affects this approach, e.g., we suspect english french would not help each other as much as english-chinese do; and it would be very interesting to see what types of syntactic ambiguities can be resolved across different language pairs.",1,2009
D09-1129,in this way we will be able to correct grammar errors too.,6,2009
D09-1129,"we also plan more experiments using the 5-grams, but backing off to 4-grams and 3-grams when needed.",1,2009
D09-1129,"in future work, we plan to extend our method to allow for deleted or inserted words, and to find the corrected strings in the google web 1t n-grams.",1,2009
D09-1130,"in the future, we plan to explore other related problems such as adjacency pairs (levinson, 1983) and discourse parsing (soricut and marcu, 2003) for large-scale online forum data.",5,2009
D09-1131,"now we define these functions intuitively based on linguistic rules, but learning methods like regression will be investigated in the future.",1,2009
D09-1131,examining the interaction of cues from word and sentence levels on the opinion sentence extraction and the opinion polarity detection is our next goal.,1,2009
D09-1132,"we are, thus, experimenting with the possibility of obtaining short definitions from the web, using the system we presented.",4,2009
D09-1134,"in the future, we would like to see the application of our approach to other tasks such as (li , 2009).",4,2009
D09-1136,"because our bayesian model employs a very simple prior, more sophisticated generative models provide a possible direction for further experimentation.",1,2009
D09-1138,"one possible direction for this research topic would be to use our model for the semi-automatic construction of verb lexicons, with the help of human curation.",4,2009
D09-1138,"however, there is also a demand for exploring other types of features that can discriminate among confusing classes.",1,2009
D09-1139,"in our future work, we plan to extend our analysis to other test collections and to query expansion methods in order to generalize our conclusions.",2,2009
D09-1139,"as the problem of language ambiguity has a high impact on the use of sr measures, we will also consider word sense disambiguation in our future experiments.",5,2009
D09-1140,we plan to apply the methods described here to these other applications in the near future.,4,2009
D09-1142,"it would be interesting to verify whether gender recognition can be boosted by using lexical resources that capture the semantics of the words, such as wordnets or knowledge extracted from wikipedia, and verify whether similarities from a semantic point of view are also responsible for gender assignments in various languages.",5,2009
D09-1142,"we plan to look closer at cases where we obtain different predictions based on the word ending and the full form of the word, and use boosting to learn weights for classifiers based on different parts of the word to see whether we can further improve the results.",5,2009
D09-1143,"regarding future work, there are many research line that may be followed: i) capturing more features by employing external knowledge such as ontological, lexical resource or wordnet-based features (basili , 2005a; basili , 2005b; bloehdorn , 2006; bloehdorn and moschitti, 2007) or shallow semantic trees, (giuglea and moschitti, 2004; giuglea and moschitti, 2006; moschitti and bejan, 2004; moschitti , 2007; moschitti, 2008; moschitti , 2008).",5,2009
D09-1143,"from constituent trees, we can extract subtrees constituted by non-terminal symbols (grammar symbols), which provide a better generalization (with a risk of underfitting).iii) design a new kernel which can integrate the advantages of the constituent and dependency tree.",1,2009
D09-1143,from dependency trees we can extract more precise but also more sparse relationships (which may cause overfit).,1,2009
D09-1143,"the new tree kernel should inherit the benefits of the three available tree kernels: st, sst or pt.",1,2009
D09-1143,"regarding future work, there are many research line that may be followed: i) capturing more features by employing external knowledge such as ontological, lexical resource or wordnet-based features (basili et al., 2005a; basili et al., 2005b; bloehdorn et al., 2006; bloehdorn and moschitti, 2007) or shallow semantic trees, (giuglea and moschitti, 2004; giuglea and moschitti, 2006; moschitti and bejan, 2004; moschitti et al., 2007; moschitti, 2008; moschitti et al., 2008).ii) design a new tree-based structures, which combines the information of both constituent and dependency parses.",5,2009
D09-1144,then we intend to compare output results of the procedure with the human responses.,2,2009
D09-1144,"in the future we plan to experiment with large role-annotated corpora for english such as propbank (approx.300 000 words, (palmer , 2005)) and the framenet-annotated corpus provided by the fn project (more than 135 000 annotated sentences, (ruppenhofer , 2006)).",2,2009
D09-1144,"in the future we plan to experiment with large role-annotated corpora for english such as propbank (approx.300 000 words, (palmer et al., 2005)) and the framenet-annotated corpus provided by the fn project (more than 135 000 annotated sentences, (ruppenhofer et al., 2006)).",2,2009
D09-1144,in the future we plan to implement a procedure making use of the extracted ap-relations which would automatically extend phrases containing implicit predicates.,1,2009
D09-1144,"additionally, a study of a possible correspondence between human agreement on associated predicates and a semantic type of an argument (e.g. concrete/abstract, natural kind/artifact) should be performed on more test arguments.",1,2009
D09-1146,"certainly, there are many other possible applications of this model, including product comparison, media bias detection, and interdisciplinary literature analysis.",4,2009
D09-1146,"cultural awareness is also important in marketing and we can use this model to investigate, for example what products and what aspects of life people in different regions focus on.",4,2009
D09-1146,"in future work, we would like to enrich the model and/or feature set to move beyond the limitations of a bag-of-words analysis.",1,2009
D09-1146,"for example, by considering negation and word polarity, we can better capture the opinions of the authors, which is an important component of such cultural analysis.",1,2009
D09-1148,"similar methods could also be used to personalize search (teevan , 2008); for queries that mean different things to different people, the yarowsky method could be applied to variables such as user, time and place, so the results reflect what a particular user intended in a particular context.",4,2009
D09-1148,"in addition to click type, there are many other features in the logs that could prove useful for classifying queries by intent, e.g., who issued the query, when and where.",6,2009
D09-1148,"similar methods could be applied in future work to many other applications such labeling queries and urls by: language, market, location, time, intended for a search vertical (such as medicine, recipes), intended for a type of answer (maps, pictures), as well as inappropriate intent (porn, spam).",1,2009
D09-1150,"more applications also will be explored, such as emotional summarization, emotional question answering; emotional topic discovering.",4,2009
D09-1150,"at the same time, new research problems will arise, for examples, how to acquiring more emotional words and to generate their emotional vectors automatically; how to generate emotional vectors for sentences, paragraphs and documents with known emotional elements in them?",5,2009
D09-1151,"as future work, we plan to consider referential properties of noun phrases in associative anaphora resolution.",1,2009
D09-1152,we will need to handle more kinds of quantifiers in our mln model.,5,2009
D09-1152,integrating our technique for qsd with discourse processing is a major challenge that we hope to address.,5,2009
D09-1154,"our next goal is to use and evaluate the term variation collected by the proposed method in an actual search scenario, as well as improving the performance of our classifier by using individual, character-dependent edit operations as features in classification.",1,2009
D09-1157,"also, the predictions with high confidence can be used as seed training material for automatically harvesting more training data.",2,2009
D09-1157,"nevertheless, we combined the two methods in a rather crude way, leaving ample room for exploring better strategies in the future.",1,2009
D09-1160,"we will in the future consider an issue of speeding up decoding with structured models (lafferty , 2001; miyao and tsujii, 2002; sutton , 2004).",5,2009
D09-1161,"in the future, we will explore more features and study the forest-based combination methods for syntactic parsing.",1,2009
D09-1162,"in future work, we will try other word combinations and investigate better ways to represent the chinese text.",2,2009
D09-1162,"in addition, we will explore how to utilize the better chinese sentence-level novelty mining result to improve the detection performance on documents.",1,2009
D09-1163,we are planning to explore these methods to compare their performance.,3,2009
D09-1163,"future work will also include the comparison between our methods with other related approaches, such as kurland and lee’s cluster based approach (kurland and lee, 2006).we are planning to explore these methods to compare their performance.",1,2009
D09-1163,also direct re-ranking can be used to improve automatic query expansion since better ranking in top retrieved documents can be expected to improve the quality of the augmented query.,1,2009
D10-1002,it would also be interesting to determine whether further increasing the accuracy of the model used for automatically labeling the unlabeled data can enhance performance even more.,5,2010
D10-1002,"finally, for this work, we always used products of 10 grammars, but we sometimes observed that subsets of these grammars produce even better results on the development set.",6,2010
D10-1002,"in future work, we plan to investigate additional methods for increasing the diversity of our self trained models.",1,2010
D10-1002,finding a way to select grammars from a grammar pool to achieve high performance products is an interesting area of future study.,1,2010
D10-1002,one possibility would be to utilize more unlabeled data or to identify additional ways to bias the models.,1,2010
D10-1002,a simple but computationally expensive way to do this would be to parse the data with an sm7 product model.,1,2010
D10-1004,"other parsing formalisms can be handled with the inventory of factors shown here鈥 among them, phrase-structure parsing.",1,2010
D10-1004,"recent progress in message-passing algorithms yield 鈥渃convexified鈥 bethe approximations that can be used for marginal inference (wainwright et al., 2005), and provably convergent max-product variants that solve the relaxed lp (globerson and jaakkola, 2008).",1,2010
D10-1004,There are several possible directions for future work.,6,2010
D10-1005,how sentiment prediction impacts the implicit wsd is left to future work.,5,2010
D10-1005,"better capturing local syntax and meaningful collocations would also improve the model鈥檚 ability to predict sentiment and model multilingual topics, as would providing a better mechanism for representing words not included in our bridges.",1,2010
D10-1005,we intend to develop such models as future work.,1,2010
D10-1006,"one of the future directions we plan to explore is to use this model to help sentence-level extraction of specific opinions and their targets, which previously was only tackled in a fully supervised manner.",4,2010
D10-1006,another direction is to extend the model to support polarity classification.,1,2010
D10-1010,the questions that are posted on community qa sites often contain spelling or grammatical errors.,5,2010
D10-1010,"consequently, we will work on interfacing the question ranking system with a separate module aimed at fixing orthographic and grammatical errors.",1,2010
D10-1010,we also plan to make the dependency tree matching more flexible in order to account for paraphrase patterns that may differ in their syntactic structure.,1,2010
D10-1010,we plan to integrate context dependent word similarity measures into a more robust question utility function.,1,2010
D10-1011,"in future work, we intend to expand our analysis of the distribution of peco elements to a larger number of citations.",4,2010
D10-1011,"one way to do that would be to automatically extract pubmed citations that contain structural markers associated to peco categories (chung, 2009).",1,2010
D10-1012,we also aim to implement a number of word sense induction algorithms and compare them in the same evaluation framework with more web search and web clustering engines.,1,2010
D10-1012,"as regards future work, we intend to combine our clustering algorithm with a cluster labeling algorithm.",1,2010
D10-1012,"finally, it should be possible to use precisely the same approach presented in this paper for document clustering, by grouping the contexts in which the target query occurs 鈥 and we will also experiment on this in the future.",1,2010
D10-1013,"however, in future work we plan to substitute our own statistical mt systems, which will permit us to experiment across a range of translation model and language model lm training set sizes, and therefore to vary quality while keeping other system details constant.",3,2010
D10-1013,"finally, we intend to explore the application of our approach in scenarios involving less-common languages, by using a more common language as a pivot or bridge (habash and hu, 2009).",4,2010
D10-1013,"one important step will be to better characterize the relationship between cost and quality in quantitative terms: how much does it cost to obtain how much quality improvement, and how does that compare with typical professional translation costs of $0.25 per word?",5,2010
D10-1013,"we hope to explore whether the targeted paraphrasing translation pipeline can improve the productivity of post-editing by bilinguals, making it easier to move toward the upper bound in a cost-effective way.",5,2010
D10-1013,these initial studies leave considerable room for future work.,6,2010
D10-1013,"we plan to implement a fully automatic targeted paraphrasing translation pipeline, using the automated methods discussed when introducing the pipeline in section 2, including translation of targeted paraphrase lattices (cf.(max, 2010; du et al., 2010)).",1,2010
D10-1014,we will continue this line of research and exploit better ways to learn syntax and apply syntactic constraints to machine translation.,4,2010
D10-1015,"we think we have built a solid framework for the second challenge, and we plan to extend it further.",5,2010
D10-1016,"since our goal is not to be literal, but to obtain a satisfactory compromise between form and meaning, it would clearly be beneficial to augment target phrases with synonyms and paraphrases, or to allow for words to be dropped or added.",2,2010
D10-1021,this again will be an interesting future work.,6,2010
D10-1021,"also, efs being a useful method for feature selection in machine learning, it would be useful to perform further experiments to investigate how well it performs on a variety of classification datasets.",1,2010
D10-1022,"finally, we would like to point out that it is conceivable that negative training data could still be useful in many cases.",6,2010
D10-1022,an interesting direction to explore is to somehow combine the extracted reliable negative data from the unlabeled set and the existing negative training data to further enhance learning algorithms.,1,2010
D10-1022,"in our future work, we plan to do more comprehensive experiments to compare the classic supervised learning and pu learning techniques with different kinds of settings, for example, by varying the ratio between positive and negative examples, as well as their sizes.",1,2010
D10-1022,it is also important to explore how to catch the best iteration of the svm/nb classifier in the iterative running process of the algorithms.,1,2010
D10-1024,"as the quality of models increases on these noisy datasets, we anticipate a consequent rise in their usefulness to researchers and historians as browsing the data and mining it for useful patterns becomes more efficient and profitable.",5,2010
D10-1025,"as future work, dda could be applied to mapping documents in many languages simultaneously.",4,2010
D10-1025,a similar model could be used for cplsa: future work will show whether such a model can outperform opca.,1,2010
D10-1027,for future work we would like to apply this algorithm to forest-based translation and hierarchical system by pruning the first-pass 鈭扡m forest.,4,2010
D10-1027,"we would also combine cube pruning with our incremental algorithm, and study its performance with higher-order language models.",1,2010
D10-1028,"we plan to investigate this further, and a natural follow-on would be to experiment with adaptation for this variety of latent structure, to produce an adapted lspm-like model analogous to adaptive naive bayes.",3,2010
D10-1028,another promising direction for this work is the application of adaptor grammar models as a way to capture both lexical and grammatical aspects of framing in a unified model.,1,2010
D10-1029,"one source of information that has not previously been exploited is the lexical fixedness (fazly , 2009) of noncompositional prefix verbs.",2,2010
D10-1029,"on the other hand, marry again is relatively frequent, indicating that remarry is compositional.",6,2010
D10-1029,incorporation of these and other n-gram counts could further improve classification accuracy.,1,2010
D10-1029,there are also other n-gram-derived features that warrant further investigation.,1,2010
D10-1032,an obvious direction for future work is to expand the annotated corpus and improve the algorithm by experimenting with additional features.,2,2010
D10-1032,"in fact, tsd can assist textual entailment as well, since the sense of a tense form may provide substantial information about the relations entailed from the sentence.",2,2010
D10-1032,using tsd in such applications is a major direction for future work.,1,2010
D10-1033,"chief among directions for further work is to continue to improve performance on noisy data, and to strengthen our findings via larger data sets.",2,2010
D10-1033,"additionally, we look forward to expanding analysis to different types of imperfect input, such as machinetranslation output, different types of mark-up, and different genres of real data.",4,2010
D10-1034,"for the future work, it is possible to adapt our one-level clustering-based sampling to the multilevel one, where for every stratum it is still possible to divide it into lower sub-strata for further stratified sampling in order to make the seeds better represent the true distribution of the data.",1,2010
D10-1035,there are numerous avenues for further examination.,6,2010
D10-1035,"we intend to use sophisticated clustering methods, such as cbc (pantel, 2003), to identify multiple negative categories across the target categories in a single iteration.",1,2010
D10-1035,we would also like to explore the suitability of neg-finder for relation extraction.,1,2010
D10-1036,we will investigate the influence of corpus selection in training lda for keyphrase extraction using tpr.,3,2010
D10-1036,"in fact, the learned topics are highly dependent on the learning corpus.",5,2010
D10-1036,"we design to obtain topics using other machine learning methods and from other knowledge bases, and investigate the influence to performance of keyphrase extraction.2.",1,2010
D10-1036,"we plan to consider topic information in other graph-based ranking algorithms such as hits (kleinberg, 1999).3.",1,2010
D10-1038,"we plan to work on both synchronous (e.g., chats, meetings) and asynchronous (e.g., blogs) domains.",4,2010
D10-1038,we are also interested in the near future to transfer our approach to other similar domains by hierarchical bayesian multi-task learning and other domain adaptation methods.,4,2010
D10-1038,"as a next step for this research, we will investigate how to exploit these features in our methods.",4,2010
D10-1041,"secondly, we plan to experiment with another feature function in the log-linear model to discount words derived from paraphrases, and use mert to assign an appropriate weight to this feature function.",1,2010
D10-1041,"as for future work, firstly we plan to propose a pruning algorithm for the duplicate paths in the lattice, which will track the edge generation with respect to the path span, and thus eliminate duplicate paths.",1,2010
D10-1043,"in the future, we are interested in testing our algorithm at forest-based tree sequence to tree sequence translation.",3,2010
D10-1044,"finally, we intend to explore more sophisticated instance weighting features for capturing the degree of generality of phrase pairs.",1,2010
D10-1044,"in future work we plan to try this approach with more competitive smt systems, and to extend instance weighting to other standard smt components such as the lm, lexical phrase weights, and lexicalized distortion.",1,2010
D10-1044,we will also directly compare with a baseline similar to the matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.,1,2010
D10-1047,we also plan to investigate using other metrics in training in order to reduce redundant information in the summaries.,1,2010
D10-1047,we will investigate various ways of incorporating these global features into our a* search.,1,2010
D10-1047,however this will incur an additional computational cost over a purely local feature model and therefore may necessitate using an approximate beam search.,1,2010
D10-1047,"in the future we plan to expand this feature set with global features, especially ones measuring lexical diversity in the summaries to reduce the redundancy in them.",1,2010
D10-1048,our code is publicly released5 and can be used both as a stand-alone coreference system and as a platform for the development of future systems.,4,2010
D10-1050,"beyond summarization, we would also like to apply our model to other generation tasks, such as paraphrasing and text simplification.",4,2010
D10-1050,"currently we consider deletions, reordering and insertions.",1,2010
D10-1050,"ideally, we would also like to model arbitrary substitutions between words but also larger constituents (e.g., subclauses, sentence aggregation).",1,2010
D10-1050,"In the future, we plan to explore how to integrate more sophisticated QG rules in the generation process.",1,2010
D10-1051,"rather than translating the source text, a program may instead use the source text for inspiration.",6,2010
D10-1051,"such a hybrid translation/generation program would not be bound to translate every word, but rather it could more freely combine lexical material from its translation tables with other grammatical and lexical resources.",6,2010
D10-1051,"interestingly, human translators sometimes work this way when they translate poetry any excellent works have been produced by people with very little knowledge of the source language.",6,2010
D10-1051,an appealing future direction is to combine translation and generation.,1,2010
D10-1054,"in the future, we will carry out experiments on deeper features and evaluate the effects of different feature sets.",3,2010
D10-1055,we expect the corpus to also prove useful for feature engineering and error analysis in developing better realization models.,1,2010
D10-1055,"in future work, the performance of the ter family of metrics on this data might be improved by optimizing the edit weights used in computing its scores, so as to avoid over-penalizing punctuation movements or under-penalizing agreement errors, both of which were significant sources of ranking errors.",1,2010
D10-1056,"these promising results suggest a new direction for future research: improving pos induction by developing methods targeted towards extracting better prototypes, rather than focusing on improving clustering of the entire data set.",1,2010
D10-1057,"our methods are promising as tools to accompany the deployment of domain adaptation algorithms, so that a complete system can first identify when a domain shift has occurred before automatically adapting to the new domain.",1,2010
D10-1059,"a possible direction to investigate in the future consists in generalizing this hybrid strategy and combining random samples where the probability distribution induced on the lattice by the current parameters is scaled by a further temperature parameter q: p鈥(e, a|f) a p(e, a|f),3 (7) where for q = 1 the random samples used in this paper are obtained, for q tending to infinite the distribution becomes peaked around the single best path, thus producing samples similar to n-best lists, and samples from other real values of the temperature can be combined.",1,2010
D10-1060,"in future, we will explore various learning methods for better estimation of families, templates and lexical items.",1,2010
D10-1060,the target linguistic knowledge that we used in this paper will provide a nice starting point for unsupervised learning algorithms.,1,2010
D10-1060,features defined on templates and families will have good generalization capability.,1,2010
D10-1060,we will also try to further exploit the factorized representation with discriminative learning.,1,2010
D10-1061,"we are now actively exploring the possibility of linking the sample selection front-end to a crowd-sourcing backend, in order to obtain 鈥渘on-expert鈥 translations using a platform such as the amazon mechanical turk.",4,2010
D10-1062,"as always, preprocessing the corpus to address a certain problem in machine translation is less principled than tackling the problem head on by integrating it into the machine translation system itself.",5,2010
D10-1062,"however, there might be some empty elements which are not annotated but nevertheless helpful for improving machine translation.",5,2010
D10-1062,there are several other issues we may consider when recovering empty categories that are missing in the target language.,5,2010
D10-1062,we only considered empty categories that are present in treebanks.,5,2010
D10-1062,"it may be beneficial to include consideration for empty elements in the decoding process, so that it can benefit from interacting with other elements of the machine translation system.",1,2010
D10-1064,"second, we will combine paraphrases obtained via different techniques and resources, which will allow us to also learn translation distributions for phrases absent from the original corpus.",2,2010
D10-1064,"our future work includes three main areas: first, we want to improve the modeling of context, by notably working on techniques inspired from information retrieval to quickly access contextually-similar examples of source phrases in bilingual corpora.",1,2010
D10-1064,"lastly, we want to also exploit paraphrases for the additional translations that they propose (such as e4 on figure 3) and that would be contextually similar in the target language to other existing translations of a given phrase or that could even represent a new sense of the original phrase.",1,2010
D10-1067,"another direction is to apply zl to other nlp tasks and ml areas, supervised and unsupervised.",4,2010
D10-1067,developing a quality assessment algorithm for dependency trees will allow us to apply confidence based zl to unsupervised dependency parsing.,1,2010
D10-1067,"future research should focus on the development of more accurate estimators of parser output quality, and experimentation with different corpora, languages and parsers.",1,2010
D10-1067,"particularly, it will enable us to explore the combination of the methods proposed in (piatkowski et al., 2010) with zl for the dmv model and to integrate the pupa score into their bootstrapping algorithm.",1,2010
D10-1068,"alternatively, it would be interesting so see how much difference it makes to train the tagger on one set of data, and use that to tag a model training set from a different domain.",3,2010
D10-1068,"finally, a user study involving a grammar engineer working on a new language would be useful to validate the results we found here and confirm whether they are indeed helpful in bootstrapping a new grammar.",5,2010
D10-1068,"the issues surrounding what makes a good tagger for this purpose, and how can we best learn one without gold training data, would be one possibly fruitful avenue for further exploration.",5,2010
D10-1068,another interesting slant would be to investigate domain effects of the tagger.,5,2010
D10-1068,there are plenty of directions for further work arising from these results.,6,2010
D10-1068,other methods of incorporating the tagger output could also be investigated.,1,2010
D10-1070,"for the future work, we will explore tree kernel based methods to further improve the performance of scope learning in better capturing the structural information, and apply our parsing approach to other kinds of scope learning.",1,2010
D10-1071,"the current accuracy of of our model, getting the correct answer among the top two choices 96.2% of the time is high enough to be highly useful for tasks such as aiding the manual annotation of arabic text; a more complete automation would require that accuracy for the single top choice.",4,2010
D10-1073,"finally in terms of evaluation, our future work also focuses on evaluating hrgs using a finegrained sense inventory, extending the evaluation on the semeval-2010 wsi task dataset (manandhar , 2010) as well as applying hrgs to other related tasks such as taxonomy learning.",3,2010
D10-1073,this consensus tree might be able to express a larger amount of topological features of the initial undirected graph.,5,2010
D10-1073,"moreover, following the work in (clauset et al., 2008), we are also working on using mcmc in order to sample more than one dendrogram at equilibrium, and then combine them to a consensus tree.",1,2010
D10-1073,"our future work focuses on using different feature types, e.g. dependency relations, second-order cooccurrences, named entities and others to construct our undirected graphs and then applying hrgs, in order to measure the impact of each feature type on the induced hierarchical structures within a wsd setting.",1,2010
D10-1074,a more systematical approach to collect and create a larger set of data is crucial.,2,2010
D10-1074,"finally, as the technology in conversation entailment is developed, its applications in nlp problems should be explored.",4,2010
D10-1074,these applications may provide new insights on the nature of the conversation entailment problem and its potential solutions.,6,2010
D10-1074,"as more techniques in semantic processing (e.g., semantic role) become available, future work should also capture deeper semantics, address pragmatics, and incorporate richer world knowledge.",1,2010
D10-1074,"example applications include information extraction, question answering, summarization from conversation scripts, and modeling of conversation participants.",1,2010
D10-1074,"innovative community-based approaches (e.g., through web) for data collection and annotation can be pursued in the future.",1,2010
D10-1076,our future work will thus aim at studying the connections between our empirical observations and the deep learning framework.,1,2010
D10-1078,investigating this and related options is left for future work.,6,2010
D10-1078,"this would not necessarily preclude the use of an iteration-dependent scaling factor, which would achieve the goal of gradually forcing the tagging to become deterministic.",1,2010
D10-1078,"an alternative would be to estimate the variance for each gaussian separately, as is usually done in em for gaussian mixtures.",1,2010
D10-1079,"finally, future work includes the application of the syromorph methodology to other under-resourced semitic languages.",4,2010
D10-1079,future work will require addressing issues encountered in this corpus.,5,2010
D10-1079,"in addition, there is much to do in getting the overall tag accuracy closer to the accuracy of individual decisions.",6,2010
D10-1079,we leave further feature engineering for the stem tagger and the exploration of possible new morphological tagging techniques for future work.,1,2010
D10-1080,"in future work, we will evaluate the use of break indexes for tagging when there is lexical error.",3,2010
D10-1080,"we would also apply the nbest rescoring method to exploit break indexes in the hmm-la bidirectional model, as this would likely produce further improvements.",1,2010
D10-1081,"as the proposed approach provides significant performance improvements, it could be utilized in the development of more sophisticated novel word induction schemes, e.g. ensemble models trained independently with different data.",1,2010
D10-1081,"of course, we are also going to explore the model鈥檚 potential in the setting of semi-supervised morphological analysis.",1,2010
D10-1083,"we hypothesize that modeling morphological information will greatly constrain the set of possible tags, thereby further refining the representation of the tag lexicon.",1,2010
D10-1083,a promising direction for future work is to explicitly model a distribution over tags for each word type.,1,2010
D10-1084,"in future work, we plan to investigate methods for automatically cleansing the data to remove typos, and taking account of temporal gaps that can sometimes arise in online chats (e.g.in table 2, there is a time gap between c:u22 brb in 1 min and c:u23 thank you for waiting).",1,2010
D10-1086,"finally, we will devote more on further developing our corpus, with the ultimate mission of annotating all the documents in cbt 6.0.",2,2010
D10-1086,"in the future work, we will consider other situations.",6,2010
D10-1088,"therefore, it is possible that a word could have a problematic lexical entry even if it only occurs in sentences which are assigned a full-span parse.",5,2010
D10-1088,"however, it would be useful to extend it, so it can work with longer n-grams.",1,2010
D10-1088,"for example, a given word could have some reading which is not yet handled in the lexicon only within a particular bi- or trigram.",1,2010
D10-1090,"another alternative is to collect parallel texts against multiple foreign languages, e.g., using europarl (koehn, 2005).",2,2010
D10-1090,we intend to obtain and evaluate paraphrases generated from real pg systems and compare their performances in a follow-up study.,3,2010
D10-1090,We leave this for future work.,6,2010
D10-1091,"in our future work, we aim at using this ilp framework to systematically assess various search configurations.",3,2010
D10-1091,"we are also concerned by determining how tight is our approximation of the bleu4 score is: to this end, we intend to compute the best bleu-4 score within the n-best solutions of the oracle decoding problem.",1,2010
D10-1091,we plan to explore how replacing non-reachable references with high-score pseudo-references can improve discriminative training of pbts.,1,2010
D10-1092,future work includes extension of the method so that it can outperform conventional methods even for similar language pairs.,1,2010
D10-1093,"also, we plan to experiment our approach on others languages.",2,2010
D10-1093,"in the future, we will consider the experimentation of the developed tool in the generation of various personalized views both in virtual and materialized versions.",3,2010
D10-1093,"also, we plan to put up our system on the web.",4,2010
D10-1094,"improving the system is also in our future work, but orthogonal to the current contribution.",6,2010
D10-1094,we would also like to compare the proposed methods to the quality of a model trained on error-tagged data.,1,2010
D10-1100,we will also investigate the relation between classes of social events and their syntactic realization.,1,2010
D10-1100,"in the future, we will use other parsers (such as semantic parsers) and explore new types of linguistically motivated structures and transformations.",1,2010
D10-1101,"for future work, we might investigate how machine learning algorithms, which are specifically designed for the problem of domain adaptation (blitzer et al., 2007; jiang and zhai, 2007), perform in comparison to our approach.",1,2010
D10-1102,"our approach can, in principle, be applied to any classification task that is well modeled by jointly solving an extraction subtask.",4,2010
D10-1104,"in the future, we plan to automatically construct confusion sets, expand our approach to more verbs and test our approach on a larger size of real data.",2,2010
D10-1104,we also plan to further validate the effectiveness of the srl-derived features under other learning methods like svms.,1,2010
D10-1104,we will try to combine the outputs of several srl systems to make our system more robust.,1,2010
D10-1105,"future work would address on how to systematically predict and recommend the bursty queries using online media, as well as a reasonable evaluation metrics upon it.",5,2010
D10-1106,"directions for future work include inducing longer inference rules, investigating better methods for combining the rules, allowing deeper inferences across multiple rules, evaluating our system on other corpora and devising better techniques for handling word sense ambiguity.",1,2010
D10-1107,our future work will include an evaluation of tarec in the context of textual inference applications.,3,2010
D10-1108,"however, further research is required to develop methods that reliably (a) identify the number of independent perspectives a concept can take (or seems to take in the domain text), and (b) classify any harvested term into one or more of them.",1,2010
D10-1108,"another promising line of research would investigate the combination of the two styles of taxonomization algorithms: first, the one described here to produce an initial (set of) taxonomies, and second, the term-insertion algorithms developed in prior work.",1,2010
D10-1109,"also, since questions on online communities are classified into categories by topic, we plan to perform joint question type inference on function-based taxonomy as well as topic based taxonomy by markov logic.",1,2010
D10-1109,the model will not only capture the relation between patterns and types but also the relation between types in different taxonomy.,1,2010
D10-1111,"finally, in the future we plan to extend our model to perform joint modeling and summarization of ideological discourse.",1,2010
D10-1112,we see two main possibilities of improvement.,6,2010
D10-1112,"this would require a larger, more heterogeneous set of training material for the latter in order to avoid overfitting.",1,2010
D10-1112,"second, a mixed approach could combine the benefits of the word based model with the n-gram model.",1,2010
D10-1112,additional training data could be extracted from the web and automatically annotated with the current model in a semi-supervised approach.,1,2010
D10-1112,"first, the rule base can be extended to better account for lexical exceptions, orthographic variation and irregular morphology.",1,2010
D10-1113,we plan to further investigate how to select or to better aggregate the entire set of features extracted from a context.,1,2010
D10-1113,"ideally, we would like to compute the collective influence of several context words on the target.",1,2010
D10-1114,"towards this end we are currently evaluating two classes of approaches for setting pruning parameters per-word instead of globally: (1) subspace clustering, i.e.unsupervised feature selection (e.g., parsons , 2004) and (2) multiple clustering, i.e.finding feature partitions that lead to disparate clusterings (e.g., shafto , 2006).",3,2010
D10-1114,with this data it would be interesting to validate the hypothesis that the percentage of features allocated to the background cluster is correlated with the degree of homonymy.,5,2010
D10-1114,"furthermore, it is straightforward to extend the model to a two tier, two-clustering structure capable of additionally accounting for commonalities between arguments.",1,2010
D10-1114,"the basic tiered clustering can be extended with additional background tiers, allocating more expressivity to model background feature variation.",1,2010
D10-1114,applying more principled feature selection approaches to vector-space lexical semantics may yield more significant performance gains.,1,2010
D10-1114,"(future work) the word similarity experiments can be expanded by breaking pairs down further into highly homonymous and highly polysemous pairs, using e.g. wordnet to determine how closely related the senses are.",1,2010
D10-1114,"this class of models covers the spectrum between a pure 6e.g., pad麓o et al.(2007) report p鉁0.515 on the same data. topic model (all background tiers) and a pure clustering model and may be reasonable when there is believed to be more background structure (e.g. when jointly modeling all verb arguments).",5,2010
D10-1115,it remains to be seen if the approach we proposed will scale up to such challenges.,5,2010
D10-1115,"ultimately, we want to compose larger and larger constituents, up to full sentences.",1,2010
D10-1116,"in addition, it is crucial to have a full evaluation of the linguistic stegosystem in terms of imperceptibility and payload capacity so we can know how much data can be embedded before the cover text reaches its maximum distortion which is tolerated by a human judge.",3,2010
D10-1116,"for future work, we would like to explore more linguistic transformations that can meet the requirements of linguistic steganography 鈥 retaining the meaning, grammaticality and style of the original text.",1,2010
D10-1117,"we envisage that in future many grammar formalisms that have been shown to be effective in supervised parsing, such as categorial, unification and tree adjoining grammars, will prove amenable to unsupervised induction using the hierarchical nonparametric modelling approaches we have demonstrated in this paper.",1,2010
D10-1118,the ibm approach demonstrated the benefit of a gradual increase of model complexity.,6,2010
D10-1118,"furthermore, an alignment-based parsing method is expected to integrate well with smt bi-lingual alignment models and may,",1,2010
D10-1118,"therefore, be suitable for combined models which use parse trees to improve word alignment (e.g., burkett et al.2010).",1,2010
D10-1118,it would be interesting to see if the two approaches could be successfully combined.,1,2010
D10-1119,"such an approach would complement ideas for using high-order unification to model a wider range of language phenomena, such as vp ellipsis (dalrymple et al., 1991).",1,2010
D10-1119,"we are also interested in developing similar grammar induction techniques for context-dependent understanding problems, such as the one considered by zettlemoyer &amp; collins (2009).",1,2010
D10-1120,"2) lexicalizing the model, and",1,2010
D10-1120,"in future work we intend to study ways to bridge this gap by 1) incorporating more sophisticated linguistically-driven grammar rulesets to guide induction,",1,2010
D10-1120,"3) combining our constraint-based approach with richer unsupervised models (e.g., headden iii et al.(2009)) to benefit from their complementary strengths.",1,2010
D10-1122,"as future work, we would like to explore the possibility of learning hash functions using 1) bilingual and monolingual data together and 2) multiple conjugate languages.",2,2010
D10-1123,future work: functionality is one of the several properties a relation can possess.,6,2010
D10-1123,these properties are very useful in increasing our understanding about these open ie relation strings.,6,2010
D10-1123,"we believe that the general principles developed in this work, for example, connecting the open ie knowledge with an existing knowledge resource, will come in very handy in identifying these other properties.",6,2010
D10-1123,"others include selectional preferences, transitivity (schoenmackers , 2008), mutual exclusion, symmetry, etc.",6,2010
D10-1124,"in some regions, estimates of many of these factors may be obtained by cross-referencing geography with demographic data.",2,2010
D10-1124,We hope to explore this possibility in future work.,6,2010
D10-1125,"finally, our approach could be used with other structured learning algorithms, e.g.meshi (2010).",4,2010
D10-1125,"in addition, our dual decomposition approach is well-suited to parallelization. finally, our approach could be used with other structured learning algorithms, e.g.meshi et al.(2010).",1,2010
D10-1125,"as described in section 7.7, the algorithms can be easily modified to consider projective structures by replacing y with the set of projective trees, and then using first-order dependency parsing algorithms in place of mst decoding.",1,2010
D10-1125,"this method could be used to derive parsing algorithms that include higher-order features, as an alternative to specialized dynamic programming algorithms.",1,2010
D10-1125,"the general approach should be applicable to other lexicalized syntactic formalisms, and potentially also to decoding in syntax-driven translation.",4,2010
D11-1001,"in the future we want to integrate weak supervision techniques to train extractors with existing biomedical databases, such as kegg, and only minimal amounts of annotated text.",2,2011
D11-1002,"in future work, we plan to delve further into dependency parsing, looking specifically at the implications of multi-headedness and disconnected subgraphs on dependency parsing.",1,2011
D11-1002,"we also intend to carry out meta-classification, combining the predictions of crfsgd and maltparser.",1,2011
D11-1005,"future work might consider exploiting a larger number of treebanks, and more powerful techniques for combining models than simple local mixtures.",1,2011
D11-1009,"apart from obtaining and experimenting with larger collections of paraphrasing rules, it would be interesting to evaluate our method in vivo, for example by embedding it in question answering systems (to paraphrase the questions), in information extraction systems (to paraphrase extraction templates), or in natural language generators (to paraphrase template-like sentence plans).",3,2011
D11-1009,"we also plan to investigate the possibility of embedding our svr ranker in the sentence paraphraser we compared against, i.e., to rank candidates produced by using several machine translation systems and pivot languages, as in zhao-eng.",1,2011
D11-1010,"in future work, we plan to extend our system to fully automatic collocation correction that involves both identification and correction of collocation errors.",4,2011
D11-1011,"in the future, we want to apply our approach to web-based taxonomy induction, which according to (kozareva and hovy, 2010) is stifled due to the lacking relations between the instances and the classes, and the classes themselves.",4,2011
D11-1015,the future work will be focused on (1) integrating more semantic and syntactic information in proposed unsupervised method; (2) extending our method to inter-sentence level and then jointly modeling intrasentence level and inter-sentence level discourse constraints on polarity to reach a global optimal inference for polarity classification.,1,2011
D11-1016,also we plan to consider other ways to initialize the matrix-space model.,1,2011
D11-1016,"the other possible direction is to use existing sentiment lexicons and employing a “curriculum learning” strategy (bengio et al., 2009; kumar et al., 2010) for our learning problem.",1,2011
D11-1016,"one interesting direction to explore might be to use non-negative matrix factorization (lee and seung, 2001), co-clustering techniques (dhillon, 2001) to better initialize words that share similar contexts.",1,2011
D11-1020,"in our future works, we will exploit the semantic information encoded in the dependency structures which is expected to further improve the translations, and replace 1-best dependency structures with dependency forests so as to alleviate the influence caused by parse errors.",1,2011
D11-1022,"non-logical constraints may also yield efficient subproblems, e.g., the length constraints in summarization and compression (clarke and lapata, 2008; martins and smith, 2009; bergkirkpatrick , 2011).",5,2011
D11-1022,"finally, dd-admm can be adapted to tighten its relaxations towards exact decoding, as in sontag et al.(2008) and rush and collins (2011).",1,2011
D11-1022,"dd-admm may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by liang et al.(2011).",1,2011
D11-1022,We defer this for future work.,6,2011
D11-1023,"association scores and counts from sketch can be used for more nlp tasks like small-space randomized language models, word sense disambiguation, spelling correction, relation learning, paraphrasing, and machine translation.",4,2011
D11-1026,it is likely that dictionary-based or corpus-based similarity measures would yield a major improvement in performance.,1,2011
D11-1026,we consider two main directions for future work: using more informative similarity metrics and making the process of segmentation hierarchical.,1,2011
D11-1027,we could also use discourse relations to aid in extracting other semantic relations between events.,1,2011
D11-1027,"there are several interesting directions for future work, including the incorporation of other knowledge sources such as coreference and semantic class predictions, which were shown to be potentially important in our error analysis.",2,2011
D11-1030,"in future work, we plan to integrate distance metric learning into our approach, allowing some features to be weighted more heavily than others.",1,2011
D11-1031,"although we have focused on ccg parsing in this work, we expect our methods to be equally applicable to parsing with other grammar formalisms including context-free grammar or ltag.",4,2011
D11-1031,in future work we plan to scale our exact loss functions to larger settings and to explore training with loss functions within loopy belief propagation.,1,2011
D11-1039,an important area for future work is to consider how this learning can be best integrated into a complete dialog system.,4,2011
D11-1043,"further study needs to analyze more summarization metrics such as those proposed at the recent nist evaluation of automatic metrics, automatically evaluating summaries of peers (aesop) (nat, 2010).",3,2011
D11-1044,"our framework allows integrating together all of these and other types of structures, with the ultimate goal of combining the strengths of multiple approaches to translation in a single model.",1,2011
D11-1044,"the obvious next step for our framework is to include bilingual rules that include source syntax (quirk et al., 2005), target syntax (shen et al., 2008), and syntax on both sides.",1,2011
D11-1045,"in future work we plan to explore using more data from automatic alignments, perhaps by considering a joint model for aligning and reordering.",2,2011
D11-1045,we also would like to explore whether the use of scores from our reordering model directly in machine translation systems can improve performance relative to using just the single best reordering.,5,2011
D11-1045,we would like to investigate the use of other loss functions and their effect on reordering performance.,1,2011
D11-1045,"we would also like to explore doing away with the requirement of having a pos tagger, using completely unsupervised methods to class words.",1,2011
D11-1047,"we still have a lot to do with respect to improving our implementation, exploring the different possibilities offered by this framework and proceeding to more experiments.",3,2011
D11-1049,"first, inference starting from both the query nodes and target nodes (richards and mooney, 1992) can be much more efficient in discovering long paths than just inference from the query nodes.",6,2011
D11-1049,there are several prominent directions for future work.,6,2011
D11-1049,"second, inference starting from the target nodes of training queries is a potential way to discover specialized paths (with grounded nodes).",1,2011
D11-1049,"third, generalizing inference paths to inference trees or graphs can produce more expressive random walk inference models.",1,2011
D11-1050,"future work will focus on ways to enhance the noun vector representations through additional contextual features, to make them denser and more articulated in structure.",1,2011
D11-1052,"another direction of research is the connection between lengthening and other orthographic conventions associated with sentiment and emphasis, such as emoticons, punctuation, and capitalization.",1,2011
D11-1052,"finally, we plan to integrate lengthening and its related phenomena into an accurate, twitter-specific, sentiment classifier.",1,2011
D11-1056,in the future we will assign a pos tag to each word in order to use segmented noun phrases in morphological analysis.,1,2011
D11-1056,we assume that the meaning of constituents in a noun phrase rarely depends on outer context.,1,2011
D11-1059,"since it is so easy to add extra features to our model, one direction for future work is to explore other possible features.",1,2011
D11-1059,"we are also interested in improving our morphology features, either by considering other ways to extract features during preprocessing (for example, including prefixes or not concatenating together all suffixes), or by developing a joint model for inducing both morphology and syntactic classes simultaneously.",1,2011
D11-1059,"for example, it could be useful to add dependency features from an unsupervised dependency parser.",1,2011
D11-1060,"in future work, we plan to apply our framework to the remaining relations in the inventory of levi (1978), and to release the resulting dataset to the research community.",4,2011
D11-1066,"in future work, we plan to extend the current research by investigating models capable of exploiting predicate argument structures for question classification and answer reranking.",1,2011
D11-1066,"the use of syntactic/semantic kernels is a promising research direction (basili et al., 2005; bloehdorn and moschitti, 2007a; bloehdorn and moschitti, 2007b).",1,2011
D11-1067,we will investigate automatic conversion of these treebanks (by flattening mwe bracketing) for mwe identification.,1,2011
D11-1068,we will also develop algorithms for argument identification.,1,2011
D11-1068,"in future, we will explore further features for connective disambiguation as well as connective-specific classification, combined with semi-supervised algorithms to alleviate data sparseness.",1,2011
D11-1069,"ultimately, we would like to identify the speech act expressions themselves because some sentences contain speech acts as well as factual information.",1,2011
D11-1069,"in future work, we believe that segmenting sentences into clauses may help to train classifiers more precisely.",1,2011
D11-1072,"our future work will consider additional semantic properties between entities (types, member of/part of, etc.)for further enhancing the coherence algorithm.",1,2011
D11-1074,"as our future work, we plan to explore how to further incorporate such world knowledge into our model in a principled way.",1,2011
D11-1075,"an alternative is to consider an approximate solution that evaluates, for instance, only the top few label assignments that are likely to maximize the likelihood of our observations.",3,2011
D11-1075,this remains as an interesting future work of this study.,6,2011
D11-1077,we also plan to apply the methodology to languages other than hebrew.,2,2011
D11-1077,"in the future, we intend to extend the evaluation to longer n-grams.",3,2011
D11-1078,future experiments include contrasting these results with other dictionaries and language pairs.,3,2011
D11-1081,"finally, we hope to exploit more features such as reordering features and syntactic features so as to further improve the performance.",1,2011
D11-1082,"future work will assess its impact on translation for the other language pairs, as well as its impact on other tasks, such as named entity projection.",3,2011
D11-1084,"in the future, we will further explore how to reflect document divergence during training and dynamically adjust cache weights according to different documents.",5,2011
D11-1085,"in future work, we plan to consider lowresource test domains and language pairs like urduenglish, where bilingual data for novel domains is sparse.",2,2011
D11-1089,"although our investigation was restricted to katakana noun compounds, one might expect that a similar approach would be useful for splitting other types of noun compounds (e.g., german noun compounds), or for identifying general word boundaries, not limited to those between nouns, in asian languages.",4,2011
D11-1089,we think these are research directions worth exploring in the future.,6,2011
D11-1091,"to this end, we plan to develop a model that simultaneously induces predicates and learns coercions, using knowledge of a predicate’s coerciveness to inform the induction mechanism.",1,2011
D11-1092,"we will also design methods for assessing the quality of mapping, and analyze their correlations with the sr methods.",3,2011
D11-1092,"third, analyses will be carried out to uncover the differences between feature combination and integration that have led to different accuracies.",5,2011
D11-1094,"secondly, we would like to subject our approach to further evaluation, in particular on a number of different evaluation tasks, such as semantic compositionality.",3,2011
D11-1094,"and thirdly, we would like to transfer the general idea of the approach presented in this paper to a tensor based framework (which is able to capture the multiway co-occurrences of words, together with their window-based and dependency-based context features, in a natural way) and investigate whether such a framework proves beneficial for the modeling of word meaning in context.",1,2011
D11-1094,the results might improve further if window-based context and dependency-based context are combined in an optimal way.,1,2011
D11-1095,"as for the constrained version of hgfc, we will conduct a larger scale experiment on the verbnet data to investigate what kind of upper level hierarchy it can propose for this resource (which currently has over 100 top level classes).",2,2011
D11-1095,"in addition, we plan to apply the unconstrained hgfc to specific domains to investigate its capability to learn novel, previously unknown classifications.",4,2011
D11-1095,"finally, we plan to compare hgfc to other hierarchical clustering methods that are relatively new to nlp but have proved promising in other fields, including bayesian hierarchical clustering (heller and ghahramani, 2005; teh et al., 2008) and the method of azran and ghahramani (2006a) based on spectral clustering.",1,2011
D11-1097,"we intend to adapt our approach for word sense disambiguation as well as related domain-specific tasks such as gene name normalisation (morgan , 2008).",4,2011
D11-1097,"a further, more speculative direction for future research is to investigate more richly structured models of context, for example capturing correlations between words in a text within a framework similar to the correlated topic model of blei and lafferty (2007) or more explicitly modelling polysemy effects as in reisinger and mooney (2010).",1,2011
D11-1097,"in future work, 7the overall average gap for thater et al.(2010) does not appear in their paper but can be calculated from the score and number of instances listed for each pos. we intend to adapt our approach for word sense disambiguation as well as related domain-specific tasks such as gene name normalisation (morgan et al., 2008).",4,2011
D11-1100,"using a bi-lingual dictionary which maps wordnet across languages, such a machine translation sub-system can be avoided.",2,2011
D11-1100,more sophisticated features which include the two need to be explored.,6,2011
D11-1100,another line of work is in the context of cross-lingual sentiment analysis.,1,2011
D11-1103,"as part of the future work, we plan to extend this technique for hypergraphs and lattices in re-scoring mt outputs with complex and long span language models.",4,2011
D11-1105,"second, we may explore more domain knowledge to improve the quality of aspect-oriented summaries.",2,2011
D11-1105,currently the sentence compression algorithm may generate meaningless subtrees.,5,2011
D11-1105,it is relatively hard to decide which clause is redundant in terms of summarization.,5,2011
D11-1105,"first, we can possibly apply more linguistic knowledge to improve the quality of sentence compression.",1,2011
D11-1105,there are a number of directions we plan to pursue in the future in order to improve our method.,1,2011
D11-1105,"third, we want to extend our event-aspect model to simultaneously find topics and aspects.",1,2011
D11-1105,"for example, we know that the “who-affected” aspect is related to person, and “when, where” are related to time and location. we can import name entity recognition to annotate these phrases and then help locate relevant sentences.",1,2011
D11-1106,"first, a large scale language model can be incorporated into our model in the search algorithm, or through reranking.",6,2011
D11-1106,potential improvements to our system can be made in several areas.,6,2011
D11-1106,"future work also includes integration with an smt system, where content word selection will be applicable.",1,2011
D11-1106,"second, a heuristic future cost (e.g.",1,2011
D11-1106,"varges and mellish (2010)) can be considered for each hypothesis so that it also considers the words that have not been used, leading to better search.",2,2011
D11-1107,in future work we plan to identify further features that will allow us to inform this choice so that we can move towards this level of performance.,1,2011
D11-1113,"future work on parse correction might focus on developing specialized models for other difficult attachment types, such as verb-phrase attachment (verb dependents account for around 15% of incorrect attachments across all four parsers).",1,2011
D11-1115,"we have shown through adapting to the question domain that it is possible to make focused improvements when we can identify the gaps in coverage (as in wh-question words), but in order to address the challenge of automatic lexicon extension fully, quite different techniques for generalising lexical entries for seen words will be require.",5,2011
D11-1116,it would also be interesting to examine the impact on final parsing accuracy of the various differences between our dependency conversion and stanford’s.,5,2011
D11-1116,we would like to change our handling of coordinating conjunctions to treat the coordinating conjunction as the head because this has fewer ambiguities than the current approach and also add the ability to produce traces for wh- words.,1,2011
D11-1118,and exploring other uses of soft clustering algorithms — perhaps as inputs to part-of-speech disambiguators — may be another fruitful research direction.,4,2011
D11-1119,"we will investigate extending, to other applications, this general methodology combining distributional, semantic and syntactic information with language models.",1,2011
D11-1120,"in future work, we will explore how well such models carry over to gender identification in other informal online genres such as chat and forum comments.",4,2011
D11-1120,"furthermore, we have been able to assign demographic features beside gender, including age and location, to our twitter dataset.",1,2011
D11-1121,"in future work we plan to improve the features of the svm classifier, and further investigate the usefulness of our approach for trading.",1,2011
D11-1126,in future work we will continue to investigate methods to mitigate quality loss.,1,2011
D11-1128,"for future work, graph reinforcement could be extended to mt to improve the coverage of aligned phrase tables.",4,2011
D11-1128,"in doing so, it is reasonable to assume that there are multiple ways of expressing a singular concept and hence multiple translations are possible.",1,2011
D11-1128,using graph reinforcement can help discover such translation though they may never be seen in training data.,1,2011
D11-1129,"the advantages and disadvantages of this method and comparisons with other systems, in particular ccg, constitutes ongoing work.",3,2011
D11-1131,"in addition, we are thinking about using an extension of the pcfg formalism that allows for some kind of ‘featurepassing’ which could lead to much smaller and more general grammars.",4,2011
D11-1131,"in future research, we plan to address the limitation of our model to a finite set of meaning representations, in particular through the use of nonparametric bayesian models such as the infinite pcfg model of liang et al.(2007) and the infinite tree model of finkel et al.(2007); both allow for a potentially infinite set of non-terminals, hence directly addressing this problem.",1,2011
D11-1134,one additional area for future research is to extend ontext to discover new categories in addition to new relations.,4,2011
D11-1137,"in the future, we plan to investigate more appropriate generative models for reranking.",4,2011
D11-1137,"as we mentioned in section 5.4, we also plan to incorporate semi-supervised learning into our framework, which may potentially improve our reranking performance.",1,2011
D11-1137,"we plan to examine to model such a complex structure (granduncle) (goldberg and elhadad, 2010) or higher-order structure than third-order for reranking which is computationally expensive for a baseline parser.",1,2011
D11-1142,the error analysis in section 5.2 also suggests natural directions for future work.,6,2011
D11-1142,"for instance, since many of reverb’s errors are due to incorrect arguments, improved methods for argument extraction are in order.",1,2011
D11-1143,we plan to confirm these results by using estimates of quality in the future.,3,2011
D11-1146,we will ensemble wtm with other content based and collaboration-based methods to build a practical social tag suggestion system.3.,1,2011
D11-1146,wtm and ewtm can only suggest the tags that have appeared in translation models.,1,2011
D11-1146,"we can exploit other word alignment methods like log-linear models (liu et al., 2010a) for social tag suggestion.2.",1,2011
D11-1146,"in future, we plan to incorporate keyphrase extraction in social tag suggestion to make it suggest more appropriate tags not only from translation models but also from the resource descriptions.",1,2011
D11-1147,"as our future work, we aim to build a system that employs our findings in this paper and the emergent patterns in the re-tweet network topology to identify whether a new trending topic is a rumor or not.",1,2011
D11-1148,"one is the use of feature schema instances that did not appear in the largely grammatical wsj; another is the extension of feature schemas; and a third is the use of a parser that does not enforce linguistic constraints such as the berkeley parser (petrov et al., 2006).",1,2011
D12-1002,"while this is an important question, in this paper, we are primarily interested in showing the importance of handling language specific phenomenon in the bridge language approaches.",5,2012
D12-1002,"in future, we would like to study the appropriateness of ipa vs. english as the bridge language and also the generalizability of our technique to other scenarios.",1,2012
D12-1003,"we are also planning an end-toend evaluation, for instance, by employing the extracted bilingual lexicon into an mt system.",3,2012
D12-1003,"we are planning to investigate the following open problems in future work: word sense disambiguation and translation of compound words as described in (daille and morin, 2005; morin , 2007).",5,2012
D12-1003,"we will utilize their random walk approach or other graph-based techniques such as modified adsorption (talukdar and crammer, 2009) for generating seed distributions.",1,2012
D12-1004,we are currently collecting and transcribing additional stories from the two groups which we would like to use as a definitive test set to verify the stability of our findings.,2,2012
D12-1004,"we plan to explore syntactic and coherence models to analyze the stories, as well as emotion analysis of the narratives.",1,2012
D12-1005,"finally, we assumed here strictly count-based features; streaming log-counting methods, tailored bloom-filters for binary feature storage, and other related topics are assuredly applicable, and should give rise to many interesting new results.",5,2012
D12-1007,it could be improved to incorporate noise resulting from the decisions made by the semantic parser.,1,2012
D12-1007,the effects of a dependence assumption between the different utterances occurring in a single user turn under the act model can also be explored.,1,2012
D12-1007,"another possible improvement is to explore the effects of introducing dependency between the slots in the user goal, which would enforce more plausible values pairings and would potentially improve the simulator鈥檚 performance.",1,2012
D12-1007,we would also like to use our simulator to train a pomdp-based dialog manager using a form of reinforcement learning.,1,2012
D12-1007,our model could be extended in a number of ways.,1,2012
D12-1008,"finally, we can confirm the human results obtained from an overhearer-style evaluation in a real interactive setting and explicitly extend our language model to discourse phenomena such as pauses or hesitations to take them into account in measuring id.",4,2012
D12-1008,"given that id is a measure influencing human language production, we could replace our template-based surface realizer by an agent that optimizes the information density of its output.",1,2012
D12-1008,"currently we learn the agent鈥檚 behavior offline, before the interaction, and then execute it statistically.",1,2012
D12-1008,more adaptivity towards individual users and situations could be achieved if the agent was able to learn from ongoing interactions.,1,2012
D12-1008,Future work can take several directions.,6,2012
D12-1009,a drawback is that the time complexity of inference as presented here is quadratic in the number of classes rather than linear.,5,2012
D12-1009,improving this may be the subject of future research.,5,2012
D12-1009,"another potential avenue of future work is to model transitions such that a dirichlet prior for the class distribution of a block, rather than the class distribution itself, depends on the previous class assignments.",1,2012
D12-1009,"this would yield a model that more closely resembles lda, but with topic priors that encode sequence information.",1,2012
D12-1011,"the initial success of these open-db ned approaches indicates that this task is a promising area for future research, including exciting extensions that link large numbers of domainspecific databases to text.",5,2012
D12-1012,"other interesting directions for future work are introducing more constructs in our framework, and applying our techniques to other languages.",2,2012
D12-1012,"therefore, we believe more work is required to devise a more comprehensive quantitative measure for interpretability, and refine our techniques in order to increase the interpretability of induced rules.",1,2012
D12-1013,"for the future work, we are interested in applying our co-selecting approach to active learning for other imbalanced classification tasks, especially those with much higher imbalanced ratio.",4,2012
D12-1015,"the immediate extension of our work is to polish each component of this framework, such as improving the accuracy of query expansion and pseudo context acquisition, using other effective polarity computing methods for each context and so on.",1,2012
D12-1015,"in addition, we will explore other query expansion strategies to generate more effective contexts.",1,2012
D12-1016,"using this set of alignments, we will then proceed to exploit contextual information in order to learn a semantic model for discourse coherence in argument structure realization.",1,2012
D12-1016,"in future work, we will enhance our model by incorporating more refined similarity measures including discourse-based criteria.",1,2012
D12-1016,"we will further explore tuning techniques, e.g., a more suitable preselection method for edges in graph construction, in order to increase either precision or recall.",1,2012
D12-1019,"we are also working on extending it to learn more powerful grammars e.g. split head-automata grammars (shag) (eisner and satta, 1999).",1,2012
D12-1019,"as future work, we are working on building an end to-end parser which would involve coming up with a spectral version of the inside-outside algorithm for our setting.",1,2012
D12-1021,we also expect that expanding our sampler beyond strict binary sampling may allow us to explore the space of hierarchical word alignments more quickly allowing for faster mixing.,4,2012
D12-1021,future work includes the obvious extension to learning scfgs that contain multiple nonterminal instead of a single nonterminal grammar.,1,2012
D12-1021,we expect with these extensions our model of grammar induction may further improve translation output.,1,2012
D12-1022,"in the future, we also plan to test the ability of the model to adapt to other multi-document summarization tasks, where the location of summary information is not as regular as it is in news articles.",1,2012
D12-1022,we would also like interface our model with sentence ordering and more generally with some notion of the coherence of the generated summary.,1,2012
D12-1023,"interestingly, even with the richest model, in some cases we found that the dependency length feature still appears to go too far in minimizing dependency length, suggesting that further counter-balancing features specially ones for the sentence-initial position (filippova and strube, 2009) arrant investigation in future work.",5,2012
D12-1024,"we intend to employ the rouge score as the score function in future work, and obtain the parameters of the state value function.",5,2012
D12-1024,"using these results, we will attempt to obtain a single learned policy by employing the rouge score or human evaluations as rewards.",1,2012
D12-1024,we also intend to consider efficient features and a score to achieve stable convergence.,1,2012
D12-1024,"in addition, we plan to use other methods of function approximation, such as rbf networks.",1,2012
D12-1025,we will continue to work in scenarios where large amount of monolingual data is readily available.,2,2012
D12-1025,"in the future, we will work with more language pairs, especially those with significant word re-orderings.",2,2012
D12-1026,"in the future work, we will focus on modeling intratense variation according to specific sentence types and using more features to improve it.",1,2012
D12-1027,"finally, we plan experiments with other language pairs and application to other linguistic problems.",2,2012
D12-1027,"in future work, we would like to add word deletion, insertion, splitting, and concatenation as allowed editing operations.",5,2012
D12-1027,we further want to explore tighter integration of word-based and phrase based paraphrasing.,1,2012
D12-1028,we would also like to employ lexicalized models that should help in situations in which the pos tags are too coarse.,1,2012
D12-1028,"furthermore, we would like to get rid of manually designed pos tags and use some kind of unsupervised clusters in order to have all the annotation process completely unsupervised.",1,2012
D12-1028,"finally, we would like to move towards deeper syntactic structures, where the tree would be formed only by content words and the function words would be treated in a different way.",1,2012
D12-1028,"in future work, we would like to estimate the hyperparameters automatically.",1,2012
D12-1029,"other research directions involve investigating the set of non-projective arcs allowed by non-projective buffer transitions, defining different variants of buffer transitions (such as nonprojective buffer transitions that work with nodes located deeper in the buffer) or using projective and non-projective buffer transitions at the same time.",5,2012
D12-1029,"therefore, future work will include an evaluation of the impact of buffer transitions on more transition based parsers.",1,2012
D12-1032,"for names, this could include learning common nicknames (nonparametrically); explicitly modeling abbreviation processes such as initials; conditioning on name components such as title and middle name; and transliterating across languages.14another future direction would be to incorporate the context of tokens in order to help reconstruct which tokens are coreferent.",1,2012
D12-1032,one direction for future work would be more sophisticated transduction models than the one we developed in 搂4.,1,2012
D12-1032,"for example, we might extend the generative story to generate a context for token (e, w) conditioned on e. combining contextual similarity with string similarity has previously proved very useful for identifying cognates (schafer and yarowsky, 2002; schafer, 2006b; bergsma and van durme, 2011).",1,2012
D12-1033,"however, further experimentation is required on other measures of syntactic complexity (e.g. dlt, gibson (2000)) as well as other levels of representation such as the semantic level.",1,2012
D12-1035,"first, questions with aggregations cannot be handled at this point.",5,2012
D12-1035,"second, queries sometimes return empty answers although they perfectly capture the original question, but the underlying data sources are incomplete or represent the relevant information in an unexpected manner.",5,2012
D12-1035,"we plan to extend our approach of combining structured data with textual descriptions, and generate queries that combine structured search predicates with keyword or phrase matching.",1,2012
D12-1035,future work includes relaxing some of the limitations that our current approach still has.,1,2012
D12-1036,this will be done by incorporating more nlp features in salience and coherence weights estimation.,6,2012
D12-1036,"in the future, we will explore the more sophisticated nlp features to improve the proposed framework.",1,2012
D12-1037,we will test our method on other translation models and larger training data.,2,2012
D12-1037,"in the future work, we will further investigate the local training method, since there are more room for improvements as observed in our experiments.",1,2012
D12-1038,"as future works, we will investigate the acceleration of the iterative training and the weight parameter tuning, and extend the optimized annotation transformation strategy to joint chinese word segmentation and pos tagging, parsing and other nlp tasks.",1,2012
D12-1039,"in the future, we intend to improve our dictionary by leveraging the constantly-growing volume of microblog data, and considering alternative ways to combine distributional and string similarity.",2,2012
D12-1039,"in addition to direct evaluation, we also want to explore the benefits of applying normalisation for downstream social media text processing applications, e.g.event detection.",4,2012
D12-1040,"in the future, we would like to develop a better lexicon learner since our pcfg approach critically relies on the quality of the learned lexicon.",2,2012
D12-1040,"in addition, we want to investigate the use of discriminative reranking (collins, 2000), which has proven effective in various other nlp tasks.",4,2012
D12-1040,"we would expect the final mr output to improve if a discriminative model, which uses additional global features, is used to rerank the top-k parses produced by our generative pcfg model.",1,2012
D12-1040,"particularly, we would like to investigate how syntactic information (such as part-of-speech tags induced using unsupervised learning) could be used to improve semantic-lexicon learning.",1,2012
D12-1041,"for example, fdts can be used in a prereordering framework.",5,2012
D12-1041,several potential research topics can be explored in future.,5,2012
D12-1041,we also plan to adapt our fdt-based model training approach to scfg-based and traditional left-to-right phrase-based systems.,1,2012
D12-1043,some further experiments are thus needed to investigate which of these facts better account for our findings on the semantic features.,3,2012
D12-1043,"further investigation on this issue would definitely be worthwhile, since several facts could explain these contradictory findings.",5,2012
D12-1043,"a last avenue of research worth mentioning would be to develop the family of specific-to-ffl predictors, to determine whether taking into account the impact of a given l1 language on the readability of l2 texts would increase performance over a generic model enough so that tuning efforts are worthwhile.",1,2012
D12-1045,"for example, document clustering and coreference resolution can be solved jointly, which we expect would improve both tasks.",1,2012
D12-1045,"furthermore, our iterative coreference resolution procedure (algorithm 1) could be modified to account for mention ordering and distance, which would allow us to include pronominal resolution in our joint model, rather than addressing it with a separate deterministic sieve.",1,2012
D12-1045,"however, our model can be improved.",1,2012
D12-1046,we will also investigate methods for joint learning as well as ways to speed up the joint decoding algorithm.,1,2012
D12-1046,"in the future work, we will compare this joint model to the pipeline approach that uses multiple candidates or soft decisions in the early modules.",1,2012
D12-1047,"first, we will utilize our approach to mine large-scale corpora by distributed infrastructure system, and investigate the use of our approach for other domains, such as speech translation system.",4,2012
D12-1047,"second, the significant improvement of lm adaptation based on cross-lingual data selection is exciting, so it will be instructive to explore other knowledge based cross-lingual data selection for lm adaptation, such as latent semantic model.",1,2012
D12-1049,"the current gibbs sampler is slower than regular lda, so future work is to speed up the algorithm.",1,2012
D12-1050,the problem of finding the right methods of vector composition cannot be pursued independent of the choice of lexical representation.,5,2012
D12-1050,"having tested many model combinations, we argue that in a good model of distributive semantics representation and composition must go hand in hand, i.e., they must be mutually learned.",1,2012
D12-1051,"secondly, we plan to apply our approach to other joint segmentation and labeling tasks, such as clause identification and named entity recognition.",4,2012
D12-1051,"firstly, we will explore applying external information, such as semantic knowledge, to represent the chunk-level features, and then incorporate them into our model to improve the performance.",1,2012
D12-1053,"furthermore, we also want to investigate the use of srl approaches for high-relational domains, and make a clear comparison with related techniques.",4,2012
D12-1053,"in future work, we intend to explore additional ways to incorporate background knowledge in a declarative way, since it renders the language learning problem more intuitive and gives a better understanding of feature contribution.",1,2012
D12-1056,integrating data driven error correction feature with these advanced features for the benefit of users is the challenge we face in the next step.,1,2012
D12-1057,we will investigate additional applications of excitation in future work.,4,2012
D12-1057,"for instance, we expect that excitation and its related semantic knowledge acquired in this study will improve the performance of why-qa system like the one proposed by oh et al.(2012).",1,2012
D12-1058,"finally, the developed paraphrase collection will be attested through applications, such as sentence compression (cohn and lapata, 2008; ganitkevitch , 2011) and machine translation (callison-burch , 2006; marton , 2009).",3,2012
D12-1058,"other interesting questions related to the work presented here are, as mentioned in section 4.2, exploitation of patterns with more than one variable, learning curve experiments with different amounts of monolingual data, and comparison of in-domain and general-purpose monolingual corpora.",5,2012
D12-1058,we will therefore investigate similarity metrics in our future work.,1,2012
D12-1058,"second, we have an interest in exploiting sophisticated paraphrase patterns; for instance, by inducing patterns hierarchically (recursively) and incorporating lexical resources such as those exemplified in (4).",1,2012
D12-1059,"in future, we plan to exploit this feature: when estimating the category-based alignment, we can interpolate predictions of multiple categories to which a word belongs, weighted by its probabilities associated with membership in each category.",1,2012
D12-1060,"finally, as all our conclusions have been drawn on a data set of 12 domain pairs, we plan to increase a number of domains to verify our findings on larger data sets.",3,2012
D12-1060,as a result of this research we have identified the following future directions.,6,2012
D12-1060,another research direction will focus on the integration of sfa into the similarity measure to overcome the problem of lexical discrepancy in the source and target domains.,1,2012
D12-1060,we expect that this modification will diminish the number of 鈥渂ad鈥 neighbors and allow us to reveal a dependency of similarity threshold on some domain properties.,1,2012
D12-1060,"first, we plan to improve the rank performance by choosing the number of neighbors on the basis of the document similarity threshold which we set equal for both in-domain and cross-domain neighbors.",1,2012
D12-1062,"while our experiments suggest that the temporal classifiers can potentially help enhance the performance of event coreference, in future work we would like to investigate into coupling event coreference with other components in a global inference framework.",1,2012
D12-1063,dbms are a step in the direction towards modeling constituent boundaries jointly with head dependencies.,1,2012
D12-1063,"further steps must involve more tightly coupling the two frameworks, as well as showing ways to incorporate both kinds of information in other state-of-the art grammar induction paradigms.",1,2012
D12-1064,"future work will involve a broader exploration of the parameter space of the adaptor grammars, in particular the number of topics and the value of 伪; a look at other non-parametric extensions of pcfgs, such as infinite pcfgs (liang et al., 2007) for finding a set of non-terminals permitting more fine-grained topics; and an investigation of how the approach can be extended to semi-supervised learning to take advantage of the vast quantity of texts with errors available on the web.",1,2012
D12-1065,it would be interesting to extend our model to structures beyond linear chains to trees and other structures.,1,2012
D12-1066,"lastly, we plan to apply such knowledge in text-to-text applications.",4,2012
D12-1066,our avenues for future work lie in three main areas.,6,2012
D12-1066,"the first one is to continue our current line of work and study the impact of additional individual acquisition techniques and better characterizations of paraphrases in context, in tandem with working on identifying parallel text pairs in large corpora.",1,2012
D12-1066,another avenue is to start from the output of high recall techniques and to attempt to characterize the contexts of possible substitution for candidate paraphrases from large corpora as a means to acquire precise paraphrases.,1,2012
D12-1068,"similarly, to extend the diagonal mahalanobis matrix to the general covariance matrix is also desirable.",1,2012
D12-1068,"last but not least, to find a more systematical way to determine the optimal k in the proposed method is also our possible future work.",1,2012
D12-1068,it would be interesting to consider the pure unsupervised tasks that have no any target annotations.,1,2012
D12-1068,"besides, to develop some better ways for document-level representation, e.g., incorporating the domain knowledge, also deserves our attentions.",1,2012
D12-1069,"in the future, we plan to increase the expressivity of our parser鈥檚 meaning representation to capture more linguistic and semantic phenomena.",1,2012
D12-1069,"in this fashion, we can make progress toward broad coverage semantic parsing, and thus natural language understanding.",1,2012
D12-1070,"even though the objective of our work is for speech recognition, our proposed cross-lingual language modeling can be easily applied to speech translation of other language pairs for efficient direct decoding from source speech to target text.",4,2012
D12-1071,"and evaluate the resulting resolver on standard evaluation corpora such as muc, ace, and ontonotes.",3,2012
D12-1071,"in addition, we plan to integrate our resolver into a general-purpose coreference system",1,2012
D12-1072,"for the task more broadly, it would be beneficial to compare methods of finding indirect and mixed quotes, and to evaluate how well quote attribution performs on those quotes as opposed to just direct quotes.",3,2012
D12-1072,we will also explore other approaches to representing quote attribution with a crf.,1,2012
D12-1072,"in future work, we intend to further explore the sequence features that have a large impact on accuracy, and to find similar features or proxies for the sequence features that would be beneficial.",1,2012
D12-1073,"meanwhile we will also apply sshlda to other media forms, such as image, to solve related problems in these areas.",4,2012
D12-1073,"in the future, we will continue to explore novel topic models for hierarchical labeled data to further improve the effectiveness;",1,2012
D12-1076,"in addition, graphical model, which has been studied in academic author disambiguation, may be a good choice to cope with the noises and non-standard forms in web data.",1,2012
D12-1076,jointly modeling entity linking and person (entity) disambiguation tasks will be an interesting direction where the two tasks are closely related and usually need to be considered at the same time.,1,2012
D12-1076,"investigating the person name disambiguation task in different web applications will also be of great importance, e.g., disambiguating a name in streaming data or during knowledge base construction.",1,2012
D12-1078,"in future, we will explore more alternatives in integrating parsing information and alignment information, such as discriminative word alignment using a lot of features from parser.",1,2012
D12-1080,a prime direction for future work is combining our model with a probabilistic relation extraction system.,1,2012
D12-1080,"furthermore, the consistency component can be extended with new question types to incorporate non-temporal constraints as well.",1,2012
D12-1080,this could be accomplished by using the marginal probabilities on the extracted relations and multiplying them with the probabilities from the classifier and consistency components.,1,2012
D12-1080,inference would require an additional step which could add or drop candidate fluents.,1,2012
D12-1081,"we consider that the notion of constancy will even be beneficial in acquiring world knowledge, other than relations between entities, from text; we aim at extending the notion of constancy to other types of knowledge involving real-world entities, such as concept-instance relations.",4,2012
D12-1081,"we also plan to start a spin-off research that acquires paraphrases by grouping values of arg2s for each value of arg1 in a constant, unique relation.",1,2012
D12-1081,we will utilize the identified properties of the relations to adopt an appropriate strategy to compile their instances.,1,2012
D12-1082,"for detecting new entities, we are interested in seeing if timestamped twitter data could be analyzed to increase both recall and precision.",5,2012
D12-1082,an area we are continuing to improve the system on is textual ambiguity.,1,2012
D12-1082,"last, we would like to feed back our system output to improve system performance.",1,2012
D12-1082,"for predicting semantic types, (kozareva et al., 2011) proposed additional techniques which we have not fully explored.",1,2012
D12-1082,"also, we can incorporate additional signals such as shared term heads when they are available, in order to help find terms that are likely to share types.",1,2012
D12-1082,"we have ideas for how to detect ambiguous entities using mutual exclusion (carlson, 2010) and functional relations.",1,2012
D12-1082,"we also plan to continue improving our techniques, as there is still plenty of room for improvement on both subtasks.",1,2012
D12-1082,"for example, non-entity noun phrases that make it to the typing step might lead to particular predicted type distributions that indicate an error occurred earlier in the process.",1,2012
D12-1083,"in ongoing work, we plan to generalize our dcrf-based parser to multi-sentential text and also verify to what extent parsing and segmentation can be jointly performed.",3,2012
D12-1083,"once we achieve similar performance on graph structures, we will perform extrinsic evaluation to determine their relative utility for various nlp tasks.",3,2012
D12-1083,"a longer term goal is to extend our framework to also work with graph structures of discourse, as recommended by several recent discourse theories (wolf and gibson, 2005).",1,2012
D12-1084,"for future work, we plan to use msa to align single clauses rather than whole sentences.",4,2012
D12-1084,"in a long-term view, it would be interesting to see how aligned discourse trees could help to extract paraphrases from arbitrary parallel text.",5,2012
D12-1084,"additionally, we plan to generalize the method for other parallel texts by preprocessing them with a temporal classifier.",1,2012
D12-1084,"in a more advanced step, we will also use the aligned paraphrases to help resolving discourse structure, e.g. for coreference resolution, which could lead to a high-performance bootstrapping system.",1,2012
D12-1087,"for applications in which a human end-user will interact with learned topics, the flexibility of lda and the coherence advantages of lda warrant strong consideration.",1,2012
D12-1088,"as future work, we would like to combine our approach with significance pruning, since both approaches are orthogonal and address different issues.",1,2012
D12-1088,we also plan to improve the pruning step of our algorithm to find the optimal set of phrase pairs to prune given the pruning threshold.,1,2012
D12-1089,"the entropy pruning criterion could be applied to hierarchical machine translation systems (chiang, 2007).",4,2012
D12-1089,we could also include other phrase models such as p( 锟絝|e) and the language model.,1,2012
D12-1089,"we might obtain a better estimate by also considering the distortion costs, which penalize reordering.",1,2012
D12-1092,"in future work, we will focus on how to introduce the discourse information into the individual classifiers to capture those long-distance features and joint learning of subtasks in chinese event extraction.",1,2012
D12-1093,"second, relation paths that contain constant nodes (lexicalized features) and conjunction of random walk features are potentially very useful for extraction tasks.",1,2012
D12-1093,"first, bidirectional search from both query and target nodes can be an efficient way to discover long paths.",1,2012
D12-1097,how to use lexical cohesion devices appropriately instead of frequently is thus an important issue to tackle before we can adopt them in mt and mt evaluation by a suitable means.,5,2012
D12-1097,"our future work will continue to explore the relationship of lexical cohesion to translation quality, so as to identify, apart from its use frequency, other significant aspects for mt evaluation at the document level.",5,2012
D12-1098,"moreover, we will apply graph theoretic models on graphs constructed using flag for solving a large variety of nlp applications.",4,2012
D12-1098,"in future, we will apply flag to construct graphs using several kinds of contexts like lexical, semantic, syntactic and dependency relations or a combination of them.",1,2012
D12-1099,"first, we plan to compare our system with supervised systems to identify the gap between the two systems.",3,2012
D12-1099,"second, as in (poon and domingos, 2010), we plan to explore a joint learning method to combine the tasks of tokenization, forming the main concept cluster and forming the attribute clusters; these tasks depend on the outputs of one another.",1,2012
D12-1099,"to improve our system further, we plan the following works.",1,2012
D12-1099,"finally, we plan to explore that external knowledge resources such as dbpedia (auer et al., 2007) and freebase (bollacker et al., 2008) can be used to further improve performance.",1,2012
D12-1100,"however, it may be challenging to combine this with conservative update.",5,2012
D12-1100,"future work is to reduce the bit size of each counter (instead of the number of counters), as has been tried for other summaries (talbot and osborne, 2007; talbot, 2009; van durme and lall, 2009a) in nlp.",1,2012
D12-1101,"motivated by our positive results, we will also study the application of this approach to other approximate inference techniques, such as belief propagation and variational inference.",4,2012
D12-1101,"in particular, we will explore dynamic sampling, in which we sample fewer factors during the initial, burn in phase, but sample more factors as we get close to convergence.",1,2012
D12-1101,"based on the ideas presented in the paper, we will consider additional sampling strategies.",1,2012
D12-1101,"since training is often a huge bottleneck for information extraction, we will also explore its applications to parameter estimation.",1,2012
D12-1102,one direction of future work is to take advantage of the fact that the inference problem can be split into smaller sub-problems.,6,2012
D12-1104,"we would also like to investigate the enhanced interplay of information extraction and pattern extraction, and possible applications for question answering.",1,2012
D12-1104,an interesting future direction is to study this generalized setting.,1,2012
D12-1106,we also want to analyze genre differences to understand if the strength of these coherence dimensions varies with genre.,5,2012
D12-1106,We plan to explore these ideas in future work.,6,2012
D12-1108,"we believe that smt research has reached a point of maturity where discourse phenomena should not be ignored any longer, and we consider our decoder to be a step towards this goal.",6,2012
D12-1110,"it generalizes several models in the literature, can learn propositional logic, accurately predicts sentiment and can be used to classify semantic relationships between nouns in a sentence.",1,2012
D12-1113,addressing the aforementioned challenges is a subject for future work.,6,2012
D12-1114,we will also attempt to introduce more predicates and transform structure learning techniques for mln into coreference problems.,1,2012
D12-1114,in the future we will try to design more global constraints and explore deeper relations between training instances generation and mention clustering.,1,2012
D12-1115,"second, we will experiment with a two-stage resolution approach.",3,2012
D12-1115,"finally, during annotation, we noted a number of issue patterns (e.g., an open question is x, x is under debate); a possible extension is extracting issues and problems from text using these patterns as seed patterns.",4,2012
D12-1115,a number of extensions are planned for this work.,6,2012
D12-1115,"first, we will extend the work to resolve other abstract anaphors (e.g., this decision, this problem).",1,2012
D12-1115,"third, we would like to explore the effect of including serious discourse structure features in our model.(the feature sets sc and c encode only shallow discourse information.)",1,2012
D12-1118,"in addition, using larger corpora would allow us to have more comprehensive doubly-hierarchical language models (wood and teh, 2009).",2,2012
D12-1118,"we are also interested in adding richer models of opponents to the state space that would adaptively adjust strategies as it learned more about the strengths and weaknesses of its opponent (waugh et al., 2011).",1,2012
D12-1119,can we learn class-bias for unsupervised domain adaptation?,5,2012
D12-1119,limiting the multi-domain improvements to a small set of parameters raises an interesting question: can these parameters be adapted to a new domain without labeled data?,5,2012
D12-1120,"important considerations for future work include identifying further effective and tractable biases, and extending beyond sequence-labeling to other types of nlp tasks.",4,2012
D12-1121,"for future work, we plan to combine unambiguity regularization with other types of priors and regularizations for unsupervised grammar learning, to apply it to more advanced grammar models, and to explore alternative formulations of unambiguity regularization.",1,2012
D12-1122,"also, we will apply our model to additional opinion analysis tasks such as fine-grained opinion summarization and relation extraction.",4,2012
D12-1122,"in future work, we hope to explore better ways of utilizing parsing information with less cost.",1,2012
D12-1123,and we think that it鈥檚 useful to add some prior knowledge of opinion words (sentiment lexicon) in our model for estimating candidate opinion relevance.,3,2012
D12-1123,"meanwhile, we will add some syntactic information into wtm to constrain the word alignment process, in order to identify opinion relations between words more precisely.",1,2012
D12-1123,"in future work, we plan to use other word alignment methods, such as discriminative model (liu et al., 2010) for this task.",1,2012
D12-1123,"moreover, we believe that there are some verbs or nouns can be opinion words and they may be helpful for opinion target extraction.",1,2012
D12-1124,"more broadly, this paper is an example of using extrinsic variables to drive model-building for linguistic data, and future work might explore richer extrinsic variables toward a goal of task-driven notions of semantics.",2,2012
D12-1126,"we would also like to investigate these features in more applications of natural language processing, such as name entity recognition, information extraction, etc.",1,2012
D12-1126,"for future works, we plan to improve our approximate tagging algorithm to reduce error propagation.",1,2012
D12-1126,"in addition, we will refer to an english dictionary to generate some useful features to distinguish between 鈥淣r鈥 and 鈥淣n鈥 in chinese-english mixed texts and add some statistical features derived from english resources, such as the most common tag of each english word.",1,2012
D12-1127,"it would be very interesting and perhaps necessary to incorporate this additional data in order to tackle challenges that arise across a larger number of language types, specifically non-european languages.",4,2012
D12-1128,"in addition, we plan in the very near future to generalize our multilingual joint approach and apply it to high-end tasks such as multilingual textual entailment (mehdad , 2011) and sentiment analysis (lu , 2011) 鈥 so as to provide a general framework for knowledge-rich multilingual nlp.",4,2012
D12-1129,"additionally, our framework can be applied to any language of interest, provided enough glossaries are available online, by simply translating the keywords used for our queries.",4,2012
D12-1130,"a natural avenue for future work would be to develop semantic representation models that exploit perceptual data that is both naturally occurring and easily accessible (e.g., images, physical simulations).",1,2012
D12-1130,in the future we plan to experiment with feature selection methods in an attempt to represent perceptual information more succinctly.,1,2012
D12-1131,"in future work, we intend to explore efficient techniques for joint parameter learning for both the global mrf and the local models.",1,2012
D12-1133,in future work we intend to explore joint models that incorporate not only basic part-of-speech tags but also more fine-grained morphological features.,1,2012
D12-1134,3) modeling multi-modality data.,2,2012
D12-1134,4) evaluation of the identified bursts.,3,2012
D12-1134,1) variable-length context.,1,2012
D12-1134,2) incorporation of more useful features.,1,2012
D12-1136,our analysis (entropy estimates along with upper-bound numbers observed from experiments) suggest that there can be interesting future work to explore the contextual information provided by the stimulus more effectively and further improve the response completion task.,1,2012
D12-1138,"we also verify that through the use of rich features, we can further improve the accuracy of our query spelling correction system.",3,2012
D13-1002,"in future work, we will like to explore how to better exploit the various discourse analysis frameworks for temporal classification.",5,2013
D13-1002,we believe it is interesting to examine how such information can help.,5,2013
D13-1002,"we are also interested to apply discourse features in the context of a global inferencing system (yoshikawa et al., 2009; do et al., 2012), as we think such analyses will also benefit these systems as well.",1,2013
D13-1003,we think that the good results motivate research into more integrated combinations of noise reduction approaches.,1,2013
D13-1005,"the model’s correspondence with human behavioral results is by no means exact, but we believe these kinds of predictions might help guide future research on infant phonetic and word learning.",1,2013
D13-1006,"our method has the added advantage of interpretability, which we believe will be useful when using it as a component in a larger system.",4,2013
D13-1007,future work may consider whether sequential monte carlo can offer similar advantages in other unsupervised nlp tasks.,4,2013
D13-1007,"an additional benefit of our joint statistical approach is that it may be combined with other downstream language processing tasks, such as part-of-speech tagging (gimpel , 2011) and named entity resolution (liu , 2012b).",4,2013
D13-1008,"in future work, we shall attempt to build normalizations for other languages.",4,2013
D13-1008,"we shall also attempt to learn an unsupervised normalization model with only monolingual data, similar to the work for mt in (ravi and knight, 2011).",1,2013
D13-1009,"we also will investigate non-technical areas, where there might be no strongly distinct notion of experts and non-experts.",4,2013
D13-1009,"in the future, we would like to explore predicting question difficulty from the text of question descriptions.",5,2013
D13-1011,"in the future, we plan to investigate alternative training data selection techniques, disfluency handling strategies, search heuristics, and novel transduction grammar induction models.",1,2013
D13-1012,"in future work, the proposed framework could readily be extended to model other aspects of scientific influence, such as the effects of authors and journals on topical influence, and to exploit the context in which citations occur.we envision that this line of work will also be useful for building visualization tools to help researchers explore scientific corpora.",4,2013
D13-1013,"another challenge is related to the parser speed, since the number of candidates and features are much greater than the number used in classical dependency parsers.",1,2013
D13-1013,"in future work, we will explore different learning algorithms which can help us address the sparsity problem and improve the model accuracy.",1,2013
D13-1014,"• experiments on other semantics tasks, such as paraphrase detection, word sense induction, and word meaning in context.",3,2013
D13-1014,• explore further the potential synergy between distributional semantics and the generative lexicon.,5,2013
D13-1014,"• experiments on other semantics tasks, such as paraphrase detection, word sense induction, and word meaning in context.• extension to more holistic sentence-level composition using a matrix-vector recursive framework like (socher et al., 2012).• explore further the potential synergy between distributional semantics and the generative lexicon.",1,2013
D13-1015,"in the meantime, we hope that the results we reported here provide convincing evidence of the usefulness of compositional distributional semantics in tackling topics, such as recursive adjectival modification, that have been of traditional interest to theoretical linguists from a new perspective.",5,2013
D13-1015,"in our future work, we would like to develop an order model that exploits semantic, metrical and lexicalization features jointly for maximal classification accuracy.",1,2013
D13-1017,we are comparing the performance of a single measure (say spmid or pmis) against the best measure for each task.,3,2013
D13-1018,"as future work, we plan to study the ability of protodog to acquire domain glossaries at different levels of specificity (i.e., domains vs. subdomains).",3,2013
D13-1018,"finally, we will adapt protodog to other languages, by translating the glossary keyword used in step (2), along the lines of (de benedictis , 2013).",4,2013
D13-1019,"in the future, we plan to apply this technology to develop asrs for more languages.",4,2013
D13-1020,if mctest is used we will collect more story sets and will continue to refine the collection process.,2,2013
D13-1020,"we plan to use this dataset to evaluate approaches for machine comprehension, but are making it available now so that others may do the same.",3,2013
D13-1020,one interesting research direction is ensuring that the questions are difficult enough to challenge state-of the-art techniques as they develop.,1,2013
D13-1020,removing such questions may increase the difficulty for machines as well.,1,2013
D13-1020,we will also experiment with timing the raters as they answer questions to see if we can find those that are too easy for people to answer.,1,2013
D13-1020,one idea for this is to apply existing techniques automatically during story set creation to see whether a question is too easily answered by a machine.,1,2013
D13-1020,"by requiring authors to create difficult questions, each data set will be made more and more difficult (but still answerable by humans) as the state-of-the-art methods advance.",1,2013
D13-1020,"additionally, any divergence between how easily a person answers a question vs. how easily a machine does may point toward new techniques for improving machine comprehension; we plan to conduct research in this direction as well as make any such data available for others.",1,2013
D13-1021,future work will investigate the proposed method in other domains and language pairs.,4,2013
D13-1021,it is also worth extending the approach into word alignment in statistical machine translation.,4,2013
D13-1023,"while dalm has outperformed state-of-the-art language model implementations methods in our experiments, we should continue to consider ways to optimize the method for higher-order ngrams.",1,2013
D13-1023,"in future work, we will develop a faster algorithm for building double-array structures.",1,2013
D13-1025,"as the translator works and corrects the proposed translations, the translation engine will be able to make better predictions.",2,2013
D13-1025,"specifically, measures that estimate the cognitive load involve in reading, understanding and detecting an error in a translation (foster , 2002), in contrast ksmr simply considers a constant cost.",3,2013
D13-1025,"more recently, ortiz-martinez et al.(2010) described a set of techniques to obtain an incrementally updateable imt system, solving technical problems encountered in previous works.• more sophisticated measures to estimate the human effort.",1,2013
D13-1025,• adaptive translation engines that take advantage of the user’s corrections to improve its statistical models.,1,2013
D13-1026,"we would like to investigate whether further improvement can be achieved by incorporating such features, especially the context model (shen et al., 2009) in the future.",1,2013
D13-1026,"because our proposed model is quite general, we are also interested in applying this method to induce linguistically motivated synchronous grammars for syntax-based smt.",1,2013
D13-1029,"finally, we would also like to explore the extent to which a joint probabilistic model (e.g., (durrett and klein, 2013)) might be used to learn how to best make this tradeoff.",1,2013
D13-1030,"first, we plan to use both kinds of data, csn and asn antecedent data, which will give us a basis for developing a better performing asn resolver.",2,2013
D13-1030,"that said, for the same nouns, the antecedents were in the first four ranks about 76% to 81% of the times, suggesting that in future research, our models can be used as base models to reduce the large search space of asn antecedent candidates.",1,2013
D13-1030,"finally, we will examine whether a model trained for one shell noun can be generalized to other shell nouns from the same semantic category.",1,2013
D13-1030,"we also plan to incorporate contextual features (e.g., right-frontier rule (webber, 1991) and context ranking (eckert and strube, 2000)).",1,2013
D13-1031,"as our future work, we plan to apply our method to other natural language processing tasks, such as text chunking.",4,2013
D13-1033,making use of the improved morphology in the dependency parser is not straight-forward and requires more investigation in the future.,5,2013
D13-1036,in future work we want to explore in more detail the differences in performance of the different contingency measures.,3,2013
D13-1036,"in future work we also want to explore ways of inducing larger event structures than event pairs, such as the causal chains, scripts, or narrative schemas of previous work.",1,2013
D13-1040,"an appealing direction would be to learn these automatically e.g., via a procedure that optimizes some clustering objective.",1,2013
D13-1040,"in the future, we would like to explore additional types of rules such as seed rules, which would assign tuples complying with the “seed” information to distinct relations.",1,2013
D13-1040,"aside from devising new rule types, an obvious next step would be to explore different ways of extracting the rule set based on different criteria (e.g., the most general versus most specific rules).",1,2013
D13-1040,"finally, it should be interesting to use some form of distant supervision (mintz et al., 2009) either as a means of obtaining useful rules or to discard potentially noisy or uninformative rules.",1,2013
D13-1046,"equally important, the size of the domain can be adapted so as to find enough context for all the words in domain reference lists.",2,2013
D13-1046,"given that comparability of article versions in the source and the target language varies, we will evaluate algorithms for filtering out concepts from the target language that have low alignment with their source language versions.",1,2013
D13-1046,"second, given a word in a context, we currently exploit all similar concepts from the target language.",1,2013
D13-1046,"first, we will pursue the integration of our method, notably through comparable corpora creation using the data driven domain delimitation technique described in subsection 3.5.",1,2013
D13-1046,a final line of work is constituted by the use of distributional properties of texts in order to automatically rank parts of concept descriptions (i.e. articles) by their relatedness to the candidate word.,1,2013
D13-1046,"similar to the second direction, this process involves finding comparable text blocks but rather at a paragraph or sentence level than at the article level.",1,2013
D13-1047,"and report results using multiple evaluation metrics (nenkova , 2007; louis and nenkova, 2012) as well as performing human evaluations.",3,2013
D13-1047,"in future, we would like to further explore the reinforcement relationship between keywords and summaries (wan et al., 2007),",1,2013
D13-1047,"improve the readability of the sentences generated from the guided compression system,",1,2013
D13-1049,"promising directions for future work are joint parsing and reordering models, and measuring the influence of parsing accuracy on preordering and final translation quality.",1,2013
D13-1050,"In the future, we plan to explore more approaches for phrase table pruning.",1,2013
D13-1051,"though, our best system may not overpass he and toutanova (2009) who combine all the modules into a unified training procedure, we believe our method could boost many work on the higher modules of the pipeline to obtain a further improvement to match their work.",1,2013
D13-1053,the general framework that uses an external reordering model in hierarchical models via features can also be naturally extended to use multiple reordering models.,1,2013
D13-1053,"it also might be beneficial to look beyond syntactic constituent pairs when modeling reordering, given that phrasal cohesion does not always hold in translation.",1,2013
D13-1054,"finally, it is possible to apply our method to other phrase-based and even syntax-based systems.",4,2013
D13-1054,another problem with our system is that the decoding speed is much slower than the baseline system because of the computational overhead introduced by raes.,1,2013
D13-1054,it is necessary to investigate more efficient decoding algorithms.,1,2013
D13-1054,"first, replacing the maxent classifier with a neural one redefines the conditions for risk-free hypothesis recombination.",1,2013
D13-1054,"therefore, we plan to use forest reranking (huang, 2008) to alleviate this problem.",1,2013
D13-1054,"second, it is interesting to follow socher et al.(2013) to combine linguistically-motivated labels with recursive neural networks.",1,2013
D13-1055,"a larger amount of labeled data would certainly help to improve the classifier performance for weak categories (e.g.vandalism and paraphrase) and sparse categories (e.g.template-d, markup-m).",2,2013
D13-1055,"with respect to future work, we plan to include more resources, e.g. the pan-wvc10 (potthast and holfeld, 2011) or wicopaco (max and wisniewski, 2010) to increase the size of training data.",2,2013
D13-1055,"based on our trained classifier, annotating more examples can be alleviated with the help of active learning.",1,2013
D13-1056,future work includes aligning discontinuous (gappy) phrases and integrating alignment more closely in nlp applications.,1,2013
D13-1060,we look forward to generalizing our approach to other types of noncompositional phrases.,4,2013
D13-1061,"(2) incorporate some common techniques, such as cascading, voting, and ensemble;",1,2013
D13-1061,and (3) use the special network architecture tailored for the tasks of interest.,1,2013
D13-1061,"although we focus on the question of how far we can go for chinese word segmentation and pos tagging without using the extra task-specific features in this study, there are at least three ways to further improve the performance of the networks, which are worthy to be explored in the future: (1) introduce specific linguistic features (e.g. gazetteer features) that are helpful for the tasks;",1,2013
D13-1062,"besides, we would also like to investigate for other nlp tasks which have different annotation-style corpora.",2,2013
D13-1062,"in the future, we will continue to refine the proposed model in two ways: (1) we wish to use the unsupervised method to extract the loose mapping relation between the different annotation standards, which is useful to the corpora without loose mapping guideline.(2) we will analyze the shared information (weights of the features derived from the tags which have the mapping relation) in detail and propose a more effective model.",1,2013
D13-1064,it would also be interesting to experiment with more diverse languages types.,2,2013
D13-1064,"embedding cluster-identifiers in a logical form allows us to also model logical operators, such as negation and quantifiers, which may help to improve the translation of these.",1,2013
D13-1064,"as we use a flat clustering of relations, we are only able to model synonyms and not hypernyms.",1,2013
D13-1064,there is much potential for future extensions to address the limitations of the process described here.,1,2013
D13-1067,"in the future work, we will explore more kinds of social context information and investigate better ways of incorporating them into profile summarization and a wider range of social network mining.",1,2013
D13-1069,"however, the data type is also important to impact the summary length.",2,2013
D13-1069,"in addition, in the experiments, we only use the imbalanced datasets as the example that intuitively needs varying the summary length.",2,2013
D13-1069,"in future we may consider more human factors, and prove the summary length determined by our system agrees with human preference.",3,2013
D13-1069,"in future, we may extend the work by studying more cases that need varying summary length.",6,2013
D13-1070,"in the future we would like to experiment further with refining the sentence selection method so as to consider criteria for local cohesion, such as lexical chains.",3,2013
D13-1070,we also would like to perform a human based evaluation of coherence and explore the full potential of these summaries as alternatives to author-written abstracts.,3,2013
D13-1070,we would also like to perform comparisons with automatically induced content models and check their viability for scientific articles.,3,2013
D13-1071,we are also interested in exploring a* heuristics to further speed up our dp best-first parsing.,1,2013
D13-1071,for future work we would like to improve the performance of the probabilistic models that is required by the best-first search.,1,2013
D13-1072,"in the future, we want to further investigate the problem of domain adaptation when applying general language models to a new image dataset.",4,2013
D13-1072,this problem can be integrated into the energy-based model during the training phase.,1,2013
D13-1072,we plan to extend work on human action recognition by including the relative position between the human and object in the images.,1,2013
D13-1073,"we hope to tackle the annotation bottleneck in future work on definition extraction, common in many data-driven learning fields.",5,2013
D13-1073,"we can use the lexicon obtained from different years to carry out trend prediction, which we have illustrated here.",1,2013
D13-1073,we think future work could pursue more in-depth analysis of the distributional and demographic properties of automatically extracted lexica.,1,2013
D13-1073,"downstream systems may predict which term will become popular, or could alert an author if their definition of a term significantly differs from the original source.",1,2013
D13-1073,"in addition, with respect to modeling, although we showed that doing definition classification before term classification does not improve over our single stage classifier, we hope to study whether suitable joint inference models can benefit from the interaction between the two classification processes.",1,2013
D13-1073,"we plan to explore iterative, semi-supervised methods to best manage human effort to maximize the effectiveness of future annotation.",1,2013
D13-1078,"but given an expression like monday, it would still be impossible to decide whether it refers to the future or the past, since the surrounding context, e.g.tense of the governing verb, is needed for such a judgment.",5,2013
D13-1078,"in future work, we plan to replace the heuristic for selecting between ambiguous parses with a more principled approach.",1,2013
D13-1078,"it would be a simple extension to support a probabilistic grammar, as in (angeli et al., 2012).",1,2013
D13-1078,a more promising approach would be to train a classifier that selects between the ambiguous parses based on features of the surrounding context.,1,2013
D13-1079,"specifically, we call for the standardization of an ie rule language and outline an ambitious research agenda for nlp researchers who wish to tackle research problems of wide interest and value in the industry.",5,2013
D13-1081,"• to replace the exhaustive translation rule setwith a compact meta grammar that can create and parameterize new translation rules dynamically, which is the ultimate goal of this line of work.",2,2013
D13-1081,"to try other prior distributions to generate the number of source tokens. unsupervised and semi-supervised learning of hidden models. to incorporate rich models into the generative process, e.g.reordering, non-terminals, structural information and lexical models. to improve the posterior model with better parameter estimation, e.g. bayesian methods.",1,2013
D13-1082,"our future work is to compare our conversion method with that of (arsoy , 2013).",3,2013
D13-1084,"future work could focus on cases with more than two languages, and languages that are typologically less distinct from each other or dialects (trieschnigg , 2012).",5,2013
D13-1087,"in addition to improving search, large-scale restarts can also provide a novel perspective when performing exploratory analysis, here letting us argue in support for the hypothesis that zodiac 340 is not a row-major homophonic cipher.",3,2013
D13-1090,"these ideas may have applicability in other semantic similarity tasks, and we are also eager to apply them to new, large-scale automatically-induced paraphrase corpora (ganitkevitch , 2013).",4,2013
D13-1091,"in future work, we intend to design new similarity measurements that can make best of the advantages of twitter data.",1,2013
D13-1095,"as future work, we plan to improve the author/reader detection model to improve the zero reference resolution.",1,2013
D13-1096,this dataset will be valuable for both training and testing automatic response models for short texts.,1,2013
D13-1098,we also intend to adapt the described system to other languages than english.,4,2013
D13-1098,"there is no doubt that reaction detection can be improved a lot, by going beyond simple lexical features and discovering specific patterns.",1,2013
D13-1099,our future work will focus on the pruning algorithm for the syntactic structures and analyzing errors in depth in order to get more effective features for the scope detection on different corpora.,1,2013
D13-1100,"we will consider incorporating periodicities in other applications, such as topic models.",1,2013
D13-1100,"in future work, we aim to model time continuously and to perform discriminative clustering in order to make better use of the learned periodicities.",1,2013
D13-1101,we will also evaluate the effect of adding a decoding step to the constituent approach.,3,2013
D13-1101,this work provides an accurate and complete quotation extraction and attribution system that can be used for a wide range of tasks in information extraction and opinion mining.,4,2013
D13-1101,"future work will include extending these methods to extract all attributions, i.e. beliefs, eventualities, and facts, as well as the source spans.",1,2013
D13-1103,"in the future, we plan to extend this work to more precisely pinpoint the answer location on a page, and consequently incorporate searcher behavior into subsequent answer extraction and ranking stages of question answering.",1,2013
D13-1103,"we also plan to further investigate the examination data to better understand how searchers find correct (and incorrect) answers using both general web search engines and qa systems – in order to inform and further improve query suggestion, result snippet generation, and result ranking algorithms.",1,2013
D13-1104,for the latter we have already started collecting parallel text in russian and english.,2,2013
D13-1104,"for the future work we plan to use the corpus as a research tool to tackle the following problems: (i) automatic part of speech tagging, (ii) morphological disambiguation, (iii) statistical machine translation.",5,2013
D13-1105,"we are interested in extending our approach to languages with different morphological systems, e.g., agglutinative or reduplicative.",4,2013
D13-1105,we will explore ideas from unsupervised morphology learning to minimize the need for morphological annotations.,1,2013
D13-1106,"we would also like to apply recent advances in tackling the vanishing gradient problem (pascanu , 2013) using a regularization term to maintain the magnitude of the gradients during back propagation through time.",5,2013
D13-1106,"thus, the model itself can decide which encoding is best for the task.",1,2013
D13-1106,"finally, we would like to integrate the recurrent model directly into first-pass decoding, a straightforward extension of lattice rescoring using the algorithm we developed.",1,2013
D13-1106,"we also plan to change the cross entropy objective to a bleu-inspired objective in a discriminative training regime, which we hope to be more effective.",1,2013
D13-1106,in future work we plan to directly learn representations of the source-side during training of the joint model.,1,2013
D13-1107,"furthermore, since our approach is a general training method, we may also combine this approach with other domain adaptation methods to get more performance improvement.",4,2013
D13-1107,"for other texts, we use the general system to translate them.",1,2013
D13-1107,"for those domains that are identified with high confidence, we use the domain specific system to translate the texts.",1,2013
D13-1107,"in the future, we will pre-define more popular domains and develop automatic domain classifiers.",1,2013
D13-1108,"since constituency tree binarization can lead to more constituency-to-string rules and syntactic phrases in rule extraction and decoding, which improve the performance of constituency-to-string systems, for future work, we would like to do research on encoding binarized constituency trees to dependency trees to improve translation performance.",1,2013
D13-1109,"recent work has identified nts words in new-domain corpora (carpuat , 2013b), and in future work we plan to incorporate discovered translations for such words into mt.",2,2013
D13-1109,"although the use of marginal matching is, to the best of our knowledge, novel in mt, there are related threads of research that might inspire future work.",5,2013
D13-1110,it has been shown to be useful for hiero in some languages therefore it is promising to improve translation quality in lrhiero which suffers from lack of modeling power of non-gnf target side rules.,2,2013
D13-1110,"in future work, we plan to apply lexicalized reordering models to lr-hiero.",1,2013
D13-1110,we also plan to extend the glue rules in lr-hiero to provide a better reordering model.,1,2013
D13-1111,"future work could replace m-best lists with diverse lists in these and related tasks, whether for mt or other areas of structured nlp.",4,2013
D13-1112,"for future work, we will consider other translation paradigms such as hierarchical phrase-based or syntax-based mt.",1,2013
D13-1114,"we suspect that this result will generalize to the inference of other demographic characteristics (e.g., age and political orientation), though this must be explored in future work.",4,2013
D13-1114,"this stands out as a clear direction for future work, particularly since apparent issues with the large number of unigrams used by japanese will create issues for handling (mandarin) chinese, the world’s most-spoken language.",5,2013
D13-1114,"though there has been relatively little investigation into latent attribute inference outside of english language content, we consider it both a fruitful and important area for future research.",1,2013
D13-1114,identifying and leveraging such features will be an interesting and fruitful direction for future work.,1,2013
D13-1114,the positive results suggest that latent attribute inference in the non-english context as a research direction worthy of further attention.,1,2013
D13-1116,"as well as repeating these experiments for languages which rely more on function annotation, we also plan to apply our method to other types of annotations, e.g.more linguistically motivated binarization strategies or – of particular interest to us – annotation of empty elements.",2,2013
D13-1117,investigating how to better optimize this non-convex regularizer online and convincingly scale it to the semisupervised setting seem to be promising future directions.,5,2013
D13-1120,"finally, we would like to explore just how much the statistic properties of our data dictate the success of the model by looking at related problems like morphological analysis of unsegmented languages such as japanese.",5,2013
D13-1120,"analysis of the tagging errors still being made have suggested some possibly avoidable inconsistencies in the grammar and treebank, which have been fed back to the developers, hopefully leading to even better results in the future.",1,2013
D13-1120,"in future work, we will investigate more advanced smoothing methods to try and boost the uber tagging accuracy.",1,2013
D13-1120,"we also intend to more fully explore the domain adaptation potentials of the lexical model that have been seen in other parsing setups (see rimell and clark (2008) for example), as well as examine the limits on the effects of more training data.",1,2013
D13-1121,"thus, although this paper focused on the activepassive alternation in japanese, our framework is applicable to the other types of case alternation and to other languages, especially similar languages such as korean.",4,2013
D13-1121,we plan to apply our framework to other types of case alternation such as case alternation between intransitive and transitive verbs.,4,2013
D13-1122,"for future work, we would like to explore knowledge from more sources to enhance our model, such as semantic thesauri and info boxes in encyclopedias.",1,2013
D13-1126,our next step is to try these same techniques with spoken questions and spoken answers in a low-resource language using the test collection that is being developed for the mediaeval 2013 question answering(qa) for the spoken web task.,2,2013
D13-1126,"in the long term, many of the questions we are exploring will also has implications for open-domain web search in other hands- or eyes-free applications such as driving a car or operating an aircraft.",5,2013
D13-1126,another potentially productive direction for future work would be to somehow filter the queries in ways that improve the rankings.,1,2013
D13-1127,"as a future work, we plan to adapt parameters automatically on the basis of different types of datasets.",2,2013
D13-1127,"furthermore, our framework is universal, so that the media other than text and image can be adopted as well.",4,2013
D13-1127,improving the layout quality of story map by concerning the interactivity of different media (e.g. images order) is also significant.,1,2013
D13-1128,"future work will focus on improvements to the image parser, on exploring this representation in open domain data sets, and on using the output of an object detector to obtain a fully automated model.",1,2013
D13-1129,"the idea described in this paper is general and can be applied to other nlp applications, such as part-of-speech tagging and chinese word segmentation, in future work.",4,2013
D13-1130,"further work can be considered to improve segmentation of documents characterized by small segments and few words repetitions, such as using semantic relations or vectorization techniques to better exploit implicit relations not considered by lexical reoccurrence.",1,2013
D13-1131,"in the future, we would like to investigate other methods for generating robust inter-feature laplacians that include deep syntactic and semantic features.",1,2013
D13-1132,"for future work, we will consider incorporating a trend detection component into our method, which can be more flexible to adapt to various trend signals.",1,2013
D13-1132,we can also refine the method of the product keyword extraction by using more principled solutions.,1,2013
D13-1133,"more data (e.g.kosinski (2013)) and more sophisticated models (e.g.supervised lda, blei and mcauliffe (2008), and extensions such as nguyen (2013)) will be the key to further progress.",2,2013
D13-1133,"more data (e.g. kosinski et al.(2013)) and more sophisticated models (e.g. supervised lda, blei and mcauliffe (2008), and extensions such as nguyen et al.(2013)) will be the key to further progress.",1,2013
D13-1134,and evaluate whether this actually improves task performance.,3,2013
D13-1134,"the immediate next step for future research is to extend our model to an implemented end-to-end situated nlg system for the give challenge,",4,2013
D13-1134,we will furthermore improve pobs by switching to a more temporally dynamic probability model.,1,2013
D13-1136,"our modeling approach is general and should apply to other settings, e.g.for the task of entity linking.",4,2013
D13-1137,"as future work, we plan to combine deep learning models with richer information such as predicate argument structures.",1,2013
D13-1138,in future work we plan to apply our method to a wider range of languages.,4,2013
D13-1139,future work will investigate our methods usefulness on various language datasets.,4,2013
D13-1139,"we plan to study more general methods that use word alignments to embed swap information in trees (galley et al., 2006).",1,2013
D13-1141,"further, our results offer suggestive evidence that bilingual word embeddings act as high-quality semantic features and embody bilingual translation equivalence across languages.",1,2013
D13-1142,"based on evaluation results and despite differences between the evaluators with background knowledge and the crowds, we can conclude that that our approach for automatic construction of in-text links rivals manual creation by professional writers and bloggers and is thus a promising direction for further research.",1,2013
D13-1144,"the approach can generalize to any task involving structural and local similarity, and arbitrary node similarity measures.",4,2013
D13-1145,and evaluate it on more varied sentence types.,3,2013
D13-1145,we expect that semi-supervised learning techniques could better recover the missing labels and boost overall performance.,1,2013
D13-1145,"we also think it should be possible to scale the detection approach, perhaps with automatic dictionary definition discovery,",1,2013
D13-1146,"we further plan to test our method on a wider range of datasets, allowing a more direct comparison with other approaches.",3,2013
D13-1146,"in future work, we plan to broaden the scope of this work to other steps in document preparation,such as normalization of punctuation, and their interaction with segmentation.",4,2013
D13-1146,"finally, we plan to explore the possibility of a statistical universal segmentation model for mutliple languages and domains.",1,2013
D13-1149,"future work will explore using an individual annotators history across trials to weight that users contributions, something that verbcorner was specifically designed to allow (see above). future research will be needed to assess this tradeoff.",3,2013
D13-1152,"in future, we are interested in training parsers favoring the dynamic feature selection setting, for example, parsers that are robust to missing features, or parsers optimized for different stages.",1,2013
D13-1156,"as graph cut is a general method, applying it to other binary structured learning tasks is also an interesting direction.",4,2013
D13-1156,"the other is to adapt it to social media text summarization task, where text is much more noisy.",4,2013
D13-1156,"one idea is to apply it to the language model based compression method (clarke and lapata, 2008).",1,2013
D13-1156,there are several possibilities for further research involving our graph cut algorithms.,1,2013
D13-1157,"in the future, we would like to tackle more challenging domains, such as nfl recaps, financial articles and biographies (howald , 2013; schilder , 2013).",4,2013
D13-1157,"our models could also benefit from the development of more sophisticated planners either via grammar refinement or more expressive grammar formalisms (cohn et al., 2010).",1,2013
D13-1159,"in the future, we plan to employ document summarization techniques to shorten the depth of cets.",1,2013
D13-1159,"we also aim to incorporate semantic analysis and normalize named entities to canonical entities, which make cet more suitable for practical use.",1,2013
D13-1160,"learning these composite predicates would drastically increase the possible space of logical forms, but we believe that the methods proposed in this paper— alignment via distant supervision and bridging—can provide some traction on this problem.",4,2013
D13-1161,"although we focused exclusively on qa applications, the general two-stage analysis approach should allow for the reuse of learned grammars across a number of different domains, including robotics or dialog applications, where data is more challenging to gather.",4,2013
D13-1163,"in the future, we would like to use lexical chains to identify coherence and incorporate both cohesion and coherence into document-level machine translation.",1,2013
D13-1164,future work will consider ways to speed up our algorithm and extensions of the method to more complex alignment models.,5,2013
D13-1165,"ultimately, we would like to apply these methods to the normalization of social media text, especially to find alternative spellings based on alternative pronunciations.",4,2013
D13-1166,"as for future work, we aim to investigate the application of this procedure to the regression model of grefenstette (2013).",4,2013
D13-1167,"for instance, the knowledge encoded by mrlsa can be enriched by adding more relations from a variety of linguistic resources, including the co-occurrence relations from large corpora.",2,2013
D13-1167,"following the strategy of using siamese neural networks to enhance pilsa (yih et al., 2012), training mrlsa with a multi-task discriminative learning setting can be a promising approach as well.",1,2013
D13-1167,"on model refinement, we notice that mrlsa can be viewed as a 3-layer neural network without applying the sigmoid function.",1,2013
D13-1167,"for future work, we plan to explore directions that aim for improving both the quality and word coverage of the model.",1,2013
D13-1168,"finally, we also plan to test the robustness of our fully corpus-based bootstrapping approach by porting it to more language pairs.",3,2013
D13-1168,"furthermore, we plan to study other confidence functions and explore if asymmetric translation candidates could also contribute to the bootstrapping method.",5,2013
D13-1168,"in future work, we will investigate other models of similarity besides topicbc and responsebc (e.g., the method from (haghighi et al., 2008)) that could be used as preliminary models for constructing an initial bilingual vector space.",1,2013
D13-1171,"in future work, we hope to explore further methods for teasing apart sentiment polarity expressed towards a target.",3,2013
D13-1172,"in our future work, we plan to incorporate aspect specific sentiments in the mclda model.",1,2013
D13-1174,"therefore, an important extension of our work is to explore the interaction of our approach with more sophisticated language models that more directly model morphology, e.g., the models of bilmes and kirchhoff (2003), or, alternatively, ways to incorporate target language context in the inflection model.",1,2013
D13-1175,"while this technique is useful to develop patent retrieval systems, it would be interesting to see if our results transfer to patent retrieval scenarios where full patent documents are used instead of only abstracts, or to standard clir scenarios that use short search queries in retrieval.",5,2013
D13-1177,"last, we would like to integrate our method into qa systems and allow non-factoid questions that require deeper reasoning to be answered by matching the questions against the learned process structures.",1,2013
D13-1177,"in future work, we want to perform trigger identification jointly with extraction of event event relations.",1,2013
D13-1177,"alternatively, we can search on the web for redundant descriptions of the same process and use this redundancy to improve classification.",1,2013
D13-1178,our future plans are to build upon our event schemas to create an open-domain event extractor.,1,2013
D13-1180,"because of the diversification of writing features of essays associated with different prompts, a viable approach is to explore more generic writing features that can well reflect the writing quality.",1,2013
D13-1180,"in our future work, we plan to continue the research on generic rating model.",1,2013
D13-1182,"ultimately, we would like to correlate patterns in physician communication (as gleaned from the model) with objective, measured health outcomes (e.g., patient satisfaction and adherence to arvs).",1,2013
D13-1182,we also plan on extending this model to investigate qualitative questions surrounding patient physician communication quantitatively.,1,2013
D13-1182,"to explore this, we can add additional components to the transition probability terms corresponding to different hospitals and doctors.",1,2013
D13-1183,"in order to spur future research, we are releasing an annotated corpus of time-stamped news articles and our harvested relation clusters.",2,2013
D13-1184,"one possibility for future work is to supply this framework with a richer set of relations from the text, such as verbal relations.",1,2013
D13-1184,it will also be interesting to incorporate high-level typed relations and relax the relation arguments to be general concepts rather than only named entities.,1,2013
D13-1185,there is ample room for improvement and future research in event schema induction.,1,2013
D13-1187,"in future work we plan to develop new models for joint modeling of personalized sentiment, user demographics e.g., age and user preferences e.g., political favorites in social media.",1,2013
D13-1189,"in future work, we will try to collect and annotate data for microblogs in other languages to test the robustness of our method.",2,2013
D13-1189,the repost and reply messages can also be integrated into our graph model to help improve the results.,1,2013
D13-1192,"furthermore, the event-topic association inferred by our model can help an event recommendation task and organize events by topics.",1,2013
D13-1194,studies of this kind may also inform future data annotation efforts in that certain ways of anchoring the elements of a comparison linguistically may be more helpful than others.,3,2013
D13-1194,we also want to further study the different possible linguistic anchorings of comparisons and their effect on classification performance.,3,2013
D13-1194,"to address the inherent diversity of expressions typical for user generated content, we want to employ generalization techniques, e.g., to detect product names.",1,2013
D13-1194,"for future work, we plan to include features that have been tailored specifically to the task of detecting product comparisons.",1,2013
D13-1194,"we also believe that the explicit modeling of different types (equative, superlative, non-equal gradable) of comparisons will have a positive effect on performance.",1,2013
D13-1196,"we plan to experiment next with more linguistically motivated ways to adapt the latter to recursive composition, including hybrid methods where ans and nns are treated differently.",1,2013
D13-1196,"we would also like to consider more sophisticated semantic plausibility measures (e.g., supervised ones), and apply them to other ambiguous syntactic constructions.",1,2013
D13-1198,"in the future, we plan to investigate this finding in the context of other, similar ranking problems in natural language processing.",5,2013
D13-1200,"as future work, we plan to evaluate how the quality is impacted by the time dimension (adaptation delay, cache reset,etc.).",3,2013
D13-1202,"collecting our own fmri data will also allow us to move beyond exploratory analysis, to test sharper predictions about distributional models and their brain area correlates.",3,2013
D13-1202,in future experiments it may prove valuable to configure a fmri stimulus set where text-based and image-based interrelationships are maximally different.,3,2013
D13-1202,"there are also many opportunities for focusing analyses on different subsets of brain regions, with the semantic system identified by binder et al.(2009) in particular presenting one interesting avenue for investigation.",1,2013
D13-1202,"this not only presents an optimistic outlook for the future use of image-based models as an interpretative tool to explore issues of cognitive grounding, but also demonstrates that they are capturing useful additional aspects of meaning to the text models, which are likely relevant for computational semantic tasks.",1,2013
D13-1204,"furthermore, we are optimistic that both count transforms and model recombination could be usefully incorporated into sampling methods: although symmetrized models may have higher cross-entropies, hence prone to rejection in vanilla mcmc, they could work well as seeds in multi-chain designs;",1,2013
D13-1204,"future parsing models, in grammar induction, may benefit by modeling head-dependent relations separately from direction.",1,2013
D13-1204,"existing algorithms, such as mcmcmc (geyer, 1991), which switch contents of adjacent chains running at different temperatures, may also benefit from introducing the option to combine solutions, in addition to just swapping them.",1,2013
D14-1002,we believe that the prior browsing and interaction history recorded in the session provides additional signals for predicting interestingness.,2,2014
D14-1002,"one area of future work is to extend our method to model interestingness given an entire user session, which consists of a sequence of browsing events.",1,2014
D14-1002,"one potentially effective model for such a purpose is based on the architecture of recurrent neural networks (e.g., mikolov et al.2010; chen and deng, 2014), which can be incorporated into the deep semantic model proposed in this paper.",1,2014
D14-1002,"to capture such signals, our model needs to be extended to adequately represent time series (e.g., causal relations and consequences of actions).",1,2014
D14-1003,"besides its good performance in practice, the bidirectional architecture is of theoretical interest as it allows the exact modeling of posterior probabilities.",1,2014
D14-1004,we also want to further investigate the advantages and disadvantages of having different embedding matrices for different argument positions in our multi-way neural network.,3,2014
D14-1004,"we want to investigate the benefit of our approach, compared to a model that shares the distributed word representation among different argument positions.",3,2014
D14-1004,"first of all, we would like to investigate how our neural network approach might be improved by incorporating information from other sources.",5,2014
D14-1004,"in particular, we think of initializing our embedding matrices with distributed representations that come from a large-scale neural language model (mikolov et al., 2013).",1,2014
D14-1004,"finally, we want to investigate more advanced neural network architectures for the acquisition of selectional preferences.",1,2014
D14-1004,we conclude with a number of issues for future work.,6,2014
D14-1005,"in future work, we will investigate whether our system can be further improved by including concreteness information or a substitute metric such as image dispersion, as has been suggested by other work on multi-modal semantics (kiela et al., 2014).",1,2014
D14-1005,"furthermore, a logical next step to increase performance would be to jointly learn multi-modal representations or to learn weighting parameters.",1,2014
D14-1005,"another interesting possibility would be to examine multi-modal distributional compositional semantics, where multi-modal representations are composed to obtain phrasal representations.",1,2014
D14-1006,"for future work, we plan to extend our studies to larger corpora, to integrate our classifiers in writing environments, and to investigate their effectiveness for supporting students.",2,2014
D14-1009,"in future work we plan to include probabilistic and distributional features from a top-down incremental parser e.g. roark et al.(2009), and use stir distributional features to classify repair type.",1,2014
D14-1010,in future work we will examine partial-label learning with this more enforced lattice constraint in depth.,1,2014
D14-1011,another future research is to speed-up our model.,1,2014
D14-1012,"for future work, we are exploring a novel and a theoretically more sounding approach of introducing embedding kernel into the linear models.",1,2014
D14-1015,"in the future, we will explore more features to refine the model and try to utilize contextual information in target sentences.",1,2014
D14-1016,"in the future, we will refine the method by considering neighbor words and alignments when discarding noisy links.",1,2014
D14-1017,"as our future work, we will investigate more precise methods for deciding function words and content words for better alignment and translation qualities.",1,2014
D14-1018,"in future work, we intend to obtain training data from multiple domains that enables us to verify cross-domain scalability of pos-mtus.",3,2014
D14-1018,another future direction for this work is leveraging sentence level translation direction detection to improve statistical machine translation output quality.,1,2014
D14-1018,"in addition, observing linguistic phenomena that occur in one translation direction but not the other can be very informative in improving statistical machine translation quality.",1,2014
D14-1018,"finally, further investigation of the linguistic interpretation of individual feature that are most discriminating between opposite translation directions can lead to discovery of new linguistic phenomena that occur during the translation process.",1,2014
D14-1019,another interesting direction is to determine the appropriate number of clusters for each corpus and the initialization method for clustering.,1,2014
D14-1019,we expect to make a new objective that includes the terminal symbols and the reordering of nonterminal symbols that were ignored in this work.,1,2014
D14-1019,future work involves improving the optimization criterion.,1,2014
D14-1023,the connecting phrase-based method can also be applied to lm adaptation.,4,2014
D14-1024,"further, we will apply our linguistically informed method to other phenomena which cause similar issues for smt.",4,2014
D14-1024,"in future research, we will extend our method to other language pairs which exhibit the same type of translation asymmetries when it comes to pvs.",4,2014
D14-1024,"when it comes to mwes, we will pay special attention to the compositionality aspect since it seems to have contributed most to the good performance achieve by our method in the study presented here.",1,2014
D14-1026,"the size and diversity of the topics in the corpus, along with its relatively high annotation quality (measured by iaa scores) makes it a useful resource for future research on arabic mt.",2,2014
D14-1026,"moreover, the strong performance of our albleu metric is a positive indicator for future exploration of richer linguistic information in evaluation of arabic mt.",1,2014
D14-1027,"this would also include more semantic information, e.g., in the form of brown clusters or using semantic similarity between the words composing the structure calculated with latent semantic analysis (saleh et al., 2014b).",1,2014
D14-1027,"in the future, we plan to work towards our long term goal, i.e., including more linguistic information in the skl framework and showing that this can help.",1,2014
D14-1027,"we further want to show that the proposed framework is flexible and can include information in the form of quality scores predicted by other evaluation metrics, for which a vector of features would be combined with the structured kernel.",1,2014
D14-1030,future work the work described here is our first attempt along the promising endeavor of matching complex computational models of language with brain processes using brain recordings.,1,2014
D14-1030,we plan to extend our efforts by (1) collecting data from more subjects and using various types of text and (2) make the brain data help us with training better statistical language models by using it to determine whether the models are expressive enough or have reached a sufficient degree of convergence.,1,2014
D14-1032,"this may involve filtering the perceptual input stream for concepts according to concreteness, and possibly more elaborate model architectures that facilitate distinct representational frameworks for abstract and concrete concepts.",1,2014
D14-1032,"in future we will address the question of whether type iii concepts can ever be enhanced via multi-modal learning, and investigate multimodal models that optimally learn concepts of each type.",1,2014
D14-1034,"we will also adapt it to a multilingual setup, aiming to model a wide range of languages.",1,2014
D14-1034,in the future we intend to improve our model by encoding additional information in it.,1,2014
D14-1035,"having established this procedure and its relative tolerance for low amounts of data, we would like to extend the model to make use of partial bracketing information instead of complete trees, perhaps in the form of fragmentary unlabeled dependency grammar annotations (schneider et al., 2013).",1,2014
D14-1038,"in the future, we plan to extend renoun to extract triples whose components are not limited to freebase ids.",1,2014
D14-1038,"as an example, extending renoun to handle numerical or typed attributes would involve extending our extraction pattern learning to accommodate units (e.g., kilograms) and other special data formats (e.g., addresses).",1,2014
D14-1039,such methods could be combined with hierarchical classification to yield further gains.,1,2014
D14-1039,an interesting extension of this work is to rely upon the natural clustering of related documents.,1,2014
D14-1040,it is also worth studying other models that induce latent semantic concepts from multilingual data (see sect.2) within this framework of context-sensitive clss modeling.,1,2014
D14-1040,"one may also investigate a similar approach to context sensitive clss modeling that could operate with explicitly defined concept categories (gabrilovich and markovitch, 2007; cimiano et al., 2009; hassan and mihalcea, 2009; hassan and mihalcea, 2011; mccrae et al., 2013).",1,2014
D14-1043,we would like to annotate more games to improve our dataset.,2,2014
D14-1043,"we did not attempt to learn this information in our process, but it is likely that modeling the event transition probabilities could provide better results.",1,2014
D14-1043,a larger future work would extend the method outlined herein to produce templates for automated commentary generation.,1,2014
D14-1043,we could improve our model by encoding the dynamics of the environment.,1,2014
D14-1045,"we anticipate that this line of research will also be of interest for a range of related tasks beyond traditional srl, including predicate-argument structure alignment (roth and frank, 2012) and implicit argument linking (gerber and chai, 2012).",4,2014
D14-1045,"in future work, we plan to apply more sophisticated models of compositionality to better represent predicate-argument structures and to guide classification decisions towards outcomes that are semantically more plausible.",1,2014
D14-1046,"we are planning to extend this work to include domain clusters to improve the domain assignment results, namely in terms of recall.",4,2014
D14-1047,"as future work, we plan to use these filters to build thesauri from larger corpora.",2,2014
D14-1047,"finally, we would like to compare the proposed heuristics with more sophisticated filtering strategies like singular value decomposition (landauer and dumais, 1997) and non-negative matrix factorization (van de cruys, 2009).",3,2014
D14-1047,we would like to generalize our findings to other syntactic configurations (e.g.noun-adjective) as well as to other similarity and informativeness measures.,4,2014
D14-1048,"as the proposed system learns the alignments automatically using very little domain knowledge, it can be applied in any domain and for any language with minor adaptations.",4,2014
D14-1048,"hence, a natural extension to this work will be automatically parsing english sentences into amr and generating english sentences from amr.",4,2014
D14-1048,computing the alignments between english sentences and amr graphs is a first step for extraction of semantic interpretation and generation rules.,1,2014
D14-1049,future work should explore further approaches to parse partial syntactic structure specific to some target semantic relations.,1,2014
D14-1050,"in the future, we plan to investigate the use of semantic similarity from distributional and other sources (mihalcea , 2006; pado and lapata, 2007), e.g., wikipedia (strube and ponzetto, 2006; mihalcea and csomai, 2007), wiktionary (zesch , 2008), wordnet (pedersen , 2004; agirre , 2009), framenet, verbnet (shi and mihalcea, 2005), babelnet (navigli and ponzetto, 2010), and lsa, and for different domains.",5,2014
D14-1050,"in the future, we plan to investigate the use of semantic similarity from distributional and other sources (mihalcea et al., 2006; pad麓o and lapata, 2007), e.g., wikipedia (strube and ponzetto, 2006; mihalcea and csomai, 2007), wiktionary (zesch et al., 2008), wordnet (pedersen et al., 2004; agirre et al., 2009), framenet, verbnet (shi and mihalcea, 2005), babelnet (navigli and ponzetto, 2010), and lsa, and for different domains.",2,2014
D14-1051,"in a future work, we plan to evaluate this new representation of textual documents in other information retrieval tasks, such as keyword extraction or automatic summarization systems.",3,2014
D14-1052,"in the future, we intend to test our model on sentiment classification at the sentence-level, based only on document-level supervision .",3,2014
D14-1052,"in the longer term, we plan to investigate new methods to estimate instance weights at prediction time, and to evaluate the impact of assigned weights on sentence ranking, segmentation or summarization.",1,2014
D14-1052,"moreover, we will experiment with other model settings, such as regularization norms other than e2 and feature spaces other than bow or tf-idf.",1,2014
D14-1054,"in the future, we plan to apply the joint model on other domains, such as movie/product reviews.",4,2014
D14-1055,"in future work, we will discuss the application of our proposed method in the massive dataset.",2,2014
D14-1056,"our next planned task is clustering different shell nouns based on the kind of complements they take in different usages similar to verb clustering (merlo and stevenson, 2000; schulte im walde and brew, 2002).",1,2014
D14-1057,"since the number of clusters has an influence on the quality of the ensuing semantic classification, we will also be running our experiments with different settings of k to explore whether this also influences the overall results of our evaluation.",3,2014
D14-1058,"in addition, we plan to focus on incrementally collecting domain knowledge to deal with missing information gaps.",2,2014
D14-1058,another possible direction is to improve parsing and coreference resolution.,1,2014
D14-1061,"given the positive results we obtain by using the joint approach to improve word alignment, we are inspired to apply this approach to help find translations for out of vocabulary words, and to explore other possible ways to improve machine translation with decipherment.",1,2014
D14-1062,"in future work, we plan to explore generative bayesian models as well as discriminative learning approaches with different ways for estimating the latent domain relevance models.",1,2014
D14-1064,another possible future endeavor is to extend these ideas to (i) other query translation approaches and (ii) document translation.,4,2014
D14-1064,"while the exact same problem can be formulated for learning to translate documents effectively, a more complicated infrastructure and longer running times are two challenges that need to be considered.",5,2014
D14-1064,"finally, we hope this to be a significant step towards more context-dependent and robust clir models, by taking advantage of modern translation technologies, as well as machine learning techniques.",1,2014
D14-1065,our tensor-based representation of topically-segmented multilingual documents can also be applied to cross-lingual information retrieval or multilingual document categorization.,4,2014
D14-1066,"furthermore, we want to incorporate features using n-grams computed on a corpus from the domain and include co-occurrence features.",1,2014
D14-1066,"in future work, we will work on integrating dts using other context features, as we could see an impact of using two different dts.",1,2014
D14-1068,"finally, our future work will explore kle and homoglyph correction bidirectionally, as opposed to the unidirectional approach explored in this work.",1,2014
D14-1070,"a promising avenue for future work would be to incorporate wikipedia data into qanta by transforming sentences to look like quiz bowl questions (wang , 2007) and to select relevant sentences, as not every sentence in a wikipedia article directly describes its subject.",2,2014
D14-1070,"syntax-specific annotation (sayeed , 2012) may help in this regard.",2,2014
D14-1070,"having learned many facts about entities that occur in question text, a dt-rnn could add new facts to a knowledge base or check existing relationships.",1,2014
D14-1070,"finally, we could adapt the attribute space learned by the dt-rnn to use information from knowledge bases and to aid in knowledge base completion.",1,2014
D14-1071,"for our future work, we will build concept-level context embeddings by leveraging latent meanings of nles rather than their surface n-grams with the aligned logical features on kb.",1,2014
D14-1073,"and of course it would be better if this standard dataset was multilingual instead of billingual, for obvious reasons.",2,2014
D14-1073,further we would also need sets of human gold-standard query biased summaries in l1 and l2.,2,2014
D14-1073,"these standards and data would allow us to compare method-to-method across different languages, while simultaneously allowing us to tease apart other variables such as: when and what to translate, translation quality, methods for biasing, and type of summarization strategy (sentences, words, etc).",3,2014
D14-1074,"finally, we hope that some of the work described here might be of relevance to other generation tasks such as summarization, conceptto-text generation, and machine translation.",4,2014
D14-1074,"we would like to generate poems across different languages and genres (e.g., engish sonnets or japanese haiku).",6,2014
D14-1074,we would also like to make the model more sensitive to line-to-line transitions and stylistic conventions by changing its training objective to a combination of cross-entropy error and bleu score.,1,2014
D14-1075,"therefore we intend to pursue future research in utilizing word-sense disambiguation and synonyms, as well as other techniques for furthering reaper query similarity metrics in order to improve its rouge and human evaluation scores on query-focused tasks.",3,2014
D14-1077,"as future work, we plan to extend the data set with more clusters and more reference summaries, as well as to develop sentence compression methods for turkish mds.",2,2014
D14-1080,"additionally, alternative notions of depth that are orthogonal to stacking, as in pascanu et al.(2013) can be investigated for this task.",1,2014
D14-1080,another direction is to investigate the impact of finetuning the word vectors during supervised training.,1,2014
D14-1080,one potential future direction is to explore the effects of pre-training on the architecture.,1,2014
D14-1082,"there is still room for improvement in our architecture, such as better capturing word conjunctions, or adding richer features (e.g., distance, valency).",1,2014
D14-1082,an interesting line of future work is to combine our neural network based classifier with search based models to further improve accuracy.,1,2014
D14-1087,"future work includes supporting multi-argument templates, disambiguating headwords of category names and applying our approach to general short text template mining.",4,2014
D14-1088,"in the future, we will apply the proposed taxonomy construction method to other domains such as biomedicine and integrate it into other frameworks such as ontology authoring.",4,2014
D14-1089,"in addition, we intend to expand the bootstrapping experiments with variations over the training data.",2,2014
D14-1089,"in the future, we intend to assess how specific slots are affected by recall and search space tradeoff, and perform evaluation over all slot types: names, values and strings.",3,2014
D14-1090,(2) transfer the results of this investigation to other complex nlp tasks that can potentially benefit from joint inference;,4,2014
D14-1090,"and (3) develop scalable inference and learning algorithms (ahmadi et al., 2013).",1,2014
D14-1090,"in future work, we plan to (1) improve our joint model by incorporating co-reference information and developing model ensembles;",1,2014
D14-1096,"in future work, we would like to improve the performance of dpm by collecting more parallel data.",2,2014
D14-1096,we would like to experiment with different source languages other than english.,2,2014
D14-1096,duong (2013a) pointed out that using a different source language can greatly alter the performance of the target language pos tagger.,2,2014
D14-1100,"in our future research, we will exploit the proposed framework to resolve other parsing difficulties in chinese, e.g., n-n combination.",4,2014
D14-1100,"finally, for real world knowledge learning, we will continue to learn more useful knowledge by auto-parsing to improve the parsing performance.",1,2014
D14-1100,we will also extend the semantic type predication algorithm (figure 2) to deal with all chinese words.,1,2014
D14-1101,we may study the integration of these embeddings into our approach as future work.,1,2014
D14-1103,the different granularities of the hierarchy induced by split-merge training are potentially useful.,1,2014
D14-1103,"we would like to use additional information (e.g., from the dependency trees) to identify useless splits.",1,2014
D14-1103,we think that coupling parents and children in the tag hierarchy might be one way to force a consistent hierarchy.,1,2014
D14-1104,"for future work we plan to extend this work to further weight functions, data sets and nlp tasks.",4,2014
D14-1105,"we plan to continue our work in that direction, specifically for conversational text in social media in a multilingual context.",2,2014
D14-1105,"the challenges and issues identified in this study are likely to hold for many other languages as well, which makes this a very important and globally prevalent problem.",5,2014
D14-1106,"finally, we plan to investigate why our system performed so much better on enni than on nsr.",5,2014
D14-1106,"second, we hope to explore features that could be useful for identifying grammatical errors in multiple data sets.",5,2014
D14-1106,there are several key areas we plan to investigate in the future.,6,2014
D14-1106,"first, we would like to explore different update functions for the parser; the predicted error codes are a byproduct of parsing, but we do not care what the parse itself looks like.",1,2014
D14-1107,"one interesting angle would be to increase the amount of information in ccgbank lexical entries, to further reduce the search space for the parser.",2,2014
D14-1107,"incorporating specific models for such decisions may improve accuracy, while still allowing fast and exact search鈥攆or example, we intend to try including coppola et al.(2011)鈥檚 model for prepositional phrase attachment.",1,2014
D14-1110,2.we can explore other wsd methods based on sense vectors to improve our performance.,5,2014
D14-1110,"there are still several open problems that should be investigated further: 1.because the senses of words change over time (new senses appear), we will incorporate cluster-based methods in our model to find senses that are not in the sense inventory.",5,2014
D14-1110,"for example, (li et al., 2010) used lda to perform data-driven wsd in a manner similar to our model.",1,2014
D14-1110,we may integrate the advantages of these models and our model together to build a more powerful wsd system.,1,2014
D14-1110,"3.to learn better sense vectors, we can exploit the semantic relations (such as the hypernym and hyponym relations defined in wordnet) between senses in our model.",1,2014
D14-1111,"further numerical techniques for improving the estimation of the class decision boundary, and consequently the f-score, will also constitute future work.",1,2014
D14-1112,"we plan to apply our similarity method on a corpus of spoken language, and to extend our analysis to other languages as well, as we gain access to available resources.",4,2014
D14-1112,we further intend to combine our orthographic approach with syntactic and semantic evidence for a wider perspective on language similarity.,1,2014
D14-1113,in future work we plan to use the multiple embeddings per word type in downstream nlp tasks.,1,2014
D14-1114,"there are several directions for future work, including using both types and domains in freebase schema, diving into refiners and looking for a proper weighting method, developing a query recommendation framework based on the intent topic graph and user interest modeling.",1,2014
D14-1116,"and d) the learning system has the advantage of being easily adapted to new settings, and we plan to extend it to other domains and languages (liang and potts, 2014).",4,2014
D14-1116,"in the future, we plan to address the following limitations that still exist in the current system: a) numerous hand-labeled data are required for training the mln, and we could use a latent form of semantic item query graphs (liang , 2013);",5,2014
D14-1116,b) more robust solutions can be developed to find the implicit relations in questions;,1,2014
D14-1118,2) apply rcm to non-technical domains.,4,2014
D14-1118,it is worthy investigating whether rcm still works on such domains.,4,2014
D14-1118,"we would like to investigate how to deal with the bottleneck, e.g., via parallel or distributed computing.",5,2014
D14-1118,"as future work, we plan to 1) enhance the efficiency and scalability of rcm.",1,2014
D14-1121,"given that manual lexica are already extensively employed in social sciences such as psychology, economics, and business, using lexical representations of data-driven models allows the utility of our models to extend beyond the borders of the field of nlp.",4,2014
D14-1124,"finally, we plan to apply our novel feature set to other corpora (e.g., argue) in order to study the utility of these features across genres and with respect to the accuracy of the discourse parser.",2,2014
D14-1124,"furthermore, we will also explore how to create a discourse tree from the thread structure of a conversation (instead of from its temporal structure), and verify whether this improves the accuracy of the relation graphs, especially when the temporal structure is not representative of the reply-to relationships.",5,2014
D14-1124,"additionally, we will incorporate generalized dependency and pos features (abbott et al., 2011), which were not used in this analysis due to the very small number of training samples in our dataset.",1,2014
D14-1124,"in future work, we will improve sentiment features by considering methods to detect opinion topic pairs in conversation, similar to somasundaran and wiebe (2009).",1,2014
D14-1124,"the fragment quotation graph features did not perform as well as we expected, and in future work we would like to investigate this further.",1,2014
D14-1125,it would be promising to combine our method with other methods to enable it to find +effect and -effect senses that are outside the coverage of wordnet.,1,2014
D14-1128,"in future work, we would like to show that our findings generalize from the case of 鈥渉ard鈥 to the entire sentiment lexicon.",2,2014
D14-1129,"besides, we will evaluate the effectiveness of our mined data on mt or other applications.",3,2014
D14-1129,"in the future work, we will study some method on extracting parallel resource from existing parallel page pairs, which are challenging tasks due to the diversity of page structures and styles.",1,2014
D14-1131,"there are a number of directions that we intend to investigate to speed up our decoder, such as: (1) error-safe pruning based on search error bounds; (2) use of reinforcement learning to guide the decoder in choosing which n-gram contexts to extend; and (3) grouping edges into partial edges, effectively reducing the size of the hypergraph and ultimately computing inside weights in less time.",1,2014
D14-1133,another avenue for improvement lies in the possibility to perform the training of our rewriter by providing it with more reference translations.,2,2014
D14-1133,"first, we could use a larger set of rewriting operations (langlais , 2007), including the rewrite (sic) operation introduced in (marie and max, 2013) that paraphrases source phrases and then translates them.",2,2014
D14-1133,"it is furthermore worth noticing that our work proposes a potential answer to an original question: contrarily to typical works on sub-sentencial mt confidence estimation, which predict whether a word or phrase is correct or not, our rewriter system could be used to determine automatically whether a rewriting system could (if asked to) attempt to improve locally a translation, or whether a human post-editor should already tackle working on improving it.",5,2014
D14-1133,"we could also possibly consider any phrase segmentation compatible with a specific word alignment rather than rely on specific phrase segmentations. more features could also be used, for instance to model more fine-grained syntax (post, 2011) or document-level lexical coherence (hardmeier et al., 2012).",1,2014
D14-1133,"contrarily to (madnani and dorr, 2013), we could bias the paraphrasing table so that it only contains paraphrases that correspond to target phrases of high confidence values, which would add new n-grams likely of being produced by rewriter.",1,2014
D14-1133,"however, anticipating that some features might be very expensive to compute, we could adapt our procedure to work in several passes: initial passes would tend to restrict the search space more and more using an initial set of features, before a more expensive pass would concentrate on a limited number of hypotheses.",1,2014
D14-1133,"as these are typically not readily available, we could resort to targeted paraphrasing (madnani and dorr, 2013) to rewrite reference translations into acceptable paraphrases that reuse n-grams from the best hypotheses of the system so far.",1,2014
D14-1133,Our work could be extended in several directions.,6,2014
D14-1134,"in the future, we wish to study various aspects of learning more robust lexicons.",1,2014
D14-1134,"for example, in our current approach, words not appearing in the training set are treated as unknown and ignored at inference time.",1,2014
D14-1134,"to alleviate this problem, we intend to further explore learning novel lexical templates.",1,2014
D14-1134,we would like to study the benefit of using large amounts of unlabeled text to allow the model to better hypothesize the meaning of such previously unseen words.,1,2014
D14-1135,"we would also like to study how to generalize these gains to languages other than english, by inducing more of the syntactic structure.",1,2014
D14-1136,"in the future, we intend to extend this model for interpreting requirements in un-restricted, or less restricted, english, endowed with a more sophisticated discourse interpretation function.",1,2014
D14-1137,"furthermore, as a general string-to-tree structured prediction model, this work may find applications in other areas within nlp.",4,2014
D14-1137,"being able to efficiently exploit features defined over individual words, our model also opens up the possibility for us to exploit alternative representations of words for learning (turian et al., 2010), or to perform joint learning of both distributional and logical semantics (lewis and steedman, 2013).",1,2014
D14-1137,"future works include development of efficient algorithms for feature-based semantic parsing with alternative loss functions (zhou et al., 2013), development of feature-based language generation models (lu et al., 2009; lu and ng, 2011) and multilingual semantic parsers (jie and lu, 2014), as well as the development of efficient semantic parsing algorithms for optimizing the performance of certain downstream nlp tasks with less supervision (clarke et al., 2010; liang et al., 2013).",1,2014
D14-1138,"finally, visualizable spaces offer the potential to produce interactive environments for semisupervised topic reconstruction.",1,2014
D14-1140,"a natural next step is expanding this work to other languages, such as japanese, which not only has sov word order but also requires tokenization and morphological analysis, perhaps requiring sub-word prediction.",4,2014
D14-1142,"to gain additional insights into why this technique is working well, the features selected by the classifier as being more discriminating can be analyzed in future work. these features would offer insights into two kinds of language transfer effects, namely word choice (lexical transfer) and morphological differences.",1,2014
D14-1143,"for our future work, because the graph used in this paper was constructed manually, we plan to automatically create a graph suitable for active learning and classification.",1,2014
D14-1143,"there are several algorithms that create graphs from feature-based representations of words, but these have never been used for active learning of this task.",1,2014
D14-1144,the use of unsupervised learning methods such as bayesian mixture models may be appropriate here.,1,2014
D14-1144,"another avenue is to implement weight-based ranking methods to further refine and re-rank the lists, potentially by incorporating the measures mentioned in section 2 to assign weights to features.",1,2014
D14-1144,the first relates to clustering the data within the lists.,1,2014
D14-1144,"in addition to these further technical investigations, we see as a particularly useful direction the development of an sla research tool to conduct a large sla study with a wide range of experts.",1,2014
D14-1144,"finally, the use of other linguistic features such as context-free grammar phrase structure rules or tree substitution grammars could provide additional insights.",1,2014
D14-1144,"our intuition is that there might be coherent clusters of related features, with these clusters characterizing typical errors or idiosyncrasies, that are predictive of a particular l1.",1,2014
D14-1144,"as the corpus we used includes learner proficiency metadata, it may also be possible to create proficiency-segregated models to find the features that characterize errors at each language proficiency level.",1,2014
D14-1144,"for parse features, tree kernels could help measure similarity between the trees and fragments (collins and duffy, 2001).",1,2014
D14-1145,"we are planning to extend our study to dialects in other immigrant settings (e.g., turkish in germany) and to other types of multiword expressions (e.g., [n n] compounds).",2,2014
D14-1147,we are going to study on these issues in the future.,6,2014
D14-1147,potential future works may include using semi-supervised methods to incorporate unlabeled data and design reasonable features from large corpora.,1,2014
D14-1149,"another avenue for future research would be to use mixed membership community detection (gopalan and blei, 2013).",1,2014
D14-1149,future work could seek to combine graph-theoretic notions of centrality and intuitions about the defining features of term clusters.,1,2014
D14-1149,"using cooccurrence networks to extract clusters of specialist terms, though an important task, is perhaps only a starting point for exploring the observed lexicon.",1,2014
D14-1150,"although we illustrated the benefits of leveraging inter-linked document networks for keyphrase extraction from scientific documents, the proposed model can be extended to other types of documents such as webpages, emails, and weblogs.",2,2014
D14-1150,another aspect of future work would be the use of external sources to better identify candidate phrases.,2,2014
D14-1152,"as a direction for further research, it is interesting and important to provide more analysis on the expanded words via the continuous vector representations of words.",1,2014
D14-1153,"another interesting experiment we plan to carry out in the future is to use the ngram classes along with the traditional stylistic features such as the vocabulary richness, average sentence length, etc.",1,2014
D14-1153,"as future work, it would be interesting to combine the most precise classes of different n-gram lengths in order to improve the precision.",1,2014
D14-1153,it would be important as well to try other segmentation strategies and postprocessing techniques in order to improve the granularity.,1,2014
D14-1156,"as to future work, we would like to investigate jointly integrating proximity and other different kinds of relevance and lexical/semantic information cues into the process of feedback document selection so as to improve the empirical effectiveness of such query modeling.",1,2014
D14-1157,"in future work, we will explore a model that captures individuals inherent topic shift propensities, while also capturing their fluctuations due to social factors.",1,2014
D14-1160,"finally, we plan to investigate the impact of using sensory information for metaphor detection and interpretation based on our observations during the evaluation.",3,2014
D14-1160,"for instance, the synesthetic metaphor bittersweet symphony could be detected by determining the sensorial characterizations of its components.",1,2014
D14-1161,"second, we will apply the method to other tasks that require completing a word relatedness matrix.",4,2014
D14-1161,"possible additional perspective slices include lsa for topic relatedness, and corpus occurrences in engineered or induced semantic patterns.",1,2014
D14-1161,one straight-forward idea is that the dot product of perspective vectors pk 路 pl should be a measurement of correlation between perspectives.,1,2014
D14-1161,"we evaluated the performance of our model on creating / recreating one perspective of word relatedness: antonymy. perhaps using vectors generated from many kinds of perspectives would improve the performance on other nlp tasks, such as term matching employed by textual entailment and machine translation metrics.",1,2014
D14-1161,"third, if our model does learn the relation between semantic similarities and distributional similarities, there may be fruitful information contained in the vectors vi and pk that can be explored.",1,2014
D14-1161,"first, in this model we only use a three-way tensor with two slices, while more relations may be able to add into it directly.",1,2014
D14-1161,"for future works, we will extend the model and its applications in three main directions.",1,2014
D14-1166,we believe that such kernels could improve the relatively low recall obtained so far by weakly supervised method for relation extraction.,1,2014
D14-1166,"we would like to explore the use of kernels, such as the ones introduced by zelenko et al.(2003), culotta and sorensen (2004) and bunescu and mooney (2005), in future work.",1,2014
D14-1168,"in addition, we plan to develop and evaluate an end to-end system, in which the aspect extraction and polarity estimation of aspects are automated.",1,2014
D14-1168,"in future work, we plan to extend the microplanning phase by taking advantage of the highly weighted rhetorical relations between the aspects and select connective phrases based on the discourse relations specified in the aspects tree.",1,2014
D14-1170,"in future work, we will make use of citation sentences to improve our system.",2,2014
D14-1171,"another line of future research is to use the pm algorithm in other nlp tasks, where finding the pairs having some particular elements in common is necessary: for example, comparing parsing trees or dependency trees.",4,2014
D14-1171,we think that pm can be used in other nlp tasks as well and we hope the community can take advantage of it.,4,2014
D14-1173,"in the future, we plan to explore combining multiple source words which are aligned to the same target words.",1,2014
D14-1174,"In the future, we plan to explore more effective approaches.",1,2014
D14-1175,"future work will investigate the benefits of coupling our bnn models with target language models that also exploit abstract word representations, such as botha and blunsom (2014) and auli et al.(2013).",1,2014
D14-1176,"in this paper, we have focused on rather elementary dependency relations, which we are planning to expand on in future work.",1,2014
D14-1176,"in particular, we are interested in exploring ways to better capture the notion of syntactic cohesion in translation (fox, 2002; cherry, 2008) within our framework.",1,2014
D14-1177,"it would be interesting to investigate the contribution of different clues for various experimental parameters, e.g., domain, distance of languages, types of comparable corpora.",3,2014
D14-1177,"as future work, we plan to improve the quality of the extracted dictionary further by exploiting additional translation signals.",1,2014
D14-1180,"another interesting extension for our methods would be to also define types for the edge variables, and then sample both cut and edge types jointly.",1,2014
D14-1180,one possible solution would be using better averaging methods instead of simply averaging over every few iterations.,1,2014
D14-1183,the proposed method is also useful for many classification problems in natural language processing that require large-scale data.,4,2014
D14-1186,"in the future, we plan to explore how to combine more features such as part-of-speech tags into our model.",1,2014
D14-1190,"as it was discussed in section 4.3, we plan to further explore the possibility of using nongaussian likelihoods with the gp models.",1,2014
D14-1190,"another research avenue we intend to explore is to employ multiple layers of metadata, similar to the model proposed by cohn and specia (2013).",1,2014
D14-1191,future works should also investigate how fully unsupervised methods can be extended to match our performance.,5,2014
D14-1195,"as several recent efforts have focused on extracting large-scale parallel corpus for sentence compression (filippova and altun, 2013), we would like to study how larger corpora can affect tree transduction and our joint decoding solution.",5,2014
D14-1195,"meanwhile, we would like to explore on how other text-rewriting problems can be formulated as a joint model and be applicable to similar strategies described in this work.",5,2014
D14-1199,"as future work, we will reconsider the architecture of the neural network and we will refocus on creating a deep learning model while taking advantage of a larger set of types of information such as syntactic information, following (levy and goldberg, 2014), or semantic information, following (yu and dredze, 2014).",1,2014
D14-1200,"as future work, we plan to apply this approach to other relation extraction tasks and explore more suitable search orders for relation extraction tasks.",4,2014
D14-1200,we also plan to investigate the potential of this table representation in other tasks such as semantic parsing and co-reference resolution.,4,2014
D14-1201,we plan to investigate the use of ore in improving syntactic analysis in future work.,1,2014
D14-1203,"future work will explore the use of nel in distantly supervised relation extraction further, tuning a confidence parameter for the nel system, and determining whether different confidence parameters should be used for training and extraction.",1,2014
D14-1203,another possible direction is interleaving nel with relation extraction by using newly extracted facts to try to improve nel performance.,1,2014
D14-1204,our future work will include exploring ways to improve automatic eventuality type and modality labeling accuracy to further improve tense inference accuracy.,1,2014
D14-1205,"for future work, we will address the nil issue of el where we currently assume all entities should be linked to a kb.",5,2014
D14-1205,"it would be also interesting to jointly model the two subtasks through structured learning, instead of joint inference only.",1,2014
D14-1207,we also would like to investigate time-stamped corpora of finer-grained granularity such as day.,2,2014
D14-1207,"for future work, we would like to investigate how our method can be improved to dp better at detecting fact end times.",5,2014
D14-1208,"in the future, we would like to experiment with other constraints like modeling the selectional preferences of entity types.",1,2014
D14-1208,"as part of immediate future work, we would like to improve the system recall.",1,2014
D14-1208,our ilp formulation provides a good framework to add new type of constraints to the problem.,1,2014
D14-1210,"for future work, we will seek to relax this consideration and jointly reason about non-terminal categories and derivation structures.",1,2014
D14-1211,we expect our corpus to be a rich resource for social scientists interested in the effect of power and gender on language use.,4,2014
D14-1211,"we will investigate several other sociolinguistic-inspired research questions; for example, do the strategies managers use for effectiveness of communication differ based on gender environments?",5,2014
D14-1211,"in future work, we will explore machine learning algorithms which capture the interactions between features better than our svm with quadratic kernel.",1,2014
D14-1211,"while our findings pertain to the enron data set, we believe that the insights and techniques from this study can be extended to other genres in which there is an independent notion of hierarchical power, such as moderated online forums.",4,2014
D14-1212,we also plan to apply the proposed method to content recommendations and trend analysis in twitter to investigate this method further.,4,2014
D14-1212,"in future work, we plan to extend the proposed method to capture the birth and death of topics along the timeline with a variable number of topics, such as the model proposed by ahmed (ahmed and xing, 2010).",1,2014
D14-1213,"third, we can look at the relationship between self-disclosure behavior and general online social network usage beyond conversations.",1,2014
D14-1213,"first, we can improve our modeling for higher accuracy and better interpretability.",1,2014
D14-1213,"for instance, sdtm only considers first-person pronouns and topics. second, the number of topics for each level is varied, and so we can explore nonparametric topic models (teh et al., 2006) which infer the number of topics from the data.",1,2014
D14-1215,future investigation into the causal effects of these interactions could lead to a better understanding of the role of figurative language in persuasion and rhetorics.,5,2014
D14-1215,this suggests that future work on automatic detection of figurative language should consider contextual parameters such as the topic and community where the content appears.,1,2014
D14-1217,an interesting avenue for future research is to automatically learn how to parse text describing scenes into formal representations by using more advanced semantic parsing methods.,1,2014
D14-1217,"another interesting line of future work would be to explore the influence of object identity in determining when people use ego-centric or object centric spatial reference models, and to improve resolution of spatial terms that have different interpretations (e.g., 鈥渢he chair to the left of john鈥 vs 鈥渢he chair to the left of the table鈥).",1,2014
D14-1217,we can also improve the representation used for spatial priors of objects in scenes.,1,2014
D14-1217,"finally, a promising line of research is to explore using spatial priors for resolving ambiguities during parsing.",1,2014
D14-1217,we can improve the representation by modeling whether a surface is an interior or exterior surface.,1,2014
D14-1219,"in the future, we would like to apply our reranker to the document-level parses.",4,2014
D14-1219,"however, this will require a better hypotheses generator.",1,2014
D14-1220,"our future work will focus on extending discourse-level distributed presentations to related tasks, such as implicit discourse relation identification or dialogue analysis.",4,2014
D14-1222,"furthermore, better methods to model the semantics of the specific context need to be explored in the future.",1,2014
D14-1223,"as a future work, we would like to adapt our model for different languages and include other features from multi modality including gesture or geo-location.",1,2014
D14-1224,"in the future work, we will focus on enlarging the scale of the corpus annotation and developing a complete chinese discourse parser.",2,2014
D14-1225,"future work should apply this general idea to other natural language processing tasks including dependency parsing (nivre , 2007) and information extraction (li , 2013).",4,2014
D14-1225,"we would expect more beneficial behavior with the pruning constraints for problems with large action sets (e.g., labeled dependency parsing).",1,2014
D14-1225,"it would be interesting and useful to generalize this approach to search spaces where there are multiple target paths from the initial state to the terminal state, e.g., as in the easy-first framework.",1,2014
D14-1226,"as future work, we plan to explore various other forum specific features such as user reputation and quality of content to improve summarization performance.",6,2014
D15-1001,future directions include tackling high-level planning and strategy learning to improve the performance of intelligent agents.,1,2015
D15-1002,"finally, it is still not clear how to extend the current approach beyond words and phrases directly denoting an entity (amur) to other kinds of definite descriptions (this dog).",5,2015
D15-1003,"in future work, we will test our system on such context-specific examples, using contextualised vector representations such as the ones proposed by e.g.erk and pad´o (2008) and dinu and lapata (2010).",3,2015
D15-1004,"although experiments show significant improvements over baselines, our model has limitations that can be avenues for future work.",1,2015
D15-1005,"there are various direction we would like to explore, the most obvious of which are integrating the learned reordering with other feature functions in a discriminative setting, and extending the model to deal with non-contiguous minimal phrases.",1,2015
D15-1006,"while there exist approaches that extract syntactic tree transformation rules automatically, one of the difficulties is that most parallel corpora is dominated by lexical paraphrasing instead of syntactic paraphrasing.",5,2015
D15-1009,"to further improve the prediction accuracy, in the future, we will extend our current study by incorporating new features such as the properties of a brand as well social influence from people in one’s social network.",1,2015
D15-1010,"as future work, we aim to improve the efficiency of our entire workflow, such that the annotation can become an end-to-end service.",6,2015
D15-1010,"we also aim to improve the context similarity between entities and the topic, for example by using a deeper distributional semantics-based method, instead of language models as in our current work.",1,2015
D15-1012,it is interesting to explore how will the performance be affected if we are only provided with parallel sentences and then alignments can only be derived using an independent aligner.,5,2015
D15-1016,"2) apply this framework to other cross lingual tasks such as paraphrase detection, question answering, aspect based opinion mining etc",4,2015
D15-1016,learning different weight matrices at different nodes to capture complex relations between words and phrases.,1,2015
D15-1020,"moreover, joint detection of events from both sides is our ultimate goal, however, we need to explore the mapping among events from both text and visual sides, and automatic detection of a wide range of objects and events from news video itself is still challenging.",6,2015
D15-1022,"future work could include nonprepositional terms like verbs, having prepositions modify verbs, adding word2vec embeddings to the structured prediction model, and providing stronger features – whether textual, visual or geometric.",1,2015
D15-1023,"for further research, we note that picking the optimal hi,j,k is an open question, so provably finding and justifying the choice is one topic of interest.",5,2015
D15-1025,"in the future, we plan to extend these models to other tasks such as syntactic parsing and machine translation.",4,2015
D15-1025,"moreover, we will also investigate other architectures to infer word embeddings from the character level.",1,2015
D15-1025,"for instance, preliminary experiments show that bidirectional recurrent network can achieve very competitive and promising results.",1,2015
D15-1029,"in future work, we can assess the impact of this model on a wider array of feed-forward embedding-based neural network models, such as the dssm (huang , 2013).",3,2015
D15-1030,"in future work, we plan to engage in further model analysis and comparison, to explore alterations to model structure, e.g. introducing hierarchical topic models, to use other clustering methods to obtain priors, and to explore the value of predicted links for downstream tasks such as friend recommendation (pennacchiotti and gurumurthy, 2011) and inference of user attributes (volkova et al., 2014).",1,2015
D15-1033,"third, while we have shown that distributional initialization improves the quality of representations of rare words, we did not investigate whether distributional initialization for rare words has any adverse effect on the quality of representations of frequent words for which one-hot initialization is applied.",5,2015
D15-1033,"since rare and frequent words are linked in the mixed model, this possibility cannot be dismissed and we plan to investigate it in future work.",1,2015
D15-1033,"it remains to be investigated whether there are interactions between these two properties of our model, e.g., a high rare-frequent separator may work well for words whose corpus frequency is much smaller than the separator.",1,2015
D15-1034,we leave the study of more general paths to future work.,6,2015
D15-1037,personalized topic modeling is also an interesting future direction in which the model will generate a personalized topic structure based on the user’s preferences or interests.,1,2015
D15-1037,"for all these applications, an efficient learning algorithm is a crucial prerequisite.",1,2015
D15-1039,"future work should consider application of the method to a broader set of languages,",2,2015
D15-1039,and application of the method to transfer of information other than dependency structures.,4,2015
D15-1040,"in future work, we plan to extend joint training to several languages, and further explore the idea of learning and exploiting crosslingual embeddings.",4,2015
D15-1042,"in the future, we are planning to experiment with more “interesting” paraphrasing models which translate the input not into a zero-one sequence but into words.",1,2015
D15-1043,future work includes application of the system on text-to-text problem such as machine translation.,4,2015
D15-1044,both pose additional challenges in terms of efficient alignment and consistency in generation.,1,2015
D15-1044,"as a next step we would like to further improve the grammaticality of the summaries in a data-driven way, as well as scale this system to generate paragraph-level summaries.",1,2015
D15-1047,"in our future work, we plan to verify the soundness of the results by applying our method on large volume corpus of both english and chinese.",3,2015
D15-1047,"in addition, we will investigate other ways of computing the word coupling matrices, such as incorporating word coherency or semantics, and develop efficient merging strategies which can be used for training classification models, as well as for building graphs.",1,2015
D15-1048,"this is relevant, if a technique like filtering is used to include more relevant class examples in a dataset than provided with an original sample – a necessary step to realize a labeled dataset for model learning of a rare-class task.",2,2015
D15-1048,there are two main topics for our future work.,6,2015
D15-1048,"first, we will investigate the performance of models generated with biased datasets on unfiltered datasets.",1,2015
D15-1048,"second, we will work on using novel features for the creation of generalized models.",1,2015
D15-1048,"one example is the utilization of the semantic web to generate abstract features, utilizing a technique called semantic abstraction (schulz et al., 2015a).",1,2015
D15-1050,"expanding to additional corpora will probably require development of additional features, to capture signals unique to each corpus.",2,2015
D15-1050,"this poses additional challenges in gathering labeled data, as it will require a mechanism to decide which documents to label per claim and will probably increase the number of documents to be labeled.",5,2015
D15-1051,we also plan to investigate the prototype directly in query expansion as part of the search backend.,1,2015
D15-1051,"for this, we feel that adding full word level models will help to overcome the somewhat limited context present in our character sequence based models.",1,2015
D15-1051,We plan to further invest in improving precision.,1,2015
D15-1053,"while we were able to demonstrate the viability of our hybrid model when using only simple surface statistics of text, future work shall include application of our models to more semantic-oriented representations, such as those leveraged in building log-linear language models (mikolov et al., 2013).",1,2015
D15-1054,one of these directions is to extend the vocabulary by identifying significant phrases (as well as words) before training word vectors.,2,2015
D15-1054,"in case the computational cost of such methods are too high to be practical for sponsored search, we can employ them only for a small fraction of ads filtered by faster methods.",5,2015
D15-1054,there are multiple interesting research directions for future work.,6,2015
D15-1054,we also like to investigate more structured embedding methods such as rnns (probably for ad descriptions).,1,2015
D15-1055,"in future work, we will address the task of temporal expression normalization.",1,2015
D15-1056,this could alleviate ner errors and enable experimentation with other relationship types.,1,2015
D15-1056,"in future work, more robust entity-linking approaches, as proposed by hoffart et al.(2011), could be included in our pre-processing pipeline.",1,2015
D15-1059,"therefore, another line of future work is to also learn from these unlabeled documents.",2,2015
D15-1059,as future work we would also explore the feasibility of learning preconditions of verbs from wikipedia revisions.,1,2015
D15-1061,"while these initial results are encouraging, we hope to apply entice on other knowledge graphs,",4,2015
D15-1061,and also experiment with other normalization and entity linking algorithms as part of future work.,1,2015
D15-1063,"in the future, we thus plan to constantly update heideltime’s automatically created resources.",2,2015
D15-1066,"for future work, we wish to explore more useful document-type features and apply more proper combination strategies to improve the latent document type model.",1,2015
D15-1067,"for future work, it would be interesting to see if more sophisticated dnn training techniques (e.g. unsupervised pre-training and different optimization algorithms) would yield a better performance.",1,2015
D15-1068,"we further plan to test our framework on other cqa datasets, including on other languages.",3,2015
D15-1068,"in future work, we would like to improve the pairwise classifiers with richer features, as this is currently the bottleneck for improving the performance in the global model.",1,2015
D15-1068,"last but not least, we are interested in extending this research with even more global information, e.g., by modeling global decision consistency across multiple threads.",1,2015
D15-1069,there are several directions we can explore in our future research.,6,2015
D15-1069,we would also investigate domain adaptation techniques to learn key concept identification models from other data sources.,1,2015
D15-1069,"firstly, our key concept identification methods are not optimized for the retrieval results, but for the identification subtask only.",1,2015
D15-1069,we hypothesize that directly optimizing the key concept identifier for retrieval would lead to better performance.,1,2015
D15-1071,"for future research, it would be interesting to validate the results by conducting the study on a larger scale.",3,2015
D15-1071,"the approach could also be used for other document types, for example analyst reports or internal memos, or in other industries.",4,2015
D15-1072,"in future sentiment analysis approaches, sentiment flows may therefore rather serve as pivot features for domain adaptation.",1,2015
D15-1075,we hope that snli presents valuable training data and a challenging testbed for the continued application of machine learning to semantic representation.,4,2015
D15-1076,"finally, future work will also explore applications of our annotation.",4,2015
D15-1076,"alternatively, the annotation could be used for active learning: we envisage a scheme where parsers, when faced with ambiguous attachment decisions, can generate a human-readable question whose answer will resolve the attachment.",1,2015
D15-1076,"a joint syntactic and semantic parser, such as that of lewis et al.(2015), could be trained directly on the annotations to improve both the syntactic and semantic models, for example in domain transfer settings.",1,2015
D15-1076,"most obviously, the annotation can be used for training question-answering systems, as it directly encodes question-answer pairs.",1,2015
D15-1076,"more ambitiously, the annotation has the potential to be used for training parsers.",1,2015
D15-1077,"in the future, we are interested in refining the prior estimation by using the ontology and extending this work to detect the target entities that are not in a list while performing the disambiguation task.",1,2015
D15-1081,we are also experimenting with our collective validation algorithm to incorporate the impact of more distant kb entities other than just the neighbors.,1,2015
D15-1081,"in the future, we plan to improve the source document processing such that the system can better extract the mention context without involving extensive linguistic knowledge.",1,2015
D15-1082,"in future, we will explore the following research directions: (1) this paper only considers the inference patterns between direct relations and relation paths between two entities for learning.",5,2015
D15-1082,"(2) there are some extensions for transe, e.g., transh and transr.",1,2015
D15-1082,"it is non-trivial for them to adopt the idea of ptranse, and we will explore to extend ptranse to these models to better deal with complicated scenarios of kbs.",1,2015
D15-1082,we may take advantages of first-order logic to encode these inference patterns for representation learning.,1,2015
D15-1084,"we plan to further exploit sense-enhanced unified representations of relations in various ways: providing an ontological structure for the unified kb, exploring complementary approaches for capturing semantic relation alignments, and incorporating multilinguality.",1,2015
D15-1086,"in future work, the proposed approach could be combined with other approaches to solve typical issues arising in the context of distant supervision, such as dealing with overlapping relations (hoffmann et al., 2011), improving heuristic labelling of sentences (takamatsu et al., 2012) or dealing with incomplete knowledge bases (min et al., 2013).",1,2015
D15-1087,"to facilitate comparison with future work on this task, we released the source code of our spatial relation extraction system.",3,2015
D15-1089,future work might involve designing new kernels for syntactic parse trees with appropriate similarity measures between non-terminal nodes as well as exploring recently proposed phrase embeddings for more accurate phrase kernels.,1,2015
D15-1090,"further, we plan to build on simmr to get closer to the milk representation.",1,2015
D15-1090,"we also plan on parsing a large corpus of text recipes to provide structural features on a large scale that will allow us to discover new patterns of similarity across and within cuisines, as well as generate new recipes.",1,2015
D15-1091,we will also explore how to incorporate prior knowledge about topic relations (such as causation and correlation) into topic modeling.,1,2015
D15-1091,"in future work, we will study how to discover overlapping clusters, i.e., allowing one document to be grouped into multiple topic clusters.",1,2015
D15-1092,"in future work, we would like to investigate the other gating mechanisms for better modeling the feature combinations.",1,2015
D15-1093,"for quantitative evaluation, we need to apply our mehtod to other data such as acm-dl and a large, heterogeneous collection of web content in addition to the experiment to examine the performance agasint the ratio between same-period and diff-period training data.",3,2015
D15-1093,"in the future, we will try to extend the framework to address this issue.",1,2015
D15-1093,"for quantitative evaluation, we need to apply our method to other data such as acm-dl and a large, heterogeneous collection of web content in addition to the experiment to examine the performance against the ratio between same-period and diff-period training data.",3,2015
D15-1095,"besides, we will build upon the outcome of this study by extracting the event sequence in a persons life starting from the complete biographies retrieved from wikipedia.",2,2015
D15-1095,"in the future, we plan to compare our approach based on section titles with more sophisticated approaches considering also the sections’ content, to assess whether the latter improves over our simple methodology.",3,2015
D15-1096,"besides, we would like to compare our algorithm with the algorithms designed for specific word problems, such as (hosseini , 2014).",3,2015
D15-1096,"our future work will focus on studying the performance of applying nonlinear kernel function to the qp problem (3),",4,2015
D15-1096,"and using the word embedding vector (bengio et al., 2003; mikolov et al., 2013) to replace current lexicalized features.",1,2015
D15-1098,"for the future work, we plan to devise embedding models based together on the composition of component-character and of character-word.",1,2015
D15-1098,the two types of compositions will serve in a coordinate fashion for the distributional representations.,1,2015
D15-1100,"for future work, we plan on employing alternative comparison criteria in our model such as those derived from named entity recognition and paraphrase detection.",1,2015
D15-1102,"future work include explorations of efficient algorithms for other information extraction tasks, such as joint mention and relation extraction (li and ji, 2014) and event extraction (li et al., 2013).",1,2015
D15-1104,"for future works, we would like to study how to leverage existing partial labeled data, either for ner or for linking only, in joint optimization, and incorporate more nlp tasks together for multitasks joint optimization.",1,2015
D15-1105,"for example, the hmm alignment model cannot “cross off” a source word and stop trying to translate it.",1,2015
D15-1105,"also possible are phrase-based translation, neural nets, or as-yet-unanticipated pattern finding algorithms.",1,2015
D15-1105,"first, we would like to develop and exploit better predictive translation modeling.",1,2015
D15-1108,"we could also explore other serial update schemes, which generally speed up message-passing algorithms over parallel update.",1,2015
D15-1110,"our next step is to apply the method to other corpora and to more complex text, where the identification of non-participating segments (which are irrelevant for the argumentation) needs to be accounted for.",4,2015
D15-1110,"furthermore, we plan to investigate structured models that not only jointly predict but jointly learn the different aspects of the argumentation graph.",1,2015
D15-1114,"we also plan to use our techniques to support related tasks, such as instructional recipe generation.",4,2015
D15-1114,"future work includes learning a more comprehensive model of locations (e.g., identifying nested locations such as an oven and a pan in the oven), enriching action graphs with greater semantic coverage (e.g., durations, tools, amounts), and training and evaluating on larger datasets.",1,2015
D15-1115,"in the future, we are planning to design a reading comprehension task where we use this framework for answering comparison questions from a paragraph containing various interrelated comparisons.",6,2015
D15-1116,"in future, we plan to perform additional experiments to study the issue of noisy data.",3,2015
D15-1116,"we hope that the release of our datasets will stimulate other studies related to the sarcasm detection problem, including addressing the issue of noisy data.",5,2015
D15-1116,we want to follow their experiments to study whether parameter tuning in pmi based disambiguation can improve its performance.,1,2015
D15-1116,we also plan to study the effect of hyperparameters in designing the dsms.,1,2015
D15-1116,"recently, levy et al.(2015) have argued that parameter settings have a large impact on the success of word embedding models.",1,2015
D15-1117,"we will also enhance the accuracy measure of trustiness, based on the observation that some untrusted sites copy information from other sites to make them look more trustful.",1,2015
D15-1117,"as future work, we will investigate into the task of automatically constructing patterns for the pattern matching methods in sections 3.3 and 3.4, to improve coverage.",1,2015
D15-1119,we are investigating further properties of seg rev and plan to extend it to achieve greater stability and efficiency.,1,2015
D15-1122,our future work aims to incorporate syntactic or semantic information into our paraphrasing framework.,1,2015
D15-1125,"as future work, it would be interesting to explore various distributed word representations for quality estimation and joint models that look at both the source and the target sentences simultaneously.",1,2015
D15-1128,"looking to the future, one important benefit of taking a hierarchical approach is that the re-ordering process is made explicit, and in further research we wish to explore the possibility of introducing of new interpretation-oriented rules into the stream decoding process.",1,2015
D15-1129,"in future work, we will use different techniques to improve the diversity of the string-to-tree rules considered during decoding in our system.",1,2015
D15-1130,"overall, however, we are encouraged to pursue our goal of personalized machine translation.",6,2015
D15-1130,"we would also like to understand the true relationship between linguistic features and traits across languages, along with how native speakers naturally observe these traits.",1,2015
D15-1131,"we are still exploring promising properties of the generated vectors and their applications in other nlp tasks (sentiment analysis, ner...).",4,2015
D15-1134,"in future work, we intend to extend the model to learn larger templates that include syllable structure and phonological tiers (goldsmith, 1976).",1,2015
D15-1135,we hope to extend our techniques to handling general math word problems and to other domains (like physics and chemistry) in the future.,4,2015
D15-1137,our future work is to focus on how to choose/combine different ways of computation.,5,2015
D15-1137,"for instance, we might replace the max pooling by different pooling operations such as mean pooling, k-max pooling (kalchbrenner et al., 2014), and stochastic pooling (zeiler and fergus, 2013).",1,2015
D15-1138,"future work might extend this approach to tasks like question answering, where logicbased approaches have been successful.",4,2015
D15-1139,"in future work, it may be worthwhile to study the impact of alignment techniques on overall system performance in other string transduction problems such as transliteration, lemmatization, and spelling error correction.",5,2015
D15-1141,"in future work, we would like to adopt the bidirectional recurrent neural network (schuster and paliwal, 1997) to process the sequence in both directions.",1,2015
D15-1144,it is also interesting to develop consistency-aware training algorithms for word alignment.,1,2015
D15-1144,"in the future, we plan to apply our approach to syntax-based models (galley et al., 2006; liu et al., 2006; shen et al., 2008) and include the constituency constraint in the optimization objective.",1,2015
D15-1145,"in the future, we plan to further improve our model by capturing semantic relatedness among source words.",4,2015
D15-1145,"additionally, we also want to jointly model different levels of context information in a unified framework for smt.",1,2015
D15-1146,"additionally, we also want to apply our model to other bilingual tasks, e.g., learning bilingual terminology or paraphrases.",4,2015
D15-1146,"in the future, we would like to derive more features from bcorrrae, e.g., consistency/inconsistency scores of bilingual phrases, to further enhance smt.",1,2015
D15-1148,another important future direction lies in text simplification.,2,2015
D15-1148,a similar analysis using ideas from this work can be useful in identifying sentences that needs simplification in the first place.,4,2015
D15-1150,"in the near future, this technique will enable us to release a suite of pos taggers for hundreds of lowresource languages.",4,2015
D15-1152,we also plan to host a demo and make our system available through the website of the computational approaches to modeling language (camel) lab: www.camel-lab.com.,6,2015
D15-1152,"in the future, we plan to investigate the development of joint morphological disambiguation and syntactic parsing models.",1,2015
D15-1152,other possible directions include using more sophisticated machine learning techniques and richer lexical features.,1,2015
D15-1152,we will also work on improving the quality of arabic parsing which is behind many of the errors according to our error analysis.,1,2015
D15-1154,"in future, we are interested in extending our parser to higher-order factorization by increasing horizontal context (e.g., from siblings to trisiblings) and vertical context (e.g., from siblings to grand-siblings) and validating its effectiveness via a wide range of nlp applications.",4,2015
D15-1156,other future work could examine the effectiveness of the approach in the opposite direction (japanese to english) or on other language pairs.,3,2015
D15-1156,we would like to report the results in the future version of this paper.,6,2015
D15-1156,"since the accuracy of the empty category detection implemented as a post-process highly depends on that of the underlying parser, we want to explore models that can solve them jointly, such as the lattice parsing approach of (cai et al., 2011).",1,2015
D15-1157,our next steps include learning error suffixes during a prior tagging phase and experimenting with the french social media bank.,3,2015
D15-1160,"lastly, it would be interesting to adapt fusion to other structured prediction tasks where n-best lists are available.",4,2015
D15-1160,we also intend to explore how to better apply fusion to converted dependencies from constituency parsers.,1,2015
D15-1160,"future work includes applying fusion to n-best dependency parsers and additional (parser, language) pairs.",1,2015
D15-1162,in future work we plan to combine such neural network models with a version of our parser that incorporates a much larger set of nonmonotonic parsing transitions.,1,2015
D15-1163,"in future work, we would like to include translations for infrequent phrases which are not oovs.",2,2015
D15-1163,we would like to explore new propagation methods that can directly use confidence estimates and control propagation based on label sparsity.,1,2015
D15-1164,we also would like to extend our work by using more contextual lexical information to derive semantic vectors for nonterminals.,1,2015
D15-1167,how to simultaneously learn document structure and composition function is an interesting future work.,5,2015
D15-1168,"in the future, we would like apply our models to other fine-grained opinion mining tasks including opinion expression detection and characterizing the intensity and sentiment of the opinion expressions.",4,2015
D15-1168,we would also like to explore to what extent these tasks can be jointly modeled in an rnn-based multi-task learning framework.,1,2015
D15-1170,"therefore, another direction of our future work is to explore specific problems that will emerge when employing tree-based smt systems to semantic parsing, and provide solutions to them.",5,2015
D15-1171,and scaling up the dataset.,2,2015
D15-1171,"future work includes expanding the geometry language and the reasoning to address a broader set of geometry questions, reducing the amount of supervision, learning the relevant geometry knowledge,",1,2015
D15-1177,"as a future step, we aim to test the proposed models on a convolutional compositional architecture, similar to that of kalchbrenner (2014).",3,2015
D15-1178,"we also plan to evaluate the usefulness of conversation trees in tasks such as predicting if a thread is resolved, and user expertise.",3,2015
D15-1178,a main goal for future work is to incorporate further domain specific constraints on the models to improve parsing speed and at the same time allow more flexible trees.,1,2015
D15-1179,future work on these models could explore methods to fit non-common topics for both collections.,1,2015
D15-1181,future work could extend this model to related tasks including question answering and information retrieval.,4,2015
D15-1183,"through learning such latent factors, important summary information of documents would be acquired, which are useful in various applications.",3,2015
D15-1183,"in the future work, we will incorporate global latent factors into this generative model, such as topics, sentiments, or writing styles, and develop more elaborate models of documents.",1,2015
D15-1184,"one possible direction of future work would explore ways to fully represent the rich information in texts by extending the text features and language representations like continuous bag of-words (cbow) models (mikolov et al., 2013) or global vectors for word representation (glove) (pennington et al., 2014).",1,2015
D15-1187,"therefore, in future work, we will focus on investigating an effective method to integrate the local lexical/syntactic information and the global contextual discourse information.",1,2015
D15-1189,"there is significant room for future work to improve the results, including jointly modeling the factuality of multiple events and integrating factuality models into information extraction and question answering systems.",1,2015
D15-1190,we are considering an extrinsic evaluation for these data such as the rte test in future research.,3,2015
D15-1191,"we would also like to test the feasibility of other such strategies, e.g., initializing sme by transe, so as to combine the benefits of both models.",3,2015
D15-1191,"moreover, our approach actually reveals the possibility of a broad idea, i.e., initializing an embedding model by another embedding model.",1,2015
D15-1191,"as future work, we plan to 1) investigate the efficacy of longer ccps (i.e. knowledge paths with lengths longer than 5). 2) design a joint model that encodes lcps and ccps simultaneously.",1,2015
D15-1192,"in general, the proposed approach may prove beneficial for additional tasks that model word meaning in context, such as lexical substitution and sense induction.",4,2015
D15-1192,"we are further interested in creating specialized models that fit different word classes, e.g., of particular part-of-speech.",1,2015
D15-1193,promising future directions would be applying the framework of refd to other contexts such as measuring the prerequisite relations or reading orders between papers and textbooks.,4,2015
D15-1193,also it would be meaningful to explore ranking different prerequisites of a concept.,5,2015
D15-1193,"in addition, refd can be incorporated into existing supervised models for a more accurate measure.",1,2015
D15-1194,"for future work, we aim to investigate the modelling of learned vector representation, such as cbow and glove, within a phrase-based mt model when normalizing medical terms.",1,2015
D15-1196,(2) we will evaluate the performance of our oiwe models in various nlp applications.,3,2015
D15-1196,"(3) we will also investigate possible extensions of our oiwe models, including multiple-prototype models for word sense embeddings (huang et al., 2012; chen et al., 2014), semantic compositions for phrase embeddings (zhao et al., 2015) and knowledge representation (bordes et al., 2013; lin et al., 2015).",1,2015
D15-1196,"in future, we will explore the following research issues: (1) we will extensively investigate the characteristics of oiwe with respect to various hyperparameters including dimension numbers.",1,2015
D15-1202,"hence a similar approach can be targeted towards algebra word problems, a direction we wish to investigate in the future.",5,2015
D15-1205,"moreover, as the model provides a general idea for representing both sentences and sub-structures in language, it has the potential to contribute useful components to various tasks, such as dependency parsing, srl and paraphrasing.",4,2015
D15-1205,we plan to explore the above applications of fcm in the future.,4,2015
D15-1205,"also as kindly pointed out by one anonymous reviewer, our fcm can be applied to the tac-kbp (ji , 2010) tasks, by replacing the training objective to a multi-instance multilabel one (e.g.surdeanu (2012)).",4,2015
D15-1205,"also as kindly pointed out by one anonymous reviewer, our fcm can be applied to the tac-kbp (ji et al., 2010) tasks, by replacing the training objective to a multi-instance multilabel one (e.g. surdeanu et al.(2012)).",4,2015
D15-1205,our next steps in improving fcm focus on enhancements based on task-specific embeddings or loss functions as in hashimoto et al.(2015; dos santos et al.(2015).,1,2015
D15-1208,"we also plan to examine closer the differences between perceived human and fictional personality, and the relationship between the personality of the reader and the characters.",3,2015
D15-1208,"in the future we aim on collecting a more detail and rigorous gold standard through gamification and expanding our work on all five personality traits from the five factor model and their facets, and ultimately extend our system to a semi-supervised model dealing with notably larger amount of data.",1,2015
D15-1209,it is a interesting question why giza++ achieved competitive bleu scores though its alignment accuracy measured by f1 was substantially lower.,5,2015
D15-1209,our future work will focus on increasing the gains in end to-end translation quality through the proposed leave-one-out aligner.,1,2015
D15-1209,"in addition, we plan to improve the proposed method by integrating kneser-ney smoothing.",1,2015
D15-1210,"in the future, we plan to investigate more loss functions to account for syntactic constraints.",1,2015
D15-1212,"as suggested in this paper, future work for parsing morphologically rich languages will require to focus both on feature selection and on the interface between syntax and morphology, which means in our case the interface between the segmenter, the tagger and the parser.",1,2015
D15-1215,"in future work, we would also like to extend our grnns for the other nlp tasks.",4,2015
D15-1216,"in future work, we plan to perform ttp analysis in the case of real users and to optimize the hand-crafted rules introduced here to operate the floor management in the system (when to take/give the floor and according to which ttp scheme) by using reinforcement learning (sutton and barto, 1998; lemon and pietquin, 2012).",1,2015
D15-1220,"in future work, we intend to extend our study to compressive summarization.",4,2015
D15-1221,"in future work, we plan to use more data to train our model, making it easier for our system to actually identify rhyming pairs and use them in new contexts.",2,2015
D15-1221,"in terms of evaluation, we hope to incorporate some method to evaluate the fluency of generated lyrics (addanki and wu, 2014).",3,2015
D15-1221,"lastly, to further avoid over-fitting to the training data and reproducing lyrics with a high similarity, we plan to use weight noise (jim et al., 1996) to regularize our model.",1,2015
D15-1221,"furthermore, we plan to generate lyrics from artists with a varying vocabulary size to see if it is easier to generate lyrics for an artist with a smaller vocabulary.",1,2015
D15-1221,we also plan to encode phoneme features of words to improve rhyme discovery.,1,2015
D15-1222,an effective and accurate automatic evaluation measure will be a big boon to our quest for better text summarization systems.,3,2015
D15-1223,"in the future, we intend to apply the mdl method to keyword extraction, headline generation, and other related tasks.",4,2015
D15-1224,"in future, we will compare existing reg algorithms on our dataset, in a similar experiment to mitchell (2013b).",3,2015
D15-1224,"then, we will extend existing algorithms to take into account other properties such as material (e.g. ""wooden”), components of the referred object (e.g. ""balconies”) etc.",1,2015
D15-1224,"finally, we will incorporate such an algorithm in interactive settings to investigate the influence of user dialogue behavior and the influence of visual features, such as salience (clarke et al., 2013), in order to improve the fit of our predictive model.",1,2015
D15-1225,"moreover, we will also explore the impact of different scale of the dependencies from historical epochs on the distributions of the current epoch.",5,2015
D15-1225,"in future work, we will consider modelling background topics explicitly and investigating more principled ways in setting the weight parameters of the statistics gathered in the historical epochs.",1,2015
D15-1227,"we also deployed coursemirror in a statistics class in spring 2015 and have created gold-standard summaries, which will allow us to both replicate the intrinsic evaluation of this paper with a new and larger dataset as well conduct an extrinsic evaluation beyond rouge scores.",2,2015
D15-1227,"in the future, we plan to have additional annotation to evaluate the relative importance using the student coverage numbers.",3,2015
D15-1227,"finally, we are interested in applying our summarization approach to other types of user-generated content from mobileapplications (e.g., review comments).",4,2015
D15-1230,"in future work, we intend to combine this discourse-based view of coherence with a content based view to create a unified statistical discourse planner.",1,2015
D15-1230,"in addition, we will explore additional stochastic models of discourse that look at other, non-sequential collocational information.",1,2015
D15-1232,this direction should be useful for query-focused summarization tasks.,4,2015
D15-1232,"in future research, we will explore other scaling functions suitable for our problem or different problems.",1,2015
D15-1232,a promising direction is to consider a relative scaling function to extract a biased summary of a document.,1,2015
D15-1234,"finally, it is worth remembering that language provides only a partial description of depicted characters, so we should aim to augment with aural/visual information.",2,2015
D15-1234,we also intend to collect more movie and character level metadata to be used in analysis.,2,2015
D15-1234,"future work will include the use of further metrics, with those describing emotions being the first candidates.",3,2015
D15-1239,future work involves improving the classification algorithm by using new approaches to learning about rare events.,1,2015
D15-1244,"future work will focus on incorporating a robust model of lexical knowledge (lewis and steedman, 2013; tian et al., 2014) to our framework.",1,2015
D15-1247,one can further ensure the advantage of the joint model using a larger corpus.,2,2015
D15-1247,another future work is to incorporate other components of events into the model.,1,2015
D15-1247,"one could leverage them as other learning targets or constraints, and investigate further benefits of joint modeling.",1,2015
D15-1247,"our preliminary experiment on the ace 2005 corpus shows that due to its larger document size and event types, one will need to reduce training time by a distributed learning algorithm such as mini-batches (zhao and huang, 2013).",1,2015
D15-1254,"in future work, we would like to explore more user-specific information for dialect classification, apply the classifier for arabic-to-english mt systems, and extend the approach to a larger family of languages and dialects.",4,2015
D15-1256,"another crucial task is to expand this investigation beyond the united states, as the varying patterns of use for social media across countries (pew research center, 2012) implies that the findings here cannot be expected to generalize to every international context.",4,2015
D15-1257,we hope this work will encourage others to further investigate the most reportable event.,6,2015
D15-1257,"in future work we hope to be able to generate a text description the full mre, which would be better suited to summarization or generating headlines, rather than identifying sentences that refer to it.",1,2015
D15-1258,analysis of classification errors highlighted the challenges associated with the task.,5,2015
D15-1258,"furthermore, the relation between sentiments and suggestions seem to be worth investigating.",5,2015
D15-1258,"the classification results have scope for improvement, and therefore the task calls for advanced semantic features and dedicated models, which will be our future direction.",1,2015
D15-1260,"as future work, we are planning to use commonsense knowledge, such as causality (hashimoto et al., 2014) and script-like knowledge (sano et al., 2014), that has been automatically acquired from big data for accurate subject sharing recognition to improve intersentential zero anaphora resolution for cases not focused on in this work.",1,2015
D15-1262,"in future work, we first plan to extend our comparative framework to a larger set of relations and to other languages.",4,2015
D15-1262,"we also want to explore methods for learning embeddings that are directly related to the task of discourse relation classification, potentially using existing embeddings as initialization (labutov and lipson, 2013).",1,2015
D15-1268,"to this end, we are planning to organize an evaluation workshop on dialogue breakdown detection.",3,2015
D15-1268,"to accurately detect dialogue breakdowns, dialogue systems researchers will need to collaborate.",6,2015
D15-1268,"for future work, we plan to consider ways to improve systems on the basis of our findings and also verify the generality of the results on data using other systems.",1,2015
D15-1269,"in future work, we aim to implement a nonparametric bayes model that will be able to estimate the number of concepts automatically.",1,2015
D15-1274,"since creating such tools is a labor-intensive task, we expect our diacritization approach to promote the development of speech recognizers for other languages and dialects.",1,2015
D15-1274,"in future work, we intend to incorporate our discretization system in a speech recognizer.",1,2015
D15-1276,"we also intend to apply our method to unsegmented languages other than japanese, such as chinese and thai.",4,2015
D15-1276,"in the future, we will design features derived from rnnlm models, and integrate them into a unified learning framework.",1,2015
D15-1277,"it is interesting to apply the symbol grounding results to an embedding model-based word segmentation approach (ma and hinrichs, 2015).",1,2015
D15-1277,it is also interesting to extend our method to deal with other types of non-textual information such as images and economic indices.,1,2015
D15-1277,"as future work, we will apply other deep neural network models to our approach.",1,2015
D15-1280,"in future work, we would like to investigate the other feedback mechanism between the short-term and long-term memories.",1,2015
D15-1281,"in order to develop a fully automated deception deception system, our future work will address the use of automatic gesture and facial expression identification and automated speech transcription.",5,2015
D15-1281,our goal is to move forward towards a real-time deception detection system.,1,2015
D15-1283,"in the future, we plan to extend co-training to include active learning for more robust classification.",1,2015
D15-1283,"moreover, it would be interesting to extend the co-training approach to multi-views that could potentially handle more than two feature spaces, e.g., it could include topics by latent dirichlet allocation (blei et al., 2003) as an additional view.",1,2015
D15-1284,"in the future, we would like to step further into the discovery of humor characteristics and apply our findings to the process of humor generation.",1,2015
D15-1287,"as future work, we will fully integrate our model into a pbsmt decoder and evaluate it on other language pairs with different reordering distributions.",1,2015
D15-1289,"in our future work, we will consider to use the ontology matching approach to the matching between different nlp-oriented ontologies such as wordnet, freebase, yago, etc.",1,2015
D15-1293,another interesting possibility is to improve auditory representations by training a neural network classifier on the audio files and subsequently transferring the hidden representations to tasks in semantics.,1,2015
D15-1293,"in future work, it would be interesting to investigate different sampling strategies for the early fusion joint-learning approach and to investigate more sophisticated mixing strategies for the middle and late fusion models, e.g. using the “audio dispersion” of a word to determine how much auditory input should be included in the multi-modal representation (kiela et al., 2014).",1,2015
D15-1294,"finally, the next steps in this line of research are to integrate the aspectual information attributed to clauses by our model into models of temporal discourse structure, which in turn are useful for information extraction and text understanding tasks in general.",1,2015
D15-1294,costa and branco (2012) are the first to show that aspectual information is relevant here; we hope to show in the future that temporal processing profits from integrating more fine-grained aspectual information.,1,2015
D15-1295,"in the future, we would like to investigate how our approach generalizes across languages and tasks.",5,2015
D15-1299,1.increase the size of the dataset.,2,2015
D15-1299,2.discuss the issue of unbalanced dataset and text classification.,5,2015
D15-1299,3.extend the generated method either automated or manually.,1,2015
D15-1303,our future work will focus on extracting more relevant features from the visual modality.,1,2015
D15-1303,we will employ deep 3d convolutional neural networks on this modality for feature extraction.,1,2015
D15-1303,we will use a feature selection method to obtain key features; this will ensure the scalability as well as stability of the framework.,1,2015
D15-1303,"we will continue our study of reasoning over text (jimenez et al., 2015; pakray et al., 2011; sidorov et al., 2014; sidorov, 2014) and in particular of concept based sentiment analysis (poria et al., 2014b).",1,2015
D15-1304,we also plan to study the cases where english and arabic translations have different sentiments due to cultural differences.,3,2015
D15-1304,"additionally, the work will be extended to the arabic dialects for which aramorph-like morphological analyzers are available.",4,2015
D15-1304,the future plans include manually correcting slsa to reach a nearly 100% accuracy.,1,2015
D15-1305,"in future, we will explore the applications of dependence distributed representation.",4,2015
D15-1308,we also plan to explore larger datasets and features based on network structures.,2,2015
D15-1308,"future research might diverge to other types of online collective action, such as online petitions and open source communities.",6,2015
D15-1309,future work could use a larger training set from multiple online sites to analyze the patterns of online abuse discourse across varied forums.,2,2015
D15-1311,an interesting direction for future work would be adding non-textual features.,1,2015
D15-1311,"for example, the rumour diffusion pattern (lukasik et al., 2015) may be a useful cue for judgement classification.",1,2015
D16-1001.pdf,"in the future,  we hope to achieve still better results using beam search, which is relatively straight-forward given that the parsing system already uses a fixed number of actions.",1,2016
D16-1002.pdf,"regarding the qa application, there are two nat-ural extensions that we want to address, namely todevelop general and automatic entity and predicatelinking mechanisms for large knowledge bases, andto test our approach in datasets that require higherlevels of compositionality such as the qald challenges (unger , 2015) or those datasets pro-duced by wang (2015).",3,2016
D16-1002.pdf,"another extension would be to make the rule extraction more robust against parsing errors, using pairs of forests instead of pairs of trees, similarly as in liuet al. (2009).",1,2016
D16-1002.pdf,one step further in the generalization of the rule ex-traction is to remove the necessity of explicitly pro-viding  cost  functions  such  as  word-to-word  hard-alignments  or  costs  between  tree  fragments,1,2016
D16-1004.pdf,"in particular, it would be fruitful to apply our idea into constituent structure induction for which, to our knowledge, there has been no successful pcfg-based learning algorithm.",4,2016
D16-1004.pdf,future work includes applying our dep constraint into other pcfg-based grammar induction tasks beyond dependency grammars.,4,2016
D16-1008.pdf,"we note that an extension similar to semi-markov or weak semi-markov  (muise  and  lu,  2016)  is  possible  for  our models. we leave this for future investigations.",1,2016
D16-1010.pdf,"in  the  future, we  aim  to  analyze  the  role  of  fuller  syntactic  distributions in restricting overgeneralization patterns.",1,2016
D16-1010.pdf,we aim to further analyze manual and automated methods for semantic feature extraction in future work.,1,2016
D16-1011.pdf,"we could also apply variance reduction techniques to increase stability  of  stochastic  training (cf.   (weaver  and  tao,2001; mnih et al., 2014; ba et al., 2015; xu et al.,2015)).",1,2016
D16-1011.pdf,the encoder and generator  can  be  realized  in  numerous  ways  with-out changing the broader algorithm.,1,2016
D16-1012.pdf,"in future work, we would like to investigate the other sharing mechanisms of neural network based multi-task learning.",1,2016
D16-1013.pdf,"in future work, we plan to test our framework with alternative models for natural language inference (e.g., wang and jiang(2016)), and explore the effect of pretraining such a model specifically on an inference task.",3,2016
D16-1014.pdf,"extending this work beyond causality, we hypothesize that additional embedding spaces customized to the different information needs of questions would allow for robust performance over a larger variety of questions, and that these customized embedding models should be evaluated both directly and indirectly to accurately characterize their performance.",3,2016
D16-1015.pdf,"in the future, we would like to investigate how the abstraction and explicit type inference can be incorporated in the early stage of semantic parsing for generating better candidate logical forms.",1,2016
D16-1017.pdf,"in future work, we intend to test the potential contribution of this model when applied to larger tasks such as entailment and inference tasks as well as semantic surprisal-based prediction tasks.",3,2016
D16-1018.pdf,"finally, we plan to evaluate our model with more nlp tasks.",3,2016
D16-1018.pdf,"also,  we  will  study unsupervised methods to link the learned senses to existing inventories and to automatically determine the numbers of senses.",1,2016
D16-1018.pdf,"besides, we plan to use shared senses instead of lexemes in our model to improve the  generality  of  our  model.",1,2016
D16-1018.pdf,"for the future work, we plan to try learning our model with soft em.",1,2016
D16-1019.pdf,"for future work, we would like to (i) investigate the efficacy of incorporating other types of logical rules such as∀x, y, z: (x,capital-of, y)⇒¬(x,capital-of, z). (ii) investigate the possibility of modeling logical rules using only relation embeddings as suggested by demeester et al. (2016),e.g., modeling the above rule using only the embed-ding associated with capital-of.  this avoids grounding, which might be time and space inefficient especially for complicated rules. (iii) investigate the use of automatically extracted rules which are no longer hard rules and tolerant of uncertainty.",1,2016
D16-1020.pdf,"future work includes extending the representations to new contexts – such as the alternative lexicalization an-notated in the pdtb, the modals or some adverbs– using more sophisticated weighting schemes (le-bret  and  collobert,  2014)  and  testing  this  strategy for other languages and domains.",1,2016
D16-1024.pdf,the sentiment lexicon is also another kind of useful resource for classification.,3,2016
D16-1024.pdf,we will explore how tomake full usages of these resources in the proposed framework.,3,2016
D16-1024.pdf,"in future work, we will evaluate the performance of our model on more datasets and more language pairs.",3,2016
D16-1025.pdf,"while nmt proved superior to pbmt with respect to all error types that were investigated, our analysis also pointed out some aspects of nmt that deserve further work, such as the handling of long sentences and the reordering of particular linguistic constituents requiring a deep semantic understanding of text.",1,2016
D16-1026.pdf,"we expect even higher improvement with more languages, but it must be tested thoroughly in the future.",3,2016
D16-1026.pdf,"considering that this is the first attemptat such zero-resource, or extremely low-resource, translation using neural machine translation, we expect a large progress in near future.",6,2016
D16-1029.pdf,"we  hope  the  proposed  approach  can  shed light on how to leverage data on the web, and eventually  improves  other  semantic  parsing  tasks  suchas knowledge base question answering and mappingnatural instructions to actions.",4,2016
D16-1032.pdf,"the neural checklist model can also be adapted to handle multiple checklists,  such as checklists over composite entities created over the course of a recipe(see kiddon (2016) for an initial proposal).",1,2016
D16-1032.pdf,"the neural check-list model is sensitive to hyperparameter initialization,  which  should  be  investigated  in  future  work.",1,2016
D16-1035.pdf,we will try extending attention mechanism to obtain the representationof  a  text  span  by  referring  to  another  text  span  at minimal additional cost.,1,2016
D16-1036.pdf,we will extend our framework to response generation approaches in our future work.,1,2016
D16-1036.pdf,we believe it will help construct a  better  representation  of  context  in  the  encoding phrase of dnn-based generation model and thus improve the performance.,1,2016
D16-1037.pdf,"furthermore,  we  are  also  interested  in  adapting  our model to other similar tasks, such as nature language inference.",4,2016
D16-1037.pdf,"in the future, we would like to exploit the utilization of discourse instances with explicit relations for implicit drr.",1,2016
D16-1037.pdf,for this we can start from two directions:  1)  converting  explicit  instances  into  pseudo implicit instances and retraining our model;  2) developing  a  semi-supervised  model  to  leverage  semantic information inside discourse arguments.,1,2016
D16-1038.pdf,"one of the key research directions is to extend this unsupervised approach to a range of other relations among events, including temporal and causality relations, as is (do et al., 2011; do etal., 2012).",1,2016
D16-1040.pdf,"as part of future work, we hope to analyze sictf further, as-421 sign labels to induced categories, and also apply the model to more domains.",4,2016
D16-1041.pdf,for future work we are planning to apply this strategy to learn large-scale semantic relations beyond hypernymy.,4,2016
D16-1041.pdf,"finally, we see potential in the domain clustering approach for improving graph-based taxonomy learning systems, asit can serve as a weighting measure as to how pertinent a given set of concepts in a taxonomy are for a specific domain.",4,2016
D16-1041.pdf,"as mentioned in section 5.2.2, we are also planning  to  combine  our  distributional  approach  with rule-based heuristics, following the line of work introduced by shwartz et al. (2016).",1,2016
D16-1041.pdf,"in the context of semantic web, we would like to include semantic parsers and distant supervision to our algorithm in order to capture n-ary relations between pairs of concepts to further create and im-prove existing kbs.",1,2016
D16-1042.pdf,another  idea  we  want  to  explore  is  to  use different  distributions  as  a  prior  to  multinomials.,1,2016
D16-1043.pdf,we hope that this work may be used as a reference in determining some of the choices that can be made when developing multi-modal models.,1,2016
D16-1046.pdf,more efforts are also needed in developing an effective yet robust method for multi-task learning.,1,2016
D16-1046.pdf,"our work also points out some future directions of research, for example, we would like to analyze the effect of different mult strategies.",1,2016
D16-1047.pdf,"clearly we would expect this model to be more effective in languages with richer morphological structure than english, and we plan to explore this possibility in future work.",1,2016
D16-1047.pdf,"if we follow tsvetkovet al. (2015) in the argument that word embeddings should correspond to lexical semantic features, then an  inventory  of  such  features  could  be  used  as  a source of partial supervision,  thus locking dimensions of the word embeddings to specific semantic properties.",1,2016
D16-1048.pdf,"hard similarization could be more suitable for cross-lingual applications, and we leave thisaspect for future research.",4,2016
D16-1048.pdf,"the cross-lingual similarization in this paper is still soft similarization, it is worth to investigate the hard similarization, where the  syntactic  structures  are  totally  isomorphic  between two languages.",1,2016
D16-1048.pdf,"of course, in such syntactic structures, the syntactic nodes should be super-node, that is, a graph containing one or more basic syntac-tic nodes.",1,2016
D16-1049.pdf,"although we evaluated the correlation between the estimated system performance scores and the wmt official scores, other evaluation procedures might also be considered.",3,2016
D16-1049.pdf,it will be required to investigate correlation between the estimates and expert decisions.,1,2016
D16-1049.pdf,"in  the  future  work,  we  will  incorporate  active learning  to  the  proposed  method  so  that  we  could reduce the total number of comparisons to obtain final results.",1,2016
D16-1050.pdf,"in  the  future,  since  the  latent  variable  in  our model is at the sentence level,  we want to explore more  fine-grained  latent  variables  for  neural  machine translation, such as the recurrent latent vari-able model(chung et al., 2015).",1,2016
D16-1050.pdf,We are also interested in applying our model to other similar tasks.,4,2016
D16-1053.pdf,"in the future, we hope to develop more linguistically plausible neural architectures able to reason over nested structures and neural models that learn to discover compositionality with weak or indirect supervision.",1,2016
D16-1055.pdf,another extension of this work would be to apply the same translation for translating answers into the question language (in addition to question translation).,2,2016
D16-1055.pdf,"one potential next step is to learn bilingual embeddings directly for the task of qa,  for  which  we  have  started  adapting  some  re-lated work (bai et al., 2010).",1,2016
D16-1055.pdf,"finally,  since one of the take-away messages of our work is that a deeper understanding of linguistic context can improve qa effectiveness via more sophisticated question translation, we are hoping to see even more improvements by creating features based on word embeddings.",1,2016
D16-1055.pdf,more sophisticated merging of multiple ranked lists of answers need to be explored.,1,2016
D16-1055.pdf,learning  to  rank  between  answers  from  different languages might be more effective than heuristics.,1,2016
D16-1057.pdf,"in the future our method could also be integrated with supervised domain-adaption (e.g.,yang andeisenstein, 2015) to further improve these domain-specific results.",4,2016
D16-1057.pdf,we  hope  these tools will facilitate future quantitative studies on the domain-dependency of sentiment.,1,2016
D16-1058.pdf,"as future work, an interesting and possible direction would be to model more than one aspect simultaneously with the attention mechanism.",1,2016
D16-1061.pdf,"in future work, we will investigate the efficiency ofthe proposed approach in other datasets and exploreother methods in capturing the inter-relations of e-motions.",2,2016
D16-1062.pdf,future work can adapt this analysis to create evaluation mechanisms for other nlp tasks.,3,2016
D16-1065.pdf,"in addition, we are interested in further incorporating the incremental semantic role labeling into our incremental  framework  to  allow  bidirectional  information flow between the two closely related tasks.",1,2016
D16-1065.pdf,"in  future  work,  we  plan  to  improve  the  parsing performance  by  exploring  more  features  from  the coreference resolution,  word sense disambiguation system  and  other  external  semantic  resources.",1,2016
D16-1067.pdf,"in  future  research, we aim to automatically estimate the number  of  topics  to be  used  in  the  eup  models.",1,2016
D16-1067.pdf,we also  plan  to  explore  the  use  of  more  external  resources and  novel  latent  semantic  models to  enhance performance.,1,2016
D16-1069.pdf,"in the future, we plan to integrate the  dynamic  dictionary  into  the  term  construction model in information retrieval.",1,2016
D16-1071.pdf,"as morphological analyzers are becoming more widely available, our method – which is easy to implement, only requiring running the analyzer should become applicable to more and more languages.",4,2016
D16-1073.pdf,"in addition, we plan to apply our approach to other unsupervised tasks such as word alignment and sentence cluster-ing.",4,2016
D16-1073.pdf,"for future work, we plan to extend our approach in  learning  lexicalized  dmv  models.",1,2016
D16-1075.pdf,"in the future, we plan to generalize the stream summarization problem to various streams such associal (e.g., twitter), image (e.g., imgur) and evenvideo streams (e.g., youtube), which would yield many interesting and practical applications (lu etal., 2016) to deal with the information overload challenge in the big data era.",4,2016
D16-1076.pdf,"moving  forward,  we  plan  to  explore  additional mechanisms for exploiting supervision at lower levels in neural architectures.",1,2016
D16-1076.pdf,"furthermore, we believe an alternative approach may be a hybrid of the at-cnn  and  ra-cnn  models,  wherein  an  auxiliary loss  might  be  incurred  when  the  attention  mechanism output disagrees with the available direct supervision on sentences.",1,2016
D16-1078.pdf,this  suggests our future direction to extend the model from token level  to  parse  tree  level in  better  capturing long-distance syntactic dependency and to address the cross-domain adaptation issue.,1,2016
D16-1082.pdf,these results suggest that the structured prediction models are good directions for improving the exact phrase extraction for clinical entities.,1,2016
D16-1083.pdf,"in future work, we plan to explore a more effective way to learn the embeddingsof review text.",1,2016
D16-1084.pdf,future work will investigate further the challenge of stance detection for tweets which do not contain explicit mentions of the target.,5,2016
D16-1085.pdf,"in the future, we plan to investigatethe non-consecutive architecture on other problemssuch as relation extraction or slot filling.",5,2016
D16-1086.pdf,"in the future, studies targeting less similar languages could further evaluatethe portability of props.",3,2016
D16-1086.pdf,"directions for future work on propsde are extensions of the rule set to better cover  complex  coordination  constructions,  nested sentences and nominal predicates.",2,2016
D16-1088.pdf,"while our approach is certainly tailored to the civil unrest domain, we believe that this method is applicable to many other domains within the scope of news reports, including health, economics and even politics, where reporters overwhelmingly rely on outside opinion to present the facts of the story and provide the summary themselves.",4,2016
D16-1090.pdf,"one possibility includes identifying the premise entailedin a question, as opposed to just stating true- or false-premise.",6,2016
D16-1092.pdf,our work thus poses an interesting question for future work – what is the right semantic space in which it is meaningful to talk about necessary and sufficient attention maps for humans?,5,2016
D16-1093.pdf,"as to future work, we plan to apply residual learning to other sequence tasks such as language modeling, and rnn based neural machine translation (sutskever ,2014) (cho , 2014).",4,2016
D16-1094.pdf,"in  future  work, we will integrate our language model into the moses machine translation pipeline to intrinsically measure its impact on translation qualities, which is of particular use for out-of-domain scenario.",1,2016
D16-1095.pdf,in  future  work  we could  also  seek  to  find  methods  that  can  auto-matically  optimize  these  parameters.,1,2016
D16-1097.pdf,"in future work, we hope to develop models to exploit this.",1,2016
D16-1099.pdf,"this interpretation suggests that it might be interesting to investigate hybrid models that use different processing models — or at least different parameterizations — for different frequency ranges, and for different data sizes.",1,2016
D16-1100.pdf,"as future work, we would like to 1) investigate more complicate composition manners among radicals, characters, and words, e.g., a hierarchical structure of them. 2) explore the semantic composition of higher level language units such as phrases, sentences, and even documents.",1,2016
D16-1101.pdf,"in future work, we will plan to apply numerically grounded models to other tasks, such as numeric error correction.",4,2016
D16-1101.pdf,"for sec, a trainable  hypothesis generator can potentially improve the coverage of the system.",1,2016
D16-1101.pdf,"we will explore alternative ways for deriving the numeric representations, such as  accounting  for  verbal  descriptions  of  numbers.",1,2016
D16-1104.pdf,"our word embedding-based features may work better if the similarity scores are computed for a subset of words in the sentence, or using weighting  based  on  syntactic  distance  instead  of  linear distance as in the case of our weighted similarity-based features.",1,2016
D16-1105.pdf,"thus,  automatic  detection  of  targets  and  inferring the  stance  towards  all  of  the  targets  is  the  next step toward creating a practical weakly-supervised stance classifier.",1,2016
D16-1106.pdf,"applying core nlp technologies to local news reports of gun violence couldtransform raw text into structured, queryable datathat public health researchers can use.",4,2016
D16-1107.pdf,"planned future work includes  a  more  flexible  decoupling  of  the  language and structure fits (in light of section 5),  and moving from pre-existing lid systems to joint models where lid scores are directly informed by structural information.",1,2016
D16-1111.pdf,"in the future, we plan to create cross-lingual models by applying multilingual word embeddings (ammar et al., 2016).",1,2016
D16-1113.pdf,"in future work, by taking advantage of the promising results of robust gram, we intend to explore the model’s  behavior  in  various  settings.",1,2016
D16-1113.pdf,"we also plan to enhance the underlying optimization by designing elastic constraints (zouand hastie, 2005) specialized for word embeddings.",1,2016
D16-1113.pdf,"in  particular, we plan to model various corpora, i.e. predictive modeling of sequentially arriving network packages.",1,2016
D16-1113.pdf,"another  future  direction  might  be  encoding  avail-able domain knowledge by additional regularization terms, for instance, knowledge on synonyms can be used to reduce the degrees of freedom of the optimization.",1,2016
D16-1116.pdf,a natural extension would also be a formulation where semi-supervised training was performed in both x and y.,1,2016
D16-1117.pdf,"our future work will focus on lifting this restriction,in order to allow relations expressed across multiple sentences and multiple relations expressed in thesame sentence.",5,2016
D16-1118.pdf,"another extension of the current work is to investigate a similar approach for other modality markers such as nouns (e.g., possibility, chance), adjectives (e.g. necessary, probable, ) and certain verbs(e.g., claim, suggests).",1,2016
D16-1118.pdf,"we believe better results could be obtained by incorporating features capturing knowledge in the context of the modal construction, including other clauses in the same sentence, and the previous and next sentences.",1,2016
D16-1120.pdf,n  interesting  direction for future research would be to combine distant supervision with unsupervised linguistic models to au-tomatically uncover such underrecognized dialectal language.,1,2016
D16-1121.pdf,"how do such preferences vary across topics, users and other multilingual communities?",5,2016
D16-1121.pdf,how representative of the society is this kind of social media study?,5,2016
D16-1121.pdf,"is it the case that english being the language of aspiration in india, it is preferred for positive expression?",5,2016
D16-1121.pdf,"or is it because hindi is specifically preferred for swearing and therefore, is the language of preference for negative emotion?",5,2016
D16-1121.pdf,this raises some intriguing socio-linguistic questions.,5,2016
D16-1121.pdf,we plan to explore some of these questions in the future.,6,2016
D16-1122.pdf,"we anticipate that capsule, and ourvisualization tool, will be useful for historians, political scientists, and journalists who wish to explore and understand large corpora of documents.",4,2016
D16-1123.pdf,"since cnns are quite different in nature, we believe that a fruitful line of future research could focus  on  integrating  the  convolutional  layer  into  are current  structure  for  language  modeling,  as  well as other sequential problems, perhaps capturing the best of both worlds.",1,2016
D16-1124.pdf,"as the framework discussed here is general, it is also possible that they could be used in other tasks that perform sequential prediction of words such asneural machine translation (sutskever , 2014)or dialog response generation (sordoni , 2015).",4,2016
D16-1124.pdf,"in  addition,  given  the  positive  results  using  block dropout for hybrid models, we plan to develop more effective  learning  methods  for  mixtures  of  sparse and dense distributions.",1,2016
D16-1125.pdf,we believe that this general strategy of using reasoning to obtain novel contextual behavior from neural decoding models might be more broadly applied.,4,2016
D16-1126.pdf,"we hope that future work will provide more discourse structure and function to automatic poetry, while  maintaining  the  syntax,  semantics,  and  creative phrasing we observe.",1,2016
D16-1128.pdf,"in this paper, we have only focused on generating the first sentence and we will tackle the generation oflonger biographies in future work.",2,2016
D16-1128.pdf,a loss function that could assess factual accuracy would certainly improve sentence generation by avoiding such mistakes.,3,2016
D16-1131.pdf,"one potential path would be to leverage data where discourse relations are explicitly annotated, such as that in the penn discourse treebank (prasad , 2008).",2,2016
D16-1131.pdf,"in closing, we would like to return to the larger question of effectively handling ellipsis.",5,2016
D16-1131.pdf,"in future work, we hope to improve the performance of several of our features.",1,2016
D16-1131.pdf,"in addition, although our content and correlate features were not useful alongside the  others,  we  hope  that  more  refined  versions  of those could provide some assistance.",1,2016
D16-1131.pdf,"we also noted that  our  performance  was  impacted  by wh-types, and therefore it might be helpful to learn differen tfeatural weights per type.",1,2016
D16-1132.pdf,"as future work, we plan to use our mcnn architecture for inter-sentential zero an aphora resolution and develop highly accurate nlp applications using our intra-sentential subject zero an aphora resolution method.",4,2016
D16-1133.pdf,"finally, we will investigate downstream applications of our alignment methods, in the areas of both language documentation and speech translation.",4,2016
D16-1133.pdf,"we plan to try incorporating ibm model 3 or the hmm alignment model(vogel  et  al.,  1996)  instead.",1,2016
D16-1134.pdf,"we  believe that hume, and a future automated version of hume, allows for a finer-grained analysis of translation quality, and will be useful in informing the development of a more semantically aware approach to mt.",1,2016
D16-1136.pdf,"our  combination  techniques during training, especially using regularization, are highly effective and could be used to im-prove monolingual word embeddings.",1,2016
D16-1137.pdf,future work will examine scaling this approach to much larger datasets.,2,2016
D16-1138.pdf,for future work we would like to incorporate attention-based models to our framework to enable such models to process data online.,1,2016
D16-1139.pdf,we anticipate that methods described in this paper can be used to similarly train smaller models in other domains.,4,2016
D16-1142.pdf,"in future, we shall look into more linguistic evidences to improve our model.",1,2016
D16-1143.pdf,"this framework has good scalability and can be applied on other concrete features, such as users book reading behaviors and music listening behaviors.",4,2016
D16-1144.pdf,"the proposed objective function is general and can be considered to incorporate various language features, to conduct integrated modeling of multiple sources, and to be extended to distantly-supervised relation extraction.",1,2016
D16-1144.pdf,"as future work,  it would be interesting to study topical features as the context cues of the entity mentions, to leverage multi-sensing embedding to represent linguistic features with multiple senses, and to exploits other effective modeling methods to inject type hierarchy information.",1,2016
D16-1145.pdf,"in the future, we are interested in exploring mining formulas directly in the distributional spaces which may resolve the sparsity of formulas.",1,2016
D16-1146.pdf,"in future work,  we want to extend the proposed ideas  beyond  implications  towards  general  first-order logic rules.",1,2016
D16-1146.pdf,"furthermore, we want to integrate these ideas into neural methods beyond matrix factorization approaches.",1,2016
D16-1146.pdf,"we believe that supporting conjunctions, disjunctions and negations would enable to debug and improve representation learning based knowledge base completion.",1,2016
D16-1147.pdf,"these models could be applied to storingand reading memories for other tasks as well, andfuture work should try them in other domains, such as in a full dialog setting.",4,2016
D16-1149.pdf,"our  current  plans  include  continued corpus development (recall section 4.4), and using more sophisticated methods than dyad averaging(e.g., using weighting based on team process measures) to move from dyads to teams.",1,2016
D16-1152.pdf,"we would also like to investigate other metadata attributes that are relevant to the task, such as spatial and temporal signals.",3,2016
D16-1152.pdf,"social networks arise in other settings besides microblogs, such as webpages and academic research articles; exploiting these networks is a possible direction for future work.",6,2016
D16-1153.pdf,we believe these results to be encouraging and look forward to replicating these results on more language pairs in different language families and further automating the process of obtaining phonological character representations.,4,2016
D16-1154.pdf,"this work, however, did not investigate the nature of the long and short context encodedby this network or its possible applications for other nlp tasks.",4,2016
D16-1155.pdf,our future work will  explore  incremental  learning  through  human-agent dialogue to acquire grounded task structures.,1,2016
D16-1156.pdf,"while we have demonstrated our approach ona task involving simultaneous reasoning about language and vision, our approach is general and canbe used for other applications.",4,2016
D16-1158.pdf,"another future area is to see if the techniques formaking bayesian networks discriminative can fix the length bias of encoder decoder networks (peharz etal., 2013; guo , 2012).",5,2016
D16-1158.pdf,an interesting future work is to explore if the ed model can be used to generate a candidate set of responses which are then reranked by our globally conditioned model.,1,2016
D16-1160.pdf,"we also plan to apply our methodsin other languages, especially for low-resource languages.",4,2016
D16-1160.pdf,"in  the  future,  we  would  like  to  design  smarter mechanisms to distinguish real data from synthetic data in self-learning algorithm, and attempt to pro-pose better models for handling source-side mono-lingual  data.",1,2016
D16-1162.pdf,"for future work, we are interested in conductingthe experiments on larger-scale translation tasks.",2,2016
D16-1162.pdf,"we also plan to do subjective evaluation, as we expectthat improvements in content word translation arecritical to subjective impressions of translation re-sults.",3,2016
D16-1162.pdf,"finally, we are also interested in improve-ments to the linear method where λ is calculated based on the context, instead of using a fixed value.",1,2016
D16-1164.pdf,"considering the usage of other kinds ofdata (e.g., tags, comments) as additional “spaces” to extend the cqa clustering problem is an interestingdirection for future work.",2,2016
D16-1164.pdf,the extension of the formulation to include a weight learning step may be appropriate for scenarios  where  prior  information  on  the  relative  importance of the different spaces is not available.,1,2016
D16-1165.pdf,"we further want to apply a similar network to other semantic similarity problems, such as textual entailment.",4,2016
D16-1165.pdf,"in future work, we plan to use the labels for sub-tasks a and b, which are provided in the datasets in order to pre-train the corresponding components of the full network for answer ranking.",1,2016
D16-1166.pdf,in the future we want extend our system to multi-relation questions.,4,2016
D16-1167.pdf,"while the experiments only focused on sequence-to-sequence decoding, our preliminary experiments with otherhigh-capacity deep neural nets seem promising.",3,2016
D16-1167.pdf,another future work direction is to derive efficient mechanisms to guide the humans that are creating the data generation programs.,1,2016
D16-1168.pdf,"future work could improve the thematicity andsolvability components by incorporating domain-specific and commonsense knowledge, leveraginginformation extraction.",2,2016
D16-1168.pdf,"finally, we in tend to incorporate the generated problems in educational technology and tutoring systems.",1,2016
D16-1168.pdf,"additionally,  we  plan  to study rewriting in other domains such as children's short stories and extend the model to generate math word problems directly from equations.",1,2016
D16-1171.pdf,we will take advantages of those information in sentiment analysis infuture.,2,2016
D16-1171.pdf,we will explore the effectiveness of our model on aspect level sentiment classification.,1,2016
D16-1172.pdf,"for future work, we are going to design a strategyto dynamically adjust the forgetting rates for fine-grained document-level sentiment analysis.",4,2016
D16-1173.pdf,"in the future work, we plan to apply our framework too ther text and vision applications.",4,2016
D16-1174.pdf,we also plan to evaluate the performance of our approach with other decay functions as well as with other initial word representations.,3,2016
D16-1174.pdf,we intend to apply our technique to the task of harmonizing biomedical terms in the phenebank project.,4,2016
D16-1174.pdf,"as future work, we plan to investigate the possibility of using larger semantic networks, such as free-base and babelnet,  which would also allow us to apply the technique to languages other than english.",1,2016
D16-1175.pdf,we further more plan to investigate whether the number of neighbours required for improving elementary vector representations remains as high for other compositional tasks and longer phrases as in this study.,5,2016
D16-1175.pdf,in future work we aim to scale our approach to semantic composition with distributional inference to longer phrases and full sentences.,1,2016
D16-1176.pdf,"in future work, we would like to incorporate some gating strategies into the depth dimension of our proposed models, like highway or residual network, to enhance the interactions between depth and other dimensions thus training more deep and powerful neural networks.",1,2016
D16-1177.pdf,"in future work, we intend to further revise and extend these protocols as well as produce novel protocols aligned with decomp.",1,2016
D16-1179.pdf,we also seek to improve the antecedent identification approach by extracting stronger features.,1,2016
D16-1179.pdf,"in  future  work,  we  would  like  to  further  investigate  other  margin-based  optimizations  similar  to mira,  but  perhaps  even  more  resilient  to  over-fitting.",1,2016
D16-1181.pdf,"for future work, a natural directions to explore integrated super tagging and parsing ina single neural model (zhang and weiss, 2016).",1,2016
D16-1182.pdf,"while there are some previous works on  fixing  the  unnecessary  words  in  the  literature(xue and hwa, 2014), it is worthy to develop better nlp methods for catching and mitigating the missing word errors prior to parsing.",1,2016
D16-1186.pdf,"since there was no assistant system to check the  quality  for  many  years  (hussain  et  al.,  2007)we  plan  to  extend  the  provided  system  in  order to  provide  some  quality  analysis  of  the  extracted information.",1,2016
D16-1186.pdf,"based on these insights, we want  to  implement  a  system  for  the  resolution  of vagueness and incompleteness of nl requirements.",1,2016
D16-1187.pdf,"for future work, we plan to create much more ground truth review data from other review domains and different applications like forums or microblogs, and further test our proposed models for deceptive opinion spam detection.",1,2016
D16-1187.pdf,"we also plan to incorporate our model into a practical opinion mining system, in this way, more reliable opinion and sentiment analysis results can be then expected.",1,2016
D16-1188.pdf,"future work could involve a more thorough investigation on how to create and cluster graphs, i. e.covering weighted and/or signed cases.",5,2016
D16-1188.pdf,"additionally, we plan on exploring alternative regularization algorithms diverging from the group-lasso method.",1,2016
D16-1188.pdf,"gaussian mixture models (mclachlan and basford, 1988) could be applied in order  to  capture  overlapping  groups  at  the  cost  of high complexity.",4,2016
D16-1188.pdf,"furthermore, topical word embeddings (liu et al.,  2015) can be considered for regularization.",1,2016
D16-1189.pdf,"it would also be of interest to explore implementations of backtracking in the sub-action space (incurring accost),  in  order  to  recommend comments  that  were not selected earlier but have become highly popular.",1,2016
D16-1190.pdf,"finally,analyzing further languages and data sets helps tofurther complete our findings.",2,2016
D16-1190.pdf,whether learning edit scriptson such intricate transformations is possible is anopen question and valuable future research.,5,2016
D16-1192.pdf,"in future work, we plan to increase the numberof sentences in our data set, so that additional morefine-grained features might be considered.",2,2016
D16-1192.pdf,"finally, we are interested in the contribution of context in understanding the meaning of an unknown word.",1,2016
D16-1192.pdf,another goal is to obtain absolute difficulty labels for sentences by calibrating ordinal ranges based on the relative ranking.,1,2016
D16-1192.pdf,"our use of the crowd-based labels was intended to reduce noise in the ranking analysis, but we also intend to use the pairwise predictions produced by the logistic model as the input to the aggregation model, so that rankings can be obtained for previously unseen sentences in operational settings.",1,2016
D16-1199.pdf,we plan to test our models ability to generalize to other typesof infobox-like tables on the web.,3,2016
D16-1203.pdf,"our motive is simply to better understandcurrent generation models via their behaviors, anduse these observations to guide future choices – dowe need novel model classes? or dataset with dif-ferent biases? etc.",5,2016
D16-1207.pdf,"for future work, we plan to apply our regular-ization method to other models and tasks to deter-mine how generally applicable our method is.",4,2016
D16-1207.pdf,"also,we will explore methods for more realistic linguisticnoise, such as lexical, syntactic and semantic noise,to develop models that are robust to the kinds of dataoften encountered at test time.",1,2016
D16-1210.pdf,"in future work, we plan to incorporate a phrasalitg (cherry and lin, 2007) instead of a bracketingitg to efficiently handle many-to-many alignments.",1,2016
D16-1212.pdf,this method can also be used in other probabilistic methods.,1,2016
D16-1213.pdf,we hope that this contribution will bring brain imaging tests to the masses and encourage discussion around the testing of ds models against brain imaging data.,3,2016
D16-1214.pdf,our ongoing work explores creating a syntax-semantics loop where each benefits the other with no human (annotation) in the loop.,1,2016
D16-1215.pdf,"although tested on english, the proposedmethodology can be applied to all languages in the ppdbeven to the ones that do not dispose of a de-pendency parser (as shown by the high performanceof the bow.vec models).",4,2016
D16-1215.pdf,"we consider using a vector space  model  of  semantic  composition  to  calculate the meaning of longer candidate paraphrases (dinuet al., 2013; paperno et al., 2014) and select appropriate substitutes for phrases in context.",1,2016
D16-1217.pdf,our findings suggest that further work isneeded to understand when automatic translation oflanguage-based models will lead to competitive sen-timent translation on social media and how suchtranslations can be improved.,5,2016
D16-1217.pdf,"we expect that language indicators of personality and emotion willsimilarly translate poorly, but that remains to bestudied.",5,2016
D16-1218.pdf,"many questions remain; in providing a quantitative descriptionof this dataset, we hope to highlight its potential foranalysis, and encourage other work in this domain.",5,2016
D16-1222.pdf,"also, we plan to extend the constrained decoding idea to slot tagging with neural networks (kim et al., 2016), which achieved gains over crfs.",1,2016
D16-1222.pdf,"one of the future directions of research is to extend the same idea to the intent modeling, where we can re-use intent data built for different applica-tions and domains for a new domain.",1,2016
D16-1223.pdf,leveraging our encoder-labeler lstm approach in joint training should be worth trying.,1,2016
D16-1225.pdf,"future work includes applications in informal texts, such as tweets andshort messages (muis and lu, 2016).",4,2016
D16-1228.pdf,"we  believe that, for future work, fluency measures could be further improved with other methods, such as using existing gec systems to detect errors, or even using an ensemble of systems to improve coverage (indeed, ensembles have been useful in the gec task itself  (susanto  et  al.,  2014)).",1,2016
D16-1230.pdf,future work should examine how retrieving additional responses affects the correlation with word-overlap metrics.,1,2016
D16-1231.pdf,"hence, the prediction ofwhether torespondin a multi-party conversation would be animportant next challenge.",5,2016
D16-1231.pdf,our future objective to tackle the task of predict-ingwhether to respondto a particular utterance.,5,2016
D16-1232.pdf,"this empirical evidence suggests that the value-based formulation is a promising approach for arbitrary slot filling tasks, which is worth exploring further in future work.",1,2016
D16-1233.pdf,it would be very helpful if the same comparison could be conducted inother application domains such as machine transla-tion or image caption generation so that a wider viewof the effectiveness of these approaches can be as-sessed.,3,2016
D16-1233.pdf,"furthermore, removing slot-value delexical-isation and learning confirmation behaviour in noisy speech conditions are also main research problems from the system development prospective.",5,2016
D16-1235.pdf,in future work we aim to apply suchmethods to the task of verb acquisition.,5,2016
D16-1236.pdf,"future work includes jointly processing all 61 languages inthe corpus, rather than considering them pair-wise, to build a resource for all languages.",2,2016
D16-1239.pdf,"as a first step towards this goal, we plan to carry out an in-depth analysis of disagreement in the collected data, characterize the main sources of inconsistent annotation and subsequently formulate further strategies for improving annotation accuracy.",2,2016
D16-1239.pdf,"we believe that better understanding of human disagreements and their relation to disagreements between humans and parsers will also contribute to advancing evaluation methodologies for pos tagging and syntactic parsing in nlp, an important topic that has received only limited attention thus far (schwartz , 2011; plank , 2015).",3,2016
D16-1239.pdf,"at the same time, it is expected to achieve annotation quality comparable to human-based annotation by avoiding parser specific bias, which plays a pivotal role in the reduced quality of single-parser reviewing pipelines.",1,2016
D16-1242.pdf,"the attractiveness of a logic-based system is that it is highly modular and can be extended with other components such as a robust knowledge base (lewis and steed-man, 2013; beltagy et al., 2013; bjerva et al., 2014).such an extension will be a focus of future work.",1,2016
D16-1243.pdf,"another extension we plan to investigate is modeling of context, to capture how people describe colors differently to contrast them with other colors via pragmatic reasoning (devault and stone, 2007; gol-land et al., 2010; monroe and potts, 2015).",1,2016
D16-1243.pdf,"one  natural  extension  is  theuse of character-level sequence modeling to capture complex  morphology  (e.g.,  “-ish”  in  “greenish”).",1,2016
D16-1247.pdf,"it is possible to adopt existing models of creating vector representation of dependency sub-trees such as the model using recursive neural net-works (liu et al., 2015) and convolutional neural networks (mou et al., 2015).",1,2016
D16-1248.pdf,"in future work, we hope to determine how other  parts  of  the  translator  work,  especially  with reference to grammatical structure and transformations.",1,2016
D16-1250.pdf,"in the future, we would like to study non-linear mappings (lu et al., 2015) and the additional techniques in (lazaridou et al., 2015).",1,2016
D16-1252.pdf,we hope that future open ie systems can make use of this new resource to easily and objectively measure and compare their performance.,1,2016
D16-1253.pdf,"since the lack of labeled data is a major challenge for implicit discourse relation classification, our proposed bisyndata can enrich the training data and then benefit future work.",2,2016
D16-1254.pdf,we expect that heuristic backtracking could be applied to any other transition-based parser with similar benefits.,4,2016
D16-1254.pdf,"we plan on experimenting with various heuristics and cutoff models, such as adapting the attention based models of bahdanau et al. (2014) to act as a guide for both the heuristic search and cutoff.",1,2016
D16-1257.pdf,we also wish to develop a complete parsing model using the lstmlm framework.,1,2016
D16-1257.pdf,"we suspect building large models with character embeddings would lead to further improvement as in language modeling (kim et al., 2016; jozefowicz et al., 2016).",1,2016
D16-1258.pdf,"we hope to scale these methods to large unlabelled corpora or other languages, to provide data for re-training parsers.",4,2016
D16-1258.pdf,"future work will explore the possibility of asking questions about other types of parsing uncertainties, such as nominal and adjectival argument structure, and a more thorough treatment of prepositionalphrase attachment, including distinctions between arguments and adjuncts.",5,2016
D16-1260.pdf,"as future work: (1) we will incorporate the valid time of facts. (2) some time-sensitive facts lack temporal information in yago2, we will mine such temporal information from texts.",2,2016
D16-1262.pdf,"in future work, we will apply our approach to other structured prediction tasks, where neural networks—and greedy beam search—have become ubiquitous.",4,2016
D16-1263.pdf,"future work should use a universal phoneme recognizer or acoustic model of a similar language, thus making a step towards true generalizability",1,2016
D17-1001,"in addition, we will apply our method to parallel parsing and other grammar, e.g., projective dependency trees.",4,2017
D17-1001,we plan to extend our method to align comparable paraphrases that are partially paraphrase sentences.,1,2017
D17-1001,"furthermore, we will apply such syntactic paraphrases to phrase embedding.",1,2017
D17-1002,"two possibilities are to create even better training methods, and to find some way to extend our run-time improvements to other transition systems.",1,2017
D17-1002,it would also be interesting to further investigate relationships between graph-based and dependency-based parsing.,1,2017
D17-1003,it would be interesting to extend these lines of work to decrease the complexity of our quartic algorithm.,1,2017
D17-1005,another is to incorporate openie methods to automatically find domain specific patterns and generate pattern-based labeling functions.,1,2017
D17-1005,one is to apply transfer learning techniques to handle label distributions’ difference between training set and test set.,1,2017
D17-1005,There exist several directions for future work.,6,2017
D17-1007,"in this work we directly use the default ranker in lucene for similar sentences, which can be improved in future work.",1,2017
D17-1008,following bennett (2016) we will also experiment on more realistic estimates of p(s|w) in formula 5 as well as other assumptions made in our work.,3,2017
D17-1008,"as future work we plan to extend our approach to verbs, adjectives and adverbs.",1,2017
D17-1009,"in the future, we would like to explore how this framework can benefit applications such as summarization (liu et al., 2015) and machine reading (sachan and xing, 2016).",5,2017
D17-1010,"in this paper, the mimick model was trained using characters as input, but future work may consider the use of other subword units, such as morphemes, phonemes, or even bitmap representations of ideographic characters (costa-jussa et al.` , 2017).",1,2017
D17-1011,"creating a common map of tense along the lines of figure 1, but unifying the three tenses addressing shortcomings of the way we compute alignments: (i) generalizing character n-grams to more general features, so that templates in templatic morphology, reduplication and other more complex manifestations of linguistic features can be captured;",1,2017
D17-1011,"(ii) use n-gram features of different lengths to account for differences among languages, e.g., shorter ones for chinese, longer ones for english;",1,2017
D17-1011,"(iii) segmenting verses into clauses and performing alignment not on the verse level (which caused many errors in our experiments), but on the clause level instead;",1,2017
D17-1011,"(iv) using global information more effectively, e.g., by extracting alignment features from automatically induced bi- or multilingual lexicons.",1,2017
D17-1012,"in future work, we investigate the effectiveness of our approach in different types of target tasks.",4,2017
D17-1013,"in the future, it might be helpful to analyze the benefits of jointly learning other related tasks together with machine translation, to provide further control of the learning process.",1,2017
D17-1014,"this work raises several compelling possibilities which we intend to address in future work, such as improving decoding speed, integrating additional constraints such as word coverage and fertility into decoding, and applying our method to other intractable structured prediction problems.",5,2017
D17-1015,our dataset can be used to investigate future models that expand to handle relativization and other recursive phenomena in language.,1,2017
D17-1016,"in this work, we used the bivariate gaussian distribution in the mdn’s mixture leaving the use of other distributions which might better suit the geolocation task for future research.",4,2017
D17-1020,"in the future work, we will focus on the self transition of adjustable affinity preserving random walk, which could be used to remove the redundancy between summary sentences.",1,2017
D17-1021,"in future work we will design such a model, and offer it candidates chosen not only from sentences containing the antecedent, but the larger context.",1,2017
D17-1026,our hope is that these results can enable learning paraphrastic sentence embeddings with powerful neural architectures across many languages and domains.,1,2017
D17-1027,there could be several directions to be explored for future work.,6,2017
D17-1027,the structure of chinese characters and the positions of components in the character may be considered to fully leverage the component information of chinese characters.,1,2017
D17-1027,"first, we use the average operation to integrate the sub character components as the context to predict the target word.",1,2017
D17-1027,"to solve this problem, attention models may be used to adaptively assign weights to word context, character context, and subcharacter context.",1,2017
D17-1027,"second, for any target word, we simply use word context, character context, and subcharacter context to predict it and do not distinguish compositional words and non-compositional words.",1,2017
D17-1029,"future work includes applying the proposed method to other aspects of nominal semantics, such as understanding compound nouns in other languages, and to explore the compositionality of words and compounds.",1,2017
D17-1030,"we would like to systematically evaluate, in particular, how fast the system can gain an understanding of a concept which is fully equivalent to a vector built from big data.",3,2017
D17-1030,"in order to validate our system as a suitable, generic solution for word learning, we will have to test it on various data sizes, from the type of low- to middle-frequency terms found in e.g.the rare words dataset (luong , 2013), to highly frequent words.",3,2017
D17-1030,our future work will thus investigate how to capture and measure informativeness.,5,2017
D17-1030,we believe that both quality and speed of learning will be strongly influenced by the ability of the algorithm to detect what we called informative sentences.,1,2017
D17-1032,"in future work, we wish to explore embedding signatures that leverage richer knowledge of the practitioners corpus and task.",2,2017
D17-1032,"finally, we hope to extend vecshare’s embedding selection methods to consider syntheses of multiple distinct embedding sets, tailored to the practitioner’s task and corpus.",1,2017
D17-1033,we also intend to extend the experiments to other nlp tasks in addition to word similarity such as word analogies.,4,2017
D17-1033,"in future work we intend to extend the experiments to include other original pre-trained embeddings, and other algorithms for manifold learning.",1,2017
D17-1034,"in the future, we plan to investigate reinforcement learning methods to incorporate multi-sense word representations for downstream nlp tasks.",1,2017
D17-1035,"as future work, we will investigate if those methods are either less dependent on the hyperparameters or are less dependent on the random seed value, e.g.if they avoid converging to bad local minima.thus, there is a good chance that these configurations require less tuning when applied to new tasks or domains.",5,2017
D17-1037,such studies would further extend the potential of word embedding methods.,1,2017
D17-1037,"one possibility is to explore extending other embedding methods such as glove (pennington et al., 2014) to incremental algorithms.",1,2017
D17-1037,the success of this work suggests several research directions to be explored in the future.,6,2017
D17-1040,"we also encourage evaluating our models on other tasks that must deal with long sequences but have compact representations, such as summarization and question-answering, and further exploration of their effect on memory and training speed.",3,2017
D17-1040,we encourage future work that explores the optimal values of k for various language tasks and examines whether or not it is possible to predict k based on the task at hand.,1,2017
D17-1041,"second, we will investigate the interpretability of hidden structures of neural networks for nlp tasks such as (yang , 2016; li , 2016), when the rotated word vectors are used as an embedding layer.",3,2017
D17-1041,we will examine the issues in future work.,6,2017
D17-1041,"first, we apply target rotation (harman, 1960; browne, 1972a,b) to incorporate prior knowledge when constructing the rotated word vector representations.",1,2017
D17-1041,"based on the meanings, we can remove irrelevant dimensions for a specific task of interest, in order to secure more efficient storage of the vectors and decrease the complexity of downstream nlp models.",1,2017
D17-1041,We plan to explore following issues.,6,2017
D17-1042,an interesting application would be to infer dependencies between textual and image features in image-to-text prediction (e.g. image captioning).,1,2017
D17-1042,"also, we used a vae-based sampling for object perturbations but other approaches are possible depending on the nature of the domain or data.",1,2017
D17-1043,"future work should explore other natural language processing tasks, where the data is likely to arise from complex, multi-modal latent factors.",4,2017
D17-1044,"in our future work, we intend to also explore how to complement `1 penalties with terms penalizing more explicitly the processing time; we also wish to study how these ideas can be used in combination with neural models.",1,2017
D17-1045,"our future work is to test this approach on systems with expensive communication cost, such as multi-node environments.",3,2017
D17-1046,"we would like to build a deeper understanding of which aspects of an unsupervised objective, near uniform initialization, and non-identifiability contribute to these issues and to discover other learning problems that may share these issues.",1,2017
D17-1047,we may also try other memory weighting strategies to distinguish multiple targets in one comment more clearly.,1,2017
D17-1047,"therefore, we need a mechanism to stop the attention process automatically if no more useful information can be read from the memory.",1,2017
D17-1048,future work includes using other eye-tracking information such as saccade and fixation.,2,2017
D17-1048,the incorporation of other information such as user-product information can also be explored.,2,2017
D17-1050,"thirdly, and perhaps most usefully for future work by others, this feedbackbased dataset will be made available for use by other researchers and in other evaluations.",4,2017
D17-1051,another direction would be to use topic models and see whether reviewers are more inclined to compare different types of references when talking about certain aspects of restaurants or other products.,1,2017
D17-1051,a different approach to identifying helpful reviews would be to create entertaining and informative summaries.,1,2017
D17-1053,"in future work, we will investigate using more advanced document embedding techniques (e.g., cnn, rnn) to directly model document-level sentiment information.",1,2017
D17-1053,We will also extend our model to other languages.,4,2017
D17-1056,future work will evaluate the proposed method on another datasets.,3,2017
D17-1056,more experiments will also be conducted to provide more in-depth analysis.,3,2017
D17-1057,"in future, we would like to extend our work by creating an end to end stock prediction system where the system would predict the future stock prices based on the sentiment score and stock value of the company.",1,2017
D17-1060,"also, to address the problematic scenario when the kg does not have enough reasoning paths, we are interested in applying our rl framework to joint reasoning with kg triples and text mentions.",4,2017
D17-1060,"for future studies, we plan to investigate the possibility of incorporating adversarial learning (goodfellow et al., 2014) to give better rewards than the human-defined reward functions used in this work.",1,2017
D17-1060,"instead of designing rewards according to path characteristics, a discriminative model can be trained to give rewards.",1,2017
D17-1061,"(2) using information from modalities other than text,",2,2017
D17-1061,and (3) better reinforcement learning algorithms for a partially-observable environment.,1,2017
D17-1061,"in the future, more research is necessary in the directions of (1) iterative reformulation under the proposed framework.",1,2017
D17-1062,"beyond sentence simplification, the reinforcement learning framework presented here is potentially applicable to generation tasks such as sentence compression (chopra , 2016), generation of programming code (ling , 2016), or poems (zhang and lapata, 2014).",4,2017
D17-1062,"in the future, we would like to explicitly model sentence splitting and simplify entire documents (rather than individual sentences).",1,2017
D17-1064,another direction for future work concerns the exploitation of the extended webnlg corpus.,2,2017
D17-1064,"we plan to exploit this extended corpus to make available a correspondingly extended websplit corpus, to learn optimised split-and-rephrase models and to explore sentence fusion (converting a sequence of sentences into a single complex sentence).",2,2017
D17-1064,"in future work, it would be interesting to see whether and if so how, sentence splitting can be learned in the absence of explicit semantic information in the input.",5,2017
D17-1065,further explorations of gan-based techniques to model contextual information in dialogue problems will be addressed in our future research.,1,2017
D17-1066,in future work we plan to apply our vae model to semi-supervised nlp tasks and experiment with conditioning generation on text attributes such as sentiment and writing style.,4,2017
D17-1068,"for example, future work might aim at identifying strategies for tuning the parameter n to account for the different degrees of selectivity of each verb-specific role.",1,2017
D17-1068,"another possible extension would be the inclusion of a mechanism for updating the role prototypes depending on how the other roles are filled, which would be the key for a more realistic and dynamic model of thematic fit expectations (lenci, 2011).",1,2017
D17-1071,"given this interpretative ability, we believe that our logic-based system may also be of benefit to other natural language processing tasks, such as question answering and text summarization.",4,2017
D17-1071,"in future work, we will refine our system so that it can be applied to other tasks such as question answering.",4,2017
D17-1072,"we believe there are many exploration directions for this new task, among which we are particularly interested in three in the near future: 1) improving our benchmark approaches by considering task-specific features and neural network architectures, 2) verifying the usefulness of mws to highlevel applications such as mt, 3) integrating mws with syntactic parsing in some way by exploiting existing treebanks.",3,2017
D17-1073,"finally, we aim at applying our models to arabic dialects and other languages.",4,2017
D17-1073,"we expect that character-level embeddings will have a bigger role in scenarios with noisy input, such as non-standard spontaneous orthography used in social media.",1,2017
D17-1073,"we also intend to further investigate the role of syntax features in morphological disambiguation, and explore additional techniques for more accurate tagging.",1,2017
D17-1073,"future directions include exploring additional deep learning architectures for morphological modeling and disambiguation, especially joint and sequence-to-sequence models.",1,2017
D17-1077,"in providing a labeled dataset for others to use, we hope to encourage other work that reasons about the structure present in alternative representations (such as images) as well.",6,2017
D17-1078,"future work should focus on extending the neural morphological tagger to a joint lemmatizer (muller ¨ , 2015) and evaluate its functionality in the low-resource setting.",3,2017
D17-1079,"it can also be useful for other nlp tasks with small labeled training data, but a large unlabeled data.",2,2017
D17-1080,"as an anonymous reviewer suggested, a possible direction of future work is to leverage another word segmentation approach which uses linguistic features, such as the stanford word segmenter (tseng et al., 2005) with k-best segmentations.",1,2017
D17-1081,"while this paper focused on harvesting geometry axioms from textbooks as a case study, it can be extended to obtain valuable structured knowledge from textbooks in areas such as science, engineering and finance.",2,2017
D17-1082,we hope this dataset will stimulate the development of more advanced machine comprehension models.,1,2017
D17-1084,"additionally, we plan to apply our findings to generating math problem.",4,2017
D17-1084,we would like to leverage semantic parsing and transform math problems to a more structured representation.,1,2017
D17-1084,then we can reason with small equation units to generate a final equation in testing.,1,2017
D17-1084,"for long tail problems with a template size less 2, we want to utilize the fine grained expressions we have learned and decompose equations for learning.",1,2017
D17-1084,"based on our error analysis, we plan to improve our model by detecting quantity types more accurately, learning relations and incorporating commonsense knowledge.",1,2017
D17-1085,"2) another direction of research is to incorporate sest with deeper neural networks such as vd-cnn (conneau et al., 2017) to improve learning capacity for syntactic embedding.",1,2017
D17-1085,"since there are no structures in the bidaf models to specifically optimize for syntactic information, an attention mechanism that is designed for to utilize syntactic information should be studied.",1,2017
D17-1085,3) tree structured information such as knowledge graphs and ontology structure should be studied and improve question answering tasks using similar techniques to the ones proposed in the paper.,1,2017
D17-1086,"for future work, we plan to examine the effects of other knowledge sources.",2,2017
D17-1086,we plan to explore the idea of external knowledge integration further in future research.,2,2017
D17-1086,"thus, one potential direction for future work would be to incorporate both relational information and lexical definitions.",1,2017
D17-1088,"as future work, we plan to extend our model to be able to generate equation systems and nonlinear equations.",1,2017
D17-1089,"further, we would like to see how laser-qa generalizes to beyond text; one immediate scenario of interest is to explore how pictures and multimedia within qas may be leveraged within laser-qa.",4,2017
D17-1089,future work: a study on the correlation between the knns in the laser-qa embedded space and the original question and answer spaces would provide insights into the extent of correlation between manifolds in the original spaces.,1,2017
D17-1091,"the framework can be used for other natural language processing tasks which are sensitive to the variation of input (e.g., textual entailment or summarization).",4,2017
D17-1091,"we would also like to explore more advanced paraphrase scoring models (parikh et al., 2016; wang and jiang, 2016) as well as additional paraphrase generators since improvements in the diversity and the quality of paraphrases could also enhance qa performance.",1,2017
D17-1095,we are looking forward to try out richer and more suitable features for multimodal translation (ie.dense captioning features).,6,2017
D17-1095,another interesting approach would be to use visually grounded word embeddings to capture visual notions of semantic relatedness.,1,2017
D17-1096,we hope our work motivates further efforts on understanding and relating onomatopoeia words to “regular” words.,1,2017
D17-1098,"in future work we hope to use more powerful image taggers, and to consider the use of constrained beam search within an expectation maximization (em) algorithm for learning better captioning models from weakly supervised data.",1,2017
D17-1099,"third, future research on action attributes should ideally include videos to better capture attributes that require temporal signals.",2,2017
D17-1099,several possibilities remain open for future work.,6,2017
D17-1099,"in particular, since our experiments show that unsupervised word embeddings significantly help performance, it might be desirable to learn data-driven attributes in an end to-end fashion directly from a large corpus or from dictionary definitions.",1,2017
D17-1099,"first, more attributes could be collected and evaluated, possibly integrating other linguistic theories about verbs, with more accurate modeling of context.",1,2017
D17-1102,"we hope our new dataset can encourage further multilingual, multimodal research.",4,2017
D17-1103,"in future work, we are applying our entailment-corrected rewards to other directed generation tasks such as image captioning and document summarization (using the new multi-domain nli corpus (williams , 2017)).",4,2017
D17-1105,"in future work, we will conduct a more systematic study on the impact that synthetic back translated data brings to multi-modal nmt, and run an error analysis to identify what particular types of errors our models make (and prevent).",1,2017
D17-1107,another untapped source of information is the acoustic signal in the reading aloud trial.,2,2017
D17-1107,in future work we also plan to explore the connection between eye movements and reading comprehension.,5,2017
D17-1107,"one avenue for future research will be to design more sophisticated ways of incorporating linguistic information into the eye-tracking model, especially features that take into account context, rather than operating at the single word level.",1,2017
D17-1108,we plan to build on the notable improvements shown here and expand this study to deal with additional temporal reasoning problems in natural language text.,4,2017
D17-1109,existing evaluation metrics for tasks with a generation component— such as summarization or dialogue—leave much to be desired.,3,2017
D17-1111,"as future work we plan to apply the gnr to other question answering datasets such as ms marco (nguyen , 2016) or newsqa (trischler , 2016a), as well as investigate the applicability and benefits of type swaps to other tasks like named entity recognition, entity linking, machine translation, and summarization.",4,2017
D17-1111,"finally, we believe there a broad range of structured prediction problems (code generation, generative models for images, audio, or videos) where the size of original search space makes current techniques intractable, but if cast as learning-to-search problems with conditional computation, might be within reach.",5,2017
D17-1112,we plan to conduct more detailed parameter tuning in the acoustic domain and to segment the xitsonga dataset supplied with the zerospeech 2015 challenge.,2,2017
D17-1112,in the future we hope to pursue a number of lines of inquiry.,6,2017
D17-1112,and we plan to explore clustering techniques that would allow our system to discover categories in addition to probable segmentation points.,1,2017
D17-1112,"we also intend to introduce additional layers into the autoencoder network so as to allow for joint acquisition of phone-like, morph-like, and/or word-like units in the acoustic signal; this may benefit from the alternate model structure of chung et al.(2017).",1,2017
D17-1113,an interesting avenue for future work would be to investigate the variance of results amongst individual participants (figure 1).,6,2017
D17-1113,"however, understanding how individual variations in participants can impact modeling decisions would be of great value to the computational semantics community.",1,2017
D17-1114,"we also plan to enlarge ourmms dataset, specifically to collect more videos.",2,2017
D17-1116,"and, naturally, applying the same frame-work to other annotation tasks.",4,2017
D17-1117,"we plan to consider user-specific information(e.g., ratio of comments rejected in the past)(cheng , 2015; waseem and hovy, 2016)",2,2017
D17-1117,"we  plan  to  consider  user-specific  information (e.g.,   ratio  of  comments  rejected  in  the  past) (cheng  et  al.,  2015;  waseem  and  hovy,  2016) and explore character-level rnns orcnns (zhanget al., 2015), e.g., as a first layer to produce embeddings of unknown words from characters (dossantos  and  zadrozny,  2014;  ling  et  al.,  2015), which  would  then  be  passed  on  to  our  current methods that operate on word embeddings.",1,2017
D17-1118,"it remains for future research to establish whether other approaches to word representation, e.g. (blei , 2003; mikolov ,2013), have inherent biases.",5,2017
D17-1120,"as future work, we plan to extend our evaluation to larger sense-annotated corpora (raganato, 2016) as well as to different sense inventories and different languages.",3,2017
D17-1120,"we also plan to exploit  the  flexibility  of  our  models  by  integrating them into downstream applications,  such as machine translation and information extraction.",1,2017
D17-1122,"in the future work, wewill improve the inter-weighted layer with moresophisticated module and evaluate our model onother large scale datasets.",3,2017
D17-1124,"in future work, we would like to investigate more complicated idiom-enriched nlp tasks, suchas machine translation.",4,2017
D17-1126,future work could include expanding into many different languages present in social media and developing language-independent automatic paraphrase identification models.,1,2017
D17-1132,"future  works  should  also consider incorporating word-sense frequencies or developing  word-sense  agnostic  features,  with  a particular focus on asymmetric features.",1,2017
D17-1133,"in the future, we would like to improve parsing accura-cy by leveraging unlabeled text rather than relying exclusively on human annotated training data.",2,2017
D17-1140,"considering other criteria, and also more sophisticated search strategies  to  explore  the  huge  space  of  possible patterns, is left for future work.",1,2017
D17-1141,"also, we will further explore whether thereare important types of evidence in editorials andsimilar texts that we have not considered in thispaper so far, such as analogies.",5,2017
D17-1141,"in future work, we plan to investigate argumentation strategies across different genres and provenances.",1,2017
D17-1142,"for  future  work,  an  immediate  next  step  is to  explore  the  usage  of  automatically  extracted arguments  in  helpful  reviews  identification:   in this work,  all argument-based features are based on manually annotated arguments;  deep-learning based argument mining (li et al., 2017; eger et al.,2017)  has  produced  some  promising  results  recently, and we plan to investigate whether the automatically extracted arguments can be used to identify helpful reviews, and how the errors made in  the  argument  extraction  stage  will  influence the performance of helpful reviews identification.",5,2017
D17-1142,We also plan to investigate the effectiveness ofargument-based features in other domains.,4,2017
D17-1143,"lastly,  future  work  can  integrate  subtasks  1  and 4  into  the  model.",1,2017
D17-1144,"in future work, we plan to test our model oncorpora such as the language of opposition from aifdb4(by converting the finer-grained relationtypes used in this corpus to argumentative relations of the kind we considered), on datasets proposed for different tasks.",3,2017
D17-1144,it would be interesting to see whether any corpora for identifying discourse relations could be useful for furthering our experimentation.,5,2017
D17-1144,"finally, we plan to incorporate an attention-based mechanism as well as additional features (e.g. extracted through sentiment analysis) to determine which parts of the texts are most relevant in identifying attack and support.",1,2017
D17-1145,"promising avenues for future work are investigating  consensus  networks  (mangu  et  al.,  2000)for potential gains in terms of speed or quality as compared to lattice inputs, explicitly dealing with rare or unknown words in the lattice, and facilitating gpu training via auto batching (neubig et al.,2017b).",1,2017
D17-1146,"future  work  will  investigate  better  model-memory integration, e.g., by joint training.",1,2017
D17-1148,"in the future, we plan to focus on integration of phrasal components into nmt training, including better coverage constraints, as well as methods for context-dependent translation override within our hybrid search algorithm.",1,2017
D17-1152,our framework is also easily extendable to incorporate more bilingual and auxiliary signals of translation equivalence.,1,2017
D17-1153,"anobvious question is whether we could extend ourframework to model individual annotator preferences (passonneau and carpenter, 2014) or learnpersonalized models (mirkin , 2015; rabi-novich , 2017), and handle heteroscedasticnoise (park, 1966; kersting , 2007; antos, 2010).",5,2017
D17-1153,"another direction is to apply active learning techniques to reduce the sample complexity required to improve the systems or to extend to richer action spaces for problems like simultaneous translation, which requires prediction (gris-som ii et al., 2014) and reordering (he et al., 2015)among other strategies to both minimize delay and effectively translate a sentence (he et al., 2016).",1,2017
D17-1154,"in  the  future,  we  plan  to  integrate weight quantization into our method.",1,2017
D17-1155,"in  the  future,  we  will  try to study a specific sentence weighting method for nmt domain adaptation.",1,2017
D17-1156,"our techniques are not specific to neural machine translation, and we propose that they could be also tried for other neural network architectures and other tasks.",4,2017
D17-1157,"future work should consider integration of the method with more recent models, in particular neural translation models.",1,2017
D17-1158,"while initial results showonly moderate improvements over the baseline andfall short against using synthetic parallel data, we believe there is value in pursuing this line of research further to simplify training procedures.",6,2017
D17-1159,"more generally, given simplicity of gcns andtheir applicability to general graph structures (notnecessarily trees), we believe that there are many nlp tasks where gcns can be used to incorporate linguistic structures (e.g., syntactic and semantic representations of sentences and discourse parsesor co-reference graphs for documents).",4,2017
D17-1160,analyzing the errors made by our parser suggests that improving entity linking and using the table structure are two directions for future work.,1,2017
D17-1161,"for example, this work ignores modifiers such as always and usually, which often carry valuable information that could be incorporated via model expectation constraints.",2,2017
D17-1161,future work can also incorporate other modes of supervision from language.,2,2017
D17-1161,this improvement seems to indicate that a system which would compute named entities in parallel with normalization would be useful.,1,2017
D17-1161,"as hidden markov models have been used both for name finding (bikel et al.(1997)) and tokenization (cutting et al.(1992)), this seems to be a promising research possibility.",1,2017
D17-1163,"finally, it may also be possible to adapt our model to extract other types of social behavior events.",4,2017
D17-1163,"furthermore, our dataset could be used to contribute to communication studies, by exploring research questions about the dynamics of media attention (for example, the effect of race and geography on coverage of police killings), and discussions of historical killings in news—for example, many articles in 2016 discussed michael brown’s 2014 death in ferguson, missouri.",4,2017
D17-1163,"our model allows for the use of mention level semantic parsing models; systems with explicit trigger/agent/patient representations, more like traditional event extraction systems, may be useful, as would more sophisticated neural network models, or attention models as an alternative to disjunction aggregation (lin et al., 2016).",1,2017
D17-1163,"one goal is to use our model as part of a semi-automatic system, where people manually review a ranked list of entity suggestions.",1,2017
D17-1164,"from a technical standpoint, future work could also augment the representation of questions and answers presently used in our framework, beyond our heuristic of using root arcs without noun phrases.",1,2017
D17-1165,including a ‘speaker’ level to know which parliamentarians discuss which topics is another interesting path to follow.,1,2017
D17-1165,"also, adding a ‘bill’ level could be beneficial as speeches about the same bill should share the same high-level topic.",1,2017
D17-1165,"in future work, we plan to improve the model through complex priors or semantic similarity strategies.",1,2017
D17-1167,"in the future, we will explore effective ways to model discourse relations among clauses and develop a qa system which can directly output the cause of emotions as answers.",1,2017
D17-1168,"while this is the best-performing model till date on this task, our analysis indicates a need for even deeper analysis of human behavior and societal norms to further improve our understanding.",1,2017
D17-1169,we release our pretrained deepmoji model with the hope that other researchers will find good use of them for various emotion-related nlp tasks4.,4,2017
D17-1172,this work could benefit from several extensions.,6,2017
D17-1172,"high-order scores on arcs like grandparent or siblings can be handled in subproblem p2 with the algorithms described in (koo et al., 2010).",1,2017
D17-1172,bigram scores on spines could be added at the expense of a third subproblem in the dual objective.,1,2017
D17-1173,future work will involve exploring ways of augmenting the parser with a more innovative architecture than the relatively simple one used in current neural graph-based parsers.,1,2017
D17-1175,"in the future, we plan to apply our approach in more languages and other transition-based system, such as arc-eager or arc-hybrid.",4,2017
D17-1175,another direction we are interested in is to train our model with complex training approaches proposed in weiss et al.(2015) and andor et al.(2016).,1,2017
D17-1176,"in the future, we plan to study higher degrees of lexicalization or full lexicalization, as well as even larger training corpora (such as the wikipedia corpus).",2,2017
D17-1176,we would also like to experiment with other grammar induction approaches with lexicalization and big training data.,1,2017
D17-1178,we look forward to exploring these directions in future work.,6,2017
D17-1178,"we believe that further refinements of our search procedure can continue to push the bar higher, such as the use of a learned heuristic function for forward score estimation, or a more sophisticated approximate decoding scheme making use of specific properties of the model.",1,2017
D17-1180,"these positive results suggest that tag can provide the foundation of nlp systems for tasks requiring deeper analysis than current dependency parsers provide, and we will apply our parser to such tasks in the future.",4,2017
D17-1180,"nonetheless, a large discrepancy remains in parser performance with gold supertags and predicted supertags, indicating that supertagging is still a bottleneck.",5,2017
D17-1180,we will explore ways to leverage our super tagger's high β-pruning accuracy in parsing.,1,2017
D17-1181,"furthermore, we plan to verify our results on other datasets in future work.",3,2017
D17-1181,an interesting future direction is the extension of the linear-chain crf to jointly normalize all predictions for table filling in a single model pass.,1,2017
D17-1183,"we hope to extend kgeval to incorporate varying evaluation cost, and also explore more sophisticated evaluation aggregation.",3,2017
D17-1184,"these findings suggest a rich area of future research, determining new strategies to extend embeddings to cope with sparse and unreliable data.",1,2017
D17-1184,"three promising approaches include revising the closed-world assumption frequently used in training embeddings, combining embeddings and collective probabilistic models that perform well on extracted kgs, and devising an optimization approach for embeddings that exploits confidence from knowledge extraction systems.",1,2017
D17-1185,"as future work, we plan to develop similar models based on explicit specialization tensors for detecting symmetric relations (e.g., synonymy, antonymy).",1,2017
D17-1185,"we will also seek to exploit the dual tensor model in different downstream tasks, e.g., hypernymy detection for taxonomy induction (faralli et al., 2017) or recognizing textual entailment.",1,2017
D17-1186,"in the future, we will explore the following directions: (1) we will explore the combination of relation paths from both plain texts and kbs for relation extraction.",1,2017
D17-1186,"(2) we may take advantages of probabilistic graphical model or recurrent neural network to encode more complicated correlations between relation paths, e.g. multi-step relation paths, for relation extraction.",1,2017
D17-1188,it would be possible to replace their sentence-level feature extractor with our model.,1,2017
D17-1188,"for instance, lin et al.(2016) extract sentence-level features and then combine features from multiple sentences with a selective attention mechanism.",1,2017
D17-1188,our approach can be easily applied to other types of relation extraction models as well.,1,2017
D17-1189,"in the future, we plan to develop a new measurement for the reliability of certain distantly supervised label by evaluating the corresponding similarity between certain instance and the potential correctly labeled instances instead of using heuristically set confidence vector.",3,2017
D17-1189,"in addition, we tend to find a more suitable sentence encoder rather than piece-wise cnn for our soft-label method.",1,2017
D17-1190,"in the future, we will extend our sequential models to predict temporal relations for event pairs spanning across multiple sentences, for instance by incorporating discourse relations between sentences in a sequence.",1,2017
D17-1192,"in the future work, we plan to develop more sophisticated noise models for features and labels separately, and try to explore logical information, particularly in this context of nonparametric noise modeling, for further benefiting this task.",1,2017
D17-1193,unsupervised relation classification is a very challenging task for several reasons.,1,2017
D17-1193,"moreover, semantic relations differ with respect to the semantic constraints they put on their arguments.",1,2017
D17-1193,"on the other hand, contextual relation instances tend to have relation-specific indicators when they co-occur, but their individual vectors will not reveal this information (unless they co-occur very often).",1,2017
D17-1193,"any future development towards an automated unsupervised classification needs to take these aspects into account and work towards a hybrid solution by separating relations with semantically constrained arguments from free ones, as well as adapting the clustering method to handle outliers.",1,2017
D17-1193,"some relation instances are lexical by nature and, therefore, can be expected to show up in the same cluster based on distributional cues.",1,2017
D17-1194,"we observed this performance drop in our experiments, and it would be interesting to know more about the regularities governing this deterioration.",3,2017
D17-1194,"also, for the particular task of analyzing armed conflicts, we plan to research ways of improving accuracy in predicting completely new armed groups not present in the training data, and the methods of filtering out locations not involved in armed conflicts.",1,2017
D17-1194,"in terms of future work, we plan to trace how quickly incremental updates to the model ‘dilute’ the projections, rendering them useless with time.",1,2017
D17-1196,we are still working on these improvements and we will hope to get a complete model soon.,1,2017
D17-1196,"with this contribution we would like to raise also some interest in the community to analyze and develop more effective techniques, both on the modelling and minimization/learning sides, to allow to build real world application based on this framework.",1,2017
D17-1196,qmt and its probability calculus seem to be promising methodologies to enhance the performances of our systems in nlp and certainly deserve further investigations.,1,2017
D17-1197,"in task oriented dialogues, we can also try human evaluation to see if the model can reply users’ query accurately.",3,2017
D17-1197,it is also interesting to use reinforcement learning to learn the actions in each step in coref based lm.,1,2017
D17-1199,"morphological segmentation is a noteworthy alternative to syllabification: a simple morpheme-aware model which sums morpheme embeddings looks promising, and its study is deferred to future work.",1,2017
D17-1200,in future work we plan to explore joint learning of more and different views.,5,2017
D17-1200 ,we invite the community to join us in exploring the full space of opportunities and evaluating induced representations holistically in the future.,3,2017
D17-1200 ,"another interesting avenue for future work would be to apply the framework to questions arising in the digital humanities, e.g., to extract different views from news articles.",4,2017
D17-1200 ,"in the future we plan to define more targeted input, e.g., by using semantic and syntactic information from dependency parses.",1,2017
D17-1201,it still requires further exploration to apply our method to fields beyond nlp.,4,2017
D17-1203,"we plan to merge tree prior construction and the topic modeling into a unified framework (teh et al., 2007; gor¨ ur and teh ¨ , 2009; hu et al., 2013).",1,2017
D17-1207,future work should also evaluate the earth movers distance between more languages to assess its quality as language distance.,3,2017
D17-1208,"unfolding and shrinking diverse networks could be possible, for example by applying the technique only to the input and output layers or by some other scheme of finding associations between units in different models, but we leave this investigation to future work as models in nmt ensembles in current research usually have the same topology (bojar et al., 2016; sennrich et al., 2016a; chung et al., 2016; neubig, 2016; wu et al., 2016; durrani et al., 2017).",1,2017
D17-1209,"since gcns are capable of encoding any kind of graph-based structure, in future work we would like to go beyond syntax, by using semantic annotations such as srl and amr, and co-reference chains.",1,2017
D17-1210,"the proposed trainable greedy decoding is a generic idea that can be applied to any recurrent language modeling, and we anticipate future research both on the fundamentals of the trainable decoding as well as on the applications to more diverse tasks such as image caption generating and dialogue modeling.",4,2017
D17-1211,we plan to go beyond the binary classification and explore satire degree estimation.,2,2017
D17-1211,"we will investigate efforts to model satire at the paragraph level following our conclusion and theoretical backgrounds, such as (ermida, 2012).",1,2017
D17-1211,"we will generalize our approach to reveal characteristics of figurative language (joshi et al., 2016), where different paragraphs or sentences may reflect different degrees of sarcasm, irony, and humor.",1,2017
D17-1212,as future work we foresee a larger ground-truth and more robust approaches which take into account factors such as a reference being irrelevant to a citing paragraph and cases where the evidence for a paragraph is implied rather than explicitly stated in the reference.,1,2017
D17-1213,"we are now deploying the same edit intention taxonomy for italian wikipedia, and plan to apply it to other low resourced languages in wikipedia.",2,2017
D17-1213,"finally, beyond the context of wikipedia, similar taxonomies can be designed for analyzing the collaboration and interaction happened in other online contexts such as academic writing (e.g., google docs or sharelatex, etc).",1,2017
D17-1214,"however, these methods require careful configuration and tuning to succeed, and making them more robust presents an excellent opportunity for future work.",1,2017
D17-1215,we hope that our work will motivate the development of more sophisticated models that understand language at a deeper level.,1,2017
D17-1216,"currently, there are little labeled training instances for commonsense machine comprehension, for future work we want to address this issue by developing semi-supervised or unsupervised approaches.",1,2017
D17-1218,"to further overcome the problem of domain dependence, multitask learning is a framework that could be explored (sgaard and goldberg, 2016) for different conceptualizations of claims.",1,2017
D17-1219,it would also be interesting to see if the generated questions can be used to help improve question answering systems.,5,2017
D17-1219,"in future work, we would like to investigate approaches to identify question-worth concepts rather than question-worthy sentences.",6,2017
D17-1220,"as future work we suggest injection of structured and unstructured external knowledge (ahn et al., 2016) and explicit modeling of references (yang et al., 2016).",1,2017
D17-1224,"in future work, we will investigate supervised learning methods for passage ranking and selection, and try to paraphrase the selected passages.",5,2017
D17-1226,"in the future, we plan to continue to investigate the distinct characteristics of wd and cd coreferent event mentions in order to further improve event coreference performance.",1,2017
D17-1226,"especially, we are interested in including additional discourse-level features for improving wd coreference merge performance, such as, features indicating the distance between two event mentions in a document.",1,2017
D17-1229,"in the future, we plan to combine our strategies with more complex neural architectures, and explore their application to other sequence-labeling problems.",4,2017
D17-1230,exploring this relationship further is a focus of our future work.,6,2017
D17-1231,"in our future work, we plan to apply these approaches to other tasks, such as intent recognition and slot filling in language understanding.",4,2017
D17-1232,these findings in addition to the fact that many function words are not filtered out as background may help inform future model design.,1,2017
D17-1234,we expect to deploy our proposed framework in real-world scenarios collaborating with real human teachers to verify the results presented in this paper and discover more potential challenges of on-line dialogue system development.,4,2017
D17-1234,so future work is needed to take more qualities into consideration to achieve better user experience.,6,2017
D17-1234,There are several directions for our future work.,6,2017
D17-1235,"while the focus of this work has been on conversation modeling, we expect some of these results to carry over to other sequence-to-sequence settings, such as machine translation or image-captioning.",4,2017
D17-1237,but more complex tasks might require multiple levels of hierarchy.,5,2017
D17-1237,this challenge calls for future work on automatic learning of hierarchies for complex dialogue tasks.,5,2017
D17-1237,"thus, it is valuable to extend our approach to handle such deep hierarchies, where a subtask can invoke another subtask and so on, taking full advantage of the options framework.",1,2017
D17-1237,the promising results suggest several directions for future research.,6,2017
D17-1239,"future work on this task might include approaches that process or attend to the source records in a more sophisticated way, generation models that attempt to incorporate semantic or reference-related constraints, and approaches to conditioning on facts or records that are not as explicit in the box- and line-scores.",1,2017
D17-1240,"finally, we would also like to incorporate our findings into other standard tasks of multilingual ir and multilingual speech synthesis (for example to render the appropriate native accent to the borrowed word).",2,2017
D17-1240,future directions: it would be interesting to understand and develop theoretical justification for the metrics.,5,2017
D17-1240,"further, it would be useful to study and classify various other linguistic phenomena closely related to core borrowing, such as: (i) loanword, where a form of a foreign word and its meaning or one component of its meaning gets borrowed, (ii) calques, where a foreign word or idiom is translated into existing words of native language, and (iii) semantic loan, where the word in the native language already exists but an additional meaning is borrowed from another language and added to existing meaning of the word.",1,2017
D17-1241,"we expect that with more training data, the performance of spe-based methods can be further improved.",3,2017
D17-1242,"we regard this as a first step toward demographic-aware nlp, and in future work we plan to address other more advanced nlp tasks while accounting for demographics.",5,2017
D17-1243,"recently, a dataset with dialogue act annotations on reddit discussions is published and can be used for a dialogue act prediction task (zhang , 2017).",3,2017
D17-1243,a potential future direction is to explore whether the comment embeddings derived from the unsupervised factored neural model can be useful across multiple tasks.,4,2017
D17-1243,"identifying or ranking persuasive arguments in the changemyview subreddit (as studied in (tan , 2016; wei , 2016)) and asking for favors in the randomactsofpizza subreddit (used in (althoff , 2014)) are also interesting for future work.",5,2017
D17-1243,"identifying or ranking persuasive arguments in the changemyview subreddit (as studied in (tan et al., 2016; wei et al., 2016)) and asking for favors in the randomactsofpizza subreddit (used in (althoff et al., 2014)) are also interesting for future work.",5,2017
D17-1244,"the latter would allow us to, for example, analyze how the relationship between two individuals changes over time, and determine which events make a relationship go from superficial to intense and vice versa.",5,2017
D17-1244,"our future plans include studying values of dimensions for selected relationships (e.g., coworker), and investigating changes on the dimensions of the relationship over time.",1,2017
D17-1245,"among them, we are currently extending the dataset of annotated tweets both in terms of annotated tweets per topic, and in terms of addressed topics (e.g.",2,2017
D17-1245,"brexit after the referendum, trump), in order to have more instances of facts and sources.",2,2017
D17-1245,"on such extended dataset, we plan to run experiments using the three modules of the system as a pipeline.",3,2017
D17-1245,"moreover, we plan to extend our pipeline by considering also the links provided in the tweets to verify their sources, i.e., if a tweet claims to report information from the cnn but the link actually redirects towards an advertisement website the source is not indubitably the cnn.",4,2017
D17-1245,"being it a work in progress, several open issues have to be considered as future research.",5,2017
D17-1246,we are also interested in finding the meanings of the detected non-standard usages.,5,2017
D17-1246,we are interested in expanding our method for detecting words that have non-standard usages.,1,2017
D17-1248,"in addition to methodology, future work will also need to take into account ethical implications of this personalization.",2,2017
D17-1248,"as humans, we automatically perform this adaptation through multiple additional channels: speech tone, frequency, facial expression; which the agent can not alter.",6,2017
D17-1248,"in future work, we will experiment with automatically altering or generating text while keeping topic constant, as our current results are in part topically driven.",1,2017
D17-1249,"preliminary results on the 2016 us campaign tweets show that the topics used by both candidates were different from their speech: the tweets, being shorter than speeches, emphasize more oversimplified criticism of the opponent rather than justified political ideas.",2,2017
D17-1250,"consumers of comments, typically, desire information that ultimately leads to real utility benefits, and this domain is not the only one where objective quality can be obtained: for example, one could: (1) ask consumers of restaurant reviews to indicate if one convinces them to go to the restaurant and then follow up on their experience, (2) consider evaluating research paper quality – reviewer ratings versus citation count (influence), (3) determine whether the answer to the question about how to drive to conserve fuel lead to the reader actually using less gas?",3,2017
D17-1251,the future work includes the reduction or estimation of the computation time and memory usage.,5,2017
D17-1251,"the proposed method should be used with effective methods of identifying auto-generated contents, bots, and spammers in microblogs.",5,2017
D17-1251,"they increase as the target document set grows or, specifically the number of occurrences of bursty n-grams increases.",1,2017
D17-1252,improving mmrn for dealing with large search space is an important future direction as well.,4,2017
D17-1252,"in the future, we plan to apply maximum margin reward networks on other structured learning tasks.",1,2017
D17-1253,"while the noise from mining errors might qualify some of our findings, we also expect that larger corpora will allow us to discover more reliable and discriminative patterns.",1,2017
D17-1254,"in future work, we aim to use more advanced cnn architectures (conneau , 2016) for learning generic sentence embeddings.",3,2017
D17-1255,there are several avenues for future work including the extent to which our rbf model and its kernels could be combined with curriculum learning or leitner system to either predict easiness of novel training instances to inform curriculum learning or incorporate leitner’s queueing mechanism to the rbf model.,1,2017
D17-1255,"other directions include extending rbf to dynamically learn the recall confidence parameter with respect to network behavior, or developing more flexible delay functions with theoretical analysis on their lower and upper bounds.",1,2017
D17-1257,we hope researchers will take advantage of the code for further improvements and applications to other tasks.,1,2017
D17-1259,"there remains much potential for future work, particularly in exploring other reasoning strategies, and in improving the diversity of utterances without diverging from human language.",4,2017
D17-1259,"we will also explore other negotiation tasks, to investigate whether models can learn to share negotiation strategies across domains.",5,2017
D17-1261,future work can leverage optimal inputs to create a language model that can become an automated debate agent.,1,2017
D17-1261,"however, since the input is partially based on the knowledge of talking points, there is a potential for an information retrieval-based task to provide the talking points for the debate agent (if one desires a fully-automated system than can work without the presence of introductory remarks, from which talking points are currently extracted).",1,2017
D17-1261,"finally, future work can also examine the trained model itself in further detail, seeking to understand the debate strategy.",1,2017
D17-1263,but there are also several ways to improve and expand upon our challenge set approach itself.,2,2017
D17-1263,"first, though our human judgments of output sentences allowed us to precisely assess the phenomena of interest, this approach is not scalable to large sets, and requires access to native speakers in order to replicate the evaluation.",3,2017
D17-1263,the existence of human judgments for this set provides a goldstandard by which proposed automatic judgments may be meta-evaluated.,3,2017
D17-1263,a method to automatically create such a challenge set for a new language pair would be extremely useful.,5,2017
D17-1263,"it is our hope that the insights derived from our challenge set evaluation will help inspire future mt research, and call attention to the fact that even “easy” language pairs like english–french still have many linguistic issues left to be resolved.",5,2017
D17-1263,it would be interesting to see whether similar scores could be achieved through automatic means.,5,2017
D17-1263,localizing a divergence within a difficult sentence pair would be another useful subtask.,5,2017
D17-1263,"finally, we would like to explore how to train an mt system to improve its performance on these divergence phenomena.",1,2017
D17-1263,"this could take the form of designing a curriculum to demonstrate a particular divergence to the machine, or altering the network structure to capture such generalizations.",1,2017
D17-1263,"second, the construction of such a challenge set requires in-depth knowledge of the structural divergences between the two languages of interest.",1,2017
D17-1263,"one could imagine approaches that search for divergences, indicated by atypical output configurations, or perhaps by a system’s inability to reproduce a reference from its own training data.",1,2017
D17-1264,"first, due to advances in methods for extracting general purpose knowledge (mitchell , 2015; nakashole , 2013; wijaya , 2014), the use of semantic knowledge has been explored for several natural language tasks (nakashole and mitchell, 2015; yang and mitchell, 2017).",4,2017
D17-1264,"second, although we focus on bilingual dictionary induction, our knowledge distillation training objective that enables seamless use of paths of rich resource languages as teachers of low resource languages is general and can be applied to problems such as multilingual tagging and parsing.",4,2017
D17-1264,"however, for bilingual dictionary induction, and more generally, machine translation, the role of semantic knowledge has not been fully explored.",1,2017
D17-1265,possible improvements include improving the choice of linguistic information used and using this work to explore how tqs’ functions are portrayed in languages other than english.,2,2017
D17-1265,"interesting future work would be to compare the opposite approach to the task; augmenting source sentences with disambiguating information prior to translation, particularly within an nmt framework, which has good potential for handling nonlocal context and integrating extra features.",3,2017
D17-1265,"as a stylistic aspect, its prediction and evaluation are complex and should be further explored.",5,2017
D17-1268,"we hope that this study will encourage additional use of typological features in downstream nlp tasks, and inspire further techniques for missing knowledge prediction in under-documented languages.",5,2017
D17-1271,"one idea for future work is thus to leverage additional projected annotations from similar phenomena in additional languages, possibly improving overall performance by combining complementary information.",2,2017
D17-1271,"hence, telicity recognition is also relevant for machine translation research and could be a useful component in computer aided language learning systems, helping learners to select appropriate aspectual forms.",5,2017
D17-1271,"when translating, telic and atelic constructions also require different lexical choices and appropriate selection of aspectual markers.",1,2017
D17-1271,clustering more than two languages may also enable us to induce clusters corresponding to the different usages of imperfective verbs in czech.,1,2017
D17-1271,we plan to leverage similar ideas as presented in this paper to create temporal discourse parsing models for such languages.,1,2017
D17-1272,"since our objectives are agnostic of the choice of the underlying model πw, it is also possible to transfer our techniques to non-linear models such as neural machine translation.",1,2017
D17-1273,future work includes: i) improving our work for short text knowledge extraction and ii) designing a general framework for cross-lingual ugc relation extraction.,1,2017
D17-1274,"in the future, we will combine additional rules, patterns, and constraints with dnn techniques to further improve slot filling.",1,2017
D17-1276,"future work includes further investigations on how to apply the multigraph approach to other structured prediction tasks, as well as applications of the proposed model in other related nlp tasks that involve the prediction of overlapping structures, such as equation parsing (roy , 2016).",4,2017
D17-1277,"in the future, we would like to extend this system to perform nil detection, coreference resolution and mention detection.",1,2017
D17-1279,"therefore, adding more in-domain unlabeled data may help when combined with selection schemes such as the ulm algorithms proposed here.",2,2017
D17-1279,"other future work includes leveraging global context, information of citation network.",6,2017
D17-1279,it would be useful to assess the impact of matched unlabeled data for the physics and material science domain.,1,2017
D17-1281,we hope that this first application of asynchronous deep rl algorithms will open up more adoption of such techniques in the nlp community.,4,2017
D17-1282,"while avoiding them is one of the strengths of our model, generating more consistent parses to reduce this kind of named entities would be one possible direction for future research.",5,2017
D17-1283,"in the future we hope to extend this work to nlp tasks with richer structured output, such as parsing.",4,2017
D17-1284,"further research will include encoding more structured knowledge about the entities, such as their relations to other entities, to make their representations semantically richer.",1,2017
D17-1284,"we will investigate how we can use unstructured resources, such as the corpus of unlabeled webpages used by plato, and noisy supervision from the wikilinks corpus (singh et al., 2012) in order to further improve the model.",1,2017
D17-1284,"we will also evaluate our approach on substantially varied domains, such as discussion forums, and social media posts.",1,2017
D17-1287,"however, when used alone, the character features still yield reasonable performance, which suggests that more meaningful character-based features could lead to story detectors with even better performance.",1,2017
D17-1287,In future work we plan to develop richer character-based features.,1,2017
D17-1289,"while promising, our model still cannot explicitly handle multidefendant cases, and there is also a clear gap between our model and the upper bound improvement that relevant articles can achieve.",5,2017
D17-1289,we will leave these challenges for future work.,6,2017
D17-1291,"we hope this work provides insights for further studies on outlier document texts in specific domains, and in more challenging settings such as detecting outliers from crowdsourced data.",5,2017
D17-1292,"in our following work, we collect a sequence of causal chains verified by domain experts for more solid evaluation of generating explanations.",3,2017
D17-1293,"for instance, we can adopt the proposed algorithm to streaming documents, e.g. webpage click streams, since our method can model the document-document sequences.",1,2017
D17-1294,planned future work for this project will include the creation of a browser plug-in to present opt-out hyperlinks to internet users.,5,2017
D17-1301,"our models benefit from larger contexts, and would be possibly further enhanced by other document level information, such as discourse relations.",1,2017
D17-1301,we propose to study such models for full length documents with more linguistic features in future work.,1,2017
D17-1304,"in the future, we will try to exploit a general framework for utilizing richer syntax knowledge.",1,2017
D17-1305,another issue is the use of a commercial image search api as a black box to retrieve images.,5,2017
D17-1305,"our future work is to assess the applicability of our approach into other rte problems such as the rte challenges, snli (bowman et al., 2015) and multinli (williams et al., 2017) datasets and further investigate what syntactic or semantic units can be best represented using visual denotations.",1,2017
D17-1306,"in this way sequence bias effects can be considered as independent noise sources, rather than a systematic bias, and consequently the aggregate results over several workers will remain unbiased.",5,2017
D17-1306,"another important question for future work is whether sequence bias is detectable in expert annotators, not just crowd workers.",5,2017
D17-1307,"by revealing that 95% of real use cases fit into this paradigm, we hope to convince the reader that this is a valuable problem that requires more attention.",5,2017
D17-1307,"while firstorder qa might seem like a solved problem, there is clearly still room for improvement.",5,2017
D17-1307,"while an ablation study revealed the importance of both entity detection and relation prediction, we are hoping to further study the degree of which improvements in either component affect qa accuracy.",1,2017
D17-1307,"even though deep learning has opened the potential for more generic solutions, we believe that taking advantage of problem structure yields a more accurate and efficient solution.",1,2017
D17-1310,a novel lstm unit with extended memories is developed for memory interactions.,1,2017
D17-1311,"one major question left open by this analysis is what happens when multiple transformations are applied hierarchically, and future work might focus on extending the techniques in this paper to explore recursive structure.",5,2017
D17-1312,future work includes performing further investigations to better understand and to visualize what types of information has been transferred across domains and how such information influence different types of down-stream nlp tasks.,4,2017
D17-1312,it is also important to understand how such an approach will work on other types of models such as neural networks based nlp models.,1,2017
D17-1313,"finally, we will incorporate additional biological constraints (e.g., focus on pathways that exist in specific species) into the search itself.",2,2017
D17-1313,there are many exciting directions in which to take this work.,1,2017
D17-1313,"second, we can expand focused reading to efficiently search for multiple paths between s and d.",1,2017
D17-1313,"first, more of the focused reading algorithm can be subject to rl, with the chooseendpoints policy being the clear next candidate.",1,2017
D17-1314,"this will enable the system to learn by self to achieve continual or lifelong learning (chen and liu, 2016).",1,2017
D17-1314,"we also plan to improve model performance during testing (shu et al., 2017).",1,2017
D17-1314,"in our future work, we plan to improve the cumulative or incremental learning method in (fei et al., 2016) to learn new classes without training on all past and new classes of data from scratch.",1,2017
D17-1315,an obvious extension to our work is to try similar models on multiple languages.,2,2017
D17-1316,"as our data comes from a vulnerable population, obtaining a larger data set is challenging, but essential for future work.",2,2017
D17-1316,"in fact, two of the authors are in the process of collecting data from a total of 120 chr individuals.",2,2017
D17-1316,"this would enable a more thorough investigation of a larger and more sophisticated suite of linguistic features, and especially a more fine grained analysis of the interaction of metaphor and emotional language in schizophrenia.",1,2017
D17-1318,"on the computational side, we will to extend our approach to crosslingual data, in order to enable computer-assisted political analysis across different languages.",2,2017
D17-1318,"in the future, we hope that the pipeline presented in this paper will support political science researchers in studying topics such as party polarization through the analysis and comparison of electoral manifestos, parliamentary proceedings and campaign speeches.",1,2017
D17-1319,"zipporah currently selects sentences based on the individual quality only, and we plan in future work to also consider other factors, e.g.encourage selection of a subset that has a better n-gram coverage.",2,2017
D17-1321,"our goal is simply to improve understanding and interpretability of invented languages in multiagent dialog, place recent work in context, and inform fruitful directions for future work.",5,2017
D17-1323,"future work also includes applying bias reducing methods in other structured domains, such as pronoun reference resolution (mitkov, 2014).",4,2017
D17-1323,"more extensive analysis could explore the interaction among predictor, bias measurement, and bias de amplification method.",1,2017
D17-1323,our work is the first to demonstrate structured prediction models amplify bias and the first to propose methods for reducing this effect but significant avenues for future work remain.,1,2017
D18-1005,"however, we hope to test generalizability in future work by applying our methods to other gang-related corpora, because there is variation in language, local concepts, and behavior across gangs.",4,2018
D18-1005,"in the future, we are also interested in further experimenting with the context features introduced in this work; for instance, by extending our pairwise interaction features to take into account direction between users.",1,2018
D18-1005,"finally, we intend to explore other types of context, such as reference to specific events that may trigger the emotions of either aggression or loss.",1,2018
D18-1008,future work might further extend the recovery of analogy as part of information extraction.,1,2018
D18-1011,we believe that one of the promising future directions is to learn from how humans learn and store semantic word representations to build a more effective computational model.,1,2018
D18-1012,"we also plan to explore advanced video features such as activity recognition, person identification, etc.",5,2018
D18-1012,"in future work, we plan to investigate the effects of multiple users, i.e., the multi-party aspect of this dataset.",5,2018
D18-1014,"in future work, we are interested in merging our model with memory-based fusion methods since they have complementary strengths as discussed in subsection 5.1.",1,2018
D18-1019,"we anticipate this model could be leveraged in other similar sequence modeling tasks that involve predicting overlapping structures such as recognizing overlapping and discontinuous entities (muis and lu, 2016) which frequently exist in the biomedical domain.",4,2018
D18-1021,"in the future, we will enrich semantics of lowresourced languages by cross-lingual linking to rich-resourced languages, and extend more crosslingual words and entities to multi-lingual settings.",2,2018
D18-1022,"as our airline-product results indicate, increasing the domain gap harms our results, and we expect the same with more diverse language pairs.",2,2018
D18-1022,"in future work we wish to improve our results for large domain gaps and for more dissimilar languages, particularly in the important lazy setup.",2,2018
D18-1023,"in the future, we will further extend our approach to multi-lingual multimedia common semantic space construction.",2,2018
D18-1024,"for future work, we plan to investigate how our method can be extended to work with other bwe frameworks, in order to overcome the instability issue of lample (2018b).",4,2018
D18-1025,"a newly collected dataset for evaluating bilingual contextual word similarity is presented, which provides potential research directions for future work.",2,2018
D18-1026,"we also plan to extend the method to asymmetric relations (e.g., hypernymy) and to more target (resource-lean) languages.",2,2018
D18-1026,"in future work, we will explore more sophisticated adversarial models such as cycle-gan (zhu et al., 2017).",1,2018
D18-1026,"moreover, we will experiment with bootstrapping approaches to extract new lexical constraints from post-specialized embeddings.",1,2018
D18-1029,we hope that nlp/lm practitioners will find the datasets for 50 languages put forth in this work along with benchmarked lms useful for future developments in (language-agnostic as well as typologically-informed) multilingual language modeling.,2,2018
D18-1029,this study calls for next-generation solutions that will additionally leverage typological knowledge for improved language modeling.,1,2018
D18-1032,"furthermore, how to iteratively discover new entity alignments in the framework of our approach is another interesting direction that we will study in the future.",5,2018
D18-1032,"in the future work, we will explore more advanced gcn models for kg alignment task, such as relational gcns (schlichtkrull et al., 2017) and graph attention networks (gats) (velickovic et al., 2017).",1,2018
D18-1033,"(2) in fact, our framework for cross-lingual lexical sememe prediction can be transferred to other cross-lingual tasks.",4,2018
D18-1033,"in the future, we will explore the following research directions: (1) in this paper, for simplification, we ignore the rich hierarchy information in hownet and also ignore the fact that a word may have multiple senses.",1,2018
D18-1033,we will explore the effectiveness of our model in these tasks such as cross-lingual information retrieval.,1,2018
D18-1033,we will extend our models to consider the structure information of sememe and multiple senses of words;,1,2018
D18-1035,"as our model is agnostic to both the model architecture and the target metric, we see the exploration of more diverse and ambitious model target metric pairs as a clear avenue for future work.",1,2018
D18-1037,"we plan to explore the applications of sync in nmt with more structured attention mechanisms, and potentially a hybrid phrase-based nmt systems with sync, in which the model can benefit from sync to be more extensible when handling larger lexicons.",4,2018
D18-1038,we believe our method can be further extended into a general purpose multi-lingual transfer framework to resolve other nlp matching or classification problems.,4,2018
D18-1039,"in the future, we want to extend the concept of the contextual parameter generator to more general settings, such as translating between different modalities of data (e.g., image captioning).",1,2018
D18-1039,"furthermore, based on the discussion of section 3.3, we hope to develop an adaptable, never-ending learning (mitchell et al., 2018) nmt system.",1,2018
D18-1041,"besides, our method is also general to other nmt models.",4,2018
D18-1041,"therefore, we plan to apply our method to the nmt with complex architectures, for example, lattice-to-sequence nmt (su , 2017), hierarchy-to-sequence nmt (su , 2018), nmt with context-aware encoder (zhang , 2017) and transformer (vaswani , 2017) and so on.",4,2018
D18-1041,"therefore, we plan to apply our method to the nmt with complex architectures, for example, lattice-to-sequence nmt (su et al., 2017), hierarchy-to-sequence nmt (su et al., 2018), nmt with context-aware encoder (zhang et al., 2017) and transformer (vaswani et al., 2017) and so on.",4,2018
D18-1041,"in the future, we would like to extend the proposed word-level cost weighting strategy to source words.",1,2018
D18-1044,"second, explore new method for knowledge distillation.",5,2018
D18-1044,"first, use object function beyond maximum likelihood to improve the modeling of long-distance dependencies.",1,2018
D18-1044,"in the future, we plan to investigate better methods for training the sat to further shrink the performance gap between the sat and the transformer.",1,2018
D18-1044,"we also plan to extend the sat to allow the use of different group sizes k at different positions, instead of using a fixed value.",1,2018
D18-1045,"in future work, we would like to investigate an end to-end approach where the back-translation model is optimized to output synthetic sources that are most helpful to the final forward model.",1,2018
D18-1046,"performing model combination, either by developing hybrid transliteration models (nicolai et al., 2015) or by ensembling (finch et al., 2016), can further improve low resource transliteration.",1,2018
D18-1046,"jointly leveraging similarities between related languages, such as writing systems or phonetic properties (kunchukuttan et al., 2018), also shows promise for low-resource settings.",1,2018
D18-1047,"a promising line of future work is to explore a best of both worlds approach, combining neighborhood sensitivity with the methods that achieve superior performance on nearby languages.",1,2018
D18-1048,"more specifically, we can extend the policy network to choose the backward decoding except for forward decoding and halting.",1,2018
D18-1048,one promising direction is to incorporate the backward decoding into our architecture.,1,2018
D18-1049,"in the future, we plan to further validate the effectiveness of our approach on more language pairs.",2,2018
D18-1053,"as future works, we plan to further improve paragraph ranker based on the researches on learning to rank.",5,2018
D18-1054,"future work can focus for example on designing an inexpensive preprocess layer, and other strategies for improved performance on answer generation.",1,2018
D18-1056,"understanding when biases induce highly non-convex landscapes, and how to make adversarial training less sensitive to such scenarios, remains an open problem, which we think will be key to the success of unsupervised machine translation and related tasks.",5,2018
D18-1057,"as future works, we suggest further investigating the characteristics of context overlap in diversified ways.",1,2018
D18-1058,"in our future work, we will apply word-pair embeddings from nlra to various downstream tasks related to lexical relational information.",4,2018
D18-1062,"future directions include validate our model on more realistic scenarios (dinu , 2015) as well as combine our algorithms with more sophiscated adversarial networks (arjovsky , 2017; gulrajani , 2017).",3,2018
D18-1062,"future directions include validate our model on more realistic scenarios (dinu et al., 2015) as well as combine our algorithms with more sophiscated adversarial networks (arjovsky et al., 2017; gulrajani et al., 2017).",3,2018
D18-1065,"an interesting direction for future work is to extend beam-joint to multi-head attention architectures as in (vaswani et al., 2017; xu chen, 2018).",1,2018
D18-1073,"also, we plan to look at specialized tasks that naturally evaluate the influence of external knowledge, to help the model to generate diverse responses.",3,2018
D18-1073,"in future work, we aim to add more experiments with dialogue tasks that require understanding a supplementary source of knowledge to solve the task.",6,2018
D18-1074,"in future work, we will investigate other external sources of knowledge, such as acoustic cues and videos to further improve the performance of the model.",1,2018
D18-1078,other types of questions can be formulated over this data in following work.,5,2018
D18-1080,this suggests that future improvements may come from finding other such sources of data as much as from modeling.,2,2018
D18-1082,"in future work, we would like to devise more sophisticated solutions to bridge the semantic gap in rg and explore linguistic patterns in conversations like what has been done in discourse analysis (lei , 2018) .",5,2018
D18-1084,"in future work, we hope to further address the language issues of paragraph generation as well as extend this simple approach to other tasks requiring long-form text or paragraph generation.",4,2018
D18-1085,"since there is not such dataset at the time of writing this paper, we can continue building on this work by using model summaries, which are abstractive in nature, as a proxy.",1,2018
D18-1086,in future work we aim to improve summarization performance by jointly training the guiding process with the amr-based summarization process.,1,2018
D18-1087,"to broadly substantiate our findings, we propose future work that would follow our assessment methodology over test samples from current datasets (e.g.cnn/dailymail), judging performance of current systems and utilizing current manual evaluation protocols.",3,2018
D18-1088,"in the future, we plan to explore ways to train compression models tailored to our summarization task.",1,2018
D18-1090,one of the future directions can be exploring other kinds of scoring actions than classification under the reinforcement learning framework.,1,2018
D18-1092,"in the future, we are interested in extending this approach to more natural language tasks.",4,2018
D18-1093,"future work will include: (i) incorporating lexical semantics such as named entities and domain specific senses for further improvement, (ii) extending the method to utilize label dependency constraints (bi and kwok, 2011), and (iii) improving the accuracy of the top ranking categories to deal with p@1 and ndcg@1 metrics.",1,2018
D18-1094,"as a future direction, we will advance our model to automatically construct the hierarchical taxonomy in order to improve text classification with a large number of classes.",1,2018
D18-1097,we are also interested in applying our approach to other languages and dialects.,2,2018
D18-1098,as future work we intend to explore ways that combine both the topic coherence and topic intrusion for topic model evaluation.,3,2018
D18-1099,"further, we have planned to generate titles for prose text essentially creating micro-summaries of text, a relatively unexplored area.",5,2018
D18-1099,"in the future, we intend to build upon our methods and enable automatic detection of subheaders.",5,2018
D18-1108,"our methods generality opens up several avenues for future work: since it supports any structure for which map inference is available (e.g., matchings, alignments), and we have no restrictions on the downstream p(y | h, x), we may design latent versions of more complicated state-of-the-art models, such as esim for nli (chen et al., 2017).",1,2018
D18-1109,"in the future, we would like to investigate the effectiveness of rnfs on a wider range of nlp tasks, such as natural language inference and machine translation.",4,2018
D18-1113,"in future work, we plan to investigate to what extent these methods can be used to support the automatic generation of grammar exercises.",3,2018
D18-1119,"in future work, we would like to encourage the development of more natural word meanings by enforcing the agent representations to stay more faithful to the perceptual input they receive.",1,2018
D18-1119,"moving ahead, it is fundamental to design setups where agents would have stronger reasons to develop human-like communication strategies.",1,2018
D18-1120,"besides, dynamic routing could also be useful to improve other natural language processing tasks such as the sequenceto-sequence task and so on.",4,2018
D18-1120,"in the future, we tend to resolve the situation of how to assign predicted relationship to multi entity pairs when two entities have multi-relations by utilizing prior knowledge such as entity type and joint training with named entity recognition.",5,2018
D18-1120,"we will also try to optimize the model in terms of speed and focus on other problems by leveraging class ties between labels, specially on multilabel learning problems.",1,2018
D18-1121,"lme may also be extended to other tasks that also suffer from noises and incompleteness of ds, such as relation extraction (takamatsu , 2012; ritter , 2013; lin , 2016).",4,2018
D18-1121,"for example, (1) how to train a language model that is sensitive with incorrect labels; (2) how to combine meaning of labels with the hierarchical structure of types; (3) how to find the optimal  easily for a new dataset.",1,2018
D18-1123,"for future work, we aim to expand our experimental study to a larger scale.",2,2018
D18-1123,we further consider extracting publication strings from academic homepages of the same organization.,5,2018
D18-1123,we also plan to investigate adaptive alternating model training schemes as well as external memory mechanism such as memory networks.,1,2018
D18-1124,"another direction that we plan to work on is to apply this model to recognizing overlapping and entities that involve discontinuous spans (muis and lu, 2016) which frequently exist in the biomedical domain.",4,2018
D18-1125,"in the future, we will extend our framework to cross-lingual and cross-domain information extraction tasks.",4,2018
D18-1128,future work involves closing the loop and evaluating the extent to which improved human performance at fp and kp translates to improved success of human-ai teams at accomplishing a shared goal.,3,2018
D18-1131,"one idea is to try source-pivot-target transfer, similarly to the way this is done for machine translation (wu and wang, 2007).",5,2018
D18-1131,"another promising direction is to have an attention mechanism (luong , 2015) for question similarity which can be adapted across domains.",5,2018
D18-1131,"in future work, we plan to develop better methods for adversarial adaptation based on these observations.",1,2018
D18-1134,"specifically, we will look at how to better model the interaction between the reader and the relation embedding model and how to improve the relation embedding model itself by adopting ideas from the relation extraction literature (miwa and bansal, 2016; peng et al., 2017; ammar et al., 2017).",1,2018
D18-1134,"in the future, we would like to invest in building better sequential question answering models that push the accuracy beyond the presented baselines.",1,2018
D18-1136,it is also interesting to see whether such parameterized cnn architecture could benefit other natural language processing tasks involving text pairs like question answering task.,4,2018
D18-1139,"furthermore we observed consistently better performance of cnn architectures in otherwise comparable scenarios, which suggests that cnns cope better with the irregularities of user-written texts on social media, a research question we leave to future work.",5,2018
D18-1144,"in the future, we will develop a model which uses a series of posts as a feature.",1,2018
D18-1146,it would be interesting to quantify the synergy in the learning process; 5. exploring ml applications to other aspects of the service component.,4,2018
D18-1146,"future work could improve on: 1. fine-tuning the open-domain question answering for wine knowledge; 2. connecting our hslda or hierarchical multilabel classification to robotic sensors to fully mimic the blind tasting task; 3. exploring other simpler and more efficient ml models for pairing tasks; 4. training a joint multi-task model, since it is accepted in the industry that a solid knowledge of wine theory helps immensely in blind tasting and wine service.",4,2018
D18-1147,"given our initial success, in the future, it would be valuable to construct a large health-related dataset to cover other types of emotions, e.g., anger or fear.",2,2018
D18-1148,"future work in this area can look at adjusting models to account for other meta-data such as temporal variation and diversity and to adjust for selection biases present in social media, where the user base on social media is not representative of the population of the community (greenwood et al., 2016).",1,2018
D18-1149,"lastly, further work on sequence-to-sequence model architectures could yield better results in non-autoregressive sequence modeling.",1,2018
D18-1151,"we hope that our data set, and future extensions to other phenomena and languages, will make it possible to measure progress in syntactic language modeling and will lead to better understanding of the syntactic generalizations captured by language models.",1,2018
D18-1153,"In the future, we plan to apply LD-Net to other applications.",4,2018
D18-1154,"another direction of study is large-scale semantic relation discovery, for example, frames and scripts, with a focus on salient discourse units.",1,2018
D18-1154,"In the future, we plan to investigate these complex settings.",6,2018
D18-1155,"as a direction for future work, it would be very interesting to extend the current models, diving further into direct time-line models, and learn to predict absolute time-lines, i.e. making the time-lines directly mappable to calendar dates and times, e.g.by exploiting complementary data sources such as the event times corpus (reimers et al., 2016) and extending the current loss functions accordingly.",1,2018
D18-1156,"in the future, we plan to exploit the information of one argument which plays different roles in various events to do better in event extraction task",1,2018
D18-1159,"in the future, we will explore the possibility of removing the projective constraint and the tree requirement, extending the applicability of valency patterns to other tasks such as semantic role labeling.",4,2018
D18-1160,"future work might explore more sophisticated invertible projections, or recurrent projections that jointly transform the entire input sequence.",6,2018
D18-1163,"we are also interested in experimenting with richer families of permutation distributions, as well as conservative distributions that tend to prefer the original source order.",2,2018
D18-1163,"we would also like to consider more sensitive divergence measures that go beyond bigrams, for example using recurrent neural network language models (rnnlms) for q and p.",1,2018
D18-1163,"we could use entropy regularization (grand valet and bengio, 2005) to encourage more deterministic patterns of realization in the synthetic languages.",1,2018
D18-1165,"it would also be interesting to compare the effectiveness of the opportunistic active learning framework, as well as the policy learning, across a variety of applications.",3,2018
D18-1165,"it would be interesting to examine how a policy learned using a dataset such as visual genome generalizes to a different domain such as images captured by a robot operating in an indoor environment, possibly with some fine-tuning using a smaller in-domain dataset.",4,2018
D18-1165,"the simulation could also potentially be improved using positive unlabeled learning methods (liu et al., 2002; li and liu, 2003) instead of assuming that an object or attribute not labeled in an image region is not present in the image.",1,2018
D18-1166,"in the future, we also intend to extend the dataset by collecting natural language questions-answer pairs via crowdsourcing.",2,2018
D18-1166,we also hope that recipeqa will serve other purposes for related research problems on cooking recipes as well.,4,2018
D18-1167,"thus, future work also includes integrating better temporal cues.",2,2018
D18-1167,"additionally, temporal reasoning is crucial for answering the tvqa questions.",6,2018
D18-1167,"another direction is to exploit human-object relations in the video and subtitle, as we observe that a large number of questions involve such relations.",6,2018
D18-1167,"to narrow the gap, one possible direction is to enhance the interactions between videos and subtitles to improve multimodal reasoning ability.",6,2018
D18-1167,we hope this novel multimodal dataset and the baselines will encourage the community to develop stronger models in future work.,1,2018
D18-1168,"additionally, in table 5, even when the mllc model can properly localize context, it does not always properly localize temporal sentences indicating that improved temporal reasoning can also improve our results.",1,2018
D18-1168,"we believe our dataset, analysis, and method are an important step towards better moment retrieval models that effectively reason about temporal language.",1,2018
D18-1169,"the significant gap between state of the art and iaa (around 50.0) encourages future research to take this dataset as a challenging, yet reliable, evaluation benchmark.",5,2018
D18-1171,"for example, we want to extend the notion of pom to nouns and adjectives, and investigate other a priori measures for metaphor novelty.",1,2018
D18-1171,"further, we want to jointly detect metaphors and score their novelty.",1,2018
D18-1171,another interesting direction is to investigate the correlation between perceived novelty and the existence of dictionary definitions for metaphoric senses of a token or expression.,1,2018
D18-1171,"in future work, we want to develop more sophisticated methods to detect and distinguish novel metaphors.",1,2018
D18-1173,"in the future work, we will explore more effective memory addressing and updating approaches to boost the few-shot representation learning.",1,2018
D18-1175,"second, there are many intuitively promising sources of information that we have not explored, such as coreference.",2,2018
D18-1175,"and third, our models rely on pairwise similarity-based coherence learning, which leads to the natural question of whether structured prediction would improve performance.",1,2018
D18-1176,"in addition, it would be interesting to explore how the attention weights change during training, and if, e.g., introducing entropy regularization (or even negative entropy) might improve results or interpretability further.",3,2018
D18-1177,how to incorporate visual domain knowledge more explicitly into the model would be an interesting direction for future research.,1,2018
D18-1177,"while we build on skip-gram, the idea of pixie could be extended to other word embedding models, e.g., glove (pennington et al., 2014), elmo (peters et al., 2018), etc.",1,2018
D18-1178,"in future work, we plan to incorporate other nlp tasks defined over these frameworks to learn nouncnoun compound interpretation using tl and mtl.",4,2018
D18-1178,"such tasks include semantic role labeling of nominal predicates in nombank annotations as well as verbal predicates in propbank (kingsbury and palmer, 2002).",1,2018
D18-1179,one open question is to what extent can the quality of bilm representations be improved by simply scaling up model size or data size?,5,2018
D18-1179,"as our results have show that computationally efficient architectures also learn high quality representations, one natural direction would be exploring the very large model and data regime.",1,2018
D18-1179,an alternate direction for future work combines the purely unsupervised bilm training objective with existing annotated resources in a multitask or semisupervised manner.,1,2018
D18-1181,"firstly, more work is needed to evaluate the representations on other languages and tasks.",2,2018
D18-1181,"secondly, solving downstream tasks requires representations for the inflected words as well.",5,2018
D18-1181,"to address it in future work, we might want to split word representations into a lexical and a morphological part.",1,2018
D18-1183,"in future, we plan to extend this work in several aspects: (1) enrich the simile component structure by adding shared properties or events so that the extracted structures would be more useful for metaphor processing;",1,2018
D18-1183,(2) improve representation learning for recognition by incorporating external knowledge;,1,2018
D18-1183,(3) apply simile recognition to study the use of figurative language in writings.,1,2018
D18-1184,"first, the success of span comparisons over word-level comparisons suggests that it may be advantageous to include such comparisons in more complex models, either for comparing two sentences directly, or as intermediate parts of models for more complex tasks, such as reading comprehension.",1,2018
D18-1184,we hope to find a more efficient way to accelerate this dynamic programming method on a gpu.,1,2018
D18-1184,"second, our models ability to infer trees from a semantic objective is intriguing, and suggestive of future opportunities in grammar induction research.",1,2018
D18-1184,the use of the inside-outside algorithm unavoidably renders the full model er (by 5c 8 times) compared to the decomposable attention model.,1,2018
D18-1186,"in future work, we hope to improve the extensibility of cin and apply it to other nlp tasks, such as machine comprehension.",4,2018
D18-1188,"in future work, we would like to make use of table information and external knowledge to improve our qg model.",1,2018
D18-1188,We also plan to apply the approach to other tasks.,4,2018
D18-1190,"in future work, we would like to automatically learn a delexicalizer from data, tackle zero-shot parsing when the structure distribution in the target domain is very different from the source domains, and apply our framework to datasets where only denotations are provided.",2,2018
D18-1191,"since the investigation on the characteristics of the representations can lead to interesting findings, it is worthwhile evaluating them intrinsically and extrinsically.",3,2018
D18-1191,the use of frame knowledge to reduce these confusions is a straightforward approach.,1,2018
D18-1191,an interesting direction for future work concerns evaluating span representations from our span-based model.,1,2018
D18-1191,another promising direction is to explore methods of incorporating frame knowledge into srl models.,1,2018
D18-1194,"the representations for cross-lingual decompositional semantics, the evaluation metric, and the evaluation dataset provided in this work will be beneficial to the increasing interests in semantic analysis and cross-lingual applications.",1,2018
D18-1195,"we hope that the problem we introduce in this work, together with the dataset that we release, inspires the community to develop models that can learn language (e.g., semantic parsers) through flexible natural language conversation with end users.",1,2018
D18-1196,"this includes applying more sophisticated tagging techniques such as bidirectional lstms, attention, and dynamic oracles, but most importantly developing new data and tasks to which the approach can be applied.",2,2018
D18-1196,we hope deepcx will inspire further work on scl.,1,2018
D18-1198,"in the future, we would also like to investigate if it is possible for alignments to be treated as latent variables, which can be learned in a joint manner within the current framework.",1,2018
D18-1201,we also plan to explore multilingual applications.,2,2018
D18-1201,"our model is capable of scoring bundles of new edges, and in future work, we plan to explore the possibility of combining m3gm with a search algorithm, to automatically extend existing knowledge graphs by linking in one or more new entities.",1,2018
D18-1204,"we also would like to take the citation sentences of each reference into consideration, which is another concise and universal data source for scientific summarization (chen and hai, 2016; cohan and goharian, 2017).",2,2018
D18-1204,"in future work, an appealing direction is to organize the selected sentences in a logical fashion, e.g., by leveraging a topic hierarchy tree to determine the arrangement of the related work section (cong and kan, 2010).",5,2018
D18-1204,"at the end of this paper, we believe that extractive methods are by no means the final solutions for literature review generation due to plagiarism concerns, and we are going to put forward a fully abstractive version in further studies.",1,2018
D18-1206,"in the future, we would like to create more linguistically-aware encoders and decoders incorporating co-reference and entity linking.",1,2018
D18-1207,including mechanisms to promote paraphrase generation in the summary generator could be an interesting direction.,5,2018
D18-1207,"future work could be done on closing the gap to match human levels of abstraction, which is still very far ahead from our model in terms of novel n-grams.",1,2018
D18-1209,"another interesting direction is to learn binary and compact network embedding, which could be more efficient in terms of both computation and memory, relative to its continuous counterpart (shen et al., 2018b).",1,2018
D18-1209,"in future work, we aim to leverage abundant unlabeled text data to abstract more informative sentence representations (dai and le, 2015; zhang et al., 2017; shen et al., 2017; tang and de sa, 2018) .",1,2018
D18-1213,"as part of future work, we would like to incorporate external knowledge as a side information for improved time-stamping of documents.",2,2018
D18-1215,future directions include: combining dpl with deep generative models; exploring alternative optimization strategies; applications to other domains.,1,2018
D18-1219,"in the future, we will study how to integrate context outside of nps for the task of choosing antencedents for bridging anaphors.",5,2018
D18-1219,also we hope that our word representation resource will facilitate other related research problems such as semantic role labeling.,1,2018
D18-1220,in the future we will study how this commonsense reasoning technique can contribute to solving edge cases and difficult examples in more general coreference tasks.,5,2018
D18-1221,"the reasonable next step will be to extend our methods for modelling the relations (edges) of a kb graph, which will allow applications in tasks such as link prediction and kb completion.",4,2018
D18-1221,"furthermore, having a mechanism that translates arbitrary text to points in a continuous space creates many opportunities for interesting research.",6,2018
D18-1221,the exciting question of how can we exploit this extra informationfor instance in order to enrich the knowledge base with new dataconstitutes one of our future directions.,2,2018
D18-1222,"in our future work, we will explore the following research directions: (1) sphere is a simple model to represent a concept in semantic space, but it still have some limits since it is too naive. we will try to find a more expressive model instead of spheres to represent concepts.(2) a concept may have different meanings in different triples.",1,2018
D18-1222,we will try to use several typical vectors of instances as a concepts centers to represent different meanings of a concept.,1,2018
D18-1223,our future work might consider incorporating external text data and also enhancing our model to make better use of multiple training examples in the few-shot learning case.,1,2018
D18-1224,one direction of future work is to use our framework to diagnose when a re is broken over a text stream.,5,2018
D18-1225,"in future, we would like to incorporate type consistency information to further improve our model and also integrate hyte with open-world knowledge graph completion (shi and weninger, 2018).",1,2018
D18-1225,we are hopeful that our proposed temporal representation learning algorithm will motivate further research on temporal kg embedding learning.,1,2018
D18-1226,"future directions include investigations on employing alternative neural architectures such as convolutional neural networks (cnns) as adaptation layers, as well as on how to learn the optimal value for from the data automatically rather than regarding it as a hyper-parameter.",1,2018
D18-1227,"in the future, we plan to extract more patterns to obtain more comparative sentences, so that we may more accurately demonstrate how useful performing comparative analysis after linking the business mentions can be.",2,2018
D18-1229,"assessing the effectiveness of visual supervision for these tasks is outside of the scope of this paper, however, and we will consider these studies in future work.",6,2018
D18-1230,"also, the proposed framework can be further extended to other sequence labeling tasks, such as noun phrase chunking.",4,2018
D18-1230,"in future, we plan to further investigate the power and potentials of the auto ner model with tie or break scheme in different languages and domains.",4,2018
D18-1230,"moreover, going beyond the classical ner setting in this paper, it is interesting to further explore distant supervised methods for the nested and multiple typed entity recognitions in the future.",1,2018
D18-1232,"we also plan to further study the biased dis- tillation problem and explore the compatibility of our approach in other nlp tasks such as natural language inference (bowman , 2015), answer sentence selection (yang , 2015) and so on.",4,2018
D18-1232,"in future work, we will explore new distillation methods that have better compression capabilities for mrc tasks, such as distilling knowl- edge from a single model instead of the ensemble without lossing performances, adding weights on knowledge based on the distilling quality and so on.",1,2018
D18-1235,"in the future, we plan to design models which could generate all possible answers for a single question.",1,2018
D18-1236,"lastly, one can also interested in developing task-oriented rewards for adapting the agent to a specific task, for example, the answer generation task for open-domain kbqa system.",4,2018
D18-1236,"our work suggests at least three future research topics: firstly, one can enrich the theoretical study of the duality between the oie and oin tasks.",6,2018
D18-1236,"secondly, one can investigate how to conquer the barrier of the absence of an extensive collection of reasonable sets of open-domain facts and incorporate unsupervised information into this logician orator dual learning structures for further improvement.",1,2018
D18-1237,future work includes developing a scalable read/write accessing mechanism to handle a large-scale external memory to reason over multiple documents.,1,2018
D18-1238,"while our proposed encoder demonstrates promise on reasoning and understanding natural language, we believe that our encoder is generalizable to other domains beyond reading comprehension.",4,2018
D18-1239,"future work should explore scaling the model to other question answering tasks, using more general composition modules, and introducing additional module types.",4,2018
D18-1240,"there is significant room for further elaboration of this approach, for example, by expanding feature spaces with more syntactic and semantic features, employing new types of kernels for measuring the inter-question/answer pair similarity or trying to implement the same idea in dnn architectures.",1,2018
D18-1242,possible future work includes supporting more complex semantics like implicit time constraints.,1,2018
D18-1243,"furthermore, we will conduct research in how to utilize entity information to assign more appropriate initial parameters of the relation extractor.",1,2018
D18-1243,"in the future, we will incorporate the sdp and stp to obtain more precise shortened sentences.",1,2018
D18-1245,"in future work, we will build a domain-specific distant supervision dataset with a higher ratio of multiple instances and compare our system with others.",2,2018
D18-1245,"furthermore, we will consider not using rnns or cnns, but a deeper neural networks with only attentions for distantly supervised re, similar to the work in vaswani et al.(2017).",1,2018
D18-1246,"for future work, we consider adding coreference information as an entity mention can have coreferences, which help on information collection.",2,2018
D18-1246,"not only content words, but also propositions can introduce word sense problem (gong et al., 2018).",1,2018
D18-1246,confusing caused by word senses can be a severe problem.,1,2018
D18-1246,Another possible direction is including word sense information.,1,2018
D18-1247,"in the future, we plan to explore the following directions: (1) it will be promising to adopt extra information to help train more efficient models for solving the long-tail relation problem.",2,2018
D18-1247,(2) we may also combine our attention method with recent denoising methods to further improve model performance.,1,2018
D18-1248,"in the future, we plan to utilize more information in knowledge graphs to improve the distant supervision signal.",2,2018
D18-1248,"for instance, the reasoning path can introduce new prior knowledge, which is a key direction in current works of kg.",1,2018
D18-1250,"our experiments also highlighted the existing challenges for neural relation classification models, including cross-sentence relations and imbalanced data.",5,2018
D18-1253,"third, we would like to generalize sdn to a wide range of complex goal-oriented tasks beyond dialogue, such as the particularly challenging atari game of montezumas revenge (kulkarni , 2016).",4,2018
D18-1253,"first, we want to integrate subgoal discovery into dialogue policy learning rather than treat them as two separate processes.",1,2018
D18-1253,"second, we would like to extend sdn to identify multi-level hierarchical structures among subgoals so that we can handle more complex tasks than those studied in this paper.",1,2018
D18-1254,"several future research directions are enabled by our study, ranging from the use of neural clustering models to the application of our models to fast and semi-automatic prototyping of dialog systems.",1,2018
D18-1255,"going forward, we would like to build models which are a hybrid of span prediction models and generation models.",1,2018
D18-1256,"we hope this work will spur more research in hybrid approaches that can work in open-ended, goal-oriented settings.",6,2018
D18-1257,"secondly, to use the generated questions more effectively, the representative-based semisupervised approach might be improved by techniques studied in active learning and hard example mining (settles, 2009; shrivastava et al., 2016; chang et al., 2017).",1,2018
D18-1257,"in our future work, we would like to design algorithms to better model a long context, to utilize external knowledge, and to explore more effective semi-supervised learning approaches.",1,2018
D18-1257,"firstly, we would like to investigate efficient ways of utilizing external knowledge such as paraphrasing and semantic concepts like prior works (dong et al., 2017; dasigi et al., 2017).",1,2018
D18-1260,"we leave closing this gap for future research, and illustrate, via oraclestyle experiments, the potential of better retrieval and reasoning on this task.",3,2018
D18-1263,"future work can employ this linearization function within downstream applications, as was done with syntactic linearization, or extend this framework with other graph-based representations, such as universal dependencies (nivre , 2016) or amr (banarescu , 2013).",4,2018
D18-1265,"future work includes exploring alternative approaches such as transition-based methods (nivre et al., 2006; chen and manning, 2014) for semantic parsing with latent dependencies, applying our dependency-based hybrid trees on other types of logical representations (e.g., lambda calculus expressions and sql (finegan-dollak et al., 2018)) as well as multilingual semantic parsing (jie and lu, 2014; susanto and lu, 2017a).",1,2018
D18-1266,"for the future, we plan to apply the proposed learning framework to more semantic parsing tasks and consider new methods for policy shaping.",4,2018
D18-1268,"for future work, we would like to extend this work in the semi-supervised setting where insufficient bilingual dictionaries are available.",2,2018
D18-1269,"we found that cross-lingual encoder baselines provide an encouraging and efficient alternative, and that further work is required to match the performance of translation based methods.",1,2018
D18-1270,similar techniques can be applied to other information extraction tasks like relation extraction to extend them to multilingual settings.,4,2018
D18-1270,"for all xel approaches, the task of candidate generation is currently limited by existence of a target language wikipedia and remains a key challenge.",5,2018
D18-1270,"a joint inference framework which enforces coherent predictions (cheng and roth, 2013; globerson et al., 2016; ganea and hofmann, 2017) could also lead to further improvements for xel.",1,2018
D18-1271,"for future work, we plan to 1) conduct more experiments and analyses following this preliminary study to verify our approachs effectiveness for more languages and domains (e.g., social stream vs news stream);",3,2018
D18-1271,3) apply our approach to real-time coordinated text streams for never-ending knowledge mining and use the mined knowledge to improve the downstream applications.,4,2018
D18-1271,"for future work, we plan to 1) conduct more experiments and analyses following this preliminary study to verify our approach effectiveness for more languages and domains (e.g., social stream vs news stream); 2) attempt to use word embedding (e.g., word2vec (mikolov et al., 2013), glove (pennington et al., 2014) and elmo (peters et al., 2018)) for local context encoding and use it as a clue for decipherment; 3) apply our approach to real-time coordinated text streams for never-ending knowledge mining and use the mined knowledge to improve the downstream applications.",3,2018
D18-1272,we also focus on automatically generating homographic puns.,6,2018
D18-1272,"in future work, we would like to find an appropriate way in incorporating the external linguistic knowledge to improve the performance of homographic puns recognition.",1,2018
D18-1273,"in this paper, we do not put too many efforts into model design for csc, which we leave as potential future work.",1,2018
D18-1277,future work on dataset creation can include generating similar challenge datasets for different key ambiguities in nlp.,2,2018
D18-1277,a third direction for research would be using this dataset to evaluate different contextual modeling approaches and investigate the creation and using such context sensitive dataset to create simpler and smaller models that can capture a lot of contextual word representation.,1,2018
D18-1277,"future work can include exploring ways to incorporate more context into the tagger, possibly by using information from dependency tree.",1,2018
D18-1277,"also investigating more downstream tasks and explore if this dataset can be used directly in downstream tasks in a way similar to what have been done in swayamdipta et al.(2017) and (eriguchi et al., 2017; niehues and cho, 2017; kiperwasser and ballesteros, 2018) for injecting syntax in semantic role labeling and translation tasks.",1,2018
D18-1279,"in the future, we would like to explore using the intnet model for other nlp tasks.",4,2018
D18-1280,"in the future, we plan to test icon on other relevant dialogue-based applications and also use it for empathetic dialogue generation.",4,2018
D18-1282,"encouraged by our findings, future work includes the exploration of the model and its learned frame-semantic representations for tasks such as the interpretation of multimodal scenes and stories and referring expressions.",1,2018
D18-1283,our future work will extend the model and findings from this work to vision processing that will not only identify commonsense evidence but also explain where and how in the perceived environment the evidence is gathered.,1,2018
D18-1284,"because we use short text and can leverage domain knowledge, we believe future work could use our models for applications such as personalized dialogue systems.",4,2018
D18-1285,"in the future, we intend to add a generative model along with a physical simulation allowing the learner to imagine scenarios where a predicate might not hold.",1,2018
D18-1285,we intend to combine the weakly supervised parser with an unsupervised parser and learn to determine whether a sentence should be grounded visually during training.,1,2018
D18-1286,"in future work, we are interested in investigating mechanisms to improve generalization to new environments.",6,2018
D18-1286,"for example, pointer and graph networks (vinyals et al., 2015; defferrard et al., 2016) are a promising direction to help supervise translation models and predict motion behaviors.",1,2018
D18-1287,"another important open question concerns automated evaluation, which remains especially challenging when instructions do not only specify goals, but also constraints on how to achieve them.",5,2018
D18-1287,achieving this while retaining an interpretable goal representation that clearly determines the execution is an important direction for future work.,6,2018
D18-1290,"relevant directions include improving the transition logic of our parser, the bilstm ne model and the interactions between the two models.",1,2018
D18-1290,in future work we intend to explore methods for closing the performance gap our algorithms still have for msg queries (both uas and segmentation f1) and for ssg queries (uas only).,1,2018
D18-1291,"in future work it would be interesting to investigate whether the patterns observed here also hold true for other types of models in dependency parsing; possible variations to examine include alternative character models such as convolutional neural networks, joint tagging-parsing models, and graph-based parsers.",1,2018
D18-1293,"finally, extending this method to work for pcfgs and more complex probabilistic models is an important open problem.",5,2018
D18-1298,"as pretraining leads to a considerable improvement in performance, future work could be done fine-tuning this model for various dialog systems.",1,2018
D18-1298,future work may also entail building more advanced strategies to select a limited number of personas for each user while maximizing the prediction performance.,1,2018
D18-1299,"our future work is to evaluate the performance of our models in the scenario where there are new slots and more unobserved slot values, and to evaluate the domain-transferring ability of our models.",1,2018
D18-1302,"although our work is preliminary, we hope that our work can further develop the discussion of evaluating nlp systems in different directions, not merely focusing on performance metrics like accuracy or auc.",4,2018
D18-1302,"although this work focuses on gender terms, the methods we proposed can easily be extended to other identity problems like racial and to different tasks like sentiment analysis by following similar steps, and we hope to work on this in the future.",4,2018
D18-1303,"in future work, we hope to experiment with the transferability of our model to other datasets to encompass the diverse mediums through which these personal stories are shared.",2,2018
D18-1306,one interesting problem that our models do not account for is the existence of overlapping and non-continuous entity spans.,5,2018
D18-1308,"in future work we thus hope to apply this model to larger datasets, and to address the efficiency issue.",2,2018
D18-1308,"another promising direction would be to move to convolutional encoder and decoder architectures, designing the latter in a way similarly capitalizes on the label space tree structure.",5,2018
D18-1309,we also consider modeling the dependencies between regions.,1,2018
D18-1309,"For future work, we would like to investigate the use of region-level information.",6,2018
D18-1311,"as for future work, we are testing on languages other than english.",2,2018
D18-1312,it would be interesting to have a similar measure for headmarking situations with dependencies marked on the governor.,5,2018
D18-1316,we hope our work encourages researchers to pursue improving the robustness of dnns in the natural language domain.,1,2018
D18-1320,"the proposed approaches can also be applied to other languages such as min nan or hakka, which are spoken languages that are even less well-documented than cantonese.",2,2018
D18-1320,"for future work, recursive neural network (tai et al., 2015) can be used as it is better suited for the hierarchical logographic decomposition.",1,2018
D18-1320,"besides, incorporating more detailed relationship between radicals (e.g.(zhuang et al., 2017)) can help improve the model.",1,2018
D18-1322,"for future work, we would like to evaluate our approaches in more applications.",3,2018
D18-1322,"in addition, we would like to continue improving the marginal estimation by experimenting with recent density estimation techniques such as nade (uria et al., 2016).",1,2018
D18-1322,"for example, we can use the marginal statistics for information extraction, or to detect and remove abnormal phrases in text generation.",1,2018
D18-1325,"in future work, we plan to explicitly model discourse connections with the help of annotated data, which may further improve translation quality.",1,2018
D18-1326,"for future work, we plan to extend our strategies on many-to-many multilingual translation scenarios, and explore other effective strategies to balance parameter sharing.",4,2018
D18-1327,"in the future, we will explore adding back translated (sennrich et al., 2016a) or copied (currey et al., 2017) target data to our multi-source system.",1,2018
D18-1328,"in addition, we would like to measure the performance of our model after applying subword tokenisation, as well as using multiple lstm layers, a technique well known to capture hierarchical structure in the context of mt.",3,2018
D18-1328,"we plan to use our model to predict sentence embeddings over monolingual corpora, allowing to collect parallel pairs through vector similarity measures.",1,2018
D18-1329,"we will also apply this evaluation method to other tasks that use additional context, e.g.images in visualquestion answering, or part-of-speech tags in neural machine translation.",4,2018
D18-1329,future work includes augment existing multimodal translation models with an additional adversarial objective that forces the model to perform better in the presence of the congruent image than a random incongruent image.,1,2018
D18-1331,"in the future, we hope to find out more patterns and generalized rules to explain the models learning of the temperature.",1,2018
D18-1332,in the future we would like to apply local optimizers in distributed setting where the communication latency between local and remote devices varies significantly we could use local optimizers to synchronize remote models less often.,1,2018
D18-1333,"in future work we plan to: 1) build a fully end to-end nmt model for dp translation, which does not depend on any external component (i.e.",1,2018
D18-1333,"dpp predictor); 2) exploit cross-sentence context (wang et al., 2017) to further improve dp translation; 3) investigate a new research strand that adapts our model in an inverse translation direction by learning to drop pronouns instead of recovering dps.",1,2018
D18-1334,"in the future, we would like to conduct further manual evaluation on the translations to further analyze the differences with the baseline system.",3,2018
D18-1334,"furthermore, we aim to experiment with other ways of integrating speaker information.",1,2018
D18-1335,we consider the results promising and try more language pairs and fine-tune the hyperparameters.,2,2018
D18-1335,"as future work, we aim to develop a bidirectional 2dlstm and consider stacking up 2dlstms for a deeper model.",1,2018
D18-1336,"another direction of improving the model might be efficient implementation of beam search which can contain rescoring using an external language model as often done in speech recognition (graves et al., 2013).",1,2018
D18-1336,"as a future work, we can try to improve the performance of the model by iterative denoising as done by lee et al.(2018) while keeping the nonauto regressive nature of the decoder.",1,2018
D18-1339,we leave as future work a deeper analysis of the level of character-awareness encoded in representations of the bpe segments as a byproduct of training.,6,2018
D18-1341,future work includes inducing macro-actions composed of simpler building block actions.,6,2018
D18-1344,"as future work, it will be interesting to explore techniques that can generate artificial cm data following the lexical, semantic and pragmatic constraints, or develop novel embedding techniques that can appropriately interpolate between real and artificial cm data to learn collocations that arise due to not only syntactic but also lexical, semantic and pragmatic aspects of code-mixing.",1,2018
D18-1347,"this problem is treated in recent work on pos-tagging for social media text (owoputi , 2013; gimpel , 2011), but these issues have yet to be fully explored in code-switched contexts.",5,2018
D18-1347,"this problem is treated in recent work on pos-tagging for social media text (owoputi et al., 2013; gimpel et al., 2011), but these issues have yet to be fully explored in code-switched contexts.",5,2018
D18-1349,"we plan to make use of the rest unannotated abstracts or full texts to pre-train our model and then fine tune it to the target annotated datasets inspired by the work from (howard and ruder, 2018) so that the performance can be further boosted.",1,2018
D18-1352,"for our method to be useful for human coders, it is important to develop an accurate novelty detector.",1,2018
D18-1352,"if we can take advantage of all this structured and unstructured information via methods such as transfer learning or multi-task learning, then we may be able to predict infrequent labels better.2.",1,2018
D18-1352,"we plan to study methods for determining if an instance contains an infrequent label and if it does, how many infrequent labels it should be annotated with.",1,2018
D18-1353,"can we quantify some other intractable criteria, e.g, poeticness?",5,2018
D18-1353,how to design the methods of communication among many generators?,5,2018
D18-1353,would the collaboration of more learners lead to better results?,5,2018
D18-1354,"in future works, we will explore the use of other attributes of responses such as part-of-speech (pos) tags and chunking sequences as additional conditions for better response generation.",3,2018
D18-1355,"in future, we would like to investigate deeper into the different effects of additional memory and internal memory.",3,2018
D18-1358,"in the future, we will utilize more sophisticated models to leverage the hrs information, e.g, (1) utilize the embeddings of the three layers in a more sophisticated way instead of sum them together; (2) determine the number of relation clusters and sub-relations automatically instead of manually.",1,2018
D18-1360,we also plan to extend our multi-task framework to information extraction tasks in other domains.,4,2018
D18-1360,future work includes improving the performance using semisupervised techniques and providing in-domain features.,1,2018
D18-1361,"in this way, we can further build an end to-end framework for the large-scale q20 games in the real world.",1,2018
D18-1361,"as for the future work, we plan to explore methods to use machine reading to automatically construct the state transition dynamics from corpora like wikipedia.",1,2018
D18-1362,"in future work, we would like to investigate learnable reward shaping and action dropout schemes and apply model-based rl to this domain.",1,2018
D18-1367,"as a future work, we plan to investigate if our qq-based models are enhanced by extra-linguistic knowledge, and to incorporate contextual, multimodal features along semantic ones.",1,2018
D18-1368,"in the future, we plan to incorporate se type information in various downstream applications, e.g., many information extraction applications that require distinguishing specific fact descriptions from generic statements.",4,2018
D18-1370,"in the future work, we would like to extend the collection of scientific text to other fields.",4,2018
D18-1370,"next, we intend to explore a wider range of mtl models, especially those involving more than two tasks.",1,2018
D18-1370,"having annotated argumentative relations, we will work on models for their automated identification in scientific publications.",1,2018
D18-1371,"therefore, in future work we plan to develop joint models that simultaneously extract events and time expressions, and parse their temporal dependency structure.",1,2018
D18-1374,this could also be explored for better feature representation in our problem.,5,2018
D18-1374,"many avenues for further work can be identified, including: finding a better metric capturing novelty of entities, analyzing the influence of the size of the input passage to quality of predictions, and experimenting with new models.",1,2018
D18-1374,"moreover, recent work has considered incorporating knowledge graph information for better use of entity features (dalton et al., 2014; liu et al., 2018).",1,2018
D18-1375,"as future work, we will extend the proposed method to include components that learn weights for individual feature elements and not only the entire feature type.",1,2018
D18-1378,"in future work, we plan to extend limbic to capture long-distance discourse relations and the influence decay of discourse relations between seus as their distance increases.",1,2018
D18-1379,"in the future, we will explore the possibility of learning a topic model and an emotion ranking function simultaneously in a unified framework.",1,2018
D18-1382,"future direction of work also include adding more dimensions, e.g.emotion analysis & intensity prediction.",6,2018
D18-1382,"in future, we would like to investigate new techniques, and explore the ways to handle implicit sentiment and sarcasm.",1,2018
D18-1383,"the proposed framework could be potentially adapted to other domain adaptation tasks, which is the focus of our future studies.",4,2018
D18-1389,we are also interested in characterizing the factuality of reporting for media in other languages.,2,2018
D18-1389,"in future work, we plan to address the task as ordinal regression, and further to model the interdependencies between factuality and bias in a joint model.",1,2018
D18-1390,"in the future, we will seek to explore the following directions: (1) we will explore more ljp subtasks and more scenarios of cases such as multiple defendants and charges to investigate the effectiveness of topjudge.",4,2018
D18-1390,"(2) we will explore how to incorporate into ljp the temporal factors, which are not considered in this work.",1,2018
D18-1395,"we would also like to test the classifiers defined here in the more challenging scenario of smaller text chunks (e.g., 10c20 sentences rather than the 100-sentence text chunks we used here).",2,2018
D18-1395,"furthermore, we plan to devise unsupervised approaches to the identification of native language with the same dataset.",1,2018
D18-1395,"finally, we are currently experimenting with adversarial learning models for this task.",1,2018
D18-1396,"for future works, we will study more left/rightbranching languages as well as other languages that have no obvious branching characteristics.",2,2018
D18-1396,"we will also investigate how language branching influences other natural language tasks, especially for neural networks based models.",4,2018
D18-1399,"in the future, we would like to extend our approach to semi-supervised scenarios with small parallel corpora, which we expect to be particularly helpful for tuning purposes.",2,2018
D18-1399,"moreover, we would like to try a hybrid approach with nmt, using our unsupervised smt system to generate a synthetic parallel corpus and training an nmt system over it through iterative backtranslation.",1,2018
D18-1400,"we also want to change the visual pre-training model from an image classification dataset to other datasets that have both objects and actions, to further improve translation performance.",1,2018
D18-1400,"in the future, we will continue exploring different methods to ground the visual context into the translation model, such as learning a multimodal shared space across image, source language text, as well as target language text.",1,2018
D18-1401,"besides, we would like to test the effectiveness of the proposed approach to qa-style sentiment classification in some other languages.",3,2018
D18-1401,"in the future, we would like to investigate some other network structures to explore deeper information in each qa text pair.",6,2018
D18-1402,"furthermore, we are experimenting with language adaptation and plan to extend the tool to the german language.",2,2018
D18-1402,we also intend to investigate methods for grouping similar arguments.,1,2018
D18-1403,we would also like to develop methods for abstractive opinion summarization using weak supervision signals.,1,2018
D18-1403,"in the future, we plan to develop a more integrated approach where aspects and sentiment orientation are jointly identified, and work with additional languages and domains.",1,2018
D18-1404,we plan to employ transfer learning methods with the proposed enriched patterns and test on other emotion-related problems such as sentiment classification and sarcasm detection.,3,2018
D18-1404,the proposed methodology is also being expanded to support spanish and japanese emotion recognition tasks.,4,2018
D18-1404,"in the future work, we aim to investigate the graph-based patterns more in-depth and provide a more comprehensive and advanced theoretical discussion of how they are constructed.",6,2018
D18-1404,we also hope to keep improving the pattern weighting mechanism so as to improve the overall performance on emotion recognition tasks and minimize trade-off between pattern coverage and performance.,1,2018
D18-1409,"in future work, we will explore the direction of adding an extra coherence reward (wu and hu, 2018) to improve the quality of extracted summaries in terms of sentence discourse relation.",6,2018
D18-1410,"for future work, we would like to extend our lexicon to cover specific domains, different target users and languages.",4,2018
D18-1411,"for future study, one could collect large-scale data and texts in other domains where more complex grounding on phrases such as increasing trends should be done.",4,2018
D18-1411,"to enhance modeling power, unsupervised discriminative models that utilize rich features (berg-kirkpatrick et al., 2010) could also be explored.",1,2018
D18-1411,we are also interested in collecting more high-quality parallel data to induce grounded compositional logic representations.,1,2018
D18-1413,"though the proposed model works well empirically, understanding exactly what is learned in the latent variables is non trivial, and is a possible direction for future work.",1,2018
D18-1417,"in future works, we plan to improve our model by introducing extra knowledge.",1,2018
D18-1421,"in the future, we plan to apply the framework and training techniques into other tasks, such as machine translation and dialogue.",4,2018
D18-1422,"we will also extend the set of operations to accommodate historical data, graph data and detect the unsupported facts in the generation within the single framework.",2,2018
D18-1422,"as applying operations on a large number of records greatly increases the search space for the attention mechanism, we will extend our model to automatically detect the relevant operations to reduce computing complexity.",1,2018
D18-1424,"our future work lands in the following directions: incorporate rich features, such as pos and entity, in input passages; directly optimize sequence-level metrics with policy gradient; relax the constraint on answer to accept abstractive answers; jointly model question generation and question answering; ask multiple questions simultaneously with diverse perspectives.",1,2018
D18-1426,"in future work, we will focus on the semi-supervised approach to make the dae also suitable for problems where instead of all, only a subset of the structured information should be included in the output.",1,2018
D18-1429,"as future work, we would like to design better metrics for answerability and check if a non-linear combination of different elements in the q-metric leads to better correlation with human judgments.",3,2018
D18-1430,"for future works, we will consider adopting the mutual information regularization for other text generation tasks which encourage stylistic generation or diversity to improve their performances.",6,2018
D18-1430,"besides, our model simplifies the prior of style distribution as a uniform distribution.",1,2018
D18-1430,another intriguing direction is to refine our model for more task-specific scenarios.,1,2018
D18-1432,we also plan to verify the results with a human evaluation study.,3,2018
D18-1432,this will allow us to use extend the notion of coherence to account for phenomena such as topic shifts.,4,2018
D18-1432,"in future work, we plan to replace the glove based measure of coherence with a trained discriminator that distinguishes between coherent and incoherent responses (li and jurafsky, 2017).",1,2018
D18-1433,"in future work, we are increasing the size of dataset and exploring other knowledge-centric metrics for this task.",2,2018
D18-1434,we also aim to consider the generalisation of this approach to other vision and language tasks.,4,2018
D18-1434,"in future, we would like to analyze means of obtaining composite embeddings.",1,2018
D18-1435,we will also make further research on context-aware fine-grained entity typing to train a better template generator.,1,2018
D18-1435,another research direction based on this work is to develop an end to-end neural architecture to make the model more flexible without generating a template in the middle.,1,2018
D18-1435,"in the future, we will expand the entity-aware model to incorporate the relations between candidates when the model fills in the slots, which can avoid the cases such as cristiano ronaldo of barcelona.",1,2018
D18-1439,"our future work will focus on two areas: investigation on multi-document keyphrase generation, and incorporation of structure or syntax information in keyphrase generation.",1,2018
D18-1443,"preliminary work that investigates similar bottom-up approaches in other domains that require a content selection, such as grammar correction, or data-to-text generation, have shown some promise and will be investigated in future work.",4,2018
D18-1445,"on the experimental side, a logical next step is to implement an interactive user interface for april and conduct a larger evaluation study comparing the summary quality before and after the interaction.",3,2018
D18-1445,"we also plan to apply april to more nlp applications, including machine translation, information exploration and semantic parsing.",4,2018
D18-1445,"on the technical side, we plan to employ more advanced apl and rl algorithms in april, such as sample-efficient bayesian-based apl algorithms (e.g., simpson and gurevych, 2018) and neural rl algorithms (e.g.mnih et al., 2015) to further reduce the sample complexity of april.",1,2018
D18-1447,"for future work, we will 1) leverage unlabeled data to study domain adaptation or transfer learning for keyphrase generation; and 2) investigate novel models to improve absent keyphrase generation when limited labeled data is available based on semi-supervised learning.",1,2018
D18-1449,"for example, future research includes applying learning-to-rank regarding all outputs as features, conducting active learning to select a new model setting online, and developing boosting-like-ensemble based on the bagging of training data.",1,2018
D18-1451,"in future work, we hope to use extra discriminators to control the style and sentiment of the generated summaries.",1,2018
D18-1452,"it would be also interesting to extend the framework to a cross-domain (shah , 2018) or a cross-language setting (da san martino , 2017; joty , 2017).",2,2018
D18-1452,trying an ensemble of neural networks with different initial seeds is another possible research direction.,1,2018
D18-1452,"from a modeling perspective, we want to strongly couple crf and dnn, so that the global errors are backpropagated from the crf down to the dnn layers.",1,2018
D18-1452,"in future work, we plan to model text complexity (mihaylova et al., 2016), veracity (mihaylova et al., 2018), speech act (joty and hoque, 2016), user profile (mihaylov et al., 2015), trollness (mihaylov et al., 2018), and goodness polarity (balchev et al., 2016; mihaylov et al., 2017).",1,2018
D18-1455,"current directions for future work include c (1) extending graft-nets to pick spans of text as answers, rather than only entities and (2) improving the subgraph retrieval process.",1,2018
D18-1456,employing such a nil-aware answer span extractor in practical ir-style qa tasks will be interesting future work.,6,2018
D18-1457,"future directions include validating our approach on other architectures such as rnn (bahdanau , 2015) or cnn (gehring , 2017) based nmt models, as well as combining with other advanced techniques (shaw , 2018; shen , 2018; yang , 2018; li , 2018) to further improve the performance of transformer.",3,2018
D18-1457,"future directions include validating our approach on other architectures such as rnn (bahdanau et al., 2015) or cnn (gehring et al., 2017) based nmt models, as well as combining with other advanced techniques (shaw et al., 2018; shen et al., 2018; yang et al., 2018; li et al., 2018) to further improve the performance of transformer.",3,2018
D18-1459,"in the future, we will continue to examine the effectiveness of atr on different neural models for nmt, such as the hierarchical nmt model (su , 2018b) as well as the generative nmt model (su , 2018a).",3,2018
D18-1459,"we are also interested in adapting our atr to summarization, semantic parsing etc.",4,2018
D18-1462,"however, even with the best human evaluation results, the error analysis shows that there are still many challenges in narrative story generation, which we would like to explore in the future.",5,2018
D18-1467,"the existence of semantic neighbors occurring in similar contexts (e.g., the in- fluence of standard intensifier very on nonstandard intensifier af) may prevent a new word from reaching widespread popularity (grieve, 2018).",1,2018
D18-1467,future work should also investigate more semantically-aware definitions of linguistic dissemination.,1,2018
D18-1468,"in the future, we would like to investigate the inferred trees in detail.10 the source code is publicly available at https://github.com/murawaki/ lattyp.",6,2018
D18-1471,"as two of the most used functions of vulgar words relate to expressing sentiment or emotions, we will also explore collecting sentiment annotations for joint sentiment and vulgar word function inference and use this to improve the task of sentiment analysis using multi-task methods.",1,2018
D18-1471,"future work will use this linguistic information to inform more complex machine learning models, e.g., deep neural networks, in an attempt to increase predictive gains.",1,2018
D18-1475,"it is also interesting to combine with other techniques (shaw et al., 2018; shen et al., 2018a; dou et al., 2018; li et al., 2018) to further improve the performance of transformer.",1,2018
D18-1475,"another promising direction is to design more powerful localness modeling techniques, such as incorporating linguistic knowledge (e.g. phrases and syntactic categories).",1,2018
D18-1481,"in the future, we will adapt our mean-max aae to other low-resource languages for learning universal sentence representations.",2,2018
D18-1484,"in future work, we would like to explore other learning layers and generalize mtle to address other nlp tasks, for example, sequence labeling and sequence-to-sequence learning.",4,2018
D18-1486,"in future work, we would like to investigate the relations of various tasks in multi-task learning by exploiting the potential of capsule network.",6,2018
D18-1490,it might also be useful in very different applications such as image processing.,4,2018
D18-1490,"in future work, we also intend to investigate other applications of the auto-correlational kernel.",5,2018
D18-1491,"in future, we plan to study the performance of prus on different tasks, including machine translation and question answering.",4,2018
D18-1491,"in addition, we will study the performance of the pru on language modeling with more recent inference techniques, such as dynamic evaluation and mixture of softmax.",1,2018
D18-1493,"as for future work, we plan the following research directions: (1) in language modeling, given a sequence of words, a sequence of corresponding sememes can also be obtained.",4,2018
D18-1493,we will extend our model with the hierarchical sememe tree for more accurate relations between words and their sememes.(3) it is imaginable that the performance of sdlm will be significantly influenced by the annotation quality of sememe knowledge.,1,2018
D18-1493,"we will also devote to further enrich the sememe knowledge for new words and phrases, and investigate its effect on sdlm.",1,2018
D18-1493,we will utilize the context sememe information for better sememe and word prediction.(2) structural information about sememes in hownet is ignored in our work.,1,2018
D18-1494,"with the development of deep learning techniques, we also plan to de-emphasize irrelevant words with an attention mechanism.",6,2018
D18-1494,"for future work, we plan to speed-up the training process of sltm by gpus and distributed algorithms.",1,2018
D18-1495,our following work will consider adopting fastgcn to speed up the process.,1,2018
D18-1497,"going forward, disentangled representations may afford additional advantages in nlp, e.g., by facilitating transfer (zhang et al., 2017), or supporting aspect-focused summarization models.",1,2018
D18-1499,the psycholinguistic plausibility of these models can be tested in future work.,4,2018
D18-1499,"for example, the model of jaech and ostendorf (2018) adapts to environmental factors, so it could potentially draw on independent experiences with female speakers and with lawyer speech in order to initialize a model of adaptation to a new female lawyer (see also mikolov and zweig, 2012; kleinschmidt, 2018).",1,2018
D18-1500,future work can investigate why this is the case and how we can leverage this information to improve model performance and interpretability.,1,2018
D18-1505,future work includes a fine-grained quantitative study of elmo word vectors for logically complex sentences along the lines of peters et al.(2018b).,1,2018
D18-1508,"as for future work, we plan to apply our label wise attention mechanism to understand other interesting linguistic properties of human-generated text in social media, and other multi-class or multilabel classification problems.",1,2018
D18-1509,the interesting result that syntax-free trees outperform their syntax-driven counterparts elicits a natural question for future work: how do we better model syntactic structure in these models?,5,2018
D18-1509,it would also be interesting to study the effect of using source-side syntax together with the target-side syntax supported by trdec.,1,2018
D18-1512,"we expect that this will require further efforts in creating document-level training data, designing appropriate models, and supporting research with discourse-aware automatic metrics.",2,2018
D18-1512,it will be interesting to explore to what extent existing and future techniques for document-level machine translation can narrow this gap.,5,2018
D18-1514,future researches may consider incorporating commonsense knowledge or improved causal modules.,6,2018
D18-1518,"it worth exploring if our model can improve the performance of other natural language processing applications whose inputs contain multiple sentences, for example, reading comprehension, dialog generation, and sentiment analysis.",4,2018
D18-1519,"future work includes relation discovery, which is to identify new relations besides hypernyms in an unsupervised manner.",4,2018
D18-1520,it would also be vital for future work to explore efficient combinations with other refinement methods using language resources.,2,2018
D18-1520,"in future work, we have to modify the refinement method by relevance propagation to be more effective by exploring the mechanism of how the internal knowledge of word vectors is extracted by multilayer neural networks and examining the effectiveness of other relevance propagation methods.",1,2018
D18-1521,future directions include extending the proposed approach to model other properties of words such as sentiment and generalizing our analysis beyond binary gender.,1,2018
D18-1522,our future plans include exploring the value of the proposed methodology with other languages and additional properties.,2,2018
D18-1524,"we also intend to evaluate on additional benchmark tasks such as glue (wang , 2018a), explore using the learned word representations as contextualized embeddings and perform downstream fine-tuning.",4,2018
D18-1524,"in future work, we would like to explore using contextualized word embeddings, such as cove (mccann et al., 2017) and elmo (peters et al., 2018), as input to our models as opposed to noncontextualized representations.",1,2018
D18-1525,"the next step for this work is to evaluate the performance of the proposed loss functions in state of-the-art models for the nlp tasks that leverage sentence-level semantic representation, such as the ones we explored in the present study.",1,2018
D18-1529,"in this work, we showed that further research in chinese segmentation must overcome two key challenges: (1) rigorous tuning and testing of deep learning architectures and (2) more effort should be made on exploring resources for further performance gain.",2,2018
D18-1530,"as future work, we intend to tackle the harder samasa problem which requires semantic information of a word in addition to the characters context.",1,2018
D18-1533,future work could extend the fst to model orthographic changes suggested by an error analysis of the current models predictions (see appendix a.6).,1,2018
D18-1536,"compared with existing corpora, it is of high quality and challenging, and is hopefully useful for research on ssei, crosslingual and cross-domain learning.",4,2018
D18-1537,our future work will extend our study to consider other nlp tasks and models with the goal of producing useful insights for further improving these models.,4,2018
D18-1538,semi-supervised training from the scratch and examination of semi-supervised setting on large dataset remains as part of the future work.,1,2018
D18-1539,another area of future research is how the underlying system should recover when domain-adjacent instances are detected.,6,2018
D18-1539,future work includes exploring alternative ways of incorporating information outside of the given training set and experimenting with various combinations of semantic parsers and upstream domain-adjacency models.,1,2018
D18-1541,"among possible future work is using generative adversarial networks as corruption engines, and developing better sequence alignment methods.",6,2018
D18-1541,"some preliminary results with simple corruptions using word substitution and word dropout (iyyer et al., 2015) appear to be promising, and may feature as components of a future corruption system.",1,2018
D18-1543,"in future work, we plan to investigate what happens when training on more than 2 languages.",2,2018
D18-1543,it would be interesting to experiment with using datasets that are not balanced with respect to size.,2,2018
D18-1543,it would be interesting to experiment with different parsing architectures as well as varying those hyperparameters.,1,2018
D18-1545,"furthermore, we plan to extend this work by evaluating the augmentation on other nlp benchmarks such as language modeling, dependency parsing and semantic role labeling.",3,2018
D18-1545,"following these encouraging results, method can be improved by (1) considering the preferred chunk order of the language during rotation, (2) taking language specific flexibilities into account (e.g., spanish typically allows free subject inversion (unlike object)).",1,2018
D18-1548,"future work will explore improving lisas parsing accuracy, developing better training techniques and adapting to more tasks.",4,2018
D18-1549,"its an open question whether there are more effective instantiations of these principles or other principles altogether, and under what conditions our iterative process is guaranteed to converge.",5,2018
D18-1549,Future work may also extend to the semisupervised setting.,1,2018
D19-1001,"in the future, we would like to apply our approach to other sequence generation tasks.",4,2019
D19-1001,"additionally, we wonder if a further performance increase could be achieved if the pre-training of bert would employ our placeholder strategy.",1,2019
D19-1002,"additional future directions for this line of work include application on other tasks such as sequence modeling and multi-document analysis (nli, qa); extension to languages other than english; and adding a human evaluation for examining the level of agreement with our measures.",4,2019
D19-1002,we view the conditions under which adversarial distributions can actually be found in practice to be an important direction for future work.,6,2019
D19-1003,together these findings raise serious concerns regarding the efficacy of active learning in practice.,5,2019
D19-1005,future work will involve incorporating a diverse set of domain specific kbs.,2,2019
D19-1006,another direction for future work is generating static word representations from contextualized ones.,1,2019
D19-1006,"therefore, adding an anisotropy penalty to the language modelling objective – to encourage the contextualized representations to be more isotropic – may yield even better results.",1,2019
D19-1007,"moreover, our framework models different semantic change scenarios, and future work could focus on approaches that are able to distinguish between these different scenarios.",1,2019
D19-1009,"thanks to the flexibility and scalability of our model, as future work we plan to explore in depth its use in different tasks, such as the creation of sentence (document) embeddings and lexical substitution.",4,2019
D19-1009,"in fact, we believe that using disambiguated sense vectors, as shown in the contextsensitive embeddings and paraphrase detection studies, can offer a more accurate representation and improve the quality of downstream applications such as sentiment analysis and text classification (see, e.g., (pilehvar , 2017)), machine translation and topic modelling.",4,2019
D19-1009,"encouraged by the good results achieved in our exploratory studies, we plan to develop a new model for contextualized word embeddings based on a gametheoretic framework.",1,2019
D19-1011,"in the future, we will study how to solve the logical consistency problem between utterances and candidate responses to improve selection performance.",5,2019
D19-1012,"one of the possible extensions of this work would be incorporating it with persona (zhang , 2018a) and task-oriented dialogue systems (gao , 2018; madotto , 2018; wu , 2019, 2017, 2018a; reddy , 2018; raghu , 2019).",6,2019
D19-1012,"one of the possible extensions of this work would be incorporating it with persona (zhang et al., 2018a) and task-oriented dialogue systems (gao et al., 2018; madotto et al., 2018; wu et al., 2019, 2017, 2018a; reddy et al., 2018; raghu et al., 2019).",6,2019
D19-1012,"having a persona would allow the system to have more consistent and personalized responses, and combining open-domain conversations with task-oriented dialogue systems would equip the system with more engaging conversational capabilities, hence resulting in a more versatile dialogue system.",1,2019
D19-1014,"in the future, we plan to collect a fashion retrieval visual dialog dataset which simulates a realistic application for multi-modal dialog systems.",2,2019
D19-1014,"furthermore, we will explore additional task-oriented settings where we can decouple task accomplishment from language generation to evaluate the extent our framework can generalize to other conversational tasks.",4,2019
D19-1014,"to address the limitation of a high image retrieval rate with just the use of captions from the visdial dataset, we plan to format a challenging candidate image pool in which images are visually similar to each other.",1,2019
D19-1015,"future works will focus on incorporating multimodal information into dialoguegcn, speaker-level emotion shift detection, and conceptual grounding of conversational emotion reasoning.",2,2019
D19-1015,we also plan to use dialoguegcn in dialogue systems to generate affective responses.,1,2019
D19-1016,"in addition, given that nrc vad is the only emotion-specific component, our model can be adapted as a generic model for conversation analysis.",1,2019
D19-1025,"in the future, we are interested in incorporating entity structural knowledge to enhance text representation (cao et al., 2017, 2018b), or transfer learning (sun et al., 2019) to deal with massive rare words and entities for low-resource name tagging, or introduce external knowledge for further improvement.",1,2019
D19-1026,the scalability of dca based models make it possible to handle large scale data with long documents.,1,2019
D19-1027,"in future work, we will explore incorporating external knowledge (e.g. word relatedness contained in word embeddings) into the learning framework for event extraction.",1,2019
D19-1027,"besides, exploring nonparametric neural event extraction approaches and detecting the evolution of events over time from news articles are other promising future directions.",1,2019
D19-1029,"in the future work, we will explore the use of the structured tuples to bridge the gap between text content and knowledge-based applications, such as knowledge-based scientific literature search.",1,2019
D19-1030,in the future we will explore more language-universal representations such as visual features from topically-related images and videos and external background knowledge.,1,2019
D19-1032,"as this work shows promising results for the end to-end dee, expanding the inputs of doc2edag from pure text sequences to richly formatted ones (wu et al., 2018) is appealing, and we leave it as future work to explore.",1,2019
D19-1033,"in future work, we will conduct experiments on more languages with and without explicit word delimiters.",2,2019
D19-1033,"in addition, we will try developing a dynamic mechanism to selectively consider the sense-level information rather than take all the senses of characters and words into account.",1,2019
D19-1034,"for future work, we consider to model the dependencies among entity regions explicitly and improve the performance of boundary detection module which is important for entity categorical label prediction.",1,2019
D19-1039,"interesting future work includes: (1) given that information from 2-hop ds is redundant and noisy, we can explore smarter sampling and/or better bag-level aggregation methods to capture the most representative information.(2) metadata in web tables like headers and column names also contain rich information, which can be incorporated to further improve re performance.",2,2019
D19-1040,"as shown by our experimental results, the contextualized entity encoder benefits more from this hyperlink-based training objective, suggesting future works to prioritize encoding entity description from its mention context.",6,2019
D19-1041,"since a better event model is generally helpful for relation extraction, another promising direction would be to incorporate multiple datasets to enhance the performance of our event extraction systems.",2,2019
D19-1041,"future research can focus on creating more robust structured constraints between events and relations, especially considering event types, to improve the quality of global assignments using ilp.",6,2019
D19-1042,"for future work, we will explore the effectiveness of the proposed framework on other base models and forms of data (e.g., images).",2,2019
D19-1042,we will introduce more losses covering other aspects in the objective function to further improve the performance of our framework.,1,2019
D19-1043,"in future, we would like to investigate the application of our theory in various tasks including reading comprehension and machine translating.",4,2019
D19-1045,"in the future, we will contribute new text dataset to few-shot learning, explore better feature extractor networks and do some industrial application.",2,2019
D19-1046,another promising direction is to understand whether more concentrated important features (lower entropy) lead to better human performance in supporting decision making.,5,2019
D19-1047,"in future work, we plan to validate its effectiveness for multi-label classification.",3,2019
D19-1047,"besides, we are interested in incorporating more powerful unsupervised methods into our architecture.",1,2019
D19-1048,"future work will explore the performance of latent generative classifiers in other challenging experimental conditions, including testing robustness to data shift and adversarial examples as well as zero-shot learning.",3,2019
D19-1048,"another thread of future work is to explore the performance of discriminative models with latent variables, and investigate combining pretrained representations with both generative and discriminative classifiers.",1,2019
D19-1049,other efforts could involve the use of a variable number of research topics for each reviewer and exploring ways to render reviewer profiles human interpretable.,5,2019
D19-1049,"such efforts could consider, for instance, the temporal variation of research interests in order to capture the relevance of a given reviewer to a given topic based on the recency of the contributions to a given area.",5,2019
D19-1049,a concrete direction for future work would be to consider enhancements in representing reviewers’ profiles.,6,2019
D19-1050,"future work should investigate how more complex transforms linking brain and machine can reveal parallel structure between the two systems. future work should integrate brain images derived from different behavioral tasks, and study which model–brain relationships are conserved across these behaviors.",1,2019
D19-1051,"we hope that this critique provides the summarization community with practical insights for future research directions that include the construction of datasets, models less fit to a particular dofigure 2: pairwise similarities between model outputs computed using rouge.",2,2019
D19-1053,"in future work, we plan to avoid the need for costly human references in the evaluation of text generation systems, and instead base evaluation scores on source texts and system predictions only, which would allow for ‘next-level’, unsupervised (in a double sense) and unlimited evaluation (louis and nenkova, 2013; bohm ¨ , 2019).",3,2019
D19-1056,further challenges for this novel architecture are to extend it to joint predicate and role labeling for more than one predicate at a time.,1,2019
D19-1056,"in future work we also aim to make the system more flexible, by extending it to few-shot or zero shot learning, to alleviate the need for an initial big annotated set, and thus to be able to generate srl data for truly resource-poor languages.",1,2019
D19-1057,"apart from our tentative experiments on the english dataset, applying the approach to other languages will be an interesting research direction to work on in the future.",2,2019
D19-1058,"we also plan to extend our methodology to nouns and adjectives, in a similar fashion to (o’gorman et al., 2018) and connect the resulting frames to those in verb atlas.",1,2019
D19-1058,"as future work, we plan to take full advantage of the novel semantic features available in verbatlas, such as wide-coverage selectional preferences and synset-level information, by exploiting them in multilingual srl and word sense disambiguation tasks.",1,2019
D19-1058,"our plans include integrating the selectional preferences from syntagnet (maru et al., 2019), a new, large-scale lexical-semantic combination resource.",1,2019
D19-1059,"in future work, we plan to consider subwords into the model and explore more geometric structures in sentences.",1,2019
D19-1060,"while our learning criteria showed benefit on certain classes of tasks, our hope is that the discoeval evaluation suite can inspire additional research in capturing broad discourse context in fixed-dimensional sentence embeddings.",1,2019
D19-1063,we are also exploring ways to deploy and evaluate our methods on real-world platforms.,3,2019
D19-1063,"future work aims to provide more natural, linguistically realistic interaction between the agent and humans (e.g., providing the agent the ability ask a natural question rather than just signal for help), and to establish a theoretical framework for modeling human assistance.",1,2019
D19-1064,"as future work, we plan to use visual information to specifically target complex downstream tasks requiring commonsense and reasoning such as question answering or visual dialogue.",2,2019
D19-1065,we hope that continued study of this area will produce models that can aid humans in critical domains like citizen science.,1,2019
D19-1066,the results of our questionnaire and system evaluation suggest the need for future work on supporting complex forms of reference within el systems; the datasets we provide can be used to evaluate such approaches.,2,2019
D19-1066,"another important direction is to either reach a consensus on the el task (perhaps in a similar style to muc-6 for ner), or define protocols for evaluation in the absence of such a consensus; our fuzzy recall/f1 metrics are concrete steps in this direction.",3,2019
D19-1068,"in the future, we plan to investigate more language-independent patterns in crosslingual transfer to circumvent this dependency.",2,2019
D19-1069,these results indicate that end to-end models might be a promising alternative to the traditional pipeline approach.,1,2019
D19-1071,"in the future, we may apply our pre-training method to other language pairs and delve into the performance of the pre-trained encoders on other nlp tasks, such as name entity recognition.",4,2019
D19-1072,further extensions are needed to tackle tree-structured syntax information.,1,2019
D19-1073,"as our approach is transparent to model architectures, we plan to further verify the effectiveness of our approach on other downstream applications of nmt such as post-editing and interactive mt in the future.",4,2019
D19-1075,"for future work, we plan to explore more powerful kg embedding methods.",1,2019
D19-1075,and we also have the idea of using categorical attributes or hierarchical types to guide the negative sampling process.,1,2019
D19-1076,this approach has applications beyond the scope of this paper and can be used in any applications requiring solving opp or gpp.,4,2019
D19-1077,"with such strong cross-lingual nlp performance, it would be interesting to prob mbert from a linguistic perspective in the future.",1,2019
D19-1078,"finally, we will apply our framework into other translation models (bahdanau , 2015; su , 2018; song , 2019), so as to verify the generality of our framework.",4,2019
D19-1078,"besides, how to leverage monolingual sentences of different domains to refine our proposed framework.",1,2019
D19-1078,"finally, we will apply our framework into other translation models (bahdanau et al., 2015; su et al., 2018; song et al., 2019), so as to verify the generality of our framework.",4,2019
D19-1078,"In the future, we plan to extend our framework to multi-domain NMT.",1,2019
D19-1079,"in the future, we will focus on training with more agents by translating apart of one sentence considering its advantage for each agent, rather than translating the whole sentence.",6,2019
D19-1080,"future work will be zero-shot translation without step-wise pre-training, i.e., combining individually pre-trained encoders and decoders freely for a fast development of nmt systems for a new non-english language pair.",1,2019
D19-1081,"while in the current work we used text fragments of 4 sentences, in future work we would like to consider longer contexts.",2,2019
D19-1082,"as our approach is not limited to specific tasks, it is interesting to validate the proposed model in other tasks, such as reading comprehension, language inference, and sentence classification.",4,2019
D19-1083,"in the future, we would like to extend our model to other sequence-to-sequence tasks, such as summarization and dialogue generation, as well as adapt the idea to other generative architectures (zhang et al., 2016, 2018).an open question is whether there are other structural issues that limit the benefits of increasing the depth of the transformer architecture, or whether the benefit of very deep models is greater for other tasks and dataset.",4,2019
D19-1084,"future work will explore different sets of languages paired with new, higher-quality elicited alignments of existing bitext.",2,2019
D19-1084,we expect that asking annotators to only align named entities (or some other token type) will greatly increase the annotation speed.,2,2019
D19-1084,these experiments will extend beyond ner to other tokenlevel tasks such as coreference resolution and event detection.,4,2019
D19-1084,"we will also explore architecture modifications to allow for task-specific annotation, training, and inference for a given ie task, where only certain spans are of interest (i.e. training our aligner only on ner-span alignments).",1,2019
D19-1085,"first, we will evaluate our method on other implication phenomena (or called unaligned words (takeno , 2017)) such as tenses and article words for nmt.",3,2019
D19-1085,"second, we will investigate the impact of different context aware models on zp translation, including multiattention (jean et al., 2017b) and context-aware transformer(voita et al., 2018).",1,2019
D19-1087,we will also investigate how to make better evaluation metrics with the help of error detection.,3,2019
D19-1087,we will also study how to improve machine translation model through error detection.,1,2019
D19-1087,"for example, how to improve mt models according to different class of errors.",1,2019
D19-1087,"in the future, we will explore more advanced techniques such as neural networks for error detection.",1,2019
D19-1088,"our approach can benefit from advanced exploitation of the gradients or other useful intermediate information, which we leave to the future work.",4,2019
D19-1089,we will consider more languages (hundreds or thousands) to study our methods in larger scale setting.,2,2019
D19-1089,"on the other hand, we will also consider other pre-training methods (song , 2019) for multilingual and low-resource nmt.",2,2019
D19-1089,"for future work, we will test our methods for many-to-many translation.",3,2019
D19-1089,"we will also study how to obtain language embeddings from monolingual data, which will make our method scalable to cover those languages with little or without bilingual data.",5,2019
D19-1091,"language-specific or language-family-specific improvements (i.e. proper dealing with different alphabets, or using an adversarial language discriminator) could potentially further boost performance.",1,2019
D19-1092,"our method is complementary with several other methods for cross-lingual transfer, such as annotation projection, and thus can be further integrated with these methods.",1,2019
D19-1094,"we would like to apply the proposed model in low-resource settings, e.g., to transfer roles from english to another language via annotation projection or to learn an srl model from weak supervision where only annotations for dependency labels are available.",1,2019
D19-1099,"for example, we can take an inspiration from either declarative constraints used in the previous work (punyakanok et al., 2008) or from literature on lexical semantics of verbs, studying patterns of event and argument realization (e.g., levin 1993).",1,2019
D19-1099,"for the future work, the structured refinement network can be further improved.",1,2019
D19-1100,"in the future, we would like to explore ways to extend our method to languages not supported by google translate through the use of pivot languages.",1,2019
D19-1101,"in future work, we plan to adapt active learning methods for easy deployment on crowdsourcing platforms, and to investigate techniques for automatically selecting good hyperparameters without recourse to a development set, which is often unavailable at the start of a crowdsourcing process.",1,2019
D19-1102,"finally, while the techniques presented in this paper might be applicable to other low-resource languages, we want to also highlight the importance of understanding the characteristics of languages being studied. different language pairs might benefit from other types of similarity (e.g., morphological) and investigating this would be another interesting future work for low-resource dependency parsing.",1,2019
D19-1103,"in the future, we plan to study the design and incorporation of fine-grained constraints considering multipule languages for cross-lingual transfer.",2,2019
D19-1103,"we also plan to adapt this constrained inference framework to other cross-lingual structured prediction problems, such as semantic role labeling.",1,2019
D19-1104,"some future directions include exploring the effects of contents in the memory, automating memory extraction from dataset, and improving the encoder.",1,2019
D19-1104,we found that the bilstm encoders are likely bottlenecks for the model.,1,2019
D19-1106,the original methodology presented in this paper paves the way for further computational work in quantifying parsing complexity and thus fine-grained modelling of human sentence processing.,1,2019
D19-1108,as future work we will apply the framework for other text reading tasks.,4,2019
D19-1108,another promising direction is to explore the benefits of text classification model in an edge-device setting.,1,2019
D19-1109,"we also see potential benefit in the development of a more expansive set of evaluation methods for commonsense knowledge mining, which would strengthen the validity of our conclusions.",3,2019
D19-1109,"in the future, we hope to explore whether this approach can be extended to mining facts that are not commonsense and to generating new commonsense knowledge outside of any given database of candidate triples.",4,2019
D19-1110,"we hope our method will facilitate the development of “thin” nlp models, that are faster, consume less memory, and are interpretable (schwartz et al., 2019).",1,2019
D19-1112,future directions include integrating more sophisticated training strategies of meta-learning algorithms as well as validating our algorithms on other datasets.,1,2019
D19-1113,"we seek to extend the use of par to other contextualized embeddings (devlin , 2019; mccann , 2017) in future work.",4,2019
D19-1115,"while our work shows the potential for high-quality efficient steganography and the realizable optimality of arithmetic coding, future advancements in language modeling can push steganographic performance even further.",1,2019
D19-1117,"besides, we will also conduct experiments on several other current summarization datasets like new york times (nyt) (paulus , 2018) and newsroom (grusky , 2018).",3,2019
D19-1117,further exploration on this and combination with other approaches like rl remains as our future exploration.,1,2019
D19-1120,we plan to explore weighting methods to better evaluate the importance of translation pairs.,3,2019
D19-1120,we will also study how to improve topic transformation with the topic link weight matrices.,5,2019
D19-1121,"our next steps are to further study the correlation with time lags, and to incorporate more sophisticated event extraction techniques.",6,2019
D19-1125,"in the future, we plan to experiment with other intermediate signals like dialogue acts.",3,2019
D19-1125,"further improvements could potentially be obtained from employing more advanced regularization losses (oliver et al., 2018).",1,2019
D19-1128,"in the future, we would like to extend our negative sampling strategies to other tasks.",4,2019
D19-1130,"a few directions for improvement have also been identified: 1) improving the performance on non-critical slots, 2) tuning the decoder with rl, 3) text generation from gcas.",1,2019
D19-1132,"motivated by the promising success of our model in this short paper, we will apply it to other diverse nlp tasks in future work.",4,2019
D19-1133,"we believe that the method can be extended and improved by examining other dimensionality reduction methods and alternatives to gmm, and by introducing other heuristics into the feature vector, such as sentence length and punctuation marks5.",1,2019
D19-1138,"future directions include adding the languageadversarial task during bert pre-training on the multilingual wikipedia corpus, which may further improve zero-resource performance, and finding better stopping criteria for zero-resource crosslingual tasks besides using the english dev set.",4,2019
D19-1139,"in future work, we plan to extend this method to unsupervised nmt (sun , 2019) and other natural language processing tasks, such as dependency parsing (li , 2018b) and reading comprehension (zhang , 2018b).",4,2019
D19-1139,"in future work, we plan to extend this method to unsupervised nmt (sun et al., 2019) and other natural language processing tasks, such as dependency parsing (li et al., 2018b) and reading comprehension (zhang et al., 2018b).",4,2019
D19-1140,"in future work, we will target new application scenarios, covering multi-class classification and regression tasks.",4,2019
D19-1144,this algorithm has the potential in other incremental tasks such as streaming asr and incremental tts.,1,2019
D19-1145,"furthermore, the structural position encoding can be also applied to the decoder with rnn grammars (dyer et al., 2016; eriguchi et al., 2017), which we leave for future work.",1,2019
D19-1145,"future directions include inferring the structure representations from the amr (song et al., 2019) or the external smt knowledge (wang et al., 2017).",1,2019
D19-1146,"in the future, we will explore the utility of multistage fine-tuning for many-to-one and many-tomany nmt.",3,2019
D19-1146,"we will also try to explicitly determine the impact of corpora sizes, language similarity, and domain on our approach, and propose further improvements according to our findings.",3,2019
D19-1147,future work include designing more sophisticated architectures and combination strategies as well as validating our model on other language pairs and datasets.,1,2019
D19-1148,"in future work, we plan to investigate the effectiveness of our approach in other types of induction tasks.",4,2019
D19-1152,"our code will be publicly released, and we hope this will serve as a robust base for furthering progress on training visual dialog agents with rl for other multi-agent grounded language games, adapting to learn to talk about novel visual domains, etc.",1,2019
D19-1153,this suggests an interesting future research direction toward data selection for cross-lingual transfer learning problems.,5,2019
D19-1156,our method can be extended to evaluate other text generation tasks.,4,2019
D19-1158,"finally, we also strive to acquire larger datasets.",2,2019
D19-1158,"this might be achieved by using a composition sub-word representation for modifiers, such as character-level encoding.",1,2019
D19-1158,"in future work, we intend to develop more accurate modifier representations to allow for better generalization to unseen modifiers.",1,2019
D19-1159,"as both the components of press can be easily integrated, future models can consider building upon them as a strong baseline system.",1,2019
D19-1160,"we obtain modest but positive improvements, which opens the question about how to increase the leverage of eye-tracking or other complementary data that is only available during training or comes from a different dataset.",5,2019
D19-1161,"the results indicate that grammar induction with types is viable using recent neural network-based models, and our analysis warrants further exploration in this area.",1,2019
D19-1162,"in future work, we hope to expand the size of convbank to improve its utility to researchers in training parsing models and studying conversational systems.",2,2019
D19-1162,we also hope to improve the performance of our parsing system by jointly training dependency parsing with semantic role labeling.,1,2019
D19-1163,"apart from standard accuracy improvement techniques such as better token embeddings and ensembling, possible future directions include a more fine-grained control of the recall trade-off, modeling the tokens outside the non-terminals instead of ignoring them, incorporating the parent’s embedding in edge scores, and a more efficient or approximate decoder similar to the greedy decoder from stern et al.(2017).",1,2019
D19-1165,we hope that this work would motivate further research into massively multitask and universal translation models.,1,2019
D19-1166,our datasets and models thus provide a foundation to investigate strategies for a tighter control on output complexity in future work.,1,2019
D19-1167,with this work we hope to inspire future work on understanding multitask and multilingual nlp models.,4,2019
D19-1168,"in our future work, we plan to enrich our hm-gdc model to solve discourse phenomena such as (zero) anaphora.",1,2019
D19-1169,"future studies on cross-lingual machine reading comprehension will focus on 1) how to utilize various types of english reading comprehension data; 2) cross-lingual machine reading comprehension without the translation process, etc.",5,2019
D19-1169,experiments on two chinese machine reading comprehension datasets indicate that the proposed model could give consistent and significant improvements over various state-of-the-art systems by a large margin and set baselines for future research on clmrc task.,1,2019
D19-1170,"as future work, we would like to consider handling additional types such as sorting or multiplication/division.",6,2019
D19-1170,we also plan to explore more advanced methods for performing complex numerical reasoning.,1,2019
D19-1171,"for instance, future work could extend upon this by using our methods to obtain more training data in cross-lingual qa setups (joty , 2017; ruckle , 2019b), or by combining them with other training strategies, e.g., using our methods for pre-training.",2,2019
D19-1171,"for instance, future work could extend upon this by using our methods to obtain more training data in cross-lingual cqa setups (joty et al., 2017; ruckl ¨ e et al.´ , 2019b), or by combining them with other training strategies, e.g., using our methods for pre-training.",2,2019
D19-1172,"in the future, we plan to improve our dataset by including more complex ambiguous situations for both single-turn and multi-turn questions, such as multi-hop questions, aggregation questions, etc.",1,2019
D19-1172,"we also plan to integrate the clarification-based models into existing kbqa system, and study how to iteratively improve the model based on human feedback.",1,2019
D19-1174,"directions for future work include devising approaches that perform sexism classification more accurately, enhancing the categorization scheme, and developing other ways to help counter sexism.",1,2019
D19-1177,"enhancing the prediction of helpful reviews with unlabeled data: as a small proportion of reviews could be heuristically regarded as helpful or unhelpful, it thus becomes a promising study to automatically predict the helpfulness of online reviews based on the small amount of labeled data and a vast amount of unlabeled data.",2,2019
D19-1177,"cross-domain helpfulness prediction of online reviews (chen , 2018b): given that it costs a lot on manually annotating a sufficient number of helpful reviews in a new domain, we should explore effective approaches on transferring useful knowledge from limited labeled samples in another domain.",4,2019
D19-1178,"an immediate extension is to further scale up the experiments to web-scale datasets consisting of millions of users, as has been successfully done for face recognition (kemelmachershlizerman , 2016).",2,2019
D19-1178,we are also considering further applications of the proposed approach beyond those in this paper.,4,2019
D19-1178,"it would also be interesting to explore community composition on the basis of the proposed embeddings (newell , 2016; waller and anderson, 2019).",5,2019
D19-1179,do architectures which transfer well across one style variable (e.g gender) generalize to other style variables (e.g age)?,5,2019
D19-1179,studying these and other aspects of the content-style relationship in pastel could be an interesting direction.• does any external variable co-varying with the text qualify to be a style variable/facet?,5,2019
D19-1179,what are the categories of style variables/facets?,5,2019
D19-1180,we will also scale the tripod dataset and move to a multi-modal setting where tps are identified directly in video data.,2,2019
D19-1180,"in future work, we will investigate the usefulness of tps for summarization and question answering.",1,2019
D19-1182,"in future work, we aim to expand this study to multiple languages.",2,2019
D19-1183,"apart from that, we will consider alternative evaluation criteria to account for rich surface variability of natural speech.",3,2019
D19-1183,we will also explore ways to enable more efficient copying from the input which is crucial for correctly handling entities and therefore attaining high goal oriented performance of the system.,1,2019
D19-1183,"in our own future work, we will try and find ways to improve the unsupervised representation (shi et al., 2019) in order to increase the transfer potential.",1,2019
D19-1184,a particularly interesting application would be to generalize this method to language generation tasks.,4,2019
D19-1184,"first, this method is general and broadly applicable, which suggests that it may improve performance on other tasks and domains.",4,2019
D19-1184,"third, while this paper focuses on capturing multiple representations at different levels of granularity, it would be interesting to generalize mgt to learning multiple representations along several different axes (e.g., domains, styles, intents, etc.).",6,2019
D19-1184,"second, a useful improvement on top of mgt would be a more sophisticated method of combining the multiple granularities of representations.",1,2019
D19-1186,"instead of specifying a dialog act manually, the most appropriate one can be decided automatically.",1,2019
D19-1186,"In the future work, we will explore the act interactions with HRG.",1,2019
D19-1188,we would like to explore the effectiveness of the approach regarding other structures in future work.,4,2019
D19-1190,"in future work, we will use this technique to distill information from other nonparallel datasets, such as external informative text (qin , 2019; galley , 2019).",2,2019
D19-1193,"in the future, we will explore models to make better use of dialogue partners’ persona for response selection.",1,2019
D19-1196,"due to the flexibility of our hierarchical encoder decoder framework and the cmr decoder, abundant future research direction remains as applying the transformer structure, incorporating open vocabulary and copy mechanism for explicit unseen words generation, and inventing better dialogue history access mechanism to accommodate efficient inter-turn reasoning.",1,2019
D19-1199,"in the future, we will further investigate better utterance models associated with additional information such as topics or knowledge.",1,2019
D19-1199,"with the help of learned addressee structure, we can build a general structural dialogue system for complex multiparty conversations.",1,2019
D19-1200,"however, the error analysis shows that there are still challenges in dialogue generation, which we would like to explore in the future.",5,2019
D19-1203,a potential direction for future work is to study how different game objectives interact with each other.,5,2019
D19-1203,"future work should evaluate if our training scheme generalizes to a fully open-ended recommendation system, thus making our task not only useful for research and model development, but a useful end-product in itself.",3,2019
D19-1204,its language and discourse diversity and crossdomain setting raise exciting open problems for future research.,5,2019
D19-1204,"future work as discussed in section 5, some examples in cosql include ambiguous and unanswerable user questions and we do not study how a system can effectively clarify those questions or guide the user to ask questions that are answerable.",5,2019
D19-1204,"we urge the community to investigate these problems in future work in order to build practical, robust and reliable conversational natural language interfaces to databases.",6,2019
D19-1204,"its language and discourse diversity and cross domain setting raise exciting open problems for future research. future work as discussed in section 5, some examples in cosql include ambiguous and unanswerable user questions and we do not study how a system can effectively clarify those questions or guide the user to ask questions that are answerable. we urge the community to investigate these problems in future work in order to build practical, robust and reliable conversational natural language interfaces to databases.",5,2019
D19-1206,"in the future we plan to include more domain using various domain-adaptive methods (qian and yu, 2019; tran and nguyen, 2018; gasi ˇ c et al.´ , 2017) to support multi-domain dialog system research, and incorporate our work into more and more standardized dialog system platforms (lee et al., 2019).",1,2019
D19-1210,"future work could incorporate better models of sequence within document context (kim et al., 2015; alikhani and stone, 2018).",1,2019
D19-1213,"furthermore, the proposed asgn has a good flexibility which can be employed to the other language and vision fields, such as image captioning, visual question answering, and so on.",4,2019
D19-1215,"in future versions, talk2car will be expanded to include the above annotations and dialogues.",2,2019
D19-1216,"in future work, we plan to extend the dataset with more examples, to try other features, e.g., from social media and from metadata,15 and to adapt the system to work with other languages.",2,2019
D19-1216,we further plan experiments with fact-checking claims about videos.,3,2019
D19-1218,an important direction for future work is to remove our full observability assumption.,6,2019
D19-1218,"other future directions include experimenting with using the interaction history, expanding the learning example aggregation to error cases beyond incorrect start positions, and making agent reasoning interpretable to reduce user frustration",1,2019
D19-1220,"in our future work, we plan to extend the metric to other visualtext generation tasks such as storytelling.",4,2019
D19-1220,the improvements with grounding models focusing on the latent correspondence between object-level image regions and descriptions will allow tiger to be further improved in the future.,1,2019
D19-1221,"to enhance both the interpretability, as well as the attack stealthiness, future research can find grammatical triggers that work anywhere in the input. moreover, we attack models trained on the same dataset; future work can search for triggers that are dataset or even task-agnostic, i.e., they cause errors for seemingly unrelated models. in future work, we aim to both attribute and defend against errors caused by adversarial triggers.",1,2019
D19-1223,further speedups are possible by leveraging more parallelism in the bisection algorithm for computing α-entmax.,1,2019
D19-1225,"in future work, it may be worth extending this model to learn a latent manifold on content as well as style, which could allow for reconstruction of previously unseen character types, or generalization to other domains where the notion of content is higher dimensional.",1,2019
D19-1226,"furthermore, our method is amenable be extended to contextualized embeddings (lauscher et al., 2019) and/or other wordnet lexical relations such as hypernyms and hyponyms.",1,2019
D19-1227,"our results support the hypothesis on the interaction of metaphor and emotion, and suggest that it may be beneficial to incorporate a model of metaphor into emotion- and sentiment-related nlp applications in the future.",1,2019
D19-1229,future work will investigate generation and validation of unseen color terms.,6,2019
D19-1229,"finally, given that the divide between basic and secondary color terms is so blurred, future computational models of these should employ models of graded membership, such as fuzzy set theory.",1,2019
D19-1230,"in the future, we intend to review the annotated guidelines for negative focus with the problematic annotation cases mentioned in this paper, and explore the ways to combine additional patterns and constraints from negative focus definition with neural network techniques to further improve negative focus detection.",1,2019
D19-1234,"in future work, we will enrich our weak supervision system by giving the lfs access to more sophisticated contexts that take into account global structuring constraints in order to see how they compare to the simple, exogenous decoding constraints like mst.",3,2019
D19-1234,we think we can put much more sophisticated decoding constraints that don’t just work off local probabilities of arcs but on the full information about the arc in its discourse context.,1,2019
D19-1236,"in future experiments, more attention to the contribution of each author of the article might lead to further improvements in the prediction performance.",1,2019
D19-1239,"the next step would be to interpret the semantics of the extracted slang adrs, and linking them to medical ontologies to allow for further structured analysis.",1,2019
D19-1240,it would be also interesting to adopt rlta for other types of data.,2,2019
D19-1240,one future direction is to generate privacy preserving text rather than embeddings.,5,2019
D19-1240,a future direction is to apply different rl algorithms and investigate how it impacts results.,1,2019
D19-1240,we also adopt deep q-learning to train the agent.,1,2019
D19-1243,the substantial headroom (25.6%) between the best model performance and human encourages future research on contextual commonsense reasoning.,1,2019
D19-1246,"other future directions include reducing the increased parameters in the proposed block circulant matrices, such as by using multiplicative l1 regularization for complex (manabe et al., 2018).",1,2019
D19-1247,"for future work, we investigate error cases by analyzing the error distributions of 100 examples.it implies that it is still intractable to evaluate generated questions.",3,2019
D19-1247,"although we additionally evaluate on predicate identification and answer coverage, these metrics may be coarse and deserve further study.",3,2019
D19-1248,"in the future, we will test our proposed framework on more datasets and investigate potential approaches to handle spurious logical forms for weakly-supervised kb-qa.",2,2019
D19-1249,"first, we would like to create more parallel triples by adding more novels to make instances more balanced between the two languages.",2,2019
D19-1249,"second, we want to create questions with non-extractive answers.",5,2019
D19-1249,"third, we are also interested in adding multi-passage questions or questions based on the entire novels to bipar.",5,2019
D19-1250,"moreover, assessing multi-token answers remains an open challenge for our evaluation setup.",3,2019
D19-1250,"in addition to testing future pretrained language models using the lama probe, we are interested in quantifying the variance of recalling factual knowledge with respect to varying natural language templates.",1,2019
D19-1250,"this suggests that while relation extraction performance might be difficult to improve with more data, language models trained on ever growing corpora might become a viable alternative to traditional knowledge bases extracted from text in the future.",1,2019
D19-1251,how to incorporate more sophisticated symbolic reasoning abilities into mrc systems is also a valuable future direction.,5,2019
D19-1251,"in the future, we will explore the following directions: (1)as we use a pre-defined reasoning graph in our model, it is incapable of handling reasoning process which involves intermediate numbers that not presented in the graph.",1,2019
D19-1251,"therefore, how to combine both of their abilities to develop a more powerful numerical mrc model is an interesting future direction.(3) symbolic reasoning plays a crucial role in human reading comprehension.",1,2019
D19-1251,"how to incorporate dynamic graph into our model is an interesting problem.(2) compared with methods proposed for arithmetic word problems (awps), our model has better natural language understanding ability.",1,2019
D19-1251,"our work integrates numerical reasoning, which is a special case of symbolic reasoning, into traditional mrc systems.",1,2019
D19-1255,"in future work, we will explore even tighter integration of symbolic knowledge and stronger reasoning methods.",1,2019
D19-1257,"a promising future direction would be to include additional external knowledge such as commonsense and world knowledge, and learn all annotations jointly with the downstream task.",2,2019
D19-1258,the work can give general guidelines on mrs modeling and inspire future research on the relationship between semantic retrieval and downstream comprehension in a joint setting.,1,2019
D19-1259,learning a harder but more informative auxiliary task of long answer generation might lead to further improvements.,1,2019
D19-1259,(2) we use binary bow statistics prediction as a simple demonstration for additional supervision of long answers.,1,2019
D19-1259,"there are several interesting future directions to explore on pubmedqa, e.g.: (1) about 21% of pubmed qa contexts contain no natural language descriptions of numbers, so how to properly handle these numbers is worth studying;",5,2019
D19-1263,"in future work, we plan to add support for other complex questions whose queries require union, group by, or numerical comparison.",5,2019
D19-1263,"also, we are interested in mining natural language expressions for each query substructures, which may help current parsing approaches.",1,2019
D19-1264,"we would also like to research how to utilize gat, msr and mrr into other kg related tasks, such as kg representation, relation clustering and kb-qa.",4,2019
D19-1264,"in the future, we are interested in utilizing multi-task learning, to enable the model to learn reasoning paths for several query relations simultaneously.",1,2019
D19-1265,"in the future, we will try to investigate how to update kgs when entities are involved in several successive events.",5,2019
D19-1267,"in the future, we will further investigate the effect of the network depth on sentence matching and explore introducing external knowledge, such as pre-trained language model bert (devlin et al., 2018) and paraphrase database (ganitkevitch et al., 2013), to help learning more accurate and robust sentence representation.",1,2019
D19-1268,"(2) we will explore other techniques to fuse the ordered relation information from different paths (liu et al., 2019).",1,2019
D19-1268,"in the future, we will explore the following research directions: (1) we will study the applications of the proposed models in various domains, like personalized recommendation (liu et al., 2018);",4,2019
D19-1272,"lastly, our human evaluation is limited in size and a larger and more diverse participant pool is needed.",3,2019
D19-1272,"potential directions for future work include adding specific methods to control sentiment, and fine-tuning smerti for preservation of persona or personality.",1,2019
D19-1272,"experimenting with other text infilling models (e.g. fine-tuning bert (devlin et al., 2019)) is also an area of exploration.",1,2019
D19-1273,"in addition, we plan to create larger human-curated datasets with variable size scenarios.",2,2019
D19-1273,"our current model sometimes gets misled by superficially similar sentences, and it will be an important future direction to move towards deeper reasoning for the task.",1,2019
D19-1274,"besides, iteratively discovering new entity alignments based on the framework of kecg is another interesting direction.",1,2019
D19-1274,"for future work, we will extend the knowledge embedding model of kecg to other kg representation learning methods, such as transd (ji et al., 2015), to gain a stronger ability of modeling relationships.",1,2019
D19-1276,"one possibility that does not require supervised data is to create artificial tasks, such as reproducing the input sentence or predicting missing parts of the input (such as affixes and function words).",2,2019
D19-1277,"in a broader perspective, we hope that future studies on dependency parsing will take the results obtained here into account and extend them by investigating other parsing approaches and neural network architectures.",1,2019
D19-1277,"indeed, given the rapid development of new representations and architectures, future work should include analyses of how all components in neural parsing architectures (embeddings, encoders, decoders) contribute to distinct error profiles (or lack thereof).",1,2019
D19-1278,"we also plan to explore modelling alternatives, such as taking different graph generation orders into account (bottom-up vs. top-down) as well as predicting the components of a fragment (type, number of edges, edge labels) separately.",1,2019
D19-1278,"future work should focus on extending our parser to other formalisms (amr, mrs, etc.).",1,2019
D19-1280,"future work: based on our present efforts to tackle qa it, we propose the following directions for future work. sometimes attributes require long procedures to verify, and thus, we believe that denser rewards would help with this problem.",5,2019
D19-1280,one possible solution is to provide intermediate rewards whenever the agent achieves a sub-task.,1,2019
D19-1281,"this work opens up the possibility of focusing on other kinds of knowledge gaps and extending this approach to other datasets and tasks (e.g., span prediction).",4,2019
D19-1282,"future directions include better question parsing methods to deal with negation and comparative question answering, as well as incorporating knowledge to visual reasoning.",1,2019
D19-1285,"in parallel, the present work could be extended to other gas and threshold functions.",4,2019
D19-1285,"in the long term, we aim to move to natural images.",5,2019
D19-1285,"moreover, it might be worth exploring whether equipping models with similar inductive biases as those leading speakers of any language to develop abstract, compositional representations of size adjectives is needed to properly handle these expressions.",1,2019
D19-1285,"an interesting open question, which we plan to explore in future work, is whether training models to jointly learn superlative, comparative, and positive gas (similarly to how pezzelle et al.(2018) did for quantities), or framing the task in a dialogue setting (as monroe et al.(2017) did for colors) could lead to more compositional models.",1,2019
D19-1287,it remains an open question why all but one of the models tested were unable to leverage the numerous examples of gender agreement seen in various contexts during training to drive correct subject/predicate expectations.,5,2019
D19-1289,"third, in addition to understanding what goes into an explanation, we need to understand what makes an explanation effective.",6,2019
D19-1289,"second, it is important to connect the words in explanations that we investigate here to the structure of explanations in psychology (lombrozo, 2006).",1,2019
D19-1289,"a better understanding of explanations not only helps develop explainable ai, but also informs the process of collecting explanations that machine learning systems learn from (hancock et al., 2018; rajani et al., 2019; camburu et al., 2018).",1,2019
D19-1289,"first, although /r/changemyview has the useful property that its explanations are closely connected to its explananda, it is important to further investigate the extent to which our findings generalize beyond /r/changemyview and reddit and establish universal properties of explanations.",1,2019
D19-1290,future work on framing will focus on its application to down-stream argument mining tasks such as analyzing argument quality.,4,2019
D19-1290,especially interesting is whether specific frames are expected to persuade an audience.,5,2019
D19-1290,a clear problem here is how to label a frame given its arguments in order to deliver short labels for the user.,5,2019
D19-1290,a follow-up question will be whether frames in an argumentative text should be delivered in a specific sequence to achieve the persuasion of an audience.,5,2019
D19-1291,"in the future, we plan to experiment further with language model fine-tuning on other sources of data.",2,2019
D19-1291,"as the rst parser is not perfect, we want to incorporate other features from these trees that allow us to better recover from errors.",1,2019
D19-1291,we also plan to investigate whether this approach can be extended to comparing programs: in this casenon-alpha numeric symbols would be candidates for fingerprinting.,1,2019
D19-1292,we hope that the findings and metrics that we present in this paper help continue the discussion on improving the robustness of models and enable better comparisons between adversarial attacks to be made that consider instance correctness.,1,2019
D19-1293,"therefore, we leave it as future work to develop a method that is effective even when workers are aware of quality control methods that employ nonsensical reasons, possibly by making the nonsensical reasons change automatically between annotations.",1,2019
D19-1294,we would also like to extend the work to other discourse phenomena.,4,2019
D19-1294,"in future work, we want to handle cases where multiple pronouns are equally suitable in a given context.",6,2019
D19-1295,"in the future, we will identify new types of commonsense knowledge for further improving the performance of discourse parsing.",1,2019
D19-1295,"for example, antonyms (e.g., warm vs. cold) can directly indicate a contrast relation between two situations, and this type of knowledge has potential to further improve the performance on comparison discourse relations.",1,2019
D19-1298,"as for evaluation, we would like to elicit human judgments, for instance by inviting authors to rate the outputs from different systems, when applied to their own papers.",3,2019
D19-1298,"more long term, we will study how extractive/abstractive techniques can be integrated; for instance, the output of an extractive system could be fed into an abstractive one, training the two jointly.",5,2019
D19-1298,"furthermore, we would like to explore more sophistical structure of documents, like discourse tree, instead of rough topic segments.",6,2019
D19-1298,"more generally, we plan to combine of traditional and neural models, as suggested by our results.",1,2019
D19-1298,"for future work, we initially intend to investigate neural methods to deal with redundancy.",1,2019
D19-1298,"then, it could be beneficial to integrate explicit features, like sentence position and salience, into our neural approach.",1,2019
D19-1299,"besides, we are also interested in integrating external knowledge into other types of datasets beyond wikipedia infobox-to-text datasets.",2,2019
D19-1299,it will be worthwhile especially when we extend the task to multiple sentence generation.,4,2019
D19-1299,the main challenge in integrating multi-hops knowledge graph is the large search space.,5,2019
D19-1299,"in the future, we plan to investigate integrating multi-hops knowledge graph behind the data which has potential to further improve the inference ability of neural models.",1,2019
D19-1299,we plan to employ reinforcement learning based techniques to allow the model to search the optimal inference paths by trial and error.,1,2019
D19-1303,"future work can be improving the sensationalism scorer and investigating the applications of dynamic balancing methods between rl and mle in textgan(yu , 2017).",4,2019
D19-1303,"our work also raises the ethical questions about generating sensational headlines, which can be further explored.",5,2019
D19-1305,"in future work, we plan to explore the development of a joint model that simultaneously handles morphological realization and word ordering while using finer grained word representations, such as fasttext embeddings (bojanowski et al., 2017) or byte pair encoding (bpe; gage, 1994; sennrich et al., 2016).",1,2019
D19-1307,"for future work, we plan to test our method in other summarisation tasks (e.g.multi-document summarisation) and downstream tasks of summarisation (e.g.investigating whether users can correctly answer questions by reading our summaries instead of the original documents).",4,2019
D19-1307,"also, we believe the “learning reward from human judgements” idea has potential to boost the performance of rl in other natural language generation applications, e.g. translation, sentence simplification and dialogue generation.",4,2019
D19-1308,future work involves incorporating selector for other generation tasks such as diverse image captioning.,4,2019
D19-1309,"in future work, we intend to accelerate the training of our encoders and decoders with the techniques in (huo , 2018) and apply our architecture and training techniques to other nlp tasks.",4,2019
D19-1312,one future direction is to model larger contexts with this approach.,1,2019
D19-1312,"it would also be interesting to integrate our model into an existing language generation system (e.g., as part of an abstractive summarization system).",1,2019
D19-1313,"in the future, we would like to apply our framework to increase reference texts for automatic evaluation or augment training data for text classification.",3,2019
D19-1314,"in the future, we will consider integrating deep generative graph models to express probabilistic dependencies among amr nodes and edges.",1,2019
D19-1315,"as a future work, we would like to explore whether our method is applicable to other tasks such as multi-task learning for object detection and image caption generation.",4,2019
D19-1316,we will also investigate how we can efficiently evaluate generation results.,3,2019
D19-1316,"in future work, we will investigate how we can generate more flexible feedback comments.",5,2019
D19-1316,"one way of achieving it is to apply a more sophisticated retrieval-based method such as (hashimoto et al., 2018) to this task.",1,2019
D19-1317,"for future work, there are some interesting dimensions to explore, such as difficulty levels (gao , 2019a), paragraph-level information (zhao , 2018) and conversational question generation (gao , 2019c).",5,2019
D19-1317,"for future work, there are some interesting dimensions to explore, such as difficulty levels (gao et al., 2019a), paragraph-level information (zhao et al., 2018) and conversational question generation (gao et al., 2019c).",5,2019
D19-1319,"another direction of the future research, however, lies in developing novel methods that distinguish the type of reviews described in the paper and organic reviews.",5,2019
D19-1319,"as a part of the future work, we would like to improve the review generation process in a way that could receive several key words from users as input and generate reviews based on these prior information.",6,2019
D19-1321,our planning-based model may be inspiring to other long text generation tasks such as long text machine translation and story generation.,4,2019
D19-1326,"as a future work, we would like to extend refnet to have the ability to decide whether a refinement is needed on the generated initial draft.",1,2019
D19-1327,different domains of datasets except for news articles pose new challenges to the appropriate design of summarization systems.,2,2019
D19-1327,similar studies can be applied to other aspects and their combinations in various systems and different domains of corpora.• one can repeat our bias study on evaluation metrics.,3,2019
D19-1327,"a more theoretical study of summarization, and the various aspects, is required.",6,2019
D19-1327,developing such bias-free or robust models can be very important for future directions.• nobody has clearly defined the deeper nature of meaning abstraction yet.,1,2019
D19-1330,"for the future work, we plan to extend our method on more than two target languages and explore other effective interactive approaches to improve the translation quality further.",2,2019
D19-1332,we hope that this study will inspire further research on temporal commonsense.,6,2019
D19-1332,our analysis sheds light on the capabilities as well as limitations of current models.,1,2019
D19-1332,"we find that systems equipped with state-of-the-art language models such as elmo and bert are still far behind humans, thus motivating future research in this area.",1,2019
D19-1333,"in the future, we will investigate more methods for reducing the limitations of qa infomax and improving the capability of generalization in qa systems.",1,2019
D19-1335,"with the release of an increasing number of finegrained inference tasks aimed at these abilities (roemmele , 2011; morgenstern , 2016; wang , 2018; rashkin , 2018; mccann , 2018), the issue of experimental validity in csr will also become even more important.",5,2019
D19-1335,"with the release of an increasing number of finegrained inference tasks aimed at these abilities (roemmele et al., 2011; morgenstern et al., 2016; wang et al., 2018; rashkin et al., 2018; mccann et al., 2018), the issue of experimental validity in csr will also become even more important.",5,2019
D19-1336,"pun-gan is generic and flexible, and may be extended to other constrained text generation tasks in future work.",4,2019
D19-1337,"in future work, we will adopt the auxiliary language modeling task to other neural generation systems to test its generalization ability.",4,2019
D19-1339,"in this paper, we use manually selected keywords and phrases to generate text, which, while an appropriate scope to quantify the biases that appear in nlg systems, could be expanded to more automatic methods and help generalize our findings.",1,2019
D19-1340,"since our approach is implemented as a data pre-processing step, it can potentially help any neural method that learns from text and is likely to be used out of domain.",4,2019
D19-1341,"moving forward, we suggest using our symmetric dataset in addition to the current retrieval-based fever evaluation pipeline.",2,2019
D19-1343,future work will conduct more detailed analysis to continue enhancing the proposed method.,1,2019
D19-1344,"in the future, we will go deeper into the taxonomy and try to explore the hierarchical relations between labels and improve the scalability of models for finer grained label sets.",1,2019
D19-1346,"in the future, we plan using the same approach to tackle similar problems, including lexical selection in machine translation, or word sense disambiguation (lala and specia, 2018).",4,2019
D19-1346,"tasks that lie at the intersection of computer vision and nlp, such as the challenges posed in the new breakingnews dataset (popularity prediction, automatic text illustration) could also benefit from our results (ramisa , 2018).",4,2019
D19-1346,"we believe our approach could also be useful in multimodal machine translation, where an image caption must be translated using not only the text but also the image content (barrault , 2018).",4,2019
D19-1348,"performance improvements are expected as these features are incorporated, and as additional labeled data is created.",2,2019
D19-1348,"visual region detection can serve as a precursor to numerous existing information extraction techniques or adaptations of them, including parsing of reference (lammey, 2015), tables (rastan , 2015), and equations (smithies , 2001), as well as data extraction from figures (tummers, 2006).",4,2019
D19-1348,and so it is valuable to all these efforts to be able to accurately and quickly segment document pages into regions of interest.,5,2019
D19-1348,alternative detection frameworks are also being explored.,1,2019
D19-1348,"in ongoing work, additional contextual features are being explored, including roi oversampling (i.e. looking at a region’s surroundings), random feature map sampling (i.e. looking at patterns in a whole page or whole article), and all-region positional information (i.e. the position of other predicted rois in an article).",1,2019
D19-1349,"for future work, we intend to work with larger datasets to investigate neural solutions to combine features from different metrics, as well as to apply our findings to other variants of lda models trained on low-resource languages, where high-quality external corpora are usually not available (hao , 2018).",2,2019
D19-1350,"in future work, we will explore extending our model for temporal topic modelling.",1,2019
D19-1351,"although our method has been explored in the context of systematic reviews, it could be applied to a variety of other document retrieval problems.",4,2019
D19-1352,"important future work includes more detailed analyses of these transfer effects, as well as a closer look at the contributions of document-level and sentence-level scores.",3,2019
D19-1353,adapting neural models for use in an mt-ir setting and addressing the properties of mt output most harmful to their performance are promising avenues for future work to enable low-resource clir.,1,2019
D19-1356,"for future work, we will further improve the quality of phrasing knowledge or incorporate other concept-level knowledge in text-to-sql.",2,2019
D19-1357,"in our future work, we will extend our methods for phrase-level definition generation (ishiwatari et al., 2019).",1,2019
D19-1358,"thus, it would be an attractive direction to take as future work developing an end to-end system.",1,2019
D19-1359,iii) establish a common evaluation framework to compare the contribution of lexical-semantic combination resources;,3,2019
D19-1359,"iv) employ and assess syntagnet in other nlp tasks, such as word and sense similarity (navigli and martelli, 2019) or semantic role labeling (where a newly released wordnet-linked resource, verbatlas (di fabio , 2019), would greatly benefit from collocational information).",4,2019
D19-1359,"as future work, we plan to: i) enrich syntagnet with more combinations, so as to surpass supervised english wsd;",1,2019
D19-1359,ii) include information from adjectives;,1,2019
D19-1362,"further, the feature-dependent confusion matrices are taskindependent and could be used for other nlp tasks, which is one possible direction of future research.",4,2019
D19-1363,a possible future research direction would involve finding efficient optimization methods where the orthogonality constraint could be slightly relaxed.,1,2019
D19-1366,"also, designing proper evaluation metrics is still an open problem for text style transfer.",3,2019
D19-1366,"although our model achieves better content preservation, the general quality of our transferred sentences can be further improved.",1,2019
D19-1367,we plan to consider the network density problem in search and apply i-darts to more tasks in our future study.,4,2019
D19-1368,"applying jobi to non-bilinear models is also possible, but left for future work.",1,2019
D19-1371,"because these language models are costly to train, we aim to build a single resource that’s useful across multiple domains.",2,2019
D19-1371,"for future work, we will release a version of scibert analogous to bert-large, as well as experiment with different proportions of papers from each domain.",2,2019
D19-1375,we believe our method can also be applied to slu systems with other semantic representations (e.g. semantic frame).,1,2019
D19-1377,"our dataset can serve as a starting point for further research on this task, which can be beneficial to the investigation of chinese qa and dialogue models.",1,2019
D19-1379,another line of our future work is to apply these transductive methods to various nlp tasks and investigate their performance.,4,2019
D19-1379,one interesting line of future work is to explore effective transductive methods for task-dependent (neural) layers.,4,2019
D19-1379,"for instance, as some unsupervised domain adaptation methods can be applied to transudative learning, integrating them with transudative lm fine-tuning may further improve their performance.",1,2019
D19-1381,"in addition, it can also be applied to other dag structures such as nested/discontiguous entities (muis and lu, 2016; ju , 2018).",4,2019
D19-1381,these results set the first focussed benchmark of our model and next steps include applying it to other event datasets in the biomedical and general domain.,5,2019
D19-1381,further analyses on the development set revealed some desirable characteristics of the model such as its computational efficiency while yielding higher f1-score performance.,1,2019
D19-1382,it also leaves considerable headroom as a new challenging benchmark to drive multilingual research on the problem of paraphrase identification.,2,2019
D19-1383,"for future work, we would like to explore methods for better encoding long sequences using pretrained language models.",1,2019
D19-1384,"the setting in this paper included a very simple vision task, and we suspect emergent linguistic phenomena would be more pronounced and even more interesting to study in more sophisticated settings.",5,2019
D19-1384,"in future work, it would be useful to investigate more complicated environments and more complex agent interactions.",1,2019
D19-1386,"our experimental results demonstrate that the topic and partial summary help the extractive and abstractive models, but the task remains a significant challenge with room for improvement in future work.",1,2019
D19-1387,"although we mainly focused on document encoding for summarization, in the future, we would like to take advantage the capabilities of bert for language generation.",1,2019
D19-1390,"by integrating off-the-shelf knowledge bases to clearly model the transition relation embedding, it should further improve the interpretability and might be especially helpful under low-resource settings, which we leave for future work.",1,2019
D19-1392,"in the future, we hope to explore its potential in crossframework and cross-lingual semantic parsing.",3,2019
D19-1393,it is interesting to explore the use of some revision mechanisms when the initial steps go wrong.,6,2019
D19-1395,"in the future, we will explore other useful information (e.g., correlations among the relations from kgs) available to improve the label embeddings.",2,2019
D19-1397,all of these findings support our argument that shifted label distribution can severely hinder model performance and should be handled properly in future research.,6,2019
D19-1397,we hope that the analysis presented will provide new insights into this long-overlooked factor and encourage future research of creating models robust to label distribution shift.,1,2019
D19-1397,"based on these observations, we suggest that in addition to label noise, more attention be paid to the shifted label distribution in distantly supervised relation extraction research.",1,2019
D19-1397,we also hope that methods such as threshold techniques and bias adjustment become useful tools in future research.,1,2019
D19-1399,"as statistics shows that most of the entities form subtrees under the dependency trees, future work includes building a model for joint ner and dependency parsing which regards each entity as a single unit in a dependency tree.",1,2019
D19-1400,"in future work, we would like to generalize our approach to the multilingual case of multiple languages and a multi-class target task, explore applications beyond text classification, and study transformations that eliminate the need for a translation system.",2,2019
D19-1405,"our framework is easily extensible to other domains with rich output structure, e.g., entity relation extraction, and multilabel classification.",4,2019
D19-1409,this may affect the design of objectives for the next generation of nlg models.,1,2019
D19-1412,"for future work, we would like to validate our model on other tasks such as response generation, explore more effective unsupervised sequence-tosequence pre-training methods, and handle crosslingual tasks such as machine translation.",4,2019
D19-1413,"in the future, we would like to abstract multi-view data from multi-lingual and multi-modal sources and investigate the effectiveness of av-kmeans on a wider range of tasks in the multi-lingual or multi-modal settings.",2,2019
D19-1415,"argumentation theory, applied to the active landmark semantics and the source input example as captured by the kernel, provides a very rich framework to design future and more complex justification mechanisms.",1,2019
D19-1416,"one research direction is to explore how to generate long-form text with deep generative models organized in multiple layers of latent variables, due to natural language characterizing a hierarchical structure.",1,2019
D19-1417,"to list a few: (a) a deeper look into the nature of sampled data - their distribution in the feature space, as well as their importance for the task at hand; (b) the creation of surrogate datasets for a variety of applications, including hyperparameter and architecture search, etc; (c) an extension to other deep models (beyond ftz) and beyond classification models; and, (d) an extension to semi-supervised, online and continual learning.",1,2019
D19-1418,"future work includes learning to automatically detect dataset bias, which would allow our method to be applicable with less specific prior knowledge.",4,2019
D19-1421,further research should investigate the potential gains of using γ-divergences for appropriate downstream tasks.,4,2019
D19-1423,"we hope that our work is a stepping stone towards models that are robust against an even wider, harder-to characterize space of possible attacks.",1,2019
D19-1424,"a further understanding of how multi-task training with bert (liu , 2019b) improves fine-tuning, and how it affects the geometry of loss surfaces are worthy of exploration, which we leave to future work.",5,2019
D19-1424,"in addition, we would like to apply the proposed methods for other pretrained models.",1,2019
D19-1424,"moreover, the results motivate us to develop fine-tuning algorithms that converge to wider and more flat optima, which would lead to better generalization on unseen data.",1,2019
D19-1425,"we plan to evaluate our proposed solution on other tasks where topics can be latent confounds, like predicting gender bias (voigt , 2018).",4,2019
D19-1426,"future work can also extend the approach to other supervised learning tasks, as well as bootstrap from natural dialog data.",4,2019
D19-1427,"our proposed evaluation framework and dataset will facilitate future work by providing the ability to meaningfully compare the performance of different methods to each other, an ability that was sorely missing in previous work.",3,2019
D19-1430,"for future work, we would like to verify our training strategy on more language pairs and other sequence-to-sequence tasks.",4,2019
D19-1430,"furthermore, we are interested in studying our noised training with other data augmentation approaches.",1,2019
D19-1431,we may consider obtaining more valuable information about sparse entities in few-shot link prediction in kgs in the future.,2,2019
D19-1433,"we are also interested to more thoroughly explore how to combine domain-adaptive and task-specific fine-tuning within the framework of continual learning (yogatama , 2019), with the goal of balancing between these apparently conflicting objectives.",4,2019
D19-1434,"with the methods presented here, an interesting direction for future work is to further examine why certain examples are more difficult than others.",5,2019
D19-1434,"having difficulty and ability estimates for machine learning data sets and models can lead to very interesting work around such areas as active learning, curriculum learning, and meta learning.",3,2019
D19-1435,in the future we plan to apply the pie model to more ambitious transduction tasks like translation.,4,2019
D19-1437,one potential direction for future work is to leverage iterative refinement techniques such as masked language models to further improve translation quality.,1,2019
D19-1437,"another exciting direction is to, theoretically and empirically, investigate the latent space in flowseq, hence providing deep insights of the model, even enhancing controllable text generation.",1,2019
D19-1438,"we hope this work opens a path to encourage machines learn quickly and generalize widely like humans do, and to make machines more helpful in various tasks.",4,2019
D19-1439,we hope that the community will make use of our released wikicrem dataset to further improve the pronoun resolution task.,2,2019
D19-1440,"future work will concentrate on extending the model to cope with relative attributes, the inclusion of additional data sources to increase model coverage, such as large-scale definition sets.",1,2019
D19-1441,we will also investigate patient-kd in more complex settings such as multi-task learning and meta learning.,4,2019
D19-1441,designing more sophisticated distance metrics for loss functions is another exploration direction.,1,2019
D19-1441,"for future work, we plan to pre-train bert from scratch to address the initialization mismatch issue, and potentially modify the proposed method such that it could also help during pre-training.",1,2019
D19-1443,we hope our empirical study may potentially allow others to design better attention mechanisms given their particular applications.,4,2019
D19-1445,"one of the directions for future research would be to study self-attention patterns in different languages, especially verb-final langauges and those with free word order.",2,2019
D19-1446,"besides, we will study using weakly paired documents from other data resources, such as news websites.",2,2019
D19-1446,"for future work, we will apply our method to more other language pairs.",2,2019
D19-1446,"furthermore, we will investigate better ways to utilize such weakly paired documents, going beyond mining sentence pairs and aligning topic distributions.",1,2019
D19-1449,"future work should move beyond the restrictive assumption by exploring new methods that can, e.g., 1) increase the isomorphism between monolingual spaces (zhang et al., 2019) by distinguishing between language-specific and language-pair invariant subspaces;",1,2019
D19-1449,2) learn effective non-linear or multiple local projections between monolingual spaces similar to the preliminary work of nakashole (2018);,1,2019
D19-1449,3) similar to vulic and korhonen ´ (2016) and lubin et al.(2019) “denoisify” seed lexicons during the self-learning procedure.,1,2019
D19-1449,"finally, this paper demonstrates that, in order to enable fair comparisons, future work on clwes should focus on evaluating the clwe methods’ constituent components (e.g, components c1-c3 from this work) instead of full-blown composite systems directly.",3,2019
D19-1450,extending our concept-based cross-lingual mapping to contextualized word representations will be the focus of our future work.,1,2019
D19-1451,"in the future, we would like to apply our methods to other multilingual datasets such as yago and babelnet.",2,2019
D19-1451,"also, since literal descriptions of entities are not always available, we will investigate alternative ways to design graph-based models that can better capture structured knowledge for this task.",1,2019
D19-1452,"in future, we will extend our language adaptation model to document retrieval and check-worthy claim detection tasks.",4,2019
D19-1453,"to train our model in a single run, we would like to investigate a training method which alternates between alignment extraction and model training.",1,2019
D19-1456,this notion of fairness allowed us to rigorously pose the question of whether specific nli models can learn to do robust natural logic reasoning.,5,2019
D19-1457,"in addition, given additional hand-labeled training data, a system might learn to directly predict dependency links (instead of deriving them heuristically from state changes).",5,2019
D19-1458,"we hope that by using this benchmark suite, progress can be made in building more compositional, modular, and robust nlu systems.",1,2019
D19-1460,the data collection and annotation methodology that we use to gather multidogo can efficiently scale across languages.,2,2019
D19-1460,several pilot experiments aimed at collecting spanish dialogues in the same domains have shown preliminary success in quality assessment.,3,2019
D19-1461,"future work could consider classes of offensive language separately (zampieri , 2019), or explore other dialogue tasks, e.g.from social media or forums.",4,2019
D19-1461,"another interesting direction is to explore how our build it, break it, fix it strategy would similarly apply to make neural generative models safe (henderson et al., 2018).",1,2019
D19-1462,we would also like to adapt the proposed methods to document-level neural machine translation in the future.,4,2019
D19-1462,we will further improve our model by incorporating syntactic and location information.,1,2019
D19-1464,"second, domain knowledge can be incorporated.",6,2019
D19-1464,we plan to design a specific graph neural network that takes into consideration the edge labels.,1,2019
D19-1464,"last but not least, the asgcn model may be extended to simultaneously judge sentiments of multiple aspects by capturing dependencies between the aspects words.",1,2019
D19-1466,"in the future, the proposed sal method can be potentially extended to other domain adaptation methods and applied to more general sequence labeling tasks including named entity recognition (zhou , 2019c), part-of-speech tagging (zhou , 2019a), etc.",4,2019
D19-1468,"in future work, we plan to extend our framework to multi-task settings, and to incorporate interaction to learn better seed words.",4,2019
D19-1469,"of course our new dataset and the baseline classifier models are just a preliminary effort, and future work will need to examine larger datasets, consider additional data such as hashtags, richer classification schemes, and more sophisticated classifiers.expanding our taxonomies with richer sets like these is an important goal.",2,2019
D19-1469,"nonetheless, the fact that we found multimodal classification to be most helpful in cases where the image and text diverged semiotically points out the importance of these complex relations, and our taxonomies, dataset, and tools should provide impetus for the community to further develop more complex models of this important relationship.",1,2019
D19-1471,there are several important directions remain for future research: (1) the fusion mechanism of private and shared features; (2) how to represent meta-data of fake news better to integrate into inputs.,1,2019
D19-1472,"future research should evaluate the appropriateness of the framework to probing moral change from a diverse range of cultures and linguistic backgrounds, and the extent to which moral sentiment change interacts and crisscrosses with linguistic meaning change and lexical coinage.",3,2019
D19-1473,"we will then be able to accurately identify inappropriate causal language use in weaker study design types, and also to compare among countries and languages in each specific study design types.",3,2019
D19-1473,"toward this new goal, we also plan to develop a classification method to identify study types in other domains such as psychology and education.",1,2019
D19-1473,"in the future we will further our investigation by developing prediction models to automatically identify study designs in more fine-grained categories, such as cross-sectional, case-control, retrospective cohort, and prospective cohort.",1,2019
D19-1473,We also plan to extend our work to other domains in the future.,4,2019
D19-1474,"the different annotation labels and comparable corpora would help us perform transfer learning and investigate how multimodal information on the tweets, additional unlabeled data, label transformation, and label information sharing may boost the classification performance in the future.",2,2019
D19-1477,"in future work, we plan to perform a deeper investigation of cases in which social information does not prove beneficial, and to assess the ability of our model to dynamically update the representation of the same author in different contexts, a task that, due to the nature of the data, was not possible in present work.",1,2019
D19-1480,we plan to incorporate temporal states to capture location changes in future work.,1,2019
D19-1480,"for future work, we would like to explore ways to combine graph-level classification methods and our user-level learning model.",1,2019
D19-1480,"besides, our model assumes each post of one user all comes from one single home location but ignores the dynamic user movement pattern like traveling.",1,2019
D19-1481,"it would be exciting to consider finer-grained forecasting of conversational trajectories, accounting for the natural—and sometimes chaotic—ebband-flow of human interactions.",5,2019
D19-1482,we intend to make our dataset freely available to facilitate further exploration of hate speech intervention and better models for generative intervention.,1,2019
D19-1484,our future plans include extending this dataset with conversational context and investigating additional aspects of online written multilingual discourse in richer contexts.,2,2019
D19-1484,"the released dataset suggests a wealth of both intra- and inter-sentential code-switched utterances, and investigation of sociolinguistic differences between the two introduces an interesting research question.",5,2019
D19-1485,"in future work, we shall explore to incorporate external context (derczynski , 2017; popat , 2018), and extend our model to multi-lingual scenarios (wen , 2018).",2,2019
D19-1485,"moreover, we shall investigate the diffusion process of rumors from social science perspective (vosoughi et al., 2018), draw deeper insights from there and try to incorporate them into the model design.",1,2019
D19-1485,"in future work, we shall explore to incorporate external context (derczynski et al., 2017; popat et al., 2018), and extend our model to multi-lingual scenarios (wen et al., 2018).",2,2019
D19-1486,we also plan to apply our model in dialogue systems for low-resource languages such as cantonese.,4,2019
D19-1486,one is to customize our model for few-shot intent classification.,1,2019
D19-1486,another is to extend our framework to deal with multiple-intent classification.,1,2019
D19-1487,"as future work, we will work along this line and investigate the design interpretable solutions to domain adaptation for person-job fit.",4,2019
D19-1487,we will also consider how to model the domain relationship for effectively transferring information across domains.,1,2019
D19-1491,the novel architecture performs simplification recursively and can be adapted to different levels of language complexity in the future;,1,2019
D19-1491,"in the future, we plan to extend this work with a syntactic simplification component and test the architecture extrinsically with non-native readers.",3,2019
D19-1492,in the future we would also like to explore using both i2b2 and clvc together to see if a multi-task learning approach would provide additional improvements.,1,2019
D19-1493,"the second one is how to incorporate the activities of users on social media platforms, since the opinions, experiences and events shared by users on social media are usually useful clues to indicate their interests.",5,2019
D19-1493,"the first one is how to incorporate the interactions between different kinds of user behaviors, since the relatedness of them may be useful for modeling the interest evolution of users.",1,2019
D19-1493,"the third one is how to incorporate language models to generate context-aware word embeddings, which may enhance the representation learning of contexts in news and other kinds of user generated content.",1,2019
D19-1494,"second, since there are also other kinds of user and item information, we will explore how to incorporate heterogeneous user and item information to benefit recommendation.",2,2019
D19-1494,"third, we will work on how to reduce the computational cost of our approach, which can improve the applicability of our approach.",1,2019
D19-1494,"first, since we only use the local neighbors of nodes in the user-item graph, we will explore the use of other kinds of graph neural networks to further enhance the learning of users and items.",1,2019
D19-1497,"as future work, we will consider how to extend our work by modeling sentiments of review text.",1,2019
D19-1498,we hope that this study will inspire the community to further investigate the usage of edgeoriented models on re and other related tasks.,4,2019
D19-1498,"as future work, we plan to improve the inference mechanism and potentially incorporate additional information in the document-graph structure.",1,2019
D19-1500,"we hope this contribution leads to multidisciplinary efforts to precisely understand user intent and reconcile it with information in policy documents, from both the privacy and nlp communities.",4,2019
D19-1501,"in the near future, we aim to fully prevent the generation of unfaithful descriptions and bring fpdg online.",6,2019
D19-1504,"we hope that our work, built upon the theoretical insights on spectral inference, provide a complete guide to correlated topic modeling for both researchers and practitioners.",1,2019
D19-1505,"we also hope to apply our modeling approach to practical downstream applications, including: i) detecting completed to-dos based on related edits; ii) localizing the paragraphs that could be edited to address a given comments; iii) summarizing document revisions.",4,2019
D19-1505,"in future work we plan to explore sequences of revisions through the lifecycle of a document from creation to completion, with the ultimate goal of modeling document evolution.",1,2019
D19-1506,"in the future, we want to extend this approach to more natural language tasks and languages.",4,2019
D19-1507,they both are also possible applications of the subword-level modeling.,4,2019
D19-1507,"query suggestion (sordoni et al., 2015; dehghani et al., 2017) and query reformulation (nogueira and cho, 2017) are related to qac and well-established problems.",4,2019
D19-1507,a student model can learn to match an estimation of query probability with that of a teacher model.,1,2019
D19-1507,"another interesting research direction is learning segmentation jointly with language model (kawakami et al., 2019; grave et al., 2019) rather than using fixed pretrained segmentation algorithms. proposed method can be extended to wide range of tasks.",1,2019
D19-1508,"in the future, we will build a larger symptom graph and use external medical information to further improve the performance of symptom diagnosis on dialogues.",1,2019
D19-1510,"in future work, we would like to experiment with more light-weight tagging architectures (andor et al., 2016) to better understand the trade-off between inference time and model accuracy.",1,2019
D19-1511,"furthermore, our work of utilizing rl and gan with answer information is general, and it can be easily extended to existing work.",4,2019
D19-1516,"further case studies also demonstrate that jointly using visual information and contextual information is an essential path for fully understanding human language, especially dialogues.",2,2019
D19-1517,we will make the dataset publicly accessible in order to support the research investigation in fine-grained semantic comprehension.,1,2019
D19-1518,"moving forward, we are going to narrow the gap between top-down and bottom-up models, and design a hybrid framework exploring both choices.",1,2019
D19-1519,one solution to overcome it is to apply meta learning.,1,2019
D19-1519,"in future, we plan to extend our framework into an iterative setting, similar to those boosting algorithms.",1,2019
D19-1519,"in this way, we can identify label mistakes more accurately and obtain a series of weighted models at the end.",1,2019
D19-1519,we can first train a meta model and only fine-tune on different training data on each fold.,1,2019
D19-1520,"in future, to account for different levels of annotator expertise, we plan to combine proactive learning (li et al., 2017) with our proposed method.",1,2019
D19-1521,"in the future, we plan to extend openkp with more annotated documents and connect it with downstream applications.",4,2019
D19-1522,future work might include exploring how to incorporate background knowledge on individual relation properties into the existing model.,1,2019
D19-1523,"lastly, we consider evaluating on other datasets and other advanced architectures beneficial future work as it may reveal further interesting qualities of the explanation methods.",3,2019
D19-1524,"for future work, we will try using pre-trained bert for scientific domain such as the scibert (beltagy et al., 2019) and explore using our frameworks to help more tasks such as the evaluation, prediction and knowledge graph construction for scientific resources.",1,2019
D19-1525,"we recommend future work to study the scope of adversarial reprogramming for other nlp applications such as machine translation, text to speech synthesis and text to image synthesis where the input space is discrete.",4,2019
D19-1525,"furthermore, due to the threat presented by adversarial reprogramming, we recommend future work to study defenses against such attacks.",5,2019
D19-1527,"in the future, we will try to move the state-of-the-art frontier further, not only for top-1 mips but also for top-n, n > 1, mips results.",4,2019
D19-1527,it would be interesting to adopt these measures in nlp tasks.,4,2019
D19-1527,"another promising direction is to adopt a gpu-based system for fast anns or mips, which has been shown highly effective for generic anns tasks (li , 2012; johnson , 2017; zhao , 2019).",4,2019
D19-1527,developing gpu-based algorithms for mips is still a topic which has not been fully explored.,1,2019
D19-1527,"besides of metric measures (e.g., ‘2distance and cosine similarity) and inner product, more complicated measures has been studied, for example (tan et al., 2019).",1,2019
D19-1530,future work could extend the names intervention to names from other languages beyond the usbased gazetteer used here.,2,2019
D19-1531,directions for future work include testing monolingual and bilingual word embeddings on downstream tasks like machine translation to measure bias and also test the performance for mitigation methods.,4,2019
D19-1531,"moreover, the number of noun classes for other languages with grammatical gender ranges from two to several tens (corbett, 1991) and future work can extend methods proposed in this paper to address grammatical gender in languages with more gender forms.",1,2019
D19-1534,"there are still many fruitful areas for future research, including discovering why numeracy naturally emerges in embeddings, and what other properties are similarly emergent.",5,2019
D19-1535,"for future work, we may extend our method to other natural language tasks.",4,2019
D19-1536,"in the future, we would like to investigate how such an approach can be applied to more complicated math word problems, like algebra word problems where a problem usually maps to an equation set.",4,2019
D19-1536,another interesting direction is to investigate how to incorporate world knowledge into the graph-based approach to boost the performance.,5,2019
D19-1539,"in particular, we had initial success sharing the parameters of the two towers which allows training much deeper models without increasing the parameter count.",1,2019
D19-1539,"In future work, we will investigate variations of our architecture.",1,2019
D19-1541,"moreover, the comparison of sentence performance indicates that there is still a lot of work to do to better integrate syntactic information and bert representation.",1,2019
D19-1542,"additionally, we will apply our model to improve the alignment quality of the phrase alignment model.",4,2019
D19-1542,"in the future, we plan to investigate the effects of our method on different sizes of bert models.",1,2019
D19-1543,"in addition, we also plan to extend our approach to the tasks with question-denotation pairs as supervision.",4,2019
D19-1543,"in the future, we plan to improve the performance of the anonymization model by exploring more efficient expected reward.",1,2019
D19-1544,"in future work, we would like to extend the approach to modeling interaction between multiple predicate-argument structures in a sentence as well as to other semantic formalisms (e.g., abstract meaning representations (banarescu , 2013)).",4,2019
D19-1547,we outline several future directions to further improve misp-sql and develop misp systems for other semantic parsing tasks: improving agent components.,1,2019
D19-1547,"one can also use learning-based approaches, such as a reinforced decision policy (yao et al., 2019), to increase the rate of identifying wrong and solvable predictions.",1,2019
D19-1547,"one can augment the probability-based error detector in misp-sql with probability calibration, which has been shown useful in aligning model confidence with its reliability (guo et al., 2017).",1,2019
D19-1547,"learning from user feedback is a promising solution for lifelong semantic parser improvement (iyer et al., 2017; padmakumar et al., 2017; labutov et al., 2018).",1,2019
D19-1547,"in the context of dialog systems, padma kumar et al.(2017) suggests that this effect can be mitigated by jointly updating the dialog policy and the semantic parser batchwisely.",1,2019
D19-1547,"to this end, one can improve misp from at least three aspects: (1) using more intelligent interaction designs (e.g., free-form text as user feedback) to speed up the hypothesis space searching globally, (2) strengthening the world model to nail down a smaller set of plausible hypotheses based on both the initial question and user feedback, and (3) training the agent to learn to improve the parsing accuracy while minimizing the number of required human interventions over time.",1,2019
D19-1547,Lifelong Learning for Semantic Parsing.,1,2019
D19-1548,"in future work, we would like to do semi-supervised learning and use silver data to test how much improvements could be further achieved.",1,2019
D19-1549,we would also like to combine such a graph-based model with a sequence-based model to avoid potential noise from dependency parsing errors.,1,2019
D19-1549,"since this work only uses the dependency graph and ignores various types of relations in the graph, we plan to incorporate dependency relation types into our model and take part-of-speech tagging into consideration as well in the future.",1,2019
D19-1549,future work could consider using an attention mechanism to focus on important words in the aspect.,1,2019
D19-1550,future work includes exploring approaches to capture explicit and implicit structural information to other sentiment analysis tasks and other structured prediction problems.,4,2019
D19-1551,"in the future, our theory can be generalized to other tasks that highly depends on the attention mechanism.",4,2019
D19-1551,"For example, reading comprehension and machine translating.",4,2019
D19-1554,"first, to address the problem of low-quality phrases in generated text, we will explore how to keep the syntactic correctness of the generated text.",5,2019
D19-1554,"second, we would like to figure out the detailed effects of the generated examples on the robustness of sentiment classification models.",1,2019
D19-1554,"by removing useless examples, we can obtain higher training speed.",1,2019
D19-1555,"one direction is to learn disentangled latent representations for attributes in neural network’s space, such as for disentangling aspects (jain et al., 2018), text style (john et al., 2019), and syntax and semantics (chen et al., 2019; bao et al., 2019).",1,2019
D19-1555,"another direction is to develop a content-based recommender based on trait, since it provides an effective unsupervised solution for generating profiles based on different attributes.",1,2019
D19-1556,future work will explore the use of structured output learning methods dedicated to the opinion structure.,1,2019
D19-1557,"due to size and memory complexities of available implementation of large scale language models such as bert, an easy integration with our proposed adaptation framework does not currently exist.",4,2019
D19-1557,"as future work, we will interface the adaptation layer for use recently introduced large scale language models such as bert.",1,2019
D19-1560,"furthermore, we would like to apply our hrl approach to other sentiment analysis tasks, such as aspect and opinion co-extraction, and dialog-level sentiment analysis.",4,2019
D19-1560,"in our future work, we would like to solve other challenges in dasc, e.g., negation detection problem, to further improve the performance.",4,2019
D19-1561,"while baselines are provided only for detecting gp-claims in spoken content, future work should aim to solve the problem as a whole - either by developing algorithms that determine relevance and stance of gp-claims to given motions, or by forgoing these stages, and successfully deciding whether a claim was mentioned in a speech, without first focusing on relevant claims.",1,2019
D19-1563,"in the future, we will exploit how to incorporate discourse parse tree or discourse relations into emotion cause analysis task to further improve the performance.",4,2019
D19-1564,"in future work we intend to further investigate this approach, as well as explore in more detail the low fraction of cases where these two schemes led to clearly different results.",1,2019
D19-1565,"in the mid-term, we plan to build an online platform where professors in relevant fields (e.g., journalism, mass communication) can train their students to recognize and annotate propaganda techniques.",6,2019
D19-1566,"In future, we would like to extend our work towards the multi-party dialogue.",2,2019
D19-1571,channel models are particularly effective with large context sizes and an interesting future direction is to iteratively refine the output while conditioning on previous contexts.,1,2019
D19-1573,"in the future, we will focus on designing new architectures and training methods for nart models to achieve comparable accuracy as art models.",1,2019
D19-1574,"ultimately, we would like to design models that can directly leverage typological compositionality for distant languages.",1,2019
D19-1575,"for future work, we are interested in unsupervised cross-lingual alignment, inspired by prior success on static embeddings (lample , 2018; alvarez-melis and jaakkola, 2018), which demands a deeper understanding to the geometry of the multilingual contextualized embedding space.",5,2019
D19-1576,"while in this work we follow previous work and perform unlexicalized parsing, the proposed model can be extended for lexicalized parsing by replacing pos tag embeddings with cross-lingual word embeddings, which we leave for future work.",1,2019
D19-1577,we leave the investigation of less frequent inanimate nouns for future work.,5,2019
D19-1578,"future work could employ our method to study various group biases such as nationality, caste, and religion, since person names may function as significant markers for many such demographic associations.",4,2019
D19-1578,"our method could also be easily extended to other kinds of nlp models (beyond classification) as well as other types of entities (locations, organizations etc.).",4,2019
D19-1579,"finally, we plan to continue widening the scope of our study – for example, expanding our methods to include non-binary gender identities, evaluating changes in gender norms over time, and spreading to more domains, such as the political sphere.",3,2019
D19-1579,"in future work, we hope to use our findings to improve performance on tasks such as abusive language detection.",4,2019
D19-1579,"we also hope to delve into finer-grained analyses, exploring how language around gender interacts with other variables, such as sexual orientation or profession (e.g. actresses versus female athletes).",1,2019
D19-1583,the next challenge is to incorporate this into actual noisy ie extraction pipelines.,1,2019
D19-1584,"in the future, we will further explore to leverage other kinds of inductive bias from human experience to improve extensive tasks with our modular networks .",4,2019
D19-1585,future work could extend the framework to other nlp tasks and explore other approaches to model higher-order interactions like those present in event extraction.,4,2019
D19-1586,"it would be interesting to see whether it has the same effect on implicit discourse relation classification task, we’d like to leave that in the future work.",5,2019
D19-1586,future work should explore the joint representation of discourse expectations through implicit representations that are learned during training and the inclusion of external knowledge.,1,2019
D19-1589,"a potential direction for future study is to directly couple them together and see whether one form contains the other, or vice versa.",3,2019
D19-1589,another direction is to check their effectiveness on top of the recent pre-trained language models.,1,2019
D19-1590,"as future work, we are examining the possibility of using an adversarial learning framework (goodfellow et al., 2014), which was recently used in the why-qa task (oh et al., 2019) for causality recognition.",1,2019
D19-1591,it would be meaningful to study how the neurons are trained to encode label-specific information and what exactly the information is.,5,2019
D19-1591,"it would be interesting to see what will happen if they are removed, and furthermore, how this can help model compression and hidden size selection.(3) transformers (vaswani et al., 2017; devlin et al., 2019) have been increasingly popular in nlp, and it would be important to extend our work to understanding these architecture.",1,2019
D19-1596,"as future work, we plan to look into improving our automatically generated consistent qa pairs using external knowledge-bases.",2,2019
D19-1597,"in future work, we will work towards more effective solutions to meet the challenges.",6,2019
D19-1598,"in future work, we aim at developing models to solve the newly proposed tasks.",1,2019
D19-1598,"therefore, solving the tasks we propose can only be considered a prerequisite for a fully functional theory of mind which will ultimately have to be evaluated in real-world scenarios.",3,2019
D19-1599,"in future, we plan to consider inter-correlation among passages for open-domain question answering (wang , 2018b; song , 2018).",5,2019
D19-1600,"we hope the release of this dataset could bring language diversity in machine reading comprehension task, and accelerate further investigation on solving the questions that need comprehensive reasoning over multiple clues.",5,2019
D19-1603,"to handle larger tables, we will investigate sharding the table row-wise, running the model on all the shards first, and then on the final table which combines all the answer rows.",1,2019
D19-1603,"as future work, we plan to expand our model with pre-trained language representations (e.g., bert (devlin et al., 2018)) in order to improve performance on initial queries and matching of queries to table entries.",1,2019
D19-1604,"as future work, we plan to further investigate the generality of the approach with other tasks and base models.",4,2019
D19-1605,"an interesting avenue for future work is to incorporate deeper context, either from other modalities (das et al., 2017) or from other dialog comprehension tasks (sun et al., 2019).",1,2019
D19-1605,a natural question is whether reinforcement learning could learn to retain the necessary context to rewrite questions in cqa.,5,2019
D19-1605,"this dataset helps start that conversation; the next steps are developing and evaluating models that efficiently decide when to ask for human assistance, and how to best use this assistance.",3,2019
D19-1609,"we plan to generalize this model to more complex and compositional answers, with better searching and pruning strategies of the derivations.",1,2019
D19-1610,"in addition, we plan to apply our self-attention memory network on other sentence matching tasks such as natural language inference, paraphrase identification, or measuring semantic relatedness.",4,2019
D19-1610,"in the future, we plan to investigate more transfer learning techniques for utilizing the large volume of existing cqa data.",1,2019
D19-1613,"our future work includes generating structured representations of recipes to handle ingredient properties, as well as accounting for references to collections of ingredients (e.g.dry mix).",6,2019
D19-1616,"as the next step, we plan to investigate: i) synthetic summarization data, and ii) applying transfer learning on text summarization for the multilingual low resource data set with little or no ground truth summaries (keneshloo , 2018).",2,2019
D19-1617,"furthermore, conducting a study just on the evaluation itself makes it possible for us to evaluate the adequacy of the used evaluation metric in evaluating computer generated poetry.",3,2019
D19-1618,a concrete estimation of performance in this setting will be part of future work.,3,2019
D19-1618,"it could thus be applied to other text generation tasks, such as natural language generation and sentence compression.",4,2019
D19-1618,"future work involves extending the sum-qe model to capture content-related aspects, either in combination with existing evaluation metrics (like pyramid and rouge) or, preferably, by identifying important information in the original text and modelling its preservation in the proposed summaries.",1,2019
D19-1618,"finally, sum-qe could serve to assess the quality of other types of texts, not only summaries.",3,2019
D19-1620,we will also carry out deeper study of the properties of dearly and dlate type documents and use them to inform new solutions.,6,2019
D19-1620,"while our auxiliary loss method achieves significant improvement, we note that there is a large gap which better methods can hope to bridge in the future.",1,2019
D19-1620,"one approach, building on ours, is to examine other ways to combine loss signals (finn et al., 2017), and to encourage exploration (haarnoja et al., 2018).",1,2019
D19-1628,future directions include trying our method on different tasks and different relevant data.,4,2019
D19-1630,"there is much room for improvement, and the cb dataset will be a useful testbed to assess models’ progress on such reasoning.",3,2019
D19-1631,"with the emergence of knowledge graphs in different domains, the proposed approach can be tried out in other domains as well for future exploration.",4,2019
D19-1633,"while there are still open problems, such as the need to condition on the targets length and the dependence on knowledge distillation, our results provide a significant step forward in nonautoregressive and parallel decoding approaches to machine translation.",5,2019
D19-1633,"in a broader sense, this paper shows that masked language models are useful not only for representing text, but also for generating text efficiently.",1,2019
D19-1636,"in the future, we would like to extend this framework for cross-lingual and cross-cultural sarcasm and irony generation.",1,2019
D19-1637,"in the future, we plan to explore: (1) applying the umt model in the tasks where the abstraction levels of source and target languages are different (e.g., unsupervised automatic summarization); (2) improving the quality of generated poems via better structure organization approaches.",4,2019
D19-1640,"in the future, we will investigate more sophisticated methods to improve ss’s performance, e.g., enable self-attention mechanism for contextualized information modelling.",1,2019
D19-1641,"we also plan to utilize fine-grained entity typing results in more downstream applications, such as coreference resolution and event extraction.",4,2019
D19-1641,"in the future, we will further improve the performance of fine-grained types, which is still lower than that of general types due to less training instances and distant supervision noise.",1,2019
D19-1642,"therefore, we should move along that direction to collect more high-quality data, which can facilitate more advanced learning algorithms in the future.",2,2019
D19-1644,"this issue is similar to exposure bias (wiseman and rush, 2016) and it might be beneficial if the classifier of segment merging is exposed to incorrect segments during training.",5,2019
D19-1649,"though achieving promising improvements, these commonly-used techniques are still not the satisfactory solutions for few-shot da and fewshot nota, which requires further explorations in these two real-world challenges.",5,2019
D19-1651,we expect that crowdsourced annotation will also be able to help the training of open ie systems as it has helped their evaluation – we leave the creation of a suitably large crowdsourced training set for open ie to future work.,3,2019
D19-1652,"various other formulations for this task are possible: for example, one might involve marginalizing over the tags and using this for predicting a label; another could perform attention over spans instead of tokens.",1,2019
D19-1653,the annotated corpus and neural models will be applicable for future persuasion reasoning or evaluation of persuasiveness.,1,2019
D19-1655,we believe that learning with noisy labels is a promising direction as it is often easy to collect noisy-labeled training data.,5,2019
D19-1656,interesting future directions for this work include: 1. incorporating common-sense knowledge into emotion analysis to capture semantic context and 2. using few-shot learning to bootstrap and improve performance of underrepresented emotions.,1,2019
D19-1656,"finally, as narrative passages often involve interactions between multiple emotions, one avenue for future datasets could be to focus on the multiemotion complexities of human language and their contextual interactions.",1,2019
D19-1657,"future work includes exploiting unsupervised learning to generate target-related lexicon and incorporating more labels (e.g., emotion classification) for multi-task learning.",2,2019
D19-1660,confirming whether our method works well for other datasets including ones that have only shorter label texts is left for our future work.,2,2019
D19-1661,"we believe could be addressed in future work by placing informative priors on multiple dimension to capture more complex concepts, and move away from simplistic antonym-driven dimension definitions.",5,2019
D19-1661,"finally, while being flexible and easily extended to other probabilistic word embedding models, our approach performs better or on par with sota.",1,2019
D19-1665,"in future work, we will explore how to integrate the textual descriptions of the labels in our our approach which has been shown to be beneficial in the case of large label sets (mullenbach , 2018).",2,2019
D19-1665,"in future work, we will explore how to integrate the textual descriptions of the labels in our  approach which has been shown to be beneficial in the case of large label sets (mullenbach et al., 2018).",2,2019
D19-1666,important future directions include examining the temporal aspect of bias as well as developing more precise mention identification techniques.,1,2019
D19-1668,"by open-sourcing pythia, and phi-ml’s processing pipeline, we hope to aid future research and inspire further interdisciplinary work.",4,2019
D19-1669,"it is interesting to further investigate a unified tensor embedding model to combine not only lexical, but also other features that are important for the sense of humor.",1,2019
D19-1672,our experiment on bengali shows that leveraging knowledge from wikipedia will be a promising direction for future research.,2,2019
D19-1674,more efficient use of gpus in performing the dfs procedure is a promising future work.,1,2019
D19-1676,"it is an open question on whether these results generalize to other language pairs.in general, more investigation is needed to assess how well the current generation of multi-lingual document representations supports cross-lingual transfer and if there are differences in how well the representations between languages align.",2,2019
D19-1678,"in the future, we expect to improve more using advanced models for the clinical notes since text summarizes expert knowledge about a patient’s condition.",1,2019
D19-1680,our next step is to annotate more documents and make them available to the research community to foster research in this area.,2,2019
D19-1680,we also plan to add location and time annotation on top of the intervention annotation.,2,2019
D19-1681,"we further intend to add additional signals, for instance coming from vision (c.chem. et al.(2018)), for more accurate localization.",1,2019
D19-1681,"in the future we plan to extend the world-state representation, and enable the model to ground generic and abstract concepts as well.",1,2019
N00-1001,"further work remains to make the parser really efficient, and much work remains to make the language coverage complete within reasonable limits.",1,2000
N00-1001,it is an open question whether the system of this kind will be a preferred way of offering information to the public.,5,2000
N00-1002,"although these languages are not so similar as czech and slovak, we hope that an addition of a simple partial noun phrase parsing might provide results with the quality comparable to the fullfledged syntactic analysis based system ruslan (this is of course true also for the czechoto-slovak translation).",1,2000
N00-1004,we hope to be able to demonstrate in the near future that a fine-tuned english-chinese translation model can provide query translations for clir with the same quality produced by mt systems.,3,2000
N00-1006,"in addition, we intend to apply other extra-linguistic information to a dialogue translation system.",1,2000
N00-1006,we are considering ways to enable identification of the agent of an action in an utterance and to expand the current framework to improve the level of politeness even more.,5,2000
N00-1010,"the other is that the system needs to be more communicative about its current understanding of the user's goals, even at points in the dialogue at which it might be assumed that user and system were in agreement.",1,2000
N00-1010,one is that the system needs to be better able to recognize and deal with problem situations in which the dialogue is not advancing.,5,2000
N00-1010,"while preliminary, these results point to two directions for future work.",6,2000
N00-1011,"in addition, the design of rees is highly portable for future addition of new relations and events.",1,2000
N00-1012,"however, our aim has been to highlight the problem as one worthy of further exploration within the field of nlp and to establish some baselines (human and algorithmic) against which further work may be compared.",5,2000
N00-1013,"on the other hand, it is computationally expensive to keep the complete previous context in the systems ""working memory"" to evaluate the few presuppositions which may refer back over a large number of questions.",3,2000
N00-1013,solving this problem will likely require comparing a variety of different settings.,3,2000
N00-1013,"a future challenge is to turn dp into a dip (detector of incorrect presuppositions), that is, to reduce the number of reported presuppositions to those likely to be incorrect.is a dip system feasible?",5,2000
N00-1013,"on the one hand, one could limit the considered context to, say, three questions and risk missing the critical question.",5,2000
N00-1015,"though in its infancy, we believe this approach holds vast potential for sls development.",1,2000
N00-1015,we are developing a system to exploit these characteristics to automatically generate javox grammars from an application's compiled code.,1,2000
N00-1015,the automatically-generated grammars are intended to serve as a starting point for developers - though they may certainly require some hand crafting.,1,2000
N00-1015,it is imaginable that additional data sources - such as a sample corpus - will allow us to more accurately generate grammars for an application.,2,2000
N00-1017,"ultimately, it is hoped that the use of a scheme of this type will permit much more widespread 'plug-and-play' among members of the nlg community.",4,2000
N00-1018,we are now considering the possibility of integrating transcheck in an off-the-shelf text editor to cross the ascii barrier.,1,2000
N00-1019,one obvious direction for future research is to revise our current strategy of decoupling the selection of units from their bilingual context.,1,2000
N00-1020,"currently, we are also considering the effects of a bootstrapping algorithm for multilingual coreference resolution.to be able to develop such a learning approach, we must first develop a method for automatic recognition of multilingual referential expressions.",1,2000
N00-1020,"we intend to analyze the performance of sidizzle when it is used as a module in an ie system, and separately in a question/answering system.",3,2000
N00-1022,a reorganization of the existing three-level category system into a semantically consistent tree structure would allow us to explore the nonterminal nodes of the tree for multi-layered sml.,1,2000
N00-1022,"where system-generated answers are acceptable to customers, a straightforward extension of ice-mail can provide this functionality.",1,2000
N00-1022,further task-specific heuristics aiming at general structural linguistic properties should be defined.,1,2000
N00-1022,this includes heuristics for the identification of multiple requests in a single e-mail that could be based on key words and key phrases as well as on the analysis of the document structure.,1,2000
N00-1022,"our initial experiments with the integration of germanet (hamp and feldweg, 1997), the evolving german version of wordnet, seem to confirm the positive results described for wordnet (de buenaga rodriguez et al., 1997) and will thus be extended.",3,2000
N00-1022,"in a further industrial project with german telekom, the icc-mail technology will be extended to process multi-lingual press releases.",4,2000
N00-1022,this places additional requirements on the knowledge engineering task and thus needs to be thoroughly investigated for pay-off.,5,2000
N00-1022,we intend to explore its use within an information broking assistant in document classification.,5,2000
N00-1022,"technically, we expect improvements from the following areas of future work.",6,2000
N00-1022,"for the application in hand, this was not the case.",6,2000
N00-1025,"the semantic type filter will be greatly improved by the addition of a named entity tagger, but we believe that additional gains can be attained by augmenting named entity identification with information from wordnet.",1,2000
N00-1025,"in addition, we believe that the retrieval and summarization components can be improved by incorporating automatic relevance feedback (buckley, 1995) and coreference resolution.",1,2000
N00-1025,we also plan to reconsider paragraph-based summaries given their coverage on the test corpus.,1,2000
N00-1025,incorporation of such a tagger will be a focus of future work.,1,2000
N00-1025,"the most critical area for improvement, however, is the linguistic filters.",1,2000
N00-1027,"consequently, the methodology is promising and the segmentation system would be helpful for the application system such as machine translation and information retrieval.",4,2000
N00-1028,"future work includes extending the study of corpus-based grammar specialization from firstorder grammar pruning to higher-order grammar pruning, thus extending previous work on explanation-based learning for parsing, aad applying it to the lfg fornaalism.",1,2000
N00-1029,"moreover, these algorithms can be used in any applications, which deal with grammars.",4,2000
N00-1031,"it is a very interesting future research topic to determine the advantages of either of these approaches, to find the reason for their high accuracies, and to find a good combination of both.",1,2000
N00-1033,the core idea is to call a deep parser only to the separated field elements which contain sequences of simple nps and pps (already determined by the shallow parser).,1,2000
N00-1033,"thus seen the shallow parser is used as an efficient preprocessor for dividing a sentence into syntactically valid smaller units, where the deep parser's task would be to identify the exact constituent structure only on demand.",1,2000
N00-1035,"the coverage of the grammar, in particular the treatment of genitive phrases, should also be further developed.",1,2000
N00-1035,"in addition, two aspects of the system that have only be touched on in this paper would be worth further attention: one is the mechanism for the treatment of split-ups and run-ons, which as mentioned earlier is not well-integrated at the moment; the other is the weight adjustment process, which is done manually at the moment, and for which the adoption of a semiautomatic tool could be considered.",1,2000
N00-1035,"therefore, more extensive testing and debugging should also be carried out.",3,2000
N00-1035,"in particular, future development should focus on treating stylistic matters such as capitalisation and punctuation which have not been in focus in the current prototype.",5,2000
N00-1039,"we are working on capturing the rich information about concept classes which is currently returned as part of our pattern discovery procedure, to build up a concept dictionary in tandem with the pattern base.",1,2000
N00-1039,"we are also considering the proper selection of weights and thresholds for controlling the rankings of patterns and documents, criteria for terminating the iteration process, and for dynamic adjustments of these weights.",1,2000
N00-1039,"we feel that the generalization technique in pattern discovery offers a great opportunity for combating sparseness of data, though this requires further research.",1,2000
N00-1039,"lastly, we are studying these algorithms under several unrelated scenarios to determine to what extent scenario-specific phenomena affect their performance.",1,2000
N00-1039,"we are at the early stages of understanding how to optimally tune these techniques, and there are number of areas that need refinement.",5,2000
N00-1041,"another area of future work is to extend the entity-extraction component of the system to handle arbitrary types (mountain ranges, films etc.).it is likely that as more features are added a trainable statistical or machine learning approach to the problem will become increasingly desirable.",1,2000
N00-1041,"this entails developing a training set of question-answer pairs, raising the question of how a relatively large corpus of questions can be gathered and annotated.",2,2000
N00-1043,"in the future, we would like to integrate our sentence reduction system with extraction-based summarization systems other than the one we have developed, improve the performance of the system further by introducing other sources of knowledge necessary for reduction, and explore other interesting applications of the reduction system.",1,2000
N00-1046,"future research should be searching for more such tasks, and developing more general toolkits that support rapid adaptation of multimodal technologies to support them.",1,2000
N00-2002,"our model is still unable to determine correctly how to re-package sentences into paragraphs; a better understanding of the notion of ""paragraph"" is required in order to improve this.",5,2000
N00-2003,"there are two main reasons for this: first, the proposed sortal class annotation scheme needs further work,  second, the relationship between sortal class and pronominalization may well be too intricate to be modelled by the factor class alone.",1,2000
N00-2003,"large corpora can be annotated relatively quickly with this information, which can then be used for statistical pronoun generation.",2,2000
N00-2004,"it would be interesting to corot)are c99 with the multi-source method described in (beeferman et al., 1999) using the tdt corpus.",1,2000
N00-2004,we would also like to develop a linear time and multi-source version of the algorithiil.,1,2000
N00-2004,"although our evaluation scheme is sufficient for this comparative study, further research requires a large scale, task independent benchmark.",3,2000
N00-2006,"the inclusion of this information in a large database which is mainly intended for computational applications can be very useful, mainly because it may help in resolving ambiguities and may be used to draw inferences of different nature.",2,2000
N00-2007,another approach would be to use different classification algorithms and combine the results.,1,2000
N00-2007,we are working on this but we are still to overcome the practical problems which prevent us from obtaining acceptable results with the other learning algorithms.,5,2000
N00-2012,we are currently working on the other kinds of weak verbs: defective and assimilated verbs.,1,2000
N00-2012,"other categories of words can be handled in a similar manner, and we will turn our attention to them next.",1,2000
N00-2013,"we would like to compare more taggers using still other methodologies, especially the mbt tagger, which achieved the best results on slovene but which was not available to us at the time of writing this paper.",1,2000
N00-2013,"obviously, we would also like to use the classifter combination method on them, to confirm the really surprisingly good results on romanian and test it on the other languages as well.",1,2000
N00-2013,we would also like to enrich the best taggers available today (such as the maximum entropy tagger) by using the dictionary information available and compare the results with the exponential featurebased tagger we have been using in the experiments here.,3,2000
N00-2014,"we are currently expanding the number of templates in our grammar in an attempt to obtain full coverage of the rm2 corpus using only templateexpanded rm sentences.we are also developing a stochastic version of cdg that uses a statistical arv, which is similar to a supertag (srinivas, 1996).",1,2000
N00-2016,a particularly promising research direction for this is to harness knowledge and training resources across languages.,2,2000
N00-2017,in future work we intend to study ways that determine the appropriate confusion set in a way to makes use of the current task properties.,5,2000
N00-2018,"ultimately it is this flexibility that let us try the various conditioning events, to move on to a markov grammar approach, and to try several markov grammars of different orders, without significant programming.",1,2000
N00-2018,"indeed, we initiated this line of work in an attempt to create a parser that would be flexible enough to allow modifications for parsing down to more semantic levels of detail.",1,2000
N00-2018,it is to this project that our future parsing work will be devoted.,6,2000
N00-2020,future work in this area involves modeling the corpora with other probability distributions.,1,2000
N00-2020,the method is very sensitive to the effectiveness of the probability model in modeling the normal elements.,1,2000
N00-2020,extensions to the probability distributions presented here such as adding information about endings of words or using more features could increase the accuracy of the probability distribution and the overall performance of the anomaly detection system.,1,2000
N00-2020,other future work involves applying this method to other marked corpora.,4,2000
N00-2021,"an obvious extension of this work, which we hope will be persued in the future, is to apply these techniques in broadcoverage feature-based tag parsers.",4,2000
N00-2022,"finally, applications like verbmobil favour prioritized best-first rather than all-paths parsing.",1,2000
N00-2022,"firstly, we will enhance the unpacking phase to take advantage of the large number of equivalence packings we observe.",1,2000
N00-2022,"secondly, many application contexts and subsequent layers of semantic processing will not require unfolding the entire parse forest; here, we need to define a selective, incremental unpacking procedure.",1,2000
N00-2022,"using slightly more sophisticated accounting in the agenda, we plan to investigate priority propagation in a best-first variant of our parser.",1,2000
N00-2022,we intend to develop the approach presented in this paper in several directions.,6,2000
N00-2023,"in future work, we plan to modify the forests our system produces so they conform to the penn treebank corpus (marcus et al., 1993) annotation style,",1,2000
N00-2023,and then do experiments using models built with treebank data.,3,2000
N00-2024,we are preparing the task-based evaluation of the overall system.,3,2000
N00-2024,we also plan to evaluate the portability of the system by testing it on another corpus.,3,2000
N00-2024,we will also extend the system to query-based summarization and investigate whether the system can be modified for multiple document summarization.,5,2000
N00-2028,"finally, we plan to integrate the learned rulesets into the hmihy dialogue system to improve the system's overall performance.",1,2000
N00-2028,in future work we intend to use the (noisy) output from this classifier as input to our problematic dialogue classifier with the hope of improving the performance of the fully automatic feature sets.,1,2000
N00-2028,"in addition, since it is more important to minimize errors in predicting problematic dialogues than errors in predicting tasksuccess dialogues, we intend to experiment with ripper's loss ratio parameter, which instructs ripper to achieve high accuracy for the problematic class, while potentially reducing overall accuracy.",3,2000
N00-2029,our ultimate goal is to provide prosodicallybased mechanisms for identifying and reacting to asr failures in sds systems.,1,2000
N00-2029,"we are examining the w99 corpus, which was collected in a spoken dialogue system that supported registration, checking paper status, and information access for the ieee automatic speech recognition and understanding workshop (asru99) (rahim et al., 1999).",2,2000
N00-2029,we also are extending our toot corpus analysis to include prosodic analyses of turns in which users become aware of misrecognitions and correct them.,2,2000
N00-2029,we are currently replicating our experiment on a new domain with a new speech recognizer.,3,2000
N00-2029,"in addition, we are exploring whether prosodic differences can help explain the ""goat"" phenomenon -- the fact that some voices are recognized much more poorly than others (doddington et al., 1998; hirschberg et al., 1999).",5,2000
N00-2031,a more systematic investigation into the advantages of different feature trees would be useful.,1,2000
N00-2031,"we could add to the feature tree the values of other categories of function tag, or the function tags of various tree-relatives (parent, sibling).",1,2000
N00-2032,"since our method does not use any japanese-dependent heuristics, we also hope to test it on chinese or other languages as well.",3,2000
N00-2032,"in future work, we plan to experiment on japanese sentences with mixtures of character types, possibly in combination with morphological analyzers in order to balance the strengths and weaknesses of the two types of methods.",3,2000
N00-2033,"given the diverse nature of these grammars, we conclude that our techniques based on the lc transform are likely to be applicable to a wide range of cfgs used for natural-language processing.",4,2000
N00-2034,"however, a considerably larger corpus would be required to overcome the sparse data problem for other rsa alternations.",5,2000
N00-2035,there are ways for further improvement of the performance of our system by combining it with a word-based system which encodes specific behavior for individual words.,1,2000
N00-2035,another avenue for further development is to extend the system to other languages.,4,2000
N00-2036,"we perceive that these results can be extended to other language models that properly embed bilexical context-free grammars, as for instance the more general history-based models used in (ratnaparkhi, 1997) and (chelba and jelinek, 1998).",4,2000
N00-2037,"with better estimates of subject preferences, we can then proceed to our larger goal of comparing the usefulness and user acceptability of spoken language dialogue models with and without acknowledgment behavior (c.f.walker, 1993).",1,2000
N00-2037,our immediate plans include extending this study to a larger and gender-balanced group of subjects so that we can draw firmer quantitative conclusions about the percentage of people who are likely to prefer this style of interaction.,4,2000
N00-2037,we also plan to explore the effect of the quality of the synthesized voice on the incidence of acknowledgment behavior.,5,2000
N00-2038,"the goal of my current research is to combine the new alignment algorithm with a cognate identification procedure, the alignment of cognates is possible only after the pairs of words that are suspected of being cognate have been identified.",1,2000
N00-2038,"moreover, it is hardly feasible without some kind of pre-alignment between candidate lexemes.",1,2000
N00-2038,"an integrated cognate identification algorithm would take as input unordered wordlists from two or more related languages, and produce a list of aligned cognate pairs as output.",1,2000
N00-2038,such an algorithm would be a step towards developing a fully automated language reconstruction system.,1,2000
N00-2038,"identification of cognates is, however, an even more difficult task than the alignment itself.",5,2000
N00-2039,"it is worth mentioning that our method can be transferred into a two-level transducer setting without major difficulties (walther, 1999, appendix b).",1,2000
N00-2040,"given the result of roche and schabes (1997), an obvious next step is to compile the induced rules into an actual transducer, and to compose this with the hand-crafted transducer.",1,2000
N00-2040,"it should be noted, however, that the number of induced rules is quite large in some of the experiments, so that the compilation procedure may require some attention.",1,2000
N00-2041,"in some cases, such as with copular constructions and with adjunct prepositional phrases, it would be useful to introduce some non-determinism so that, for example, semantic selectional restrictions between the object of the preposition and the semantic structure that the prepositional phrase is attaching to can more easily play a role in selecting the appropriate semantic relationship.",1,2000
N00-2041,exploring approaches for achieving this non-determinism efficiently is one of our current objectives.,1,2000
N00-2042,"on the other hand, (gardent and webber, january 2000) shows that minimality is also an important tool for disambiguating nouncompounds, logical metonymy and definite descriptions.",1,2000
N00-2042,another area that deserves further investigation concerns the use of minimality for disambiguation.,1,2000
N00-2042,"in further work, we intend to look at other ambiguous natural language constructs and to identify and model the ranking criteria determining their preferred interpretation.",1,2000
N00-2042,plurals are a first obvious choice.,1,2000
N00-2042,this suggests that one of the most promising application of model generators is as a device for developing and testing preference systems for the interpretation of natural language.,4,2000
N00-2042,"but more generally, we hope that looking at a wider range of data will unveil a broader picture of what the general biases are which help determine a preferred reading -- either in isolation, as here, or in context, as in (gardent and webber, january 2000) -- and of how these biases can be modelled using automated reasoning systems.",5,2000
N00-2042,"as the paper shows though, many questions remains open about this use of minimality for disambiguation which are well worth investigating.",5,2000
N00-3001,"our experiment could be extended in many aspects, for example, validating the evaluation function through empirical analysis of human assessments of the generated texts, and experimenting with the interaction between aggregation and referring expression generation.",3,2000
N00-3001,"to generalise our claim, a larger scale experiment is needed.",3,2000
N00-3001,the architecture based on the genetic algorithm can also be used for testing interactions between or within other text generation modules.,4,2000
N00-3002,we are also looking into replacing the fixed pruning and constituent extraction strategy with one learned from training data.,1,2000
N00-3002,we are also investigating ways of learning the number of levels and grammar rule partitioning among levels.,1,2000
N00-3002,there are a number of research issues that we are planning to address soon.,3,2000
N00-3002,"first, a more thorough evaluation is required, as described in the previous section.",3,2000
N00-3002,we are currently looking for ways to perform such an evaluation.,3,2000
N00-3004,we could imagine that rule design process can be partially automated and we intend to pursue some research on developing methods for both assisted rule design and corpus based rule induction.,1,2000
N00-3005,"we plan to extend the system to other qualitatively different types of errors, such as those involving agreement between the main components of the sentence, which is very rich in basque, errors due to incorrect use of subcategorization and errors in postpositions.",1,2000
N00-3005,adding new kinds of errors.,1,2000
N00-3005,"in any case, it must be examined whether automatic methods reach the high precision and reliability obtained by hand-coded rules.",3,2000
N00-3005,"another interesting aspect is the reusability of the linguistic patterns: in the process of treating errors in dates some patterns describe general linguistic facts that can be reused, while others pertain to idiosyncratic facts of dates.",5,2000
N00-3005,Automatic acquisition of error detecting patterns.,1,2000
N00-3006,"there is certainly a great deal of room for further investigation into the use of metadata in spelling correction in general, however.",4,2000
N00-3007,"the next step in our work will be to evaluate our wsd algorithm against the manually sensetagged semcor corpus for validation, and then integrate our wsd algorithm into cindor's processing and evaluate directly the impact on retrieval performance.",3,2000
N00-3007,and also extend our evaluation to multiple languages and to other parts of speech.,3,2000
N00-3007,we hope to verify that word sense disambiguation leads to improved precision in cross-language retrieval.,3,2000
N00-3007,"in our future work we must tackle issues associated with the fine granularity of some wordnet sense distinctions, synsets which are proper subsets of other synsets and are therefore impossible to distinguish,",5,2000
N01-1002,"we should also try to increase the size of the annotated corpus for training statistical models, which might help achieving higher accuracy rate.",2,2001
N01-1002,A more fine-grained analysis of the annotated corpus is needed.,2,2001
N01-1003,"in future work, we intend to build on the work reported in this paper in several ways.",1,2001
N01-1003,"these include features based on the discourse context, and features that encode relationships between the sp-tree and the dsynts.",1,2001
N01-1003,"first, we believe that we could utilize additional features as predictors of the quality of a sentence plan.",1,2001
N01-1003,"we will also expand the capabilities of the spg to cover additional sentence planning tasks in addition to sentence scoping, and duplicate the methods described here to retrain spot for our extended spg.",1,2001
N01-1004,the general framework opens up the possibility of assigning weights to more sophisticated lexical questions that is consistent with the popular notion of idf.,1,2001
N01-1006,"furthermore, our algorithm scales better with training data size; therefore the relative speed-up obtained will increase when more samples are available for training, making the procedure a good candidate for large corpora tasks.",1,2001
N01-1010,"for other future work, we plan to investigate an automatic way of detecting and filtering unrelated relations.",1,2001
N01-1010,"for those tasks, we anticipate that our extraction methods may be useful in deriving characteristics of the domains or given corpus, as well as customizing the lexical resource.",4,2001
N01-1010,"we are also planning to compare our sense partitions with the systematic disagreement obtained by (wiebe, et al., 1998) automatic classifier.",3,2001
N01-1014,"however, in spite of the fact that the main focus of this paper is diachronic phonology, the techniques and findings presented here may also be applicable in other contexts where it is necessary to identify cognates, such as bitext alignment.",4,2001
N01-1016,"also of interest are models that compute the joint probabilities of the edit detection and parsing decisions 鈥 that is, do both in a single integrated statistical process.",1,2001
N01-1017,"in the future, we will address the issue of the amount of training data for the si model.",2,2001
N01-1017,we would also like to investigate the point at which the size of the various kinds of data used for adaptation stops making improvements in recognition accuracy.,2,2001
N01-1017,we believe that using such confidence measures can improve the generation of semi-literal transcriptions considerably.,1,2001
N01-1017,"also, current atrs system does not take advantage of various confidence scores available in leading recognition engines.",1,2001
N01-1020,"one possible area of investigation is to use larger dictionaries and assess how much better stochastic transducers, and distance metrics derived from them, perform with more training data.",2,2001
N01-1020,the first is whether there exists a better string transformation algorithm to use in the induction step.,1,2001
N01-1020,"another option is to investigate the use of multi-vowel or multi-consonant compounds which better reflect the underlying phonetic units, using an more sophisticated edit distance measure.",1,2001
N01-1020,it is an interesting research question as to whether we can augment these methods with translation probabilities estimated from statistical frequency information gleaned from loosely aligned or unaligned bilingual corpora for non-cognate pairs.,5,2001
N01-1020,"various machine learning techniques, including co-training and mutual bootstrapping, could employ these additional measures in creating better estimates.",1,2001
N01-1024,"we also believe that some findings of this work can benefit other areas of linguistic induction, such as part of speech.",4,2001
N01-1024,"for the future, we expect improvements could be derived by coupling this work, which focuses primarily on inducing regular morphology, with that of yarowsky and wicentowski (2000), who assume some information about regular morphology in order to induce irregular morphology.",4,2001
N01-1027,"the next steps, developing techniques to interpret these turns more accurately and to use correction prediction to drive modifications in dialogue strategy, are both subjects of our future research.",1,2001
N01-1028,"more work needs to be done to determine, if possible, under which conditions such improvements can be obtained.",3,2001
N01-1029,in the future we plan to evaluate full em reestimation of the models on the trainset using the formulas given in this paper.,3,2001
N01-1030,"it is unclear to us how large the cumulative effect may become when detailed modelling of many such phenomena is added to an atheoretical coarse-grained language model, and it is seems plausible that it could be considerably greater than that produced by the single phenomenon we have investigated here.",5,2001
N01-1030,it is feasible to investigate these questions empirically by modifying large-scale linguistically motivated systems like the psa grammar described in this paper; we hope to present further results in due course.,1,2001
N01-1030,"looking further ahead, it is also important to remember that grammatical agreement is only one of a large range of linguistic phenomena currently ignored by most practical implementors.",5,2001
N01-1031,"based on the promising results on classification of text, we are looking at ways of incorporating this with existing techniques for identifying non-native speakers based on acoustic features, with the goal of obtaining even greater reliability from combining the two.",1,2001
N01-1031,thus we would also like to explore minimizing the number of words needed to be spoken before the system can identify nativeness.,5,2001
N01-1031,"finally, it may be helpful to apply some of the features discovered in this work to the problem of identifying native and non-native writing, which may be of relevance to writer identification.",4,2001
N03-1005,the long-term objective of this work is to learn the pronunciations and spellings of general oov data in spoken dialogue systems on domains where oov proper nouns are prevalent.,4,2003
N03-1005,future experiments will involve general classes of unknown words such as names of geographical locations or businesses.,2,2003
N03-1007,"work also needs to be done on using information given by answers, not just questions in recognizing clarification dialogue and on coping with the cases in which clarification dialogue recognition is not enough to retrieve an answer and where other, more complex, techniques need to be used.",5,2003
N03-1007,future work will be directed on extending wordnet in this direction and in providing other useful semantic relationships.,1,2003
N03-1011,"thus the learning procedure and the validation procedure are generally applicable and we intend to use the method for the detection of other semantic relations such as manner, influence, and others.",1,2003
N03-1011,the inconvenience of the method is that for a very precise learning the number of examples (both positive and negative) should be very large.,5,2003
N03-1011,we also intend to automate the detection of lexicosyntactic patterns and to discover constraints for all the part-whole patterns.,1,2003
N03-1012,"currently, we are also beginning to investigate whether the proposed method can be applied to scoring sets of potential candidates for resolving the semantic interpretation of ambiguous, polysemous and metonymic language use.",4,2003
N03-1012,"as future work we will examine how the computation of a discourse dependent semantic coherence score, i.e. how well a given srh fits within domain model with respect to the previous discourse, can improve the overall score.",1,2003
N03-1012,"additionally, we intend to calculate the semantic coherence score with respect to individual domains of the system, thus enabling domain recognition and domain change detection in complex multi-modal and multi-domain spoken dialogue systems.",1,2003
N03-1013,"additionally, we are interested in measuring the applied contribution of using the catvar in natural-language applications such as information retrieval.",3,2003
N03-1013,"and finally, we intend to incorporate catvar into new applications such as parallel corpus word alignment.",1,2003
N03-1013,future work includes improving the word-cluster ratio and absorbing more of the single-word clusters into existing clusters or other single-word clusters.,2,2003
N03-1013,we are also considering enrichment of the clusters with types of derivational relations such as “nominal-event” or “doer” to complement part-of-speech labels.,2,2003
N03-1013,"other lexical semantic features such telicity, sentience and change of-state can also be induced from morphological cues (light, 1996).",1,2003
N03-1018,we are also exploring the possibility of tuning a statistical machine translation model to be used with our model to exploit parallel text.,1,2003
N03-1018,"finally, we plan to challenge our model with other languages, starting with arabic, turkish, and chinese.",2,2003
N03-1018,"if a translation of the ocr’d text is available, a translation model can be used to provide us with a candidate-word list that contains most of the correct words, and very few irrelevant words.",1,2003
N03-1018,chinese will require more work due to the size of its character set.,2,2003
N03-1018,"we are optimistic that the power and flexibility of our modeling framework will allow us to develop the necessary techniques for these languages, as well as many others.",1,2003
N03-1018,"arabic and turkish have phonetic alphabets, but also pose the problem of rich morphology.",2,2003
N03-1021,"in future work, we hope to extend the formalism to cover some of the aspects that would not raise the computational complexity of its recognition, such as discontinuous and/or phrasal terminals.",4,2003
N03-1021,"concurrently, we shall explore the empirical properties of mtg, by inducing stochastic mtgs from real multitexts.",1,2003
N03-1023,"ther less studied single-view weakly supervised algorithms in the nlp community such as co-training with different learning algorithms (goldman and zhou, 2000) and graph mincuts (blum and chawla, 2001) can be similarly applied to these problems to further test our original hypothesis.",1,2003
N03-1024,we also wish to extract more abstract paraphrase patterns from the current representation.,1,2003
N03-1024,"such patterns are more likely to get reused – which would help us get reliable statistics for them in the extraction phase, and also have a better chance of being applicable to unseen data.",1,2003
N03-1024,"in our future work, we wish to experiment with more flexible merging algorithms and to integrate better the top-down and bottom-up processes that are used to induce fsas.",1,2003
N03-1025,"we are currently investigating more challenging problems like multiple category classification using the reuters-21578 data set (lewis, 1992) and subjective sentiment classification (turney, 2002).",5,2003
N03-1028,"full discriminative parser training faces significant algorithmic challenges in the relationship between parsing alternatives and feature values (geman and johnson, 2002) and in computing feature expectations.",5,2003
N03-1031,we hope these studies will reveal ways to combine the strengths of co-training and active learning to make better use of unlabeled data.,1,2003
N03-1031,"finally, we are conducting experiments to compare corrected co-training with other active learning methods.",3,2003
N03-1034,a key area for future work is to devise a truly reusable qa evaluation infrastructure.,3,2003
N03-1035,we will also study the impact of semantic differences between terms on user preferences and investigate whether terms which are preferred for information access are equally suitable for other nlp tasks.,1,2003
N03-1037,headline generation is another task that we can approach equipped with our large restructured web the performance is reported on four metrics.,5,2003
N03-2004,"given sufficient training, we might even take question features into account, learning that certain systems are better at certain types of questions.",1,2003
N03-2004,we would like to pursue the use of these and other evidence sources in the future.,2,2003
N03-2007,"further investigation is needed into the best way to measure overall labeling effort, and into refinements of the active learning process to optimize that labeling effort.",1,2003
N03-2008,"by treating semantic classification as a single tagging problem, we hope to create a unified, practical, and high performance system for frame element tagging.",1,2003
N03-2008,"in future work, we will extend the strategies outlined here to incorporate frame element identification into our model.",1,2003
N03-2013,we are planning to integrate this method into a form of automatic evaluation.,3,2003
N03-2014,"ongoing work includes improving the mention detection and mention tracking by adding morphological, syntactic (derived from parse trees) and semantic (e.g. wordnet) information streams, and extracting relations between the detected entities using statistical models.",1,2003
N03-2016,"in the future, we plan to develop a method of incorporating the cognate information directly into the training algorithm.",1,2003
N03-2018,"we will next explore the utility of a wider variety of features representing many knowledge sources (including acoustic, prosodic, lexical, syntactic, semantic, discourse, and local and global contextual dialogue features), using ablation studies.",1,2003
N03-2018,"we will perform our learning using and comparing large corpora of both human-human and human computer data for training and testing, and will evaluate our results using a variety of metrics (e.g. recall, precision, and f-measure).",2,2003
N03-2018,we will also investigate a variety of emotion annotations with the goal of producing a reliable annotation scheme for the emotions associated with our tutoring domain.,1,2003
N03-2018,"previous studies have shown low inter-annotator reliability (around 70%, kappa values around 0.47 (narayanan, 2002)), which originates partly in vague descriptions of the emotions to be labeled.",2,2003
N03-2019,"future work will examine the role of aspectual features, learning from skewed distributions dominated by at (an overwhelming majority of news events occur at the reference times), and the incorporation of unsupervised learning methods.",1,2003
N03-2027,we are currently investigating the performance of other dbn topologies on pos tagging.,1,2003
N03-2032,"map task is appropriate because besides dialogue acts it is annotated for syntactic information, while callhome is not.4) experiment with flsa on other tasks, such as assessing text coherence.",4,2003
N03-2032,"2) include other features in flsa, e.g. syntactic roles.",1,2003
N03-2032,"thus, we will pursue the following directions.1) further investigate the correlation of the performance of (semi)clustered lsa with the size of the corpus and / or of the target classification.",1,2003
N03-2035,our future works will investigate other machine learning techniques such as snow and svm.,1,2003
N03-2038,"first, when modifying the decoder to use gtm, we used viterbi approximation at word boundaries, which means trajectory information is lost upon word transition.",1,2003
N03-2038,"second, we plan to extend gtm to handle deletions in sloppy speech, a major challenge in lvcsr.",1,2003
N03-3001,we believe we have only taken a first step in this direction and much remains to be done as part of future work.,6,2003
N03-3001,"additionally, it has been suggested that statistical models such as the aspect model (hoffman, 1999) and the latent dirichlet allocation (blei et al., 2001) which generate words from a mixture of aspect-models can be exploited by modeling semantic classes as the aspects.",1,2003
N03-3001,we will be studying the applicability of these ideas to the current task as part of our future work.,1,2003
N03-3001,we believe the main contribution of our work lies in our attempt at incorporating semantic information in the language modeling framework and combining scores in a principled way.,1,2003
N03-3003,we have plans to carry out additional reading experiments with good and bad readers to investigate whether the constraints we tighten to adjust the language models for poor readers actually produce more readable results.,3,2003
N03-3003,plans for future work include expanding the size of our corpus analysis and automating at least some of the analysis and data reconfiguration.,2,2003
N03-3003,"further on, we plan to take discourse coherence considerations into account.",1,2003
N03-3003,we plan further development of the micro planner to prevent incoherent solutions being generated.,1,2003
N03-3003,we will measure reading speeds and comprehension as in our preliminary experiment.,3,2003
N03-3003,"we will generate texts under the default “good reader” models and under the constrained, poor reader, models.",1,2003
N03-3004,we are also considering the use of other measures of similarity beyond the matching coefficient and the cosine.,1,2003
N03-3004,"we are also exploring a number of other types of features, as well as varying the formulation of the features we are currently using.",1,2003
N03-3004,"we have already conducted a number of experiments that vary the window sizes employed with bigrams and second order co–occurrences, and will continue in this vein.",3,2003
N03-3005,future work will focus on improving the indexing techniques analyzed in this paper.,1,2003
N03-3005,"possible areas of investigation are substitution tree indexing (graf, 1995) for nonstatistical methods, or restructuring decision trees (utgoff et al., 1997), while trying to maintain index operation costs at a minimum.",1,2003
N03-3005,performance profiling combined with software and database engineering techniques will be used to determine the optimum trade-off between indexing efficiency and implementation cost.,1,2003
N03-3007,"furthermore, we also need to investigate the performance when applying such an approach to the speech recognition results.",4,2003
N03-3007,"finally, a unified framework for word fragment and the disfluency detection is also a future direction of our work.",1,2003
N03-3009,"in future experiments we plan to compare select’s performance on written and spoken news texts with two recently proposed systems, u00 (utiyama 2001) and cwm (choi 2001), which have marginally outperformed the c99 algorithm on choi’s (2000) test corpus.",3,2003
N03-3010,how to build a balanced training set with single finite state machine model will remain our important work in the future.,1,2003
N03-3010,"for the learning mechanism, naïve bayesian learning requires more understanding of different factors’ roles and their importance.",1,2003
N03-3010,"for the statistical learning model, the quality and the different configurations of training set highly affect the performance of models trained and thus their abilities to process sentences.",2,2003
N03-3010,the balance of training set is also a big issue.,1,2003
N04-1002,"second, we intend to extend our work to include new comparison and clustering approaches.",1,2004
N04-1002,we look forward to trying out techniques on that data when it is available.,1,2004
N04-1002,"and the subject information has apparently value in some cases, so we hope to determine how to use the information more broadly.",1,2004
N04-1002,"first, colleagues of ours are working on a more realistic corpus that is not just large but also contains a much richer set of marked up entities.",2,2004
N04-1002,it appears that sentence-based snippets and within document coreference information may provide a small gain.,1,2004
N04-1003,"(3) integration of this work with other aspects of general coreference resolution (e.g., other terms like pronouns that refer to an entity) and named entity recognition (which we now take as given);",1,2004
N04-1003,(4) scalability issues in applying the system to large corpora.,4,2004
N04-1003,"some of the issues that will be included in future steps are: (1) integration with more contextual information (like time and place) related to the target entities, both to support a better model and to allow temporal tracing of entities;",2,2004
N04-1004,"one possible future project would be to develop a set of constraints for speech-gesture alignment, and investigate the effect of these constraints on both accuracy and speed.",1,2004
N04-1004,"only the construction of such a comprehensive end to-end system will reveal whether the algorithm and features that we have chosen are sufficient, or whether a more sophisticated approach is required.",1,2004
N04-1004,"in this section, we describe four possible avenues of future work: dynamic programming, deeper syntactic analysis, other anaphora resolution techniques, and user adaptation.",1,2004
N04-1009,"when the results are released to the participants, they would be asked not to look at these equating questions, and not to use them to train their systems in the future.",6,2004
N04-1009,of particular interest is application to technology areas that use metrics other than percent of items processed correctly.,4,2004
N04-1010,"one possible approach will be to combine our methods with alternative techniques, which were actually examined in our experiments.",1,2004
N04-1010,our second goal is to extend our method so that it can handle multi-word hypernyms.,4,2004
N04-1010,the first goal of our future work is to further improve the precision of our method.,1,2004
N04-1010,"if we can obtain a multi-word hypernym such as “automobile manufacturer,” it can provide more useful information to various types of natural language processing systems.",2,2004
N04-1012,"thus, it is likely in such situations that we will be able to develop better evolved models only after the data is annotated and more has been learned about the task.",1,2004
N04-1012,it is then necessary to see whether improved models benefit from the examples selected using al techniques with an earlier model more than they would have if random sampling had been used.,3,2004
N04-1012,"another issue we will explore in future work is that for a scenario in which we label a data set from scratch, it is quite possible that we will not know how best to model the task we are labeling that data for.",5,2004
N04-1016,another possibility that needs further investigation is the combination of web-based models with supervised methods.,1,2004
N04-1016,this can be done with ensemble learning methods or simply by using web-based frequencies (or probabilities) as features (in addition to linguistically motivated features) to train supervised classifiers.,1,2004
N04-1019,"second, creating an initial pyramid is laborious so large-scale application of the method would require an automated or semi-automated approach.",1,2004
N04-1019,"first, pyramid scores ignore interdependencies among content units, including ordering.",1,2004
N04-1020,an important future direction lies in modelling the temporal relations of events across sentences.,1,2004
N04-1020,an important question for further investigation is the contribution of linguistic and extra-sentential information to modelling temporal relations.,5,2004
N04-1020,"apart from taking more features into account, in the future we plan to experiment with models where main and subordinate clauses are not assumed to be conditionally independent and investigate the influence of larger data sets on prediction accuracy.",3,2004
N04-1022,"by extending the search space of the decoder to a much larger space than the -best list, we expect further performance improvements.",1,2004
N04-1022,we expect new automatic mt evaluation metrics to emerge frequently in the future.,3,2004
N04-1022,in future work we plan to extend the search space of mbr decoders to translation lattices produced by the baseline system.,1,2004
N04-1022,we are developing efficient lattice search procedures for mbr decoders.,1,2004
N04-1022,we expect new automatic mt evaluation metrics to emerge frequently in the future.in future work we plan to extend the search space of mbr decoders to translation lattices produced by the baseline system.we are developing efficient lattice search procedures for mbr decoders.,3,2004
N04-1024,"we hope that in further investigation of this richly annotated data set, we will be able to build on the current prototype and develop a full-scale writing instruction capability that provides feedback on the coherence dimensions described in this paper.",1,2004
N04-1025,"the problem of reading difficulty prediction lies in an interesting region between classification and regression, with close connections to ordinal regression (maccullagh, 1980) and discriminative ranking models (crammer and singer, 2001).",5,2004
N04-1026,"we are also exploring methods of combining information other than by feature level combination, such as data fusion across multiple classifiers (lee et al., 2002; batliner et al., 2003).",1,2004
N04-1026,"we are currently exploring the use of other emotion annotation schemas for emotion prediction, such as those that incorporate categorizations encompassing multiple dimensions (craggs, 2004; cowie et al., 2001) and those that examine emotions at smaller units of granularity than turns (batliner et al., 2003).",1,2004
N04-1026,"finally, when itspoke’s evaluation is completed, we will address the same questions for our human-computer dialogues that we have addressed here for our corresponding human-human dialogues.",5,2004
N04-1026,"for evaluation, we would like to see whether the ordering preferences among feature sets (as in figure 5) are the same when recall, precision, and f-measure are plotted instead of accuracy.",3,2004
N04-1026,"furthermore, we are investigating whether greater tutor response to emotions correlates with greater student learning.",5,2004
N04-1027,"such communicative behavior, then has to be implemented in dialogue systems, to make their way of communicating more like that of their human partners.",1,2004
N04-1027,"that is, finding out just how much extra-propositional signaling is needed to guarantee a felicitous dialogue.",1,2004
N04-1027,as future work we propose to take the wizard and operator test paradigm introduced herein and to change and adjust the parameters of the computer-human interaction - while performing subsequent measurements of the ensuing effects - until an acceptable degree of dialogue efficiency is reached.,1,2004
N04-1027,"in our minds, achieving dialogue quality remains an important challenge for the scientific community, but - as we have shown herein and seen in recent evaluations - dialogue efficiency constitutes another necessary condition for achieving dialogue felicity.",5,2004
N04-1028,we will also work on integrating the confirmation prompt generation method proposed in this work with state-of-the-art confidence annotation methods.,1,2004
N04-1028,"in the near future, our priority is to collect more data to improve the acoustic models of the system and address the specific issues related to a general non-native population, which does not share a common native language.",1,2004
N04-1029,"to effectively use information such as user’s familiarity with the topic, the purpose of the user’s search or the user’s genre preferences we need more complex linguistic and stylistic analysis techniques.",1,2004
N04-1031,one of our future work is to examine what kind of spoken language is suitable for such a kind of application that was illustrated in the introduction.,3,2004
N04-1031,"however, there are other kinds of paraphrasing which are necessary in order to paraphrase written language text into spoken language.",3,2004
N04-1033,"also, slightly relaxing the monotonicity constraint in a way that still allows an efficient search is of high interest.",5,2004
N04-1033,"for further improvements, we will investigate the usefulness of additional models, e.g. modeling the segmentation probability.",1,2004
N04-1033,"in spirit of the ibm reordering constraints of the single-word based models, we could allow a phrase to be skipped and to be translated later.",1,2004
N04-1034,lack of parallel corpora is a major bottleneck in mt research.,5,2004
N04-1035,"we suspect that such probabilistic rules could be also used in conjunction with statistical decoders, to increase the accuracy of statistical machine translation systems.",1,2004
N04-1037,"on the other hand, predicate-argument statistics appear to provide a poor substitute for the world knowledge that may be necessary to correctly interpret the remaining cases.",5,2004
N04-1038,"in future work, we plan to follow-up on this approach and investigate other ways that contextual role knowledge can be used.",1,2004
N04-1039,"in the future, we would like to use our technique of examining the distribution of model parameters to see if other problems exhibit other priors besides gaussian and laplacian/exponential, and if performance on those problems can be improved through this observation.",3,2004
N04-1040,"our method can be used as postprocessing to the methods developed by other researchers, such as topic-specific models, to create a system with even better performance.",4,2004
N04-1042,"we also suggest better evaluation metrics to facilitate future research in this task—especially field-f1, rather than word accuracy.",3,2004
N04-1042,fundamental advances in regularization for crfs remains a significant open research area.,5,2004
N04-1043,"similarly, alternative clustering techniques, perhaps based on different contextual features or different distance measures, could further improve performance.",1,2004
N04-1043,"on the application side, it would be interesting to apply the technique to other language problems.",2,2004
N04-2001,"significant amount of research remains in handling spontaneous speech effects and nonverbal sounds, which are common in real world data.",5,2004
N04-2002,it would be interesting to try these approaches for our task and compare them with naive bayes and n-gram approaches discussed here.,3,2004
N04-2002,we intent to incorporate other types of features including context based features with this work.,1,2004
N04-2003,alternative indicator clustering techniques will be explored too. a theoretical description of the relationship between em and soft tagging would potentially be able to identify convergency properties of the bootstrapping framework.,1,2004
N04-2003,"in the future, we will try bootstrapping with bnc data.",2,2004
N04-2004,"lacking quantitative measures currently, the merits of my proposed framework can only be gauged on theoretical grounds and its future potential to better capture a variety of linguistic phenomena.",3,2004
N04-2005,another important issue being examined is how to evaluate the asl animation output of an mt system – in particular one that produces classifier predicates.,5,2004
N04-2005,"currently, this project is finishing the specification of the multi-path design and investigating the following issues: deep generation techniques for creating multiple interrelated classifier predicates, surface generation of individual classifier predicates from compositional rules or parameterized templates, and asl morphological and syntactic representations for the transfer pathway.",5,2004
N04-2006,"we plan to train and test our model on other corpora and, if possible, on writing samples of non-native speakers.",3,2004
N04-2006,"on the other hand, the articles that were already present in the sentence provided strong hints about the correct article; this points to the need for better methods for estimating the underlying confusion matrix of a sentence.",1,2004
N04-2006,"finally, we would like to investigate how well the rules learned by our model generalize to other genres of texts.",1,2004
N04-2006,"for native speakers of languages that do not inflect nouns and verbs, it is a common mistake to use the root forms of nouns and verbs instead of the inflected form.",6,2004
N04-2006,"we have also identified a few other common categories of grammatical mistakes, such as the number of the np head (singular vs. plural), and the verb tenses (present vs. past vs. continuous).",6,2004
N04-2006,We would like to improve the performance of the article generator.,1,2004
N04-2007,"also, a window of characters will introduce new features to learn from.",1,2004
N04-2007,"since bacchant decision tree learns based on context, greater context may allow for better learning, and a window of characters will expand context.",2,2004
N04-2007,"since a decision tree's features determine how it learns from context, adding better features to the decision tree may help the tree learn better.",1,2004
N04-2007,"as hidden markov models have been used both for name finding (bikel (1997)) and tokenization (cutting (1992)), this seems to be a promising research possibility.",1,2004
N04-2008,"(1) using transitional probability (p(sk+1 |sk)) and mutual information measures over two adjacent segments as cues to the likelihood of word boundaries between those two segments, as suggested in e.g., (brent, 1999a).(2) developing more plausible models for approximating word-length distributions from utterance-length information, distances between stressed vowels, pause information, and other salient cues available to children.(3) incorporating stress cues (as potentially signaling both beginnings and approaching ends of content words) both alone and in combination with segmental cues.",1,2004
N04-2009,the work on the system is ongoing and the efforts are continuing to implement a verb sense disambiguation component and to test the conceptual graph construction module.,1,2004
N04-2010,an interesting new topic for speaker adaptation could be joint structure and parameter adaptation.,5,2004
N04-3002,"our next step will be to modify the dialogue manager inherited from why2-atlas to use new tutorial strategies optimized for speech, and to enhance itspoke to predict and adapt to student emotion.",1,2004
N04-3006,"we believe that the semantic parser will prove useful for a range of language processing applications that require knowledge of text meaning, including word sense disambiguation, information extraction, question answering, machine translation, and others.",4,2004
N04-4001,"finally, the potential impact of our findings with respect to recent evaluation metrics should not be overlooked.",3,2004
N04-4001,"in the future, we wish to apply our method to other corpora, and to explore the extent to which different summarization goals, such as describing an event or providing a biography, affect the degree to which humans employ rewriting as opposed to extraction.",4,2004
N04-4001,"we would like to empirically quantify to what extent current summarization systems reformulate text, by applying the techniques presented in this paper to system output.",4,2004
N04-4003,future work will also focus on the integration of the proposed rescoring formula in the decoding process.,1,2004
N04-4006,"furthermore, it is important to closely match testing conditions for perceptron training.",1,2004
N04-4008,we expect the final bilingual framenet will provide a valuable resource for multi-lingual or cross-lingual natural language processing.,4,2004
N04-4008,"the next step is to automatically extract semantically annotated chinese sentences based on the annotated english sentences in framenet, the aligned framenet lexical entries, and bilingual corpora.",1,2004
N04-4009,"it is thus at least conceivable that a self-trained approach, coupled with a large set of features and a large corpus of raw data, could eventually overtake the performance of the best supervised models.",1,2004
N04-4011,a successful approach will need to consider both algorithmic requirements and technology limitations.,1,2004
N04-4012,"without a gold standard for a generation system for dynamic multimodal user interfaces to qualitatively compare against, controlled user trials will allow us to evaluate the usability of the interfaces we have created.",3,2004
N04-4012,"task completion times, user frustration levels, and user satisfaction can then be used to evaluate the success of this model of multimodal interactions.",3,2004
N04-4012,"as possible examples for future applications, we see a multimodal interface that allows mobile users or users with sensory impairments to traverse information-rich social networks, and a kiosk for multimodal, multilingual access to public transportation options.",5,2004
N04-4012,further evaluation will show whether the fitness function can accurately mirror user satisfaction with a given output variant and whether our form of adaptivity is actually an advantage to users on the go.,3,2004
N04-4013,we are investigating multi-document summarization techniques which might allow users to better pinpoint the category in which a relevant document might be found.,1,2004
N04-4013,we would like users to be able to more readily identify the class into which a relevant document (if one exists) would be found.,6,2004
N04-4016,the creation of rules for correction utterances based on the dialog history could be applicable to dialog systems which use speech recognition or natural language processing and other kinds of rules beyond regular grammars; we plan to study this in future work.,4,2004
N04-4016,"we also plan to apply the proposed method to other types of dialogs, such as user-initiative dialogs and mixed-initiative dialogs.",4,2004
N04-4016,we are also planning to develop an algorithm to improve the precision of corrections that are based on the set of recognition candidates for the correction utterance and an error recovery strategy.,1,2004
N04-4018,we also plan to extend these methods to additional tasks such as disfluency detection.,4,2004
N04-4018,"this will provide opportunities to investigate the interaction of automatic word recognition and structural metadata, hopefully resulting in reduced wer.",1,2004
N04-4018,future work will involve a tighter integration of su detection and word recognition by including su events directly in the recognition lattice.,1,2004
N04-4019,we also plan to refine speech graffiti’s runtime help facilities in order to assist users more effectively in saying the right thing at the right time.,1,2004
N04-4019,"in addition to these core interface goals, we plan to extend the functionality of speech graffiti beyond information access to support the creation, deletion and modification of information in a database.",4,2004
N04-4019,"to improve the habitability of speech graffiti, we plan to explore allowing more natural language-esque interaction while retaining an application-portable structure.",1,2004
N04-4019,how can we help users who are having severe difficulties with an interface learn how to use it better and faster?,5,2004
N04-4019,"for speech graffiti, scores for habitability (represented by statements like “i always knew what to say to the system”) were typically the lowest of any of the six user satisfaction factors, suggesting that this is a prime area for further work.",4,2004
N04-4019,these users have become a motivator of future work: what can be done to make the interface work for them and others like them?(future studies will focus on a broader population of adults.),5,2004
N04-4020,an implementation of this “sharpening” algorithm is currently under development.,1,2004
N04-4020,"repeating this alternation until the weights show minimal change will minimize the contributions of unreliable annotators and poorly annotated messages to the assignment of labels to messages, thereby increasing confidence in the results.",6,2004
N04-4021,"in addition, we are currently investigating extensions to the model, including context-dependent feature substitutions.",1,2004
N04-4021,we also plan to extend this study to a larger data set and to multi-word utterances.,2,2004
N04-4021,we are currently working on a new formulation in which the synchronization constraints can be trained via em.,1,2004
N04-4022,"instead, the focus of the work is to improve the specific aspects of the asr output that may adversely affect a user-centered task like information retrieval.",1,2004
N04-4022,"while we have not formally evaluated the impact of our error detection and correction on retrieval performance, there is an obvious benefit to correcting misrecognitions of the specific query term that a user is seeking.",3,2004
N04-4025,"however, up to recently, the large amounts of transcript data have limited researchers from performing analyses of team discourse.",5,2004
N04-4025,research into team discourse is a new but growing area.,5,2004
N04-4025,"we are currently exploring two promising avenues to predict performance in real time: integration of speech recognition technology, and inter-turn tag sequences.",1,2004
N04-4027,"we intend to learn predictors for some other thread aspects such as thread category and question-answer pairs, and then use these as input to the sentence extraction procedure.",1,2004
N04-4027,we also intend to perform an evaluation based on human feedback.,3,2004
N04-4027,"we can use this insight to instruct human annotators, and to improve the automatic extraction.",1,2004
N04-4028,"we hypothesize this is because crfs are already discriminative (not joint, generative) models; furthermore, this may suggest that future discriminative parsing methods will also have the benefits of discriminative reranking built-in directly.",1,2004
N04-4030,"in addition, we are currently designing usability studies in which we will present different categorization suggestions to information architects to organize.",1,2004
N04-4030,"their subjective reactions, the amount of time it takes them to create the organizations, and the resulting quality and coverage of the organizations, as measured by users performing navigation tasks using the hierarchies, will be compared to other techniques.",6,2004
N04-4032,"in addition, just as the slm is useful for both parsing and language modeling, it could be used to predict metadata for its own sake or to improve word recognition, with or without the word-based representation.",4,2004
N04-4032,"further research using the structured language model could incorporate these metadata directly into the model, allowing it to take advantage of higher-level metadata without reducing the effective number of words available to the model.",1,2004
N04-4034,latent semantic analysis of stream a might also be usefully employed here.,1,2004
N04-4034,"one can imagine that higher level information (e.g., a dialog or other speech act) about the other speakers might be particularly important.",2,2004
N04-4034,"furthermore, more than one word from stream a can be included in the context to provide additional predictive ability.",1,2004
N04-4035,"we also plan to explore the integration of prosodic and textual features and investigate the identification of more fine-grained sub-topic structure, to provide more focused units for information retrieval, summarization, and anaphora resolution.",1,2004
N04-4035,we would like to integrate speaker identification for normalization and speaker change detection.,1,2004
N04-4036,"unfortunately, meaningful comparisons to these efforts are difficult due to differing evaluation metrics.",5,2004
N04-4037,"although we have flattened the trees in the propbank corpus for our experiments, the proposed language structure supports flat annotation from scratch, which we believe is useful for porting the method to other domains and languages.",4,2004
N04-4037,"while our initial results have been encouraging, this work must be extended and enhanced to produce the quality of semantic parse produced by systems using a full syntactic parse.",1,2004
N04-4038,"we are currently trying to improve the performance of the systems by using additional features, a wider context and more data created semi-automatically using an unannotated large arabic corpus.",2,2004
N04-4038,"in addition, we are trying to extend the approach to semantic chunking by hand labeling a part of arabic treebank with arguments or semantic roles for training.",1,2004
N04-4039,another important direction is to evaluate the effectiveness of agent gestures in actual human-agent interaction.,3,2004
N04-4039,"we expect that if our model can generate gestures with appropriate timing for emphasizing important words and phrases, users can perceive agent presentations as being more alive and comprehensible.",1,2004
N04-4039,"we plan to enhance our model by incorporating more general discourse level information, though the current system exploits cue words as a very partial kind of discourse information.",1,2004
N04-4039,we plan to conduct a user study to examine this hypothesis.,1,2004
N04-4040,"it does appear to be a promising technique for future research however. eventually it may prove that more elaborate acoustic cues will be needed to identify these edits, at which point a model of interruption points could be included as a feature in the rules learned by the system.",1,2004
N06-1002,we plan to consider other approaches for conditioning on context.,1,2006
N06-1002,we hope to replace them with effective models without the brittleness and sparsity issues of heavy lexicalization.,1,2006
N06-1003,"finally, we plan to formalize our targeted manual evaluation method, in the hopes of creating a evaluation methodology for machine translation that is more thorough and elucidating than bleu.",3,2006
N06-1003,"in future work, we plan to determine how much data is required to learn useful paraphrases.",2,2006
N06-1004,"once we have found the best segmentation model, we will improve the system’s current naïve single-word segmentation of the remaining source sentence during decoding, and construct a more accurate future cost function for beam search.",1,2006
N06-1004,"another obvious system improvement would be to incorporate more advanced word-based features in the dts, such as questions about word classes (tillmann and zhang 2005, tillmann 2004).",1,2006
N06-1004,we also plan to apply scms to rescoring n-best lists from the decoder.,1,2006
N06-1004,"finally, we are contemplating an entirely different approach to dt-based scms for decoding.",1,2006
N06-1006,"in future, we aim to combine more precise modeling of monotonicity effects with better modeling of paraphrase equivalence.",1,2006
N06-1007,"it is very likely that many common event participants appearing in such proximity are referred to by coreferential expressions, and therefore noticeable improvement can be expected from applying coreference resolution to the corpus prior to learning entailment patterns from it.",2,2006
N06-1011,"to this end, we would like to compare the performance of an ner system trained on a corpus tagged using this approach to one trained on a hand-tagged corpus.",3,2006
N06-1012,"in this work, we choose the subsets based on our intuition of which features are complementary for this task, but automatically determining the feature subsets is an interesting area for future work.",1,2006
N06-1013,"however, additional studies are needed to investigate why huge improvements in aer result in relatively smaller improvements in bleu scores.",5,2006
N06-1014,"while aer is only a weak indicator of final translation quality in many current translation systems, we hope that more accurate alignments can eventually lead to improvements in the end to-end translation process.",1,2006
N06-1017,"possibilities include associating items with similar existing senses (widdows, 2003; curran, 2005; burchardt et al., 2005) or clustering them into approximate senses.",1,2006
N06-1017,"once items have been identified as unknown, they are available for further processing: if possible one would like to assign some measure of sense information even to these items.",6,2006
N06-1017,"our immediate goal is to use unknown sense detection in combination with wsd, to filter out items that the wsd system cannot handle due to missing senses.",1,2006
N06-1018,finally we would like to expand our coverage of temporal expressions to include other types of expressions such as recurrence expressions10.,1,2006
N06-1018,we also need to investigate the disambiguation procedure and possibly migrate the functionality into a separate discourse module.,1,2006
N06-1018,"in addition, the co-referencing tendency of noun-modifying expressions could lead to a better way to anchoring this particular type of temporal expressions.",1,2006
N06-1021,in future work we intend to explore the discriminative reranking of n-best lists produced by these parsers and the incorporation of morphological features.,1,2006
N06-1023,"in the future, by incorporating ellipsis resolution, we will develop an integrated model of syntactic, case and ellipsis analysis.",1,2006
N06-1024,"we also expect that productivity of syntactic annotation of further genres of english will be significantly enhanced by the use of this new tool, and hope to have practical evidence of this in the near future.",4,2006
N06-1024,"most importantly from the point of view of the authors, we have constructed a system that recovers sufficiently rich syntactic structure based on the penn treebank to provide rich syntactic guidance for the recovery of predicate-argument structure in the near future.",4,2006
N06-1025,"future work will include turning wikipedia into an ontology with well defined taxonomic relations, as well as exploring its usefulness of for other nlp applications.",4,2006
N06-1027,we are also exploring the application of graph-based algorithms to other structured-objects ranking problems in nlp so as to improve system performance while relieving human costs.,1,2006
N06-1027,the tradeoff and balance between system performance and human cost for different learning algorithms is of great interest.,1,2006
N06-1027,supervised learning is another approach to detecting conversation focus that might be explored.,1,2006
N06-1028,"furthermore we are aware that, maybe with the exception of the classes related to fluency, rate and length, our feature set is as of yet quite rudimentary and will need significant expansion in order to obtain a broader coverage of communicative competence.",6,2006
N06-1028,"in summary, future work will focus on improving speech recognition, and on significantly extending the feature sets in different categories.",1,2006
N06-1028,an important step for future work will be to train the acoustic and language models of the speech recognizer directly from our corpus; we are additionally planning to use automatic speaker adaptation and to evaluate its benefits.,1,2006
N06-1029,we also hope to integrate a richer contextual representation of tone and intonation consistent with phonetic theory within this unsupervised and semisupervised learning framework.,1,2006
N06-1029,"future work will consider a broader range of tone and intonation classification, including the richer tone set of cantonese as well as bantu family tone languages, where annotated data truly is very rare.",1,2006
N06-1029,we will further explore improvements in classification accuracy based on increases in labeled and unlabeled training examples.,1,2006
N06-1032,"future work thus will concentrate on improvements of in-coverage translations e.g., by stochastic generation.",1,2006
N06-1032,"furthermore, we intend to apply our system to other language pairs and larger data sets.",2,2006
N06-1033,"we believe that extensions of our technique to more powerful models such as synchronous tree-adjoining grammar (shieber and schabes, 1990) is an interesting area for further work.",1,2006
N06-1034,in future work we will use such features in our prediction models.,1,2006
N06-1034,"finally, we are also annotating tutor and student dialogue acts and automating the tutor act annotations; when complete we can investigate their usefulness in our prediction models; dialogue acts have also been used in prior paradise applications (moller, 2005a).",2,2006
N06-1035,"in the short term, we are investigating whether other metrics such as entropy and confidence bounds can better indicate the usefulness of a feature.",1,2006
N06-1035,we are investigating how well an automated certainty and frustration detection algorithm will impact the % policy change.,1,2006
N06-1035,"with respect to future work, we are annotating more human-computer dialogue data and will triple the size of our test corpus allowing us to create more complicated states since more states will have been explored, and test out more complex tutor actions, such as when to give hints and restatements.",2,2006
N06-1036,we have not yet been able to combine the benefits of both an hbm and prosody information.,5,2006
N06-1037,"in the future, we would like to test our algorithm on the other version of the ace corpus and to develop fast algorithm (vishwanathan and smola, 2002) to speed up the training and testing process of convolution kernels.",3,2006
N06-1038,inference in these highly connected models will likely require approximate methods.,1,2006
N06-1038,"additionally, we wish to focus on extracting implicit relations, dealing more formally with the precision-recall trade-off inherent in applying noisy rules to improve extraction.",1,2006
N06-1038,"also, we plan to improve upon iterative database construction by performing joint inference among distant relations in an article.",1,2006
N06-1038,"in the future, we wish to explore extending this methods to larger datasets, where we expect relational patterns to be even more interesting.",2,2006
N06-1040,"more generally, the ideas and method for determining structural zeros (vs. sampling zeros) can be used in other contexts for a variety of other learning tasks.",4,2006
N06-1045,"we plan for our weighted determinization algorithm to be one component in a generally available tree automata package for intersection, composition, training, recognition, and generation of weighted and unweighted tree automata for research tasks such as the ones described above.",1,2006
N06-1046,"another important issue, not addressed in this work, is the interaction of our aggregation method with content selection and surface realization.",5,2006
N06-1046,an appealing future direction lies in integrating learning and inference in a unified global framework.,1,2006
N06-1047,there are likely to be differences regarding usefulness of certain features due to the icsi meetings being relatively unstructured and informal and the ami hub meetings being more structured with a higher information density.,6,2006
N06-1047,"finally, we will apply these methods to the ami corpus [18] and create summaries of comparable length for that meeting set.",2,2006
N06-1047,"of particular interest to us are features relating to speaker status, i.e. features that help us determine who is leading the meeting and who it is that others are deferring to.",5,2006
N06-1047,we would also like to more closely investigate the relationship between areas of high speaker activity and informative utterances.,5,2006
N06-1047,"in the immediate future, we will incorporate these features into a machine-learning framework, building support vector models trained on the extracted and non-extracted classes of the training set.",1,2006
N06-1050,"thus, bpref may offer a solution to the incompleteness problem and we intend to investigate its potential use in our future evaluations.",3,2006
N06-1050,"when finished, we hope our test collection will be a generally useful ir resource.",1,2006
N06-1050,"in particular, we expect the collection to be useful for experimentation with citation information, for which there is currently no existing test collection with the properties that ours offers.",3,2006
N06-1051,these matter any statistical inference on shallow pools.,6,2006
N06-1051,it is necessary to include other metasearch methods for further study.,1,2006
N06-1051,this will allow us to validate not only the impact of the metasearch training principle based on pairwise ranking error rloss but also the capacity of automatic feature selection of the two ranking algorithms used in this paper.,3,2006
N06-1051,"second, some topics might even have no relevant document in shallow pools.",6,2006
N06-1054,"our constraint relaxation method should be tested on problems other than semantic role labeling. the method should also be evaluated on a task with longer sequences: though the finite-state operations we use do scale up linearly with the sequence length, longer sequences have more chance of violating a global constraint somewhere in the sequence, requiring us to apply that constraint explicitly.",3,2006
N06-1055,"our results also show that the k-best propositions produced by the local classifier have a very high oracle score, which perhaps indicates a promising path that deserves further exploration, based on careful analysis of the errors.",5,2006
N06-1055,we intend to continue to experiment with new features and parameters for the reranking algorithm.,1,2006
N06-1056,"in the future, we would like to develop a word-based alignment model that is aware of the mrl syntax, so that better lexicons can be learned.",1,2006
N06-1057,we plan to use paraeval to investigate the impact of these changes on paraphrase quality under the assumption that better paraphrase collections lead to better summary evaluation results.,3,2006
N06-1057,"and with paraeval, it is possible for us to evaluate systems that do incorporate some level of abstraction, especially paraphrasing.",3,2006
N06-1058,"in the future, we would like to incorporate substitutions at the level of phrases and syntactic trees.",1,2006
N06-1059,"(2) tune the bayesian smoothing parameter p to further examine the effect of smoothing,",1,2006
N06-1059,(3) develop better content generation model and,1,2006
N06-1059,"with the initial success of this study, we would like to: (1) verify the results with other set of data, for example, duc 2003 data,",2,2006
N06-1059,(4) add synonym and paraphrase matching capability in the future .,1,2006
N06-1061,"possible applications include information retrieval, text clustering/classification and summarization.",4,2006
N06-1061,"based on the success of our model, we will investigate various graph based relationships for explaining semantic structure of text collections in the future.",1,2006
N06-1062,"future work will be the further development of also the grammatical morph-based language models and comparison of that to the current approach, as well as extending this evaluation work to new languages.",1,2006
N06-2002,our future research includes building a corpus navigation system to dynamically explore the full feature space.,1,2006
N06-2002,large scale experiments will be conducted in the near future to see if the success of the smaller experiments will carry over to a larger scale.,2,2006
N06-2002,using machine learning we will use information detected from translated sentences in order to decide what parts of the feature space are redundant and what parts must be explored and translated next.,1,2006
N06-2002,"additionally, we will change from using humans to write sentences and context fields to having them generated by using a natural language generation system (alvarez et al.2005).",1,2006
N06-2003,we are currently exploring ways of bootstrapping a model from a small amount of hand labeled data in combination with lexical cohesion (tuned for high precision and consequently low recall) and some reliable discourse markers.,1,2006
N06-2004,"in future work, we plan to explore distributional methods for modeling relatedness, as well as the use of text-based information to improve correlations with the human data, as judgments are situated in specific textual contexts.",1,2006
N06-2007,in the future we would like to investigate how to select more useful feature stream and whether feature selection method can improve the performance of our graph based semi-supervised relation extraction.,1,2006
N06-2008,further work is being carried out to extend the system to chinese and arabic.,2,2006
N06-2008,current research is aiming at improving the accuracy of the classifier by using the non-periodic components and improving the combined classification method.,1,2006
N06-2009,"in future work, we are interested in developing effective filtering techniques to reduce our candidate set to a small number of high precision paraphrases, in experimenting with state-of-the-art paraphrasers, and in using paraphrasing to improve the stability of the qa system.",1,2006
N06-2011,"to do this, we could use information from the eigenvalues or the distribution of points in the clusters.",1,2006
N06-2011,we would like to locate and extend this region for spectral clustering.,1,2006
N06-2011,"as future work, we would like to analyze the variation in performance as the amount of data increases.",3,2006
N06-2011,"also, it would be interesting to compare the clusters obtained with spectral clustering and the part of speech tags of the words in the same cluster, especially for languages such as english where good taggers are available.",1,2006
N06-2011,"finally, an important direction of research is in automatically selecting the number of clusters for the clustering algorithm.",1,2006
N06-2013,we plan to study additional variants that these results suggest may be helpful.,6,2006
N06-2013,"in particular, we plan to include more syntactic knowledge and investigate combination techniques at the sentence and subsentence levels.",1,2006
N06-2014,"in addition, it is important to assess the impact of semi-supervised training with recognizer output, where gains from using unlabeled data may be greater than with reference transcripts as in (hillard et al., 2003).",3,2006
N06-2015,"in intent and in many details, ontonotes is compatible with all these efforts, which may one day all participate in a larger multilingual corpus integration effort.",2,2006
N06-2016,"the challenges of clef cl-sr task will continue to expand in subsequent years as new collections are introduced (e.g., czech interviews in 2006).",5,2006
N06-2016,in future work we plan to investigate methods of removing or correcting some of the speech recognition errors in the asr transcripts using semantic coherence measures.,1,2006
N06-2016,in ongoing further work we are exploring the relationship between properties of the collection and the weighting schemes in order to better understand the underlying reasons for the demonstrated effectiveness of the mpc.ntn weighting scheme.,1,2006
N06-2017,we will also try to supplement m.nocb with other features of coherence to improve its performance.,1,2006
N06-2017,"in our future work, we intend to directly evaluate their method using a substantially large number of alternative orderings and m.nocb as the baseline.",3,2006
N06-2019,we also plan to evaluating the benefit of disfluency modeling in bleaching speech data for text-based machine translation.,3,2006
N06-2020,in the future we plan to investigate the influence of the context size on sense discrimination performance.,2,2006
N06-2021,"we will investigate other features to improve the classification results, such as name information, acoustic or prosodic features, and speaker clustering results (considering that the same speaker typically has the same role tag).",1,2006
N06-2021,"we plan to examine the effect of using speech recognition output, as well as automatic speaker segmentation and clustering results.",3,2006
N06-2021,analysis of difference news sources may also reveal some interesting findings.,2,2006
N06-2022,we will then be able to test whether the accuracies we achieve are sufficient and explore methods for improving them.,3,2006
N06-2022,"in future work, we plan to integrate these models in a dialogue system to adapt the system’s language generation;",1,2006
N06-2027,"future work includes an implementation of a system with full access for alternative devices, expansion of the underlying lexicon for hebrew generation, and adding voice output.",2,2006
N06-2028,"currently, we are performing more sophisticated experiments on different ways to exploit additional audiovisual cues.",1,2006
N06-2028,our next plan is to conduct an extensive comparison between glossex and the proposed scheme.,3,2006
N06-2028,there is also room for improving the calculation of the incentive values of keywords.,1,2006
N06-2029,further investigations should analyze the characteristics of the variant corpora in more detail and focus on the automatic identification of specific linguistic phenomena that could be helpful to measure how good an input sentence is covered by a specific model.,1,2006
N06-2030,"modern yi stands as a single, but questionable, counterexample to this observation, and for it to be visible in sproat’s grid (with writing systems arranged along only the diagonal), one would need an objective and verifiable means of discriminating between consonantal and vocalic scripts.",3,2006
N06-2031,future systems may also align their output with their recognition capabilities and actively align with the user to signal understanding.,1,2006
N06-2032,our further research will address the study of vocal cues to segmentation in arabic bn.,2,2006
N06-2034,"there are many future directions, which include 1) applying other machine learning methods,",1,2006
N06-2034,2) analyzing discourse relation categorization strategy,1,2006
N06-2034,3) including a longer context beyond two sentences.,2,2006
N06-2036,we intend to explore this possibility in future extensions of this work.,6,2006
N06-2037,we are currently exploring ways to use multiple bagged in-domain language models for the selection process.,1,2006
N06-2037,"instead of sequential scan of the corpus, we are exploring the use of rank-and-select methods to give a better search sequence.",1,2006
N06-2038,"to combine the strengths of different tagging strategies, ensemble meta-strategies utilizing the results of multiple strategies could be explored.",1,2006
N06-2038,future work will be to observe how well these results generalize in the context of other classifiers and other corpora.,4,2006
N06-2039,"in summary, we observe very interesting clusters of verbs which indeed require more in depth lexical semantic study as msa verbs in their own right",1,2006
N06-2040,"in the future, the generation procedure for our interactive agent will be further developed in areas such as spatial descriptions and surface realization.",4,2006
N06-2040,"we also plan to investigate whether different object types in the domain require differential processing, as prior work on spatial semantics would suggest.",1,2006
N06-2043,"our directions for future research include experimenting with other machine learning techniques, utilizing the newly-gained knowledge of the tickets’ sublanguage grammar, as well as testing sublanguage analysis technology on other types of field service reports.",1,2006
N06-2044,we also plan to investigate other approaches to strategy summarization.,1,2006
N06-2044,"finally, we will evaluate our approach against purely rl-based methods.",3,2006
N06-2044,"in future work, we intend to exploit this generalization feature further by developing systems that require much larger state representations.",1,2006
N06-2045,"there are many things that remained to be done with retriever, including extracting paragraphs from non-html documents, auto hyperlinking topics within retriever pages (as in wikipedia), finding more up-to-date sources for categorization, and verticalizing retriever page generation for different types of topics (e.g. treating movies differently than people and both differently than diseases).",1,2006
N06-3001,"in the next stage of my research, i will focus on integrating previous efforts into a complete multimodal model for structural event detection.",1,2006
N06-3001,my research is expected to support adding multimodal perception capabilities to current human communication systems that rely mostly on speech.,4,2006
N06-3001,i am also interested in investigating mutual impacts among the structural events.,5,2006
N06-3001,"for example, we will study sus and their relationship to floor control structure.",1,2006
N06-3001,"in my thesis, i hope to better understand the role that the non-verbal cues play in assisting structural event detection.",1,2006
N06-3001,"given progress in structural event detection in human communication, i also plan to utilize the detected structural events to further enhance meeting understanding.",1,2006
N06-3001,i will also investigate alternative integration architectures to the hmm shown in figure 2.,1,2006
N06-3001,"in particular, i will improve current gesture feature extraction, and expand the non-verbal features to include both eye gaze and body posture.",1,2006
N06-3001,"a particularly interesting task is to locate salient portions of a meeting from multimodal cues (chen, 2005) to summarize it.",1,2006
N06-3002,"it is hoped that this research will not only improve processing of spoken natural language, but also enhance our understanding of how speakers use gesture to structure their discourse.",4,2006
N06-3002,"in one case, we are looking for a similarity between the disfluency and the repair point; in the other case, we are looking for similarities across all disfluencies, or across all repair points.",1,2006
N06-3002,"using the subsection of the corpus in which no explanatory aids were provided, i will investigate how to assess the similarity of such dynamic gestures, in the hope that coreference resolution can still benefit from gestural cues in this more general case.",3,2006
N06-3002,disfluency repair is another plausible domain in which gesture might improve performance.,4,2006
N06-3002,"alternatively, gesture could play a pragmatic function, if there are characteristic gestures that indicate restarts or other repairs.",1,2006
N06-3003,we hope to explore the space between batch mode and a fully interactive system to discover the optimal setting which allows the system to only ask the user for further interaction when it cannot determine the appropriate refinement operation or when it would be impossible to correctly refine the grammar and the lexicon automatically.,1,2006
N06-3003,"however, if we can detect such rule dependencies before the refinement process, then we can try to find an optimal ranking, given the current set of cis, which should result in higher translation accuracy, as measured on a test set.",1,2006
N06-3003,"in an interactive mode, the system can use active learning to produce minimal pairs to further investigate which refinement operations are more robust, treating the bilingual speaker as an oracle.",1,2006
N06-3003,another interesting future direction is enhancing the rule refinement system to allow for further user interaction.,1,2006
N06-3005,"the unique sentence level perspective modeling can automatically identify sentences that are strongly representative of the perspective of interest, and we plan to manually evaluate their quality in the future work.",3,2006
N06-3007,our goal is to develop a multilevel measure of document similarity that will be helpful for summarization and information extraction.,1,2006
N06-3009,"in the future, templates that can match long contextual relations and coordinated nes may be applied to ner postprocessing.",4,2006
N06-3010,future work will focus on the interaction structure and construction and testing of the integrated system.,1,2006
N06-3010,through this we hope to improve information retrieval and human computer interaction.,1,2006
N06-4008,"future plans include adding more scalable embedding algorithms, and allowing other output formats.",1,2006
N07-1001,the prosody labels by themselves may or may not improve the translation accuracy but they provide a framework where one can obtain prosody labels in the target language from the speech signal rather than depending on a lexical prosody prediction module in the target language.,1,2007
N07-1002,"finally, prominence prediction classifiers need to be incorporated in a speech synthesis system and their performance should be gauged via listening experiments that test whether the incorporation of prominence leads to improvement in synthesis.",3,2007
N07-1002,thus in future work we plan to incorporate more acoustic and phonological features.,1,2007
N07-1003,"in our future work, we plan to examine initiative conflicts in face-to-face multi-party conversation, such as the icsi corpus (shriberg et al., 2004).",2,2007
N07-1003,"in our future work, we plan to build an initiative model to capture this negotiation process.",1,2007
N07-1004,another problem that has been pointed out in section 6 and in section 7 is the different functional roles of dm dialogue acts in current annotations.,5,2007
N07-1005,we are also planning to expand the method and improve the accuracy of the automatic sub-goal generation and determination of the rate of accomplishment of sub-goals.,1,2007
N07-1005,"in the near future, we would like to expand the test set to improve the upper bound obtained by our method.",2,2007
N07-1005,we believe that future research would allow us to develop high-quality mt systems by tuning the system parameters based on the automatic mt evaluation measures.,3,2007
N07-1005,further advanced generation and estimation would give us information about the erroneous parts of mt results and their quality.,3,2007
N07-1005,the sub-goals of a given sentence should be generated by considering the complexity of the sentence and the alignment information between the original source-language sentence and its translation.,1,2007
N07-1006,"by examining the contribution of each component metric, we find that metrics showing different properties of a sentence are more likely to make a good combined metric.",1,2007
N07-1007,we plan to extend and generalize the current approach to cover these phenomena in morphologically complex languages in general in the future.,1,2007
N07-1008,"many city names and dates in arabic can not be handled by such blocks and in future work we intend to investigate the utilization of more complex blocks as necessary. however, in future work we intend to investigate feature selection using the language model as a prior which should result in much smaller systems.",1,2007
N07-1011,"additionally, we are investigating more sophisticated inference algorithms that will reduce the greediness of the search procedures described here.",1,2007
N07-1011,future work will extend our approach to a wider variety of tasks.,4,2007
N07-1012,"in addition to using srms for retrieval, we are currently extending the ideas to provide field validation and suggestions for data entry and validation: the same ideas used to find documents with missing field values can also be used to suggest potential values for a field and to identify values that seem inappropriate.",1,2007
N07-1012,our work is continuing by exploring methods for handling fields with incorrect or corrupted values.,1,2007
N07-1012,"the challenge becomes more than just inferring what values might be there; it requires combining likely missing values with confidence in the values already present: if an audience field contains ’undergraduate’, it should be unlikely that ’k-6’ would be a plausible value, too.",5,2007
N07-1012,"we have also begun explorations toward using inferred values to help a user browse when starting from some structured information— e.g., given values for two fields, what values are probable for other fields.",1,2007
N07-1013,"in addition, we will explore the issue of parameter learning, and user feedback (e.g., “this item should be ranked higher.”).",5,2007
N07-1013,"we also plan to apply grasshopper to a variety of tasks, including information retrieval (for example ranking news articles on the same event as in google news, where many newspapers might use the same report and thus result in a lack of diversity), image collection summarization, and social network analysis for national security and business intelligence.",4,2007
N07-1013,"as future work, one direction is “partial absorption,” where at each absorbing state the random walk has an escape probability to continue the random walk instead of being absorbed.",1,2007
N07-1014,"for further work, an obvious step is to improve the word generator so that it produces morphologically more plausible sequences of letters and to intertwine both generators for the emergence of word categories.",1,2007
N07-1014,"furthermore, it is desirable to embed the random generator in models of communication where speakers parameterize language generation of hearers and to examine, which structures are evolutionary stable (see jäger, 2003).",1,2007
N07-1014,this would shed light on the interactions between different levels of human communication.,6,2007
N07-1015,"in our future work, we will study how to automatically conduct task-oriented feature search, feature pruning and feature weighting using statistical methods instead of heuristics. in the future, we will study the effectiveness of these global features.",1,2007
N07-1017,"in the future, we plan to compare lsa to other term similarity measures, to train the lsa model on large open domain corpora and to apply our technique to both generic and specific corpora in different domains.",4,2007
N07-1017,"we want also to increase the level of integration of the lsa technique in the espresso algorithm, by using lsa as an alternative reliability measure at each iteration.",1,2007
N07-1017,"we will also explore the domain restriction property of semantic domains to develop open domain ontology learning systems, as proposed in (gliozzo, 2006).",1,2007
N07-1017,the domain restriction hypothesis has potential to greatly impact many applications where matching textual expressions is a primary component.,5,2007
N07-1017,"it is our hope that by combining existing ranking strategies in applications such as information retrieval, question answering, information extraction and document classification, with knowledge of the coherence of the underlying text, one will see significant improvements in matching accuracy.",1,2007
N07-1021,the lesson from nlu and mt appears to be that higher quality results when the symbolic and statistical paradigms join forces.,6,2007
N07-1024,future work may explore alternative ways to combine these models to make better use of contextual information.,1,2007
N07-1025,"given that wikipedia is growing at a fast pace, the curve suggests that the accuracy of the word sense classifiers built on this data is likely to increase for future versions of wikipedia.",1,2007
N07-1025,another aspect we were interested in was the correlation in terms of sense coverage with respect to other sense annotated data currently available.,6,2007
N07-1028,"as part of future work, we plan to conduct a more elaborate study with more interaction strategies included.",1,2007
N07-1028,better techniques to select effective subqueries are also in the pipeline.,1,2007
N07-1028,"since we used mutual information as the basis for most of our subquery selection procedures, we could not consider sub-queries that comprised of a single term.",5,2007
N07-1028,we plan to address this issue too in future work.,6,2007
N07-1029,it would also be interesting to investigate how much different systems contribute to the overall gain achieved via system combination.,5,2007
N07-1029,the focus of the future work will be to address the individual issues in the combination methods mentioned above.,5,2007
N07-1030,"in future work, we will explore the use of global constraints, similar to those used by (barzilay and lapata, 2006) to improve both precision and recall.",1,2007
N07-1030,"we will also consider linguistic constraints (e.g., restrictions on pronouns) in order to improve precision.",2,2007
N07-1030,"for example, we expect transitivity constraints over coreference pairs, as well as constraints on the entire partition (e.g., the number of entities in the document), to help considerably.",1,2007
N07-1033,a natural approach would start with log-linear models in place of svms.,1,2007
N07-1033,"while our experiments in this paper used a discriminative svm, we plan to explore generative approaches.",1,2007
N07-1033,"we would like to explicitly model rationale annotation as a noisy process that reflects, imperfectly and incompletely, the annotator’s internal decision procedure.",1,2007
N07-1035,"as an extension of this work, we are currently investigating in more detail what makes some mdp’s reliable or unreliable for a certain data size (such as the case where baseline 2 does not converge but a more complicated model does, such as concept repetition).",1,2007
N07-1036,"during human machine conversation, how is eye gaze aligned with speech production?",5,2007
N07-1036,are there any other factors such as interface design and visual properties that will affect eye gaze behavior and therefore attention prediction?,5,2007
N07-1036,how reliable is eye gaze for attention prediction?,5,2007
N07-1036,the answers to these questions will affect how eye gaze should be appropriately modeled and used for language processing.,5,2007
N07-1037,we assumed that each word has a semantic orientation.,6,2007
N07-1037,"however, word senses and subjectivity have strong interaction (wiebe and mihalcea, 2006).the value of α must be properly set, because lower α can be better for the seed words added by the classifier, • to address word-segmentation problem discussed in section 5.3, we can utilize the fact that the heads of compound nouns often inherit the property determining the semantic orientation when combined with an adjective.• the semantic orientations of pairs consisting of a proper noun will be estimated from the named entity classes of the proper nouns such as person name and organization.",1,2007
N07-1038,we also plan to theoretically analyze the convergence properties of this and other joint perceptron algorithms.,3,2007
N07-1038,an avenue for future research is to consider the impact of additional rhetorical relations between aspects.,1,2007
N07-1039,"immediate future work includes extending the approach to include other types of appraisal expressions, such as where an attitude is expressed via a noun or a verb.",3,2007
N07-1039,"in this regard, we will be examining extension of existing methods for automatically building lexicons of positive/negative words (turney, 2002; esuli and sebastiani, 2005) to the more complex task of estimating also attitude type and force.",1,2007
N07-1039,"as well, a key problem is the fact that evaluative language is often context-dependent, and so proper interpretation must consider interactions between a given phrase and its larger textual context.",2,2007
N07-1040,"in particular, we plan to investigate the use of scientific attribution information for the citation function classification task.",1,2007
N07-1042,an interesting area of future work is the application of data mining to search for appropriate constraints to integrate into this model.,1,2007
N07-1043,"in future, we intend to apply the proposed method to automatically extract synonyms from the web.",4,2007
N07-1044,"future work must assess whether the models presented in this paper can be extended to alternative sense inventories (e.g., dictionary definitions) that may differ in granularity and structure.",3,2007
N07-1044,an important future direction lies in evaluating the disambiguation potential of our models across domains and languages.,3,2007
N07-1044,we will also experiment with a wider range of lexical association measures for quantifying the similarity of a word and its synonyms.,1,2007
N07-1045,we plan to explore the question of which inventory of near-synonyms or similar words is the most suitable for use in the intelligent thesaurus.,5,2007
N07-1045,"in case the target word selected by the writer has multiple senses, they could trigger several groups of near-synonyms.",6,2007
N07-1045,the system will decide which group represents the most likely senses by computing the semantic coherence scores averaged over the near-synonyms from each group.,6,2007
N07-1045,future work includes a word sense disambiguation module.,1,2007
N07-1047,"it has been reported that syllabification can potentially improve pronunciation performance in english (marchand and damper, 2005).",6,2007
N07-1047,we are investigating the possibility of integrating syllabification information into our system.,2,2007
N07-1047,"we plan to explore other sequence prediction approaches, such as discriminative training methods (collins, 2004), and sequence tagging with support vector machines (svm-hmm) (altun et al., 2003) to incorporate more features (context information) into the phoneme generation model.",1,2007
N07-1047,We are also interested in applying our approach to other related areas such as morphology and transliteration.,4,2007
N07-1048,"the flms might work well also for the other languages, and in fact, to do justice to the more advanced morph models from later versions of morfessor, flms or some other refined techniques may be necessary as a complement to the currently used standard n-grams.",1,2007
N07-1049,"as an issue for further investigation, we mention that in this framework, asin reranking, it is possible to exploit global features in the revision phase; e.g., semantic features such as those produced by named-entity detection systems.",1,2007
N07-1052,"in the future, we plan to investigate alternative objective functions and error-driven methods for learning heuristic bounds.",1,2007
N07-1054,"we would also like to experiment with combining the two approaches, i.e.by applying the syntactic heuristics to an instance set extracted using topic segmentation constraints.",1,2007
N07-1054,"while the topic-segmentation filtering approach achieves significant improvement and the best results overall, our analysis of the syntactic filtering approach indicates that refined heuristics and a larger set of parsed data can further improve those results.",2,2007
N07-1055,domains with more natural writing styles will make lexical prediction a much more difficult problem.,5,2007
N07-1055,"finding optimal orderings is a difficult task even for short documents, and will become exponentially more challenging in longer ones. multi-paragraph documents also pose a problem for the τ metric itself.",5,2007
N07-1055,"in documents with clear thematic divisions between their different sections, a good ordering metric should treat transposed paragraphs differently than transposed sentences.",1,2007
N07-1055,"on the other hand, the wider variety of grammatical constructions used may motivate more complex syntactic features, for instance as proposed by (siddharthan et al., 2004) in sentence clustering.",1,2007
N07-1056,"the algorithm has been tested on title generation, but the decoder is not specific to this task and can be applied to other generation and summarization applications.",4,2007
N07-1057,"for the former, search facilities could be built over the data that would allow linguists to find syntactically marked up data for a large variety of languages, and could even accommodate cross-linguistic comparisons and analyses.",1,2007
N07-1057,"for the latter, we could automatically discern grammars and transfer rules from the aligned and marked up data, where these computational artifacts could act as bootstraps for the development of additional tools and resources.",1,2007
N07-1058,"ongoing work aims to improve grammar-based readability by reducing noise in training data, automatically creating larger grammar feature sets, and applying more sophisticated modeling techniques.",1,2007
N07-1059,"with the data we have collected and labelled (and the effort is ongoing), it becomes feasible to examine the use of data-driven methods.",3,2007
N07-1059,"in future work, we plan to apply machine learning techniques to this problem.",1,2007
N07-1059,"we also plan to expand our application to many other domains appropriate for language learning, and test the effectiveness of the translation game as a means for language learning.",4,2007
N07-1060,"in future applications, we envision using our automated measure to allow a form of feedback for intelligent language tutors, so that the system can automatically adapt its behavior based on the student’s test responses.",4,2007
N07-1060,"looking beyond definition scoring, we believe automated methods for assessing word learning have great potential as a new scientific tool for language learning researchers, and as a key component of intelligent tutoring systems that can adapt to students.",3,2007
N07-1061,"in future work, we will further investigate the pivot strategies described in this paper to confirm that the phrase translation strategy is better than the sentence translation strategy in the intended situation as well as with the europarl corpus.",3,2007
N07-1062,it would be interesting to investigate if the described techniques and data structures are applicable for reducing the memory requirements of language models.,3,2007
N07-1063,we also hope to address the question of how much search error is tolerable to 3analysis of total lm calls made by each method (not presented here) shows the h.search makes significantly fewer (1/2) total lm calls than cp to achieve each model cost.,5,2007
N07-1063,we plan to evaluate the impact of these more powerful models in future work.,3,2007
N07-1064,"in such a setting, the phrase-based system becomes a sort of combination mt system.",1,2007
N07-1064,we intend to explore such alternatives in the near future as well.,6,2007
N07-1065,"first, we plan to move to a discriminative approach to combining scores and weighting unit features using a small labeled set.",1,2007
N07-1065,"secondly, we will look at incorporating units into the information retrieval process. making the information retrieval process aware of the desired answer types will be an important future direction of qa research.",1,2007
N07-1066,"as our current framework is based on the assumption that each answer is independent, we are building another probabilistic framework which does not require any independence assumption, and uses an undirected graphical model to estimate the joint probability of all answer candidates.",1,2007
N07-1066,we plan to improve our framework by adding regularization and selecting the final answers among candidates returned from all extractors.,1,2007
N07-1067,a major challenge is to ascertain whether the mention of the target is indeed involved in the recognized justice event.,5,2007
N07-1067,"but ultimately, we would like to adapt our system to arbitrary topic areas.",4,2007
N07-1067,"the first test would reasonably be conflict events, for which the ace program has training data.",3,2007
N07-1067,"in addition, we are working to produce answers using text generation, to bring more sophisticated summarization techniques to make a better presentation than an unordered list of sentences.",1,2007
N07-1067,"finally, we will look into applying the techniques used here on other topics.",4,2007
N07-1067,"within the gale program, we are limited to the defined corpus, but in the general case, we could add more varied resources.",2,2007
N07-1067,we are planning to obtain various pieces of information from additional secondary queries to the search engine.,1,2007
N07-1068,"in the future, we plan to integrate the other useful features in videos to support multi-model-based multimedia question answering.",4,2007
N07-1069,"finally, we plan to explore the effect of different thematic role groupings on system performance.",3,2007
N07-1069,"in future work, we will map the propbank-ed brown corpus to verbnet as well, which will allow much more thorough testing of our hypothesis.",3,2007
N07-1069,we will also examine back-off to verb class membership as a technique for improving performance on out of vocabulary verbs.,1,2007
N07-1071,future work in this direction includes further exploration of the appropriate inventory of semantic classes used as sp’s.,1,2007
N07-1072,"we believe this approach can generalize to other domains where phrases, sentences, or other short texts need to be compared.",1,2007
N07-2001,"in our future work, we are going to further investigate whether the trends shown in this paper generalize to on-line mdp policy learning.",4,2007
N07-2002,grammatical features with low precision and recall results (mass and pcomp) show that some more research should be carried out for finding relevant linguistic cues to be used as learning features.,1,2007
N07-2006,it would be interesting to compare the performance if the statistics is done using the metric-best path on a smaller amount of data to the performance if the statistics is done using the model-best path on a larger amount (as there is no reference translation necessary).,3,2007
N07-2006,it could also be beneficial to sequentially prune the phrase pairs and always re-calculate the statistics after removing a certain number of phrase pairs.,1,2007
N07-2007,we also plan to evaluate the influence of our alignment improvement on mt quality.,3,2007
N07-2007,we plan to use more sophisticated machine learning models such as support vector machines for combination and make use of more available parallel data.,1,2007
N07-2007,"in the future, we plan to extend our system with additional models at the phrase and multi-word levels for both alignment and alignment combination improvement.",1,2007
N07-2008,the answer to that question might have implications for the range of text types that ought to be used to comprehensively test parallel document identification systems.,5,2007
N07-2008,"for example, if there were no document in the language b set that shared more than the minimum number of unique words with the document da in language a, then the approach might return no parallel for that document.",6,2007
N07-2008,"a system relying on non-binary word similarity measures rather than on total identity of words would be more complex and slower, but also more robust across different domains of text.",6,2007
N07-2008,this could take the form of establishing a score or significance threshold.,1,2007
N07-2008,"however, it is also a limitation because many cross language cognates are not orthographically identical.",5,2007
N07-2008,"the exact matching of words is a critical feature of our approach, which enables it to perform quick comparisons of documents by representing them as sets of low-frequency words stored in hash tables.",5,2007
N07-2008,"second, it might be revealing to run further tests with this approach on other types of text than parliamentary proceedings.",4,2007
N07-2008,what types of text would require a more sophisticated approach?,5,2007
N07-2008,"first, the problem definition could be expanded to include cases where there is no valid parallel for a given language a document in the language b document set.",6,2007
N07-2009,we are also working on new best-first search generalizations of our depth-first search inference to improve decoding time.,1,2007
N07-2009,"we believe that, in addition to the finite-state transducer approach, a graphical model framework such as ours would be well suited for this scientific and engineering endeavor.",1,2007
N07-2009,"we also plan to design better approximate inference strategies for training highly connected graphs such as ibm models 3 and 4, and some novel extensions.",1,2007
N07-2009,"as there has been increased interest in end to-end task such as speech translation, dialog systems, and multilingual search, a new challenge is how best to combine the complex components of these systems into one framework.",5,2007
N07-2009,"in future work, we intend to implement phrase-based mt models.",1,2007
N07-2010,"in future work we will examine the effects of applying situated models of meaning to other tasks (e.g., machine translation).",4,2007
N07-2011,"these responses will be based both on our hypotheses about why uncertainty is significantly associated with these contexts, as well as on analyses of human tutor responses in these contexts, using our human tutoring corpus, which was collected with our first itspoke corpus using the same experimental procedure.",6,2007
N07-2011,"we plan to examine more contexts, such as a topic repetition variable that tracks similar questions about a topic (e.g. gravity) across dialogues.",2,2007
N07-2011,our next step will be to use the significant dependencies to develop system responses to uncertain answers in these contexts.,1,2007
N07-2011,"we also plan to investigate context dependencies for other affective states, such as student frustration.",1,2007
N07-2012,"present and future work includes evaluating the method as a component of a real-time dialog system, where its usefulness at decreasing waiting time can be tested.",3,2007
N07-2012,and finally we are experimenting with larger datasets.,2,2007
N07-2012,"we are also working on methods for feature selection and compression to obtain further speedup,",1,2007
N07-2013,"in addition, the syntactic features will be expanded to include frequent grammatical alternations such as active / passive.",1,2007
N07-2013,extensions of this method can potentially be used to determine if a given essay was written by a native or a non-native speaker.,1,2007
N07-2013,"in the immediate future, we plan to extend the set of features to include non-verbatim similarity, such as synonyms and words derived by lsa-type comparison (landauer et al.1998).",1,2007
N07-2014,"however, a large portion of the unknown words are in fact foreign words and names, and it is not clear whether the models learned handle such words well.",5,2007
N07-2017,"future work will investigate additional classifiers, classifier combination, and expanded training data.",1,2007
N07-2017,we are also interested in applying a language model to decode an alignment network that has been scored with our classifier.,1,2007
N07-2018,our future work will focus on more thorough evaluation of the algorithm and integrating it into a summarization system.,3,2007
N07-2019,"in future work, we plan to exploit other sources of metadata such as e-mails, as well as the structure of the meetings themselves.",2,2007
N07-2020,"continuing along these lines, we are currently creating two new tests.",1,2007
N07-2020,"we are constructing a new arabic dlpt-star test, tailoring the document selection more specifically for comprehension testing and ensuring texts and tasks are at the intended ilr levels.",3,2007
N07-2020,we intend for both of these tests to be available for a public machine translation evaluation to be conducted in 2007.,3,2007
N07-2020,we are also constructing a mandarin chinese test with similar design specifications.,3,2007
N07-2020,in future tests we wish to include level 1 documents and questions.,2,2007
N07-2022,"in future work, we could consider using various smt features (as would be required for a phrase-based smt system).",1,2007
N07-2024,our ultimate goal is to identify the errors in the non-native sentences and propose corrections.,6,2007
N07-2024,we also plan to explore techniques for combining large mt training corpora and smaller non-native training corpora.,1,2007
N07-2026,"in addition, we plan to test this framework using automatic speech recognition output, speaker segmentation, and soundbite segment detection.",3,2007
N07-2026,"our future work will focus on exploring more useful features, such as part-of-speech and semantic features.",1,2007
N07-2027,"in future work, we would like to improve individual components of icetagger and icemorphy, with the purpose of further increasing the tagging accuracy.",1,2007
N07-2030,"context-dependent stream weights can also model feature asynchrony to some extent, so that this approach not only improves automatic speech recognition, but might also be an interesting starting point for future work in speaker clustering, speaker identification, or other applications in speech analysis.",1,2007
N07-2031,"finally, while unification grammars are reversible for use in generation, good generation methods remain an open research problem.",5,2007
N07-2034,"in future work we intend to make a deeper study on the performance of the multi-target system as the amount of targets increase, since the amount of parameters to be estimated also increases.",1,2007
N07-2036,i plan to further validate the collection of agreed-upon certainty markers on a much larger dataset and by using the typology as input data to machine learning algorithms for certainty identification and extraction.,1,2007
N07-2036,a baseline for future attempts to improve the calibration of levels and their boundaries was established.,1,2007
N07-2036,"in the future studies, i intend to revise the number of the discrete categories on the epistemic continuum and further re-define certainty levels conceptually.",1,2007
N07-2037,our future work will be directed towards tight integration of all available information by predicting the entire mlpt (besides leaves).,1,2007
N07-2038,"the next step is hence to use the recorded data to train the simulator, and to then retrain the dm policy.",1,2007
N07-2039,"in future work, we plan to evaluate the effectiveness of the model for automatic new word acquisition in spoken dialogue systems.",3,2007
N07-2040,additional experiments would be conducted on a larger data set to extract more robust patterns.,2,2007
N07-2040,we plan to replicate the study with automatic error detection experiment.,1,2007
N07-2041,"we also plan to train a graphical model based on all extracted bp, pl and bpl relations to infer relations from multiple sentences and documents.",1,2007
N07-2041,"in future work, we plan to let the discriminative model take the output of our parser and refine our current results further.",1,2007
N07-2043,"finally, we intend to evaluate the benefit of having a human in the loop in the first few iterations to filter out patterns chosen by the system.",3,2007
N07-2043,we also plan to detail how the various features in our classification model contribute to ranking of candidate patterns.,1,2007
N07-2043,we plan to verify the strength of our approach evaluating against other ground truth data sets.,3,2007
N07-2043,an additional area of envisioned improvement regards the use of a random sub selection of negative candidate patterns as training samples to counteract the presence of sentence fragments among low ranking candidate patterns.,1,2007
N07-2044,we plan to improve word prediction and validate these results using aac users as future work.,3,2007
N07-2045,the success of using a state-of-the-art language model in determiner selection also suggests that one would be helpful in making other decisions in the surface realization stage of text generation.,1,2007
N07-2046,"it may thus be that today’s exciting emerging work in “unsolved” areas – semantics, reference, and learning – could come to play a key role in what is sometimes maligned as yesterday’s boring solved problem.",5,2007
N07-2047,"subset selection techniques could give a solution to this problem, of which we will leave the further exploration to future work.",1,2007
N07-2048,we also plan to appraise our proposal on other languages.,3,2007
N07-2048,"in the future, we will evaluate different choices of words for the sets of positive and negative reference words.",3,2007
N07-2049,"future work will investigate the pitch reset phenomenon in cantonese broadcast news, because cantonese is another major chinese dialect with more complicated tonal characteristics.",2,2007
N07-2049,we also plan to incorporate prosodic cues with lexical cues to further improve performance in chinese story segmentation.,1,2007
N07-2050,"directions for future work include learning the structure of mlns automatically and using mlns for information extraction and statistical relational learning (e.g., entity relation identification).",1,2007
N07-2051,"in addition, improving sub-sentence root finder and sentence root finder will also be considered in the future.",1,2007
N07-2052,"as the simple pl measure performs remarkably well, we should also consider computing sr based on the wikipedia article graph instead of the category graph.",1,2007
N07-2052,future research should focus on improving the strategies for combining complementary knowledge sources.,1,2007
N07-2052,we also need to evaluate a wider range of measures to validate our findings.,3,2007
N07-2055,we are currently preparing to deploy our design into full scale evaluation exercises.,3,2007
N07-3001,"still, a big challenge will be to work out how to deal with polysemic and homonymic words and words with medical and non-medical facets.",5,2007
N07-3002,"next, i am going to apply my approaches to parse other languages, such as czech, german, spanish and french, and analyze the performance of my parsers on these different languages.",4,2007
N07-3002,"first and most important, i plan to investigate new advanced machine learning methods (e.g., structured boosting or unsupervised / semi-supervised algorithms (xu et al., 2006)) and apply them to the dependency parsing problem generally, since the goal of my research is to learn natural language parsers in an elegant and principled manner.",1,2007
N07-3002,"furthermore, i plan to apply my parsers in other domains (e.g., biomedical data) (blitzer et al., 2006) besides treebank data, to investigate the effectiveness and generality of my approaches.",4,2007
N07-3003,"further work will then concentrate on making this information available to our coreference resolution system, e.g. semantic similarity computation.",2,2007
N07-3003,"finally, since wikipedia is available in many languages, we believe it is worth performing experiments in a multilingual setting.",3,2007
N07-3003,work in the near future will accordingly concentrate on automatically inducing the semantics of the relations between wikipedia categories.,1,2007
N07-3003,"we believe that our work opens up exciting new challenges for the ai and nlp research community, e.g. how to handle the noise included in such knowledge bases and how to fully structure the information given in the form of only partially structured text and relations between knowledge base entries.",5,2007
N07-3003,we aim in the future to induce an ontology from its collaboratively generated categorization graph.,1,2007
N07-3004,"i will build on this work in the coming months as i prepare for two evaluations: a study on the usability of natural language and graphical tools for navigating a knowledge base, and a task-based evaluation on labeled image retrieval.",3,2007
N07-3004,these evaluations should bring closure to the work as a contribution in the field of semantic analysis of text.,6,2007
N07-3005,"i will also describe evaluation criteria used to assess the quality of the automatic summaries, for example informativeness and readability.",3,2007
N07-3005,"next, i will make recommendations for the presentation of summarization evaluation results, based on the knowledge acquired from my analysis of 22 scientific papers, and from previous evaluation campaigns.",3,2007
N07-3005,"in the next couple of months, i plan to analyze evaluation methods identified in my corpora, for example comparing automatic summaries with gold standard or baseline summaries, and asking judges to give their opinion on the quality of automatic summaries.",3,2007
N07-3007,"beyond the simple enumeration of all the published systems, the aim is to create a categorization of dialogue systems according to the tasks they allow and to the type of knowledge they use independent of the used knowledge representation primitives (classes, relations and axioms).",1,2007
N07-3007,the next step to achieve the main goal of this work is to study the existing dialogue systems with emphasis on the performed tasks and the used knowledge sources.,1,2007
N07-3008,another application of the semantic frames i am interested in is prosody prediction.,4,2007
N07-3008,the project will be further developed by adding to the automatic import program rules discovered though the analysis of the mismatching cases.,1,2007
N07-3008,"within the institute of computer science, i have begin to work at a syntax-prosody interface for romanian based on fdg trees of sentences and other syntactical information to discover the phonological entities underlying the written text and the topic/focus articulation.",2,2007
N07-3008,the algorithm for finding sentence focus uses the semantic roles as a main component.,1,2007
N07-3009,i will investigate how combining evidence sources can increase their applicability to new content domains.,5,2007
N07-3009,"this will include, for example, understanding how (vocabulary independent) std systems can be paired with fixed vocabulary asr.",1,2007
N07-3009,"lastly, i will explore how these new evidence sources may themselves be improved.",1,2007
N07-3009,this will include utilizing temporal domain knowledge for classification and improving the robustness of phone based std systems.,1,2007
N07-3009,i will investigate multiple methods for combining the evidence presented by both std and classification systems with conventional asr output (transcripts or word lattices).,1,2007
N07-4002,the pilot will evaluate the tool’s effectiveness in terms of measurable learning gains in reading comprehension and english language skills.,3,2007
N07-4008,"in the future, we plan to incorporate additional web-based functionality, with the ultimate goal of creating a general-purpose dialog interface to web applications and services.",4,2007
N09-1001,"future work will explore other graph construction methods, such as the use of morphological relations as well as thesaurus and distributional similarity measures.",1,2009
N09-1001,We will also explore other semisupervised algorithms.,1,2009
N09-1004,"continue to build the knowledge base, enlarge the coverage and improve the system performance.",2,2009
N09-1004,"it is unlikely that a reader examines all surrounding words when determining the sense of a word, which calls for a smarter and more selective matching strategy than what we have tried in section 4.1; 3.",6,2009
N09-1004,test our wsd system on fine-grained semeval 2007 wsd task 17.,3,2009
N09-1004,wsd is often an unconscious process for human beings.,6,2009
N09-1004,the experiment results in section 4.2 clearly show that more word instances can improve the disambiguation accuracy and recall scores; 2.,2,2009
N09-1004,"although we only evaluated our approach with coarse-grained senses, our method can be directly applied to fine grained wsd without any modifications.",4,2009
N09-1005,"in future work, we would like to develop more powerful decipherment models and techniques, and we would like to harness the information available from a wide variety of monolingual resources, and use it to further narrow the gap between parallel-trained and non-parallel-trained approaches.",1,2009
N09-1006,we are working on creating a gold standard corpus of children鈥檚 transcripts annotated with pos tags.,2,2009
N09-1006,our current efforts are devoted to improving prediction accuracy by refining our feature set.,2,2009
N09-1006,this data set will help us improve accuracy on our pos based features.,2,2009
N09-1006,"we are also exploring the use of socio-demographic features such as the educational level of parents, the gender of children, and enrollment status on free lunch programs.",1,2009
N09-1007,"the latent variable model handles latent-dependencies naturally, and can be easily extended to other labeling tasks.",4,2009
N09-1007,"since the latent variable model allows a wide range of features, so the future work will consider how to integrate open resources into our system.",2,2009
N09-1010,a future challenge lies in incorporating constraints from additional languages even when parallel text is unavailable.,5,2009
N09-1012,we would also like to explore using our smoothing technique in other models such as hmms.,1,2009
N09-1012,future work includes using lexical information more deeply in the model by conditioning argument words and valence on the lexical head.,1,2009
N09-1012,"finally, we would like to learn the parts-of-speech in our dependency model from text and not rely on the gold-standard tags.",1,2009
N09-1012,we suspect that successfully doing so will require using much larger datasets.,2,2009
N09-1012,"for instance, we could do unsupervised hmm part-of-speech induction by smooth a tritag model with a bitag model.",1,2009
N09-1013,further translation experiments will be carried out.,1,2009
N09-1014,the latter could be quite easily incorporated into a string kernel or the related tree kernel similarity measure.,6,2009
N09-1014,more gains can be expected when using better domain knowledge in constructing the string kernels.,2,2009
N09-1014,"additionally, we will investigate the effectiveness of this approach on larger translation tasks.",3,2009
N09-1014,"future work will include testing different graph construction schemes, in particular better parameter optimization approaches and better string similarity measures.",1,2009
N09-1014,"this may include e.g. similarity measures that accommodate pos tags or morphological features, or comparisons of the syntax trees of parsed sentence.",2,2009
N09-1015,"the most straightforward one is to use our approaches with more different languages, such as chinese and arabic, and incompatible corpora, for example, different segments of europarl.",2,2009
N09-1015,the main focus of such experiments should be verifying the conclusions we had in this paper.,6,2009
N09-1016,"for languages where npi lists are not extensive, one could envision applying an iterative co-learning approach: use the newly-derived de operators to infer new npis, and then discover even more new de operators given the new npi list.",1,2009
N09-1016,"the prospect that our method might potentially eventually be refined in such a way so as to shed at least a little light on linguistic questions is a very appealing one, although we cannot be certain that any progress will be made on that front.",1,2009
N09-1016,"on the other hand, although the results are already quite good for english, it would be interesting to see what improvements could be gained by using more sophisticated syntactic information.",1,2009
N09-1017,"as described in section 5.2, many nominals do not have enough labeled training data to produce accurate argument models.",6,2009
N09-1017,"such inferences would help connect entities and events across sentences, providing a fuller interpretation of the text.",6,2009
N09-1017,the generalization procedures developed by gordon and swanson (2007) for propbank srl and pad麓o et al.(2008) for nombank srl might alleviate this problem.,1,2009
N09-1017,"additionally, instead of ignoring nominals with implicit arguments, we would prefer to identify the implicit arguments using information contained in the surrounding discourse.",1,2009
N09-1017,Our results also suggest interesting directions for future work.,6,2009
N09-1019,"how much improvement is possible in practice, and whether joint inference can also improve named-entity performance, remain interesting questions for future work.",1,2009
N09-1019,this suggests that it should indeed be possible to improve on their coreference results without using a supervised named-entity model.,1,2009
N09-1020,"however wordnet has a low coverage problem, i.e. there are some words in the data which do not exist in it.",5,2009
N09-1020,"one solution to this low coverage problem is to combine trees generated by the clustering algorithms mentioned in this paper and wordnet, which we leave as a future work.",1,2009
N09-1020,"an interesting avenue of research is to construct the vocabulary tree based on wordnet, as a way to inject independent prior knowledge into the model.",1,2009
N09-1021,"second, we might consider training the degradation model in a discriminative framework (e.g., training to optimize a measure that will penalize degradations which cause false alarms, even if they are good candidates from the perspective of mle).",1,2009
N09-1021,"one solution, for future work, might be to incorporate a false alarm model (e.g., down-weighting putative occurrences which look suspiciously like non-query words).",1,2009
N09-1021,we hope that the ideas presented in this paper will provide a solid foundation for this future work.,6,2009
N09-1022,"in the future, we also plan to investigate other methods, such as plsi (hofmann, 1999), to deal with data sparseness in computing semantic similarity.",1,2009
N09-1023,"our work raises three important questions: (1) to what extent is the single test collection that we have used representative of the broad range of exact chat applications?,",5,2009
N09-1023,"our work raises three important questions: (1) to what extent is the single test collection that we have used representative of the broad range of 鈥渢ext chat applications?, (2) to what extent do the measures we have reported correlate to effective performance of downstream tasks such as summarization or automated response?, and (3) can we re-conceptualize the formalized problem in a way that would result in greater inter-annotator agreement, and hence provide scope for further refinements in our technique.",5,2009
N09-1023,These problems will be the focus of our future work.,6,2009
N09-1024,"future directions include applying our model to other inflectional and agglutinative languages, modeling internal variations of morphemes, leveraging parallel data in multiple languages, and combining morphological segmentation with other nlp tasks, such as machine translation.",2,2009
N09-1027,in future work we plan to focus on methods to improve on the integration of the pgyn(d) feature during decoding and techniques that allow us consider more of the search space through less pruning.,1,2009
N09-1028,it would be very interesting to investigate ways to have efficient procedure for training em models and getting word alignments using word lattices on the source side of the parallel data.,1,2009
N09-1028,"along this line of research, we think some kind of tree-to-string model (liu et.al., 2006) could be interesting directions to pursue.",1,2009
N09-1028,"although there is clearly room for improvements, we also feel that using one reordering during training may not be good enough either.",1,2009
N09-1029,"we will investigate more linguistic ways to classify words in future work, especially on target language.",1,2009
N09-1029,"for example, using word hierarchical structures in wordnet (fellbaum, 1998) system provides more linguistic and semantic information than statistically-motivated classification tools.",1,2009
N09-1030,"much work remains in this new research area, including the creation of more types of features.",1,2009
N09-1030,"also, due to the difficulty in obtaining wish annotated training data, we plan to explore semisupervised learning for wish detection.",1,2009
N09-1032,"to further reduce the accuracy loss, we will explore informative sampling to capture fine-grained data difference in the domain transfer.",1,2009
N09-1033,it is our hope that most of the errors identified in this work could be automatically discovered without any manual judgments.,6,2009
N09-1033,a complimentary way to improve performance would be to investigate the addition of relevant candidate expansions that are not already in the initial expansion.,1,2009
N09-1033,we are currently investigating extensions to fmm that can efficiently add new candidate expansions to the set by computing the similarity between modified centroids and all terms occurring in a large body of text.,1,2009
N09-1033,we are also investigating ways to use the findings of this work to a priori remove ambiguous seed instances (or their ambiguous contexts) before running the initial expansion algorithm.,1,2009
N09-1035,"in the future, we plan to explore a hybrid approach, which would benefit from both the generality of linguistic principles and the smooth exception-handling of supervised techniques, in order to make best use of whatever data is available.",1,2009
N09-1037,"in the future, we would like to add other levels of annotation available in the onto notes corpus to our model, including word sense disambiguation and semantic role labeling.",1,2009
N09-1040,rather it may be necessary to model discourse connectors and lexical semantics explicitly.,1,2009
N09-1040,"it is interesting to consider how multi-scale segmentation might be extended to finer-grain segments, such as paragraphs.",1,2009
N09-1040,the development of more comprehensive bayesian models for discourse structure seems an exciting direction for future research.,5,2009
N09-1040,"the lexical counts at the paragraph level will be sparse, so lexical cohesion alone is unlikely to be sufficient.",6,2009
N09-1043,together with more elaborate confidence handling a system could quickly generate hypotheses and then refine the associated confidences over time.,1,2009
N09-1043,We will explore this in future work.,6,2009
N09-1044,"we also plan to explore the impact of integrating language modeling with search, and to examine the impact of these different language modeling approaches on performance of a trainable dialog manager that takes n-best output from the speech recognizer.",3,2009
N09-1044,"in future work, we will optimize the parameters in our algorithm for geo-centric lm computation and merging.",1,2009
N09-1045,"we also will develop a similar approach to handling dialectical arabic speech using the magead morphological analyzer (habash and rambow, 2006).",1,2009
N09-1045,"a larger goal is to employ the msa and dialectical phone recognizers to aid in spoken arabic dialect identification using phonotactic modeling (see (biadsy et al., 2009)).",1,2009
N09-1045,"in future work, we will address several issues which appear to hurt our recognition accuracy, such as handling the words that mada fails to analyze.",5,2009
N09-1046,"second, incorporating target language information into a segmentation model holds considerable promise for inducing more effective translation models that perform especially well for segmentation lattice inputs.",1,2009
N09-1047,"in future work, we plan to explore selection methods based on potential phrases, adaptive sampling using features other than decoder confidence and the use of features from confidence estimation in mt (ueffing and ney, 2007).",1,2009
N09-1050,"(3) we will add other standard varieties of english (such as british, canadian, australian, etc) to the training corpus for the reference pronunciation model as the current model is trained on primarily north american english (nae).",2,2009
N09-1050,"(2) we will investigate other aspects of pronunciation, e.g., consonant quality and word stress;",1,2009
N09-1050,"we plan to continue our research in the following directions: (1) we will improve the native speech norms for vowel durations, such as using the distribution of vowel durations rather than just the mean of durations in our feature computations;",1,2009
N09-1055,"additionally, we plan to exploit more information, such as background knowledge, to improve the performance.",2,2009
N09-1055,we will conduct deeper research in this area in future work.,6,2009
N09-1056,"given the diverse range of topics present in our dataset, addressing topic-dependency is also an interesting future research direction.",5,2009
N09-1056,"there are several interesting avenues for future research, including improving the current method for exploiting the site structure.",1,2009
N09-1057,"because we computed opus features for opinionated as well as non-evaluative language in our corpora, obtaining overall positive results, we believe these features may also improve conventional opinion labeling for subjective text.",1,2009
N09-1057,This will be investigated in future work.,6,2009
N09-1058,"we also believe that stream counts can be applied to other problems involving higher order lms such as speech recognition, information extraction, spelling correction and text generation.",5,2009
N09-1062,"while our results on the full treebank are well shy of the best available parsers, we have proposed a number of improvements to the model and the parsing algorithm that could lead to state-of-the art performance in the future.",6,2009
N09-1066,"creating readily consumable surveys is a hard task, especially when using only raw text and simple summarization techniques.",5,2009
N09-1066,"given the overlapping content of abstracts and citation texts, discovered in the current study, it is clear that redundancy detection will be an integral component of this future work.",1,2009
N09-1066,we next plan to generate surveys using both citation texts and abstracts together as input.,2,2009
N09-1066,therefore we intend to combine these summarization and bibliometric techniques with suitable visualization methods towards the creation of iterative technical survey tools systems that present surveys and bibliometric links in a visually convenient manner and which incorporate user feedback to produce even better surveys.,1,2009
N09-1068,"in the future we would like to see if the model could be adapted to improve performance on data from a new domain, potentially by using the top-level weights which should be less domain-dependent.",1,2009
N09-1070,"for the future work, we plan to investigate different weighting algorithms for the graph-based approach.",1,2009
N09-1070,we also need a better way to decide the number of keywords to generate instead of using a fixed number.,1,2009
N09-1070,"furthermore, since there are multiple speakers in the meeting domain, we plan to incorporate speaker information in various approaches.",1,2009
N09-1070,"more importantly, we will perform a more rigorous human evaluation, and also use extrinsic evaluation to see whether automatically generated keywords facilitate tasks such as information retrieval or meeting browsing.",3,2009
N09-1072,"therefore, a wider array of studies across populations and genres would be required before a more general theory of conversational styles is established.",5,2009
N09-1073,"more complicated methods for using both sets that also achieve linear complexity (perhaps with a smaller constant), or that achieve o(nlogn) complexity rather than o(nlog2n), may exist.",1,2009
N09-1075,"first, we make predictions for where local coherences should obtain for an arbitrary scfg, not just one particular class of sentences.",1,2009
N09-1075,such a hypothesis for how comprehends approximate the prior could be tested by manipulating the frequency of the relevant substrings in sentences with local coherences.,1,2009
N09-1075,"while it may be unlikely that they calculate these probabilities for sequences directly from their grammar as we do in this paper, there could be a number of ways to approximate this prior: for example, given a large enough corpus, these probabilities could be approximated for any string of words that appears sufficiently often by merely tracking the structures the string has each time it occurs.",4,2009
N09-1075,"interestingly, the fact that the prior is actually more difficult to compute than the posterior suggests that the only way it would be available more rapidly is if it is precomputed.",1,2009
N09-1075,another possibility relates to the problem of correlations between the different components of the prior and posterior vectors.,1,2009
N09-1075,"this allows the model to scale up for use with a broad coverage grammar and to make predictions for arbitrary sentences, which was not possible with a model such as tabor &amp; hutchins (2004).",4,2009
N09-1075,"for example, in our small grammar, whenever a root category begins, so does an s, an s-base, and an np-base.",6,2009
N09-2002,currently we use our bigram language model in a brute-force manner: in order to generate the ilp we evaluate the probability of all possible bigrams of english candidate tokens in advance.,5,2009
N09-2002,this leads us to wonder if there may be a way to provide tighter integration of program generation and solving.,5,2009
N09-2002,"one possible solution may be the use of so-called delayed column generation strategies which incrementally add parts of the objective function (and hence the language model), but only when required by the ilp solver.",5,2009
N09-2002,a further extension is to reformulate higher-level mt models (phrase- and syntax-based) within the ilp framework.,1,2009
N09-2002,these representations could be more desirable from a linguistic constraint perspective as the formulation of constraints may be more intuitive.,5,2009
N09-2002,the first issue is that the generation of the ilp programs can take a long time.,5,2009
N09-2002,related to this issue is how to tackle the incorporation of higher order language models.,5,2009
N09-2002,such an integration would avoid the need to query the models in advance for all possible model components the solver may require.,5,2009
N09-2002,these challenges raise some interesting research questions and practical issues one must consider when embarking on exact inference using ilp.,5,2009
N09-2006,"in the future, we plan to integrate domain specific heuristics via approximated derivatives of evaluation metrics or mixture of them to guide the optimizers move toward better solutions for simplex-downhill algorithms.",1,2009
N09-2007,"future work could address some limitations of the present study by using bidirectional translation models, considering other language families and source languages other than english, and applying query expansion techniques.",1,2009
N09-2009,"future research is proposed to increase the method recall via broader coverage lexical reference resources, and to improve its precision through better context models than lsa, which was found rather noisy for quite a few categories.",1,2009
N09-2011,future plans include training svms on the results of the rule-based annotation.,1,2009
N09-2011,we also plan to involve two annotators in order to collect a more robust dataset based on interannotator agreement.,2,2009
N09-2011,further work is also needed in improving image marker referent identification and co-reference resolution.,1,2009
N09-2012,we also plan to use this corpus as test and training data for algorithms to automatically annotate such information.,4,2009
N09-2012,"though currently we are only annotating pursuit with location references, future plans include extending tesla to support the annotation of movement, orientation, and path descriptions.",1,2009
N09-2013,"another important step is to correlate the dialogue profile of each tutoring session, as revealed by the hmm, to learning and affective outcomes of the tutoring session.",1,2009
N09-2013,"in addition, leveraging knowledge of the task state as well as surface-level utterance content below the dialogue act level are promising directions for refining the descriptive and predictive power of these models.",1,2009
N09-2013,future evaluation of the hmm presented here will include comparison with other types of graphical models.,3,2009
N09-2014,"we are currently working on the extensions to the nlu model that will allow for the use of different types of context features, and investigating interesting ways in which agents can take advantage of early interpretations.",1,2009
N09-2017,"in the next step, we will focus on acquisition of adjectives.",1,2009
N09-2018,"however, we expect that the malay glosses will block readings of indonesian classifiers, and classifiers in other languages will require different strategies; we intend to examine this in future work.",1,2009
N09-2021,this will allow us to compare word-based detection to published syllable-based results.,3,2009
N09-2021,"in future work, we will explore a number of techniques to transfer word based predictions to syllables.",1,2009
N09-2022,"however, our current results show that future research on complex spoken dialog systems is enabled to exploit automatically generated frame semantics, which is our very direction.",1,2009
N09-2023,we can also discover hidden links in the graph by exploring new dialog flows with random walks.,1,2009
N09-2023,we can improve the clustering performance by using a distance metric learning algorithm to consider the correlation between features.,1,2009
N09-2023,there are several possible subjects for further research on our approach.,6,2009
N09-2024,we hope that a purely sentence-level processing might result in a more productive pair extraction in future.,1,2009
N09-2024,"this way, we will be able to maintain our search-driven extraction approach.",1,2009
N09-2024,"we are also re-implementing ir-based techniques to preselect translation pairs at the document-level, to gauge the effect of this additional filtering step.",1,2009
N09-2025,"we plan to extend our method to include more complex features, and apply it to structured output learning.",4,2009
N09-2027,"in the near future, we plan to experiment with prediction models of the speaker鈥檚 self reported level of certainty.",1,2009
N09-2028,in future work we will experiment with acoustic and prosodic features and detect disfluencies from the speech recognition output.,1,2009
N09-2031,we will try other thresholds and syntactic parsers to see their effects on dictionary extraction in the future.,1,2009
N09-2031,"besides, because the proposed approach is based on the syntactic analysis of sentences with no more than k words (see section 4.1), the parsing accuracy and the setting of threshold k will affect the correctness of dependency heterogeneity vector learning.",5,2009
N09-2031,"there are several future works under consideration including corpora cleaning, extending the proposed approach from single-noun dictionary extraction to multiword, and adapting the proposed approach to other language pairs.",2,2009
N09-2033,"particularly useful would be for the system to be able to provide feedback, including generating nprs; we have started investigating this reverse problem, of obtaining nprs from pronunciations, and are encouraged by the initial results.",1,2009
N09-2033,"moreover, it appears that novice users don't have much difficulty generating useful nprs on their own; we expect that their skill would increase with use.",4,2009
N09-2034,"furthermore, finding a way to calculate confidence scores of speech understanding results is on our agenda.",1,2009
N09-2034,we will conduct more experiments in other domains or with other resources to evaluate the effectiveness of our framework.,3,2009
N09-2034,we plan to investigate the case in which a smaller amount of the training data is used to estimate the coefficients of the logistic regressions.,2,2009
N09-2037,"in our ongoing work, we plan to conduct a similar error analysis for these problems in order to evaluate the generality of the findings reported here.",3,2009
N09-2038,"we wish to further investigate automatic adaptation based on implicit confidence scores, or even active participation of the user e.g.by marking bad utterance which could be excluded from the adaptation.",1,2009
N09-2039,we plan to increase the number of cases considered in the sample required to delimit the perplexity classes.,2,2009
N09-2039,the equation (2) may be developed further in order to obtain exactly the number of required cases for each perplexity class.,1,2009
N09-2040,"our goals for the future include further development of the answer credibility model to include not only terms from a question context, but terms that can be deduced to be in an answer context.",1,2009
N09-2041,"in future work, we plan to investigate the unexpected drop in hyper tagger performance on our ne-collapsed corpus, which we conjecture may be resolved by taking advantage of vadas and curran鈥檚 (2008) corrections to the ccgbank鈥檚 np structures.",2,2009
N09-2042,we believe this framework can be applied to any category of queries once a query classification and a score detector have been implemented.,4,2009
N09-2043,"in the future, we plan to incorporate our alignment-based soft pattern matching method into the tree kernel method for ie.",1,2009
N09-2044,"many open questions remain, including which other factors can or cannot be captured by our current feature set and classifier, and whether noisy label learning methods could address the problem of uncertainty in the labels for particular features and genres.",5,2009
N09-2045,"as future work, we will demonstrate the impact of our simplification method on other text mining tasks, such as relationship extraction.",4,2009
N09-2048,"we also plan additional experiments on adaptation in let鈥檚 go!, including an analysis of the time course of adaptation and further analyses of the impact of adaptation on asr performance.",3,2009
N09-2048,"in future work, we plan to confirm these results using transcribed data.",2,2009
N09-2049,one solution might be to develop methods for detecting when the problem is in acoustics and to trust the language model more in these regions.,1,2009
N09-2053,"for future work, we intend to explore an approach to conducting cross-lingual event extraction and investigate whether the cross-lingual inference can bootstrap either side when running two language event extraction systems in parallel.",1,2009
N09-2054,"in the future work, we will investigate automatic data selection methods to choose materials that are most suitable for self-training and evaluate the effect of the amount of automatically labeled data.",1,2009
N09-2055,"a future improvement for this would be to integrate the process in the core of the translator鈥檚 workflow, so that on-the-fly evaluation can be made.",3,2009
N09-2055,"in addition, several possible sources of errors have been identified which will help develop future system enhancements.",1,2009
N09-2056,"in order to explore the question of pivot selection further and arrive at firmer conclusions, future work will have to investigate in detail what kind of features are important in selecting a pivot language for a given language pair.",1,2009
N09-2056,"in addition, concerning the question of how the pivot language selection criteria depends on the choice of the pivot translation method, future work will also have to investigate the effects of pivot language selection for the other pivot translation approaches described in section 2.",5,2009
N09-2056,"based on these findings, we plan to determine the contribution of different language characteristics on the system performance automatically to obtain useful indicators that could be used to train statistical classification models to predict the best pivot language for a new language pair and improve the usability of machine translation between under-resourced languages further.",1,2009
N09-2058,"in future work, we plan a human evaluation of our results to see if more features could lead to performance gains.",3,2009
N09-2059,"as future work, we want to compare this approach of estimating entropy with other methods for estimating sense distributions which do not require hand-labelled data or parallel texts.",3,2009
N09-2059,we wish to couple the confidence in the mfs with contextual evidence and investigate application on coarse-grained datasets.,2,2009
N09-2060,"some potential future directions that would make greater use of this flexibility include the following: a simple extension from sense-kernels to word-kernels involves adding word nodes to the wordnet graph, with an edge linking each word to each of its possible senses.",1,2009
N09-2060,"incorporating other wordnet relations such as meronymy and topicality gives a way of kernelizing semantic association or relatedness; one application of this might be in developing supervised methods for spelling correction (budanitsky and hirst, 2006).a wordnet graph can be augmented with information from other sources, such as links based on corpus-derived similarity.",1,2009
N09-2060,"alternatively, the graph-based kernel functions could be applied to graphs constructed from parsed corpora (minkov and cohen, 2008).",4,2009
N09-2061,future work includes further experiments with structured classification to treat the three classes appropriately.,3,2009
N09-2063,"our study can be extended via further experimentation with the methods we excluded in section 4.1, with other parsers, with other languages and with other zipfian cues of language (e.g.",3,2009
N09-2065,"in future work, we would like to experiment with larger datasets, include semantic features, and trial other learners amenable to structured learning tasks.",3,2009
N09-2066,we expect that further tuning of the method might help reduce these differences.,1,2009
N09-2067,"our future work involves handling audio in foreign languages, that is robust to both asr and machine translation noise.",2,2009
N09-2068,"in the future, other parametric forms can be utilized to better model the posterior score distributions.",1,2009
N09-2069,we plan to collect multiple abbreviations for reference.,2,2009
N09-2069,"after that we are going to combine the abbreviation modeling in the voice search system to alleviate the weakness of speech recognition for unknown abbreviation words, which are unlikely to be correctly recognized due to the out of vocabulary problem.",1,2009
N09-2071,"encouraged by the results of our experiments, we plan to explore other relevance metrics that can encode more sophisticated constraints such as the relative coherence of the terms within a query.",3,2009
N09-3001,"in future, we plan to improve (1) the entity alignment strategy, (2) the majority voting technique by setting a minimum threshold for the majority vote and better tie-breaking, and (3) the boosting algorithm to automatically optimize the parameters that have been manually set in this paper.",1,2009
N09-3001,another possible avenue for future work would be to test these combination techniques with other coreference resolution systems.,3,2009
N09-3003,"besides annotating and using more dialogue data as more people talk to our iqa system, we plan to implement a state-of-the-art topic-shift detection algorithm as proposed in (yang et al., 2006), training and testing it on our own fu q data.",1,2009
N09-3003,we then plan to build dedicated logistic regression models for the different sub-classes of topic continuation fu qs.,1,2009
N09-3003,"we will attempt to improve this system by adding action based features, and then extend it to distinguish three classes: topic shifts, (topic continuation) fu qs that are fully specified, and (topic continuation) context-dependent fu qs.",1,2009
N09-3003,"also, from comparing the different models, we are interested in studying the specific properties of different fu q types.",1,2009
N09-3003,"if each model uses a specific set of predictors, we hope to improve the overall rank of correct answers across the different classes of fu qs.",1,2009
N09-3007,further classification is possible with both manual and automatic methods by utilizing individual contextual features in the optimal model.,1,2009
N09-3008,"similarly, for building models from unaligned sequences, the addition of domain knowledge would likely prove beneficial.",1,2009
N09-3008,"since profile hmm training is highly sensitive to the choice of initial model, we would like to explore more informed methods of constructing the initial model.",1,2009
N09-3008,"we also plan to investigate better pseudo count methods, as well as the possibility of using n-grams as output symbols.",1,2009
N09-3010,"in the future, we will conduct an annotation experiment with real users to evaluate the usefulness of rationales in terms of clock time.",3,2009
N09-3011,our k-best framework can also be easily extended to cases where one name has multiple foreign translations that are equally likely.,4,2009
N09-3011,"in future work, we may imagine penalizing insertions and deletions higher than substitutions and other non-uniform schemes for better transliteration performance.",1,2009
N09-3013,"further work includes the improvement of the polarity classification component by using machine learning over annotated corpora and other techniques, such as anaphora resolution.",1,2009
N09-3013,"moreover, we plan to study the manner in which opinion sentences of blogs/bloggers can be coherently combined.",5,2009
N09-3014,"future work will have to concentrate on this aspect of sentence ordering, as it appears to coincide with the structure of the summaries for the duc 2005 dataset.",2,2009
N09-3015,"along this perspective, da recognition could serve also as a basis for conversational analysis aimed at improving a fine-grained opinion mining in dialogues.",1,2009
N09-3015,"regarding future developments, we will investigate how to include in the framework a wider context (e.g. the previous n utterances), and the introduction of new linguistic markers by enriching the preprocessing techniques.",1,2009
N09-3015,"in particular, it would be interesting to exploit the role of slanted or affective loaded lexicon to deal with the misclassification of opinions as statements.",2,2009
N09-3016,first is the minimum error rate training (mert) and the second is the decoding phase.,1,2009
N09-3016,"we also plan to introduce a decoding scheme similar to the substring based transducer (sherif and kondrak, 2007) to improve the usage of lower order language models.",1,2009
N09-5001,"in addition, we plan to automatically adjust cross-document event aggregation operations according to specific compression ratios provided by the users.",1,2009
N10-1001,"however in many interactions, the social signals are at least as important as the propositional content of the words (pentland, 2008); it is a major challenge to develop meeting interpretation components that can infer and take advantage of such social cues.",5,2010
N10-1001,it is a major challenge to develop algorithms that are unsupervised and adaptive to free us from the need to collect and annotate large amount of data each time we are interested in a new domain.,1,2010
N10-1003,it remains to be seen if a similar approach can be used in other cases where em converges to widely varying local maxima.,5,2010
N10-1004,"additionally, our model separates domain and syntax estimation, but a future direction is to learn these jointly.",1,2010
N10-1004,"applying our methods to other generative parsers (such as (collins, 1999; petrov and klein, 2007)) is trivial, but it is less clear how our methods can be applied to the discriminative reranker component of the two stage parser.",5,2010
N10-1004,2) application of the review summary generation approach in other domains and other languages;,4,2010
N10-1004,3) data collection on user engagement with our dialogue systems involving reviewsummary evaluation.,3,2010
N10-1006,"in our future work, we will investigate probability normalization methods and other techniques for term weighting to cope with these problems.",1,2010
N10-1008,future work will focus on: 1) applying the sentiment scoring model to noun/verb sentiment assessment; 2) application of the review summary generation approach in other domains and other languages; 3) data collection on user engagement with our dialogue systems involving review summary evaluation.,4,2010
N10-1009,further research will also explore using the extracted entities from advertisements to improve downstream sponsored search tasks.,1,2010
N10-1011,future improvements include developing a nonparametric version that jointly learns how many visual terms and topics are optimal. another extension concerns the creation of visual terms.,1,2010
N10-1011,"out with cognitive science, we hope that some of the work described here might be of relevance to more applied tasks such as thesaurus acquisition, word sense disambiguation, multimodal search, image retrieval, and summarization.",4,2010
N10-1016,we would also like to investigate new methods to incorporate automatically learned translation boundaries more efficiently into decoding in an attempt to further improve search in both speed and accuracy.,1,2010
N10-1016,"future work in this direction will involve trying different methods to define more informative translation boundaries, such as a boundary to begin/end a swapping.",1,2010
N10-1017,we intend also to apply htp to learning paraphrases in languages other than english and investigate the impact of the learned paraphrases on resource-sparse machine translation systems.,4,2010
N10-1017,"as future work, we plan to learn the distribution of weights on edges to phrase nodes and feature nodes automatically from data, rather than tuning them manually, and to develop a probabilistic model supporting htp.",1,2010
N10-1018,while it is quite possible that the article correction system presented here can be improved 鈥 we would like to explore improving the system by using a more sophisticated feature set 鈥 we believe that the performance gap due to the error driven training paradigms shown here will remain.,1,2010
N10-1018,"finally, while this study focused on the problem of correcting article mistakes, we plan to apply the proposed training paradigms to similar text correction problems.",4,2010
N10-1019,we believe that the logical next step is to combine more primary models in the meta-classifier.,1,2010
N10-1019,"candidates for additional primary models include (1) more classifiers trained either on different data sets or with a different classification algorithm, and (2) more language models, such as skip models or part-of-speech n-gram language models.",1,2010
N10-1020,"in the future, we wish to scale our models to the full corpus, and extend them with more complex notions of discourse, topic and community.",2,2010
N10-1020,"ultimately, we hope to put the learned conversation structure to use in the construction of a data-driven, conversational agent.",4,2010
N10-1022,we also plan to expand linear interpolation language model scheme to include history specific (context dependent) weights.,1,2010
N10-1022,"as a future work, we plan to use the proposed criterion for adapting log-linear models used in machine translation, conditional random fields (crf) and other applications.",1,2010
N10-1029,"it would therefore be interesting to conduct this study on a larger scale, using more general mwe definitions such as automatically learned collocations (smadja, 1993) or verb-noun constructions (diab and bhutada, 2009).",2,2010
N10-1030,"in the future, we will use an automatic word sense disambiguation (wsd) system to obtain word senses and study the function of wsd for srl.",1,2010
N10-1031,our extensions are not specific to hter tasks; improved alignments and additional features should improve performance on any task having sufficient tuning data.,4,2010
N10-1033,this suggests that faster composition algorithms that incorporate top down filtering may still be discovered.,1,2010
N10-1036,in the future we aim to set up an online news article analysis system and perform larger and regular utility evaluations.,3,2010
N10-1041,these results provide a starting point for future work in interactive summarization.,6,2010
N10-1042,"for our future work, we plan to investigate other adaption methods, and try to address some of the problems identified in our error analysis.",1,2010
N10-1043,future work in discourse analysis will need to develop better understanding of how the two types of coherence interact.,5,2010
N10-1050,"in the future, we hope that litgs will be a spring board towards full itgs, with more interesting nonterminal than the bitgs seen in the literature so far.",6,2010
N10-1051,one future experiment would be to see whether these results are reliable across other publications in the domain.,3,2010
N10-1051,"another set of experiments would be to determine the optimum number of annotators; we assumed three, but cross-domain results may be more stable with more annotators.",3,2010
N10-1052,using xx rules to model legitimate word insertions is a topic for future work.,1,2010
N10-1054,"future work will widen the study by (i) looking at a wider range of words and languages,",2,2010
N10-1054,and (iii) integrating unsupervised subjectivity features into cross-lingual lexical substitution.,1,2010
N10-1056,further evaluation could include comparison with machine-translation and paraphrasing algorithms.,3,2010
N10-1056,another idea would be to estimate simplification priors based on a model of inherent lexical complexity; some possible starting points are number of syllables (which is used in various readability formulae) or word length.,1,2010
N10-1056,it would be interesting to use our proposed estimates as initialization for em-style iterative re-estimation.,4,2010
N10-1057,"the next step is to explicitly identify undesired worker behavior, such as not editing the mt output at all, or using the human reference as is instead of editing the mt output.",6,2010
N10-1059,"in future, we will try more syntactic structure generalization strategies.",1,2010
N10-1062,in the near future we also hope to test the online em setup in an application setting such as a computer aided translation or crowdsourced generated streams via amazon鈥檚 mechanical turk.,3,2010
N10-1063,We plan to address this question in future work.,5,2010
N10-1068,"an extension of fsts along the lines of recursive transition networks may be appropriate, but we leave details for future work.",1,2010
N10-1070,"in future work, we would like to explore further enhancements to weighting in lda.",1,2010
N10-1070,there are many variants which can be considered: one example is the incorporation of word order and context through an n-gram model based on conditional probabilities.,1,2010
N10-1070,"we also aim to evaluate lda against lsi with a view to establishing whether one can be said to outperform the other consistently in terms of precision, with appropriate settings held constant.",3,2010
N10-1070,"finally, we would like to determine whether other techniques which have been shown to benefit lsi can also be usefully brought to bear in lda, just as we have shown here in the case of term weighting.",5,2010
N10-1071,"in future work, we would like to extend our approach to other models, e.g., sparse combinations of lexicalized features.",1,2010
N10-1072,"in the future, we will try to develop more sophisticated features in entity linking and design a typical learning to rank method for the entity linking task.",1,2010
N10-1076,"our framework is applicable for any sequence labeling task that can be done at either a word or a sub-word (e.g., character) level.",4,2010
N10-1076,"since obtaining data for these tasks is substantially more expensive, we hope to use active learning to obtain more data.",2,2010
N10-1076,"also, other approximate decoders used in pipeline approaches could be explored as alternatives to the one we used (e.g., finkel et al., 2006).",1,2010
N10-1076,"finally, for the sake of completeness, we note that more recent work has been done based on our baseline models that has emerged since the preparation of the current work, particularly zitouni et al.(2009) and mohamed et al.(2009).",1,2010
N10-1076,we wish to address any improvements captured by this more recent work such as the use of different data sets and addressing problems with the hamza to decrease error rates.,2,2010
N10-1076,segmentation and lemmatization are particularly promising tasks to which our approach could be applied.,4,2010
N10-1076,"additionally, we wish to include our model as a stage in a pipeline that segments, diacritizes, and labels morphemes.",1,2010
N10-1076,"In future work, we would like to consider using CRFs in place of MEMMs.",1,2010
N10-1077,such analysis will reveal whether an affix is more inclined towards joining or occurs freely more frequently.,5,2010
N10-1077,in future work bi-gram statistics can be used to merge morphemes.,1,2010
N10-1077,more data can be tagged to find out joining probabilities for the affixes that occur as free morpheme.,2,2010
N10-1077,similarly a corpus can be tagged on compounds.,2,2010
N10-1078,we believe that there is ample opportunity to provide additional assistance and we will explore this in future work.,6,2010
N10-1080,we hope future work on developing new evaluation metrics will explicitly explore the translation quality of models trained to them.,3,2010
N10-1084,"another interesting question that we have not addressed is whether some languages are better suited to linguistic steganography than others, or whether some languages are better suited to particular linguistic transformations than others.",5,2010
N10-1084,how best to evaluate the imperceptibility of such a system we leave to future work.,3,2010
N10-1085,we intend to explore deriving such labels from resources such as wordnet or ontonotes.,2,2010
N10-1085,"finally (and perhaps most importantly), we expect that our model would benefit from additional training data, and plan to train on a larger, automatically-parsed corpus.",2,2010
N10-1085,we also plan to continue exploration of msa training methods.,1,2010
N10-1087,"it might be possible to discover some interesting regularities about the (preferential) uses of terms within semantic domains, as reflected in term network connectivity.",1,2010
N10-1087,"one direction of our ongoing work is to determine this distribution, and to empirically derive its parameters.",1,2010
N10-1088,there is a great deal of room for future work in expanding the ability of gloss extraction systems to extract sense glosses that more closely match the meanings of a word.,1,2010
N10-1088,"an important first step in this direction is to extract relations, rather than ngrams, that make up the definition a word鈥檚 senses.",1,2010
N10-1089,"furthermore, the evaluation results presented in this paper could be strengthened by adding manual multiword expression annotation to some treebank.",3,2010
N10-1089,"thus, unifying multiword expressions longer than two words would potentially contribute more to shallow parsing accuracy.",1,2010
N10-1089,future work will focus in conducting similar experiments for multiword expressions longer than two words.,3,2010
N10-1089,"one would expect that due to their size, a wrong interpretation of their structure would affect the shallow parser output more than it does for multiword expressions consisting of two words.",1,2010
N10-1092,"as future work, we plan to proove the correctness of the modified algorithm and to study the impact of these modifications on the use of sitgs for machine translation, and the estimation of sitgs.",1,2010
N10-1093,"experiments can be done in future, to find out if there is a label bias to the clause boundary, which also helps in reducing the search space for specific labels.",3,2010
N10-1094,"however, our hope, and expectation, is that the vast majority of real-life dtree queries will be local (parent,child,sister) searches on the derivation tree, since each node of the derivation tree already encodes small chunks of structure.",1,2010
N10-1097,"in future work, we plan to further investigate the connection between an initiation-response pairs from multiple dimensions, such as topical coherence, semantic relatedness, conversation acts, etc.",1,2010
N10-1097,"one important current direction is to develop a richer operationalization of the interaction that accounts for the way posts sometimes respond to a user, a collection of users, or a user鈥檚 posting history, rather than specific posts per se.",1,2010
N10-1099,"we plan to evaluate this method on additional data sets, and in the context of automated essay scoring.",3,2010
N10-1100,we are experimenting with using a front-end classifier for producing trending topics within distinct categories and then summarizing around these topics in order to generate an automated real-time newspaper.,3,2010
N10-1100,"presently, we are working on extending our pr algorithm to providing real-time summaries within specific topics.",1,2010
N10-1102,"options to explore include the use of language identification probabilities as features in the transliteration system (li et al., 2007), as well as splitting the data into sets that are not necessarily disjoint, allowing separate transliteration models to learn from potentially useful common information.",1,2010
N10-1102,"in the future, we plan to investigate other methods of incorporating language identification in machine transliteration.",1,2010
N10-1105,"in the future, we will investigate knowledge-richer methods for segmentation.",1,2010
N10-1105,"in particular, we will investigate whether an automatic vocalization step previous to segmentation will improve pos tagging accuracy for unknown words.",5,2010
N10-1107,the algorithm that builds the lexicon creates a more or less hierarchical structure subwords tend to be composed from those extracted at the previous iteration.,1,2010
N10-1107,"our method of subword extraction could also be applied to phrase extraction for machine translation, or in finding subwords for related problems like transliteration.",4,2010
N10-1108,"one way to extend this work is to use the automatically extracted phrase patterns as initial features, and then employ supervised or semi-supervised learning techniques to learn a more discriminative feature set.",1,2010
N10-1109,this technique may be applicable to classifying other phenomena.,4,2010
N10-1110,"in the future, we intend to examine techniques by which these representations could be used to further improve word recognition results.",3,2010
N10-1111,"additionally, various sensitivity, convergence, and robustness properties of the method need to be analyzed.",3,2010
N10-1111,In future work we will investigate our approach in such settings.,1,2010
N10-1112,future work might exploit approximate inference for more expressive cost functions.,1,2010
N10-1113,this corpus will be of use in future research on syntactic role preferences and for the training of monolingual subject-object disambiguates.,4,2010
N10-1115,"we hope that further work on this non-directional parsing framework will pave the way to better understanding of an interesting cognitive question: which kinds of parsing decisions are hard to make, and which linguistic constructs are hard to analyze?",5,2010
N10-1116,future work could explore unifying these techniques with other state-of-the-art approaches.,1,2010
N10-1117,"in future work we plan to apply relaxed marginal inference to larger joint inference problems within nlp, and test its effectiveness with other marginal inference algorithms as solvers in the inner loop.",4,2010
N10-1119,"a primary question is whether such lexicons improve performance over a translate-to-english strategy (banea et al., 2008).",5,2010
N10-1119,"in the future we plan to investigate the construction of web-derived lexicons for languages other than english, which is an active area of research (mihalcea et al., 2007; jijkoun and hofmann, 2009; rao and ravichandran, 2009).",2,2010
N10-1122,"so far, we focused on adjectives as sentiment indicators, however, there have been studies showing that other parts of speech can be very helpful for this task (e.g., pang et al.2002; benamara et al.2007).",1,2010
N10-1122,"also, it would be interesting to take a closer look at the interactions between aspect and sentiment, especially at a multiple-sentence level (see snyder and barzilay 2007).",1,2010
N10-1122,"finally, we feel that the true test of the usability of our system should be through an application, and intend to proceed in that direction.",3,2010
N10-1122,this work has opened many avenues for future research and improvements.,6,2010
N10-1123,"directions for future work include: leveraging additional joint-inference opportunities, better integration of syntactic parsing and event extraction, and applying this approach to other extraction tasks and domains.",4,2010
N10-1124,"in a similar perspective, we are continuing our efforts to construct a larger manually annotated collection of abstracts.",2,2010
N10-1124,"as the distribution in abstracts is not the same among pico elements, it is expected that differentiated weighting schemes could result in better retrieval effectiveness.",1,2010
N10-1124,we plan to extend the coverage of queries to other topics in the future.,4,2010
N10-1126,"again, the goal will be to model the most successful wizards, based upon data from recognition results, nlu, and voice search results.",1,2010
N10-1126,our next experiment will collect full dialogues with embedded wizards whose actions will again be restricted through an interface. we will select wizards who perform well during pilot tests.,3,2010
N10-1126,that wizard actions can be modeled using system features bodes well for future work.,1,2010
N10-1131,"in future work, we plan to support more complex transformations, instead of only removing words and experiment with different sizes of training data.",2,2010
N10-1132,"based on the email error analysis, we plan to pursue domain adaptation techniques to improve performance on different types of emails.",1,2010
N10-1132,future work will focus on the generation component and on applying the summarization system to conversations in other modalities such as blogs and instant messages.,4,2010
N10-1133,"in the future, we would like to apply a similar exhaustive search strategy, but this time with different compression ratios, in order to see the impact of compression ratios on the pdf of each domain.",1,2010
N10-1133,such an analysis would allow us to see whether these extracts exhibit certain properties which could be used in training machine learning systems.,1,2010
N10-1133,"furthermore, we would also like to analyze the high scoring extracts found by the exhaustive search, in terms of coherence, position and other features.",1,2010
N10-1135,"we see two main avenues for future work: (1), the construction of properly bilingual models where source language information can also help to further improve the target language model (diab and resnik, 2002); (2), the extension of our cross-lingual mapping for the argument position to mappings that hold across multiple predicates as well as argument dependent mappings like the spanish direct objects, whose realization depends on their animacy.",1,2010
N10-1137,we also aim to embed and test our role induction method within a full srl system that is also concerned with argument identification.,1,2010
N10-1137,"importantly, the framework can incorporate different probabilistic models for detection and canonicalization which we intend to explore in the future.",1,2010
N10-1137,"eventually, we also intend to replace the treebank-trained parser with a chunkier.",1,2010
N10-1138,"our system achieves improvements over the state of the art at each stage of processing and collectively, and is amenable to future extension.",4,2010
N10-1140,"in future work, we plan to extend the parameterization of phrase based lexicalized reordering models to be sensitive to these discontinuities, and we will also consider adding syntactic features to our models to penalize discontinuities that are not syntactically motivated (marton and resnik, 2008; chiang et al., 2009).",1,2010
N10-1144,we are also in the process of developing a framework within which we can use transfer rules and a bidirectional grammar to automate such complex syntactic reformulation.,1,2010
N10-2004,"we will continuously develop the tools by improving their functionalities through user-testing and feedback, and also by applying them to more languages.",1,2010
N10-2007,"investigating how to score these general aspects, and presenting this information in an intuitive way, are directions for our continued development of this tool.",5,2010
N10-2007,"based on our observations, however, the error analysis support could still be improved by directing users towards features that not only point to differences and similarities between different subsets of instances, but also to more information about how features are being used in the trained model.",1,2010
N10-2007,"this can be implemented either in algorithm-specific ways (such as displaying the weight of features in an svm model) or in more generalizable formats, for instance, through information gain.",1,2010
N10-2010,"future work includes improvements to the client side (e.g., confidence measures as a visual aid, multimodality), as well as exploring other kinds of parsing algorithms for the server side (e.g., adaptative parsing).",1,2010
N10-3005,"furthermore, we started with evaluation on the task which has been proposed for the evaluation of </bodytext> <page confidence=""0.992678""> 27 </page> <bodytext confidence=""0.999687428571429""> word space models at the level of word meaning.",3,2010
N10-3005,"next steps include, amongst others, evaluating the suggested model with a bigger data corpus as well as using stemming and more sophisticated filling of word matrices, e.g., by introducing advanced weighting schemes into the matrices instead of simple counts.",2,2010
N10-3005,"we need, however, to evaluate the model for the tasks where word order information matters more, e.g. on selectional preferences or paraphrasing.",3,2010
N10-3005,"last but not least, we plan to address the issue of modeling compositional meaning with matrix-based distributional model of meaning.",1,2010
N10-3007,"we are currently working on multiple extensions of this work, including investigating how the results can be applied to other corpora, adding additional features, and finally methods for post-processing extractive summaries.",2,2010
N10-3009,"opinion targets are more difficult to be identified than opinion holders, and deserve more attention in the nlp field, and we also would extend the targets to verb phrases and embedded clauses in addition to noun phrases.",1,2010
N10-3009,"the proposed approach is highly dependent on dependency parser, and we would like to further investigate machine learning approaches (including the crf model) by treating dependency structures as one of the linguistic features, which could be more robust to parsing errors.",1,2010
N10-3009,to explore the effectiveness of our approach with english data such as mpqa is another direction.,1,2010
N10-3010,we will also look into the linguistic clause based reordering features which would help in reordering of distant pair of languages.,1,2010
N10-3010,to further evaluate the approach we would also try the approach on some other distant language pairs.,3,2010
N10-3010,manual evaluation of the output will throw some light on the effectiveness of this system.,3,2010
N10-3011,"we are very excited about the future stages of this study, and its potential contribution to the linguistic perspective of the field of machine translation.",5,2010
N12-1001,"interesting future directions include exploring supervised narrative disentanglement, combining mnd with narrative induction (chambers and jurafsky, 2009) and applying mnd to non-fictional texts.",1,2012
N12-1002,in future work we will explore this direction and go more thoroughly into individual differences in entrainment behavior.,1,2012
N12-1005,"given that moses only implicitly uses ngram based information, adding soul translation models is expected to be even more helpful.",1,2012
N12-1005,"future work will thus aim at introducing them into conventional phrase-based systems, such as moses (koehn et al., 2007).",1,2012
N12-1006,the gains we observed from using msa morphological segmentation can be further increased with dialect-specific segmenters.,2,2012
N12-1006,topic adaptation is another important problem to tackle if the large msa linguistic resources already developed are to be leveraged for dialectal arabic-english mt.,5,2012
N12-1006,input preprocessing can also be used to decrease the noise of the user-generated data.,2,2012
N12-1009,the next direction in this research is to extract the scope of a given reference as a standalone grammatical sentence.,1,2012
N12-1009,"in many cases, the scope identified by our method can form a grammatical sentence with no or minimal postprocessing.",1,2012
N12-1009,"in other cases, more advanced text regeneration techniques are needed for scope extraction.",1,2012
N12-1011,"for contextual based tasks, the test takers are asked to read or listen to some stimulus material, and answer a question based on this information.",6,2012
N12-1011,we can build models using these materials to check the correctness and relatedness of the spoken responses.,1,2012
N12-1011,we will also explore generating other features measuring the higher-level aspects of the spoken responses.,1,2012
N12-1011,"for example, we can extract features assessing the responses’ relatedness to the stimulus of an opinion-based task.",1,2012
N12-1011,"we will also investigate whether the content features can provide additional information for automated speech scoring, and help build better scoring systems when they are combined with other non-content features, such as the features representing fluency, pronunciation, prosody, vocabulary diversity information.",5,2012
N12-1011,"in our future work, we will compare models trained on human transcripts and on asr outputs, and investigate whether we should use matching data for training and evaluation, or whether we should not introduce noise during training in order to maintain the validity of the models.",5,2012
N12-1014,"more generally, we offer our approach as an intriguing new tool to help semisupervised learning benefit from very large datasets.",4,2012
N12-1016,"in current and future work, we will show how s can be used to analyze hierarchical segmentations, and illustrate how to apply s to linear segmentations containing multiple boundary types.",4,2012
N12-1020,"future work will focus on parsing classical chinese poems of other poets, and on enriching the corpus with semantic information, which would facilitate not only deeper study of parallelism but also other topics such as imagery and metaphorical coherence (zhu and cui, 2010).",1,2012
N12-1024,"one avenue we are investigating is the use of a non-deterministic g, which would allow us to encode latent variables (dreyer et al., 2008), such as loosely defined “regions” within a string, and to allow for the encoding of alignments between the input strings.",1,2012
N12-1024,"we would also like to extend these methods to other combinatorial optimization problems involving strings, such as inference in graphical models over strings (dreyer and eisner, 2009).",4,2012
N12-1025,"for future work, we want to test the effect of improved tm in the context of different nlp applications such as mt and cross language ir.",3,2012
N12-1026,"in future work, we would like to investigate other objectives with a more direct task loss, such as maxmargin (taskar et al., 2004), risk (smith and eisner, 2006) or softmax-loss (gimpel and smith, 2010), and different regularizes, such as li-norm for a sparse solution.",1,2012
N12-1026,"instead of n-best approximations, we may directly employ forests for a better conditional log-likelihood estimation (li and eisner, 2009).",1,2012
N12-1026,"we would also like to explore other mixing strategies for parallel training which can directly minimize the training objectives like those proposed for a cutting-plane algorithm (franc and sonnenburg, 2008).",1,2012
N12-1026,"in future work, we would like to investigate other objectives with a more direct task loss, such as maxmargin (taskar , 2004), risk (smith and eisner, 2006) or softmax-loss (gimpel and smith, 2010), and different regularizers, such as li-norm for a sparse solution.",1,2012
N12-1031,"we note that this work also experimented with dependency parsing, and then automatically converting the results to ps, a further basis of comparison.",3,2012
N12-1031,"one other aspect of future work is to implement the algorithm in wang and zong (2010), using our own dependency representation, since this would allow a precise investigation of what the phrase structure parser is contributing as compared to our automatic conversion.",1,2012
N12-1031,we will also experiment with dependency parsing for the ptb dependency representation discussed in this paper.,3,2012
N12-1031,"following this, we will then experiment with parsing the arabic dependency representation, converting to phrase structure, and evaluating the resulting phrase structure representation as usual for parsing evaluation.",3,2012
N12-1031,"habash and roth (2009) discuss an already-existing dependency representation of parts of the atb and it will be interesting to compare the conversion accuracy using the different dependency representations, although we expect that there will not be any major differences in the representations.",1,2012
N12-1036,"as for the further work, we intend to evaluate and improve our system further in learner productivity in terms of output quality, typing speed, and the amount of using certain keys such as delete and backspace.",3,2012
N12-1039,"future work will consider unsupervised or semisupervised approaches to word origin recognition for this task, and methods to tune the smoothing weights a at the language rather than the global level.",1,2012
N12-1041,we plan to model interspeaker topics in the graph-based approach in the future.,1,2012
N12-1043,we would like to investigate in future work if the information provided by the two types of classes is indeed largely redundant or if a more sophisticated combination would perform better than the simple linear interpolation we have used here.,5,2012
N12-1045,the model can serve as a basis for several further extensions.,1,2012
N12-1046,we might also be able to productively group similar documents into clusters in which the vocabulary choices are (or should be) mutually reinforcing.,1,2012
N12-1047,"in the future, we intend to investigate improved sentence-bleu approximations to help narrow the gap between mira and the direct optimizers.",2,2012
N12-1048,"we are also exploring new algorithms for performing reordering aware incremental speech-to-speech translation, i.e., translating source phrases such that text-to-speech synthesis can be rendered incrementally.",1,2012
N12-1049,we hope to improve detection and explore system performance on multilingual and complex datasets in future work.,2,2012
N12-1051,"additionally, the taxonomies inferred with the hrg do not currently admit term ambiguity which we could remedy by modifying our technique for constructing a consensus hierarchy to reflect the sampled frequency of observed subtrees. a small number of items, consequently limiting the scope of any computational model based on normed data.",1,2012
N12-1051,"besides exploring the performance of our algorithm on more specialized domains (e.g., mathematics or geography) we would also like to create an incremental version that augments an existing taxonomy with missing information.",1,2012
N12-1051,Avenues for future work are many and varied.,6,2012
N12-1054,possible future work includes experiments using cascades to explore much higher-order models.,3,2012
N12-1056,"one future avenue is to explore the use of this structure in applications such as machine translation, as potentially enabling greater use of long distance dependencies than in prior work, such as by hasan et al.(2008).",4,2012
N12-1057,we intend to develop a better dialog act tagger which we can use to automatically obtain dialog act labels for odp classification.,1,2012
N12-1058,"in the near future, we will integrate a module which can resolve pronouns and deictics to textual antecedents, including type information provided by indefinite descriptions.",1,2012
N12-1058,this will make the system fully multi-modal.,1,2012
N12-1058,Additionally we intend to study issues of timing.,5,2012
N12-1059,the major open question is how our trait-based combination interacts with multi-system combination.,5,2012
N12-1059,or can you jointly combine all of the trait hypotheses and get an even greater relative gain?,5,2012
N12-1059,"if you independently improve all three using trait-based combination, will the relative gain from multi-system combination be reduced?",5,2012
N12-1063,future studies in early childhood readability need to take visual content into account.,6,2012
N12-1066,"future research on mixed-initiative user interfaces might try to detect and encourage these kinds of annotator behaviors, and potentially improve interactive machine learning outcomes.",6,2012
N12-1073,future work directions may include improving the detection algorithms by filtering the context sentences more intelligently.,1,2012
N12-1073,"context features may also be used for first filtering citations which have been mentioned only in passing, and then applying context based sentiment classification to the remaining significant citations.",1,2012
N12-1074,"these may include clique-specific language features, more properties of the user’s social network, mentions of named entities and topics of tweets.",1,2012
N12-1074,another direction is to distinguish between replies and retweets and to predict the number of responses and the length of conversations that a tweet may generate.,6,2012
N12-1074,"there is also potential in learning models for the prediction of other measures of impact, such as hashtag adoption and inclusion in “favorites” lists.",1,2012
N12-1074,the natural path for future work is to improve performance using new features.,1,2012
N12-1075,"looking forward, there are multiple directions to explore.",6,2012
N12-1077,"while this result is interesting and encouraging, it also raises several research questions, such as how to enhance the quality of each vector space model and whether the models can be combined more effectively3.",5,2012
N12-1077,"we also would like to study whether similar techniques can be useful when comparing longer text segments like phrases or sentences, with potential applications in paraphrase detection and recognizing textual entailment.",4,2012
N12-1081,"future work could test this claim, compare other graphical models, such as dynamic bns, and aim for a comprehensive human evaluation.",3,2012
N12-1087,"our technique is amenable to be combined with many existing variations of em (berg-kirkpatrick et al., 2010).",1,2012
N12-1087,We leave this as future work.,6,2012
N12-1088,"but there are alternative approximation techniques that scale well to large data sets (halko et al., 2009).",1,2012
N12-1089,"additionally, although this work focused solely on dialogue related features, future work may wish to take a closer look at the autocorrection mistakes themselves (e.g., which words are most likely to be mistakenly corrected, etc.).",1,2012
N12-1090,"second, we will perform an empirical comparison of two approaches to projecting coreference annotations, our translation-based approach and camargo de souza and orasan’s (2011) approach, where annotations are projected via a parallel corpus.",3,2012
N12-1090,"first, we will isolate the impact of each factor that adversely affects its performance, including errors in projection, translation, and coreference resolution in the resource-rich language.",2,2012
N12-1090,"fourth, since the success of our projection approach depends heavily on the accuracies of machine translation as well as coreference resolution in the source language, we will determine whether their accuracies can be improved via an ensemble approach, where we employ multiple mt engines and multiple coreference resolvers.",1,2012
N12-1090,"finally, we plan to employ our approach to alleviate the corpus-annotation bottleneck, specifically by using the annotated data it produces to augment the manual coreference annotations that capture the specific properties of the target language.",4,2012
N12-1090,"third, rather than translate from the target to the source language, we will examine whether it is better to translate all the coreference-annotated data available in the source language to the target language, and train a coreference model for the target language on the translated data.",2,2012
N12-1090,"to gain additional insights into our approach, we plan to pursue several directions.",1,2012
N12-1092,we additionally intend to cast the sentence selection problem as a separate learning problem that can also be trained from human judgments.,5,2012
N12-1092,"in our future work, we hope to expand the set of features as described in section 7.",1,2012
N12-1093,"we would also like to scale our model to more challenging domains (e.g., product descriptions) and to enrich our generator with some notion of discourse planning.",4,2012
N12-1093,an interesting question is how to extend the pcfg-based approach advocated here so as to capture discourse-level document structure.,5,2012
N12-1093,"in the future, we plan to remedy this by using forest reranking, a technique that approximately reranks a packed forest of exponentially many derivations (huang, 2008).",1,2012
N12-1095,"in doing so, we will develop a system that can automatically induce high quality, human-readable bilingual dictionaries from large corpora using unsupervised learning methods.",1,2012
N12-1095,"in future work, we hope to combine our clustering method with a system for automatically generating translation sets.",1,2012
N12-1096,we hope new opportunities will arise as this work explores a new research area for topic models.,1,2012
N12-1096,"for selectional preference, components could correspond to semantic features that intersect to define semantic classes (gormley et al., 2011).",1,2012
N12-2001,we will continue this line of work using both technical evaluation measures as well as user focused evaluations.,3,2012
N12-2001,"finally, our gold standard collection penalizes a data-driven approach, which might offer a broader range of experts.",1,2012
N12-2001,"consideration could be given to term dependence and positional models as in metzler and croft (2005), which might improve our proximity-based scoring function.",1,2012
N12-2001,"for example, modelling the user background and interests could increase the system’s effectiveness.",1,2012
N12-2001,some more realistic end-user studies could be used to evaluate the systems.,3,2012
N12-2001,There are a number of directions for future work.,6,2012
N12-2003,"other future work might apply subjectivity features to cluster adjectives into classes pertinent to ao, perhaps in combination with independent distributional measures of semantic similarity.",1,2012
N12-2003,"therefore, beyond the adjustments discussed above, the next stage in this research will evaluate the effects of combining semantic features with direct evidence in a single system.",1,2012
N12-2005,we will also put effort in the automatic extraction of symptom identification rules by analyzing the classification predictions and the corresponding document feature vectors.,1,2012
N12-2005,"as a follow up to this study we will try to generalise this algorithm to a more abstract level so that it can be transferable for the identification of other health conditions, medication etc.",1,2012
N12-2006,"however, we expect the results to generalize at least to similar tasks with other precision grammars, and probably treebank-derived parsers as well.",4,2012
N12-2006,exploration of how well these results hold for other tasks and for other types of parsers is an excellent subject for future research.,4,2012
N12-2007,"future work includes comparing our approach with esa, experimenting on more courses from more universities, and adapting our work to courses in other languages.",3,2012
N12-2010,future work will also include extensive evaluation of our proposed models.,3,2012
N12-2011,investigating how df can be extended and utilized in a multi-lingual environment is an interesting future direction.,5,2012
N12-2012,"we did not find any relevant correlation studies between age, origin, and other attributes with humor, but such research has likely been explored.",6,2012
N12-3001,"as future work we aim at allowing for the comparison at fragment level, where a fragment is considered a part of a function, a group of functions.",1,2012
N12-3001,we plan in the next future to extend its functionality to other common programming languages.,2,2012
N13-1001,it would also be interesting to study whether the insight of using minimal units for modeling and phrase-based search would hold for hierarchical smt.,5,2013
N13-1001,"in future work, we would like to study whether a phrase-based system like moses or phrasal can profit from an osm-style or n-gram style feature.",1,2013
N13-1001,vaswani et al.(2011) recently showed that a markov model over the derivation history of minimal rules can obtain the same translation quality as using grammars formed with composed rules.,1,2013
N13-1001,feng et al.(2010) previously showed that adding a linearized source-side language model in a phrase-based system helped.,1,2013
N13-1002,"we are incorporating the models into first pass decoding, in hopes of even greater gains.",1,2013
N13-1002,"in future work, perhaps we would see larger gains by including additional decomposition orders (e.g., top-down in a dependency tree), and taking this idea deeper into the machine translation model, down to the word-alignment and language-modeling levels.",1,2013
N13-1002,we were surprised to find n-best reranking so effective.,1,2013
N13-1003,"in the future, we would like to investigate how to incorporate useful future cost estimates for our sparse reordering features.",1,2013
N13-1003,"we would also like to investigate features inspired by transition-based parsing, such as features that look further down the reordering stack.",1,2013
N13-1005,"in future research, we plan to explore these issues in more depth to design a more general multi-faceted event recognition system, and we plan to investigate new ways to use these event dictionaries for event extraction as well.",1,2013
N13-1005,an open question for future work is to investigate whether the same multi-faceted approach to event recognition will work well for other types of events.,5,2013
N13-1005,"our belief is that many different types of events have characteristic agent terms, but additional types of facets will need to be defined to cover a broad array of event types.",1,2013
N13-1005,the syntactic constructions used to harvest dictionary items may also vary depending on the types of event information that must be learned.,1,2013
N13-1006,"the final ilp-based bilingual ner tagger with soft constraints is publicly available at: github.com/carfly/bi_ilp future work could apply the bilingual constraint based method to other tasks, such as part-of-speech tagging and relation extraction.",4,2013
N13-1008,in future work we also plan to integrate universal entity types and attributes into the model.,1,2013
N13-1010,"dlt, on the other hand, predicts that there is a storage cost for maintaining unresolved dependencies during a parse (gibson, 2000).",1,2013
N13-1010,future work could use these sequential cueing operations to investigate further claims of the dynamic recruitment hypothesis.,1,2013
N13-1010,"one of the implications of the hypothesis is that recruitment of resources alleviates the initial encoding cost, which allows the parser to continue on as before the embedding.",1,2013
N13-1013,"the models presented here are very simple, and in future work we would like to explore more complex approaches such as quasi-synchronous grammars (smith and eisner, 2009; li et al., 2012) or automatic treebank transformation (niu et al., 2009).",1,2013
N13-1015,"there are natural ways to extend the approach to semi-supervised learning; for example the svd step, where representations of outside and inside trees are learned, could be applied to unlabeled data parsed by a first pass parser.",1,2013
N13-1015,"there are a large number of parameters in the model, and we suspect that more sophisticated regularization methods than the smoothing method we have described may improve performance.",1,2013
N13-1016,"other possible extensions to this work include exploring alternative approaches to generating candidate images and developing techniques to automatically identify abstract topics for which suitable images are unlikely to be found, thereby avoiding the problem cases described in section 6.",1,2013
N13-1018,"incorporating a better source of prior knowledge in the generalization phase (e.g., yago or dbpedia) is also an interesting research direction towards a better phrase aggregation step.",2,2013
N13-1018,"on the other hand, we plan to apply a ranking strategy to select the top candidate phrases generated by our framework.",1,2013
N13-1019,"for example, in order to compare with sits, one can make an assumption that each document just has one speaker.",3,2013
N13-1019,"currently, our model uses marginal boundary probabilities to generate the final segmentation.",1,2013
N13-1019,"instead, we could develop a metropolis-hasting sampling algorithm to move one boundary at a time, given the gold-standard number of segments.",1,2013
N13-1019,"in future work, we would like to make the model fully nonparametric and investigate the effects of adding different cues in texts, such as cue phrases, pronoun usage, prosody, etc.",1,2013
N13-1021,"first, word alignment models can be extended to process asr lattices or word confusion networks as part of the unsupervised alignment learning algorithm, and incorporated into our approach.",1,2013
N13-1021,"second, the contextual features can be refined (e.g., concatenated features instead of smoothed features) when large amounts of training data is available.",1,2013
N13-1021,"In future work, we would like to investigate two questions left open by these results.",5,2013
N13-1023,"as part of future work, we plan to extend the framework presented in this work for performing speech-to-speech translation.",1,2013
N13-1023,we also plan to address the challenges involved in s2s translation across languages with very different word order.,5,2013
N13-1024,future work will concentrate on more elaborate selection functions as well as more sophisticated ways to combine the different resources.,1,2013
N13-1025,"in particular, we hope this framework will facilitate incorporation of richer linguistic knowledge into machine translation.",1,2013
N13-1027,"we also believe that there is much work to be carried out and that induction from summaries should be complemented with a process that explores full event reports, in order to reinforce some induced concepts, discard others, and discover additional ones.",1,2013
N13-1029,"in future work, we plan to work towards the development of an automatic stopping criterion, a more principled test for whether each successive iteration of label collapsing provides some useful benefit to the underlying grammar.",3,2013
N13-1030,"similar to the work of hasan et al.(2006) in the machine translation field, we plan to experiment with high order pos language models reranking.",3,2013
N13-1030,"in future work, we intend to examine how grammaticality of the generated compressions can be enhanced.",1,2013
N13-1031,our future aim is to add a semantic level to the annotation.,1,2013
N13-1032,"as future work, we would like to evaluate our algorithm on other language pairs.",3,2013
N13-1032,"also, it would be interesting to model the segmentation explicitly, where the aim would be to first segment the sentence and then use a two level hierarchical reordering model which first reorders these segments and then reorders the words within the segment.",1,2013
N13-1032,we also plan to integrate the score assigned by our model into the decoder to avoid having to do n decoding for every source sentence.,1,2013
N13-1033,we believe that explorations of modern parallel hardware architectures is a fertile area of research: the field has only begun to examine the possibilities and there remain many more interesting questions to tackle.,5,2013
N13-1033,"parallelism is critical not only from the perspective of building real-world applications, but for overcoming fundamental computational bottlenecks associated with models that researchers are developing today.",1,2013
N13-1036,we are interested in studying how our approach can be combined with solutions that simply add more dialectal training data since the two directions are complementary in that they address linguistic normalization and domain coverage.,5,2013
N13-1036,we also plan to automatically learn additional rules from limited available data (da-msa or da-english).,1,2013
N13-1036,"in the future, we plan to extend elissa’s coverage of phenomena in the handled dialects and to new dialects.",4,2013
N13-1036,"we also would like to do additional mt experiments where we use elissa to preprocess the training data, comparable to experiments done by sawaf (2010).",3,2013
N13-1038,"for future work, we would like to explore parallelized quadratic optimization and larger minibatch sizes, and eventually apply it to machine translation.",5,2013
N13-1041,"as future work, we would like to explore how to incorporate textual contents without opinionated expressions.",5,2013
N13-1041,one possible way is to consider the combination of matrix factorization and topic modeling as studied by wang and blei (2011) where we can use topic modeling to study textual contents.,1,2013
N13-1043,sentimental language is an unexplored avenue for improving natural language systems that operate in situated settings.,2,2013
N13-1044,"in the future, we intend to perform further feature engineering to improve the results of mada-arz, and extend the system to handle other das.",1,2013
N13-1045,it would also be worth investigating the performance of using our sentence-level model to re-rank n-best outputs of a confusion network model.,1,2013
N13-1045,there are several avenues for future work: we have focused on bigram dependencies in our models; extension to more than two dependent elementary trees is straightforward.,1,2013
N13-1046,we plan to expand the features and apply the classifier to new languages.,4,2013
N13-1046,and conduct mt experiments in domains other than news.,3,2013
N13-1048,"we will also explore the use of mrf-based translation models for translation systems that go beyond simple phrases, such as hierarchical phrase based systems (chiang 2005) and syntax-based systems (galley et al.2004).",1,2013
N13-1048,in future work we strive to fully realize the potential of the mrf model by developing features that can capture more sophisticated translation dependencies that those used in this study.,1,2013
N13-1050,"future work can investigate how to more tightly integrate our beam-search decoder for text normalization with a standard mt decoder, e.g., by using a lattice or an n-best list.",5,2013
N13-1053,"we have also identified other features that vary with the sentiment, such as first person singular use, although further work is required to determine if these differences may be exploited to improve deception detection performance.",5,2013
N13-1053,"indeed, future work may wish to jointly model sentiment and deception in order to better determine the effect each has on language use.",1,2013
N13-1055,"we also plan to extract multiword corrections for other types of errors and to examine the usefulness of including error contexts in our confusion distributions (e.g., preposition confusions following verbs versus those following nouns).",3,2013
N13-1055,"in future work, we will examine whether the results we obtain for english generalize to other wikipedia languages.",3,2013
N13-1058,this would help to ensure that no unintentional biases have skewed our results.,3,2013
N13-1058,"in future explorations of this topic, we would like to expand the size of the biographical corpus and reaffirm its correctness through the use of cross-validation between multiple annotators.",2,2013
N13-1058,"in addition, we would like to further investigate feature selection to find a best-case subset for performance on the bio corpus.",1,2013
N13-1060,"in the future work, we will extend the pas reordering model to include useful context, e.g., the head words and the syntactic categories of arguments.",1,2013
N13-1061,"for this area, there is some connection to the work of goldberg and elhadad (2010) and dickinson (2010), which are both concerned with examining dependency structures of more than one edge.",1,2013
N13-1061,one main area for future work is the application of this work to parser evaluation as well as iaa.,4,2013
N13-1061,"the connection is that those works are focused on dependency representations, and the kbm system does phrase structure analysis using a tag-like derivation tree, which strongly resembles a dependency tree (rambow and joshi, 1997).",1,2013
N13-1063,"in the future, we will research on task specific representations for sub-structures, such as phrases and sub-trees based on word embeddings and documents representations (xu et al., 2012).",1,2013
N13-1066,"In future work, we plan to extend our approach to other Arabic dialects.",4,2013
N13-1067,"for future work, we plan to use the output of this research in several applications such as predicting future prominence of publications, studying the dynamics of research, and designing more accurate bibliometric measures.",4,2013
N13-1072,"in the future we plan to address the related tasks of improving existing respellings, and assisting writers in creating respellings without direct access to the phonemic representations.",1,2013
N13-1078,"more work is clearly needed to improve evaluation: some of our seeds could fall into multiple stylistic categories, so a more detailed annotation would be useful.",2,2013
N13-1078,"beyond bayesian models, vector space and graphical approaches should be compared.",3,2013
N13-1081,"it will also require the creation of new training corpora and related resources, temporarily threatening comparability.",2,2013
N13-1081,one of the risks for the field in proceeding to investigations of how to deal with the question of met subjectivity is one familiar in natural language processing as a whole: there is a strong risk that these techniques will—initially and for a nontrivial quantity of time—cause the incremental performance gains in existing research to be lost or damaged.,5,2013
N13-1082,"in the future, we plan to explore more factors to better understand deletion behavior and regret, including users’ recent posts, historical behavior, and other statistics related to their specific social network.",5,2013
N13-1084,"in future work, we plan to use a development set to determine the optimal number of topical words to select during the topic estimate filtering stage of the pipeline in order to maintain improvements in precision without a loss in recall.",2,2013
N13-1084,"we would also like to investigate using part-of-speech, word sense, and parse information to improve our approaches for both semantic expansion and topic estimation.",1,2013
N13-1087,"as a future work, we will increase the size of the ood data and examine other methods like relative entropy based ood selection.",2,2013
N13-1088,in the future we aim to perform an in-depth analysis of clue words that aid humans in sense disambiguation.,1,2013
N13-1088,the distance of clue words from the target word and their and pattern of occurrence could give us significant insights into building a ‘discrimination net’.,1,2013
N13-1093,"in future, we would like to exploit the scope of more negation cues, apart from the three cues that are used in this study.",2,2013
N13-1094,future work will be aimed towards employing our hitting time based method in combination with a richer feature set.,1,2013
N13-1098,future work may also attempt to address these differences directly.,6,2013
N13-1098,"intentional wizard error could be introduced to frustrate the user into responding as she would to a less accurate system, analogous to intentional errors produced in user simulation in spoken dialogue systems (lee and eskenazi, 2012).",1,2013
N13-1100,"possible directions for future work include richer representations of discourse structure, and the combination of discourse-level and sentence-level valence and subjectivity shifters.",1,2013
N13-1104,"in future work, we would like to further investigate frame induction evaluation, particularly in evaluating event clustering.",3,2013
N13-1107,"for further work, we intend to improve open ie by tackling the conjunction and apposition structure problem.",1,2013
N13-1107,another direction will be to extract relation words for implicit relations.,1,2013
N13-1113,a clear direction for future research is the design of more fine-grained constraint taxonomies which can enable efficient usage of other constraint types and can result in further improvements in performance.,4,2013
N13-1118,one of our key future research objectives is to investigate the use and adaptation of the created conceptual graph to perform metaphor interpretation.,4,2013
N13-1118,"in addition, we plan to extend this work to cover nominal and adjectival metaphors, by harvesting salient nominal and adjectival features.",4,2013
N13-1119,this would enable to find out advantages and disadvantages of our three variants with respect to an application.,6,2013
N13-1119,"while we found it important to directly evaluate our lexical chaining algorithms on manually annotated data, a natural next step in this line of research is to use our lexical chaining methods as pre-processing steps for applications such as summarization, text segmentation or word sense disambiguation.",4,2013
N13-1120,"third, it is intriguing to see that the directional similarity model based on the rnnlm vectors performs strongly, even though the rnnlm training process is not related to the task of relational similarity.",1,2013
N13-1120,investigating how to select models to combine for each specific relation or relation group individually will be our next step for improving this work.,1,2013
N13-1120,"second, because the labeling process of relational similarity comparisons is inherently noisy, it is unrealistic to request a system to correlate human judgments perfectly.",5,2013
N13-1120,investigating the effects of different vector space models and proposing some theoretical justifications are certainly interesting research topics.,1,2013
N13-1120,conducting some user study to estimate the performance ceiling in each relation category may help us focus on the weaknesses of the final system to enhance it.,1,2013
N13-1120,"first, as shown in our experimental results, the model combination approach does not always outperform individual models.",1,2013
N13-1120,"finally, we would like to evaluate the utility our approach in other applications, such as the sat analogy problems proposed by turney (2006) and question answering.",3,2013
N13-1120,"In the future, we plan to pursue several research directions.",6,2013
N13-1122,there are plenty of research problems left to be addressed.,5,2013
N13-1122,developing a better algorithm for mining contextual words is an important research topic.,1,2013
N13-1122,it would also be interesting to design a method that jointly learns ner models and entity linking models.,1,2013
N13-1123,"we did not study how to summarize the discovered viewpoints in this work, which is also something we will look into in our future work.",5,2013
N13-1123,"in our further work, we plan to consider more accurate methods using deeper linguistic analysis.",1,2013
N13-1123,currently we use a simple heuristic-based classifier to predict interaction polarity.,1,2013
N13-1123,we will consider these strategies in our future work.,6,2013
N13-1125,"in the future, we intend to work toward resolving ecs to their antecedents when ec detection can be done with adequate accuracy.",5,2013
N13-1125,"we also plan to test our approach on the penn (english) treebank, with the first step being converting the penn treebank to a dependency representation with the ecs preserved.",3,2013
N13-1128,"we are also interested in discovery of less rigid dialogue goals, for example, a library patron who would be satisfied by an alternative book, and goals involving information aggregation where user utterances map to sophisticated queries.",1,2013
N13-1128,"questions that arise from this work include how to extend focus discovery and vocabulary selection to numerical databases, how to extract strategies for goals other than tuple-selection from a database, and how to automatically infer intelligible table and attribute labels.",5,2013
N13-1128,"explore more sophisticated models of user domain knowledge,",1,2013
N13-1128,"future work will also scale to mixed-initiative open dialogue management,",4,2013
N13-1128,we would like to investigate how optimal policies learned through reinforcement learning vary across domains.,1,2013
N13-1129,"in future work, we will explore additional machine learning models that leverage richer training data, and investigate further the combination of explicit and predictive techniques.",1,2013
N13-1130,"specifically, as future work, we plan to utilize the generated pseudo sense-tagged dataset to perform an in-depth study of different wsd paradigms.",2,2013
N13-1130,We also plan to extend our work to other part-of-speech tags.,1,2013
N13-1131,one major issue to be improved upon in future work is how named entities are handled.,5,2013
N13-1131,"going forward, an issue that needs to be addressed with this method is its dependence on knowing the set of possible languages a priori.",5,2013
N13-1131,"alternatively, the problem of language-independent named entity recognition has received some attention in the past (tjong kim sang and de meulder, 2003), and it may be beneficial to incorporate such a system in a robust word-level language identification system.",5,2013
N13-1131,"because we don’t see an easy way to adapt this method to accurately label words in documents from a possible set of thousands of languages when the document itself may only contain two or three languages, we would propose the following future work.",5,2013
N13-1131,we could simply choose not to evaluate a system on the named entity tokens in a document.,1,2013
N13-1131,"a straightforward way to approach this may be to create another label for named entities, which (for the purposes of evaluation) would be considered not to belong to any of the languages in the document.",1,2013
N13-1135,"we would like to explore other evaluation metrics (e.g., rouge-2, -su4, pyramid (nenkova et al., 2007)) and the human evaluation in future.",3,2013
N13-1135,we will also explore better ways of integrating the sub-event detection and summarization approaches.,1,2013
N13-1136,"we believe this research has applications to other areas of summarization such as update summarization and query based summarization, and we are interested in investigating these topics in future work.",4,2013
N13-1137,"for example, modeling the conditional probability of generating reference for a property vn given the previously generated context p(vn|v1 ... vn−1) may bring us closer to human-like output.",1,2013
N13-1137,we may also be able to build more sophisticated graphical models as larger corpora become available.,1,2013
N13-1137,"there are several additional issues that do not arise in this evaluation, but we expect must be accounted for when referring to naturalistic objects in improves performance.",5,2013
N13-1139,"for future work, we intend to apply the data set selection strategies to other nlp tasks, such as the optimal selection of sentences for tagging and parsing.",4,2013
N13-1140,"in future work, we plan to apply these techniques to languages such as kinyarwanda, a resource-poor but morphologically rich language spoken in rwanda.",4,2013
N13-2002,"at the same time, there are some unique challenges and opportunities that can be further investigated for our annotation scheme on unbalanced data.",5,2013
N13-2002,another open question is how the degree of unbalance between classes in the corpora affects overall annotation quality.,5,2013
N13-2002,"secondly, the features and machine learning algorithms used in semi-supervised annotation are also domain specific.",1,2013
N13-2002,"we suggest that if the data is not unbalanced, the total amount of effort that can be reduced will be lower.",2,2013
N13-2002,"for example, even though the cost matrix method can achieve a high recall for binary classification problem, whether it can be generalized to other tasks (e.g., multi-class classification tasks) is an unanswered question.",5,2013
N13-2004,we are considering the corpus based approach in our future work for the message generation and a more automated approach in knowledge collection.,1,2013
N13-2005,"for instance, since languages in the same family tend to share basic vocabulary, it may have some level of transfer to l2 that could be captured by a synonym-based classifier.",1,2013
N13-2005,"for instance, using wide age ranges as the broader class for classifying age of anonymous authors, or personality prototypes for personality type identification.",2,2013
N13-2005,"in addition, we can further explore the notion of increasing accuracy by applying knowledge of a broader class on the task applied in other stylometry based information extraction tasks.",4,2013
N13-2005,"first, trying new feature sets that may capture other similarities between languages in the same family.",1,2013
N13-2006,"instead of measuring similarity between the vectors directly using cosine, we will investigate the application of esa to calculate the similarities between short texts by taking their linguistic variations into account (aggarwal et al., 2012).",1,2013
N13-2007,"in the future, we plan to test our approach on other languages that have morphological characteristics similar to arabic.",3,2013
N13-2012,"in our influencing experiments, we will attempt to influence a user to speak in a way that will optimize asr performance simply by changing the system’s own voice.",3,2013
N13-2012,"we will build a framework for implementing the results of our studies of entrainment in human conversations into prediction models, which we hypothesize will improve their accuracy and can be used to improve a system’s performance.",1,2013
N15-1001,"for our model, the reconstruction objective can be easily combined with the likelihood objective, yielding a potentially powerful semi-supervised method.",1,2015
N15-1002,"future work should extend the problem formulation of predicate argument alignment to consider incremental linking: starting with a pair of documents, perform linking, and then continue to add in documents over time.",1,2015
N15-1003,"in future work, we can investigate more sophisticated methods of vector clustering (such as expectation maximization and non-negative matrix factorization), interactions with verb and noun frequency, and interactions with number of word senses from a task-general knowledge-base such as wordnet.",1,2015
N15-1003,"it would be especially useful to evaluate this system of a dataset of human judgements with verbs that systematically vary in polysemy, as this would more clearly expose the general trends we wish to model computationally.",1,2015
N15-1004,"additionally, incorporating such constraints into an interpretable model allows for a deeper exploration of performance in the context of evaluation tasks.",1,2015
N15-1006,"providing information about lexical category probabilities (auli and lopez, 2011) assigned by the super tagger can be useful during parsing.",1,2015
N15-1006,"ultimately, we intend to evaluate the impact of our incremental parser extrinsically in terms of language modeling for smt or asr.",3,2015
N15-1006,we would like to explore the limited use of a beam to handle lexical ambiguity by only keeping analyses derived from distinct lexical categories in the beam.,1,2015
N15-1007,the question is now to establish whether will this be verified in other semantic data sets?,5,2015
N15-1007,"from the parsing of deep syntax treebanks a la meaning text theory (ballesteros et al., 2014), to framenet semantic parsing (das et al., 2014) or data-driven approaches closer to ours (flanigan et al., 2014), it is difficult to know which models will predominate from this bubbling field and what kind of semantic data sets will benefit the most from syntax.",1,2015
N15-1007,"from the parsing of deep syntax treebanks a la meaning text theory (ballesteros , 2014), to framenet semantic parsing (das , 2014) or data-driven approaches closer to ours (flanigan , 2014), it is difficult to know which models will predominate from this bubbling field and what kind of semantic data sets will benefit the most from syntax.",4,2015
N15-1010,the fact that we identified significant differences in performance by selecting n-gram categories that are related to affixation in this poorly inflected language suggests that we may find even larger differences in performance in morphologically richer languages.,1,2015
N15-1012,"for future work, we will study the incorporation of large-scale language models, and the integration of morphology generation and linearization.",1,2015
N15-1013,"in particular, we are very interested to test our system’s performance on automatically generating a technical survey of a scientific paradigm, which thanks to the authors of (mohammad et al., 2009; qazvinian et al., 2013), has been established as a well-defined task with high-quality open data.",3,2015
N15-1013,we also plan to apply our method to the problem of multi-document summarization.,5,2015
N15-1013,"finally, while we have shown that our approach is effective in summarizing a scientific paper’s major contributions using its citation summary text, further experiments are required to test our method’s effectiveness on more generic summarization tasks and texts genres.",3,2015
N15-1013,"in the future, we plan to extend our approach to higher order n-grams and see whether larger information units (phrases) would help boost summarization performance.",1,2015
N15-1013,"finally, while we have shown that our approach is effective in summarising a scientific papers major contributions using its citation summary text, further experiments are required to test our methods effectiveness on more generic summarisation tasks and texts genres.",3,2015
N15-1014,"exploring more advanced features that manage to detect abstract semantic relationships or discourse flows in the compressed article.• complementing our system with a separate translation model capable of transforming to “headlinese” the titles generated with the language used in the bodies of articles.• attempting to achieve a more objective evaluation of our generated headlines, through the use of semantic-level measures.",1,2015
N15-1015,"additionally, we believe that combining visual and linguistic cues may help overcome longstanding challenges to language understanding, such as anaphora resolution and word sense disambiguation.",1,2015
N15-1015,"in the future, we hope to use this labeled corpus to train visual action detectors, which can then be combined with the existing visual object detectors to interpret novel videos.",2,2015
N15-1016,"their incremental nature makes them well-suited for cognitive simulations of grounded language acquisition, an avenue of research we plan to explore further.",4,2015
N15-1018,we hope this work will help computer scientists and social scientists engage in deeper conversations about research reproducibility for large-scale computer-assisted text analysis.,4,2015
N15-1019,"finally, additional research should quantify how responsive label regularization approaches are to the changing linguistic patterns common in online data.",1,2015
N15-1019,most pressing is the need to directly address the sampling bias created when constraints derived from the overall population are applied to online users.,1,2015
N15-1019,we plan to explore alternative optimization strategies to explicitly address this issue.,5,2015
N15-1020,future progress in this area will also greatly benefit from thorough study of automated evaluation metrics.,3,2015
N15-1020,we anticipate that there is much room for improvement if we employ more complex neural network models that take into account word order within the message and context utterances.,1,2015
N15-1020,direct generation from neural network models is an interesting and potentially promising next step.,1,2015
N15-1021,"in addition to improving this model, we are interested in developing systems that can select component words for portmanteaux and reconstruct component words from portmanteaux.",1,2015
N15-1021,we also plan to research other applications for multi-input/output models.,1,2015
N15-1022,future work involves developing text simplification techniques using the introduced datasets.,2,2015
N15-1022,"in addition, we plan to improve our current alignment technique with better text preprocessing (e.g., coreference resolution (hajishirzi et al., 2013)), learning similarities, as well as phrase alignment techniques to obtain better partial matches.",1,2015
N15-1024,future work should focus on additional methods for identifying relevant kb candidates given inaccurate transcriptions of mention strings.,1,2015
N15-1026,"for future work, we plan to explore other strategies and constraints for noise reduction in the document graph.",5,2015
N15-1028,"future work could compare dcca to other nonlinear approaches discussed in §5, compare different languages as multiview context, and extend to aligned phrase pairs, and to unaligned data.",3,2015
N15-1030,we will further explore the feasibility of using neural networks to resolve empty categories: to link ecs to their antecedents.,1,2015
N15-1031,"we leave for future work investigation of a version of the model that can ground language with raw(er) information from the world (e.g., vision information), eliminating the need to determine properties.",1,2015
N15-1033,"can we improve results by considering all languages available (lardilleux and lepage, 2009)?",5,2015
N15-1033,"can we relax this constraint and use comparable data, or apply mscfgs to pivot translation?",5,2015
N15-1033,"second, we are currently performing alignment independently for each target.",2,2015
N15-1034,"investigating the impact of richer feature sets, including a combination of phonotactic and morphological features, would be an excellent topic for future work.",1,2015
N15-1034,it would be interesting to extend this approach to a wider range of phonological processes in addition to the word-final /t/ and /d/ deletion studied here.,4,2015
N15-1034,"because this model enumerates the possible surface/underlying/context triples before beginning to search for potential surface and underlying words, its memory requirements would grow dramatically if the set of possible surface/underlying alternations were increased.",1,2015
N15-1036,we believe that typology provides important clues for long-term language change.,2,2015
N15-1036,"the currently available database only contains modern languages, but we expect that data of some ancestral languages could greatly facilitate computational approaches to diachronic linguistics.",2,2015
N15-1038,"we free the neural network from its complex hmm infrastructure, which we view as the first step towards the next wave of advances in speech recognition and language understanding.",1,2015
N15-1038,we hope the simplicity of our approach will facilitate future research in improving lvcsr with ctc-based systems and jointly training lvcsr systems for slu tasks.,4,2015
N15-1040,in future work we plan to continue to perfect our parser via improved learning and decoding techniques.,1,2015
N15-1043,"finally, an open question is whether it is possible to learn the latent domain alignment model in a fully unsupervised style.",5,2015
N15-1043,"another interesting direction might be to integrate our model into advanced mixing multiple translation models, improving smt systems trained on the heterogeneous data (razmara et al., 2012; sennrich et al., 2013; carpuat et al., 2014).",1,2015
N15-1043,"one obvious direction for future work might be to integrate the model into fertility-based alignment models (brown et al., 1993), as well as other recently advanced alignment frameworks, e.g., (simion et al., 2013; tamura et al., 2014; chang et al., 2014).",1,2015
N15-1043,this challenge deserves more attention in future work.,6,2015
N15-1045,"further, our results support the suggestion that in order for downstream applications to produce accurate results, in most cases it is necessary to take a broad view of the normalization task the looks beyond simple word replacements.",1,2015
N15-1046,"first, we hope to expand summaries, similarity judgments, and systems to several topics beyond gay marriage.",4,2015
N15-1046,"we believe, for example, that the features and the system we have trained for afs will apply to other domains without retraining, since none of the features are topic specific, but we have not shown that.",4,2015
N15-1046,"in addition, we aim to develop additional features and improve on the results reported here.",1,2015
N15-1046,"for example, we believe that it is possible that other off-the-shelf systems, such as for example one for sentence specificity (louis and nenkova, 2011; louis and nenkova, 2012), might possibly help with aspects of this task.",1,2015
N15-1046,"in addition, in future, we aim to automatically identify central propositions without the mediation of human summarizers and evaluators.",1,2015
N15-1046,"given the summaries that we have collected for each dialog, we plan to examine the relationship between the contributors to the related pyramid and the original source text, to determine whether indeed there are surface features of the source that would allow us to treat central proposition detection as an extractive task.",1,2015
N15-1046,"In future work, we aim to expand on this work in several ways.",1,2015
N15-1050,"this suggests that generating substitute vectors with better language models, such as neural language models, is a potential path to further improvements.",1,2015
N15-1051,"our future work would focus on performing a similar task on bigrams consisting of adverb-adjective pairs (e.g., somewhat unclear < quite hard < very difficult) that exhibit properties of gradability.",1,2015
N15-1052,"moreover, one oft-heralded advantage of the translation-as-annotation scheme (carpuat and wu, 2007) is that it naturally integrates into a machine translation framework, since one is learning to predict precisely what is necessary for successful translation; evaluating whether this hypothesis is true is currently an open question.",5,2015
N15-1054,"in future work, we plan to use information from entity linked documents to improve performance and also explore active leaning, and other humanin-the-loop methods to get more training data.",1,2015
N15-1058,"therefore we are exploring methods for performing discriminative optimization of weights assigned to views, for purposes of task-based customization of learned representations.",1,2015
N15-1058,"we believe that the results could be improved by (1) either using recent methods for handling missing values mentioned in footnote 1 or by using the heuristic count dependent non-linear weighting mentioned by pennington et al.(2014) and that sits well within our framework as exemplified in expression 12 (2) by using even more views, which look at the future words as well as views that contain pmi values.",1,2015
N15-1059,we also intend to use our approach on the task of multilingual word sense disambiguation.,4,2015
N15-1059,"as future work we plan to integrate nasari into babelnet and apply our representation to a multilingual setting, enabling the comparison of pairs of concepts across languages.",4,2015
N15-1060,"future work might usefully explore automated sentence quality estimation, as a component both of grammatical error correction systems and of their evaluation, in order to ameliorate the issue that any set of gold standard references will underspecify the set of possible corrections.",3,2015
N15-1061,a promising next step is to look to techniques from speech retrieval for insights that might be applicable to the zero-resource setting.,5,2015
N15-1061,one possibility in this regard is to explore extending the zero-resource term matching techniques to generate a lattice representation from which expected pseudo-term counts could be computed.,1,2015
N15-1062,can we automatically identify a donor language (or its phonological properties) for a borrowed word?,5,2015
N15-1062,"since languages may borrow from many sources, can jointly modeling this process lead to better performance?",5,2015
N15-1062,is it possible to monolingually identify borrowed words in a language?,5,2015
N15-1062,can we reduce the amount of language specific engineering required to deploy our model?,5,2015
N15-1062,can we integrate knowledge of borrowing in downstream nlp applications?,5,2015
N15-1062,we intend to address these questions in future work.,6,2015
N15-1063,"another promising direction is to encourage invertibility not only between words, but between their senses and synonyms.",1,2015
N15-1063,"as future work, we plan to apply mir on largescale mt decipherment (ravi and knight, 2011; dou and knight, 2013), where, so far, only a single directional model has been used.",1,2015
N15-1065,designing an overall model to harmonize such heterogeneous lexicons is an important issue.,1,2015
N15-1065,(i) paraphrase lexicons created by different methods and sources have different properties.,1,2015
N15-1065,"(iii) we will apply our method to various languages for demonstrating its applicability, extending it for a wider range of lexical variants depending on the targeted language.",4,2015
N15-1065,"we are therefore interested in determining to what extent our paraphrase lexicons can improve the performance of application tasks such as machine translation, text summarization, and text simplification.",5,2015
N15-1065,(ii) we aim to investigate an extensive collection of corpora: there are far more corpora than those we used in this experiment.,2,2015
N15-1065,(iv) paraphrases are the fundamental linguistic phenomena that affect a wide range of nlp tasks.,1,2015
N15-1067,"besides, taking into account the fast development of the word embedding research (mikolov et al., 2013; pennington et al., 2014), we will try different word embeddings.",1,2015
N15-1067,our future work is to exploit other existing supervised parsers that fit our framework.,1,2015
N15-1069,"by combining feature embeddings with metadata domain attributes, we can perform domain adaptation across a network of interrelated domains, distilling the domain-invariant essence of each feature to obtain more robust representations.",4,2015
N15-1070,"our current approach assumes a fixed ontology, but we hope to explore a more bi-directional relationship between ontology and vsm in future work.",1,2015
N15-1070,we also hope to extend our research to the multi-lingual domain.,4,2015
N15-1070,we are particularly excited by the idea of using multi-lingual wordnets to learn sense specific semantic vectors that generalize across languages.,2,2015
N15-1070,in particularly we envisage simultaneously incrementing ontologies with structure learning in addition to improving vsms.,1,2015
N15-1070,we propose to use sense-specific vectors as features in downstream applications such a word sense disambiguation.,1,2015
N15-1075,"in response to these observations, we would like to investigate more flexible representations, perhaps similar to those of botha and blunsom (2014), who use a linear combination of morpheme vectors to create representations that can generalize across words with similar forms.",1,2015
N15-1077,incorporating reasoning and other sources of indirect supervision.,2,2015
N15-1077,future directions include: pubmed-scale pathway extraction;,2,2015
N15-1077,learning vector-space representations for complex states;,1,2015
N15-1077,incorporating additional complex states to address syntax-semantics mismatch;,1,2015
N15-1085,"in order to fully support users in complex problem-solving dialogues, the field must move toward richer grounding of natural language utterances within complex artifacts across many domains.",4,2015
N15-1085,"additionally, generating specific and tailored dialogue feedback grounded in the artifact is a complementary area of research that holds the potential to increase the effectiveness of dialogue systems for supporting problem solving.",4,2015
N15-1085,it is hoped that this line of investigation will lead to dialogue systems that smoothly support a much broader range of human endeavors.,1,2015
N15-1086,"we consider all acquired relations equally salient, but future work will examine how to rank relation saliency.",5,2015
N15-1086,"our method generalizes to other non-factoid qa tasks which could usefully employ relations, such as arithmetic word problems (hosseini et al., 2014) and biology reading comprehension questions (berant et al., 2014).",4,2015
N15-1086,"however, our collected corpus of real human-system dialogs can be used to improve our system in further iterations.",1,2015
N15-1086,"we will also examine how dialog features can help distinguish between paraphrase, entailment, and negative relations.",1,2015
N15-1087,"in future work, it would useful to compare the interannotator agreement between trained human annotators to determine an upper bound for the accuracy.",3,2015
N15-1087,future work will look at adapting the standard algorithms to improve performance in the impaired case.,1,2015
N15-1090,"for future work, we would like to maximize other performance measures, such as area under the curve, for information extraction models.",1,2015
N15-1090,"furthermore, we would like to explore our approach for other latent variable models in nlp, such as those in machine translation.",1,2015
N15-1091,"in the future, we plan to apply bi-cnn-mi to sentence matching, question answering and other tasks.",4,2015
N15-1092,"beyond query classification and web search, we believe there are many other knowledge sources (e.g. sentiment, paraphrase) that can be incorporated either as classification or ranking tasks.",2,2015
N15-1092,a comprehensive exploration will be pursued as future work.,6,2015
N15-1093,"in the future, we would like to apply our method to noneuropean languages, with different morphological systems.",4,2015
N15-1093,"we also plan to investigate methods of extracting morphological tags from a corpus, including differentiating syncretic forms in context.",1,2015
N15-1094,"in future, instead of choosing λ, we plan to reduce λ as pep runs.",1,2015
N15-1094,"this serves to gradually refine the approximations, yielding an anytime algorithm whose beliefs approach the bp beliefs.",1,2015
N15-1094,we hope that the introduction of faster inference algorithms will increase the use of graphical models over strings.,1,2015
N15-1094,"as nlp turns its attention to lower-resource languages and social media, it is important to model the rich phonological, morphological, and orthographic processes that interrelate words.",1,2015
N15-1095,"in the future, we would like to further explore the idea of using interlingual representations for transliteration without parallel training data.",2,2015
N15-1096,"in the future we plan to completely automatize the process, by employing segmental durations obtained with signal based methods for speech segmentation.",1,2015
N15-1096,"finally, prosody was introduced here by way of a discrete symbol, forcing us to make a binary decision.",1,2015
N15-1096,"a more integrated model would enable to associate prosodic break with a probability distribution, over acoustic features, thereby achieving the joint learning of segmentation and prosody.",1,2015
N15-1097,future lines of research will explore the contributions that accounting for compatibility can make to these tasks.,5,2015
N15-1100,"as future work, we plan to extend our approaches to obtain word embeddings for other semantic relations (gao et al., 2014).",1,2015
N15-1101,"these positive results for small datasets suggest vectors derived from slower but more accurate analysis of these resources may be practical for lexical semantic applications, and we hope by providing this result, future researchers may be more aware of the viability of smaller-scale resources like simple english wikipedia (or presumably wikipedia in other languages which are substantially smaller in size than english wikipedia), that can still produce high quality vectors despite a much smaller size.",2,2015
N15-1102,"possible areas for future study include the use of discourse or and other contextual information to determine morphological agreement, application to other languages pairs/morphological agreement types, and learning the annotation rules from data.",4,2015
N15-1104,"nevertheless, locating word vectors on a hypersphere opens a door to study the properties of the word embedding in a space that is yet less known to us.",1,2015
N15-1109,"further technical improvements may be made by having the system automatically attempt to bootstrap the identification of spelling variants, a process that could complement our approach through an active learning setup.",1,2015
N15-1111,"as future work, it might be interesting to explore a more sophisticated model where the regression models in different layers are trained simultaneously by back-propagating the error of the upper-layer, as in neural networks.",1,2015
N15-1113,"finally, our long-term goal is to be able to generate loglines as well as movie plot summaries.",6,2015
N15-1113,"in the future, we plan to explore model performance in a wider range of movie genres as well as its applicability to other nlp tasks (e.g., book summarization or event extraction).",1,2015
N15-1113,we would also like to automatically determine the compression rate which should presumably vary according to the movie’s length and content.,1,2015
N15-1114,experiments show the approach to be promising and suggest directions for future research.,6,2015
N15-1115,"specifically, we plan to incorporate world knowledge into the framework of discourse role matrix (lin et al., 2011; feng et al., 2014).",1,2015
N15-1115,"in our future work, we wish to explore the effect of our world knowledge in conjunction with discourse relations.",1,2015
N15-1115,"in addition, we also plan to develop a more sophisticated feature encoding by distinguishing different types of predicates in world knowledge triples.",1,2015
N15-1116,"since the performance of our resolver is limited in part by the errors made by sinocoreferencer's subsystems, we plan to mitigate this problem by performing joint inference for entity coreference, event extraction and event coreference in future work.",1,2015
N15-1117,"systems should be able to distinguish who is likely to marry whom, identify the titles of books from roundabout descriptions, and intuit family relationships from raw text.",1,2015
N15-1117,the next challenge is to incorporate the necessary world knowledge to solve these harder coreference problems.,5,2015
N15-1118,"finally, we will investigate methods to automatically mine commonsense knowledge for injection into embeddings from additional resources such as probase (wu et al., 2012) or directly from text using a semantic parser (zettlemoyer and collins, 2005).",1,2015
N15-1119,"we have only applied a subset of amr representations to the el task, but we aim to explore how more amr knowledge can be used for other more challenging information extraction and knowledge base population tasks.",5,2015
N15-1122,"we will further address the application of semi-supervised variants of the proposed techniques (e.g., self-training) to other domains, where no sizable corpora of temporally aligned data can be found.",4,2015
N15-1122,"future work will address the extension of the feature set and model, and the application of this model to temporal semantics and planning tasks.",1,2015
N15-1123,"in future work, we would like to address efficiency, e.g.by investigating the possibility of incorporating an inverted index into online applications of forced decoding.",5,2015
N15-1125,does this multilingual leveraging help in a situation where we have large size corpora like europarl corpora? how much of an impact can treatment (morphological or syntactic) of the pivot language help in improving translation quality? can good reordering information be extracted by pivoting? can multi source and multi pivot setting further enhance quality? how can the noise induced by pivoting be controlled by methods other than probability cutoffs and finally? can simpler but more effective methods compared to triangulation be exploited in a multilingual scenario?,5,2015
N15-1125,how can one choose a set of good pivot languages amongst available choices?,5,2015
N15-1125,does this multilingual leveraging help in a situation where we have large size corpora like europarl corpora?c. how much of an impact can treatment (morphological or syntactic) of the pivot language help in improving translation quality?d. can good reordering information be extracted by pivoting?e. can multi source and multi pivot setting further enhance quality?f. how can the noise induced by pivoting be controlled by methods other than probability cutoffs?and finally g. can simpler but more effective methods compared to triangulation be exploited in a multilingual scenario?,5,2015
N15-1126,"in the future, we will aim to explore how to generate a trigger list for a “surprise” new fact type within limited time.",5,2015
N15-1129,"for future work, we plan to improve word alignment and translation quality in a more data restricted case where there are very weak source-pivot resources: for example, word alignment of malagasy english via french, using only a malagasy-french dictionary, or pashto-english via persian.",2,2015
N15-1130,we also plan on testing with different feature vector combinations.,3,2015
N15-1130,"in future, we plan to make a larger dataset of cognacy judgments for other language families in a richer phonetic transcription and integrate articulatory phonetic features into the feature vectors for the purpose of cognate identification.",2,2015
N15-1131,"in the future work, the self-contained knowledge, such as those identified key-phrases, and the external knowledge-base should be integrated to guide topic modeling.",1,2015
N15-1132,"also, we will explore this approach for other languages and for other parts-of-speech.",4,2015
N15-1132,"in future, we plan to improve on the performance of our model for english, even for infrequent words.",1,2015
N15-1134,we are currently exploring the possibility to use it in data-driven parsing.,5,2015
N15-1136,we will study semantic text similarity to improve the sentence similarity matrix.,1,2015
N15-1136,we will then apply the proposed method in query-based multi-document summarization.,4,2015
N15-1137,we also intend to perform task-based evaluation of the manually checked versus automatically generated lexicons.,3,2015
N15-1137,"we are continuing to improve the multilingual semantic taggers and extend them to cover more languages, such as spanish and dutch, aiming to develop a large-scale multilingual semantic annotation and analysis system.",4,2015
N15-1139,"future work will focus on improving svo extraction, especially adding consideration for negations of predicate verbs.",1,2015
N15-1139,"in addition we will analyze other hashtags in use in the trend and perform further analysis of the trend itself, implement advanced text normalization rather than relying on a dictionary, and determine the roles features from linked webpages and framenet or other semantic resources play in making sense of domestic abuse.",1,2015
N15-1140,we are interested in the application of our embeddings to morphological tagging and other tasks.,4,2015
N15-1140,future work will handle such integration of character-level features.,1,2015
N15-1141,our observations suggest that there is an increasing need for such integration in various domains.,4,2015
N15-1143,"· more accurately collecting physical objects, · sense disambiguation of words in clues, · use of superlative sentences, · filtering out descriptions of rare events, · a more effective way of using glosses, · application to other attributes, e.g., weight, · handling idioms.",1,2015
N15-1144,it would be interesting to see if we can apply this approach to other tasks which require generative modeling of textual observations such as language modeling and grammar induction.,4,2015
N15-1146,we believe that the corpus will be a valuable new resource for developing entity/event-level sentiment analysis systems to facilitate nlp applications such as automatic question answering.,2,2015
N15-1149,"for example, a good fit for our task of response-based learning for smt might be bannard and callison-burch (2005)’s approach to paraphrasing via pivoting on smt phrase tables.",1,2015
N15-1149,"in future work, we would like to investigate additional techniques for paraphrasing and synonym extension.",1,2015
N15-1150,"alternatively, we may try different weighting systems depending on whether a token is from the same speaker as the current utterance or a different speaker, since it would generally make more sense for a particular speaker not to repeat him/herself.",1,2015
N15-1150,an improvement is likely to come from attempting different methods to extract the core tokens from the past utterances.,1,2015
N15-1150,"in future work, we hope to look at different types of network information for label propagation, more precise propagation methods to deal with non-local interactions, and also efficient ways of utilizing both textual and network information in a joint model.",1,2015
N15-1154,"in future, we plan to do experiments in a cross domain setup and enhance our algorithm for domain adaptation paraphrase identification.",3,2015
N15-1156,hence our method should be easily extended to german.,4,2015
N15-1156,"other languages are also possible with language resources, in particular english.",2,2015
N15-1158,"further work could look at improving the similarity metric used, applying these sampling techniques to other streaming problems or adding a mention compression component.",1,2015
N15-1159,"in particular, (tang et al., 2014) showed that learning sentiment-specific word embeddings and using them as features can boost the accuracy of existing sentiment classifiers.",1,2015
N15-1159,"in the future, we plan to experiment with constructing ml lexicons from larger twitter corpora also using hashtags.",2,2015
N15-1160,nli is a young but rapidly growing field of research and this study is but a first step in shifting efforts towards a more interpretive approach to the task.,1,2015
N15-1160,we hope that the new dataset and directions presented here will galvanize future work.,4,2015
N15-1160,we believe this will motivate future work by equipping researchers with a large-scale corpus that is highly suitable for nli.,2,2015
N15-1160,"the next phase of this research will focus on developing tools to derive and browse ranked lists of the most discriminative cross-corpus features, which will then be used to formulate sla hypotheses.",1,2015
N15-1160,"subject to availability of data, this could be expanded to a multiple cross-corpus methodology, using three or more corpora.",1,2015
N15-1160,its application to other languages besides english is also of interest.,4,2015
N15-1161,"in addition, the availability of the automatically annotated disfluencies makes it possible to study the variation in rates for different cases and speakers over an extended time period.",5,2015
N15-1161,"possible extensions of this work include exploring graph-based semi-supervised approaches (e.g., (subramanya et al., 2010)) and combining the text-based approach with flexible asr forced alignment allowing optional insertion of filled pauses and words that are common as repetitions.",1,2015
N15-1163,"future work will focus on performance improvements, using the kernel on higher order parts, and integrating the kernel directly into a graph based dependency parser.",1,2015
N15-1164,"furthermore, since we now evaluated extrinsically, it would be useful to devise intrinsic sense-based evaluation schemes, e.g. a sense analogy task similar to the word analogy task used by mikolov et al.(2013).",3,2015
N15-1164,"in future work, we plan to investigate whether the sense vectors are useful for retrieving rarely occurring senses in corpora.",5,2015
N15-1165,"it would also be interesting to investigate the influence of the different semantic relations in wordnet, either by removing certain relations or by assigning different weights to them.",1,2015
N15-1165,"in fact, we think that our technique opens up exciting opportunities to combine distributional and knowledge based word representations.",1,2015
N15-1165,in the future we want to overcome this sparsity problem by combining both textual and kb based embeddings into a unified model.,1,2015
N15-1167,"furthermore, we would expect that by applying supervised learning or combining with gazetteer-based method, and by extending the current method to recognizing other types of names in the texts, our system can work even better as an automation tool for such an annotation task.",1,2015
N15-1169,"longer-term future work will pursue more sophisticated methods for taxonomy enrichment to improve the quality of integrated content and will aim to integrate additional dictionaries, with a special emphasis on adding domain-specific terminology.",1,2015
N15-1169,immediate future work will add support for including new lemmas as synonyms in existing synsets and linking newly-created synsets with all appropriate types of wordnet semantic relationship.,1,2015
N15-1171,the data set of annotations released with this paper may also prove a valuable resource for future analyses of framing.,2,2015
N15-1171,similarly informative insights could be gained by comparing lay-persons’ annotations with framing experts’.,3,2015
N15-1171,"future work should compare these results with similar analyses of texts containing more explicit framing, such as opinion columns, campaign speeches, or political advertisements.",3,2015
N15-1171,"future work should examine more closely the role that structural features play, or perhaps do not play, in invoking framing.",1,2015
N15-1171,differences in how framing is identified may give important clues to how framing operates in different contexts.,5,2015
N15-1172,"as future work, we will investigate the impact of other phonetic devices such as assonance, consonance and rhythm on persuasiveness.",2,2015
N15-1172,"it would also be interesting to focus on the connection between sound symbolism and persuasiveness, and investigate how the context or domain of persuasive statements interacts with the sounds in those statements.",5,2015
N15-1173,"we will release our caffe-based implementation, as well as the model and generated sentences.",1,2015
N15-1173,"however our approach falls short in better utilizing the temporal information in videos, which is a good direction for future work.",1,2015
N15-1174,"in the future, we would like to develop better content selection models (e.g., identify surprising aspects in a scene) and more accurate grounding strategies (e.g., via discriminative alignment).",1,2015
N15-1176,"furthermore, we would like to apply redundant bit vectors to other nlp tasks.",4,2015
N15-1176,"in future work, we would like to investigate more elaborate projection schemes that use contextual information from the source side or non-linear projections.",1,2015
N15-1177,"experiments with discriminative joint tagging of mwes and super senses establish a strong baseline for future work, which may incorporate new features, richer models, and indirect forms of supervision (cf.grave et al., 2013; johannsen et al., 2014) for this task.",1,2015
N15-1177,"we also expect future investigations will apply our tagger to a downstream task such as semantic parsing or machine translation (for further discussion of potential applications, see schneider, 2014, pp.179–189).",4,2015
N15-1178,"in future, we plan to augment our method with more complex levels of detail,",1,2015
N15-1178,"finally, using act predictions to bootstrap supervised learning could also yield improvements.",1,2015
N15-1178,and evaluate act for more precise and detailed sentiments.,3,2015
N15-1178,"gather more extensive datasets,",2,2015
N15-1180,we may also investigate new ways to re-rank nbest lists.,1,2015
N15-1180,"language model scoring is a good start, but we may prefer vivid, concrete, or other types of words, or we may use text data associated with the user (papers, emails) for secure yet personalized password generation.",1,2015
N15-1181,"we will also develop an incremental learning algorithm for joint category and feature learning (e.g., using sequential monte carlo methods such as particle filtering).",1,2015
N15-1181,"finally, the bcf model can be applied to tasks beyond those discussed here.",4,2015
N15-1181,extending the model in a way that allows to learn qualitatively different types of features is desirable from a cognitive perspective.,1,2015
N15-1181,"in addition, it would be interesting to investigate the emergence of feature types with nonparametric bayesian methods.",1,2015
N15-1181,"an interesting direction for future work would be to learn feature types from multiple modalities (not only text) and to investigate how different information sources (e.g., visual or pragmatic input) influence feature learning.",1,2015
N15-1181,"in addition to such descriptive features (e.g., behavior) categories also possess defining features (e.g., animate) which are bound to one particular value.",1,2015
N15-1181,the bcf model learns descriptive feature types represented as a collection of feature values.,1,2015
N15-1182,but future work should test this hypothesis across a wider variety of structures and contexts.,3,2015
N15-1185,"future work should evaluate the capability of this approach to induce asymmetric signed networks, the utility of partial or distant supervision, and applications to non-fictional dialogues.",3,2015
N15-1186,we acknowledge that certain languages exhibit phenomena (such as word-compounds in german) that require a more focused approach for solving them.,1,2015
N16-1002,"we plan to continue working on feature design for insertion position choice, and in the future would like to consider using neural networks for learning these features.",1,2016
N16-1002,"we believe that it is important to continue to explore approaches that exploit more general target-side syntax, faithful to the tree-to-tree translation paradigm.",1,2016
N16-1003,"in addition, softer syntactic constraints that allow annotation of phrases with variables (chiang, 2007) such as “one of the preceding x” are another interesting avenue of future work.",1,2016
N16-1003,"particularly, as the proposed method selected segments that took more time to translate due to technical terms, the combination with methods to harvest unknown words (daume iii and ja- ´ garlamudi, 2011) or optimize the selected segments based on the time required (sperber et al., 2014) is potentially useful.",1,2016
N16-1005,"future work could aim to automatically predict it from the english source text based on textual features such as titles and names, or meta-textual information about the discourse participants.",1,2016
N16-1009,in the future we wish to perform joint extraction and clustering instead of our current pipelined approach.,1,2016
N16-1010,"we will also investigate whether the proposed approach benefits other informal text such as product reviews, social media discussions or spontaneous speech conversations, in which we expect the same sparsity issue occurs and the language expression is diverse.",5,2016
N16-1010,"in the future, we may take advantage of the high quality student responses (luo and litman, 2016) and explore helpfulness-guided summarization (xiong and litman, 2014) to improve the summarization performance.",2,2016
N16-1011,obtaining world knowledge or common-sense rules at high precision and scale continues to be the key nlp challenge in this area.,5,2016
N16-1013,"our treatment of cdus in discourse annotations proposes a new distributional translation of those annotations into dependency graphs, which we think is promising for future work.",4,2016
N16-1013,"this gives us good reason to believe that in subsequent work, we will be able to predict cdus and attack the problem of hierarchical discourse structure seriously.",5,2016
N16-1014,we have focused on the algorithmic dimensions of the problem.,1,2016
N16-1014,"since the challenge of producing interesting outputs also arises in other neural generation tasks, including image-description generation, question answering, and potentially any task where mutual correspondences must be modeled, the implications of this work extend well beyond conversational response generation.",4,2016
N16-1015,"in our future work, we intend to focus on training the generator on the fly with real user feedback during conversation.",2,2016
N16-1016,"as future work we plan to use a virtual agent system to collect a set of human-robot humorous interactions, and adapt our model to predict humor from them.",1,2016
N16-1017,"one could explore more complex representations of talking points and discussion points, for instance using topic models or word embeddings.",1,2016
N16-1017,we expect that improving our retrieval model will also improve the robustness of our idea flow analysis.,1,2016
N16-1017,more explicitly comparing and contrasting monologic and interactive dynamics could lead to better models of conversations.,3,2016
N16-1017,"furthermore, augmenting the flow of content in a conversation with the speakers’ linguistic choices could better capture their intentions.",1,2016
N16-1017,a better model of discussion points could also provide more insight into the role of these points in persuading the audience.,1,2016
N16-1017,"in addition, it would be interesting to study the interplay between our conversational flow features and relatively monologic features that consider the argumentative and rhetorical traits of each side separately.",1,2016
N16-1017,"more explicitly comparing and contrasting monologic and interactive dynamics could lead to better models of conversations.for instance, by using a basic unigrambased definition of discussion points, we do not account for the context or semantic sense in which these points occur.",1,2016
N16-1019,one way to address this is to treat alignment and grounding as a joint problem.,5,2016
N16-1019,causality models for verbs can potentially provide top-down information to guide intermediate representations for visual processing and improve grounded language understanding.,1,2016
N16-1019,"first, the current alignment between a video clip and a sentence is generated by some heuristics which are error-prone.",5,2016
N16-1019,"second, our current visual features have not shown effective especially when they are extracted based on automatic visual processing.",1,2016
N16-1019,"recent advances in object tracking algorithms (yang et al., 2013; milan et al., 2014) together with 3d sensing can be explored in the future to improve visual processing. thus another direction is to systematically study causality of verbs.",1,2016
N16-1020,"multimodal embeddings are also likely to provide useful information for the models of metaphor translation, as they have already proved successful in bilingual lexicon induction more generally (kiela et al., 2015b).",1,2016
N16-1020,"in the future, it would be interesting to apply multimodal word and phrase embeddings to automatically interpret metaphorical language, e.g.by deriving literal or conventional paraphrases for metaphorical expressions (similarly to the task of shutova (2010)).",1,2016
N16-1021,we also release a new multilingual image caption benchmark (mic benchmark) which will help in further research in this field5.,4,2016
N16-1025,"we expect the general framework will be applicable to models using other types of neural networks such as feed-forward or lstm nets, and to shift-reduce parsers for constituent and dependency parsing.",1,2016
N16-1026,"future work will explore using our parser to recover other representations from ccg, such as universal dependencies (mcdonald et al., 2013) or semantic roles.",1,2016
N16-1026,"we will also explore new a∗ parsing algorithms that explicitly model the global parse structure using neural networks, while maintaining optimality guarantees.",1,2016
N16-1026,the major obstacle is the mismatch between these representations and ccgbank—we will therefore investigate new techniques for obtaining other representations from ccg parses.,1,2016
N16-1027,future work will investigate improving performance on rare categories.,5,2016
N16-1028,we also would like to explore more subsequent tasks beyond sequence labeling problems.,5,2016
N16-1028,"in the future, we can try to sequentially stack two crfs (one for word segmentation and one of subsequent task).",1,2016
N16-1029,we will also develop methods to make the tagger capable of active self-assessment to produce the best workflow within time bounds.,1,2016
N16-1029,in the future we will exploit broader and deeper entity prior knowledge to improve name identification.,1,2016
N16-1029,we will aim to make the framework more transparent for native speakers so the survey can be done in an automatic interactive question-answering fashion.,6,2016
N16-1031,"in the future, we will experiment our approach on more nlp tasks such as dependency parsing and conference resolution where induced features should play a more critical role.",4,2016
N16-1033,we are also interested in investigating the integration of more sophisticated event-event relation models of causality and temporal ordering.,1,2016
N16-1033,"for future work, we plan to integrate entity and event coreference as additional components into the joint inference framework.",1,2016
N16-1034,"in the future, we plan to apply this joint model on the event argument extraction task of the kbp evaluation as well as extend it to other joint tasks such as mention detection together with relation extraction etc.",4,2016
N16-1037,"first, it would enable the model to scale up to the large datasets needed for competitive language modeling.",1,2016
N16-1037,"future work will investigate the possibility of learning from partially-labeled training data, which would have at least two potential advantages.",5,2016
N16-1037,"second, by training on more data, the resulting vector representations might support even more accurate discourse relation prediction.",2,2016
N16-1040,"in the future, we will explore the uses of the idiom paraphrases in nlp applications such as machine translation and intelligent tutor for second-language learners.",2,2016
N16-1042,"in future work, we would like to explore other ways to address the rare word problem in nmt-based gec, such as incorporating the soft-alignment information generated by the attention-based decoder, or using character-based models instead of word-based ones.",4,2016
N16-1044,"for future works, we plan to apply the structured svm on top of other advanced models.",2,2016
N16-1047,we plan to collect more data on which we can employ richer models that also take into account utterance sequence and dialogue features.,2,2016
N16-1048,"indeed, it would be interesting in future research to explore which factors account for this within-form variation.",5,2016
N16-1050,"in future work we plan to employ our bootstrapping solution in other regression problems, and to further explore potential uses of automatically learned psycholinguistic features.",5,2016
N16-1051,we would also combine this work with more document representations methods as well.,1,2016
N16-1051,further study will investigate the adaptive methods for constructing robust feature spaces.,1,2016
N16-1052,"in future we plan to explore more sophisticated tagging and parsing models like deep neural networks (weiss et al., 2015), recurrent neural networks (dyer et al., 2015), and bi-directional lstms (lewis et al., 2016) for shift-reduce ccg parsing.",1,2016
N16-1054,"in future work we plan to extend stranse to exploit relation path information in knowledge bases, in a manner similar to lin et al.(2015a), garciaduran et al.(2015a) or guu et al.(2015).",1,2016
N16-1054,thus it is a suitable candidate for serving as future baseline for more complex models in the link prediction task.,1,2016
N16-1056,we believe that adding a probabilistic graphical model framework for structured output prediction would further improve the performance of our system.,1,2016
N16-1056,this experiment remains as our future work.,3,2016
N16-1058,"future work include the testing of this model in a linearization shared task (belz et al., 2011) and investigating the integration of large scale training data (zhang et al., 2012; liu and zhang, 2015).",3,2016
N16-1058,"future work include the testing of this model in a linearization shared task (belz , 2011) and investigating the integration of large scale training data (zhang , 2012; liu and zhang, 2015).",2,2016
N16-1059,applying rnns approach to word level qe and studying other ways to make quality vectors better are remained for the future study.,4,2016
N16-1059,our experiments have showed that rnns approach is a meaningful step for qe research.,1,2016
N16-1060,we are also interested in the representation of other word classes such as adverbs for which no evaluation set currently exists.,5,2016
N16-1060,future work includes developing a model that successfully combines the various context types explored in this paper.,1,2016
N16-1061,our future work includes designing a more robust solution that still works well when the number of known classes is small.,1,2016
N16-1063,"our future work also includes setting the initialization weight in a more sophisticated way and combining the proposed method with other nn-based methods (kim, 2014; johnson and zhang, 2015).",1,2016
N16-1066,future work will focus on building useful dimensional sentiment applications based on the constructed resources.,1,2016
N16-1067,future work should further develop automated methods for participant labelling.,1,2016
N16-1070,"in particular, we plan to develop and evaluate models for idea flow and (dis)agreement, using more advanced features (e.g., from dependency relations and knowledge graphs).",1,2016
N16-1070,our full control over the game permits manipulation and intervention experiments that can further advance research on teamwork.,3,2016
N16-1070,"in future work, we envision applying our framework to settings where teamwork takes place online, such as open-source software development, wikipedia editing, or massive open online courses.",4,2016
N16-1070,"in future work, improved classifiers could lead to a system that can intervene in non-constructive discussions early on, steering them on track and preventing wasted time.",1,2016
N16-1070,"further improving classification performance on such a difficult task will hinge on better conversation processing tools, adequate for the domain and robust to the informal language style.",1,2016
N16-1072,an immediate future direction following our work is to improve the title candidate generation process so that it can handle the case where the corresponding titles only exist in the english wikipedia.,1,2016
N16-1073,"our method offers search engine providers with a novel method to identify and analyze user task-behavior, and better support task decisions on their platforms.",1,2016
N16-1073,"further, using an embedding based distancing scheme, we offer an improvement in empirical performance over prior clustering approaches that have used either a bag-of-words or tf-idf based solution.",1,2016
N16-1073,"in future work, we intend to consider hierarchical extensions for extracting hierarchies of tasks subtasks.",1,2016
N16-1074,we plan to apply this approach in the future to construct semantic parsers for more challenging tasks.,4,2016
N16-1075,"in the future, we would like to assess the impact of secos in task-based settings as well as apply it to other compounding languages.",3,2016
N16-1078,"in future work, we plan to adapt our methods for learning compounding morphology for languages such as greek, that have a special compound marker.",1,2016
N16-1079,preliminary experiments indicate that the unigram word probabilities are a somewhat strong feature for pun recognition but further work is needed.,1,2016
N16-1079,extending the edit model further is a fruitful area for future work but will likely require additional data. of interest for future work is joint recognition of the pun and its target.,1,2016
N16-1081,in future work we would like to improve the query analysis performance of our algorithms.,1,2016
N16-1081,"in addition, we plan to assess the contribution of query parsing to ir tasks such as document retrieval and query reformulation.",3,2016
N16-1082,"our future work includes using results of the visualization be used to perform error analysis, and understanding strengths limitations of different neural models.",1,2016
N16-1083,"in future, we aim to extend the approach to learn multilingual semantic spaces with more labels/classes.",1,2016
N16-1084,"in this setting, we want to experiment with (i) ranking comments (instead of classifying them), (ii) exploiting the similarities between the new question and the questions in the database and also the relations between comments across different answer-threads.",3,2016
N16-1084,"in the near future, we plan to apply the fccrf model to the full cqa task, i.e., finding good answers to newly-asked questions using previously asked questions and their answer threads.",4,2016
N16-1085,"future work includes the application of the weak semi-crf model to other structured prediction problems, as well as performing investigations on handling other types of informal or noisy texts such as speech transcripts.",4,2016
N16-1088,"an online version of our natural language interface to osm will be enabling for various interesting directions of future research: besides the possibility to gain new and more realistic data which we can use to extend the corpus, the semantic parser can be improved itself by response-based learning, where supervision signals can be extracted from the executed parses of new user queries against the database (kwiatowski et al.(2013), berant et al.(2013), goldwasser and roth (2013), inter alia).",2,2016
N16-1090,"in the future, we propose to explore other options, such as additional models to combine the contextual information from all siblings in the tree structure, and extending our model to structures beyond trees.",1,2016
N16-1091,"in future work, we will explore the possibility of factoring all information present in an embedding into a dozen or so orthogonal subspaces.",5,2016
N16-1093,"in our future work, instead of selecting aspects based on frequency, we will leverage domain knowledge to improve the selection.",1,2016
N16-1095,we hope these findings will encourage further use of best–worst scaling in linguistic annotation.,2,2016
N16-1096,future work will explore the use of our multilingual verb resource for relation extraction by reading natural language text in multiple languages.,2,2016
N16-1100,"however for more complex optimization scenarios or for problems that require lengthy searches, parallelization might be needed to keep the computations required for optimization in line with what is needed to measure translation speed and quality.",5,2016
N16-1101,"finally, an interesting future work is to use the proposed model to translate between a language pair not included in a set of training corpus.",2,2016
N16-1101,"also, all the other techniques proposed recently, such as ensembling and large vocabulary tricks, need to be tried together with the proposed multilingual model to improve the translation quality even further.",1,2016
N16-1101,more research on this phenomenon in the future will result in further improvements from using the proposed model.,1,2016
N16-1102,"in future work we intend to investigate the model performance on larger-scale datasets, and incorporate further linguistic information, such as morphological representations.",2,2016
N16-1103,we would also like to avoid the entity detection problem by using a deep architecture to both identify entity mentions and identify relations between them.,1,2016
N16-1103,in future work we will apply this model to many more languages and domains besides newswire text.,4,2016
N16-1106,"while we specifically treat lstm in this paper, it should be rather straightforward to adapt the proposed idea to other architectures of recurrent neural networks.",4,2016
N16-1106,"this is an intriguing direction for us, as most nlp tasks lack training data, compared with speech recognition or image classification where neural models have achieved more significant successes.",2,2016
N16-1107,"we intend to explore adaptation to other sts applications and with additional sts features (e.g., word and character ngram overlap) in future.",4,2016
N16-1107,"unsupervised and semisupervised domain adaptation techniques that do not assume the availability of in-domain annotations or that learn effective domains splits (hu et al., 2014) provide another avenue for future research.",1,2016
N16-1109,this work provides a proof of-concept that we hope will spur future work towards solving this important problem in a true low resource language.,2,2016
N16-1110,"as future work, we plan to extend our feature sets to information theoretic aspects of character-level indicators, such as character n-grams frequencies and language models, encoding complexity and surprisal respectively.",1,2016
N16-1111,"another future direction is to investigate cognitive inference (chernov, 2004), which is useful for semantic/syntactic prediction during interpretation (grissom ii et al., 2014; oda et al., 2015).",1,2016
N16-1112,"in future, we are planning to extend our word based lstm reordering model to phrase-based reordering model, in order to dissolve much more ambiguities and improve reordering accuracy.",1,2016
N16-1112,"furthermore, we are also going to integrate our neural reordering model into neural machine translation systems.",1,2016
N16-1113,"in future work, we plan to extend our work to different genres, languages and other kinds of dropped words to validate the robustness of our approach.",3,2016
N16-1114,"future work will examine improving recall, and more sophisticated approaches to global training.",5,2016
N16-1116,another one is to add more precise or even event-based features to improve the model’s performance.,1,2016
N16-1116,one possible direction for future work is to differentiate more resolution modes.,1,2016
N16-1118,further improvements can often be achieved by combining complementary word embeddings of different context types with the right dimensionality.,1,2016
N16-1119,we argue that our method may be used to improve word embeddings of other language whose internal structure is similar to chinese.,4,2016
N16-1120,we are also currently exploring the usefulness of incremental analysis for psycholinguistic data by switching off the lookahead feature.,3,2016
N16-1120,"we can explore the usefulness of our system on other datasets like onestopenglish (ose) corpus (vajjala and meurers, 2016) or the dataset from xu et al.(2015).",2,2016
N16-1121,"in our future work, we intend to study how this training strategy behaves for other transition-based systems or, more generally, for other nlp scenarios using partially annotated data.",4,2016
N16-1124,"this value represents a lower bound on what is possible with this technique and in future work we intend to study the introduction of additional features into the log-linear model to encourage or discourage the use of interlocking phrases during decoding, and investigate the effect of increasing the number of interlocked words.",1,2016
N16-1125,"additionally we plan to improve the feature set to take into account phenomena such as early termination, i.e. when an evaluator makes a judgment before finishing reading a translation.",2,2016
N16-1125,"in the future, we plan to extend our experiments to a large set of users and different language pairs.",2,2016
N16-1125,we plan to deepen our analysis to determine what kind of information is being used beyond fluency and adequacy.,2,2016
N16-1127,"we will explore less redundant, more compact representations of the two dimensions since some annotations can be factorized between the two dimensions (e.g.",1,2016
N16-1127,mwes with irregular syntax) and some can easily be induced from others (e.g. sequential linking between lexical units).,1,2016
N16-1128,"we intend to use scl-opp in the following applications: (1) to automatically create a large coverage sentiment lexicon of multi-word phrases and apply it in downstream applications such as sentence-level sentiment classification, and (2) to investigate how the human brain processes sentiment composition.",4,2016
N16-1131,"in future work, our measure could be simplified by implementing the bias as a single scaling parameter.",1,2016
N16-1133,"in future work, we will use the adaptive regularization of weight vectors (arow) algorithm (crammer et al., 2009) instead of the averaged perceptron.",1,2016
N16-1133,"in addition, we will apply the pairwise approach to ranking (herbrich et al., 1999) used in information retrieval to rerank of grammatical error correction.",4,2016
N16-1136,we hope that our framework will help facilitate comparison between results while allowing researchers to focus on targets such as all grammatical problems or all single equation problems.,5,2016
N16-1136,"as current results show, designing general algorithms that can address different problem types while still being robust to the template or lexical variations has remained a challenge.",1,2016
N16-1137,our future research will focus on joint modeling of cross-genre event extraction in the training stage through cross-genre knowledge enrichment.,1,2016
N16-1138,"thus in future work we will explore ways of incorporating large amounts of raw text in training stance classification models, possibly using a neural network architecture.",1,2016
N16-1138,"in future work we will develop methods for the other tasks involved, such as classifying the stance of a whole article towards a claim and truth assessment.",4,2016
N16-1140,"we are interested in building an unsegmented, word-level language model that can provide meaningful estimates for morphological segments, which would improve scoring for out-of-context phrases and incomplete words.",1,2016
N16-1140,"inspired by the disambiguating de segmentation system of el kholy and habash (2012), we would like to extend our system to propose multiple de segmentation candidates for each word, and allow the decoder to select the correct form using its other features.",1,2016
N16-1142,"these could be addressed in future work by adapting existing methods for learning sense-specific representations for dense vectors (jauhar et al., 2015; ettinger et al., 2016; reisinger and mooney, 2010; guo et al., 2014; huang et al., 2012; neelakantan et al., 2015) to our sparse representations, and target cross-lingual textual entailment tasks, which focus on full sentences rather than isolated words.",5,2016
N16-1142,"we also plan to study lexical entailment on more languages and example types, as well as investigate the usefulness of our bilingual representations in higher level multilingual applications such as machine translation.",4,2016
N16-1142,"these could be addressed in future work by adapting existing methods for learning sense-specific representations for dense vectors (jauhar , 2015; ettinger , 2016; reisinger and mooney, 2010; guo , 2014; huang , 2012; neelakantan , 2015) to our sparse representations, and target cross-lingual textual entailment tasks, which focus on full sentences rather than isolated words.",1,2016
N16-1143,to overcome this limitation we are currently designing a classifier based on conditional random fields.,1,2016
N16-1144,future work concerns using cm fusion to  align and canonicalize multiple such knowledge bases to solve the knowledge fusion problem.,2,2016
N16-1146,"in future work, we plan to use the inferred properties to improve affective polarity recognition in similes.",1,2016
N16-1148,we will also conduct real-world experiments to see how this new imt framework works when human translators are actually involved.,3,2016
N16-1148,"further improvement could be achieved by supporting other type of interactions, such as reordering operations, or building the system with stronger statistical models.",1,2016
N16-1149,"our future work will investigate the model performance on a closely-related task, i.e., neural machine translation (sutskever et al., 2014; bahdanau et al., 2015).",1,2016
N16-1149,"furthermore, we will explore learning methods to combine utterances with and without the auxiliary side information.",1,2016
N16-1151,these results are encouraging; they suggest that useful extensions of current methods are possible with two-phase embeddings.,1,2016
N16-1152,"finally, it would be also very interesting to exploit structural kernels in the network layers.",5,2016
N16-1152,"In the future, we would like to embed CNN similarity in CTKs.",1,2016
N16-1154,"one possible direction for future research is in learning distributed document representations and the ranking simultaneously and applying more sophisticated recurrent models such as long short term memory (lstm) (hochreiter and schmidhuber, 1997) neural networks, that have been shown to be effective in similar tasks (wang and nyberg, 2015; zhou et al., 2015).",1,2016
N16-1156,"the effectiveness of our approach suggests its potential application to a broader range of nlp tasks that require word-level multilingual transfer, such as multilingual parsing and machine translation.",4,2016
N16-1159,"in the future, we intend to continue creating more annotated code-mixed social media data.",2,2016
N16-1159,we would also like to improve upon the challenging problem of normalization of monolingual social hindi sentences.,5,2016
N16-1159,"also, we would further extend our pipeline and build a full parser which has aplenty applications in nlp.",1,2016
N16-1161,"although we focus on phonology, our approach is general, and can be applied in problems that integrate divergent modalities, e.g., topic modeling, and multilingual tagging and parsing.",4,2016
N16-1162,"while we have focused on models using naturally-occurring training data, in future work we will also consider supervised architectures (including convolutional, recursive and character-level models), potentially training them on multiple supervised tasks as an alternative way to induce the ’general knowledge’ needed to give language technology the elusive human touch.",1,2016
N16-1163,"finally, it will be important to evaluate the effectiveness of the retrofitted word embeddings on extrinsic tasks that require disambiguating word meaning in context.",3,2016
N16-1163,"based on these results, it would be interesting to evaluate further refinements of the sense graph: alignment-based senses could be clustered, or further filtered to reduce the impact of alignment noise; new edges could be added using other multilingual resources.",3,2016
N16-1165,"besides a robust identification of argumentative segments, search engines will also need to decide which arguments are the most relevant to a given query— a very promising future research direction in the field of argumentation mining.",5,2016
N16-1166,"in future work, we will aim to improve the accuracy of the extracted persuasive argumentation features by exploring other methods for identifying persuasive argumentations from text.",1,2016
N16-1167,"also, we see that our adaption of kneser-ney smoothing to graphs may be useful for research in subgraph mining in general.",4,2016
N16-1167,in future work we want to apply lcg to essay scoring as well.,4,2016
N16-1168,"while the corpora used in this study cannot be published because of the lack of required irb, we are starting a user study project (zhang et al., 2016) on the application of our proposed techniques and will publish the data collected from this project.",2,2016
N16-1168,"also, we plan to investigate whether revision dependencies exist in other types of corpora such as wikipedia revisions.",2,2016
N16-1168,"in the future, we plan to investigate whether performance can be further improved when more sentences in the context are included.",2,2016
N16-1169,we believe that more annotations and a learning algorithm that scores jointly all positive interpretations generated from each negation (as opposed to individually) would yield better results.,1,2016
N16-1170,"a future direction would be to incorporate other resources such as the paraphrase database (ganitkevitch et al., 2013) into the learning process so that such prior knowledge can be utilized.",2,2016
N16-1171,learning topics and sub-activities under workplace activities is a promising research direction which we will explore in future work.,5,2016
N16-1173,"if successful, an adaptation using grammatical relations rather than semantic roles represents a promising possibility to create isrl annotation and isrl annotation tools for other languages, as universal dependencies are becoming increasingly available for major and low-resourced languages and can be projected to others.",2,2016
N16-1181,"these observations are not limited to the question answering domain, and we expect that they can be applied similarly to tasks like instruction following, game playing, and language generation.",4,2016
N18-1001,"for future work, we plan to jointly perform ner and entity linking for better cross-specialty media structural information extraction.",1,2018
N18-1002,"considering type information is valuable in various nlp tasks, we can incorporate results produced by our fetc system to other tasks, such as relation extraction, to check our model’s effectiveness and help improve other tasks’ performance.",4,2018
N18-1002,"in addition, tasks like relation extraction are complementary to the task of fetc and therefore may have potentials to be digged to help improve the performance of our system in return.",1,2018
N18-1002,more work can be done to further develop hierarchical loss normalization since currently it’s very simple.,1,2018
N18-1006,"ii) we plan to benefit from other types of information such as syntactic and semantic annotations to boost the decoder, as the table is not limited to morphological information alone and can preserve other sorts of information.",1,2018
N18-1006,as our future work we follow three main ideas. we try to find more efficient ways to supply morphological information for both the encoder and decoder.,1,2018
N18-1006 ,"iii) finally, we target sequence generation for fusional languages.",1,2018
N18-1007,"we thus plan to investigate aligning the treebank and ms-state versions of switchboard for future work. however, it remains an open question as to whether dependency, constituency or other parsing frameworks are better suited to leveraging prosody.",1,2018
N18-1008,"for future work, we aim to extend these methods to settings where we don’t necessarily have sentence triples, but where some audio is only transcribed and some audio is only translated.",1,2018
N18-1009,"one additional interesting application of this work is to bring to the surface occasions where a speaker uses typical applause-seeking devices but does not receive applause (the “please clap” moments); we leave to future work identifying the reverse, when speakers receive applause without invoking common techniques (for example, to identify instances of claques paid to clap).",4,2018
N18-1010,we leave the question of how to transfer contextual information from the overall discussion as future work.,5,2018
N18-1011,"in future work, integrating insights from theoretical linguistic approaches to focus and the notion of focus projection established there (cf., e.g., de kuthy and meurers 2012) could provide more guidance for ensuring contiguity of focus domains.",1,2018
N18-1014,"since the stylistic data selection noticeably improved the diversity of our system’s outputs, we believe this is a method with future potential, which we intend to further explore.",1,2018
N18-1015,"for future directions, we plan to further extend the proposed model for capturing other aspects of lyrics/melody discourse structure such as repetitions, verse-bridge-chorus structure, and topical coherence of discourse segment.",1,2018
N18-1017,"in the future, we plan to extend our work through 1) investigating more sophisticated structures in the memory such as knowledge graph, 2) solving more complex questions, such as those involving deep reasoning over multiple facts.",5,2018
N18-1019,"we expect syntactic information to further enhance the quality of the proposed substitutions, ensuring the functional similarity of the lexical substitutions to the target word.",1,2018
N18-1019,we plan to extend our method in this direction in future work.,1,2018
N18-1019,we will explore other ways for promoting high-quality substitutions without hurting the overall coverage of the system in the future.,1,2018
N18-1019,"in future work, we plan to experiment with syntactic substitution models and with syntax-based word embeddings like the ones used in the initial addcos implementation (melamud et al., 2015).",3,2018
N18-1019,"furthermore, we intend to integrate lexical and syntactic simplification, both crucial steps towards text simplification.",1,2018
N18-1026,"as future work, we would like to investigate supervised methods for cross-session task extraction.",1,2018
N18-1033,"future work may investigate better use of already generated candidates since invoking generation for each batch slows down training by a large factor, e.g., mixing with fresh and older candidates inspired by mert (och, 2003).",5,2018
N18-1036,"first, we have ignored metainformation of the debate participants, such as their overall activity (i.e., whether they are spammers or trolls).",6,2018
N18-1036,"second, the proposed typology of ad hominem causes has not yet been post-verified empirically.",3,2018
N18-1036,"third, we expect that personality traits of the participants (big5) may also play a significant role in the argumentative exchange.",1,2018
N18-1037,"we also plan to investigate parsing scene graph with cyclic structures, as well as whether/how the image information can help boost parsing quality.",5,2018
N18-1037,"in the future, we plan to tackle more computer vision tasks with this improved scene graph parsing technique in hand, such as image region grounding.",4,2018
N18-1038,"it could well be that our methods are even more suited for more concrete tasks, such as visual question answering, visual storytelling, or image-grounded dialogue— an avenue worth exploring in future work.",4,2018
N18-1038,"in addition, it would be interesting to explore multi-task learning for sentence representations where one of the tasks involves grounding.",1,2018
N18-1039,"for instance, can these methods be successfully applied to datasets of real scenes?",2,2018
N18-1039,"since linguistic expressions of quantity are grounded on a non-symbolic system, we might expect that a model trained on one modality can be applied to another, at least to some extent.",1,2018
N18-1039,"even further, jointly learning representations from both modalities might represent an even more natural, human-like way to learn and refer to quantities.",1,2018
N18-1042,"in future work, we aim to investigate the effect of adding different contextual information, and we plan to test the resulting models in various applications.",3,2018
N18-1043,"in future we plan to investigate the effectiveness of the joint representation on other nlp tasks like text classification, sentence completion challenge, evaluation of common sense stories etc.",4,2018
N18-1043,the overall aim is to prepare a better generalized representation of words which can be used across languages in different nlp tasks.,4,2018
N18-1047,"future work will test the versatility of our proposals, rntn-diag and rntn-comp, in other tasks that deal with data sets exhibiting carious structures.",2,2018
N18-1048,we will also investigate more sophisticated non-linear functions.,1,2018
N18-1048,"in future work, we plan to extend our approach to specialization for asymmetric relations such as hypernymy or meronymy (glava and ponzetto, 2017; nickel and kiela, 2017; vulic and mrki ′ c′, 2018).",1,2018
N18-1049,future work could focus on augmenting the model to exploit data with ordered sentences.,1,2018
N18-1049,"furthermore, we would like to investigate the model’s ability to use pre-trained embeddings for downstream transfer learning tasks.",1,2018
N18-1052,"in the future, we will explore the possibility of extending the current framework by detecting emotions at more fine-grained level, for example, emotions associated with specific events reported in text.",1,2018
N18-1053,"in future, we would like to explore the application of proposed method in another aspect level sentiment analysis task known as aspect term extraction or opinion target extraction.",4,2018
N18-1054,"we recommend evaluation with comparable dev and test set sizes for future work, as this enables more reliable evaluation.",3,2018
N18-1059,"in future work, we plan to train our model directly from weak supervision, i.e., denotations, and to extract information not only from the web, but also from structured information sources such as web tables and kbs.",1,2018
N18-1061,we also like to extend our work with the link to more behavioral study and analysis.,5,2018
N18-1061,"acknowledging the possible limitations of this study including the quality of the sentiment classifier and a low recall of the present temporal orientation, in future, we will consider more sophisticated sentiment classifier for better performance and explore more linguistic insight into consideration to improve the performance of the temporal orientation classifier.",1,2018
N18-1062,examining these models on similarity vs. relatedness using our proposed measures will be left for the future.,1,2018
N18-1069,"in future work, we shall enhance this methodology of phrasal axiom injection to predict unseen paraphrases.",1,2018
N18-1070,"in future work, we plan to extend the inference component to select an optimal set of explanations for each prediction, and to explain the model as a whole, not only at the instance level.",1,2018
N18-1071,"in the future, we would like to investigate the effectiveness of sgtb on other information extraction tasks, such as relation extraction and coreference resolution.",4,2018
N18-1074,we believe that fever will provide a stimulating challenge for claim extraction and verification systems.,5,2018
N18-1075,joint embedding with global statistics remains an open problem.,5,2018
N18-1075,another interesting venue for future research is to construct much larger-scale distant supervision datasets to train general-purpose textual relation embedding that can help a wide range of downstream relational tasks such as question answering and textual entailment.,2,2018
N18-1076,"we also plan to investigate how the extracted implicit arguments can be integrated into a downstream task that makes use of event information, in particular we would like to experiment with reading comprehension.",4,2018
N18-1076,we currently use a relatively simple model for local narrative coherence; in the future we will turn to models that can test global coherence for an implicit argument candidate.,1,2018
N18-1079,"in future work, it would be interesting to learn global dependencies between the output labels for such a hypergraph structure and training the model globally.",1,2018
N18-1079,"we can also experiment with different representations such as the one in finkel and manning (2009) and use the recent advances in neural network approaches (vinyals et al., 2015) to learn the constituency parse tree efficiently.",1,2018
N18-1080,"our model also lends itself to other pairwise scoring tasks such as hypernym prediction, co-reference resolution, and entity resolution.",4,2018
N18-1080,"however, this could be ameliorated by integrating our model into open relation extraction architectures such as universal schema (riedel et al., 2013; verga et al., 2016b).",1,2018
N18-1081,"for example, open ie would directly benefit from an automatic qa-srl extractor, while a more exhaustive or extensive annotation of qamr would improve open ie’s performance on a wider range of predicates.",1,2018
N18-1082,"we expect our vector representations of prepositions to be widely used in more complicated downstream nlp tasks where prepositional role is crucial, including “text to programs” (guu et al., 2017).",4,2018
N18-1085,"another possible extension would be to allow each step of q to propose a sequence of actions, effectively making the tag set size ∞.",1,2018
N18-1085,"for example, we can learn the generative model and proposal distribution jointly; we can also infuse them with hand-crafted structure, or use more deeply stacked architectures; and we can try training the proposal distribution end to-end (footnote 10).",1,2018
N18-1087,we also plan to explore knowledge transfer and adaptation models for more dialects with limited resources.,2,2018
N18-1087,"Future directions include exploring additional deep learning architectures for morphological modeling and disambiguation, especially joint and multitasking architectures.",1,2018
N18-1092,we are currently expanding the notion of distributional context to multiple auxiliary foreign languages at once.,4,2018
N18-1092,another direction worth exploring is to extend the model’s hierarchy with respect to how parallel sentences are generated.,1,2018
N18-1092,"for example, modelling sentence level latent variables may capture global constraints and expose additional correlations to the model.",1,2018
N18-1093,"besides, we plan to study how to leverage other information to facilitate the training of word embeddings under the low-resource setting.",1,2018
N18-1093,"in the future, we would like to conduct experiments on other languages where available text corpora are relatively hard to obtain.",2,2018
N18-1093,"we are also interested in applying the proposed approach to domains, such as legal documents and clinical notes, where the amount of accessible data is small.",5,2018
N18-1094,"furthermore, it would be interesting to study how people’s prior beliefs affect their other activities on the website and the language they use while interacting with people with the same and different prior beliefs.",5,2018
N18-1094,"finally, one could also try to understand in what aspects and how the language people with different prior beliefs/backgrounds use is different.",5,2018
N18-1097,"however, we expect that for tasks where individual words are not predictive, the current evaluation methods and local explanation approaches may not be sufficient.",3,2018
N18-1098,"in future work, we foresee to investigate learning dynamics in variable number of topics over time.",5,2018
N18-1098,it would also be an interesting direction to investigate the effect of the skewness in the distribution of papers over all years.,5,2018
N18-1098,"further, we see a potential application of the proposed model in learning the time-aware i.e. dynamic word embeddings (aitchison, 2001; basile et al., 2014; bamler and mandt, 2017; rudolph and blei, 2018; yao et al., 2018) in order to capture language evolution over time, instead of document topics.",4,2018
N18-1099,"for example, human judgment is more difficult to measure than in monolingual settings, and it is still an open question on how to design a reliable and accurate survey for multilingual quality judgments.",5,2018
N18-1099,"as the first study on evaluation for multilingual topic models, there is still room for improvement and further applications.",3,2018
N18-1100,"from the linguistic side, we plan to integrate the document structure of discharge summaries in mimic-iii, and to better handle non-standard writing and other sources of out-of-vocabulary tokens.",1,2018
N18-1100,"from the application perspective, we plan to build models that leverage hierarchy of icd codes (choi et al., 2016), and to attempt the more difficult task of predicting diagnosis and treatment codes for future visits from discharge summaries.",1,2018
N18-1102,our model bears not only word pair representations but also dependency path representations as context vectors.,1,2018
N18-1102,"in future work, we will explore unsupervised learning with a neural path encoder.",1,2018
N18-1102,"thus, we intend to apply these representations to various tasks, which path representations contribute to.",4,2018
N18-1103,"we will also extend the model to reason over words unseen in input lexical resources, similar to the recent post-specialization model oriented towards specialization of unseen words for similarity (vulic et al.′ , 2018).",1,2018
N18-1103,we also plan to test the usefulness of le specialized vectors in downstream natural language understanding tasks.,4,2018
N18-1103,"in future work, we plan to apply a similar methodology to other asymmetric relations (e.g., meronymy), as well as to investigate finegrained models which can account for differing path lengths from the wordnet hierarchy.",4,2018
N18-1106,simple algorithms for lexical and structural alignment establish baselines for the new alignment task; we expect statistical models will be brought to bear on this task in future work.,4,2018
N18-1107,we will explore more applications of our tag parsers in future work.,1,2018
N18-1108,"on the other, we would like to expand our empirical investigation by focusing on other long-distance phenomena, such as overt case assignment (blake, 2001) or parasitic gap licensing (culicover and postal, 2001).",3,2018
N18-1108,"in future work, we would like to better understand what kind of syntactic information rnns are encoding, and how.",1,2018
N18-1109,"future work includes generalizing our method to non-nlp problems, as well as applying the task-clustering idea to other few-shot learning frameworks (ravi and larochelle, 2017; finn et al., 2017; mishra et al., 2017; cheng et al., 2017).",4,2018
N18-1112,we also intend to extend the reach of our approach to cross-lingual setups.,2,2018
N18-1112,in future we intend to extend pblm so that it could deal with nlp tasks that require the prediction of a linguistic structure.,1,2018
N18-1112,"for example, we believe that pblm can be smoothly integrated with recent lstm-based parsers (e.g.(dyer et al., 2015; kiperwasser and goldberg, 2016; dozat and manning, 2017)).",1,2018
N18-1113,we also plan to extend our method to multisource classification cases and utilize the multiagent communication environment to boost the classification performance.,1,2018
N18-1113,"for future studies, we will investigate the data selection policies of other semi-supervised methods and try to learn these policies automatically.",1,2018
N18-1115,"in the future, we plan to investigate the effectiveness of carnn units in other sequence modelling tasks.",1,2018
N18-1117,"for the future work, we will combine dense connections with other deep architectures, such as rnns (wu et al., 2016) and self attention networks (vaswani et al., 2017).",1,2018
N18-1119,we imagine that there are further optimizations in reach that could improve this even further.,6,2018
N18-1121,"we hope that this paper will spark discussion on the topic, and future work will propose even more focused architectures.",6,2018
N18-1122,"in the future, we would like to try multi-adversarial framework which consists of multi discriminators and generators for gan.",1,2018
N18-1123,"for future work, we would like to investigate architectures which allow automatic parameter tying among the tasks (ruder et al., 2017).",1,2018
N18-1124,"moreover, we will incorporate relative positional information to the attention function.",1,2018
N18-1124,"as future work, we plan to enrich the present attention mechanism with the key-value-prediction technique (daniluk et al., 2016; miller et al., 2016) which was shown to be useful for language modeling.",1,2018
N18-1125,"in future work, it is promising to exploit other target foresight information such as word cluster besides the pos tags in this paper, and it is also interesting to apply this idea on top of other attention models such as the local attention in luong et al.(2015b).",1,2018
N18-1134,"it would be interesting to evaluate the multimodal architecture on other predicate-argument frameworks, e.g., script knowledge or verbnet style semantic role labeling. more precisely, future work should consider using implicit knowledge not only from images of the participants of the situation, but also from the entire scene in order to directly capture relations between the participants.",3,2018
N18-1134,"this could provide access to a more holistic understanding of the scene. regarding the combination of embeddings from different modalities, we suggest to experiment with different fusion strategies complementing the middle fusion (concatenation) and the mapping (imagined method).",1,2018
N18-1136,these findings open several avenues for future work: how can we improve divergence detection further?,5,2018
N18-1136,"how do divergent examples impact other applications, including cross-lingual nlp applications and semantic models induced from parallel corpora, as well as tools for human translators and second language learners?",1,2018
N18-1136,can we characterize the nature of the divergences beyond binary predictions?,5,2018
N18-1137,"it would also make sense for the model to decode hierarchically, taking sequences of words and sentences into account (zhang and lapata, 2014; lebret et al., 2015).",1,2018
N18-1137,"in the future, it would be interesting to investigate a more sophisticated representation of the input (vinyals et al., 2016).",1,2018
N18-1139,"given the multilingual nature of the new datasets, as future work, we would like to build models which can jointly learn to generate natural language descriptions from structured data in multiple languages.",1,2018
N18-1139,one idea is to replace the concepts in the input infobox by wikidata concept ids which are language agnostic.,6,2018
N18-1140,"one direction for future research is improving the reading models on the queries that are currently the most challenging, i.e. those requiring world and background domain knowledge.",1,2018
N18-1140,better representing background knowledge by inducing embeddings for entities or otherwise integrating ontological knowledge is in our opinion a promising avenue for future research.,1,2018
N18-1141,another potential direction is to jointly learn the collaboration detector together with the qa and qg models.,1,2018
N18-1141,how to conduct joint inference is an interesting future work. improving the diversity of the samples requires different sampling mechanisms.,5,2018
N18-1143,"in addition, since the original data format of the toefl listening comprehension test is audio instead of text, it is worth trying to initialize the embedding layer of the qacnn with semantic or acoustic word embeddings learned directly from speech (chung and glass, 2018, 2017; chung et al., 2016) instead of those learned from text (mikolov et al., 2013; pennington et al., 2014).",1,2018
N18-1143,one area of future research will be generalizing the transfer learning results presented in this paper to other qa models and datasets.,2,2018
N18-1145,"besides, we are interested to apply the method of combing topic model and deep learning into some traditional nlp tasks.",4,2018
N18-1145,"in the future, we plan to evaluate more complex networks for recommendation tasks under the framework proposed by ltmf.",3,2018
N18-1146,"this is a promising new direction for nlp research, one that we hope will help computational (and non-computational!)social scientists better interpret linguistic variables and their relation to outcomes.",1,2018
N18-1146,"this includes algorithmic innovation, theoretical bounds for performance, and investigating rich social questions with these powerful new techniques.",1,2018
N18-1149,"more importantly, we hope that other researchers will identify novel opportunities which we have not explored to analyze the peer reviews in this dataset.",3,2018
N18-1149,"as a concrete example, it would be interesting to study if the accept/reject decisions reflect author demographic biases (e.g., nationality).",5,2018
N18-1152,"furthermore, we want to investigate whether we can reduce the number of required preferences with smarter sampling methods.",1,2018
N18-1152,"in future work, we would like to investigate whether we can use crowd-sourcing platforms to collect pairwise preferences on a large scale.",2,2018
N18-1154,"in the future, we intend to explore more about gbn’s applications as well as its provable computational and statistical guarantees.",1,2018
N18-1156,"in future work, we will explore the extension of our proposed model to cater for varying number of storylines automatically and also better deal with intermittent storylines.",1,2018
N18-1157,"the techniques studied in this field may be useful to develop better algorithms for stkp, which we leave for future work.",1,2018
N18-1158,"in the future we would like to focus on smaller discourse units (mann and thompson, 1988) rather than individual sentences, modeling compression and extraction jointly.",1,2018
N18-1159,"because of this heterogeneity, we argue that future attempts to summarize relationships will likely require a diversity of models and techniques.",1,2018
N18-1160,"in the future, we would like to investigate how attribute-specific features can improve performance compared to our more general feature set which is invariant for each sentence type.",1,2018
N18-1160,it would also be possible to equip the model with a hierarchical decoder which generates a document instead of individual sentences.,1,2018
N18-1160,"finally, we would like to examine whether the content analysis presented here can extend to different types of fiction such as novels or short stories.",2,2018
N18-1161,"instead of learning to predict rouge recall scores, the regress and can simply be exchanged and the rouge precision can be used instead.",1,2018
N18-1161,"for future works, it is very simple to incorporate the findings presented in this paper.",6,2018
N18-1163,"we also plan to extend the work to detect egregious conversations in real time (e.g., for escalating to a human operators), and create log analysis tools to analyze the root causes of egregious conversations and suggest possible remedies.",4,2018
N18-1163,"in this context, future work includes collecting more data and using neural approaches (e.g., rnn, cnn) for analysis, validating our models on a range of domains beyond the two explored here.",2,2018
N18-1166,it is worth conceiving a more general solution to improve the limitations of torder in the future work.,5,2018
N18-1167,we also plan to explore entity embeddings obtained using other graph densifying methods.,1,2018
N18-1167,"in future, we plan to enhance elden using el of pseudo entities to estimate entity prior of entities not present in kg.",1,2018
N18-1168,"in the future: 1) more advanced technologies like reinforcement learning (sutton and barto, 1998) can be introduced to generate latent fact details such as the time of theft more accurately;",1,2018
N18-1168,"2) in this work, we only generate rationales in court views omitting charge prediction, it is interesting to see whether jointly generating the two parts will benefit both of the tasks;",5,2018
N18-1168 ,3) studying verification mechanism is meaningful to judge whether generated court views can really be adopted which is important for court-view-gen in practice;,3,2018
N18-1168 ,4) more complex cases with multiple charges and multiple defendants will be considered in the future.,2,2018
N18-1171,"future work involves leveraging raw human annotations to improve sentiment analysis classifiers, and finding ways to better detect and understand the “complicated” property in these samples that cause high annotator disagreement.",1,2018
N18-1171,"additionally, we encourage researchers to use mtsa in the development of other methods for short text sentiment analysis, including unsupervised, lexicon-based, and rule based methods.",4,2018
N18-1173,"since this contribution was restricted to the vad format of emotion representation, in future work we will examine whether mtl yields similar gains for other representational schemes, as well.",4,2018
N18-1174,"in future work, obtaining more human annotations will be useful to build a better human needs categorization system.",2,2018
N18-1174,"in addition, applying and analyzing the human needs of affective events in narrative stories and conversations is a fruitful and interesting direction for future research.",5,2018
N18-1175,"in the short run, we plan to draw more attention to this topic by running a semeval 2018 shared task.12",6,2018
N18-1176,"finally, we plan to conduct an empirical analysis of deception behavior across personality types.",3,2018
N18-1176,"in future work, we plan to conduct similar analysis in additional deception corpora in other domains, in order to identify consistent domain-independent deception indicators.",2,2018
N18-1176,"in addition, we plan to conduct cross-corpus machine learning experiments, to evaluate the robustness of these and other feature sets in deception detection.",3,2018
N18-1176,"we also would like to explore additional feature combinations, such as adding acoustic-prosodic features.",1,2018
N18-1178,"there are many possible extensions to this work, including: (a) learning multilingual word embeddings with domain information; and (b) modeling other policy related scores from text, such as “support for eu integration”.",1,2018
N18-1179,fully uncovering these factors in current nli datasets is a pre-requisite for the construction of more effective resources in the future.,2,2018
N18-1179,further work is required to understand what these interactions are and how they contribute to performance.,5,2018
N18-1180,this will enable developing novel approaches to language proficiency assessment which will integrate task based performance with real time monitoring of cognitive and linguistic processing.,3,2018
N18-1180,"in future work, we plan to extend the analysis of the validity and consistency of our approach, and further explore its applications for language proficiency evaluation.",1,2018
N18-1180,we will further study the consistency of our scores for repeated tests by the same participants.,6,2018
N18-1181,ultimately we plan to complement our corpus analysis with real-time language production experiments to more thoroughly test hypotheses about speaker choice.,2,2018
N18-1182,"our work can be extended by: (a) utilizing several predictions for each training instance, (b) investigating the extent to which a more sophisticated and effective downstream learner can affect the performance of different spotters, (c) developing models to better distinguish hard genuine instances from spurious ones, and (d) developing ranking algorithms to improve the performance of models on real-world datasets.",1,2018
N18-1185,"for the future work, we will expand the annotation for more entity types and automatically link mentions with respect to their entities using an entity linker.",2,2018
N18-1186,applying to group chatting with more than two speakers and reasoning over emotion embeddings or knowledge vectors included from an external knowledge base/graph are taken as our future directions.,1,2018
N18-1188,"besides, injecting external information during response’s generation would be another challenging work.",5,2018
N18-1188,"in the future studies, we would explore the possibility of promoting diversity on the learning procedure, by directly optimizing diversity loss in the cost function.",1,2018
N18-1189,"In the future, we plan to investigate three limitations of our current model.",1,2018
N18-1192,"and, we will research methods to accelerate the training and reduce the memory consumption during taring.",1,2018
N18-1192,"in the future, we will study how to improve the performance of the bllm model.",1,2018
N18-1198,"future work includes (i) investigating how object category information can be better used or expanded to improve ic; (ii) analyzing end to-end ic systems by using interpretable representations that rely on other explicit detectors (e.g. actions, scenes, attributes).",1,2018
N18-1199,"our work suggests the following future directions: evaluating algorithms: because concreteness scores are able to predict performance prior to training, evaluations could be reported over concrete and abstract instances separately, as opposed to aggregating into a single performance metric. designing datasets: when constructing a new multimodal dataset, or augmenting an existing one, concreteness scores can offer insights regarding how resources should be allocated. curriculum learning: during training, instances could be up/down-weighted in the training process in accordance with concreteness scores.",3,2018
N18-1201,"another issue we plan to explore is using textual explanations (park et al., 2016) for vqa.",5,2018
N18-1201,"finally, we hope to apply our approach to additional problems beyond vqa.",4,2018
N18-1201,"therefore, in the future, we would like to focus on explaining the results of an ensemble.",5,2018
N18-2002,"we hope that by drawing attention to this issue, future systems will be designed in ways that mitigate gender-based overgeneralization.",6,2018
N18-2006,"for example, it would be interesting to investigate whether standard low-level tasks, such as pos tagging or chunking, are effective for am.",3,2018
N18-2007,"noise in the coreference annotations has a detrimental effect on the performance (figure 3), hence we also aim to explore joint models which learn to do coreference resolution and reading together.",1,2018
N18-2007,"in future work, we aim to apply this model to other problems where long term dependencies at the document level are important.",4,2018
N18-2012,"we also plan to investigate how this method transfers to related tasks, such as evaluating open-domain dialogue responses, e.g.(lowe et al., 2017).",4,2018
N18-2015,future investigation will be needed to determine whether this is a property inherent to human storytelling or a form of bias introduced during data collection.,6,2018
N18-2018,"in future work, we would conduct more rigorous comparative evaluation with human humor recognition and look into how the humorous texts can be generated using deep learning models as well.",3,2018
N18-2020,future work will conduct user studies to assess the relative importance of different evaluation criteria.,3,2018
N18-2020,a better understanding of how these interact may lead to improved semantic evaluation that will alleviate the need for a high number of references.,1,2018
N18-2021,"in the future, we wish to explore the effectiveness of r-spen on various tasks using domain knowledge functions with varying degrees of complexity.",4,2018
N18-2022,"future work may also compare the catalonian situation with multilingual societies in which a minority language is discouraged (karabakh, 2013), or in which the languages are more equally distributed (blommaert, 2011).",5,2018
N18-2023,"as future work, amr-covington produces sequences of no-arcs which could be shortened by using non-local transitions (qi and manning, 2017; fernandez-gonz ′ alez and g ′ omez- ′ rodr′?guez, 2017).",1,2018
N18-2025,the results shed light on future work on language-independent paraphrase identification and multilingual paraphrase acquisition where pretrained word embeddings on large corpora are not readily available in many languages.,1,2018
N18-2029,"as future work, we plan to experiment with more advanced neural architectures and finer-grained relations.",1,2018
N18-2029,we also intend to port the model to more languages.,2,2018
N18-2030,"in the future, we would like to work on bli of multiword translations and compound words.",5,2018
N18-2034,the most natural next step would be to study similar extensions of other representation-learning models.,1,2018
N18-2037,"future work includes investigation of the accuracy of these methods for different clinical conditions, and languages.",3,2018
N18-2039,"is a one-hit accuracy sufficiently informing on success in word analogy, or do we need a softer measure from for example the ranking world?",5,2018
N18-2040,"in addition, we perform a detailed comparative analysis of the transition distributions for english and chinese as well as errors in chinese amr parsing that we hope will inform future chinese amr parsing research.",2,2018
N18-2044,"in future, we plan to address the implicit and sarcastic medical sentiments that account to the majority of the errors.",5,2018
N18-2046,future work would require a human evaluation effort to draw more conclusions.,3,2018
N18-2051,"for future work, we want to investigate the impact of using pre-trained weights to initialize the lstm cells in the seq2seq model from a language modelling task, as well the grammatical diversity of generated paraphrases.",1,2018
N18-2053,we also plan to extend convkb for a new application where we could formulate data in the form of triples.,4,2018
N18-2056,we also plan to extend the knowledge base cluster to include related entities.,5,2018
N18-2056,"furthermore, runtime and computational complexity of the system should be studied.",5,2018
N18-2057,"as part of future investigation, we will experiment with other types of encoders such as convolutional and recurrent networks.",1,2018
N18-2057,"furthermore, we aim to scale this approach to larger datasets.",2,2018
N18-2063,we intend to infer cognate sets from the combined system and use them to infer phylogenies and evaluate the inferred phylogenies against the gold standard trees.,3,2018
N18-2063,"as future work, we intend to create a cognate identification system that combines the output of different algorithms in a more systematic way.",1,2018
N18-2064,"in future work, we would like to evaluate the impact of annotation conventions on other kind of parsers and to find the properties of a dependency tree that facilitate its prediction.",3,2018
N18-2064,we also plan to find ways to easily annotate sentences with multiple references (e.g.by indicating that the head of word can be chosen arbitrarily) and eliminate the constraint that references should be trees.,2,2018
N18-2068,"we believe that with these extensions our variability measures will offer a unified framework for describing variability profiles of mwes, which should be useful both in theoretical and applied research.",1,2018
N18-2072,future work will explore comparison and combination with other generalization methods exploiting datasets deeply as well as our method.,2,2018
N18-2073,"importantly, we also plan to evaluate our models on standard clir test sets such as trec (schauble and sheridan ¨ , 1997), ntcir (2007), fire (2013) and clef (2016).",2,2018
N18-2073,"future work includes: (a) expansion of the dataset to more languages, (b) extraction of different types of queries and relevant judgments from wikipedia, and (c) development of other ranking models.",2,2018
N18-2074,we are also interested in nonlinear compatibility functions to combine input representations and edge representations.,1,2018
N18-2074,"for future work, we plan to extend this mechanism to consider arbitrary directed, labeled graph inputs to the transformer.",1,2018
N18-2075,another important direction is developing a structured global model that will take all local predictions into account and then perform a global segmentation decision.,1,2018
N18-2075,"in future work, we will explore richer neural models at the sentence-level.",1,2018
N18-2077,"it would be straightforward to extend the addcos method to handle multi-word paraphrases by training embeddings for multi-word phrases, keeping in mind that longer substitutions might require restructuring the produced sentences to preserve grammaticality.",1,2018
N18-2078,"it would be interesting in future work to both understand when semantics appears beneficial, and also to see which components of semantic structures play a role.",1,2018
N18-2078,experiments on other language pairs are also left for future work.,2,2018
N18-2079,in the future we would like to change the training model to dynamically build the encoder and the attention model in order to match our incremental decoder.,1,2018
N18-2082,"in future work, we would like to study how well nmt encoders capture other semantic phenomena, possibly by recasting other datasets.",1,2018
N18-2091,we hope that these robustness methods are generalizable to other insertion-based adversaries for q&a tasks.,4,2018
N18-2092,we also want to explore how to adapt cloze style pretraining to nlp tasks other than qa.,4,2018
N18-2092,"for future work, we plan to explore the active learning setup for this task – specifically, which passages and / or types of questions can we select to annotate, such that there is a maximum performance gain from fine-tuning.",1,2018
N18-2093,"in this way, we can study the performance of a generalized model on a more realistic text-to-sql task which includes many complex sql and different databases.",1,2018
N18-2093,"in the future, we plan to advance this work by exploring other more complex datasets under the database-split setting.",2,2018
N18-2095,"in the near future, we will look at multi-task helpfulness prediction to further transfer knowledge across domains.",5,2018
N18-2098,"as future work, we propose to tackle general tabular summarization where the schema can vary across tables in the whole dataset.",1,2018
N18-2100,we also plan to use key2vec in other domains such as news articles and extend the methodology for other related tasks like summarization.,4,2018
N18-2100,"in the future, we plan to use other existing procedures for training phrase embeddings and study their effects.",3,2018
N18-2105,"in future work, we would like to apply ranking algorithms that leverage the specific structure of our graph representation, such as the one proposed in (becker, 2013).",1,2018
N18-2106,"future work can also explore other domains (e.g., newswire and literary fiction) and evaluate character and event alignments between narratives based on established ground truths.",4,2018
N18-2107,"as future work, we plan to extend our models by considering the prediction of more than one emoji per social media post and also considering a bigger number of labels.",5,2018
N18-2110,potential future work includes using more conversational context and implementing multi-class classification to differentiate among stages of ad.,1,2018
N18-2110,"we also plan to apply this generalizable model to other similar neurological diseases, such as diffuse lewy body disease and huntington’s disease (heindel et al., 1989).",4,2018
N18-2111,"we plan to work on these improvements for future work, and also explore evaluation methods which go beyond language model perplexities, and capture model aspects closer to the task and domain.",1,2018
N18-2112,a promising approach would be to substitute the handcrafted feature functions used in this work by neural feature extractors trained jointly with the policy.,1,2018
N18-2113,"despite these mixed results, we hope that the evaluation guidelines presented here will help promote work in this area, in order to eventually provide better tools for working with historical text collections.",3,2018
N18-2114,"in future work, we would like to investigate the effect of memory mechanism for multi-task learning, which is similar to gate mechanism but more complex.",5,2018
N18-2115,we also would like to understand this approach better by testing it on more natural language processing tasks.,4,2018
N18-2115,"in the future, we plan to explore more variations of the meta-learning setup, such as using different relevance functions, including the ones that are jointly learned.",1,2018
N18-2120,"we plan to train our models on dialogue/image pairs from datasets such as comics (iyyer et al., 2017) and visual storytelling (huang et al., 2016); these models could also help learn powerful joint representations of vision and language to improve performance on downstream prediction tasks.",1,2018
N18-2122,"the techniques explored here could be use to combat adversarial attacks on cnns, detect misclassifications, or possibly guide the improvement of cnn architectures, and eventually help to unite the study of semantics in computer vision and computational linguistics.",1,2018
N18-3002,we are now planning to extend them with image data.,2,2018
N18-3004,"in ongoing work, we explore reinforcement learning techniques to reach the goal state quicker thereby reducing the number of interactions.",1,2018
N18-3007,"furthermore, we plan to develop qe methods for languages other than english, where the amount of training data is much smaller.",2,2018
N18-3007,"as future work, we plan to bring those research and pilot systems into production and gather experience on their use; as well as extending them to multi-class prediction for finer-grained qe, directly predicting the error severity classes (good, p3, p2, p1).",4,2018
N18-3009,we would also like to utilize human evaluators to judge the rankings produced in the review reranking experiments.,6,2018
N18-3009,"for future work, we wish to investigate alternative ways to aggregate and compute user profiles and compute distances between objects to rank and user profiles.",1,2018
N18-3012,"in the future, we plan transfer these findings to production settings by performing regular nmt model updates with batches of collected user behavior data, especially focusing on improving translation of ambiguous and rare terms based on rewards from implicit partial feedback.",1,2018
N18-3014,we also plan to explore training with low precision for faster experiment turnaround time.,5,2018
N18-3014,in the future we plan to implement a multilayered matrix multiplication that falls back to the naive algorithm for matrix-panel multiplications.,1,2018
N18-3017,"in future work, we plan to apply our approach to further languages and explore bootstrapping new domains for an existing nlu system.",2,2018
N18-3018,"in the future, we plan to explore unsupervised methods for transfer learning and the effect of semantic similarity between source and target tasks.",1,2018
N18-3021,"in the future, we plan to focus on the normalization of heads (e.g.pnl and panal to panel) and parts (e.g.lft valve and left vlv to left valve) from panda extracted results.",1,2018
N18-3025,"we would also like to investigate good-turing frequency estimation (good, 1953).",5,2018
N18-3025,"for future work we would like to try complex neural network architectures, regularization, and semantic embeddings or other abstracted relations to enhance the signatures.",1,2018
N18-3027,"in future work, we want to extend the weighting scheme by integrating ontology and keyword information in order to improve the similarity search.",1,2018
N19-1002,"in future work, we intend to explore how the encoding pattern we found varies across network architectures and hyperparameters, as well as across languages and domains.",5,2019
N19-1002,we also would like to investigate the time course of emergence of the found behavior over training time.,5,2019
N19-1002,"we conjecture a similar distinction between sr and lr units to be found in the human brain, as well as an interaction between syntax-processing and feature carrying units such as the lr units, and plan to test these in future work.",3,2019
N19-1002,"more generally, we hope that our study will inspire more analyses of the inner dynamics of lstms and other sequence-processing networks, complementing the currently popular “black-box probing” approach.",1,2019
N19-1003,"there are several venues for future work including (a): finer-grained data sampling at queue level,",2,2019
N19-1003,"(c): applying this model to areas where neural networks have not been investigated, e.g. due to limited availability of labeled data.",1,2019
N19-1003,"b): extending our model to other machine learning algorithms that employ iterative training, such as boosting approaches,",1,2019
N19-1005,"in future work, we will further explore what information is encoded into the model representations when neural and behavioral data are used to train neural networks, and how these representations differ from the representations in a model trained on language alone.",1,2019
N19-1008,an important next step is to test the system using asr rather than hand transcripts.,3,2019
N19-1008,"finally, we expect that the innovation model of prosody can benefit other nlp tasks, such as sarcasm and intent detection, as well as detecting paralinguist information.",4,2019
N19-1010,we plan to assess the effectiveness of our approach in the near future by integrating it in a heads-up display cai system and performing a user study.,3,2019
N19-1010,"additionally, speech features could be extracted from the source or interpreter audio to reduce the dependence on a strong asr system.",1,2019
N19-1011,"first, we can further expand the scope of audio caps.",1,2019
N19-1011,"second, our model is integrable with speech counterparts to achieve more complete auditory captioning tasks.",1,2019
N19-1011,"additionally, if we can better understand what makes a headline funny, we may be able to automatically generate funny headlines and even personalize them to particular readers.",1,2019
N19-1012,future work with this data could include deeper features for assessing humor.,3,2019
N19-1013,"one significant research challenge in the space of free text generation problems when the set of possible outputs is large, is that of automatic evaluation (lowe et al., 2016): in our results we saw some correlation between human judgments and automatic metrics, but not enough to trust the automatic metrics completely.",5,2019
N19-1013,"following mostafazadeh et al.(2016), we could combine text input with image input in the amazon dataset (mcauley and yang, 2016) to generate more relevant and useful questions.",2,2019
N19-1013,"lastly, we hope to integrate such a question generation model into a real world platform like stackexchange or amazon to understand the real utility of such models and to unearth additional research questions.",4,2019
N19-1017,"however, our new training procedure can be applied to any task, so a future work would be to use it to perform phylogenetic pos tagging.",4,2019
N19-1017,other directions for the future are designing better sampling methods as well as better ways to measure training convergence at each level.,1,2019
N19-1018,"finally, we plan to adapt our system to non-projective dependency parsing and semantic graph parsing.",1,2019
N19-1019,we also plan to see how the different experiments we have made to identify annotation errors and inconsistencies can be used during the annotation process to reduce the workload of annotators and help them creating high-quality corpora.,2,2019
N19-1026,"as a future work, we will be working on devising automated-assisted methods for detection of paraphrasing issues.",5,2019
N19-1029,potential directions may include ranking the subgraph by assigning each edge (relation) a closeness score and evaluating the length of the shortest path between any two path-connected entity nodes.,3,2019
N19-1029,"one could also consider extending our approach to complex questions, e.g., multi-hop questions where more than one supporting facts is required.",4,2019
N19-1029,"in the future work, one could further improve the performance on simple question answering tasks by exploring relation ranking, different embedding strategies and network structures, dealing with open questions and out-of-scope questions.",1,2019
N19-1030,"for future work, we plan to explore the directions of (1) constructing multihop query and (2) developing end to-end retriever reader model via reinforcement learning.",1,2019
N19-1031,we expect in the future that incorporating a beam search may further improve performance.,1,2019
N19-1032,our future work will be making use of more complex relations between entities and building graphs in more general way without candidates.,1,2019
N19-1034,"in the future, we would like to explore the other dimensions to our multi-task framework, e.g., sentiment classification & intensity prediction, emotion classification & intensity prediction and all the four tasks together.",4,2019
N19-1035,"in the future, we will apply this conversion method to other similar tasks.",4,2019
N19-1036,"in the future, we plan to explore better target opinion word extraction approaches to find better “supervision” signals.",1,2019
N19-1037,"in the future, we plan to explore semi-supervised learning methods to address the problem of data scarcity in this task.",1,2019
N19-1043,"in future work, we will investigate the use of differentiable reconstruction from sampled sequences in unsupervised and semi-supervised sequence generation tasks.",4,2019
N19-1043,"in particular, we will exploit monolingual corpora in addition to parallel corpora for nmt.",2,2019
N19-1044,"in the future, we will study how the copy success rate and the bleu scores interact when different sampling strategies are taken to obtain augmented training corpus and when the amount of augmented data grows.",2,2019
N19-1044,"another direction is to validate the performance when applying this approach to language pairs that contain a number of identical letters in their alphabets, such as english to french and english to italian.",4,2019
N19-1046,"in the future, we want to dig deeply into the subspace regularities of the learned representations for more fine-grained understanding.",1,2019
N19-1048,"furthermore, it would be interesting to investigate whether the proposed architecture is also beneficial for languages typologically different from english, e.g., morphologically rich languages.",2,2019
N19-1048,"in future work, one might investigate whether attention mechanisms on the word level (cf.ling et al., 2015) can further improve the model’s performance.",1,2019
N19-1049,"the idea of using both x and x0, akin to how we train automated classifiers of naturalness (section 4.3), can be extended to construct a perplexity-based metric that also takes into account the perplexity of input x.",1,2019
N19-1049,it is worth studying the distinction between style words and content words in the vocabulary of each such dataset.,3,2019
N19-1049,another avenue for future work could be evaluating on datasets with a different style or number of style classes.,3,2019
N19-1052,"finally, future work could expand on our methodology to formulate other more general tasks aiming to understand the reasons why a person is sharing a personal story.",4,2019
N19-1053,"to make use of such systems, one needs to develop mechanisms to recognize valid argumentative structures.",1,2019
N19-1053,"in addition, we ignore trustworthiness and credibility issues, important research issues that are addressed in other works.",2,2019
N19-1054,"as bert pre-training includes a next-sentence prediction task, we expect this model to be effective for modeling argumentative context and to be beneficial for predicting premise or justifications for these claims and the relations between these argumentative components.",1,2019
N19-1054,"in the future, we plan to expand this work beyond single sentences as the data-set for lm fine tuning used in our experiments consists of sentences containing im(h)o without additional context.",1,2019
N19-1055,"as future work, we would like to explore other architectures to directly model dependencies between slot labels and intents.",1,2019
N19-1055,we will also test the proposed approaches against real-world scenarios to understand their generality across various domains.,3,2019
N19-1056,"ultimately, of course, we hope to leverage such corpora to build and apply better models of multimodal communication.",1,2019
N19-1063,"future work may consider revisiting choices like the use of semantically bleached sentence inputs, the aggregation applied to models that represent sentences with sequences of hidden states, and the use of cosine similarity between sentence representations.",1,2019
N19-1069,"future work could investigate the effect of other potential confounding variables in satire detection, such as the distribution of time and region of the articles.",3,2019
N19-1069,"further, we propose to perform more quantitative but also more qualitative analysis to better understand the behavior of the two classifier con- figurations in comparison.",1,2019
N19-1071,"in future work, we plan to explore the potential of seq3 in other tasks, such as unsupervised machine translation (lample et al., 2018a; artetxe et al., 2018) and caption generation (xu et al., 2015).",4,2019
N19-1072,"in particular, it may be possible to extend our design of crowdsourcing tasks to supply indications for these complementary measurements as well.",1,2019
N19-1075,"in future work, we will explore cross lingual transfer for super tagging and semantic role labeling.",2,2019
N19-1078,future work will therefore examine methods to learn weighted pooling of previous mentions.,1,2019
N19-1078,we will also investigate applicability of our proposed embeddings to tasks beside ner.,4,2019
N19-1079,"although we focused on the task of named entity recognition in this work, we believe the proposed approach may find applications in some other sequence labeling tasks or other more general structured prediction problems where the issue of incomplete annotations is involved.",4,2019
N19-1082,future work includes the exploration of automatically learning the underlying graphical structure of the input data.,1,2019
N19-1083,some future extensions include exploring more advanced graph embedding techniques without modeling entity-specific parameters and using text encoders as additional signals.,1,2019
N19-1089,these experiments suggest plausible pathways to achieving human-level performance on this task that are both challenging and interesting problems for future research.,2,2019
N19-1091,"moreover, studies will be conducted on courtesy transfer for the other domains, and also transfer learning from one domain to the another (like customer care to hospitality).",4,2019
N19-1091,"in future, along with the opportunity of extending the architectural designs and training methodologies to enhance the performance of our systems, we look forward to designing a specific component to enhance the natural language generation component of an end to end chatbot, by including appropriate mechanisms to interact with all its components (memory, database, and the dialog manager).",1,2019
N19-1092,"for future work, we will explore techniques to generate metaphors without extracting its fit words in the corpus and improve the quality of generated metaphors.",2,2019
N19-1093,"since the proposed model adopted an extensible structure, one possible future work is to explore the best way to enhance it with more complicated knowledge resources such as knowledge graphs.",1,2019
N19-1094,"in the future work, we will use transformer, which is proved to be more powerful than lstm, as the encoder of our unsupervised deep structured semantic models, and we will collect a larger corpus from common crawl to train our model.",2,2019
N19-1100,a final and fascinating direction of future work is to explain the non-normality of certain types of word vectors (and in particular the presence of outliers) by analysing their training procedures.,1,2019
N19-1101,"in future work we plan to address this by adding explicit cross-sentence semantic knowledge (joshi et al., 2018).",1,2019
N19-1103,it is worth investigating modeling these interactions while keeping the merit of fast training and evaluation.,1,2019
N19-1103,"as future work, we plan to devise convolutional paradigms that can maximize interactions not only between subject entities and relations, but also between object entities and relations.",1,2019
N19-1103,"in convr, we use 1-to-many scoring to speed up training and evaluation.",3,2019
N19-1104,"in future work, we will extend grank to use more complex patterns.",1,2019
N19-1105,"in the future, we plan to explore the following directions: (1) we will extend our method to further extract event arguments and perform event extraction.",1,2019
N19-1105,"(2) we will develop a large-scale and clean dataset for ed based on our method, which will benefit further research in this field.",2,2019
N19-1106,"moreover, ablation study results suggest that a nonlinear propagation model to reconstruct the full label set may be of benefit.",1,2019
N19-1106,"in future, we can improve our model by using nonlinear training model instead of a simple linear regression model for the selected subset of the labels.",1,2019
N19-1107,"in the future, we plan to further explore the effect of different encoding modules like bi-lstm or self-attention and try to model temporal information with more sophisticated choices.",1,2019
N19-1108,"in the future, we plan to extend our framework to do multi-label classification with a larger amount of data, and also study how semantic units defined by linguists can be used in the zero-shot scenario.",1,2019
N19-1109,"in future, we would like to explore ways of applying a similar graph based formalism for learning vectors for documents.",1,2019
N19-1110,"moreover, we would like to explore a more sophisticated smoothing technique where the number of gaussian components is adapted for each word.",1,2019
N19-1110,"as future work, one can hypothesize that the area a word covers in the mapped space reveals its semantic range.",1,2019
N19-1110,"finally, it may 1060 be helpful to investigate non-linear mappings between semantic spaces using deep neural network architectures.",1,2019
N19-1110,given that gaussian mixture embeddings could capture the uncertainty of a words representation in the semantic space one could also investigate different metrics for measuring the semantic relationship between word pairs that go be? yond their point-wise comparison.,1,2019
N19-1110,"in this direction, a refinement of the semantic anchor selection approach could be explored in an iterative way assuming that the variance of a words gaussian distribution denotes its degree of polysemy (vilnis and mccallum, 2015).",1,2019
N19-1115,"finally, we plan to look into applying recursive approaches to language modelling as a pre-training step and measure if it has the same impact on downstream tasks as sequential models.",3,2019
N19-1115,"in the future, we would like to explore further relaxation-based techniques for learning the parser, such as rebar (tucker et al., 2017) or relax (grathwohl et al., 2017).",1,2019
N19-1116,"potential avenues include training larger models over much larger corpora, extra unsupervised or weakly-supervised phrase classification objectives, and other modeling enhancements.",2,2019
N19-1116,we are also eager to apply diora to other domains and languages which do not have rich linguistically annotated training sets.,2,2019
N19-1117,"in future work, we look to model other types of world knowledge beyond named entities using predictive learning and training on large corpora of text without additional information, and to make kalm more robust against corrupted entities.",1,2019
N19-1119,"in the future, we are mainly interested in: (i) exploring more difficulty heuristics, such as measures of alignment between the source and target sentences (kocmi and bojar, 2017), sentence length discrepancies, or even using a pre-trained language model to score sentences, which would act as a more robust replacement of our sentence rarity heuristic, and (ii) exploring more sophisticated competence metrics that may depend on the loss function, the loss gradient, or on the learners performance on held-out data.",1,2019
N19-1119,"furthermore, it would be interesting to explore applications of curriculum learning to multilingual machine translation (e.g., it may be easier to start with high-resource languages and move to low-resource ones later on).",4,2019
N19-1119,"we would also like to explore the usefulness of our framework in more general machine learning tasks, outside of nmt.",4,2019
N19-1121,"another interesting future direction would be to explore different hand-engineered or learned data representations, which one could use to encourage models to agree on during training (e.g., make translation models agree on latent semantic parses, summaries, or potentially other data representations available at training time).",2,2019
N19-1122,another promising direction is to directly augment transformer encoder on recurrence modeling without the additional encoder.,1,2019
N19-1122,"Future work includes validating the proposed model in other tasks, such as reading comprehension, language inference, and sentence classification.",4,2019
N19-1125,"in future work, we will provide theoretical justification of the effectiveness of the proposed regularization terms.",6,2019
N19-1130,(2) literary criticism will be used as a testbed for evaluating representations.,3,2019
N19-1132,our future study will further examine the robustness of several existing evaluation metrics and explore new metrics appropriate for cross-corpora and/or cross-domain evaluation.,3,2019
N19-1133,"moreover, we also want to introduce more nlp prior knowledge into the model.",1,2019
N19-1133,"in the future work, we will investigate the ability of star-transformer by unsupervised pre-training on the large corpus.",2,2019
N19-1135,"furthermore, we will expand our work by investigating additionally the criminal offense of incitement to hatred ( 130 stgb) and its implication on the freedom of expression.",5,2019
N19-1135,"in future work, we will investigate the usefulness of layman-annotated data for an automated classification.",3,2019
N19-1136,future studies will explore the possibility of applying the title-guided attention mechanism to other large datasets on major social media platforms.,2,2019
N19-1136,"it is also interesting to see whether the semantic based loss regularizes can be adapted to improve the performance of the recent pre-trained transferable deep learning models, such as the bidirectional encoder representations from transformers (bert) (devlin et al., 2018).",1,2019
N19-1136,"it is also interesting to see whether the semanticbased loss regularisers can be adapted to improve the performance of the recent pre-trained transferable deep learning models, such as the bidirectional encoder representations from transformers (bert) (devlin , 2018).",1,2019
N19-1137,"in future, we would like to explore character embedding as this can give us crucial linguistic features from noisy twitter data.",1,2019
N19-1138,possible future research direction might be detecting cyber security related events in different languages.,2,2019
N19-1143,a promising future direction is to use hierarchical clustering to create better cluster-level representations.,1,2019
N19-1144,"we further plan to create similar datasets for other languages, following olids hierarchical annotation scheme.",2,2019
N19-1145,"in the future, we plan to explore a broader range of properties from kb to facilitate biomedical information extraction tasks.",4,2019
N19-1146,"furthermore, the mechanisms that represent linguistic features into symmetric spaces should be analyzed within the context of explainable ai.",1,2019
N19-1146,"in the future, the agreement among modalities concept may be applied to design objective functions for training classifiers in various tasks, and from other data sets (for example, education and occupation modalities for the bank marketing prediction task).",2,2019
N19-1147,we conjecture that these may serve well for cross-sentence relation extraction in long pieces of texts.,2,2019
N19-1147,"also, we only considered one relation type between each entity and bridge token but it is possible, and very likely that two different relation types may lead to a third relation type.",1,2019
N19-1147,"despite restricting ourselves to sors, it should be noted that the proposed method can be generalized to third and fourth order relations.",1,2019
N19-1148,"we expect to apply our trained model to various text-based psychotherapy applications, such as extracting and summarizing counseling dialogues or using the information to build a model addressing the privacy issue of training data.",4,2019
N19-1148,we hope our categorization scheme and our convmfit model become a stepping stone for future computational psychotherapy research.,1,2019
N19-1148,"convmfit is a seq2seq model for counselor-client conversation, however, another approach would be to model with existing non-goal oriented conversation models incorporating variational autoencoder (vae) (serban et al., 2017; park et al., 2018b; du et al., 2018).",1,2019
N19-1149,"as a future study, we will explore how these similarity measures predict performance of pretrained models in other nlp tasks.",4,2019
N19-1150,"in future work, routing strategies based on instance difficulty could be further investigated for budget-quality trade-off.",5,2019
N19-1151,"in addition, we also plan to explore the learning of emotional based representations by means of a deep neural network from which we could exploit local invariance properties to model fine-grained emotions.",1,2019
N19-1152,another possibility is the expansion of the test-set for a more accurate capture of the variance in the corpus.,2,2019
N19-1152,"future work includes manually correcting the human phenotype annotations that did not match any hpo identifier, with the potential of expanding the number of human phenotype annotations almost 2-fold and increasing the overall recall.",2,2019
N19-1152,"further, we intend to use semantic similarity to validate the human phenotype gene relations.",1,2019
N19-1152,"finally, the effect of different ner systems applied to the pipeline should be studied.",4,2019
N19-1152,"also, we intend to expand the corpus by identifying more missed gene annotations using pattern matching, which is possible due to our approach being fully automated.",2,2019
N19-1154,"we would also like to explore representations from robustly trained systems, which should improve performance on noisy input (belinkov and bisk, 2018; heigold et al., 2018).",1,2019
N19-1154,"finally, it would be interesting to study representations in other nlp tasks besides neural machine translation.",4,2019
N19-1154,"in future work, we plan to study how different units affect representation quality in non-recurrent models such as the transformer (vaswani et al., 2017) as well as in convolutional architectures (gehring et al., 2017).",1,2019
N19-1160,"there are various avenues to improving and extending derivation projection: alignment ambiguity could be handled with a global score, and multiple possible parses could be included in the target-language set, potentially improving the tradeoff between the number of projected derivations and the amount of noise.",1,2019
N19-1160,"to increase the range of structural differences between languages that can be handled, derivation projection could be extended to consider sub-token units and to handle 1:n translation units in addition to n:1 ones.",1,2019
N19-1161,"in the future work, we will integrate gaussian embeddings (vilnis and mccallum, 2015) with our approach.",1,2019
N19-1166,"developing a more expressive algorithm (e.g., one that incorporates reply-structure relationships) could boost predictive performance, and enable textual features to be less brittle.",1,2019
N19-1166,one promising avenue for future work is to examine higher-quality textual representations for conversation trees.,1,2019
N19-1168,"future work might investigate end to-end approaches, or develop alternative approaches that generate titles more similar to how humans write titles.",1,2019
N19-1170,"future work includes optimizing control settings automatically, and building more convincingly human-like chatbots.",1,2019
N19-1172,"language models conflate the two, so developing methods that are nuanced enough to recognize this difference is key to future progress.",1,2019
N19-1173,"in the future, we would like to generalize sumo to abstractive summarization (i.e., to learn latent structure for documents and sentences) and perform experiments in a weakly-supervised setting where summaries are not available but labels can be extrapolated from the articles title or topics.",1,2019
N19-1174,we also plan to improve the dataset by improving our models for contrastive pair prediction to reduce noise.,2,2019
N19-1174,"going forward, we hope to classify the viewpoint of the original claim and then generate a claim with a desired orientation.",5,2019
N19-1174,"furthermore, we hope to improve on the generation task by identifying the types of claims we encounter.",6,2019
N19-1175,"it has the potential of triggering novel research on multimodal deception data, specifically for speech and the dialogue dimension, which should be explored in the future.",5,2019
N19-1177,"in particular, we are looking into the bayesian method of simpson and gurevych (2018), which takes advantage of the sequential dependencies between bio tags, and works more robustly with noisy, subjective data such as ours.",1,2019
N19-1177,"for future work, we are investigating alternatives to mace, which was designed for categorical annotations rather than the sequence labelling of our task.",1,2019
N19-1178,"in the future, we may use more sophisticated ways to encode the entity information into the latent states.",1,2019
N19-1179,"in the future, we will continue to enrich document-level causal structures, e.g., by considering segment-wise topic layouts and rhetorical discourse structures.",2,2019
N19-1185,"in the future, we have a plan to leverage external knowledge and generalize our model for target independent stance detection in the same domain.",1,2019
N19-1186,our future work will improve we wpi to obtain high-quality evaluation scores in combination with other metrics.,3,2019
N19-1186,"moreover, we will use we wpi to improve nmt quality.",1,2019
N19-1187,we plan to further investigate the performance of testing time greedy decoding with beam search optimization during training.,3,2019
N19-1188,"moreover, we have shown that ihs outperforms even supervised models on downstream tasks of multilingual dependency parsing and document classification, and this anomaly requires further investigation in future work.",4,2019
N19-1193,"therefore, evaluating vbsix on other domains is a natural next step for our research.",3,2019
N19-1195,"our future work focuses on further increasing the accuracy of the self-generated advice model, so we can achieve better performance with no human effort.",1,2019
N19-1196,"furthermore, we plan to constrain our model to always predict paths that exist in the graph, as we discussed above.",1,2019
N19-1198,"other future directions of this work include adding temporal attention in order to handle more complicated temporal references and extending this approach to work for longer, and thereby more challenging videos such as movies.",2,2019
N19-1199,"also, our model only uses lexicosyntax features, and ignores acoustic features (e.g., pause duration) which are significant for dementia detection in english.",1,2019
N19-1199,"future work will investigate the use of automatic speech recognition to reduce the need for manual transcripts, which are impractical in a clinical setting.",5,2019
N19-1202,"thanks to a scalable corpus creation procedure initialized with constantly expanding ted talks data, future extensions will increase the coverage of the already present target languages and introduce new ones.",2,2019
N19-1208,"in the future, we intend to improve our approach by eliminating the static exploration schedule and binning strategy, and extend it to handle additional data attributes such as domain, style, and grammatical complexity.",1,2019
N19-1210,"future research should experiment with other datasets (reddits from other domains, other online communities) and also alternative models that address the challenges described here.",2,2019
N19-1213,"in future work, we plan to move from twitter to more generic domains and evaluate our approach to more tasks.",4,2019
N19-1213,"finally, we want to explore approaches for improving the adaptive layer unfreezing process and the contribution of the language model objective (value of ) to the target task.",1,2019
N19-1213,"additionally, we aim at exploring ways for scaling our approach to larger vocabulary sizes (kumar and tsvetkov, 2019) and for better handling of out-of-vocabulary words (oov) (mielke and eisner, 2018; sennrich et al., 2015) in order to be applicable to diverse datasets.",1,2019
N19-1215,we will develop a generic method of inducing personalized word embeddings for any subjective text.,1,2019
N19-1215,we plan to analyze relationships between semantic variations and user factors of writers who used the target words such as age and gender.,1,2019
N19-1216,"we further plan to go beyond left vs. right, which is not universal and can exhibit regional specificity (tavits and letki, 2009), and to model other kinds of biases, e.g., eurosceptic vs. europhile, nationalist vs. globalist, islamist vs. secular, etc.",5,2019
N19-1216,"in future work, we want to try more auxiliary tasks, and to experiment with other languages.",2,2019
N19-1217,"research on puns for other languages such as chinese is still under-explored, which could also be an interesting direction for our future studies.",2,2019
N19-1217,future research includes the investigations on how to make use of richer semantic and linguistic information for detection and location of puns.,1,2019
N19-1218,"a future line of work we are interested in is to test whether the knowledge learned with this dataset could be transferred to real-word actions (i.e. real-domain setups), or if such transfer is not possible and a model needs to be trained from scratch.",4,2019
N19-1223,"in future work, we would like to integrate a semantic parser into our model (yin et al., 2018).",1,2019
N19-1223,"this opens up scope for data augmentation for downstream nlp tasks, such as machine translation.",2,2019
N19-1223,"in addition, we would be able to generate text-to-text paraphrases by parsing into amr first and then carrying out the paraphrase generation procedure described in this paper (iyyer et al., 2018).",1,2019
N19-1223,"by integrating a component which parses into amr into our model, we can do semi-supervised learning on plentiful unannotated natural language sentences, and improve our amr generation performance even further.",1,2019
N19-1231,we will consider strategies such as wang et al.(2018a)s shift-reduced-based lstm architecture or sohrab and miwa (2018)s method of modeling the contexts of overlapping potential named entity spans with bidirectional lstms.,1,2019
N19-1231,"additionally, we will expand her to model hierarchically nested entity labels.",1,2019
N19-1231,"in future work, we will investigate sources of noise in performance to see if these are due to gaps in the model, idiosyncrasies of corpora, or both.",3,2019
N19-1234,"furthermore, the powerful transformer (vaswani et al., 2017) can be applied for the auto-encoder which could improve the performance of the proposed approaches.",1,2019
N19-1234,pre-train the auto-encoder before adversarial training can also improve the performance.,1,2019
N19-1234,"rl or self-attention (zhang et al., 2018) techniques can be used as a tool to accommodate this weakness.3.",1,2019
N19-1235,"for future work, we are interested in applying graph-to sequence neural networks (beck et al., 2018; song et al., 2018) to mrs-to-text generation.",4,2019
N19-1236,"additionally, the planning stage allows explicit user-control and generating diverse sentences, to be pursued in future work.",5,2019
N19-1237,"we plan to research over generating questions and using the reward signals to rerank the outputs, thereby including the inductive bias the rewards represent without allowing the model to exploit them.",5,2019
N19-1238,future work could address the problem of repetition and entity coverage in the generated texts.,5,2019
N19-1239,"one interesting direction is to investigate whether neuron can be extended to work on open-domain qa corpus, which may not be restricted to any specific domain.",2,2019
N19-1243,"in future, we plan to explore the option of augmenting falcon with deep learning methods for further improvement in performance specially in entity and relation extraction module.",1,2019
N19-1245,"in future work, we plan to extend our representation language and models to cover currently unsolvable problems, including sequence and high-order polynomial problems.",1,2019
N19-1247,"also, the proposed methods can be further extended to other chinese nlp tasks, such as cws, text classification, and sentiment analysis.",4,2019
N19-1247,"in the future, we plan to further improve and perfect the proposed method, such as exploring some strategies to handle oov words.",1,2019
N19-1253,"in the future, we plan to explore multisource transfer and incorporating prior linguistic knowledge into the models for better cross-lingual transfer.",1,2019
N19-1255,"self-discriminative learning shows potential 2473 for real-world scarcely-labeled scenarios, and our future work will focus on joint training of representations for semi-supervised learning.",1,2019
N19-1258,in future we would like to adapt our method to other domain adaptation tasks and consider more effective alternatives for the generator regularizer.,4,2019
N19-1259,"in addition, an end to-end opinion extractive summary method without given golden targets is also a future work.",1,2019
N19-1259,"in future works, towe could be utilized to further improve the performance on downstream sentiment analysis tasks with building a more interpretable model, such as enhanced-feature or multitask learning.",1,2019
N19-1261,evaluation of these techniques for benchmarking automated summarization systems is one direction for our future research.,3,2019
N19-1262,"on a more general level, the interactive setup and the active learning strategies presented can also be used for other natural language processing tasks, such as question answering, to transfer a model to a new domain or genre.",4,2019
N19-1262,"first, we intend to investigate further applications of our interactive setup, e.g., in movie subtitle compression or television closed captions where there is no sufficient training data to build neural models.",4,2019
N19-1266,"future work will possibly be visualizing the visual and language features encoded by amfe to find more straightforward interpretations, as well as trying our method on more complex structures, discriminative models, and on discriminative tasks such as vqa and visual reasoning.",4,2019
N19-1269,future work could also explore changes that modify existing content rather than simply appending.,5,2019
N19-1273,one potential future work direction is investigating whether they extend to other structured prediction problems beyond semantic parsing.,5,2019
N19-1275,"in future, we will further develop our system using structured attention (kim et al., 2017) and try to improve the accuracy of parsers in multi-tasking scenarios.",1,2019
N19-1276,"in future work, we will explore (1) the relationship between vocabulary coverage and segmentation performance, and (2) the effect of using pre-trained word vectors learned from different domain texts in domain adaptation scenarios.",1,2019
N19-1277,"in the future, we hope to apply our model to other downstream tasks and other logographic writing systems.",4,2019
N19-1281,"furthermore, our tagging approach should be universal and work with other tasks like named entity recognition.",4,2019
N19-1281,using retraining also seems to be a natural extension for this work.,1,2019
N19-1281,"we will consider how to make the model to recognize new words, which is an important feature for a practical analyzer.",1,2019
N19-1281,"it is easy to provide diverse models, required for tri-training, by using different types of encoder and varying network parameters.",1,2019
N19-1281,"a method to incorporate tags with a large number of possible values (like readings and lemmas) without introducing embeddings for them, hence keeping the models small, could also be a useful extension.",1,2019
N19-1282,in future work we hope to integrate syntactic parsing more closely with automatic speech recognition.,1,2019
N19-1282,"it may also be possible to more directly integrate an attention-based syntactic parser with a speech recognizer, perhaps trained in an end to end fashion.",1,2019
N19-1282,"a first step is to develop parsing models that parse asr output, rather than speech transcripts.",1,2019
N19-1284,"for future work, it could be interesting to explore guiding some latent modes with a few examples to pick up specific user features such as personality traits.",1,2019
N19-1285,"in future work, we hope to reduce the dependence on fluent target data during training through decoder pretraining on external non-conversational corpora or multitask learning.",2,2019
N19-1285,"further, standard metrics alone do not tell the full story for this task; additional work on evaluation metrics may better demonstrate the differences between such systems.",3,2019
N19-1287,"in the future work, we will consider to detect events and their sentence-level and document level factuality with a joint framework, and we will also continue to expand the scale of our dlef corpus.",2,2019
N19-1288,to deal with the multi-label problem of relation extraction and to integrate external knowledge into our model will be the tasks of our future work.,1,2019
N19-1292,one interesting future work is to incorporate the similar documents themselves into keyphrase generation.,1,2019
N19-1294,"as a part of future investigation, we plan to apply the approach to other distantly supervised tasks, such as relation extraction.",4,2019
N19-1295,"we also plan to investigate the use of variational inference (jordan et al., 1999) as a means of training our model.",1,2019
N19-1295,using variational inference might improve the stability and performance of our model.,1,2019
N19-1295,"for future work, we plan to apply our model to other nlp tasks such as relation extraction and named entity recognition.",4,2019
N19-1298,we aim to address them and further extensions of our model in future works.,1,2019
N19-1299,"in the future, we would like to explore effective ways of modeling more complex types of constraints (e.g., ordinal, comparison and aggregation).",1,2019
N19-1300,"future work could include building a document-level version of this task, which would increase its difficulty and its correspondence to an end-user application.",6,2019
N19-1302,"results on two challenging qa datasets, as well as our ablation study, indicate that entailment based qa can achieve state-of-the-art performance and is a promising direction for further research.",1,2019
N19-1306,"in the future, we plan to explore the following directions: (1) we may combine our method with recent denoising methods to further improve performance.(2) we may combine rule mining and reasoning technologies to learn better class embeddings to boost performance.(3) it will be promising to apply our method to zero-shot re and further adapt to other nlp scenarios.",4,2019
N19-1308,future directions include extending the framework to encompass more structural ie tasks such as event extraction.,1,2019
N19-1309,"in the future, we would like to improve extraction by training a model to extract (predicate, object) pairs directly without having to train on particular predicates.",1,2019
N19-1309,"such a model could potentially be based on visual clues common across websites, so a single model could be applied to many sites.",1,2019
N19-1313,"in future work, we plan to dig deeper on the benefits of sparse attention in terms of better interpretability of context aware nmt models.",1,2019
N19-1315,"in future work, it is interesting to use our method in generative adversarial networks to further improve the sentence generation models.",1,2019
N19-1318,we plan to apply our work towards building a notification system for incoming helpful posts.,4,2019
N19-1320,we will explore and incorporate other metrics to improve other aspects of generated texts such as the structural diversity in the future work.,1,2019
N19-1322,"in future, we will focus on each category to devise novel methodology for mitigating the errors of that category and in turn further increase the accuracy of g2p system in bangla.",1,2019
N19-1326,we are going to test deep architectures to combine the n-grams in misspellings to better capture various interdependencies of n-grams and correct versions of words.,3,2019
N19-1326,"finally, we will assess the robustness of both character-based (kim et al., 2016) and context-dependent embeddings (devlin et al., 2018), (peters et al., 2018) with respect to misspellings.",3,2019
N19-1326,"in the future, we will test different ways of training embeddings for misspellings including the extension of the same technique to multi-lingual embeddings.",1,2019
N19-1326,"finally, we will assess the robustness of both character-based (kim , 2016) and context-dependent embeddings (devlin , 2018), (peters , 2018) with respect to misspellings.",3,2019
N19-1327,"as future work, we plan to continue our investigation by extending the method with other ideas.",1,2019
N19-1327,"finally, we plan also to explore the use of analogy embeddings in other tasks, such as question answering and knowledge base population.",4,2019
N19-1327,"for instance, the use of positional embeddings, as well as the use of placeholders replacing the entities in the textual mentions are promising future directions.",1,2019
N19-1329,"a possible direction for future work would be to explore which of these explanations is true, possibly by decorrelating particular aspects of linguistic structure from language modeling representations.",1,2019
N19-1330,"in future work, we would like to obtain a better theoretical understanding of why starting the rnnlm from a zero state forms an effective n-gram regularize.",6,2019
N19-1330,"we would also like to extend our regularization approach to bilstms (peters et al., 2017) and transformers (alec radford and sutskever, 2018; devlin et al., 2018).",1,2019
N19-1331,"as a future direction, we expect to lift this assumption, for example, by updating the common direction statistics at a sentence level using autoconceptors (jaeger, 2014, section 3.14).",1,2019
N19-1331,"finally, the continual learning based sentence encoders should be applied to downstream applications in areas such as open domain nlp systems.",4,2019
N19-1334,scaling the gains derived from structural supervision is a challenge for data-scarce nlp and is the basis for future work.,5,2019
N19-1344,"in future work, we plan to consider multiple predicates and event-nouns.",1,2019
N19-1346,"future work includes leveraging external knowledge bases to disambiguate chunks and entities that appear within chinese addresses, as well as designing algorithms that are able to capture longer-range dependencies among chunks using alternative structures.",1,2019
N19-1347,"finally, un-supervised discourse-level structure extraction of fake/real news documents is a worthwhile research topic.",5,2019
N19-1347,"second, investigating the hierarchical structure at the word-level will be an exciting research inquiry.",5,2019
N19-1347,"first, we in-tend to define more advanced properties from the discourse dependency trees.",1,2019
N19-1348,we would like to expand them to more input sentences in future work.,2,2019
N19-1350,"as future work, we plan to modify our model to use multiple contexts in text to improve the quality of descriptions, considering the one sense per discourse hypothesis (gale et al., 1992).",1,2019
N19-1351,"for instance, we can make use of more lenient patterns that capture an even wider 3485 range of discourse markers, such as multi-word markers.",2,2019
N19-1351,"in future work, we also aim to increase the coverage of our method.",1,2019
N19-1352,"in the future, we plan to investigate broader applications like summarization, translation, question answering, etc.",4,2019
N19-1354,"we would also like to explicitly quantify the uncertainty captured in our framework under different sampling strategies or mcmc-sg methods (e.g., similar to mcclure and kriegeskorte (2016); teye et al.(2018)).",1,2019
N19-1356,"in future work, a human experiment based on the agreement prediction task can help determine whether the difficulty of our languages is consistent across humans and rnns.",3,2019
N19-1359,we expect that the two kinds of approaches can complement each other to further improve the expressiveness of multi-head attention.,1,2019
N19-1359,"future work includes combining our information aggregation techniques together with other advanced information extraction models for multihead attention (li et al., 2018).",1,2019
N19-1361,another relevant line of work is adapting our model to other domains containing documents with similar linked structured such as wikipedia articles.,4,2019
N19-1361,an interesting line of future work is to explore the design of such tasks or explore the properties or similarities between the auxiliary and the main tasks.,5,2019
N19-1361,future work may benefit from replacing elmo with other types of contextualized representations such as bert in our scaffold model.,2,2019
N19-1362,potential avenues for future work include multitasking bert with pair2vec in order to more directly incorporate reasoning about word pair relations into the bert objective.,1,2019
N19-1364,"in the future, we plan to build a richer taxonomy of persuasion strategies and incorporate additional neural architectures such as variational autoencoders to better represent sentences in each message to further assist the modeling of persuasiveness.",1,2019
N19-1364,"beyond the text, images and even audios may provide additional insights on the successes of persuasive requests.",2,2019
N19-1364,"our model also has important applications to other domains, such as in computational advertisements, micro-funding platforms and political campaigns.",4,2019
N19-1365,"it is our hope that these lessons extend to other richly compositional, context-sensitive language understanding tasks.",4,2019
N19-1365,"we believe exploring more powerful variants of dispatching is an interesting avenue for future work, as is pretraining routing models on language model tasks using large corpora.",2,2019
N19-1366,"further exploration of graph encoders is left to future work, which may result crucial to improve performance further.",1,2019
N19-1367,"future work will involve extending the set of features involved, incorporating data from other languages, and testing whether similar techniques can be effective for detecting earlier stages of cognitive decline, such as mci.",2,2019
N19-1368,"in future work, we will investigate structured and unstructured knowledge sources to find explanations for sentiments and emotions.",2,2019
N19-1371,this motivates a key future research direction: designing more sophisticated attention mechanisms that (conditionally) identify spans of evidence pertinent to a given prompt.,5,2019
N19-1374,future work could include incorporating contextual information that would help emoticons to better capture emotional content.,1,2019
N19-1376,"moving forward, we would like to explore a more realistic multi-modal topic spotting system.",1,2019
N19-1379,"for future work, we consider extending the model to handle unknown words.",1,2019
N19-1379,"also, we want to find a more principled way to down sample the negative exemplars.",1,2019
N19-1380,"this presumably makes sense for the english-thai transfer learning case since these two languages use different writing systems but given the results by lin et al.(2018) and yang et al.(2017), we would expect additional improvements by sharing character embeddings for languages with similar writing systems.",1,2019
N19-1380,"second, one could try to include a specific learning objective to embed translations into a similar vector space as used by yu et al.(2018a) and conneau et al.(2018) for multilingual sentence representations.",1,2019
N19-1380,"despite the range of models that we considered in this paper, we only scratched at the surface of possible cross-lingual (embedding) models, and hence there are many future directions of this work.",1,2019
N19-1383,"in the future, we will further explore selecting the feature-consistent annotations from the source language and add to the target language, and explore unsupervised pretrained cross-lingual language models (peters et al., 2018; radford et al., 2018; devlin et al., 2018; lample and conneau, 2019) for cross-lingual low resource name tagging.",1,2019
N19-1384,"in our future work, we will study the impact of using more partial translations of better quality to train nmt systems.",2,2019
N19-1384,"we will also analyze whether our partial translations are useful because of their noisy nature, since noisy synthetic data have recently been proven useful in some specific configurations (edunov et al., 2018).",3,2019
N19-1384,we assume that we can collect better partial translations by searching in more monolingual data.,2,2019
N19-1385,future work should investigate the effect of our proposed reordering methods on truly low resource machine translation.,3,2019
N19-1386,"in future work, we plan to incorporate knowledge from the similarity space in our adversarial framework.",1,2019
N19-1387,we would also like to explore alternative methods to address word-order divergence which do not require expensive parsing of the assisting language corpus.,1,2019
N19-1387,"further, use of pre-ordering to address word-order divergence for multilingual training of other nlp tasks can be explored.",4,2019
N19-1387,"while the current work focused on indian languages, we would like to validate the hypothesis on a more diverse set of languages.",2,2019
N19-1388,understanding and improving zero-shot performance in such scenarios is also a promising direction for future work.,1,2019
N19-1388,"there are many possible avenues for future work, including semi-supervised learning in such settings, exploring ways to reduce the performance degradation when increasing the number of languages, or using such models for multilingual transfer learning (mccann et al., 2017; eriguchi et al., 2018; artetxe and schwenk, 2018).",1,2019
N19-1390,"an interesting direction to pursue would be to use multiple related languages, to aid lexical relation classification in an under resourced language, instead of transferring supervision from a single language (english).",2,2019
N19-1390,"beyond classification, another direction for future work is to extend our approach to distinguish synonyms and antonyms from unrelated word pairs.",1,2019
N19-1390,"the simplicity of our approach allows to easily incorporate other features (e.g., morphological marking) and it can be extended to further languages or lexical relations.",2,2019
N19-1391,"in future work, we will explore the viability of the sentence mapping approach on other sentence embedding models.",1,2019
N19-1393,"whereas a delexicalized parser offers a simple experimental setup, it impacts parsing performance.",1,2019
N19-1393,"as future work, we plan to study the influence of typological features on each dependency type.",3,2019
N19-1400,"since our task has two elements, summarization and chit-chat, the focus of our future work will be a more sophisticated multitask model that considers these relations.",1,2019
N19-1400,we also plan to improve the proposed method so that it can generate even better initial utterances.,1,2019
N19-1400,"in that case, depending on the users interest, the model needs to determine whether to do a usual chat or talk about the news contents.",1,2019
N19-1400,"as a natural next step, we plan to develop a more sophisticated conversation model, which can not only generate initial utterances but also continue the conversation for the given news contents (yoshino and kawahara, 2014).",1,2019
N19-1403,"in the future, we intend to generalize our model to other relationships beyond strict entailment and contradiction relations.",1,2019
N19-1404,"for future work, we will extend our study to examine saliency learning on nlp tasks in an active learning setting where real explanations are requested and provided by a human.",3,2019
N19-1408,one potential extension of this work is to conduct a comprehensive ablation study to determine the relative contribution of each of the regularization and optimization techniques.,3,2019
N19-1408,"finally, the examined regularization and optimization methods deserve exploration in other nlp tasks as well.",4,2019
N19-1409,in future research we will investigate ways to improve the decoder with pre-trained representations.,1,2019
N19-1410,"future work might allow finer-grained modeling of the tradeoff between under- and over-informativity within the sequence generation pipeline (e.g., with a learned communication cost model) or explore applications of pragmatics for content selection earlier in the generation pipeline.",1,2019
N19-1414,"going forward, we will test this model on other tasks, diagnostic and otherwise, to see its generalizability.",4,2019
N19-1417,"also, motivated by our findings, we will work on utilizing continuous space representations as side information in sampling the parameters of bkn, i.e. similar to zhao et al.(2018), which potentially can reduce the gap between bkn and neural models.",1,2019
N19-1419,"beyond this actionable insight, we suggest our probe may be useful for testing the existence of different types of graph structures on any neural representation of language, an exciting avenue for future work.",2,2019
N19-1420,"furthermore, a deeper and robust quantum inspired neural architecture in a higher-dimension hilbert space like (zhang et al., 2018b) is also worth to be investigated for achieving stronger performances with better explanatory power.",1,2019
N19-1420,"another possible direction is to borrow other quantum concepts to capture the interaction and non-interaction between word semantics, such as the fock space (sozzo, 2014) which considers both interacting and noninteracting entities in different hilbert spaces.",1,2019
N19-1420,"despite the effectiveness of the current network, we would like to further explore the phase part in complex-valued word embedding to directly link to concrete semantics such as word sentiment or word position.",1,2019
N19-1422,"in the future, we would like to devise models that can learn when and how to integrate multiple modalities by taking care of the complementary and redundant aspects of them in an intelligent way.",1,2019
N19-1424,"finally, although we focus on english, we expect our method will work well for other languages, but leave this direction for future work.",2,2019
N19-2002,"we plan to address several issues, including but not limited to: 1) how can we capture and share knowledge of common patterns of utterances belonging to the same domain but written in different languages across different locales?2) how can we prevent a locale from interfering with other locales using different language for learning linguistic context of utterances?",5,2019
N19-2005,"furthermore, we plan to extend the graph convolution framework to other tasks in vrds, such as document classification.",4,2019
N19-2006,m-cvae thus opens up new avenues to improve the quality of responses further through personalization and stylization.,1,2019
N19-2007,"in future work, we plan to develop a more comprehensive annotation guide for error analysis in real-world goal-oriented systems.",2,2019
N19-2010,"future research will include verifying how much our headline generation model can affect practical performance indicators, such as click-through rate.",3,2019
N19-2018,we would also like to jointly model the relation extractor and the entity linker to improve the model performance.,1,2019
N19-2018,"in future work, we would like to investigate its effectiveness and robustness in a cross-lingual setting.",3,2019
N19-2019,"as a future research direction for conversational ai, we think that to train and test a k-nn model for predicting which dialog move to take will be beneficial as well.",1,2019
N19-2020,"we share the challenges of annotating a user reported bug dataset with non-technical annotators, as opposed to using annotations from engineers.",2,2019
N19-2023,"in the future, the proposed approach could be applied to other dissimilar language pairs, e.g. english and chinese.",2,2019
N19-2023,other possible extensions include using multi-lingual embeddings that could complement the currently transferred weights.,1,2019
N19-2025,"in future work, we plan to address this through several directions, including end to end de-id (ghannay et al., 2018), lattice-based techniques (ladhak et al., 2016), and diarization and segmentation of the audio as part of the transcription process (cerva et al., 2013).",5,2019
N19-2025,"in future work, we plan to address this through several directions, including end-to-end de-id (ghannay , 2018), lattice-based techniques (ladhak , 2016), and diarization and segmentation of the audio as part of the transcription process (cerva , 2013).",1,2019
P00-1004,we expect further improvement by assigning translation scores according to corpus statistics.,1,2000
P00-1004,this will be the main focus for future work.,6,2000
P00-1005,and research on restoring the necessary information in english sentence has to be done.,1,2000
P00-1005,"in the future, it is needed to use the syntactic collocational information of korean to reduce ambiguities in pattern matching step.",1,2000
P00-1006,"finally, i suggest that it may be fruitful to explore the idea of using a memd model for p (w i h, s) as an alternative to the noisy-channel approach to smt.",1,2000
P00-1007,"in a future work, we plan to use the system for providing instance candidates, and disambiguate them using an algorithm more suitable for handling lexical information.",1,2000
P00-1010,"we also hope to handle a wider class of time expressions, as well as further improve our extraction and evaluation of event chronologies.",3,2000
P00-1010,"in the future, we expect to improve the integration of various modules, including tracking the temporal focus in the time resolver, and interaction between the event order and the event-aligner.",1,2000
P00-1012,"furthermore, any realistic dialog system would make use of some limited vocabulary  for which semantic information would be available.",1,2000
P00-1012,"first, while semantic information is not available for all adjectives, it is clearly available for some.",1,2000
P00-1012,future work will pursue at least two directions for improving the results.,1,2000
P00-1014,a promising approach considers the mutual information between the prepositional relationship of candidate attachments and n2.,1,2000
P00-1015,these experiments triggered an interesting future research challenge: how to cluster certain basenp rules into certain identifiers so as to improve the precision of both basenp and pos tagging.,5,2000
P00-1021,"we showed how a set of importance scores and inference rules can be used as the basis for agents with different discourse strategies, and how the discourse control techniques of interruption, abbreviation, repetition and silence can be used not just to moderate the discourse of an individual agent, but also the interaction between agents.",1,2000
P00-1024,"in addition, the difficulty of this problem depends on the number of attributes available for describing an object in the domain; our nominal expression generator has to correctly make four different decisions to achieve an exact match to human performance.",5,2000
P00-1024,"in future work, we plan to perform similar experiments on different corpora with different communications settings and problem types (e.g.planning, scheduling, designing) to determine whether our findings are specific to the genre of dialogues that we examine here, or whether they are more general.",3,2000
P00-1030,"our results show that, of all the collocation measures we investigated, bigram word predictability has the strongest correlation with pitch accent assignment.",1,2000
P00-1030,"however, our combined model performs best of all, suggesting that both contextual and non-contextual features of a word are important in determining whether or not it should be accented.",1,2000
P00-1031,"compared to the baseline of system, our system gets approximate 30% error reduction.",1,2000
P00-1032,the future research may include: pruning the approximate word matching result before they take part in the approximate segmentation.,1,2000
P00-1038,"furthermore, using faqs allows us to assess the effectiveness of applying standard statistical learning machinery maximum-likelihood estimation, the em algorithm, and so on—to the qrs problem.",3,2000
P00-1038,"although this work is meant as an opening salvo in the battle to conquer summarization with quantitative, statistical weapons, we expect in the future to enlist linguistic, semantic, and other non-statistical tools which have shown promise in condensing text.",1,2000
P00-1041,this resulted in a working system that could simultaneously translate and summarize japanese documents.8 the performance of the system could be improved by improving either content selection or linearization.,1,2000
P00-1050,"based on our clarified definition, we can easily see that the alignment problem is essentially the problem of bilingual word similarity.",5,2000
P00-1051,"e.g., we might have a center of coherence, analogous to sidner’s discourse focus, and that can be realized indirectly; and a center of salience, similar to her actor focus, and that can only be realized directly.",1,2000
P00-1051,we believe however more work is still needed to identify a completely satisfactory way of breaking up sentences in utterance units.,1,2000
P00-1053,"by comparing the types of referring expressions for which vt and the stack-based model fail, we also show that vt provides a better model for determining dras.",1,2000
P00-1058,"one, also suggested by (chen and vijay-shanker, 2000), is to group elementary trees into families and relate the trees of a family by transformations.",1,2000
P00-1058,"as for future work, there are still possibilities made available by tag which remain to be explored.",6,2000
P00-1058,another possibility is the use of multiplyanchored trees.,1,2000
P00-1059,"since sparseness of data is a major problem, we intend to train the models on the new brown corpus of 30,000,000 words.",2,2000
P00-1061,"furthermore, it is important to show that embased methods can be applied successfully also to other statistical parsing frameworks.",4,2000
P00-1068,"the tasks to be achieved are: 1. to establish ontology of semantic relationship description, 2. efficient methodology for preparing the lexical items comprising semantic constraints, 3. to communicate semantic contexts and situations to the students through assisting reading the texts by way of bidirectionally linking the text words with an electronic dictionary, 4. to deal with anaphora.",1,2000
P00-1069,"thus, the proposed method seems especially effective and useful for the languages for which a largescale sense-tagged corpus is not available yet.",4,2000
P00-1070,we have analysed and measured the effects of applying pronominal anaphora resolution in qa systems.,3,2000
P00-1070,the analysis of information referenced pronominally in documents has revealed to be important to tasks where high level of recall is required.,4,2000
P00-1071,"it is not sufficient to classify only the types of questions alone, since for the same question the answer may be easier or more difficult to extract depending on how the answer is phrased in the text.",5,2000
P00-1072,"if we make a matrix from any given information once, we can use the reduced matrix for estimating probability.",1,2000
P00-1078,"to develop continuous speech recognition, a large-scale speech corpus is needed.",2,2000
P01-1003,we will also be working on the evaluation of the word-error rate (wer) of the wsme model.,3,2001
P01-1008,"in this paper, we presented a method for corpus based identification of paraphrases from multiple english translations of the same source text.",2,2001
P01-1014,"the larger implication is that we begin to see that there are underlying discourse elements in essays that can be identified, independent of the topic of the test question.",5,2001
P01-1015,"a revision based architecture might require synthetic events to “wake up” a module to do some more work, after it has finished its first pass.",1,2001
P01-1017,we believe that the adaptation of prosodic information to parsing use is a worthy topic for future research.,1,2001
P01-1017,we believe that the resulting text grossly underrepresents the useful grammatical information available to speech-recognition systems.,5,2001
P01-1017,"first, we believe that information about rare or even truly unknown words would be useful.",2,2001
P01-1017,"finally, we have noted two objections to immediate-head language models: first, they complicate left-to-right search (since heads are often to the right of their children) and second, they cannot be tightly integrated with trigram models.",5,2001
P01-1017,the possibility of trigram-less language models makes the second of these objections without force.,5,2001
P01-1023,we also are interested in further evaluating the technique in an unrestricted domain such as the wall street journal (wsj) with shallow semantics such as the wordnet top-category for each np-head.,3,2001
P01-1023,"from our learned results, we have inferred placement constraints of the new information in relation to the previous plan elements without further interviews with experts.",1,2001
P01-1024,"although we provided here an account specific to german, our framework intentionally permits the definition of arbitrary language-specific topologies.",1,2001
P01-1026,"future work would include generating information associated with more complex interrogations, such as ones related to how and why, so as to enhance web based natural language understanding.",1,2001
P01-1026,"in addition, we proposed a question answering system, which answers interrogative questions associated with what, by using a web-based encyclopedia as a knowledge base.",1,2001
P01-1026,"in addition, when we used both resources, the performance was further improved.",2,2001
P01-1027,we believe that by performing a rescoring on translation word graphs we will obtain a more significant improvement in translation quality.,1,2001
P01-1028,"and similarly, whether the description based treatment of discourse parsing sketched in (duchier and gardent, 2001) could be used to generate discourse.",5,2001
P01-1028,"in particular, we intend to investigate whether the dependency grammar presented in (duchier, 1999), once equipped with a semantics, could be used not only for parsing but also for generating.",5,2001
P01-1031,even choosing among the responses in (5) might be a pretty knowledge intensive business.,5,2001
P01-1034,"as a final stage, we may find it useful to follow kasper (1999) and have a ‘fallback’ strategy for failed parses where the best partial analyses are assembled in a robust processing phase.",1,2001
P01-1034,"moreover, we have not yet incorporated any domain specific lexical knowledge from, e.g., umls but we would expect this to contribute to improved performance.",1,2001
P01-1034,"although we are only able to parse between 30 and 40 percent of the corpus, we will be able to improve on that figure quite considerably in the future through continued development of the pre-processing component.",2,2001
P01-1035,another interesting issue is the evaluation method used for taggers.,3,2001
P01-1040,"we are, therefore, at a point where the creation and use of annotated data and concerns about the way it is represented can be treated separately鉂憈hat is, researchers can focus on the question of what to encode, independent of the question of how to encode it.",5,2001
P01-1041,we hope our method can be applicable to other languages.,4,2001
P01-1043,"future improvements of the tool will consist in adding a module to annotate syntactic functions, and complete valency information for verbs, with the help of a lexicon (kinyon, 00).",1,2001
P01-1043,"finally, from a theoretical point of view, it may be interesting to see if our rules could be acquired automatically from raw text (although this might not be worth it in practice, considering the small number of rules we use, and the fact that acquiring the rules in such a way would most likely introduce errors).",5,2001
P01-1048,"our future research will focus upon combining these sources of information identifying system errors and user corrections, and investigating strategies to make use of this information, including changes in dialogue strategy (e.g.from user or mixed initiative to system initiative after errors) and the use of specially trained acoustic models to better recognize corrections.",1,2001
P01-1050,"the second sentence is correctly translated only when the system uses a tmem seed; and fortunately, the translation of highest probability is the one obtained using the tmem seed.",1,2001
P01-1053,future work is to apply our method to a variety of other languages.,4,2001
P01-1056,"for this type of problem it is possible to evaluate the generator by the degree to which it matches human performance (yeh and mellish, 1997).",3,2001
P01-1057,"we hope that more such evaluations are performed in the future, and that their results are reported whether they are positive or negative.",3,2001
P01-1059,"future directions could include improved sentential descriptions as well as further intrinsic and extrinsic evaluations of the summarizer as a whole (i.e., including canned text).",3,2001
P01-1060,"in collaboration with pranav anand and eric breck, we have incorporated governor markup in the question answering prototype, but not debugged or evaluated it.",3,2001
P01-1060,this idea could be exploited in other markup tasks.,4,2001
P01-1069,"furthermore, the success of regularized winnow in text chunking suggests that the method might be applicable to other nlp problems where it is necessary to use large feature spaces to achieve good performance.",4,2001
P01-1070,"in the longer term, we plan to explore the use of coverage results to enable an enhanced qa system to compose an appropriate answer from information found in the retrieved documents.",4,2001
P01-1070,"we also propose to investigate predictive models which return more informative predictions than those returned by our current model, e.g., a distribution of the probable informational goals, instead of a single goal.",1,2001
P01-1070,we intend to use the insights obtained from this experiment to construct models which can capture probabilistic dependencies among variables.,1,2001
P02-1001,we are particularly interested in the potential for quickly building statistical models that incorporate linguistic and engineering insights.,1,2002
P02-1001,"bringing diverse models into the same declarative framework also allows one to apply new optimization methods, objective functions, and finite-state algorithms to all of them.",1,2002
P02-1001,"for example, it should be possible to do end-to-end training of a weighted relation defined by an interestingly parameterized synchronous cfg composed with tree transducers and then fsts.",1,2002
P02-1003,"in the future, it will first of all be necessary to lift the restrictions we have placed on the tag grammar: so far, the nodes of the elementary trees are only equipped with nonterminal labels and indices, not with general feature structures, and we allow only a restricted form of adjunction constraints.",1,2002
P02-1003,"it should be possible to either encode these constructions directly in the dependency grammar (which allows user-defined features too), or filter out wrong realizations in a post-processing step.",1,2002
P02-1004,it appears that much of the feature extraction and many of the linguistic operations are reusable.,5,2002
P02-1013,"once disjunctive and negative relations are used, interesting questions arise as to how these should be realized.",5,2002
P02-1013,"how should conjunctions, disjunctions and negations be realized within the sentence?",5,2002
P02-1013,one area that deserves further investigation is the relation to surface realisation.,1,2002
P02-1014,"as noted above, for example, it is important to automate the precision-oriented feature selection procedure as well as to investigate other methods for feature selection.",1,2002
P02-1014,"we also plan to investigate previous work on common noun phrase interpretation (e.g.sidner (1979), harabagiu (2001)) as a means of improving common noun phrase resolution, which remains a challenge for state-of-the-art coreference resolution systems.",1,2002
P02-1014,"nevertheless, there is substantial room for improvement.",6,2002
P02-1016,"we also want to test the algorithms developed here on other domains (e.g., wall street journal corpus).",3,2002
P02-1016,improving speed of sentence clustering is also worthwhile.,1,2002
P02-1018,there are medications and variations on this algorithm that are worth exploring in future work.,1,2002
P02-1019,a subject of future research is looking for a better way to combine the two error models or building a single model that can recognize whether there is a phonetic or typographic error.,1,2002
P02-1020,we are adapting the gst algorithm to deal with simple rewrites (e.g. synonym substitution) and to observe the effects of rewriting upon finding longest common substrings.,1,2002
P02-1021,"in the future, i am planning to test the assumption that abbreviations and their expansions occur in similar contexts by testing on hand-labeled data.",2,2002
P02-1021,it will also be necessary to extend this approach to other medical and possibly non-medical domains with larger data sets.,2,2002
P02-1021,"finally, i will experiment with combining the umls abbreviations table with the mayo clinic specific abbreviations.",1,2002
P02-1021,i also plan to vary the size of the window used for determining the local context from two words on each side of the expression in question as well as the cutoff used during me training.,1,2002
P02-1022,one future direction is the integration of processing resources which learn in the background while the user is annotating corpora in gates visual environment.,2,2002
P02-1023,"for our future work, more experiments will be performed on other language models such as word-based bigram and trigram for chinese and english.",3,2002
P02-1023,more pruning criteria and their combinations will be investigated as well.,1,2002
P02-1026,"for example, sentences will differ in how much useful contextual information they carry.",5,2002
P02-1026,are there useful generalizations to be made?,5,2002
P02-1026,"e.g., might the previous sentence always be the most useful, or, perhaps, for newspaper articles, the first sentence?",5,2002
P02-1026,can these measurements detect such already established contextual relations as the given-new distinction?,5,2002
P02-1026,what about other pragmatic relations?,5,2002
P02-1026,All of these deserve further study.,5,2002
P02-1027,"thus, the apparent ability of certain indicators which were developed for one class, and hence one type of thematic assignment, to become useful indicators for other classes seems to suggest that the inventory of thematic roles that we have explored here should be decomposed into finer-grained primitives.",1,2002
P02-1027,"we are currently exploring that approach, along with our on-going investigation of other languages, and additional verb classes.",1,2002
P02-1028,we also would like to apply the proposed method to a word selection task in machine translation.,4,2002
P02-1028,we are planning to extend our dictionary based paraphrasing system to more complicated phrases and sentences.,4,2002
P02-1029,"future work will concern the extension of the clustering experiments to a larger number of verbs, both for the scientific purpose of refining our understanding of the semantic and syntactic status of verb classes and for the more applied goal of creating a large, reliable and high quality lexical resource for german.",3,2002
P02-1029,"for this task, we will need to further refine our verb classes, further develop the repertoire of syntactic frames which we use, perhaps improve the statistical grammar from which the frames were extracted and find techniques which allow us to selectively include such information about selectional preferences as is warranted by the availability of training data and the capabilities of clustering technology.",1,2002
P02-1030,we would like to extend this analysis to at least one billion words for at least the most successful methods and try other tools and parsers for extracting the contextual information.,2,2002
P02-1032,"for example, we would like to be able to interpret titles in terms of semantic relations, for example, transforming congenital anomalies of tracheobronchial branching patterns into a form that allows questions to be answered such as 鈥淲hat kinds of irregularities can occur in lung structure?鈥 we hope that by compositional application of relations to entities, such inferences will be possible.",1,2002
P02-1037,future work will include evaluating performance when scores from the acoustic and/or n-gram models are incorporated for ranking competing candidate parses.,3,2002
P02-1041,"in future work, we intend to combine techniques for building wide-coverage statistical parsers for ccg (hockenmaier and steedman, 2002; clark , 2002) with corpora that have explicitly marked semantic dependency relations (such as the prague dependency treebank and negra) to produce hlds terms as the parse output.",1,2002
P02-1042,in future work we will present an evaluation which teases out the differences in extracted and insitu arguments.,3,2002
P02-1045,"while we restricted ourselves in this work to rather small sets of labeled training data, future work on co-training will include further experiments with larger data sets.",2,2002
P02-1049,"in future work, we hope to improve our results by trying different machine learning methods; including the user dialogue act types as input features; and testing these methods in new domains.",1,2002
P02-1050,"to improve the quality of the input analyses, we are adapting active learning and co-training techniques (hwa, 2000; sarkar, 2001) to exploit the most reliable data.",2,2002
P02-1051,we would like to apply to other languages such as chinese and japanese and to investigate whether the current algorithm would perform as well or whether new algorithms might be needed.,2,2002
P02-1052,"it will be interesting to see how the similarity and clustering method will work in conjunction with other word alignment algorithms, as the dictionary rebuilding algorithm is independent of the actual word alignment method used.",1,2002
P02-1052,"furthermore, we plan to explore ways to improve the similarity scoring algorithm.",1,2002
P02-1052,"in general, we hope to move automated dictionary extraction away from pure surface form statistics and toward dictionaries that are more linguistically motivated.",1,2002
P02-1055,we therefore plan to experiment with a pos tagger and an attenuated words variant that use exactly the same word form information.,3,2002
P02-1055,in addition we also want to pursue using the combined chunker and grammatical function tagger described here as a first step towards grammatical relation assignment.,1,2002
P02-1058,we are analyzing the duc evaluation scores in the hope of suggesting improved and more stable metrics.,3,2002
P02-1058,we would like to apply some compression techniques or use linguistic units smaller than sentences to improve our retention score.,1,2002
P03-1003,"however, building dedicated systems that employ more sophisticated, qa-motivated generative stories is likely to yield significant improvements.",1,2003
P03-1003,"it is remarkable that a statistical machine translation system can do so well in a totally different context, in question answering.",4,2003
P03-1007,further research is warranted to improve the results further.,2,2003
P03-1008,"in the future, we will experiment with combining grammatical features and local/topical cooccurrences.",1,2003
P03-1009,"in the future, we plan to investigate the use of soft clustering (without hardening the output) and develop methods for evaluating the soft output against polysemous gold standards.",3,2003
P03-1010,it has attracted the attention of people both inside and outside the nlp community.,6,2003
P03-1010,"it is therefore reasonable to expect that they can be applied to any language pair and still retain good performance, particularly since their effectiveness has been demonstrated in such a disparate language pair as japanese and english.",4,2003
P03-1013,testing our sister-head model on these languages is a topic for future research.,2,2003
P03-1013,such annotation schemes are often used for languages that (unlike english) have a free or semi-free wordorder.,4,2003
P03-1013,it can be hypothesized that this finding carries over to other treebanks that are annotated with flat structures.,4,2003
P03-1016,our future work will extend synonymous expressions of the collocations to words and patterns besides collocations.,1,2003
P03-1018,"we hope that these preliminary aspects will be initial gains in developing a concrete and effective system for learning, representing and composing aspects of lexical meaning.",1,2003
P03-1019,future work will include the automatic extraction of the bilingual grammar as well as the use of this grammar for the translation process.,1,2003
P03-1021,the following important questions should be answered in the future: how many parameters can be reliably estimated using unsmoothed minimum error rate criteria using a given development corpus size?,5,2003
P03-1021,we expect that directly optimizing error rate for many more parameters would lead to serious overfitting problems.,5,2003
P03-1021,is it possible to optimize more parameters using the smoothed error rate criterion?,5,2003
P03-1021,which error rate should be optimized during training?,5,2003
P03-1021,this relates to the important question of which automatic evaluation measure is optimally correlated to human assessment of translation quality.,5,2003
P03-1023,"in our future work, we intend to adopt a looser filter together with an anaphor city determination module (bean and riloff, 1999; ng and cardie, 2002b).",1,2003
P03-1023,"furthermore, we would like to incorporate more syntactic features into our feature set, such as grammatical role or syntactic parallelism.",1,2003
P03-1023,"only if an encountered np is determined as an anaphor, we will select an antecedent from the candidate set generated by the looser filter.",1,2003
P03-1023,these features may be helpful to improve the performance of pronoun resolution.,1,2003
P03-1027,"our early exploration of the application of this work for corpus analysis (u.s. state of the union addresses) has produced interesting results, and we expect that the continued development of this resource will be important to the success of future corpus analysis and human-computer interaction projects.",4,2003
P03-1029,"in particular, we would like to relax the restraint that all the fills must be tagged with their proper ne tags by introducing a generic place-holder into the extraction patterns.",1,2003
P03-1029,there are several ways in which our pattern model may be further improved.,1,2003
P03-1029,also patterns with a generic place-holder can be applied to slots that are not names.,1,2003
P03-1030,"in future work, we plan to evaluate the impact of the hellinger metric on recall.",3,2003
P03-1030,"in addition, we plan to use anaphora resolution which was shown to improve recall (pirkola and jrvelin, 1996) to enhance the ned system.",1,2003
P03-1031,"these include the use of statistical information other than the probability of a dialogue act type sequence and the collocation probability of dialogue states and dialogue acts, the optimization of weighting factors α, β, γ, other default parameters that we used in the experiments, and more experiments in larger domains.",1,2003
P03-1031,there still remain several issues that we need to explore.,5,2003
P03-1031,"despite these issues, the present results have shown that our approach is promising.",1,2003
P03-1038,"because all the markov models are represented as decision trees in the framework, the models are hu man readable and we are planning to develop editing tools for self-organizing markov models that help experts to put human knowledge about language into the models.",1,2003
P03-1038,"by adopting x2-test as the criterion for potential improvement, we can control the degree of context extension based on the confidence level.",1,2003
P03-1040,"the next step would be verb clauses, where modeling of the subcategorization of the verb is important.",1,2003
P03-1040,our long term goal is to address additional syntactic constructs in a similarly dedicated fashion.,4,2003
P03-1046,"future work will address the question whether these models can be run with a less aggressive beam search strategy, or whether a different parsing algorithm is more suitable.",5,2003
P03-1050,"we are planning to experiment with different languages, translation model alternatives, and to extend task-based evaluation to different tasks such as machine translation and cross-lingual topic detection and tracking.",2,2003
P03-1053,"future work: though the ldd has been validated against childes data in certain respects, we intend to extend this work by adding distributions to the ldd that correspond to actual distributions of child-directed speech.",1,2003
P03-1056,"in addition to further pcfg refinements, tuning the dependency model may lead to improved performance.",1,2003
P03-1056,"for the future, we believe that there is still room for considerable improvement in ctb parsing under our model.",1,2003
P03-1060,"in the future, we plan to extend the context to improve the performance.",1,2003
P03-1062,"moreover, we plan to investigate the perceptual cost of false insertions and deletions of accents and breaks in experiments with human listeners.",3,2003
P03-1065,"in future research, we plan to extend the successful experiment on phrasal verbs to other types of multi-word expressions and idioms using the same expert lexicon formalism.",3,2003
P03-1066,correlating the accuracy of the dependency parser as a parser vs. its utility in cer reduction may suggest a useful direction for further research.,1,2003
P03-1066,"in particular, as discussed in section 6, syntactic dependency structure is believed to capture useful information for informed language modeling, yet further improvements may be possible by incorporating non-syntax-based dependencies.",1,2003
P03-1066,there are many possibilities for future improvements.,6,2003
P03-1070,one of the most important future directions is to establish a more comprehensive model of face to-face grounding.,1,2003
P03-1071,"in future work, we would like to investigate the effects of adding prosodic features, such as pitch ranges, to our segmenter, as well as the effect of using errorful speech recognition transcripts as opposed to manually transcribed utterances.",1,2003
P03-2002,the next step in our project is the selection of relevant words given the concepts annotating them and the topic segments where they appear.,1,2003
P03-2002,selection will be based on a combination of a probabilistic model taking into account the probability of observing a concept given a word and the probability of observing that concept given a relevant topic.,1,2003
P03-2004,"in particular, we want to use semantic plausibility to rescore/rerank n-best recognition hypotheses.",3,2003
P03-2004,first we want to try to further improve the results presented in this paper by using better optimization methods for the machine learners (e.g. cross-validation optimization to avoid over-fitting on the development data).,1,2003
P03-2004,we also want to do a more thorough investigation of the rule sets generated by ripper to find out which features were most important for classification.,5,2003
P03-2004,"for example, we can add the words in the recognition hypothesis as a set-valued feature when using ripper.",1,2003
P03-2004,further improvement of the results might also be achieved by considering other features for prediction.,1,2003
P03-2004,a long-term goal is to combine the (acoustic) quality prediction with a notion of semantic plausibility in an actual dialog system.,1,2003
P03-2004,Future work aims in two directions.,6,2003
P03-2005,future work will focus on user adaptation and on the user interface to make best use of mmif.,1,2003
P03-2006,"we also plan to develop the approach by using iteration of our non-local relations extraction algorithm, i.e., by running the algorithm, inserting the found non-local dependencies, running it again etc., until no new dependencies are found.",1,2003
P03-2008,"our future work is to improve a model, corpus etc.to improve the ranked retrieval for structured texts.",1,2003
P03-2009,"ongoing work involves seeing how accurately a new corpus can be tagged with discourse chunks, even when the da tags are unknown.",2,2003
P03-2010,"in the future, we will study the korean time adverbials with mane and zero particle.",1,2003
P03-2010,the first temporal marker is believed to signal the telicity of the event and the second appears very frequently in informal discourses.,1,2003
P03-2011,future work will explore the use of the contextual information of the unknown words and the contextual information of the lexicons in the predicted category of the unknown words to boost predictive power.,1,2003
P03-2015,we are currently conducting even more detailed experiments to demonstrate the usefulness of a spoken dialogue interface for television control and to examine problem areas.,3,2003
P03-2021,we plan to conduct a user-based evaluation of the system to compare users鈥 satisfaction with both the automatically generated summaries and summaries produced by ineats.,3,2003
P03-2021,we plan to extend the system by adding temporal visualization that places the documents on a timeline based on the date and time values extracted from the text.,1,2003
P03-2022,future work could focus on texts in fields other than sports that is used in this paper,2,2003
P03-2025,"ongoing research is focused on the integration of other linguistics-based techniques and combination to transliteration for katakana, the special phonetic alphabet to japanese language.",1,2003
P03-2029,we plan to construct an algorithm for an automatic pattern acquisition from large scale corpora based on those biological approaches.,2,2003
P03-2031,"in the future, we plan to apply more sophisticated natural language processing schemes for automatic generation of more accurate ne tagged corpus.",2,2003
P03-2033,"words, pos/labels and features of the subsentences can be clues to infer the causes of parsing errors.",1,2003
P03-2033,"in the future, we will try to modify willex to infer causes of parsing errors (semi-)automatically.",1,2003
P03-2033,"it is difficult to find a point of parsing failure automatically, because subsentences that have no correspondent partial results are not always the failed point.",5,2003
P03-2033,"hence, we will expand willex to find the longest subsentences that are parsed successfully.",1,2003
P03-2034,in the future we will continue tightening the integration of the components of the system and port the interface to phones and palm or pocket pc devices.,1,2003
P03-2036,we will also conduct experiments on trade-offs between the degree of cf approximation and the size of approximated cfgs as in maxwell iii and kaplan (1993).,3,2003
P03-2036,we are going to integrate the advantage of the cf approximation of hpsg into that of ltag in order to establish another cfg filtering for ltag.,1,2003
P04-1001,our future work will address how to extend this approach to optimize the overall interpretation of user multimodal inputs.,5,2004
P04-1004,we also plan to investigate the interaction of modal verbs with the argumentative structure of the proof.,1,2004
P04-1005,it would also be interesting to combine this probabilistic model of speech repairs with the word classifier approach of charniak and johnson (2001).,1,2004
P04-1005,"still, more sophisticated models may yield better performance.",1,2004
P04-1006,"improving the quality of the parses selected by the first stage should reduce the need for generating such a large number of candidates prior to pruning, improving efficiency as well as overall accuracy.",1,2004
P04-1006,"we believe that attention shifting, or some variety of this technique, will be an integral part of efficient solutions for word-lattice parsing.",1,2004
P04-1009,future research will address issues of graceful recovery from recognition error.,5,2004
P04-1009,we plan to compare the efficacy of our language models built from simulated data with those trained from real user data.,3,2004
P04-1009,we believe that the framework of using simulated dialogs possibly with synthesized speech input augmented with controlled levels of additive noise can be an effective way to develop and evaluate error recovery strategies.,1,2004
P04-1009,however further work is needed to incorporate greater configurability to the dialog flow.,1,2004
P04-1009,increased flexibility for customizing the model of the dialog is needed to enable the software to be applied to the development of other kinds of dialog systems.,1,2004
P04-1010,"for advanced speech recognition, we hope to train our asr on new acoustic data.",2,2004
P04-1010,we plan to port the system to a new domain: from telephone banking to information-technology support.,4,2004
P04-1010,"we also plan to expand our dialogue act classification so that the system can recognize more types of acts, and to improve our classification reliability.",1,2004
P04-1012,future work will focus on analyzing the data collected through the evaluations of the complete athosmail system with real users.,3,2004
P04-1012,another future research topic is to apply machine-learning and statistical techniques in the implementation of the user expertise model.,1,2004
P04-1012,through the user studies we will also collect data which we plan to use in re-implementing the dasex decision mechanism as a bayesian network.,2,2004
P04-1014,future work will investigate extending the feature sets used by the log-linear models with the aim of further increasing parsing accuracy.,1,2004
P04-1015,"first, we will look to include more useful features that are difficult for a generative model to include.",1,2004
P04-1015,"secondly, combining with the generative model can be done in several ways.",1,2004
P04-1015,Future research will look in two directions.,6,2004
P04-1017,we would like to investigate the influence of the coreferential factors on general np reference resolution in our future work.,5,2004
P04-1017,"in fact, the coreferential information of candidates is expected to be also helpful for non-pronoun resolution.",1,2004
P04-1019,"current and future work will also include incorporating the methods tested here in an actual anaphora resolution system, the guitar system (poesio and alexandrov-kabadjov, 2004).",1,2004
P04-1019,"we are also working on methods for automatically recognizing bridging descriptions, and dealing with other types of (non-associative) bridging references based on synonymy and hyponymy.",1,2004
P04-1021,we expect to see the proposed model to be further explored in other related areas.,4,2004
P04-1022,"in future work, we are interested in extending our method to solving the problem of noncompositional collocation translation.",4,2004
P04-1022,we are also interested in incorporating our triple translation model for sentence level translation.,1,2004
P04-1023,we plan to investigate whether it is feasible to use active learning to select which examples will be most useful when aligned at the word-level.,5,2004
P04-1024,"lastly, we will explore whether the proposed romanji to kanji backtransliteration approach applies to other types of names such as place names and study the effectiveness of the approach for backtransliterating romanji names of chinese origin and korean origin to their respective kanji representations.",5,2004
P04-1024,"based on the results of this study, our future work will involve testing the effectiveness of the current method in real clir applications, applying the method to other types of proper names and other language pairs, and exploring new methods for improving precision and recall for romanji name back-transliteration.",4,2004
P04-1024,"to further improve precision and recall, one promising technique is fuzzy matching (meng et al, 2001) for dealing with phonological transformations in name generation that are not considered in our current approach (e.g., “matsuda” vs “matsuta”).",1,2004
P04-1025,"furthermore, we also intend to reuse the recognition of named entities to extract other, specific types of interactions between biological entities.",1,2004
P04-1026,"finally, with the right types of corpora, the worth of the technique for actual application scenarios could be investigated.",2,2004
P04-1026,"the next step in the investigation of linguistic profiling for this task should be a more exhaustive charting of the parameter space, and especially the search for an automatic parameter selection procedure.",1,2004
P04-1026,another avenue of future research is the inclusion of even more types of features.,1,2004
P04-1028,we plan to continue development of our proof-of-concept system to explore those areas.,1,2004
P04-1029,further work on efficient implementations and data structures is therefore required.,2,2004
P04-1030,"a speedup may be achieved, and additional training data could be used.",2,2004
P04-1030,tuning of parameters using em has lead to improved wer for other models.,1,2004
P04-1030,"an investigation into the relevant importance of each parameter for the speech recognition task may allow a reduction in the size of the parameter space, with minimal loss of recognition accuracy.",1,2004
P04-1030,we encourage investigation of this technique for lexicalized head-driven lattice parsing.,4,2004
P04-1032,"a more detailed evaluation is an important task for future research, but if our “net hypothesis” is true, a system that tests whether all outputs of a grammar are nets (or a formal “safety criterion” that would prove this theoretically) could be a useful tool for developing and debugging grammars.",3,2004
P04-1033,"since our method depends on title words and keywords, we need additional studies about the characteristics of candidate words for title words and keywords according to each data set.",2,2004
P04-1035,"directions for future research include developing parameterselection techniques, incorporating other sources of contextual cues besides sentence proximity, and investigating other means for modeling such information.",1,2004
P04-1036,"in the future, we will perform a large scale evaluation on domain specific corpora.",3,2004
P04-1036,"in particular, we will use balanced and domain specific corpora to isolate words having very different neighbours, and therefore rankings, in the different corpora and to detect and target words for which there is a highly skewed sense distribution in these corpora.",2,2004
P04-1037,we would also like to perform additional validation of the learned secondary language sense inventory.,3,2004
P04-1037,"In future work, we plan to investigate the use of additional monolingual context.",2,2004
P04-1038,future work is to extend our coverage and to apply the semantic taxonomy and the same types of features to supervised wsd in chinese.,4,2004
P04-1038,"since the experimental results suggest that a general semantic taxonomy and more constrained lexical sets are both beneficial for wsd tasks, we will develop automatic methods to build large-scale semantic taxonomies and lexical sets for chinese, which reduce human effort as much as possible but still ensure high quality of the obtained taxonomies or lexical sets.",1,2004
P04-1039,an investigation into the feasibility of combining these different factors with the different attributes of the experimental conditions for salaam to automatically predict when the noisy training data can reliably replace manually annotated data is a matter of future work.,3,2004
P04-1042,the second is to incorporate nonlocal dependency information into the category structure of cf trees.,1,2004
P04-1042,"the third would be to incorporate nonlocal dependency information into the edge structure parse trees, allowing discontinuous constituency to be explicitly represented in the parse chart.",1,2004
P04-1043,other studies may relate to the use of scf to generate verb clusters.,1,2004
P04-1043,"in the future we plan to design other structures and combine them with scf, paf and standard features.",1,2004
P04-1043,in this vision the learning will be carried out on a set of structural features instead of a set of flat features.,1,2004
P04-1044,"future work points in two directions: first, integrating our methodology into working isu-based dialogue systems and determining whether or not they improve in terms of standard dialogue evaluation metrics (e.g. task completion).",3,2004
P04-1044,"second, it will be interesting to investigate the impact of different dialogue and task features for classification and to introduce a distinction between generic features that are domain independent and application-specific features which reflect properties of individual systems and application scenarios.",1,2004
P04-1045,"finally, we will explore how the recognized emotions can be used to improve system performance.",5,2004
P04-1045,"we continue to manually annotate itspoke data, and are exploring partial automation via semi-supervised machine learning (maeireizo-tokeshi , 2004).",1,2004
P04-1045,"further manual annotation might also improve reliability, as understanding systematic disagreements can lead to coding manual revisions.",2,2004
P04-1045,"we are also expanding our feature set to include features suggested in prior dialogue research, tutoring-dependent features (e.g., pedagogical goal), and other features available in our logs (e.g., semantic analysis).",2,2004
P04-1047,we hope to be able to apply our lexical acquisition methodology beyond existing parse-annotated corpora (penn-ii and penniii): new text is parsed by our pcfg-based lfg approximations into f-structures from which we can then extract further semantic forms.,4,2004
P04-1047,"currently, we are migrating the technique to spanish, which has freer word order than english and less morphological marking than german.",1,2004
P04-1048,"in our future work, we will experiment with the more recent release of wordnet (2.0).",1,2004
P04-1048,"this version provides derivational morphology links between nouns and verbs, which will promote far greater precision in the linking of verb senses based on morphology than was possible in our initial implementation.",1,2004
P04-1050,"moreover, although we consider the parameter configuration of centering used here a plausible choice, we intend to apply our methodology to study different instantiations of the centering parameters, e.g.by investigating whether “indirect realisation” reduces the classification rate for m.nocb compared to “direct realisation”, etc.",4,2004
P04-1050,"In our future work, we would like to experiment with more metrics.",3,2004
P04-1052,"in future work, we plan to use this algorithm as part of a system for generation from a database of user opinions on products which has been automatically extracted from newsgroups and similar text.",1,2004
P04-1053,"in the future, we are planning to discover less frequent pairs of named entities by combining our method with bootstrapping as well as to improve our method by tuning parameters.",1,2004
P04-1054,"the most immediate extension is to automatically learn the feature compatibility function c(vq, vr).",1,2004
P04-1054,a first approach might use tf-idf to weight each feature.,1,2004
P04-1054,another approach might be to calculate the information gain for each feature and use that as its weight.,1,2004
P04-1054,further investigation is also needed to understand why the sparse kernel performs worse than the contiguous kernel.,5,2004
P04-1054,"it is worthwhile to characterize relation types that are better captured by the sparse kernel, and to determine when using the sparse kernel is worth the increased computational burden.",1,2004
P04-1055,in the future we plan to assess additional relation types.,3,2004
P04-1056,"regarding future work, a richer set of features for the local templates would likely improve performance.",1,2004
P04-1056,"besides exploring improvements to loopy belief propagation that increase computational cost (yedidia , 2000), we intend to examine alternative approximate-inference methods.",1,2004
P04-1058,obvious questions for future work arise: are these two techniques the best way to split non-homogeneous classes into homogeneous ones?,5,2004
P04-1058,is there an optimal splitting?,5,2004
P04-1063,"one possible approach would be to use a dynamic language model which adapts itself for a new domain by re-training itself on data sampled from the web (berger and miller, 1998).",1,2004
P04-1063,an interesting question that remains to be addressed is how we might deal with translations from a novel domain.,5,2004
P04-1064,"further investigations are required to apply this technique on different association measures, and to measure the influence that onmf may have, eg on a phrase-based machine translation system.",3,2004
P04-1065,there is still room to improve the rwth fsa toolkit.,1,2004
P04-1066,thus the next step in this research must be to test whether the improvements in aer we have demonstrated for model 1 lead to improvements on task-based performance measures.,3,2004
P04-1072,"in the future, our work is directed to fine tune this system and increase its capabilities towards processing questions of higher complexity.",1,2004
P04-1074,we will try to automate or at least semi-automate feature selection process.,1,2004
P04-1074,another future work worth investigating is temporal indicator clustering.,1,2004
P04-1075,"furthermore, we will study how to overcome the limitation of the strategy 1 discussed in section 3 by using more effective clustering algorithm.",1,2004
P04-1075,another interesting work is to study when to stop active learning.,5,2004
P04-1080,it is necessary to incorporate these more structural information to improve the performance of word sense learning.,1,2004
P04-1081,we can thus make use of the vast amounts of cheap unannotated data to augment the model presented in this paper.,1,2004
P04-1081,"given the positive results, we plan next to combine large amounts of unsupervised data with reasonable smaller amounts of supervised data such as the senseval lexical sample.",2,2004
P04-1081,earlier we mentioned that one of the promising advantages of kpca is that it computes the transform purely from unsupervised training vector data.,1,2004
P04-1084,"in future work, we shall explore the empirical properties of gmtg, by inducing stochastic gmtgs from real multitexts.",2,2004
P04-1085,"in future work, we plan to extend our inference process to treat speaker ranking (i.e. ap identification) and agreement/disagreement classification as a single, joint inference problem.",5,2004
P04-1085,contextual information about agreements and disagreements can also provide useful cues regarding who is the addressee of a given utterance.,1,2004
P04-1085,we also plan to incorporate acoustic features to increase the robustness of our procedure in the case where only speech recognition output is available.,1,2004
P04-1086,"in the near future we would like to incorporate reliable acoustic information, controlling for individual speaker difference and also apply different discriminative sequence labeling techniques to pitch accent prediction task.",1,2004
P04-1087,in future work we aim to extend our work in two directions.,6,2004
P04-1087,"firstly, we will consider finer-grained classification tasks, such as learning whether a causal discourse marker introduces a cause or a consequence, e.g.distinguishing because from so.",1,2004
P04-1087,"secondly, we would like to see how far our results can be extended to include adverbial discourse markers, such as instead or for example, by using just features of the clauses they occur in.",1,2004
P04-2001,"in the future, the system will be modified to handle various term formations such as abbreviated form.",1,2004
P04-2001,morphological structure analysis of words is also needed to use the morpheme level information.,1,2004
P04-2001,finally we will apply the proposed methods to terms of other domains and terms in general domains such as wordnet.,4,2004
P04-2003,the main directions for our future work are thorough evaluation of the model and optimization of the parameters.,1,2004
P04-2005,we intend to carry out more extensive evaluation to further explore this new resource’s properties and potential.,3,2004
P04-2007,"on the one hand, the theoretical basis of the manual verb classification suggests that, although the syntactic behavior of verbs is an important criteria for a semantic classification, other properties of the verbs should be taken into account.",1,2004
P04-2007,"for the purpose of evaluation, the gold standard classification could also be organized in the form of similarity rankings, based on the distance between the verbs in the hierarchy.",3,2004
P04-2007,"for this reason, a new approach could be envisaged for this task, in the direction of the work by (weeds and weir, 2003), by building rankings of similarity for each verb.",1,2004
P04-2007,"therefore, the description of verbs could be further enhanced with features that reflect on meaning components and event structure.",1,2004
P04-2007,"the incorporation of name entity recognition in the experiments reported here is a first step in this direction, but it is probably a too sparse feature in the data to make any significant contributions.",3,2004
P04-2007,"then, the rankings for each verb could be evaluated.",3,2004
P04-2008,"in the future, i need to evaluate the quality of the resulting scfs by manual analysis and by using the extended lexicons to improve parsing.",3,2004
P04-2008,"i will investigate other clustering methods such as hierarchical clustering, and use other information for clustering such as semantic preference of arguments of scfs to have more accurate clusters.",1,2004
P04-2009,"next, we will experiment with an algorithm (johnson, 2002) that can insert empty-category information into data from charniak’s parser, allowing replication of features that need this.",1,2004
P04-2009,cross-validation experiments will be performed to negate the effects the small test set may cause.,3,2004
P04-2010,"to examine whether the system would yield comparable results in unrestricted text, it needs to be trained on a more diverse and possibly larger corpus.",2,2004
P04-2011,"in addition to reducing lexical ambiguity, it would be interesting to see if structural ambiguity can be reduced.",5,2004
P04-2011,"an alternative approach is to use an extended model that assigns chunk tags and pos tags simultaneously, as was done for finite verb occurrence and pos tags in the current work.",1,2004
P04-2011,another possible application is tagging of german.,4,2004
P04-2011,"in this way, relations between pos tags and chunk tags can be modeled in both directions.",1,2004
P04-2012,i also plan to apply these techniques to describe the morphologies of a variety of languages beyond english and spanish.,4,2004
P04-2012,i intend to implement the search strategies outlined in this paper.,1,2004
P04-3002,"second, we will use the alignment adaptation results in other applications.",4,2004
P04-3002,"first, we will seek other adaptation methods to further improve the domain-specific word alignment results.",1,2004
P04-3002,Our future work includes two aspects.,6,2004
P04-3004,we could then analyze the data and find useful information for future research.,2,2004
P04-3004,"we are also adding on the basic functions to include a log of user activities, which will record the users’ query behavior and their background.",2,2004
P04-3012,it is part of our future work to confirm that machine learning techniques can really induce syntactic information from such a corpus.,5,2004
P04-3015,"in our future work, we will use our approach for other parts of speech and other types of word.",4,2004
P04-3015,"moreover, we will compare with current alternative approaches such as those based on sentence patterns.",3,2004
P04-3016,results indicate that the classification module has to be improved.,1,2004
P04-3016,"given the highly modular architecture of carpanta, adaptation to other languages has a very low cost of development, provided the required nlp tools are available.",2,2004
P04-3017,one of the future works is to extend pas method to handle events in nominalized forms.,1,2004
P04-3019,"in the future, we will apply our method to more types of collocations, to pave the way for more comprehensive applications.",4,2004
P04-3022,"we believe our approach of combining many kinds of evidence can potentially scale better to problems (like ace), where we have a lot of relation types with relatively small amounts of annotated data.",2,2004
P04-3023,a possible solution to this problem is to consider the shortest string on both sides and have “remainder strings” like we have remainder weights in the weighted case.,1,2004
P04-3024,assessing the error introduced by this approximation is a topic for future work.,3,2004
P04-3024,"in order to keep the computational cost low, we use an approximation instead of the exact kldivergence.",1,2004
P04-3025,"further investigation using larger datasets is necessary for the purposes of fully exploiting topic information where it is available, but the present results suggest that this is a worthwhile direction to investigate.",2,2004
P04-3026,by operating on a part-of-speech tagged corpus those sense distinctions that have an effect on part of speech can be taken into account.,2,2004
P04-3028,"we will also try to address the limitation of noise in our co-training system, and generalize our solution to a corresponding corpus of human computer data (litman and forbes-riley, 2004).",4,2004
P04-3028,we will also conduct experiments comparing cotraining with other semi-supervised approaches such as self-training and active learning.,3,2004
P04-3028,"in the future, we will verify the generalization of our results to other partitions of our data.",2,2004
P05-1002,"we also plan to develop higher order crfs, using error-correcting codes to curb the increase in complexity.",1,2005
P05-1002,"we plan to apply error-correcting coding to dynamic crfs, which should result in better modelling of naturally layered tasks, while increasing the efficiency and scalability of the method.",1,2005
P05-1003,in future we intend to investigate cooperative training of lop-crf weights and the parameters of each expert in an expert set.,1,2005
P05-1005,"in the future, we hope to improve on these results: instead of using wordnet unique beginners, using more natural semantic classes based on word usage would possibly improve the accuracy, and finding such classes would be a worthwhile area of research.",1,2005
P05-1005,"as seen from our results, selecting correct similarity measure has an impact on the final outcome.",1,2005
P05-1005,we hope to work on similarity measures that are more applicable in our task.,1,2005
P05-1007,"we are pursuing new exciting directions in a new domain, that of basic data structures and algorithms.",1,2005
P05-1007,"we are investigating what distinguishes expert from novice tutors, and we will implement our findings in an its that tutors in this domain.",5,2005
P05-1008,the results reported here also make us think that a possible avenue for future work is to explore the issue of what types of problems the generalization induced by our framework (which will be discussed below) can be applied to.,4,2005
P05-1011,"future work includes the investigation of such features, as well as the abstraction of lexical dependencies like semantic classes.",1,2005
P05-1014,"finally, further investigation of combining the distributional and the co-occurrence pattern-based approaches over the web is desired.",1,2005
P05-1015,"in the future, we would like to apply our methods to other scale-based classification problems, and explore alternative methods.",4,2005
P05-1017,since the importance of each word consisting a gloss depends on its syntactic role.syntactic information in glosses should be useful for classification. another is active learning.,1,2005
P05-1017,"to decrease the amount of manual tagging for seed words, an active learning scheme is desired, in which a small number of good seed words are automatically selected.",1,2005
P05-1017,One is the incorporation of syntactic information.,1,2005
P05-1018,an important future direction lies in augmenting our entity-based model with lexico-semantic knowledge.,1,2005
P05-1020,"previous attempts on bootstrapping coreference classifiers have only been mildly successful (e.g., m¨uller et al.(2002)), and this is also an area that deserves further research.",1,2005
P05-1021,we believe that the semantic information under such a configuration would be even more effective on technical domains where neutral pronouns take the majority in the pronominal anaphors.,1,2005
P05-1021,our future work would have a deep exploration on such domains.,4,2005
P05-1025,"we are currently working on a framework for soft-labeling of grs, which will allow us to manipulate the precision/recall trade-off as discussed in (carroll and briscoe, 2002).",1,2005
P05-1028,"we will also extend our work to other kinds of information graphics such as line graphs and pie charts, and to complex graphics, such as grouped and composite bar charts.",2,2005
P05-1028,we currently limit ourselves to recognizing what appears to be the primary communicative intention of an information graphic; in the future we will also consider secondary intentions.,5,2005
P05-1029,we also intend to implement multilogue protocols in clarie so it can simulate multilogue.,2,2005
P05-1029,we will then evaluate its ability to process nsus from the bnc.,3,2005
P05-1029,"we plan to carry out more detailed work, both corpus–based and experimental, in order to evaluate the status of mag and, correspondingly to assess just how local acceptance and grounding interaction really are.",3,2005
P05-1030,"in current work, we are studying data collected in a wizard-of-oz study in a multi-modal setting, in order to study clarification behavior in multi-modal dialogue.",2,2005
P05-1031,"in future work, we will try a modified approach, where the detection of fragments is integrated with a classification of utterances as backchannels, fragments, or full sentences, and where the antecedent task only ranks pairs, leaving open the possibility of excluding a supposed fragment by using contextual information.",1,2005
P05-1031,"lastly, we are planning to integrate our classifier into a processing pipeline after the pronoun resolution step, to see whether this would improve both our performance and the quality of automatic meeting summarisations.9",1,2005
P05-1033,"our primary goal for the future is to move towards a more syntactically-motivated grammar, whether by automatic methods to induce syntactic categories, or by better integration of parsers trained on annotated data.",2,2005
P05-1033,"in any case, future improvements to this system will maintain the design philosophy proven here, that ideas from syntax should be incorporated into statistical translation, but not in exchange for the strengths of the phrase-based approach.",1,2005
P05-1034,"following up on ideas introduced by (cherry &amp; lin, 03) we plan to explore ways to leverage the dependency tree to improve alignment quality.",1,2005
P05-1036,"however, more training data is always the easiest cure to statistical problems.",2,2005
P05-1036,if we can find much larger quantities of training data we could allow for much richer rule paradigms that relate compressed to original sentences.,2,2005
P05-1036,one example of a rule we would like to automatically discover would allow us to compress all of our design goals or in the limit such rules blur the distinction between compression and paraphrase.,1,2005
P05-1037,"in future work, we need to prioritize information according to the perceived knowledgeability of each participant in the discussion, in addition to identifying informative content and recognizing dialogue structure.",1,2005
P05-1038,"in future work, we plan to test this prediction for korean, a flexible word order language whose treebank (penn korean treebank) has a non-flat annotation.",2,2005
P05-1039,"because of the strong impact of pos tagging on parsing results, we conjecture that increasing pos tagging accuracy may be another fruitful area for future parsing research.",2,2005
P05-1039,"furthermore, while pos tagging is highly accurate, the error analysis also shows it does have surprisingly large effect on parsing errors.",1,2005
P05-1045,"in particular, it could in the future be applied to statistical parsing.",4,2005
P05-1049,in the future we may extend the evaluation of lp algorithm and related cluster assumption based algorithms using more benchmark data for wsd.,1,2005
P05-1049,another direction is to use feature clustering technique to deal with data sparseness and noisy feature problem.,1,2005
P05-1051,"also, as information extraction is extended to capture cross-document information, we should expect further improvements in performance of the earlier stages of analysis, including in particular name identification.",1,2005
P05-1052,"to deal with sparse data, we can also use deeper text analysis to capture more regularities from the data.",2,2005
P05-1052,one solution is to use general purpose corpora to create clusters of similar words; another option is to use available resources like wordnet.,2,2005
P05-1052,we can explore other kernel properties to integrate the existing syntactic kernels.,1,2005
P05-1053,"in the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research.",1,2005
P05-1053,"therefore, it would be interesting to see how imperfect edt affects the performance in relation extraction.",5,2005
P05-1054,future work will examine empirical differences in other features such as dialog acts or turntaking.,3,2005
P05-1056,"another future direction is to investigate how to effectively incorporate prosodic features more directly in the maxent or crf framework, rather than using a separate prosody model and then binning the resulting posterior probabilities.",1,2005
P05-1056,"to improve su detection results on the stt condition, we plan to investigate approaches that model recognition uncertainty in order to mitigate the effect of word errors.",1,2005
P05-1056,"in future work, we will examine the effect of viterbi decoding versus forward-backward decoding for the crf approach, since the latter better matches the classification accuracy metric.",1,2005
P05-1061,we also plan on running these algorithms on more data sets to test if the algorithms empirically generalize to different domains.,1,2005
P05-1061,the binary relation classifier we employ is quite simplistic and most likely can be improved by using features over a deeper representation of the data such as parse trees.,1,2005
P05-1061,"other more powerful binary classifiers should be tried such as those based on tree kernels (zelenko et al., 2003).",1,2005
P05-1061,"as for future work, there are many things that we plan to look at.",6,2005
P05-1062,"as our future works, we will apply fsa or learned rules to improve the precision and recall of some personal detailed information (such as zip code and mobile).",1,2005
P05-1062,we hope to continue this work in the future by investigating the use of other well researched ie methods.,1,2005
P05-1063,"in addition, we plan to explore the alternative parameter estimation methods described in (roark , 2004a; roark , 2004b), which were shown in this previous work to give further improvements over the perceptron.",1,2005
P05-1063,future work will include a further investigation of parser– derived features.,1,2005
P05-1064,"in monolingual spoken document categorization, we suggest that the semantic domain can be characterized by latent phonotactic features.",1,2005
P05-1065,"further experiments are planned on the generalizability of our classifier to text from other sources (e.g. newspaper articles, web pages); to accomplish this we will add higher level text as negative training data.",2,2005
P05-1065,"we also plan to test these techniques on languages other than english, and incorporate them with an information retrieval system to create a tool that may be used by teachers to help select reading material for their students.",2,2005
P05-1065,"future work includes testing additional classifier features, e.g. parser likelihood scores and features obtained using a syntax-based language model such as chelba and jelinek (2000) or roark (2001).",1,2005
P05-1067,future work includes a full-fledged version of sdig and a more sophisticated mt pipeline with possibly a tri-gram language model for decoding.,1,2005
P05-1068,"in the near future, we will try to solve the data sparseness problem and to increase the coverage and accuracy of verb-noun collocations.",2,2005
P05-1077,we hope that randomized algorithms will make other nlp tools feasible at the tera scale and we believe that many algorithms will benefit from the vast coverage of our newly created noun similarity list.,1,2005
P05-2001,"first, it will be useful to identify a statistical model that achieves higher precision for disyllabic words, as this seems to be the bottleneck.",1,2005
P05-2001,"it will also be relevant to apply advanced statistical models that can incorporate various useful information to this task, e.g., the maximum entropy model (ratnaparkhi, 1996).",1,2005
P05-2001,"second, for better evaluation, it would be helpful to use a larger corpus and evaluate the individual models on a held-out dataset, to compare our model with other models on more comparable datasets, and to test the model on other logographic languages.",2,2005
P05-2001,"finally, as part of a bigger project on chinese unknown word resolution, we would like to see how well the general methodology used and the specifics acquired in this task can benefit the identification and sense-tagging of unknown words.",5,2005
P05-2001,"third, some grammatical constraints may be used for the detection and correction of tagging errors in a post-processing step.",1,2005
P05-2001,several avenues can be taken for future research.,6,2005
P05-2003,"in the future, we will focus especially on improving quality of the training and testing data, employing other classification and attribute-selection techniques, and performing experiments on english data.",2,2005
P05-2003,a necessary part of the work will be a rigorous theoretical study of all applied methods and appropriateness of their usage.,1,2005
P05-2003,"finally, we will attempt to demonstrate contribution of collocations in selected application areas, such as machine translation or information retrieval.",4,2005
P05-2004,"a promising approach is to learn the mappings using decision trees or random forests, which has recently achieved good results in a similar problem in language modeling (xu and jelinek, 2004).",1,2005
P05-2004,"finally, we plan to integrate the tagger/chunker in an end to-end system, such as a factored language model (bilmes and kirchhoff, 2003), to measure the overall merit of joint labeling.",1,2005
P05-2004,"three directions for future research are planned: first, we will augment the fhmm such that its accuracies are competitive with state-of-the-art taggers and chunkers.",1,2005
P05-2004,"this includes adding word features to improve accuracy on oov words, augmenting the context from bigram to trigram, and applying advanced smoothing techniques.",1,2005
P05-2004,"second, we plan to examine the switching fhmm further, especially in terms of automatic construction of the α and q function.",3,2005
P05-2008,"other extensions of this work are to collect more text marked-up with emoticons, and to experiment with techniques to automatically remove noisy examples from the training data.",2,2005
P05-2008,in future work we will perform further tests to determine the nature of dependency in machine learning techniques for sentiment classification.,3,2005
P05-2010,"we can now subject these putative chains to a direct test; in fact, this is the immediate future research direction.",3,2005
P05-2016,"among these are discontinuous constituents, head switching, phrasal translation, english word stemming, and improved modeling of structural changes.",1,2005
P05-2016,our first priority is to complete the final pieces so that we have an end-to-end system to experiment with.,3,2005
P05-2016,"once we are able to evaluate our system output, our first priority will be to analyze the system errors and adjust the model accordingly.",3,2005
P05-2016,"we recognize that our independence assumptions are generally too strong, and improving them is a hight priority.",5,2005
P05-2016,"with this will come sparse data issues, so it will also be important for us to incorporate smoothing into the model.",1,2005
P05-2016,there are many interesting subproblems which deserve attention and we hope to examine at least a couple of these in the near future.,3,2005
P05-2018,we will also include centrality measures as sentence features in producing extractive summaries.,1,2005
P05-2018,"to make this research more robust, we will include reference resolution into our study.",1,2005
P05-2021,"in the near future, we would like to test our approach on agglutinative languages, where the problems with high oov are even more challenging.",3,2005
P05-2021,We would also like to experiment with more complex language models.,1,2005
P05-2024,we expect that the accuracy can be improved through further elaboration of the grammar design and disambiguation method.,1,2005
P05-2026,"to further improve fluency, these could also be combined with a scoring function that takes longer-range dependencies into account, as well as penalizing extraneous content.",1,2005
P05-2026,a weighted average of the dependency score with an n-gram model would already offer improvement.,1,2005
P05-3003,"but the new chart data structure that utool computes is a more explicit packed representation of the possible readings, and still relatively small in practice.",1,2005
P05-3003,thus it could open up avenues for more theoretical future research as well.,6,2005
P05-3006,"to achieve automatic pattern generation, we will try to apply machine learning technique like the boosting algorithm.",1,2005
P05-3006,"finally, we will compare with other systems which participated in trec by translating definitional questions of trec in korean.",3,2005
P05-3006,our further works will concentrate on reducing human efforts for building descriptive patterns.,1,2005
P05-3016,"in future, we are going to study translation qualities, prepare error-handling mechanisms for brittle ocr, mt and its combination, and explore new application areas of language computation.",4,2005
P05-3018,"furthermore, we believe that the aggregate visualization tool will also help us uncover additional characteristics of potentially informative training examples.",1,2005
P05-3021,we are also compiling more annotated data in order to provide more training data for machine learning approaches to tlink extraction.,2,2005
P05-3021,"sputlink currently uses only qualitative temporal infomation, it will be extended to use quantitative information, allowing it to reason over durations.",1,2005
P05-3021,"in the nearby future, we will experiment with more strategies to extract temporal relations from texts.",3,2005
P05-3026,the focus of our future work will thus be on identifying features that support improved hypothesis scoring.,1,2005
P05-3028,we work on support for queries like pairs of referring expressions that are a certain number of referring expressions apart.,1,2005
P05-3028,"we also want to include wild cards and proximity searches, and support for automatic markable creation from query results.",1,2005
P05-3031,"we plan to improve the performance by, for example, using larger amount of training examples.",1,2005
P05-3031,finding other reformatting strategies in addition to the ones proposed in this paper is also important future work.,1,2005
P06-1001,we are especially interested in the relationship between alignment and decoding and the effect of preprocessing scheme on both.,5,2006
P06-1002,future work will involve an investigation into how the phrase extraction and scoring should be adjusted to take the nature of the alignment into account and how the phrase-table size might be reduced without sacrificing the mt output quality.,5,2006
P06-1003,we are currently investigating the addition of non-lexical features as observed outputs in  our unsupervised generative model.,1,2006
P06-1003,"we are also investigating improvements into the lexical model as presented here, firstly via simple techniques such as word stemming and replacement of named entities by generic class tokens (barzilay and lee, 2004); but also via the use of multiple asr hypotheses by incorporating word confusion networks into our model.",1,2006
P06-1004,we will explore how the interaction between the generation and segmentation components can improve the performance of such a system as a whole.,1,2006
P06-1004,our ultimate goal is to automatically generate tables of content for lectures.,2,2006
P06-1004,we plan to investigate strategies for generating titles that will succinctly describe the content of each segment.,1,2006
P06-1007,"in so doing we can find the places where the prediction problem faced both by the hspm and the machines that aspire to emulate it actually warrants the greater power of structurally sensitive models, using this knowledge to mine large corpora for future experiments with human subjects.",3,2006
P06-1007,"one attractive future direction is to carry out simulations that compare the evolution of probabilities in the tagger with that in a theoretically more powerful model trained on the same data, such as an incremental statistical parser (kim , 2002; roark, 2001).",1,2006
P06-1008,this information will be validated by means of acceptability judgments acquired on the basis of a sparse sampling strategy.,1,2006
P06-1008,our parser will associate a grammatical index to each sentence.,1,2006
P06-1008,the next step of our work will be its validation on large corpora.,2,2006
P06-1013,in the future we plan to integrate more components into our ensembles.,2,2006
P06-1013,"increasing the number of components would allow us to employ more sophisticated combination methods such as unsupervised rank aggregation algorithms (tan and jin, 2004).",1,2006
P06-1013,"these include not only domain driven disambiguation algorithms (strapparava , 2004) but also graph theoretic ones (mihalcea, 2005) as well as algorithms that quantify the degree of association between senses and their co-occurring contexts (mohammad and hirst, 2006).",1,2006
P06-1014,"in a future work, we plan to investigate the contribution of coarse disambiguation to such real-world applications.",1,2006
P06-1014,"to this end, we aim to set up an open mind-like experiment for the validation of the entire mapping from wordnet to ode, so that only a minimal error rate would affect the experiments to come.",3,2006
P06-1015,"for the former, we plan to investigate the use of wordnet to automatically learn selectional constraints on generic patterns, as proposed by (girju et al.2006).",1,2006
P06-1015,there are many avenues of future work both in improving system performance and making use of the relations in applications like question answering.,1,2006
P06-1015,we expect here that negative instances will play a key role in determining the selectional restrictions.,5,2006
P06-1016,"in the future work, we will explore the hierarchical learning strategy using other machine learning approaches besides online classifier learning approaches such as the simple perceptron algorithm applied in this paper.",1,2006
P06-1016,this will be done by integrating the relation extraction system with our previously developed ner system as described in zhou and su (2002).,5,2006
P06-1016,"Therefore, it would be interesting to see how imperfect NER affects the performance in relation extraction.",5,2006
P06-1017,"in the future, we would like to investigate more effective feature set or use feature selection to improve the performance of this graph-based semisupervised relation extraction method.",1,2006
P06-1018,"further investigations must concern the computational properties of pugs, notably restrictions allowing polynomial time parsing.",1,2006
P06-1020,"we also intend to use the turkish treebank (oflazer et al., 2003), as a resource to extract statistical information along the lines of frank et al.(2003) and o鈥橠onovan et al.(2005).",2,2006
P06-1020,"we also intend to use the turkish treebank (oflazer , 2003), as a resource to extract statistical information along the lines of frank (2003) and o鈥橠onovan (2005).",2,2006
P06-1020,"our current and future work involves extending the coverage of the grammar and lexicon as we have so far included in the grammar lexicon only a small subset of the root lexicon of the morphological analyzer, annotated with the semantic and subcategorization features relevant to the linguistic phenomena that we have handled.",2,2006
P06-1022,"therefore, we want to examine the effectiveness by conducting the parsing experiment of long sentences in written language such as newspaper articles.",3,2006
P06-1022,"after that, we plan to investigate techniques for identifying the dependency relations over clause boundaries.",1,2006
P06-1022,future research will include making a thorough investigation into the relation between dependency type and the type of clause boundary unit.,1,2006
P06-1022,"furthermore, as the experiment described in this paper has shown the effectiveness of our technique for dependency parsing of long sentences in spoken monologues, so our technique can be expected to be effective in written language also.",4,2006
P06-1024,"we will also carry out a more direct comparison with the hybrid strategies learned by (henderson et al., 2005).",3,2006
P06-1024,we will continue to investigate whether we can maintain tractability and learn superior strategies as we add incrementally more high-level contextual information to the state.,5,2006
P06-1024,"in the slightly longer term, we will test our learned strategies on humans using a full spoken dialogue system.",3,2006
P06-1024,"cfs subset evaluation (rieser and lemon, 2006)) on in order to determine which contextual features this suggests are important.",3,2006
P06-1024,we also intend to use feature selection techniques (e.g.,1,2006
P06-1026,"in future work, we will address the use of data-driven dialog management to improve slu.",5,2006
P06-1031,We will investigate how to exploit these sources of information in future work.,2,2006
P06-1032,"in particular, we expect to be looking into alternative word alignment models and possibly enhancing our system鈥檚 decoder using some of the richer, more structured language models that are beginning to emerge.",1,2006
P06-1034,"finally, while we believe that this technique will apply across domains, it would be useful to test it on domains such as movie reviews or product reviews, which have more complex domain ontologies.",3,2006
P06-1034,"an alternative would be to leave some sentences unparsed and use them as templates with hybrid generation techniques (white and caldwell, 1998).",1,2006
P06-1035,"in further work, we plan to improve both the quantity and the quality of the data.",2,2006
P06-1035,"likewise, we plan to develop the method for identifying cognates.",1,2006
P06-1041,"another interesting goal of future work might be to even consider dynamic predictors, which can change their behavior according to text type and perhaps even to text structure.",1,2006
P06-1044,"in addition, we plan to apply similar technology to other interesting domains (e.g. tourism, law, astronomy).",4,2006
P06-1044,"in the future, we plan to improve the accuracy of automatic classification by seeding it with domain-specific information (e.g. using named entity recognition and anaphoric linking techniques similar to those of vlachos et al.(2006)).",1,2006
P06-1044,we also plan to conduct a bigger experiment with a larger number of verbs and demonstrate the usefulness of the bigger classification for practical bio-nlp application tasks.,2,2006
P06-1045,"although the result also shows the possibility that the bigger the corpus is, the better the performance will be, the contents and size of the corpora we used are diverse, so their relationship, including the effect of the window radius, should be examined as the future work.",2,2006
P06-1046,we intend to use this knowledge to process even larger corpora to produce more accurate results.,2,2006
P06-1046,"having set out to improve the efficiency of distributional similarity searches while limiting any loss in accuracy, we are producing full nearestneighbour searches 18 times faster, with only a 2% loss in accuracy.",1,2006
P06-1047,we are interested in the issue of how to improve an event representation in order to build a more powerful event-based summarization system.,5,2006
P06-1047,we also want to see how concepts rather than sentences are selected into the summary in order to develop a more flexible compression technique and to know what characteristics of a document set is appropriate for applying event-based summarization techniques.,1,2006
P06-1047,This would be one of our future directions.,6,2006
P06-1048,"another important future direction lies in applying the unsupervised model presented here to languages with more flexible word order and richer morphology than english (e.g., german, czech).",2,2006
P06-1048,finding such a mechanism is an avenue of future work.,1,2006
P06-1048,we would also like to enhance the word based model with more linguistic knowledge; we plan to experiment with syntax-based language models and more richly annotated corpora.,1,2006
P06-1049,"a future direction of this study would be to explore the application of the proposed framework to more generic texts, such as documents without chronological information.",2,2006
P06-1051,"in the future, we would like to study approaches to improve the computational complexity of our kernel function and to design approximated versions that are valid mercer’s kernels.",1,2006
P06-1053,"in the future, we intend to explore the consequences of introducing lexicalization into the parser.",5,2006
P06-1053,future work will also involve the use of smoothing to increase the benefit of priming for parsing accuracy.,1,2006
P06-1054,"for future work, we will further improve the speed and accuracy of our models, and apply them to more chinese and multilingual natural language applications that require high speed and accurate parsing.",1,2006
P06-1056,"in future work we plan to try different representations of the data, to use knowledge of the relations that exists between the partial cognate and the context words, and to run experiments when we iterate the mb and bb steps more than once.",2,2006
P06-1057,"we speculate that there is a great potential for such approaches, and suggest that sense matching may become an appealing problem and possible track in lexical semantic evaluations.",5,2006
P06-1058,"in future work, we will examine the effectiveness of ep-based method in other wsd techniques.",1,2006
P06-1061,"in the future, we intend to design segment retrieval methods that do not require documents to be segmented before retrieval, hence avoiding the possibility of early-stage errors introduced from the text segmentation step.",1,2006
P06-1061,a very promising idea is to adapt a naive bayes ie to perform redundant extractions directly on an entire document to retrieve filler-containing text segments for a segment hmm ie system.,1,2006
P06-1064,"since the tiger corpus contains complete morphological and lemma information for all words, future work will address the question of how to identify and apply a set of (non-recursive) lexical rules (carpenter, 1992) to the extracted ccg lexicon to create a much larger lexicon.",5,2006
P06-1064,"since tiger corpus is of comparable size to the penn treebank, we hope that the work presented here will stimulate research into statistical widecoverage parsing of free word order languages such as german with deep grammars like ccg.",2,2006
P06-1066,"the future work is to investigate other valuable features, e.g. binary features that explain blocks from the syntactical view.",1,2006
P06-1070,for the future we try to explore also the use of a word sense disambiguation all-words system.,1,2006
P06-1073,"as future work, we plan to incorporate buckwalter morphological analyzer information to extract new features that reduce the search space.",1,2006
P06-1075,there are rich features generated during the translation procedure.,5,2006
P06-1075,this question is what we would like to answer in our future work.,5,2006
P06-1075,will such features be helpful to clir?,5,2006
P06-1076,further research is required to understand how well the abstract tested corpus-weighting schemes will perform in a full-text environment.,5,2006
P06-1082,in particular we like to compare the proposed scheme with the famous ibm models.,3,2006
P06-1082,we hope that with a much larger corpus size we shall be able to make the necessary comparisons in near future.,2,2006
P06-1086,we will also investigate ways of using morphologically tagged corpora to assign weights to the arcs in the transducer so that the analyses returned by magead are ranked.,2,2006
P06-1086,"in future work, we will investigate the derivation of words with morphemes from more than one variant (code switching).",1,2006
P06-1087,We plan to address this issue in future work.,6,2006
P06-1088,"another possibility is to investigate whether similar techniques can improve other tagging tasks, such as named entity recognition.",1,2006
P06-1088,in future work we will investigate maintaining tag ambiguity further down the language processing pipeline and exploiting the uncertainty from previous stages.,5,2006
P06-1088,"in particular, we will incorporate real-valued pos tag and lexical category features in the statistical parsing model.",1,2006
P06-1093,"we would also like to link our model to technical manuals, catalogs, etc. already available on the different topics in the given domain.",1,2006
P06-1093,in future we would like to semantically cluster the topic specific information so that redundant topics are eliminated from the list.,1,2006
P06-1093,"we can use automatic taxonomy generation(atg) algorithms for document summarization (kummamuru , 2004) to build topic taxonomies.",1,2006
P06-1094,"future work will examine whether the model can be used to describe the semantics of nouns (such as corner) that express vague spatial extent, and how the model relates to the functional aspects of spatial reasoning.",1,2006
P06-1095,"future research will investigate this effect further, as well as examine factors that enhance or mitigate this effect in different corpora.",2,2006
P06-1097,additional feature functions will be also investigated that were proved successful for phrase-based models together with feature functions useful for a tree based modeling.,1,2006
P06-1097,"as our future work, we are in the process of experimenting our model for other languages with rich resources, such as chinese and arabic, as well as similar language pairs, such as french and english.",3,2006
P06-1097,we hope that minimum error / maximum likelihood training using the emd algorithm can be used for a wide diversity of tasks where there is not enough labeled data to allow supervised estimation of an initial model of reasonable quality.,4,2006
P06-1100,the induction of conceptual instances has opened the way for many avenues of future work.,1,2006
P06-1109,"since parsers are becoming increasingly important in applications like syntax-based machine translation and structural language models for speech recognition, one way to go would be to compare these different parsing methods by isolating their contribution in improving a concrete nlp system, rather than by testing them against gold standard annotations which are inherently theory-dependent.",3,2006
P06-1109,"the initially disappointing results of inducing trees entirely from raw text was not so much due to the difficulty of the bootstrapping problem per se, but to (1) the poverty of the initial models and (2) the difficulty of finding theoryindependent evaluation criteria.",5,2006
P06-1110,"lastly, we plan to give the model linguistically more sophisticated features.",1,2006
P06-1114,we believe that these results suggest that current supervised machine learning approaches to the recognition of textual entailment may provide open-domain q/a systems with the inferential information needed to develop viable answer validation systems.,1,2006
P06-1116,"a more sophisticated model of syntactically weighted vector space (pado and lapata, 2003) may help improve the lexical acquisition phase.",1,2006
P06-1120,"in future work, we consider investigating other levels of the significance list, extending the evaluation to other languages, comparing against shallow-parsing methods instead of the window method, and performing recall-based evaluation as well.",3,2006
P06-1120,our study finally clear the doubts on the usefulness of parsing for collocation extraction.,5,2006
P06-1122,in future work we will investigate the use of linguistic resources to define feature sets for the mrf prior.,2,2006
P06-1122,"lexical redundancy would ideally be addressed in the context of phrases, however, computation and statistical estimation may then be significantly more challenging.",5,2006
P06-1124,"in the future we plan to study in more detail the differences between our model and the variants of kneser-ney, to consider other approximate inference schemes, and to test the model on larger data sets and on speech recognition.",3,2006
P06-1124,"the hierarchical pitman-yor language model is a fully bayesian model, thus we can also reap other benefits of the paradigm, including having a coherent probabilistic model, ease of improvements by building in prior knowledge, and ease in using as part of more complex models; we plan to look into these possible improvements and extensions.",1,2006
P06-1128,this framework will thus be applicable to other domains such as patent documents.,4,2006
P06-1128,"for example, query expansion and relevancy feedback can be integrated in a straightforward way in order to improve accuracy.",1,2006
P06-1130,"more research is required to implement such a model efficiently using packed representations (carroll and oepen, 2005).",1,2006
P06-1134,"second, a number of methods for subjectivity or sentiment analysis start with a set of seed words and then search through wordnet to find other subjective words (kamps and marx, 2002; yu and hatzivassiloglou, 2003; hu and liu, 2004; kim and hovy, 2004; esuli and sebastiani, 2005).",1,2006
P06-1134,"while much work remains to be done, this first attempt has proved the feasibility of correctly assigning subjectivity labels to the fine-grained level of word senses.",1,2006
P06-1135,in particular we feel that much more attention should be paid to the problem of determining if two entities are the same (or “close enough”).,5,2006
P06-1135,"the outcome is a re-ranking of the candidate answers, with the possible insertion of nil (no answer in corpus) as the top answer.",1,2006
P06-1136,"in the future, we are interested in enhancing the centroid learning using human knowledge sources such as encyclopedia.",2,2006
P06-1138,"a linguistic study on corpora might determine what types of elements are actually emancipated and in particular what types of elements can be emancipated simultaneously, i.e.what list of slashed element are possible, given that this is the main factor of complexity of the algorithm (see kiefer 1 for similar heuristic considerations for hpsg parsing).",2,2006
P06-1138,real values on efficiency will not be available as long as the grammar does not surpass experimental size.,1,2006
P06-1140,"finally, we plan to examine whether gains in quality can be achieved with an off-the-shelf, general purpose voice that are similar to those we have observed using comic's limited domain voice.",3,2006
P06-1140,"we also plan to investigate whether additional features derived from the synthesizer can better detect unnatural pauses or changes in speech rate, as well as f0 contours that fail to exhibit the targeting accenting pattern.",5,2006
P06-1140,"in future work, we intend to verify the results of our cross-validation study in a perception experiment with naive subjects.",3,2006
P06-1143,"pmt is an endeavor to bridge the ethnical, cultural and geographical divisions between the punjabi speaking communities.",1,2006
P06-1144,"in addition, we will study if changing the order of the bilingual and monolingual comparison steps the performance varies significantly for different type of corpus.",2,2006
P06-1146,"in future work, we will investigate minimal tree edit distance (bille, 2005) and related formalisms which are defined on tree structures and can therefore model divergences explicitly.",1,2006
P06-2001,"nevertheless, we think the presented results for the basque language could be improved.",2,2006
P06-2001,"finally, we contemplate building an icall (intelligent computer assisted language learning) system to help learners to put commas correctly.",1,2006
P06-2001,"however, we have not obtained as good results as we hoped in the task of placing commas (we get a precision of 69.6% and a recall of 48.6%).",5,2006
P06-2001,"these results, anyway, confirm our hypothesis and our diagnosis of the detected problems.",3,2006
P06-2002,"concerning future work, we are currently trying to improve the estimation of the patterns accuracy for the pruning step.",1,2006
P06-2002,we also plan to apply the obtained patterns in a system for automatically generating biographical knowledge bases from various web corpora.,4,2006
P06-2009,"our future work includes trying to generalize this work to non-projective dependency parsing, as well as attempting to incorporate additional sources of information (e.g., shallow parsing information) into the pipeline process.",2,2006
P06-2009,"we have addressed the problem of using learned classifiers in a pipeline fashion, where a task is decomposed into several stages and stage classifiers are used sequentially, where each stage may use the outcome of previous stages as its input.",1,2006
P06-2009,this is a common computational strategy in natural language processing and is known to suffer from error accumulation and an inability to correct mistakes in previous stages.,5,2006
P06-2010,we can use general pur-pose corpus to create clusters of similar words oruse available resources like wordnet.,2,2006
P06-2010,"we can also use the hybrid kernel method into other tasks, such as relation extraction in the future.",4,2006
P06-2016,"further research in both experimental and theoretical aspects of this work is planned, specifically in the area of reconstructing hierarchies where recursive formations are present and formal analysis and testing of techniques.",1,2006
P06-2018,one limitation of our method is the fact that it treats the classification task separately for each target node.,1,2006
P06-2019,an appealing future direction is the incorporation of discourse-based constraints into our models.,1,2006
P06-2019,"we plan to apply our method to languages with more flexible word order than english (e.g., german) and more challenging spoken domains (e.g., meeting data) where parsing technology may be less reliable.",2,2006
P06-2020,we introduced an oracle score based upon the simple model of the probability that a human will choose to include a term in a summary.,1,2006
P06-2021,"in future work we hope to extend this algorithm to provide a more efficient algorithmic implementation, and also to apply the algorithm in areas such as the machine translation of nounnoun compounds, where the identification of semantic relations in compounds is a crucial step in the translation process.",4,2006
P06-2027,our approach features the use of corpus statistics derived from both lexical and syntactic analysis across documents.,1,2006
P06-2030,"future work includes (i) extending the method to an incremental approach for extracting story pairs, (ii) comparing our clustering method with the other existing methods such as x-means(pelleg, 2000), and (iii) applying the method to the tdt4 for quantitative evaluation.",1,2006
P06-2031,we are currently carrying out further work in the aspects of (1) improving the accuracy of source word frame identification and (2) incorporating bilingual frame semantics in a full fledged  machine translation system.,1,2006
P06-2032,"we are currently exploring ways in which the xmg formalism could be extended to automatically enforce linguistically-based wellformedness principles such as for instance, a kind of head feature principle for tag.",1,2006
P06-2034,"experimenting with other effective reranking algorithms, such as svms (joachims, 2002) and maxent (charniak and johnson, 2005), is also a direction of our future research.",1,2006
P06-2034,"we also plan to explore other types of reranking features, such as the features used in semantic role labeling (srl) (gildea and jurafsky, 2002; carreras and m`arquez, 2005), like the path between a target predicate and its argument, and kernel methods (collins, 2002b).",1,2006
P06-2035,"therefore, it is possible to plug any external and exogenous component in our architecture to improve the overall quality.",1,2006
P06-2037,we believe that coarse-grained is sufficient for the purpose of mt.,1,2006
P06-2037,"a substantial increase is consistently obtained according to standard mt evaluation metrics, which has been shown to be statistically significant in the case of bleu.",3,2006
P06-2042,"we will also study use of information on quotations and inserted clauses to text formatting, such as text summarization.",2,2006
P06-2042,"in the future, we plan to solve the problems found in the experiments and investigate the robustness of our method when the without boundaries of quotations open 81.0% and inserted clauses closed 90.3% with boundaries of quotations and open 81.7% inserted clauses (automatically detected) closed 90.3% with boundaries of quotations open 82.8% and inserted clauses (correct) closed 91.3% results of automatic speech recognition are given as the inputs.",3,2006
P06-2043,"however, approaches that are more complicated still exist theoretically, for instance, some scf types unseen by the hypothesis generator may be recalled by integrating semantic verb classification information into the system.",1,2006
P06-2045,the evaluation is divided to test each of the two processes underlying the framework.,3,2006
P06-2046,"what remains to be done is two things; one is to reveal all the subclasses of class c and all the disambiguation knowledge, and the other is to apply a machine learning technique to disambiguating those cases that the current technique is unable to handle, i.e., cases without visible evidence.",1,2006
P06-2048,"first, the set of features we selected were chosen with simplicity in mind, to see how well a simple and unadorned set of features would work, given our probabilistic model.",5,2006
P06-2048,we can extend this research in multiple directions.,6,2006
P06-2050,"in conclusion, the goal of this research is set to survey the unique characteristics of chinese ideographs.",5,2006
P06-2054,applying and extending our approach to other natural language tasks (which are difficult to apply a parser to) such as information extraction from e-mail data or biomedical named entity recognition is a topic of future work.,4,2006
P06-2055,retrieving related articles from a large collection should increase the likelihood of finding a name instance with a disambiguating context.,2,2006
P06-2055,"finally, our cross-document coreference is currently performed only within the (small) test corpus.",2,2006
P06-2057,"while training, we tried to balance the selection somewhat, but applying the projection methods on other types of parallel corpora (such as novels available in both languages) may produce a better training corpus.",2,2006
P06-2058,further experimentation is clearly needed in this area needs to address the impact of this interdependency.,3,2006
P06-2058,an interesting experiment would be to explore how this approach applies to different types of corpora like email messages.,3,2006
P06-2061,sia uses stochastic word mapping to allow soft or partial matches between the mt hypotheses and the references.,1,2006
P06-2062,"for example, some russian rules only make sense as a part of language-specific modules while some rules that were considered universal at first are not directly applicable to russian.",1,2006
P06-2062,syntactic types and some combination rules are more problematic.,1,2006
P06-2066,"experimental evaluation based on data from two treebanks shows, that a combination of the wellnestedness constraint and parametric constraints on discontinuity (formalized either as gap degree or edge degree) gives a very good fit with the empirical linguistic data.",1,2006
P06-2067,we have also shown that it is feasible to apply the current scf extraction technology to spoken language.,4,2006
P06-2071,"in most corpora used to date for research on illustrated text, word sense is an entirely secondary phenomenon, whereas our data set was collected as to emphasize possible ambiguities associated with word sense.",2,2006
P06-2071,"also, it remains an unsolved problem how to enumerate iconographic senses, and use them in manual annotation and classification.",5,2006
P06-2071,"in particular, we intend to explore the notion of iconographic senses; surprisingly good results on image classification by (chapelle, haffner, and vapnik, 1999) using image features suggest that iconography plays an important role in the semantics of images.",1,2006
P06-2071,"future work will involve learning the algorithms parameters without supervision, and develop a semantically meaningful image taxonomy.",1,2006
P06-2071,"it is remarkable how high purity is, considering that we are using relatively simple image and text representation.",5,2006
P06-2071,an important aspect is to enhance our understanding of the interplay between text and image features for this purpose.,1,2006
P06-2073,"the application of our statistical models to other tasks (like verb mobil (alexanderson et al., 1998)) would allow us to confirm our conclusions and compare results with other works.",4,2006
P06-2073,"the application of our statistical models to other tasks (like verbmobil (alexandersson , 1998)) would allow us to confirm our conclusions and compare results with other works.",4,2006
P06-2075,"the main contribution of this paper is a novel integration of the pattern-based and distributional approaches for lexical semantic acquisition, applied to lexical entailment.",1,2006
P06-2077,we will also explore a method for including instances tagged with question in training data by using the proposed method and bootstrapping.,1,2006
P06-2077,it can reinforce almost any earlier method.,1,2006
P06-2077,"even to handcoded rules, it can be applied as long as they give predictions with their confidences.",4,2006
P06-2077,the second is that the proposed method is unsupervised.,1,2006
P06-2077,This further gives an additional advantage.,1,2006
P06-2082,"first, we shall continue to build dependency-analyzed corpora.",2,2006
P06-2083,"a future direction of this study would be to incorporate other types of relations expressed with parenthesis such as synonym, paraphrase, etc.",1,2006
P06-2085,"the next step towards a rl-based system is to add task-level and reward-level annotations to calculate reward functions, as discussed in (rieser , 2005).",1,2006
P06-2088,"to realize simultaneous translation our method utilizes the feature that word order is flexible in japanese, and determines the word order of a translation based on dependency structures and japanese dependency constraints.",1,2006
P06-2089,this simple combination of the two models produces an f-score of 90.8% for the standard wsj test set.,1,2006
P06-2090,"from a technical point of view, the inferential model presented in this paper is a simple starting point for reflection on a number of issues in automatic identification of genres in web pages.",1,2006
P06-2090,"although parameters need a better tuning and text type and genre palettes need to be enlarged, it seems that the inferential approach is effective, as shown by the preliminary evaluation reported in section 4.3.",1,2006
P06-2090,"more importantly, this model instantiates a theoretical characterization of genre that includes hybridism and individualization, and interprets these two elements as the forces behind genre evolution.",1,2006
P06-2092,"if possible, we want to conduct experiments that involve further languages and additional kinds of corpus annotation, like e.g. detailed morphological information as annotated e.g. within the croco project (neumann and hansen-schirra, 2005).",2,2006
P06-2093,"another promising direction that we have not yet explored, is to build long-span lm, i.e. with n much greater than 4.",1,2006
P06-2093,one could expect that the target language model plays a different role in a phrase-based system since the phrases induce some local coherency on the target sentence.,1,2006
P06-2095,"furthermore, dictionaries also tend to translate words using the same pos.",2,2006
P06-2095,"even if the procedure for producing similarity classes does not impose restrictions on pos properties, nevertheless words in the similarity class tend to follow the pos of the original word, because of the similarity of their contexts of use.",1,2006
P06-2095,"currently we are working on an option to identify semantic contexts by means of semantic signatures obtained from a broad-coverage semantic parser, such as usas (rayson , 2004).",1,2006
P06-2102,"moreover, we are exploring the possibility of improving the gold standard clusters by examining the lexical semantic attributes of the msa verbs.",1,2006
P06-2102,We are in the process of manually cleaning the English translations corresponding to the MSA verbs.,2,2006
P06-2105,we plan to improve our logic prover to detect false entailments even when the two texts have a high word overlap and expand our axiom set.,1,2006
P06-2106,"we start with three asian languages, chinese, japanese and thai, on top of the existing framework which was designed mainly for european languages.",2,2006
P06-2108,"in other directions, we will extend it to other chinese nlp research topics, especially word segmentation, main verb identification and subject-verb-object (svo) auto construction.",4,2006
P06-2108,"although there is room for improvement, we believe it would not produce a noticeable effect as far as the stw accuracy of poly-syllabic words is concerned.",5,2006
P06-2108,"we will continue to improve our wsm to cover more characters of the udn2001 and the as corpus by those word-pairs comprised of at least one mono-syllabic word, such as “我們 (we)-是(are)”.",2,2006
P06-2109,"we presented a maximum entropy model to extend the sentence compression methods described by knight and marcu (knight and marcu, 2000).our proposals are two-fold.",1,2006
P06-2109,"to improve our approaches, we can introduce more feature functions, especially more semantic or lexical features, and to deal with these features, we need a larger corpus.",2,2006
P06-2111,the last thing that remains for future work is to find a more adequate way to combine the  syntax-based and the alignment-based methods.,1,2006
P06-2112,"first, we will further investigate the effect of the size of corpora on the alignment results.",2,2006
P06-2112,"second, we will investigate different parameter combination of the induced model and the original model.",1,2006
P06-2113,"we have also discussed our experience in directly porting a generative model to a conditional model, and demonstrated that it may not be beneficial at all if we still think generatively in conditional modeling; more specifically, replicating the feature set of a generative model in a conditional model may not help much.",5,2006
P06-2114,the proposed g2p conversion mechanism will be useful in various applications in the speech domain.,4,2006
P06-2114,"in this paper, the problem of sinhala graphemeto-phoneme conversion is addressed with a special focus on dealing with the schwa epenthesis.",5,2006
P06-2116,we plan next to extend the model by incorporating ontological and linguistic knowledge for additional disambiguation leverage.,1,2006
P06-2118,"we believe this represents a rich area of exploration and we intend to experiment with more verbs with further customization of features, including experimenting with automatic feature selection.",1,2006
P06-2119,in the forthcoming work we will investigate their validity in the lexical task of senseval-3.,3,2006
P06-2122,initializing the model with good dependency parameters is a possible adjustment.,1,2006
P06-2122,we would also like to point out that alignment task is simpler than decoding where a stronger component of reordering is required to produce a fluent english sentence.,1,2006
P06-2122,investigating the impact of bilexical dependencies on decoding is our future work.,1,2006
P06-2123,"by setting the confidence threshold, r-oov and r-iv can be changed accordingly.",1,2006
P06-3002,"to really judge the benefit of an unsupervised tagging system, it should be evaluated in an application-based way.",3,2006
P06-3003,"in the future, we will try to improve the algorithm as well as perform more extensive evaluations on different language pairs.",1,2006
P06-3005,"it is as yet unclear what the final form of a cognitively accurate model along these lines would be, but it is clear from our study that it is worthwhile, for the sake of clarity and explicit testability, to consider models that are simpler and more precisely specified than those assumed by dominant theories of human sentence processing.",1,2006
P06-3005,"One attractive future direction is to carry out simulations that compare the evolution of probabilities in the tagger with that in a theoretically more powerful model trained on the same data, such as an incremental statistical parser (Wang , 2004; Roark, 2001).",1,2006
P06-3007,experiment showed that this idea achieved promising results.,6,2006
P06-3007,"compared with close related work, we achieved encouraging improvement.",1,2006
P06-3008,"we plan to get a larger team to work on the project, so as to make it more comparable to the english and german rst treebanks.",1,2006
P06-3008,"since the distinctive nucleus status of eudas ended with these pms may be useful in deciding growth point for rs-tree construction or for tree pruning in summarization, we are also interested in testing how well a baseline relation classifier performs if it always predicts the most frequent relations for these pms.",1,2006
P06-3009,"in the future we intend to develop more sophisticated models implementing closer interaction between morphology and syntax, by means of which we hope to boost parsing accuracy and improve morphological disambiguation.",1,2006
P06-3010,"in future work we intend to carry out experiments with different settings: (a) combinations of certain kss; (b) other sample corpora, of different sizes, genres / domains; and (c) different parameters in aleph regarding search strategies, evaluation functions, etc.",2,2006
P06-3015,"we are also currently working on scoring modules that incorporate language modelling (with discriminative training), and prosody-based co-analysis.",1,2006
P06-3015,"second, the architecture lends itself to further parallelism-specifically by permitting concurrent processing units to dynamically decide whether to employ the generaliser or specifier, based on the sizes of shared active subspaces.",1,2006
P06-3015,"first, applying the parser to directed two-party dialogue will explore contextsensitivity and a more complex grammar.",1,2006
P07-1001,future work includes implementing the idea in alternative alignment models and also exploiting prior knowledge derived from such as manually-aligned data and pre-existing linguistic resources.,2,2007
P07-1002,"in the future, we would like to explore tighter integration of our order model with the smt system and to develop more accurate algorithms for constructing projective target dependency trees in translation.",1,2007
P07-1003,"while it remains to be seen whether these improvements impact final translation accuracy, it is reasonable to hope that, all else equal, alignments which better respect syntactic correspondences will be superior for syntactic mt.",5,2007
P07-1005,"finally, besides our proposed approach of integrating wsd into statistical mt via the introduction of two new features, we could explore other alternative ways of integration.",1,2007
P07-1005,"for future work, an immediate step would be for the wsd classifier to provide translations for longer chinese phrases.",4,2007
P07-1005,"also, different alternatives could be tried to match the translations provided by the wsd classifier against the chunks of rules.",1,2007
P07-1006,"we plan to further evaluate our approach for other sets of words, including other parts-of-speech to allow further comparisons with other approaches.",3,2007
P07-1009,"qualitatively, our model can recover many well-known implications as well as many more potential implications that can be the object of future linguistic study.",1,2007
P07-1014,"in the current study we have used the binary features provided in upsid, which could be very well replaced by other representations, including multi-valued feature systems; we look forward to do the same as a part of our future work.",1,2007
P07-1017,"the most obvious next step of our research, therefore, is to further pursue the integration of the proposed model to the end to-end mt scenario.",1,2007
P07-1017,"another area of investigation is capturing longer-distance agreement phenomena, which can be done by implementing a global statistical model, or by using features from dependency trees more effectively.",1,2007
P07-1025,"an important area for future research will be to explore the correlation between our distance metric for syntactic similarity and various quantitative measures of semantic similarity (pedersen, et al., 2004).",1,2007
P07-1025,particularly interesting would be to explore whether different senses of a given verb exhibited markedly different profiles of syntactic context.,5,2007
P07-1026,"the extension of our work is to improve the performance of the entire semantic role labeling system using the grammar-driven tree kernel, including all four stages: pruning, semantic role identification, classification and post inference.",1,2007
P07-1026,"in addition, a more interesting research topic is to study how to integrate linguistic knowledge and tree kernel methods to do feature selection for tree kernel based nlp applications (suzuki et al., 2004).",5,2007
P07-1028,"next steps will be to test the similarity-based model “in vivo”, in an srl task; to test the model in a wsd task; to evaluate the model on a primary corpus that is not semantically analyzed, for greater comparability to previous approaches; to explore other vector spaces to address the coverage issue; and to experiment on domain transfer, using an appropriate generalization corpus to induce selectional preferences for a domain different from that of the primary corpus.",3,2007
P07-1031,"firstly, nps with genuine flat structure are currently treated as implicitly right branching.",1,2007
P07-1031,"secondly, there is still ambiguity in determining the head of a noun phrase.",5,2007
P07-1031,there are several distinctions that our annotation currently ignores that we would like to identify correctly in the future.,5,2007
P07-1031,"we would like to be able to identify these multi-head constructs properly, rather than simply treating them as a single entity (or even worse, as two different entities).",1,2007
P07-1033,the most important avenue of future work is to develop a formal framework under which we can analyze this (and other supervised domain adaptation models) theoretically.,1,2007
P07-1033,an additional future direction is to explore the kernelization interpretation further: why should we use 2 as the similarity between domains—we could introduce a hyperparamter α that indicates the similarity between domains and could be tuned via cross-validation.,1,2007
P07-1034,"the framework opens up many interesting future research directions, especially those related to how to more accurately set/estimate those weighting parameters.",1,2007
P07-1035,"much remains to be done in applying infinite models to language structure, and an interesting extension would be to develop inference algorithms that permit completely unsupervised learning of dependency structure.",1,2007
P07-1037,"we expect more work on system integration to improve results still further, and anticipate that similar increases are to be seen for other language pairs.",1,2007
P07-1039,"as for future work, we first plan to consider different confidence measures for the filtering of the alignment candidates.",3,2007
P07-1039,"finally, we would like to apply this method to other corpora and language pairs.",2,2007
P07-1039,"we also want to bootstrap on different word aligners; in particular, one possibility is to use the flexible hmm word-to-phrase model of deng and byrne (2005) in place of ibm model 4.",1,2007
P07-1040,better alignment methods which take synonymy into account should be investigated.,1,2007
P07-1048,"in future, we hope to conduct a longer term study with repeat users to see how previous experience influences use of newer kinds of inputs such as multimodal and handwriting.",3,2007
P07-1054,"also, we see this work as a proof of concept for the applicability of general random-walk algorithms (and not just pagerank) to the determination of the semantic properties of synsets.",1,2007
P07-1056,thus we can use the adistance to select source domains to label which will give low target domain error.,1,2007
P07-1056,"in the future, we wish to include some of the more recent advances in sentiment classification, as well as addressing the more realistic problem of ranking.",1,2007
P07-1056,we are also actively searching for a larger and more varied set of domains on which to test our techniques.,2,2007
P07-1063,"in future work, we hope to directly compare the direct generation method of section 6.1 with the over generate and rank method of section 6.2, and to use these results to refine personage鈥檚 parameter settings.",3,2007
P07-1063,"research by andre (2000); piwek (2003) uses personality variables to affect the linguistic behaviour of conversational agents, but they did not systematically manipulate parameters, and their generators were not evaluated.",1,2007
P07-1063,cassell and bickmore (2003) show that extraverts prefer systems utilizing discourse plans that include small talk.,1,2007
P07-1063,"reeves and nass (1996) demonstrate that manipulations of personality affect many aspects of user perceptions, but their experiments use handcrafted utterances, rather than generated utterances.",3,2007
P07-1064,"we can also explore an unsupervised fusion of these two sources of information; for instance, we can induce informative prosodic cues by using distributional evidence.",1,2007
P07-1064,"in a supervised framework, we can further enhance audio-based segmentation by combining features derived from pattern analysis with prosodic information.",1,2007
P07-1065,"in a companion paper (talbot and osborne, 2007) we have proposed a framework for deriving conventional smoothed n-gram models from the logfrequency bf scheme allowing us to do away entirely with the standard n-gram model in an smt system.",1,2007
P07-1067,"also, we would like to explore the approach in technical (e.g., biomedical) domains, where jargons are frequently seen and the need for external knowledge is more compelling.",1,2007
P07-1071,the complete set of predictions on the test set can be found at http: //ml.nec-labs.com/software/senna.would improve with more hand-built features.,1,2007
P07-1074,figure (6) depicts the pattern learning and new seed extracting behavior during the iterations for the first experiment.,1,2007
P07-1074,"therefore, we can reach most events in a few iterations.",1,2007
P07-1078,we achieved a 50% (20-33%) reduction in annotation cost for the in-domain (out-of-domain) seed data scenarios.,2,2007
P07-1078,a direction for future research is combining self-training data from various domains to enhance parser adaptation.acknowledgement.,2,2007
P07-1081,"figure 3 shows that constructing a corpus with four or more transliterators, the range of possible word accuracies achieved is less than that of using fewer transliterators.",2,2007
P07-1081,"in addition to computing agreement, we also in vestigated the transliterator perception of difficulty of the transliteration task with the ensuing word accuracy of the systems.",1,2007
P07-1081,"interestingly, when using corpora built from transliterators that perceive the task to be easy, there is a large difference in the word accuracy between the two systems, but on corpora built from transliterators who perceive the task to be more difficult, the gap between the systems narrows.",2,2007
P07-1083,"in particular, we plan to investigate approaches that do not require the bilingual dictionaries or bitexts to generate training data.",1,2007
P07-1083,"furthermore, we have provided a natural framework for future cognate identification research.",1,2007
P07-1088,"scalability is a particularly important concern for open information extraction, the task of extracting large numbers of relations that are not specified in advance.",5,2007
P07-1089,we will also conduct experiments on large scale training data to further examine our design philosophy.,2,2007
P07-1098,"forms of generalization for predicates and arguments within pasns like lsa clusters, wordnet synsets and framenet (roles and frames) information also appear as a promising research area.",1,2007
P07-1098,"in the future, we aim to study ways to capture relations between predicates so that more general se 1 n 1 where n is the number of questions and rank n l-锟絠=1 ranki , i is the rank of the first correct answer to question i. mantics can be encoded by pasn.",1,2007
P07-1099,"in our previous work, we evaluated the performance of the framework for english qa using questions from past trec evaluations (ko , 2007).",3,2007
P07-1100,we should note here that extending the system鈥檚 dialogue component library will automatically increase the state space and thus policy generation and optimization will become more difficult and require more training data.,2,2007
P07-1100,"in our system, state representations are composed of multiple context feature values (e.g.communication problem earlier in the dialogue, the confidence of the utterance classifier).",1,2007
P07-1100,"as part of that, we will look at automatically mining libraries of dialogue components from existing dialogue transcript data (e.g.available scripts or transcripts of films, tv series and interviews containing real-life examples of different types of dialogue).",2,2007
P07-1101,"finally, we are also interested in examining speaker entrainment in cue phrase usage, or how subjects adapt their choice and production of cue phrases to their conversation partner鈥檚.",5,2007
P07-1104,"meanwhile, me estimation with l1 regularization achieves the same level of performance while at the same time producing sparse models, and the averaged perceptron provides an excellent compromise of high performance and fast training.",1,2007
P07-1104,"the choice of which to implement should come down to other considerations: if model sparsity is desired, choose me estimation with l1 regularization (or feature selection methods such as blasso); if quick implementation and training is necessary, use the averaged perceptron; and me estimation with l2 regularization may be used if it is important to achieve the highest obtainable level of performance.",1,2007
P07-1105,"investigating where to add probabilities (ontology, grammar rules, or both) is part of our planned future work.",5,2007
P07-1110,"whether the genre of text used as training data affects the absolute rate of retrieval precision for text of a different genre (e.g. news articles, shopping websites) is a separate question, and one we intend to address more fully in future work.",2,2007
P07-1112,"such an approach is especially interesting either for lexicographers aiming at constructing lexicons, but even more for natural language processing systems relying on deep lexical knowledge as represented by qualia structures.",1,2007
P07-1117,"for instance, describing vowel harmony using a partitioning based on morphemes takes necessarily several rules corresponding to the cases where the harmony is within a morpheme or across several morphemes.",1,2007
P07-1119,"we plan to investigate whether using methods like discriminative reranking (och and ney, 2002) on such an n-best list could improve performance.",1,2007
P07-1119,"we tested both dynamic programming and finite-state transducer implementations, the latter of which enabled us to use a word unigram language model to improve the accuracy of generated transliterations.",1,2007
P07-1123,"in this paper, we described two approaches to generating resources for subjectivity annotations for a new language, by leveraging on resources and tools available for english.",1,2007
P07-1126,the combined salience and visualness provide a score that signals the probability that the entity is present in the accompanying image.,1,2007
P07-1127,we would also like to see how frequent argumentative queries are in other domains (such as tv talk shows or political debates) in order to generalize our results.,4,2007
P07-1130,we presented a system for electronic career guidance utilizing nlp and ir techniques.,1,2007
P07-1130,"furthermore, we applied these measures to an ir task, whereby they were used either in combination with a set of heuristics or the wikipedia based measure was used to directly compute semantic relatedness of topics and documents.",4,2007
P07-1130,we intrinsically evaluated and analyzed the properties of two semantic relatedness measures utilizing the lexical semantic information in a german wordnet and wikipedia on the tasks of estimating semantic relatedness scores and answering multiple-choice questions.,3,2007
P07-1130,"given a natural language professional profile, relevant professions are computed based on the information about semantic relatedness.",1,2007
P07-1131,the counts from the corpus certainly help to filter out false information which would otherwise be difficult to filter.,2,2007
P07-2002,how to fully integrate these functions into the system is our next challenge.,5,2007
P07-2002,"although the effectiveness of this interface is yet to be fully examined in real-world situations, the basic concept should be useful as the idea of awareness level comes from feedback by monitors who used the first version of the system.",5,2007
P07-2004,"as (nesson and shieber, 2006) indicates, this extraction in fact makes the resulting system a special case of synchronous tag where the semantic trees are isomorphic to the syntactic trees and unification variables across the syntactic and semantic components are interpreted as synchronous links.",1,2007
P07-2010,"it does not do either complex reasoning or analogical structurematching as in our own att-meta metaphor system (barnden, 2006) or the cited approaches of fass, hobbs, martin, narayanan and veale.",1,2007
P07-2010,"however, we plan to eventually add simplified versions of att-meta-style reasoning, and in particular to add the att-meta view-neutral mapping adjunct feature to implement the default carry-over of affect (see section 2) and certain other information, as well as handling more signals.",1,2007
P07-2010,"the system currently only deals with a small minority of our own list of metaphoricity signals (see section 3.1), and these signals are only present in a minority of cases of metaphor overall.",1,2007
P07-2014,"of course, many questions are yet to be explored, among them the following: can a singular value decomposition (to be in effect only temporarily for the purpose of clustering) reduce the problem of data sparseness?",5,2007
P07-2014,"can biclustering (also referred to as co-clustering or two-mode clustering, i.e. the simultaneous clustering of the rows and columns of a matrix) improve results?",5,2007
P07-2015,"in the future we plan to include features from adjacent sentences (fisher and roark, 2006) and use rouge scores to initially select negative examples.",1,2007
P07-2015,we explore different possibilities for applying them in training svms to rank sentences in order of relevance to the query.,1,2007
P07-2015,the experiments have provided some insights on which can be the best way to exploit the annotations.,1,2007
P07-2017,"on the other hand, we also propose a new method for speeding up classification which is independent to the polynomial kernel degree.",1,2007
P07-2027,"classifier accuracy on two-party data is reasonable, and we see promising results on multiparty data with a basic set of features.",2,2007
P07-2027,we expect the accuracy to go up once we train and test on same genre data and also add features that are more specific to multi-party data.,2,2007
P07-2028,"following the att-meta claim metaphors often convey crucial information via vnmas, we can reanalyze example (1) so that the effects of the necessities as food mapping are obtained by vnmas.",1,2007
P07-2028,"moreover, map-transcending entities pose a problem for analogy-based approaches to metaphor interpretation (falkenhainer , 1989), which require the discovery of an elaborate structural similarity between the source and target domains and/or the imposition of unmapped source domain structures on the target domain, whereas part of our approach is that the unmapped source domain structure introduced by the utterance is by default not carried over.",5,2007
P07-2029,we tested the application for real-time text correction produced in a real-world application.,3,2007
P07-2030,bow denotes a bow feature.,1,2007
P07-2030,experimental results show that our ranking model significantly outperforms baselines that use single ir-related and positional measures for ranking.,1,2007
P07-2033,"in the above wer experiments a 4-gram baseline model was used, which was trained on nearly 1 billion words.",1,2007
P07-2038,"one possible explanation for the unsuitability of the measures of (patwardhan and pedersen, 2006) for the coordinate similarity task could be based on how context is defined when building context vectors.",1,2007
P07-2038,"that is, the distributional similarity measure presented in section 2 defines two words as similar if they occur in coordination patterns with a similar set of words and with similar distributions.",1,2007
P07-2038,"for example, nouns in lists are often semantically similar, and we did not exclude nouns extracted from lists from the non-coordinate test set.",1,2007
P07-2038,"although not all coordinate noun pairs are semantically similar, it seems clear, on inspection of the two sets of data, that they are more likely to be semantically similar than modifier-head word pairs, and the tests carried out for most of the measures of semantic similarity detect a significant difference between the similarity scores assigned to coordinate pairs and those assigned to non-coordinate pairs.",5,2007
P07-2038,"it is not possible to judge, based on the significance tests alone, which might be the most useful measure for the purpose of disambiguation.",5,2007
P07-2038,"a wordnet-based measure of semantic similarity, however, might give a high score to both of the noun pairs.",1,2007
P07-2039,"in addition, we would like to explore the possibility of extending this proposal to other language pairs.",2,2007
P07-2044,we have described a two-stage machine learning approach to event-event temporal relation classification.,1,2007
P07-2049,these facts could be an unplanned side-effect from the way the test topics were produced: annotators might have been influenced by information in the input to be summarizied when defining their topic.,5,2007
P07-2051,"however, we have made idealized assumptions (small inventory of dependency relations and treebank derived chunks) that clearly must be replaced by a realistic setting in our future work.",5,2007
P07-2054,"the extensions consist of accepting and generating word graphs, and introducing two n-gram lm鈥檚 over source and target tagged words.",1,2007
P07-2057,svm-based parsers that have the same characteristics can be constructed if we introduce multi-class classifiers.,1,2007
P07-2057,the other characteristics of our method are using crfs and that long dependencies are parsed in one labeling process.,1,2007
P07-3002,"for a system such as  this to take advantage of the patterns that arise out of the text itself, a much more fine-grained perspective is necessary, since the performance of individual category-assignments to words being the focus of the task.",1,2007
P07-3005,in this paper we proposed an algorithm for the automatic generation of cognates from two different languages sharing the same alphabet.,1,2007
P07-3005,"even if accuracy is rather poor, if we consider that no knowledge repository other than an initial list of cognates was available, we feel that the results are still quite encouraging.",5,2007
P07-3009,"although this seems to work well, we would like to investigate this further and possibly devise a cost based prior that is both theoretically well-grounded and performs well in practice.",1,2007
P07-3009,"in the spv update, the cost was incorporated into the map model in a rather ad-hoc fashion.",1,2007
P07-3009,their performance is on par with other state-of-the-art online learning algorithms for cost-sensitive problems.,1,2007
P07-3014,another interesting avenue for future work in this area is investigation into exactly how joining terms relate to semantic relations.,1,2007
P08-1001,"in the future, we would like to find a way to automatically generate the list of key words and phrases for useful english language categories.",1,2008
P08-1001,"this could implement the work of kazama and torisawa, in particular.",1,2008
P08-1004,"we also plan to explore the capacity of open ie to automatically provide labeled training data, when traditional relation extraction is a more appropriate choice.",5,2008
P08-1006,"in order to obtain general ideas on parser performance, experiments on other tasks are indispensable.",3,2008
P08-1007,"since the accuracy of dependency parsers is not perfect, a possible future work is to identify when best to incorporate such syntactic information.",5,2008
P08-1007,"possible future directions include adding semantic role information, using the distance between item pairs based on the token position within each sentence as additional weighting consideration, etc.",1,2008
P08-1007,"also, we have seen that dependency relations help to improve correlation on the nist dataset, but not on the acl-07 mt workshop datasets.",2,2008
P08-1011,"in the future, we plan to investigate more features and enlarge coverage to improve the quality of measure word generation, especially reduce the errors found in our experiments.",1,2008
P08-1013,it would therefore be interesting to investigate ways of introducing word information into our grammar-based model.,1,2008
P08-1014,in the future we plan to integrate additional knowledge sources into our statistical method in order to more specifically address each of the various phenomena encountered in spontaneous dictation.,2,2008
P08-1014,"while the present experiments have used a separate auto punctuation step, future work will aim to eliminate it by integrating the punctuation features into the transformation step.",2,2008
P08-1015,"in future work, we will examine the strengths and limitations of grounded language modeling in these domains.",1,2008
P08-1015,"also, we are examining extending this approach to other sports domains such as american football.",4,2008
P08-1015,"in future work, we will examine the ability of grounded language models to improve performance for other natural language tasks that exploit text based language models, such as machine translation.",1,2008
P08-1017,"first, we will study the same problem in some specialized domain, such as biology literature, to see whether the proposed approach could be generalized to the new domain.",4,2008
P08-1017,"second, the fact that using axiomatic approaches to incorporate linguistic information can improve retrieval performance is encouraging.",1,2008
P08-1017,"we plan to extend the axiomatic approach to incorporate more linguistic information, such as phrases and word senses, into retrieval models to further improve the performance.",1,2008
P08-1017,there are many interesting future research directions based on this work.,6,2008
P08-1018,"finally, it would be interesting to test the approaches using real web data.",2,2008
P08-1018,"for example, we could integrate other features in the regression model, and use other nonlinear regression models, such as bayesian regression models  (rasmussen and williams, 2006).",1,2008
P08-1018,our methods can be further improved in several aspects.,1,2008
P08-1019,"thus, as future work, we will try to investigate the use of the proposed approach for other kinds of web services.",4,2008
P08-1022,"finally, further efforts to engineer a grammar suitable for realization from the ccgbank should provide richer feature sets, which, as our feature ablation study suggests, are useful for boosting hypertagging performance, hence for finding better and more complete realizations.",1,2008
P08-1022,"this suggests that further improvements to the hypertagger will lead to more complete realizations, hence more high-quality realizations.",1,2008
P08-1023,"for future work, we would like to use packed forests not only in decoding, but also for translation rule extraction during training.",1,2008
P08-1025,"as future work, new metrics for the final pass may be able to better approximate bleu.",1,2008
P08-1025,"as the bigram decoding pass currently takes the bulk of the decoding time, better heuristics for this phase may speed up the system further.",1,2008
P08-1027,we intend to pursue this promising direction in future work.,6,2008
P08-1028,future directions include constraining the number of free parameters in linguistically plausible ways and scaling to larger datasets.,2,2008
P08-1029,our future work is focused on designing an algorithm to optimally choose a smoothing regime for the learned feature trees so as to better exploit the similarities between domains while neutralizing their differences.,1,2008
P08-1029,"along these lines, we are working on methods to reduce the amount of labeled target domain data needed to tune the prior-based models, looking forward to semi-supervised and unsupervised transfer methods.",1,2008
P08-1030,"ultimately we would like to extend the system to perform essential, although probably lightweight, event prediction.",1,2008
P08-1032,an interesting future direction concerns the application of the proposed model in a semi-supervised setting where the annotation output is iteratively refined with some manual intervention.,4,2008
P08-1034,among the most promising directions for future research in the direction laid out in this paper is the deployment of more advanced classifiers and feature selection techniques that can further enhance the performance of the ensemble of classifiers.,1,2008
P08-1035,"if it is, the whole scheme of ours would fall under what is known as linear programming crfs (tasker, 2004; roth and yih, 2005).",5,2008
P08-1035,an interesting future exercise would be to explore whether it is feasible to rewrite eq.5 as a linear integer program.,5,2008
P08-1036,the primary area of future work is to incorporate the model into an end to-end sentiment summarization system in order to evaluate it at that level.,3,2008
P08-1040,"another interesting extension involves incorporating parser uncertainty into the model; in particular, our simplification system is capable of seamlessly accepting a parse forest as input.",1,2008
P08-1044,"in addition, our results suggest a rich area for future research in further analyzing the variability of both lexical and prosodic effects on asr behavior for different speakers.",1,2008
P08-1047,future work will be to refine the matching method and to construct even larger gazetteers. their gazetteers.,2,2008
P08-1048,we also plan to study methods of automatically updating the 1911 roget鈥檚 thesaurus with modern words.,1,2008
P08-1048,another longer-term direction of future work could be merging roget’s thesaurus with wordnet.,2,2008
P08-1050,"for our future work, we aim to test whether an automatically created verb classification can be beneficial to other nlp tasks.",5,2008
P08-1051,"we believe that this information, especially the answer agreement of workers, can be successfully used in post-processing and analysing the data, as well as automatically accepting and rejecting workers submissions in similar future data collection exercises.",2,2008
P08-1052,"using a parser with a richer set of syntactic dependency features, e.g., as proposed by pad麓o and lapata (2007), is another promising direction for future work.",1,2008
P08-1055,"in future work we hope to test these algorithms in other domains, and show that intentional summaries can not only be automatically derived but also lead to reduced task times and increased task success.",1,2008
P08-1059,"in the future, we would like to include more sophistication in the design of a lexicon for a particular language pair based on error analysis, and extend our pre-processing to include other operations such as word segmentation. however, the improvement in arabic is not statistically significant on this 100 sentence set.",1,2008
P08-1060,further research is needed to determine whether these observations generalize to other knowledge mining patterns.,5,2008
P08-1080,the obvious next step is to determine how to automatically classify queries according to their predicted result length and type.,5,2008
P08-1080,We plan to pursue this further in future work.,6,2008
P08-1084,"in the future, we hope to apply similar multilingual models to other core unsupervised analysis tasks, including part-of-speech tagging and grammar induction, and to further investigate the role that language relatedness plays in such models.7",4,2008
P08-1086,another interesting direction of further research is to evaluate the use of the presented clustering technique for language model size reduction.,3,2008
P08-1086,we furthermore expect to be able to increase the gains resulting from using class-based models by using more sophisticated techniques for combining them with word-based models such as linear interpolations of word- and class-based models with coefficients depending on the frequency of the history.,1,2008
P08-1089,in the future we wish to exploit more feature functions in the log-linear model.,1,2008
P08-1089,"in addition, we will try to make better use of the context information when replacing paraphrase patterns in context sentences.",1,2008
P08-1091,"finally, we would like to experiment with automatic parses and different syntactic formalisms such as dependencies and shallow parses.",1,2008
P08-1091,"for future work, we would like to explore further explicit morphological features such as aspect tense and voice as well as richer pos tag sets such as those proposed in (diab, 2007).",1,2008
P08-1092,in future we plan to explore such a generalization of our procedures to such domains.,4,2008
P08-1092,"since our task is to produce concise summaries, one focus of our future research will be to simplify the sentences we extract before classifying them as biographical or non-biographical.",1,2008
P08-1093,"an important future work is to construct larger test sets (e.g., of biomedical literature) to facilitate evaluation of impact summarization.",2,2008
P08-1094,a more complete characterization of summarization input will be necessary in the future.,1,2008
P08-1096,"in our future work, we would like to investigate more sophisticated clustering methods that would lead to global optimization, e.g., by keeping a large search space (luo et al., 2004) or using integer programming (denis and baldridge, 2007).",1,2008
P08-1101,"therefore, given the flexibility of the feature-based linear model, an obvious next step is the study of open features in the joint segmentor and pos tagger.",1,2008
P08-1104,"in future, we are to explore more domain independent evidences and evaluate the proposed model on the basis of the data from other domains.",3,2008
P08-1105,"other future work concerns indicator selection: instead of taking all indicators on board, consider selected indicators only, in a topic dependent fashion.",3,2008
P08-1106,"future work will focus on developing methodologies for computer-assisted back-of-the-book indexing, as well as on the use of the automatically extracted indexes in improving the browsing of digital libraries.",1,2008
P08-1107,we are also interested in exploring iterative approaches to jointly resolving mentions.,1,2008
P08-1107,"in future work, we plan to extend our test collection with mention queries that must be resolved in the 鈥渓ong tail鈥 of the identity distribution where less evidence is available.",2,2008
P08-1108,"directions for future research include a more detailed analysis of the effect of feature-based integration, as well as the exploration of other strategies for integrating different parsing models.",1,2008
P08-1110,"an interesting line for future work is to use relationships between schemata to find nonprojective parsers that can be derived from existing projective counterparts. an alternative framework that formally describes some dependency parsers is that of transition systems (mcdonald and nivre, 2007).",1,2008
P08-1116,"in the future work, we will try to use syntactic and context constraints in paraphrase generation to enhance the acceptability of the paraphrases.",1,2008
P08-1116,"in addition, we will extract paraphrase patterns that contain more structural variation and try to combine the smt-based and pattern-based systems for sentence level paraphrase generation.",1,2008
P08-2005,automating the creation of inventories of pedagogically important concepts may represent an important step towards scalable intelligent tutoring systems.,1,2008
P08-2009,we conclude with an error analysis to provide future direction.,1,2008
P08-2009,this indicates that further work on data driven methods may still improve the state of the art.,1,2008
P08-2010,"we intend to further explore the idea of boosting on n-best lists, drawing inspirations from the large body of work on boosting for classification whenever possible.",5,2008
P09-1001,"finally, we will explore the theoretical foundations and limitations of heterogeneous transfer learning as well.",5,2009
P09-1001,"we may also consider clustering, topic modeling, question answering, etc., to be done using data in different feature spaces.",2,2009
P09-1001,"in natural language processing, there are many future opportunities to apply heterogeneous transfer learning.",2,2009
P09-1001,"we can consider data in different modalities, such as video, image and audio, as the training data.",2,2009
P09-1001,"in (ling et al., 2008) we have shown how to classify the chinese text using english text as the training data.",2,2009
P09-1002,"also, we will extend the current dataset using more annotators and exploring additional lexicon resources.",2,2009
P09-1002,"as a next step, we will automatically cluster the judgments we obtained in the wssim and usim experiments to further explore the degree to which the annotation gives rise to sense grouping.",5,2009
P09-1002,we will also use the ratings in both experiments to evaluate automatically induced models of word meaning.,1,2009
P09-1004,"adapting our algorithm to use the output of these models, either to reduce the little supervision our algorithm requires (pos tagging) or to provide complementary syntactic information, is an interesting challenge for future work.",1,2009
P09-1005,"potential future work may focus on developing an improved ccg parser using the revised (syntactic) adjunct-argument distinctions (guided by the propbank annotation) described in (boxwell and white, 2008).",1,2009
P09-1006,"future work includes further investigation of our conversion method for other pairs of grammar formalisms, e.g., from the grammar formalism of the penn treebank to more deep linguistic formalism like ccg, hpsg, or lfg.",1,2009
P09-1012,"we believe this will be particularly beneficial to build a language model on such texts as speech transcripts, colloquial texts or unknown languages, where word boundaries are hard or even impossible to identify a priori.",1,2009
P09-1014,kernel functions could also be used to automatically create a richer feature space; preliminary experiments have shown gains in performance using polynomial and rbf kernels with our stress ranker.,1,2009
P09-1014,"in the future, we will investigate additional features to leverage syllabic and morphological information, when available.",1,2009
P09-1016,"a high quality alignment on a small verified corpus such as xinhua can be effectively used to validate a large noisy corpus, such as ldc05.",2,2009
P09-1016,we propose using alignment distance to validate transliterations.,1,2009
P09-1016,"we believe that this property would be useful in transliteration extraction, cross-lingual information retrieval applications.",4,2009
P09-1017,to really know if the carried out theoretical work is valuable we would like to try it out in a real search setting in a search engine and see if the users appreciate the new algorithm鈥檚 results.,1,2009
P09-1017,work with the new affix lemmatizer has until now focused on the algorithm.,1,2009
P09-1023,it is challenging but possibly fruitful to recast the normal documents with wiki styles so as to adopt edcl for free text and enrich the research efforts on other nlp tasks.,1,2009
P09-1023,"more generally, the knowledge hidden in nontextual features of wikipedia allow the model to harvest better definition summaries.",1,2009
P09-1024,"moreover, a promising direction is to consider hierarchical discourse formalisms such as rst (mann and thompson, 1988) to supplement our template-based approach.",1,2009
P09-1024,"diseases and american film actors exhibit fairly consistent article structures, which are successfully captured by a simple template creation process.",1,2009
P09-1024,"however, with categories that exhibit structural variability, more sophisticated statistical approaches may be required to produce accurate templates.",1,2009
P09-1024,This work opens several directions for future research.,6,2009
P09-1025,an important future direction concerns a more detailed assessment of our search procedure.,3,2009
P09-1027,"we will employ the structural correspondence learning (scl) domain adaption algorithm used in (blitzer et al., 2007) for linking the translated text and the natural text.",1,2009
P09-1027,"in future work, we will improve the sentiment classification accuracy in the following two ways: 1) the smoothed co-training approach used in (mihalcea, 2004) will be adopted for sentiment classification.",1,2009
P09-1028,"as a topic of vigorous current activity, there are several very recently proposed competing methodologies for sentiment analysis that we would like to benchmark against.",1,2009
P09-1028,These are topics for future work.,6,2009
P09-1033,future work will investigate another basic property of semantic role labelling schemes: cross-linguistic validity.,1,2009
P09-1034,future work will have to assess the effectiveness of individual features and investigate ways to customize rte systems for the mt evaluation task.,3,2009
P09-1034,"nevertheless, it must be an important focus of future research to develop robust meaning-based metrics for other languages that can cash in the promise that we have shown for evaluating translation into english.",3,2009
P09-1034,fig.2) and may find use in uncovering systematic shortcomings of mt systems.,4,2009
P09-1036,"in the future work, we will use other language pairs to test our models so that we could know whether our method is language-independent.",3,2009
P09-1040,"in addition, we want to explore alternative oracle functions, which try to minimize the number of swaps by allowing the stack to be temporarily 鈥渦unsorted鈥.",1,2009
P09-1040,future research will include an in-depth error analysis to find out why the system works better for some languages than others and why the exact match score improves even when the attachment score goes down.,5,2009
P09-1046,we believe that such efforts can further improve cross document coreference performance.,1,2009
P09-1046,future research directions include developing rich feature sets and using corpus level or external information.,2,2009
P09-1047,this indicates that the framework can reduce the cost of preparing training data in new languages with the help of our english and japanese strong classifiers.,1,2009
P09-1047,Our future work focuses on this issue.,6,2009
P09-1049,"we also plan to explore whether lexicons can be constructed using only the back-off method for hyponym extraction, to make asia completely language independent.",5,2009
P09-1049,"in the future, we plan to investigate the possibility of constructing hypernym hierarchy automatically using semi-structured documents.",2,2009
P09-1051,we also intend to consider the rule base as a directed graph and exploit the graph structure for further rule extraction and validation.,1,2009
P09-1056,further experiments are of course necessary to investigate distributional representations as smoothing techniques.,3,2009
P09-1056,one particularly promising area for further study is the combination of smoothing and instance weighting techniques for domain adaptation.,1,2009
P09-1056,"whether the current techniques are applicable to structured prediction tasks, like parsing and relation extraction, also deserves future attention.",5,2009
P09-1058,"in future work, we plan to apply our framework to other asian languages, including thai and japanese.",2,2009
P09-1059,"in the future, we will continue to research on annotation adaptation for other nlp tasks which have different annotation-style corpora.",2,2009
P09-1059,"especially, we will pay efforts to the annotation standard adaptation between different treebanks, for example, from hpsg lingo redwoods treebank to ptb, or even from a dependency treebank to ptb, in order to obtain more powerful ptb annotation-style parsers.",4,2009
P09-1060,"to demonstrate practicality of our method for automatic speech transcription, an experiment using a continuous speech recognition system will be performed in the future.",3,2009
P09-1061,"we also plan to use this algorithm to annotate different types of data, such as spontaneous speech, and incorporate prosodic events in spoken language applications.",2,2009
P09-1061,"for the future work, we will perform analysis of loss function of each classifier in order to estimate parameters without labeled development data.",1,2009
P09-1061,"in addition, we plan to compare this to other semi-supervised learning techniques such as active learning.",3,2009
P09-1062,"additional analysis suggests that the acoustics-based approach is useful in overcoming situations where out-ofvocabulary error may be more prevalent, and we suggest that a hybrid approach of traditional asr with acoustics-based pattern matching may be the most desirable future direction of research.",1,2009
P09-1064,"therefore, future work may show additional benefits from fast consensus decoding.",1,2009
P09-1065,"in the future, we plan to optimize feature weights for max-translation decoding directly on the entire packed translation hypergraph rather than on n-best derivations, following the latticebased mert (macherey , 2008).",1,2009
P09-1066,"in the future, we will extend our method to use lattice or hypergraph to compute consensus statistics instead of n-best lists.",1,2009
P09-1068,finding the best argument representation is an important future direction.,1,2009
P09-1068,we hope in the future to show that a range of nlu applications can benefit from the rich inferential structures that narrative schemas provide.,1,2009
P09-1068,"the exact balance between lexical units, clusters, or more general (traditional) semantic roles remains to be solved, and may be application specific.",5,2009
P09-1072,we look forward to future research to extend the intermediate representation and to experiment with different modeling methodologies.,1,2009
P09-1073,"because their work basically builds on inductive logic programing, we can naturally extend this to incorporate our caching mechanism into the global optimization by expressing cache constraints as predicate logic, which is one of our next challenges in this research area.",1,2009
P09-1075,"borrowing techniques from generic global optimization meta algorithms such as simulated annealing (kirkpatrick et al., 1983) should allow us to better deal with issues of local optimality while retaining acceptable time-complexity.",1,2009
P09-1075,"future directions for this work notably include a better tree-building algorithm, with improved exploration of the solution space.",1,2009
P09-1081,we will investigate experiment 3 by using real questions from different sources and construct different test datasets.(2) we will use other distance measures to better explain entailment between q/a pairs and compare with other semisupervised and transductive approaches.,1,2009
P09-1082,"in the future, we would like to further evaluate the models presented in this paper for different tasks, such as question paraphrase retrieval, and larger datasets.",3,2009
P09-1082,we also plan to improve question analysis by automatically identifying question topic and question focus.,1,2009
P09-1085,"that is, beginning with only characters in the lexicon and using the training data to alter the current lexicon in each iteration.",1,2009
P09-1085,"this can be amended by involving the discriminative language model adaptation in the iteration, which results in a unified language model and lexicon adaptation framework.",1,2009
P09-1085,"however, there still remain lots to be improved.",6,2009
P09-1085,"moreover, the same procedure can be used in the construction.",1,2009
P09-1087,we plan to pursue other research directions using dependency models discussed in this paper.,4,2009
P09-1087,"while we use a dependency language model to exemplify the use of hierarchical structure within phrase based decoders, we could extend this work to incorporate dependency features of both sourceand target side.",1,2009
P09-1088,"in future we envision it will be possible to use the techniques developed here to directly induce grammars which match state-of-the-art decoders, such as hiero grammars or tree substitution grammars of the form used by galley et al.(2004).",1,2009
P09-1089,"for future work we suggest generating entailed texts with a more extensive set of rules, in particular lexical-syntactic ones.",2,2009
P09-1089,"this is an approach that we intend to explore in future work, as a way to efficiently handle the different source language alternatives generated by entailment rules.",1,2009
P09-1089,developing better context-models to be applied on the source is expected to further improve our method鈥檚 performance.,1,2009
P09-1089,"specifically, we suggest taking into account the prior likelihood that a rule is correct as part of the model score.",1,2009
P09-1089,combining rules from monolingual and bilingual resources seems appealing as well.,1,2009
P09-1090,this course of future action is suggested by the fact that smaller sentences are much more fluent in translation compared to medium length and long sentences.,2,2009
P09-1090,future work consists of investigations into (i) how the internal structure of constituents can be strictly preserved and (ii) how to glue together correctly the syntactically well-formed bits and pieces of the sentences.,5,2009
P09-1094,"first, we will improve the components of the method, especially the paraphrase planning algorithm.",1,2009
P09-1094,"the algorithm currently used is simple but greedy, which may miss some useful paraphrase units.",1,2009
P09-1094,"second, we will extend the method to other applications, we hope it can serve as a universal framework for most if not all applications.",4,2009
P09-1094,our future work will be carried out along two directions.,6,2009
P09-1097,another interesting and practically useful problem that we have left for future work is to design an unsupervised learning algorithm for cfsg similar to its phrase structure counterpart: inside-outside algorithm (baker 1979).,1,2009
P09-1097,"note that although strict ordering constraints such as those imposed by psg is not appropriate for modeling query structure, it might be helpful to take ordering information into account when resolving ambiguity.",1,2009
P09-1097,"having such a capability, we are able to automatically learn the underlying structure of queries by processing the huge amount of available unlabeled queries.",1,2009
P09-1098,"in the future, we would like to integrate the text segmentation module with the seed mining and pattern learning module to improve the accuracy of text segmentation.",1,2009
P09-1098,we also want to evaluate the usefulness of our mined data for machine translation or other applications.,3,2009
P09-1100,our long term goal is to provide guidance of how to effectively build user simulations for different dialog system development tasks given limited resources.,2,2009
P09-1100,"in the future, we will conduct follow up studies to confirm our current findings since there are several factors that can impact our results.",3,2009
P09-1101,"we also plan to use the information from the segmentation to examine the structure of segments, especially the sequences of dialogue acts within them, with a view to improving a dialogue act tagger.",1,2009
P09-1102,"since there are numerous unlabeled full forms on the web, it is possible to use a semisupervised approach in order to make use of such raw data.",1,2009
P09-1102,This is an area for future research.,6,2009
P09-1103,"therefore, further work of our studies includes the optimization of the large rule set of the snctssg based model.",2,2009
P09-1105,for future work we would like to explore richer models to estimate alignment posterior probability.,1,2009
P09-1105,"in most cases, exact calculation by summing over all possible alignments is impossible, and approximation using n-best alignments is needed.",5,2009
P09-1109,we will also pursue integration of our method with parsers.,1,2009
P09-1109,"we will investigate similarity measures different from sequence alignment, to better capture the symmetry of these conjuncts.",3,2009
P09-1109,"because they have advantages in different coordination phrase types, their integration looks promising.",1,2009
P09-1113,"it remains for future work to see whether simpler, chunk-based syntactic features might be able to capture enough of this gain without the overhead of full parsing, and whether coreference resolution could improve performance.",5,2009
P09-1117,future research is needed to empirically investigate into this area and quantify the savings in terms of the time achievable with sesal in the ner scenario.,1,2009
P09-1118,"moreover, for a thorough analysis on the effect of our framework, additional experiments on larger and more realistic collections (e.g. the web environment) would be required.",2,2009
P09-1118,These will be our future work.,6,2009
P09-1119,"as to (i), we first want to determine whether a query should be expanded, and next select the appropriate expansion model.",1,2009
P09-1119,future work focuses on two themes: (i) topic dependent model selection and (ii) improved estimates of components.,1,2009
P09-1119,"finally, we can also include the estimation of formula, the importance of a document in the collection.",1,2009
P09-1119,Another possibility is to look at solutions used in distributed IR.,1,2009
P09-1120,"finally, we will consider other alternatives to the decision tree framework when combining the results of the models with their confidence scores.",1,2009
P09-1120,"furthermore, we would like to improve the accuracy of classifier2 with additional non-linguistic features.",1,2009
P09-1120,we are also planning to extend the number of languages in our data set.,2,2009
P09-1120,"in future, we would like to improve the accuracy of our data generation system by considering additional features proposed in the studies of automated query taxonomy, and doing a more careful examination in the assignment of the parameter values.",2,2009
P09-1121,"furthermore, we may involve more sophisticated monolingual features that do not transfer cross-lingually but are asymmetric for either side, such as clustering, document classification features built from domain taxonomies like dmoz.",1,2009
P09-2003,"this way, one could process predicates whose range boundaries are better known first.",1,2009
P09-2003,we plan to include this strategy in future work.,1,2009
P09-2009,"third, if this system can be expanded to trigrams or even n-grams using a larger training corpus, we believe that the tagging accuracy will increase.",2,2009
P09-2009,we are currently working on developing a small and more compact tag set.,2,2009
P09-2009,"first, the size of the manually tagged part of the corpus will have to be increased.",2,2009
P09-2009,"second, a suitable procedure for handling unknown proper nouns will have to be developed.",1,2009
P09-2009,we propose the following additional work for improved performance.,1,2009
P09-2010,"in terms of future work, a more extensive error analysis will be performed to locate the pre cise benefits of the parser combination.",1,2009
P09-2010,"we will also investigate the application of the method directly to raw text and application to a task which may benefit specifically from the combined analyses, such as semantic role labeling or semantic verb classification.",4,2009
P09-2011,"to process indirect left-recursive structures, we need to extend our method.",1,2009
P09-2011,"in future work, we will investigate the incremental parser for head-final language such as japanese.",1,2009
P09-2011,"in this paper, we dealt with direct left-recursive structures only.",1,2009
P09-2013,"furthermore, we believe it would be interesting to discuss linguistically and psycholinguistically the differences between japanese and other european languages such as english.",3,2009
P09-2013,we also expect that structure analysis of compound nouns can be incorporated by extending the dependency relation types.,1,2009
P09-2013,in future work we plan to combine morphological analysis or word segmentation into our proposed algorithm.,1,2009
P09-2013,we would like to know what differences lead to easiness of analyzing a japanese sentence.,5,2009
P09-2017,it would be interesting to explore ways to substitute querying yahoo! so as to make the system quicker.,1,2009
P09-2017,experimentation with more sophisticated graph connectivity measures could possibly improve accuracy.,3,2009
P09-2018,in future work we plan to apply disambiguation techniques to address this problem.,1,2009
P09-2018,"we also plan to evaluate the performance of directional measures in additional tasks, and compare it with additional symmetric measures.",3,2009
P09-2019,"we are particularly interested in domain adaptation, and whether distributional similarities can profit from domain corpora for better performance.",2,2009
P09-2019,current efforts are devoted to study the integration of the selectional preference models presented in this paper in a in-house srl system.,1,2009
P09-2020,"future work involves studying the robustness of our discourse segments on other corpora, such as formal texts from the medical domain and other informal texts.",2,2009
P09-2021,our future work includes adding more expert knowledge through error analysis to incrementally improve the performance.,1,2009
P09-2021,"furthermore, actual development and evaluation of a db-call system will be arranged so that we may investigate how much the cost of collecting data and evaluation would be reduced by using language learner simulation.",3,2009
P09-2022,"however, because our analyzer has scalability that can freely add new features, for our future work, we hope to adopt the case frames as new features and compare their effect.",3,2009
P09-2023,"thus, our future work will include integrating the proposed features with bottom-up information such as acoustic-score based confidence measures.",1,2009
P09-2023,"additionally, we simply assumed in this study that all affirmative and negative responses following the explicit confirmation are correct.",5,2009
P09-2024,"in the future, we will further investigate the relationships between the concept clusters in the question and the answers.",1,2009
P09-2026,"in our future work, we hope to explore alternative approaches that allow reordering or paraphrasing along with deleting words to make compressed sentences more grammatical and coherent.",1,2009
P09-2027,"since the notion of query-focus is apparently missing in any or all of the algorithms, the future summarization algorithms must try to incorporate this while designing new algorithms.",1,2009
P09-2030,"meanwhile, we would like to investigate more appropriate techniques to use feedback, and we are interested in applying co-frank to the other applications, such as opinion summarization where the integration of opinion-biased and document-biased ranking is necessary.",4,2009
P09-2030,"although we show the promising achievements of co-frank from the perspective of experimental studies, we expect a more theoretical analysis on co-frank.",1,2009
P09-2030,There is still a lot of work to be done on this topic.,6,2009
P09-2035,"in the future, we will extend our work from decoding to training time, where we divide the bilingual sentences accordingly.",4,2009
P09-2036,"in future work, we look forward to evaluating the wide array of forest binarization strategies that are enabled by our asynchronous approach.",3,2009
P09-2038,the handling of negative words and metaphors and their impact in detecting sentence level emotion along with document level analysis are the future areas to be explored.,5,2009
P09-2039,"in our future work, we plan to classify comparative types and to extract comparative relations from identified comparative sentences.",1,2009
P09-2042,"in future work, we plan to extend the proposed hierarchical learning method to the case where the hierarchy is a dag instead of tree and scale up the method further.",1,2009
P09-2043,we plan to find out product attributes that contribute most to modeling the interaction among the proposed clues in effective sentiment summarization.,1,2009
P09-2044,"the transient nature of the weasel tag suggests to use the wikipedia edit history for future work, since the edits faithfully record all occurrences of weasel tags.",2,2009
P09-2047,"in future, we will further explore a new method of parameter k selection to achieve higher performance.",1,2009
P09-2047,our approach will be capable of extracting semantic concepts from queries.,1,2009
P09-2047,"Besides, it can extended to Chinese word segmentation.",4,2009
P09-3002,we propose to look into these for further insights.,5,2009
P09-3002,"as the hydt grows, we are bound to come across more instances as well as more types of non-projective constructions that could bring forth interesting phenomenon.",5,2009
P09-3007,"although there have been speculations and trails on things that function labels might help with, it remains to be important to discover how function labels contribute to other nlp applications, such as the japanesechinese machine translation system we have been working on.",4,2009
P09-3011,one of the main directions of our future work will be how to improve the performance of personal name disambiguation.,1,2009
P09-3011,"moreover, word-based text features haven't solved two difficult problems of natural language problems: synonym and polysemy, which seriously affect the precision and efficiency of clustering algorithms.",5,2009
P09-3011,text representation based on concept and topic may solve the problem.,1,2009
P09-3011,computing weight based on a window around names may be helpful.,1,2009
P09-3012,the next step will be to compared the output of a standard clustering algorithm to the gold standard.,3,2009
P09-3012,this gold standard will be used for further experiments on clustering for multi-document summarization.,3,2009
P10-1001,"one idea would be to consider extensions and modifications of our parsers, some of which have been suggested in sections 5 and 7.4.",1,2010
P10-1001,a second area for future work lies in applications of dependency parsing.,4,2010
P10-1002,"however, considering its lower performance on human-annotated treebanks, the dependency parsing method itself still need a lot of investigations, especially on the training method of the classifier.",1,2010
P10-1003,"first, we may attempt to apply the bilingual subtree constraints to transition based parsing models (nivre, 2003; yamada and matsumoto, 2003).",4,2010
P10-1003,"third, larger unannotated data can be used to improve the performance further.",2,2010
P10-1003,"second, we may apply the proposed method for other language pairs such as japanese-english and chinese-japanese.",2,2010
P10-1003,"here, we may design new features for the models.",1,2010
P10-1004,"in the future, it will be interesting to explore how these semantic representations can be used in applications.",4,2010
P10-1005,"as discussed, this requires a contextually appropriate selection of the quantifier restriction8, as well as determining inheritance of properties from classes to individuals and the formalization of defaults.",1,2010
P10-1005,"as a next step, we will apply our approach to the classification of generic sentences.",4,2010
P10-1005,"although our results are satisfying, in future work we will extend the range of features for further improvements.",1,2010
P10-1005,"in particular, we will address lexical semantic features, as they tend to be effected by sparsity.",1,2010
P10-1006,"for future work, we want to develop a framework which can uniformly model the semantic knowledge and the contextual clues for named entity disambiguation.",1,2010
P10-1008,"in future work, we will evaluate the learned policies with real users to examine how well they adapt, and examine how real users evaluate them (subjectively) in comparison to baselines.",3,2010
P10-1008,whether the learned policies perform better or as well as a hand-coded policy painstakingly crafted by a domain expert (or learned using supervised methods from an expert-layperson corpus) is an interesting question that needs further exploration.,5,2010
P10-1008,"also, it would also be interesting to make the learned policy account for the user's learning behaviour and adapt accordingly.",1,2010
P10-1009,"we list below some possible future extensions: 1) integrating different selection strategies, e.g., the listwise strategy that defines the loss function on all the sentences associated with a document to be summarized, into this framework, 2) exploring different modeling approaches for this framework, 3) investigating discriminative training criteria for training the component models in this framework, and 4) extending and applying the proposed framework to multidocument summarization tasks.",1,2010
P10-1010,we need leaner methods for building machine translation systems; new algorithms for cross-linguistic bootstrapping via multiple paths; more effective techniques for leveraging human effort in labeling data; scalable ways to get bilingual text for unwritten languages; and large scale social engineering to make it all happen quickly.,1,2010
P10-1011,"it would be interesting to further investigate this observation with other sources of lexicons (e.g., obtained from parallel or comparable corpora) and for other tasks, such as cross-lingual word sense disambiguation and information retrieval.",2,2010
P10-1011,"in fact, nas can be viewed as a general measure for word similarity between languages.",1,2010
P10-1012,future studies will also include experiments using data of various languages.,2,2010
P10-1012,"future studies will improve our method, enabling it to achieve high correlation in sentence-level fluency.",1,2010
P10-1013,"in the future, we plan to run woe over the billion document cmu clueweb09 corpus to compile a giant knowledge base for distribution to the nlp community.",2,2010
P10-1013,"other data sources, such as freebase, could be used to create an additional training dataset via self-supervision.",2,2010
P10-1013,"we are also interested in merging lexicalized and open extraction methods; the use of some domain-specific lexical features might help to improve woe’s practical performance, but the best way to do this is unclear.",1,2010
P10-1013,"finally, we wish to combine woeparse with woepos (e.g., with voting) to produce a system which maximizes precision at low recall.",1,2010
P10-1019,"in the future we hope to develop a domain with more realistic speech acts and a more difficult dialogue task that will, among other things, highlight this situation.",4,2010
P10-1019,"we also plan on implementing a fully functional idtb system, using an incremental processing architecture that not only detects, but generates, a wide array of turn-cues.",1,2010
P10-1021,a key objective for future work will be to investigate models that integrate semantic constraint with syntactic predictions more tightly.,1,2010
P10-1021,"at the same time, the semantic model should have access to syntactic information, i.e., the composition of word representations should take their syntactic relationships into account, rather than just linear order.",1,2010
P10-1021,"for example, we could envisage a parser that uses semantic representations to guide its search, e.g., by pruning syntactic analyses that have a low semantic probability.",1,2010
P10-1023,"as future work, we plan to apply our method to other languages, including eastern european, arabic, and asian languages.",2,2010
P10-1023,"we plan in the near future to apply babelnet to the challenging task of cross-lingual wsd (lefever and hoste, 2009).",2,2010
P10-1023,"finally, we aim to apply babelnet to a variety of applications which are known to benefit from a wide-coverage knowledge resource.",4,2010
P10-1023,"we also intend to link missing concepts in wordnet, by establishing their most likely hypernyms.",1,2010
P10-1024,this calls for a future study into the semantics of prepositions and their relation to the core-adjunct distinction.,1,2010
P10-1025,"moreover, dimensionality reduction methods alternative to lsa, as currently studied on semisupervised spectral learning (johnson and zhang, 2008), will be experimented.",1,2010
P10-1025,"future work will study the application of the flexible srl method proposed to other languages, for which less resources are available and worst training conditions are the norm.",4,2010
P10-1027,"for example, we can also evaluate its effectiveness and costs during the operation of a discussion forum, where the discussion thread is continually updated by new comments and votes.",3,2010
P10-1027,"indeed, its power is yet to be further improved and investigated.",1,2010
P10-1027,This study can be extended in a few interesting ways.,1,2010
P10-1028,"for example, in future work we plan to investigate the combination of the clickthrough data collected from a web browser with the noisy but large query sessions collected from a commercial search engine.",2,2010
P10-1029,"In future work, we plan to investigate whether these semantic taggers can be used to improve other tasks.",4,2010
P10-1031,developing and testing alternate methods for hierarchical modeling in ontousp;,1,2010
P10-1031,investigating the theoretical properties of ontousp learning approach and generalizing it to other tasks;,4,2010
P10-1031,directions for future work include: exploiting the ontological structure for principled handling of antonyms and (more generally) expressions with opposite meanings;,1,2010
P10-1031,scaling up learning and inference to larger corpora;,2,2010
P10-1031,answering questions that require inference over multiple extractions;,5,2010
P10-1033,"as the success of hp-ditg illustrates the merit of hierarchical phrase pair, in future we should investigate more features on the relationship between span pair and hierarchical phrase pair.",1,2010
P10-1037,we have a plan to incorporate our proposed methods to the annotation tool.,4,2010
P10-1037,we will use it to accelerate building of the large annotated corpus to improved our japanese parser.,2,2010
P10-1037,"it would be interesting to explore the use of partially labeled constituents in a sentence in another language, e.g., english, for active learning.",2,2010
P10-1040,"future work should explore methods for inducing phrase representations, as well as techniques for increasing in accuracy by using word representations in compound features.",1,2010
P10-1042,how the classification performance will be affected by variances of the generated sots is worthy of study.,5,2010
P10-1042,we plan to investigate on these issues in our future work.,6,2010
P10-1043,"in the future work, we will integrate the subjectivity summarization strategy (pang and lee, 2004) to help discard noisy objective sentences.",1,2010
P10-1043,"another interesting and practical idea is to integrate active learning (settles, 2009), another popular but principally different kind of semi-supervised learning approach, with our two-view learning approach to build high-performance systems with the least labeled data.",1,2010
P10-1043,"moreover, we need to consider the cases when both x and y appear in a sentence.",5,2010
P10-1044,"in the future, we wish to apply our model to automatically discover new inference rules and paraphrases.",4,2010
P10-1047,this is an avenue of research that we intend to look at in the very near future.,6,2010
P10-1050,"we hope this work opens a new perspective on decoding algorithms for a wide range of nlp problems, not just sequence labeling.",1,2010
P10-1052,"in the future, we intend to study how sparsity can be used to speed-up training in the face of more complex dependency patterns (such as higher-order crfs or hierarchical dependency structures (rozenknop, 2002; finkel et al., 2008).",1,2010
P10-1052,"from a performance point of view, it might also be interesting to combine the use of large-scale feature sets with other recent improvements such as the use of semi-supervised learning techniques (suzuki and isozaki, 2008) or variable-length dependencies (qian , 2009).",1,2010
P10-1057,our future goal is to combine summarization and bibliometric techniques towards building automatic surveys that employ context information as an important part of the generated surveys.,1,2010
P10-1058,an obvious next step is to examine how the model generalizes to other domains and text genres.,4,2010
P10-1058,"we would also like to generalize the model to arbitrary rewrite operations, as our results indicate that compression rates are likely to improve with more sophisticated paraphrasing.",4,2010
P10-1058,"although coherence is not so much of an issue for highlights, it certainly plays a role when generating standard summaries.",1,2010
P10-1058,Future extensions are many and varied.,6,2010
P10-1060,"we would also like to extend potential opinion targets to include multi-word phrases (nps and vps), in addition to individual words.",1,2010
P10-1060,"finally, we do not identify polarity yet: this can be partially inherited from the initial lexicon and refined automatically via bootstrapping.",1,2010
P10-1060,we also want to look at more complex syntactic patterns: choi et al.(2009) report that many errors are due to exclusive use of unigrams.,1,2010
P10-1060,existing sentenceor phrase-level (trained) sentiment classifiers can also be used easily: when collecting/counting targets we can weigh them by prior score provided by such classifiers.,1,2010
P10-1060,"as to future work, we intend to combine our method with known methods for topic-specific lexicon expansion (our method is rather concerned with lexicon “restriction”).",1,2010
P10-1061,"for future work, we aim extend this work to constructing a multilingual sentiment analysis system and evaluate it with multilingual datasets such as product reviews collected from different countries.",2,2010
P10-1061,"we also plan to resolve the lexiconbased classifiers' classification bias towards subjective meanings with a list of objective words (esuli and sebastiani, 2006) and their multilingual expansion (kim , 2009), and evaluate the multilanguage-comparability of systems constructed with resources from different sources.",1,2010
P10-1062,"future work in this direction involve detecting particular error types such as incorrect positions, inappropriate/unnecessary words (elliott, 2006) and automatically correcting errors.",6,2010
P10-1062,"therefore our approach can be used for other machine translation systems, such as rule-based or example-based system, which generally do not produce n-best lists.",4,2010
P10-1064,"furthermore, we want to experiment and improve on the adaptability of this method, as the current experiment is on a specific domain and language pair.",1,2010
P10-1066,it is also possible to introduce certain rules or constraints to selectively form template slots rather than treating all words labeled with d as template slots.,1,2010
P10-1066,"first, we can possibly apply linguistic knowledge to improve the quality of sentence patterns.",1,2010
P10-1066,we plan to study how to use linguistic knowledge to guide the construction of sentence patterns and make them more meaningful.,1,2010
P10-1066,"second, we have not quantitatively evaluated the quality of the template slots, because our judgment is only at the whole sentence pattern level.",3,2010
P10-1066,we plan to get more human judges and more rigorously judge the relevance and usefulness of both the sentence patterns and the template slots.,3,2010
P10-1067,"in the future, we would like to improve extraction pattern application and mine rare extraction patterns.",1,2010
P10-1067,we also plan to develop methods to summarize answers pooled by a given comparator pair.,1,2010
P10-1067,how to identify comparator aliases such as lv and louis vuitton and how to separate ambiguous entities such “paris vs. london” as location and “paris vs. nicole” as celebrity are all interesting research topics.,5,2010
P10-1071,the latest developments in the lexical acquisition technology will in the near future enable fully automated corpus based processing of metaphor.,1,2010
P10-1073,"in future, we plan to model entity relations which constitute 24% of implicit entity no relation cases, thus to improve the accuracy of relation detection.",1,2010
P10-1074,"future directions for this work include automatically learning the variances, qm and q* in the hierarchical model, so that the degree of information sharing between the models is optimized based on the training data available.",1,2010
P10-1074,"we are also interested in ways to modify the objective function to place more emphasis on learning a good joint model, instead of equally weighting the learning of the joint and single-task models.",1,2010
P10-1075,"given that the rule representations and comparison methods use both pos and dependency information, a next step in evaluating and improving the methods is to examine automatically pos tagged data.",3,2010
P10-1075,"furthermore, although we have indicated that differences in accuracy can be linked to differences in the granularity and particular distinctions of the annotation scheme, it is still an open question as to which methods work best for which schemes and for which constructions (e.g., coordination).",5,2010
P10-1077,"in the future, an important task would be the refinement or unsupervised generation of new hierarchies, using information theoretic or data-driven approaches.",1,2010
P10-1077,"with regard to algorithms, we are also interested in other formulations for structural svms and their large-scale implementation as well as the combination of different distance measures, for example in ensemble learning.",1,2010
P10-1079,"it would also be interesting to test the impact of another lexical language model, learned on nonsms sentences.",3,2010
P10-1080,"one open question that we would like to investigate in the future is whether l2p conversion accuracy could be improved by treating letter-phoneme alignment links as latent variables, instead of committing to a single best alignment.",5,2010
P10-1081,"in addition, extending the approach to cross-document information, following (ji and grishman 2008), may be able to further improve performance.",1,2010
P10-1082,the participants will also be interviewed in order to find out which version of the system is more pleasant to use.,1,2010
P10-1082,the strategies will then be evaluated through user tests where the participants will compare an application with these strategies with an application without them.,3,2010
P10-1082,next we intend to implement strategies for interruption and resumption in the dico dialogue system.,1,2010
P10-1086,"in our future work, we may try this algorithm on syntax-based mt systems and phrase-based mt systems with different context features.",4,2010
P10-1087,"this can be extended to include mappings from multiple languages to wordnet synsets, with the hope that the weights and link structure will then allow the algorithm to make the final disambiguation decision.",1,2010
P10-1087,additional scenarios include dealing with co-reference on the linked data web or mappings between thesauri.,1,2010
P10-1087,"in future work, we would like to investigate how our algorithm behaves in extended settings, e.g. we can use heuristics to connect isolated, unconnected articles to likely candidates in other wikipedias using weighted edges.",1,2010
P10-1088,"also, we would like to test with more languages, increase the amount of data we can gather, and investigate stopping criteria further.",2,2010
P10-1088,"also, we would like to investigate increasing the efficiency of the selection algorithm by addressing issues such as the one raised by the 12 may example presented earlier.",1,2010
P10-1090,one possible solution is to do the probability rescaling off-line before kernel calculation.,1,2010
P10-1090,"in addition, as suggested by one reviewer, we may consider rescaling the probabilities (exponentiating them by a constant value) that are used to compute the fractional counts.",1,2010
P10-1090,"in the future, we would like to verify the forest kernel in more nlp applications.",3,2010
P10-1090,this would be a very interesting research topic of our future work.,6,2010
P10-1093,"thus, another avenue of further research is to generalize from the linear approach.",1,2010
P10-1093,"instead, we might need some in-between application of simple nonlinear functions in the spirit of quantum-collapsing of a ""superposed"" mental state (such as the winner takes it all, survival of the top-k vector entries, and so forth).",4,2010
P10-1094,"in future work, we will manually translate english reference summaries into chinese reference summaries, and then adopt the rouge metrics to perform automatic evaluation of the extracted chinese summaries by comparing them with the chinese reference summaries.",3,2010
P10-1096,"the future for this work would involve natural extensions such as mixing over the space of word alignments; this would allow application to mt-like tasks where flexible word reordering is allowed, such as abstractive sentence compression and paraphrasing.",1,2010
P10-1097,"another direction for further study will be the generalization of our model to larger syntactic contexts, including more than only the direct neighbors in the dependency graph, ultimately incorporating context information from the whole sentence in a recursive fashion.acknowledgments.",1,2010
P10-1099,"there are several potential avenues for further progress towards this goal, including the development of more portable srl pipeline systems, and especially parsers.",1,2010
P10-1099,developing techniques that can incrementally adapt to new domains without the computational expense of retraining the crf model every time would help make open-domain srl more practical.,1,2010
P10-1100,we will restrict the user input to lexicon words to avoid manual orthography correction.,6,2010
P10-1100,"as far as the data collection is concerned, we plan to replace the web form with a browser game, following the example of von ahn and dabbish (2008).",2,2010
P10-1100,"to realize this goal, we are going to automatize several processing steps that were done manually for the current study.",6,2010
P10-1100,"further, we will implement some heuristics to filter unusable instances by matching them with the remaining data.",1,2010
P10-1101,"perhaps with the help of semantic feedback the system can automatically improve predicate identification, which in turn allows it to correct the observed intransitive sentence error.",1,2010
P10-1104,"future work should consider the dynamics in more detail, develop more complex models (for example, by relaxing the infinite population assumption, allowing for stochastic dynamics), and quantitatively compare model predictions and observed dynamics.",1,2010
P10-1107,our model currently operates purely on the vocabulary level and thus fails to take this contextual information into account.,3,2010
P10-1107,our model fails to take into account the known frequency of hebrew words and morphemes.,1,2010
P10-1107,we hope to address several issues in future work.,6,2010
P10-1110,For future work we plan to extend it to constituency parsing.,1,2010
P10-1114,future work includes the extension of cl-scl towards a general approach for cross-lingual adaptation of natural language processing technology.,4,2010
P10-1115,it should be very interesting to explore how to assign weights to the edges and study whether weighted graphs can further improve performance.,5,2010
P10-1115,"second, it would also be interesting to further extend pclsa to accommodate discovering topics in each language that aren't well-aligned with other languages.",1,2010
P10-1115,our work opens up some interesting future research directions to further explore.,6,2010
P10-1115,"first, in this paper, we have only experimented with uniform weighting of edge in the bilingual graph.",3,2010
P10-1118,"still, future work will be required to optimize a cost model for eventual application where even more accurate cost models may be required.",1,2010
P10-1120,"computationally, the inclusion of markov logic allowed the discourse module to compute well-formed coreference chains, and opens two avenues of future research.",1,2010
P10-1120,"second, we would like to make greater use of the logical elements by applying it to problems where inference is necessary, such as resolving bridging anaphora (haviland and clark, 1974).",4,2010
P10-1120,"first, it ought to be possible to make the probabilistic logic more naturally incremental, rather than re-running from scratch at each word.",1,2010
P10-1123,the next challenge is to translate the theoretical gain into practical benefit.,5,2010
P10-1123,"our analysis demonstrates that improvements are necessary both on the side of discourse reference resolution systems, which need to cover more types of references, as well as a better integration of discourse information in entailment systems, even for those relations which are within the scope of available resolvers.",1,2010
P10-1124,"this paper used distributional similarity, but other sources of information are likely to improve performance further.",2,2010
P10-1124,"this will introduce a challenge to our current optimization algorithm due to complexity issues, and will require careful handling of predicate ambiguity.",1,2010
P10-1124,"in future work, we would like to learn general entailment graphs over a large number of nodes.",1,2010
P10-1124,"additionally, we will investigate novel features for the entailment classifier.",1,2010
P10-1125,"firstly, we will further improve the performance of our method by adopting the nontextual features.",1,2010
P10-1125,"secondly, more research will be taken to put forward other architectures of the deep networks for qa detection.",1,2010
P10-1126,"we could, however, improve grammaticality more globally by generating a well-formed tree (or dependency graph).",1,2010
P10-1126,"rather than adopting a two-stage approach, where the image processing and caption generation are carried out sequentially, a more general model should integrate the two steps in a unified framework.",1,2010
P10-1126,we also believe that our approach would benefit from more detailed linguistic and non-linguistic information.,1,2010
P10-1126,"indeed, an avenue for future work would be to define a phrase-based model for both image annotation and caption generation.",1,2010
P10-1127,one is to explore how dependency patterns could be used to produce generative summaries and/or perform sentence trimming.,4,2010
P10-1127,"finally, we also plan to analyze automated ways for learning information structures (e.g. what is the flow of facts to describe a location) from existing image descriptions to produce better summaries.",1,2010
P10-1127,evaluation should be extended to investigate the utility of the automatically generated image descriptions for image retrieval.,3,2010
P10-1127,"another is to investigate how dependency patterns might be automatically clustered into groups expressing similar or related facts, rather than relying on manual categorization of dependency patterns into categories such as type, year, etc. as was done here.",1,2010
P10-1127,there are a number of avenues to pursue in future work.,6,2010
P10-1128,"second, representing distractors in a reference resolution model is also a key.",1,2010
P10-1128,"finally, more investigation is needed for considering other extra-linguistic information, such as eye-gaze, for exploring what kinds of information is critical to recognizing reference in dialogue.",2,2010
P10-1128,"in order to enhance this kind of reference resolution, there are several possible future directions.",1,2010
P10-1128,"first, in the current problem setting, we exclude zero-anaphora (i.e.omitted expressions refer to either an expression in the previous utterances or an object on a display deictically).",2,2010
P10-1128,"however, zero-anaphora is essential for precise modeling and recognition of reference because it is also directly related with the recency of referents, either textually or situationally.",1,2010
P10-1129,an interesting avenue of future work is to explore an alternative approach which learns these phenomena by combining linguistic information with knowledge gleaned from an automatically induced environment model.,1,2010
P10-1130,a logical next step would be to explore the connection between syntax and mark-up for genres other than a news-style blog and for languages other than english.,2,2010
P10-1132,in future work we intend to analyze the output of such algorithms in order to improve pos tag sets.,1,2010
P10-1133,"hence a promising direction would be to use our approach in combination with wikipedia data and with additional manually created attribute rich sources such as web tables, to achieve the best possible performance and coverage.",2,2010
P10-1133,we would also like to explore the incorporation of approximate discovered numerical attribute data into existing nlp tasks such as noun compound classification and textual entailment.,1,2010
P10-1134,"in the near future, we aim to apply the output of our classifiers to the task of automated taxonomy building, and to test the wcl approach on other information extraction tasks, like hypernym extraction from generic sentence fragments, as in snow (2004).",4,2010
P10-1136,"in the future, it would be useful to explore other approaches to automatic lexicon discovery to improve the quality or to increase the coverage of both ih and im lexicons, and to systematically evaluate their impact on query understanding performance.",1,2010
P10-1137,"as part of future work, we plan to vary the model interpolation parameters dynamically to improve the performance in case of multiple assisting languages.",1,2010
P10-1139,"it would be interesting to study opinion holders, e.g. its seniority, for opinion retrieval.",1,2010
P10-1141,in the future we plan to test the proposed weighting functions in other domains such as topic classification and additionally extend the approach to accommodate multi-class classification.,4,2010
P10-1145,"in the future, we will do more experiments on rule coverage to compare the constituency-to constituency model with our model.",3,2010
P10-1145,"furthermore, we will replace 1-best dependency trees on the target side with dependency forests to further increase the rule coverage.",1,2010
P10-1147,"in future work, we look forward to developing extraction set models for richer formalisms, including hierarchical grammars.",1,2010
P10-1148,"in addition to refinements of our work, our next step is to develop a method for representing and extracting actual experiences from experience-revealing sentences.",1,2010
P10-1148,"furthermore, considering that only 13% of the blog data we processed contain experiences, an interesting extension is to apply the methodology to extract other types of knowledge such as facts, which are not necessarily experiences.",4,2010
P10-1148,"in order to increase the coverage even further and reduce the errors in lexicon construction, i.e., verb classification, caused by data sparseness, we need to devise a different method, perhaps using domain specific resources.",1,2010
P10-1149,"topics for future work include the incorporation of other kinds of semantic constraint for improved class-instance acquisition, further investigation into per-node sparsity constraints in graph-based ssl, and moving beyond bipartite graph constructions.",1,2010
P10-1150,"we plan to use the algorithm described in this paper to learn the selectional restrictions of numerous other relations, in order to build a rich knowledge repository",1,2010
P10-1153,"the modified failure detection concept suggests several directions for future work, including evaluation of the new encodings in the context of a large-scale hpsg parser; incorporation of further developments in constraint solvers; and the possibility of approximate encodings that would permit one-sided errors as in traditional bloom filtering.",1,2010
P10-1154,"however, since wordnet++ is part of a multilingual semantic network (navigli and ponzetto, 2010), we plan to explore the impact of this knowledge in a multilingual setting.",2,2010
P10-1154,"moreover, while the mapping has been used to enrich wordnet with a large amount of semantic edges, the method can be reversed and applied to the encyclopedic resource itself, that is wikipedia, to perform disambiguation with the corresponding sense inventory (cf.the task of wikification proposed by mihalcea and csomai (2007) and milne and witten (2008b)).",4,2010
P10-1155,"as future work, we would like to test our work on the environment domain data which was released as part of the semeval 2010 shared task on “allwords word sense disambiguation on a specific domain”.",3,2010
P10-1156,adding semcor examples to transcont should have a positive impact on performance.,2,2010
P10-1156,also adding more languages as illustrated by the dr02 work should also yield much better performance.,2,2010
P10-1157,"nevertheless, future work should investigate whether syntactic information can improve performance in more complex domains.",5,2010
P10-1159,"an interesting topic for future work, for instance, is to expand our notion of context by taking visual and discourse salience into account when generating res.",1,2010
P10-1159,"in addition, we plan to experiment with assigning costs to planning operators in a metric planning problem (hoffmann, 2002) in order to model the cognitive cost of an re (krahmer et al., 2003) and compute minimal-cost instruction sequences.",1,2010
P10-2001,"in future work, we plan to apply this method with paraphrases derived from a massive corpus such as the web corpus and apply this method to a hierarchical phrase based smt.",4,2010
P10-2002,a challenge might exist when running the me training toolkit over a big size of training instances from the large scale training data.,2,2010
P10-2002,"in the future work, we will explore more useful features and test our method over the large scale training corpus.",2,2010
P10-2003,"in addition, how to further improve the reordering model by distinguishing the derivations with different probabilities will become another study emphasis in further research.",1,2010
P10-2004,the idea of selecting syntactic constraints is compatible with the idea of using constraints softly; we plan to combine the two ideas and obtain further improvements in future work.,1,2010
P10-2008,"finally, we would like to compare the performance of our method to other state-of-the-art approaches to authorship prediction.",3,2010
P10-2008,an interesting extension of our current approach is to consider discriminative training of pcfgs for each author.,1,2010
P10-2009,"therefore, designing better prompts would be the key factor in improving learning and user satisfaction.",1,2010
P10-2009,one way to improve the help messages may be to have the system indicate more clearly when user terminology is a problem.,1,2010
P10-2012,"in machine translation, evaluating the fluency of system output is crucial, and a model that predicts processing difficulty could be used for this, or to guide the choice between alternative translations, and maybe even to inform human post-editing.",3,2010
P10-2013,"because the masc is an open resource that the community can continually enhance with additional annotations and modifications, the project serves as a model for community-wide resource development in the future.",1,2010
P10-2013,"past experience with corpora such as the wall street journal shows that the community is eager to annotate available language data, and we anticipate even greater interest in masc, which includes language data covering a range of genres that no existing resource provides.",2,2010
P10-2014,"in future work, we will explore a method of increasing the recall of error correction by constructing a wide-coverage stsg.",1,2010
P10-2015,"in future work with mncd as an mt evaluation measure, we are planning to evaluate synonym dictionaries for other languages than english.",2,2010
P10-2015,the synonym module for english does not distinguish between different senses of words.,1,2010
P10-2017,we will experiment with those methods to determine the tradeoff of runtime and accuracy for this task.,1,2010
P10-2017,"another area of future work is to move beyond bag-of-words context: it is known from wsd that syntactic and bag-of-words contexts provide complementary information (florian , 2002; szpektor , 2008), and we hope that they can be integrated in a more sophisticated exemplar model.",1,2010
P10-2017,"finally, we will to explore task-based evaluations.",3,2010
P10-2018,a further research direction we are investigating is exploitation of unlabeled texts.,2,2010
P10-2019,it is still an open question what kinds of syntactic information is most important for chinese srl.,5,2010
P10-2019,we suggest that our attempt at semantics-driven shallow parsing is a possible way to better exploit this problem.,1,2010
P10-2020,future research into association measures that are not based on the independence assumption will also include considering different em variants and other automatically learnable models besides the amms used in this paper.,1,2010
P10-2020,a possible obstacle in the adoption of amms in collocation extraction is that we have not provided any heuristic for setting the number of classes for the amms.,5,2010
P10-2020,we hope to be able to look into this question in future research.,6,2010
P10-2021,"yet another direction of this research is to investigate if our methodology is applicable to other types of collocations, such as an and pn in addition to vn dealt with in this paper.",1,2010
P10-2021,we will conduct a user study to investigate whether our system would improve a learner's writing in a real setting.,3,2010
P10-2025,future work will involve examining the proposed method for different language pairs such as english-chinese and english japanese and evaluating the impact of our proposed method on smt performance.,2,2010
P10-2025,we will also apply our proposed method to a larger data sets of multiple domains since we can expect a further improvement in word alignment accuracy if we use more bilingual sentences and more monolingual knowledge.,2,2010
P10-2026,"besides, the quality of the parser is another effect for this method.",1,2010
P10-2026,"in the future, we  are plan to exploit some discriminative approach to train parameters of this feature, such as em algorithm (hasan et al., 2008) or maximum entropy (he et al., 2008).",1,2010
P10-2026,"in the future, we we are plan to exploit some discriminative approach to train parameters of this feature, such as em algorithm (hasan , 2008) or maximum entropy (he , 2008).",1,2010
P10-2032,future research aims at the investigation of improved cms to be integrated in our imt system.,1,2010
P10-2033,"in the future, we plan to improve robustness to parsing errors by using not just one, but multiple subject boundary hypotheses.",1,2010
P10-2033,we will also investigate the integration of vs reordering in smt decoding.,1,2010
P10-2036,"in the future, we would also like to try applying similar constraints to the more complex task of joint induction of pos tags and dependency parses.",4,2010
P10-2040,we believe that the svd2 algorithm presented here could provide a launching pad for an approach that would successfully address the disambiguation challenge.,1,2010
P10-2040,it would do so by allowing a gradual and carefully controlled amount of ambiguity into an initially non disambiguating model.,1,2010
P10-2042,"while it is difficult to extend the local gibbs sampler to the case where the tree is not observed, the dynamic program for our blocked sampler can be easily used for unsupervised inference by omitting the tree matching constraints.",1,2010
P10-2042,a particularly interesting avenue for further research is to employ our blocked sampler for unsupervised grammar induction.,1,2010
P10-2043,"possibilities for future work include the pairing of this method with algorithms for dynamic clustering, as well as exploring algorithms for different distances (e.g., l2) and estimators (e.g., asymmetric estimators (dong et al., 2009)).",1,2010
P10-2044,"in future work we will extend the types of questions that we consider, and also allow for multiword answers.",1,2010
P10-2045,"in future work we plan to investigate combining framenet and wordnet rule-sets in a transitive manner, instead of their simple union.",1,2010
P10-2047,"another direction is exploring words as members of semantic fields 鈥 while word use might be insufficiently consistent within a perspective, selection of a semantic domain might show better consistency.",1,2010
P10-2047,"In future work, we plan to experiment with additional features.",1,2010
P10-2048,incorporating the confidence from the translator may further improve the performance.,1,2010
P10-2048,"also, in the current work, we select the pivot features by simple ranking with mutual information, which only considers the distribution information.",1,2010
P10-2048,as future research we believe a promising avenue of exploration is to construct a probabilistic version of the scl approach which could offer a more explicit model of the relations between the two domains and the relations between the search engine results and the model parameters.,1,2010
P10-2051,"another issue of our two-step crf method is that the training complexity increases quadratically according to the size of the label set, and how to reduce the training time needs more research.",5,2010
P10-2051,in future work we are planning to combine our system with multilingual systems.,2,2010
P10-2051,also we want to make use of acoustic information in machine transliteration.,1,2010
P10-2051,we are currently investigating discriminative training as a method to further improve the jscm.,1,2010
P10-2053,one line of future research is to investigate other types of domain-independent frames that exhibit useful regularities.,1,2010
P10-2056,we plan to investigate this problem in the future since the choice of hyperparameters has a strong impact on the performance of the model.,1,2010
P10-2057,"in future work, we plan to (a) investigate cascaded learning methods (sutton , 2007) to improve the detection of ddas further by using detected decision regions and (b) extend hgms beyond three levels in order to integrate useful semantic information such as topic structure.",1,2010
P10-2060,we also plan to model the order of sentences globally.,1,2010
P10-2060,one future work is to include important information other than sentiments in the summaries.,1,2010
P10-2061,"based on the rating results we will draw polarity profiles in order to see where, within customer reviews, polarity is best manifested and whether there are other 鈥渃andidates鈥 sentences that would serve as useful polarity indicators.",1,2010
P10-2061,the profiles will be used as a feature in our computational analysis.,1,2010
P10-2063,"we also will be using this system as a preprocessing step for a parser, as part of a complete arabic nlp pipeline.",4,2010
P10-2063,obvious future work starts with the need to include determiner information in the pos tags and the important noun/adj distinction.,1,2010
P10-2063,we will be implementing and comparing these alternatives.,3,2010
P10-2065,"we would like to further investigate the role of parsing in error detection by looking at other error types and other text types, e.g. machine translation output.",3,2010
P10-2065,"for our immediate future work, we plan to carry out the esl evaluation on a larger test set to better gauge the usefulness of a parser in this context, to carry out a detailed error analysis to understand why certain parse features are effective and to explore a larger set of features.",3,2010
P10-2066,"in addition, we also plan to combine some syntactic patterns (etzioni et al.2005; sarmento et al.2007) to further improve the results.",1,2010
P10-2066,"in our future work, we plan to experiment with various other pu learning methods (liu 2003; lee and liu, 2003; li 2007; elkan and noto, 2008) on this entity set expansion task, as well as other tasks that were tackled using distributional similarity.",3,2010
P10-2067,in future we wish to work with word alignments for other language pairs like arabic and english.,2,2010
P10-2067,we have tested out the feasibility of obtaining human word alignment data using amazon mechanical turk and plan to obtain more data reduce the cost of annotation.,2,2010
P10-2070,in future work we plan on working towards improving the quality of our sentence reconstruction step in order to produce better and more readable sentences.,1,2010
P10-3002,"besides the implementation, future work will focus on refining the theoretical foundations of relational pomdps for dialogue (including how to specify the transition, observation and reward functions in such a relational framework), as well as investigating the use of reinforcement learning for policy optimisation based on simulated data.",1,2010
P10-3005,we intend to compare our system to other available work in the future.,3,2010
P10-3005,our future work will include a further examination of the merits of its application for knowledge-sparse languages.,4,2010
P10-3006,other future work includes optimizing the segmentation of both sides of the corpus and experimenting with other language pairs.,2,2010
P10-3006,one avenue for future work is to relax some of the several independence assumptions made in the generative model.,1,2010
P10-3006,"for example, independence of consecutive morphs could be relaxed by an hmm model for transitions between morphs (creutz and lagus, 2007).",1,2010
P10-3007,"furthermore, we plan to integrate the proposed interface within an computer-based interactive platform for speech therapy.",4,2010
P10-3007,a formal usability study is needed in order to establish the degree of utility and satisfaction with the interface.,6,2010
P10-3008,"in addition, a new affective lexicon could be automatically detected based on learning correlation of the blog text and the moods tagged.",6,2010
P10-3008,another direction is to integrate negation information to learn more cohesive association in affect scores between moods and affective words.,1,2010
P10-3008,future work will take into account the temporal dimension to trace changes in mood patterns over time in blogosphere.,1,2010
P10-3009,we can discover how words are organized and connected within this network.,6,2010
P10-3009,"in addition, we want to build a network of chinese word association.",1,2010
P10-3009,we plan to remove noisy words in the future.,2,2010
P10-3009,"fourthly, how to deal with ambiguous query word is also left as our future work.",5,2010
P10-3009,"furthermore, we want to take the advantage of learning to rank literature (liu, 2009) to further improve the performance of related word retrieval task.",1,2010
P10-3009,another important issue is how to build a complete and accurate ground truth for related word retrieval task.,5,2010
P10-3009,and this word association network will be quite useful for foreigners to learn chinese.,4,2010
P10-3010,another possibility is to extend the feature set in a more critical way than what is done now.,2,2010
P10-3010,another comparison we would like to do is with linear svms.,3,2010
P10-3010,for instance the combination of a pos-tag and cpostag for a given word is now included.,1,2010
P10-3010,a possible solution is to use kernels with confidence-weighted classification in the same way they are used with the svms.,1,2010
P10-3010,we will also try to do feature selection on a more general level as this can boost accuracy a lot.,1,2010
P10-3011,we hope that future research is able to use this feature to provide more specific individual frames.,4,2010
P10-3011,"because cyc is consistently changing and growing, an approach that uses cyc relationships will be able to improve as the knowledge base improves its coverage.",1,2010
P10-3013,"in order to measure the impact of the frame clusters for the sp acquisition, we plan to run the system for sp acquisition without performing the clustering step, thus defining all constructions as singleton sets containing one frame each.",1,2010
P10-3013,"for this, a larger automatically parsed corpus will be necessary.",2,2010
P10-3014,"also, the system will be expanded to identify arguments using a tree distance algorithm.",1,2010
P10-3014,"future work will focus on improving the performance of the system by: a) trying to extend the sub-trees which will contain more contextual information, b) using different approaches to label semantic relations discussed in section 5.",1,2010
P10-3015,we are also exploring the possibility of including some semantic information about the words while defining weights.,1,2010
P10-3015,the sandhi with white spaces also needs to be handled.,2,2010
P10-3016,"we are currently planning to port this setting to co-training, another bootstrapping algorithm.",4,2010
P10-3016,another direction can be adapting bootstrapping parameters to fit the semantic role labeling complexity.,1,2010
P10-3016,one direction for future work can be adapting the architecture of the srl system to better match with the bootstrapping process.,1,2010
P10-3017,"also, we are going to analyze the influence of single features on the classification and determining optimal feature sets, as well as the question of including patterns in the feature set.",1,2010
P10-3017,in future work we will integrate word sense disambiguation as well as information about predicate-argument structure.,1,2010
P10-4002,"further training pipelines are under development, including minimum risk training using a linearly decomposable approximation of bleu (li and eisner, 2009), and mira training (chiang et al., 2009).",1,2010
P10-4002,we are also improving support for parallel training using hadoop (an open-source implementation of mapreduce).,1,2010
P10-4003,"we also plan to annotate the data we collected for evidence of misunderstandings, i.e., situations where the system arrived at an incorrect interpretation of a student utterance and took action on it.",2,2010
P10-4003,"in dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment.",5,2010
P10-4003,the annotated data will be used to evaluate the accuracy of existing paraphrasing and textual entailment approaches and to investigate how to combine such algorithms with the current deep linguistic analysis to improve system robustness.,1,2010
P10-4003,we are also planning experiments that will allow us to evaluate the effectiveness of individual strategies implemented in the system by comparing system versions using different tutoring policies.,3,2010
P10-4004,"in future work, we plan to adapt the tool so that it can be used with wordnets for other languages as well.",1,2010
P10-4005,"for that purpose, an integration of the escidoc research environment3 into weblicht is planned.",1,2010
P10-4005,"in the future, an online workspace has to be implemented so that annotated text corpora created with weblicht can also be stored in and retrieved from the net.",4,2010
P10-4006,"future work will be focused on providing support for syntactic features, including dependency parsing as described by (pado and lapata, 2007), reference implementations of algorithms that use this information, non-linear dimensionality reduction techniques, and more advanced clustering algorithms.",1,2010
P10-4007,on the one hand we will focus on single components like hybrid parsing of input utterances and dialog interpretation in terms of precision and recall.,1,2010
P10-4007,on the other hand an evaluation of the two different scenarios regarding the usability are planned in experiments with end users.,3,2010
P10-4007,moreover we will integrate some opinion mining and sentiment analysis functionality which can be helpful to better detect and understand the users' preferences in the furniture sales agents scenario.,1,2010
P10-4010,"our preliminary work on risk maps can be put on a more theoretical footing (hunter, 2000).",6,2010
P10-4010,"extracted negative and also positive risks can be used in many applications, ranging from e-mail alerts to determinating credit ratings.",4,2010
P11-1002,"for future work, it will be interesting to see if we can exploit both parallel and non-parallel data to improve on both.",2,2011
P11-1003,"also, it is interesting to automatically learn a word set for realigning.",4,2011
P11-1003,it will be valuable to investigate the feasibility to re-align all the target words to source tree fragments.,3,2011
P11-1003,"finally, we intend to extend the proposed approach to tree-to-tree translation frameworks by this idea comes from one reviewer, we express our thankfulness here.re-aligning subtree pairs (liu , 2009; chiang, 2010) and consistency-to-dependency frameworks by re-aligning consistency-tree-to-dependency-tree pairs (mi and liu, 2010) in order to tackle the rulesparseness problem.",4,2011
P11-1004,in future work we hope to explore the utility of phrases with productive morpheme boundaries and explore why they are not used more pervasively in the decoder.,5,2011
P11-1004,evaluation measures for morphologically complex languages and tuning to those measures are also important future work directions.,3,2011
P11-1004,"also, we would like to explore a non-pipelined approach to morphological preand post-processing so that a globally trained model could be used to remove the target side morphemes that would improve the translation model and then predict those morphemes in the target language.",1,2011
P11-1005,we also plan to improve the filtering heuristics and to explore further ways of detecting human coder errors.,1,2011
P11-1005,future work should focus on finding optimal parameter settings to make the filtering method more robust even for noisier data sets.,1,2011
P11-1005,"finally, we plan to test our method in a real-world annotation scenario.",3,2011
P11-1006,"as future work, we plan to add contextual features into the model and apply our method to other data sets in other tasks.",4,2011
P11-1007,"the primary area of the future work is to apply our method to structured prediction problems in nlp, such as syntactic parsing or semantic role labeling, where construction of auxiliary tasks proved problematic.",4,2011
P11-1007,"another direction is to favor domain invariability not only of the expectations of individual variables but rather those of constraint functions involving latent variables, features and labels.",1,2011
P11-1011,"we are encouraged by the success of our joint query annotation technique, and intend to pursue the investigation of its utility for ir applications.",4,2011
P11-1011,"in the future, we intend to research the use of joint query annotations for additional ir tasks, e.g., for constructing better query formulations for ranking algorithms.",4,2011
P11-1013,"second, it might be interesting to study the effect of introducing a tradeoff parameter to balance the effect of original and new features.",1,2011
P11-1013,"first, polarity-bearing topics generated by the jst model were simply added into the original feature space of documents, it is worth investigating attaching different weight to each topic maybe in proportional to the posterior probability of sentiment label and topic given a word estimated by the jst model.",1,2011
P11-1016,we are also interested in exploring relations between twitter accounts for classifying the sentiments of the tweets published by them.,1,2011
P11-1016,"as mentioned in section 4.1, in future we would like to explore the relations between a target and any of its extended targets.",3,2011
P11-1019,"we plan to experiment with better error detection techniques, since the overall error-rate of a script is one of the most discriminant features.",1,2011
P11-1022,"first, we plan to apply confidence estimation to perform a second-pass constraint decoding.",1,2011
P11-1022,"moreover, we also intend to perform a user study on our visualization prototype to see if it increases the productivity of post-editors.",3,2011
P11-1025,"for future work, there are other interesting decipherment tasks where our method can be applied.",4,2011
P11-1025,"one challenge is to crack the unsolved zodiac-340 cipher, which presents a much harder problem than the solved version.",5,2011
P11-1030,as future work we would like to explore the use of lowbow representations for profile-based aa and related tasks.,4,2011
P11-1030,"also, we would like to develop model selection strategies for learning what combination of hyperparameters works better for modeling each author.",1,2011
P11-1031,"many interesting research questions still remain pertaining to the best way to select and partition the training corpora, align adult and intermediate lsa models, correlate the results with real school grade levels, as well as other free parameters in the model.",5,2011
P11-1032,"many additional approaches to detecting deceptive opinion spam are also possible, and a focus on approaches with high deceptive precision might be useful for production environments.",1,2011
P11-1032,"possible directions for future work include an extended evaluation of the methods proposed in this work to both negative opinions, as well as opinions coming from other domains.",3,2011
P11-1033,"in future work, we would like to apply the joint learning idea to other learning frameworks (e.g.",4,2011
P11-1033,"svms), and to extend the proposed model to handle word-level parallel information, e.g. bilingual dictionaries or word alignment information.",4,2011
P11-1033,another issue is to investigate how to improve multilingual sentiment analysis by exploiting comparable corpora.,2,2011
P11-1034,"second, we will evaluate using other extraction units, such as applying preprocessing to remove disfluencies and concatenate incomplete sentence segments together.",3,2011
P11-1034,"in addition, it would be interesting to test our system on speech recognition output and automatically generated da boundaries to see how robust it is.",3,2011
P11-1034,"in future work, we will address some issues identified from our error analysis.",5,2011
P11-1034,"first, we will investigate ways to represent a sentence’s topic relevance.",1,2011
P11-1035,discovering and developing methods for issues which involve more than two disputants groups is a future work.,1,2011
P11-1037,"firstly, we hope to develop tweet normalization technology to make tweets friendlier to the ner task.",1,2011
P11-1037,"secondly, we are interested in integrating new entities from tweets or other channels into the gazetteers.",2,2011
P11-1037,"in future, we plan to further improve the performance of our method through two directions.",1,2011
P11-1038,"first, we plan to improve our ill-formed word detection classifier by introducing an oov word whitelist.",1,2011
P11-1038,"furthermore, we intend to alleviate noisy contexts with a bootstrapping approach, in which ill-formed words with high confidence and no ambiguity will be replaced by their standard forms, and fed into the normalization model as new training data.",1,2011
P11-1038,"in future work, we propose to pursue a number of directions.",6,2011
P11-1041,"in the future, we plan to generalize our approach so that it can be applied to the task of generating transliterations, and to combine data from distinct g2p dictionaries.",4,2011
P11-1041,"we would also like to apply our approach to web data; we have shown that it is possible to use noisy transliteration data, so it may be possible to leverage the noisy ad hoc pronunciation data as well.",4,2011
P11-1041,"finally, we plan to investigate earlier integration of such external information into the g2p process for single systems; while we noted that re-ranking provides a general approach applicable to any system that can generate n-best lists, there is a limit as to what re-ranking can do, as it relies on the correct output existing in the n-best list.",1,2011
P11-1045,"first, we will try combining our approach with constituent-level coarse-to fine pruning.",1,2011
P11-1045,we plan to explore a number of remaining questions in future work.,5,2011
P11-1046,whether np-hardness can be shown for unrestricted parsing strategies is an important question for future work.,5,2011
P11-1048,we also plan to revisit the idea of combined training.,1,2011
P11-1048,"in future work we plan to integrate the pos tagger, which is crucial to parsing accuracy (clark and curran, 2004b).",1,2011
P11-1053,"moreover, extending word clustering to phrase clustering (lin and wu, 2009) and pattern clustering (sun and grishman, 2010) is worth future investigation for relation extraction.",1,2011
P11-1053,"based on the experimental results, we plan to investigate additional ways to improve the performance of relation detection.",1,2011
P11-1056,there are probably many near misses when we apply our structure patterns on predicted mentions.,5,2011
P11-1056,relaxing this might potentially recover additional valid mention pairs and improve performance.,5,2011
P11-1056,"for instance, for both premodifier and possessive structures, we require that one mention completely includes the other.",5,2011
P11-1056,it will also be interesting to feedback the predictions of the structure patterns to the mention entity typing classifier and possibly retrain to obtain a better classifier.,1,2011
P11-1056,we could also try to learn classifiers to automatically identify and disambiguate between the different syntactico-semantic structures.,1,2011
P11-1062,this would require explicit modeling of predicate ambiguity and using approximation techniques when an optimal solution cannot be attained.,1,2011
P11-1062,"in future work, we aim to scale the algorithm further and learn entailment rules between untyped predicates.",1,2011
P11-1063,"by increasing the beam size and distortion limit of the baseline system, future work may examine whether a baseline system with comparable runtimes can achieve comparable translation quality.",1,2011
P11-1063,our future work seeks to incorporate largescale n-gram language models in conjunction with incremental syntactic language models.,1,2011
P11-1064,"for future work, we plan to refine hlen to use a more appropriate model of phrase length than the uniform distribution, particularly by attempting to bias against phrase pairs where one of the two phrases is much longer than the other.",1,2011
P11-1064,"in addition, we will test probabilities learned using the proposed model with an itg-based decoder.",3,2011
P11-1064,"we will also examine the applicability of the proposed model in the context of hierarchical phrases (chiang, 2007), or in alignment using syntactic structure (galley et al., 2006).",1,2011
P11-1066,"first, question structure should be considered, so it is necessary to combine the proposed approach with other question retrieval methods (e.g., (duan et al., 2008; wang et al., 2009; bunescu and huang, 2010)) to further improve the performance.",1,2011
P11-1066,"second, we will try to investigate the use of the proposed approach for other kinds of data set, such as categorized questions from forum sites and faq sites.",4,2011
P11-1067,"in the future, we suggest reporting ned along with the current measures.",1,2011
P11-1072,"we plan to further explore our new lexicons performance for other languages and tasks, such as oov spoken term detection.",4,2011
P11-1073,we will further experiment with syntactic complexity measures to balance construct richness and model simplicity.,1,2011
P11-1073,"furthermore, we can also experiment with additional types of machine learning models and tune parameters to derive scoring models with better performance.",1,2011
P11-1074,"in our future work, we will develop models with only within-word context, and thus allowing us to explore lattice rescoring, which we expect will yield more performance gain.",1,2011
P11-1075,"it would be interesting to model triggers as latent variables in the document coding process, in a manner similar to how latent subjective sentences have been used in document level sentiment analysis (yessenalina et al., 2010).",1,2011
P11-1075,this would allow us to employ a learned matching component that is trained to compliment our classification component.,1,2011
P11-1075,"in the future, we would like to augment our dictionary-based matching component with entity recognition technology.",1,2011
P11-1076,future work will concentrate on improving the quality of the answer alignments by training a model to directly output graph-to-graph alignments.,1,2011
P11-1077,"in the future we plan on using age and other metadata to improve results in larger tasks such as identifying opinion, persuasion and power by targeting our approach in those tasks to the identified age of the person.",1,2011
P11-1077,"another approach that we will experiment with is the use of ranking, regression, and/or clustering to create meaningful age groups.",1,2011
P11-1078,"in addition, we had promising early results for classification of author-recipient links with 200 to 500 words, so we plan to explore performance improvements for links of few words.",1,2011
P11-1078,"finally, we hope to investigate how spm and sna can enhance one another, and explore other lect classification problems for which the ground truth can be found.",1,2011
P11-1078,"in early, unpublished work, we had promising results with generative model-based approach to spm, and we plan to revisit it; language models are a natural fit for lect modeling.",1,2011
P11-1080,"although we demonstrate scalability to more than a million mentions, we plan to explore performance on datasets in the billions.",2,2011
P11-1080,"our work enables cross document coreference on very large corpora, and we would like to explore the downstream applications that can benefit from it.",4,2011
P11-1080,"since our approach supports parameter estimation, we expect significant accuracy gains with additional features and supervised data.",1,2011
P11-1080,we also plan to examine inference on complex coreference models (such as with entity-wide factors).,1,2011
P11-1080,Another possible avenue for future work is that of learning the factors.,1,2011
P11-1081,"finally, we would like to test our model with english constructions which closely resemble zero anaphora.",3,2011
P11-1081,we plan to examine ways of appropriately estimating an absolute score from a set of relative scores for further refinement.,1,2011
P11-1081,we also intend to experiment with introducing more sophisticated antecedent identification models in the ilp framework.,1,2011
P11-1084,"in the future, we plan to improve the learning of translation rules with binarized forests.",1,2011
P11-1091,we also plan to explore aspects of corpus maintenance in dynamic (constantly changing) domains.,6,2011
P11-1091,"in future work, we plan to explore how to utilize additional domain knowledge to better estimate the correlation between words.",1,2011
P11-1094,"for future work, we plan to use a more accurate language model, and add more types of complex error models, such as word deletion and word ordering error models to improve performance and address other types of errors.",1,2011
P11-1097,"in future work, we plan to develop more efficient methods of using search results for cross-domain generalization to avoid the cost of issuing a large number of queries to search engines.",4,2011
P11-1097,"in the future, we hope to be able to show that other nlp tasks can also benefit from such an enriched context representation. future work.",4,2011
P11-1098,we will investigate new text sources and algorithms to try and capture more knowledge.,1,2011
P11-1098,"while all learning algorithms require parameters, we think it is important for future work to focus on removing some of these to help the algorithm be even more robust to new domains and genres.",1,2011
P11-1101,"moreover, it would be interesting to carry over our methodology to a purely statistical linearization system where the relation between an input representation and a set of candidate realizations is not so clearly defined as in a grammar-based system.",4,2011
P11-1101,"in future work, we will extend our experiments to a wider range of alternations and try to capture inter-sentential context more explicitly.",1,2011
P11-1102,further use of contextual features will more thoroughly represent the information we want our model to take into account.,2,2011
P11-1102,"we are now interested in how we can apply this to the larger questions of positioning we began this paper by asking, especially in describing speaker positioning at various instants throughout a single discourse.",4,2011
P11-1102,"our segmentation accuracy is also fairly low, and further examination of segmentation accuracy using a more sophisticated evaluation metric, such as windowdiff (pevzner and hearst, 2002), would be helpful.",3,2011
P11-1102,this will be the main thrust of our future work.,6,2011
P11-1109,"first, we will release extracted paraphrases from all of the 29,661,812 definition sentence pairs that we acquired, after human annotators check their validity.",1,2011
P11-1109,Our future work is threefold.,6,2011
P11-1110,"we also plan to build a generation system that employs the yule model (yule, 1925) to determine the importance of each aspect (e.g. who, when, where, etc.) in order to produce summaries that include diverse aspects of a story.",1,2011
P11-1110,"in the future, we plan to move to content from other collective systems on web.",6,2011
P11-1110,"in order to generalize our findings, we plan to examine blog comments, online reviews, and tweets (that discuss the same url).",1,2011
P11-1112,an important direction for future work lies in investigating how the approach generalizes across languages as well as reducing our systems reliance on a treebank-trained parser.,1,2011
P11-1114,"in future work, we hope to identify additional types of document genre styles and incorporate genre directly into the extraction model.",1,2011
P11-1114,coreference resolution and discourse analysis will also be important to further improve event extraction performance.,1,2011
P11-1119,another important direction for future work involves more fully exploring the ways in which affect expression differs between textual and spoken dialogue.,1,2011
P11-1119,"finally, as automatic facial tagging technologies mature, they may prove powerful enough to enable broadly deployed dialogue systems to feasibly leverage facial expression data in the near future.",4,2011
P11-1123,"second, predicting projectable constituents is for improving machine translation and we are integrating the component into a syntax-based machine translation system.",1,2011
P11-1123,"first, the results for predicting function tags and chinese empty elements were obtained on human-annotated trees and it would be interesting to do it on parse trees generated by system.",5,2011
P11-1124,"moreover, we also plan to experiment with phrase-by-phrase classification instead of sentence by-sentence classification presented in this paper, in order to obtain more stable classification results.",1,2011
P11-1124,"furthermore, we will explore methods to promote translation consistency at document level.",1,2011
P11-1124,"in the future, we plan to investigate the impact of tm quality on translation consistency when using our approach.",3,2011
P11-1124,"we also plan to label the training examples using other sentence-level evaluation metrics such as meteor (banerjee and lavie, 2005), and to incorporate features that can measure syntactic similarities in training the classifier, in the spirit of (owczarzak , 2007).",1,2011
P11-1126,we also plan to investigate more complicated reordering models in hm decoding.,1,2011
P11-1126,"in the future, we will include more smt models and explore more features, such as syntax-based features, helping to improve the performance of hm decoding.",1,2011
P11-1128,"we plan to introduce left to right target generation (huang and mi, 2010) into the stag tree to string system.",1,2011
P11-1129,we are also interested in exploring more morphologically- or syntactically informed triggers.,1,2011
P11-1129,"in future work, we would like to integrate the backward language model into a syntax-based system in a way that is similar to the proposed algorithm shown in figure 1.",1,2011
P11-1130,"in future work, we want to improve the probability estimations for our paraphrasing models.",1,2011
P11-1130,we also want to experiment with other morphologically complex languages and other smt models.,1,2011
P11-1134,on the other side we will investigate more sophisticated methods to exploit the acquired lexical knowledge.,1,2011
P11-1134,"our future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for te and clte.",2,2011
P11-1134,"one possible direction is to consider linguistically motivated approaches, such as the extraction of syntactic phrase tables as proposed by (yamada and knight, 2001).",1,2011
P11-1137,in the future we hope to consider richer linguistic models capable of identifying multi-word expressions and syntactic variation.,1,2011
P11-1139,a natural avenue for future work is the extension of our method to other nlp tasks.,4,2011
P11-1140,"in the future, the weights of the cost function should be learned automatically by optimizing an appropriate error function.",1,2011
P11-1145,"the second general direction is the use of the unsupervised methods we propose to expand the coverage of existing semantic resources, which typically require substantial human effort to produce.",2,2011
P11-1145,"first, we would like to relax some of unrealistic assumptions made in our model: for example, proper modeling of alterations requires joint generation of syntactic realizations for predicateargument relations (grenager and manning, 2006; lang and lapata, 2010), similarly, proper modeling of nominalization implies support of arguments not immediately local in the syntactic structure.",1,2011
P11-1145,We plan to explore at least two directions in our future work.,6,2011
P11-1148,"first of all, we would like to evaluate the approach presented here using a more balanced and unbiased corpus, and compare its performance on such a corpus to local approaches.",3,2011
P11-1148,"secondly, we would also like to include grammatical dependency information in the disambiguation step of the algorithm.",1,2011
P11-1148,"for now, the disambiguation step only uses a word’s context words; enriching the feature set with dependency information is likely to improve the performance of the disambiguation.",1,2011
P11-1148,We conclude with some issues for future work.,6,2011
P11-1149,"particularly, we intend to explore new approaches for confidence estimation and their usage in the unsupervised and semi-supervised versions of the task.",1,2011
P11-1149,in future work we hope to further improve unsupervised semantic parsing performance.,1,2011
P11-1151,future approaches may be able to do away with this arbitrary separation of features by training a local classifier to consider all words in terms of their impact on content-only classification and their relations to neighbors.,1,2011
P11-1151,an opportunity for future work is to consider normalization approaches for other classifiers.,1,2011
P11-1152,"we also would like to look at other back off mechanisms (in addition to history length and classes) and incorporate them into the model, e.g., similarity and topic.",1,2011
P11-1152,we would like to explore training regimes that lie between unique-event clustering and all-event clustering and upweight rare events less.,1,2011
P11-1152,"finally, training classes on unique events is an extreme way of highly weighting rare events.",1,2011
P11-1152,"in future work, we would like to find a theoretical justification for the surprising fact that polynomial discounting does at least as well as kneser-ney discounting.",5,2011
P11-1153,"besides, advanced nlp techniques for document analysis, e.g., shallow parsing, may also be used to further improve structure finding.",1,2011
P11-1153,"our work can be extended by incorporating richer features, such as named entity and co-reference, to enhance the models capability of structure finding.",1,2011
P11-1155,"though our attempt to use giza++ for evaluating the similarity between chinese sentences and english sentences failed, we will exploit more advanced measures based on statistical alignment model for cross-language similarity computation.",1,2011
P11-1155,"in future work, we will investigate to use the machine translation quality factor to further improve the fluency of the chinese summary, as in wan et al.(2010).",1,2011
P11-1158,"in future work we plan to examine better a* heuristics for ccg, and to look at principled approaches to combine the strengths of a*, adaptive super tagging, and other techniques to the best advantage.",1,2011
P11-1159,"in future work, we intend to improve the prediction of functional morphological features in order to improve parsing accuracy.",1,2011
P11-1159,we plan to make our parser available to other researchers.,4,2011
P11-1159,we also intend to investigate how these features can be integrated into other parsing frameworks; we expect them to help independently of the framework.,1,2011
P11-1159,please contact the authors if interested.,6,2011
P11-1164,our first plan is to complete the mining process on all the types of sentences.,6,2011
P11-1164,"since we perform task 1 and task 2 separately, we need to build an end to end system.",1,2011
P11-1164,the second one is to conduct more experiments for obtaining better performance.,3,2011
P11-1164,"in our future work, we have the following plans.",6,2011
P11-1164,the final one is about an integrated system.,1,2011
P11-2002,"we also plan to experiment with other unsupervised techniques, such as clustering and outlier detection, that can lead to better retrieval of rare classes.",1,2011
P11-2002,"finally, we plan to investigate the applicability of our approach to a multi-class scenario.",4,2011
P11-2002,"this should result in better performance on the rare classes, which is currently still low.",1,2011
P11-2002,our plans for future work include improving our lm by incorporating syntactic information such as pos tags.,1,2011
P11-2004,"going beyond our results provided for synthetic data, future work will explore applications of this technique, such as in experiments with streaming social media like twitter.",4,2011
P11-2008,we also believe that the annotated data can be useful for research into domain adaptation and semi-supervised learning.,2,2011
P11-2010,considering sources other than western languages as well as targets other than japanese is the future work.,2,2011
P11-2013,"in the future, we would like to compare our method with a statistical machine translation approach performed at the letter level, evaluate the system using sentences by incorporating context word information, and consider many-to-one letter transformation in the model.",3,2011
P11-2014,we would also like to test our method on a range of languages and texts.,2,2011
P11-2014,"some possible extensions of our work include automatically generating the set of possible rhyme schemes r, and incorporating partial supervision into our algorithm as well as better ways of using and adapting pronunciation information when available.",1,2011
P11-2015,we also are investigating how we can better utilize the output of our pcfg parsers for classification.,1,2011
P11-2015,we look to automate the expansion of the training set of vandalized revisions to include examples from outside of wikipedia that reflect similar language styles.,1,2011
P11-2018,interesting future work should be devoted to address the use of structural kernels for the proposed reranker.,5,2011
P11-2020,"in future, we will extend this analysis to the complementary turn-taking category of turn yielding cues and explore how a spoken dialogue system may take advantage of information about entrainment to improve dialogue coordination and the user experience.",4,2011
P11-2025,we hope that availability of this corpus motivates more research on statistical scope disambiguation.,4,2011
P11-2025,"since world knowledge plays a major role in scope disambiguation, we believe that leveraging unlabeled domain specific data in order to extract lexical information is a promising approach for scope disambiguation.",1,2011
P11-2025,our goal is to expand the corpus up to twice in size.20% of the corpus will be annotated and the rest will be left for the purpose of semisupervised learning.,2,2011
P11-2027,"as future research, we plan to study the impact of different dataset sizes and vector space model parameters for improving the performance of the am component of the metric.",1,2011
P11-2027,"finally, we also plan to study alternative uses of am-fm within the context of statistical machine translation as, for example, a metric for mert optimization, or using the am component alone as an additional feature for decoding, rescoring and/or confidence estimation.",1,2011
P11-2027,"this will include the study of learning curves based on the amount of training data used, and the evaluation of different vector model construction strategies, such as removing stop-words and considering bigrams and word categories in addition to individual words.",3,2011
P11-2029,a future direction for smt is to develop translation models that can effectively employ such information.,1,2011
P11-2034,"the integration of knowledge from parse disambiguation and fluency ranking could be beneficial for tasks which combine aspects of parsing and generation, such as word-graph parsing or paraphrasing.",1,2011
P11-2035,"in the future, we plan to apply our joint training technique to other rich filtering regimes (zhang et al., 2010), and to other nlp problems that combine the predictions of overlapping classifiers.",4,2011
P11-2037,we also plan to extend our work here to recover coindexation information (links between a moved element and the trace which marks the position it was moved from).,2,2011
P11-2037,"as a step towards shallow semantic analysis, this may further benefit other natural language processing tasks such as machine translation and summary generation.",4,2011
P11-2037,"chung and gildea (2010) have shown that such information indeed helps translation, and we plan to extend this work by handling more empty categories (rather than just *pro* and *pro*), and to incorporate them into a syntax-based translation model instead of a phrase-based model.",1,2011
P11-2039,"also, we plan to extend our approach to abstractive summarization.",4,2011
P11-2039,"moreover, as the principles of qsbp are basically language independent, we will investigate the effectiveness of qsbp in other languages.",2,2011
P11-2043,"the next step in this work would be to build a style transformation system that uses the features discussed in this paper as the bases for determining when, where, and how to do the style transformation.",1,2011
P11-2044,"finally, we analyze the alignments produced and suggest that further improvements are possible through careful feature/constraint design, as well as the use of named-entity recognition and additional resources.",1,2011
P11-2047,"future work will focus on using synonym-based expansion in the contexts (not just the time expressions headwords), and on incorporating contextual information and syntactic transformations.",1,2011
P11-2053,"second, the user is supposed to gain a better understanding of semantic change by interactively exploring a corpus.",4,2011
P11-2053,"for future research, we want to test our methodology on a broader range of terms, texts and languages and develop novel interactive visualizations to aid investigations in two ways.",4,2011
P11-2053,"as a first aim, the user should be allowed to check the validity and quality of the visualizations by experimenting with parameter settings and inspecting their outcome.",4,2011
P11-2055,"in future work, we want to run additional experiments with different classifiers (svm) and apply a genetic algorithm to perform joint feature selection, parameter optimization and instance selection.",1,2011
P11-2055,we also plan to expand our feature set by including global context features (content words from the english sentence) and to examine the relationship between the performance and the number (and nature) of languages that is added to the feature vector.,1,2011
P11-2056,we hope to shorten the gap to supervised systems with more unlabeled data.,6,2011
P11-2056,"we also plan on training our models with em with features (berg-kirkpatrick et al., 2010).",1,2011
P11-2059,"future work will explore incorporating lu predictions to predict the social roles played by the participants in a thread, for example using persuasion and credibility to establish which participants in a discussion are serving as informal leaders.",4,2011
P11-2062,"in future work, we plan to use both corpus annotations and agreement rules to automatically learn functional features for unseen words and detect and correct annotation errors.",1,2011
P11-2062,we also plan to extend agreement rules to include complex structures beyond bigrams.,1,2011
P11-2066,"we will continue working on this line of research and improve our discriminative learning model in the future, for example, by adding more phrase level features.",1,2011
P11-2069,integrating the use of patterns within an edit rate computation technique will however raise new difficulties.,5,2011
P11-2069,"our future work also includes the acquisition of paraphrase patterns (e.g.(zhao , 2008)) to generalize the acquired equivalence units to more contexts, which could be both used in applications and to attempt improving further paraphrase acquisition techniques.",1,2011
P11-2074,further improvement may be achieved with other feature space partition approaches in the future.,1,2011
P11-2076,"in the future, we want to apply our proposed method to other language pairs and domains.",4,2011
P11-2078,we plan to test its effectiveness in hierarchical and syntax-based smt systems.,3,2011
P11-2078,we also plan to investigate the relative usefulness of lm biasing as we move from low resource languages to those for which significantly larger parallel corpora and lm training data are available.,1,2011
P11-2081,in future work we would like to study the impact of non-determinism on higher order models in the standard alignment model sequence and to gain more insight into the impact of finer-grained features in alignment.,1,2011
P11-2082,"on the technical level, we would like to apply a sequence model to account for the dependencies among sentences, and obtain more meaningful features for formal and informal address.",1,2011
P11-2082,"in order to remove idiosyncratic features like names, we will only consider features that occur in several novels; furthermore, we will group words using distributional clustering methods (clark, 2003) and predict t/v based on cluster probabilities.",1,2011
P11-2082,Our analyses suggest a number of directions for future research.,6,2011
P11-2084,"our next steps involve experiments with other topic models and other corpora, and combining this unsupervised approach with other tools for lexicon extraction and synonymy detection from unrelated and comparable corpora.",1,2011
P11-2085,below we sketch three possible directions for the future work: (1) we should consider position features in analyzing pinyin errors.,1,2011
P11-2085,"for example, it is less likely that users make errors in the first letter of an input pinyin.(2) we aim at designing a selfadaptive input method that provide error-tolerant features (chen and lee, 2000; zheng , 2011a).(3) we want to build a chinese spelling correction system based on extracted error-correction pairs.",1,2011
P11-2086,"we are in the process of obtaining more documents in the domain, which will allow the use of more complex models and more sophisticated representations.",2,2011
P11-2086,"in particular, we are considering clusters of terms and probabilistic topic models such as lda (blei et al., 2003).",1,2011
P11-2087,"future work because the method does not place any restrictions on the complex and simple corpora, we plan to validate it on different domains and expect it to be easily portable.",2,2011
P11-2087,"we also plan to extend androutsopoulos, ion and prodromos malakasiotis.2010.",1,2011
P11-2088,"finally, we would like to integrate our helpfulness model into a web-based peer-review system to improve the quality of both peer reviews and paper revisions.",1,2011
P11-2088,"therefore, we are planning to investigate the impact of these different helpfulness ratings on the utilities of features used in modeling peer-review helpfulness.",3,2011
P11-2088,"in the future, we would like to replace the manually coded peer-review specialized features (cogs) with their automatic predictions, since we have already shown in our prior work that some important cognitive-science constructs can be successfully identified automatically.",1,2011
P11-2088,"also, it is interesting to observe that the average helpfulness ratings assigned by experts (used as the gold standard in this study) differ from those given by students.",3,2011
P11-2091,"in the future, we plan to add more sophisticated sub-systems in this framework, and also explore combining ranking outputs from different sub-systems.",1,2011
P11-2091,"furthermore, we will incorporate negative seeds into the process of interactive suggestion.",6,2011
P11-2092,"we then want to extend our model to other languages, which could be more challenging, as certain languages have a more complex morphology than english, but also worthwhile, if the unknown word rate is higher.",2,2011
P11-2092,we plan to use our model for domain adaptation in applications like machine translation.,4,2011
P11-2092,the model could be further improved by using contextual information for the word clustering and training a classifier based on morphological features to assign oov words to these clusters.,1,2011
P11-2093,future work in this area will include examination of performance on other tasks and languages.,4,2011
P11-2094,we plan to carry out more transliteration experiments on other language pairs in the future.,2,2011
P11-2095,"one area for future work is a full investigation of the performance of these algorithms in polysynthetic languages such as inuktitut, where each word contains many morphemes.",2,2011
P11-2095,"it is likely that in such languages, the algorithms will find morphs rather than words.",2,2011
P11-2096,"in the future, we would like to see more empirical evaluations and detailed studies comparing the practical merits of various paraphrase generation techniques.",3,2011
P11-2096,"as madnani and dorr (madnani and dorr, 2010) suggested, it would be beneficial to the research community to develop a standard, shared evaluation that would act to catalyze further advances and encourage more meaningful comparative evaluations of such approaches moving forward.",3,2011
P11-2097,"future work will include: (i) applying the method to other part-of-speech words, (ii) comparing the method with existing other automated method, and (iii) extending the method to find domain-specific senses with unknown words.",4,2011
P11-2098,"accordingly we plan to explore relaxing this strict conjunctive behavior through models such as noisy-and (pearl, 1988).",2,2011
P11-2098,"we also intend to explore the contribution of our model, and particularly its estimated parameter values, within a complex system that integrates multiple levels of inference.",1,2011
P11-2101,our proposed method determines feature polarity not only by opinion words that modify the features but also by its surrounding context.,1,2011
P11-2101,Our future work will focus on improving the precision.,1,2011
P11-2105,"in the future, we will investigate our method in the larger and more noisy data.",2,2011
P11-2108,we also plan to explore the fusion of multi-modal features to enhance recognition and increase our understanding of multi-modal rapport behavior.,1,2011
P11-2108,"in future research, we would like to extend our work to exploit sequential learning frameworks to predict verbal feedback.",1,2011
P11-2108,"we will also work to analyze how quickly people can establish rapport, as the short duration of our spanish dyads poses substantial challenges.",5,2011
P11-2110,"moreover, optimizing memory usage, for example via feature pruning or randomized algorithms, would allow incorporation of richer feature sets and would likely lead to further improvements, as indicated by the experiments in this paper.",1,2011
P11-2110,"future work includes further evaluation of the vmm, e.g.as a language model within a speech recognition or machine translation system.",3,2011
P11-2110,"we also intend to evaluate the performance of the vmm on other lexical prediction tasks and more generally, on other classification tasks with similar characteristics.",3,2011
P11-2113,"some future work includes applying this model to areas such as topic tracking and text segmentation, and coherently adjusting it to fit an n-gram modeling approach.",4,2011
P11-2114,"in future work, we will utilize more semantic information such as localized latent topics to help capture comparative aspects, and use machine learning technologies to tune weights of concepts.",1,2011
P11-2117,"in the future, we hope to explore alignment techniques more tailored to simplification as well as applications of this data to text simplification.",1,2011
P11-2121,"in the future, we will test the robustness of these approaches in more languages.",2,2011
P11-2122,we are focusing on accounting for these issues in current work to allow such automatic correction.,5,2011
P11-2122,"however, because the derivation trees and etrees are somewhat abstracted from the actual trees in the treebank, it can be challenging to automatically correct the structure in every location to reflect the correct derivation tree fragment.",2,2011
P11-2122,this is because of details concerning the surrounding structure and the interaction with annotation style guidelines such as having only one level of recursive modification or differences in constituent bracketing depending on whether a constituent is a “single-word” or not.,1,2011
P11-2124,agreement information could be very useful for disambiguating various constructions in hebrew and other morphologically rich languages.,1,2011
P11-2128,"also, comparisons with the previous works are remaining work.",3,2011
P11-2128,"from another perspective, we are considering the use of graph-based approaches (komachi et al., 2008) incorporated with the topic information using phits (cohn and chang, 2000), to further enhance entity extraction accuracy.",1,2011
P11-2128,"to resolve this problem, we will incorporate the active learning or the distributional approaches.",1,2011
P11-3001,"in the future, we will investigate combining word alignments on language pairs where both languages have no explicit word boundaries such as chinese-japanese.",2,2011
P11-3002,adapting the proposed scheme to multi-document summary generation is the ongoing work we are engaged in.,6,2011
P11-3002,"in the next step, we will experiment with alternative sentence representations and ordering algorithms to achieve better performance.",1,2011
P11-3004,since non-english versions of wikipedia often are less extensive than the english version we find it promising to combine wikipedia versions of different languages and to use them as a source for multilingual ned.,2,2011
P11-3004,In future work we plan to explore multilingual data for NED.,2,2011
P11-3005,"we also consider harvesting data sources from the web such as lists of cities, common names and companies in pakistan and india.",2,2011
P11-3005,another area for future work is to extend the extraction and classification to trigrams to improve the results especially for locations and person names.,4,2011
P11-3006,"as future works, we will conduct experiments with various types of data and query, and further investigate the characteristic of our proposed method.",3,2011
P11-3009,we will continue to train smt systems on automatically labeled discourse connectives in large corpora.,1,2011
P11-3009,"we will try to better model the context of a connective, for instance by integrating word similarity distances from wordnet as features.",1,2011
P11-3012,future work needs to explore features that can address the difference in language usage that the different authors use.,2,2011
P11-3012,"further work is also needed to classify extra-sentential relations, as the current methods look only at relations occurring within a single sentence thus ignoring a large percentage of relations between entities.",1,2011
P11-3013,future work should examine similar experiments with maltparser and other machine translation systems.,1,2011
P11-3015,future directions include trying to improve the performance by modelling negations using a more sophisticated approach.,1,2011
P11-3015,exploring longer citation scopes by including citation contexts might also improve citation sentiment detection.,1,2011
P11-3017,future work in this direction would include growing the set of features by adding more prosodic ones and introducing lexical ones such as bi-grams and uni-grams.,1,2011
P11-3017,"when a large feature bank has been developed, significant cues will be used in conjunction with machine learning techniques to build a model for turn taking which can be implemented in a spoken dialogue tutoring system.",1,2011
P11-3017,thus the first line of future inquiry is to redo this method using a smaller silence boundary (50 ms) and different set of prosodic features so that it is truly comparable to gravano and hirschberg’s (2009) work with the game corpus.,1,2011
P11-3019,as future work we are going to explore other ways of representing morphemes in the model.,3,2011
P11-3019,"here we represented morphemes as separate states, but including them as features together with the root state may produce better models.",1,2011
P11-3019,"another approach we will also focus is dividing words into characters and applying character-level models (klein et al., 2003).",1,2011
P11-3022,"in the future, we will analyze the technique on other learning methods such as kmeans++ and experiment on various real-data nlp tasks.",1,2011
P11-4002,"future work, for which we are urgently seeking funding, could include integration of further nlp-based features such as coreference resolution or question answering, as well as citation classification and graphical navigation along the ideas in schafer and kasterka (2010).",1,2011
P11-4006,"therefore, our future works are threefold: for sentiment analysis, we will consider more sophisticated ways to improve the baseline accuracy and to aggregate individual posts into a collective consensus.",1,2011
P11-4006,"for music generation, we plan to add more instruments and exploit learning approaches to improve the selection of chords.",2,2011
P11-4006,"for visualization, we plan to add more interactions between music, sentiments, and users.",6,2011
P11-4008,"additionally, we are actively mining other language pairs to build a multi-language learning system.",2,2011
P11-4008,"in future work, we are examining extracting language knowledge from the real-time web for translation in news scenarios.",1,2011
P11-4009,but these problems lead to another research issue that may be termed as cross lingual sentiment synset linking.,5,2011
P11-4009,presently we are giving a closer look to the qualitative analysis of developed multilingual psycho-sentiment lexicons.,1,2011
P11-4012,future work includes researching further on the benefits provided by our online learning techniques with experiments involving real users.,2,2011
P11-4014,"future work will aim at improving the relevance of semantic search, at modeling context to  improve timing of results, and at inferring relevance feedback from users.",1,2011
P11-4020,another important direction is to refine the interaction design through task-based user studies.,1,2011
P11-4020,"to this end, a future improvement that we plan to use is a variant on mmr (maximum marginal relevance) (carbonell , 1998), which can be used to optimize the diversity of selected text tiles as well as the relevance based ordering of clusters, i.e., so that more diverse sets of extracts from the co-cited articles will be placed at the ready fingertips of users.",1,2011
P11-4024,we plan to provide additional functionality such as pattern frequency counts.,1,2011
P11-4024,the annotation librarian has been enhanced over the course of a number of biomedical nlp use cases and we plan to continue to enhance the interface as new use cases arise.,1,2011
P12-1002,"in future work, we would like to investigate more sophisticated features, better learners, and in general improve the components of our system that have been neglected in the current investigation of relative improvements by scaling the size of data and feature sets.",1,2012
P12-1003,"we believe that it should be possible to use insights from this paper in an active learning setting, to select, from an available monolingual source, a subset of a given size for manual translation, in such a way at to yield the highest performance, and we plan to extend our work in this direction.",1,2012
P12-1005,"in the future, we plan to explore phonological context and use more flexible topological structures to model acoustic units within our framework.",1,2012
P12-1007,"our future work will be to fully implement an end-to-end discourse parser using our rich linguistic features, and focus on improving performance on cross-sentence instances.",1,2012
P12-1010,"in future work, we plan to investigate how to best apply the dependency structure approach to such domains.",4,2012
P12-1011,"although we focused on year mentions here, there are several avenues for future study, including explorations of how other types of time expressions might inform the task.",1,2012
P12-1013,"we also plan to examine the benefit of tnf in learning similar structures, e.g., taxonomies or ontologies.",3,2012
P12-1013,"in future work, we aim to evaluate tnf on large graphs that are automatically generated from huge corpora.",3,2012
P12-1016,one potential avenue of future work would be to adapt our component models to new genres by self-training them on the target side of a large bitext.,1,2012
P12-1022,future work could consist in combining both strategies: pre-grouping could suggest a set of potential mwe segmentations in order to make it more flexible for a parser; final decisions would then be made by the reranker.,1,2012
P12-1027,"as future work, we plan to apply this fast learning method on other large-scale natural language processing tasks.",4,2012
P12-1030,"in addition, while our novel components were motivated by the search space of textual inference, we foresee their potential utility in other application areas for search, such as automated planning and scheduling.",4,2012
P12-1032,"in the future, we will explore other consensus features and other similarity measures, which may take document level information, or syntactic and semantic information into consideration.",1,2012
P12-1037,a primary goal for future research is developing an on-line structure learner for blps that can directly learn probabilistic first-order rules from uncertain training data.,1,2012
P12-1046,future work will involve examining the srtsg model for different languages and for unsupervised grammar induction.,2,2012
P12-1048,"furthermore, since the in-domain phrase-topic distribution is currently estimated with simple smoothing interpolations, we expect that the translation system could benefit from other sophisticated smoothing methods.",1,2012
P12-1048,"finally, the reasonable estimation of topic number for better translation model adaptation will also become our study emphasis.",1,2012
P12-1049,"in future work, we plan to adapt our approach to language pairs where one language is alphabetic and the other language is non-alphabetic such as english/japanese.",2,2012
P12-1050,"finally, we may try to exploit not only the ranking, but also the scores produced by the reordered lms, as an additional decoding feature.",1,2012
P12-1054,"in the future, we plan to extend the coranking framework so as to incorporate information credibility and temporal recency.",1,2012
P12-1055,"second, we are interested in incorporating knowledge mined from wikipedia into our factor graph.",2,2012
P12-1055,"first, we are going to develop advanced tweet normalization technologies to resolve slang expressions and informal abbreviations.",1,2012
P12-1055,"in the future, we plan to explore two directions to improve our method.",1,2012
P12-1056,a more interesting and useful task is to detect real time bursts in an online fashion.,1,2012
P12-1056,this is one of the directions we plan to study in the future.,6,2012
P12-1056,another limitation of the current method is that the number of topics is predetermined.,1,2012
P12-1058,"in the future, we plan to explore using constrained crf for more accurate dependency tagging.",1,2012
P12-1058,we will also use the result from this work in other tasks such as answer quality ranking and answer summarization.,4,2012
P12-1060,"in the future, we will work on leveraging parallel sentences and word alignments for other tasks in sentiment analysis, such as building multilingual sentiment lexicons.",4,2012
P12-1065,"as future work, we would like to apply our algorithm to a structured task such as part of speech tagging.",4,2012
P12-1069,we are in the process of extending our head-driven parser for non-projective structures as our future work.,1,2012
P12-1074,"as future work, we plan to improve the quality of the output by considering word sense disambiguation techniques to reduce the effect of inappropriate ingredients.",2,2012
P12-1074,we also want to extend the model to include multiword ingredients and to generate not only words but also short phrases.,1,2012
P12-1078,a key aspect of future work will be to extend the sparse mixed-effects paradigm to other problems within the social sciences where metadata is available but qualitative analysis at a large scale is difficult or impossible.,4,2012
P12-1079,"in the future, we are interesting to find ways to exploit topic model on bilingual data without document boundaries, thus to enlarge the size of training data.",2,2012
P12-1085,"however, we did not address the issue of property value cleaning and normalization.",5,2012
P12-1085,This is an important direction for future work.,6,2012
P12-1087,"thus, techniques that improve the accuracy of crowd-sourced answers are an interesting direction for future work.",1,2012
P12-1089,"finally, it is worth exploring scaling the approach to unrestricted event extraction, and jointly model extracting more than one relation per document.",1,2012
P12-1091,"for future work, we would like to compare the text modeling performance of wtmf with lsa and lda on regular length documents.",3,2012
P12-1095,"in the future work, we will extend our predicate translation model to translate both verbal and nominal predicates.",1,2012
P12-1096,we also expect to explore better way to integrate ranking reorder model into smt system instead of a simple penalty scheme.,1,2012
P12-1096,"in future work, we plan to extend the ranking model to handle reordering between multiple levels of source trees.",1,2012
P12-1098,"in future work, we plan to tune the free parameter a for each language pair.",2,2012
P12-1099,future work includes extending this approach to use multiple translation models with multiple language models in ensemble decoding.,1,2012
P12-1107,"in the future we would like to investigate how we can boost the number of edits the system performs, while still producing grammatical and meaning preserving output.",1,2012
P12-1110,"in future work, probabilistic pruning techniques such as the one based on a maximum entropy model are expected to improve the efficiency of the joint model further because the accuracies are apparently still improved if a larger beam can be used.",1,2012
P12-1111,"there are more interesting ways in applying these constraints, which we are going to study in future work.",1,2012
P12-1111,these two simple applications suggest that it is of interest to explore data-driven deterministic constraints learnt from training examples.,4,2012
P12-2001,"however, higher-order constituent parsing inevitably leads to a high computational complexity.",6,2012
P12-2001,we intend to deal with the efficiency problem of our model with some advanced parallel computing technologies in our future works.,1,2012
P12-2008,"in the future, we plan to go a step further to see whether we can enhance dissimilarity with penalizing phrase tables used in both of the translation processes.",1,2012
P12-2010,"for future work, we plan to relieve the complexity problem for dealing with more expanded graph structure to improve the performance of our proposed approach.",1,2012
P12-2025,probably the most important goal for future work is improving the recall achieved in the complete disambiguation pipeline.,1,2012
P12-2026,"additionally, an interesting direction to explore is to identify phrase types and train type-specific crf model.",1,2012
P12-2026,"for example, existing query expansion methods could be implemented to retrieve more webpages containing translations.",2,2012
P12-2026,"in addition, natural language processing techniques such as word stemming and word lemmatization could be attempted.",1,2012
P12-2026,Many avenues exist for future research and improvement.,6,2012
P12-2027,"future research here could profitably focus on this relationship, especially for terms whose success in the english and german hip hop communities is highly disparate.",1,2012
P12-2032,"using sophisticated nlp techniques, we may be able to enrich the network and use standard sna metrics to predict the dominance relations in the gold standard.",1,2012
P12-2033,"in future work, we will incorporate more syntactic information in the model to better evaluate sentence quality.",1,2012
P12-2033,"we also plan to perform a human evaluation for the compressed sentences, and use sentence compression in summarization.",3,2012
P12-2036,"although we have presented our approach in the context of 3d virtual worlds, we believe our technique is also applicable to other domains such as the web, video games, or human robot interaction.",4,2012
P12-2037,how to automatically classify the extracted patterns is also an interesting future issue.,1,2012
P12-2037,"these patterns are potentially useful for many other applications, which will be studied in the future work.",4,2012
P12-2040,"as future work, we intend to expand the current size of the collection from 0.7k to 2k movies, as well as to improve some of our parsing and postprocessing algorithms for reducing the amount of noise still present in the collection and enhance the quality of the current version of the dataset.",2,2012
P12-2042,"other directions of subsequent research may include address more elaborate models of events, and the investigation of the relationship between relation words and taxonomies of discourse relations.",1,2012
P12-2042,"another prospective field of application can be seen in nlp applications, where selection preferences for relation words may serve as a cheap replacement for full-fledged discourse parsing.",4,2012
P12-2043,"for future work, we would like to try the proposed technique on other languages, because it would likely be effective in automatically learning character-level morphological transformations as well as overcoming some of the problems associated with stemming.",2,2012
P12-2045,"as well as using event linking to add referentially precise hyperlinks to a news archive, further characteristics of news will emerge by analyzing the graph of event references.",1,2012
P12-2047,"an area for future work is to transfer other syntactic information, such as parse structures or super tags using a similar transfer approach.",1,2012
P12-2048,the jumbled nature of the dig site means that the process of assembling new texts from this site will be one of the major tasks in for hittite scholars in the near future.,5,2012
P12-2051,"in future work, we plan to do an in-depth analysis of the features that best characterize the changes in word usage over time, and develop representations that allow us to track sense changes.",1,2012
P12-2055,"in our future work, it is worth studying how to combine the best of our approach and discriminative word alignment models to improve rule extraction for smt models.",1,2012
P12-2056,"we believe that in this framework, using other finer-grained segmentation, with fewer ambiguities than character, would better parameterize the alignment models, while using other coarser-grained segmentation as wsr can help capture more linguistic knowledge than word to get better translation.",1,2012
P12-2064,a better automatic evaluation metric would be needed as further research.,3,2012
P12-2065,"for future work, we propose to incorporate prior knowledge of latent variables to the model.",1,2012
P12-2066,"we expect our method to be complementary with sophisticated methods used in state-of-the-art sentiment classification systems, which is to be explored in future work.",4,2012
P12-2068,"as future work, we will compare our model with the other state-of-the-art systems.",3,2012
P12-2068,we will also investigate the correlation between readability and srlbased score by manual evaluations.,3,2012
P12-2068,"furthermore, we would like to combine discourse constraints with sr constraints.",1,2012
P12-2069,we are now expanding the variety and complexity of the abstraction schemes and generation patterns to deal with more aspects and other categories.,1,2012
P12-2069,"although fully abstractive summarization is a daunting challenge, our work shows the feasibility and usefulness of this new direction for summarization research.",5,2012
P12-2071,"for future work, we will experiment with more diverse training and testing data and also more sophisticated algorithms.",1,2012
P12-2072,"for this reason, it can be expected to be easily portable across languages enabling good quality processing of languages with complex morphology and scarce resources.",2,2012
P12-2072,"the adaptive general classification model used in our approach makes use of different sources of information that can be found in a small annotated corpus, with no need for comprehensive, manually constructed morphological dictionaries.",1,2012
P12-2073,our next step is to utilize the collected data and analysis results to build online and offline spelling correction models.,1,2012
P12-2075,"if it is, automatic clustering of the input data may be an important pre-processing step for this kind of systems.",1,2012
P12-2075,this suggest that consistency of the input data is as important as the amount of data.,6,2012
P12-2075,this hypothesis has to be confirmed in futur studies.,3,2012
P12-3004,"in addition, a hadoop-based mapreduce-parallelized version is underway and will be released in near future.",6,2012
P12-3006,we would also like to further enhance the system to convert the translated english chat messages back to the social media language as defined by the user.,1,2012
P12-3007,"as future work, we intend to improve iris performance by addressing some of the already identified common failures.",1,2012
P12-3010,"in the future, it would be interesting to extend the parallel corpus to the internet to retrieve more rich data for the computer assisted translation.",2,2012
P12-3017,"making the illume components real products like the home lighting system, the intelligent table lamp, or the music album promoter is also a future plan.",4,2012
P12-3017,"we will continue collecting annotated materials and user feedbacks for learning, and make the materials a corpus for the research community.",2,2012
P13-1005,"finally, we would like to perform extensive controlled experimentation to examine the relative contribution of the various aspects of our approach.",3,2013
P13-1005,we will look to identify the issues with such models and provide general guidelines for prepping models prior to processing.,1,2013
P13-1006,"such can be handled with discriminative training, a topic we plan to address in the future.",1,2013
P13-1006,"as such, it might require more training data for convergence than a method that also makes use of negative training sentences that are not true of a given video.",2,2013
P13-1007,"this work can be improved in many directions, among which are scoping more elements such as other scopal operators and implicit entities, deploying more complex learning models, and developing models which require less supervision.",1,2013
P13-1008,"to improve the accuracy of end to-end ie system, we plan to develop a complete joint framework to recognize entities together with event mentions for future work.",1,2013
P13-1008,also we are interested in applying this framework to other ie tasks such as relation extraction.,4,2013
P13-1010,future work should first investigate the integration of information about entities.,1,2013
P13-1013,future work includes investigations of our parser and annotations on chinese nlp tasks.,1,2013
P13-1015,"in the future, we will explore irtg binarization with fanout increase.",1,2013
P13-1017,"for future work, we will investigate more settings of different hyper-parameters in our model.",1,2013
P13-1017,"secondly, we want to explore the possibility of unsupervised training of our neural word alignment model, without reliance of alignment result of other models.",1,2013
P13-1025,"we hope the publicly available collection of annotated requests enables further study of politeness and its relation to social factors, as this paper has only begun to explore this area.",4,2013
P13-1026,we make our thesis clarity annotations publicly available in order to stimulate further research on this task.,6,2013
P13-1026,"in addition to developing these models, we proposed novel features for use in our thesis clarity error model and employed these features, each of which was explicitly designed for one or more of the error types, to train our scoring model.",1,2013
P13-1030,"the most important direction of future work for ostag is the development of a principled grammar induction model, perhaps using the same techniques that have been successfully applied to tsg and tig.",1,2013
P13-1033,in this paper the model was only used to infer word alignments; in future work we intend to develop a decoding algorithm for directly translating with the model.,1,2013
P13-1034,"in future work, we plan to explore utilizing richer statistics from the unlabeled data, beyond word marginals.",1,2013
P13-1034,"lastly, we also wish to explore ensemble approaches that combine the best supervised classifiers with the improved class-conditional estimates provided by mnb-fm.",1,2013
P13-1034,"further, we plan to experiment with techniques for unlabeled data sets that also include continuous-valued features.",1,2013
P13-1037,"another avenue for future research is to study variation in possessive use across genres, including scientific and technical genres.",1,2013
P13-1037,"instead of trying to find the one-best interpretation for a given possessive example, we would like to produce a list of all appropriate interpretations.",6,2013
P13-1038,"because we confirmed in this paper that these two approaches have different characteristics, it would be interesting to incorporate textual clues into the distribution-based approach by using, for example, machine learning techniques.",1,2013
P13-1038,one is to explore a more sophisticated approach for precisely modeling the contexts of numbers.,1,2013
P13-1038,There are three important future directions for this research.,6,2013
P13-1040,"furthermore, we aim to extend the existing dictionaries and possibly our training data with terms extracted from comparable corpora.",2,2013
P13-1040,"finally, we plan to investigate the usefulness of the terms in different application scenarios, including computer assisted translation and machine translation.",4,2013
P13-1040,"exploring ways to add contextual or distributional features to our term representations is also an avenue for future work, though it clearly significantly complicates the approach, one of whose advantages is its simplicity.",1,2013
P13-1046,"in future, we also hope to explore unsupervised online adaptation, where the trained model can be updated as test data is processed.",1,2013
P13-1049,in the future we will apply the hierarchies on finer feature spaces to make more accurate optimizations.,1,2013
P13-1049,"instead of cutting product-hierarchies, we will employ usual techniques to build decision trees10 and apply our cutting method on their structure.",1,2013
P13-1049,we also plan to extend our method by breaking the symmetry of our hierarchies.,1,2013
P13-1049,"the objective is twofold: first, we will get rid of the sequence of indicators as parameter.",1,2013
P13-1051,"that will be a complementary point of view, and thus a natural direction of future work for us.",6,2013
P13-1051,"while the focus of this paper is to explore and describe the expressive power of various annotation styles, we did not address the learnability of the styles by parsers.",5,2013
P13-1052,"we also plan to exploit the acquired html patterns for implementing an open-source glossary crawler, along the lines of google define.",1,2013
P13-1053,"in future work, the framework we have presented here should be tested more extensively, not only against a gold standard but also in terms of the usefulness of the derived collective annotations for training supervised learning systems.",3,2013
P13-1053,"on the theoretial side, it would be interesting to study the axiomatic properties of the methods of aggregation we have proposed here in more depth and to define axiomatic properties of aggregators that are specifically tailored to the task of collective annotation of linguistic resources.",1,2013
P13-1054,"in future pargrambank releases, we will provide more theory-neutral dependencies along with the lfg representations.",1,2013
P13-1057,"despite the consistent superiority of type annotations in our experiments, it of course may be the case that techniques such as active learning may better select sentences for token annotation, so this should be explored in future work.",1,2013
P13-1059,"furthermore, we are interested in extending this framework to translate other out-of-vocabulary terms.",2,2013
P13-1059,"in the future we intend to improve the framework by training a discriminative model to automatically assign weights to combine name translation and baseline translation with additional features including name confidence values, name types and global validation evidence, as well as conducting lm adaptation through bilingual topic modeling and clustering based on name annotations.",1,2013
P13-1063,"secondly, knowledge in a minor language may also help improve extraction performance for a major language due to the cultural and religion differences.",2,2013
P13-1063,"last but not least, we will try to extract multiple attr-value pairs at the same time for each article.",1,2013
P13-1063,"firstly, more attributes in more info box templates should be explored to make our results much stronger.",1,2013
P13-1064,"monolingual and cross-lingual textual entailment in particular would be interesting applications, because they require finding shared meaning on two text fragments.",2,2013
P13-1064,"we also plan to explore further uses for this language bridge, at a finer semantic level.",1,2013
P13-1066,"in our future work, we intend to extend the model to account for stances, and issue specific interactions which would pave the way for user profiling and behavioral modeling.",1,2013
P13-1067,"not on a last place, we would like to improve the built valence prediction models and to collect more data for spanish, russian and farsi.",2,2013
P13-1067,in the future we are interested in studying the affect of metaphors for domains different than governance.,4,2013
P13-1067,"we want to conduct studies with the help of social sciences who would research whether the tagging of affect in metaphors depends on the political affiliation, age, gender or culture of the annotators.",5,2013
P13-1068,"from the tagging perspective, our future plans include testing the system on other highly inflectional languages such as czech and slovene and investigating different methods for automatically determining a more suitable custom network topology, such as genetic algorithms.",1,2013
P13-1071,"in future work, we aim to extend our quality flaw detection system to not only find articles that contain a particular flaw, but also to identify the flaws within the articles, which can be achieved by leveraging the positional information of in-line cleanup templates.",1,2013
P13-1072,we believe incorporating informal word normalization into the inference process may help address this important source of error.,2,2013
P13-1073,"finally, it would be interesting to see how our algorithm performs on other news domains.",1,2013
P13-1073,"in future work, we want to generate more diverse and intriguing questions by selecting relevant named entities for template instantiation that do not appear in the article.",1,2013
P13-1073,"another direction would be take a supervised approach, training classifiers over a labeled dataset for filtering irrelevant templates and incorrect instantiations.",1,2013
P13-1074,"in future work, we will try our method on other languages such as chinese and japanese, where treebank data is available.",2,2013
P13-1074,we would also like to test the mt performance over transcribed speech texts with punctuation symbols inserted based on our method proposed in this paper.,3,2013
P13-1075,"in the future, we will compare this method with self-training to better illustrate the importance of boundary information, and give error analysis on what types of errors are reduced by the method to make this investigation more complete.",1,2013
P13-1075,"we will also investigate more efficient algorithms to leverage more massive web text with natural annotations, and further extend the strategy to other nlp problems such as named entity recognition and parsing.",1,2013
P13-1078,we plan to explore more work on the additive neural networks in the future.,1,2013
P13-1078,"for example, we will train word embedding matrices for source and target languages from a larger corpus, and take into consideration the bilingual information, for instance, word alignment; the multi-layer neural network within the additive neural networks will be also investigated in addition to the single-layer neural network; and we will test our method on other translation tasks with larger training data as well.",2,2013
P13-1079,"furthermore, we will investigate any tradeoffs between the accuracy of the probability estimation and the coverage of phrase pairs.",3,2013
P13-1079,"in future work, we will also introduce incremental learning for phase pair extraction inside a domain, which means using the current translation probabilities already obtained as the base measure of sampling parameters for the upcoming domain.",1,2013
P13-1081,"we also applied the predicted ecs to a large-scale chinese-to english machine translation task and achieved significant improvement over two strong mt base lines, i.e. a hierarchical phase-based system and a tree-to-string syntax-based system.",4,2013
P13-1082,"future work could involve merging our translation model framework with the online adaptation of other models, or the log-linear weights.",4,2013
P13-1083,"our experiments showed that a more favorable pos tagset can be induced by integrating aligned information, and furthermore, the pos tagset generated by the proposed method is more effective for smt than an existing pos tagset (the ipa pos tagset).",1,2013
P13-1084,"as future work, we plan to incorporate the question structure (e.g., question topic and question focus (duan et al., 2008)) into the question representation for question retrieval.",1,2013
P13-1085,at this stage no further information flows from the scf and sp models to the clustering model.,1,2013
P13-1085,a natural extension of our unified framework is to construct a joint model in which the predictions for all three tasks inform each other at all stages of the prediction process.,1,2013
P13-1086,"the signals generated by this algorithm could improve the prediction of a financial time series model, such as ads (rydberg and shephard, 2003).",4,2013
P13-1086,"our future work will consider the contextual information for sentence selection, and an aggregation of weighted news content based on the decay effect over time for individual companies.",2,2013
P13-1086,we have presented a model for predicting stock price movement from news.,1,2013
P13-1086,it also facilitates human interpretable analysis to understand the relation between a company's market value and its business activities.,1,2013
P13-1089,"in this paper, we have described two approaches for amortizing inference costs over datasets.",1,2013
P13-1090,"as a future work, we will investigate using session data, namely the entire dialog between the human and the computer.",2,2013
P13-1090,"rather than using single turn utterances, we hope to utilize the context information, e.g., information from previous turns for improving the performance of the semantic tagging of the current turns.",2,2013
P13-1093,a further incremental increase is achieved by considering only transcriptions above a minimum length.,6,2013
P13-1093,"for instance, confidence can be approximated in mlps by the entropy across continuous-valued output nodes, and in rfs by the number of component decision trees that agree on a classification.",6,2013
P13-1093,"this phenomenon appears to be manifested in the current study by the extent to which classification increases generally across the 5-, 6-, and 7-yearold groups, as shown in table 5.",1,2013
P13-1094,"our experiments demonstrated that sentiment relevance and subjectivity are related, but different.",1,2013
P13-1096,"in future work, we plan to explore alternative multimodal fusion methods, such as decision-level and meta-level fusion, to improve the integration of the visual, acoustic, and linguistic modalities.",1,2013
P13-1098,"finally, in the application level, we aim at an indepth analysis of patterns and characteristics in the extracted sets of features by collaborating with domain experts (e.g., political analysts).",1,2013
P13-1098,future work may investigate further modelling improvements achieved by applying different regularization functions as well as the adaptation of the presented models to classification problems.,1,2013
P13-1098,"the application domain in this paper was politics, though the presented methods are generic and could be easily applied on various other domains, such as health or finance.",4,2013
P13-1099,"from a series of experiments, we found that there is little difference between the two ilp modules, and that the improved system performance is attributed to the fact that our proposed supervised bigram estimation module can successfully gather the important bigram and assign them appropriate weights.",1,2013
P13-1100,"this is an interesting extension of their previous submodular framework and while the new formulation permits more complex functions, the resulting function is still submodular and hence can be combined with the dispersion measures proposed in this paper.",4,2013
P13-1100,"a different body of work uses determinantal point processes (dpp) to model subset selection problems and adapt it for document summarization (kulesza and taskar, 2011).",1,2013
P13-1100,"in a very recent work, lin and bilmes (2012) demonstrate a further improvement in performance for document summarization by using mixtures of submodular shells.",1,2013
P13-1101,our future work will improve the local search algorithm to remove this restriction.,1,2013
P13-1103,the obtained treebank is then transformed into ccg derivations.,1,2013
P13-1104,"in the future, we will experiment with more advanced dependency representations (de marneffe and manning, 2008; choi and palmer, 2012b) to show robustness of our approach.",1,2013
P13-1105,"in the future work we will investigate such kind of strategies, such as bilingually unsupervised induction.",1,2013
P13-1107,"our ongoing work includes identifying candidate morphs from scratch, as well as discovering morphs for a given target based on anomaly analysis and textual coherence modeling.",1,2013
P13-1107,both of the meta-path based and social correlation based semantic similarity measurements are proven powerful and complementary.,1,2013
P13-1109,"in this scenario, we also can look for paraphrases and translations for phrases containing oovs and add them to the phrase-table as new translations along with the translations for unigram oovs.",2,2013
P13-1110,we plan to investigate its utility elsewhere in nlp (e.g. for parsing) as well as in other domains involving high-dimensional structured prediction.,4,2013
P13-1111,"in this paper, we focus on the problem of ambiguities for pass.",5,2013
P13-1112,"based on the resulting trees, we have then hypothesized that the following relation holds in mother tongue interference: interfamily distance &gt; non-nativeness &gt; intrafamily distance.",1,2013
P13-1114,"although the work presented here established that more than word-to-word normalization was necessary to produce parser-ready normalizations, it remains unclear which specific normalization tasks are most critical to parser performance.",5,2013
P13-1114,the proposed framework builds a statistical model over a series of replacement generators.,1,2013
P13-1114,this work presents a framework for normalization with an eye towards domain adaptation.,1,2013
P13-1119,our setup is entirely reproducible: we have built a static web search environment consisting of a search engine along with a means to browse a large corpus of web pages as if it were the 鈥渞eal鈥 web.,1,2013
P13-1123,"this study will contain additional evaluation categories, such as the understandability or informativeness of system utterances.",3,2013
P13-1123,"finally, we would like to explore methods for unsupervised data labelling so as to facilitate portability across domains further.",1,2013
P13-1123,we have presented a novel technique for surface realisation that treats generation as a sequence labelling task by combining a crf with tree-based semantic representations.,1,2013
P13-1123,"in addition, we may compare different sequence labelling algorithms for surface realisation (nguyen and guo, 2007) or segmented crfs (sarawagi and cohen, 2005) and apply our method to more complex surface realisation domains such as text generation or summarisation.",3,2013
P13-1123,"in a human rating study, we confirmed that judges rated our output as better phrased, more natural and less repetitive than systems that just take local features into account.",3,2013
P13-1125,"we hope to be able to learn lexical information such as how many arguments a verb takes, what nouns are potential subjects for a given verb by gathering statistics from an english parser and projecting to the source language via our word/phrase translation table.",1,2013
P13-1127,"programmers read the specifications, then develop source code that parses inputs in the format.",2,2013
P13-1129,"a joint approach to resolving speaker attribution, relationship extraction, co-reference resolution, and alias-to-character mapping would not only improve the accuracy on all these tasks, but also represent a step towards deeper understanding of complex plots and stories.",1,2013
P13-1130,hbms have been successfully used for a number of language acquisition tasks capturing both patterns of under- and overgeneralization found in child language acquisition.,4,2013
P13-1131,a natural extension of our work would be to extend our two level model to accommodate context sensitive lexical similarity.,1,2013
P13-1131,they learn an lda model on the source language side of the training corpus with the purpose of identifying implicit sub-domains.,2,2013
P13-1131,"finally, they train a classifier to translate a given target word based on these tables and the inferred topic distribution of the given document in which the target word appears.",1,2013
P13-1138,the development time to adapt our system to new domains is small compared to other nlg systems; around a week to adapt the system to weather and biography domains.,4,2013
P13-1139,this is the first time a concrete answer to this question has been provided.,5,2013
P13-1142,"to demonstrate this point, we carried out an evaluation on a creative sentence generation benchmark showing that brainsup can effectively produce catchy, memorable and successful sentences that have the potential to inspire the work of copywriters.",4,2013
P13-1142,"we have presented brainsup, a novel system for creative sentence generation that allows users to control many aspects of the creativity process, from the presence of specific target words in the output, to the selection of a target domain, and to the injection of phonetic and semantic properties in the generated sentences.",1,2013
P13-1142,"the system has been designed as a supporting tool for a variety of real-world applications, from advertisement to entertainment and education, where at the very least it can be a valuable support for time-consuming and knowledgeintensive sentence generation needs.",4,2013
P13-1142,"to our best knowledge, this is the first systematic attempt to build an extensible framework that allows for multi-dimensional creativity while at the same time relying on syntactic constraints to enforce grammaticality.",1,2013
P13-1143,experiments on the hoo 2011 shared task show that ilp inference achieves state-of-the-art performance on grammatical error correction.,1,2013
P13-1147,we proposed syntactic tree kernels enriched by lexical semantic similarity to tackle the portability of a relation extractor to different domains.,1,2013
P13-1148,"to conclude, we presented a model that provides a clean framework to test the usefulness of different factors for word segmentation and handling phonological variation in a controlled manner.",1,2013
P13-1149,"we would also like to apply composition to inflectional morphology (that currently lies outside the scope of distributional semantics), to capture the nuances of meaning that, for example, distinguish singular and plural nouns (consider, e.g., the difference between the mass singular tea and the plural teas, which coerces the noun into a count interpretation (katz and zamparelli, 2012)).",4,2013
P13-1149,"finally, in our current setup we focus on a single composition step, e.g., we derive the meaning of inoperable by composing the morphemes in- and operable.",1,2013
P13-1151,"for other simplification tasks, the optimal parameters will need to be investigated.",1,2013
P13-1151,these improvements are achieved over a simple-only model that uses all simple english data currently available in this domain.,1,2013
P13-1151,"for both tasks, the best improvements were seen when using language model adaptation techniques, however, the adaptation results also indicated that the role of normal data is partially task dependent.",2,2013
P13-1151,"on the perplexity task, the best results were achieved with an equal weighting between the simple-only and normal-only model.",1,2013
P13-1151,"for example, on the lexical simplification task, when using a linearly interpolated model, the model combining 100k simple sentences with all the normal data achieved comparable results to the model combining all the simple sentences with all the normal data.",1,2013
P13-1153,transliteration mining requires limited amounts of training examples.,2,2013
P13-1153,"we believe that the proposed cross-lingual features can be used to help ner for other languages, particularly languages that lack good features that generalize well.",4,2013
P13-1155,this shows a characteristic of the proposed approach; it is very conservative in proposing normalization which is desirable as a preprocessing step for nlp applications.,1,2013
P13-1156,we have presented a technique to combine phrasebased features and tree-based features into one model.,1,2013
P13-1157,"we plan to extend our method to detect machine-translated sentences produced by different mt systems, e.g., a rule-based system, and develop a unified framework for cleaning various types of noise in web-mined data.",1,2013
P13-1157,"in addition, we will investigate the effect of source and target languages on translation in terms of mt detection.",1,2013
P13-1157,"therefore, we expect that our method is basically effective on different language pairs.",2,2013
P13-1157,"as lopez (2008) describes, a phrase-salad is a common phenomenon that characterizes current smt results.",5,2013
P13-1158,"second, we would like to generalize the question understanding framework to produce more complex queries, constructed within a compositional semantic framework, but without sacrificing scalability.",4,2013
P13-1158,"using only a seed lexicon, the approach automatically learns a lexicon and linear ranking function that demonstrated high accuracy on a held-out evaluation set.",1,2013
P13-1159,we are planning to deploy our system and release model files of the classifiers to assist relief efforts in future crisis scenarios.,4,2013
P13-1159,"through a series of experiments, we demonstrated that the performance of the problem-aid matching can be improved with the usage of semantic orientation of excitation polarities, proposed in (hashimoto , 2012), and trouble expressions.",1,2013
P13-1162,our study of bias in wikipedia has implications for linguistic theory and computational linguistics.,4,2013
P13-1166,systematic testing can provide an indication of this variation.,4,2013
P13-1167,"for all evaluations, however, a justification for the biased/unbiased metrics used should be given, and more than one metric should be reported so as to allow a reader to ascertain for themselves whether a particular automatic segmenter鈥檚 bias in some manner is cause for concern or not.",3,2013
P13-1168,our future work in this area would specifically target understanding and formalization of the theoretical model underpinning a query.,1,2013
P13-1169,"with the manually labeled data set, we first predict the deceptive answers with traditional classification method.",1,2013
P13-1171,"second, because the task of answer sentence selection is very similar to paraphrase detection (dolan , 2004) and recognizing textual entailment (dagan , 2006), we would like to investigate whether systems for these tasks can be improved by incorporating enhanced lexical semantic knowledge as well.",1,2013
P13-1172,"other semantic relations, such as the topical associations between opinion targets (or opinion words) should also be employed.",1,2013
P13-1172,we believe that considering multiple semantic associations will help to improve the performance.,1,2013
P13-1174,"we presented a broad-coverage connotation lexicon that determines the subtle nuanced sentiment of even those words that are objective on the surface, including the general connotation of realworld named entities.",1,2013
P13-1174,"via a comprehensive evaluation, we provided empirical insights into three different types of induction algorithms, and proposed one with good precision, coverage, and efficiency.",1,2013
P13-2004,experiments showed that dc-admm drastically reduced model complexity in terms of the degrees of freedom in trained models while maintaining the performance.,1,2013
P13-2004,"this paper also introduced a feasible algorithm, dcadmm, which can vanish the infeasible combinatorial optimization part from the entire learning algorithm with the help of the admm technique.",1,2013
P13-2010,"however, to scale up compositionally beyond the simplest constructions, cdsms must deal with grammatical terms such as determiners.",1,2013
P13-2010,"thus, a top priority in future work is to explore different contextual features, such as adverbs and grammatical terms, that might carry information that is more directly relevant to the semantics of determiners.",1,2013
P13-2010,theoretical considerations would lead one to expect a 鈥渇unctional鈥 approach to determiner representations along the lines of baroni and zamparelli (2010) and coecke (2010) to outperform those approaches that combine vectors separately representing determiners and nouns.,1,2013
P13-2011,we perform uncertainty identification experiments on the generated dataset to explore the effectiveness of different types of features.,1,2013
P13-2014,"future work will explore sampling based approaches to belief update and decision making (doshi and gmytrasiewicz, 2009) to overcome these problems.",1,2013
P13-2014,we showed that implicatures arise in cooperative contexts from nested belief models.,1,2013
P13-2017,"finally, this data is available on an open source repository in the hope that the community will commit new data and make corrections to existing annotations.",2,2013
P13-2017,"it will also allow the inclusion of language-specific functional or morphological markers (case markers, topic markers, classifiers, etc.)at the leaves of the tree, where they can easily be ignored in applications that require a uniform cross-lingual representation.",1,2013
P13-2025,"we showed, on two corpora, that knowing multiple scores for each example instead of a single score results in a more reliable estimation of the quality of a nlp system.",1,2013
P13-2032,in this paper we have presented an effective yet simple approach to chinese word segmentation on micro-blog texts.,1,2013
P13-2040,"by sharing parameters across different groundings, we should be able to identify semantic neighborhoods with fewer training instances.",1,2013
P13-2041,"we have presented a fully unsupervised humor generation system for generating jokes of the type i like my relationships like i like my source, open i like my coffee like i like my war, cold i like my boys like i like my sectors, bad i like my x like i like my y, z, where x, y, and z are slots to be filled in.",1,2013
P13-2042,"evaluated on the wsj corpus, the proposed td and to models reduced the bigram鈥檚 and trigram鈥檚 perplexities up to 23.5% and 14.0%, respectively.",1,2013
P13-2044,"furthermore, the possible presence of crowd-working scammers (only partially filtered by the gold standard questions) could have reduced the statistical power of our analysis.",5,2013
P13-2044,"finally, the adopted humor generation task (based on a single word substitution) is extremely simple and the constraints might have not been sufficiently capable to produce a detectable increase of humor appreciation.",5,2013
P13-2044,"the statistical significance is particularly high, even though there were several limitations in the experimental setting.",5,2013
P13-2044,the statistically strong results that we obtained can make this evaluation approach attractive for related tasks.,3,2013
P13-2044,"in our methodology, we focused attention to the correlation between the parameters of the system (in our case, the constraints used in lexical selection) and the performance of humor generation.",1,2013
P13-2044,this would offer a novel way to intentionally control the humorous effect.,1,2013
P13-2045,"tables 4 and 5 show the results of the various methods on the cartoon captions and crossword clues datasets, respectively.",2,2013
P13-2045,"on the crossword clues datasets, the random-walk-based methods are clearly superior to the other methods tested, whereas simple clustering is more effective on the in some sense, the two datasets in this paper both represent difficult domains, ones in which authors are intentionally obscure.",1,2013
P13-2045,the good results acheived on the crossword clues dataset indicate that this obscurity can be overcome when discourse units are short.,5,2013
P13-2046,"in this paper, we introduced our machine learning based approach for identifying lvcs in hungarian and english free texts.",1,2013
P13-2047,to this end we will re-use human evaluation data gathered within the 2013 campaign.,2,2013
P13-2047,we will also address the problem of tailoring automatic evaluation measures to russian accounting for complex morphology and free word order.,5,2013
P13-2052,"in future, we will revise the equivalence measure and also experiment with clustering algorithms such as (beeferman et al., 2000).",1,2013
P13-2052,we will also study the contribution of individual components of the measure in such task.,6,2013
P13-2054,"in future, we plan to first develop stemming algorithms for both sorani and kurmanji and then leverage those algorithms to examine the lexical differences between the two dialects.",1,2013
P13-2055,"as igt data is available for hundreds of languages through the odin database and other sources, one could produce a small parallel treebank for a language pair after spending a few hours manually correcting the output of a projection algorithm.",2,2013
P13-2057,maybe there are wrong translations with high probability which the language model cannot remove them from the best translations.,5,2013
P13-2058,the key of our approach is a translation-based high-quality retrieval model which gradually adapts to the target domain by iteratively re-training the underlying smt model on a few thousand parallel sentences retrieved in the step before.,1,2013
P13-2059,"propositions can also be changed, or even induced, by existing written sign representation languages such as zebedee (filhol, 2008) or hamnosys (hanke, 2004), mainly for the sake of extend ability.",2,2013
P13-2059,"for verification and model extraction, further optimizations are expected, including the handling of data inconsistencies and repairing broken queries when verifying the graph.",1,2013
P13-2059,"from the application side, we still need to create an extensive sign database codified in pdlsl and try recognition on other corpora, with different tracking information.",1,2013
P13-2059,"the traits we have chosen to represent were imposed by the limits of the tracking tools we had to our disposition, most notably working with 2d coordinates.",1,2013
P13-2062,extending the study to different language pairs and studying the applicability of this technique for machine translation quality estimation are also on the agenda.,2,2013
P13-2063,"we presented a novel sequence labelling based, context-sensitive pruning method for a string-totree mt model.",1,2013
P13-2063,our method achieves more than 60% speed-up over a state-of-the-art baseline on a full-scale translation task.,4,2013
P13-2066,"in this paper, we present an rst-based translation framework for modeling semantic structures in translation model, so as to maintain the semantically functional integrity and hierarchical relations of edus during translating.",1,2013
P13-2067,"we presented the first ever results to demonstrate that tuning an smt system against meant produces much adequate translation than tuning against bleu or ter, as measured across all other commonly used metrics and human subjective evaluation.",3,2013
P13-2071,our modification to the osm model produces the best results giving significant improvements in most cases.,1,2013
P13-2071,we have addressed the problem of the independence assumption in pbsmt by integrating ngram-based models inside a phrase-based system using a log-linear framework.,1,2013
P13-2071,"although our modifications to the osm model enables discontinuous mtus, we did not fully utilize these during decoding, as moses only uses continous phrases.",1,2013
P13-2072,other flavors of partial entailment should be investigated as well.,1,2013
P13-2073,"in the future, we plan to explore other features, e.g., the number of the pivot phases used in connecting the source and target phrase pair and the similarity between these pivot phrases.",4,2013
P13-2078,we compared the kullback-leibler divergence to a simple self-information measure.,3,2013
P13-2079,"besides, learning-based approaches to extract phenomena and multi-class te recognition will be explored in the future.",1,2013
P13-2080,"future research may add additional features and more complex feature combination methods, such as weighted sums tuned by machine learning.",1,2013
P13-2082,"the learning process can still utilize the transferred knowledge, as it provides scaffolding for the latent learning process, resulting in a significant improvement in performance.",1,2013
P13-2087,"while we have shown promising results in a single task, we believe that the method is general enough to be applied to a range of supervised tasks and source embeddings.",4,2013
P13-2088,"furthermore, we also plan to explore using arabic-specific and more powerful features.",2,2013
P13-2092,"future work will include building a system able to perform the task we have defined, as well as extending this work to include indirect quotes.",1,2013
P13-2095,the aim of this work was to revisit these tasks as classical supervised learning problems that usually carry to high accuracy levels with high performance when faced with standard machine learning techniques.,5,2013
P13-2095,"we presented an approach to reveal definitions and extract underlying hypernym relations from plain text, making use of local syntactic information fed into a support vector machine classifier.",1,2013
P13-2098,we presented a set of experiments on incorporating features into an existing ocr system via nbest list reranking.,3,2013
P13-2100,"we presented an ilp model for nlg that jointly considers the choices in content selection, lexicalization, and aggregation to avoid greedy local decisions and produce more compact texts.",1,2013
P13-2102,"in this paper, we described a pipeline for the generation of scientific surveys starting from a topic query.",1,2013
P13-2102,one of the main contributions of this work is a manually annotated data set for evaluating both the tasks.,2,2013
P13-2103,we further intend to use this scheme and computational frameworks to serve a wide cross-parser investigation on inferring functional structures across languages.,2,2013
P13-2109,"we decoded with ad3, an accelerated dual decomposition algorithm which we adapted to handle large components, including specialized head automata for the third-order features, and a sequence model for head bigrams.",1,2013
P13-2109,we presented new third-order non-projective parsers which are both fast and accurate.,1,2013
P13-2112,"we have proposed a method for unsupervised pos tagging that performs on par with the current stateof-the-art (das and petrov, 2011), but is substantially less-sophisticated (specifically not requiring convex optimization or a feature-based hmm).",1,2013
P13-2114,identifying further explicit sources of temporal information applicable to new sets of relations may reveal promising paths for investigation.,2,2013
P13-2116,our ongoing work is to apply these ideas to a much larger corpus from each of the three domains.,2,2013
P13-2120,"in particular, we have observed that there is a huge room for improvements in the aggregation step.",1,2013
P13-2127,we will consider a sense-assignment method (voting or mace) as more appropriate if it provides the sense tags that are easiest to learn by our wsd system.,1,2013
P13-2128,the only information strictly necessary for the methods we propose is a grouping of lemmas into derivationally related classes.,1,2013
P13-2128,the estimation of generic semantic similarity can profit more from derivational smoothing than the induction of specific lexical relations.,1,2013
P13-2129,we have demonstrated the merits of using das for verb clustering compared to the scf data from which they are derived on standard verb classification datasets and when integrated in a stateof-the-art verb clustering system.,2,2013
P13-2133,this method disambiguates polysemous words in context vectors by selecting only the most relevant translations.,1,2013
P13-2133,five semantic similarity and relatedness measures were used for this purpose.,1,2013
P13-2134,gains from fixing resources may sometimes even exceed what the best possible algorithmic improvements can provide.,6,2013
P13-2137,"we have described the construction of dm. hr, a syntax-based distributional memory for croatian built from a dependency-parsed web corpus.",1,2013
P13-2142,we examined the under-studied task of stance classification of ideological debates.,3,2013
P13-2153,"our findings show that the low- and highexposure groups produced similar numbers of words, but the high-exposure group tended to produce longer sequences of phonetically similar words.",1,2013
P13-3003,"with regard to interactions with others, the prominence of different individuals and associated affect differed depending on condition.",5,2013
P13-3003,previous research has found that fibromyalgia patients report a lack of understanding from medical practitioners and others around them (e.g.,5,2013
P13-3004,"in this paper, we presented a mildly supervised method for identifying metaphorical verb usage by taking the local context into account.",1,2013
P13-3005,as future work we would like to run more experiments with predicted supertags.,1,2013
P13-3005,"ptb tags and supertags are complementary, and for all three parsers we observe slight benefits from being supplied with both types of tags.",1,2013
P13-3005,"in the absence of a specialized supertagger, we can follow the pipeline of (ytrestl, 2011) who reached the stateof-the-art supertagging accuracy of 95%.",1,2013
P13-3006,"the best performing classifier is crf-based and combines lexical, syntactical and semantical features in order to obtain an f-score of 79.35%.",1,2013
P13-3009,information implicitly stated in the sentence should be stated explicitly.,6,2013
P13-3011,"moreover, methods for facilitating annotated corpus construction will be explored, potentially adding new knowledge to the science of annotation.",1,2013
P13-3015,low precision is a constant factor in all techniques and future research should aim to address this.,5,2013
P13-3016,we hope to address these topics in the future.,6,2013
P13-3017,"that is because, when ambiguity increases, the baseline method inaccurately assigns one pos tag to word types.",1,2013
P13-3017,"on the other hand, the gap statistic method is not fully efficient in guessing the number of clusters.",1,2013
P13-3018,"in the next phase of our work we will extend the existing experiments and also apply some more techniques like, crowd sourcing and language games to collect more relevant rt and compositionality data.",1,2013
P13-3019,it will be determined how much are the segment level evaluation results influenced by these ranking orders.,3,2013
P13-3019,"the accuracy of cesm can be further increased by the use of paraphrases, which can be obtained by using a german thesaurus or a lexical resource like germanet (hamp and feldweg, 1997).",2,2013
P13-3023,"the results from the evaluation on six different languages from the conll 2009 data sets indicate that the system is able to learn most morphological rules correctly and is able to cope with previously unseen input, performing significantly better than a dictionary learned from the same amount of data.",1,2013
P13-3024,future work is needed to assess the extent to which the sdt-p measure and its word-level variant provide a general framework for dsms evaluation without external resources.,3,2013
P13-3024,"such a word-specific measure could assess the semantic stability of different parts of the lexicon such as concrete vs. abstract word categories, or the distribution properties of different linguistic categories (verb, adjectives, ..).",3,2013
P13-4001,"besides, webanno supports project definition, import/export of tag and tagsets.",1,2013
P13-4004,a beta version is currently open to researchers for experimentation.,3,2013
P13-4005,"in this demo paper, we presented a system that links mainstream media stories to tweets that comment on the events covered.",1,2013
P13-4005,"the system retrieves relevant tweets, extracts the links they contain and subsequently performs sentiment analysis.",1,2013
P13-4006,"in particular, several distributional models of word meaning in context share important similarities with composition models, and we plan to add them to dissect.",1,2013
P13-4007,"the framework is under active development, with work on several new features planned or in progress.",1,2013
P13-4009,"besides, we will also optimize the algorithms and codes to improve the system performances.",1,2013
P13-4010,"in the future, specialized plugins are planned to work with different linguistic annotations, e.g. cross-sentence annotations as used to annotate coreference chains.",2,2013
P13-4011,edgar has been tested with real users for the last year and we are currently performing a detailed evaluation of it.,3,2013
P13-4012,"in this paper, we introduce a simple chatterbot for answering non-obstructive psychological questions.",1,2013
P13-4016,"first, we plan to support advanced rule extraction techniques, such as fuller support for count regularization and forest-based rule extraction (mi and huang, 2008), and using the em algorithm to choose attachments for null-aligned words (galley , 2006) or the direction of rule binarization (wang , 2007).",1,2013
P13-4023,"hence, we also provide hyena-live as a json compliant entity classification web-service.",3,2013
P13-4023,these nodes can be further expanded in order to check which sub-classes have been accepted or rejected by hyena-live.,1,2013
P13-4025,"in the future, we plan to use this tool for analyzing the nature of pivoted paraphrases.",4,2013
P13-4029,our initial evaluations have shown encouraging results and further evaluations are now planned.,3,2013
P13-4030,our evaluation on small scale data shows that the multistage approach performs better than complete sentence translation.,1,2013
P13-4032,"in the future, we would like to add automatic localization strategies, new aggregation functions and a completely new package for fusing image- and text-based representations.",1,2013
P13-4033,we expect that the availability of a document-level decoder will make it substantially easier to leverage discourse information in smt and make smt models explore new ground beyond the next sentence boundary.,1,2013
P14-1001,"while we aimed for an exhaustive study, including multiple on-learning algorithms, different conversions to batch and derandomizations, we are aware that the problem we studied is very rich and admits many more facets and scenarios that we plan to investigate in the future.",5,2014
P14-1006,further experiments and analysis support our hypothesis that bilingual signals are a useful tool for learning distributed representations by enabling models to abstract away from mono-lingual surface realizations into a deeper semantic space.,1,2014
P14-1007,"it would also be interesting to try a task-specific adaptation of the erg parse ranking model, for example retraining on the pre-existing treebanks but giving preference to analyses that lead to correct crawler results downstream.",4,2014
P14-1007,"in future work, we will seek to better understand the division of labor between the systems involved through contrastive error analysis and possibly another oracle experiment, constructing gold-standard mrss for part of the data.",1,2014
P14-1008,other directions of our future work include further exploitation of the new semantic representation.,1,2014
P14-1008,"for example, since abstract denotations are readily suited for data querying, they can be used to verify newly generated assumptions by fact search in a database.",3,2014
P14-1008,this may open a way towards a hybrid approach to rte wherein logical inference is intermingled with large scale database querying.,4,2014
P14-1008,"as such, our current rte system is at a proofof-concept stage, in that many of the above techniques are yet to be implemented.",1,2014
P14-1010,"in the future, we plan to explore introducing multiple segmentation options into the lattice, and the application of our method to a full morphological analysis (as opposed to segmentation) of the target language.",4,2014
P14-1010,"eventually, we would like to replace the functionality of factored translation models (koehn and hoang, 2007) with lattice transformation and augmentation.",4,2014
P14-1011,"in the future work, we will explore four directions.1) we will try to model the decoding process with dnn based on our semantic embeddings of the basic translation units.",1,2014
P14-1013,"since the translation of the current sentence is usually influenced by the topic of previous sentences, we plan to leverage recurrent neural networks to model this phenomenon, where the history translation information is naturally combined in the model.",1,2014
P14-1013,"in the future research, we will extend our neural network methods to address document-level translation, where topic transition between sentences is a crucial problem to be solved.",4,2014
P14-1014,"for future work, we would like to investigate the two-phase approach to more challenging tasks, such as web domain syntactic parsing.",1,2014
P14-1016,"another direction involves incorporating richer feature space for better inference performance, such as multi-media sources (i.e. pictures and video).",2,2014
P14-1016,"one direction of our future work involves exploring more general categories of user profile at tributes, such as interested books, movies, hometown, religion and so on.",2,2014
P14-1017,"moreover, understanding the underlying psychological and cultural mechanisms that establish the effectiveness of these features is a fundamental problem of interest.",1,2014
P14-1017,"in future work, it will be interesting to examine how these features generalize to longer and more extensive arguments.",1,2014
P14-1018,"in addition, we aim to experiment with neighborhood-specific classifiers applied towards the tweets from neighborhood-specific streams e.g., friend classifier used for friend tweets, retweet classifier applied to retweet tweets etc.",4,2014
P14-1018,"in future work, we plan to incorporate iterative model updates from newly classified communications similar to online perceptron-style updates.",1,2014
P14-1022,"moreover, we show that our parser is adaptable to other tree-structured tasks such as sentiment analysis; we outperform the recent system of socher (2013) and obtain state of the art performance on their dataset.",4,2014
P14-1023,"to give just one last example, distributional semanticists have looked at whether certain properties of vectors reflect semantic relations in the expected way: e.g., whether the vectors of hypernyms 鈥渄istributionally include鈥 the vectors of hyponyms in some mathematical precise sense.",1,2014
P14-1025,using a newly gathered corpus we measured the effects of various forms of weak supervision on performance.,2,2014
P14-1029,"we further make the models to be dependent on the text being modified by negators, through adaptation of a state-of the-art recursive neural network to incorporate the syntax and semantics of the arguments; we discover this further reduces fitting errors.",1,2014
P14-1029,"the detailed analysis reveals the differences in the behavior among negators, and we argue that they should always be modeled separately.",1,2014
P14-1032,"in future work, we plan to extend the proposed method to jointly mine product features along with customers' opinions on them.",1,2014
P14-1035,our method establishes the possibility of representing the relationship between character and narrative form in a hierarchical bayesian model.,4,2014
P14-1035,"it is also worth noting that the models tested above diverge from many structuralist theories of narrative (propp, 1998) by allowing multiple instances of the same persona in a single work.",1,2014
P14-1035,"learning structural limitations on the number of “protagonists” likely to coexist in a single story, for example, may be another fruitful area to explore.",1,2014
P14-1035,"in all cases, the machinery of hierarchical models gives us the flexibility to incorporate such effects at will, while also being explicit about the theoretical assumptions that attend them.",1,2014
P14-1037,"in our case, we only have the natural language query, which presents the more difficult problem of associating the entity class in the query (e.g., hiking trails) to concrete entities (e.g., avalon super loop).",5,2014
P14-1038,"for the first time, we addressed this challenging task by an incremental beam-search algorithm in conjunction with structured perceptron.",1,2014
P14-1039,we leave these application-centered issues for investigation in future work.,4,2014
P14-1040,"in the future, we plan to remedy this using a ranking approach such as proposed in (velldal and oepen, 2006; white and rajkumar, 2009).",1,2014
P14-1041,"in the future, we would like to investigate how our framework deals with such discourse level simplifications i.e., simplifications which involves manipulation of the coreference and of the discourse structure.",5,2014
P14-1041,"as argued by siddharthan (2006), correctly capturing the interactions between these phenomena is essential to ensuring text cohesion.",1,2014
P14-1042,detailed analysis reveals some important directions for future investigation.,6,2014
P14-1043,"for future work, among other possible extensions, we would like to see how our approach performs when employing more diverse parsers to compose the parse forest of higher quality for the unlabeled data, such as the easyfirst non-directional dependency parser (goldberg and elhadad, 2010) and other constituent parsers (collins and koo, 2005; charniak and johnson, 2005; finkel , 2008).",1,2014
P14-1047,we conclude that multiagent rl of dialogue policies is a promising alternative to using single-agent rl and sus or learning directly from corpora.,1,2014
P14-1047,"furthermore, we intend to apply multi-agent rl to more complex negotiation domains, e.g., experiment with more than two types of resources (not just apples and oranges) and more types of actions (not just offers and acceptances).",4,2014
P14-1047,the advantage of this approach is that it does not require sus to train against or corpora to learn from.,1,2014
P14-1047,two agents interacted with each other and both learned at the same time.,1,2014
P14-1048,"last but not least, as reflected by the low mafs in our experiments, some particularly difficult relation types might need specifically designed features for better recognition.",1,2014
P14-1048,"in this paper, we presented an efficient text-level discourse parser with time complexity linear in the total number of sentences in the document.",1,2014
P14-1049,"in future work, we will focus on exploring more contextual discourse information via the graph model and better ways of integrating intra and inter-sentence information on negation focus identification.",1,2014
P14-1049,"in this graph model, the relatedness between words is calculated by word co-occurrence, wordnetbased similarity, and topic-driven similarity.",1,2014
P14-1049,evaluation on the *sem 2012 shared task corpus indicates the usefulness of contextual discourse information on negation focus identification and our graph model in capturing such global information.,3,2014
P14-1050,"in order to extract new sentiment words from large-scale user-generated content, this paper proposes a fully unsupervised, purely data-driven, and weibo post with/without new sentiment words.",1,2014
P14-1050,"from linguistic perspectives, our framework is capable to extract adjective new words because the lexical patterns usually modify adjective words.",1,2014
P14-1052,"struct formalizes the generation problem as an mdp and applies a version of the uct algorithm, a fast online mdp planner, to solve it.",1,2014
P14-1053,"the next step would be to integrate bitext alignment across texts in two natural languages, inevitably introducing another stochastic component into the pipeline.",4,2014
P14-1053,"for generating effecting l2 content, it is important that the user be kept in a one of proximal development and a tight region where the level of the taught content is at just the right difficulty.",1,2014
P14-1058,we evaluated the proposed method in two domain adaptation tasks: cross-domain pos tagging and crossdomain sentiment classification.,3,2014
P14-1068,"in the future, we would like to apply our model to other tasks, such as image and text retrieval (hodosh et al., 2013; socher et al., 2013b), zero-shot learning (socher et al., 2013a), and word learning (yu and ballard, 2007).",4,2014
P14-1070,"we experimented strategies to predict both mwe analysis and dependency structure, and tested them on the dependency version of french treebank (abeill麓e and barrier, 2004), as instantiated in the spmrl shared task (seddah , 2013).",3,2014
P14-1071,this paper presented a novel framework called error case frames for correcting preposition errors with feedback messages.,1,2014
P14-1072,we have demonstrated the effectiveness of our method in two such areas and showed significant improvements in both.,1,2014
P14-1073,"our primary contribution consists of new modeling ideas, and associated inference techniques, for the problem of cross-document coreference resolution.",1,2014
P14-1076,"because of the practical importance of domain adaptation for relation extraction due to lack of labeled data in new domains, we hope our study and findings will lead to further investigations into this problem.",5,2014
P14-1076,experimental results on ace 2004 and yago have shown that the our domain adaptation method achieves the best performance on f1 measure compared with the other baselines when only few labeled target instances are used.,1,2014
P14-1080,our contributions can be summarized as: 1) the new translation rules are more discriminative and sensitive to cohesive information by converting the source string into a css-based taggedflattened string; 2) the new additional features embedded in the log-linear model can encourage the decoder to produce transitional expressions.,1,2014
P14-1084,"in this paper, we describe a memory-based approach in which we use a corpus of past news to learn valid syntactic sentence structures.",1,2014
P14-1091,"any method that can generate answers to questions, such as the web-based qa approach, can be integrated into this framework, by using them in the question translation component.",1,2014
P14-1091,"as we discussed in the experiment part, how to mine high-quality question patterns is worth further study for the qa task; (ii) we plan to integrate an ad-hoc ner into our kb-qa system to alleviate the entity detection issue; (iii) in fact, our proposed qa framework can be generalized to other intelligence besides knowledge bases as well.",5,2014
P14-1100,"empirically, our algorithm performs favorably to the ccm of klein and manning (2002) without the need for careful initialization.",1,2014
P14-1102,"in future, it would be interesting to incorporate lexicalization into the model presented in this paper, as this feature seems likely to bridge the gap between this model and babysrl in transitive settings.",1,2014
P14-1102,this fact suggests that the findings of gagliardi and lidz (2010) may be partially explained by a frequency effect: perhaps the input to children is simply biased such that wh-relatives are much more common than that-relatives (as shown in table 5).,5,2014
P14-1102,"this model also initially reflects the 1-1 role bias observed in children (gertner and fisher, 2012) as well as previous models (connor , 2008; connor , 2009; connor , 2010) without sacrificing accuracy in canonical intransitive settings.",1,2014
P14-1102,a lexical model could potentially pick up on clues which could indicate when that is a relativizer or simply improve on its comprehension of wh-relatives even more.,1,2014
P14-1102,it is interesting to note that the cuurent model does not make use of that as a cue at all and yet is still slower at acquiring that-relatives than wh-relatives.,1,2014
P14-1105,"we use this approach to create a new dataset from the ibc, which is labeled at both the sentence and phrase level.",2,2014
P14-1112,"poor entity mention detection is a major source of error in both cases, suggesting that future work should consider integrating entity linking with joint syntactic and semantic parsing.",1,2014
P14-1112,"by including the hypernym relation constraints while training word embeddings, we expect to improve the embeddings such that they become more suitable for this task.",1,2014
P14-1114,"third, we try to improve our generated summary by resolving coreferences and incorporating speaker information (e.g., names) in the clustering and sentence generation phases.",1,2014
P14-1114,"first, we are trying to improve our model by incorporating conversational features (e.g., speech acts).",1,2014
P14-1114,"second, we aim at implementing a strategy to order the clusters for generating more coherent abstracts.",1,2014
P14-1122,"the first game, infection, validates concept-concept relations, and the second, the knowledge towers, validates image-concept relations.",3,2014
P14-1122,"all annotated resources, demos of the games, and a live version of the top-ranking items for each concept are currently available online.5 in the future we will apply our video games to the validation of more data, such as the new wikipedia bitaxonomy (flati , 2014).",4,2014
P14-1122,"in experiments involving online players, we demonstrate three contributions.",3,2014
P14-1122,"first, games were released in two conditions whereby players either saw financial incentives for playing or a personal satisfaction incentive where they were thanked by us.",6,2014
P14-1130,future work involves extending the tensor component to capture higher-order structures.,2,2014
P14-1130,our parser outperforms the turbo and mst parsers across 14 languages.,1,2014
P14-1130,we implement the approach on first-order to third-order dependency parsing.,1,2014
P14-1131,"we also presented extensions of cosimrank for a number of applications, thus demonstrating the flexibility of cosimrank as a similarity measure.",1,2014
P14-1136,"finally, we presented results on propbank-style semantic role labeling with a system that included the task of automatic verb frame identification, in tune with the framenet literature; we believe that such a system produces more interpretable output, both from the perspective of human understanding as well as downstream applications, than pipelines that are oblivious to the verb frame, only focusing on argument analysis.",1,2014
P14-1138,"we also plan to enrich each hidden layer in our model with multiple layers following the success of yang et al.(2013), in which multiple hidden layers improved the performance of the ffnn-based model.",1,2014
P14-1138,"in future, we plan to employ contexts composed of surrounding words (e.g., c(fj) or c(eaj) in the ffnn-based model) in our model, even though our model implicitly encodes such contexts in the alignment history.",1,2014
P14-1139,the algorithm increases the number of exact solution found on the model of denero and macherey (2011) from 6% to 86%.,1,2014
P14-1146,these methods typically only model the context information of words so that they cannot distinguish words with similar context but opposite sentiment polarity (e.g.good and bad).,1,2014
P14-2002,"with these results in hand, future work may now consider the automatic construction of a properly balanced text collection, such as originally desired by the creators of the brown corpus.",2,2014
P14-2005,"the proposed blanc is free from this assumption, and we have shown that it subsumes the original blancgold.",1,2014
P14-2005,"since blanc works on imperfect system mentions, we have used it to score the conll 2011 and 2012 coreference systems.",4,2014
P14-2006,"we have cleared several misunderstandings about coreference evaluation metrics, especially when a response contains imperfect predicted mentions, and have argued against mention manipulations during coreference evaluation.",5,2014
P14-2006,"furthermore, we have a reference implementation of these metrics that has been rigorously tested and has been made available to the public as open source software.",2,2014
P14-2007,"as a future work, we would like to investigate how sac of a test sentence can be used to choose a classifier from an ensemble, and to determine the pre-processing steps (entity relationship extraction, for example).",1,2014
P14-2007,"first, the process of data preparation through eye tracking, labeled with the sac score was elaborated.",2,2014
P14-2010,"in future work, we also envision the possibility of sprinkling knowledge from background knowledge sources like wikipedia (gabrilovich and markovitch, 2007) to realize an alignment of topics to wikipedia concepts.",2,2014
P14-2012,our results show that page-rank in conjunction with re-ranking by initial confidence score can be used as an effective approach to collectively disambiguate named entity textual mentions in a document.,1,2014
P14-2012,"our proposed features are very simple and easy to extract, and work well when employed in pr.",1,2014
P14-2015,"we found that the entity status within the document is one of the important clues for solving the relevance detection problem, and showed that this information can be effectively automatically extracted using supervised classification.",1,2014
P14-2016,"in future work, we hope to explore the ability of the method to detect domain-specific dictionaries (e.g.training over domain-specific dictionaries from other language pairs), and low-density languages where there are few dictionaries and wikipedia articles to train the method on.",1,2014
P14-2017,"we are interested to find out if the orthographic rules depend on the source language, or if they are rather specific to the target language.",5,2014
P14-2017,"finally, we plan to make a performance comparison on cognate pairs versus word-etymon pairs and to investigate false friends (nakov , 2007).",3,2014
P14-2019,"as a next step, we will focus on morphological analysis and disambiguation of turkish words.",2,2014
P14-2019,"after determining the correct morphological analysis of turkish words, we will use the parts of these analyses to replace the leaf nodes that we intentionally left as 鈥*none*鈥.",1,2014
P14-2031,"as far as we are aware, this is the first study to automatically analyze this relationship involving the textual content of edits and turns.",1,2014
P14-2031,"based on the types of turn and edit in an edit-turn-pair, we have operationalized the notion of corresponding and noncorresponding edit-turn-pairs.",1,2014
P14-2036,"and to make the most of external knowledge, better ways to build topic space should be considered.",1,2014
P14-2037,we presented a new probabilistic model for learning bilingual word representations.,1,2014
P14-2041,"we also found that the most effective importance models are those that equate importance of an n-gram with its preferential use in higherscoring essays than in lower-scoring ones, above and beyond merely looking at the n-grams used in good essays.",1,2014
P14-2041,"this demonstrates the utility of using not only gold, high-quality human summaries, but also sub-standard ones when developing content importance models.",1,2014
P14-2042,"in our error analysis, we believe that by exploring the characterlevel pos and the internal word structure (zhang , 2013) at the same time, it is possible to further improve the performance of morphological analysis and parsing.",1,2014
P14-2043,"in future work, we aim to perform a more fine-grained error analysis to gain a better understanding where the improvement in accuracy takes place.",1,2014
P14-2043,one could also attempt to optimize the compound label splits to maximize prediction accuracy instead of applying a priori partitions.,1,2014
P14-2052,"hence, we plan to conduct evaluations with people7.",3,2014
P14-2052,we only used the rhetorical structures between sentences in this study.,1,2014
P14-2054,"in the future, we will study the non-projective cases based on the recent parsing techniques for 1-endpoint-crossing trees (pitler et al., 2013).",1,2014
P14-2057,our future work will extend the work by including more views such as the stylistic and vocabulary richness views.,2,2014
P14-2062,"in general, we find that the use of a dictionary tends to make aggregations more useful, irrespective of aggregation method.",1,2014
P14-2063,"similar approaches can be extended to other nlp tasks using different semantic links, specific dictionary and special seed words.",4,2014
P14-2064,"in this setting, we showed that the presence of difficult instances in training data misleads a machine learner into misclassifying clear-cut, easy cases.",5,2014
P14-2072,"the main findings from these experiments are: 1) we can build deception classifiers for different cultures with accuracies ranging between 60-70%, with better performance obtained when using psycholinguistic word classes as compared to simple unigrams; 2) the deception classifiers are not sensitive to different topics, with cross-topic classification experiments leading to results comparable to the within-topic experiments; 3) we can use data originating from one culture to train deception detection classifiers for another culture; the use of psycholinguistic classes as a bridge across languages can be as effective or even more effective than the use of translated unigrams, with the added benefit of making the classification process less costly and less time consuming.",1,2014
P14-2079,we conclude that it is possible to learn interpretable type systems directly from data.,2,2014
P14-2081,"in our future work, we plan to design better data representation which can well fit into the two-stage hashing theme; we also intend to apply the proposed hashing approach to more informal genres (e.g., tweets) and other down-stream nlp applications (e.g., first story detection).",1,2014
P14-2081,"in this paper, we proposed a novel two-stage unsupervised hashing framework for efficient and effective nearest neighbor search in massive document collections.",1,2014
P14-2083,"we propose to embrace such disagreements rather than using annotation guidelines to optimize inter-annotator agreement, which would bias our models in favor of some linguistic theory.",1,2014
P14-2084,"the obvious next step is to develop new machine learning models that exploit the contextual information available in the corpus we have curated (e.g., previous comments by the same user, the thread topic).",1,2014
P14-2084,the data comprises comments scraped from the social news website reddit.,2,2014
P14-2084,"we have shown that annotators rely on contextual cues (in addition to word and grammatical features) to discern irony and argued that this implies computers should, too.",1,2014
P14-2085,"we have described a new, context-aware approach to automatically predicting aspectual class, including a new set of distributional features.",1,2014
P14-2085,we have also introduced two new data sets of clauses labeled for aspectual class.,2,2014
P14-2087,"we would also like to explore how to incorporate syntactic features and employ alternative statistical methods (e.g., parametric models) to improve probability estimation and inference.",1,2014
P14-2087,the base model on average more than doubled the accuracy of lesk in senseval-2 on both fine- and coarse-grained tracks.,1,2014
P14-2090,"in addition, the greedy+dp algorithm uses only one feature per a position in this paper.",1,2014
P14-2093,"in the future, we are interested in applying  our methods into domain adaptation task of statistical machine translation in model level.",4,2014
P14-2094,"also, the findings from the time/productivity tradeoffs indicate that more time efficient algorithms and implementations should be explored.",1,2014
P14-2097,"compression methods for removing visually irrelevant information (kuznetsova , 2013) may also help increase the relevance of extracted captions.",1,2014
P14-2103,evaluation on a standard data set shows that our method consistently outperforms the supervised state-of-the-art method for the task.,1,2014
P14-2107,"it is worth pointing out that other dependency parsing frameworks (e.g., transitionbased parsing (zhang and clark, 2008; zhang and nivre, 2011)) could also benefit from modeling structural diversity in search.",1,2014
P14-2107,"it was shown that by keeping a diverse beam significant improvements could be achieved on standard benchmarks, in particular with respect to difficult attachment decisions.",3,2014
P14-2108,"adjusting for two major differences that are a matter of annotation convention, we showed that the ppcmbe can be parsed at approximately the same level of accuracy as the ptb.",1,2014
P14-2112,the approach is fully general and not just limited to language modelling.,1,2014
P14-2114,experimental results show our proposed framework outperforms the state-of-the-art baseline by over 7% in f-measure.,1,2014
P14-2114,our proposed model has been evaluated on the fsd corpus.,3,2014
P14-2138,"we showed that a small set of high-frequency function words have disproportionate influence on the accuracy of a bigram-based nli classifier, and that the majority of the indicative bigrams appear to be independent of l1.",1,2014
P14-3004,this knowledge can be used to estimate a network of support or opposition.,1,2014
P14-3004,"another idea would be to make use of the speaker's given party affiliations and bootstrap an approach to analyze their positions: if we assume that a majority of the speakers actually does follow their parties' lines, we can train a classifier for each party for each topic, and apply it to the same data to detect outliers.",1,2014
P14-3007,"firstly, the most general hypernym of subordinating conjunctions exerts an initial restrict to the following splitting step.",1,2014
P14-3008,"the approach developed in this paper can also be applied to parse tree querying and manipulation problem (levy and galen, 2006).",4,2014
P14-3008,another obvious direction is applying tree kernels to classify short texts based on standard corpus data.,2,2014
P14-3010,we see that this implicit handling is preferable to having no sense handling and also to having a full wsd module as part of a pipeline.,1,2014
P14-3011,"our future plans include implementation of shallow parsing and syntactic n-grams (sidorov , 2012; sidorov , 2013; sidorov , 2014; sidorov, 2013a; sidorov, 2013b), as well as learning techniques, and analysis of their influence on the system鈥檚 performance.",1,2014
P14-3011,we have introduced an approach to open ie based on syntactic constraints over pos tag sequences targeted at spanish language.,1,2014
P14-5001,evaluation has shown that the method can identify more keywords and rank them higher in the candidate list than monolingual keas.,1,2014
P14-5001,"as for future work, we would like to explore the possibility of incorporating the articles鈥 reader feedback into keyword extraction.",5,2014
P14-5005,"as for the large corpus, on which the recommended words and sentences are based, and the corpus mining based on nlp techniques (e.g., word vector representation and topic model lda), experimental results show that our system is both helpful and meaningful.",2,2014
P14-5005,"overall, our system provides syntactically and semantically related words, as well as recommends contextually related sentences to users.",1,2014
P14-5006,we expect that a supervised system using a large variety of features would improve the state of the art in keyphrase extraction.,1,2014
P14-5009,we also plan to build tools to import and export the work done in welt in order to facilitate collaboration among linguists working on similar languages or cultures.,1,2014
P14-5014,"future plans include enriching the feature set, adding a tree-based language model and considering forest input for multiple parses to provide robustness against parsing errors.",1,2014
P15-1003,"for future work, we will consider encoding more complex linguistic structures to further enhance the joint model.",2,2015
P15-1004,"building on the success of this paper, we plan to develop other neural network-based features, and to also relax the limitation of current rule extraction heuristics by generating translations word-by-word.",1,2015
P15-1005,another direction for future work would be to use a n-gram based language model constrained by the structured predicted in vdr.,1,2015
P15-1006,another important problem not addressed here is the role of context and discourse in interpreting scene descriptions.,5,2015
P15-1006,"however, additional data collection and experiments are necessary to confirm this and identify challenges specific to other languages.",2,2015
P15-1006,an obvious improvement would be to expand our learned lexical grounding approach to include spatial relations.,1,2015
P15-1006,"thus, we expect that our method for lexical grounding can facilitate development of text-to-scene systems in other languages.",4,2015
P15-1009,"as future work, we would like to: 1) construct the manifold regularization terms using other data sources.",2,2015
P15-1009,"we would try entity similarities derived in different ways, e.g., specified by users or calculated from entities textual descriptions.2) enhance the efficiency and scalability of sse.",1,2015
P15-1011,"from our own viewpoints, creating more evaluation data for measuring further progress in contrasting-meaning modeling, e.g., handling real oov issues, is interesting to us.",3,2015
P15-1011,"also, the degree of contrast may be better formulated as a regression problem rather than a classification problem, in which finer or even real-valued annotation would be desirable.",5,2015
P15-1013,future work should explore how to combine efficiently different transformations.,5,2015
P15-1015,"future work will remove the fixed ordering for feature templates, and dynamically add additional features based on the current scores of different labels.",1,2015
P15-1020,"in addition, we hope to expand the methods proposed here to a more incremental setting, where both parsing and decoding are performed incrementally, and the information from these processes can be reflected in the decision of segmentation boundaries.",1,2015
P15-1020,"as future work, we are planning to devise more sophisticated methods for language modeling using constituent tags, and ways to incorporate previously translated segments into the estimation process for left-hand constituents.",1,2015
P15-1021,"future work includes developing a bottom-up btg parser with latent variables, and comparing the results to the top-down parser.",1,2015
P15-1022,our future objective is to extend our evaluation to streams of data coming from a larger number of domains.,3,2015
P15-1022,"however, we are confident that the gradual shift of the translation industry towards human mt post-editing will not only push for further research on these problems, but also provide data for larger scale evaluations in a short time.",2,2015
P15-1024,"in the future, we plan to use logic-like semantic representations of texts, questions and answers and explore approaches to perform structured inference over richer semantic representations.",1,2015
P15-1025,"for the future work, we will explore how to incorporate more types of metadata information, such as the user ratings, like signals and poll and survey signals, into the learning process to obtain more powerful word representations.",2,2015
P15-1026,"furthermore, as our model is capable of detecting the most important words in a question, it would be interesting to use the results to mine effective question patterns.",1,2015
P15-1026,"for instance, we are integrating more external knowledge source, such as clueweb (lin et al., 2012), to train mccnns in a multi-task learning manner.",2,2015
P15-1027,"in particular, we want to devise more semantically-motivated methods to select chimera components and negative samples.",1,2015
P15-1027,"both chimera and the intruder methods are flexible, and we plan to explore them further in future research.",1,2015
P15-1028,"and finally, we would like to further explore the performance of the lexical function model and generalized lexical function model on different datasets, which involve more complex compositional phenomena.",2,2015
P15-1028,"we could, e.g., try to separate the intersective adjectives from non-intersective adjectives.",6,2015
P15-1028,"in future work, we would like to test different sizes of dimensionality reduction, in order to optimize our generalized lexical function model.",1,2015
P15-1028,"moreover, it is possible that better results may be obtained by proposing multiple generalised lexical functions, rather than a single one.",1,2015
P15-1035,"In the future, we are interested in extending these techniques to also exploit unlabeled data.",1,2015
P15-1036,"as future work, we would like to use our method to extract more than just binary relations.",4,2015
P15-1037,"skip-node kernel and its approximations are particularly effective for comparison identification, and potentially applicable to other relation extraction or natural language tasks (the direction of our future work).",4,2015
P15-1038,"in the future, we plan to add regular expression search over parses, and sorting within results tables.",1,2015
P15-1038,we also leave for future work the comparison of these parsers across languages.,3,2015
P15-1038,our hope is that the results from the evaluation as well as the tool will give non-experts in parsing better insight into which parsing tool works well under differing conditions.,1,2015
P15-1038,we also hope that the tool can be used to facilitate evaluation and be used as a teaching aid in nlp courses.,4,2015
P15-1038,"in addition, it may be possible to achieve good performance in particular genres by doing “mini-ensembles” trained on general purpose data (e.g.wb) and genre-specific data.",2,2015
P15-1041,"in the future, we will explore more suitable knowledge representations and knowledge validation in the credboost framework.",1,2015
P15-1042,"in the following work, we will further investigate the relationship between semantic and sentiment information for clsc, and balance their functions to optimize their combination for clsc.",1,2015
P15-1043,"since the different content models use different kinds of lexical information, further gains might be obtained by combining some of these models into a joint model.",1,2015
P15-1043,We plan to explore this in future work.,6,2015
P15-1044,"in order to improve the performance of our generator and remove the dependency on domain specific features, we plan to replace the perceptron ranker with a neural network.",1,2015
P15-1044,"the generator source code, along with configuration files for experiments on the bagel data set, is available for download on github.11 in future work, we plan to evaluate our generator on further domains, such as geographic information (kate , 2005), weather reports (liang , 2009), or flight information (dahl , 1994).",3,2015
P15-1045,"in addition, we plan to investigate other methods about multisentence compression and sentence fusion, such as supervised methods.",1,2015
P15-1045,"firstly, we plan to introduce event relations to learning event salience.",6,2015
P15-1045,"For future work, we plan to explore two directions.",6,2015
P15-1048,"in future work, we will try to add new classes of features to further improve performance by capturing the property of disfluencies.",2,2015
P15-1048,we would also like to make an end to-end mt test over transcribed speech texts with disfluencies removed based on the method proposed in this paper.,1,2015
P15-1049,"in the future, we would like to investigate the advantages and disadvantages between tree based models and other non-linear models such as deep neural networks or recurrent neural networks.",3,2015
P15-1051,"in our future work, we wish to explore a better way to encode distributional semantics by proposing a modified lda for better triples representation.",1,2015
P15-1054,it would be an interesting future problem to estimate the value of k automatically in our setting.,5,2015
P15-1054,"as future work, we also plan to extend our techniques to produce a hierarchical summary of topics and scale it across heterogeneous collection of objects (from different domains) to bring all of them under the same topic dag and investigate interesting cases thereon.",1,2015
P15-1055,"in future work we aim to evaluate how our method performs on entities and relationships of  any type and popularity, including tail entities and miscellaneous relationships.",3,2015
P15-1055,"we also want to investigate moving beyond wikipedia and extract candidate sentences from documents that are not related to the knowledge graph, such as web pages or news articles.",5,2015
P15-1056,"moreover, we plan to study the generation of entity-driven event chronicles, leveraging more fine-grained entity and event extraction approaches.",1,2015
P15-1056,"in the future, we plan to investigate automatically adapting an event’s granularity and learn the principle of summarizing the event according to the reference event chronicle.",1,2015
P15-1057,our future work includes exploiting the profiles of target entities as feedback to refine the results of morph mention extraction.,6,2015
P15-1057,we will also extend the framework for event morph decoding.,1,2015
P15-1058,"in future work, new objectives should be integrated into the framework and existing objectives could be enhanced.",6,2015
P15-1063,"in the future, we plan to test our approaches over longer time spans and to design the way to automatically “explain” temporal counterparts by outputting “evidence” terms for clarifying the similarity between the counterparts.",1,2015
P15-1064,"in the future we will explore more effective features to improve the negation and speculation identification in chinese language, and focus on joint learning of the two subtasks.",1,2015
P15-1065,an interesting future direction is to use products of these random walk features to express their conjunctions.,1,2015
P15-1067,we will seek methods to complete knowledge graphs with new triplets whose entities and relations come from plain texts.,1,2015
P15-1067,one possible way to obtain these new triplets is to extract facts from plain texts.,1,2015
P15-1069,"in future, we plan to integrate a larger diversity of surface, semantic and linguistic information for relevant sentence selection.",2,2015
P15-1069,"to move further in this direction, we plan to focus on exploiting morphological term variations taking advantage of the alternative terms provided by dbpedia.",1,2015
P15-1070,"first, we would like to try adapting and evaluating some additional wsd algorithms for use with puns.",1,2015
P15-1070,"second, we would like to investigate alternative tie-breaking strategies, such as the domain similarity measures used by mihalcea et al.(2010).",1,2015
P15-1070,"finally, whereas in this paper we have treated only the task of sense disambiguation for the case where a word is known a priori to be a pun, we are interested in exploring the requisite problem of pun detection, where the object is to determine whether or not a given context contains a pun, and more precisely whether any given word in a context is a pun.",5,2015
P15-1071,"in future, we plan to apply the proposed method to other types of domain adaptation tasks such as cross-domain part of-speech tagging, named entity recognition, and relation extraction.",4,2015
P15-1074,"one possible next step would be to expand this work to more complex semantic spaces which include stronger notions of compositionality, semantic roles, and so on, such as the distributional approaches of baroni and lenci (2010), sayeed and demberg (2014), and greenberg (2015) that contain grammatical information but rely on vector operations.",1,2015
P15-1075,"other directions include developing methods to learn the sbt structure from data, as well as applying the sbt prior to other tasks, such as sequential language modeling.",1,2015
P15-1075,one limitation of the current work is that the sbt is defined only implicitly.,5,2015
P15-1075,we plan to investigate explicit representations of the sbt prior or related variants.,1,2015
P15-1075,There are several directions of future work.,6,2015
P15-1077,"an interesting extension to our work would be the ability to handle polysemous words based on multi-prototype vector space models (neelakantan et al., 2014; reisinger and mooney, 2010) and we keep this as an avenue for future research.",1,2015
P15-1078,we further plan to incorporate features from the source sentence.,1,2015
P15-1078,"in future work, we would like to experiment with an extension that allows for multiple references.",6,2015
P15-1080,"as future work, it is necessary to integrate more features into our learning framework.",1,2015
P15-1080,it is also interesting to see how the non-linear modeling fits in to more complex learning tasks which involves domain specific learning techniques.,4,2015
P15-1083,"for the future work, we aim to use automatic prediction of deceivers to help truth-tellers win games more easily.",6,2015
P15-1083,"in addition, it is worth exploring the impact of cross-cultural analysis in detecting deception.",6,2015
P15-1083,"it will be interesting to study non-verbal features such as blink rate, gaze aversion and pauses (granhag and stromwall, 2002) when people play this game face-to-face and combine the non-verbal and verbal features for deception detection.",1,2015
P15-1087,one of our future goals is to reduce the initial lexicon to be even smaller by further automating the nl2kr gui interaction component.,1,2015
P15-1090,"first, we plan to consider how to conduct nsw detection and normalization at the same time.",6,2015
P15-1090,"third, we want to investigate the impact of nsw and normalization on other nlp tasks such as parsing in social media data.",4,2015
P15-1090,"second, we like to try a joint method to  simultaneously train the nsw detection and ner models, rather than just combining models in decoding.",1,2015
P15-1092,"in the future, we would like to apply sp interpolation to multilingual sp learning, i.e. integrating data from multiple languages for more accurate sp induction and projecting universal semantic relations to low-resource languages.",2,2015
P15-1092,"it is also interesting to investigate sp learning at the level of semantic predicates (e.g. automatically inducing framenet-style frames), where combining the visual and linguistic knowledge is likely to outperform text-based models on their own.",1,2015
P15-1092,"in the future, it would be interesting to derive the information about predicate-argument relations from low-level visual features directly.",1,2015
P15-1093,"since this framework is applicable to the argument classification in srl, applying our methods to that task is an interesting line of the future research.",4,2015
P15-1093,"for future work, we plan to incorporate external resources for our joint methods.",2,2015
P15-1094,"we also intend to explore more systematic ways to incorporate supervised signals into learning, to fine-tune c-phrase vectors to specific tasks.",1,2015
P15-1094,"having established a strong empirical baseline with this parsimonious approach, in future research we want to investigate the impact of possible extensions on both lexical and sentential tasks.",3,2015
P15-1094,"when combining the vectors, either for induction or composition, we will try replacing plain addition with other operations, starting with something as simple as learning scalar weights for different words in a phrase (mitchell and lapata, 2010).",1,2015
P15-1094,"we plan to perform a systematic analysis of both existing benchmarks and natural corpus data, both to assess the actual impact that such factors have on the aspects of meaning we are interested in (take two sentences in an entailment relation: how often does shuffling the words in them make it impossible to detect entailment?), and to construct new benchmarks that are more challenging for additive methods.",3,2015
P15-1095,we hope our decomposition provides a useful framework to guide future work in ner++ and amr in general.,1,2015
P15-1095,a clear direction of future work is improving the coverage of the defined actions.,6,2015
P15-1095,"for example, a richer lemmatizer could shift the burden of lemmatizing unknown words into the amr lemma semantics and away from the dictionary lookup component.",1,2015
P15-1096,"in future work, we also plan to integrate our work with robobrain (saxena et al., 2014) to leverage these existing systems for building a robotic system capable of working with physical world data.",1,2015
P15-1097,"additionally, we would like to test our models on other rte challenges and on several qa datasets, which for space constraints we could not do in this work.",2,2015
P15-1097,"in the future, it would be interesting defining graph kernels that can combine more than two substructures.",1,2015
P15-1097,another possible extension regards the use of node similarity in graph kernels.,6,2015
P15-1100,future work will include expanding the corpus and experimenting with datasets outside of the political domain.,2,2015
P15-1100,"we also plan to evaluate this strategy on data from different online sources, e.g., twitter or youtube.",2,2015
P15-1101,"in the future work, we would like to explore better ways of modeling the label and context dependence and apply our dfg approach in more applications, e.g. micro-blogging emotion classification.",1,2015
P15-1107,"in any case, we look forward to more sophi4d applications of neural models to the important task of natural language generation.",4,2015
P15-1111,"we are extending our work to other popular dependency parsers and non-projective parsing algorithms, and hope to develop features to improve and mitigate the cascading impact of punctuation attachment errors in parsing.",1,2015
P15-1112,"moreover, we also wish to investigate the ability of our model for other nlp tasks.",4,2015
P15-1112,"for the future research, we will develop an integrated parser to combine rcnn with a decoding algorithm.",1,2015
P15-1112,we believe that the integrated parser can achieve better performance without the limitation of base parser.,1,2015
P15-1113,"in the future, we plan to apply our neural network structure to dependency parsing.",4,2015
P15-1113,"we are also interested in using long short-term memory neural networks (hochreiter and schmidhuber, 1997) to better model the locality of propagated information from the stack and queue.",1,2015
P15-1113,the parameter estimation under semi-supervised setting will be investigated further.,6,2015
P15-1114,"in the future, we would like to extend our technique to other real valued kernels such as the string kernels and tagging kernels.",4,2015
P15-1115,"in future work, we plan to combine our incremental parsing/role labeling approach with a compositional model of semantics, which would have to be modified to take semantic role triples as input (rather than words or word pairs).",1,2015
P15-1116,"furthermore, we will experiment with larger feature sets that add lexical information.",2,2015
P15-1116,"in future work, we will concentrate on methods that could remedy the data sparseness concerning discontinuous constituents, such as self-training.",1,2015
P15-1116,an formal investigation of the expressivity of our parsing model is currently under way.,1,2015
P15-1118,we plan to investigate methods which use paraphrases to augment parsing models created at train time.,1,2015
P15-1118,"as part of future work, we plan to integrate existing larger paraphrase resources, such as wikianswers (fader et al., 2013) and ppdb (ganitkevitch et al., 2013).",2,2015
P15-1118,poor alignments are one of the larger sources of errors and improving alignments could help dramatically.,5,2015
P15-1118,ppdb includes phrasal and syntactic alignments which could supplement our existing alignments or be used as proxies for paraphrases.score).+x indicates extending the above system with x. bllip-st is bllip using the self-trained model.,1,2015
P15-1118,one simple extension is to use multiple paraphrases and their alignments instead of just one.,1,2015
P15-1122,"in the near future, we plan to create and release a larger null instantiation corpus.",2,2015
P15-1122,"for our future work, we plan to manually annotate coreference information so that we can compare with more methods.",3,2015
P15-1122,"finally, we hope to exploit some additional knowledge resources, such as hownet, which could  potentially further improve the performance of our proposed method.",2,2015
P15-1123,"no large corpora exist to date, but studying the interaction of these phenomena is on our research agenda.",2,2015
P15-1123,"another related distinction is the one between habitual, stative and episodic sentences (mathew and katz, 2009), which applies to both what we call generic and non-generic sentences.",4,2015
P15-1123,"we have not attempted to tackle the task of classifying the genericity status of other dependents, as they are even harder to classify than subjects, and a concise annotation scheme has to be worked out in order achieve an acceptable inter-annotator agreement on this task.",5,2015
P15-1124,it would also be interesting to formally investigate the theoretical merits and algorithmic possibility of solving the variance weighted objective in eq.(6).,1,2015
P15-1124,using cca to induce representations other than word embeddings is another important future work.,1,2015
P15-1124,"even though the objective is hard to optimize in the worst case, it may be tractable under natural conditions.",1,2015
P15-1125,another interesting aspect of future work is to incorporate other sources of knowledge to further enrich the semantics.,2,2015
P15-1126,future work ought to pursue models in which all morphemes contribute both semantic and syntactic content to the word representations.,1,2015
P15-1127,"it would also be interesting to gather data with compositional phenomena, such as negation and disjunction, and study its impact on the performance of the semantic parser.",2,2015
P15-1128,applying other structured-output prediction methods to graph generation will also be investigated.,4,2015
P15-1128,"in the future, we would like to extend our query graph to represent more complicated questions, and explore more features and models for matching constraints and aggregation functions.",1,2015
P15-1129,"we believe that our methodology is a promising way to build semantic parsers, and in future work, we would like to extend it to handle anaphora and nested quantification.",4,2015
P15-1131,"to overcome it, tslda should be extended as a non-parametric topic model estimating the number of topics inherent in the data.",1,2015
P15-1131,This will be done in our future work.,6,2015
P15-1135,"in summary, we hope to begin to pull back the veil on the types of information that a truly unsupervised system, if one should ever exist, would need to learn, and we pose a challenge to the community to find ways that a learner might discover this knowledge without hand-engineering it.",5,2015
P15-1137,"because our approach automatically learns intermediate representations given raw features, directions for further research might alternately explore including additional (perhaps semantic) raw features, as well as developing loss functions that further discourage learning representations that allow for common errors (such as those involving pleonastic pronouns).",1,2015
P15-1139,"in future work, we plan to pursue the new framework suggested by our analyses, investigating the interaction of issue polarization and framing-based polarization.",1,2015
P15-1142,a natural direction is to combine the logical compositionality of this work with the even broader knowledge source of general web pages.,4,2015
P15-1142,"pasupat and liang (2014) used a framework similar to ours to extract entities from web pages, where the “logical forms” were xpath expressions.",1,2015
P15-1143,"furthermore, we will investigate methods for speeding up graph parsing further, e.g. with different heuristics.",1,2015
P15-1143,a challenge for grammar-based semantic parsing is grammar induction from data.,5,2015
P15-1143,we will explore this problem in future work.,6,2015
P15-1145,"at the end, we plan to apply the swe word embedding models for more natural language processing tasks.",4,2015
P15-1145,"as for the future work, we would incorporate more types of knowledge, such as knowledge graphs and framenet, into the learning process for more powerful word representations.",2,2015
P15-1145,we also expect that some common sense related semantic knowledge may be generated as ordinal inequality constraints by human annotators for learning semantic word embeddings.,1,2015
P15-1150,our results suggest further lines of work in characterizing the role of structure in producing distributed representations of sentences.,2,2015
P15-1152,"in future work, we would consider adding the intention (or sentiment) of users as an external signal of decoder to generate responses with specific goals.",1,2015
P15-1153,"another aspect is to improve time efficiency of our framework, and its major bottleneck is the time consuming ilp optimization.",1,2015
P15-1153,"for future work, one aspect is to enhance the grammar quality of the generated new sentences and compressed sentences.",1,2015
P15-1158,future work will test to what extent this latent discourse information could affect the model predictions.,3,2015
P15-1158,"in future work we hope to look at a broader range of referring expressions, such as null pronouns and definite descriptions, and to explore the extent to which our model can be applied to other linguistic phenomena that rely on discourse information.",4,2015
P15-1159,a challenge for future work is to find reliable linguistic cues that generalize well between such settings.,5,2015
P15-1160,"in the future, we plan to consider a greater variety of diseases and symptoms in order to develop applications for public health, e.g., monitoring the mental condition of individuals.",2,2015
P15-1160,"thus, we can not only improve the accuracy of subject identification but also enhance the generality of this task.",1,2015
P15-1167,"we would like to try using convolutional neural network to automatically encode ngram-like features, in order to further shrink parameter space.",1,2015
P15-1167,it is also interesting to study whether extending our model with deep architectures can benefit cws.,4,2015
P15-1167,"in the future, we plan to investigate methods for our model to better utilize external resources.",1,2015
P15-1167,"lastly, it might be useful to adapt our model to tasks such as pos tagging and name entity recognition.",1,2015
P15-1171,"since semi-supervised learning requires generative models in advance, our proposed bayesian generative model will also lay foundations to such an extension.",1,2015
P15-1171,"in order to adapt to human standards given in supervised data, it is important to conduct a semisupervised learning with discriminative classifiers.",1,2015
P15-1172,"for future, our immediate plan is to annotate more data with both ctb and pd tags (a few thousand sentences), and to investigate our coupled model with small amount of such annotation as extra training data.",2,2015
P15-2003,"considering the coverage of word senses in our training data, in future work we plan to filter out those sense vectors which are under-represented in the training corpus.",2,2015
P15-2003,we will also further investigate the feasibility of applying the multi-prototype word embeddings in a wide range of nlp tasks.,3,2015
P15-2009,"for example, we can explore more effective ways to incorporate tweets for sentence compression; we can study joint models to combine both sentence extraction and compression with the help of relevant tweets; it will also be interesting to use the parallel dataset of the news articles and the tweets for timeline generation for a specific event.",1,2015
P15-2009,There are some interesting future directions.,6,2015
P15-2013,"as future work, it is necessary to further this investigation by taking into account various degrees of discrimination.",5,2015
P15-2013,"as suggested in (gatt , 2013), the effect of discrimination may be perceived as a continuum, and in that case a practical reg algorithm should be able to make more complex decisions that those presently implemented.",1,2015
P15-2019,in future we plan to upgrade the current word prediction pathway to a sentence reconstruction and/or sentence paraphrasing task in order to encourage the formation of representations of full sentences.,2,2015
P15-2019,"we also want to explore the acquired structure further, especially for generalizing the grounded meanings to those words for which visual data is not available.",1,2015
P15-2020,"in future work, we plan to explore more sophisticated visual generality measures, other semantic relations and different ways of fusing visual representations with linguistic knowledge.",1,2015
P15-2022,in the future we will investigate stronger network structure such as lstm to further improve the prediction power of our model.,1,2015
P15-2024,our future work includes experiments with other types of slpt problems that focus on different aspects of translation quality and language understanding.,3,2015
P15-2030,"in the future, we will investigate ways for automatically optimizing the hyperparameters of the network (snoek et al., 2012) and various extensions to recursive or hierarchical convolutions.",1,2015
P15-2030,"in the future, we will investigate ways for automatically optimising the hyperparameters of the network (snoek , 2012) and various extensions to recursive or hierarchical convolutions.",1,2015
P15-2033,future research will be devoted to find models to effectively combine tks and dnn.,1,2015
P15-2033,"this as well as further research will be integrated in our cp system described in (barlacchi et al., 2015).",1,2015
P15-2033,"in particular, our previous model exploiting linked open data in qa (tymoshenko , 2014) seems very promising to find correct answer to clues.",1,2015
P15-2035,a focus of our future work will be to manually annotate the data to determine the frequency and nature of the topic excursions.,2,2015
P15-2035,we also plan to apply our methods to asr output rather than manual transcripts.,4,2015
P15-2037,"in the future, it would be interesting to improve our method to cover more kb predicates, and extend our nn model with more advanced structures to further improve the performances and also simultaneously characterize the target and comparison set involved.",1,2015
P15-2039,we are planning to port our corpus and compare our scheme with ud to contribute to the improvement of ud for japanese.,2,2015
P15-2043,"as future work, we plan to exploit the full tree structure of synthetic words to improve not only cws but also additional downstream tasks such as sentence parsing.",1,2015
P15-2045,furthermore we would like to examine the pra-reduced data in more detail.,2,2015
P15-2045,in future we would like to explore alternative  methods for selecting pra relation paths to identify false negatives.,1,2015
P15-2045,we would like to find which kind of entity pairs are detected by our proposed method and whether the reduced data can also be used to extend the positive training data.,2,2015
P15-2045,we would also like to apply the approach to other domains and alternative knowledge bases.,4,2015
P15-2045,finally it would be interesting to compare our approach to other state of the art relation extraction systems for distant supervision or biased-svm approaches such as liu et al.(2003).,3,2015
P15-2046,we will also investigate other ways of collapsing different types of tags in the lexicalized tree representation.,1,2015
P15-2046,"in the future, we plan to mitigate the performance drop on the clueweb set by adding information about context words around relation words.",1,2015
P15-2051,"in future work, we plan to experiment with applying more expressive machine learning techniques to this task.",1,2015
P15-2055,"as part of on-going and future work, we will be incorporating additional retrieval models, such as the okapi bm25, in our evaluation framework.",1,2015
P15-2057,"in future work, we will investigate how to update the category topic hierarchy incrementally with the creation of new related articles.",5,2015
P15-2059,"as future work, we plan to explore in more detail this research line by applying more sophisticated approaches in the temporal analysis at document level.",1,2015
P15-2060,"in the future, our plans include: (i) to explore the joint approaches for event extraction with cnns; (ii) and to investigate other neural network architectures for information extraction.",1,2015
P15-2061,"future research may explore additional features and knowledge resources, investigate alternative approaches for creating effective seed lists, and extend our approach to argument labeling.",4,2015
P15-2063,"we believe it will be useful for web scale extraction problems, where language identification and coarse language modeling are used to filter large amounts of data.",6,2015
P15-2063,we plan to investigate a new hardware version that intel is preparing.,6,2015
P15-2064,"for the future work, we would like to investigate the generality of our approach in broader languages and domains.and application of a wide-coverage multilingual semantic network.",4,2015
P15-2066,additional research is needed to further explore this idea.,6,2015
P15-2066,"the top features induced for each classification task can also be interpreted as our systems ability to discover new feature spaces, which can be utilized independently or along with a simpler feature space (e.g., bag of words) to learn a better classification model.",1,2015
P15-2071,we intend to further account for finer-grained characteristics of the words and to extend our experiments to more languages.,2,2015
P15-2071,"the method we propose is language-independent, but we believe that incorporating language-specific knowledge might improve the system’s performance.",1,2015
P15-2074,"in the future, the evaluation of proposed method needs to be extended to large-scale test corpus and detailed context sensitive rules are used to identify tibetan unknown words.",2,2015
P15-2077,we plan to extend this work by studying how the combination of the unsupervised selection of examples and their use for training supervised classifiers can be exploited for improving distributional thesauri through feature selection.,1,2015
P15-2077,"we will also investigated the interest of taking into account word senses in this framework, as in (huang et al., 2012) or (reisinger and mooney, 2010).",1,2015
P15-2080,"in the future, we plan to study the influence of other factors such as temporal information to btm and its variants.",3,2015
P15-2082,"in our future work, we work to automatically determine the number of authors of a multi-author document.",5,2015
P15-2082,"furthermore, we will explore an adaptive learning method to select the optimal value of the threshold q for the probability indication procedure.",1,2015
P15-2084,further work will include extending the dependency tree language modeling to long short-term memory rnns to handle longer syntactic dependencies.,1,2015
P15-2086,"in the future, we plan to apply this approach to other cross-domain prediction tasks such as named entity recognition or semantic role labeling.",4,2015
P15-2086,we also plan to extend our method to learn cross-lingual representations with auxiliary resources such as bilingual dictionaries or parallel sentences.,4,2015
P15-2088,"in the future, we will try to exploit contextual information at the target side (e.g., partial translations).",2,2015
P15-2090,we hope that this release sparks more interesting research on decipherment and its applications to machine translation.,4,2015
P15-2091,"going further, we could study the effect of using other hypotheses instead of the rerank one-best to perform the comparison with the moses one-best hypothesis.",3,2015
P15-2091,"for our future work, we also plan to study approaches that can enhance the diversity in the k-best lists (chatterjee and cancedda, 2010; gimpel , 2013) between each iteration of the multi-pass decoding to train a better rerank after each decoding pass.",1,2015
P15-2091,"another area for improvement lies in the addition of yet more complex features, for instance to allow a better dis6interestingly, a control experiment showed that using iteration-specific tables yields slightly better performance than fusioning all bi-phrases of a given type in a non iterationspecific table, possibly allowing later tunings to prefer the contents of the most recent, and possibly more reliable tables.course coherence modelling over iterations (ture , 2012; hardmeier , 2012).",1,2015
P15-2092,this suggests that future work on improving smt across genres needs to investigate approaches that increase model coverage.,1,2015
P15-2093,its application in statistical machine translation and cross-lingual model transfer remains to be explored.,4,2015
P15-2093,learning multiple embeddings per word and compositional embeddings with matrix factorization are also interesting future directions.,1,2015
P15-2094,"in the future, we plan to explore more refined methods to devising effective intermediate expressions, and improve estimation of probabilities for triangulated rules.",1,2015
P15-2101,"in the future, we want to incorporate wordnet knowledge to further reduce perplexity on infrequent words.",2,2015
P15-2102,"to further improve the work, we will incorporate more information to enrich the hierarchical representation in the future.",2,2015
P15-2102,the experimental results demonstrate the advantage of using 5-level pam and semantic enhancement against n-gram models and lda-like models.,1,2015
P15-2104,"we also plan to investigate more nuanced methods for differentiating between global and local celebrity nodes, to be able to filter out global celebrity nodes but preserve local nodes that can have high geolocation utility.",1,2015
P15-2104,"as future work, we plan to use temporal data and also look at improving the text-based geolocation model using sparse coding (cha et al., 2015).",1,2015
P15-2105,"in future work, we plan to use the keyword extraction to perform numerous nlp tasks on the twitter domain, such as document summarization.",4,2015
P15-2106,we plan to study other ways to retrieve such a context like the conversation thread.,1,2015
P15-2107,we expect the dataset to be helpful for studies on email overload problems.,2,2015
P15-2107,we plan to increase the size of our dataset through amt.,2,2015
P15-2107,"we believe features regarding such information (e.g.the recipients email history with the contact, the recipients personal preference in categorizing emails, etc.)should also be incorporated for importance prediction.",2,2015
P15-2107,"meanwhile, we are aware that the current corpus lacks social and personal information.",2,2015
P15-2108,"in future work, we plan to introduce this method to normalize the nonstandard language used in twitter, applying the methods to problems in search and other areas.",4,2015
P15-2109,"in future, we will: 1) collect more ntas texts from various users; 2) do further work on how to fully leverage ntas to improve word segmentation; 3) call for dominant text editors to record ntas.",2,2015
P15-2110,"in the future, we plan to further study this problem by focusing on omission detection, verb tense preference from the view of pragmatics, and jointly learning the local and global predictors.",5,2015
P15-2110,"in addition, we will study predicting the tense of multiple predicates in a sentence and identifying imperative sentences in a conversation, which is also a challenge of tense prediction.",5,2015
P15-2113,"in future work, we plan to (i) experiment with more sophisticated thread-level features, as well as with other features that model context in general; (ii) try data from other cqa websites, e.g., where dialogue between users is marked explicitly; and finally, (iii) integrate sequence, precedence, dependency information with global — thread-level— features in a unified framework.",1,2015
P15-2117,"based on this work, more research can be conducted on topic recognition and semantic roles labeling for human-human conversations in real-world.",5,2015
P15-2117,"in the future, we plan to explore the methods on training the unbalance data to improve the overall performances of our approach.",1,2015
P15-2121,"since our new model involves simpler features, including unigram features defined over individual semantic unit – word pairs, we believe our new model would aid the joint modeling of both distributional and logical semantics (lewis and steedman, 2013) within a single framework.",1,2015
P15-2121,We plan to explore this avenue in the future.,6,2015
P15-2122,"for future work, we would like to employ more complicated features like the sentiment of the context, and dictionary features based on an npi lexicon.",1,2015
P15-2122,"also, if available, prosodic information like focus, pauses, and intonation may be useful.",1,2015
P15-2124,we are currently exploring a more robust incorporation of inter-sentential incongruity for sarcasm detection.,1,2015
P15-2124,"our error analysis points to potential future work such as: (a) role of numbers for sarcasm, and (b) situations with subjective sentiment.",5,2015
P15-2125,"in future work, we would like to explore the relation among emotions and caused languages for detecting the emotion and caused languages collectively.",3,2015
P15-2126,"in the future, we plan to further explore this linear transformation based adaptation from different perspectives, e.g., sharing adaptation operations across users or review categories.",4,2015
P15-2127,"in the future, we plan to further refine and employ it to other nlp applications.",4,2015
P15-2127,"also, additional work can be done on combining statistical models into different components of pba.",1,2015
P15-2128,"as future work, we plan to investigate the exact effect of the reordering constraints in terms of possible translation model phrase pairs and target language model n-grams which may not be used depending on the constraint parameters, in order to find the best configuration.",1,2015
P15-2129,future work will focus on extending the va prediction from the word-level to the sentence- and document-levels.,2,2015
P15-2130,"in our future work, we intend to focus on initializing good belief tracking models when no annotated dialogs are available for the new dialog domain.",1,2015
P15-2139,"in future work, we would like to investigate joint training on the source and target languages.",2,2015
P15-2141,the extensions include designing a new action to infer abstract concepts and training the parser with additional semantic role labeling and coreference based features.,2,2015
P15-2142,"the same model can be applied as an efficient syntactic language model, and for future work it should be integrated into language generation tasks such as machine translation.",4,2015
P15-2144,one interesting direction for further research would be to show the effect of this feature in other natural language processing tasks.,4,2015
P15-3002,"more generally, we will explore the encoding of coreference constraints into probabilistic models that can be combined with smt systems, so that coreference constraints are considered in the decoding process.",1,2015
P15-3002,"in the future, we will generalize this constraint to complex noun phrases which are not compounds.",2,2015
P15-3004,"for future work, we will first conduct experiments on how well the dynamic-eager algorithm performs on different treebanks, including multi-head dependencies (such as the danish treebank (kromann, 2003)).",3,2015
P15-3004,"secondly, we will conduct experiments on previously described static oracle parsing algorithms by using different classifiers such as support vector machines.",1,2015
P15-3005,"apart from that, an information extraction approach that looks for more specific patterns should be verified.",1,2015
P15-3005,"finally, we would like to adopt these findings to improve the prediction of epidemics.",4,2015
P15-3005,"as future work, we would like to disambiguate functional expressions using sequence labeling techniques (utsuro , 2007); we would also like to identify the predicate–argument structure of disease events (kanouchi , 2015).",1,2015
P15-3007,"to verify this, we hope to submit our improved alignment results to a state-of-the-art amr parser, and evaluate the parsing results.",3,2015
P15-3007,"therefore, increasing the training data size from the release is one solution to improve the performance of our aligner from the unsatisfactory results.",2,2015
P15-3007,"in the future, we will be following these steps to develop the proposed rewrite-based parser: implementation of our rewrite-based amr parser: we would like to implement the proposed rewrite-based amr parser.",1,2015
P15-3007,"using external lexical resources, like wordnet, is another promising solution to extend to snyonyms.",2,2015
P15-3007,we also plan to experiment with the data generated by an automatic dependency parser.,2,2015
P15-4001,"in the future, we attempt to integrate more feedback content such as video tutorial or articulation animation for teaching pronunciation.",6,2015
P15-4003,"for future work, we plan to speed up the learning process (e.g.by saving feature vectors instead of re-calculating them), and also add the ability for users to configure the features used to train the classifier, e.g.incorporating lemmata or named entities instead of only using the parse tree.",1,2015
P15-4007,"in future work, we plan to extend vex with functionality for visualizing additional error types, and for exploring entities not only in a single document, but across documents.",2,2015
P15-4007,"given the structural similarities entities in coreference resolution and entities in entity linking share, we also will add methods for visualizing entities found by coreference resolution systems.",1,2015
P15-4009,"in the future, we plan to investigate how to further represent and utilize these extracted concepts efficiently in more nlp tasks which call for deep language understanding.",5,2015
P15-4014,"in the future, we would like to release the solver to allow researchers to contribute to the project and make the system even more competitive.",6,2015
P15-4015,"in the future, we intend to incorporate in lexenstein approaches for complex word identification, as well as more approaches for the remaining tasks of the usual ls pipeline.",1,2015
P15-4016,"we will focus in particular on modular extensions to the specification for supporting search, tagging, and bulk modifications.",1,2015
P15-4016,"in future work, we will continue to develop the api specification further in collaboration with the relevant standardization efforts and interested parties using a fully open process.",1,2015
P15-4016,"we will also continue to develop and extend the tools, with emphasis on reversible conversions between oa json-ld and major related formats.",1,2015
P15-4017,"hadoop clusters), thus enabling the processing of large volumes of data.",2,2015
P15-4017,we are now investigating how data annotation can run on multiple machines in a distributed environment (e.g.,5,2015
P15-4018,"furthermore, we are working on incorporating more complex pre-processing for the holing operation in the visualization, e.g.aggregating context features over co-reference chains, as well as relation extraction and frame-semantic parsing for term–context representations.",1,2015
P15-4023,future work could include enriching properties of a story using wikipedia info box and better summarizing events and stories.,2,2015
P15-4024,"for example, corpora for different language levels, genres (e.g., emails, news) could be used to make the suggestions more relevant to users with diverse proficiency levels and interests.",2,2015
P15-4024,"nlp, ir, and machine learning techniques could be used to provide more relevant ranking, to pin-point grammatical errors, or to generate finer-grained semantic patterns (e.g., assist someone in something or attend activity/institution) additionally, an interesting direction is identifying grammar patterns using a crf sequence labeller.",1,2015
P15-4024,many avenues exist for future research and improvement of writeahead.,6,2015
P15-4025,"in addition, we will develop a faster constituent parsers by using recurrent neural network.",1,2015
P15-4025,"first of all, we will integrate a new subsystem which conducts dependency-based semantic role labeling.",1,2015
P15-4025,"in our future work, we will add more function alities to niuparser.",1,2015
P16-1001,"we anticipate the approaches that we have found useful in the case of amr to reduce the impact of noise, efficiently support large action spaces with targeted exploration, and cope with unbounded trajectories in the transition system will be of relevance to other structured prediction tasks.",1,2016
P16-1002,there has been growing interest in applying neural networks to semantic parsing and related tasks.,4,2016
P16-1002,"wang and yang (2015) use a similar strategy, but identify similar words and phrases based on cosine distance between vector space embeddings.",1,2016
P16-1004,"beyond semantic parsing, we would also like to apply our seq2tree model to related structured prediction tasks such as constituency parsing.",4,2016
P16-1005,in the future we aim to label slot types based on contextual information as well as sentence structures instead of trigger gazetteers only.,1,2016
P16-1005,"we attempt to combine multi-prototype approaches (e.g., (reisinger and mooney, 2010)) to better disambiguate senses of trigger words.",1,2016
P16-1006,"in the future we will apply visual pattern recognition and concept detection techniques to perform deep content analysis of the retrieved images, so we can do matching and inference on concept/entity level instead of shallow visual similarity.",1,2016
P16-1006,our long-term goal is to extend this framework to other knowledge extraction and population tasks such as event extraction and slot filling to construct multimedia knowledge bases effectively from multiple languages with low cost.,4,2016
P16-1007,the complementary strengths of both systems suggest future work in combining these techniques.,1,2016
P16-1009,future work will explore the effectiveness of our approach in more settings.,3,2016
P16-1010,"in the future, we will extend this model to allow discontinuity on target sides and explore the possibility of directly encoding reordering information in translation rules.",1,2016
P16-1010,we are also interested in using graphs for neural machine translation to see how it can translate and benefit from graphs.,1,2016
P16-1011,our future work will extend the current approach with dialogue modeling to learn more reliable hypothesis spaces of resulting states for verb semantics.,1,2016
P16-1012,as future work we aim to make our approach completely knowledgefree by eliminating this dependency.,1,2016
P16-1012,"we will explore unsupervised acquisition of relational similarity (mikolov et al., 2013b) for this task.",1,2016
P16-1015,"finally, we would like to highlight two insights that the experiments provide.",3,2016
P16-1015,we think that the transition systems with more active tokens or the combination with edges that span over more words provide very attractive transition systems for possible future parsers.,1,2016
P16-1018,"finally, it would be interesting to investigate modeling metaphorical mappings as nonlinear mappings within the deep learning framework.",1,2016
P16-1018,our work is also directly extendable to other syntactic constructions.,4,2016
P16-1019,"in addition, we also plan to compare our work to the method of sporleder et al.(2010) as well apply our work on the idx corpus (sporleder et al., 2010) and to other languages.",3,2016
P16-1019,in future work we plan to investigate the use of sent2vec to encode larger samples of text - not only the sentence containing idioms.,2,2016
P16-1019,the focus of these future experiments will be to test how our approach which is relatively less dependent on nlp resources compares with these other methods for idiom token classification.,3,2016
P16-1019,we also plan to further analyse the errors made by our “general” model and investigate the “general” approach on the skewed part of the vnc-tokens dataset.,1,2016
P16-1019,we also plan to investigate an end-to-end approach based on deep learning-based representations to classify literal and idiomatic language use.,1,2016
P16-1020,"in future work, we will apply our method to other kinds of phrases and tasks.",4,2016
P16-1021,our proposed features can be expanded to other domains.,4,2016
P16-1023,"based on this paper, there are serveral lines of investigation we plan to conduct in the future.(i) we will attempt to support our results on artificially generated corpora by conducting experiments on real natural language data.(ii) we will study the coverage of our four criteria in evaluating word representations.(iii) we modeled the four criteria using separate pcfgs, but they could also be modeled by one single unified pcfg.",2,2016
P16-1023,but the validity of the assumption that embedding spaces can be decomposed into “linear” subspaces should be investigated in the future.,1,2016
P16-1023,see rothe et al.(2016) and rothe and schutze (2016) for work that makes ¨ similar assumptions.,1,2016
P16-1024,"encouraged by the excellent results, we also plan to test the portability of the approach to more language pairs, and other tasks and applications.",3,2016
P16-1024,"in future work, we plan to investigate other methods for seed pairs selection, settings with scarce resources (agic et al., 2015; zhang et al., 2016), ′ other context types inspired by recent work in the monolingual settings (levy and goldberg, 2014a; melamud et al., 2016), as well as model adaptations that can work with multi-word expressions.",1,2016
P16-1025,"in the future, we will extend this framework to other information extraction tasks.",4,2016
P16-1026,"in future work, we will investigate scalable and parallel model learning to explore the performance of our model for large-scale real-time event extraction and visualization.",1,2016
P16-1028,"in future work, we plan to apply semlms to other semantic related nlp tasks e.g.machine translation and question answering.",4,2016
P16-1030,"our work also empirically explores different methods of inducing and modelling these connotation frames, incorporating the interplay between relations within frames.",1,2016
P16-1031,"first, since deep learning may obtain better generalization on large-scale data sets (bengio, 2009), a straightforward path of the future research is to apply the proposed btdnns for domain adaptation on a much larger industrial-strength data set of 22 domains (glorot et al., 2011).",4,2016
P16-1031,"second, we will try to investigate the use of the proposed approach for other kinds of data set, such as 20 newsgroups and reuters21578 (li et al., 2012; zhuang et al., 2013).",2,2016
P16-1032,"experiments demonstrated that the approach can infer implied sentiment and point toward potential directions for future work, including joint entity detection and incorporation of more varied types of factual relationships.",1,2016
P16-1033,"for future work, we would like to advance this study in the following directions.",6,2016
P16-1033,"our plan is to study which syntactic structures are more suitable for human annotation, and balance informativeness of a candidate task and its suitability for human annotation.",1,2016
P16-1036,"also, we would like to experiment with other deep neural architectures such as recurrent neural networks, long short term memory networks, etc.to form the sub-networks.",1,2016
P16-1036,"as part of future work, we would like to enhance scqa with the meta-data information like categories, user votes, ratings, user reputation of the questions and answer pairs.",2,2016
P16-1038,"4 future directions for this work include further improving the number and quality of g2p models, as well as performing external evaluations of the models in speech- and text-processing tasks.",3,2016
P16-1038,we plan to use the presented data and methods for other areas of multilingual natural language processing.,2,2016
P16-1043,the approach is quite general and we hope that this paper will encourage more nlp researchers to explore curriculum learning in their own works.,1,2016
P16-1044,"potential future work include: 1) evaluating the proposed approaches for different tasks, such as community qa and textual entailment; 2) including the sentential attention mechanism; 3) integrating the hybrid and the attentive mechanisms into a single framework.",3,2016
P16-1045,"we believe it offers interesting challenges that go beyond the scope of this paper – such as question parsing, or textual entailment – and are exciting avenues for future research.",5,2016
P16-1046,it would also be interesting to apply the neural models presented here in a phrase-based setting similar to lebret et al.(2015).,1,2016
P16-1046,"one way to improve the word-based model would be to take structural information into account during generation, e.g., by combining it with a tree-based algorithm (cohn and lapata, 2009).",1,2016
P16-1047,can we transfer our model to other languages?,5,2016
P16-1047,future work can be directed towards answering two main questions: can we improve the performance of our classifier?,5,2016
P16-1047,"most importantly, we are going to test the model using word-embedding features extracted from a bilingual embedding space.",3,2016
P16-1047,"to do this, we are going to explore whether adding language-independent structural information (e.g.universal dependency information) can help the performance on exact scope matching.",1,2016
P16-1049,we leave better triggering component and multiple rounds of conversation handling to be addressed in our future work.,1,2016
P16-1051,this unified information-theoretic perspective may eventually allow us to identify further systematic patterns of information exchange between dialogue participants.,1,2016
P16-1053,"moreover, applying the models to other tasks, such as semantic relatedness measurement and paraphrase identification, would also be interesting attempts.",4,2016
P16-1053,"so, we are going to extend sin to tree-based sin for sentence modeling as future work.",1,2016
P16-1056,"finally, we use our best performing neural network model to generate a corpus of 30m question and answer pairs, which we hope will enable future researchers to improve their question answering systems.",1,2016
P16-1058,"an obvious direction for future work is to automatically induce such a strategy, based on confidence measures that automatically predict the trust-worthiness of a word for an object.",1,2016
P16-1058,"another extension that we have planned for future work is to implement relational expressions, similar to (kennington and schlangen, 2015).",1,2016
P16-1059,"deep learning has recently been used in mutliple nlp applications, including parsing (chen and manning, 2014) and translation (bahdanau et al., 2014).",4,2016
P16-1059,"the dynamics of the underlying state can be modeled by recurrent neural networks or lstms (bahdanau et al., 2014).",1,2016
P16-1059,"the model can also readily be applied to other structured prediction problems in language processing, such as selecting antecedents in coreference resolution.",4,2016
P16-1062,"future work can also explore finer clusters within these datasets, such as clustering clue by word sense of the answers and toon by joke sense.",2,2016
P16-1062,future work can manipulate datasets’ text properties to confirm that a specific property is the cause of observed differences in clustering.,2,2016
P16-1062,"future work will explore further how the goals of short text authors translate into measurable properties of the texts they write, and how measuring those properties can help predict which similarity metrics and clustering methods will combine to provide the best performance.",5,2016
P16-1065,"as next steps, we plan to explore model variations to support a wider range of use cases.",1,2016
P16-1065,"in the spirit of treating links probabilistically, we plan to explore application of the model in suggesting links that do not exist but should, for example in discovering missed citations, marking social dynamics (nguyen et al., 2014), and identifying topically related content in multilingual networks of documents (hu et al., 2014).",4,2016
P16-1065,"we are also interested in modeling changing topics and vocabularies (blei and lafferty, 2006; zhai and boyd-graber, 2013).",1,2016
P16-1066,this is a possible future research direction.,6,2016
P16-1066,this simple lexicon-based method could be further enhanced by incorporating arabic valence shifters and certain linguistic rules to handle them.,1,2016
P16-1066,"we also plan to make the classification multi-way: positive, negative, neutral and mixed.",1,2016
P16-1073,"furthermore, we also want to employ sentence rewriting techniques for other challenges in semantic parsing, such as the spontaneous, unedited natural language input, etc.",5,2016
P16-1073,"in future work, we will explore more advanced sentence rewriting methods.",1,2016
P16-1075,"future work will aim to study different dimensions of the prompt (e.g.genre, topic) using multitask learning at a finer level.",1,2016
P16-1075,we also aim to further study the characteristics of the multi-task model in order to determine which features transfer well across tasks.,1,2016
P16-1076,future work could be extending the proposed method to handle more complex questions.,1,2016
P16-1080,"another avenue of future research can look at the annotators’ own traits and how these relate to perception (flekova et al., 2015).",5,2016
P16-1081,the idea of shared model adaptation is general and can be further extended.,1,2016
P16-1082,we are releasing human annotations of concept nodes and possible dependency edges learned from the acl anthology as well as implementations of the methods described in this paper to enable future research on modeling scientific corpora.,2,2016
P16-1083,"in future work, we plan to improve performace of feature weight tuning and investigate more general features.",1,2016
P16-1085,"as future work, we plan to investigate the possibility of designing word representations that best suit the wsd framework.",1,2016
P16-1087,"in future work, we plan to explore the effects of pre-training (bengio et al., 2009) and scheduled sampling (bengio et al., 2015) for training our lstm network.",3,2016
P16-1087,we would also like to explore re-ranking methods for our problem.,1,2016
P16-1087,"with respect to the fine-grained opinion mining task, a potential future direction to be able to model overlapping and embedded entities and relations and also to extend this model to handle cross-sentential relations.",1,2016
P16-1088,"in future work, we will develop lexical features which are captured by nonlocal dependencies.",1,2016
P16-1089,"however, it would be interesting to see how siamese cbow embeddings would affect results in supervised tasks.",5,2016
P16-1089,"it would be interesting to see how embeddings for larger pieces of texts, such as documents, would perform in document clustering or filtering tasks.",5,2016
P16-1090,"a natural next step is to explore our framework with additional modeling improvements—especially in dealing with context, structure, and noise.",1,2016
P16-1090,"another avenue for providing user confidence is probabilistic calibration (platt, 1999), which has been explored more recently for structured prediction (kuleshov and liang, 2015).",1,2016
P16-1091,"firstly, the architectures based on a single convolutional layer and a single bi-directional recurrent layer in the proposed models can be extended by adding more layers as well as utilizing more advanced components including hierarchical cnns (kalchbrenner et al., 2014b) to deal with utterance compositionalities or attention mechanisms (denil et al., 2012) to focus on more important segments in dialogue sequences.",1,2016
P16-1091,"furthering this work, there would be still much room for improvement in future.",6,2016
P16-1091,the other direction of our future work is to investigate joint models for tracking dialogue topics and states simultaneously.,1,2016
P16-1093,"in future work, we plan to conduct larger-scale evaluations to further validate these results, and to apply these methods on other common learner errors.",3,2016
P16-1095,the future work has two main directions.one is semi-supervised learning.,1,2016
P16-1095,the other direction is to promote the random walk step.,1,2016
P16-1097,"in the future, we plan to apply our approach to real-world non-parallel corpora to further verify its effectiveness.",2,2016
P16-1097,"it is also interesting to extend the phrase translation model to more sophisticated models such as ibm models 2-5 (brown et al., 1993) and hmm (vogel and ney, 1996).",1,2016
P16-1098,"in future work, we would like to investigate our model on more text matching tasks.",4,2016
P16-1099,"additionally we recognize that the hashtag inventory used to discover business accounts from job-related topics might need to change over time, to achieve robust performance in the future.",1,2016
P16-1099,this is left for future work.,6,2016
P16-1099,we did not study whether providing contextual information in our humans-in-the-loop framework would influence the model performance.,1,2016
P16-1100,"for future work, we hope to be able to improve the memory usage and speed of purely character-based models.",1,2016
P16-1101,another interesting direction is to apply our model to data from other domains such as social media (twitter and weibo).,4,2016
P16-1101,"since our model does not require any domain- or taskspecific knowledge, it might be effortless to apply it to these domains.",4,2016
P16-1101,there are several potential directions for future work.,6,2016
P16-1102,further exploration of different topic vector representations and their combinations is necessary in future work.,1,2016
P16-1103,"therefore, one important extension of our work is to further study the interaction between our model and the underlying language model.",1,2016
P16-1104,we propose to augment this work in future by exploring deeper graph and gaze features.,1,2016
P16-1107,"also, we will apply our contextaware argumentative relation mining to different argument mining corpora to further evaluate its generality.",4,2016
P16-1107,our next step will investigate uses of topic segmentation to identify context sentences and compare this linguistically-motivated approach to our current window-size heuristic.,3,2016
P16-1107,the results obtained in this preliminary study are promising and encourage us to explore more directions to enable contextual features.,1,2016
P16-1107,we plan to follow prior research on graph optimization to refine the argumentation structure and improve argumentative relation prediction.,1,2016
P16-1108,"in the future, we plan to expand our method to predict morphological analyses, as well as to incorporate other information such as parts-of-speech.",1,2016
P16-1110,"finally, with slight changes to what the system considers a document, we believe alto can be extended to nlp applications other than classification, such as named entity recognition or semantic role labeling, to reduce the annotation effort.",4,2016
P16-1110,we can further improve alto to help users gain better and faster understanding of text corpora.,2,2016
P16-1111,access to longer time-spans—along with varying data sources such as grants and patents—would also allow us to more completely model the trajectory of a topic as it moves from being active area of research to potentially impacting commercial industries and economic development.,2,2016
P16-1112,"as part of future work, it would be beneficial to investigate the effect of automatically generated training data for error detection (e.g., rozovskaya and roth (2010)).",2,2016
P16-1114,the future work includes using those prediction models in a real service to take targeted actions to users who are likely to stop using intelligent assistants.,1,2016
P16-1115,all this is future work.,6,2016
P16-1115,"the design of the model, as mentioned in the introduction, makes it amenable for use in interactive systems that learn; we are currently exploring this avenue.",4,2016
P16-1116,"future work may be done to integrate our method into a joint approach, use some global feature, which may improve our performance.",1,2016
P16-1117,"in the future, we plan to extend our model to incorporate coreference resolution and intersentential zero anaphora resolution.",1,2016
P16-1119,"future work can use our annotated corpus to develop classifiers that deal better with prepositional and adjectival modifiers, which require deeper semantic analysis.",1,2016
P16-1120,"as future work, we would like to verify the effectiveness of the proposed models for other datasets or other cross-lingual tasks, such as cross-lingual document classification (ni et al., 2009; platt et al., 2010; ni et al., 2011; smet et al., 2011) and cross-lingual information retrieval (vulic et al., ′ 2013).",3,2016
P16-1122,in the future we plan to apply our inner-attention intuition to other neural networks such as cnn or multi-layer perceptron.,4,2016
P16-1122,our models can be further extended to other nlp tasks such as recognizing textual entailments where attention mechanism is important for sentence representation.,4,2016
P16-1123,"we expect this sort of architecture to be of interest also beyond the specific task of relation classification, which we intend to explore in future work.",4,2016
P16-1124,it will be interesting to study whether one can merge the clustering step and the coupling step so as to have a richer inter-task dependent structure.,1,2016
P16-1124,there are still many interesting topics to study.,6,2016
P16-1124,we will investigate such topics in our future work.,6,2016
P16-1124,"we would like to design new mechanisms to discover loosely correlated relations, and investigate whether coupling such relations still provides benefits.",1,2016
P16-1125,"lastly, it is important to evaluate the impact of the proposed largercontext models in downstream tasks such as machine translation and speech recognition.",3,2016
P16-1125,"second, more analysis, beyond the one based on part-of-speech tags, should be conducted in order to better understand the advantage of such larger-context models.",1,2016
P16-1125,"to explore the potential of such a model, there are several aspects in which more research needs to be done.",1,2016
P16-1127,"but there are other possible choices that might make the encoding even more easily learnable by the lstm, and we would like to explore those in future work.",1,2016
P16-1127,"in order to improve performance, other promising directions would involve adding re-reranking techniques and extending our neural networks with attention models in the spirit of (bahdanau et al., 2015).",1,2016
P16-1129,another important direction is to focus on the construction of datasets in larger scale.,2,2016
P16-1129,one feasible approach is to use a speech recognition system on live videos or broadcasts of sports games to collect huge amount of transcripts as our raw data source.,2,2016
P16-1129,we would like to extend our system to produce sports news beyond pure sentence extraction.,2,2016
P16-1130,"for future work, we will explore more sophisticated features for the csrs model, such as syntactic dependency relationships and head words, since only simple lexical features are used in the current incarnation.",1,2016
P16-1133,"in addition, we will also explore the possibility of using more complex neural network models such as convolutional neural network and recurrent neural network to build bilingual document representation system.",1,2016
P16-1133,our future work will focus on extending the bilingual document representation model into the multilingual scenario.,1,2016
P16-1133,we will try to learn a single embedding space for a source language and multiple target languages simultaneously.,1,2016
P16-1134,"in future work, we are interested in exploring better ways of utilizing vast unlabelled data to improve grsemi-crfs, e.g., to learn phrase embeddings from unlabelled data or designing a semisupervised version of grsemi-crfs.",1,2016
P16-1135,"although we have focused exclusively on wikipedia, these methods could be adapted to other domains and languages.",4,2016
P16-1135,"in the future, we may improve this step using a machine learning approach.",1,2016
P16-1135,"linguistic expressions of causality in other languages is another avenue for future research, and it would be interesting to note if other languages have the same variety of expression.",2,2016
P16-1135,to evaluate on the intermediate step would have required an additional annotation process.,3,2016
P16-1135,"ultimately, the focus of this work is to improve detection of causal relations.",1,2016
P16-1136,"in the future, we would like to study the impact of relation paths for additional basic kb embedding models and knowledge domains.",1,2016
P16-1137,"in future work, we will explore how to use our model to improve downstream nlp tasks, and consider applying our methods to other knowledge bases.",4,2016
P16-1140,it would also be a promising direction to incorporate the factor of language typological diversity when designing advanced word representation model for languages other than english.,1,2016
P16-1141,the two statistical laws we propose have strong implications for future work in historical semantics.,1,2016
P16-1141,we extend these lines of work by rigorously comparing different approaches to quantifying semantic change and by using these methods to propose new statistical laws of semantic change.,1,2016
P16-1141,we show how distributional methods can reveal statistical laws of semantic change and offer a robust methodology for future work in this area.,1,2016
P16-1142,we believe that combining semantic roles and other semantic representation in a similar fashion to the one used in this paper could be useful to infer knowledge beyond spatial inferences.,1,2016
P16-1143,another obvious extension would be to further explore the alignment component of hca-wsi.,1,2016
P16-1143,"in particular, we intend to expand lexsemtm by applying hca-wsi across the vocabularies of languages other than english, and also to multiword lemmas.",4,2016
P16-1143,the most immediate extension of our work would be to apply our sense learning method to a broader range of data.,4,2016
P16-1143,"this could be used to expand existing sense inventories with new senses, for example using the methodology of cook et al.(2013).",2,2016
P16-1145,"in light of this finding, we suggest some focus areas for future research.",6,2016
P16-1147,"in future work, we will extend this idea beyond sequence modeling to improve models in nlp that utilize parse trees as features.",1,2016
P16-1151,"the methodology is also useful for observational data—for studying the effects of complicated treatments, such as how a legislator’s roll call voting record affects their electoral support.",2,2016
P16-1153,"future work includes: (i) adding an attention model to robustly analyze which part of state/actions text correspond to strategic planning, and (ii) applying the proposed methods to more complex text games or other tasks with actions defined through natural language.",4,2016
P16-1154,"for future work, we will extend this idea to the task where the source and target are in heterogeneous types, for example, machine translation.",4,2016
P16-1156,future work will consider the role of derivational morphology in embeddings as well as noncompositional cases of inflectional morphology.,1,2016
P16-1159,"as our approach is transparent to loss functions and architectures, we believe that it will also benefit more end-to-end neural architectures for other nlp tasks.",4,2016
P16-1159,"in the future, we plan to test our approach on more language pairs and more end-to-end neural mt systems.",3,2016
P16-1159,it is also interesting to extend minimum risk training to minimum risk annealing following smith and eisner (2006).,1,2016
P16-1160,"however, this has allowed us a more fine-grained analysis, but in the future, a setting where the source side is also represented as a character sequence must be investigated.",1,2016
P16-1162,"one avenue of future research is to learn the optimal vocabulary size for a translation task, which we expect to depend on the language pair and amount of training data, automatically.",2,2016
P16-1164,"on a more general level, we believe that quotation detection is interesting as a representative of tasks involving long sequences, where markov assumptions become inappropriate.",5,2016
P16-1165,"in the future, we would like to combine crfs with lstms for doing the two steps jointly, so that the lstms can learn the embeddings using the global thread-level feedback.",1,2016
P16-1165,"we would also like to apply our models to conversations, where the graph structure is extractable using the meta data or other clues, e.g., the fragment quotation graphs for email threads (carenini et al., 2008).",4,2016
P16-1166,"a next major step in our research agenda is to integrate se type information into various applications, including argument mining, temporal reasoning, and summarization.",1,2016
P16-1166,"among others, we plan to create subtypes of the state label, which currently subsumes clauses stativized by negation, modals, lexical information or other aspectual operators.",1,2016
P16-1166,"here we focus on the automatic identification of se types, leaving the identification of discourse modes to future work.",1,2016
P16-1167,"finally, we provide a dataset depicting 12 scenarios with ～1.5 m images for future research.",2,2016
P16-1167,future directions could include exploring nuances in the type of temporal knowledge that can be learned across different scenarios.,5,2016
P16-1171,"in the future, we plan to build a resource for modeling physical causality for action verbs.",2,2016
P16-1172,"for example, we plan to incorporate lexicalsemantic information in the feature representation and leverage large-scale unsupervised pretraining.",1,2016
P16-1172,the promising results we obtained for summarization with a basic learner (see section 4.3) encourage future work on plugging in more sophisticated supervised learners in our framework.,1,2016
P16-1174,"instead of the sparse indicator variables used here, it may be better to decompose lexeme tags into denser and more generic features of tag components9 (e.g., part of speech, tense, gender, case), and also use corpus frequency, word length, etc.",1,2016
P16-1175,"we also leave as future work the integration of this model into an adaptive system that tracks learner understanding and creates scaffolded content that falls in their zone of proximal development, keeping them engaged while stretching their understanding.",1,2016
P16-1175,we plan a deeper investigation into how learners detect and combine cues for incidental comprehension.,5,2016
P16-1176,we also plan to investigate how the results vary when limited to specific l1s.,5,2016
P16-1176,"we are interested in expanding the preliminary results of this work: we intend to replicate the experiments with more languages and more domains, investigate additional varieties of constrained language and employ more complex lexical, syntactic and discourse features.",2,2016
P16-1178,"as future work, we plan to investigate other linguistic representations that can improve the automated extraction of the proposed aspects to better predict the article’s perceived quality.",1,2016
P16-1179,13 the analysis of the acquired information and the error analysis show several avenues for future work.,1,2016
P16-1179,"first larger corpora should allow to increase the applicability of the similarity resource, and specially, that of the dependency templates, and also provide better quality resources.",2,2016
P16-1180,another task for future work is semantic alignment.,1,2016
P16-1180,we leave these to future work.,6,2016
P16-1183,we intend to explore these possibilities in future work.,6,2016
P16-1184,our results provide a strong baseline for future work in weakly supervised morphological tagging.,1,2016
P16-1185,"another interesting direction is to enhance the connection between source-to-target and target-tosource models (e.g., letting the two models share the same word embeddings) to help them benefit more from interacting with each other.",1,2016
P16-1185,"as our method is sensitive to the oovs present in monolingual corpora, we plan to integrate jean et al.(2015)’s technique on using very large vocabulary into our approach.",1,2016
P16-1185,it is also necessary to further validate the effectiveness of our approach on more language pairs and nmt architectures.,2,2016
P16-1186,"an interesting future direction is to combine complementary approaches, either through combined parameterization (e.g.hierarchical softmax with differentiated capacity per word) or through a curriculum (e.g.transitioning from target sampling to regular softmax as training progresses).",1,2016
P16-1186,further promising areas are parallel training as well as better rare word modeling.,1,2016
P16-1187,"as future work, we plan to examine the use of a voting scheme for combining the output of complementary dsms.",3,2016
P16-1187,"moreover, we also plan to combine additional sources of information for building the models, such as multilingual resources or translation data, to improve even further the compositionality prediction.",2,2016
P16-1191,"in follow-up work, we aim to apply our embedding method on smaller, yet gold-standard corpora such as semcor (miller et al., 1994) and streusle (schneider and smith, 2015) to examine the impact of the corpus choice in detail and extend the training data beyond wordnet vocabulary.",2,2016
P16-1192,it would also be interesting to combine the strengths of the condensed and sibling-finder algorithms for further efficiency gains.,1,2016
P16-1193,"within this new vector space, the entailment operators and inference equations apply, thereby generalising naturally from these lexical representations to the compositional semantics of multi-word expressions and sentences.",1,2016
P16-1194,another possible way to extend the model is to consider modeling long distance dependency between latent states.,1,2016
P16-1194,this may further improve the model’s performance.,1,2016
P16-1195,"in future work, we plan to develop better models for capturing the structure of the input, as well as extend the use of our system to other applications such as automatic documentation of source code.",1,2016
P16-1196,"in addition we plan to further investigate how to fine-tune some of the hyper parameters of the cpm such as spline scaling, single global scaling factor, convergence tolerance, and initialization of the latent trace with a centroid.",1,2016
P16-1196,"in addition, to aid cpm convergence to a good local optimum, in future work we will investigate dimensionality reduction approaches that are reversible such as principal component analysis (pearson, 1901) and other pre-processing approaches similar to (listgarten, 2007), where the training data set is coarsely pre-aligned and pre-scaled based on the center of mass of the time series.",1,2016
P16-1196,"in subsequent work, we would like to explore alternatives for enhancing cpms by incorporating contextual features in the training data set such as timing of hand movements, and preceding, succeeding, and co-occurring facial expressions.",1,2016
P16-1196,"while this work used the latent trace as the basis for animation, in future work, we also plan to explore methods for sampling from the model to produce variations in face and head movement.",1,2016
P16-1197,"deriving sentiment labels for supervised training is an important topic for future study, as is inferring the sentiment of published news from stock price fluctuations instead of the reverse.",1,2016
P16-1198,"in addition, we are currently exploring potential extensions of the techniques presented in this paper to higher order, projective and non-projective, dependency parsing.",1,2016
P16-1198,it may also be utilized as an inference/initialization subroutine as a part of more complex approximation frameworks such as belief propagation (e.g.,1,2016
P16-1198,we believe this contribution has the potential to affect future research on additional nlp problems.,5,2016
P16-1198,we intend to investigate all of these directions in future work.,6,2016
P16-1199,"in the next step, we plan to exploit fine-grained discourse structures, e.g., dialogue acts (ritter et al., 2010), and propose a unified model that jointly inferring discourse roles and topics of posts in context of conversation tree structures.",1,2016
P16-1200,"in the future, we will explore the following directions: ?",6,2016
P16-1200,"in the future, we will incorporate our instance-level selective attention technique with those models for relation extraction.",1,2016
P16-1200,we will explore our model in other area such as text categorization.?,4,2016
P16-1201,"in the future, we will extend this work to the complete event extraction task.",1,2016
P16-1201,the key of this research is to detect events in fn.,1,2016
P16-1201,we plan to refine the event schemas by the finer-grained frames defined in fn (i.e.,1,2016
P16-1202,our future plan is to apply this model to general arithmetic problems which require multiple applications of formulas.,4,2016
P16-1203,our future research will test the correlation between the polarity and the name of a fictional character beyond the movie domain.,3,2016
P16-1208,"for future work, we intend to study the problem in the context of other languages.",2,2016
P16-1210,"in the future, we would like to extend this work in several ways.",6,2016
P16-1210,"second, since the svd basis-shift seems to be the source of much of the gains, we would like to explore replacing the svd with other algorithms, such as independent component analysis.",1,2016
P16-1212,"in the future, we will conduct experiments on large corpus in different domains.",2,2016
P16-1214,our future research includes improving the search performance for embeddings with heavy-tailed distributions and creating embeddings that can keep both task quality and search performance high.,1,2016
P16-1214,"since we need to implement additional glue codes for running flann and sash, our code would be useful for researchers who want to compare their results with ours.",3,2016
P16-1215,"we expect that several further studies will use the new dataset not only for distributed representations of relational patterns but also for other nlp tasks (e.g., paraphrasing).",2,2016
P16-1216,"in future work, we look to incorporate methods that incur less cost, possibly tolerating some error in the formation of sentences without significantly degrading performance.",1,2016
P16-1216,we look to devote future work to handling such cases.,6,2016
P16-1217,"in future work, we will explore to make use of all the three kinds of labels together to improve the users’ experience when they want to browse, understand and leverage the topics.",2,2016
P16-1217,"in future work, we will try to make the summary label more coherent by considering the discourse structure of the summary and leveraging sentence ordering techniques.",1,2016
P16-1220,our future work involves exploring other alternatives such as treating structured and unstructured data as two independent resources in order to overcome the knowledge gaps in either of the two resources.,2,2016
P16-1223,"as future work, we need to consider how we can utilize these datasets (and the models trained upon them) to help solve more complex rc reasoning tasks (with less annotated data).",2,2016
P16-1224,"looking forward, we believe that the illg setting is worth studying and has important implications for natural language interfaces.",5,2016
P16-1224,monroe and potts (2015) uses learning to improve the pragmatics model.,1,2016
P16-1225,"finally, we would like to test our model using other representations of word-form and wordmeaning.",3,2016
P16-1225,future work could also test whether a more interpretable meaningspace representation such as that provided by binary wordnet feature vectors reveals patterns of systematicity not found using a distributional semantic space.,3,2016
P16-1225,future work may investigate to what extent the smlkr model can predict human intuitions about form-meaning systematicity in language.,1,2016
P16-1225,"however, it would be interesting to verify our results in a phonological setting, perhaps using a monodialectal corpus.",3,2016
P16-1225,we succeed in applying this algorithm to the problem of finding form-meaning systematicity in the monomorphemic english lexicon.,4,2016
P16-1226,"finally, our architecture seems straightforwardly applicable for multi-class classification, which, in future work, could be used to classify term-pairs to multiple semantic relations.",4,2016
P16-1227,a further avenue of future research is improving models such as that presented in elliott et al.(2015) by crucial components of neural mt such as “attention mechanisms”.,1,2016
P16-1227,a similar mechanism is used in xu et al.(2015) to decide which part of the image should influence which part of the generated caption.,1,2016
P16-1227,combining these two types of attention mechanisms in a neural caption translation model is a natural next step in caption translation.,1,2016
P16-1227,"in future work, we plan to evaluate our approach in more naturalistic settings, such machine translation for captions in online multimedia repositories such as wikimedia commons16 and digitized art catalogues, as well as e-commerce localization.",3,2016
P16-1227,"learning semantically informative distance metrics using deep learning techniques is an area under active investigation (wu et al., 2013; wang et al., 2014; wang et al., 2015).",1,2016
P16-1228,"finally, we also would like to generalize our framework to automatically learn the confidence of different rules, and derive new rules from data.",1,2016
P16-1228,"the encouraging empirical results indicate a strong potential of our approach for improving other application domains such as vision tasks, which we plan to explore in the future.",4,2016
P16-1228,we plan to explore these diverse knowledge representations to guide the dnn learning.,1,2016
p17-1002,"its suitability for the end-to-end learning task is scope for future work, but its adequacy for component classification and relation identification has been investigated in potash et al.(2016).",1,2017
p17-1004,"hence, the word-level multi-lingual attention, which may discover implicit alignments between words in multiple languages, will further improve multi-lingual relation extraction.",1,2017
p17-1004,"in future, we will extend mnre to more languages and explore its significance.",2,2017
p17-1004,"we will explore the effectiveness of word-level multilingual attention for relation extraction as our future work.(2) mnre can be flexibly implemented in the scenario of multiple languages, and this paper focuses on two languages of english and chinese.",2,2017
p17-1004,"we will explore the following directions as future work: (1) in this paper, we only consider sentence-level multi-lingual attention for relation extraction.",2,2017
p17-1005,"aside from relaxing strict isomorphism, we would also like to perform crossdomain semantic parsing where the first stage of the semantic parser is shared across domains.",1,2017
p17-1006,"future work will focus on other potential sources of morphological knowledge, porting the framework to other morphologically rich languages and downstream tasks, and on further refinements of the post-processing specialisation algorithm and the constraint selection.",2,2017
p17-1010,"second, we will develop other neural network architecture to make it more appropriate for zero pronoun resolution task.",1,2017
p17-1010,"the future work will be carried out on two main aspects: first, as experimental results show that the unknown words processing is a critical part in comprehending context, we will explore more effective way to handle the unk issue.",1,2017
p17-1011,"in future, we plan to exploit discourse mode identification for providing novel features for more downstream nlp applications.",1,2017
p17-1012,"also, we plan to investigate the effectiveness of our architecture on other sequence-tosequence tasks, e.g.summarization, constituency parsing, dialog modeling.",4,2017
p17-1014,"for future work, we would like to extend our work to different meaning representations such as the minimal recursion semantics (mrs; copestake et al.(2005)).",1,2017
p17-1014,"taking a step further, we would like to apply our models on semantics-based machine translation using mrs as an intermediate representation between pairs of languages, and investigate the added benefit compared to directly translating the surface strings, especially in the case of distant language pairs such as english and japanese (siegel, 2000).",4,2017
p17-1016,"in future work, it would be useful to investigate models based on morphemes, rather than characters, which offers potentially superior performance for complex and rare words (luong et al., 2013), which are common in poetry.",1,2017
p17-1018,"as for future work, we are applying the gated self-matching networks to other reading comprehension and question answering datasets, such as the ms marco dataset (nguyen et al., 2016).",2,2017
p17-1019,"and the future work includes: a) lots of questions cannot be answered directly by facts in a kb (e.g.鈥淲ho is jet li鈥檚 father-in-law?鈥), we plan to learn qa system with latent knowledge (e.g.",5,2017
p17-1019,"kb embedding (bordes et al., 2013)); b) we plan to adopt memory networks (sukhbaatar et al., 2015) to encode the temporary kb for each question.",1,2017
p17-1020,"in future work we would like to deepen the use of structural clues and answer questions over multiple documents, using paragraph structure, titles, sections and more.",1,2017
p17-1020,incorporating coreference resolution would be another important direction for future work.,1,2017
p17-1023,"as we have tested these approaches on a rather small vocabulary, which may limit generality of conclusions, future work will be devoted to scaling up these findings to larger test sets, as e.g.recently collected through conversational agents (das et al., 2016) that circumvent the need for human-human interaction data.",2,2017
p17-1024,"we note that the addition of this extra step will move this work closer to the textual/visual explanation research (e.g., (park et al., 2016; selvaraju et al., 2016)).",4,2017
p17-1024,"with our work, we would like to push the community to think of ways that models can better merge language and vision modalites, instead of merely using one to supplement the other",1,2017
p17-1027,"while we used a perceptron classifier for our experiments, our oracle could also be used in neuralnetwork implementations of greedy transitionbased parsing (chen and manning, 2014; dyer et al., 2015), providing an interesting avenue for future work.",4,2017
p17-1028,"we also plan to investigate extension of the crowd component in our hmm method with hierarchical models, as well as a fully-bayesian approach.",1,2017
p17-1028,"we expect our methods to be applicable to and similarly benefit other sequence labeling tasks, such as pos tagging and chunking (hovy et al., 2014).",4,2017
p17-1029,"future work will adapt this framework to other sequence transduction scenarios such as machine translation, dialogue generation, question answering, where continuous and discrete latent variables can be abstracted to guide sequence generation.",4,2017
p17-1032,"finally, the optimization of the kda methodology through the suitable parallelization on multicore architectures, as well as the exploration of mechanisms for the dynamic reconstruction of kernel spaces (e.g., operating over hn y) also constitute interesting future research directions on this topic.",1,2017
p17-1032,"future work will address experimentations with larger scale datasets; moreover, it is interesting to experiment with more landmarks in order to better understand the trade-off between the representation capacity of the nystrom approximation 篓 of the kernel functions and the over-fitting that can be introduced in a neural network architecture.",2,2017
p17-1034,we are going to explore more effective models in future.,1,2017
p17-1035,"our future agenda includes: (a) optimizing the cnn framework hyper-parameters (e.g., filter width, dropout, embedding dimensions, etc.)to obtain better results, (b) exploring the applicability of our technique for documentlevel sentiment analysis and (c) applying our framework to related problems, such as emotion analysis, text summarization, and questionanswering, where considering textual clues alone may not prove to be sufficient.",1,2017
p17-1037,"for our immediate future work, we plan to embed the topic and user vectors to create a crosstopic stance detector.",2,2017
p17-1037,"it is possible to generalize our work to model heterogeneous signals, such as interests and behaviors of people, for example, 鈥渢hose who are interested in a also support b,鈥 and 鈥渢hose who favor a also vote for b鈥.",1,2017
p17-1037,"therefore, we believe that our work will bring about new applications in the field of nlp and other disciplines.",4,2017
p17-1038,"in the future, we will use the proposed automatically data labeling method to more event types and explore more models to extract events by using automatically labeled data.",1,2017
p17-1039,in the future we will try our analytical method on other parts of language.,2,2017
p17-1042,"finally, we would like to apply our model in the decipherment scenario (dou et al., 2015).",4,2017
p17-1042,"in addition to that, we would like to explore non-linear transformations (lu et al., 2015) and alternative dictionary induction methods (dinu et al., 2015; smith et al., 2017).",1,2017
p17-1042,"in the future, we would like to delve deeper into this direction and fine-tune our method so it can reliably learn high quality bilingual word embeddings without any bilingual evidence at all.",1,2017
p17-1043,but it would also be fairly easy to add gazetteer information to the network features in order to remove the need for ner preprocessing.,1,2017
p17-1043,there are changes which could be made to eliminate all pre-processing and to further improve parser performance.,1,2017
p17-1045,"effective implementation of this, however, requires the e2e agent to learn quickly and this is the research direction we plan to focus on in the future.",1,2017
p17-1046,"in the future, we shall study how to model logical consistency of responses and improve candidate retrieval.",1,2017
p17-1047,"additionally, by collecting a second dataset of captions for our images in a different language, such as spanish, our model could be extended to learn the acoustic correspondences for a given object category in both languages.",1,2017
p17-1047,"the same framework we use here could be extended to video, enabling the learning of actions, verbs, environmental sounds, and the like.",4,2017
p17-1048,"future work will apply this technique to the other languages including english, where we have to solve an issue of long sequence lengths, which requires heavy computation cost and makes it difficult to train a decoder network.",4,2017
p17-1049,"the major trends relate to loan translations (jahr, 1999), or the impact of canonical texts, such as luther鈥檚 translation of the bible to german (russ, 1994) or the case of the king james translation to english (crystal, 2010).",2,2017
p17-1049,we are presently trying to extend these results to translations in a different domain (literary texts) into a very different language (hebrew).,2,2017
p17-1049,we leave this as a direction for future research.,6,2017
p17-1050,"in future work, we also plan to explore the role of native and second language writing system characteristics in second language reading.",3,2017
p17-1050,"more broadly, our methodology introduces parallels with production studies in nlp, creating new opportunities for integration of data, methodologies and tasks between production and comprehension.",4,2017
p17-1051,"for future work, we plan to address the limitations of morse: minimal supervision, greedy inference, and concatenative orthographic model.",1,2017
p17-1051,"moreover, we plan to computationally optimize the training stage for the sake of wider adoption by the community.",1,2017
p17-1053,"for future work, we will investigate the integration of our hr-bilstm into end-to-end systems.",1,2017
p17-1053,"we will also investigate new emerging datasets like graphquestions (su et al., 2016) and complexquestions (bao et al., 2016) to handle more characteristics of general qa.",2,2017
p17-1054,"in the future, we are interested in comparing the model to human annotators and using human judges to evaluate the quality of predicted phrases.鈥 our current model does not fully consider correlation among target keyphrases.",3,2017
p17-1054,it would also be interesting to explore the multiple-output optimization aspects of our model.,1,2017
p17-1054,"our future work may include the following two directions.鈥 in this work, we only evaluated the performance of the proposed model by conducting off-line experiments.",3,2017
p17-1055,"in this context, we are planning to investigate the problems that need comprehensive reasoning over several sentences.",5,2017
p17-1055,the future work will be carried out in the following aspects.,6,2017
p17-1055,"we believe that our model is general and may apply to other tasks as well, so firstly we are going to fully investigate the usage of this architecture in other tasks.",4,2017
p17-1056,"more broadly, these results suggest ways that quantitative methods can be used to make precise application of concepts like 鈥渃ultural fit鈥 at scale.",1,2017
p17-1057,going forward we would like to compare the representations learned by our model to the brain activity of people listening to speech in order to determine to what extent the patterns we found correspond to localized processing in the human cortex.,3,2017
p17-1058,"we hope that by comparing and combining our methodology with other approaches of studying dialogue, we can reach a more comprehensive and holistic understanding of this common yet mysterious human practice.",3,2017
p17-1059,"for future work, we wish to extend this model by investigating language generation conditioned on other modalities such as facial images and speech, and to applications such as dialogue generation for virtual agents.",4,2017
p17-1060,future work can include an extension of domain experts to take into account dialog history aiming for a holistic framework that can handle contextual interpretation as well.,1,2017
p17-1061,all of the above suggest a promising research direction.,6,2017
p17-1061,"in addition to dialog acts, we plan to apply our kgcvae model to capture other different linguistic phenomena including sentiment, named entities,etc.",4,2017
p17-1062,"in future work, we plan to extend hcns by incorporating lines of existing work, such as integrating the entity extraction step into the neural network (dhingra et al., 2017), adding richer utterance embeddings (socher et al., 2013), and supporting text generation (sordoni et al., 2015).",1,2017
p17-1062,"more broadly, hcns are a general model for stateful control, and we would be interested to explore applications beyond dialog systems 鈥 for example, in nlp medical settings or humanrobot nl interaction tasks, providing domain constraints are important for safety; and in resourcepoor settings, providing domain knowledge can amplify limited data.",4,2017
p17-1062,"of course, we also plan to deploy the model in a live dialog system.",4,2017
p17-1062,"we will also explore using hcns with automatic speech recognition (asr) input, for example by forming features from n-grams of the asr n-best results (henderson et al., 2014b).",1,2017
p17-1063,"finally, it would be interesting to combine our algorithm with a speech synthesis system.",1,2017
p17-1063,"furthermore, we could use this data to refine the costs c(d), c(ia) etc.for the edit operations, possibly assigning different costs to different edit operations.",1,2017
p17-1063,"in future work, we will complement the overhearer experiment presented here with an end-toend evaluation in an interactive nlg setting.",3,2017
p17-1063,it will also give us the opportunity to investigate empirically the limits of the corruption model.,1,2017
p17-1063,this will allow us to further investigate the quality of the correction strategies and refine the shortening strategy.,1,2017
p17-1064,"in our future work, we expect several developments that will shed more light on utilizing source syntax, e.g., designing novel syntactic features (e.g., features showing the syntactic role that a word is playing) for nmt, and employing the source syntax to constrain and guild the attention models.",1,2017
p17-1065,"in addition, we will apply our method to other sequenceto-sequence tasks, such as text summarization, to verify the effectiveness.",4,2017
p17-1065,"in future work, along this research direction, we will try to integrate other prior knowledge, such as semantic information, into nmt systems.",1,2017
p17-1066,"in the future, we will focus on improving the rumor detection task by exploring network representation learning framework.",1,2017
p17-1066,"moreover, we plan to investigate unsupervised models considering massive unlabeled rumorous data from social media.",1,2017
p17-1068,"another direction of future study will look at political ideology prediction in other countries and cultures, where ideology has different or multiple dimensions.",2,2017
p17-1068,"in addition, our work on user-level modeling can be integrated with work on message-level political bias to study how this is revealed across users with various levels of engagement.",1,2017
p17-1068,"while our study focused solely on text posted by the user, follow-up work can use other modalities such as images or social network analysis to improve prediction performance.",2,2017
p17-1071,"among the potential extensions of this work are the inclusion of different kinds of weights such as tf-idf, embedding relatedness and semantic relatedness.",1,2017
p17-1071,"we also intend to test other variants around the same concept, including considering the matched words and sequences to have a negative weight to balance further the weight of missing words.",3,2017
p17-1072,it remains as a further stage of analysis to understand the underlying reasons that lead to these relations between ideas.,1,2017
p17-1072,there are many potential directions to improve our method to account for complex relations between ideas.,1,2017
p17-1075,"in future work, we plan to use the analysis from the present study in constructing a system that can be applied to multiple datasets.",4,2017
p17-1079,one interesting avenue of future work is to automatically learn encodings and error correcting codes that are well-suited for the type of binary code prediction we are performing here.,1,2017
p17-1080,"another area for future work is to extend the analysis to other word representations (e.g.byte-pair encoding), deeper networks, and more semantically-oriented tasks such as semantic rolelabeling or semantic parsing.",1,2017
p17-1081,"as future work, we plan to develop a lstmbased attention model to determine the importance of each utterance and its specific contribution to each modality for sentiment classification.",1,2017
p17-1082,our hope is that these stance dimensions will be valuable as a convenient building block for future research on interactional meaning.,1,2017
p17-1084,"as a future work, we would like to explore the feasibility of marrying our semantic and neural models to exploit the benefits that each of them has to offer.",1,2017
p17-1085,"in future, we plan to explore pretraining methods for our model which were shown to improve recall on entity and relation performance by miwa and bansal (2016).",1,2017
p17-1085,"it would also be interesting to see the effect of reranking (collins and koo, 2005) on our joint model.",1,2017
p17-1085,we also plan to extend the identification of entities to full entity mention span instead of only the head phrase as in lu and roth (2015).,1,2017
p17-1085,"we also plan to use sparsemax (martins and astudillo, 2016) instead of softmax for multiple relations, as the former is more suitable for multi-label classification for sparse labels.",1,2017
p17-1086,"in the future, we wish to test these ideas in more domains, naturalize a real pl, and handle paraphrasing and implicit arguments.",3,2017
p17-1087,another possibility is to use thesauri and word vector representations together with word sense disambiguation to generate semantically similar clusters for multiple senses of words.,1,2017
p17-1087,"finally, we plan to extend the hard signed clustering presented here to probabilistic soft clustering.",1,2017
p17-1087,"furthermore, signed spectral clustering has broader applications such as cellular biology, social networking, and electricity networks.",4,2017
p17-1087,"our signed spectral clustering method could be applied to a broad range of nlp tasks, such as prediction of social group clustering, identification of personal versus non-personal verbs, and analyses of clusters which capture positive, negative, and objective emotional content.",4,2017
p17-1088,"in addition, our framework can also be applied to multi-task learning, promoting a finer sharing among different tasks.",4,2017
p17-1088,"in the future, we plan to enable itransf to perform multi-step inference, and extend the sharing mechanism to entity and relation embeddings, further enhancing the statistical binding across parameters.",1,2017
p17-1092,"these findings motivate further improvements to discourse parsing, especially for new genres.",1,2017
p17-1094,"our study opens several future directions, ranging from defining algorithms based on automatically learned loss functions to learning more effective measures from expert examples.",1,2017
p17-1095,"it would also be interesting to explore more expressive parameterizations, such recurrent neural networks for hy.",1,2017
p17-1095,our model may be useful in the context of active learning where efficient re-estimation and performance in low-data conditions are important.,4,2017
p17-1095,there are many potential avenues for future work.,6,2017
p17-1096,"in the future, we plan to apply our approach to more question answering datasets in different domains.",4,2017
p17-1096,it will also be intriguing to generalize gdans to other applications.,4,2017
p17-1098,the diversification model proposed is general enough to apply to other nlg tasks with suitable modifications and we are currently working on extending this to dialog systems and general summarization.,4,2017
p17-1102,"in the future, it would be interesting to explore the performance of positionrank on other types of documents, e.g., web pages and emails.",3,2017
p17-1103,an important direction for future work is modifying adem such that it is not subject to this bias.,6,2017
p17-1103,an important direction of future research is building models that can evaluate the capability of a dialogue system to have an engaging and meaningful interaction with a human.,3,2017
p17-1103,"such models are necessary even for creating a test set in a new domain, which will help us determine if adem generalizes to related dialogue domains.",3,2017
p17-1103,we leave investigating the domain transfer ability of adem for future work.,1,2017
p17-1104,"a parser for ucca will enable using the framework for new tasks, in addition to existing applications such as machine translation evaluation (birch et al., 2016).",4,2017
p17-1104,"future work will evaluate tupa in a multilingual setting, assessing ucca鈥檚 cross-linguistic applicability.",3,2017
p17-1104,"in addition, we will explore different conversion procedures (kong et al., 2015) to compare different representations, suggesting ways for a data-driven design of semantic annotation.",1,2017
p17-1104,"we believe ucca鈥檚 merits in providing a cross-linguistically applicable, broadcoverage annotation will support ongoing efforts to incorporate deeper semantic structures into various applications, such as sentence simplification (narayan and gardent, 2014) and summarization (liu et al., 2015).",4,2017
p17-1104,"we will also apply the tupa transition scheme to different target representations, including amr and sdp, exploring the limits of its generality.",4,2017
p17-1105,"our results demonstrate their promise for tree prediction tasks, and we believe their application to more general output structures is an interesting avenue for future work.",4,2017
p17-1106,"in the future, we plan to apply our approach to more nmt approaches (sutskever et al., 2014; shen et al., 2016; tu et al., 2016; wu et al., 2016) on more language pairs to further verify its effectiveness.",4,2017
p17-1106,it is also interesting to develop relevancebased neural translation models to explicitly control relevance to produce better translations.,1,2017
p17-1108,there is lots of future work we can do.,6,2017
p17-1112,"we believe that there are many future avenues to explore to further increase the accuracy of such parsers, including different training objectives, more structured architectures and semisupervised learning.",1,2017
p17-1113,"in the future work, we will replace the softmax function in the output layer with multiple classifier, so that a word can has multiple tags.",1,2017
p17-1115,future work may involve testing prewin on an ner task to see if and how it can generalise to a different classification task and how the results compare to the sota and similar methods such as that of collobert et al.(2011) using the conll 2003 ner datasets.,5,2017
p17-1115,"if it does perform better, this will be of considerable interest to classification research (and beyond) in nlp.",4,2017
p17-1116,"additionally, we plan to apply the proposed model to other social media analyses such as gender analysis and age analysis.",4,2017
p17-1116,"as future works of this study, we are planning to expand the proposed model to handle multiple locations and a temporal state to capture location changes and states like traveling.",1,2017
p17-1117,"in future work, we are applying our entailment-based multi-task paradigm to other directed language generation tasks such as image captioning and document summarization.",4,2017
p17-1118,"in future work, we intend to explore other methods to enrich cn, such as the recurrent language model, and use other metrics to characterize an adjacency network.",1,2017
p17-1120,"although we used only text data to perform chat detection, we can also utilize contextual information such as the previous utterances (xu and sarikaya, 2014), the acoustic information (jiang et al., 2015), and the user profile (sano et al., 2016).",2,2017
p17-1120,"an important future work is to develop a sophisticated dialogue manager to handle such utterances, for example, by making clarification questions (schloder and fernandez, 2015).篓 we manually investigated the dialogue acts in the chat detection dataset (c.f., section 3.2).",1,2017
p17-1120,further efforts on improving nontask-oriented dialogue systems is an important future work.,1,2017
p17-1120,incorporating these techniques into our methods is also an important future work.,1,2017
p17-1120,it is an interesting research topic to use such contextual information beyond text.,2,2017
p17-1120,it is interesting to automatically determine the dialogue acts to help producing appropriate system responses.,1,2017
p17-1120,"some related studies exist in such a research direction (meguro et al., 2010).",6,2017
p17-1120,"to facilitate future research, we are going to release the dataset together with the feature values derived from the tweets and web search queries.",2,2017
p17-1121,"in future, we would like to include other sources of information in our model.",2,2017
p17-1121,"our initial plan is to include rhetorical relations, which has been shown to benefit existing grid models (feng et al., 2014).",1,2017
p17-1121,"we would also like to extend our model to other forms of discourse, especially, asynchronous conversations, where participants communicate with each other at different times (e.g., forum, email).",4,2017
p17-1122,"finally, we believe that future work should be evaluated in situ, to examine if, and to what extent, the generated responses participate in and affect the discourse (feed) in social media.",3,2017
p17-1122,"some future avenues for investigation include improving the relevance and human-likeness results by improving the automatic parses quality, acquiring more complex templates via abstract grammars, and experimenting with more sophisticated scoring functions for reranking.",1,2017
p17-1122,"with the emergence of deep learning, we further embrace the opportunity to combine the sequence-to-sequence modeling view explored so far with conditioning generation on speakers agendas and user profiles, pushing the envelope of opinionated generation further.",1,2017
p17-1123,"besides this, it would also be interesting to consider to incorporate mechanisms for other language generation tasks (e.g., copy mechanism for dialogue generation) in our model to further improve the quality of generated questions.",1,2017
p17-1123,here we point out several interesting future research directions.,6,2017
p17-1123,we would like to explore how to better use the paragraph-level information to improve the performance of qg system regarding questions of all categories.,1,2017
p17-1124,"as future work, we plan to investigate more sophisticated sampling strategies based on active learning and concept graphs to incorporate lexicalsemantic information for concept selection.",1,2017
p17-1124,"we also plan to look into ways to propagate feedback to similar and related concepts with partial feedback, to reduce the total amount of feedback.",1,2017
p17-1125,future work involves investigating a better memory selection scheme.,1,2017
p17-1125,"other regularization methods (e.g., norm or drop out) are also interesting and may alleviate the over-fitting problem",1,2017
p17-1126,"in future work, we plan to apply our model to descriptions of time-series data in various domains such as weather forecasts and sports, which share the above writing-style characteristics.",4,2017
p17-1126,we also plan to use multiple time-series as input such as multiple brands of stock.,2,2017
p17-1128,"in the future, we will study how to construct a taxonomy from texts in chinese.",1,2017
p17-1129,another direction to explore is joint learning of syntactic parser and chain-of-trees lstm.,1,2017
p17-1129,"for future work, we will investigate the wider applicability of chain-of-trees lstm as a general text encoder that can simultaneously capture local syntactic structure and long-range semantic dependency.",1,2017
p17-1129,"we will also apply the tree-guided attention mechanism to nlp tasks that need syntaxaware attention, such as machine translation, sentence summarization, textual entailment, etc.",4,2017
p17-1131,"conversation content: empathic counselors use reflective language and talk about behavior change, while less empathic counselors persuade more and focus on client resistance toward change.",1,2017
p17-1131,"in the future, we plan to build upon the acquired knowledge and the developed classifiers to create automatic tools that provide accurate evaluative feedback of counseling practice.",3,2017
p17-1133,"promising future directions would be to investigate how to utilize user interaction in moocs for better prerequisite learning, as well as how deep learning models can be used to automatically learn useful features to help infer prerequisite relations.",1,2017
p17-1134,a more sophisticated approach would be to incorporate features into the unsupervised model.,1,2017
p17-1134,"for the literary analysis, as well, to bridge the gap between work like morzinski (1994) and a computational application, it remains to be seen how precise an annotation is possible for this task.",5,2017
p17-1134,the incorporation of these features for this segmentation task could be a potentially fruitful avenue for future work.,1,2017
p17-1136,"apart from it, we intend to propose a neural architecture that accomplishes the joint learning of lemmas with other morphological attributes.",1,2017
p17-1136,"in our work, we use the 鈥榢eras鈥 software keeping 鈥榯heano鈥 as backend.",1,2017
p17-1137,"to further validate the performance of our model on different languages, we collected multilingual wikipedia corpus for 7 typologically diverse languages.",2,2017
p17-1137,we will investigate a model which can marginalise word segmentation as latent variables in the future work.,1,2017
p17-1138,"in future work, we would like to apply the presented non-linear bandit learners to other structured prediction tasks.",4,2017
p17-1140,"in the future, we plan to validate the effectiveness of our model on more language pairs.",2,2017
p17-1141,"in future work, we hope to evaluate gbs with models outside of mt, such as automatic summarization, image captioning or dialog generation.",3,2017
p17-1141,"we also hope to introduce new constraintaware models, for example via secondary attention mechanisms over lexical constraints.",1,2017
p17-1142,"as new obfuscated words are introduced in escort advertisements, our hope is that character models will stay invariant to these obfuscations.understanding images.",1,2017
p17-1142,future direction involves using graphical modeling to understand interactions in the scene.,1,2017
p17-1142,"in order to eliminate the need for retraining the word vectors as the language of the domain evolves, we plan to use character models to learn a better language model for trafficking.",1,2017
p17-1143,we hope that this paper and the accompanying database serve as a first step towards nlp being applied in cybersecurity and that other researchers will be inspired to contribute to the database and to construct their own datasets and implementations.,4,2017
p17-1144,"some potential augmentations include more fine-grained revision categories, revision properties such as statement strength (tan and lee, 2014) and quality evaluations, and sub-sentential revision scopes.",3,2017
p17-1144,we also plan to augment the corpus to support additional types of research on revision analysis.,2,2017
p17-1144,"while in this paper we explored language as one factor influencing rewriting behavior, our corpus also contains information about other potential factors such as gender and education level which we plan to investigate in the future.",2,2017
p17-1146,"because our neural models are applicable to srl, applying our models for multilingual srl tasks presents an interesting future research direction.",4,2017
p17-1146,"in future work, we plan to explore effective methods for exploiting large-scale unlabeled data to learn the neural models.",1,2017
p17-1148,"other document-level features, such as example input-output pairs, unit tests, might be useful in this endeavor.",1,2017
p17-1149,"in the future, we will improve the scalability of our model and learn multi-prototype embeddings for the mentions without reference entities in a knowledge base, and introduce compositional approaches to model the internal structures of multiword mentions.",1,2017
p17-1150,future work can explore deep neural network to alleviate feature engineering.,1,2017
p17-1150,one of the important questions to address in the future is how to learn new predicates through interaction with humans.,5,2017
p17-1150,our future work will incorporate interactive learning of verb semantics with task learning to enable autonomy that can learn by communicating with humans.,1,2017
p17-1151,"alternatively, we could imagine a dependent mixture model where the distributions over words are evolving with time and other covariates.",1,2017
p17-1151,"in the future, multimodal word distributions could open the doors to a new suite of applications in language modelling, where whole word distributions are used as inputs to new probabilistic lstms, or in decision functions where uncertainty matters.",4,2017
p17-1152,"future work interesting to us includes exploring the usefulness of external resources such as wordnet and contrasting-meaning embedding (chen et al., 2015) to help increase the coverage of wordlevel inference relations.",2,2017
p17-1152,"modeling negation more closely within neural network frameworks (socher et al., 2013; zhu et al., 2014) may help contradiction detection.",1,2017
p17-1153,future work includes expanding the analyses to non-english movies and combining the linguistic metrics with character networks.,2,2017
p17-1154,"as future work, we plan to apply the linguistic regularizers to tree-lstm to address the scope issue since the parsing tree is easier to indicate the modification scope explicitly.",4,2017
p17-1155,designing automatic measures is hence left for future research.,1,2017
p17-1155,future research directions rise from cases in which the sign models left the tweet unchanged.,5,2017
p17-1155,several challenges are still to be addressed in future research so that sarcasm interpretation can be performed in a fully automatic manner.,5,2017
p17-1155,these include the design of appropriate automatic evaluation measures as well as improving the algorithmic approach so that it can take world knowledge into account and deal with cases where the sentiment of the input tweet is not expressed with a clear sentiment words.,3,2017
p17-1155,we hope this new resource will help researchers make further progress on this new task.,2,2017
p17-1158,"in future, we will strive to implement cane on a wider variety of information networks with multi-modal data, such as labels, images and so on.(2) cane encodes latent relations between vertices into their context-aware embeddings.",2,2017
p17-1158,"thus, we want to explore how to incorporate and predict these explicit relations between vertices in ne.",1,2017
p17-1158,we will explore the following directions in future: (1) we have investigated the effectiveness of cane on text-based information networks.,1,2017
p17-1159,possible future work include expanding the investigation to other regional languages such as malay and indonesian.,2,2017
p17-1160,an interesting avenue for future work is to explore higher order factorizations for noncrossing digraphs and the related inference.,1,2017
p17-1160,"we are planning to extend the coverage of the approach by exploring 1-endpointcrossing and mhk trees (pitler et al., 2013; gomez-rodr 麓 麓谋guez, 2016), and related digraphs 鈥 see (yli-jyra篓, 2004; gomez-rodr 麓 麓谋guez et al., 2011).",1,2017
p17-1160,we would also like to have more insight on the transformation of mso definable properties to the current framework and to logspace algorithms.,1,2017
p17-1163,"in future work, we intend to explore applications of the nbt for multi-domain dialogue systems, as well as in languages other than english that require handling of complex morphological variation.",4,2017
p17-1164,"moreover, we also use events from fn to augment the performance of the proposed approach.",1,2017
p17-1166,"in the future, we will focus on two aspects: (1) our method in this paper considers pairwise intersections between labels, so to better exploit class ties, we will extend our method to exploit all other labels鈥 influences on each relation for relation extraction, transferring second-order to high-order (zhang and zhou, 2014); (2) we will focus on other problems by leveraging class ties between labels, specially on multi-label learning problems (zhou et al., 2012) such as multi-category text categorization (rousu et al., 2005) and multi-label image categorization (zha et al., 2008).",1,2017
p17-1167,"because of the dynamic nature of our framework, it is not trivial to leverage the computational capabilities of gpus using minibatched training; we plan to investigate ways to take full advantage of modern computing machinery in the near future.",1,2017
p17-1167,"for instance, although our current formal language design covers most question types in sqa, it is nevertheless important to extend it further to make the semantic parser more robust (e.g., by including union or allowing comparison of multiple previous answers).",1,2017
p17-1167,"in the future, we plan to investigate several interesting research questions triggered by this work.",5,2017
p17-1169,it would also be interesting to investigate several possible extensions to the current clustering work.,1,2017
p17-1169,one direction is to learn a proper ground distance for word embeddings such that the final document clustering performance can be improved with labeled data.,1,2017
p17-1169,"the work by (huang et al., 2016; cuturi and avis, 2014) have partly touched this goal with an emphasis on document proximities.",1,2017
p17-1170,"also, given the promising results observed for supersenses, we plan to investigate taskspecific coarsening of sense inventories, particularly wikipedia, or the use of sentiwordnet (baccianella et al., 2010), which could be more suitable for polarity detection.",1,2017
p17-1170,"as future work, we plan to investigate the extension of the approach to other languages and applications.",4,2017
p17-1170,we hope that our work will foster future research on the integration of senselevel knowledge into downstream applications.,1,2017
p17-1171,future work should aim to improve over our drqa system.,1,2017
p17-1174,"in addition, we plan to combine our decoder with other encoders that capture language structure, such as a hierarchical rnn (luong and manning, 2016), a tree-lstm (eriguchi et al., 2016b), or an order-free encoder, such as a cnn (kalchbrenner and blunsom, 2013).",1,2017
p17-1174,"in future work, we will explore the optimal structures of chunk-based decoder for other free word-order languages such as czech, german, and turkish.",2,2017
p17-1175,"in the future, we will incorporate coverage into our model and study how to apply it to other natural language processing tasks.",4,2017
p17-1176,"in the future, we plan to test our approach on more diverse language pairs, e.g., zero-resource uyghur-english translation using chinese as a pivot.",2,2017
p17-1176,it is also interesting to extend the teacherstudent framework to other cross-lingual nlp applications as our method is transparent to architectures.,4,2017
p17-1177,"for future work, it will be interesting to make use of node labels from the tree, or to use syntactic information on the target side, as well.",1,2017
p17-1178,"in the future, we will explore the topological structure of related languages and exploit cross-lingual knowledge transfer to enhance the quality of extraction and linking.",2,2017
p17-1178,the general idea of deriving noisy annotations from kb properties can also be extended to other ie tasks such as relation extraction.,4,2017
p17-1179,"future work also includes investigating other divergences that adversarial training can minimize (nowozin et al., 2016), and broader mathematical tools that match distributions (mohamed and lakshminarayanan, 2016).",1,2017
p17-1179,our work is likely to benefit from advances in techniques that further stabilize adversarial training.,1,2017
p17-1180,"as future directions, we plan to extend gwld to several other languages and conduct similar sociolinguistic studies on cs patterns including not only more languages and geographies, but also other aspects like topic and sentiment.",2,2017
p17-1181,cognates detection is an interesting and challenging task.,5,2017
p17-1182,"in the future, we want to develop methods to make better use of languages with different alphabets or morphosyntactic features, in order to increase the applicability of our knowledge transfer method.",1,2017
p17-1183,"future work may include applying our model to other nearly-monotonic alignand-transduce tasks like abstractive summarization, transliteration or machine translation.",4,2017
p17-1184,we plan to explore these effects in future work.,6,2017
p17-1185,possible direction of future work is to apply more advanced optimization techniques to the step 1 of the scheme proposed in section 1 and to explore the step 2 鈥 obtaining embeddings with a given low-rank matrix.,1,2017
p17-1186,"because our techniques apply to labeled directed graphs in general, they can easily be extended to incorporate more formalisms, semantic or otherwise.",1,2017
p17-1186,in future work we hope to explore cross-task scoring and inference for tasks where parallel annotations are not available.,3,2017
p17-1187,we will explore the effectiveness of sememe information for wrl in other languages.,2,2017
p17-1187,"we will explore the following research directions in future: (1) the sememe information in hownet is annotated with hierarchical structure and relations, which have not been considered in our framework.",2,2017
p17-1187,we will explore to utilize these annotations for better wrl.(2) we believe the idea of sememes is universal and could be wellfunctioned beyond languages.,2,2017
p17-1188,"in the future, we hope to further generalize this method to other tasks such as pronunciation estimation, which can take advantage of the fact that pronunciation information is encoded in parts of the characters as demonstrated in fig.1, or machine translation, which could benefit from a wholistic view that considers both semantics and pronunciation.",4,2017
p17-1188,"we also hope to apply the model to other languages with complicated compositional writing systems, potentially including historical texts such as hieroglyphics or cuneiform.",4,2017
p17-1189,"so in the future, we might try to combine more heterogeneous semantic data for other tasks like event extraction and relation classification, etc.",2,2017
p17-1189,we hope that it will be helpful in providing common benchmarks for future work on chinese srl tasks.,3,2017
p17-1190,"future work will explore additional data sources, including from aligning different translations of novels (barzilay and mckeown, 2001), aligning new articles of the same topic (dolan et al., 2004), or even possibly using machine translation systems to translate bilingual text into paraphrastic sentence pairs.",2,2017
p17-1191,"moreover, the methods described in this paper can be extended to other kinds of structured knowledge sources like freebase which may be more suitable for tasks like question answering.",4,2017
p17-1191,this approach may be extended to other nlp tasks that can benefit from using encoders that can access wordnet information.,4,2017
p17-1193,"in particular, we enhance the maximum subgraph model with new parsing algorithms for 1ec/p2 graphs.",1,2017
p17-1193,we leave this for future investigation.,6,2017
p17-1194,future work could investigate the extension of this architecture to additional unannotated resources.,2,2017
p17-1194,the model is incentivised to discover useful features in order to learn the language distribution and composition patterns in the training data.,1,2017
p18-1001,"future work includes an investigation into the trade-off between learning full covariance matrices for each word distribution, computational complexity, and performance.",1,2018
p18-1001,other future work involves co-training pft on many languages.,2,2018
p18-1002,"both in this area and for n-grams there is great scope for combining our approach with compositional approaches (bojanowski et al., 2016; poliak et al., 2017) that can handle settings such as zero-shot learning.",1,2018
p18-1002,"extensions of the mathematical formulation, such as the use of word weighting when building context vectors as in arora et al.(2018b) or of spectral information along the lines of mu and viswanath (2018), are also worthy of further study.",1,2018
p18-1002,"more work is needed to understand the usefulness of our method for representing (potentially cross-lingual) entities such as synsets, whose embeddings have found use in enhancing wordnet and related knowledge bases (camachocollados et al., 2016; khodak et al., 2017).",1,2018
p18-1002,"of particular interest is the replacement of simple window contexts by other structures, such as dependency parses, that could yield results in domains such as question answering or semantic role labeling.",1,2018
p18-1004,"in future work, we will investigate explicit retrofitting methods for asymmetric relations like hypernymy and meronymy.",1,2018
p18-1004,we also intend to apply the method to other downstream tasks and to investigate the zero-shot language transfer of the specialization function for more language pairs.,4,2018
p18-1005,"besides, we decide to make more efforts to explore how to reinforce the temporal order information for the proposed model.",1,2018
p18-1005,"in the future, we would like to investigate how to utilize the monolingual data more effectively, such as incorporating the language model and syntactic information into unsupervised nmt.",2,2018
p18-1005,unsupervised nmt opens exciting opportunities for the future research.,6,2018
p18-1006,"by introducing another rich language, our method can better exploit the additional language pairs to enrich the original low-resource pair.",2,2018
p18-1006,"in the future, we may extend our architecture to other scenarios, such as totally unsupervised training with no bilingual data for the rare language.",4,2018
p18-1007,"additionally, we would like to explore the application of subword regularization for machine learning, including denoising auto encoder (vincent et al., 2008) and adversarial training (goodfellow et al., 2015)",4,2018
p18-1007,"promising avenues for future work are to apply subword regularization to other nlp tasks based on encoder-decoder architectures, e.g., dialog generation (vinyals and le, 2015) and automatic summarization (rush et al., 2015).",4,2018
p18-1008,"our focus on a standard single-language-pair translation task leaves important open questions to be answered: how do our new architectures compare in multilingual settings, i.e., modeling an interlingua?",5,2018
p18-1008,"we hope that our work will motivate nmt researchers to further investigate generally applicable training and optimization techniques, and that our exploration of hybrid architectures will open paths for new architecture search efforts for nmt.",1,2018
p18-1010,"while this work already demonstrates considerable improvement over non-hierarchical modeling, future work will explore techniques such as box embeddings (vilnis et al., 2018) and poincare麓 embeddings (nickel and kiela, 2017) to represent the hierarchical embedding space, as well as methods to improve recall in the candidate generation process for entity linking.",1,2018
p18-1015,"on the other hand, we plan to test our system on the other tasks such as document-level summarization and short text conversation.",3,2018
p18-1015,we believe our work can be extended in various aspects.,6,2018
p18-1016,future work will leverage ucca鈥檚 cross-linguistic applicability to support multi-lingual ts and ts pre-processing for mt.,4,2018
p18-1018,"we expect that future work will develop methods to scale the annotation process beyond requiring highly trained experts; bring this scheme to bear on other languages; and investigate the relationship of our scheme to more structured semantic representations, which could lead to more robust models.",1,2018
p18-1019,"this dataset fills a need for larger scale corpora to facilitate research on nlp methods for processing the biomedical literature, which have the potential to aid the conduct of ebm.",2,2018
p18-1020,the significant gains demonstrated suggests easl as a promising approach for future dataset curation and system evaluation in the community.,3,2018
p18-1023,"to obtain further insights into the nature of counterarguments, deeper linguistic analysis along with supervised learning may be needed, though.",1,2018
p18-1023,"we did not aim to engineer the best approach to this retrieval task, but to study whether we can model the simultaneous similarity and dissimilarity of a counterargument to an argument computationally.",1,2018
p18-1023,"we provide a corpus to train respective approaches, but leave the according research to future work.",2,2018
p18-1023,"while the model can be considered open-topic, a next step will be to study counterargument retrieval open-source.",1,2018
p18-1024,"for future work, we would like to extend the current evaluation of our work from a two-graph setting to multiple graphs.",3,2018
p18-1024,"in real-world setting, we envision our method to be integrated in a large scale system that would include various other components for tasks like conflict resolution, active learning and human-in-loop learning to ensure quality of constructed super-graph.",4,2018
p18-1024,we believe that this work opens a new research direction in joint representation learning over multiple knowledge graphs.,1,2018
p18-1025,an exciting direction is the incorporation of multi-relational data for general knowledge representation and inference.,2,2018
p18-1026,incorporating both ideas to our architecture is an research direction we plan for future work.,1,2018
p18-1026,we believe this is an interesting research direction in terms of applications.,6,2018
p18-1027,"these findings not only help us better understand these models but also suggest ways for improving them, as discussed in section 7.",1,2018
p18-1027,"while observations in this paper are reported at the token level, deeper understanding of sentence-level interactions warrants further investigation, which we leave to future work.",1,2018
p18-1029,"future work can model how these parameters can be adapted in a task specific way (e.g., cases such as cancer prediction where base rates are small), and provide better models of quantifier semantics.e.g., as distributions, rather than point values.",1,2018
p18-1030,"next directions also include the investigation of s-lstm to more nlp tasks, such as machine translation.",4,2018
p18-1030,we leave such investigation to future work.,6,2018
p18-1031,another direction is to apply the method to novel tasks and models.,4,2018
p18-1031,"given that transfer learning and particularly fine-tuning for nlp is under-explored, many future directions are possible.",1,2018
p18-1031,"language modeling can also be augmented with additional tasks in a multi-task learning fashion (caruana, 1993) or enriched with additional supervision, e.g.syntax-sensitive dependencies (linzen et al., 2016) to create a model that is more general or better suited for certain downstream tasks, ideally in a weakly-supervised manner to retain its universal properties.",1,2018
p18-1033,developers of future large-scale datasets should incorporate joins and nesting to create more human-like data.,2,2018
p18-1033,our analysis has clear implications for future work.,6,2018
p18-1034,"in the future, we plan to improve the accuracy of the column prediction component.",1,2018
p18-1034,we also plan to build a large-scale dataset that considers more sophisticated sql queries.,2,2018
p18-1034,"we also plan to extend the approach to low-resource scenarios (feng et al., 2018).",1,2018
p18-1035,"future work will investigate whether a single algorithm and architecture can be competitive on all of these parsing tasks, an important step towards a joint many-task model for semantic parsing.",1,2018
p18-1037,another promising direction is integrating character seq2seq to substitute the copy function.,1,2018
p18-1037,this should also improve the handling of negation and rare words.,1,2018
p18-1039,"in addition, we plan to expand the coverage of our meaning representation to support more mathematic concepts.",1,2018
p18-1039,"in the future, we will focus on tackling this challenge.",6,2018
p18-1040,"in the future, we plan to model document-level representations which are more in line with drt and the gmb annotations.",1,2018
p18-1042,"we are actively investigating ways to apply these two new resources to downstream applications, including machine translation, question answering, and additional paraphrase generation tasks.",4,2018
p18-1044,"in the future, we will apply this semi-supervised training method to other nlp tasks.",4,2018
p18-1045,"in the future, we will explore the use of additional discourse structures that correlate highly with event coreference chains.",1,2018
p18-1045,"moreover, we will extend this work to other domains such as biomedical domains.",4,2018
p18-1047,another future work is test our model in other nlp tasks like event extraction.,4,2018
p18-1047,our future work will concentrate on how to improve the performance further.,1,2018
p18-1048,"considering this possibility, we suggest to drive the two discriminators in our self-regulation framework to cooperate with each other.",1,2018
p18-1048,"therefore, in the future, we will encode the global information by neural networks and use the self-regulation strategy to reduce the negative influence of noises.",1,2018
p18-1050,"for the future work, we plan to expand event temporal knowledge acquisition by dealing with event sense disambiguation and event synonym identification (e.g., drag, pull and haul).",1,2018
p18-1051,"in future work, we intend to thoroughly study the impact of tds given morphosyntactic information.",1,2018
p18-1053,"in the future, we plan to explore neural network models for efficaciously resolving anaphoric zero pronoun documents and research on some specific components which might influence the performance of the model, such as the embedding.",1,2018
p18-1053,"meanwhile, we plan to research on the possibility of applying adversarial learning (goodfellow et al., 2014) to generate better rewards than the human-defined reward functions.",1,2018
p18-1056,"therefore, we suggest that future work on social power and language use should consider other (maybe higher-level) linguistic elements.",2,2018
p18-1056,"we call for the inclusion of a wider range of factors in future studies of social influences on language use, especially low-level but interpretable cognitive factors.",3,2018
p18-1057,for future work we plan to continue the collection and annotation of resources and to separately explore each of the above research tasks.,2,2018
p18-1059,"indeed, if outputs all similarly under-correct, correlation studies will not be affected by whether an rbm is sensitive to undercorrection.",5,2018
p18-1060,"alternatively, we could give up on measuring absolute scores, and seek instead to find techniques stably rank methods and thus improve them.",1,2018
p18-1060,we hope our work provides some clarity on to how to make it more cost effective.,6,2018
p18-1065,"consequently, we encourage researchers to evaluate the usefulness of these features and study these moderating factors in different domains, platforms, and languages, possibly identifying new features and moderating factors.",3,2018
p18-1065,"however, there have been exciting developments in helpfulness prediction: systems that have attempted to exploit user and reviewer information, along with those based on sophisticated models (e.g., probabilistic matrix factorization, hmm-lda) and neural network architectures, are promising prospects for future work.",1,2018
p18-1065,there are multiple ways to approach this task.,1,2018
p18-1065,we conclude our survey with several recommendations for future work on computational modeling and prediction of review helpfulness.,1,2018
p18-1066,"future directions include: 1) mining cross-cultural differences in general concepts other than names and slang, 2) merging the mined knowledge into existing knowledge bases, and 3) applying the socvec in downstream tasks like machine translation.",2,2018
p18-1067,"in future works, we will exploit the interesting connections between moral foundations and frames for the analysis of more detailed ideological leanings and stance prediction.",1,2018
p18-1068,"in the future, we would like to apply the framework in a weakly supervised setting, i.e., to learn semantic parsers from question-answer pairs and to explore alternative ways of defining meaning sketches.",4,2018
p18-1069,directions for future work are many and varied.,6,2018
p18-1069,"the proposed framework could be applied to a variety of tasks (bahdanau et al., 2015; schmaltz et al., 2017) employing sequence-to-sequence architectures.",4,2018
p18-1072,we hope that this work will guide further developments in this new and exciting field.,6,2018
p18-1073,"in the future, we would like to extend the method from the bilingual to the multilingual scenario, and go beyond the word level by incorporating embeddings of longer phrases.",1,2018
p18-1074,"the next step of this research is to apply this architecture to other types of tasks, such as event extract and semantic role labeling that involve structure prediction.",4,2018
p18-1074,we also plan to explore the possibility of integrating incremental learning into this architecture to adapt a trained model for new tasks rapidly.,1,2018
p18-1076,"in future work, we will investigate even tighter integration of the attended knowledge and stronger reasoning methods.",1,2018
p18-1079,"developing more effective ways of leveraging the expert鈥檚 time to close the loop, and facilitating more interactive collaboration between humans and sears are exciting areas for future work.",1,2018
p18-1079,"table 5b), which recent work on paraphrasing (iyyer et al., 2018) or generation using gans (zhao et al., 2018) may address.",1,2018
p18-1079,"we demonstrated that seas and sears can be an invaluable tool for debugging nlp models, while indicating their current limitations and avenues for future work.",1,2018
p18-1080,in future work we intend to apply this technique to debiasing sentences and anonymization of author traits such as gender and age.,4,2018
p18-1080,"in the future work, we will also explore whether an enhanced back-translation by pivoting through several languages will learn better grounded latent meaning representations.",1,2018
p18-1081,"in future work, we hope to explore the potential of this architecture on further kinds of data, including multimodal data (long et al., 2018), from which one can extract structured signals.",2,2018
p18-1083,"we believe there are still lots of improvement space in the narrative paragraph generation tasks, like how to better simulate human imagination to create more vivid and diversified stories.",5,2018
p18-1084,in future work we plan to improve our methods by exploiting the internal structure of images and sentences as well as by effectively integrating signals from more than two languages.,1,2018
p18-1085,"in future work, we would like to explore other aspects of search engines for language grounding as well as the effect these embeddings may have on learning generic sentence representations (kiros et al., 2015b; hill et al., 2016; conneau et al., 2017a; logeswaran and lee, 2018).",1,2018
p18-1085,"recently, contextualized word representations have shown promising improvements when combined with existing embeddings (melamud et al., 2016; peters et al., 2017; mccann et al., 2017; peters et al., 2018).",1,2018
p18-1085,we expect that integrating picturebook with these embeddings to lead to further performance improvements as well.,1,2018
p18-1086,"we also plan to incorporate action-effect prediction to humanrobot collaboration, for example, to bridge the gap of commonsense knowledge about the physical world between humans and robots.",1,2018
p18-1086,"we plan to apply more advanced approaches in the future, for example, attention models that jointly capture actions, image states, and effect descriptions.",1,2018
p18-1088,we believe that there will be more effective solutions coming in the near future.,6,2018
p18-1090,"for future work, we would like to explore a fine-grained version of sentiment-to-sentiment translation that not only reverses sentiment, but also changes the strength of sentiment.",1,2018
p18-1091,future works involve the choice of discourse markers and some other transfer learning sources.,2,2018
p18-1092,it might be interesting to analyze how other reasoning modules can improve different weaknesses of the model.,1,2018
p18-1092,"while other models have focused on different parts of these components, we think that is important to find ways to combine these different mechanisms if we want to build models capable of complex reasoning.",1,2018
p18-1095,"in the future we want to apply our marginal utility based framework to other metrics, such as mean average precision (map).",4,2018
p18-1098,"we will address these two issues in future by investigating diversity-promoting regularization (xie et al., 2017) and leveraging an external knowledge base that maps medical abbreviations into their full names.",1,2018
p18-1101,"our findings also suggest promising future research directions, including learning better context-based latent actions and using reinforcement learning to adapt policy networks.",1,2018
p18-1103,we would like to explore in depth how attention can help improve neural dialogue modeling for both chatbots and taskoriented dialogue systems in our future work.,1,2018
p18-1105,another direction will be to apply fluctuation analysis in formulating a statistical test to evaluate the structural complexity underlying a sequence.,3,2018
p18-1105,"our future work will include an analysis using other kinds of data, such as twitter data and adult utterances, and a study of how taylor鈥檚 law relates to grammatical complexity for different sequences.",2,2018
p18-1105,taylor鈥檚 law and its exponent can also be applied to evaluate machine-generated text.,3,2018
p18-1106,"in our investigation of kauhanen鈥檚 basic assumptions, we discover how seemingly innocuous decisions about population size and learning conspire to drive simulation results.",1,2018
p18-1107,we further plan to empirically evaluate our lexicalization on an alignment task and to offer a comparison against the lexicalization due to zhang and gildea (2005).,3,2018
p18-1107,"we plan to verify whether rank-k pl-rstag is more powerful than rank-k scfg in future work, and to reduce the rank of the transformed grammar if possible.",3,2018
p18-1108,"since the architecture of model is no more than a stack of standard recurrent and convolution layers, which are essential components in most academic and industrial deep learning frameworks, the deployment of this method would be straightforward.",1,2018
p18-1111,"in the future, we plan to take generalization one step further, and explore the possibility to use the bilstm for generating completely new paraphrase templates unseen during training.",1,2018
p18-1113,future work will introduce weighted cbow and skip-gram to learn positional information within sentences.,1,2018
p18-1114,"in the future, we intend to evaluate our models for some morpheme-rich languages like russian, german and so on.",3,2018
p18-1115,"a latent factor model such as darn (gregor et al., 2014) would consider several sources simultaneously.",1,2018
p18-1115,"extension to other architectures: introducing latent variables into non-autoregressive translation models such as the transformer (vaswani et al., 2017) should increase their translation ability further.",1,2018
p18-1116,"as future work, we plan to design some more elaborate structures to incorporate the score layer into the encoder.",1,2018
p18-1116,we will also apply the proposed linearization method to other tasks.,4,2018
p18-1117,"future work could also investigate whether context-aware nmt systems learn other discourse phenomena, for example whether they improve the translation of elliptical constructions, and markers of discourse relations and information structure.",5,2018
p18-1118,"for future work, we intend to investigate models which incorporate specific discourse-level phenomena.",1,2018
p18-1119,future work may see our methods applied to document geolocation to assess the effectiveness of scaling geodesic vectors from paragraphs to entire documents.,3,2018
p18-1120,challenges also remain in how to evaluate the accuracy of goal knowledge extracted from text corpora.,3,2018
p18-1120,"in future work, we hope to see if we can take advantage of more contextual information as well as other external knowledge to improve the recognition of goalacts.",2,2018
p18-1120,"nevertheless, our work represents a first step toward learning goal knowledge about locations, and we believe that learning knowledge about plans and goals is an important direction for natural language understanding research.",1,2018
p18-1123,in our future work we plan to extend the proposed method to these other applications.,4,2018
p18-1123,"while in this work, we apply the exemplar encoder decoder network on conversational task, the method is generic and could be used with other tasks such as question answering and machine translation.",4,2018
p18-1124,we further investigate the usability of dialsql in a real life setting by conducting human evaluations.,3,2018
p18-1125,a promising line of future work could consider the complementary problem of identifying pragmatic strategies that can help bring uncivil conversations back on track.,5,2018
p18-1125,our approach has several limitations which open avenues for future work.,5,2018
p18-1125,"while our analysis focused on the very first exchange in a conversation for the sake of generality, more complex modeling could extend its scope to account for conversational features that more comprehensively span the interaction.",1,2018
p18-1127,"the difficulties in basing validation on system outputs may be applicable to other text-to-text generation tasks, a question we will explore in future work.",5,2018
p18-1130,"another interesting direction is to further improve our model by exploring reinforcement learning approaches to learn an optimal order for the children of head words, instead of using a predefined fixed order.",1,2018
p18-1130,"first, we intend to consider how to conduct experiments to improve the analysis of parsing errors qualitatively and quantitatively.",3,2018
p18-1131,"while the cross-domain strategies we presented can greatly increase accurate parsing of these features, narrowing the performance gap between aae- and mae-like tweets, much work remains to be done for accurate parsing of even linguistically well-documented features.",1,2018
p18-1133,"for our future work, we will consider advanced instantiations for sequicity, and extend sequicity to handle unsupervised cases where information and requested slots values are not annotated.",1,2018
p18-1137,"in future work, we plan to further investigate the impact of risksensitive objective functions, including the relations between model robustness and diverse generations.",1,2018
p18-1137,"while if we want to generate diverse responses, a risk-sensitive objective functions is helpful.",1,2018
p18-1138,"in future work, we plan to introduce reinforcement learning and knowledge base reasoning mechanisms to improve the performance.",1,2018
p18-1139,"as for future work, we will investigate how to apply the technique to multi-turn conversational systems, provided that the most proper sentence function can be predicted under a given conversation context.",4,2018
p18-1140,we believe this approach can be easily generalized to other domains given its end-to-end training procedure and task independence.,4,2018
p18-1142,"another possible research direction is learning the mapping between structures from parallel texts jointly with a main task, in the spirit of quasi-synchronous grammars (smith and eisner, 2009).",1,2018
p18-1142,future work will look into automating the tree processing procedure.,1,2018
p18-1143,"further, we would like to study sampling techniques motivated by natural distributions of linguistic structures.",1,2018
p18-1143,"hence, as a future work, we would like to compare the usefulness of different linguistic theories and different constraints within each theory in our proposed lm framework.",3,2018
p18-1145,"because the mismatch between words and extraction units is a common problem in information extraction, we believe our method can also be applied to many other languages and tasks for exploiting inner composition structure during extraction, such as named entity recognition.",4,2018
p18-1147,"first of all, we plan to explore the use of more advanced forms of entity detection and linking, including propagating features from the edl system forward for both unary and binary deep models.",1,2018
p18-1147,"in addition we plan to exploit unary and binary relations as source of evidence to bootstrap a probabilistic reasoning approach, with the goal of leveraging constraints from the kb schema such as domain, range and taxonomies.",1,2018
p18-1148,"besides, we would like to examine whether the induced latent relations could be helpful for relation extract.",3,2018
p18-1148,"in future work, we would like to use syntactic and discourse structures (e.g., syntactic dependency paths between mentions) to encourage the models to discover a richer set of relations.",1,2018
p18-1148,"in this way we also hope it will lead to interesting follow-up work, as individual relations can be informed by injecting prior knowledge (e.g., by training jointly with relation extraction models).",2,2018
p18-1148,we also would like to combine ment-norm and relnorm.,1,2018
p18-1153,"for future work, we hope to improve the results by using the pun data and design a more proper way to select candidates from associative words.",2,2018
p18-1154,an interesting point to explore is whether such pragmatically trained game state representations can be leveraged for the task of game commentary generation.,1,2018
p18-1154,generating commentary for such multi-moves is a potential direction for future work.,1,2018
p18-1155,"given the numerous potential applications of such an oracle, we believe improving its accuracy will be a promising future direction.",1,2018
p18-1157,"further, we also would like to explore san on other tasks, such as text classification and natural language inference for its generalization in the future.",4,2018
p18-1161,"in the future, we will explore the following directions: (1) an additional answer re-ranking step can further improve our model.",1,2018
p18-1161,"we will explore how to effectively re-rank our extracted answers to further enhance the performance.(2) background knowledge such as factual knowledge, common sense knowledge can effectively help us in paragraph selection and answer extraction.",1,2018
p18-1161,we will incorporate external knowledge bases into our ds-qa model to improve its performance.,1,2018
p18-1162,"in future work, we will try to incorporate more hand-crafted features in our model.",1,2018
p18-1163,"it is also necessary to further validate our approach on more advanced nmt architectures, such as cnn-based nmt (gehring et al., 2017) and transformer (vaswani et al., 2017).",3,2018
p18-1164,"additionally, we also intend to apply these methods to other sequence-to-sequence tasks, including natural language conversation.",4,2018
p18-1164,"in future work, we will explore further strategies to bridge the source and target side for sequence-to-sequence and tree-based nmt.",1,2018
p18-1166,"in the future, we plan to apply our model on other sequence to sequence learning tasks.",4,2018
p18-1166,we will also attempt to improve our model to enhance its modeling ability so as to consistently outperform the original neural transformer.,1,2018
p18-1168,in future work we plan to extend this work and automatically learn such a lexicon.,2,2018
p18-1169,"finally, our approach to collecting feedback can also be transferred to other domains.",4,2018
p18-1169,"for example, (yih et al., 2016) designed a user interface to help freebase experts to efficiently create queries.",1,2018
p18-1170,"in future work, we will speed it up through the use of pruning techniques.",1,2018
p18-1170,"in particular, advanced methods for alignments, as in lyu and titov (2018), seem promising.",1,2018
p18-1170,we will also look into more principled methods for splitting the amrs into elementary as-graphs to replace our hand-crafted heuristics.,1,2018
p18-1171,"while we are focused on amr parsing in this paper, in future work our cache transition system and the presented sequence-to-sequence models can be potentially applied to other semantic graph parsing tasks (oepen et al., 2015; du et al., 2015; zhang et al., 2016; cao et al., 2017).",4,2018
p18-1172,"in future, we will extend our proposed all-sample learning scheme to deep learning methods, which are more expressive than the shallow embedding model.",1,2018
p18-1172,"lastly, we are interested in exploring the recent adversarial learning techniques to enhance the robustness of word embeddings.",1,2018
p18-1172,"moreover, we will integrate prior knowledge, such as the words that are synonyms and antonyms, into the word embedding process.",1,2018
p18-1176,we also believe that other qa models may share these weaknesses.,1,2018
p18-1177,it would also be interesting to apply our approach to incorporating coreference knowledge to other text generation tasks.,4,2018
p18-1179,we extend the work on dag automata in chiang et al.(2018) and propose a general method to build flexible dag transducer.,1,2018
p18-1184,"in our future work, we plan to integrate other types of information such as user properties into the structured neural models to further enhance representation learning and detect rumor spreaders at the same time.",1,2018
p18-1184,we also plan to use unsupervised models for the task by exploiting structural information.,1,2018
p18-1185,we hope this work will encourage more research on multimodal social media in the future and we plan on making our benchmark available upon request.,2,2018
p18-1185,we plan to expand our model to tasks such as fine-grained name tagging or entity liking in the future.,4,2018
p18-1187,"in future work, we are interested in modelling the extent to which a social interaction is caused by geographical proximity (e.g.using user鈥搖ser gates).",1,2018
p18-1193,"one possible direction for future work is to use an estimator to predict rewards for all actions, rather than observing them.",1,2018
p18-1196,"our approaches in modelling and evaluation can be used in future work in tasks such as approximate information extraction, knowledge base completion, numerical fact checking, numerical question answering, and fraud detection.",4,2018
p18-1198,"in future work, we would like to extend the probing tasks to other languages (which should be relatively easy, given that they are automatically generated), investigate how multi-task training affects probing task performance and leverage our probing tasks to find more linguistically-aware universal encoders.",2,2018
p18-1201,"in the future, we will extend this framework to other information extraction problems.",4,2018
p18-1203,one interesting topic for future research is exploration in planning.,6,2018
p18-1203,"to this end, we want the agent to explore in the environment, but not so much that the performance would be greatly degraded.",1,2018
p18-1203,"we need to deal with the challenge of adapting the world model in a changing environment, as exemplified by the domain extension problem (lipton et al., 2016).",5,2018
p18-1204,the work can be extended to multi-turn conversation generation by including an additional detector predicting when to ask a question.,4,2018
p18-1205,we believe persona-chat will be a useful resource for training components of future dialogue systems.,2,2018
p18-1206,"in future work, we plan to incorporate various types of context (e.g.anaphora, device-specific capabilities) and dialogue history into a large-scale nlu system.",1,2018
p18-1209,"future work on similar topics could explore the applications of using low-rank tensors for attention models over tensor representations, as they can be even more memory and computationally intensive.",1,2018
p18-1211,"more generally, similar approaches can explore a wider range of scenarios involving sequences of text.",1,2018
p18-1213,"while our work only use information present in our dataset, we view our dataset as a future testbed for evaluating models trained on any number of resources for learning common sense about emotional reactions and motivations.",3,2018
p18-1214,"in the future, we plan to enrich the architecture of dazer to allow few-shot document filtering by incorporating several labeled examples.",1,2018
p18-1217,"for future work, we are interested in various extensions, including combining stc with autoencoding variational bayes (avb).",1,2018
p18-1219,"in future, we plan to use use approaches, like multi-task learning (mishra et al., 2018), in estimating gaze features and using those estimated features for text quality prediction.",1,2018
p18-1223,"though mainly fouced on search, we hope our findings shed some lights on a potential path towards more intelligent neural systems and will motivate more explorations in this direction.",1,2018
p18-1225,"our rule-based generators can be expanded to cover more patterns and phenomena, and the seq2seq generator extended to incorporate per-example loss for adversarial training.",1,2018
p18-1225,"our seq2seq and knowledge-guided example generators, trained in an end-to-end fashion, can be used to make any base entailment model more robust.",1,2018
p18-1226,"lastly, since our method can capture korean syntactic features through jamo and character n-grams, we can apply the same idea to other tasks such as pos tagging and parsing.",4,2018
p18-1226,"meanwhile, we will further train our model over noisy data and investigate how it is dealing with noisy words.",1,2018
p18-1226,"since korean words are divisible once more into grapheme level, resulting in longer sequence of jamos for a given word, we plan to explore potential applicability of deeper level of subword information in korean.",1,2018
p18-1226,"we plan to apply these vectors for various neural network based nlp models, such as conversation modeling.",4,2018
p18-1227,"in the future, we will explore methods of exploiting internal information in other languages.(4) we believe that sememes are universal for all human languages.",2,2018
p18-1227,"in the future, we will take structured annotations into account.(2) it would be meaningful to take more information into account for blending external and internal information and design more sophisticated methods.(3) besides chinese, many other languages have rich subword-level information.",2,2018
p18-1227,we will explore a general framework to recommend and utilize sememes for other nlp tasks.,4,2018
p18-1227,"we will explore the following research directions in the future: (1) concepts in hownet are annotated with hierarchical structures of senses and sememes, but those are not considered in this paper.",1,2018
p18-1228,our study may facilitate further investigations on context-dependent text analysis techniques and applications.,1,2018
p18-1228,we hope that semaxis can facilitate research on other semantic axes so that we will have labeled datasets for other axes as well.,2,2018
p18-1229,"in addition, study on how to effectively encode induction history will be interesting.",5,2018
p18-1229,"in the future, we will explore more strategies towards term pair selection (e.g., allow the rl agent to remove terms from the taxonomy) and reward function design.",1,2018
p18-1230,"in the next step, we will consider integrating the rich structural information into the neural network for word sense disambiguation.",1,2018
p18-1230,there is still one challenge left for the future.,6,2018
p18-1231,"in the future, we will extend our model so that it can project multi-word phrases, as well as single words, which could help with negations and modifiers.",1,2018
p18-1234,how to leverage large-scale sentiment lexicons in neural networks would be our future work.,1,2018
p18-1237,"in future work, we plan to study how to distinguish effective from ineffective discussions based on our model as well as how to learn from the strategies used in successful discussions, in order to predict the best next deliberative move in an ongoing discussion.",1,2018
p18-1238,we hope that the availability of the conceptual captions dataset will foster considerable progress on the automatic image-captioning task.,2,2018
p18-1243,"although it might be non-trivial to extend the proposed approach to real natural language directly, we regard this work as an initial step towards this ultimate ambitious goal and our game might shed some light on designing more advanced games or performing real-world data collection.",4,2018
p18-1243,we plan to investigate the generalization and application of the proposed approach to more realistic environments with more diverse tasks in future work.,4,2018
p18-1247,we believe this framework can be extended to other sequence labeling tasks in nlp such as semantic role labeling.,4,2018
p18-1249,our results suggest that further research into different ways of encoding utterances can lead to additional improvements in both parsing and other natural language processing tasks.,1,2018
p18-1249,"the gains we see come not only from incorporating more information (such as subword features or externally-trained word representations), but also from structuring the architecture to separate different kinds of information from each other.",1,2018
p18-1251,"for future work, other potential bottlenecks could be addressed.",6,2018
p18-1251,the queue will also require a mechanism to avoid inserting duplicate tuples into the queue.,1,2018
p18-1252,"in future, we would like to advance this work in two directions: 1) proposing more effective conversion approaches, especially by exploring the potential of treelstms; 2) constructing bi-tree aligned data for other treebanks and exploiting all available single-tree and bi-tree labeled data for better conversion.",1,2018
p18-1255,"in order to move to a full system that can help users like terry write better posts, there are three interesting lines of future work.",1,2018
p18-1255,"such pragmatic principles have recently been shown to be useful in other tasks as well (golland et al., 2010; smith et al., 2013; orita et al., 2015; andreas and klein, 2016).",4,2018
p18-1256,"in future work, we would like to focus more on designing models that can deal with and be optimized for scenarios with severe data imbalance.",1,2018
p18-1256,"we would like to also explore various applications of presupposition trigger prediction in language generation applications, as well as additional attention-based neural network architectures.",4,2018
P19-1001,"in the future, we plan to integrate our ioi model with models like elmo (peters et al., 2018) and bert (devlin et al., 2018) to study if the performance of ioi can be further improved.",1,2019
P19-1002,"in the future, we plan to apply reinforcement learning to further improve the performance.",1,2019
P19-1003,we hope the collected dataset and proposed model can benefit future related research.,2,2019
P19-1004,we also foresee this paradigm being useful when building new dialog datasets to understand the kinds of information models use to solve them.,2,2019
P19-1006,"in the future, we would like to explore the effectiveness of various attention methods to solve indefinite choices task with interpretive features.",1,2019
P19-1007,"in the future, we want to incorporate this framework with much refined primal and dual models, and design more informative reward signals to make the training more efficient.",1,2019
P19-1007,"it would be appealing to apply graph neural networks (chen et al., 2018b, 2019) to model structured logical forms.",1,2019
P19-1009,"for future work, we would like to extend our model to other semantic parsing tasks (oepen et al., 2014; abend and rappoport, 2013).",4,2019
P19-1010,"for future direction, we are interested in exploring constrained decoding, better incorporating pre-trained language representations within our architecture, conditioning on additional relations between entities, and different gnn formulations.",1,2019
P19-1011,experiments on nearest-neighbor sentence retrieval further validate the effectiveness of proposed framework.,3,2019
P19-1012,we leave this question for future work.,6,2019
P19-1014,"in future work, we’d like to improve this integration in order to gain from training on examples from different domains for tags like ‘name’ and ‘location’.",1,2019
P19-1016,"our future work includes integrating advanced word representation methods (e.g., elmo and bert) and extending the proposed model to other tasks, such as event extraction and co-reference resolution.",4,2019
P19-1016,we also plan to incorporate external knowledge and common sense as additional signals into our architecture as they are important for human readers to recognize names but still absent from the current model.,1,2019
P19-1017,"for further works, we will leverage more supervised translation hops to improve the performance of unsupervised translation for distant languages.",2,2019
P19-1017,we will extend our method to more distant languages.,2,2019
P19-1019,"finally, we would like to adapt our approach to more relaxed scenarios with multiple languages and/or small parallel corpora.",2,2019
P19-1019,"in addition to that, we would like to incorporate a language modeling loss during nmt training similar to he et al.(2016).",1,2019
P19-1019,"in the future, we would like to explore learnable similarity functions like the one proposed by (mccallum et al., 2005) to compute the characterlevel scores in our initial phrase-table.",1,2019
P19-1023,"in the future, we plan to explore contextbased similarity to complement the lexical similarity to improve the overall performance.",1,2019
P19-1024,"one natural question we would like to ask is how to make use of the proposed framework to perform improved graph representation learning for graph related tasks (bastings et al., 2017).",5,2019
P19-1024,there are multiple venues for future work.,6,2019
P19-1027,"our evaluation did not include other subword representations, most notably elmo (peters et al., 2018) and contextual string embeddings (akbik et al., 2018), since, even though they are languageagnostic in principle, pretrained models are only available in a few languages.conclusions.",3,2019
P19-1029,"future research directions will involve the development of reinforcement learning model with multi-dimensional rewards, and modeling explicit credit assignment for improving the capabilities of the regulator to make context-sensitive decisions in mini-batch learning.",1,2019
P19-1029,"the proposed framework can naturally be expanded to integrate more feedback modes suitable for the interaction with humans, e.g., pairwise comparisons or output rankings.",1,2019
P19-1034,another interesting topic for future research is the comparison of domain adaptation based on our domain-specific word embeddings vs. based on word embeddings trained on much larger corpora.,2,2019
P19-1034,"in the future, we plan to investigate whether there are changes over time that significantly impact the linguistic characteristics of the data, in the simplest case changes in the meaning of a word.",5,2019
P19-1035,another strand of research will be combining both strategies and deploying the manipulation strategies in a large scale testing platform that allows the system to adapt to an individual learner over time.,1,2019
P19-1035,future manipulation strategies that take the competencies into account have the potential to train particular skills and to better control the competencies required for a placement test.,1,2019
P19-1035,our error analysis points out important directions for future work on detecting ambiguous gaps and modeling gap interdependencies for c-tests deviating from the default generation scheme.,1,2019
P19-1036,this is certainly an avenue that we seek to explore.,6,2019
P19-1036,"we have not explored whether recent advances in word embeddings from instance elmo (peters et al., 2018) and bert (devlin et al., 2018) could add further benefits.",5,2019
P19-1037,doctors and patients speak a different language and we hope that our work will help them communicate.,2,2019
P19-1037,"finally, we will explore adaptations of our methodology for general (non-medical) domains, e.g., simplified search interfaces (ananiadou et al., 2013) for semantically annotated news (thompson et al., 2017).",4,2019
P19-1037,one clear avenue of future work is to apply this system in a clinical setting and to test the results with actual patients.,4,2019
P19-1037,we could also look to use parallel simplified medical text to augment the general language parallel text used in the nts system.,2,2019
P19-1037,we will look to develop software that uses nts to identify possible simplifications for a clinician when they are writing a letter for a patient.,1,2019
P19-1038,"even though our work is an application of financial domain, we hope our multimodal learning model can also be useful in other areas (such as social media and customer service) where multimodality data is available.",4,2019
P19-1039,"lastly, we look forward to further exploring this line of research by investigating: 1. the individual differences in both detecting concealed information and concealing information, by analyzing the features across groups defined by individual personality traits (fornaciari et al., 2013, an et al., 2018), ethnics, native languages, and different dimensions of professional skills; 2. the result and model robustness by collecting and testing other field data such as board games; 3. the predictive power of phonotactic variation features; 4. the relationship between perceived information concealment and concealing information; 5. how soon can we detect concealed information; 6. how to conduct domain adaptation with regards to detecting concealed information; 7. efficient ways to make the multi-task learning framework scalable.",1,2019
P19-1039,"third, more analyses of acoustics such as pitch and tonal contour, phonotactic variations could be incorporated to further explore the space of information concealment in speech.",1,2019
P19-1040,"our framework can be extended to deal with sources that generate a spectrum of perspectives, each with a stance relative to claim and with evidence supporting it.",2,2019
P19-1040,we leave this for future work.,6,2019
P19-1043,"in the future, we would like to generalize it to multiple domains and datasets.",2,2019
P19-1044,"in the future, we plan to evaluate temporal referencing against the related dynamic embedding models on an annotated empirical lexical change dataset with multiple languages.",3,2019
P19-1045,"in our future work, we would like to improve the model structure and the adversarial learning algorithm.",1,2019
P19-1045,"last but not least, we would like to apply our approach to other heterogeneous texts-concerned nlp tasks.",4,2019
P19-1046,"in future work, we intend to explore multiple local fusion methods within our framework.",1,2019
P19-1047,"promising directions for future research include examining additional features and feature representations: pragmatic features such as formality (pavlick and tetreault, 2016) or politeness (danescu-niculescu-mizil et al., 2013); acousticprosodic features from earnings call audio; more sophisticated semantic representations such as claims (lim et al., 2016), automatically induced entity-relation graphs (bansal et al., 2017) or question-answer motifs (zhang et al., 2017) (these representations are non-trivial to construct because a single turn may contain many questions or answers); or even discourse structures.",1,2019
P19-1047,"the models used in this work aim to be just complex enough to determine whether useful signals exist for this task; future modeling work could include training a complete end-to-end system such as a hierarchical attention network (yang et al., 2016), or building industry-specific models.",1,2019
P19-1048,"the proposed architecture can potentially be applied to similar tasks such as relation extraction, semantic role labeling, etc.",4,2019
P19-1049,it would also be nice to explore about more fine-grained functional components and grammatical entities in the future works.,1,2019
P19-1050,"building upon this dataset, future research can explore the design of efficient multimodal fusion algorithms, novel erc frameworks, as well as the extraction of new features from the audio, visual, and textual modalities.",1,2019
P19-1050,we believe this dataset will also be useful as a training corpus for both conversational emotion recognition and multimodal empathetic response generation.,2,2019
P19-1052,"in order to solve the problem of lacking aspect-level labeled data, we wish to utilize the abundant document-level labeled data.",2,2019
P19-1053,these mined information can be further used to refine the model training via a regularization term.,1,2019
P19-1053,"thus, we plan to extend our approach to other neural nlp tasks with attention mechanisms, such as neural document classification (yang et al., 2016) and neural machine translation (zhang et al., 2018).",4,2019
P19-1054,future work should thus study the overlapping nature of argument clustering.,1,2019
P19-1058,"in the future work, we will focus on how to mine different representations for different discourse relation types and apply the topic information to other languages.",4,2019
P19-1059,future work should provide further exploration of the data regime in which pragmatic learning is most beneficial and its correspondence to realworld language use.,2,2019
P19-1060,"as part of future work, we would like to investigate the use of contextualized embeddings (e.g., bert, devlin et al.(2018)) for coherence assessment – as such representations have been shown to carry syntactic information of words (tenney et al., 2019) – and whether they allow multi-task learning frameworks to learn complementary aspects of language.",1,2019
P19-1061,"in future work, we will enrich our weak supervision system by giving the lfs access to more sophisticated contexts that take into account global structuring constraints in order to see how they compare to exogenous decoding constraints applied in (muller et al., 2012; perret et al., 2016).",1,2019
P19-1064,another interesting direction would be introducing intermediate step rewards for each action to better guide the behaviour of the rl agent.,1,2019
P19-1064,"there are several potential improvements to our model as future work, such as incorporating mention detection result as a part of the reward.",1,2019
P19-1065,"since implicit discourse relation identification is a key task for dialogue systems, there are still many approaches worth investigating in future work.",1,2019
P19-1066,in the future we plan to further enrich these representations by considering information from across the document.,2,2019
P19-1068,"in future work, we aim to determine if the same level of accuracy can be obtained when single sentences will be used as samples for training and testing.",3,2019
P19-1068,we leave these ideas for future exploration.,6,2019
P19-1069,"as future work we plan to exploit a subset of the wikipedia categories as coarse-grained sense inventory and enrich our dataset with coarser labels, hence enabling wsd at different granularities.",2,2019
P19-1070,we hope that this study will encourage future work on cle evaluation and analysis and help guide the development of new cle models.,3,2019
P19-1077,the integration takes place by using the automatic mention detectors as ‘players’ in the game.,1,2019
P19-1078,"in future work, transferring knowledge from other resources can be applied to further improve zero-shot performance, and collecting a dataset with a large number of domains is able to facilitate the application and study of metalearning techniques within multi-domain dst.",2,2019
P19-1079,we further explored learning these features in parallel and serial mtl architectures.,1,2019
P19-1084,"our methodology can be extended to handle biases in other tasks where one is concerned with finding relationships between two objects, such as visual question answering, story cloze completion, and reading comprehension.",4,2019
P19-1084,we hope to encourage such investigation in the broader community.,6,2019
P19-1085,"in the future, we would like to design a multi-step evidence extractor and incorporate external knowledge into our framework.",1,2019
P19-1086,we hope that sherliic will foster better modeling of lexical inference in context as well as progress in nli in general.,1,2019
P19-1087,we plan to investigate the impact of combining the two models.,1,2019
P19-1088,"in the future, we plan to build upon the acquired knowledge and the developed classifiers to generate systems able to provide actionable feedback on how to achieve high-quality counseling.",1,2019
P19-1088,we presented an extensive analysis of linguistic aspects of the collaboration process during counseling conversations in relation to counseling quality.,1,2019
P19-1089,future work could adapt our framework to examine more complex forms of conversational development.,1,2019
P19-1089,"future work could more directly model how counselors respond to texter behaviors, hence gauging the extent to which counselors evolve in their interactional practices.",1,2019
P19-1089,"other approaches, such as qualitative labeling by domain experts, could examine whether such changes in language use also result in better conversations.",3,2019
P19-1089,"our methodology could also be extended to examine other conversational contexts such as academic advising or business interactions, where individuals are expected to learn from experience.",4,2019
P19-1090,"a next step would be comprehensive error analysis for a better understanding of where the models succeed or fail in capturing semantic information, particularly for the low-resource languages.",1,2019
P19-1090,we are also working to include into our models the long tail in the distribution of template answers.,1,2019
P19-1090,"we further intend to explore transfer learning techniques (zhang et al., 2017) as well as deep architectures designed specifically for answer selection (lai et al., 2018).",1,2019
P19-1091,"for future work, we plan to investigate the model on other related tasks such as relation extraction, normalization as well as the use of advanced conditional models.",1,2019
P19-1092,we hope this work will encourage research on designing more powerful qa systems that can carry out effective information extraction and reasoning.,1,2019
P19-1093,"a possibility that we did not expand on in this paper is to pre-train one leg of the network on an argument detection data set, like the one of shnarch et al.(2018).",2,2019
P19-1093,"in addition, more careful design of the architecture details, which was not the focus of this work, will probably yield better results yet, e.g., contextualized word embeddings (peters et al., 2018), batch normalization (ioffe and szegedy, 2015; cooijmans et al., 2017), deeper networks and other architecture practical heuristics.",1,2019
P19-1093,"in the future we aim to test and adapt other improvements in the learning to rank field to our task, hoping for further improvement by those models (burges, 2010; severyn and moschitti, 2015).",1,2019
P19-1094,we plan to pursue these research directions in future work.,6,2019
P19-1096,"in the future work, we will try to build a one-step model that directly extract the emotion-cause pairs in an end-to-end fashion.",1,2019
P19-1100,our detailed observations can provide more hints for the follow-up researchers to design more powerful learning frameworks.,1,2019
P19-1101,"by aggregating over different people but in one domain, one can uncover a domain-specific k. similarly, by aggregating over many topics for one person, one would find a personalized k. these consistute promising research directions for future works.",2,2019
P19-1101,"for the background knowledge k, a promising direction would be to use the framework to actually learn it from data.",1,2019
P19-1101,"then, interesting research questions arise like which granularity offers a good approximation of semantic units?",5,2019
P19-1102,in the future we plan to explore interactions among documents beyond concatenation and experiment with summarizing longer input documents.,2,2019
P19-1103,"in the future, we would like to evaluate the attacking effectiveness and efficiency of our methods on more datasets and models, and do elaborate human evaluation on the similarity between clean texts and the corresponding adversarial examples.",3,2019
P19-1104,"our study opens up interesting avenues for future research: obfuscation by addition instead of by reduction, development of more powerful, targeted paraphrasing operators, and, theoretical analysis of the search space properties.",1,2019
P19-1105,"future work will include: (i) incorporating lexical semantics such as named entities for further improvement, (ii) comparing our model to other deep contextualized word representation such as elmo and bert, and (iii) applying the method to other domains for quantitative evaluation.",1,2019
P19-1106,we aim to include review reliability prediction in the pipeline of our future work.,1,2019
P19-1106,we intend to work upon those and also explore more sophisticated techniques for sentiment polarity encoding.,1,2019
P19-1106,"with further exploration, we aim to mould the ongoing research to an efficient ai-enabled system that would assist the journal editors or conference chairs in making informed decisions.",1,2019
P19-1108,"other ways of combining the two modules, more sophisticated classifiers for both phm detection and figurative usage detection, are possible directions of future work.",1,2019
P19-1109,"finally, we plan to investigate alternative methods to modelling phrase and multi-word expression complexity.",1,2019
P19-1109,our future research will focus on the relative nature of complexity judgements and will use the seq model to predict complexity on a scale.,1,2019
P19-1109,we will also investigate whether the seq model may benefit from sources of information other than word embeddings and character-level morphology.,2,2019
P19-1112,"as future work, we plan to investigate emphasis selection on a larger and more diverse dataset.",2,2019
P19-1112,"we also plan to investigate the role of word sentiment and emotion intensity as well as more advanced language models such as bert (devlin et al., 2018) in modeling emphasis.",1,2019
P19-1114,"after collecting more labeled data, and tuning our model using anomaly detection techniques like isolation forests (liu et al., 2008), we hope to expand this study to the stage where we are able to use unbalanced data sets.",1,2019
P19-1114,"given that, in our future work, we will be investigating those false positive cases with our collaborators to assess what the correct label for these ads should be.",3,2019
P19-1114,"moreover, since the proposed full feature set involves hundreds of features we plan to increase our sample size to have a better estimation of the performance of our final predictor.",2,2019
P19-1115,promising future work includes extension to tree-structured inputs and application to other tasks.,4,2019
P19-1117,we first introduce a representor for replacing both encoder and decoder so as to fully explore the commonality among languages.,1,2019
P19-1120,"5 as for future work, we will test our methods in the nmt transfer where the target language is switched.",3,2019
P19-1120,"we also plan to compare different algorithms for learning the cross-lingual mapping (artetxe et al., 2018a; xu et al., 2018; joulin et al., 2018) to optimize the transfer performance.",3,2019
P19-1122,"finally, we hope that future work in this area will follow our lead in using carefully-controlled experiments to enable meaningful comparisons.",3,2019
P19-1122,"while our method is currently restricted to languages that have reliable constituency parsers, an exciting future direction is to explore unsupervised tree induction methods for low-resource target languages (drozdov et al., 2019).",1,2019
P19-1123,"in future, we would like to extend the method to handle more than two curricula objectives.",1,2019
P19-1124,"in the future, we believe more work on improving cfs alignment is potential to improve translation quality, and we will investigate on using source context and target history context in a more robust manner for better predicting cfs and cft words.",1,2019
P19-1125,"as a future work, we can try to improve the performance of the nmt by introducing more powerful demonstrator with different structure (e.g.right to left).",1,2019
P19-1128,"our model can also be considered as a more generic framework for graph generation problem with unstructured input other than text, e.g.image, video, audio.",1,2019
P19-1132,"in the future work, we will explore the usage of this method with other applications.",4,2019
P19-1132,"our idea of encoding a passage regarding multiple entities has potentially broader applications beyond relation extraction, e.g., entity-centric passage encoding in question answering (song et al., 2018a).",4,2019
P19-1133,"future work will investigate more complex and recent neural network models such as devlin et al.(2018), as well as alternative losses.",1,2019
P19-1134,"in future work, we want to further investigate the extent of syntactic structure captured in deep language language representations.",1,2019
P19-1135,"in the future, we hope to improve our work by the utilization of better model-based pattern extractor, and resorting to latent variable model (kim et al., 2018) for jointly modeling instance selector.",1,2019
P19-1135,"what is more, we also hope to verify the effectiveness of our method on more tasks, including open information extraction and event extraction, and also overlapping relation extraction models (dai et al., 2019).",3,2019
P19-1137,"for the future work, we plan to extend diagnre to other ds-based applications, such as question answering (lin et al., 2018), event extraction (chen et al., 2017), etc.",4,2019
P19-1139,"there are three important directions remain for future research: (1) inject knowledge into feature-based pre-training models such as elmo (peters et al., 2018); (2) introduce diverse structured knowledge into language representation models such as conceptnet (speer and havasi, 2012) which is different from the world knowledge database wikidata; (3) annotate more real-world corpora heuristically for building larger pre-training data.",1,2019
P19-1140,"in future, we are interested in introducing text information of entities for alignment by considering word ambiguity (cao et al., 2017b); and meanwhile, through cross-kg entity proximity (cao et al., 2015).",1,2019
P19-1141,"although we specifically investigated the ner task for chinese in this work, we believe the proposed model can be extended and applied to other languages, for which we leave as future work.",4,2019
P19-1142,"also, it will be worthwhile to ascertain the efficacy of pdr for language models using transformers and in combination with frage embeddings.",3,2019
P19-1142,future work includes exploring the application of pdr to other seq2seq models that have a similar input-output symmetry.,4,2019
P19-1144,"in future work, we aim to capture better structural information and possible connections to unsupervised grammar induction.",1,2019
P19-1146,"a natural next step is to apply entmax to self-attention (vaswani et al.,2017).",4,2019
P19-1147,future work includes developing a adversarial training scheme as well as devising a more robust architecture based on our findings.,1,2019
P19-1149,"in the future, we are interested in testing lowlevel optimizations of lrn, which are orthogonal to this work, such as dedicated cudnn kernels.",3,2019
P19-1150,"in the future, we plan to apply capsule networks to even more challenging nlp problems such as language modeling and text generation.",4,2019
P19-1150,making computers perform more like humans is a major issue in nlp and machine learning.,5,2019
P19-1153,"continuing in the direction of training our model on different nlp tasks, we would like our representations to generalize well on downstream tasks while maintaining their reconstruction property.",4,2019
P19-1153,"finally, we would like to learn our sentence embeddings’ latent space, similarly to subramanian et al.(2018)’s method, so as to leverage our autoencoder’s strong reconstruction ability and generate very long sequences of text.",1,2019
P19-1153,we would also like to further explore the usage of sub-sentence representations in natural language processing.,1,2019
P19-1159,"a few interdisciplinary studies (herbelot et al., 2012; avin et al., 2015; fu et al., 2016; schluter, 2018) have emerged, and we urge more interdisciplinary discussions in terms of gender bias.",5,2019
P19-1159,"approaches from other technical fields may improve current debiasing methods in nlp or inspire the development of new, more effective methods even if the properties of the data or problem are different across fields.",1,2019
P19-1159,"as mentioned in section 1, gender bias is not a problem that is unique to nlp; other fields in computer science such as data mining, machine learning, and security also study gender bias (calders and verwer, 2010; feldman et al., 2015; hardt et al., 2016; misra et al., 2016; kleinberg et al., 2016; pleiss et al., 2017; beutel et al., 2017; kilbertus et al., 2017).",5,2019
P19-1159,future work can look to apply existing methods or devise new techniques towards mitigating gender bias in other languages as well.,1,2019
P19-1159,"non-binary genders (richards et al., 2016) as well as racial biases have largely been ignored in nlp and should be considered in future work.",5,2019
P19-1159,"to completely debias effectively, it is important to understand how machine learning methods encode biases and how humans perceive biases.",1,2019
P19-1159,"to perform gender-swapping in such languages, besides swapping those gendered nouns, we also need to change the modifiers.non-binary gender bias.",2,2019
P19-1160,"in future, we plan to extend the proposed method to debias other types of demographic biases such as ethnic, age or religious biases.",1,2019
P19-1161,"finally, we also identified avenues for future work, such as the inclusion of co-reference information.",2,2019
P19-1165,trying more complex networks is also within our scope and is left as future work.,1,2019
P19-1167,"in future work, we intend to conduct a diachronic analysis in english using the same corpus, in addition to a cross-linguistic study of gendered language.",2,2019
P19-1168,it may be of interest to extend these techniques to embed knowledge graph elements.,1,2019
P19-1169,"additionally, the mapping technique used for relation-aware semantic projection can be further improved to model different linguistic properties of lexical relations (e.g., the “one-to-many” mappings for meronymy).",1,2019
P19-1169,"in the future, we will improve our model to deal with datasets containing a relatively large number of lexical relation types and random term pairs.",1,2019
P19-1170,"finally, the preliminary results we have shown on aligning more than two languages at the same time provide an exciting path for future research.",2,2019
P19-1171,"an avenue for future work is connecting our discovered phonesthemes to putative meanings, as done by abramova et al.(2013) and abramova and fernandez ′ (2016).",1,2019
P19-1173,"future work includes joint and cross-dialectal lemmatization models, in addition to further extension to other dialects.",1,2019
P19-1174,"in future work, we will further explore the effectiveness of the reordering mechanism and apply it to other natural language processing tasks, such dependency parsing (zhang et al., 2016; li et al., 2018), and semantic role labeling (he et al., 2018; li et al., 2019).",4,2019
P19-1175,"in a next step, we plan to compare the performance of nfr to other approaches to tm-nmt integration, for example by carrying out evaluations on the jrc-acquis corpus (gu et al., 2018; koehn and senellart, 2010a; zhang et al., 2018).",3,2019
P19-1175,"we also intend to carry out further tests to potentially improve the quality of the output, for example by testing different match metrics and retrieval methods, nmt architectures (e.g.transformer), ways to include alignment information and by applying additional morphological preprocessing.",3,2019
P19-1178,"as future work, we will apply our methodology to domain adaptation.",4,2019
P19-1178,"in the same vain as unsupervised mt, we want to continue our research by using back translation for rejected pairs and dealing with phrases instead of full sentences.",2,2019
P19-1178,our architecture is also useful for data selection in data rich language pairs and we will perform experiments on cleaning noisy parallel corpora.,2,2019
P19-1178,that will allow us to extract more parallel text from a corpus and facilitate using these approaches for low-resourced languages.,2,2019
P19-1179,"our method does not introduce additional parameters: we hope to motivate future work on learning speech representations, with continued performance on lowerresource settings if additional parameters are introduced.",1,2019
P19-1180,"finally, it may be possible to extend our approach to other linguistic tasks such as dependency parsing (christie et al., 2016b), coreference resolution (kottur et al., 2018), and learning pragmatics beyond semantics (andreas and klein, 2016).",4,2019
P19-1180,"its applicability could be extended by learning shared representations across multiple modalities (castrejon et al., 2016) or integrating with pure text-domain models (such as prpn, shen et al., 2018a).",1,2019
P19-1180,"its performance may be boosted by considering structured representations of both images (e.g., lu et al., 2016; wu et al., 2019) and texts (steedman, 2000).",1,2019
P19-1180,the results suggest multiple future research directions.,6,2019
P19-1181,"for example, future extensions of vln will likely involve games (baldridge et al., 2018) where the instructions being given take the agent around a trap or help it avoid opponents.",1,2019
P19-1182,"for future work, we are going to further explore the possibility to merge the three datasets by either learning a joint image representation or by transferring domain-specific knowledge.",2,2019
P19-1184,"in future work, the data can be used to further investigate common ground and conceptual pacts; be extended through manual annotations for a more thorough linguistic analysis of co-reference chains; exploit the combination of vision and language to develop computational models for referring expression generation; or use the photobook task in the parlai framework for turing-test-like evaluation of dialogue agents.",2,2019
P19-1187,in the future work we would like to consider setups where human-annotated data is combined with naturally occurring one (i.e.distantly-supervised one).,2,2019
P19-1187,it would also be interesting to see if mistakes made by fully-supervised systems differ from the ones made by our system and other wikipediabased linkers.,5,2019
P19-1190,"as future work, we will consider integrating more kinds of syntactic features from linguistic analysis such as dependency parsing.",1,2019
P19-1191,"in the future, we plan to develop techniques for extracting entities of more fine-grained entity types, and extend paperrobot to write related work, predict authors, their affiliations and publication venues.",1,2019
P19-1191,"we build a paperrobot who can predict related entities for an input title and write some key elements of a new paper (abstract, conclusion and future work) and predict a new title.",1,2019
P19-1192,"in the future, we will investigate the possibility of incorporating additional forms of rhetoric, such as parallelism and exaggeration, to further enhance the model and generate more diverse poems.",1,2019
P19-1193,the proposed model integrates commonsense from the external knowledge base into the generator through a dynamic memory mechanism to enrich the source information.,1,2019
P19-1196,"aside from such syntax templates, in the future, we aim to explore how semantic templates contribute to type description generation.",1,2019
P19-1198,"in future, we would like to improve the system further by incorporating better architectural designs and training schemes to tackle complex simplification operations.",1,2019
P19-1201,"conala (yin et al., 2018a) and staqc (yao et al., 2018).",1,2019
P19-1201,"in the future, we will further apply dim to learn semantic parser and nl generator from the noisy datasets.",4,2019
P19-1201,we further extend supervised dim to semi-supervised scenario (semidim).,1,2019
P19-1203,"besides, it would also be interesting to consider using linguistic knowledge such as named entities or part-of-speech tags to improve the coherence of the conversation.",2,2019
P19-1203,"in the future, we would like to explore how to better select the rationale for each question.",5,2019
P19-1204,"in the future, we plan to study the effect of other video modalities on the alignment algorithm.",1,2019
P19-1206,"in future work, we will make comparisons with those of a humanannotated dataset.",2,2019
P19-1206,"our model can also be applied to other applications, such as argument mining, because arguments typically have the same discourse structure as reviews.",4,2019
P19-1208,"another interesting direction is to apply our rl approach on the microblog hashtag annotation problem (wang et al., 2019; gong and zhang, 2016; zhang et al., 2018b).",4,2019
P19-1208,"one potential future direction is to investigate the performance of other encoder-decoder architectures on keyphrase generation such as transformer (vaswani et al., 2017) with multi-head attention module (li et al., 2018; zhang et al., 2018a).",1,2019
P19-1210,"in the future, we plan to further integrate higher level participant interactions, such as gestures, face expressions, etc.",1,2019
P19-1210,"we also plan to construct a larger multimedia meeting summarization corpus to cover more diverse scenarios, building on our previous work (bhattacharya et al., 2019).",2,2019
P19-1211,"in the future, we would like to understand the usefulness of artificial titles for training the decoder relative to other factors that may impact performance, e.g., how similar the true titles or summaries are in the different domains.",1,2019
P19-1212,bigpatent can enable future research to build robust systems that generate abstractive and coherent summaries.,1,2019
P19-1215,"in future work, we intend to examine multitask approaches combining question summarization and question understanding.",1,2019
P19-1216,"in the future, we will consider extending the current approach to the single document or multiple document summarization.",1,2019
P19-1217,"in the future, we will expand it to support the questions on text span selection by using the relation type rather than the option as the terminated condition.",1,2019
P19-1219,"in the future, we plan to use some larger knowledge bases, such as conceptnet and freebase, to improve the quality and scope of the general knowledge.",2,2019
P19-1220,our future work will involve exploring the potential of our multi-style learning towards natural language understanding.,4,2019
P19-1221,"future work will concentrate on designing a fast neural pruner to replace the ir-based pruning component, developing better end-to-end training strategies, and adapting our approach to other datasets such as natural questions (kwiatkowski et al., 2019).",1,2019
P19-1222,an interesting improvement to our approach would be to allow the retriever to automatically determine whether or not more retrieval iterations are needed.,1,2019
P19-1222,we hope to tackle this problem in future work to allow learning more than two retrieval iterations.,1,2019
P19-1226,"this work demonstrates the feasibility of further enhancing advanced lms with knowledge from kbs, which indicates a potential direction for future research.",2,2019
P19-1227,we hope our work could contribute to the development of cross-lingual openqa systems and further promote the research of overall cross-lingual language understanding.,4,2019
P19-1228,learning deep generative models which exhibit such conditional markov properties is an interesting direction for future work.,1,2019
P19-1233,"in the future, we would like to extend gcdt to other analogous sequence labeling tasks and explore its effectiveness on other languages.",4,2019
P19-1234,"this work proposes a neural pcfg inducer which employs context embeddings (peters et al., 2018) in a normalizing flow model (dinh et al., 2015) to extend pcfg induction to use semantic and morphological information.",1,2019
P19-1238,the results of this initial attempt are optimistic.,6,2019
P19-1239,"in future work, we will incorporate other modality such as audio into the sarcasm detection task and we will also investigate to make use of common sense knowledge in our model.",1,2019
P19-1240,"also, our topic-aware neural keyphrase generation model can be investigated in a broader range of text generation tasks.",4,2019
P19-1240,"in the future, we will explore how to explicitly leverage the topic-word distribution to further improve the performance.",1,2019
P19-1241,our future agenda includes exploring the applicability of our analysis and system for identifying patterns and potential prevention.,4,2019
P19-1241,we also plan to use this model to solve other downstream medium-specific tasks pertaining to mental health and welfare.,4,2019
P19-1242,"although we focused on english hashtags, our pairwise ranking approach is language-independent and we intend to extend our toolkit to languages other than english as future work.",2,2019
P19-1243,"we leave alternative solutions for future work, including training embeddings from scratch or fine-tuning on the target corpus (however, these ideas are only feasible with a large target corpus, and the need for fine-tuning reduces the usefulness of pre-trained embeddings).",2,2019
P19-1244,"for the future work, beyond what we have mentioned, we plan to examine our model on different information sources.",3,2019
P19-1244,"we will also try to incorporate relevant metadata into it, e.g., author profile, website credibility, etc.",2,2019
P19-1246,"first, we wish to investigate the role of additional variables (such as age and gender).",3,2019
P19-1246,"nevertheless, there are limitations of distant labelling and social media data — with issues related specifically to the language of food (askalidis and malthouse, 2016) — that we will take into account in future work.",5,2019
P19-1247,"we intend to study fine-grained political perspectives, capturing how different events are framed.",1,2019
P19-1248,"for example, revising spoiler contents in a ‘non-spoiler’ way would be an interesting language generation task.",5,2019
P19-1248,"our new dataset, analysis of spoiler language, and positive results facilitate several directions for future work.",2,2019
P19-1249,"in future work, we plan on improving the corpus by incorporating verified accounts from other social networks, and, by inferring new labels for as of yet unlabeled celebrities through link prediction.",2,2019
P19-1250,our future work will include efficiently labeling promising comments via active learning.,1,2019
P19-1252,"directions of future research include adaptation of our methods to a large scale, sparsely connected social network.",4,2019
P19-1252,"one might also want to investigate the inductive settings of gcn (hamilton et al., 2017) to predict demographic information of a user from outside the black network.",1,2019
P19-1253,we also plan to adapt daml to multi-domain dialog tasks.,4,2019
P19-1256,"we are still on the way to find out solutions for cases with huge noise (perezbeltrachini and lapata, 2018; wiseman et al., 2017), where heavy manual intervention or external knowledge should be desperately needed.",5,2019
P19-1258,our future work will focus on how to transfer the long-term memory across different tasks.,5,2019
P19-1259,"finally, we believe that our framework can generalize to other cognitive tasks, such as conversational ai and sequential recommendation.",4,2019
P19-1259,"moreover, we expect that prospective architectures combining attention and recurrent mechanisms will largely improve the capacity of system 1 by optimizing the interaction between systems.",1,2019
P19-1259,multiple future research directions may be envisioned.,6,2019
P19-1260,"in the future, we would like to investigate explainable gnn for this task, such as explicit reasoning path in (kundu et al., 2018), and work on other data sets such as hotpotqa.",1,2019
P19-1265,"to create them on the fly, annotation aggregation methods for sequence labelling (simpson and gurevych, 2018) can be used.",1,2019
P19-1268,we also recommend publishing prediction files along with models to facilitate error analysis.,1,2019
P19-1271,"as future work, we intend to continue collecting more data for islam and to include other hate targets such as migrants or lgbt+, in order to put the dataset at the service of other organizations and further research.",2,2019
P19-1271,"moreover, as a future direction, we want to utilize conan dataset to develop a counter-narrative generation tool that can support ngos in fighting hate speech online, considering counter-narrative type as an input feature.",2,2019
P19-1272,"future work will look deeper into using the similarity between the content of the text and image (leong and mihalcea, 2011), as the text task results showed room for improvements.",1,2019
P19-1272,"we envision that our data, task and classifiers will be useful as a preprocessing step in collecting data for training large scale models for image captioning (feng and lapata, 2010) or tagging (mahajan et al., 2018) or for improving recommendations (chen et al., 2016) by filtering out tweets where the text and image have no semantic overlap or can enable new tasks such as identifying tweets that contain creative descriptions for images.",1,2019
P19-1274,"future work could study other types of accounts with similar posting behaviors such as organizational accounts, explore other sources for ground truth tweet identity information (robinson, 2016) or study the effects of user traits such as gender or political affiliation in tweeting signed content.",2,2019
P19-1275,we suggest a future research direction in sarcasm detection where the two types of sarcasm are treated as separate phenomena and socio-cultural differences are taken into account.,5,2019
P19-1277,studying few-shot relation classification with data generated by distant supervision and extending our mlman model to zero-shot learning will be the tasks of our future work.,1,2019
P19-1278,"we note that there are a wide range of future directions: (1) human prior knowledge could be incorporated into the similarity quantification; (2) similarity between relations could also be considered in multi-modal settings, e.g., extracting relations from images, videos, or even from audios; (3) by analyzing the distributions corresponding to different relations, one can also find some “meta-relations” between relations, such as hypernymy and hyponymy.",1,2019
P19-1279,"in future work, we plan to work on relation discovery by clustering relation statements that have similar representations according to bertem+mtb.",1,2019
P19-1279,we will also study representations of relations and entities that can be used to store relation triples in a distributed knowledge base.,1,2019
P19-1281,"future research directions include making selection routines more robust to evaluation outliers, relaxing our gaussian assumptions and developing more effective batch strategies.",3,2019
P19-1282,"finally, another direction for future work would be to extend the importance-ranking comparisons that we deploy here for evaluation purposes into a method for deriving better, more informative rankings, which in turn could be useful for the development of new, more interpretable models.",3,2019
P19-1282,"yet another line of work focuses on aligning models with human feedback for what is interpretable (fyshe et al., 2015; subramanian et al., 2017), which could refine our idea of what defines a highquality explanation derived from attention.",1,2019
P19-1283,"we apply the same techniques to english sentence embeddings, and show where and to what extent each representation encodes syntactic information.",4,2019
P19-1283,we plan to explore these options in future work.,6,2019
P19-1284,we leave further explorations for future work.,6,2019
P19-1287,"in the future, we would like to investigate the feasibility of our methods on non-recurrent nmt models such as transformer (vaswani et al., 2017).",1,2019
P19-1287,"moreover, we are also interested in incorporating discourse-level relations into our models.",1,2019
P19-1288,"in the future, we plan to investigate better methods to leverage the sequential information.",1,2019
P19-1288,we believe that the following two directions are worth study.,6,2019
P19-1289,"we leave many open questions to future work, e.g., adaptive policy using a single model (zheng et al., 2019).",5,2019
P19-1290,we train our model by designing an rl algorithm with the reward shaping strategy.,1,2019
P19-1292,"in future work, we would like to do an extensive analysis on the capabilities of bert and transfer learning in general for different domains and language pairs in ape.",3,2019
P19-1293,we use this system to translate test texts from 14 languages into english.,4,2019
P19-1296,"in future work, we intend to apply these methods to other natural language tasks.",4,2019
P19-1297,"in future, we would like to explore other languages with diverse linguistic characteristics.",2,2019
P19-1299,"for future work, we plan to apply man-moe to more challenging languages for tasks such as syntactic parsing, where multilingual data exists (nivre et al., 2017).",4,2019
P19-1299,"furthermore, we would like to experiment with multilingual contextualized embeddings such as the multilingual bert (devlin et al., 2018).",2,2019
P19-1300,it would be also interesting to investigate how our approach compares to the baselines given a large amount of data such as wikipedia.,3,2019
P19-1300,our future work is to exploit character and subword information in our model and see how those information affect the performance in each language pair.,1,2019
P19-1304,"in the future, we will explore more applications of the proposed idea of attentive graph matching.",4,2019
P19-1307,"in the future, we will investigate whether our method helps other downstream tasks.",4,2019
P19-1312,"as a future work, we would like to study, for training bwe, the impact of the use of synthetic parallel data generated by unsupervised nmt, or of a different nature, such as translation pairs extracted from monolingual corpora without supervision.",2,2019
P19-1312,"since our approach works on top of unsupervised mapping for bwe and uses synthetic data generated by unsupervised mt, it will directly benefit from any future advances in these two types of techniques.",2,2019
P19-1316,4 two directions for future work are (i) to extend our approach to other languages by using multilingual resources or translation data; and (ii) to explore various compositionality functions to combine the words’ representation on the basis of their grammatical function within a phrase.,2,2019
P19-1317,"alternatively, sequence encoding models such as gru, cnn, transformer, or even encoders with contextualized word embeddings like bert (devlin et al., 2018), or elmo (peters et al., 2018) can be used to replace this bilstm, however, with additional computation cost.",1,2019
P19-1318,"for future work, we would be interested in further exploring the behavior of neural architectures for nlp tasks which intuitively would benefit from having access to relational information, e.g., text classification (espinosa anke and schockaert, 2018; camacho-collados et al., 2019) and other language understanding tasks such as natural language inference or reading comprehension, in the line of joshi et al.(2019).",1,2019
P19-1322,it is an interesting future work and will make learning word embeddings more like human learning a language.,6,2019
P19-1323,"future work will offer a more principled account of aspectual classification for specific verb classes, among them speech act and communication verbs (e.g., promise or call) that occur frequently in corpora but have hitherto been neglected in aspectual analyses.",1,2019
P19-1323,"we also intend to develop a more principled treatment for the aspectual classification of metaphors, which are frequent in other corpora.",1,2019
P19-1324,"though we focused on lstm lms for english, this method can be applied to other architectures, objective tasks, and languages; possibilities to explore in future work.",4,2019
P19-1324,"we also plan to carry out further analyses aimed at individuating factors that challenge the resolution of lexical ambiguity (e.g., morphosyntactic vs. semantic ambiguity, frequency of a word or sense, figurative uses), as well as clarifying the interaction between prediction and processing of words within neural lms.",1,2019
P19-1326,future research directions include a theoretical explanation of kg2vec and applications to downstream nlp tasks.,4,2019
P19-1329,this work also raises important questions about other categories of word-like tokens that need to be treated like special cases.,5,2019
P19-1330,"also, we will explore ways of automating parts of the process, e.g.the highlight annotation.",1,2019
P19-1330,"in future work, we would like to extend our framework to other variants of summarization e.g.multi-document.",4,2019
P19-1332,"in the future, we plan to investigate more efficient methods of unsupervised domain adaptation with decomposition mechanism on other nlp tasks.",1,2019
P19-1333,"in the future, we plan to investigate the constituency type classification and rhetorical relation identification steps and port this approach to languages other than english.",2,2019
P19-1335,future variations of the task could incorporate nil recognition and mention detection (instead of mention boundaries being provided).,1,2019
P19-1335,we also expect models that jointly resolve mentions in a document would perform better than resolving them in isolation.,1,2019
P19-1337,"our approach is a general one that can be applied to other student model architectures, such as transformers (vaswani et al., 2017).",4,2019
P19-1338,"in future work, we would like to combine more potential parsers—including chartstyle parsing and shift-reduce parsing—and transfer knowledge from one to another in a co-training setting.",1,2019
P19-1340,the remarkable effectiveness of unsupervised pretraining of vector representations of language suggests that future advances in this area can continue improving the ability of machine learning methods to model syntax (as well as other aspects of language).,1,2019
P19-1344,"in our future work, we plan to apply our approach to other sequence labeling tasks, such as named entity recognition, word segmentation and so on.",4,2019
P19-1345,"furthermore, we would like to explore the effectiveness of our approach to asc-qa in other languages.",2,2019
P19-1345,"in our future work, we would like to solve other challenges in asc-qa such as data imbalance and negation detection to improve the performance.",5,2019
P19-1346,"we hope eli5 will inspire future work in all aspects of long-form qa, from the information extraction problem of obtaining information from long, multi-document input to generating more coherent and accurate paragraph-length answers.",5,2019
P19-1350,we reserve a deeper investigation of this aspect to future research.,6,2019
P19-1351,"in future work, we are investigating whether the vtqa model can be jointly trained with the paragraph captioning model.",1,2019
P19-1353,we hope this initial application inspires further research by literary scholars and computational humanists in the future.,4,2019
P19-1354,"as our approach is not limited to the nmt encoders, it is also interesting to explore how do the models trained on other nlp tasks learn word order information.",1,2019
P19-1356,it would be interesting to see if our results transfer to other domains with higher variability in syntactic structures (such as noisy user generated content) and with higher word order flexibility as experienced in some morphologically-rich languages.,4,2019
P19-1358,"in this way, a dialogue agent could both improve its dialogue ability and its potential to improve further.",1,2019
P19-1358,we leave exploration of this metalearning theme to future work.,6,2019
P19-1360,"in the future, we intend to infer the dialog acts from the annotated responses and use such noisy data to guide the response generation.",1,2019
P19-1362,"in addition, the detailed content information can be considered in the relevant contexts to further improve the quality of generated response.",2,2019
P19-1362,"in future work, we plan to further investigate the proposed recosa model.",1,2019
P19-1363,"future work may also incorporate contradiction information into the dialogue model itself, and extend to generic contradictions.",1,2019
P19-1364,another interesting direction is to design a trainable budget scheduler.,1,2019
P19-1364,"in future, we plan to investigate the effectiveness of our method on more complex task-oriented dialogue datasets.",2,2019
P19-1364,"one possible solution is transfer learning, in which we train the budget scheduler on some welldefined dialogue tasks, then leverage this scheduler to guide the policy learning on other complex dialogue tasks.",1,2019
P19-1366,"in addition, we will also explore how to integrate external knowledge in other formats, like the knowledge graph, into adversarial training so that the quality could be further improved.",2,2019
P19-1366,"in future research, we will further investigate how to better leverage larger training data to improve the reat method.",2,2019
P19-1367,"in the future, there are some promising explorations in vocabulary pyramid networks.1) we will further study how to obtain multi-level vocabularies, such as employing other clustering methods and incorporating semantic lexicons like wordnet; 2) we also plan to design deep-pass encoding and decoding for vpn; 3) we will investigate how to apply vpn to other natural language generation tasks such as machine translation and generative text summarization.",1,2019
P19-1372,directions of future work may be pursuing betterdefined features and easier training strategies.,1,2019
P19-1373,"finally, the addition of word-level pretraining methods to improve the dialog context representations should be explored.",1,2019
P19-1373,"in this paper, unsupervised pretraining has been shown to learn effective representations of dialog context, making this an important research direction for future dialog systems.",1,2019
P19-1373,"second, it would be interesting to test the representations learned using unsupervised pretraining on less-related downstream tasks such as sentiment analysis.",3,2019
P19-1373,these results open three future research directions.,6,2019
P19-1375,"theoretically, we believe this self-supervision can be generalized to other types of temporal order in different nlp tasks.",4,2019
P19-1376,"although it learns some of the relevant features anyway, it would be interesting to see whether its behaviour becomes more human-like if the correct features are provided in the input.",1,2019
P19-1376,"there are many other potential architectures and modelling decisions that could be explored, as well as other behavioural data such as developmental patterns (blything et al., 2018; ambridge, 2010) and inflection in other languages (e.g., clahsen et al., 1992; ernestus and baayen, 2004).",1,2019
P19-1378,"another interesting direction is to explore combining different semantic similarity measures (lin et al., 2015) for our task.",1,2019
P19-1378,"in future work, we will explore ensemble learning.",5,2019
P19-1379,"in addition to tracking the language evolvement in the history, we believe it is promising future work to use deep contextual embeddings in predicting the future change or trend, as well as detecting novel senses that are not included in existing dictionaries.",1,2019
P19-1381,"in future work, we would like to further our insights on the cnn aspects that are crucial for the task, our preliminary analyses of kernel width and attention.",1,2019
P19-1381,we leave a proper formulation of a tighter comparison to future work.,1,2019
P19-1384,"in future work, we will extend our set of languages, aiming at more typological variety (indoeuropean languages are greatly over-represented in our current data).",2,2019
P19-1385,"in the future, we aim to incorporate more elaborate linguistic resources (e.g.knowledge bases) and to investigate the performance of our methods on more complex nlp tasks, such as named entity recognition and sequence labelling, where prior knowledge integration is an active area of research.",2,2019
P19-1385,our approach can be applied to any rnn-based architecture as a extra module to further improve performance with minimal computational overhead.,4,2019
P19-1386,"in the future, we wish to study the use of knowref to improve performance on general coreference resolution tasks (e.g., the conll 2012 shared tasks).",4,2019
P19-1386,we also plan to develop new models on knowref and transfer them to difficult common sense reasoning tasks.,1,2019
P19-1388,"below, we discuss a few interesting issues brought up by the data collection process that should be addressed in future work.",5,2019
P19-1392,in further work we plan to carry out a multilingual alignment of the collocations in each language.,2,2019
P19-1394,"in the future, we aim to analyze how changes in the training procedure and hyperparameters of ulmfit affect resulting model performance.",1,2019
P19-1394,"on top of that, we hope to improve model generalization by augmenting negative examples with a split of jokes into setups and punchlines, as they should not be funny by themselves.",1,2019
P19-1394,we also plan to reproduce the experiment on english data.,2,2019
P19-1396,"for future study, we would like to investigate how to automatically learn the number of latent clusters with nonparametric bayesian methods.",1,2019
P19-1397,future work could potentially expand our work into an end-to-end invertible model that is able to produce high-quality representations by omnidirectional computations.,1,2019
P19-1399,"future works include applying the method to more language pairs and more domain-specific lexicon induction, e.g., terminologies.",2,2019
P19-1400,"besides, we will consider additional knowledge bases (e.g.yago and wikidata).",2,2019
P19-1400,"in future work we will aim to improve candidate selection (including different strategies to select candidate lists e+, e?).",1,2019
P19-1400,we will also use extra document information and jointly predict entities for different mentions in the document.,2,2019
P19-1408,a future direction is to investigate the effect of using mina spans not only in evaluation but also for training existing coreference resolvers.,3,2019
P19-1408,"investigating the use of mina in other nlp areas, e.g., evaluating spans in named entity recognition or reading comprehension, is another future line of work.",4,2019
P19-1409,"future directions include investigating ways to minimize the pipeline errors from the extraction of predicate-argument structures, and incorporating a mention prediction component, rather than relying on gold mentions.",1,2019
P19-1410,"also, in the future, we would like to investigate how our approach can be generalized to other discourse frameworks such as the penn discourse treebank (pdtb).",4,2019
P19-1411,"in the future work, we plan to extend the idea of multi-task learning/transfer learning with label embeddings to the problems in information extraction (e.g., event detection, relation extraction, entity mention detection) (nguyen and grishman, 2015a,b, 2016d; nguyen et al., 2016a,b,c; nguyen and nguyen, 2018b, 2019).",1,2019
P19-1412,"in the long run, to perform robust language understanding, models will need to incorporate more linguistic foreknowledge and be able to generalize to a wider range of linguistic constructions.",1,2019
P19-1413,"in the future we would like to expand this direction, and find ways to connect event and relation representation, learning and inference in a unified framework.",1,2019
P19-1414,"as a future work, we plan to introduce a wider range of background knowledge including another type of event causality (hashimoto et al., 2012, 2014, 2015; kruengkrai et al., 2017).",2,2019
P19-1415,"as for future work, we would like to enhance the ability to utilize antonyms for unanswerable question generation by leveraging external resources.",2,2019
P19-1416,"in this context, we suggest that future work can explore better retrieval methods for multi-hop questions.",1,2019
P19-1417,"in future work, we will extend the proposed idea to other qa tasks with evidence of multimodality, e.g.combining with symbolic approaches for visual qa (gan et al., 2017; mao et al., 2019; hu et al., 2019).",4,2019
P19-1421,"promising future directions would be to investigate how to utilize user interaction in moocs more adequately, as well as how attributes of course concepts can help expanding.",5,2019
P19-1423,"as future work, we plan to incorporate joint named entity recognition training as well as sub-word embeddings in order to further improve the performance of the proposed model.",1,2019
P19-1424,"finally, we plan to adapt bespoke models proposed for the chinese criminal court (luo et al., 2017; zhong et al., 2018; hu et al., 2018) to data from other courts and explore multitask learning.",1,2019
P19-1424,"providing valid justifications is an important priority for future work and an emerging topic in the nlp community.8 in this direction, we plan to expand the scope of this study by exploring the automated analysis of additional resources (e.g., relevant case law, dockets, prior judgments) that could be then utilized in a multi-input fashion to further improve performance and justify system decisions.",1,2019
P19-1424,"we also plan to apply neural methods to data from other courts, e.g., the european court of justice, the us supreme court, and multiple languages, to gain a broader perspective of their potential in legal justice prediction.",2,2019
P19-1425,"in future work, we plan to explore the direction to generate more natural adversarial examples dispensing with word replacements and more advanced defense approaches such as curriculum learning (jiang et al., 2018, 2015).",1,2019
P19-1428,"another issue to research will be the use of regression models to estimate the expected fitness of a pipeline given its features, as illustrated in section 6.",1,2019
P19-1428,"as future work, we plan to study the introduction of high-level knowledge to deal with the issue of invalid pipelines and improve the performance of the optimization process.",1,2019
P19-1428,"therefore, we plan to explore this line of research in the future, to compare our proposal with other automl frameworks in standard benchmarks.",3,2019
P19-1428,"this addition would support meta-learning algorithms, allowing to reduce the optimization time and increase its performance by learning from past executions.",1,2019
P19-1429,"for future work, we plan to investigate new auxiliary ?-learning algorithms using our ?-learning framework.",1,2019
P19-1430,"although we have used word, sense and character information in our work, more level of information can be incorporated into the mg lattice.",1,2019
P19-1430,"in the future, we will attempt to improve the ability of the mg lattice to utilize multi-grained information.",1,2019
P19-1431,"future research will look into applying such methods to reason jointly about text and kg, by attending to textual mentions of entities in addition to graph (verga et al., 2016).",4,2019
P19-1432,"consequently, in the future work, we plan to develop methods that can automatically induce the sentence structures for efp.",1,2019
P19-1436,future work includes better phrase representation learning to close its accuracy gap with qa models with query-dependent document encoding.,1,2019
P19-1436,"our phrase representations leverage sparse and dense vectors to capture lexical, semantic, and syntactic information.",1,2019
P19-1436,utilizing the phrase index as an external memory for an interaction with text-based knowledge is also an interesting direction.,1,2019
P19-1436,we believe that even further speedup and larger coverage of documents can be done with a similarity search package for dense+sparse vectors.,2,2019
P19-1437,"in this work, we aim to improve language modeling with shared grammar.",1,2019
P19-1438,promising future directions include experimenting with our zero-shot adaptation methods in the context of neural semantic parsing (after increasing the number of examples per domain) and extending the dataset to include more complicated applications and multi-utterance instructions.,2,2019
P19-1438,we hope this work will inspire readers to use our framework for collecting a larger dataset and experimenting with more approaches.,2,2019
P19-1441,"at last, we also would like to verify whether mt-dnn is resilience against adversarial attacks (glockner et al., 2018; talman and chatzikyriakidis, 2018; liu et al., 2019).",3,2019
P19-1441,"there are many future areas to explore to improve mt-dnn, including a deeper understanding of model structure sharing in mtl, a more effective training method that leverages relatedness among multiple tasks, for both fine-tuning and pre-training (dong et al., 2019), and ways of incorporating the linguistic structure of text in a more explicit and controllable manner.",1,2019
P19-1447,in the future we plan to apply the reranker to other parsers and more benchmark datasets.,2,2019
P19-1447,we will also attempt to jointly train the base semantic parser and the reranker by using the reranker’s output as supervision to fine tune the base parser.,1,2019
P19-1450,"in the future, we would like to extend our approach to sembanks which are annotated with different types of semantic representation, e.g.",2,2019
P19-1450,"sql (yu et al., 2018) or drt (abzianidze et al., 2017).",1,2019
P19-1450,we will explore latent-variable models to learn the dependency trees automatically.,1,2019
P19-1452,we employ the edge probing task suite to explore how the different layers of the bert network can resolve syntactic and semantic structure within a sentence.,1,2019
P19-1455,another direction could be to create fusion strategies that can better model incongruity among modalities to identify sarcasm.,1,2019
P19-1455,"future work could investigate advanced spatiotemporal fusion strategies (e.g., tensor-fusion (zadeh et al., 2017), cca (hotelling, 1936)) to better encode the correspondence between modalities.",1,2019
P19-1455,future work should try to leverage these factors to improve the baseline scores reported in this paper.,1,2019
P19-1455,"future work should try to overcome this issue with solutions involving pre-training, transfer learning, domain adaption, or low-parameter models.",1,2019
P19-1455,"moreover, while conducting this research, we identified several challenges that we believe are important to address in future research work on multimodal sarcasm detection.",5,2019
P19-1456,"besides, developing techniques to incorporate the claim stance and specificity detection models in argument generation to generate more coherent and consistent arguments is another interesting research direction to be explored.",1,2019
P19-1456,"for future work, it may be interesting to understand which other models would be effective in claim specificity and stance detection tasks.",1,2019
P19-1458,"in the future, we will investigate other nlp tasks such as named entity recognition (ner), question answering (qa) and aspect-based sentiment analysis (absa) (pontiki et al., 2016) to see whether results we saw in sentiment analysis is consistent across these tasks.",4,2019
P19-1458,we hope that our experimental results inspire future research dedicated to japanese.,6,2019
P19-1459,the adversarial dataset should be adopted as the standard in future work on arct.,2,2019
P19-1460,"in the future, this work can be progressed in several ways.",6,2019
P19-1462,"in future works, we will explore the extension of this approach for other tasks.",4,2019
P19-1463,"for future work, we plan to i) automatically predict relations between argument components in the uselecdeb60to16 dataset, and ii) propose a new task, i.e., fallacy detection so that common fallacies in political argumentation (zurloni and anolli, 2010) can be automatically identified, in line with the work of (habernal et al., 2018).",1,2019
P19-1464,"another direction is to explore span representations in several related tasks such as rst-style discourse parsing or new span-related argumentation mining tasks (trautmann et al., 2019).",4,2019
P19-1464,one interesting line of our future work is to investigate the performance of our model in an endto-end setting (including ac segmentation).,1,2019
P19-1466,"in the future, we intend to extend our method to better perform on hierarchical graphs and capture higher-order relations between entities (like motifs) in our graph attention model.",1,2019
P19-1466,"the proposed model can be extended to learn embeddings for various tasks using kgs such as dialogue generation (he et al., 2017; keizer et al., 2017), and question answering (zhang et al., 2016; diefenbach et al., 2018).",1,2019
P19-1467,"in future work, we intend to refine these alignment boundaries and to optimize the alignment procedure for speed.",1,2019
P19-1467,we hope that this work will raise more interest in developing alignment systems for longer paraphras.,1,2019
P19-1469,"for future work, we plan to focus on generating more realistic text and use the generated text in other tasks e.g.data augmentation, addressing adversarial attack.",2,2019
P19-1469,we expect the model could perform more natural generation via applying recent advancements on deep generative models.,1,2019
P19-1470,"these positive results point to future work in extending the approach to a variety of other types of knowledge bases, as well as investigating whether comet can learn to produce openie-style knowledge tuples for arbitrary knowledge seeds.",4,2019
P19-1473,an interesting direction would be to enable domain experts to identify and actively request for program annotations given the knowledge shared by other domains.,6,2019
P19-1473,this would further make it feasible to perform transfer learning on a new domain.,1,2019
P19-1473,we also plan to investigate the possibility of augmenting the parallel corpus by bootstrapping from shared templates across domains.,2,2019
P19-1473,we plan to explore if further fine-tuning using denotations based training on the distilled model can lead to improvements in the unified parser.,1,2019
P19-1473,we would also like to explore if guiding the decoder through syntactical and domain-specific constraints helps in reducing the search space for the weakly supervised unified parser.,1,2019
P19-1474,a prominent direction for future work is using the hyperbolic embeddings as the sole signal for taxonomy extraction.,1,2019
P19-1474,"since distributional and hyperbolic embeddings cover different relations between terms, it may be interesting to combine them.",1,2019
P19-1476,"we next plan to evaluate glen on multilingual and cross-lingual graded le datasets (vulic et al.′ , 2019) and release a large multilingual repository of le-specialized embeddings.",3,2019
P19-1477,"future work will entail adaption of the attentions, to further improve the performance.",1,2019
P19-1478,"in future work, other uses and the statistical significance of maskedwiki’s impact and its applications to different tasks will be investigated.",4,2019
P19-1479,"in the future, we would like to explore how to introduce external knowledge into the graph to make the generated comments more logical.",5,2019
P19-1480,there are several future directions for this setting.,6,2019
P19-1481,"in future work, we will explore the use of cross-lingual embeddings to further improve performance on this task.",1,2019
P19-1487,"with deferral of explanation to neural models, it will be crucial in the future to study the ethical implications of biases that are accumulated during pretraining or fine-tuning.",1,2019
P19-1489,"future work should investigate how to combine techniques that use both word meaning and nearest neighbors for a more robust, semisupervised cross-lingual evaluation.",3,2019
P19-1490,"in the future, we will work on cross-lingual extensions of monolingual hyperbolic embedding models (nickel and kiela, 2017; ganea et al., 2018).",1,2019
P19-1490,"we will also experiment with other sources of bilingual information (e.g., cross-lingual word embeddings) and port the transfer approach to more language pairs, with a particular focus on resource-poor languages.",2,2019
P19-1491,"our mixed-effects approach could be used to assess other nlp systems via parallel texts, separating out the influences on performance of language, sentence, model architecture, and training procedure.",3,2019
P19-1491,"we reevaluated the conclusions of cotterell et al.(2018) on a larger set of languages, requiring new methods to select fully parallel data (§4.2) or handle missing data.",2,2019
P19-1492,"in particular, we would like to explore new methods to jointly learn cross-lingual embeddings on monolingual corpora.",1,2019
P19-1493,it is our hope that these kinds of probing experiments will help steer researchers toward the most promising lines of inquiry by encouraging them to focus on the places where current contextualized word representation approaches fall short.,3,2019
P19-1494,"in addition to that, we would like to integrate our induced dictionaries in other downstream tasks like unsupervised cross-lingual information retrieval (litschko et al., 2018).",4,2019
P19-1494,"in the future, we would like to further improve our method by incorporating additional ideas from unsupervised machine translation such as joint refinement and neural hybridization (artetxe et al., 2019).",1,2019
P19-1495,a predictive model for identification of complaints is useful to companies that wish to automatically gather and analyze complaints about a particular event or product.,1,2019
P19-1495,"another research direction is to study the role of complaints in personal conversation or in the political domain, e.g., predicting political stance in elections (tsakalidis et al., 2018).",1,2019
P19-1495,"in the future, we plan to identify the target of the complaint in a similar way to aspect-based sentiment analysis (pontiki et al., 2016).",1,2019
P19-1495,we plan to use additional context and conversational structure to improve performance and identify the sociodemographic covariates of expressing and phrasing complaints.,1,2019
P19-1497,"finally, combining openqg with its reverse task, openqa, is also worth exploration.",1,2019
P19-1497,"first, we will explore more powerful qg structure to deal with the huge difference between the length of input and output texts.",1,2019
P19-1497,there are many future works to be done.,6,2019
P19-1499,"in the future, we plan to apply models to other tasks that also require hierarchical document encodings (e.g., document question answering).",4,2019
P19-1499,we are also interested in improving the architectures of hierarchical document encoders and designing other objectives to train hierarchical transformers.,1,2019
P19-1500,in the future we would like to apply our hierarchical transformer to question answering and related textual inference tasks.,4,2019
P19-1501,"finally, the distinct semantic representation of each word could further enhance the performance of the deep learning model.",1,2019
P19-1503,future work could be comparing language models of different types and scales in this direction.,3,2019
P19-1508,our work paves the way towards understanding the extent to which human meaning representation is impacted by negation.,1,2019
P19-1509,"a better understanding of what are the “innate” biases of standard models in highly controlled setups, such as the one studied here, should complement large-scale simulations, as part of the effort to develop new methods to encourage the emergence of more human-like language.",1,2019
P19-1509,how to incorporate “effort”-based pressures in neural networks is an exciting direction for future work.,1,2019
P19-1509,the research direction we introduced might lead to a better understanding of the biases that affect the linguistic behaviour of lstms and similar models.,1,2019
P19-1510,"we are optimistic that nne will encourage the development of new ner models that recognize structural information within entities, and therefore understand finegrained semantic information captured.",1,2019
P19-1511,"as the head-driven structures are widely spread in natural language, the solution proposed in this paper can also be used for modeling and exploiting this structure in many other nlp tasks, such as semantic role labeling and event extraction.",4,2019
P19-1512,"looking forward, our attention mechanism can also be applied to tasks such as relational networks (santoro et al., 2017), natural language inference (maccartney and manning, 2009), and qa systems (zhou et al., 2015).",4,2019
P19-1513,"also the work reported in this paper is based on a small tdm taxonomy, we plan to construct a tdm knowledge base and provide an applicable system for a wide range of nlp papers.",1,2019
P19-1513,we created two datasets in the nlp domain to test our system.,2,2019
P19-1516,"in future, we would like to assist the model with multiple linguistic aspects of social media text like figurative languages.",1,2019
P19-1517,"in the future, we would also like to investigate better models that are capable to address general arithmetic word problems, including addition, subtraction, multiplication and division.",1,2019
P19-1520,"for future work, we plan to apply the main idea of our approach to other tasks.",4,2019
P19-1522,"such features are ignored in our model, but they deserve more investigation for improving the extraction model.",1,2019
P19-1522,"therefore, for the future work, we will incorporate relation between events and relation between arguments into pre-trained language models, and take effective measures to overcome the deviation problem of roles in the generation.",1,2019
P19-1523,an error analysis is performed to shed light on possible future directions.,1,2019
P19-1524,"also, we would like to further explore the possibility to use domain-specific gazetteers or dictionaries to boost the performance of ner in various domains (shang et al., 2018), beyond the standard corpora.",1,2019
P19-1524,"future directions will include trying similarly enhanced modules on other different types of segmental models (kong et al., 2016; liu et al., 2016; zhuo et al., 2016; zhai et al., 2017; sato et al., 2017), along with richer representations for further gain.",1,2019
P19-1528,the problem of adapting this approach to higher order statistical language models (such as pcfgs) remains open.,5,2019
P19-1529,in future work we will explore how external information is best used in ensembles of models for srl and other tasks.,1,2019
P19-1529,we will also investigate whether the choice of method for injecting external information has the same impact on other nlp tasks as it does on srl.,5,2019
P19-1533,future work will consider problems where more challenging forms of neighbor manipulation are necessary for prediction.,5,2019
P19-1567,we hope that this work will raise more interest in developing alignment systems for longer paraphrase.,1,2019
P19-1568,another potential future work would be to explore other ways of providing rich supervision from textual descriptions as targets.,1,2019
P19-1569,in future work we plan to use multilingual resources (i.e.embeddings and glosses) for improving our sense embeddings and evaluating on multilingual wsd.,2,2019
P19-1569,"we expect our sense embeddings to be particularly useful in downstream tasks that may benefit from relational knowledge made accessible through linking words (or spans) to commonsense-level concepts in wordnet, such as natural language inference.",4,2019
P19-1570,a natural extension to this work would be to capture the sense distribution of sentences using the same framework.,1,2019
P19-1571,"in the future, we will explore the following directions: (1) context information is also essential to mwe representation learning, and we will try to combine both internal information and external context information to learn better mwe representations; (2) many mwes lack sememe annotation and we will seek to calculate an mwe’s scd when we only know the sememes of the mwe’s constituents; (3) our proposed models are also applicable to the mwes with more than two constituents and we will extend our models to longer mwes; (4) sememe is universal linguistic knowledge and we will explore to generalize our methods to other languages.",1,2019
P19-1572,"finally, we plan to further extend and use the humour dataset to investigate open questions on the linguistics of humour, such as what relationships hold between a pun’s phonology and its “successfulness” or humorousness (lagerquist, 1980; hempelmann and miller, 2017).",2,2019
P19-1572,"given that our model achieves good results with rudimentary, task-agnostic linguistic features, in future work we plan to investigate the use of humourand metaphor-specific features, including some of those used in past work (see §2) as well as those inspired by the prevailing linguistic theories of humour (attardo, 1994) and metaphor (black, 1955; lakoff and johnson, 1980).",1,2019
P19-1575,future work can improve this method further by examining the effects of different dimensionality reduction methods with varying properties on extracting the most informative pathways from the activations.,1,2019
P19-1576,"another exciting avenue would involve exploring cross-lingual transfer of lfs, taking advantage of recent development in unsupervised cross-lingual embedding learning (artetxe et al., 2017; conneau et al., 2017).",1,2019
P19-1576,"in the future, we would like to experiment with more data, so that enough training data can be obtained for less frequent lfs.",2,2019
P19-1576,"to this end, we could benefit from the supervised approach proposed in (rodr′?guez-fernandez et al.′ , 2016), and then filter by pairwise correlation strength metrics such as pmi.",1,2019
P19-1577,"in future, we plan to develop an automatic procedure of finding thesaurus regularities in dbag of problematic words, which can make more evident what kind of relations or senses are missed in the thesaurus.",1,2019
P19-1578,"for the future work, we hope to extend this idea proposed in this paper to train a model capable of handling different types of errors through the generative model since it can generate different lengths of results.",1,2019
P19-1579,"in future work, we will explore methods for controlling the induced dictionary quality to improve word substitution as well as m-umt.",1,2019
P19-1579,we will also attempt to create an end-toend framework by jointly training m-umt pivoting system and low-resource translation system in an iterative fashion in order to leverage more versions of augmented data.,1,2019
P19-1580,"important heads have one or more interpretable functions in the model, including attending to adjacent words and tracking specific syntactic relations.",1,2019
P19-1580,"in future work, we would like to investigate how our pruning method compares to alternative methods of model compression in nmt.",1,2019
P19-1584,future work: the automatic pseudonymization approach could serve as a data augmentation scheme to be used as a regularizer for deidentification models.,1,2019
P19-1584,private character embeddings that are learned from a perturbed source could be an interesting extension to our models.,1,2019
P19-1584,"when more training data from multiple sources become available in the future, it will be possible to evaluate our adversarially learned representation against unseen data.",3,2019
P19-1585,"despite being trained only for ner, the architecture provides intuitive embeddings for a variety of multi-word entities, a step which we suggest could prove useful for a variety of downstream tasks, including entity linking and coreference resolution.",4,2019
P19-1586,our frameworks of transfer and active learning for deep learning models are potentially applicable to low-resource settings beyond entity resolution.,4,2019
P19-1587,the proposed model offers promising future extensions in terms of directly optimizing other metrics such as recall and fβ.,1,2019
P19-1587,this work also opens up a range of questions from modeling to evaluation methodology,5,2019
P19-1588,"in the future, first, we try to utilize more eyetracking corpus and estimate more features of reading behavior.",2,2019
P19-1588,"then, we will attempt to analyze real human reading behavior on social media and thereby explore more specific human attention features on social media.",1,2019
P19-1589,"in future work, we want to extend this approach to other natural language processing tasks.",4,2019
P19-1591,"in future work we would like to develop more sophisticated trl algorithms, for both in-domain and domain adaptation nlp setups.",1,2019
P19-1591,"moreover, we would like to establish the theoretical groundings to the improved stability achieved by trl, and to explore this effect beyond domain adaptation.",1,2019
P19-1592,"additionally, we hope to apply stance to a wider-range of entity resolution tasks, for which string similarity is a component of model that considers additional features such as the natural language context of the entity mention.",4,2019
P19-1592,"in future work, we hope to further study the connections between our optimal transport-based alignment method and methods based on attention.",1,2019
P19-1593,"a key question for future work is the performance on longer texts, such as the full-length news articles encountered in ontonotes.",5,2019
P19-1593,"another direction is to further explore semi-supervised learning, by reducing the amount of training data and incorporating linguistically-motivated constraints based on morphosyntactic features.",1,2019
P19-1594,the ability of the spectral method for pnfa to estimate substring expectations can be exploited in other contexts.,4,2019
P19-1595,"achieving robust multi-task gains across many tasks has remained elusive in previous research, so we hope our work will make multi-task learning more broadly useful within nlp.",4,2019
P19-1596,"finally, we are interested in reproducing our corpus generation method on various other domains to allow for the creation of numerous useful datasets for the nlg community.",2,2019
P19-1596,"for future work, we plan on exploring other models for nlg, and on providing models with a more detailed input representation in order to help preserve more dependency information, as well as to encode more information on syntactic structures we want to realize in the output.",1,2019
P19-1596,"we are also interested in including richer, more semanticallygrounded information in our mrs, for example using abstract meaning representations (amrs) (dorr et al., 1998; banarescu et al., 2013; flanigan et al., 2014).",1,2019
P19-1597,and unsupervised approaches to leverage massive chess comments in social media is also worth exploring.,1,2019
P19-1597,another interesting direction is to extend our models to multimove commentary generation tasks.,4,2019
P19-1598,"our distantly supervised approach to dataset creation can be used with other knowledge graphs and other kinds of text as well, providing opportunities for accurate language modeling in new domains.",4,2019
P19-1598,this work lays the groundwork for future research into knowledge-aware language modeling.,1,2019
P19-1600,"in the future, we will explore the representation for the implicit information like whether a man is retired or not or how long a sportsman’s career is given starting and ending years, in the table by including some inference strategies.",1,2019
P19-1601,"in the future, we are planning to adapt our style transformer to the multiple-attribute setting like lample et al.(2019).",1,2019
P19-1603,"future work can combine the analyzer and generator via joint training, hopefully to achieve better results.",1,2019
P19-1604,"we are extending the proposed approach to other qa datasets, and adapting it to use pretrained language models such as bert (devlin et al., 2018), to evaluate the consistency of the mechanisms introduced.",3,2019
P19-1606,we plan on exploring backpropable variants as a scaffold for structure and also extend the techniques to other how-to domains in future.,1,2019
P19-1606,we plan to explore explicit evaluation of the latent structure learnt.,3,2019
P19-1608,further work on these inductive biases could help understand how a pretrained transfer learning model can be adapted in the most optimal fashion to a given target task.,1,2019
P19-1610,there are several possible future directions stemming from this work.,6,2019
P19-1610,"there is also scope for better techniques to generate more coherent question paraphrasing when significant question re-writing is required, such as for the situation when we want to paraphrase the question using context words.",1,2019
P19-1616,"similar problems may exist in other nlp tasks, which will be interesting to investigate in the future.",5,2019
P19-1617,"in the future, we may incorporate new advances in building entity graphs from texts, and solve more difficult reasoning problems, e.g.the cases of comparison query type in hotpotqa.",1,2019
P19-1618,"additionally, it would be interesting to study the behavior of nlprolog in the presence of multiple wikihop query predicates.",1,2019
P19-1618,"we are also interested in incorporating future improvements of symbolic provers, triple extraction systems and pretrained sentence representations to further enhance the performance of nlprolog.",1,2019
P19-1620,"we additionally proposed a possible direction for formal grounding of this method, which we hope to develop more thoroughly in future work.",1,2019
P19-1622,we will study further the capability of our approaches on other datasets and tasks in the future work.,2,2019
P19-1626,3 future work includes the fusion of additional operations in neural models.,1,2019
P19-1628,"in the future, we would like to investigate whether some of the ideas introduced in this paper can improve the performance of supervised systems as well as sentence selection in multi-document summarization.",1,2019
P19-1629,"in the future, we would like to more faithfully capture the semantics of documents by explicitly modeling entities and their linking.",1,2019
P19-1630,"an interesting challenge, and open research question, concerns the extent to which synthetic training impacts the overall model generalizability.",5,2019
P19-1630,"we believe that training models on heuristic, but inexpensive data sets is a valuable approach which opens up exciting opportunities for future research.",1,2019
P19-1631,"another avenue is to incorporate different model attribution strategies such as deeplrp (bach et al., 2015) into the objective function.",1,2019
P19-1631,there are several avenues we can explore as future research.,6,2019
P19-1631,we apply this technique to model fairness in toxic comment classification.,4,2019
P19-1633,"as future work, we will investigate approaches to hyperparameter tuning to find better model architectures for hierarchical multi-label text classification tasks.",1,2019
P19-1634,"in particular, we strongly suggest that future evaluations should adopt a stateless one-case-at-a-time test policy.",3,2019
P19-1635,"in future work, we plan to extend our work to further applications such as detecting exaggerated statements by investors in social media data.",4,2019
P19-1636,"finally, we plan to investigate generalized zero-shot learning (liu et al., 2018).",1,2019
P19-1636,"furthermore, experimenting with more datasets e.g., rcv1, amazon-13k, wiki-30k, mimic-iii will allow us to confirm our conclusions in different domains.",2,2019
P19-1636,we also plan to experiment with hierarchical flavors of bert to surpass its length limitations.,1,2019
P19-1636,we leave the investigation of methods for extremely large label sets for future work.,2,2019
P19-1636,"we plan to investigate more computationally efficient methods, e.g., dilated cnns (kalchbrenner et al., 2017) and transformers (vaswani et al., 2017; dai et al., 2019).",1,2019
P19-1637,"an important question for future models, especially interactive ones, is how to signal to the user when their desires do not comport with reality.",5,2019
P19-1638,"future work can investigate other embedding methods with a richer set of probe tasks, or explore a wider range of downstream tasks.",1,2019
P19-1639,"in the future, we hope to learn models that are able to sample search queries or information needs from sentences and use the output of that model to get relevant documents.",1,2019
P19-1640,a future direction is to improve the gan-based training of w-lda.,1,2019
P19-1640,another future direction is to experiment with more complex priors than the dirichlet prior.,1,2019
P19-1642,"in future work we will explore other generative models for multi-modal mt, as well as different ways to directly incorporate images into these models.",1,2019
P19-1643,"in future work, we plan to explore additional representations and architectures to improve the accuracy of our model, and to identify finer-grained alignments between visual actions and their verbal descriptions.",1,2019
P19-1647,we are also planning to investigate how sensitive our approach is to amount of data for the auxiliary task.,2,2019
P19-1652,"furthermore, it is also interesting to design a mutual reinforcement mechanisms between the vocabulary constructor and the text generator to improve both components simultaneously.",1,2019
P19-1652,"in future, we plan to study more effective ways to construct the image-grounded vocabulary.",1,2019
P19-1656,"we hope the emergence of mult could encourage further explorations on tasks where alignment used to be considered necessary, but where crossmodal attention might be an equally (if not more) competitive alternative.",5,2019
P19-1659,"in the future, we would like to extend this work to generate multidocument (multi-video) summaries and also build end-to-end models directly from audio in the video instead of text-based output from pretrained asr.",1,2019
P19-1660,"an orthogonal line of future work might involve using a visual question answering (vqa) task (such as in krishna et al.(2017)), either on its own replacing the captioning task, or in conjunction with the captioning task with a multi-task learning objective.",1,2019
P19-1660,there are several interesting avenues for future work.,6,2019
