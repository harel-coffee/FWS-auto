id,year,chapter,text
2020.acl-demos.1.txt,2020,7 Conclusion,another direction for improvement is to further enhance the ability to interact with users via a conversation interface.
2020.acl-demos.1.txt,2020,7 Conclusion,"one such important direction for future improvement is the expansion of areas that it can work in, which can be achieved through a promising approach of adopting model based technologies together with rule/template based ones."
2020.acl-demos.10.txt,2020,6 Conclusion,"in particular, we plan to incorporate human performance as a reference metric, integrating psycholinguistic experimental results and supporting easy experimental design starting from the test suite format."
2020.acl-demos.10.txt,2020,6 Conclusion,"syntaxgym is continually evolving: we plan to add new features to the site, and to develop further in response to user feedback."
2020.acl-demos.10.txt,2020,6 Conclusion,"we also plan to further incorporate language models into the lm-zoo tool, allowing broader access to state-of-the-art language models in general."
2020.acl-demos.14.txt,2020,5 Conclusion and Future Work,"for future work, we consider the following areas of improvement in the near term: • models downloadable in sta n z a are largely trained on a single dataset."
2020.acl-demos.14.txt,2020,5 Conclusion and Future Work,the amount of computation and resources available to us is limited.
2020.acl-demos.14.txt,2020,5 Conclusion and Future Work,"to make models robust to many different genres of text, we would like to investigate the possibility of pooling various sources of compatible data to train “default” models for each language"
2020.acl-demos.14.txt,2020,5 Conclusion and Future Work,"we would like to further investigate reducing model sizes and speeding up computation in the toolkit, while still maintaining the same level of accuracy.• we would also like to expand sta n z a ’s functionality by adding other processors such as neural coreference resolution or relation extraction for richer text analytics."
2020.acl-demos.16.txt,2020,4 Conclusion,"we will extend mt-dnn to support natural language generation tasks, e.g. question generation, and incorporate more pre-trained encoders, e.g. t5 (raffel et al., 2019) in future."
2020.acl-demos.17.txt,2020,6 Conclusion and Future Work,"an interesting direction to explore is re-ranking corrective suggestions, so that the suggestion more relevant to the original sentence goes to the top."
2020.acl-demos.17.txt,2020,6 Conclusion and Future Work,many avenues exist for future research and improvement of our system.
2020.acl-demos.17.txt,2020,6 Conclusion and Future Work,yet another direction of research would be to detect fine-grained error types.
2020.acl-demos.18.txt,2020,5 Conclusions,"our case study on the wmt2019 metrics shared task further highlights the potential of clir as a proxy task for mt evaluation, and we hope that clireval can facilitate future research in this area."
2020.acl-demos.19.txt,2020,5 Conclusion,we hope that convlab-2 is instrumental in promoting the research on task-oriented dialogue.
2020.acl-demos.2.txt,2020,6 Conclusion and Future Work,"for example, its usability in generation tasks such as machine translation has not been tested."
2020.acl-demos.2.txt,2020,6 Conclusion and Future Work,"in the future, we aim to integrate neural architecture search into the toolkit to automate the searching for model structures."
2020.acl-demos.2.txt,2020,6 Conclusion and Future Work,we will keep adding more examples and tests to expand textbrewer’s scope of application.
2020.acl-demos.20.txt,2020,4 Conclusions and Future Work,"additionally, we would like to explore opusfilter’s use in different scenarios and for other language pairs."
2020.acl-demos.20.txt,2020,4 Conclusions and Future Work,especially interesting would be the application in low-resource settings and various levels of noise in the original data.
2020.acl-demos.20.txt,2020,4 Conclusions and Future Work,"furthermore, the use for domain adaptation and data selection should be further explored."
2020.acl-demos.20.txt,2020,4 Conclusions and Future Work,"in future work, we would like to extend the toolbox with additional filters and classification options."
2020.acl-demos.24.txt,2020,6 Conclusion and Future Work,"we also plan to improve the performance of core models in photon, such as semantic parsing (text-to-sql), response generation (table-to-text) and context-aware user interaction (text-to-text)."
2020.acl-demos.24.txt,2020,6 Conclusion and Future Work,"we will continue to add more features to photon, such as voice input, spelling checking, and visualizing the output when appropriate to inspect the translation process."
2020.acl-demos.25.txt,2020,5 Discussion and Future Work,"a promising direction is to use gui references to help with repairing conversational breakdowns (beneteau et al., 2019; ashktorab et al., 2019; myers et al., 2018) caused by incorrect semantic parsing, intent classification, or entity recognition."
2020.acl-demos.25.txt,2020,5 Discussion and Future Work,"another promising approach to enable more robust natural language understanding is to leverage the pre-trained generalpurpose language models (e.g., bert (devlin et al., 2018)) to encode the user instructions and the information extracted from app guis.5.3 extracting task semantics from guis an interesting future direction is to better extract semantics from app guis so that the user can focus on high-level task specifications and personal preferences without dealing with low-level mundane details (e.g., “buy 2 burgers” means setting the value of the textbox below the text “quantity” and next to the text “burger” to be “2”)."
2020.acl-demos.25.txt,2020,5 Discussion and Future Work,"for itl, an interesting future challenge is to combine these user-independent domain-agnostic machine-learned models with the user’s personalized instructions for a specific task."
2020.acl-demos.25.txt,2020,5 Discussion and Future Work,we are currently exploring other ways of using multi-modal interactions to supplement natural language instructions in itl.
2020.acl-demos.26.txt,2020,7 Conclusion,"for future work, we plan to keep adding the state-of-the-art algorithms, reduce latency and fine-tune the implemented models on larger and/or more comprehensive corpus to improve performance."
2020.acl-demos.28.txt,2020,6 Conclusion and Future Work,"in future, we would like to extend the funlines data collection setup to a more general crowdsourcing framework, for example, to collect style transfer data."
2020.acl-demos.3.txt,2020,9 Conclusions,"we intend to release the code as open source, as well as providing hosted open access to a pubmed-based corpus."
2020.acl-demos.30.txt,2020,7 Conclusion,detection and control of toxic output will be a major focus of future investigation.
2020.acl-demos.30.txt,2020,7 Conclusion,"dialogpt is fully opensourced and easy to deploy, allowing users to extend the pre-trained conversational system to bootstrap training using various datasets."
2020.acl-demos.30.txt,2020,7 Conclusion,we will investigate leveraging reinforcement learning to further improve the relevance of the generated responses and prevent the model from generating egregious responses.
2020.acl-demos.32.txt,2020,5 Conclusion and Future Work,"in future work, we plan to add more media sources, especially from non-english media and regions."
2020.acl-demos.32.txt,2020,5 Conclusion and Future Work,"we further want to extend the tool to support other propaganda techniques such as cherrypicking and omission, among others, which would require analysis beyond the text of a single article."
2020.acl-demos.33.txt,2020,5 Conclusion,we will try to utilize external resources to solve the few-shot and zero-shot problem in the future.
2020.acl-demos.34.txt,2020,5 Conclusion,"in the future, we will support more corpora and implement novel techniques to bridge the gap between end-to-end and cascaded approaches."
2020.acl-demos.36.txt,2020,5 Conclusion,"in future work, we will focus on expanding the database to include additional domains and article sources."
2020.acl-demos.36.txt,2020,5 Conclusion,"we will also seek to improve discovery performance by testing more recent text embedding methods (e.g., bert (devlin et al., 2018)) and by optimizing the search for different input text lengths, such as a whole document, a paragraph, or even a single sentence."
2020.acl-demos.36.txt,2020,5 Conclusion,"we will investigate whether query expansion techniques (azad and deepak, 2019) could mitigate this issue by suggesting or automatically appending semantically related keywords to the boolean filters."
2020.acl-demos.36.txt,2020,5 Conclusion,"we will work on augmenting the workflow with automated tasks, such as suggesting references as the author writes a manuscript, or notifying users about the latest publications relevant to their work."
2020.acl-demos.37.txt,2020,4 Conclusion,"as a next step, we will improve the prototype based on the participants’ valuable feedback."
2020.acl-demos.37.txt,2020,4 Conclusion,"last, we will investigate whether using the different modalities has an impact on cognitive load during pe (herbig et al., 2019b)."
2020.acl-demos.38.txt,2020,7 Conclusion and Future Work,"finally, we hope to explore further optimizations to make core algorithms competitive with highly-optimized neural network components."
2020.acl-demos.38.txt,2020,7 Conclusion and Future Work,"in addition to the problems discussed so far, torch-struct also includes several other example implementations including supervised dependency parsing with bert, unsupervised tagging, structured attention, and connectionist temporal classification (ctc) for speech."
2020.acl-demos.38.txt,2020,7 Conclusion and Future Work,"in the future, we hope to support research and production applications employing structured models."
2020.acl-demos.39.txt,2020,5 Conclusion,"using the cl machine teaching ui, the dialog author can provide corrections to the logged user-system dialogs and further improve the cl’s dm performance."
2020.acl-demos.39.txt,2020,5 Conclusion,"we are planning to extend this work by looking into following problems: 1) investigating effectiveness of different ranking algorithms for log correction recommendation, 2) optimizing number of training samples and action masks generated from the rule-based dm, and 3) improving predictions of hcn-based dm by looking into alternative network architectures."
2020.acl-demos.4.txt,2020,4 Conclusion,"in addition to improving the banned words selection process, future work on tabouid includes generating specific lists of cards based on school programs to use the game as an educational tool, using the category system of wikipedia to let users select more or less specific categories to play with, and adapting the algorithms to leverage the wide variety of languages wikipedia is available in beyond english and french."
2020.acl-demos.40.txt,2020,6 Conclusion,"there are many open questions which we intend to research, such as whether autoregressivity in neural sentence compression can be exploited and how to compose themes over longer time periods."
2020.acl-demos.41.txt,2020,Conclusion,we hope to encourage additional research to improve the safety and benefits of dietary supplements for their consumers.
2020.acl-demos.42.txt,2020,7 Conclusion,better training methods also allow us to fight the potential generation of noisy data due to inaccurate annotation recommendations.
2020.acl-demos.42.txt,2020,7 Conclusion,we hope that our work on lean-life will allow for researches and practitioners alike to more easily obtain useful labeled datasets and models for the various nlp tasks they face.
2020.acl-demos.5.txt,2020,6 Conclusion,future work include (1) expanding the database to more papers (2) improving the qa model using the collected data to better handle question answering in the context of research domain.
2020.acl-demos.9.txt,2020,5 Conclusions,"moving forward, we hope to refine the linking of extracted snippets to structured vocabularies to run a more comprehensive user-study to evaluate the use of the system in practice by different types of users."
2020.acl-main.1.txt,2020,7 Discussion,"in future work, we intend to curate a test set with data from separate sources, which can serve as a benchmark for the models we study."
2020.acl-main.1.txt,2020,7 Discussion,"the combined test set scores are more directly comparable, but ideally, we would like to compare the generalization of both models on an independent test set."
2020.acl-main.1.txt,2020,7 Discussion,we also intend to use tools for interpreting the knowledge encoded in neural networks (such as diagnostic classifiers and representational similarity analysis) to investigate the emergent representation of linguistic units such as phonemes and words.
2020.acl-main.1.txt,2020,7 Discussion,we intend to explore how a curriculum of cds followed by ads affects learning trajectories and outcomes.
2020.acl-main.100.txt,2020,5 Conclusion,"in the future, we are interested in injecting knowledge into text representation learning (cao et al., 2017, 2018b) for deeply understanding expert language, and will help to generate knowledgeenhanced questions (pan et al., 2019) for laymen."
2020.acl-main.102.txt,2020,5 Conclusion,"since dynamic memory can be a learning mechanism more general than what we have used here for fewshot learning, we will investigate this type of models in other learning problems."
2020.acl-main.103.txt,2020,7 Conclusion and Future Work,"besides, we also propose a soft and a hard exclusion mechanisms to enhance the diversity of the generated keyphrases."
2020.acl-main.103.txt,2020,7 Conclusion and Future Work,one interesting future direction is to explore whether the beam search is helpful to our model.
2020.acl-main.105.txt,2020,4 Conclusion,other retrieval tasks may also benefit from using keyphrase information and we expect our results to serve as a basis for further improvements.
2020.acl-main.11.txt,2020,5 Conclusion and Future Work,"in future work, we plan to experiment with multi-domain span extraction architectures."
2020.acl-main.110.txt,2020,9 Conclusions,"in this scenario, automation strategies, such as natural language generation, are necessary to help ngo operators in their countering effort."
2020.acl-main.111.txt,2020,6 Conclusion,"its goal is to drive the development of better nlu models, so careful selection of tasks was crucial."
2020.acl-main.111.txt,2020,6 Conclusion,we leave it as future work.
2020.acl-main.111.txt,2020,6 Conclusion,we plan to continue the work on herbert and use the klej benchmark to guide its development.
2020.acl-main.112.txt,2020,7 Conclusion,"a promising direction would be to combine a multilingual sense inventory such as babelnet (navigli and ponzetto, 2012) with sense embeddings (camacho-collados and pilehvar, 2018)."
2020.acl-main.112.txt,2020,7 Conclusion,future work will have to deepen the way we deal with word sense ambiguity by way of exchanging the simplifying type-level approach our current work is based on with a semantically more informed sense-level approach.
2020.acl-main.113.txt,2020,7 Conclusions,"finally, we would like to test this approach for comparing different mt systems."
2020.acl-main.113.txt,2020,7 Conclusions,"first, we plan to test whether similar observations will hold for more language pairs and text domains."
2020.acl-main.113.txt,2020,7 Conclusions,"second, the score combination strategies could be improved by learning weights for each component."
2020.acl-main.113.txt,2020,7 Conclusions,this work can be extended in numerous ways.
2020.acl-main.116.txt,2020,8 Conclusion,a natural next step is to combine the datasets in a multi-task setting to investigate to what extent models can profit from combining the information annotated in the respective datasets.
2020.acl-main.116.txt,2020,8 Conclusion,"further research will investigate the joint modeling of entity extraction, typing and experiment frame recognition."
2020.acl-main.116.txt,2020,8 Conclusion,"in addition, there are also further natural language processing tasks that can be researched using our dataset."
2020.acl-main.117.txt,2020,7 Discussion and Future Work,we consider techqa to be a stepping stone on which to build future data collections and leaderboards.
2020.acl-main.117.txt,2020,7 Discussion and Future Work,"we envision a roadmap where future releases of techqa will require synergy between multiple ai disciplines, from deep-learning based mrc to reasoning, knowledge base acquisition, and causality detection."
2020.acl-main.117.txt,2020,7 Discussion and Future Work,we plan on releasing questions with answers in a broader and more diverse collection that will include documents with a less formulaic structure than the technotes.
2020.acl-main.118.txt,2020,7 Conclusion and Future Work,we believe this dataset will allow future work in sarcasm detection to progress in a setting free of the noise found in existing datasets.
2020.acl-main.119.txt,2020,7 Conclusion,an interesting future work is to make the number of inference steps adaptive to input sentences.
2020.acl-main.120.txt,2020,6 Conclusion,important challenges for future work include how to scale deep learning methods to such large amounts of source documents and how to close the gap to the oracle methods.
2020.acl-main.120.txt,2020,6 Conclusion,"we conducted extensive experiments to establish baseline results, and we hope that future work on mds will use this dataset as a benchmark."
2020.acl-main.120.txt,2020,6 Conclusion,we hope this dataset will facilitate the creation of real-world mds systems for use cases such as summarizing news clusters or search results.
2020.acl-main.121.txt,2020,6 Conclusion and Future Work,"besides, we will also explore the influence of probabilistic bilingual lexicon obtained by learning only from monolingual data on our method."
2020.acl-main.121.txt,2020,6 Conclusion and Future Work,"in our future work, we consider incorporating our method into the multi-task method."
2020.acl-main.122.txt,2020,7 Conclusion,"potential future directions include a more principled use of our proposed heuristic for detecting content relevant to specific dates, the use of abstractive techniques, a more effective treatment of the redundancy challenge, and extending the new dataset with multiple sources."
2020.acl-main.123.txt,2020,7 Conclusion and future work,"in the future, we explore a more sophisticated method to improve the relevance and truthfulness of generated headlines, for example, removing only deviated spans in untruthful headlines rather than removing untruthful headlines entirely from the supervision data."
2020.acl-main.123.txt,2020,7 Conclusion and future work,"moreover, it will be also interesting to see whether the same issue occurs in other related tasks such as data-to-text generation."
2020.acl-main.125.txt,2020,6 Conclusion,"for future work, we intend to apply our method to other transformer-based summarization models."
2020.acl-main.127.txt,2020,5 Conclusions,"in the future, we will adapt the method to more neural models especially the generation-based methods for the dialog system."
2020.acl-main.129.txt,2020,7 Conclusions and Future Work,"in future, we want to continue to investigate the possibility of using even weaker demonstrations."
2020.acl-main.13.txt,2020,4 Conclusion,we will explore cross-lingual transfer learning for supporting more languages.
2020.acl-main.130.txt,2020,5 Conclusion,we hope that this dataset facilitates future research on multi-turn conversation reasoning problem.
2020.acl-main.131.txt,2020,7 Conclusion & Future Work,"for future work, we would like to extend receiver to conversational recommender systems."
2020.acl-main.133.txt,2020,5 Conclusions,"for future work, we would like to deeply study the impacts of our perturbations on the coherence of the examined dialogues."
2020.acl-main.133.txt,2020,5 Conclusions,we will also investigate to what extent the rankings of dialogues obtained by our model correlate with human-provided rankings.
2020.acl-main.135.txt,2020,5 Conclusion and Future Works,"although dp-based and srl-based semantic parsing are widely used, more advanced semantic representations could also be explored, such as discourse structure representation (van noord et al., 2018; liu et al., 2019b) and knowledge graph-enhanced text representations (cao et al., 2017; yang et al., 2019)."
2020.acl-main.135.txt,2020,5 Conclusion and Future Works,there are at least two potential future directions.
2020.acl-main.138.txt,2020,6 Conclusions,future work will involve improvements in the proposed noise model to study the importance of fidelity to real-world error patterns.
2020.acl-main.138.txt,2020,6 Conclusions,"moreover, we plan to evaluate nat on other real noise distributions (e.g., from asr) and other sequence labeling tasks to support our claims further."
2020.acl-main.139.txt,2020,5 Conclusion,"future work will investigate how to take into account potential correlations between labelling functions in the aggregation model, as done in e.g.(bach et al., 2017)."
2020.acl-main.139.txt,2020,5 Conclusion,we also wish to evaluate the approach on other types of sequence labelling tasks beyond named entity recognition.
2020.acl-main.140.txt,2020,6 Conclusion,"in future work, we want to extend the probing tasks to also cover specific linguistic patterns such as appositions, and also investigate a model’s ability of generalizing to specific entity types, e.g.company and person names."
2020.acl-main.141.txt,2020,5 Conclusion,there are multiple avenues for future work.
2020.acl-main.142.txt,2020,7 Conclusion and Future Work,"to improve the evaluation accuracy and reliability of future re methods, we provide a revised, extensively relabeled tacred."
2020.acl-main.144.txt,2020,6 Conclusions and Further Work,"as regards distributed representations, we plan to study alternative networks to more accurately model the identification and incorporation of additional context."
2020.acl-main.144.txt,2020,6 Conclusions and Further Work,"in our future work, we plan to optimise the thresholds used with the retrieval algorithms in order to more intelligently select those translations providing richest information to the nmt model and generalize the use of edit distance on the target side."
2020.acl-main.144.txt,2020,6 Conclusions and Further Work,"we would also like to explore better techniques to inject information of small-size n-grams with possible convergence with terminology injection techniques, unifying framework where target clues are mixed with source sentence during translation."
2020.acl-main.145.txt,2020,6 Conclusion,"in future work, we will extend our analysis to include additional source and target languages from different language families, such as more asian languages."
2020.acl-main.145.txt,2020,6 Conclusion,"we will also work towards improving the training efficiency of character-level models, which is one of their main bottlenecks, as well as towards improving their effectiveness in multilingual training."
2020.acl-main.148.txt,2020,7 Conclusion and Future Work,"in the future, we will develop lightweight alternatives to lalt to reduce the number of model parameters."
2020.acl-main.148.txt,2020,7 Conclusion and Future Work,"we release opus-100, a multilingual dataset from opus including 100 languages with around 55m sentence pairs for future study."
2020.acl-main.149.txt,2020,6 Conclusion,"in future work, we plan to extend this analysis across more translation pairs, more diverse languages and multiple domains, as well as investigating the effect of translationese or source-side grammatical errors (anastasopoulos, 2019)."
2020.acl-main.151.txt,2020,6 Conclusion,"first, they portray the viability of referencefree mt evaluation and warrant wider research efforts in this direction."
2020.acl-main.152.txt,2020,6 Conclusion and Future Work,more research is needed on this problem given the prevalent usage of nmt.
2020.acl-main.153.txt,2020,6 Conclusions and Future Work,"in the future, we plan to extend the cross-lingual position encoding to non-autoregressive mt (gu et al., 2018) and unsupervised nmt (lample et al., 2018)."
2020.acl-main.155.txt,2020,6 Conclusion,"as a next step, we will integrate the participants’ valuable feedback to improve the prototype."
2020.acl-main.155.txt,2020,6 Conclusion,"while the presented study provided interesting first insights regarding participants’ use of and preferences for the implemented modalities, it did not allow us to see how they would use the modalities over a longer time period in day-to-day work, which we also want to investigate in the future."
2020.acl-main.157.txt,2020,4 Conclusions,"also, the multi-domain nature of the dataset enables future research in cross-target and cross-domain adaptation, a clear weak point of current models according to our evaluations."
2020.acl-main.157.txt,2020,4 Conclusions,"future research directions might explore the usage of transformer-based models, as well as of models which exploit not only linguistic but also network features, which have been proven to work well for existing stance detection datasets (aldayel and magdy, 2019)."
2020.acl-main.158.txt,2020,5 Discussion,this work addresses multiple open questions about syntactic evaluations and their relationship to other language model assessments.
2020.acl-main.160.txt,2020,5 Discussion,one possibility would be to include infelicitous “colorless green ideas” sentences with grammatical syntax (cf.
2020.acl-main.161.txt,2020,7 Conclusions,a more sophisticated model would incorporate similar ideas.
2020.acl-main.161.txt,2020,7 Conclusions,"additional annotations, such as how certain readers are about the outcome of the story, may also be helpful in better understanding the relationship between suspense and uncertainty."
2020.acl-main.161.txt,2020,7 Conclusions,"automated interpretability methods as proposed by sundararajan et al.(2017), could shed further light on models’ predictions."
2020.acl-main.161.txt,2020,7 Conclusions,it provides a springboard to further interesting applications and research on suspense in storytelling.
2020.acl-main.162.txt,2020,6 Conclusion,we hope this resource can benefit future research into developing techniques to model and understand human responses to document sized text.
2020.acl-main.163.txt,2020,6 Conclusion,"in future, we aim to refine our generative model to better emphasise this difference of the two tasks."
2020.acl-main.164.txt,2020,8 Conclusion,we therefore suggest three prongs for future research: 1.
2020.acl-main.165.txt,2020,6 Conclusions,"moreover, we remark that our approach can be extended to other multi-domain or multi-task nlp problems."
2020.acl-main.166.txt,2020,5 Conclusion,"in the future, we will investigate how to extend the cg to support hierarchical topic management in conversational systems."
2020.acl-main.167.txt,2020,5 Conclusions,future work will focus on incorporating better encoding of the amr graph into the current system and exploring data augmentation techniques leveraging the proposed approach.
2020.acl-main.170.txt,2020,8 Conclusions,future research directions include adaptive dropout rates for different merges and an in-depth analysis of other pathologies in learned token embeddings for different segmentations.
2020.acl-main.171.txt,2020,6 Discussion,"other work in nmt has examined this issue in the context of backtranslation (e.g., edunov et al.(2018)), and we expect the conclusions to be similar in the nar-mt case."
2020.acl-main.171.txt,2020,6 Discussion,there are several open questions to investigate: are the benefits of monolingual data orthogonal to other techniques like iterative refinement?
2020.acl-main.171.txt,2020,6 Discussion,we will consider these research directions in future work.
2020.acl-main.174.txt,2020,6 Conclusions,"although currently our approach relies solely on textual information, it would be interesting to incorporate additional modalities such as video or audio."
2020.acl-main.174.txt,2020,6 Conclusions,"besides narrative structure, we would also like to examine the role of emotional arcs (vonnegut, 1981; reagan et al., 2016) in a screenplay."
2020.acl-main.175.txt,2020,6 Conclusions,"in the future, we would like to model aspects and sentiment more explicitly as well as apply some of the techniques presented here to unsupervised single-document summarization."
2020.acl-main.175.txt,2020,6 Conclusions,our key insight is to enable the use of supervised techniques by creating synthetic review-summary pairs using noise generation methods.
2020.acl-main.177.txt,2020,9 Discussion and Conclusion,our analyses contain two ideas that may be useful for future studies of systematicity.
2020.acl-main.179.txt,2020,8 Discussion,future work needs to be done to understand more fully what biases are present in the data and learned by language models.
2020.acl-main.180.txt,2020,5 Discussion,the study suggests several directions for future work.
2020.acl-main.181.txt,2020,6 Conclusion,it is an open question how to implement ig for these postor mixed-placement adjectives; one possibility is to measure the information gained when the set of adjectives associated to a noun an is partitioned by an adjective a.
2020.acl-main.182.txt,2020,7 Conclusions,we hope the dataset we release will be used to benchmark future dialog system uncertainty research.
2020.acl-main.183.txt,2020,5 Discussion and Conclusion,"one natural extension would be to generalize these findings to other skills than the three addressed here, such as humor/wit, eloquence, image commenting, etc."
2020.acl-main.184.txt,2020,6 Conclusion and Future Work,"in future, we plan to explore how to combine knowledge with pre-trained language models, e.g."
2020.acl-main.186.txt,2020,5 Conclusion and Future Work:,"as future work, we are extending the proposed approach and test its efficacy on real human conversations."
2020.acl-main.186.txt,2020,5 Conclusion and Future Work:,"more broadly, we continue to explore strategies that combine semantic parsing and neural networks for frame generation."
2020.acl-main.187.txt,2020,7 Conclusions and Future Work,"future work can explore improving the correction models, leveraging logs of natural language feedback to improve text-to-sql parsers, and expanding the dataset to include multiple turns of correction."
2020.acl-main.190.txt,2020,6 Discussion,extending expbert to other natural language tasks where this relationship might not hold is an open problem that would entail finding different ways of interpreting an explanation with respect to the input.
2020.acl-main.190.txt,2020,6 Discussion,"however, more work will need to be done to make this approach more broadly applicable."
2020.acl-main.190.txt,2020,6 Discussion,recent progress in general-purpose language representation models like bert open up new opportunities to incorporate language into learning.
2020.acl-main.190.txt,2020,6 Discussion,we outline two such avenues of future work.
2020.acl-main.191.txt,2020,5 Conclusion,"moreover, we will investigate the potential impact of the adversarial training directly in the bert pre-training."
2020.acl-main.191.txt,2020,5 Conclusion,"this first investigation paves the way to several extensions including adopting other architectures, such as gpt-2 (radford et al., 2019) or distilbert (sanh et al., 2019) or other tasks, e.g., sequence labeling or question answering."
2020.acl-main.192.txt,2020,6 Conclusion,future directions include (1) devising hierarchical span representations that can handle spans of different length and diverse content more effectively and efficiently; (2) robust multitask learning or meta-learning algorithms that can reconcile very different tasks.
2020.acl-main.193.txt,2020,6 Conclusion,the proposed learning framework also shows promising results on other nlp tasks like text classification.
2020.acl-main.194.txt,2020,6 Conclusion,"for future direction, we plan to explore the effectiveness of mixtext in other nlp tasks such as sequential labeling tasks and other real-world scenarios with limited labeled data."
2020.acl-main.195.txt,2020,5 Conclusion,we believe our findings are generic and can be applied to other model compression problems.
2020.acl-main.196.txt,2020,5 Conclusion,"thus, we encourage future research into obtaining tighter bounds on latent lm perplexity, possibly by using more powerful proposal distributions that consider entire documents as context, or by considering methods such as annealed importance sampling."
2020.acl-main.196.txt,2020,5 Conclusion,we investigate the application of importance sampling to evaluating latent language models.
2020.acl-main.197.txt,2020,6 Conclusion,our proposed fine-tuning framework can be generalized to solve other transfer learning problems.
2020.acl-main.197.txt,2020,6 Conclusion,we will explore this direction as future work.
2020.acl-main.199.txt,2020,5 Conclusion,"in the future, we would like to figure out different strategies to merge individual gains, obtained by separate application of the dag constraint, into a setup that can take the best of both precision and recall improvements, and put forth a better performing system."
2020.acl-main.199.txt,2020,5 Conclusion,we also plan on looking into strategies to improve recall of the constructed taxonomy.
2020.acl-main.2.txt,2020,6 Conclusion,future work can further investigate temporal patterns in how language used by depressed people evolves over the course of an interaction.
2020.acl-main.2.txt,2020,6 Conclusion,we hope that this combination will encourage the research community to make more progress in this direction.
2020.acl-main.20.txt,2020,5 Conclusion,"as future work, we plan to extend our qag model to a meta-learning framework, for generalization over diverse datasets."
2020.acl-main.200.txt,2020,6 Conclusion,"for industrial applications where there is a trade-off typically between accuracy and latency, our findings suggest it might be feasible to gain accuracy for faster models by collecting more training examples."
2020.acl-main.201.txt,2020,6 Conclusion and Discussion,"there are other ways to fit the dictionary better; e.g., using a non-linear projection such as a neural network."
2020.acl-main.201.txt,2020,6 Conclusion and Discussion,"therefore, future work should focus on downstream tasks instead of bli."
2020.acl-main.201.txt,2020,6 Conclusion and Discussion,we leave the exploration of non-linear projections to future work.
2020.acl-main.202.txt,2020,8 Conclusions,"we perform extensive study of several distillation dimensions like the impact of unlabeled transfer set, embeddings and student architectures, and make interesting observations outlined in summary."
2020.acl-main.203.txt,2020,6 Conclusion,our findings point to future research opportunities to build stealthy authorship obfuscation methods.
2020.acl-main.204.txt,2020,5 Conclusions and Future Work,"there are a few interesting questions left unanswered in this paper, which would provide interesting future research directions: (1) deebert’s training method, while maintaining good quality in the last off-ramp, reduces model capacity available for intermediate off-ramps; it would be important to look for a method that achieves a better balance between all off-ramps.(2) the reasons why some transformer layers appear redundant2 and why dee-bert considers some samples easier than others remain unknown; it would be interesting to further explore relationships between pre-training and layer redundancy, sample complexity and exit layer, and related characteristics."
2020.acl-main.205.txt,2020,6 Conclusion,"finally, we also open a path to study integration of knowledge into the decoding phase, which can benefit other tasks such as neural machine translation."
2020.acl-main.206.txt,2020,6 Conclusion,"therefore future work could include different sampling methods, generation of synthetic data, or training objectives which reward models which are less conservatively drawn to the middle of the scoring scale."
2020.acl-main.207.txt,2020,8 Conclusions and Future Work,another item of future work is to develop better multitask approaches to leverage multiple signals of relatedness information during training.
2020.acl-main.207.txt,2020,8 Conclusions and Future Work,including other information such as outgoing citations as additional input to the model would be yet another area to explore in future.
2020.acl-main.207.txt,2020,8 Conclusions and Future Work,it would be interesting to initialize our model weights from more recent transformer models to investigate if additional gains are possible.
2020.acl-main.208.txt,2020,7.3 Code Piece Error Analysis,"to help the readers understand the bottleneck for code piece generation and point out important future directions, we randomly sampled 200 “hard” lines and manually analyzed why the generation fails by looking at the top 1 candidate of the model."
2020.acl-main.21.txt,2020,7 Conclusion,"besides, more powerful question clustering and coarse-to-fine generation scenarios are also worth exploration."
2020.acl-main.21.txt,2020,7 Conclusion,"finally, performing sqg on other types of inputs, e.g., images and knowledge graphs, is an interesting topic."
2020.acl-main.21.txt,2020,7 Conclusion,"for future works, the major challenge is generating more meaningful, informative but concise questions."
2020.acl-main.211.txt,2020,6 Discussion and Future Work,"however, a host of other options could be considered in future work."
2020.acl-main.212.txt,2020,6 Discussion,"an alternative would be to create training sets that adequately represent a diverse range of linguistic phenomena; crowdworkers’ (rational) preferences for using the simplest generation strategies possible could be counteracted by approaches such as adversarial filtering (nie et al., 2019)."
2020.acl-main.216.txt,2020,5 Conclusions,"finally, we will evaluate the multiresolution loss on larger datasets to analyze it’s regularizing effects."
2020.acl-main.216.txt,2020,5 Conclusions,"furthermore, we will experiment with more ellaborate, attention-based fusion mechanisms."
2020.acl-main.216.txt,2020,5 Conclusions,"in the future, we plan to alleviate this by incorporating ideas from sparse transformer variants (kitaev et al., 2020; child et al., 2019)."
2020.acl-main.218.txt,2020,7 Conclusion,we plan to continue our data-driven approach for grounded conversations by expanding our dataset through our iterative data collection process with other larger text-based open-domain dialogue corpora and extend our work to model and collect longer conversations exhibiting more complex improv-backed turns.
2020.acl-main.219.txt,2020,6 Conclusion,"the next challenge will also be to combine this engagingness with other skills, such as world knowledge (antol et al., 2015) relation to personal interests (zhang et al., 2018), and task proficiency."
2020.acl-main.219.txt,2020,6 Conclusion,"while our human evaluations were on short conversations, initial investigations indicate the model as is can extend to longer chats, see appendix g, which should be studied in future work."
2020.acl-main.220.txt,2020,5 Conclusion,"since it provides immediate continuous rewards and at the singlestep level, maude can be also be used to optimize and train better dialogue generation models, which we want to pursue as future work."
2020.acl-main.221.txt,2020,5 Conclusion,"with the advent of commercial sds systems that attempt to engage users over extended multi-turn interactions (e.g.(zhou et al., 2018)) generating realistic response behaviors is a potentially desirable addition to the overall experience."
2020.acl-main.222.txt,2020,6 Discussion,"for example, longer conversations involving memory (moon et al., 2019), or mixing open-domain conversation with task oriented goals."
2020.acl-main.222.txt,2020,6 Discussion,"future work should consider adding these tasks to the ones used here, while continuing the quest for improved models."
2020.acl-main.223.txt,2020,5 Conclusion,"first of all, we would like to experiment with different neural network architectures."
2020.acl-main.223.txt,2020,5 Conclusion,"secondly, we would like to incorporate further poetic devices, especially those based on meaning."
2020.acl-main.223.txt,2020,5 Conclusion,we conclude with a number of future research avenues.
2020.acl-main.224.txt,2020,6 Conclusion,future work will validate the effectiveness of this method on more varied data-to-text generation tasks.
2020.acl-main.225.txt,2020,8 Conclusion,"in future work, we plan to incorporate these features into co-creation systems which assist humans in the writing process."
2020.acl-main.226.txt,2020,5 Conclusions and Outlook,"one can try to use a variational autoencoder (kingma and welling, 2014) instead."
2020.acl-main.226.txt,2020,5 Conclusions and Outlook,"since sentence infilling is analogous to masked language modeling, we expect that it can also be used as a pre-training task."
2020.acl-main.228.txt,2020,6 Conclusion and Future Work,"by performing analysis on gigaword, we find that there exists room to improve summarization performance with better post-ranking algorithms, a promising direction for future research."
2020.acl-main.228.txt,2020,6 Conclusion and Future Work,"moving forward, we would like to apply this framework to other retrieve-and-edit based generation scenarios such as dialogue, conversation, and code generation."
2020.acl-main.229.txt,2020,6 Discussion,"for instance, developing agents that are robust to variations in both visual appearance and instruction descriptions is an important next step."
2020.acl-main.229.txt,2020,6 Discussion,there are a few future directions to pursue.
2020.acl-main.230.txt,2020,6 Conclusions and Future Work,"in the future, we aim to extend our framework to extract events from videos, and make it scalable to new event types."
2020.acl-main.230.txt,2020,6 Conclusions and Future Work,we also develop a novel multimedia structured common space construction method to take advantage of the existing image-caption pairs and singlemodal annotated data for weakly supervised training.
2020.acl-main.230.txt,2020,6 Conclusions and Future Work,"we plan to expand our annotations by including event types from other text event ontologies, as well as new event types not in existing text ontologies."
2020.acl-main.230.txt,2020,6 Conclusions and Future Work,"we will also apply our extraction results to downstream applications including cross-media event inference, timeline generation, etc."
2020.acl-main.231.txt,2020,8 Discussion,"future work might explore methods for incorporating richer learned representations both of the diverse visual observations in videos, and the narration that describes them, into such models."
2020.acl-main.231.txt,2020,8 Discussion,we hope that future work will continue to evaluate broadly.
2020.acl-main.234.txt,2020,6 Conclusion and Related Work,"our empirical analysis is related to htut et al.(2018), who methodologically, and successfully replicate the results of shen et al.(2018a) to study their performance."
2020.acl-main.234.txt,2020,6 Conclusion and Related Work,"studying this type of difference between expressive models and their less expressive, restricted variants remains an important direction for future work."
2020.acl-main.235.txt,2020,6 Conclusions and Future Work,further study in this direction may be interesting.
2020.acl-main.236.txt,2020,5 Conclusion,"we hope this work inspires future research on better understanding the differences between embedding methods, and on designing simpler and more efficient models."
2020.acl-main.237.txt,2020,8 Conclusion,a potential future research direction is to bridge the gap between this simple bootstrapping paradigm and the incorporation of user free-form responses to allow the system to handle free-text responses.
2020.acl-main.237.txt,2020,8 Conclusion,our modeling choices enable the system to perform zero-shot generalization to unseen classification targets and questions.
2020.acl-main.237.txt,2020,8 Conclusion,"we hope our work will encourage more research on different possibilities of building interactive systems that do not necessarily require handling full-fledged dialogue, but still benefit from user interaction."
2020.acl-main.238.txt,2020,6 Conclusion,"in the future, we would like to jointly learn discrete representations of entities as well as relations."
2020.acl-main.238.txt,2020,6 Conclusion,our approaches learn to represent entities in a kg as a vector of discrete codes in an end-to-end fashion.
2020.acl-main.240.txt,2020,6 Conclusion,"future work could find additional modular uses of mlms, simplify maskless pll computations, and use plls to devise better sentence- or document-level scoring metrics."
2020.acl-main.242.txt,2020,5 Conclusion and Future Directions,"the theoretical underpinnings of our poscal idea are not explored in detail here, but developing formal statistical support for these ideas constitutes interesting future work."
2020.acl-main.243.txt,2020,10 Conclusion,another direction is to improve the sources of weak supervision and such as interactive new constraints provided by users.
2020.acl-main.243.txt,2020,10 Conclusion,"finally, it would be interesting to explore alternative training methods for these models, such as reducing reliance on hard sampling through better relaxations of structured models."
2020.acl-main.243.txt,2020,10 Conclusion,induction of grounded control states opens up many possible future directions for this work.
2020.acl-main.245.txt,2020,6 Discussion,"edizel et al.(2019) attempt to learn typo-resistant word embeddings, but focus on common typos, rather than worst-case typos."
2020.acl-main.245.txt,2020,6 Discussion,"in computer vision, chen et al.(2019) discretizes pixels to compute exact robust accuracy on mnist, but their approach generalizes poorly to other tasks like cifar-10."
2020.acl-main.245.txt,2020,6 Discussion,"other attack surfaces involving insertion of sentences (jia and liang, 2017) or syntactic rearrangements (iyyer et al., 2018) are harder to pair with roben, and are interesting directions for future work."
2020.acl-main.245.txt,2020,6 Discussion,"using context is not fundamentally at odds with the idea of robust encodings, and making contextual encodings stable is an interesting technical challenge and a promising direction for future work."
2020.acl-main.247.txt,2020,7 Conclusion and Future Work,"in future work, we will address end-to-end question answering with pre-training for both the answer selection and retrieval components."
2020.acl-main.248.txt,2020,4 Conclusion and Future Work,our future work is to include the paragraph representation in the constraint prediction model.
2020.acl-main.248.txt,2020,4 Conclusion and Future Work,this will help our methodology to have the benefit of making informed decision while also solving constraints.
2020.acl-main.251.txt,2020,6 Conclusion,"in the future, we seek to expand upon energy-based translation using our method."
2020.acl-main.252.txt,2020,7 Conclusion and Future Directions,"future work should explore techniques like iterative back-translation (hoang et al., 2018) for further improvement and scaling to larger model capacities and more languages (arivazhagan et al., 2019b; huang et al., 2019) to maximize transfer across languages and across data sources."
2020.acl-main.253.txt,2020,6 Conclusions,"in the future, we plan to investigate more thoroughly the use of language models for evaluating fluency, the effect of domain mismatch in the choice of monolingual data, and ways to generalize this study to other applications beyond mt."
2020.acl-main.255.txt,2020,6 Conclusion,we leave it as future work to explore ways to raise accuracy on unseen synsets without harming performances on frequent synsets.
2020.acl-main.257.txt,2020,7 Conclusion and Future Work,in future work we will investigate more sophisticated neural (sub-)networks within the proposed framework.
2020.acl-main.257.txt,2020,7 Conclusion and Future Work,"we will also apply the idea of functionspecific training to other interrelated linguistic phenomena and other languages, probe the usefulness of function-specific vectors in other language tasks, and explore how to integrate the methodology with sequential models."
2020.acl-main.259.txt,2020,7 Conclusions and Future Work,"directionality of edges did not result in improvement in our models in this work, however for future, we plan to develop gcns that incorporate edge typing, which would enable us to differentiate between different mwe types and dependency relations while comparing them against the current models."
2020.acl-main.259.txt,2020,7 Conclusions and Future Work,"for future work, we plan to add vmwe annotations to the vu amsterdam corpus (steen, 2010) which is the largest metaphor dataset and extend our experiments using that resource."
2020.acl-main.260.txt,2020,5 Conclusion,we encourage researchers to look at languages with different grammatical gender (such as czech and slovak) and propose new methods to reduce the bias in multilingual embeddings as well as in cross-lingual transfer learning.
2020.acl-main.260.txt,2020,5 Conclusion,we hope this study can work as a foundation to motivate future research about the analysis and mitigation of bias in multilingual embeddings.
2020.acl-main.261.txt,2020,5 Moving Forward,looking to other scientific disciplines that have faced similar issues in the past may provide some guidance for our future.
2020.acl-main.264.txt,2020,7 Conclusion,"however, a comprehensive study is required to prove the conjecture and we leave this as future work."
2020.acl-main.265.txt,2020,6 Conclusion,"additionally, future work should further probe the source of gender bias in the model’s predictions, perhaps by visualizing attention or looking more closely at the model’s outputs."
2020.acl-main.265.txt,2020,6 Conclusion,we encourage future work to dive deeper into this problem.
2020.acl-main.265.txt,2020,6 Conclusion,"we only consider binary gender, but future work should consider non-binary genders."
2020.acl-main.265.txt,2020,6 Conclusion,"while these findings will help future work avoid gender biases, this study is preliminary."
2020.acl-main.267.txt,2020,6 Conclusion and Future Work,"first, we plan to explore why the model prefers “soft” attentions rather than “hard” ones, which is different from the findings in several prior works based on hard attention."
2020.acl-main.267.txt,2020,6 Conclusion and Future Work,"in our future work, we will explore several potential directions."
2020.acl-main.267.txt,2020,6 Conclusion and Future Work,"instead of using a fixed pooling norm for universal text representation learning, we propose to learn the norm in an end-to-end framework to automatically find the optimal ones for learning text representations in different tasks."
2020.acl-main.267.txt,2020,6 Conclusion and Future Work,"second, we plan to study how to model the differences on the characteristics of different samples and use different pooling norms, which may have the potential to further improve our approach."
2020.acl-main.267.txt,2020,6 Conclusion and Future Work,"third, we will explore how to generalize our approach to other modalities, such as images, audios and videos, to see whether it can facilitate more attention-based methods."
2020.acl-main.268.txt,2020,6 Conclusion,"in future work, apart from improving the similarity measures, it could be examined to predict mtl scores or estimate the right amount of auxiliary data or shared parameters in the neural network."
2020.acl-main.269.txt,2020,6 Conclusion,"future directions include validating our findings on other san architectures (e.g., bert (devlin et al., 2019)) and more general attention models (bahdanau et al., 2015; luong et al., 2015)."
2020.acl-main.27.txt,2020,8 Conclusion,"as future work, we will extend our framework to more complex contexts by devising efficient learning algorithms."
2020.acl-main.270.txt,2020,8 Conclusion,"by showing that sublayer ordering can improve models at no extra cost, we hope that future research continues this line of work by looking into optimal sublayer ordering for other tasks, such as translation, question answering, and classification."
2020.acl-main.271.txt,2020,7 Conclusion,"the proposed method is not limited to the two aforementioned tasks, but can be applied to any nlp as well as other tasks such as machine translation and image recognition."
2020.acl-main.273.txt,2020,5 Conclusion,"besides, how to introduce scene graphs into multi-modal nmt is a worthy problem to explore."
2020.acl-main.273.txt,2020,5 Conclusion,"finally, we will apply our model into other multi-modal tasks such as multi-modal sentiment analysis."
2020.acl-main.273.txt,2020,5 Conclusion,"in the future, we plan to incorporate attributes of visual objects and dependency trees to enrich the multi-modal graphs."
2020.acl-main.276.txt,2020,5 Conclusion,the riemannian framework allows to exploit the geometry of the doubly stochastic manifold.
2020.acl-main.278.txt,2020,7 Conclusion,"as well-calibrated confidence estimation is more likely to establish trustworthiness with users, we plan to apply our work to interactive machine translation scenarios in the future."
2020.acl-main.278.txt,2020,7 Conclusion,"through a series of in-depth analyses, we report several interesting findings which may help to analyze, understand and improve nmt models."
2020.acl-main.279.txt,2020,4 Conclusion,"in the future, we plan to enable the glyph and phonetic variation detection by integrating the variation graph representation learning, which may improve signal’s performance."
2020.acl-main.28.txt,2020,5 Conclusion and Future Work,"in the future, we plan to apply the sa framework on syntactic parse trees in hopes of generating more syntactically different sentences (motivated by our case study)."
2020.acl-main.280.txt,2020,6 Conclusion,"in the future, we plan to study complicated situations such as a law case with multiple defendants and charges."
2020.acl-main.282.txt,2020,5 Conclusion,"we believe our method can also be applied to other tasks that need to exploit hierarchical label structure and label co-occurrence, such as fine-grained entity typing and hierarchical multi-label classification."
2020.acl-main.283.txt,2020,7 Conclusion,"as recent works explore the superiority of hyperbolic space to euclidean space for serval natural language processing tasks, we intend to couple with the hyperbolic neural networks (ganea et al., 2018b) and the hyperbolic word embedding method such as poincare´glove (tifrea et al., 2019) in the future."
2020.acl-main.284.txt,2020,7 Conclusion,"our future research direction includes a thorough study of differences in this dataset with actual tickets, and potential for transfer."
2020.acl-main.284.txt,2020,7 Conclusion,"we also study the performance of the most recent recurrent neural network-based approaches to sequence labelling, on this task."
2020.acl-main.285.txt,2020,6 Conclusion and Future Work,promising future directions include: 1) utilize more types of data from mooccube to facilitate existing topics; 2) employ advanced models in existing tasks; 3) more innovative nlp application tasks in online education domain.
2020.acl-main.288.txt,2020,5 Conclusions,"we also develop two kinds of 2d transformers, i.e., window-constrained and cross-road 2d transformers, to further model the interaction of different emotion-cause pairs."
2020.acl-main.289.txt,2020,6 Conclusion and Future Work,"finally, it would be interesting to study the semantic roles of emotion (bostan et al., 2020), which considers the full structure of an emotion expression and is more challenging."
2020.acl-main.289.txt,2020,6 Conclusion and Future Work,"in future work, we shall explore the following directions."
2020.acl-main.289.txt,2020,6 Conclusion and Future Work,"second, designing effective methods to inject appropriate linguistic knowledge into neural models is valuable to emotion analysis tasks (ke et al., 2019; zhong et al., 2019)."
2020.acl-main.29.txt,2020,6 Conclusion and Future Work,"having shown that segment bounds contain useful supervisory signal, it would be interesting to examine if segment hierarchies might also contain useful signal."
2020.acl-main.29.txt,2020,6 Conclusion and Future Work,"s-lstm is agnostic as to the sentence encoder used, so we would like to investigate the potential usefulness of transformer-based language models as sentence encoders."
2020.acl-main.29.txt,2020,6 Conclusion and Future Work,"there are additional engineering challenges associated with using models such as bert as sentence encoders, since encoding entire documents can be too expensive to fit on a gpu without model parallelism."
2020.acl-main.29.txt,2020,6 Conclusion and Future Work,we would also like to investigate the usefulness of an unconsidered source of document structure: the hierarchical nature of sections and subsections.
2020.acl-main.293.txt,2020,7 Conclusion and Future work,"one can try to adapt our proposed csae architecture for an integrated approach by applying the unified tagging scheme; thereby, aspect extraction and sentiment classification can be achieved simultaneously."
2020.acl-main.296.txt,2020,5 Conclusion,"for future works, we will explore pair-wise at and ot extraction together with aspect category and sentiment polarity classification."
2020.acl-main.298.txt,2020,8 Conclusion,"for future work, we aim to develop a universal model to handle both tree and non-tree arguments."
2020.acl-main.30.txt,2020,8 Conclusions and Future Work,"also, extending our method for other types of textual data, such as short texts, multi-lingual data, and code-switched data is a potential direction."
2020.acl-main.30.txt,2020,8 Conclusions and Future Work,"in the future, we are interested in generalizing contextualized weak supervision to hierarchical text classification problems."
2020.acl-main.303.txt,2020,7 Discussion,"finally, future work should investigate whether data augmentation can fully bridge the gap between low-bias learners and structured tree lstms, and whether our conclusions apply to other syntactic phenomena besides agreement."
2020.acl-main.303.txt,2020,7 Discussion,future work should further explore both of these approaches.
2020.acl-main.303.txt,2020,7 Discussion,it seems particularly promising to explore alternative formulations of the dependency lstm (as mentioned above) and the effect of learning embeddings of non-terminal symbols for the constituency lstm.
2020.acl-main.305.txt,2020,6 Conclusion,we also analyze our model’s outputs to get more insights into user interest dynamics.
2020.acl-main.306.txt,2020,5 Conclusion,there are several future directions for this work.
2020.acl-main.306.txt,2020,5 Conclusion,"therefore, our next step is to enhance umt so as to dynamically filter out the potential noise from images."
2020.acl-main.308.txt,2020,5 Conclusion and Future Work,"finally, we plan to experiment with other languages."
2020.acl-main.308.txt,2020,5 Conclusion and Future Work,"in future work, we plan to perform user profiling with respect to polarizing topics such as gun control (darwish et al., 2020), which can then be propagated from users to media (atanasov et al., 2019; stefanov et al., 2020)."
2020.acl-main.308.txt,2020,5 Conclusion and Future Work,"we further want to model the network structure, e.g., using graph embeddings (darwish et al., 2020)."
2020.acl-main.309.txt,2020,7 Discussion and Conclusion,alleviating this restriction is an important future direction.
2020.acl-main.310.txt,2020,8 Conclusion,this study shed light on understanding the behaviors of language encoders against grammatical errors and encouraged future work to enhance the robustness of these models.
2020.acl-main.311.txt,2020,7 Conclusion,immediate attention should be paid to the investigation of how heat maps would vary during the extensive pre-training so that we have a better understanding of the dynamics of the learning processes.
2020.acl-main.312.txt,2020,6 Conclusions,"moreover, we can also examine the influence of using deep contextualized input encoders such as elmo (peters et al., 2018) or bert (devlin et al., 2018)."
2020.acl-main.312.txt,2020,6 Conclusions,there are some future directions that are worth exploring.
2020.acl-main.313.txt,2020,5 Conclusion,"in future work, we plan to extend r-men for multi-hop knowledge graph reasoning."
2020.acl-main.315.txt,2020,4 Conclusion,"furthermore, the word-aligned attention can also be applied to english plms to bridge the semantic gap between the whole word and the segmented word-piece tokens, which we leave for future work."
2020.acl-main.317.txt,2020,5 Conclusion,"in further work, we will explore more efficient ways for constructing the perturbation set."
2020.acl-main.317.txt,2020,5 Conclusion,"we also plan to generalize our approach to achieve certified robustness against other types of adversarial attacks in nlp, such as the out-of-list attack."
2020.acl-main.318.txt,2020,7 Conclusion,"in the future, we will consider combining our method with graph neural networks to update the word graphs we build."
2020.acl-main.319.txt,2020,7 Conclusion,"furthermore, we notice some exceptional cases which we call as “reinforced samples”, which we leave as the future work."
2020.acl-main.32.txt,2020,5 Conclusion,"besides, developing correlated topic modelsis another promising direction."
2020.acl-main.32.txt,2020,5 Conclusion,"in the future, we would like to devise a nonparametric neural topic model based on adversarial training."
2020.acl-main.324.txt,2020,8 Conclusion and Future Work,"in the future, we intend to extend the work to include language types such as asian languages."
2020.acl-main.324.txt,2020,8 Conclusion and Future Work,we will also introduce other effective methods to improve zero-shot translation quality.
2020.acl-main.327.txt,2020,6 Conclusion,"in future work, we will work around the problem of evaluation errors in the low da range."
2020.acl-main.328.txt,2020,9 Conclusions,a practical line of future work is embedding our plotting agent in interactive environments such as jupyter lab.
2020.acl-main.328.txt,2020,9 Conclusions,future work includes methods that get closer to human performance on the dataset.
2020.acl-main.329.txt,2020,6 Conclusion,"in future work, we would like to experiment with a multi-task setup wherein tasks with less training data can significantly benefit from those having abundant labelled data, since most code-switched datasets are often small and difficult to annotate."
2020.acl-main.329.txt,2020,6 Conclusion,"we hope that this will encourage researchers to test multilingual, cross-lingual and code-switched embedding techniques and models on this benchmark."
2020.acl-main.329.txt,2020,6 Conclusion,we would like to add more diverse tasks and language pairs to the gluecos benchmark in a future version.
2020.acl-main.33.txt,2020,5 Conclusion,"in future work, we will examine semantic relations between class labels in the auxiliary task."
2020.acl-main.33.txt,2020,5 Conclusion,"moreover, we will adapt our model to text generation tasks."
2020.acl-main.331.txt,2020,6 Conclusion and Discussion,"besides news recommendation, the mind dataset can also be used in other natural language processing tasks such as topic classification, text summarization and news headline generation."
2020.acl-main.331.txt,2020,6 Conclusion and Discussion,"in addition, besides the click behaviors, we plan to incorporate other user behaviors such as read and engagement to support more accurate user modeling and performance evaluation."
2020.acl-main.331.txt,2020,6 Conclusion and Discussion,"in the future, we plan to extend the mind dataset by incorporating image and video information in news as well as news in different languages, which can support the research of multi-modal and multi-lingual news recommendation."
2020.acl-main.331.txt,2020,6 Conclusion and Discussion,"many interesting researches can be conducted on the mind dataset, such as designing better news and user modeling methods, improving the diversity, fairness and explainability of news recommendation results, and exploring privacy-preserving news recommendation."
2020.acl-main.332.txt,2020,8 Conclusions and Future Work,"in future work, we plan to extend this work to more datasets and to more languages."
2020.acl-main.332.txt,2020,8 Conclusions and Future Work,"we further want to go beyond textual claims, and to take claimimage and claim-video pairs as an input."
2020.acl-main.333.txt,2020,6 Conclusion,it is our hope the proposed holistic metrics may pave the way towards the comparability of open-domain dialogue models.
2020.acl-main.334.txt,2020,5 Conclusion and Future Work,"future work includes i) improving projection learning to model complicated linguistic properties of hypernymy; ii) extending our model to address other tasks, such as graded lexical entailment (vulic et al., 2017) and cross-lingual graded lexical entailment (vulic et al., 2019); and iii) exploring how deep neural language models (such as bert (devlin et al., 2019), transformer-xl (dai et al., 2019), xlnet (yang et al., 2019)) can improve the performance of hypernymy detection."
2020.acl-main.335.txt,2020,7 Conclusion,"for future work, an extrinsic evaluation of our methods is needed to prove the effectiveness of learned biomedical entity representations and to prove the quality of the entity normalization in downstream tasks."
2020.acl-main.337.txt,2020,6 Conclusion and Future Work,"thus, in the future, we will take the uncertainty, polysemy, and context sensitivity of the word meanings and the frequency of words into account and explore better ways of modeling the word-class distributions in semantic vector spaces."
2020.acl-main.338.txt,2020,6 Conclusion,"in our future work, we would like to improve the performance of the asc task by using unlabeled data since our graph-based neural network approach is easy to add unlabeled data."
2020.acl-main.338.txt,2020,6 Conclusion,"moreover, we would like to apply our approach to other sentiment analysis tasks, e.g., aspect-oriented opinion summarization and multi-label emotion detection."
2020.acl-main.339.txt,2020,6 Conclusion,"in the future, we will develop a syntax-based multi-scale graph convolutional network to deal with both short and long aspects."
2020.acl-main.34.txt,2020,6 Conclusion and Future Works,"in future work, we will investigate the impact of fine-grained word categories (such as nouns, verbs, and adjectives) on the translation performance and design specific methods according to these categories."
2020.acl-main.341.txt,2020,6 Conclusion,"for future work, we will extend sentibert to other applications involving phrase-level annotations."
2020.acl-main.342.txt,2020,7 Conclusion,"in the future, one possible direction is creating complete graphs with their nodes being input clauses to achieve full coverage."
2020.acl-main.343.txt,2020,7 Conclusion,"in the future, we will further explore the connection between multimodal analysis and multi-task learning and incorporate more fusion strategy, including early- and middle-fusion."
2020.acl-main.344.txt,2020,5 Conclusion and Future Work,"besides, we expect the idea of curriculum pre-training can be adopted on other nlp tasks."
2020.acl-main.344.txt,2020,5 Conclusion and Future Work,"in the future, we will explore how to leverage unlabeled speech data and large bilingual text data to further improve the performance."
2020.acl-main.345.txt,2020,8 Summary,we will investigate this direction in future work.
2020.acl-main.346.txt,2020,6 Conclusion,"in future work, we intend to explore the idea of self-training for parsing written texts."
2020.acl-main.346.txt,2020,6 Conclusion,"the first step is to develop parsing models that parse asr output, rather than speech transcripts."
2020.acl-main.346.txt,2020,6 Conclusion,we also aim at integrating syntactic parsing and self-training more closely with automatic speech recognition.
2020.acl-main.348.txt,2020,6 Conclusion,"finally, we will explore further the generability of our meta-transfer learning approach to more downstream multilingual tasks in our future work."
2020.acl-main.350.txt,2020,7 Conclusion,"for future work, we will design more flexible policies to achieve better translation quality and lower delay in simultaneous spoken language translation."
2020.acl-main.351.txt,2020,7 Future work,"we plan to do a detailed analysis along two lines: 1) comparing if the proposed modeling technique can help bridge gap between predicted and human annotations, and 2) effect of environment variables e.g., background noise, speaker features, different languages etc."
2020.acl-main.353.txt,2020,7 Conclusion,an exciting synthesis would incorporate deception and language generation into an agent’s policy; our data would help train such agents.
2020.acl-main.354.txt,2020,5 Conclusion,we believe this work opens a new competitive avenue in the area of implicit generative models for sequential data.
2020.acl-main.355.txt,2020,6 Conclusions and Future Work,"as a potential direction for future work, it would be interesting to investigate the use of the ema technique on transformer models as well and conduct similar studies to examine needless architectural complexity in other nlp tasks."
2020.acl-main.356.txt,2020,5 Conclusion and Future Work,"in the future, we plan to extend the applicability of the presented model to other linguistics tasks as well as recommendations and medical inference tasks."
2020.acl-main.357.txt,2020,6 Conclusions,future works include applying such scoring method on broader classification tasks like natural language inference and sentiment analysis.
2020.acl-main.357.txt,2020,6 Conclusions,"we also think that our token-level scoring method could be used during the self-supervised pretraining phase to extend traditional next sentence prediction and sequence ordering tasks, bringing more commonsense knowledge in the model."
2020.acl-main.358.txt,2020,5 Conclusion and Future Work,"in the future, we plan to extend the key insights of segmenting features and facilitating interactions to other representation learning problems."
2020.acl-main.359.txt,2020,7 Conclusions and Future Work,"in the future, we plan to investigate ways to directly incorporate the rescoring metrics into the data selection process itself, so that penalising similar sentences can also be taken into account."
2020.acl-main.359.txt,2020,7 Conclusions and Future Work,we also aim to conduct a human evaluation of the translated sentences in order to obtain a better understanding of the effects of data selection and backtranslation on the overall quality.
2020.acl-main.36.txt,2020,6 Conclusion,"in the future, we will extend the investigation on the functionalities of the encoder and decoder to other sequence-to-sequence tasks such as text summarization and text style transfer to explore more applications of our model."
2020.acl-main.360.txt,2020,6 Conclusion,"in future work, we suggest performing a hyperparameter search over possible values for t in slt pruning (i.e., the number of training steps that are not discarded during model reset), and over si for the switch from slt to mp in slt-mp."
2020.acl-main.361.txt,2020,5 Conclusion and Future Work,"as future work, we plan to extend our method to other nlp tasks which rely on evidence finding, such as natural language inference."
2020.acl-main.362.txt,2020,6 Conclusion,"for future work, we aim to consider more complex relationships among the quantities and other attributes to enrich quantity representations further."
2020.acl-main.362.txt,2020,6 Conclusion,we will also explore adding heuristic in the tree-based decoder to guide and improve the generation of solution expression.
2020.acl-main.363.txt,2020,6 Conclusions,future work includes the application of cem at scales other than the ordinal.
2020.acl-main.364.txt,2020,6 Conclusion,"we believe that our method can benefit simultaneously from other compression techniques, such as pruning (han et al., 2016) and low-precision representation (ling et al., 2016)."
2020.acl-main.364.txt,2020,6 Conclusion,we leave this as an avenue for future work.
2020.acl-main.365.txt,2020,7 Conclusion,"in the future, we plan to investigate whether usage representations can provide an even finer grained account of lexical meaning and its dynamics, e.g., to automatically discriminate between different types of meaning change."
2020.acl-main.365.txt,2020,7 Conclusion,we expect our work to inspire further analyses of variation and change which exploit the expressiveness of contextualised word representations.
2020.acl-main.368.txt,2020,6 Conclusion,"furthermore, it would be interesting to explore more complex ways of incorporating surface-form information – e.g., by using a character-level cnn similar to the one of kim et al.(2016) – to balance out the potency of bertram’s form and context parts."
2020.acl-main.368.txt,2020,6 Conclusion,"in future work, we want to investigate bertram’s potential benefits for such frequent words."
2020.acl-main.369.txt,2020,7 Conclusions,"as future work, we plan to refine our approach by exploiting other strategies for weighting the words in the clusters and to leverage them for automatically building multilingual sense-tagged corpora."
2020.acl-main.370.txt,2020,5 Conclusion and Future Work,"in the future, we would like to investigate the application of our theory in these domain adaptation tasks."
2020.acl-main.370.txt,2020,5 Conclusion and Future Work,our purpose is to inject the target domain knowledge to bert and encourage bert to be domain-aware.
2020.acl-main.370.txt,2020,5 Conclusion and Future Work,"the proposed post-training procedure could also be applied to other domain adaptation scenarios such as named entity recognition, question answering, and reading comprehension."
2020.acl-main.371.txt,2020,5 Conclusion,"finally, detecting the more implicit relations between the argument and the key point, as seen in our error analysis, is another intriguing direction for future work."
2020.acl-main.371.txt,2020,5 Conclusion,"in addition, we plan to apply the methods presented in this work also to automatically-mined arguments."
2020.acl-main.371.txt,2020,5 Conclusion,the natural next step for this work is the challenging task of automatic key point generation.
2020.acl-main.372.txt,2020,7 Conclusion,"future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
2020.acl-main.373.txt,2020,7 Conclusion,"in the future, we plan to develop more complex models to be added in the next stages of the cascade classifier as well as automatically identify irony, gender stereotypes and sexist vocabulary."
2020.acl-main.374.txt,2020,7 Conclusion,"in the future, we hope to apply skep on more sentiment analysis tasks, to further see the generalization of skep, and we are also interested in exploiting more types of sentiment knowledge and more fine-grained sentiment mining methods."
2020.acl-main.375.txt,2020,7 Conclusion and Future Work,"lastly, given recent criticisms of probing approaches in nlp, it will be vital to revisit the insights produced here within a non-probing framework, for example, using representational similarity analysis (rsa) (chrupała and alishahi, 2019) over symbolic representations from treebanks and their encoded representations."
2020.acl-main.376.txt,2020,5 Conclusion,"in addition, any advances in seq2seq neural architectures or pretrained transformer-based language models (devlin et al., 2019) can be directly used to enhance our approach."
2020.acl-main.38.txt,2020,5 Conclusion,we also study the effects of deep decoders in addition to deep encoders extending previous works.
2020.acl-main.38.txt,2020,5 Conclusion,"we first investigate convergence differences between the published transformer (vaswani et al., 2017) and its official implementation (vaswani et al., 2018), and compare the differences of computation orders between them."
2020.acl-main.381.txt,2020,5 Conclusion,in future we will sample target models with a larger number of plausible combinations of factors.
2020.acl-main.381.txt,2020,5 Conclusion,in future work we hope to further disentangle these differences.
2020.acl-main.382.txt,2020,5 Summary and Outlook,"future work will focus on developing more advanced procedures for detecting inconsistencies, and on building robust models that do not generate inconsistencies."
2020.acl-main.383.txt,2020,8 Discussion & Conclusion,"we also extend our method to probe document structure, which sheds lights on bert’s effectiveness in modeling long sequences."
2020.acl-main.383.txt,2020,8 Discussion & Conclusion,"we leave it for future work to use our technique to test other linguistic properties (e.g., coreference) and to extend our study to more downstream tasks and systems."
2020.acl-main.384.txt,2020,5 Conclusion,"future work should investigate where these primitive referential abilities stem from and how they can be fostered in future architectures and training setups for language modeling, and neural models more generally."
2020.acl-main.384.txt,2020,5 Conclusion,"we find that the two models behave similarly, but the transformer performs consistently better (around 10% higher accuracy in the probe tasks).8 future work should test other architectures, like cnn-based lms and lstms with attention, to provide additional insights into the linguistic capabilities of language models."
2020.acl-main.385.txt,2020,5 Conclusion,"following this work, we can build the attention graph with effective attention weights (brunner et al., 2020) instead of raw attentions."
2020.acl-main.387.txt,2020,7 Conclusion & Future work,"as future work, we would like to extend our analysis and proposed techniques to more complex models and downstream tasks."
2020.acl-main.388.txt,2020,6 Conclusion,"in the future, it would be fruitful to develop a novel weighting strategy for the tchebycheff procedure."
2020.acl-main.39.txt,2020,8 Discussion,"this is a bottleneck for extrapolation, suggesting that removing this heuristic is key to reaching perfect extrapolation and should be investigated in future work."
2020.acl-main.390.txt,2020,7 Conclusion,additional use cases like the training of personalized recommendation models as well as the use of reinforcement learning to find a good trade-off between system and user objective remain to be investigated in future work.
2020.acl-main.391.txt,2020,7 Conclusion,"in future work, we will investigate whether bert-init can be used effectively by using methods to deal with catastrophic forgetting."
2020.acl-main.393.txt,2020,7 Conclusion,we hope to address this problem with a completely semantic-based approach in the future.
2020.acl-main.394.txt,2020,7 Conclusion,"for instance, we expect that abuse detection may also benefit from joint learning with complex semantic tasks, such as figurative language processing and inference."
2020.acl-main.394.txt,2020,7 Conclusion,"the mutually beneficial relationship that exists between these two tasks opens new research avenues for improvement of abuse detection systems in other domains as well, where emotion would equally play a role."
2020.acl-main.395.txt,2020,5 Conclusion,future adoptions to fuse will include the integration of a dialog component.
2020.acl-main.395.txt,2020,5 Conclusion,"it will be interesting to see, if we can reuse (or transfer) the machine learning models as well as the rest of the approach."
2020.acl-main.395.txt,2020,5 Conclusion,we plan to evaluate fuse in other domains.
2020.acl-main.395.txt,2020,5 Conclusion,we will also implement a sanity check that considers feasibility and meaningfulness of the sequence of actions in the method body.
2020.acl-main.396.txt,2020,4 Conclusions and Future Work,"a limitation of our work is that we considered a narrow contextual context, comprising only the previous comment and the discussion title.11 it would be interesting to investigate in future work ways to improve the annotation quality when more comments in the discussion thread are provided, and also if our findings hold when broader context is considered (e.g., all previous comments in the thread, or the topic of the thread as represented by a topic model)."
2020.acl-main.396.txt,2020,4 Conclusions and Future Work,our experiments and datasets provide an initial foundation to investigate these important directions.
2020.acl-main.397.txt,2020,6 Conclusion,"we also propose to incorporate the latent graph into other multi-task learning problems (chen et al., 2019; kurita and søgaard, 2019)."
2020.acl-main.398.txt,2020,7 Conclusion,"in future work we aim to extend the model to represent a database with multiple tables as context, and to effectively handle large tables."
2020.acl-main.399.txt,2020,7 Conclusion,"combining target inference with stance classification in future work, we can already generate basic conclusions, say, “raising the school leaving age is good”."
2020.acl-main.40.txt,2020,6 Conclusion and Future Work,"in the future, we would like to extend our model to extremely large datasets, such as wmt’14 english-to-french with about 36m sentence-pairs."
2020.acl-main.400.txt,2020,4 Conclusion,"in future work, we would like to investigate the effectiveness of our model in these tasks."
2020.acl-main.401.txt,2020,6 Conclusion,"along with investigating new techniques, we hope that assembling a bigger curated dataset with quality annotations will help in better performance."
2020.acl-main.402.txt,2020,6 Conclusion and Future Work,"in future, conversation history, speaker information, fine-grained modality encodings can be incorporated to predict da with more accuracy and precision."
2020.acl-main.403.txt,2020,7 Conclusion,"in the future, we plan to study more in depth the stylistic and figurative devices used for parody, extend the data set beyond the political case study and explore human behavior regarding parody, including how this is detected and diffused through social media."
2020.acl-main.404.txt,2020,6 Conclusion,"to remove these biases, however, presumably more sophisticated methods will be necessarily in the general case."
2020.acl-main.404.txt,2020,6 Conclusion,"while we only evaluated the strategy on one model, we believe its benefits carry over to other model architectures and similar tasks."
2020.acl-main.406.txt,2020,7 Conclusion and Future Work,the framework introduces a range of important questions both from the inference and the application perspectives.
2020.acl-main.407.txt,2020,7 Discussion,"as we argued that compositionality has, after all, desirable properties, future work could adapt methods for learning disentangled representations (e.g., higgins et al., 2017; kim and mnih, 2018) to let (more) compositional languages emerge."
2020.acl-main.408.txt,2020,7 Conclusions and Future Directions,"it also serves as an ideal starting point for several future directions such as better evaluation metrics for interpretability, causal analysis of nlp models and datasets of rationales in other languages."
2020.acl-main.408.txt,2020,7 Conclusions and Future Directions,"our hope is that eraser enables future work on designing more interpretable nlp models, and comparing their relative strengths across a variety of tasks, datasets, and desired criteria."
2020.acl-main.408.txt,2020,7 Conclusions and Future Directions,"we believe these metrics provide reasonable means of comparison of specific aspects of interpretability, but we view the problem of measuring faithfulness, in particular, a topic ripe for additional research (which eraser can facilitate)."
2020.acl-main.409.txt,2020,9 Conclusions,we view these as interesting directions for future work.
2020.acl-main.413.txt,2020,4 Conclusion,"we aim to experiment with other datasets and other domains, incorporate our synthetic data in a semi-supervised setting and test the feasibility of our framework in a multi-lingual setting."
2020.acl-main.415.txt,2020,6 Conclusion,we hope this corpus will motivate and enable further developments in both phonetic typology and methodology for working with cross-linguistic speech corpora.
2020.acl-main.417.txt,2020,11 Conclusions,we are especially interested in further extending this work into low resource languages where resources tend to be noisier and underlying models to support data mining less reliable.
2020.acl-main.418.txt,2020,6 Discussion and Moving Forward,"it may be the case that to truly build gender inclusive datasets and systems, we need to hire or consult experiential experts (patton et al., 2019; young et al., 2019)."
2020.acl-main.418.txt,2020,6 Discussion and Moving Forward,we also hope that developers of datasets or systems can use some of our analysis as inspiration for how one can attempt to measure—and then root out—different forms of bias in coreference resolution systems and nlp systems more broadly.
2020.acl-main.418.txt,2020,6 Discussion and Moving Forward,we hope this paper can serve as a roadmap for future studies.
2020.acl-main.419.txt,2020,8 Conclusion,"our research not only results in insights into significant similarities between bidirectional rnns and human attention, but also opens the avenue for promising future research directions."
2020.acl-main.421.txt,2020,7 Conclusions,"to provide a more comprehensive benchmark to evaluate cross-lingual models, we also released the cross-lingual question answering dataset (xquad)."
2020.acl-main.422.txt,2020,7 Conclusion,"an exciting direction for future work is to combine the two approaches in order to identify which linguistic properties are captured in model components that are similar to one another, or explicate how localization of information contributes to the learnability of particular properties."
2020.acl-main.422.txt,2020,7 Conclusion,"it may be insightful to compare the results of our analysis to the loss surfaces of the same models, especially before and after fine-tuning (hao et al., 2019)."
2020.acl-main.422.txt,2020,7 Conclusion,one could also study whether a high similarity entail that two models converged to a similar solution.
2020.acl-main.427.txt,2020,7 Conclusion,"we believe this data will be useful to researchers studying semantic parsing, especially interactive semantic parsing, human-robot interaction, and even imitation and reinforcement learning."
2020.acl-main.428.txt,2020,5 Conclusion,"future work could apply this same technique with other supervised data, e.g.correcting causal or commonsense reasoning errors (zellers et al., 2019; qin et al., 2019)."
2020.acl-main.431.txt,2020,9 Conclusion,"we further study the extent to which various social biases (gender, race, religion) are encoded, employing several different quantification schemas."
2020.acl-main.433.txt,2020,6 Conclusion,"future work will explore other measures and alternative game settings for the emergence of compositionality, as well as more subtle psychological effects (categeorical perception) of continuous biological systems exhibiting discrete structure, like the auditory system."
2020.acl-main.434.txt,2020,10 Conclusion,"overall, these results help to clarify the patterns of distribution of context information within contextual embeddings— future work can further clarify the impact of more diverse syntactic relations between words, and of additional types of word features."
2020.acl-main.434.txt,2020,10 Conclusion,"we apply these tests to examine the distribution of contextual information across sentence tokens for popular contextual encoders bert, elmo, and gpt."
2020.acl-main.438.txt,2020,8 Conclusions,"the learned constraints can be used for structured prediction problems in two ways: (1) combining them with an existing model to improve prediction performance, or (2) incorporating them into the training process to train a better model."
2020.acl-main.439.txt,2020,5 Discussion,future work should explore the extent to which our model could further benefit from initializing with stronger models and what computational challenges may arise.
2020.acl-main.439.txt,2020,5 Discussion,"in future work, we hope to better understand how a discourse model can also learn fine-grained relationship types between sentences from unlabeled data."
2020.acl-main.44.txt,2020,4 Conclusion and Future Work,"a more general form, f ∝ ∏k(r + γk)−βk , can be considered for further investigation."
2020.acl-main.440.txt,2020,8 Conclusion,"diy (do-it-yourself) videos and websites, for instance, are an obvious next target."
2020.acl-main.440.txt,2020,8 Conclusion,"ultimately, we believe this work will further the goal of building agents that can work with human collaborators to carry out complex tasks in the real world."
2020.acl-main.440.txt,2020,8 Conclusion,we also envision extending this work by including audio and video features to enhance the quality of our alignment algorithm.
2020.acl-main.441.txt,2020,7 Discussion & Conclusion,future work could explore a detailed cost and time trade-off between adversarial and static collection.
2020.acl-main.443.txt,2020,7 Conclusion,"we believe our corpus, stackoverflow-specific bert embeddings and named entity tagger will be useful for various language-and-code tasks, such as code retrieval, software knowledge base extraction and automated question-answering."
2020.acl-main.444.txt,2020,7 Conclusions,"in the future, we are interested in investigating the generality of our defined schema for other comedies and different conversational registers, identifying the temporal intervals when relations are valid (surdeanu, 2013) in a dialogue, and joint dialogue-based information extraction as well as its potential combinations with multimodal signals from images, speech, and videos."
2020.acl-main.445.txt,2020,6 Conclusion and Future Work,"in the future, we will investigate multi-document summarization datasets such as duc (paul and james, 2004) and tac (dang and owczarzak, 2008) to see whether our findings coincide when multiple references are provided."
2020.acl-main.449.txt,2020,5 Conclusion,"in our future work, we want to study the effective incorporation of code structure into the transformer and apply the techniques in other software engineering sequence generation tasks (e.g., commit message generation for source code changes)."
2020.acl-main.450.txt,2020,9 Conclusion,"the framework we present is general, and extending it to other conditional text generation tasks such as image captioning or machine translation is a promising directions."
2020.acl-main.451.txt,2020,6 Conclusion,"for future work, we will explore better graph encoding methods, and apply discourse graphs to other tasks that require long document encoding."
2020.acl-main.453.txt,2020,6 Discussion and Conclusion,"in future work, we plan to experiment more with this, examining how we can combine constituents to make fluent sentences without including potentially irrelevant context."
2020.acl-main.453.txt,2020,6 Discussion and Conclusion,"we would also like to further experiment with abstractive summarization to re-examine whether large, pre-trained language models (liu and lapata, 2019) can be improved for our domain."
2020.acl-main.455.txt,2020,7 Conclusions and Future Work,"in the future, we intend to investigate different meaning representation formalisms, such as amr (banarescu et al., 2013) and dynamic syntax (kempson et al., 2001) and extend to other datasets (e.g.multiplereference summarization) and tasks (e.g.response generation in dialogue)."
2020.acl-main.458.txt,2020,8 Conclusion,we hope that our work draws the community’s attention to the factual correctness issue of abstractive summarization models and inspires future work in this direction.
2020.acl-main.459.txt,2020,6 Conclusion and Future Work,"we also hope that the dataset can be added to in the future with multi-modal extractions, more granular annotations, and deeper mining of the wiki."
2020.acl-main.459.txt,2020,6 Conclusion and Future Work,"we hope crd3 offers useful, unique data for the community to further explore dialogue modeling and summarization."
2020.acl-main.46.txt,2020,5 Conclusion and Future Work,the adaptation to other types of morphological markers will necessitate more elaborate linguistic reflection.
2020.acl-main.46.txt,2020,5 Conclusion and Future Work,the second direction is towards extending the approach to morphologically rich languages.
2020.acl-main.46.txt,2020,5 Conclusion and Future Work,two orientations can be identified for future work.
2020.acl-main.462.txt,2020,7 Conclusion,we hope that a deeper appreciation of the role of construal in language use will spur progress toward systems that more closely approximate human linguistic intelligence.
2020.acl-main.463.txt,2020,10 Conclusion,with this we hope to encourage a top-down perspective on our field which we think will help us select the right hill to climb towards human-analogous nlu.
2020.acl-main.464.txt,2020,6 Conclusions,a crucial direction of future work is to develop richer ways of capturing scholarly impact.
2020.acl-main.464.txt,2020,6 Conclusions,"we used the citation counts of a subset (∼27k papers) to examine patterns of citation across paper types, venues, over time, and across areas of research within nlp."
2020.acl-main.466.txt,2020,5 Conclusion,"furthermore, we need to take the ethical issues of legalai seriously."
2020.acl-main.466.txt,2020,5 Conclusion,"in addition to these applications and tasks we have mentioned, there are many other tasks in legalai like legal text summarization and information extraction from legal contracts."
2020.acl-main.466.txt,2020,5 Conclusion,"in the future, for these existing tasks, researchers can focus on solving the three most pressing challenges of legalai combining embedding-based and symbol-based methods."
2020.acl-main.467.txt,2020,6 Conclusion and Future Work,future work in this area would benefit greatly from improvements to both the breadth and depth of available probing tasks.
2020.acl-main.467.txt,2020,6 Conclusion and Future Work,our results therefore suggest a need for further work on efficient transfer learning mechanisms.
2020.acl-main.469.txt,2020,5 Conclusion and Future Work,"for future work, it would be interesting to see if more linguistically-inspired phenomena can be systematically found in cross-modal models."
2020.acl-main.47.txt,2020,7 Conclusion and Future work,"furthermore, we would like to extend a comparison between machine and human language processing beyond the perspective of word order."
2020.acl-main.47.txt,2020,7 Conclusion and Future work,"since lms are language-agnostic, analyzing word order in another language with the lm-based method would also be an interesting direction to investigate."
2020.acl-main.47.txt,2020,7 Conclusion and Future work,"we plan to further explore the capability of lms on other linguistic phenomena related to word order, such as “given new ordering” (nakagawa, 2016; asahara et al., 2018)."
2020.acl-main.470.txt,2020,6 Discussion and Future Work,future work could bolster the measure’s usefulness in several ways.
2020.acl-main.470.txt,2020,6 Discussion and Future Work,"however, the method’s efficacy in the present setting is likely boosted by the relative uniformity of crisis counseling conversations; and future work could aim to better accomodate settings with less structure and more linguistic variability."
2020.acl-main.470.txt,2020,6 Discussion and Future Work,technical improvements like richer utterance representations could improve the measure’s fidelity; more sophisticated analyses could better capture the dynamic ways in which the balance of objectives is negotiated across many turns.
2020.acl-main.470.txt,2020,6 Discussion and Future Work,"the preliminary explorations in section 5.4 could also be extended to gauge the causal effects of counselors’ behaviors (kazdin, 2007)."
2020.acl-main.470.txt,2020,6 Discussion and Future Work,"we expect balancing problems to recur in conversational settings beyond crisis counseling, such as court proceedings, interviews, debates and other mental health contexts like long-term therapy."
2020.acl-main.470.txt,2020,6 Discussion and Future Work,"with such improvements, it would be interesting to study other domains where interlocutors are faced with conversational challenges."
2020.acl-main.478.txt,2020,8 Conclusion,"in the future, we will further improve the performance of news discourse profiling by investigating subgenres of news articles, and extensively explore its usage for various other nlp tasks and applications."
2020.acl-main.479.txt,2020,7 Conclusion and future work,"considering the importance of context in drawing both scalar and other inferences in communication (grice, 1975; clark, 1992; bonnefon et al., 2009; zondervan, 2010; bergen and grodner, 2012; goodman and stuhlmu¨ller, 2013; degen et al., 2015), the development of appropriate representations of larger context is an exciting avenue for future research."
2020.acl-main.479.txt,2020,7 Conclusion and future work,"it would be interesting to investigate how much supervision is necessary and, for example, to what extent a model trained to perform another task such as predicting natural language inferences is able to predict scalar inferences (see jiang and de marneffe (2019b) for such an evaluation of predicting speaker commitment, and jereticˇ et al.(2020) for an evaluation of different nli models for predicting lexically triggered scalar inferences)."
2020.acl-main.479.txt,2020,7 Conclusion and future work,it would be straightforward to train similar models for other types of inferences.
2020.acl-main.479.txt,2020,7 Conclusion and future work,one further interesting line of research would be to extend this work to other pragmatic inferences.
2020.acl-main.48.txt,2020,6 Conclusion,"besides, while fake news usually targets at some events, we will also extend gcan to study how to remove eventspecific features to further boost the performance and explainability."
2020.acl-main.48.txt,2020,6 Conclusion,we will explore model generalization in the future work.
2020.acl-main.480.txt,2020,5 Conclusion,"we discussed several future directions, including data augmentation for downstream transferability, applicability of pretrained encoders to discourse, and utilizing larger discourse contexts."
2020.acl-main.481.txt,2020,6 Conclusion and Future Work,"in future work, we plan to extend this work to longer documents such as the recently released dataset of bamman et al.(2019)."
2020.acl-main.483.txt,2020,7 Conclusion & Future Work,"future work includes direct extension and validation of this technique with other language models such as gpt-2 (radford et al., 2019); experimenting with other hate speech or offensive language datasets; and experimenting with these and other sets of identity terms."
2020.acl-main.483.txt,2020,7 Conclusion & Future Work,"in this work, we effectively applied this technique to hate speech classifiers biased towards group identifiers; future work can determine the effectiveness and further potential for this technique in other tasks and contexts."
2020.acl-main.487.txt,2020,7 Discussion and Conclusion,future work is required to study if our ﬁndings carry over to other languages and cultural contexts.
2020.acl-main.490.txt,2020,7 Conclusions,"in future work, we intend to expand the coverage of clams by incorporating language-specific and non-binary phenomena (e.g., french subjunctive vs. indicative and different person/number combinations, respectively), and by expanding the typological diversity of our languages."
2020.acl-main.490.txt,2020,7 Conclusions,"this issue could be mitigated in the future with architectural changes to neural lms (such as better handling of morphology), more principled combinations of languages (as in dhar and bisazza 2020), or through explicit separation between languages during training (e.g., using explicit language ids)."
2020.acl-main.492.txt,2020,7 Conclusion,"finally, we are interested in exploring how these types of explanations are actually interpreted by users, and whether providing them actually establishes trust in predictive systems."
2020.acl-main.492.txt,2020,7 Conclusion,"future work might explore how rankings induced over training instances by influence functions can be systematically analyzed in a stand-alone manner (rather than in comparison with interpretations from other methods), and how these might be used to improve model performance."
2020.acl-main.493.txt,2020,7 Discussion,future work could extend this analysis to include quantitative results on the extent of agreement with ud.
2020.acl-main.493.txt,2020,7 Discussion,"future work should explore other multilingual models like xlm and xlm-roberta (lample and conneau, 2019) and attempt to come to an understanding of the extent to which the properties we’ve discovered have causal implications for the decisions made by the model, a claim our methods cannot support."
2020.acl-main.495.txt,2020,7 Conclusion,"we encourage future work to judge model interpretability using the proposed evaluation and publicly published annotations, and explore techniques for improving faithfulness and interpretability in compositional models."
2020.acl-main.496.txt,2020,8 Conclusion,"furthermore, our method is agnostic to the underlying nature of the two objects being aligned and can therefore align disparate objects such as images and captions, enabling a wide range of future applications within nlp and beyond."
2020.acl-main.5.txt,2020,6 Conclusion,annotations complement for multiwoz dataset in the future might enable dst-sc to handle the related-slot problem more effectively and further improve the joint accuracy.
2020.acl-main.50.txt,2020,7 Conclusion and Future Work,"ideally, we would like to automatically identify such polarizing topics."
2020.acl-main.50.txt,2020,7 Conclusion and Future Work,"in future work, we plan to increase the number of topics that we use to characterize media."
2020.acl-main.500.txt,2020,5 Conclusion,"possible future directions include a systematic study of different aspects of qg diversity (e.g., lexical and factual) and controlled diversification of individual aspects in generation."
2020.acl-main.500.txt,2020,5 Conclusion,we hope that our work will encourage further exploration of diversity-promoting qg and its evaluation.
2020.acl-main.501.txt,2020,7 Conclusions,"a future direction is to extend this work to question answering tasks that require reasoning over multiple documents, e.g., open-domain qa."
2020.acl-main.501.txt,2020,7 Conclusions,"in addition, the findings may generalize to other tasks, e.g., corpus-level distantly-supervised relation extraction."
2020.acl-main.502.txt,2020,6 Conclusion,"through scde, we aim to encourage the development of more advanced language understanding models."
2020.acl-main.503.txt,2020,6 Discussion,"models trained on standard natural language inference datasets (bowman et al., 2015) generalize poorly to other distributions (thorne et al., 2018; naik et al., 2018)."
2020.acl-main.504.txt,2020,6 Conclusions and Future Work,"in future work, we plan to explore techniques to automatically learn where to place intermediate classifiers, and what drop ratio to use for each one of them."
2020.acl-main.505.txt,2020,4 Conclusion,we will evaluate our approach on other machine comprehension tasks using dialogues as evidence documents to further verify the generalizability of this work.
2020.acl-main.506.txt,2020,6 Conclusion,we hope that this work provides a complementary picture of hypothesis assessment techniques for the field and encourages more rigorous reporting trends.
2020.acl-main.511.txt,2020,6 Conclusion,"a second field of application is debate systems, where a dataset can be of use for training a system to formulate new arguments."
2020.acl-main.512.txt,2020,5 Conclusion,"apart from that, we also plan to design novel models to perform the related tasks of entity extraction and aspect extraction from comparative sentences."
2020.acl-main.512.txt,2020,5 Conclusion,our future work aims to improve the cpc performance further.
2020.acl-main.514.txt,2020,6 Conclusions,exploring the space of subsets of our preprocessing factors might yield more interesting combinations; we leave this for future work.
2020.acl-main.515.txt,2020,5 Conclusion and Future Work,"although conkadi has achieved a notable performance, there is still much room to improve.1) while ats2smmi is behind our conkadi, we find mmi can effectively enhance the ats2s; hence, in the future, we plan to verify the feasibility of the re-ranking technique for knowledge-aware models.2) we will continue to promote the integration of high-quality knowledge, including more types of knowledge and a more natural integration method."
2020.acl-main.516.txt,2020,5 Conclusion and Future Work,"in the future, we plan to extend our approach to improve the consistency of multi-turn dialogues."
2020.acl-main.519.txt,2020,6 Conclusion,"in the future, we would like to explore variants of the model architecture."
2020.acl-main.523.txt,2020,4 Conclusion,"using a larger amount of general domain texts to build pre-trained representations (peters et al., 2018; radford et al., 2018; devlin et al., 2019; clark et al., 2020) can complement with our model and is one of the directions that we plan to take in future work."
2020.acl-main.53.txt,2020,7 Conclusion,"from this result, we propose that tackling dst with our proposed problem definition is a promising future research direction."
2020.acl-main.530.txt,2020,5 Conclusions,"in future work, we plan to investigate translation of other discourse phenomena that may benefit from the use of future context."
2020.acl-main.532.txt,2020,4 Discussions,"for future work, following the work on automatic identification of translationese (rabinovich and wintner, 2015; rubino et al., 2016), we plan to investigate the impact of tagging translationese texts inside parallel training data, such as parallel sentences collected from the web."
2020.acl-main.534.txt,2020,5 Conclusion,"in this paper, we aim to evaluate document influence from a fine-grained level by additionally considering word semantic shifts."
2020.acl-main.535.txt,2020,5 Conclusion,"in future work, we intend to adapt our editor module for other learning tasks with both the structured input and structured output."
2020.acl-main.535.txt,2020,5 Conclusion,this editor learns how to extract edits from a paraphrase pair and also when and how to apply these edits to a new input sentence.
2020.acl-main.537.txt,2020,6 Future work,"these promising results point to future works in (1) linearizing the speed-speedup curve; (2) extending this approach to other pre-training architectures such as xlnet (yang et al., 2019) and elmo (peters et al., 2018); (3) applying fastbert on a wider range of nlp tasks, such as named entity recognition and machine translation."
2020.acl-main.538.txt,2020,4 Conclusion and Future Work,"in the future, evaluation by automatically executing generated code with test cases could be a better way to assess code generation results."
2020.acl-main.538.txt,2020,4 Conclusion and Future Work,"it will also likely be useful to generalize our re-sampling procedures to zero-shot scenarios, where a programmer writes a library and documents it, but nobody has used it yet."
2020.acl-main.540.txt,2020,6 Conclusion and Future Work,"in the future, we will try to increase the robustness gains of adversarial training and consider utilizing sememes in adversarial defense model."
2020.acl-main.542.txt,2020,6 Conclusion,"in the future, we look forward to extend cl strategy to the pretraining stage, and guide deep models like transformer from a language beginner to a language expert."
2020.acl-main.543.txt,2020,6 Conclusion,we hope that our work will be useful in future research for realizing more advanced models that are capable of appropriately performing arbitrary inferences.
2020.acl-main.545.txt,2020,6 Conclusion and Future Work,"in the future, we will explore more diverse and advanced paraphrase expanding methods for both sentence and paragraph level qg."
2020.acl-main.545.txt,2020,6 Conclusion and Future Work,"moreover, we will apply our methods to other similar tasks, such as sentence simplification."
2020.acl-main.546.txt,2020,6 Conclusions,"for future works, to further improve the method, we will explore the introduction of additional information, such as rules and external texts."
2020.acl-main.549.txt,2020,7 Conclusion,"a potential solution is to jointly learn evidence selection and claim verification model, which we leave as a future work."
2020.acl-main.549.txt,2020,7 Conclusion,evidence selection is an important component of fact checking as finding irrelevant evidence may lead to different predictions.
2020.acl-main.55.txt,2020,5 Conclusion,"in the future, we will provide labels that indicate “why this candidate is false” for false candidates in our test set, so that one can easily detect weak points of systems through error analysis."
2020.acl-main.550.txt,2020,8 Conclusion and Future Work,"in future work, we will consider introducing more information like the citation texts to the cited paper in other papers to help the generation."
2020.acl-main.553.txt,2020,6 Conclusion,it is also convenient to adapt our singledocument graph to multi-document with document nodes.
2020.acl-main.553.txt,2020,6 Conclusion,the introduction of more fine-grained semantic units in the summarization graph helps our model to build more complex relationships between sentences .
2020.acl-main.555.txt,2020,5 Conclusion,"in the future we would like to explore other more informative graph representations such as knowledge graphs, and apply them to further improve the summary quality."
2020.acl-main.556.txt,2020,5 Conclusion and Future Work,"in the future, we will introduce more tasks like document ranking to supervise the learning of the multi-granularity representations for further improvement."
2020.acl-main.558.txt,2020,5 Conclusions and future work,"this dataset, named mlqe, has been released to the research community3 and will be used for the wmt20 shared task on quality estimation.4 in future work, we will test the partial input hypothesis on this data."
2020.acl-main.558.txt,2020,5 Conclusions and future work,we hope it will be useful for general research in qe towards more reliable models.
2020.acl-main.559.txt,2020,8 Conclusions,we hope that the paradigm presented here will help provide coherence to such efforts.
2020.acl-main.560.txt,2020,6 Conclusion,pertinent questions should be posed to authors of future publications about whether their proposed language technologies extend to other languages.
2020.acl-main.561.txt,2020,6 Conclusions,"but of one thing we can be certain: the immense success of adapting deep learning architectures to fit with our computational-linguistic understanding of the nature of language will doubtless continue, with greater insights for both natural language processing and machine learning."
2020.acl-main.561.txt,2020,6 Conclusions,this analysis suggests that an important next step in deep learning architectures for natural language understanding will be the induction of entities.
2020.acl-main.562.txt,2020,7 The Final Frontier – An Outlook,"we would like to see them become true enablers instead, allowing queries to go far beyond of what a corpus has to offer with its bare annotations alone and for example include the following extensions to create more informed search solutions: • use knowledge bases and similar external resources to allow more generalized queries, e.g.“find verbal constructions containing a preposition in combination with some sort of furniture”.• add (semantic) similarity measures (e.g.word embeddings) and other approaches for increased fuzziness to improve example-based search.• offer true scripting support for users to extent or customize the ability provided by a system."
2020.acl-main.563.txt,2020,6 Conclusion,we will explore it in the future.
2020.acl-main.564.txt,2020,5 Conclusion,"future work will investigate other data manipulation techniques (e.g., data synthesis), which can be further integrated to improve the performance."
2020.acl-main.565.txt,2020,5 Conclusion,"besides, our model can quickly adapt to a new domain with little annotated data."
2020.acl-main.567.txt,2020,8 Conclusions and Future Work,designing a new model to address these problems may be our future work.
2020.acl-main.567.txt,2020,8 Conclusions and Future Work,"we believe that sas provides promising potential extensions, such as adapting our model on other tasks where are troubled by excessive information."
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,"recently, zhang et al.(2020) uses bert (devlin et al., 2019) to evaluate generated candidate sentences by comparing reference sentence."
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,there are several future directions to improve ssrem.
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,we will apply ssrem to various conversation tasks for evaluating the generated text automatically.
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,we will explore these directions in our future work.
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,"we will use ranking loss (wang et al., 2014; schroff et al., 2015) to learn the difference among samples."
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,we will use the contextual embedding to represent utterances.
2020.acl-main.569.txt,2020,5 Conclusion,"in the future work, we will focus on more effective discourse parsing with additional carefully designed features and joint learning with edu segmentation."
2020.acl-main.57.txt,2020,5 Conclusion and Future Work,"in the future, we plan to use more powerful encoders and evaluate our methods on real dialog data."
2020.acl-main.570.txt,2020,7 Conclusion and Future Work,future work aims at enhancing sequence feature extraction methods to improve the classification performance as those suffer from low accuracy.
2020.acl-main.572.txt,2020,5 Conclusion and Future Work,"next, we are considering to use this framework to conduct kg entity type noise detection."
2020.acl-main.572.txt,2020,5 Conclusion and Future Work,our modeling method is general and should apply to other typeoriented tasks.
2020.acl-main.573.txt,2020,5 Conclusion and Future Work,"for future work, how to combine open relation learning and continual relation learning together to complete the pipeline for emerging relations still remains a problem, and we will continue to work on it."
2020.acl-main.576.txt,2020,6 Conclusion and Future Work,"in the future, we should further leverage the internal relations in the candidate end, and try to introduce rich medical background knowledge into our work."
2020.acl-main.58.txt,2020,5 Conclusion,we hope to provide new guidance for the future slot tagging work.
2020.acl-main.580.txt,2020,8 Conclusion and Future Work,"in future work, we hope to tackle repeated fields and learn domainspecific candidate generators."
2020.acl-main.580.txt,2020,8 Conclusion and Future Work,"we are also actively investigating how our learned candidate representations can be used for transfer learning to a new domain and, ultimately, in a few-shot setting."
2020.acl-main.583.txt,2020,7 Conclusions and Future Work,future work should study this additional relation in the context of caption annotation and generation.
2020.acl-main.583.txt,2020,7 Conclusions and Future Work,the presented work has limitations that can be addressed in future research.
2020.acl-main.584.txt,2020,5 Discussion and Conclusion,"future work should look at more complex fusion strategies, possibly coupled with bottom-up recalibration mechanisms (zarrieß and schlangen, 2016; mojsilovic, 2005) to further enhance colour classification under difficult illumination conditions."
2020.acl-main.585.txt,2020,5 Conclusion,the effectiveness of vslnet (and even vslbase) suggest that it is promising to explore span-based qa framework to address nlvl problems.
2020.acl-main.588.txt,2020,6 Conclusion,"in future work, we can further improve our method in the following aspects."
2020.acl-main.588.txt,2020,6 Conclusion,"second and last, domain-specific knowledge can be incorporated into our method as an external learning source."
2020.acl-main.588.txt,2020,6 Conclusion,we plan to employ an edgeaware graph neural network considering the edge labels.
2020.acl-main.589.txt,2020,6 Conclusion,"in the future, we would like to extend our work to make a syntactically-aware window that can automatically learn tree (or phrase) structures."
2020.acl-main.59.txt,2020,7 Conclusion,"as future work, we will apply madpl in the more complex dialogs and verify the role-aware reward decomposition in other dialog scenarios."
2020.acl-main.594.txt,2020,7 Conclusion,"the pattern of training robust systems on data that has been augmented by the knowledge captured in symbolic systems could be applied to areas outside of morphological analysis, and is a promising avenue of future exploration."
2020.acl-main.596.txt,2020,7 Conclusion and Future Work,future work will aim to extend the current model to capture particularly challenging morphological patterns such as templatic non-concatenative morphology and polysynthetic composition.
2020.acl-main.596.txt,2020,7 Conclusion and Future Work,"our next step will be to attempt to automate the determination of language typology, yielding somewhat better performance with a system requiring no human intervention per language at all."
2020.acl-main.598.txt,2020,6 Conclusion,"by substituting the current transducers in our pipeline, we expect that we will be able to improve the overall performance of our system."
2020.acl-main.598.txt,2020,6 Conclusion,"in the future, we will explore the following directions: (i) a difficult challenge for our proposed system is to correctly determine the paradigm size."
2020.acl-main.598.txt,2020,6 Conclusion,"since transfer across related languages has shown to be beneficial for morphological tasks (jin and kann, 2017; mccarthy et al., 2019; anastasopoulos and neubig, 2019, inter alia), future work could use typologically aware priors to guide the number of paradigm slots based on the relationships between languages.(ii) we plan to explore other methods, like word embeddings, to incorporate context information into our feature function.(iii) we aim at developing better performing string transduction models for the morphological inflection step."
2020.acl-main.599.txt,2020,6 Conclusion,improving our graph structure of representing the document as well as the document-level pretraining tasks is our future research goals.
2020.acl-main.6.txt,2020,5 Conclusion,"in future work, we plan to analyze each turn of dialogue with reinforcement learning architecture, and to enhance the diversity of the whole dialogue by avoiding knowledge reuse."
2020.acl-main.601.txt,2020,6 Conclusions and Future Works,we will investigate the robustness and scalability of the model.
2020.acl-main.602.txt,2020,6 Conclusions,"for modeling, we plan to explore recent advances in conditional language models for jointly modeling qa with generating their derivations."
2020.acl-main.602.txt,2020,6 Conclusions,one immediate future work is to evaluate state-of-the-art rc systems’ internal reasoning on our dataset.
2020.acl-main.603.txt,2020,6 Conclusion,we also add a recurrent mechanism to allow the information to flow across segments so that the model could have knowledge beyond the current segment when selecting answers.
2020.acl-main.606.txt,2020,7 Conclusion and Future Work,"future research may involve tailoring existing parsers to learner data, combining literal and intended meanings in a unified framework, evaluating gec models in terms of speakers’ intention and parsing for other languages."
2020.acl-main.607.txt,2020,6 Conclusion,this points to future directions of applying our model to low-resource languages and cross-domain settings.
2020.acl-main.611.txt,2020,6 Conclusion and Future Work,we leave adjusting our model to different kinds of lattice or graph as our future work.
2020.acl-main.612.txt,2020,6 Conclusion,"for the future work, we are planning to extract fine-grained semantic types from unlabelled documents and use the relatedness between the finegrained types and contexts as distant supervision for entity linking."
2020.acl-main.616.txt,2020,6 Conclusion and Future Work,"in the future, it is necessary to interpret the semantics that transformer layers in different depths can convey, which is beneficial for the computing-efficiency."
2020.acl-main.617.txt,2020,6 Conclusion,"atth learns embeddings with trainable hyperbolic curvatures, allowing it to learn the right geometry for each relationship and generalize across multiple embedding dimensions."
2020.acl-main.617.txt,2020,6 Conclusion,"future directions for this work include exploring other tasks that might benefit from hyperbolic geometry, such as hypernym detection."
2020.acl-main.617.txt,2020,6 Conclusion,the proposed attention-based transformations can also be extended to other geometric operations.
2020.acl-main.618.txt,2020,4 Conclusion and Future Work,"one particularly exciting direction is the application of our classification-based self-learning framework on top of the most recent methods that induce bilingual spaces via non-linear alignments (glavasˇ and vulic´, 2020; mohiuddin and joty, 2020)."
2020.acl-main.618.txt,2020,4 Conclusion and Future Work,"this proof-of-concept work opens up a wide spectrum of interesting avenues for future research, including the use of more powerful classifiers, more sophisticated features (e.g., character-level transformers), and fine-grained linguistic analyses on the importance of disparate features over different language pairs."
2020.acl-main.62.txt,2020,6 Conclusions,"for future work, we will explore the scenarios that annotations are absent for all expert dialogues."
2020.acl-main.620.txt,2020,6 Conclusion and Future Work,another promising direction is to design more powerful training strategies to replace the baby step.
2020.acl-main.620.txt,2020,6 Conclusion and Future Work,"as our model is not limited to machine translation, it is interesting to validate the proposed framework into other nlp tasks that need to exploit cl."
2020.acl-main.622.txt,2020,6 Conclusion,"in future work, we will explore novel approaches to generate the questions based on each mention, and evaluate the influence of different question generation methods on the coreference resolution task."
2020.acl-main.623.txt,2020,8 Conclusions and Future Work,"future work would include a comparison with other, more complex, methods for uncertainty estimation, incorporating uncertainty to affect model decisions over time, and further investigating links between uncertainty values and linguistic features of the input."
2020.acl-main.624.txt,2020,6 Conclusion,"in the future, we want to investigate more powerful recommenders, combine interactive entity linking with knowledge base completion and use online learning to leverage deep models, despite their long training time."
2020.acl-main.629.txt,2020,6 Conclusions and Future work,"despite the promising results, the accuracy of our approach could probably be boosted further by experimenting with new feature information and specifically tuning hyper-parameters for the sdp task, as well as using different enhancements such as implementing the hierarchical decoding recently presented by liu et al.(2019), including contextual string embeddings (akbik et al., 2018) like he and choi (2019), or applying multi-task learning across the three formalisms like peng et al.(2017)."
2020.acl-main.630.txt,2020,6 Conclusion,"another research direction is to investigate if introducing more sophisticated topic models, such as named entity promoting topic models (krasnashchok and jouili, 2018) into the proposed framework can further improve results."
2020.acl-main.630.txt,2020,6 Conclusion,future work may focus on how to directly induce topic information into bert without corrupting pretrained information and whether combining topics with other pretrained contextual models can lead to similar gains.
2020.acl-main.631.txt,2020,6 Conclusion,"moreover, the proposed augmentation method tends not to be unique to the current task and could be applied to other low-resource sequence labeling tasks such as chunking and named entity recognition."
2020.acl-main.632.txt,2020,6 Conclusion,"in the future, we plan to consider the ethos mode of persuasion by exploring how debaters strengthen their credibility in debates."
2020.acl-main.633.txt,2020,7 Conclusions,"future research may focus on the motivation we described, but may also utilize the large speeches corpus we release as part of this work to a variety of additional different endeavors."
2020.acl-main.634.txt,2020,7 Conclusion and Future Work,"the model can be potentially improved by filtering the corpus according to different domains, or augmenting with a retrieve-and-rewrite mechanism, which we leave for future work."
2020.acl-main.636.txt,2020,6 Conclusion,"in future work, we intend to explore more with the combination of rl and dst on the basis of reward designing, trying to explore more in the internal mechanism."
2020.acl-main.636.txt,2020,6 Conclusion,"in the long run, we are interested in combing many tasks into one learning process with meta-learning."
2020.acl-main.639.txt,2020,5 Conclusion,"in the future, we plan to adapt variational neural network to refine our style transfer model, which has shown effectiveness in other conditional text generation tasks, such as machine translation (zhang et al., 2016; su et al., 2018)."
2020.acl-main.640.txt,2020,6 Conclusion,"on the other hand, we would also like to investigate how to make use of our proposed model to solve sequence-to-sequence tasks."
2020.acl-main.640.txt,2020,6 Conclusion,one is to investigate how the other graph models can benefit from our proposed heterogeneous mechanism.
2020.acl-main.640.txt,2020,6 Conclusion,there are two directions for future works.
2020.acl-main.642.txt,2020,5 Conclusion,we will explore more complicated object relation modeling in future work.
2020.acl-main.643.txt,2020,6 Conclusions,a future research direction is to combine rgcs with distant supervision by an external knowledge base to answer the visual questions that need external knowledge; for example which animal in this photo can climb a tree?
2020.acl-main.645.txt,2020,8 Conclusion,the question of knowing whether pretraining on small domain specific content will be a better option than transfer learning techniques such as fine-tuning remains open and we leave it for future work.
2020.acl-main.645.txt,2020,8 Conclusion,this paves the way for the rise of monolingual contextual pre-trained language-models for under-resourced languages.
2020.acl-main.647.txt,2020,8 Conclusion,we aim to explore those directions in a future work.
2020.acl-main.648.txt,2020,5 Takeaways and Open Questions,we leave some open questions to explore: • how can we exploit subword variations to reduce skewness in the nlu tasks?• would subword-segmentation-transfer be helpful for other nmt-nlu task pairs like we did for 2kenize (script conversion) to 1kenize (classification)?
2020.acl-main.649.txt,2020,8 Conclusion,"in future work, we intend to further fine-tune our methodological apparatus for tackling mfep."
2020.acl-main.65.txt,2020,7 Conclusion,"in future work, we plan to seek better ways to guide the learning of latent variables, such as using dynamic routing (sabour et al., 2017) method to align the latent variables and sememes, and learn more explainable latent codes."
2020.acl-main.650.txt,2020,8 Conclusion,"the techniques developed here readily apply to other types of normalization data (e.g.informal, dialectal)."
2020.acl-main.651.txt,2020,5 Conclusion and Future Work,"we hope that this dataset will encourage research into clarification question generation and, in the long run, enhance dialog and question-answering systems."
2020.acl-main.652.txt,2020,8 Conclusion and Future Work,"for the future, we would like to exploit the abstractive answers in our dataset, explore more sophisticated systems in both scenarios and perform user studies to study how real users interact with a conversational qa system when accessing faqs."
2020.acl-main.654.txt,2020,6 Conclusion,"in future work, we explore to extend this approach for other low resource tasks in nlp."
2020.acl-main.655.txt,2020,7 Conclusion,"in the future, we will further study this properties of kernel-based attentions in neural networks, both in the effectiveness front and also the explainability front."
2020.acl-main.656.txt,2020,7 Conclusions,"for future work, an obvious next step is to investigate the possibility of generating veracity explanations from evidence pages crawled from the web."
2020.acl-main.657.txt,2020,7 Conclusion & Future Work,"as future work, we will explore different heuristics for navigating in the premises graph, as researched before for textual entailment (silva et al., 2019, 2018) and selective reasoning (freitas et al., 2014)."
2020.acl-main.660.txt,2020,7 Discussion and Conclusion,"this type of research needs to be extended to the investigation of multiple tasks, multiple languages, and multiple possible pre-training regimes (words, chars, morphemes, lattices) in order to investigate whether this trend extends to other languages and tasks."
2020.acl-main.661.txt,2020,7 Conclusion,we exposed a significant space of both modeling ideas and application-specific requirements left to be addressed in future research.
2020.acl-main.662.txt,2020,5 A Call to Action,"finally, if you want to make human–computer comparisons, pick the right humans."
2020.acl-main.662.txt,2020,5 A Call to Action,"moreover, another lesson the qa community could learn from trivia games is to turn it into a spectacle: exciting games with a telegenic host."
2020.acl-main.662.txt,2020,5 A Call to Action,these skills are exactly those we want computers to develop.
2020.acl-main.663.txt,2020,6 Conclusion,"however, there is a trade-off between expressiveness and learnability: the more structure we add, the more difficult it can be to work with our representations."
2020.acl-main.663.txt,2020,6 Conclusion,"my own recent work in this direction has been to develop the pixie autoencoder (emerson, 2020a), and i look forward to seeing alternative approaches from other authors, as the field of distributional semantics continues to grow."
2020.acl-main.663.txt,2020,6 Conclusion,"to this end, there are promising neural architectures for working with structured data, such dependency graphs (for example: marcheggiani and titov, 2017) or logical propositions (for example: rockta¨schel and riedel, 2017; minervini et al., 2018)."
2020.acl-main.664.txt,2020,5 Conclusions,in the near future we plan to extend the proposed approach to several other language-vision modeling tasks.
2020.acl-main.665.txt,2020,4 Conclusion,another avenue of research would be to investigate the role of synthetic data in surface realization in other languages.
2020.acl-main.666.txt,2020,5 Conclusions,"as future work, we plan to further evaluate the impact of different sequential architectures, longer contexts, alternative sentence embeddings, and cleverer selection of distractors."
2020.acl-main.666.txt,2020,5 Conclusions,"inspired by deliberation networks and automatic post editing methods (xia et al., 2017; freitag et al., 2019), we ultimately want to apply our model to two-step generation, first selecting a sentence from a large set before refining it to fit the context."
2020.acl-main.667.txt,2020,4 Conclusion,we hope that this work can shed some light and inspire future work at this line of research.
2020.acl-main.668.txt,2020,9 Conclusion,future work will explore the use of domain adaptation techniques to enhance performance where the domains of the ci and event text differ substantially.
2020.acl-main.669.txt,2020,5 Conclusion,"we also plan to use different types of labelled data, e.g., domain specific data sets, to ascertain whether entity type information is more discriminative in sub-languages."
2020.acl-main.67.txt,2020,7 Conclusion and Future Work,"also, we plan to use large-scale unlabeled data to improve the performance further."
2020.acl-main.67.txt,2020,7 Conclusion and Future Work,"furthermore, our framework can be efficiently applied to other graph-to-sequence tasks such as webnlg (gardent et al., 2017) and syntax-based neural machine translation (bastings et al., 2017)."
2020.acl-main.67.txt,2020,7 Conclusion and Future Work,in future work we would like to do several experiments on other related tasks to test the versatility of our framework.
2020.acl-main.671.txt,2020,6 Conclusion,"furthermore, we seek to investigate the transferability of the obtained inductive bias to other commonsense-demanding downstream tasks, which are distinct from the winograd-structure."
2020.acl-main.671.txt,2020,6 Conclusion,"therefore, future work will aim at relaxing the prior of winograd-structured twin-question pairs."
2020.acl-main.671.txt,2020,6 Conclusion,"we believe in order to solve commonsense reasoning truly, algorithms should refrain from using labeled data, instead exploit the structure of the task itself."
2020.acl-main.673.txt,2020,6 Conclusions,"for future work, our model can be extended to disentangled representation learning with non-categorical style labels, and applied to zero-shot style transfer with newly-coming unseen styles."
2020.acl-main.675.txt,2020,4 Conclusion,"first, we will explore mechanisms for instance-specific translation that are more sophisticated than the aggregation of translation vectors of nearest dictionary neighbours."
2020.acl-main.675.txt,2020,4 Conclusion,"second, we plan to couple instance-based mapping with other informative features (e.g., character-level features) in classification-based bli frameworks (heyman et al., 2017; karan et al., 2020)."
2020.acl-main.675.txt,2020,4 Conclusion,we plan to extend this work in two directions.
2020.acl-main.676.txt,2020,7 Discussion,"the procedure detailed in this paper relies on exact string matching to identify common context; future work might take advantage of learned representations of spans and their environments (mikolov et al., 2013; peters et al., 2018)."
2020.acl-main.678.txt,2020,5 Conclusion,the proposed method may be an important module for future applications related to time.
2020.acl-main.680.txt,2020,7 Conclusions,"we expect our data, results, and error analysis to inform the design of similar experimental setups for other nlp tasks beyond ner, such as part-of-speech tagging or relation extraction."
2020.acl-main.681.txt,2020,5 Conclusion,"future directions to explore include incorporating noise-robust training procedures (goldberger and ben-reuven, 2017) and example weighting (dehghani et al., 2018) during self-training, and exploring lexical alignment methods from literature on learning cross-lingual embeddings."
2020.acl-main.682.txt,2020,7 Conclusions & Future Work,"however, the same framework could be applied to other multi-modal tasks."
2020.acl-main.682.txt,2020,7 Conclusions & Future Work,we hope that grolla and the compguesswhat?!data will encourage the implementation of learning mechanisms that fuse taskspecific representations with more abstract representations to encode attributes in a more compositional manner.
2020.acl-main.684.txt,2020,6 Conclusion,"future work can also explore curriculum learning in this domain, by first learning simpler tasks, which can be compositionally invoked in explanations for complex tasks."
2020.acl-main.684.txt,2020,6 Conclusion,"here, we posed the learning of web-based tasks as similar to instruction-following problem, with no aspect of interactivity or exploration of the environment."
2020.acl-main.684.txt,2020,6 Conclusion,"in future work, the possibility of learning from a mix of explanations, exploration and a limited budget of interaction with the environment can be explored."
2020.acl-main.687.txt,2020,8 Conclusion,our work provides a foundation for future work into simpler and more computationally efficient neural machine translation.
2020.acl-main.688.txt,2020,7 Conclusion,"therefore, a promising research direction to investigate would involve the development and assessment of improved initialisation methods that would more efficiently yield the benefits of the model transfer."
2020.acl-main.692.txt,2020,6 Conclusions and Future Work,"another interesting avenue is applying this to unsupervised nmt, which is highly sensitive to domain mismatch (marchisio et al., 2020; kim et al., 2020)."
2020.acl-main.692.txt,2020,6 Conclusions and Future Work,"this work just scratches the surface with what can be done on the subject; possible avenues for future work include extending this with multilingual data selection and multilingual lms (conneau and lample, 2019; conneau et al., 2019; wu et al., 2019; hu et al., 2020), using such selection methods with domain-curriculum training (zhang et al., 2019; wang et al., 2019b), applying them on noisy, web-crawled data (junczys-dowmunt, 2018) or for additional tasks (gururangan et al., 2020)."
2020.acl-main.692.txt,2020,6 Conclusions and Future Work,"we hope this work will encourage more research on finding the right data for the task, towards more efficient and robust nlp."
2020.acl-main.693.txt,2020,4 Conclusions and future work,"while the scope of this work does not extend to sampling sentences given document context, this would be an interesting direction for future work."
2020.acl-main.694.txt,2020,4 Discussions and Conclusions,"also, we plan to consider more structured latent variables beyond modeling the sentence-level variation as well as to apply our vnmt model to more language pairs."
2020.acl-main.694.txt,2020,4 Discussions and Conclusions,we plan to conduct a more in-depth investigation into actual multimodality condition with high-coverage sets of plausible translations.
2020.acl-main.695.txt,2020,6 Conclusion,"we believe our benchmark system represents a reasonable approach to solving the problem based on past work and highlights many directions for improvement, e.g.joint modeling and making better use of distributional semantic information."
2020.acl-main.699.txt,2020,4 Conclusions,future work includes a deeper qualitative analysis of which (type of) papers are being cited; a more fine-grained analysis of different research topics in nlp to determine whether changes are more prevalent within certain areas than others; or extending the analysis to a larger set of the papers in the acl anthology.
2020.acl-main.70.txt,2020,6 Conclusion,"more importantly, osdm tried to incorporate semantic information in the proposed graphical representation model to remove the term ambiguity problem in short-text clustering."
2020.acl-main.703.txt,2020,8 Conclusions,"future work should explore new methods for corrupting documents for pretraining, perhaps tailoring them to specific end tasks."
2020.acl-main.704.txt,2020,7 Conclusion,"future research directions include multilingual nlg evaluation, and hybrid methods involving both humans and classifiers."
2020.acl-main.705.txt,2020,5 Conclusion,"for future work, we will explore the extension of conditional mlm to multimodal input such as image captioning."
2020.acl-main.706.txt,2020,7 Conclusions and Future Directions,we hope that the dataset we collected will facilitate research in using natural language for physical reasoning.
2020.acl-main.707.txt,2020,5 Conclusion,"in future work, we plan to add paraphrase generation to generate diverse simple sentences."
2020.acl-main.708.txt,2020,8 Conclusion,"there are still some unsolved problems for logical nlg, e.g.how to improve the quality of automatic metrics to better help human automatically judge models’ performances."
2020.acl-main.708.txt,2020,8 Conclusion,"to promote the research in this direction, we host a logicnlg challenge2 to help better benchmark the current progress."
2020.acl-main.710.txt,2020,7 Conclusion and Future Work,"in future work, we plan to investigate how target phrase order affects the generation behavior, and further explore set generation in an order invariant fashion."
2020.acl-main.711.txt,2020,8 Conclusion,"language models conflate the two, so developing methods that are nuanced enough to recognize this difference is key to future progress."
2020.acl-main.713.txt,2020,6 Conclusions and Future Work,"in the future, we plan to incorporate more comprehensive event schemas that are automatically induced from multilingual multimedia data and external knowledge to further improve the quality of ie."
2020.acl-main.713.txt,2020,6 Conclusions and Future Work,we also plan to extend our framework to more ie subtasks such as document-level entity coreference resolution and event coreference resolution.
2020.acl-main.714.txt,2020,6 Conclusion and Future Work,"in the future work, it would be interesting to further explore how the model can be adapted to jointly extract role fillers, tackles coreferential mentions and constructing event templates."
2020.acl-main.715.txt,2020,5 Conclusion,"in the future, we plan to apply ceon-lstm to other related nlp tasks (e.g., event extraction, semantic role labeling) (nguyen et al., 2016a; nguyen and grishman, 2018a)."
2020.acl-main.716.txt,2020,7 Conclusion and Future Work,"in our ongoing research, we are investigating the expansion of this technique to language pairs where english may not be involved."
2020.acl-main.718.txt,2020,7 Conclusion,we hope that rams will stimulate further work on multi-sentence argument linking.
2020.acl-main.72.txt,2020,6 Conclusion,our systematic study will pave the way to future research about the effective construction of dictionaries for text analytics.
2020.acl-main.720.txt,2020,6 Conclusions,"in future work, we will explore whether the observed trends hold in much larger polyglot settings, e.g.the wikiann ner corpus (pan et al., 2017b)."
2020.acl-main.720.txt,2020,6 Conclusions,"with this in mind, exploring different training strategies, such as multi-objective optimization, may prove beneficial (sener and koltun, 2018)."
2020.acl-main.721.txt,2020,8 Conclusion,future extensions of this work involve a more general pre-training objective allowing for the learned representations to be useful in many tasks as well as distantly or semi-supervised approaches to benefit from more data.
2020.acl-main.722.txt,2020,6 Conclusion,possible future directions include using more sophisticated feature design and combinations of candidate retrieval methods.
2020.acl-main.724.txt,2020,6 Conclusion,"as future work, we intend to apply cluhtm in other representative applications on the web, such as hierarchical classification by devising a supervised version of cluhtm."
2020.acl-main.724.txt,2020,6 Conclusion,we also intend to incorporate some type of attention mechanism into our methods to better understand which cluwords are more important to define certain topics.
2020.acl-main.725.txt,2020,6 Conclusions,another interesting direction is to generate a class name hierarchy via language model probing.
2020.acl-main.725.txt,2020,6 Conclusions,"for example, we may expand the set {“machine translation”, “information extraction”, “syntactic parsing”} to acquire more nlp task concepts."
2020.acl-main.725.txt,2020,6 Conclusions,"in the future, we plan to expand the method scope from expanding concrete entity sets to more abstract concept sets."
2020.acl-main.726.txt,2020,5 Conclusion,"in our future work, we will consider extending it to graph-based methods such as gcn for graph data, and to generation-based methods such as gan for adversarial learning."
2020.acl-main.728.txt,2020,8 Conclusion and Future Work,"vilbert (lu et al., 2019), lxmert (tan and bansal, 2019), uniter (chen et al., 2019) etc."
2020.acl-main.735.txt,2020,6 Conclusion,"for future work, we plan to apply the same methodology to other nlp tasks."
2020.acl-main.737.txt,2020,7 Conclusion,"another research avenue that could be explored is modeling specific user preferences: since each user likely favors a certain set of character substitutions, allowing user-specific parameters could improve decoding and be useful for authorship attribution."
2020.acl-main.740.txt,2020,7 Conclusion,"our work points to numerous future directions, such as better data selection for tapt, efficient adaptation large pretrained language models to distant domains, and building reusable language models after adaptation."
2020.acl-main.744.txt,2020,5 Discussion & Conclusion,"introducing extra regularization terms to a downstream task have been shown to be beneficial in terms of both output structure consistency and prediction accuracy (e.g., minervini and riedel, 2018; hsu et al., 2018; mehta et al., 2018; du et al., 2019; li et al., 2019)."
2020.acl-main.745.txt,2020,7 Conclusion and Future Work,"finally, to extend tabert to cross-lingual settings with utterances in foreign languages and structured schemas defined in english, we plan to apply more advanced semantic similarity metrics for creating content snapshots."
2020.acl-main.745.txt,2020,7 Conclusion and Future Work,"first, we plan to evaluate tabert on other related tasks involving joint reasoning over textual and tabular data (e.g., table retrieval and table-to-text generation)."
2020.acl-main.745.txt,2020,7 Conclusion and Future Work,"second, following the discussions in § 5, we will explore other table linearization strategies with transformers, improving the quality of pretraining corpora, as well as novel unsupervised objectives."
2020.acl-main.745.txt,2020,7 Conclusion and Future Work,this work also opens up several avenues for future work.
2020.acl-main.746.txt,2020,8 Conclusion,we envision future efforts exploring the interactions between improving the underlying graphstructure prediction and ever-better correlations to human judgements on individual properties.
2020.acl-main.75.txt,2020,5 Conclusions,"moreover, we would love to apply the proposed model to other problems, such as general humor recognition, irony discovery, and sarcasm detection, as the future work."
2020.acl-main.750.txt,2020,8 Conclusions,future work will focus on domain adaptation at the embedding layer.
2020.acl-main.751.txt,2020,6 Conclusions and Future Work,"interesting future work includes applying our techniques to different taxonomies (e.g., biomedical) and training a model for different attributes."
2020.acl-main.754.txt,2020,8 Conclusion,"notably, multidds is not limited to nmt, and future work may consider applications to other multilingual tasks."
2020.acl-main.756.txt,2020,5 Conclusions,"because it is artificial to use synthetic data for training a filter classifier, future work can focus on a better objective that models parallelism more smoothly."
2020.acl-main.756.txt,2020,5 Conclusions,future work also includes extending the method to low-resource languages not covered by multilingual bert.
2020.acl-main.757.txt,2020,4 Conclusions,"in the future, we believe more work on alleviating context control problem has the potential to improve translation performance as quantified in table 3."
2020.acl-main.759.txt,2020,6 Conclusion and Future Work,"one of our next steps is to investigate the impact of tc extraction methods on a corresponding awe system (zhang et al., 2019), which uses the feature values produced by aesrubric to generate formative feedback to guide essay revision."
2020.acl-main.759.txt,2020,6 Conclusion and Future Work,"this leads to an interesting future investigation direction, which is training the aesneural using the gold standard that can be extracted automatically."
2020.acl-main.760.txt,2020,6 Discussion,future work will consider additional methods for integrating ontology structure into representation learning.
2020.acl-main.761.txt,2020,7 Conclusion,"future work in multi-hop reasoning could represent the relation between consecutive pieces of evidence and future work in temporal reasoning could incorporate numerical operations with bert (andor et al., 2019)."
2020.acl-main.764.txt,2020,8 Conclusion and Future Work,we believe that modeling domain shift is a promising future direction to improve performance prediction.
2020.acl-main.764.txt,2020,8 Conclusion and Future Work,"while investigating the systematic implications of model structures or hyperparameters is practically infeasible in this study, we may use additional information such as textual model descriptions for modeling nlp models and training procedures more elaborately in the future."
2020.acl-main.764.txt,2020,8 Conclusion and Future Work,"while this discovery is a promising start, there are still several avenues on improvement in future work."
2020.acl-main.765.txt,2020,6 Conclusion and Future Work,further investigations are thus required to fully understand how narratives can be effectively used in dialogue generation.
2020.acl-main.766.txt,2020,6 Conclusion,"in the future, we will investigate a hub language ranking/selection model a la lin et al.(2019)."
2020.acl-main.767.txt,2020,6 Conclusions,there are several directions for future work including better architecture design for utilizing structured meta-data and replacing the two-stage framework with a multi-task generation model that can jointly identify helpful context for the task and perform corresponding text generation.
2020.acl-main.769.txt,2020,7 Conclusion,"additionally, we extend our methods to combat multiple bias patterns simultaneously."
2020.acl-main.769.txt,2020,7 Conclusion,future work may include developing debiasing strategies that do not require prior knowledge of bias patterns and can automatically identify them.
2020.acl-main.77.txt,2020,6 Conclusion and Future Work,"in the future, we will do more tests and surveys on the improvement of business objectives such as user experience, user engagement and service revenue."
2020.acl-main.770.txt,2020,7 Conclusion,several challenges in this direction of research may include extending the debiasing methods to overcome multiple biases at once or to automatically identify the format of those biases which simulate a setting where the prior knowledge is unavailable.
2020.acl-main.773.txt,2020,7 Conclusion,joint efforts are needed for promoting unbiased models that learn true semantics; and we hope our paper can encourage more work towards this important direction.
2020.acl-main.773.txt,2020,7 Conclusion,"since none of our methods is biastype specific, we believe these results can also be generalized to other similar lexical biases."
2020.acl-main.775.txt,2020,6 Conclusion and Further Directions,we leave it to future work to design a non-projective decoder for joint parsing and headless structure extraction.
2020.acl-main.776.txt,2020,4 Conclusion,"another interesting line of research would be to evaluate the contribution of higher-order features in a cross-lingual setting, leveraging structure learned from larger treebanks to underresourced languages."
2020.acl-main.776.txt,2020,4 Conclusion,our results indicate that even a powerful encoder as bert can still benefit from explicit output structure modelling; this would be interesting to explore in other nlp tasks as well.
2020.acl-main.778.txt,2020,7 Conclusion,another area for future work is to explore what information treebank vectors encode.
2020.acl-main.778.txt,2020,7 Conclusion,"interpolating treebank vectors adds a layer of opacity, and, in future work, it would be interesting to carry out experiments with synthetic data, e. g. varying the number of unknown words, to get a better understanding of what they may be capturing."
2020.acl-main.778.txt,2020,7 Conclusion,"we plan to explore other methods to predict tree-bank vectors, e. g. neural sequence modelling, and to apply our ideas to the related task of language embedding prediction for zero-shot learning."
2020.acl-main.79.txt,2020,5 Conclusion,"some interesting observations include the effects of regions and the sensitivity of gnn-based models, which open potentials for further improvements that we plan to address in our future work."
2020.acl-main.8.txt,2020,7 Conclusions,"in the future, we aim to leverage these pre-trained models to advance sota on downstream conversational tasks, such as knowledge-grounded conversations or question answering."
2020.acl-main.81.txt,2020,5 Conclusions,we leave this direction to future work.
2020.acl-main.82.txt,2020,5 Conclusion,"as future work, we plan to extend soft-masked bert to other problems like grammatical error correction and explore other possibilities of implementing the detection network."
2020.acl-main.82.txt,2020,5 Conclusion,the technique of soft-masking is general and potentially useful in other detection-correction tasks.
2020.acl-main.84.txt,2020,8 Conclusion,"based on these results, we will explore ways to leverage the token assignment to domain adaption and few-shot learning."
2020.acl-main.84.txt,2020,8 Conclusion,we also plan to enhance the annotation process by automatically generating proposals for the nl questions and token assignments and letting the annotators only perform corrections.
2020.acl-main.84.txt,2020,8 Conclusion,we hope that this increases annotation efficiency even more.
2020.acl-main.9.txt,2020,5 Conclusion,"in the future, we will also explore to boost the latent selection policy with reinforcement learning and extend our pre-training to support dialogue generation in other languages."
2020.acl-main.9.txt,2020,5 Conclusion,our work can be potentially improved with more fine-grained latent variables.
2020.acl-main.90.txt,2020,8 Conclusion,incorporating our three-way attentive pooling network into open domain conversational qa systems will be interesting future work.
2020.acl-main.95.txt,2020,8 Conclusion,"another direction for future work would improve few-shot approaches to wsd, which is both important for moving wsd into new domains and for modeling rare senses that naturally have less support in wsd data."
2020.acl-main.95.txt,2020,8 Conclusion,"potential directions include finding ways to obtain more informative training signal from uncommon senses, such as with different approaches to loss reweighting, and exploring the effectiveness of other model architectures on lfs examples."
2020.acl-main.95.txt,2020,8 Conclusion,this leaves better disambiguation of less common senses as the main avenue for future work on wsd.
2020.acl-main.96.txt,2020,6 Conclusion,further we plan to investigate other nlp applications that can benefit from the simple linguistic features introduced here.
2020.acl-main.96.txt,2020,6 Conclusion,"in future, we would like to extend this work for other language pairs."
2020.acl-main.97.txt,2020,5 Conclusion,"in the future, we will extend the proposed framework by considering more context (meta data) information, such as time, storylines, and comment sentiment, to further enrich our explainability."
2020.acl-main.98.txt,2020,6 Conclusion,"the complexity in durecdial makes it a great testbed for more tasks such as knowledge grounded conversation (ghazvininejad et al., 2018), domain transfer for dialog modeling, target-guided conversation (tang et al., 2019a) and multi-type dialog modeling (yu et al., 2017)."
2020.acl-main.98.txt,2020,6 Conclusion,the study of these tasks will be left as the future work.
2020.acl-main.99.txt,2020,6 Conclusion,"in future work, we plan to conduct more empirical studies on seg and further improve its performance on new intent identification."
2020.acl-main.99.txt,2020,6 Conclusion,we also plan to conduct more case studies in applying seg to boost the performance of current zero-shot intent classification methods.
2020.acl-srw.1.txt,2020,6 Conclusion,"in this work, we extend adaptive approaches to visiolinguistic tasks to understand more about attention and adaptive mechanisms."
2020.acl-srw.1.txt,2020,6 Conclusion,"while the empirical results are encouraging, important future work includes explorations of higher efficient adaptive and sparse mechanisms that can significantly cause flops and parameter reduction with minimal loss in performance."
2020.acl-srw.10.txt,2020,6 Conclusion,"further extensions may include studying the behavior of more powerful subword combination strategies (e.g.convolutions, self-attention) and the application of subword merging to the target side."
2020.acl-srw.10.txt,2020,6 Conclusion,"future extensions to this work may include applying it to character-level instead of subword representations, and using it for morphologically richer languages, especially low-resourced agglutinative ones, where our approach, together with the incorporation of linguistic information, may provide larger improvements in translation quality."
2020.acl-srw.11.txt,2020,7 Conclusions and Future Work,"in the future, we intend to use the english translation data of north korean news articles to create an evaluation dataset that considers differences in words, and attempt to develop a translation method using a language model with context, such as bert (devlin et al., 2019)."
2020.acl-srw.14.txt,2020,5 Conclusion,"in the future, we will strengthen tags that contain semantic information to extract keywords for more accurate information, such as disease information, location, and size."
2020.acl-srw.15.txt,2020,6 Conclusions,"in future work, we plan to utilize other word segmentation methods for model training."
2020.acl-srw.15.txt,2020,6 Conclusions,we also plan to combine the proposed multi-task neural model with back-translation method to enhance the ability of the nmt model on target-side language modeling.
2020.acl-srw.16.txt,2020,5 Conclusion,"in the future, we want to extend this method to language features other than words."
2020.acl-srw.17.txt,2020,8 Conclusion and Future Work,"next, we aim to develop an actionable bayesian game-theoretic model for social talk, focusing on decomposing its utility function."
2020.acl-srw.19.txt,2020,7 Conclusion and Future Work,"future work could include finding a way to incorporate other linguistic features like case-markers, gender, number, person, tense, aspect and verb agreement information into the parser."
2020.acl-srw.2.txt,2020,4 Summary,we plan to focus mostly on studying the possible application of gcn in this task.
2020.acl-srw.2.txt,2020,4 Summary,we will perform extensive experiments and report results in future work.
2020.acl-srw.20.txt,2020,6 Conclusion,"this suggests future work to reconsider how to match the training and evaluation to the actual objective of downstream applications, and thus create more reliable evaluation metrics and benchmarks."
2020.acl-srw.22.txt,2020,5 Conclusion & Future Work,"in future, we would like to work on effective techniques to exploit monolingual data and parallel data from other languages together to improve the translation of low-resource languages."
2020.acl-srw.23.txt,2020,5 Conclusions,"further, we plan to use this decoder in an iterative, semi-supervised learning scenario akin to co-training (blum and mitchell, 1998)."
2020.acl-srw.23.txt,2020,5 Conclusions,we plan to include constraints as part of decoding to aid in rule synthesis.
2020.acl-srw.23.txt,2020,5 Conclusions,we suspect that including such validity constraints will further improve the quality of the decoded rules.
2020.acl-srw.24.txt,2020,5 Conclusion,future qualitative work could also suggest further variables whose inclusion would enhance our knowledge of humor perception.
2020.acl-srw.24.txt,2020,5 Conclusion,"this could set a new standard for shared tasks which aim to model humor in future, and could outline a methodology that can be replicated with other cultures and languages."
2020.acl-srw.26.txt,2020,6 Conclusion,"our findings shed light on the importance of modeling points of correspondence, suggesting important future directions for sentence fusion."
2020.acl-srw.28.txt,2020,5 Conclusion,"future work would entail analysing and implementing more detailed underlying morphonological rules, and investigating the cross-over from fsts to neural models."
2020.acl-srw.3.txt,2020,5 Conclusion,our future work will target demonstrating the method on other languages.
2020.acl-srw.3.txt,2020,5 Conclusion,"we also hope to address semantic paraphasia in future work and create, deploy aac systems building on the method proposed in this paper."
2020.acl-srw.30.txt,2020,5 Conclusion,"also, we plan to extend the simple label embedding calculation methods to more sophisticated ones."
2020.acl-srw.30.txt,2020,5 Conclusion,"for future work, we envision to apply our method to other tasks and datasets and investigate the effectiveness."
2020.acl-srw.31.txt,2020,7 Conclusion,"it is also interesting to apply our methods to other languages requiring word segmentation, most notably, chinese."
2020.acl-srw.31.txt,2020,7 Conclusion,"while the focus of this paper was on data construction, developing a higher-quality typo correction system is the future direction to pursue."
2020.acl-srw.32.txt,2020,6 Conclusion and Future Work,"moreover, we must develop a method for more accurately estimating the confidence scores, which is our primary focus in the next step."
2020.acl-srw.34.txt,2020,5 Conclusion,for the future we would like to apply our model on other cross-lingual nlp tasks such as xnli or cross-lingual semantic textual similarity.
2020.acl-srw.35.txt,2020,4 Conclusion,"in future work, we will extend our analysis to cover the more complex constructions mentioned in section 3."
2020.acl-srw.35.txt,2020,4 Conclusion,"we are also considering combining our system with an abduction mechanism that uses large knowledge bases (yoshikawa et al., 2019) for handling commonsense reasoning with external knowledge."
2020.acl-srw.36.txt,2020,6 Conclusion and Future Work,"in the future, we plan to introduce constraints for asymmetric relations as well as extend our proposed method to leverage them."
2020.acl-srw.36.txt,2020,6 Conclusion and Future Work,"moreover, we plan to experiment with adapting our model to a multilingual scenario, to be able to use it in a neural machine translation task."
2020.acl-srw.37.txt,2020,6 Conclusion,"in the future, we plan to experiment with even more challenging language pairs such as japanese–russian and attempt to leverage monolingual corpora belonging to diverse language families.we might be able to identify subtle relationships among languages and approaches to better leverage assisting languages for several nlp tasks."
2020.acl-srw.38.txt,2020,6 Conclusions and Future Work,it is another promising area to be looked upon for reranking.
2020.acl-srw.38.txt,2020,6 Conclusions and Future Work,one can investigate our approach with varying beam sizes and analyzing the effect of length penalty wu et al.(2016) and comparing it with methods such as yang et al.(2018).
2020.acl-srw.38.txt,2020,6 Conclusions and Future Work,we also plan to explore the work by c¸aglar gu¨lc¸ehre et al.(2017) and c¸aglar gu¨lc¸ehre et al.(2015) that introduces language models into the existing neural architecture with methods such as shallow fusion and deep fusion.
2020.acl-srw.39.txt,2020,6 Conclusion,"in the future, it would be interesting to explore weak suervision and other data augmentation techniques to improve models’ robustness further."
2020.acl-srw.39.txt,2020,6 Conclusion,we also motivate the use of manifold mixup for further improvement.
2020.acl-srw.40.txt,2020,4 Discussion and Conclusion,"for future work, we consider conducting the same experiments on cola, a dataset for judging the grammatical acceptability of a sentence (warstadt et al., 2019)."
2020.acl-srw.43.txt,2020,6 Conclusion and Future Work,our future agenda includes further bifurcating and exploring the specific types of victim blaming and the efficacy of the proposed approach on such a multi label classification task.
2020.acl-srw.43.txt,2020,6 Conclusion and Future Work,we anticipate that this study encourages further research on how victims of sexual assault are portrayed on social media.
2020.acl-srw.43.txt,2020,6 Conclusion and Future Work,we plan to explore the different weighting factors for the language modelling loss and classification loss described in section 4 to determine if weighting factors can help customize the auxiliary loss for different tasks.
2020.acl-srw.6.txt,2020,5 Summary,our research aims to apply the current work on transfer learning to new tasks and also find novel methods to obtain better multi-task learning models.
2020.acl-srw.6.txt,2020,5 Summary,transfer learning is a promising area of research for deep neural network based machine learning models.
2020.acl-srw.8.txt,2020,7 Conclusion,"this research aims to transfer word binary attributes (e.g., gender) for applications such as data augmentation of a sentence."
2020.emnlp-main.1.txt,2020,7 Conclusion,"our work could be improved also by including discourse properties (coherence, cohesiveness)."
2020.emnlp-main.1.txt,2020,7 Conclusion,we leave this analysis to future work.
2020.emnlp-main.10.txt,2020,6.5 Future Directions,a future extension could explore more robust techniques for identifying abstract chains which do not make such assumptions.
2020.emnlp-main.10.txt,2020,6.5 Future Directions,extending the proposed approaches for longer chains is an important future direction.
2020.emnlp-main.10.txt,2020,6.5 Future Directions,"nonetheless, a useful future direction is exploring answer prediction and explanation prediction as joint goals, and perhaps they can benefit each other."
2020.emnlp-main.101.txt,2020,6 Concluding Remarks,thus it will be interesting to investigate the performance of rrt in other applications of vae beyond topic modelling.
2020.emnlp-main.103.txt,2020,7 Conclusion,"future work includes using these approaches to induce model structure, develop accurate models with better interpretability, and to apply these approaches in lower data regimes."
2020.emnlp-main.105.txt,2020,6 Conclusion,"this is an interesting avenue for future work, for which zest should also be useful."
2020.emnlp-main.105.txt,2020,6 Conclusion,"to facilitate future work, we make our models, code, and data available at https://allenai.org/data/ zest."
2020.emnlp-main.109.txt,2020,7 Discussion,(2) focusing on one of the most important crises of the future: water;
2020.emnlp-main.109.txt,2020,7 Discussion,• unseen attribution factor: our model can generalize to unseen attributions factors.
2020.emnlp-main.109.txt,2020,7 Discussion,this merits a deeper exploration with a holdout attribution set we aim to investigate in future.• flint water crisis: we were curious to know how our model performs in the wild on a data set of a different water crisis.
2020.emnlp-main.110.txt,2020,6 Discussion,future semeval challenges should consider this when constructing test datasets and mention the hashtags and keywords they use for data collection.
2020.emnlp-main.110.txt,2020,6 Discussion,"in the future, we hope to explore abstract topics like ‘immigration’ where differentiating between direct and indirect stance is non-trivial and ensemble models that combine the strengths of multiple methods."
2020.emnlp-main.110.txt,2020,6 Discussion,"this suggests that future research should explore approaches like coreference resolution (for pronouns), word sense disambiguation (for epithets), and background knowledge (relationships to other entities)."
2020.emnlp-main.111.txt,2020,6 Conclusion,"in the future, we will explore how to apply our model to more domains, and enhance the interpretability of the reasoning path when the model answers questions."
2020.emnlp-main.116.txt,2020,6 Conclusions and Future Work,"in the future, we plan to focus on how to improve the performance of medical entity normalization when resources are limited."
2020.emnlp-main.12.txt,2020,6 Conclusion,"while we leave a detailed study to future work, we expect general trends regarding the value of perturbations to hold broadly."
2020.emnlp-main.120.txt,2020,7 Conclusions,we are excited about future work that could extend our motivation and further aim at incorporating stronger hierarchy into the language model architectures and the pre-training tasks.
2020.emnlp-main.122.txt,2020,6 Conclusion,"in future work, we will explore generalizing this approach to the multilingual setting, or applying it to the pre-train and fine-tune paradigm used widely in other models such as bert."
2020.emnlp-main.123.txt,2020,5 Conclusion,"furthermore, this aligner may help to build or increase semantic resources, using a promising approach as back-translation (sobrevilla cabezudo et al., 2019)."
2020.emnlp-main.123.txt,2020,5 Conclusion,"future work includes adopting multilingual word embeddings (lample et al., 2018) to produce alignments for other languages."
2020.emnlp-main.123.txt,2020,5 Conclusion,"this simple approach may be adopted for other languages with few resources, aiming to get tools for natural language understanding tasks."
2020.emnlp-main.124.txt,2020,5 Conclusions,"in the future, we want to explore semi-supervised methods for sentence embedding and its transferability across domains."
2020.emnlp-main.125.txt,2020,7 Discussion and Future Work,"we intend to expand our method to conduct forest alignments for making it robust against parsing errors, which are inevitable in handling large corpora."
2020.emnlp-main.125.txt,2020,7 Discussion and Future Work,"we plan to apply it to a comparable corpus of partial paraphrases and investigate the performance, with the aim of creating a large-scale syntactic and phrasal paraphrase dataset."
2020.emnlp-main.126.txt,2020,5 Conclusion,"the proposed method can further contribute to other semi-structured data (table, graph, etc.)related tasks, e.g."
2020.emnlp-main.126.txt,2020,5 Conclusion,there still exists plenty of potentials that require future studies in this direction.
2020.emnlp-main.126.txt,2020,5 Conclusion,"wikitablequestions (pasupat and liang, 2015) and commonsenseqa (talmor et al., 2019)."
2020.emnlp-main.128.txt,2020,6 Conclusion and Future Work,"in the future, we would adapt our method to other ie tasks to study its application scope."
2020.emnlp-main.129.txt,2020,7 Conclusion and Future work,"in the future, we will extend maven to more event-related tasks like event argument extraction, event sequencing, etc."
2020.emnlp-main.129.txt,2020,7 Conclusion and Future work,"we also explore some promising directions with analytic experiments, including modeling multiple event correlations (section 5.3), utilizing the hierarchical event schema to distinguish close types (section 5.6), and improving other ed tasks with transfer learning (section 5.5)."
2020.emnlp-main.13.txt,2020,6 Conclusion,"an investigation of how, whether, and why formalisms and their implementations affect probing results for tasks beyond role labeling and for frameworks beyond edge probing constitutes an exciting avenue for future research."
2020.emnlp-main.133.txt,2020,7 Conclusion,another direction is to generalize the way in which the table and sequence interact to other types of representations.
2020.emnlp-main.133.txt,2020,7 Conclusion,"in the future, we would like to investigate how the table representation may be applied to other tasks."
2020.emnlp-main.136.txt,2020,6 Conclusion,"in the future, we will investigate the feasibility of incorporating classical mds guidance to abstractive models with large-scale pre-training (gu et al., 2020) and more challenging settings where each document set may contain hundreds or even thousands of documents."
2020.emnlp-main.137.txt,2020,7 Conclusions and Future Work,another intriguing direction is exploring the connection between our methods and neural network interpretability.
2020.emnlp-main.137.txt,2020,7 Conclusions and Future Work,"finally, although we are motivated primarily by the widespread use of topic models for identifying interpretable topics (boyd-graber et al., 2017, ch.3), we plan to explore the ideas presented here further in the context of downstream applications like document classification."
2020.emnlp-main.137.txt,2020,7 Conclusions and Future Work,"in future work, we also hope to explore the effects of the pretraining corpus (gururangan et al., 2020) and teachers (besides bert) on the generated topics."
2020.emnlp-main.137.txt,2020,7 Conclusions and Future Work,"we believe mining this connection can open up further research avenues; for instance, by investigating the differences in such teacher-topics conditioned on the pre-training corpus."
2020.emnlp-main.138.txt,2020,6 Conclusion,future works could focus on employing the proposed model in more downstream tasks.
2020.emnlp-main.140.txt,2020,6 Conclusions and Future Work,"as a first attempt to introduce macro-level meta-features for strategy selection, we believe there is much potential to refine and improve our approach."
2020.emnlp-main.140.txt,2020,6 Conclusions and Future Work,"furthermore, the smart-kpe framework can be easily adapted to other nlp tasks, and we believe there is much potential in combining smart-kpe with different models to further boost performance on opendomain kpe and other web-related tasks."
2020.emnlp-main.140.txt,2020,6 Conclusions and Future Work,we also plan to add more types of meta-features to generate richer multimodal representations.
2020.emnlp-main.142.txt,2020,7 Conclusion,"in this work, we explore unsupervised disfluency detection by combining self-training and selfsupervised learning."
2020.emnlp-main.147.txt,2020,5 Conclusion,we will explore this direction in future work.
2020.emnlp-main.148.txt,2020,6 Conclusion,"in the future, we will further explore how to explicitly incorporate linguistics information, such as named entities into the latent states."
2020.emnlp-main.149.txt,2020,6 Conclusion and Outlook,"in future work, we will further investigate othercontent generation problems by leveraging multi-granularity copying mechanism."
2020.emnlp-main.15.txt,2020,8 Conclusion,future work will be separated into two strands.
2020.emnlp-main.15.txt,2020,8 Conclusion,"the first will focus on how to better model the distribution of embeddings given a morphosyntactic attribute; as mentioned above, this should yield a better probe overall."
2020.emnlp-main.150.txt,2020,6 Conclusion,"in the future, we will move on to develop a more general dialogue dependency parser and better incorporate dependency information into dialogue context modeling tasks."
2020.emnlp-main.152.txt,2020,4 Conclusion,"in the future, we plan to extend our nonautoregressive refiner to other natural language understanding (nlu) tasks, e.g., named entity recognition (tjong kim sang and de meulder, 2003), semantic role labeling (he et al., 2018), and natural language generation (nlg) tasks, e.g., machine translation (vaswani et al., 2017), summarization (liu and lapata, 2019)."
2020.emnlp-main.153.txt,2020,7 Conclusion,"in future work, we would like to apply our approach on document-level and multi-document nlu tasks."
2020.emnlp-main.154.txt,2020,8 Conclusion,"another possible avenue for future work is to use crows-pairs to help directly debias lms, by in some way minimizing a metric like ours."
2020.emnlp-main.16.txt,2020,7 Future Work & Conclusion,future work might use msgs as a diagnostic tool to measure how effectively new model architectures and selfsupervised pretraining tasks can more efficiently equip neural networks with better inductive biases.
2020.emnlp-main.16.txt,2020,7 Future Work & Conclusion,these models could prove to be a helpful resource for future studies looking to study learning curves of various kinds with respect to the quantity of pretraining data.
2020.emnlp-main.160.txt,2020,4 Discussion,one area for future work would be to better identify and model words that either don’t have a visual grounding or whose identified visual grounding doesn’t align with human expectation.
2020.emnlp-main.161.txt,2020,5 Conclusion,"we consider extension of our model to other videoand-language tasks as future work, as well as developing more well-designed pre-training tasks."
2020.emnlp-main.163.txt,2020,7 Summary Of Exploitable Weaknesses and Defense Directions,"last but not least, didan and neuralnews may be leveraged to supplement fact verification in detecting humanwritten misinformation in general by evaluating visual-semantic consistency."
2020.emnlp-main.163.txt,2020,7 Summary Of Exploitable Weaknesses and Defense Directions,other interesting avenues for future research is to understand the importance of metadata in this multimodal setting and investigating counter-attacks to improved generators that incorporate image-text consistency.
2020.emnlp-main.163.txt,2020,7 Summary Of Exploitable Weaknesses and Defense Directions,"we hope future work will address any potential limitations of this work, such as expanding the dataset to evaluate generalization across different news sources, and a larger variety of neural generators."
2020.emnlp-main.164.txt,2020,6 Conclusion,the performance of softproto can be further improved after introducing the large-scale external unlabeled data like yelp and amazon reviews.
2020.emnlp-main.165.txt,2020,5 Conclusion and Future Work,"as to future work, we plan to explore how to jointly extract entities and relations in federated settings."
2020.emnlp-main.167.txt,2020,7 Conclusions and Future Work,"there are many potential directions for future work on oia, including 1) more labeled data; 2) better learning algorithm; 3) becoming crosslingual by adding support for more natural languages; 4) porting existing oie strategies on oia and evaluating the performance compared with the original ones."
2020.emnlp-main.169.txt,2020,7 Conclusion,"in future work, we would like to investigate methods to stabilize the training of weight tied models and apply our model on other tasks in natural language generation."
2020.emnlp-main.171.txt,2020,8 Conclusions,"we hope that our insights, including some of our negative results, may encourage future research on learning with latent structures."
2020.emnlp-main.172.txt,2020,7 Conclusions,"the results provide new insights into the strengths and weaknesses of english part-of-speech tagging models, complementing other approaches to model comparison and interpretation."
2020.emnlp-main.173.txt,2020,6 Conclusion,"in the future, we are interested in social science topics, such as modeling the causal effect between mental health and the suicide decisions reflected through social media, which may help predict and stop the final decisions."
2020.emnlp-main.174.txt,2020,7 Conclusion,developing methods improving both memory and inference efficiency without sacrificing task performance can open the possibility of widely deploying the powerful pretrained language models to more nlp applications.
2020.emnlp-main.174.txt,2020,7 Conclusion,"future work may explore the possibility of applying masking to the pretrained multilingual encoders like mbert (devlin et al., 2019) and xlm (conneau and lample, 2019)."
2020.emnlp-main.175.txt,2020,7 Conclusion and Future Work,"in the future, we will select context sentences in larger candidate space, and explore more effective ways to extend our approach to select target-side context sentences."
2020.emnlp-main.176.txt,2020,6 Conclusion,"future directions include exploring advanced identification and rejuvenation models that can better reflect the learning abilities of nmt models, as well as validating on other nlp tasks such as dialogue and summarization."
2020.emnlp-main.177.txt,2020,7 Conclusions and Future Work,"in future work, we will explore other such applications of our proposed methods."
2020.emnlp-main.179.txt,2020,7 Conclusion and Future Works,"in the future, the proposed mgl method can potentially applied to more cross-lingual natural language understanding (xlu) tasks (conneau et al., 2018b; wang et al., 2019; lewis et al., 2019; karthikeyan et al., 2020), and be generalized to learn to learn for domain adaptation (blitzer et al., 2007), representation learning (shen et al., 2018), multi-task learning (shen et al., 2019) problems, etc. universal syntactic interpretations are valuable language interpretations, which have been developed in years of study."
2020.emnlp-main.182.txt,2020,8 Conclusion,"we believe that our framework is general and can be applied to many other structured prediction tasks in nlp, such as neural machine translation, semantic parsing and so on."
2020.emnlp-main.183.txt,2020,6 Conclusion,future work includes finding applications of our novel tagging scheme in other tasks involving extracting triplets as well as extending our approach to support other tasks within sentiment analysis.
2020.emnlp-main.184.txt,2020,6 Conclusion,"we hope that future research continues this line of work, especially by finding novel ways to devise adaptive policies – such as reinforcement learning models with the visual modality."
2020.emnlp-main.185.txt,2020,6 Conclusion and Future Work,we hope that this new challenging evaluation set will foster further research in multilingual commonsense reasoning and cross-lingual transfer.
2020.emnlp-main.186.txt,2020,6 Further Discussion and Conclusion,in future research we also plan an in-depth study of these factors and their relation to our spectral analysis.
2020.emnlp-main.186.txt,2020,6 Further Discussion and Conclusion,"our findings, suggesting that it is possible to effectively combine these two types of language distance measures, call for further research that will advance our understanding of: 1) what knowledge is captured in monolingual and cross-lingual embedding spaces (gerz et al., 2018; pires et al., 2019; artetxe et al., 2020); 2) how that knowledge complements or overlaps with linguistic knowledge compiled into lexical-semantic and typological databases (dryer and haspelmath, 2013; wichmann et al., 2018; ponti et al., 2019); and 3) how to use the combined knowledge for more effective transfer in cross-lingual nlp applications (ponti et al., 2018; eisenschlos et al., 2019)."
2020.emnlp-main.186.txt,2020,6 Further Discussion and Conclusion,we believe that the main insights from this study will inform and guide different cross-lingual transfer learning methods and scenarios in future work.
2020.emnlp-main.187.txt,2020,10 Conclusion,"the benefits are noticeable in multilingual nmt tasks, like language clustering and ranking related languages for multilingual transfer."
2020.emnlp-main.187.txt,2020,10 Conclusion,"we plan to study how to deeply incorporate our typologically-enriched embeddings in multilingual nmt, where there are promising avenues in parameter selection (sachan and neubig, 2018) and generation (platanios et al., 2018)."
2020.emnlp-main.19.txt,2020,5 Conclusions,"as future work, we would like to investigate complementary attention mechanisms like those of reformer (kitaev et al., 2020) or routing transformer (roy et al., 2020), push scalability with ideas like those from revnet (gomez et al., 2017), and study the performance of etc in datasets with even richer structure."
2020.emnlp-main.191.txt,2020,5 Conclusion,"in future, we plan to explore how to incorporate discourse parsing into the current decision making model for end-to-end learning."
2020.emnlp-main.191.txt,2020,5 Conclusion,one possibility would be to frame them as multi-task learning with a common (shared) encoder.
2020.emnlp-main.191.txt,2020,5 Conclusion,we also conduct comprehensive analyses to unveil the limitations of discern and challenges for sharc.
2020.emnlp-main.192.txt,2020,7 Conclusion,"in the future, we will make a model learn commonsense with the obtained dataset and consider applying it to semantic tasks, such as anaphora resolution and discourse parsing."
2020.emnlp-main.192.txt,2020,7 Conclusion,"to acquire a wider range of commonsense, it is possible to combine our method with other methods based on physical world resources, such as video captions used in swag."
2020.emnlp-main.194.txt,2020,7 Conclusion,researchers could also use our 140 domain-specific adapters and investigate further combination techniques to make them even more broadly applicable.
2020.emnlp-main.195.txt,2020,6 Conclusion,"it would therefore be promising to extend this line of our research to exploit larger multilingual semantic resources, in order to further improve the parsing quality."
2020.emnlp-main.195.txt,2020,6 Conclusion,these amr representations could then be integrated into downstream crosslingual tasks to investigate their added value.
2020.emnlp-main.195.txt,2020,6 Conclusion,we explored transfer learning techniques to enable high performance cross-lingual amr parsing.
2020.emnlp-main.197.txt,2020,7 Conclusion and Future Work,"as a future research, semantically challenging cases at fine-grained level with respect to complexities of abusive/offensive (targeted) and profane (untargeted) language demand further investigation."
2020.emnlp-main.198.txt,2020,4 Discussion and Conclusions,"we also plan to add domain-specific features to our model, collect more data, integrate existing suicidal risk datasets with various languages to improve performance."
2020.emnlp-main.200.txt,2020,6 Conclusion,"in addition, it is worthwhile to further model information propagation and temporal correlation of comments in the future."
2020.emnlp-main.200.txt,2020,6 Conclusion,it is also crucial to understand why a media session is detected as cyberbullying.
2020.emnlp-main.200.txt,2020,6 Conclusion,such results can encourage future studies to develop advanced graph neural networks in better representing the interactions between heterogeneous information.
2020.emnlp-main.202.txt,2020,5 Summary and Conclusions,"this raises the question of how ssnmt will perform on really distant languages (less homographs) or when using smaller bpe sizes (more homographs), which is something that we will examine in our future work."
2020.emnlp-main.206.txt,2020,6 Conclusions,the segmenter model itself could also benefit from the incorporation of additional text data as well as pre-training procedures.
2020.emnlp-main.206.txt,2020,6 Conclusions,we plan to look into additional acoustic features as well as possible ways to incorporate asr information to the segmentation process.
2020.emnlp-main.207.txt,2020,8 Conclusion and Future Works,"furthermore, we wish to explore semi-supervised and unsupervised approaches to leverage monolingual data and explore multilingual machine translation for low-resource indic languages."
2020.emnlp-main.207.txt,2020,8 Conclusion and Future Works,"in future, we plan to design segmentation-agnostic aligners or aligners that can jointly segment and align sentences."
2020.emnlp-main.207.txt,2020,8 Conclusion and Future Works,"laser fails to identify one-tomany/many-to-one sentence alignments, we want to address this."
2020.emnlp-main.207.txt,2020,8 Conclusion and Future Works,"we would also like to experiment with bert (devlin et al., 2019) embeddings for similarity search."
2020.emnlp-main.208.txt,2020,6 Conclusions and Future work,"firstly, we are interested in applying csp to other related nlp areas for code-switching problems."
2020.emnlp-main.208.txt,2020,6 Conclusions and Future work,"secondly, we plan to investigate the pre-training objectives which are more effective in utilizing the cross-lingual alignment information for nmt."
2020.emnlp-main.208.txt,2020,6 Conclusions and Future work,there are two promising directions for the future work.
2020.emnlp-main.210.txt,2020,6 Conclusion,"in future work, we will pre-train on larger corpus to further boost the performance."
2020.emnlp-main.210.txt,2020,6 Conclusion,we leave different alignment approaches to be explored in the future.
2020.emnlp-main.212.txt,2020,5 Conclusion,"in the future, we will continue investigate the learning method for effectively utilizing self-generated samples and expand to other text generation tasks."
2020.emnlp-main.212.txt,2020,5 Conclusion,"our work can employ on different text generation tasks, e.g., text summarization and dialogue, to enhance the key phrases (or terms) generation."
2020.emnlp-main.213.txt,2020,8 Conclusions and Future Work,"a primary avenue for future work on comet will look at the impact of more compact solutions such as distilbert (sanh et al., 2019)."
2020.emnlp-main.213.txt,2020,8 Conclusions and Future Work,future work will investigate the optimality of this formulation and further examine the interdependence of the different inputs.
2020.emnlp-main.214.txt,2020,6 Conclusions,"in future work, we will apply our method to languages with corpora from diverse domains and also to other languages."
2020.emnlp-main.216.txt,2020,5 Conclusion and Future Work,"while we showed that uncertainty-aware semantic augmentation with gaussian priors is effective, more work is required to investigate if such an approach will also be successful for more sophisticated priors."
2020.emnlp-main.218.txt,2020,5 Conclusion,"in the future work, we will extend our method by replacing the simple role matching score with grammatical or semantic similaritybased measures to improve the alignment accuracy."
2020.emnlp-main.219.txt,2020,5 Conclusion,future research could explore neural architectures and training losses tailored to our approach.
2020.emnlp-main.22.txt,2020,6 Discussions and Future Work,there are several directions we will further explore in the future.
2020.emnlp-main.220.txt,2020,6 Discussion and Conclusion,we hope this is a factor that designers of future syntactic treebanks will take into account.
2020.emnlp-main.222.txt,2020,4 Conclusion,our future work will include conducting experiments on dependency trees and more nlp tasks.
2020.emnlp-main.223.txt,2020,7 Conclusion,"moreover, we display the ability of ted-cdb to help address the issue of insufficient or unbalanced data on other corpora and improve the performance of models for other languages."
2020.emnlp-main.224.txt,2020,8 Conclusion,"in future work, we plan to extend the annotation process to also cover inter-sentential relations."
2020.emnlp-main.225.txt,2020,7 Conclusion,"in future, we plan to evaluate disa on other discourse analysis tasks."
2020.emnlp-main.226.txt,2020,5 Future Work,one way to mitigate this issue is to leverage longer context information to identify better keywords which is subject of the future work.
2020.emnlp-main.226.txt,2020,5 Future Work,"writingprompts (fan et al., 2018)) is a subject for future work."
2020.emnlp-main.227.txt,2020,6 Conclusion & Future Work,"in the future, we will investigate on extending our approach to more areas."
2020.emnlp-main.229.txt,2020,4 Conclusion,"however,as discussion in error analysis, there are several challenges to solve in the future."
2020.emnlp-main.235.txt,2020,5 Conclusion,"as future work, we will further study our method’s ability of extreme multi-label learning (bhatia et al., 2016) and different document encoders."
2020.emnlp-main.237.txt,2020,5 Conclusion and Future Work,"in the future, we will explore to extend the idea of disentanglement in the continual learning of other nlp tasks."
2020.emnlp-main.239.txt,2020,5 Conclusion and Future Work,"in the future, we plan to study different regularizers in the asymmetrical text matching task, for further exploring their effectiveness in bridging the gap between asymmetrical domains."
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,"first, the word clouds may reveal sensitive contents in the training data to human debuggers."
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,"for example, using relu as activation functions in lstm cells (instead of tanh) renders the features non-negative."
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,"for future work, it would be interesting to extend find to other nlp tasks, e.g., question answering and natural language inference."
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,"in order to generalize the framework beyond cnns, there are two questions to consider."
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,it is also convenient to use since only the trained model and the training data are required as input.
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,"third, it is possible that one feature detects several patterns (jacovi et al., 2018) and it will be difficult to disable the feature if some of the detected patterns are useful while the others are harmful."
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,this will require some modifications to understand how the features capture relationships between two input texts.
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,"we exemplified this with two word clouds representing each bilstm feature in appendix c, and we plan to experiment with advanced visualizations such as lstmvis (strobelt et al., 2018) in the future."
2020.emnlp-main.240.txt,2020,4 Conclusion,"we propose to solve each of them with existing techniques (artetxe et al., 2018b; alvarezmelis and jaakkola, 2018; jawanpuria et al., 2019)."
2020.emnlp-main.244.txt,2020,6 Discussions and Future Works,"on gpus we cannot expect a reduction in the number of operations to translate 1:1 to lower execution times, since they are highly optimised for parallelism.3 we leave the parallelism enhancements of skylinebuilder for future work."
2020.emnlp-main.245.txt,2020,6 Conclusion,"while our model is not a neural module network, as our model uses a single fixed layout instead of different layouts per question, we believe there are enough similarities that future work could explore combining our modules with those used in other neural module networks over text, leading to a single model that could perform the necessary reasoning for multiple different datasets."
2020.emnlp-main.246.txt,2020,7 Discussion and Future Work,we deliberately choose to leave experiments including real human annotators to future research for the following reason.
2020.emnlp-main.249.txt,2020,5 Conclusion & Future Work,"additional future investigations may include a deeper analysis of the mathematical and statistical properties of the weighted coefficients ρw, τw, as well as a rigorous derivation of the optimal values for the parameters of the data collection approach."
2020.emnlp-main.249.txt,2020,5 Conclusion & Future Work,"as future work, we plan to collect human annotations (i) to test the proposed data collection approach on real data and (ii) to assess the validity and estimate the parameters of the proposed stochastic transitivity model."
2020.emnlp-main.25.txt,2020,5 Conclusion and Future Work,we plan to address these challenges in future work.
2020.emnlp-main.250.txt,2020,5 Conclusion and Future Work,"in the future, we plan to apply mft to other language models (e.g., transformerxl (dai et al., 2019) and albert (lan et al., 2019)) and for other nlp tasks."
2020.emnlp-main.251.txt,2020,6 Conclusions,"in future work, we will focus on producing novel continuations of the user’s search intent, extending the approach to other domains, and automating the design of behavioral hypotheses."
2020.emnlp-main.252.txt,2020,7 Conclusion and Future Work,"besides, how to enable the existing emotion-cause pair extraction models to consider the effect of context is also a meaningful task."
2020.emnlp-main.255.txt,2020,5 Conclusion,"regarding the model-agnostic and task-agnostic properties of our method, they are applicable to any types of nlp model for various tasks, such as neural machine translation and visual question answering."
2020.emnlp-main.256.txt,2020,5 Conclusion,"as fc is only one test bed for adversarial attacks, it would be interesting to test this method on other nlp tasks requiring semantic understanding such as question answering to better understand shortcomings of models."
2020.emnlp-main.257.txt,2020,5 Conclusion,"the study suggests that besides improving our alignment algorithms for distant languages (vulic et al.´ , 2019), we should also focus on improving monolingual word vector spaces, and monolingual training conditions to unlock a true potential of cross-lingual learning."
2020.emnlp-main.258.txt,2020,8 Conclusion and Future Work,"in the future, we plan to apply fa-rnn to other tasks and explore other variants of fa-rnn."
2020.emnlp-main.26.txt,2020,7 Discussion and conclusion,"finally, we believe that using attention mechanisms to study the grounding of the edits, similarly to the ideas in kohn ¨ (2018), can be an important step towards understanding how the preliminary representations are built and decoded; we want to test this as well in future work."
2020.emnlp-main.261.txt,2020,6 Conclusion,we hope that this work will help the research community interpret bert for other complex tasks and explore the above open-ended questions.
2020.emnlp-main.266.txt,2020,5 Conclusion,we constrain types to reduce the solution space and add negative relationships to leverage negative training samples.
2020.emnlp-main.270.txt,2020,6 Conclusion,"examples of more sophisticated game scenarios are bidirectional conversations where multi-symbol messages are challenging to analyse (kottur et al., 2017; bouchacourt and baroni, 2019) or games with image sequences as input (santamaría-pang et al., 2019)."
2020.emnlp-main.270.txt,2020,6 Conclusion,"in light of these results, it would be interesting to explore the use of unsupervised tokenisers that work well for languages without spaces (e.g."
2020.emnlp-main.270.txt,2020,6 Conclusion,"sentencepiece kudo and richardson, 2018) prior to our approach and to try other word embedding models for diora, such as the character-based elmo embeddings8 (peters et al., 2018) or the more recent bert (devlin et al., 2019)."
2020.emnlp-main.271.txt,2020,7 Conclusion,we believe that the idea of subinstruction module and a sub-instruction annotated dataset can benefit future studies in the vln task as well as other vision-and-language problems.
2020.emnlp-main.273.txt,2020,5 Conclusion,"in addition, two pre-trained seq2seq language models: t5 (raffel et al., 2019) and bart (lewis et al., 2019) are incorporated in our framework."
2020.emnlp-main.273.txt,2020,5 Conclusion,"in future work, we plan to explore taskoriented dialogues domain-adaptive pre-training methods (wu et al., 2020; peng et al., 2020) to enhance our language model backbones, and extend the framework for mixed chit-chat and taskoriented dialogue agents (madotto et al., 2020a)."
2020.emnlp-main.274.txt,2020,5 Conclusion,"we would also like to explore different implementations in line with recent advances in dialog models, especially using large-scale pre-trained language models."
2020.emnlp-main.275.txt,2020,7 Conclusion,"in the future, we would explore three aspects: (1) more efficient posterior information representation and corresponding prediction module, (2) the interpretability of knowledge selection and (3) knowledge selection without knowledge label."
2020.emnlp-main.277.txt,2020,6 Conclusion,our method may inspire other research in low-resource nlp tasks.
2020.emnlp-main.278.txt,2020,5 Conclusions,"in this paper, one main focus is to demonstrate the differences between background and decisiontime planning."
2020.emnlp-main.278.txt,2020,5 Conclusions,this might be an interesting topic for future work.
2020.emnlp-main.28.txt,2020,6 Conclusion,"for the future work, we suggest to integrate the ranking models and generation model, e.g., in beam search stage or reinforcement learning using ranking score as reward signal."
2020.emnlp-main.283.txt,2020,6 Conclusion,for future work it would be interesting to test these sense embeddings in a wider range of applications outside wsd.
2020.emnlp-main.285.txt,2020,10 Conclusion,"as future work, we plan to exploit the information brought by our embeddings to other downstream tasks, such as multilingual semantic role labeling (di fabio et al., 2019; conia et al., 2020) and cross-lingual semantic parsing (blloshmi et al., 2020)."
2020.emnlp-main.287.txt,2020,5 Conclusion,"in some sentences, phrases or clauses rather than words indicate the given aspect category, future work could consider multi-grained instances, including words, phrases and clauses."
2020.emnlp-main.287.txt,2020,5 Conclusion,"since directly finding the key instances for some aspect categories is ineffective, we will try to first recognize all opinion snippets in a sentence, then assign these snippets to the aspect categories mentioned in the sentence."
2020.emnlp-main.288.txt,2020,5 Conclusion,"one future direction is to investigate how to integrate the two different attention mechanisms, namely the standard attention and structured attention for nlp applications."
2020.emnlp-main.289.txt,2020,4 Conclusion,"in the future, we will explore the extension of this approach to achieve full coverage."
2020.emnlp-main.29.txt,2020,8 Discussion and Conclusion,collecting multiple gold sql query references for evaluation (like machine translation) might be a potential solution.
2020.emnlp-main.29.txt,2020,8 Discussion and Conclusion,our test suites will be released for eleven datasets so that future works can conveniently evaluate test suite accuracy.
2020.emnlp-main.291.txt,2020,6 Conclusion,"furthermore, we would like to investigate other approaches (e.g., graph-based neural network) to better model the modality and label dependence in multi-modal multi-label emotion detection."
2020.emnlp-main.291.txt,2020,6 Conclusion,"in our future work, we will extend our approach to more multi-modal multi-label scenarios, such as intention detection in video conversations and aspect analysis in multi-modal reviews."
2020.emnlp-main.296.txt,2020,6 Conclusions,"in the future, we would like to generate abstractive summaries following an unsupervised approach (baziotis et al., 2019; chu and liu, 2019) and investigate how recent advances in open domain qa (wang et al., 2019; qi et al., 2019) can be adapted for query focused summarization."
2020.emnlp-main.297.txt,2020,6 Conclusion,"in the future, we would like to investigate other objectives to pre-train seq2seq models for abstractive summarization."
2020.emnlp-main.298.txt,2020,6 Conclusion,"in the future, we will continue to explore better re pre-training techniques, especially with a focus on open relation extraction and relation discovery."
2020.emnlp-main.3.txt,2020,6 Conclusion,"in future work, we would like to improve comment matching, e.g., by making it stance-aware."
2020.emnlp-main.3.txt,2020,6 Conclusion,we also plan to experiment with sequence-tosequence neural models for generating key point candidates from comments.
2020.emnlp-main.300.txt,2020,5 Conclusion,"in the future, we will explore how to improve the efficiency of our pre-training."
2020.emnlp-main.303.txt,2020,5 Conclusion,"in future work, we plan to integrate knowledge graphs and explore other document graph modeling ways (e.g., hierarchical graphs) to improve the performance."
2020.emnlp-main.304.txt,2020,6 Conclusion,"as future work, we intend to explore its application in those fields."
2020.emnlp-main.306.txt,2020,9 Conclusion,we plan to explore the utility of this architecture in other nlp problems.
2020.emnlp-main.308.txt,2020,6 Conclusion,"as future work, we plan to generalize the ept to other datasets, including non-english word problems or non-algebraic domains in math, to extend our model."
2020.emnlp-main.31.txt,2020,7 Discussion,"future work is also needed to handle attributes containing long free-form text, as autoqa currently only supports database operations without reading comprehension."
2020.emnlp-main.310.txt,2020,4 Conclusion,"in the future, we would like to extend gtm to corpora with explicit doc-doc interactions, e.g., scientific documents with citations or social media posts with user relationships."
2020.emnlp-main.310.txt,2020,4 Conclusion,replacing gcn in gtm with more advanced graph neural networks is another promising research direction.
2020.emnlp-main.311.txt,2020,5 Conclusion and Future Work,there are several directions to explore in the future.
2020.emnlp-main.312.txt,2020,6 Conclusion,"in the future, we would like to explore if the learner-like agent can be extended to materials and data beyond the example sentences for near-synonyms."
2020.emnlp-main.313.txt,2020,7 Conclusion and Future Work,"moreover, we will conduct further exploration of the multiproduct ad post form, including more vivid multimedia information, such as pictures and videos."
2020.emnlp-main.314.txt,2020,5 Conclusion,we are also releasing a part of our forms dataset to aid further research in this direction.
2020.emnlp-main.318.txt,2020,5 Conclusion,"in particular, we will enhance the practicability of chinese word segmentation to improve the effectiveness of other downstream chinese nlp tasks."
2020.emnlp-main.318.txt,2020,5 Conclusion,"in the future, we will continue studying the efficiency of the neural architecture, and pay attention to improving the speed of both training and testing steps on an ever-increasing dataset."
2020.emnlp-main.319.txt,2020,5 Conclusions,"we also plan to extend our framework to semi-supervised learning, where a small number of annotations might also be available in the target language."
2020.emnlp-main.319.txt,2020,5 Conclusions,we have also contributed two quality controlled datasets (compatible with propbank-style guidelines) which we hope will be useful for the development of crosslingual models.
2020.emnlp-main.32.txt,2020,5 Conclusion,extending our method to an abstractive setting is meaningful future work.
2020.emnlp-main.321.txt,2020,5 Conclusions,future work should address the application of our method to more and typologically more divergent languages.
2020.emnlp-main.322.txt,2020,7 Conclusions,"given that gcns over dependency and constituency structure have access to very different information, it would be interesting to see in future work if combining two types of representations can lead to further improvements."
2020.emnlp-main.322.txt,2020,7 Conclusions,"while we experimented only with constituency syntax, spangcn may be able to encode any kind of span structure, for example, co-reference graphs, and can be used to produce linguistically-informed encoders for other nlp tasks rather than only srl."
2020.emnlp-main.323.txt,2020,7 Conclusion,"in future work, one could make the a* parser more accurate by extending it to non-projective dependency trees, especially on dm, eds and amr."
2020.emnlp-main.323.txt,2020,7 Conclusion,it would also be interesting to see if our method for avoiding dead ends can be applied to other formalisms with complex symbolic restrictions.
2020.emnlp-main.327.txt,2020,7 Discussion,future work should investigate more rewards for training an open-domain dialog model such as long term conversation rewards that may need to be computed over many conversation turns.
2020.emnlp-main.333.txt,2020,6 Conclusion,"in the future, we plan to apply the transformed representations on more lexical semantics tasks such as word sense disambiguation within an application (navigli and vannella, 2013)."
2020.emnlp-main.334.txt,2020,8 Conclusions,we hope this will support work on solutions for nlp applications and resources that can better serve minorities and underrepresented groups.
2020.emnlp-main.336.txt,2020,6 Conclusion,"in the future, we plan to annotate some of the data, explore supervised segmentation models (li et al., 2018) and introduce more conversation structures like dialogue acts (oya and carenini, 2014; joty and hoque, 2016) into abstractive dialogue summarization."
2020.emnlp-main.338.txt,2020,4 Conclusion,future work may explore the use of points of correspondence and sentence fusion in the standard setting of document summarization.
2020.emnlp-main.339.txt,2020,8 Conclusion,"future work will focus on extending our models to generate extractive plans for better abstractive summarization of long or multiple documents (liu et al., 2018)."
2020.emnlp-main.34.txt,2020,6 Conclusion,our work paves the way for further research on bridging q-learning and unsupervised text summarization.
2020.emnlp-main.340.txt,2020,5 Conclusion and future work,"for future work, we think it will be interesting to look at: 1. zero-shot clir models for low-resource languages, 2. comparison of end-to-end neural rankers with traditional translation+ir pipelines in terms of both scalability, cost, and retrieval accuracy, 3. advanced neural architectures and training algorithms that can exploit our large training data, 4. building universal models for multilingual ir."
2020.emnlp-main.340.txt,2020,5 Conclusion and future work,"the large number of supported language directions allows the research community to explore and build new models for many more languages, especially the low-resource ones."
2020.emnlp-main.342.txt,2020,7 Conclusion,these findings provide opportunities for future work towards more efficient and interpretable neural ir.
2020.emnlp-main.343.txt,2020,5 Conclusions and Future work,"as a future work, we plan to utilize automatic summarization for missing abstracts, instead of taking the first 512 content tokens."
2020.emnlp-main.345.txt,2020,6 Conclusion and Future Work,"as an extension, it may even be possible to determine the number of les k from the data using recurrent neural networks (yang et al., 2017)."
2020.emnlp-main.345.txt,2020,6 Conclusion and Future Work,one avenue of future work is to learn explainable rules that domain experts can interact with on top of such embeddings.
2020.emnlp-main.345.txt,2020,6 Conclusion and Future Work,"while recent embeddings (devlin et al., 2019) may lead to improved accuracy, these remain poorly understood (moradshahi et al., 2019)."
2020.emnlp-main.35.txt,2020,6 Conclusion and future work,"in the future, we will study the effectiveness of ta on other nlp tasks, such as the document-level translation, and investigate whether ta is useful for transformer pre-training."
2020.emnlp-main.350.txt,2020,7 Conclusion,a promising research direction is to investigate the root cause behind memorization.
2020.emnlp-main.350.txt,2020,7 Conclusion,"therefore, another research avenue would be to blend the two frameworks (im et al., 2017)."
2020.emnlp-main.351.txt,2020,8 Conclusion and Future Work,our findings also suggest future work on additional ways to incorporate story principles into plot generation.
2020.emnlp-main.351.txt,2020,8 Conclusion and Future Work,"there is also further work to be done in investigating what models are best able to incorporate plots, which would enable plot improvements to be even more effective."
2020.emnlp-main.353.txt,2020,7 Conclusion,generating subsequent references with such properties has the potential to enhance user adaptation and successful communication in dialogue systems.
2020.emnlp-main.355.txt,2020,7 Conclusion,"in the future, we plan to examine the hierarchical classification architecture’s potential for reducing computational runtime."
2020.emnlp-main.356.txt,2020,6 Conclusion,we have only begun to explore the possibilities opened up by pose traces.
2020.emnlp-main.358.txt,2020,7 Conclusion,in future work we plan to incorporate crosslingual signals as vulic et al.´ (2019) argue that a fully unsupervised setting is hard to motivate.
2020.emnlp-main.36.txt,2020,5 Conclusion,"for future work, we plan to investigate the use of a more powerful language model, such as megatron-lm (shoeybi et al., 2019), as the teacher; and different strategies for choosing hard negatives to further boost the performance."
2020.emnlp-main.361.txt,2020,7 Conclusion,"future work will investigate the compositional capability of these adapters, and combine domain and monolingual adapters for nmt."
2020.emnlp-main.362.txt,2020,5 Discussion,"therefore, scaling models to more raw text and larger capacity models may be more beneficial for producing better cross-lingual models."
2020.emnlp-main.362.txt,2020,5 Discussion,"therefore, we make the following recommendations for future work on cross-lingual alignment or multilingual representations: 1) evaluations should consider average quality data, not exclusively high-quality bitext.2) evaluation must consider multiple nlp tasks or datasets.3) evaluation should report mean and variance over multiple seeds, not a single run."
2020.emnlp-main.363.txt,2020,5 Conclusion,"this finding provides a strong incentive for intensifying future research efforts that focus on cheap or naturally occurring supervision (vulic et al.´ , 2019; artetxe et al., 2020c; marchisio et al., 2020), quick and simple annotation procedure, and the more effective few-shot transfer learning setups."
2020.emnlp-main.364.txt,2020,8 Conclusions,"for future work, we plan to expand the automatic domain induction methods and test the mdkd framework on generic mt with data exhibiting varying degrees of heterogeneity: as mdkd distills domain-specific models to create multiple simpler data distributions, we want to investigate if inducing train-time specializations and using them for distillation through mdkd can lead to better quality."
2020.emnlp-main.365.txt,2020,9 Conclusion,we extensively tested the approach for various languages from different language families.
2020.emnlp-main.366.txt,2020,5 Discussion,"we also apply our method to both semantic and syntactic parsing, demonstrating our method’s broader applicability to tasks that process variable-output-length data in a sequential manner."
2020.emnlp-main.368.txt,2020,7 Conclusion,future studies will extend this work to other crosslingual nlp tasks and more languages.
2020.emnlp-main.369.txt,2020,5 Conclusion,"with these contributions, we hope that this corpus will be an important resource to the research community."
2020.emnlp-main.370.txt,2020,8 Conclusions,"together with this paper, we release our dataset16 and models17, which we hope will enable the ai research community to explore effective approaches to incorporate commonsense reasoning capabilities into various downstream tasks."
2020.emnlp-main.373.txt,2020,7 Discussion and Conclusion,"an interesting future direction is to generate each clarification in response to the previous ones, in a dialogue setup (saeidi et al., 2018)."
2020.emnlp-main.373.txt,2020,7 Discussion and Conclusion,we hope that our framework will facilitate future research in this area.
2020.emnlp-main.375.txt,2020,5 Discussion,scaling these carefully-controlled methods to the larger data setting will be an important next step.
2020.emnlp-main.376.txt,2020,6 Conclusions,"another line of future research is to compare the incremental predictions of neural models to finergrained eye-tracking evidence during sentence processing of double-object sentences (e.g.filik et al., 2004)."
2020.emnlp-main.377.txt,2020,9 Conclusions,"in the future, our approach and new evaluation measure could be applied to larger eyetracking datasets, such as the english dataset by he et al.(2019)."
2020.emnlp-main.377.txt,2020,9 Conclusions,"since different eye-tracking datasets tend to make use of different gaze encodings and formats, the amount of pre-processing and analysis steps required to apply our method to other resources was beyond the scope of this paper."
2020.emnlp-main.377.txt,2020,9 Conclusions,we leave testing whether the reported pattern of results holds across different languages to future work.
2020.emnlp-main.378.txt,2020,6 Discussion,one future direction is to consider more sophisticated mechanisms to gain stronger controlability over longer sentences while maintaining the compactness of latent representations.
2020.emnlp-main.378.txt,2020,6 Discussion,we hope that this paper will help renew interest in dgms for this purpose.
2020.emnlp-main.38.txt,2020,7 Conclusion,"this opens up the possibility of exploring large-scale meta-learning in nlp for various meta problems, including neural architecture search, continual learning, hyperparameter learning, and more."
2020.emnlp-main.380.txt,2020,7 Conclusion,"in future work, we plan to further investigate how different techniques apply to the problem of text segmentation, including data augmentation (wei and zou, 2019; lukasik et al., 2020b) and methods for regularization and mitigating labeling noise (jiang et al., 2020; lukasik et al., 2020a)."
2020.emnlp-main.383.txt,2020,6 Discussions and future work,we plan to perform large-scale pre-training and evaluation on glue datasets for the comprehensive analysis.
2020.emnlp-main.383.txt,2020,6 Discussions and future work,we will continue to explore this line in the future.
2020.emnlp-main.384.txt,2020,6 Conclusion,"in our future work, we would like to extend this idea to unseen numbers in vocabulary as a function of seen ones."
2020.emnlp-main.385.txt,2020,6 Conclusion,future work might explore combining more expressive flows with discrete latent variables.
2020.emnlp-main.386.txt,2020,6 Conclusion,"finally, the models to generate cross-lingual annotations should be thoroughly evaluated in downstream music retrieval and recommendation tasks."
2020.emnlp-main.386.txt,2020,6 Conclusion,"hence, the effectiveness of language-specific concept representations to model the culturally diverse perception could be further probed."
2020.emnlp-main.386.txt,2020,6 Conclusion,our work provides a methodological framework to study the annotation behavior across languagebound cultures in other domains too.
2020.emnlp-main.387.txt,2020,8.1 Future Work,"the next step towards this goal would be to recognize when characters refer to one another, and how this contributes to the movie-level risk behavior rating."
2020.emnlp-main.389.txt,2020,9 Conclusion,"furthermore, we used the interpretability of constituency tests to highlight and explain the parser’s strengths and shortcomings, like the “[ subject verb ]” and “adverb [ adjective noun ]” misbracketings, revealing potential next steps for improvement."
2020.emnlp-main.391.txt,2020,5 Conclusion and Future Work,"in the future, we plan to enhance the system for handling morphologically complex languages trough unsupervised morphological segmentation."
2020.emnlp-main.394.txt,2020,9 Conclusion,"in the future work wish to explore more data independent methods such as lrc, for both speed and lack of data dependency, as well as manipulation of the decay w.r.t.what we have discovered from our layer-wise analysis."
2020.emnlp-main.396.txt,2020,8 Conclusion,"first, the approach could apply the same meta-learning approach to other classes of tasks beyond span id."
2020.emnlp-main.396.txt,2020,8 Conclusion,our current study could be extended in various directions.
2020.emnlp-main.397.txt,2020,9 Conclusions and future directions,"future work can apply these tests to a broader range of models, and continue to develop controlled tests that target encoding of complex compositional meanings, both for two-word phrases and for larger meaning units."
2020.emnlp-main.4.txt,2020,6 Conclusion and Future Work,"besides the future extensions of this approach that we mentioned in our results discussion and error analysis, this work opens several interesting research paths."
2020.emnlp-main.4.txt,2020,6 Conclusion and Future Work,"therefore, we plan to complement this work with approaches for other frequently applied schemes such as arguments by expert opinion and arguments by example."
2020.emnlp-main.400.txt,2020,8 Conclusion,"furthermore, integrating information from knowledge-bases can further improve the quality of entity representation."
2020.emnlp-main.400.txt,2020,8 Conclusion,"future work can explore representations for rare or unseen entities, as well as developing less memory-intensive ways to learn and integrate entity representations."
2020.emnlp-main.402.txt,2020,5 Summary and Outlook,future work may investigate whether these results translate to other language models besides roberta as well as other training datasets besides winogrande.
2020.emnlp-main.403.txt,2020,6 Conclusion,we hope the insights provided here will help guide the development of better language models in the future.
2020.emnlp-main.404.txt,2020,6 Conclusion and Future Work,"in future work, we plan to explore topic-level bias prediction as well as going beyond left-centerright bias."
2020.emnlp-main.404.txt,2020,6 Conclusion and Future Work,"last but not least, we plan to experiment with other languages, and to explore to what extent a model for one language is transferable to another one given that the left-center-right division is not universal and does not align perfectly across countries and cultures, even when staying within the western political world."
2020.emnlp-main.404.txt,2020,6 Conclusion and Future Work,"we further want to develop models that would be able to detect specific fragments in an article where the bias occurs, thus enabling explainability."
2020.emnlp-main.405.txt,2020,5 Conclusion,"in future work, we plan to apply our semantic label smoothing technique to various sequence to sequence problems, including text summarization (zhang et al., 2019) and text segmentation (lukasik et al., 2020b)."
2020.emnlp-main.405.txt,2020,5 Conclusion,we also plan to study the relation between pretraining and data augmentation techniques.
2020.emnlp-main.409.txt,2020,6 Conclusion,"we investigate representations from pre-trained language models for task-oriented dialogue tasks, including domain identification, intent detection, slot tagging, and dialogue act prediction."
2020.emnlp-main.41.txt,2020,6 Conclusion,"future works include using other multilingual pretraining models such as xlm-roberta (conneau et al., 2019) for a more accurate model and distilmbert (sanh et al., 2019) for a more compact model."
2020.emnlp-main.41.txt,2020,6 Conclusion,"we have already applied it to bilingual sentence alignment (chousa et al., 2020) and we plan to extend it to other related problems."
2020.emnlp-main.410.txt,2020,7 Conclusion,we release our multiatis++ corpus to facilitate future research on cross-lingual nlu to bridge the gap between cross-lingual transfer and supervised methods.
2020.emnlp-main.411.txt,2020,7 Conclusion,future work includes its cross-lingual transfer and cross-dataset (or cross-task) generalization.
2020.emnlp-main.412.txt,2020,4 Conclusion,"in the future, we plan to explore maskaugment for other tasks in nlp domain."
2020.emnlp-main.413.txt,2020,7 Conclusion,"last but not least, we collect the topv2 dataset, a large-scale multi-domain task-oriented semantic parsing dataset with 8 domains and more than 180k annotated samples to evaluate our models, which we release to the research community."
2020.emnlp-main.415.txt,2020,6 Conclusions,"in the future, we plan to explore this approach with other language pairs and other generation tasks."
2020.emnlp-main.416.txt,2020,6 Discussion,"furthermore, to move towards more practical applications, we would also need to conduct communication-based evaluation (newman et al., 2020) in addition to annotating individual utterances."
2020.emnlp-main.416.txt,2020,6 Discussion,future work can aim to propose more general formulations that encapsulate more properties of the circumstance.forms of assistance.
2020.emnlp-main.416.txt,2020,6 Discussion,"future work can consider adapting experiment designs from prior work (gao et al., 2015; hohenstein and jung, 2018) to establish the impact of offering such intention-preserving paraphrases in real conversations, potentially by considering downstream outcomes."
2020.emnlp-main.416.txt,2020,6 Discussion,"future work may consider more complex stylistic aspects and strategies that are more tied to the content, such as switching from active to passive voice."
2020.emnlp-main.416.txt,2020,6 Discussion,"future work may consider more comprehensive modeling of how people form politeness perceptions or obtain more reliable causal estimates for strategy strength (wang and culotta, 2019).task formulation."
2020.emnlp-main.416.txt,2020,6 Discussion,"hence, while we work towards providing fully automated suggestions, we might also want to utilize the language ability humans possess and consider assistance approaches in the form of interpretable (partial) suggestions.evaluation."
2020.emnlp-main.416.txt,2020,6 Discussion,the results and limitations of our method open up several natural directions for future work.modeling politeness perceptions.
2020.emnlp-main.417.txt,2020,5 Conclusion and Discussion,"as for future directions, one natural extension is how we can automatically identify those attributes."
2020.emnlp-main.418.txt,2020,5 Discussion,"alternatively, it may be useful to explore generation in a non left-to-right order to improve the efficiency of inference."
2020.emnlp-main.418.txt,2020,5 Discussion,"another line of future work is to extend our model to sequence rewriting tasks, such as machine translation post-editing, that do not have existing error-tag dictionaries."
2020.emnlp-main.418.txt,2020,5 Discussion,"even though our approach is open-vocabulary, future work will explore task specific restrictions."
2020.emnlp-main.42.txt,2020,6 Conclusion,we leave it for future work to extend our study to more downstream tasks and systems.
2020.emnlp-main.420.txt,2020,5 Conclusion,"blm has plenty of future applications, including template filling, information fusion, assisting human writing, etc."
2020.emnlp-main.420.txt,2020,5 Conclusion,"such models can be used in machine translation to support editing and refining translation, as well as in dialogue systems to compose a complete sentence with given elements."
2020.emnlp-main.420.txt,2020,5 Conclusion,"while we proposed blm for language generation, it would also be interesting to compare the representations learned by blm with those produced by other pre-training methods."
2020.emnlp-main.421.txt,2020,5 Conclusion,"cod3s leads to more diverse outputs in a multi-target generation task in a controllable and interpretable manner, suggesting the potential of semantically guided diverse decoding for a variety of text generation tasks in the future."
2020.emnlp-main.422.txt,2020,7 Future Work,"we also plan to expand our methodology for extracting grammar rules from raw text to other aspects of morphosyntax, such as argument structure and word order phenomena."
2020.emnlp-main.422.txt,2020,7 Future Work,we leave a more expressive model and evaluation on more languages as future work.
2020.emnlp-main.424.txt,2020,6 Conclusion,another approach would be to compare improvements between manual-only cleaning and cleaning done by a linguist working with someone who can write scripts to automatically correct repeated patterns of noise.
2020.emnlp-main.424.txt,2020,6 Conclusion,it could be integrated into linguists’ workflow in order to improve the study of inflection and increase igt data.
2020.emnlp-main.424.txt,2020,6 Conclusion,it might also be integrated into linguistic software such as flex.
2020.emnlp-main.424.txt,2020,6 Conclusion,there is room for future improvement.
2020.emnlp-main.427.txt,2020,8 Conclusion,future work needs to look into how question and reply context can improve automatic identification of advice.
2020.emnlp-main.43.txt,2020,6 Conclusion and Future Work,our future work involves converting the monolingual data to parallel and collecting more data from the news domain.
2020.emnlp-main.43.txt,2020,6 Conclusion and Future Work,we hope these diverse baselines will serve as useful strong starting points for future work by the community.
2020.emnlp-main.430.txt,2020,9 Conclusions,"in the future, we would like to explore uses of the subevent knowledge base for other eventoriented applications such as event tracking."
2020.emnlp-main.431.txt,2020,8 Conclusion,beesl is broadly applicable to event extraction and other tasks that can be recast as sequence labeling.
2020.emnlp-main.431.txt,2020,8 Conclusion,"we release the code freely, to foster research on using beesl for other nlp tasks as well, e.g., enhanced dependency parsing, fine-grained named entity recognition, and semantic parsing."
2020.emnlp-main.433.txt,2020,7 Conclusion,"in the future, we plan to extend our annotation to include event arguments and other properties of events."
2020.emnlp-main.435.txt,2020,5 Conclusion,"in the future, we plan to apply the proposed model for the related tasks and other settings of ed, including new type extension (nguyen et al., 2016b; lai and nguyen, 2019), and few-shot learning (lai et al., 2020a,b)."
2020.emnlp-main.436.txt,2020,5 Conclusions and Future Work,"for the future, instead of using the roberta baseline model for the self-training experiments, we could run several iterations by retraining on the data produced by our best self-trained model(s); this could be a good avenue for further improvements."
2020.emnlp-main.436.txt,2020,5 Conclusions and Future Work,"in addition we plan to extend our work by moving to other languages beyond english (we currently have not tried this due to lack of data) using cross-lingual models, (subburathinam et al., 2019), applying other architectures like cnns (nguyen and grishman, 2015), incorporating tree structure in our models (miwa and bansal, 2016) and/or by handling jointly performing event recognition and temporal ordering (li and ji, 2014; katiyar and cardie, 2017)."
2020.emnlp-main.437.txt,2020,4 Conclusion,"this model size can be prohibitively expensive in resource-constrained settings, prompting future work on more efficient language models."
2020.emnlp-main.437.txt,2020,4 Conclusion,"we are therefore interested in measuring performance on question answering tasks that require reasoning capabilities such as drop (dua et al., 2019)."
2020.emnlp-main.438.txt,2020,7 Conclusion and Future Work,"in future work, we plan to extend the dataset with more questions, more subjects, and more languages."
2020.emnlp-main.438.txt,2020,7 Conclusion and Future Work,we further plan to develop new models to address the specific challenges we identified.
2020.emnlp-main.439.txt,2020,5 Conclusions,improving lm-score-based filtering is a future direction of our work.
2020.emnlp-main.439.txt,2020,5 Conclusions,it would be interesting to explore how one can adapt the generative models to the type of target domain questions.
2020.emnlp-main.44.txt,2020,8 Limitations and Future Work,there are additional avenues for future work beyond our proposed framework.
2020.emnlp-main.440.txt,2020,6 Conclusion,"in future work, we aim to extend our approach to more domains and explore more generalizable approaches for unsupervised domain adaptation."
2020.emnlp-main.446.txt,2020,6 Conclusion,addressing potential ethical concerns the goal of our work is to help to make nlp models more robust.
2020.emnlp-main.446.txt,2020,6 Conclusion,"moving forward, we hope to improve and deploy defenses against adversarial attacks in nlp, and more broadly, we hope to make security and privacy a more prominent focus of nlp research."
2020.emnlp-main.450.txt,2020,7 Conclusion,"in future work, we are interested in exploring nat using distilled ensembles with truncated distributions, and assessing how improved calibration impacts non-sequential decoding performance."
2020.emnlp-main.450.txt,2020,7 Conclusion,"non-autoregressive translation (nat) is an active area of research for nmt (gu et al., 2017; stern et al., 2019; ghazvininejad et al., 2019)."
2020.emnlp-main.455.txt,2020,7 Conclusion,"hence, we guide the data-driven tokenizer by incorporating linguistic information to learn a more efficient vocabulary and generate symbol sequences that increase the network’s robustness to inflectional variation."
2020.emnlp-main.456.txt,2020,8 Conclusion,"we note that we could further extend our measures to fuzzy partitions, which remain less explored in community detection, but are a promising avenue for future work."
2020.emnlp-main.458.txt,2020,9 Conclusion,"in the presence of one-to-many mappings between pinyin and characters, the mapping accuracy is severely downgraded, leaving open an opportunity to design more robust unsupervised vector mapping systems."
2020.emnlp-main.459.txt,2020,6 Conclusion,"in future work, we plan to improve the quality of the additional actions."
2020.emnlp-main.46.txt,2020,7 Conclusion,"possible extensions of this work could investigate enriching trump vectors by incorporating other sources of text, such as white house press releases and speeches."
2020.emnlp-main.460.txt,2020,6 Conclusion and Future Work,"another meaningful direction is to use hyperka to infer the associations between snapshots in temporally dynamic kgs (xu et al., 2020)."
2020.emnlp-main.460.txt,2020,6 Conclusion and Future Work,"for future work, we plan to incorporate hyperbolic rnns (ganea et al., 2018) to encode auxiliary information for zero-shot entity and concept representations."
2020.emnlp-main.460.txt,2020,6 Conclusion and Future Work,"we also seek to investigate the use of hyperka for cross-domain representations of biological and medical knowledge (hao et al., 2020)."
2020.emnlp-main.461.txt,2020,9 Conclusion,"we plan to apply the proposed framework on various event reasoning tasks and construct novel distributional constraints that could leverage domain knowledge beyond corpus statistics, such as the larger unlabeled data and rich information contained in knowledge bases."
2020.emnlp-main.462.txt,2020,5 Conclusion,future work involves exploring the generalization of temp to continuous tkgc and better imputation techniques to induce representations for infrequent and inactive entities.
2020.emnlp-main.463.txt,2020,7 Conclusion,"it leads to many interesting future works, including generalizing theorem 2 to other models, designing new algorithms to automatically adapt deep networks to different training configurations, upgrading the transformer architecture, and applying our proposed admin to conduct training in a larger scale."
2020.emnlp-main.464.txt,2020,6 Conclusion,"this opens a wide range of possibilities for generation tasks where monotonic orderings are not the most natural choice, and we would be excited to explore some of these areas in future work."
2020.emnlp-main.465.txt,2020,5 Conclusion,for future work we would like to explore if their success transfers to other generation tasks with mlms where inference efficiency is a concern.
2020.emnlp-main.466.txt,2020,7 Conclusion & Future Work,"furthermore, future work may build on the ambigqa task with more open-ended approaches such as (1) applying the approach to qa over structured data (such as ambiguous questions that require returning tables), (2) handling questions with no answer or ill-formed questions that require inferring and satisfying more complex ambiguous information needs, and (3) more carefully evaluating usefulness to end users."
2020.emnlp-main.466.txt,2020,7 Conclusion & Future Work,"future research developing on ambigqa models may include explicitly modeling ambiguity over events and entities or in the retrieval step, as well as improving performance on the difficult problems of answer recall and question disambiguation."
2020.emnlp-main.468.txt,2020,8 Conclusions and Future Work,extension of this work to unanswerable and boolean questions is also a future work direction.
2020.emnlp-main.468.txt,2020,8 Conclusions and Future Work,"more generally application of this work to multi dataset question generation with datasets such as multiqa (talmor and berant, 2019) is a promising avenue for future work."
2020.emnlp-main.468.txt,2020,8 Conclusions and Future Work,of particular interest for future work is handling low-resource question answering domains.
2020.emnlp-main.468.txt,2020,8 Conclusions and Future Work,we build upon existing work in large scale language modeling and question generation to push the quality of synthetic question generation.
2020.emnlp-main.468.txt,2020,8 Conclusions and Future Work,we hope that better synthetic questions will enable new breakthroughs in question answering systems and related natural language tasks.
2020.emnlp-main.469.txt,2020,5 Conclusion,"in the future, we plan to improve mrl-cqa by designing a retriever that could be optimized jointly with the programmer under the meta-learning paradigm, instead of manually pre-defining a static relevance function."
2020.emnlp-main.469.txt,2020,5 Conclusion,"other potential directions of research could be toward learning to cluster questions into fine-grained groups and assign each group a set of specific initial parameters, making the model finetune the parameters more precisely."
2020.emnlp-main.47.txt,2020,6 Conclusion,"in developing this pipeline to examine how authors depict the transmission of information within narrative texts, we hope to drive a variety of future research in this space, including not only such narratological questions as how “gossip impels plots” (spacks, 1985), but also questions pertaining to issues of bias in representation, the flow of information, and factuality."
2020.emnlp-main.470.txt,2020,6 Conclusion,"finally, we would also like to apply our models to languages with even less resources available to help coping with the problem of offensive language in social media."
2020.emnlp-main.470.txt,2020,6 Conclusion,"in future work, we would like to further evaluate our models using solid, a novel large english dataset with over 9 million tweets (rosenthal et al., 2020), along with datasets in four other languages (arabic, danish, greek, and turkish) that were made available for the second edition of offenseval (zampieri et al., 2020)."
2020.emnlp-main.470.txt,2020,6 Conclusion,"this opens exciting new avenues for future research considering the multitude of phenomena (e.g.hate speech, aggression, cyberbulling), annotation schemes and guidelines used in offensive language datasets."
2020.emnlp-main.472.txt,2020,11 Conclusion,"ultimately, we hope our work can open up new horizons for studying mds in various languages."
2020.emnlp-main.473.txt,2020,5 Conclusion,we hope our findings can pave the way for further inclusion of diverse language in future nlg models.
2020.emnlp-main.474.txt,2020,5 Conclusion,"in the future, we would like to extend our method to enhance the back-translation method in multidomain settings."
2020.emnlp-main.477.txt,2020,6 Conclusion,"it is an interesting question for future work whether strong alignment always comes at a cost, or if better training techniques will lead to models that can improve on all these measures simultaneously."
2020.emnlp-main.478.txt,2020,8 Conclusion,"additionally, it would be valuable to examine whether our method can improve the ocr on highresource languages, which typically have much better recognition rates in the first pass transcription than the endangered languages in our dataset."
2020.emnlp-main.478.txt,2020,8 Conclusion,"as future work, we plan to investigate the effect of using other available data for the three languages (for example, word lists collected by documentary linguists or the additional griko folk tales collected by anastasopoulos et al.(2018))."
2020.emnlp-main.478.txt,2020,8 Conclusion,"future work will focus on large-scale digitization of scanned documents, aiming to expand our ocr benchmark on as many endangered languages as possible, in the hope of both easing linguistic documentation and preservation efforts and collecting enough data for nlp system development in under-represented languages."
2020.emnlp-main.479.txt,2020,8 Conclusion,future directions include other pre-training or fine-tuning methods to improve retrieval performance and methods that encourage the lm to predict entities of the right types.
2020.emnlp-main.48.txt,2020,8 Conclusion,comprehensive modeling of social norms presents a promising challenge for nlp work in the future.
2020.emnlp-main.480.txt,2020,7 Conclusion & Future Works,additional work could also leverage aligned documents as supervision to learn better cross-lingual document representations.
2020.emnlp-main.480.txt,2020,7 Conclusion & Future Works,one natural followup to this work is to develop techniques to better mine parallel sentences from these aligned documents – especially for low-resource language pairs.
2020.emnlp-main.482.txt,2020,6 Conclusion and Future Work,another line of future work is to investigate alternative user interfaces.
2020.emnlp-main.482.txt,2020,6 Conclusion and Future Work,"in the future, we plan to train a policy that dynamically combines the two interactions with reinforcement learning (fang et al., 2017)."
2020.emnlp-main.482.txt,2020,6 Conclusion and Future Work,"the keyword ranking and the embedding refinement modules build upon existing methods for interpreting neural networks (li et al., 2016) and fine-tuning word embeddings (mrkšic´ et al., 2017)."
2020.emnlp-main.482.txt,2020,6 Conclusion and Future Work,"therefore, future advances in these areas may also improve clime."
2020.emnlp-main.482.txt,2020,6 Conclusion and Future Work,we also explore a simple combination of active learning and clime.
2020.emnlp-main.486.txt,2020,5 Conclusions,"for future work, we will apply hit to other languages, and further explore potential cases of overlapping entities in nested ner task."
2020.emnlp-main.489.txt,2020,7 Discussion,"this is just a first step towards the goal of fully-automated interpretable evaluation, and applications to new attributes and tasks beyond ner are promising future directions."
2020.emnlp-main.49.txt,2020,6 Conclusion,"for future work, it would be interesting to try incorporating broader context (e.g., paragraph/document-level context (ji and grishman, 2008; huang and riloff, 2011; du and cardie, 2020) in our methods to improve the accuracy of the predictions."
2020.emnlp-main.490.txt,2020,4 Conclusion,we hope to provide new guidance for the future slot filling work.
2020.emnlp-main.491.txt,2020,5 Conclusion,"since our framework can be used with any pretrained autoencoder, it will benefit from large-scale pretraining in future research."
2020.emnlp-main.494.txt,2020,7 Conclusion,"one branch of ideas involves incorporating more advanced il algorithms beyond dagger, such as lols (chang et al., 2015), to further improve the distillation process."
2020.emnlp-main.494.txt,2020,7 Conclusion,we are excited about several possible avenues for future work.
2020.emnlp-main.499.txt,2020,5 Conclusion,"in the future, we would like to explore data-free distillation on more complex tasks."
2020.emnlp-main.499.txt,2020,5 Conclusion,"to dynamically adjust synthetic samples according to students’ situations, we involve an adversarial self-supervised module to quantify students’ abilities."
2020.emnlp-main.5.txt,2020,8 Conclusions,"future work, can investigate whether finer ratings could correct the bias in favor of lower effort ratings, and how this may interact with document-level evaluation."
2020.emnlp-main.50.txt,2020,7 Conclusions and Future Work,"in the future, we aim to extend graph schemas to encode hierarchical and temporal relations, as well as rich ontologies in open domain."
2020.emnlp-main.50.txt,2020,7 Conclusions and Future Work,"we will also assemble our graph schemas to represent more complex scenarios involving multiple events, so they can be applied to more downstream applications including event graph completion and event prediction."
2020.emnlp-main.500.txt,2020,6 Conclusion,"thus, enhancing language models to generate more semantically related perturbations can be one possible solution to perfect bert-attack in the future."
2020.emnlp-main.502.txt,2020,7 Conclusion and Future Work,"in the future, we will extend the similar approach to multilingual (yu et al., 2020a) or crosslingual (upadhyay et al., 2018) lexical entailment tasks."
2020.emnlp-main.502.txt,2020,7 Conclusion and Future Work,"moreover, one interesting direction is to use hyperbolic embeddings (le et al., 2019; balazevic et al., 2019) for pattern-based models due to their inherent modeling ability of hierarchies."
2020.emnlp-main.504.txt,2020,7 Conclusion,"for future work, we intend to scale srefkb to a multilingual version and explore the possibilities of using the multilingual wordnet so that abundant knowledge regarding english can be transferred to other languages."
2020.emnlp-main.504.txt,2020,7 Conclusion,it is also worth investigating regarding how to better incorporate sense embedding into other downstream tasks.
2020.emnlp-main.508.txt,2020,6 Conclusion,we believe this approach can power future analyses of pre-trained text generation systems.
2020.emnlp-main.509.txt,2020,5 Conclusion,"the method can be extended to other text genres such as public policies to aid reader comprehension, which will be our future work to explore."
2020.emnlp-main.51.txt,2020,5 Conclusion,"for future work, we plan to extend the framework towards an end-to-end system with event extraction."
2020.emnlp-main.51.txt,2020,5 Conclusion,we also seek to extend the conjunctive constraints along with event argument relations.
2020.emnlp-main.510.txt,2020,5 Conclusions,"the promising empirical results motivate us to explore further the integration of more external knowledge and other rich forms of supervisions (e.g., constraints, interactions, auxiliary models, adversaries) (hu and xing, 2020; ziegler et al., 2019) in learning."
2020.emnlp-main.510.txt,2020,5 Conclusions,"we are also interested in extending the aspect-based summarization in more application scenarios (e.g., summarizing a document corpus)."
2020.emnlp-main.512.txt,2020,5 Conclusion,"from the decoding side, a promising direction would be to make global inference in a more efficient way."
2020.emnlp-main.512.txt,2020,5 Conclusion,there are some possible future directions from our work.
2020.emnlp-main.513.txt,2020,6 Conclusion,for future work we are interested in exploring how definition modeling could be adapted to a multilingual or cross-lingual setting.
2020.emnlp-main.515.txt,2020,5 Conclusion and Future Work,"in the future, we are interested in replacing bert with knowledge enhanced and number sensitive text representations models (cao et al., 2017; geva et al., 2020)."
2020.emnlp-main.516.txt,2020,7 Conclusion,"in the future, we want to extend our system to other few-shot sequence tagging problems such as part-of-speech tagging and slot filling."
2020.emnlp-main.517.txt,2020,4 Concluding Remarks,one immediate future work is to generate explanations for model predictions using structured vector.
2020.emnlp-main.519.txt,2020,7 Conclusion,future work includes: • enriching entity representations by adding entity type and entity graph information; • modeling coherence by jointly resolving mentions in a document; • extending our work to other languages and other domains; • joint models for mention detection and entity linking.
2020.emnlp-main.520.txt,2020,7 Conclusion,an exciting direction is to leverage visuals of each step to deal with unmentioned entities and indirect effects.
2020.emnlp-main.520.txt,2020,7 Conclusion,"as future work, we will explore more sophisticated models that can address the highlighted shortcomings of the current model."
2020.emnlp-main.521.txt,2020,7 Conclusion,"given that our experiments show a 25% increase in the candidate generation, one future research direction is to improve candidate ranking in lrl by incorporating coherence statistics and entity types."
2020.emnlp-main.521.txt,2020,7 Conclusion,"moreover, given the effectiveness of query logs, we believe it can be applied to other cross-lingual tasks like relation extraction and knowledge base completion."
2020.emnlp-main.523.txt,2020,6 Conclusions,"future work involves applying luke to domain-specific tasks, such as those in biomedical and legal domains."
2020.emnlp-main.524.txt,2020,8 Conclusion,"future directions include exploration of other knowledge bases to help the inference process and applying our simile generation approach to different creative nlg tasks such as pun (he et al., 2019), sarcasm (chakrabarty et al., 2020), and hyperbole (troiano et al., 2018)."
2020.emnlp-main.527.txt,2020,8 Conclusion and Future Work,"another interesting line of future work is to investigate the use of t2g2 for generating user utterances, which could be useful for dialogue data augmentation and user simulation."
2020.emnlp-main.527.txt,2020,8 Conclusion and Future Work,we also hope to apply t2g2 to languages other than english.
2020.emnlp-main.527.txt,2020,8 Conclusion and Future Work,"while in this paper we use standard pre-trained models, designing pre-training tasks tailored to sentence fusion is an interesting line of future work."
2020.emnlp-main.528.txt,2020,6 Conclusion,future work involves collecting data to addresses weaknesses of lerc.
2020.emnlp-main.529.txt,2020,6 Conclusion,"our results suggest several promising directions: although our ablation tests show the effect of each self-supervision module, types of plan keywords, and the amount of keywords with respect to generation quality, there are more spaces to explore in self-supervised text planning."
2020.emnlp-main.529.txt,2020,6 Conclusion,"our selfsupervised planning, in addition to other types of planning (e.g., discourse, goals, coreference, tenses) can be an important step toward modeling a long-term coherence in text generation."
2020.emnlp-main.529.txt,2020,6 Conclusion,predicting such structural plans from context and imposing them into the generator would be a potential direction for future work.
2020.emnlp-main.529.txt,2020,6 Conclusion,"second, we can extend the set of plan keywords to be more structured like a discourse tree."
2020.emnlp-main.53.txt,2020,5 Conclusion and Future Work,"in the future, we will extend this approach to argument role induction to discover complete event schemas."
2020.emnlp-main.532.txt,2020,6 Conclusions and Future Work,"in the future, we will explore fullfledged solutions to address the privacy concerns of both humans and dialogue systems."
2020.emnlp-main.532.txt,2020,6 Conclusions and Future Work,we hope this work and the dataset will pave the way for the research on privacy leakage in conversations.
2020.emnlp-main.534.txt,2020,5 Conclusion,"for future work, we will incorporate pre-trained models into our framework (e.g., bert as a teacher and gpt as a student) to further unlock the performance improvement and explore how to balance diverse prior knowledge from multiple teachers."
2020.emnlp-main.539.txt,2020,7 Conclusion and Discussion,we further test the proposed method on two downstream tasks.
2020.emnlp-main.541.txt,2020,6 Conclusion,"interesting future work includes developing a fast and efficient version of re-net, and modeling lasting events and performing inference on the long-lasting graph structures."
2020.emnlp-main.543.txt,2020,7 Conclusion & Future Work,"we hope future research to explore scenarios where human intuition is not working as well as text classification, such as graph attention (velickovic et al., 2017)."
2020.emnlp-main.544.txt,2020,5 Conclusion,"in future work, we plan to validate its effectiveness for aspect-level sentiment classification."
2020.emnlp-main.545.txt,2020,4 Conclusion,"in future work, we will try other types of single networks (e.g., (lai et al., 2015; yang et al., 2016; shimura et al., 2019))."
2020.emnlp-main.546.txt,2020,5 Conclusion,larger pre-training dataset with supervised labels or self-supervised learning strategies could be explored.
2020.emnlp-main.546.txt,2020,5 Conclusion,we plan to investigate these in future.
2020.emnlp-main.548.txt,2020,6 Conclusion and Future Work,"meanwhile, extending these models to a larger scope of question types or more complex scenarios is still a challenge, and we will further investigate the trade-off between explainability and scalability."
2020.emnlp-main.549.txt,2020,5 Conclusion,"in the future, we plan to extend our model to learn the heterogeneous graph automatically, which assures more flexibility for numerical reasoning."
2020.emnlp-main.55.txt,2020,8 Conclusion,"possible future work includes (1) exploring other applications of diverse paraphrasing, such as data augmentation; (2) performing style transfer at a paragraph level; (3) performing style transfer for styles unseen during training, using few exemplars provided during inference."
2020.emnlp-main.554.txt,2020,7 Discussion,"a first step in future work would be to test if the results of this paper hold on transformer architectures, or if instead transformers result in different patterns of structural encoding transfer."
2020.emnlp-main.554.txt,2020,7 Discussion,"future work expanding on our results could focus on ablating specific structural features by creating hypothetical languages that differ in single grammatical features from the l2, in the style of galactic dependencies (wang and eisner, 2016), and testing the effect of structured data that’s completely unrelated to language, such as images."
2020.emnlp-main.556.txt,2020,7 Ethical Considerations and Conclusion,our methodology relies on a number of limitations that should be considered in understanding the scope of our conclusions.
2020.emnlp-main.556.txt,2020,7 Ethical Considerations and Conclusion,"we hope future work may better address this limitation, as in the work of cao and daume iii ´ (2019)."
2020.emnlp-main.557.txt,2020,7 Conclusion,we hope our findings and probing dataset will provide a basis for improving pre-trained masked language models’ numerical and other concrete types of commonsense knowledge.
2020.emnlp-main.558.txt,2020,5 Conclusion and Future work,"in future work, we will consider how to interpret environment specifications to facilitate grounded adaptation in these other areas."
2020.emnlp-main.559.txt,2020,7 Conclusion and Future Work,"in the future, we will look into more accurate confidence measure via neural network calibration (guo et al., 2017) or using machine learning components (e.g., answer triggering (zhao et al., 2017) or a reinforced active selector (fang et al., 2017))."
2020.emnlp-main.559.txt,2020,7 Conclusion and Future Work,one important future work is thus to conduct large-scale user studies and train parsers from real user interaction.
2020.emnlp-main.56.txt,2020,7 Conclusion and Future Work,"based on the ac-nlg method, in the future, we can explore the following directions: (1) improve the accuracy of judgment on a claim-level.(2) add external knowledge (e.g.a logic graph) to the predictor for the interpretability of the model."
2020.emnlp-main.560.txt,2020,7 Conclusion and Future work,"for future work, we will explore methods attempting to solve hard and extra hard questions."
2020.emnlp-main.561.txt,2020,8 Conclusion and Future Work,"in the future, we are interested in distilling and reusing the common knowledge from users’ selections."
2020.emnlp-main.562.txt,2020,7 Conclusion,"for future work, we will continually improve the scale and quality of our dataset, to facilitate future research and to meet the need of database-oriented applications."
2020.emnlp-main.563.txt,2020,5 Conclusion and Future Work,we also plan to extend our approach to cope with multitable text-to-sql task spider.
2020.emnlp-main.564.txt,2020,6 Conclusion,"our study sheds light on the characteristics of text-tosql parsing for future efforts including advanced modeling, problem identification, dataset construction and model evaluation."
2020.emnlp-main.564.txt,2020,6 Conclusion,we critically examine the role of schema linking for the text-to-sql task.
2020.emnlp-main.565.txt,2020,6 Conclusion,further research may be concerned with zero-shot learning on new categories.
2020.emnlp-main.565.txt,2020,6 Conclusion,"in this paper, in order to make multi-task learning feasible for incremental learning, we proposed cne-net with different attention mechanisms."
2020.emnlp-main.566.txt,2020,4 Conclusion,"besides, there are still two important directions for future work: (1) how to apply task-guided pre-training to general domain data when the indomain data is limited.(2) how to design more effective strategies to capture domain-specific and task-specific patterns for selective masking."
2020.emnlp-main.568.txt,2020,6 Conclusion,"another promising direction is to leverage taxonomy construction algorithms (huang et al., 2020) to capture more fine-grained aspects, such as “smell” and “taste” for “food”."
2020.emnlp-main.568.txt,2020,6 Conclusion,"in the future, we plan to adapt our methods to more general applications that are not restricted to the field of sentiment analysis, such as doing multiple-dimension classification (e.g., topic, location) on general text corpus."
2020.emnlp-main.569.txt,2020,7 Conclusions and Future Work,"in the future, we will explore the latent information between peer reviews and author responses to improve argument pair extraction."
2020.emnlp-main.570.txt,2020,5 Conclusion,"in the future, we plan to further improve d-miln with aspect-level annotations and find appropriate way to combine d-miln with pre-training methods (tian et al., 2020)."
2020.emnlp-main.571.txt,2020,6 Conclusion,"to stimulate research on this topic, we make hypo-cn publicly available.8 in future work, we plan to use hypo and hypo-cn to conduct a cross-lingual study on whether there are differences in the way exaggeration is expressed in english and chinese."
2020.emnlp-main.574.txt,2020,7 Conclusions and future work,"furthermore, we expect to extend the scope of analysis from the attention to an entire transformer architecture to better understand the inner workings and linguistic capabilities of the current powerful systems in nlp."
2020.emnlp-main.574.txt,2020,7 Conclusions and future work,"in future work, we plan to apply our norm-based analysis to attention in other models, such as finetuned bert, roberta (liu et al., 2019), and albert (lan et al., 2020)."
2020.emnlp-main.574.txt,2020,7 Conclusions and future work,we hope that this paper will inspire researchers to have a broader view of the possible methodological choices for analyzing the behavior of transformer-based models.
2020.emnlp-main.576.txt,2020,8 Discussion,"although our results have some implications on them, we leave a detailed study on context-free languages for future work."
2020.emnlp-main.576.txt,2020,8 Discussion,regular and counter languages model some aspects of natural language while contextfree languages model other aspects such as hierarchical dependencies.
2020.emnlp-main.58.txt,2020,7 Conclusion,"we presented delorean, an unsupervised lmbased approach to generate text conditioned on past context as well as future constraints, through forward and backward passes considering each condition."
2020.emnlp-main.581.txt,2020,6 Conclusion and Future Work,"it is inspiring and promising to be generalized to more rewriting tasks, which will be studied as our future work."
2020.emnlp-main.582.txt,2020,6 Conclusion and Future Work,"in the future, there are several prospective research directions: (1) we introduce a distant supervision (ds) assumption in our mrp training task."
2020.emnlp-main.583.txt,2020,4 Conclusions,our results suggest that future works introducing graph structure into nlp tasks should explain their necessity and superiority.
2020.emnlp-main.583.txt,2020,4 Conclusions,this study set out to investigate whether graph structure is necessary for multi-hop qa and what role it plays.
2020.emnlp-main.584.txt,2020,7 Conclusions,"as for future work, we plan to investigate using languages other than english for training (e.g., our larger french and german training sets) in our cross-lingual transfer experiments, since english may not always be the optimal source language (anastasopoulos and neubig, 2020)."
2020.emnlp-main.586.txt,2020,5 Further Discussion and Conclusion,"given the current large gaps between monolingual and multilingual lms, we will also focus on lightweight methods to enrich lexical content in multilingual lms (wang et al., 2020; pfeiffer et al., 2020)."
2020.emnlp-main.586.txt,2020,5 Further Discussion and Conclusion,"in future work, we plan to investigate how domains of external corpora affect aoc configurations, and how to sample representative contexts from the corpora."
2020.emnlp-main.586.txt,2020,5 Further Discussion and Conclusion,"in-depth analyses of these factors are out of the scope of this work, but they warrant further investigations.opening future research avenues."
2020.emnlp-main.586.txt,2020,5 Further Discussion and Conclusion,"we will also extend the study to more languages, more lexical semantic probes, and other larger underlying lms."
2020.emnlp-main.588.txt,2020,8 Conclusion,"in future work, we hope that slurp will be a valuable resource for developing e2e-slu systems, as well as more traditional pipeline approaches to slu."
2020.emnlp-main.588.txt,2020,8 Conclusion,"the next step is to extend slurp with spontaneous speech, which would again increase its complexity, but also move it one step closer to real-life applications."
2020.emnlp-main.589.txt,2020,6 Recommendations & Conclusion,for model creators: (1) model probing and experimenting with perturbed inputs can give deep insights about how a model is reasoning (2) experimenting with adversarial inputs early on in the design process can help build better models.
2020.emnlp-main.594.txt,2020,7 Conclusions,"also, given the recent success of models such as elmo and bert, it would be interesting to explore extensions of graphglove to the class of contextualized embeddings."
2020.emnlp-main.594.txt,2020,7 Conclusions,"possible directions for future work include using graphglove for unsupervised hypernymy detection, analyzing undesirable word associations, comparing learned graph topologies for different languages, and downstream applications such as sequence classification."
2020.emnlp-main.596.txt,2020,7 Conclusion,"in the future, we aim at applying stare for node and graph classification tasks as well as extend our approach to large-scale kgs."
2020.emnlp-main.596.txt,2020,7 Conclusion,"in the future, we plan to enrich wd50k entities with class labels and probe it against node classification tasks."
2020.emnlp-main.597.txt,2020,6 Conclusion,"in future studies, we plan to increase the number of dimensions of the relational position encodings, since a scalar value may not be able to express positional information adequately."
2020.emnlp-main.598.txt,2020,8 Conclusion,"in future work, we plan to extend our methodology to new languages, and experiment with multilingual and language specific bert models."
2020.emnlp-main.60.txt,2020,6 Conclusion,we plan to investigate other automatic tools in curating more accurate denotation graphs with a complex composition of fine-grained concepts for future directions.
2020.emnlp-main.600.txt,2020,5 Conclusion,"in future work, we plan to extend our approach to further improve the reward and policy functions, and to reduce the human-labeling factor."
2020.emnlp-main.601.txt,2020,7 Conclusion,there are exciting avenues for multilingual work to account for language and cultural differences.
2020.emnlp-main.603.txt,2020,5 Conclusions and Future Work,"this work can be extended in several ways: (i) we plan to investigate into further functions for τ to enhance the exploration-exploitation tradeoff.(ii) additional strategies to assign nuclearity should be explored, considering the excessive n-nclassification shown in our evaluation.(iii) we plan to apply our approach to more sentiment datasets (e.g., diao et al.(2014)), creating even larger treebanks.(iv) our new and scalable solution can be extended to also predict discourse relations besides structure and nuclearity.(v) we also plan to use a neural discourse parser (e.g."
2020.emnlp-main.603.txt,2020,5 Conclusions and Future Work,"yu et al.(2018)) in combination with our large-scale treebank to fully leverage the potential of data-driven discourse parsing approaches.(vi) taking advantage of the new mega-dt corpus, we want to revisit the potential of discourse-guided sentiment analysis, to enhance current systems, especially for long documents.(vii) finally, more long term, we intend to explore other auxiliary tasks for distant supervision of discourse, like summarization, question answering and machine translation, for which plenty of annotated data exists (e.g., nallapati et al.(2016); cohan et al.(2018); rajpurkar et al.(2016, 2018))."
2020.emnlp-main.605.txt,2020,7 Conclusion and Future Work,"moreover, we intend to instantiate our proposed framework to other domains such as teacher/student conversations and other types of discourse such as social media narratives."
2020.emnlp-main.605.txt,2020,7 Conclusion and Future Work,we also wish to expand the current face framework to a more comprehensive politeness framework that incorporates notions of power and social distance between the interlocutors.
2020.emnlp-main.607.txt,2020,6 Conclusions,"in future work, we would like to further investigate few and zero-shot learning in lmtc, especially in bert models that are currently unable to cope with zero-shot labels."
2020.emnlp-main.608.txt,2020,9 Conclusions,"we cover background on contextualized encoders, pretraining objectives, efficiency, data, approaches in model interpretability, and research in multilingual systems."
2020.emnlp-main.609.txt,2020,8 Conclusion and future work,"this last challenge will be especially crucial for future work that seeks to verify scientific claims against sources other than the research literature – for instance, social media and the news."
2020.emnlp-main.609.txt,2020,8 Conclusion and future work,"we hope that the resources presented in this paper encourage future research on these important challenges, and help facilitate progress toward the broader goal of scientific document understanding."
2020.emnlp-main.61.txt,2020,8 Conclusion,"our evaluation verifies the effectiveness of our method, while also indicating a scope for further study, enhancement, and extensions in the future."
2020.emnlp-main.610.txt,2020,6 Conclusion,"we invite future research into further integration of syntactic methods into shallow semantic analysis in other languages and other formulations, such as frame-semantic parsing, and other semantically oriented tasks."
2020.emnlp-main.611.txt,2020,6 Conclusion and Future Work,"in addition, since parade provides entities like “machine code” for definitions, this new dataset could also be useful for other tasks like entity linking (shen et al., 2014), entity retrieval (petkova and croft, 2007) and entity or word sense disambiguation (navigli, 2009)."
2020.emnlp-main.611.txt,2020,6 Conclusion and Future Work,"in the future, we will continue to investigate effective ways to obtain domain knowledge and incorporate it into enhanced models for paraphrase identification."
2020.emnlp-main.612.txt,2020,6 Conclusions and Future Work,"a causal definition is in no way limited to this pairwise case, and future work may generalize it to the sequential case or to event representations that are compositional."
2020.emnlp-main.612.txt,2020,6 Conclusions and Future Work,"having a causal model shines a light on the assumptions made here, and indeed, future work may further refine or overhaul them, a process which may further shine a light on the nature of the knowledge we are after."
2020.emnlp-main.613.txt,2020,7 Conclusion,"based on our analysis, future work in the direction of automatic bias mitigation may include identifying potentially biased examples in an online fashion and discouraging models from exploiting them throughout the training."
2020.emnlp-main.615.txt,2020,7 Conclusions,"also, we would like to explore how to overcome the obstacles that prevent us from fully exploiting large pretrained lms (e.g., gpt-2) in low-resource settings."
2020.emnlp-main.615.txt,2020,7 Conclusions,"in future work, we intend to experiment with the lm-prior under more challenging conditions, such as when there is domain discrepancy between the parallel and monolingual data."
2020.emnlp-main.616.txt,2020,6 Conclusion,"as a continuation to this work, we intend to evaluate whether multilingual translation models are more resilient to lexical disambiguation biases and, as a consequence, are less susceptible to adversarial attacks that exploit source-side homography."
2020.emnlp-main.616.txt,2020,6 Conclusion,extending model-agnostic attack strategies to incorporate other types of dataset biases and to target natural language processing tasks other than machine translation is likewise a promising avenue for future research.
2020.emnlp-main.617.txt,2020,8 Conclusion,"in future work, we will apply mad-x to other pre-trained models, and employ adapters that are particularly suited for languages with certain properties (e.g.with different scripts)."
2020.emnlp-main.617.txt,2020,8 Conclusion,"we will also evaluate on additional tasks, and investigate leveraging pre-trained language adapters from related languages for improved transfer to truly low-resource languages with limited monolingual data."
2020.emnlp-main.618.txt,2020,7 Conclusions,"so as to facilitate similar studies in the future, we release our nli dataset,13 which, unlike previous benchmarks, was annotated in a non-english language and human translated into english."
2020.emnlp-main.619.txt,2020,7 Conclusion,"additionally, in the future, we would also want to quantify the impact of varying degrees of granularity of learning emotional features from tweets on statenet’s performance."
2020.emnlp-main.619.txt,2020,7 Conclusion,"priority-based suicide risk assessment for ranking tweets for suicidal risk, rather than classifying them forms our future direction."
2020.emnlp-main.619.txt,2020,7 Conclusion,"through this work, we aim to form a component in a larger human-in-the-loop infrastructure for analyzing potentially concerning suicide-related social media posts."
2020.emnlp-main.619.txt,2020,7 Conclusion,we plan to explore the impact of varying amounts of historical context for a user in our future work.
2020.emnlp-main.62.txt,2020,7 Conclusion and Future Work,future work would be well-suited to explore 1) methods for better understanding which datasets (and individual instances) can be rebalanced and which cannot; and 2) the non-trivial task of estimating additive human baselines to compare against.• hypothesis 2: modeling feature interactions can be data-hungry.
2020.emnlp-main.62.txt,2020,7 Conclusion and Future Work,"so, we may need models with different inductive biases and/or much more training data."
2020.emnlp-main.62.txt,2020,7 Conclusion and Future Work,"we postulate the following potential explanations, pointing towards future work: • hypothesis 1: these unbalanced tasks don’t require complex cross-modal reasoning."
2020.emnlp-main.623.txt,2020,6 Conclusion and Future work,"furthermore, we hope to explore the quality of fact-checking explanations with respect to properties other than coherence, e.g., actionability and impartiality.lastly, we plan to explore congruity between veracity prediction and explanation generation tasks, i.e., generating explanations which are compatible with the predicted label and vice versa."
2020.emnlp-main.623.txt,2020,6 Conclusion and Future work,"in order to do this, we hope to explore other subjects, in addition to public health, for which factchecking requires a level of expertise in the subject area."
2020.emnlp-main.623.txt,2020,6 Conclusion and Future work,we hope to explore the topics of explainable fact-checking and specialist fact-checking further.
2020.emnlp-main.624.txt,2020,5 Conclusion,"our formulation also bridges broader nlu/rc techniques to address other critical challenges in if games for future work, e.g., common-sense reasoning, noveltydriven exploration, and multi-hop inference."
2020.emnlp-main.626.txt,2020,9 Conclusion,we believe that the contributions made in this work would also generalize to other kinds of expert-lay dialogue like customer-service chats.
2020.emnlp-main.628.txt,2020,6 Conclusions,"in the future, we will investigate the properties of our proposed method on verifying statements with more complicated operations and explore the explainability of the model."
2020.emnlp-main.630.txt,2020,6 Conclusion,future work could investigate the use of non-expert human raters to improve the dataset quality further.
2020.emnlp-main.630.txt,2020,6 Conclusion,"in pursuit of improved entity representations, future work could explore the joint use of complementary multi-language descriptions per entity, methods to update representations in a light-weight fashion when descriptions change, and incorporate relational information stored in the kb."
2020.emnlp-main.632.txt,2020,8 Conclusion,"these preliminary results pave the way for further experiments with other language models, various architectures and new downstream tasks."
2020.emnlp-main.633.txt,2020,6 Discussion,"although our model has achieved good performance compressing bert, it would be interesting to explore its possible applications in other neural models."
2020.emnlp-main.633.txt,2020,6 Discussion,"for future work, we would like to explore the possibility of applying theseus compression on heterogeneous network modules."
2020.emnlp-main.633.txt,2020,6 Discussion,"in addition, we would like to conduct theseus compression on more types of neural networks including convolutional neural networks and graph neural networks."
2020.emnlp-main.633.txt,2020,6 Discussion,"therefore, it is potential to apply theseus compression to other large models (e.g., resnet (he et al., 2016) in computer vision)."
2020.emnlp-main.633.txt,2020,6 Discussion,"we will also investigate the combination of our compression-based approach with recently proposed dynamic acceleration method (zhou et al., 2020b) to further improve the efficiency of pretrained language models."
2020.emnlp-main.635.txt,2020,5 Conclusion,these task embeddings allow us to predict source tasks that will likely improve target task performance.
2020.emnlp-main.636.txt,2020,5 Conclusion & Future Work,"as part of future work, we will explore cvt on other sequence-labeling tasks such as chunking, elementary discourse unit segmentation and argumentative discourse unit segmentation, thus moving beyond entity-level spans."
2020.emnlp-main.636.txt,2020,5 Conclusion & Future Work,"furthermore, we intend to implement cvt as a training strategy over transformers (bert) and compare it with adaptivelypretrained bert."
2020.emnlp-main.636.txt,2020,5 Conclusion & Future Work,"moreover, other supervised tasks such as classification could also be studied in this context."
2020.emnlp-main.637.txt,2020,8 Conclusion,future work may focus on finding representations that encode the most important information for al.
2020.emnlp-main.638.txt,2020,6 Conclusions,a natural future direction is to conduct a similar empirical investigation of al over bert in the context of multi-class classification and regression tasks.
2020.emnlp-main.638.txt,2020,6 Conclusions,"it would also be interesting to investigate the realm of larger annotation budgets, and more recent bert variants (liu et al., 2019; lan et al., 2019)."
2020.emnlp-main.638.txt,2020,6 Conclusions,"the development of novel al methods, that are tailored for pre-trained models such as bert, seems like an important direction for future work."
2020.emnlp-main.638.txt,2020,6 Conclusions,"we hope that the experimental results and analyses reported here, as well as the release of the research framework we developed, would be instrumental for these and other future studies."
2020.emnlp-main.639.txt,2020,6 Conclusion,we hope that this work can help inform researchers of considerations to make when using lpx models in the presence of domain shift.
2020.emnlp-main.64.txt,2020,5 Conclusion,"in the future, we will investigate debiasing retrieval-based dialogue models and more complicated pipeline-based dialogue systems."
2020.emnlp-main.640.txt,2020,6 Conclusion and Future Work,"distilling models to their vvma counterparts would be an interesting experiment, and potentially an orthogonal enhancement to pre-existing frameworks (sanh et al., 2019)."
2020.emnlp-main.640.txt,2020,6 Conclusion and Future Work,"in future work, we plan to optimize the lowlevel code and to develop new hardware to deploy vvmas in real-world applications."
2020.emnlp-main.641.txt,2020,4 Conclusion,"we plan to extend these results by studying the mixing of such textual filler-oriented representations with acoustic representations, and further investigate the representation of fillers learnt during pre-training."
2020.emnlp-main.643.txt,2020,8 Conclusion and Future Work,"analyzing the demographic, cultural, and gender bias in research pertaining to financial disclosures, particularly earnings calls, forms a future direction of research."
2020.emnlp-main.643.txt,2020,8 Conclusion and Future Work,"experimenting with other sets of commonly used acoustic features such as mfcc coefficients, opensmile features and audeep features for representing audio utterances also form a future direction for audio feature extraction."
2020.emnlp-main.643.txt,2020,8 Conclusion and Future Work,"first, we want to improve upon the audio feature extraction."
2020.emnlp-main.643.txt,2020,8 Conclusion and Future Work,there are several promising directions of future work that we wish to explore.
2020.emnlp-main.643.txt,2020,8 Conclusion and Future Work,"we would also want to work on studying a wider set of earnings calls and companies spanning multiple languages, demographics, speakers and gender."
2020.emnlp-main.645.txt,2020,4 Conclusions and Future work,"future direction also include alternate architectures, reward schemes, and evaluation using human judges."
2020.emnlp-main.645.txt,2020,4 Conclusions and Future work,"recent works (lu et al., 2018; d’autume et al., 2019) have proposed some solutions to address these challenges and we plan to explore them."
2020.emnlp-main.646.txt,2020,7 Conclusion and future work,another important direction is to investigate how to integrate the ability to aggregate entities derived from training on tesa into an abstractive summarizer.
2020.emnlp-main.646.txt,2020,7 Conclusion and future work,"in future work, we would like to expand the domains covered by our dataset, which is biased towards topics found in the source corpus, such as politics."
2020.emnlp-main.646.txt,2020,7 Conclusion and future work,this would require models to tackle another challenging issue which we have not addressed: which set of entities should a model aggregate in the first place?
2020.emnlp-main.647.txt,2020,7 Conclusion,"in future works we plan to add other languages including arabic and hindi, and to investigate the adaptation of neural metrics to multilingual summarization."
2020.emnlp-main.649.txt,2020,9 Conclusion,our findings suggest that more intentional and deliberate decisions should be made in selecting summarization datasets for downstream modelling research and that further scrutiny should be placed upon summarization datasets released in the future.
2020.emnlp-main.65.txt,2020,6 Conclusion,an important future direction will be generating the distractors and learning the rationality coefficients.
2020.emnlp-main.65.txt,2020,6 Conclusion,"our self-conscious agents improved the base agents on the dialogue nli (welleck et al., 2019) and personachat (zhang et al., 2018) dataset, without consistency labels and nli models."
2020.emnlp-main.652.txt,2020,5 Conclusion,we hope this work will inspire and assist both dialogue and document modeling for tackling more real-life dialogue tasks.
2020.emnlp-main.654.txt,2020,7 Conclusion and Future Work,"finally, another interesting exploration is to extend the model with a jointly trainable movie recommendation and movie information modules."
2020.emnlp-main.654.txt,2020,7 Conclusion and Future Work,"then, we plan to investigate the strategy patterns for people with different personalities and movie preferences to make dialog system more personalized."
2020.emnlp-main.654.txt,2020,7 Conclusion and Future Work,"this work opens up several directions for future studies in building sociable and personalized recommendation dialog systems as follows: first, we will explore more ways of utilizing the strategies, including dynamic strategy selection after decoding."
2020.emnlp-main.655.txt,2020,7 Future Work and Conclusion,we hope that our dataset will encourage further interest in curiosity-driven dialog.
2020.emnlp-main.655.txt,2020,7 Future Work and Conclusion,we see two immediate directions for future work.
2020.emnlp-main.656.txt,2020,6 Conclusion,newly collected or constructed datasets should consider how to carefully craft the collection to mitigate bias issues from the very start.
2020.emnlp-main.657.txt,2020,8 Conclusions and Future Work,"future work may explore generating of diverse sets of hypotheses for a given premise and label, with the goal of performing data augmentation."
2020.emnlp-main.657.txt,2020,8 Conclusions and Future Work,other future work will be to measure the performance of gennli on adversarial and similarly challenging nli datasets.
2020.emnlp-main.658.txt,2020,5 Conclusion,bias mitigation in models and datasets remains a crucial direction for future work if systems based on datasets like the ones we study are to be widely deployed.
2020.emnlp-main.658.txt,2020,5 Conclusion,machine learning methods work on transfer learning could help to better understand and exploit the effects that drive the successes we have seen with nli data so far.
2020.emnlp-main.659.txt,2020,6 Conclusions,"finally, we give suggestions on future research directions and on better analysis variance reporting."
2020.emnlp-main.659.txt,2020,6 Conclusions,"however, large instability of current models on some of these analysis sets undermine such benefits and bring non-ignorable obstacles for future research."
2020.emnlp-main.659.txt,2020,6 Conclusions,we hope this paper will guide researchers on how to handle instability and inspire future work in this direction.
2020.emnlp-main.66.txt,2020,8 Conclusion,"tod-bert is easy-to-deploy and will be open-sourced, allowing the nlp research community to apply or fine-tune any task-oriented conversational problem."
2020.emnlp-main.660.txt,2020,5 Summary,our goal is to push forward the research and practical use of textual entailment in a broader vision of natural language processing.
2020.emnlp-main.660.txt,2020,5 Summary,the final entailment system ufo-entail generalizes well to open domain entailment benchmarks and downstream nlp tasks including question answering and coreference resolution.
2020.emnlp-main.661.txt,2020,7 Conclusion,"however, we also show limitations of our proposed methods, thereby encouraging future work on conjnli for better understanding of conjunctive semantics."
2020.emnlp-main.662.txt,2020,5 Conclusion,"first, we used nli-tr to analyze the effects of in-language pretraining."
2020.emnlp-main.662.txt,2020,5 Conclusion,we also used nli-tr to investigate central issues in turkish nli.
2020.emnlp-main.663.txt,2020,7 Conclusion,we enhance the target semantic model by incorporating syntax in a multitask learning framework.
2020.emnlp-main.664.txt,2020,5 Conclusion,this echoes the strong results seen with t5 and offers further motivation to explore these kinds of design decisions in other tasks.
2020.emnlp-main.666.txt,2020,7 Conclusions,"in the future, we plan to study how we can apply synsetexpan at the entity mention level for conducting contextualized synonym discovery and set expansion."
2020.emnlp-main.670.txt,2020,6 Conclusion and Future Work,"in the future, we are interested in effectively integrating different forms of supervision including annotated documents."
2020.emnlp-main.670.txt,2020,6 Conclusion and Future Work,this is another potential direction for the extension of our method.
2020.emnlp-main.672.txt,2020,6 Conclusion and Future Work,"finally, it would be interesting to do a deeper dive into variations of author strategies in chapterization, focusing more intently on books with large numbers of short chapters as being more reflective of episode boundaries."
2020.emnlp-main.672.txt,2020,6 Conclusion and Future Work,"our work opens up avenues for further research in text segmentation, with potential applications in summarization and discourse analysis."
2020.emnlp-main.672.txt,2020,6 Conclusion and Future Work,potential future work includes combining the neural and cut-based approaches into a stronger method.
2020.emnlp-main.674.txt,2020,7 Conclusion,we believe our work opens up the necessity of further investigation pertaining to careful information fusion techniques for downstream tasks.
2020.emnlp-main.675.txt,2020,6 Conclusions,"as next steps, we plan to address further types of revisions and extend our experiments to document-level settings."
2020.emnlp-main.676.txt,2020,8 Conclusion and Future Work,"another interesting direction of future research is to explore the cold start problem, where man-sf could be leveraged to predict stock movements for new stocks."
2020.emnlp-main.676.txt,2020,8 Conclusion and Future Work,"lastly, we would also like to extend man-sf’s architecture to not be limited to model all stocks together (because of its gat component) to increase scalability to cross-market scenarios."
2020.emnlp-main.676.txt,2020,8 Conclusion and Future Work,"we plan to further use news articles, earnings calls, and other data sources to capture market dynamics better."
2020.emnlp-main.677.txt,2020,7 Conclusions and Future Work,"future work will investigate ways to improve performance (and especially precision scores) on our data, in particular on low-support labels."
2020.emnlp-main.68.txt,2020,10 Conclusion,we hope that this study will facilitate discussions in the dialogue response generation community regarding the issue of filtering noisy corpora.
2020.emnlp-main.680.txt,2020,6 Conclusion,we hope that the dataset shall broaden the target domain of gec beyond learner and/or exam writing and facilitate the development of robust gec models in the open-domain setting.
2020.emnlp-main.682.txt,2020,6 Conclusion and Future Work,future work can use anomaly detection approaches operating on our model’s predicted word vectors to detect anomalies in a word’s representation across time.
2020.emnlp-main.682.txt,2020,6 Conclusion and Future Work,"we also plan to investigate different architectures, such as variational autoencoders (kingma and welling, 2014), and incorporate contextual representations (devlin et al., 2019; hu et al., 2019) to detect new senses of words."
2020.emnlp-main.684.txt,2020,5 Conclusion,the extension of the scope will be the future work.
2020.emnlp-main.685.txt,2020,7 Conclusion and Future Work,"in future work we plan to apply our model to longer, book length documents, and plan to add more structure to the memory."
2020.emnlp-main.687.txt,2020,8 Conclusion,we believe there is much potential for additional selfsupervision tasks and leave those for future work.
2020.emnlp-main.688.txt,2020,5 Conclusions,"in future work, we would like to study how to introduce acyclic rules to the walk-based systems."
2020.emnlp-main.689.txt,2020,6 Conclusion and Future Work,"as discussed in section 5, we also plan to develop a more flexible scoring function which can handle equivalent trees."
2020.emnlp-main.689.txt,2020,6 Conclusion and Future Work,"finally, we plan to evaluate bertft on other temporal relation datasets as part of a larger pipeline, which will include a mapping between tdts and other temporal relation annotation schemas such as the tempeval-3 dataset (uzzaman et al., 2013)."
2020.emnlp-main.689.txt,2020,6 Conclusion and Future Work,"for future research, we plan to explore other types of deep neural lms such as transformerxl (dai et al., 2019) and xlnet (yang et al., 2019)."
2020.emnlp-main.69.txt,2020,6 Conclusions,"future directions include the incorporation of phonological and morphosyntactic features, application to other languages, and most importantly, a model extension to infer temporal ordering."
2020.emnlp-main.691.txt,2020,6 Conclusion,"for future research, it is interesting to enhance seqmix with language models during the mixup process, and harness external knowledge for further improving diversity and plausibility."
2020.emnlp-main.692.txt,2020,8 Conclusions,"future work may want to build on our approach for more comprehensive extraction tasks, focussing on more types of result, as well as other information contained in papers such as architectural details and hyperparameters."
2020.emnlp-main.693.txt,2020,6 Conclusion and discussion,our solution for attribute value extraction can be extended to other nlp tasks.
2020.emnlp-main.693.txt,2020,6 Conclusion and discussion,we leave this as our future work.
2020.emnlp-main.694.txt,2020,6 Conclusion,"future work can focus on expanding the capabilities to generating whole paragraphs of text from graphs in kb, as well as converting large parts of text into coherent graph structures."
2020.emnlp-main.696.txt,2020,5 Conclusion,our work also suggests that standard model components like embedding tying should be retested as we continue to explore the space of language modeling.
2020.emnlp-main.698.txt,2020,5 Conclusion,"in future work, we hope to leverage sentence structure, such as the use of constituency parsing, to further enhance the design of the progressive hierarchy."
2020.emnlp-main.7.txt,2020,7 Conclusion,"as paraphrasing continues to improve and cover more languages, we are optimistic smrt will provide larger improvements across the board—including for higher-resource mt and for additional target languages beyond english."
2020.emnlp-main.70.txt,2020,5 Conclusions,examples would include variants that employ longer and more realistic contexts.
2020.emnlp-main.70.txt,2020,5 Conclusions,future work will be required to assess the extent to which these effects do in fact reflect the acquisition of a latent form of discourse modeling ability.
2020.emnlp-main.70.txt,2020,5 Conclusions,"since an experiment that collects data on this scale would require a substantial annotation effort, a more careful comparison of this sort must be left for future work."
2020.emnlp-main.70.txt,2020,5 Conclusions,we hope that this short paper will inspire further research that takes next steps in this and a variety of other directions.
2020.emnlp-main.700.txt,2020,5 Conclusions,our future work will explore the potential of training palm for longer on much more unlabeled text data.
2020.emnlp-main.702.txt,2020,5 Conclusion,"overall, teaforn is a promising approach for improving quality and/or reducing inference costs in sequence generation models."
2020.emnlp-main.704.txt,2020,6 Conclusion,"in the future, we believe that strong linguistic priors will continue to be a key ingredient for building nextlevel learning agents in these games."
2020.emnlp-main.705.txt,2020,7 Conclusion,we hope this work will motivate the image captioning field to learn to anticipate and provide for the information needs of specific user communities.
2020.emnlp-main.710.txt,2020,5 Conclusion,future work includes investigating the interaction and joint training between hgn and paragraph retriever for performance improvement.
2020.emnlp-main.712.txt,2020,6 Conclusions,we leave further exploration to future work.
2020.emnlp-main.713.txt,2020,7 Conclusion,"overall, this work opens up exciting avenues for leveraging methods in unsupervised learning and natural language generation to improve the interpretability and generalization of machine learning systems."
2020.emnlp-main.715.txt,2020,9 Conclusion and Future Work,"finally, we will carry out a thorough investigation into emotion-cause pairs (xia and ding, 2019)."
2020.emnlp-main.715.txt,2020,9 Conclusion and Future Work,"in the future, we plan to study how contextual information (i.e., different aspects of people’s interactions captured through contiguous posts in a discussion thread) affects the perceived emotions."
2020.emnlp-main.715.txt,2020,9 Conclusion and Future Work,we also plan to perform a cross-corpus analysis to investigate if emotions are expressed differently in the health domain compared to other domains.
2020.emnlp-main.717.txt,2020,6 Conclusion,"in addition, we will study more explicitly how to decouple stance models from sentiment, and how to improve performance further on difficult phenomena."
2020.emnlp-main.717.txt,2020,6 Conclusion,"in future work we plan to investigate additional methods to represent and use generalized topic information, such as topic modeling."
2020.emnlp-main.720.txt,2020,5 Conclusion,an important avenue of future work will be to assess to what extent there may be cultural differences in these associations (see discussion in section 3.1).
2020.emnlp-main.720.txt,2020,5 Conclusion,"similarly, variation with respect to age and other variables merits further study as well."
2020.emnlp-main.720.txt,2020,5 Conclusion,"temporal aspects could be considered in diachronic studies, to account for the fact that emoji use has been evolving."
2020.emnlp-main.722.txt,2020,6 Conclusion,"in the future, instead of pre-training on sentences, we will leverage raw text at passage or document level to alleviate the performance degeneration brought by short context."
2020.emnlp-main.722.txt,2020,6 Conclusion,"in this work, we aim at equipping pre-trained lms with structured knowledge via self-supervised tasks."
2020.emnlp-main.722.txt,2020,6 Conclusion,it masks informative mentions and facilitates learning structured knowledge in free-form text.
2020.emnlp-main.722.txt,2020,6 Conclusion,"moreover, we will use a combination of commonsense and ontological kgs, and large-scale corpora (e.g., common crawl) to pre-train an mlm from scratch, which we expect to benefit a wide range of tasks."
2020.emnlp-main.724.txt,2020,6 Conclusions,we also point out several directions for future work by generalizing our methods to other tasks or combining with other techniques.
2020.emnlp-main.725.txt,2020,5 Conclusion,"in the future, we plan to extend our model to cope with external word or document semantics."
2020.emnlp-main.725.txt,2020,5 Conclusion,it would also be interesting to explore alternative architectures other than cyclegan under our formulation of topic modeling.
2020.emnlp-main.726.txt,2020,8 Conclusion,"in the future, we plan to implement a more sophisticated guidance for the augmentation by adding syntactic and position features to the reward function, to enable augmentation of more diverse types of text data."
2020.emnlp-main.727.txt,2020,5 Conclusion and Future Work,it would be even better if the state detection and segmentation step can be integrated with subsequent state-independent feature extraction and rumor detection in an end-to-end framework.
2020.emnlp-main.727.txt,2020,5 Conclusion and Future Work,"therefore, one direction for future work is to explore an online state detection algorithm and perform it for each event, but at the same time ensure that the state of each event is globally defined."
2020.emnlp-main.728.txt,2020,5 Conclusion,"looking forward, we plan to leverage pymt5 for various downstream automated software engineering tasks—including code documentation and method generation from natural language statements—and develop more model evaluation criteria to leverage the unique properties of source codes."
2020.emnlp-main.729.txt,2020,7 Conclusion and Future Work,"first, we would like to explore more explainable reasoning method for question generation, such as symbolic-based models."
2020.emnlp-main.729.txt,2020,7 Conclusion and Future Work,"in the future, there can be two research directions."
2020.emnlp-main.73.txt,2020,8 Conclusion,"while we showed that iterative inference with a learned score function is effective for spherical gaussian priors, more work is required to investigate if such an approach will also be successful for more sophisticated priors, such as gaussian mixtures or normalizing flows."
2020.emnlp-main.730.txt,2020,7 Conclusions and Future Work,future work includes applying time inference models to question answering and other nlp systems.
2020.emnlp-main.730.txt,2020,7 Conclusions and Future Work,we also seek to annotate information about dates and seasons.
2020.emnlp-main.731.txt,2020,8 Conclusion,"for structural generalization cases, the results of bowman et al.(2015); evans et al.(2018) and mccoy et al.(2019) suggest that treestructured models may provide a better inductive bias."
2020.emnlp-main.731.txt,2020,8 Conclusion,what architecture would be needed to solve cogs?
2020.emnlp-main.733.txt,2020,5 Conclusion and Future Work,"in the future, we are looking forward to diving in representation learning with flow-based generative models from a broader perspective."
2020.emnlp-main.736.txt,2020,5 Conclusion,"as future work, we will explore the similar idea of designing unreferenced metrics for dialog generation."
2020.emnlp-main.737.txt,2020,5 Conclusion,"thus, future work involves extending the method to other related tasks, such as machine translation and text summarization, and investigating the potential gains from transfer learning."
2020.emnlp-main.739.txt,2020,7 Conclusion,"as future work, we would like extend the prior network to sample more than one persona sentences by expanding the sample space of the discrete random variable to generate more interesting responses."
2020.emnlp-main.74.txt,2020,5 Conclusion and Future Work,"in our future work, i) we are interested in distilling from deep nmt models into extremely small students with ckd, in the hope of achieving the same results of large models with much smaller counterparts.ii) we also try to improve the combination module and find a better alternative than concatenation.iii) finally, we plan to evaluate ckd in other tasks such as language modeling."
2020.emnlp-main.740.txt,2020,7 Conclusion and Future Work,there are some interesting directions for future work.
2020.emnlp-main.742.txt,2020,5 Conclusion and Discussion,"besides, we also release a new large-scale human evaluation bench-mark to facilitate future research on automatic metrics."
2020.emnlp-main.743.txt,2020,6 Conclusions and Future Works,"for future work, we will annotate medical entities in our datasets."
2020.emnlp-main.743.txt,2020,6 Conclusions and Future Works,such annotations can facilitate the development of goal-oriented medical dialog systems.
2020.emnlp-main.743.txt,2020,6 Conclusions and Future Works,we use transfer learning to apply these pretrained models for low-resource dialogue generation.
2020.emnlp-main.747.txt,2020,7 Concluding Discussion,"the explainable ai community hopes to use them as a guide for evaluating model explanations and, possibly, for teaching models to make robust and wellreasoned decisions."
2020.emnlp-main.748.txt,2020,5 Conclusion,it would be very interesting to see what kind of performance larger models could achieve.
2020.emnlp-main.749.txt,2020,5 Conclusion,"for future work, we plan to apply our method for other type of spans, such as noun phrases, verbs, and clauses."
2020.emnlp-main.75.txt,2020,6 Conclusion,"for future work, we are interested in investigating the proposed approach in a scaled setting with more languages and a larger amount of monolingual data."
2020.emnlp-main.75.txt,2020,6 Conclusion,"furthermore, we would also like to explore the most sample efficient strategy to add a new language to a trained mnmt system."
2020.emnlp-main.75.txt,2020,6 Conclusion,scheduling the different tasks and different types of data would be an interesting problem.
2020.emnlp-main.750.txt,2020,6 Conclusions,we hope that this work will encourage continued research into factual consistency checking of abstractive summarization models.
2020.emnlp-main.751.txt,2020,6 Implications and Future Directions,future works on meta-evaluation should investigate the effect of these settings on the performance of metrics.(2) metrics easily overfit on limited datasets.
2020.emnlp-main.752.txt,2020,7 Conclusion,"in near future, we aim to incorporate the video script information in the multimodal summarization process."
2020.emnlp-main.78.txt,2020,5 Conclusion,"future directions include continuing the exploration of this research topic for large sequenceto-sequence pre-training models (liu et al., 2020) and multi-domain translation models (wang et al., 2019b)."
2020.emnlp-main.78.txt,2020,5 Conclusion,"we also analyze the gains from perspectives of learning dynamics and linguistic probing, which give insightful research directions for future work."
2020.emnlp-main.78.txt,2020,5 Conclusion,"we will employ recent analysis methods to better understand the behaviors of rejuvenated models (he et al., 2019; yang et al., 2020)."
2020.emnlp-main.8.txt,2020,7 Conclusion and Future Work,"in future work, we would like to extend prism to paragraph- or document-level evaluation by training a paragraph- or document-level multilingual nmt system, as there is growing evidence that mt evaluation would be better conducted at the document level, rather than the sentence level (laubli et al., 2018)."
2020.emnlp-main.8.txt,2020,7 Conclusion and Future Work,we are optimistic our method will improve further as stronger multilingual nmt models become publicly available.
2020.emnlp-main.80.txt,2020,5 Conclusion,"besides, as this idea is not limited to machine translation, it is also interesting to validate our model in other nlp tasks, such as low-resource nmt model training (lample et al., 2018; wan et al., 2020) and neural architecture search (guo et al., 2020)."
2020.emnlp-main.80.txt,2020,5 Conclusion,"it is interesting to combine with other techniques (li et al., 2018; hao et al., 2019) to further improve nmt."
2020.emnlp-main.82.txt,2020,8 Conclusion,"also, we’ll try to extend our methods in a wider range of nlp tasks."
2020.emnlp-main.82.txt,2020,8 Conclusion,"in future work, firstly, since our model is randomly sampled from model distribution to generate diverse translation, it is meaningful to explore better algorithms and training strategies to represent model distribution and search for the most distinguishable results in model distribution."
2020.emnlp-main.83.txt,2020,7 Conclusion,applying these latent alignment models for parallel translation of long documents can be an interesting research direction.
2020.emnlp-main.84.txt,2020,6 Conclusion,our findings also generalize to different positions and different datasets.
2020.emnlp-main.85.txt,2020,7 Conclusion,the inclusion of counts over clusters of answers provides a very rich dataset for training and evaluation.
2020.emnlp-main.87.txt,2020,13 Discussion and Future Work,how to most efficiently and effectively adapt transformer-based qa systems remains an important topic for future research.
2020.emnlp-main.88.txt,2020,8 Conclusion,"torque has 3.2k news snippets, 9.5k hard-coded questions asking which events had happened, were ongoing, or were still in the future, and 21.2k human-generated questions querying more complex phenomena."
2020.emnlp-main.9.txt,2020,5 Discussion and Future Work,we hope that prover will encourage further work towards developing interpretable nlp models with structured explanations.
2020.emnlp-main.90.txt,2020,7 Conclusions and Future Work,"in the future, we will explore more informative generation and consider applying mgcn to other nlp tasks for better information extraction and aggregation."
2020.emnlp-main.91.txt,2020,7 Conclusion and Future Work,"moreover, future work should inspect the effect of split and rephrase on downstream tasks such as machine translation or information retrieval, and examine if models’ performance on these tasks correlate with that on our benchmarks."
2020.emnlp-main.93.txt,2020,6 Conclusions and Future Work,"in future, we plan to explore the following two directions: (1) interpolating the contexts between consecutive steps by introducing a new infilled image, and (2) addressing the underspecification problem by controlling the content in infilled image with explicit guidance."
2020.emnlp-main.96.txt,2020,6 Conclusion,"in future work, it would be interesting to investigate to what extent pretrained language models benefit from groc on such zero-resource or lowresource adaptation settings."
2020.emnlp-main.96.txt,2020,6 Conclusion,"this work indicates several other future directions for language modeling in low-resource domains: extension to other languages, scaling training to even larger vocabularies, and applying groc in a large pretraining setting to expand its zero-shot generalization."
2020.emnlp-main.97.txt,2020,8 Conclusion,"future work will explore applying ssmba to the target side manifold in structured prediction tasks, as well as other natural language tasks and settings where data augmentation is difficult."
2020.emnlp-main.98.txt,2020,5 Conclusion,combining sparse deep learning techniques with setconv is a potential solution to this issue.
2020.emnlp-main.98.txt,2020,5 Conclusion,we leave it for future work.
P16-1001,2016,7 Conclusions,"we anticipate the approaches that we have found useful in the case of amr to reduce the impact of noise, efficiently support large action spaces with targeted exploration, and cope with unbounded trajectories in the transition system will be of relevance to other structured prediction tasks."
P16-1002,2016,6 Discussion,there has been growing interest in applying neural networks to semantic parsing and related tasks.
P16-1002,2016,6 Discussion,"wang and yang (2015) use a similar strategy, but identify similar words and phrases based on cosine distance between vector space embeddings."
P16-1004,2016,5 Conclusions,"beyond semantic parsing, we would also like to apply our seq2tree model to related structured prediction tasks such as constituency parsing."
P16-1005,2016,8 Conclusions and Future Work,in the future we aim to label slot types based on contextual information as well as sentence structures instead of trigger gazetteers only.
P16-1005,2016,8 Conclusions and Future Work,"we attempt to combine multi-prototype approaches (e.g., (reisinger and mooney, 2010)) to better disambiguate senses of trigger words."
P16-1006,2016,9 Conclusions and Future Work,"in the future we will apply visual pattern recognition and concept detection techniques to perform deep content analysis of the retrieved images, so we can do matching and inference on concept/entity level instead of shallow visual similarity."
P16-1006,2016,9 Conclusions and Future Work,our long-term goal is to extend this framework to other knowledge extraction and population tasks such as event extraction and slot filling to construct multimedia knowledge bases effectively from multiple languages with low cost.
P16-1007,2016,9 Conclusion,the complementary strengths of both systems suggest future work in combining these techniques.
P16-1009,2016,6 Conclusion,future work will explore the effectiveness of our approach in more settings.
P16-1010,2016,7 Conclusion,"in the future, we will extend this model to allow discontinuity on target sides and explore the possibility of directly encoding reordering information in translation rules."
P16-1010,2016,7 Conclusion,we are also interested in using graphs for neural machine translation to see how it can translate and benefit from graphs.
P16-1011,2016,9 Conclusion,our future work will extend the current approach with dialogue modeling to learn more reliable hypothesis spaces of resulting states for verb semantics.
P16-1012,2016,6 Conclusion,as future work we aim to make our approach completely knowledgefree by eliminating this dependency.
P16-1012,2016,6 Conclusion,"we will explore unsupervised acquisition of relational similarity (mikolov et al., 2013b) for this task."
P16-1015,2016,8 Conclusion,"finally, we would like to highlight two insights that the experiments provide."
P16-1015,2016,8 Conclusion,we think that the transition systems with more active tokens or the combination with edges that span over more words provide very attractive transition systems for possible future parsers.
P16-1018,2016,7 Conclusion,"finally, it would be interesting to investigate modeling metaphorical mappings as nonlinear mappings within the deep learning framework."
P16-1018,2016,7 Conclusion,our work is also directly extendable to other syntactic constructions.
P16-1019,2016,6 Conclusions and Future Work,"in addition, we also plan to compare our work to the method of sporleder et al.(2010) as well apply our work on the idx corpus (sporleder et al., 2010) and to other languages."
P16-1019,2016,6 Conclusions and Future Work,in future work we plan to investigate the use of sent2vec to encode larger samples of text - not only the sentence containing idioms.
P16-1019,2016,6 Conclusions and Future Work,the focus of these future experiments will be to test how our approach which is relatively less dependent on nlp resources compares with these other methods for idiom token classification.
P16-1019,2016,6 Conclusions and Future Work,we also plan to further analyse the errors made by our “general” model and investigate the “general” approach on the skewed part of the vnc-tokens dataset.
P16-1019,2016,6 Conclusions and Future Work,we also plan to investigate an end-to-end approach based on deep learning-based representations to classify literal and idiomatic language use.
P16-1020,2016,8 Conclusion and Future Work,"in future work, we will apply our method to other kinds of phrases and tasks."
P16-1021,2016,6 Conclusion,our proposed features can be expanded to other domains.
P16-1023,2016,6 Conclusion and future work,"based on this paper, there are serveral lines of investigation we plan to conduct in the future.(i) we will attempt to support our results on artificially generated corpora by conducting experiments on real natural language data.(ii) we will study the coverage of our four criteria in evaluating word representations.(iii) we modeled the four criteria using separate pcfgs, but they could also be modeled by one single unified pcfg."
P16-1023,2016,6 Conclusion and future work,but the validity of the assumption that embedding spaces can be decomposed into “linear” subspaces should be investigated in the future.
P16-1023,2016,6 Conclusion and future work,see rothe et al.(2016) and rothe and schutze (2016) for work that makes ¨ similar assumptions.
P16-1024,2016,5 Conclusions and Future Work,"encouraged by the excellent results, we also plan to test the portability of the approach to more language pairs, and other tasks and applications."
P16-1024,2016,5 Conclusions and Future Work,"in future work, we plan to investigate other methods for seed pairs selection, settings with scarce resources (agic et al., 2015; zhang et al., 2016), ′ other context types inspired by recent work in the monolingual settings (levy and goldberg, 2014a; melamud et al., 2016), as well as model adaptations that can work with multi-word expressions."
P16-1025,2016,5 Conclusions and Future Work,"in the future, we will extend this framework to other information extraction tasks."
P16-1026,2016,5 Conclusions,"in future work, we will investigate scalable and parallel model learning to explore the performance of our model for large-scale real-time event extraction and visualization."
P16-1028,2016,7 Conclusion,"in future work, we plan to apply semlms to other semantic related nlp tasks e.g.machine translation and question answering."
P16-1030,2016,6 Conclusion,"our work also empirically explores different methods of inducing and modelling these connotation frames, incorporating the interplay between relations within frames."
P16-1031,2016,5 Conclusions and Future Work,"first, since deep learning may obtain better generalization on large-scale data sets (bengio, 2009), a straightforward path of the future research is to apply the proposed btdnns for domain adaptation on a much larger industrial-strength data set of 22 domains (glorot et al., 2011)."
P16-1031,2016,5 Conclusions and Future Work,"second, we will try to investigate the use of the proposed approach for other kinds of data set, such as 20 newsgroups and reuters21578 (li et al., 2012; zhuang et al., 2013)."
P16-1032,2016,8 Conclusion,"experiments demonstrated that the approach can infer implied sentiment and point toward potential directions for future work, including joint entity detection and incorporation of more varied types of factual relationships."
P16-1033,2016,7 Conclusions,"for future work, we would like to advance this study in the following directions."
P16-1033,2016,7 Conclusions,"our plan is to study which syntactic structures are more suitable for human annotation, and balance informativeness of a candidate task and its suitability for human annotation."
P16-1036,2016,9 Conclusions,"also, we would like to experiment with other deep neural architectures such as recurrent neural networks, long short term memory networks, etc.to form the sub-networks."
P16-1036,2016,9 Conclusions,"as part of future work, we would like to enhance scqa with the meta-data information like categories, user votes, ratings, user reputation of the questions and answer pairs."
P16-1038,2016,10 Conclusion,"4 future directions for this work include further improving the number and quality of g2p models, as well as performing external evaluations of the models in speech- and text-processing tasks."
P16-1038,2016,10 Conclusion,we plan to use the presented data and methods for other areas of multilingual natural language processing.
P16-1043,2016,7 Conclusion,the approach is quite general and we hope that this paper will encourage more nlp researchers to explore curriculum learning in their own works.
P16-1044,2016,6 Conclusion,"potential future work include: 1) evaluating the proposed approaches for different tasks, such as community qa and textual entailment; 2) including the sentential attention mechanism; 3) integrating the hybrid and the attentive mechanisms into a single framework."
P16-1045,2016,7 Conclusions,"we believe it offers interesting challenges that go beyond the scope of this paper – such as question parsing, or textual entailment – and are exciting avenues for future research."
P16-1046,2016,7 Conclusions,it would also be interesting to apply the neural models presented here in a phrase-based setting similar to lebret et al.(2015).
P16-1046,2016,7 Conclusions,"one way to improve the word-based model would be to take structural information into account during generation, e.g., by combining it with a tree-based algorithm (cohn and lapata, 2009)."
P16-1047,2016,6 Conclusion and Future Work,can we transfer our model to other languages?
P16-1047,2016,6 Conclusion and Future Work,future work can be directed towards answering two main questions: can we improve the performance of our classifier?
P16-1047,2016,6 Conclusion and Future Work,"most importantly, we are going to test the model using word-embedding features extracted from a bilingual embedding space."
P16-1047,2016,6 Conclusion and Future Work,"to do this, we are going to explore whether adding language-independent structural information (e.g.universal dependency information) can help the performance on exact scope matching."
P16-1049,2016,8 Conclusion,we leave better triggering component and multiple rounds of conversation handling to be addressed in our future work.
P16-1051,2016,6 Conclusion,this unified information-theoretic perspective may eventually allow us to identify further systematic patterns of information exchange between dialogue participants.
P16-1053,2016,5 Conclusion and Future Work,"moreover, applying the models to other tasks, such as semantic relatedness measurement and paraphrase identification, would also be interesting attempts."
P16-1053,2016,5 Conclusion and Future Work,"so, we are going to extend sin to tree-based sin for sentence modeling as future work."
P16-1056,2016,6 Conclusion,"finally, we use our best performing neural network model to generate a corpus of 30m question and answer pairs, which we hope will enable future researchers to improve their question answering systems."
P16-1058,2016,5 Discussion and Conclusion,"an obvious direction for future work is to automatically induce such a strategy, based on confidence measures that automatically predict the trust-worthiness of a word for an object."
P16-1058,2016,5 Discussion and Conclusion,"another extension that we have planned for future work is to implement relational expressions, similar to (kennington and schlangen, 2015)."
P16-1059,2016,7 Conclusion,"deep learning has recently been used in mutliple nlp applications, including parsing (chen and manning, 2014) and translation (bahdanau et al., 2014)."
P16-1059,2016,7 Conclusion,"the dynamics of the underlying state can be modeled by recurrent neural networks or lstms (bahdanau et al., 2014)."
P16-1059,2016,7 Conclusion,"the model can also readily be applied to other structured prediction problems in language processing, such as selecting antecedents in coreference resolution."
P16-1062,2016,7 Conclusions and Future Work,"future work can also explore finer clusters within these datasets, such as clustering clue by word sense of the answers and toon by joke sense."
P16-1062,2016,7 Conclusions and Future Work,future work can manipulate datasets’ text properties to confirm that a specific property is the cause of observed differences in clustering.
P16-1062,2016,7 Conclusions and Future Work,"future work will explore further how the goals of short text authors translate into measurable properties of the texts they write, and how measuring those properties can help predict which similarity metrics and clustering methods will combine to provide the best performance."
P16-1065,2016,7 Conclusions and Future Work,"as next steps, we plan to explore model variations to support a wider range of use cases."
P16-1065,2016,7 Conclusions and Future Work,"in the spirit of treating links probabilistically, we plan to explore application of the model in suggesting links that do not exist but should, for example in discovering missed citations, marking social dynamics (nguyen et al., 2014), and identifying topically related content in multilingual networks of documents (hu et al., 2014)."
P16-1065,2016,7 Conclusions and Future Work,"we are also interested in modeling changing topics and vocabularies (blei and lafferty, 2006; zhai and boyd-graber, 2013)."
P16-1066,2016,7 Conclusion,this is a possible future research direction.
P16-1066,2016,7 Conclusion,this simple lexicon-based method could be further enhanced by incorporating arabic valence shifters and certain linguistic rules to handle them.
P16-1066,2016,7 Conclusion,"we also plan to make the classification multi-way: positive, negative, neutral and mixed."
P16-1073,2016,6 Conclusions,"furthermore, we also want to employ sentence rewriting techniques for other challenges in semantic parsing, such as the spontaneous, unedited natural language input, etc."
P16-1073,2016,6 Conclusions,"in future work, we will explore more advanced sentence rewriting methods."
P16-1075,2016,9 Discussion and Conclusion,"future work will aim to study different dimensions of the prompt (e.g.genre, topic) using multitask learning at a finer level."
P16-1075,2016,9 Discussion and Conclusion,we also aim to further study the characteristics of the multi-task model in order to determine which features transfer well across tasks.
P16-1076,2016,7 Conclusion,future work could be extending the proposed method to handle more complex questions.
P16-1080,2016,8 Conclusions,"another avenue of future research can look at the annotators’ own traits and how these relate to perception (flekova et al., 2015)."
P16-1081,2016,5 Conclusions,the idea of shared model adaptation is general and can be further extended.
P16-1082,2016,7 Conclusions,we are releasing human annotations of concept nodes and possible dependency edges learned from the acl anthology as well as implementations of the methods described in this paper to enable future research on modeling scientific corpora.
P16-1083,2016,7 Conclusion,"in future work, we plan to improve performace of feature weight tuning and investigate more general features."
P16-1085,2016,7 Conclusions,"as future work, we plan to investigate the possibility of designing word representations that best suit the wsd framework."
P16-1087,2016,8 Conclusion,"in future work, we plan to explore the effects of pre-training (bengio et al., 2009) and scheduled sampling (bengio et al., 2015) for training our lstm network."
P16-1087,2016,8 Conclusion,we would also like to explore re-ranking methods for our problem.
P16-1087,2016,8 Conclusion,"with respect to the fine-grained opinion mining task, a potential future direction to be able to model overlapping and embedded entities and relations and also to extend this model to handle cross-sentential relations."
P16-1088,2016,7 Conclusion,"in future work, we will develop lexical features which are captured by nonlocal dependencies."
P16-1089,2016,6 Conclusion,"however, it would be interesting to see how siamese cbow embeddings would affect results in supervised tasks."
P16-1089,2016,6 Conclusion,"it would be interesting to see how embeddings for larger pieces of texts, such as documents, would perform in document clustering or filtering tasks."
P16-1090,2016,6 Discussion and related work,"a natural next step is to explore our framework with additional modeling improvements—especially in dealing with context, structure, and noise."
P16-1090,2016,6 Discussion and related work,"another avenue for providing user confidence is probabilistic calibration (platt, 1999), which has been explored more recently for structured prediction (kuleshov and liang, 2015)."
P16-1091,2016,5 Conclusions,"firstly, the architectures based on a single convolutional layer and a single bi-directional recurrent layer in the proposed models can be extended by adding more layers as well as utilizing more advanced components including hierarchical cnns (kalchbrenner et al., 2014b) to deal with utterance compositionalities or attention mechanisms (denil et al., 2012) to focus on more important segments in dialogue sequences."
P16-1091,2016,5 Conclusions,"furthering this work, there would be still much room for improvement in future."
P16-1091,2016,5 Conclusions,the other direction of our future work is to investigate joint models for tracking dialogue topics and states simultaneously.
P16-1093,2016,6 Conclusions,"in future work, we plan to conduct larger-scale evaluations to further validate these results, and to apply these methods on other common learner errors."
P16-1095,2016,7 Conclusion,the future work has two main directions.one is semi-supervised learning.
P16-1095,2016,7 Conclusion,the other direction is to promote the random walk step.
P16-1097,2016,5 Conclusion,"in the future, we plan to apply our approach to real-world non-parallel corpora to further verify its effectiveness."
P16-1097,2016,5 Conclusion,"it is also interesting to extend the phrase translation model to more sophisticated models such as ibm models 2-5 (brown et al., 1993) and hmm (vogel and ney, 1996)."
P16-1098,2016,8 Conclusion and Future Work,"in future work, we would like to investigate our model on more text matching tasks."
P16-1099,2016,6 Conclusion,"additionally we recognize that the hashtag inventory used to discover business accounts from job-related topics might need to change over time, to achieve robust performance in the future."
P16-1099,2016,6 Conclusion,this is left for future work.
P16-1099,2016,6 Conclusion,we did not study whether providing contextual information in our humans-in-the-loop framework would influence the model performance.
P16-1100,2016,7 Conclusion,"for future work, we hope to be able to improve the memory usage and speed of purely character-based models."
P16-1101,2016,6 Conclusion,another interesting direction is to apply our model to data from other domains such as social media (twitter and weibo).
P16-1101,2016,6 Conclusion,"since our model does not require any domain- or taskspecific knowledge, it might be effortless to apply it to these domains."
P16-1101,2016,6 Conclusion,there are several potential directions for future work.
P16-1102,2016,6 Conclusion and Future Work,further exploration of different topic vector representations and their combinations is necessary in future work.
P16-1103,2016,7 Conclusion,"therefore, one important extension of our work is to further study the interaction between our model and the underlying language model."
P16-1104,2016,7 Conclusion,we propose to augment this work in future by exploring deeper graph and gaze features.
P16-1107,2016,7 Conclusions and Future Work,"also, we will apply our contextaware argumentative relation mining to different argument mining corpora to further evaluate its generality."
P16-1107,2016,7 Conclusions and Future Work,our next step will investigate uses of topic segmentation to identify context sentences and compare this linguistically-motivated approach to our current window-size heuristic.
P16-1107,2016,7 Conclusions and Future Work,the results obtained in this preliminary study are promising and encourage us to explore more directions to enable contextual features.
P16-1107,2016,7 Conclusions and Future Work,we plan to follow prior research on graph optimization to refine the argumentation structure and improve argumentative relation prediction.
P16-1108,2016,7 Conclusion,"in the future, we plan to expand our method to predict morphological analyses, as well as to incorporate other information such as parts-of-speech."
P16-1110,2016,8 Conclusion and Future Work,"finally, with slight changes to what the system considers a document, we believe alto can be extended to nlp applications other than classification, such as named entity recognition or semantic role labeling, to reduce the annotation effort."
P16-1110,2016,8 Conclusion and Future Work,we can further improve alto to help users gain better and faster understanding of text corpora.
P16-1111,2016,8 Conclusion,access to longer time-spans—along with varying data sources such as grants and patents—would also allow us to more completely model the trajectory of a topic as it moves from being active area of research to potentially impacting commercial industries and economic development.
P16-1112,2016,9 Conclusion,"as part of future work, it would be beneficial to investigate the effect of automatically generated training data for error detection (e.g., rozovskaya and roth (2010))."
P16-1114,2016,7 Conclusion,the future work includes using those prediction models in a real service to take targeted actions to users who are likely to stop using intelligent assistants.
P16-1115,2016,7 Conclusions,all this is future work.
P16-1115,2016,7 Conclusions,"the design of the model, as mentioned in the introduction, makes it amenable for use in interactive systems that learn; we are currently exploring this avenue."
P16-1116,2016,7 Conclusion,"future work may be done to integrate our method into a joint approach, use some global feature, which may improve our performance."
P16-1117,2016,6 Conclusion,"in the future, we plan to extend our model to incorporate coreference resolution and intersentential zero anaphora resolution."
P16-1119,2016,7 Conclusions and Future Work,"future work can use our annotated corpus to develop classifiers that deal better with prepositional and adjectival modifiers, which require deeper semantic analysis."
P16-1120,2016,8 Conclusions,"as future work, we would like to verify the effectiveness of the proposed models for other datasets or other cross-lingual tasks, such as cross-lingual document classification (ni et al., 2009; platt et al., 2010; ni et al., 2011; smet et al., 2011) and cross-lingual information retrieval (vulic et al., ′ 2013)."
P16-1122,2016,7 Conclusion and Future Work,in the future we plan to apply our inner-attention intuition to other neural networks such as cnn or multi-layer perceptron.
P16-1122,2016,7 Conclusion and Future Work,our models can be further extended to other nlp tasks such as recognizing textual entailments where attention mechanism is important for sentence representation.
P16-1123,2016,5 Conclusion,"we expect this sort of architecture to be of interest also beyond the specific task of relation classification, which we intend to explore in future work."
P16-1124,2016,6 Conclusion,it will be interesting to study whether one can merge the clustering step and the coupling step so as to have a richer inter-task dependent structure.
P16-1124,2016,6 Conclusion,there are still many interesting topics to study.
P16-1124,2016,6 Conclusion,we will investigate such topics in our future work.
P16-1124,2016,6 Conclusion,"we would like to design new mechanisms to discover loosely correlated relations, and investigate whether coupling such relations still provides benefits."
P16-1125,2016,7 Conclusion,"lastly, it is important to evaluate the impact of the proposed largercontext models in downstream tasks such as machine translation and speech recognition."
P16-1125,2016,7 Conclusion,"second, more analysis, beyond the one based on part-of-speech tags, should be conducted in order to better understand the advantage of such larger-context models."
P16-1125,2016,7 Conclusion,"to explore the potential of such a model, there are several aspects in which more research needs to be done."
P16-1127,2016,6 Conclusion and Future Work,"but there are other possible choices that might make the encoding even more easily learnable by the lstm, and we would like to explore those in future work."
P16-1127,2016,6 Conclusion and Future Work,"in order to improve performance, other promising directions would involve adding re-reranking techniques and extending our neural networks with attention models in the spirit of (bahdanau et al., 2015)."
P16-1129,2016,8 Conclusion and Future Work,another important direction is to focus on the construction of datasets in larger scale.
P16-1129,2016,8 Conclusion and Future Work,one feasible approach is to use a speech recognition system on live videos or broadcasts of sports games to collect huge amount of transcripts as our raw data source.
P16-1129,2016,8 Conclusion and Future Work,we would like to extend our system to produce sports news beyond pure sentence extraction.
P16-1130,2016,6 Conclusion,"for future work, we will explore more sophisticated features for the csrs model, such as syntactic dependency relationships and head words, since only simple lexical features are used in the current incarnation."
P16-1133,2016,5 Conclusion and Future Work,"in addition, we will also explore the possibility of using more complex neural network models such as convolutional neural network and recurrent neural network to build bilingual document representation system."
P16-1133,2016,5 Conclusion and Future Work,our future work will focus on extending the bilingual document representation model into the multilingual scenario.
P16-1133,2016,5 Conclusion and Future Work,we will try to learn a single embedding space for a source language and multiple target languages simultaneously.
P16-1134,2016,6 Conclusions,"in future work, we are interested in exploring better ways of utilizing vast unlabelled data to improve grsemi-crfs, e.g., to learn phrase embeddings from unlabelled data or designing a semisupervised version of grsemi-crfs."
P16-1135,2016,7 Conclusion,"although we have focused exclusively on wikipedia, these methods could be adapted to other domains and languages."
P16-1135,2016,7 Conclusion,"in the future, we may improve this step using a machine learning approach."
P16-1135,2016,7 Conclusion,"linguistic expressions of causality in other languages is another avenue for future research, and it would be interesting to note if other languages have the same variety of expression."
P16-1135,2016,7 Conclusion,to evaluate on the intermediate step would have required an additional annotation process.
P16-1135,2016,7 Conclusion,"ultimately, the focus of this work is to improve detection of causal relations."
P16-1136,2016,5 Conclusions,"in the future, we would like to study the impact of relation paths for additional basic kb embedding models and knowledge domains."
P16-1137,2016,8 Conclusion,"in future work, we will explore how to use our model to improve downstream nlp tasks, and consider applying our methods to other knowledge bases."
P16-1140,2016,7 Conclusion,it would also be a promising direction to incorporate the factor of language typological diversity when designing advanced word representation model for languages other than english.
P16-1141,2016,5 Discussion,the two statistical laws we propose have strong implications for future work in historical semantics.
P16-1141,2016,5 Discussion,we extend these lines of work by rigorously comparing different approaches to quantifying semantic change and by using these methods to propose new statistical laws of semantic change.
P16-1141,2016,5 Discussion,we show how distributional methods can reveal statistical laws of semantic change and offer a robust methodology for future work in this area.
P16-1142,2016,7 Conclusions,we believe that combining semantic roles and other semantic representation in a similar fashion to the one used in this paper could be useful to infer knowledge beyond spatial inferences.
P16-1143,2016,8 Discussion and Future Work,another obvious extension would be to further explore the alignment component of hca-wsi.
P16-1143,2016,8 Discussion and Future Work,"in particular, we intend to expand lexsemtm by applying hca-wsi across the vocabularies of languages other than english, and also to multiword lemmas."
P16-1143,2016,8 Discussion and Future Work,the most immediate extension of our work would be to apply our sense learning method to a broader range of data.
P16-1143,2016,8 Discussion and Future Work,"this could be used to expand existing sense inventories with new senses, for example using the methodology of cook et al.(2013)."
P16-1145,2016,6 Conclusion,"in light of this finding, we suggest some focus areas for future research."
P16-1147,2016,6 Conclusions,"in future work, we will extend this idea beyond sequence modeling to improve models in nlp that utilize parse trees as features."
P16-1151,2016,4 Conclusion,"the methodology is also useful for observational data—for studying the effects of complicated treatments, such as how a legislator’s roll call voting record affects their electoral support."
P16-1153,2016,5 Conclusion,"future work includes: (i) adding an attention model to robustly analyze which part of state/actions text correspond to strategic planning, and (ii) applying the proposed methods to more complex text games or other tasks with actions defined through natural language."
P16-1154,2016,7 Conclusion and Future Work,"for future work, we will extend this idea to the task where the source and target are in heterogeneous types, for example, machine translation."
P16-1156,2016,9 Conclusion and Future Work,future work will consider the role of derivational morphology in embeddings as well as noncompositional cases of inflectional morphology.
P16-1159,2016,6 Conclusion,"as our approach is transparent to loss functions and architectures, we believe that it will also benefit more end-to-end neural architectures for other nlp tasks."
P16-1159,2016,6 Conclusion,"in the future, we plan to test our approach on more language pairs and more end-to-end neural mt systems."
P16-1159,2016,6 Conclusion,it is also interesting to extend minimum risk training to minimum risk annealing following smith and eisner (2006).
P16-1160,2016,8 Conclusion,"however, this has allowed us a more fine-grained analysis, but in the future, a setting where the source side is also represented as a character sequence must be investigated."
P16-1162,2016,6 Conclusion,"one avenue of future research is to learn the optimal vocabulary size for a translation task, which we expect to depend on the language pair and amount of training data, automatically."
P16-1164,2016,8 Conclusion,"on a more general level, we believe that quotation detection is interesting as a representative of tasks involving long sequences, where markov assumptions become inappropriate."
P16-1165,2016,6 Conclusions and Future Work,"in the future, we would like to combine crfs with lstms for doing the two steps jointly, so that the lstms can learn the embeddings using the global thread-level feedback."
P16-1165,2016,6 Conclusions and Future Work,"we would also like to apply our models to conversations, where the graph structure is extractable using the meta data or other clues, e.g., the fragment quotation graphs for email threads (carenini et al., 2008)."
P16-1166,2016,8 Conclusion,"a next major step in our research agenda is to integrate se type information into various applications, including argument mining, temporal reasoning, and summarization."
P16-1166,2016,8 Conclusion,"among others, we plan to create subtypes of the state label, which currently subsumes clauses stativized by negation, modals, lexical information or other aspectual operators."
P16-1166,2016,8 Conclusion,"here we focus on the automatic identification of se types, leaving the identification of discourse modes to future work."
P16-1167,2016,8 Conclusion,"finally, we provide a dataset depicting 12 scenarios with ～1.5 m images for future research."
P16-1167,2016,8 Conclusion,future directions could include exploring nuances in the type of temporal knowledge that can be learned across different scenarios.
P16-1171,2016,7 Conclusion,"in the future, we plan to build a resource for modeling physical causality for action verbs."
P16-1172,2016,Future Work,"for example, we plan to incorporate lexicalsemantic information in the feature representation and leverage large-scale unsupervised pretraining."
P16-1172,2016,Future Work,the promising results we obtained for summarization with a basic learner (see section 4.3) encourage future work on plugging in more sophisticated supervised learners in our framework.
P16-1174,2016,6 Conclusion,"instead of the sparse indicator variables used here, it may be better to decompose lexeme tags into denser and more generic features of tag components9 (e.g., part of speech, tense, gender, case), and also use corpus frequency, word length, etc."
P16-1175,2016,7 Conclusion,"we also leave as future work the integration of this model into an adaptive system that tracks learner understanding and creates scaffolded content that falls in their zone of proximal development, keeping them engaged while stretching their understanding."
P16-1175,2016,7 Conclusion,we plan a deeper investigation into how learners detect and combine cues for incidental comprehension.
P16-1176,2016,7 Conclusion,we also plan to investigate how the results vary when limited to specific l1s.
P16-1176,2016,7 Conclusion,"we are interested in expanding the preliminary results of this work: we intend to replicate the experiments with more languages and more domains, investigate additional varieties of constrained language and employ more complex lexical, syntactic and discourse features."
P16-1178,2016,7 Conclusions,"as future work, we plan to investigate other linguistic representations that can improve the automated extraction of the proposed aspects to better predict the article’s perceived quality."
P16-1179,2016,9 Conclusions and Future Work,13 the analysis of the acquired information and the error analysis show several avenues for future work.
P16-1179,2016,9 Conclusions and Future Work,"first larger corpora should allow to increase the applicability of the similarity resource, and specially, that of the dependency templates, and also provide better quality resources."
P16-1180,2016,9 Conclusion and Future Work,another task for future work is semantic alignment.
P16-1180,2016,9 Conclusion and Future Work,we leave these to future work.
P16-1183,2016,6 Conclusion,we intend to explore these possibilities in future work.
P16-1184,2016,7 Conclusion,our results provide a strong baseline for future work in weakly supervised morphological tagging.
P16-1185,2016,5 Conclusion,"another interesting direction is to enhance the connection between source-to-target and target-tosource models (e.g., letting the two models share the same word embeddings) to help them benefit more from interacting with each other."
P16-1185,2016,5 Conclusion,"as our method is sensitive to the oovs present in monolingual corpora, we plan to integrate jean et al.(2015)’s technique on using very large vocabulary into our approach."
P16-1185,2016,5 Conclusion,it is also necessary to further validate the effectiveness of our approach on more language pairs and nmt architectures.
P16-1186,2016,6 Conclusions,"an interesting future direction is to combine complementary approaches, either through combined parameterization (e.g.hierarchical softmax with differentiated capacity per word) or through a curriculum (e.g.transitioning from target sampling to regular softmax as training progresses)."
P16-1186,2016,6 Conclusions,further promising areas are parallel training as well as better rare word modeling.
P16-1187,2016,5 Conclusions,"as future work, we plan to examine the use of a voting scheme for combining the output of complementary dsms."
P16-1187,2016,5 Conclusions,"moreover, we also plan to combine additional sources of information for building the models, such as multilingual resources or translation data, to improve even further the compositionality prediction."
P16-1191,2016,8 Conclusions and Future Work,"in follow-up work, we aim to apply our embedding method on smaller, yet gold-standard corpora such as semcor (miller et al., 1994) and streusle (schneider and smith, 2015) to examine the impact of the corpus choice in detail and extend the training data beyond wordnet vocabulary."
P16-1192,2016,8 Conclusion,it would also be interesting to combine the strengths of the condensed and sibling-finder algorithms for further efficiency gains.
P16-1193,2016,6 Conclusion,"within this new vector space, the entailment operators and inference equations apply, thereby generalising naturally from these lexical representations to the compositional semantics of multi-word expressions and sentences."
P16-1194,2016,5 Conclusions,another possible way to extend the model is to consider modeling long distance dependency between latent states.
P16-1194,2016,5 Conclusions,this may further improve the model’s performance.
P16-1195,2016,8 Conclusion,"in future work, we plan to develop better models for capturing the structure of the input, as well as extend the use of our system to other applications such as automatic documentation of source code."
P16-1196,2016,4 Conclusion and Future Work,"in addition we plan to further investigate how to fine-tune some of the hyper parameters of the cpm such as spline scaling, single global scaling factor, convergence tolerance, and initialization of the latent trace with a centroid."
P16-1196,2016,4 Conclusion and Future Work,"in addition, to aid cpm convergence to a good local optimum, in future work we will investigate dimensionality reduction approaches that are reversible such as principal component analysis (pearson, 1901) and other pre-processing approaches similar to (listgarten, 2007), where the training data set is coarsely pre-aligned and pre-scaled based on the center of mass of the time series."
P16-1196,2016,4 Conclusion and Future Work,"in subsequent work, we would like to explore alternatives for enhancing cpms by incorporating contextual features in the training data set such as timing of hand movements, and preceding, succeeding, and co-occurring facial expressions."
P16-1196,2016,4 Conclusion and Future Work,"while this work used the latent trace as the basis for animation, in future work, we also plan to explore methods for sampling from the model to produce variations in face and head movement."
P16-1197,2016,5 Conclusion,"deriving sentiment labels for supervised training is an important topic for future study, as is inferring the sentiment of published news from stock price fluctuations instead of the reverse."
P16-1198,2016,6 Discussion,"in addition, we are currently exploring potential extensions of the techniques presented in this paper to higher order, projective and non-projective, dependency parsing."
P16-1198,2016,6 Discussion,it may also be utilized as an inference/initialization subroutine as a part of more complex approximation frameworks such as belief propagation (e.g.
P16-1198,2016,6 Discussion,we believe this contribution has the potential to affect future research on additional nlp problems.
P16-1198,2016,6 Discussion,we intend to investigate all of these directions in future work.
P16-1199,2016,6 Conclusion and Future Works,"in the next step, we plan to exploit fine-grained discourse structures, e.g., dialogue acts (ritter et al., 2010), and propose a unified model that jointly inferring discourse roles and topics of posts in context of conversation tree structures."
P16-1200,2016,5 Conclusion and Future Works,"in the future, we will explore the following directions: ?"
P16-1200,2016,5 Conclusion and Future Works,"in the future, we will incorporate our instance-level selective attention technique with those models for relation extraction."
P16-1200,2016,5 Conclusion and Future Works,we will explore our model in other area such as text categorization.?
P16-1201,2016,6 Conclusions and Future Work,"in the future, we will extend this work to the complete event extraction task."
P16-1201,2016,6 Conclusions and Future Work,the key of this research is to detect events in fn.
P16-1201,2016,6 Conclusions and Future Work,we plan to refine the event schemas by the finer-grained frames defined in fn (i.e.
P16-1202,2016,6 Conclusion,our future plan is to apply this model to general arithmetic problems which require multiple applications of formulas.
P16-1203,2016,6 Conclusions and Future Work,our future research will test the correlation between the polarity and the name of a fictional character beyond the movie domain.
P16-1208,2016,6 Discussion and Conclusions,"for future work, we intend to study the problem in the context of other languages."
P16-1210,2016,7 Discussion,"in the future, we would like to extend this work in several ways."
P16-1210,2016,7 Discussion,"second, since the svd basis-shift seems to be the source of much of the gains, we would like to explore replacing the svd with other algorithms, such as independent component analysis."
P16-1212,2016,5 Conclusion and Future Work,"in the future, we will conduct experiments on large corpus in different domains."
P16-1214,2016,4 Conclusion,our future research includes improving the search performance for embeddings with heavy-tailed distributions and creating embeddings that can keep both task quality and search performance high.
P16-1214,2016,4 Conclusion,"since we need to implement additional glue codes for running flann and sash, our code would be useful for researchers who want to compare their results with ours."
P16-1215,2016,6 Conclusion,"we expect that several further studies will use the new dataset not only for distributed representations of relational patterns but also for other nlp tasks (e.g., paraphrasing)."
P16-1216,2016,8 Conclusion,"in future work, we look to incorporate methods that incur less cost, possibly tolerating some error in the formation of sentences without significantly degrading performance."
P16-1216,2016,8 Conclusion,we look to devote future work to handling such cases.
P16-1217,2016,6 Conclusions and Future Work,"in future work, we will explore to make use of all the three kinds of labels together to improve the users’ experience when they want to browse, understand and leverage the topics."
P16-1217,2016,6 Conclusions and Future Work,"in future work, we will try to make the summary label more coherent by considering the discourse structure of the summary and leveraging sentence ordering techniques."
P16-1220,2016,7 Conclusion and Future Work,our future work involves exploring other alternatives such as treating structured and unstructured data as two independent resources in order to overcome the knowledge gaps in either of the two resources.
P16-1223,2016,7 Conclusion,"as future work, we need to consider how we can utilize these datasets (and the models trained upon them) to help solve more complex rc reasoning tasks (with less annotated data)."
P16-1224,2016,6 Related Work and Discussion,"looking forward, we believe that the illg setting is worth studying and has important implications for natural language interfaces."
P16-1224,2016,6 Related Work and Discussion,monroe and potts (2015) uses learning to improve the pragmatics model.
P16-1225,2016,6 Conclusion,"finally, we would like to test our model using other representations of word-form and wordmeaning."
P16-1225,2016,6 Conclusion,future work could also test whether a more interpretable meaningspace representation such as that provided by binary wordnet feature vectors reveals patterns of systematicity not found using a distributional semantic space.
P16-1225,2016,6 Conclusion,future work may investigate to what extent the smlkr model can predict human intuitions about form-meaning systematicity in language.
P16-1225,2016,6 Conclusion,"however, it would be interesting to verify our results in a phonological setting, perhaps using a monodialectal corpus."
P16-1225,2016,6 Conclusion,we succeed in applying this algorithm to the problem of finding form-meaning systematicity in the monomorphemic english lexicon.
P16-1226,2016,8 Conclusion,"finally, our architecture seems straightforwardly applicable for multi-class classification, which, in future work, could be used to classify term-pairs to multiple semantic relations."
P16-1227,2016,5 Conclusions and Further Work,a further avenue of future research is improving models such as that presented in elliott et al.(2015) by crucial components of neural mt such as “attention mechanisms”.
P16-1227,2016,5 Conclusions and Further Work,a similar mechanism is used in xu et al.(2015) to decide which part of the image should influence which part of the generated caption.
P16-1227,2016,5 Conclusions and Further Work,combining these two types of attention mechanisms in a neural caption translation model is a natural next step in caption translation.
P16-1227,2016,5 Conclusions and Further Work,"in future work, we plan to evaluate our approach in more naturalistic settings, such machine translation for captions in online multimedia repositories such as wikimedia commons16 and digitized art catalogues, as well as e-commerce localization."
P16-1227,2016,5 Conclusions and Further Work,"learning semantically informative distance metrics using deep learning techniques is an area under active investigation (wu et al., 2013; wang et al., 2014; wang et al., 2015)."
P16-1228,2016,6 Discussion and Future Work,"finally, we also would like to generalize our framework to automatically learn the confidence of different rules, and derive new rules from data."
P16-1228,2016,6 Discussion and Future Work,"the encouraging empirical results indicate a strong potential of our approach for improving other application domains such as vision tasks, which we plan to explore in the future."
P16-1228,2016,6 Discussion and Future Work,we plan to explore these diverse knowledge representations to guide the dnn learning.
p17-1002,2017,6 Conclusion,"its suitability for the end-to-end learning task is scope for future work, but its adequacy for component classification and relation identification has been investigated in potash et al.(2016)."
p17-1004,2017,5 Conclusion,"hence, the word-level multi-lingual attention, which may discover implicit alignments between words in multiple languages, will further improve multi-lingual relation extraction."
p17-1004,2017,5 Conclusion,"in future, we will extend mnre to more languages and explore its significance."
p17-1004,2017,5 Conclusion,"we will explore the effectiveness of word-level multilingual attention for relation extraction as our future work.(2) mnre can be flexibly implemented in the scenario of multiple languages, and this paper focuses on two languages of english and chinese."
p17-1004,2017,5 Conclusion,"we will explore the following directions as future work: (1) in this paper, we only consider sentence-level multi-lingual attention for relation extraction."
p17-1005,2017,5 Discussion,"aside from relaxing strict isomorphism, we would also like to perform crossdomain semantic parsing where the first stage of the semantic parser is shared across domains."
p17-1006,2017,7 Conclusion and Future Work,"future work will focus on other potential sources of morphological knowledge, porting the framework to other morphologically rich languages and downstream tasks, and on further refinements of the post-processing specialisation algorithm and the constraint selection."
p17-1010,2017,6 Conclusion,"second, we will develop other neural network architecture to make it more appropriate for zero pronoun resolution task."
p17-1010,2017,6 Conclusion,"the future work will be carried out on two main aspects: first, as experimental results show that the unknown words processing is a critical part in comprehending context, we will explore more effective way to handle the unk issue."
p17-1011,2017,6 Conclusion,"in future, we plan to exploit discourse mode identification for providing novel features for more downstream nlp applications."
p17-1012,2017,6 Conclusion,"also, we plan to investigate the effectiveness of our architecture on other sequence-tosequence tasks, e.g.summarization, constituency parsing, dialog modeling."
p17-1014,2017,9 Conclusions,"for future work, we would like to extend our work to different meaning representations such as the minimal recursion semantics (mrs; copestake et al.(2005))."
p17-1014,2017,9 Conclusions,"taking a step further, we would like to apply our models on semantics-based machine translation using mrs as an intermediate representation between pairs of languages, and investigate the added benefit compared to directly translating the surface strings, especially in the case of distant language pairs such as english and japanese (siegel, 2000)."
p17-1016,2017,6 Conclusions,"in future work, it would be useful to investigate models based on morphemes, rather than characters, which offers potentially superior performance for complex and rare words (luong et al., 2013), which are common in poetry."
p17-1018,2017,7 Conclusion,"as for future work, we are applying the gated self-matching networks to other reading comprehension and question answering datasets, such as the ms marco dataset (nguyen et al., 2016)."
p17-1019,2017,6 Conclusion and Future Work,"and the future work includes: a) lots of questions cannot be answered directly by facts in a kb (e.g.鈥淲ho is jet li鈥檚 father-in-law?鈥), we plan to learn qa system with latent knowledge (e.g."
p17-1019,2017,6 Conclusion and Future Work,"kb embedding (bordes et al., 2013)); b) we plan to adopt memory networks (sukhbaatar et al., 2015) to encode the temporary kb for each question."
p17-1020,2017,8 Conclusion,"in future work we would like to deepen the use of structural clues and answer questions over multiple documents, using paragraph structure, titles, sections and more."
p17-1020,2017,8 Conclusion,incorporating coreference resolution would be another important direction for future work.
p17-1023,2017,7 Discussion and Conclusion,"as we have tested these approaches on a rather small vocabulary, which may limit generality of conclusions, future work will be devoted to scaling up these findings to larger test sets, as e.g.recently collected through conversational agents (das et al., 2016) that circumvent the need for human-human interaction data."
p17-1024,2017,6 Conclusion,"we note that the addition of this extra step will move this work closer to the textual/visual explanation research (e.g., (park et al., 2016; selvaraju et al., 2016))."
p17-1024,2017,6 Conclusion,"with our work, we would like to push the community to think of ways that models can better merge language and vision modalites, instead of merely using one to supplement the other"
p17-1027,2017,7 Conclusion,"while we used a perceptron classifier for our experiments, our oracle could also be used in neuralnetwork implementations of greedy transitionbased parsing (chen and manning, 2014; dyer et al., 2015), providing an interesting avenue for future work."
p17-1028,2017,6 Conclusions and Future Work,"we also plan to investigate extension of the crowd component in our hmm method with hierarchical models, as well as a fully-bayesian approach."
p17-1028,2017,6 Conclusions and Future Work,"we expect our methods to be applicable to and similarly benefit other sequence labeling tasks, such as pos tagging and chunking (hovy et al., 2014)."
p17-1029,2017,7 Conclusion and Future Work,"future work will adapt this framework to other sequence transduction scenarios such as machine translation, dialogue generation, question answering, where continuous and discrete latent variables can be abstracted to guide sequence generation."
p17-1032,2017,5 Discussion and Conclusions,"finally, the optimization of the kda methodology through the suitable parallelization on multicore architectures, as well as the exploration of mechanisms for the dynamic reconstruction of kernel spaces (e.g., operating over hn y) also constitute interesting future research directions on this topic."
p17-1032,2017,5 Discussion and Conclusions,"future work will address experimentations with larger scale datasets; moreover, it is interesting to experiment with more landmarks in order to better understand the trade-off between the representation capacity of the nystrom approximation 篓 of the kernel functions and the over-fitting that can be introduced in a neural network architecture."
p17-1034,2017,6 Conclusion and Future Work,we are going to explore more effective models in future.
p17-1035,2017,9 Conclusion and Future Directions,"our future agenda includes: (a) optimizing the cnn framework hyper-parameters (e.g., filter width, dropout, embedding dimensions, etc.)to obtain better results, (b) exploring the applicability of our technique for documentlevel sentiment analysis and (c) applying our framework to related problems, such as emotion analysis, text summarization, and questionanswering, where considering textual clues alone may not prove to be sufficient."
p17-1037,2017,6 Conclusion,"for our immediate future work, we plan to embed the topic and user vectors to create a crosstopic stance detector."
p17-1037,2017,6 Conclusion,"it is possible to generalize our work to model heterogeneous signals, such as interests and behaviors of people, for example, 鈥渢hose who are interested in a also support b,鈥 and 鈥渢hose who favor a also vote for b鈥."
p17-1037,2017,6 Conclusion,"therefore, we believe that our work will bring about new applications in the field of nlp and other disciplines."
p17-1038,2017,7 Conclusion and Future Work,"in the future, we will use the proposed automatically data labeling method to more event types and explore more models to extract events by using automatically labeled data."
p17-1039,2017,6 Conclusion and future work,in the future we will try our analytical method on other parts of language.
p17-1042,2017,7 Conclusions and future work,"finally, we would like to apply our model in the decipherment scenario (dou et al., 2015)."
p17-1042,2017,7 Conclusions and future work,"in addition to that, we would like to explore non-linear transformations (lu et al., 2015) and alternative dictionary induction methods (dinu et al., 2015; smith et al., 2017)."
p17-1042,2017,7 Conclusions and future work,"in the future, we would like to delve deeper into this direction and fine-tune our method so it can reliably learn high quality bilingual word embeddings without any bilingual evidence at all."
p17-1043,2017,7 Conclusion and Future Work,but it would also be fairly easy to add gazetteer information to the network features in order to remove the need for ner preprocessing.
p17-1043,2017,7 Conclusion and Future Work,there are changes which could be made to eliminate all pre-processing and to further improve parser performance.
p17-1045,2017,7 Conclusions and Discussion,"effective implementation of this, however, requires the e2e agent to learn quickly and this is the research direction we plan to focus on in the future."
p17-1046,2017,6 Conclusion and Future Work,"in the future, we shall study how to model logical consistency of responses and improve candidate retrieval."
p17-1047,2017,6 Conclusions and Future Work,"additionally, by collecting a second dataset of captions for our images in a different language, such as spanish, our model could be extended to learn the acoustic correspondences for a given object category in both languages."
p17-1047,2017,6 Conclusions and Future Work,"the same framework we use here could be extended to video, enabling the learning of actions, verbs, environmental sounds, and the like."
p17-1048,2017,5 Summary and discussion,"future work will apply this technique to the other languages including english, where we have to solve an issue of long sequence lengths, which requires heavy computation cost and makes it difficult to train a decoder network."
p17-1049,2017,7 Conclusion,"the major trends relate to loan translations (jahr, 1999), or the impact of canonical texts, such as luther鈥檚 translation of the bible to german (russ, 1994) or the case of the king james translation to english (crystal, 2010)."
p17-1049,2017,7 Conclusion,we are presently trying to extend these results to translations in a different domain (literary texts) into a very different language (hebrew).
p17-1049,2017,7 Conclusion,we leave this as a direction for future research.
p17-1050,2017,6 Conclusion and Outlook,"in future work, we also plan to explore the role of native and second language writing system characteristics in second language reading."
p17-1050,2017,6 Conclusion and Outlook,"more broadly, our methodology introduces parallels with production studies in nlp, creating new opportunities for integration of data, methodologies and tasks between production and comprehension."
p17-1051,2017,7 Conclusions and Future Work,"for future work, we plan to address the limitations of morse: minimal supervision, greedy inference, and concatenative orthographic model."
p17-1051,2017,7 Conclusions and Future Work,"moreover, we plan to computationally optimize the training stage for the sake of wider adoption by the community."
p17-1053,2017,7 Conclusion,"for future work, we will investigate the integration of our hr-bilstm into end-to-end systems."
p17-1053,2017,7 Conclusion,"we will also investigate new emerging datasets like graphquestions (su et al., 2016) and complexquestions (bao et al., 2016) to handle more characteristics of general qa."
p17-1054,2017,7 Conclusions and Future Work,"in the future, we are interested in comparing the model to human annotators and using human judges to evaluate the quality of predicted phrases.鈥 our current model does not fully consider correlation among target keyphrases."
p17-1054,2017,7 Conclusions and Future Work,it would also be interesting to explore the multiple-output optimization aspects of our model.
p17-1054,2017,7 Conclusions and Future Work,"our future work may include the following two directions.鈥 in this work, we only evaluated the performance of the proposed model by conducting off-line experiments."
p17-1055,2017,8 Conclusion,"in this context, we are planning to investigate the problems that need comprehensive reasoning over several sentences."
p17-1055,2017,8 Conclusion,the future work will be carried out in the following aspects.
p17-1055,2017,8 Conclusion,"we believe that our model is general and may apply to other tasks as well, so firstly we are going to fully investigate the usage of this architecture in other tasks."
p17-1056,2017,7 Conclusions,"more broadly, these results suggest ways that quantitative methods can be used to make precise application of concepts like 鈥渃ultural fit鈥 at scale."
p17-1057,2017,5 Conclusion,going forward we would like to compare the representations learned by our model to the brain activity of people listening to speech in order to determine to what extent the patterns we found correspond to localized processing in the human cortex.
p17-1058,2017,8 Conclusions,"we hope that by comparing and combining our methodology with other approaches of studying dialogue, we can reach a more comprehensive and holistic understanding of this common yet mysterious human practice."
p17-1059,2017,6 Conclusions and Future Work,"for future work, we wish to extend this model by investigating language generation conditioned on other modalities such as facial images and speech, and to applications such as dialogue generation for virtual agents."
p17-1060,2017,6 Conclusion,future work can include an extension of domain experts to take into account dialog history aiming for a holistic framework that can handle contextual interpretation as well.
p17-1061,2017,6 Conclusion and Future Work,all of the above suggest a promising research direction.
p17-1061,2017,6 Conclusion and Future Work,"in addition to dialog acts, we plan to apply our kgcvae model to capture other different linguistic phenomena including sentiment, named entities,etc."
p17-1062,2017,7 Conclusion,"in future work, we plan to extend hcns by incorporating lines of existing work, such as integrating the entity extraction step into the neural network (dhingra et al., 2017), adding richer utterance embeddings (socher et al., 2013), and supporting text generation (sordoni et al., 2015)."
p17-1062,2017,7 Conclusion,"more broadly, hcns are a general model for stateful control, and we would be interested to explore applications beyond dialog systems 鈥 for example, in nlp medical settings or humanrobot nl interaction tasks, providing domain constraints are important for safety; and in resourcepoor settings, providing domain knowledge can amplify limited data."
p17-1062,2017,7 Conclusion,"of course, we also plan to deploy the model in a live dialog system."
p17-1062,2017,7 Conclusion,"we will also explore using hcns with automatic speech recognition (asr) input, for example by forming features from n-grams of the asr n-best results (henderson et al., 2014b)."
p17-1063,2017,8 Conclusion,"finally, it would be interesting to combine our algorithm with a speech synthesis system."
p17-1063,2017,8 Conclusion,"furthermore, we could use this data to refine the costs c(d), c(ia) etc.for the edit operations, possibly assigning different costs to different edit operations."
p17-1063,2017,8 Conclusion,"in future work, we will complement the overhearer experiment presented here with an end-toend evaluation in an interactive nlg setting."
p17-1063,2017,8 Conclusion,it will also give us the opportunity to investigate empirically the limits of the corruption model.
p17-1063,2017,8 Conclusion,this will allow us to further investigate the quality of the correction strategies and refine the shortening strategy.
p17-1064,2017,7 Conclusion,"in our future work, we expect several developments that will shed more light on utilizing source syntax, e.g., designing novel syntactic features (e.g., features showing the syntactic role that a word is playing) for nmt, and employing the source syntax to constrain and guild the attention models."
p17-1065,2017,6 Conclusion and Future Work,"in addition, we will apply our method to other sequenceto-sequence tasks, such as text summarization, to verify the effectiveness."
p17-1065,2017,6 Conclusion and Future Work,"in future work, along this research direction, we will try to integrate other prior knowledge, such as semantic information, into nmt systems."
p17-1066,2017,6 Conclusion and Future Work,"in the future, we will focus on improving the rumor detection task by exploring network representation learning framework."
p17-1066,2017,6 Conclusion and Future Work,"moreover, we plan to investigate unsupervised models considering massive unlabeled rumorous data from social media."
p17-1068,2017,7 Conclusions,"another direction of future study will look at political ideology prediction in other countries and cultures, where ideology has different or multiple dimensions."
p17-1068,2017,7 Conclusions,"in addition, our work on user-level modeling can be integrated with work on message-level political bias to study how this is revealed across users with various levels of engagement."
p17-1068,2017,7 Conclusions,"while our study focused solely on text posted by the user, follow-up work can use other modalities such as images or social network analysis to improve prediction performance."
p17-1071,2017,5 Conclusion,"among the potential extensions of this work are the inclusion of different kinds of weights such as tf-idf, embedding relatedness and semantic relatedness."
p17-1071,2017,5 Conclusion,"we also intend to test other variants around the same concept, including considering the matched words and sequences to have a negative weight to balance further the weight of missing words."
p17-1072,2017,6 Concluding Discussion,it remains as a further stage of analysis to understand the underlying reasons that lead to these relations between ideas.
p17-1072,2017,6 Concluding Discussion,there are many potential directions to improve our method to account for complex relations between ideas.
p17-1075,2017,7 Conclusion,"in future work, we plan to use the analysis from the present study in constructing a system that can be applied to multiple datasets."
p17-1079,2017,5 Conclusion,one interesting avenue of future work is to automatically learn encodings and error correcting codes that are well-suited for the type of binary code prediction we are performing here.
p17-1080,2017,7 Conclusion,"another area for future work is to extend the analysis to other word representations (e.g.byte-pair encoding), deeper networks, and more semantically-oriented tasks such as semantic rolelabeling or semantic parsing."
p17-1081,2017,5 Conclusion,"as future work, we plan to develop a lstmbased attention model to determine the importance of each utterance and its specific contribution to each modality for sentiment classification."
p17-1082,2017,8 Conclusion,our hope is that these stance dimensions will be valuable as a convenient building block for future research on interactional meaning.
p17-1084,2017,8 Conclusion,"as a future work, we would like to explore the feasibility of marrying our semantic and neural models to exploit the benefits that each of them has to offer."
p17-1085,2017,8 Conclusion,"in future, we plan to explore pretraining methods for our model which were shown to improve recall on entity and relation performance by miwa and bansal (2016)."
p17-1085,2017,8 Conclusion,"it would also be interesting to see the effect of reranking (collins and koo, 2005) on our joint model."
p17-1085,2017,8 Conclusion,we also plan to extend the identification of entities to full entity mention span instead of only the head phrase as in lu and roth (2015).
p17-1085,2017,8 Conclusion,"we also plan to use sparsemax (martins and astudillo, 2016) instead of softmax for multiple relations, as the former is more suitable for multi-label classification for sparse labels."
p17-1086,2017,7 Related work and discussion,"in the future, we wish to test these ideas in more domains, naturalize a real pl, and handle paraphrasing and implicit arguments."
p17-1087,2017,9 Conclusion,another possibility is to use thesauri and word vector representations together with word sense disambiguation to generate semantically similar clusters for multiple senses of words.
p17-1087,2017,9 Conclusion,"finally, we plan to extend the hard signed clustering presented here to probabilistic soft clustering."
p17-1087,2017,9 Conclusion,"furthermore, signed spectral clustering has broader applications such as cellular biology, social networking, and electricity networks."
p17-1087,2017,9 Conclusion,"our signed spectral clustering method could be applied to a broad range of nlp tasks, such as prediction of social group clustering, identification of personal versus non-personal verbs, and analyses of clusters which capture positive, negative, and objective emotional content."
p17-1088,2017,7 Conclusion and Future Work,"in addition, our framework can also be applied to multi-task learning, promoting a finer sharing among different tasks."
p17-1088,2017,7 Conclusion and Future Work,"in the future, we plan to enable itransf to perform multi-step inference, and extend the sharing mechanism to entity and relation embeddings, further enhancing the statistical binding across parameters."
p17-1092,2017,8 Conclusion,"these findings motivate further improvements to discourse parsing, especially for new genres."
p17-1094,2017,7 Conclusions,"our study opens several future directions, ranging from defining algorithms based on automatically learned loss functions to learning more effective measures from expert examples."
p17-1095,2017,6 Conclusion,"it would also be interesting to explore more expressive parameterizations, such recurrent neural networks for hy."
p17-1095,2017,6 Conclusion,our model may be useful in the context of active learning where efficient re-estimation and performance in low-data conditions are important.
p17-1095,2017,6 Conclusion,there are many potential avenues for future work.
p17-1096,2017,6 Conclusions,"in the future, we plan to apply our approach to more question answering datasets in different domains."
p17-1096,2017,6 Conclusions,it will also be intriguing to generalize gdans to other applications.
p17-1098,2017,8 Conclusion,the diversification model proposed is general enough to apply to other nlg tasks with suitable modifications and we are currently working on extending this to dialog systems and general summarization.
p17-1102,2017,5 Conclusion and Future Work,"in the future, it would be interesting to explore the performance of positionrank on other types of documents, e.g., web pages and emails."
p17-1103,2017,7 Discussion,an important direction for future work is modifying adem such that it is not subject to this bias.
p17-1103,2017,7 Discussion,an important direction of future research is building models that can evaluate the capability of a dialogue system to have an engaging and meaningful interaction with a human.
p17-1103,2017,7 Discussion,"such models are necessary even for creating a test set in a new domain, which will help us determine if adem generalizes to related dialogue domains."
p17-1103,2017,7 Discussion,we leave investigating the domain transfer ability of adem for future work.
p17-1104,2017,7 Conclusion,"a parser for ucca will enable using the framework for new tasks, in addition to existing applications such as machine translation evaluation (birch et al., 2016)."
p17-1104,2017,7 Conclusion,"future work will evaluate tupa in a multilingual setting, assessing ucca鈥檚 cross-linguistic applicability."
p17-1104,2017,7 Conclusion,"in addition, we will explore different conversion procedures (kong et al., 2015) to compare different representations, suggesting ways for a data-driven design of semantic annotation."
p17-1104,2017,7 Conclusion,"we believe ucca鈥檚 merits in providing a cross-linguistically applicable, broadcoverage annotation will support ongoing efforts to incorporate deeper semantic structures into various applications, such as sentence simplification (narayan and gardent, 2014) and summarization (liu et al., 2015)."
p17-1104,2017,7 Conclusion,"we will also apply the tupa transition scheme to different target representations, including amr and sdp, exploring the limits of its generality."
p17-1105,2017,5 Conclusion,"our results demonstrate their promise for tree prediction tasks, and we believe their application to more general output structures is an interesting avenue for future work."
p17-1106,2017,6 Conclusion,"in the future, we plan to apply our approach to more nmt approaches (sutskever et al., 2014; shen et al., 2016; tu et al., 2016; wu et al., 2016) on more language pairs to further verify its effectiveness."
p17-1106,2017,6 Conclusion,it is also interesting to develop relevancebased neural translation models to explicitly control relevance to produce better translations.
p17-1108,2017,5 Conclusion and Future Work,there is lots of future work we can do.
p17-1112,2017,7 Conclusion,"we believe that there are many future avenues to explore to further increase the accuracy of such parsers, including different training objectives, more structured architectures and semisupervised learning."
p17-1113,2017,6 Conclusion,"in the future work, we will replace the softmax function in the output layer with multiple classifier, so that a word can has multiple tags."
p17-1115,2017,7 Conclusions and Future Work,future work may involve testing prewin on an ner task to see if and how it can generalise to a different classification task and how the results compare to the sota and similar methods such as that of collobert et al.(2011) using the conll 2003 ner datasets.
p17-1115,2017,7 Conclusions and Future Work,"if it does perform better, this will be of considerable interest to classification research (and beyond) in nlp."
p17-1116,2017,7 Conclusion,"additionally, we plan to apply the proposed model to other social media analyses such as gender analysis and age analysis."
p17-1116,2017,7 Conclusion,"as future works of this study, we are planning to expand the proposed model to handle multiple locations and a temporal state to capture location changes and states like traveling."
p17-1117,2017,6 Conclusion,"in future work, we are applying our entailment-based multi-task paradigm to other directed language generation tasks such as image captioning and document summarization."
p17-1118,2017,6 Conclusions and Future Work,"in future work, we intend to explore other methods to enrich cn, such as the recurrent language model, and use other metrics to characterize an adjacency network."
p17-1120,2017,6 Future Work,"although we used only text data to perform chat detection, we can also utilize contextual information such as the previous utterances (xu and sarikaya, 2014), the acoustic information (jiang et al., 2015), and the user profile (sano et al., 2016)."
p17-1120,2017,6 Future Work,"an important future work is to develop a sophisticated dialogue manager to handle such utterances, for example, by making clarification questions (schloder and fernandez, 2015).篓 we manually investigated the dialogue acts in the chat detection dataset (c.f., section 3.2)."
p17-1120,2017,6 Future Work,further efforts on improving nontask-oriented dialogue systems is an important future work.
p17-1120,2017,6 Future Work,incorporating these techniques into our methods is also an important future work.
p17-1120,2017,6 Future Work,it is an interesting research topic to use such contextual information beyond text.
p17-1120,2017,6 Future Work,it is interesting to automatically determine the dialogue acts to help producing appropriate system responses.
p17-1120,2017,6 Future Work,"some related studies exist in such a research direction (meguro et al., 2010)."
p17-1120,2017,7 Conclusion,"to facilitate future research, we are going to release the dataset together with the feature values derived from the tweets and web search queries."
p17-1121,2017,7 Conclusion and Future Work,"in future, we would like to include other sources of information in our model."
p17-1121,2017,7 Conclusion and Future Work,"our initial plan is to include rhetorical relations, which has been shown to benefit existing grid models (feng et al., 2014)."
p17-1121,2017,7 Conclusion and Future Work,"we would also like to extend our model to other forms of discourse, especially, asynchronous conversations, where participants communicate with each other at different times (e.g., forum, email)."
p17-1122,2017,8 Conclusion,"finally, we believe that future work should be evaluated in situ, to examine if, and to what extent, the generated responses participate in and affect the discourse (feed) in social media."
p17-1122,2017,8 Conclusion,"some future avenues for investigation include improving the relevance and human-likeness results by improving the automatic parses quality, acquiring more complex templates via abstract grammars, and experimenting with more sophisticated scoring functions for reranking."
p17-1122,2017,8 Conclusion,"with the emergence of deep learning, we further embrace the opportunity to combine the sequence-to-sequence modeling view explored so far with conditioning generation on speakers agendas and user profiles, pushing the envelope of opinionated generation further."
p17-1123,2017,7 Conclusion and Future Work,"besides this, it would also be interesting to consider to incorporate mechanisms for other language generation tasks (e.g., copy mechanism for dialogue generation) in our model to further improve the quality of generated questions."
p17-1123,2017,7 Conclusion and Future Work,here we point out several interesting future research directions.
p17-1123,2017,7 Conclusion and Future Work,we would like to explore how to better use the paragraph-level information to improve the performance of qg system regarding questions of all categories.
p17-1124,2017,6 Conclusion and Future Work,"as future work, we plan to investigate more sophisticated sampling strategies based on active learning and concept graphs to incorporate lexicalsemantic information for concept selection."
p17-1124,2017,6 Conclusion and Future Work,"we also plan to look into ways to propagate feedback to similar and related concepts with partial feedback, to reduce the total amount of feedback."
p17-1125,2017,6 Conclusions,future work involves investigating a better memory selection scheme.
p17-1125,2017,6 Conclusions,"other regularization methods (e.g., norm or drop out) are also interesting and may alleviate the over-fitting problem"
p17-1126,2017,5 Conclusion and Future Work,"in future work, we plan to apply our model to descriptions of time-series data in various domains such as weather forecasts and sports, which share the above writing-style characteristics."
p17-1126,2017,5 Conclusion and Future Work,we also plan to use multiple time-series as input such as multiple brands of stock.
p17-1128,2017,5 Conclusion,"in the future, we will study how to construct a taxonomy from texts in chinese."
p17-1129,2017,5 Conclusions and Future Work,another direction to explore is joint learning of syntactic parser and chain-of-trees lstm.
p17-1129,2017,5 Conclusions and Future Work,"for future work, we will investigate the wider applicability of chain-of-trees lstm as a general text encoder that can simultaneously capture local syntactic structure and long-range semantic dependency."
p17-1129,2017,5 Conclusions and Future Work,"we will also apply the tree-guided attention mechanism to nlp tasks that need syntaxaware attention, such as machine translation, sentence summarization, textual entailment, etc."
p17-1131,2017,8 Conclusions,"conversation content: empathic counselors use reflective language and talk about behavior change, while less empathic counselors persuade more and focus on client resistance toward change."
p17-1131,2017,8 Conclusions,"in the future, we plan to build upon the acquired knowledge and the developed classifiers to create automatic tools that provide accurate evaluative feedback of counseling practice."
p17-1133,2017,6 Conclusions and Future Work,"promising future directions would be to investigate how to utilize user interaction in moocs for better prerequisite learning, as well as how deep learning models can be used to automatically learn useful features to help infer prerequisite relations."
p17-1134,2017,6 Conclusion and Future Work,a more sophisticated approach would be to incorporate features into the unsupervised model.
p17-1134,2017,6 Conclusion and Future Work,"for the literary analysis, as well, to bridge the gap between work like morzinski (1994) and a computational application, it remains to be seen how precise an annotation is possible for this task."
p17-1134,2017,6 Conclusion and Future Work,the incorporation of these features for this segmentation task could be a potentially fruitful avenue for future work.
p17-1136,2017,4 Conclusion,"apart from it, we intend to propose a neural architecture that accomplishes the joint learning of lemmas with other morphological attributes."
p17-1136,2017,4 Conclusion,"in our work, we use the 鈥榢eras鈥 software keeping 鈥榯heano鈥 as backend."
p17-1137,2017,8 Conclusion,"to further validate the performance of our model on different languages, we collected multilingual wikipedia corpus for 7 typologically diverse languages."
p17-1137,2017,8 Conclusion,we will investigate a model which can marginalise word segmentation as latent variables in the future work.
p17-1138,2017,6 Conclusion,"in future work, we would like to apply the presented non-linear bandit learners to other structured prediction tasks."
p17-1140,2017,6 Conclusions,"in the future, we plan to validate the effectiveness of our model on more language pairs."
p17-1141,2017,6 Conclusion,"in future work, we hope to evaluate gbs with models outside of mt, such as automatic summarization, image captioning or dialog generation."
p17-1141,2017,6 Conclusion,"we also hope to introduce new constraintaware models, for example via secondary attention mechanisms over lexical constraints."
p17-1142,2017,7 Conclusion and Future Work,"as new obfuscated words are introduced in escort advertisements, our hope is that character models will stay invariant to these obfuscations.understanding images."
p17-1142,2017,7 Conclusion and Future Work,future direction involves using graphical modeling to understand interactions in the scene.
p17-1142,2017,7 Conclusion and Future Work,"in order to eliminate the need for retraining the word vectors as the language of the domain evolves, we plan to use character models to learn a better language model for trafficking."
p17-1143,2017,6 Conclusion,we hope that this paper and the accompanying database serve as a first step towards nlp being applied in cybersecurity and that other researchers will be inspired to contribute to the database and to construct their own datasets and implementations.
p17-1144,2017,5 Conclusion and Future Work,"some potential augmentations include more fine-grained revision categories, revision properties such as statement strength (tan and lee, 2014) and quality evaluations, and sub-sentential revision scopes."
p17-1144,2017,5 Conclusion and Future Work,we also plan to augment the corpus to support additional types of research on revision analysis.
p17-1144,2017,5 Conclusion and Future Work,"while in this paper we explored language as one factor influencing rewriting behavior, our corpus also contains information about other potential factors such as gender and education level which we plan to investigate in the future."
p17-1146,2017,7 Conclusion,"because our neural models are applicable to srl, applying our models for multilingual srl tasks presents an interesting future research direction."
p17-1146,2017,7 Conclusion,"in future work, we plan to explore effective methods for exploiting large-scale unlabeled data to learn the neural models."
p17-1148,2017,7 Future Work,"other document-level features, such as example input-output pairs, unit tests, might be useful in this endeavor."
p17-1149,2017,7 Conclusions and Future Work,"in the future, we will improve the scalability of our model and learn multi-prototype embeddings for the mentions without reference entities in a knowledge base, and introduce compositional approaches to model the internal structures of multiword mentions."
p17-1150,2017,6 Conclusion,future work can explore deep neural network to alleviate feature engineering.
p17-1150,2017,6 Conclusion,one of the important questions to address in the future is how to learn new predicates through interaction with humans.
p17-1150,2017,6 Conclusion,our future work will incorporate interactive learning of verb semantics with task learning to enable autonomy that can learn by communicating with humans.
p17-1151,2017,5 Discussion,"alternatively, we could imagine a dependent mixture model where the distributions over words are evolving with time and other covariates."
p17-1151,2017,5 Discussion,"in the future, multimodal word distributions could open the doors to a new suite of applications in language modelling, where whole word distributions are used as inputs to new probabilistic lstms, or in decision functions where uncertainty matters."
p17-1152,2017,6 Conclusions and Future Work,"future work interesting to us includes exploring the usefulness of external resources such as wordnet and contrasting-meaning embedding (chen et al., 2015) to help increase the coverage of wordlevel inference relations."
p17-1152,2017,6 Conclusions and Future Work,"modeling negation more closely within neural network frameworks (socher et al., 2013; zhu et al., 2014) may help contradiction detection."
p17-1153,2017,6 Conclusion,future work includes expanding the analyses to non-english movies and combining the linguistic metrics with character networks.
p17-1154,2017,6 Conclusion and Future Work,"as future work, we plan to apply the linguistic regularizers to tree-lstm to address the scope issue since the parsing tree is easier to indicate the modification scope explicitly."
p17-1155,2017,8 Discussion and Future Work,designing automatic measures is hence left for future research.
p17-1155,2017,8 Discussion and Future Work,future research directions rise from cases in which the sign models left the tweet unchanged.
p17-1155,2017,8 Discussion and Future Work,several challenges are still to be addressed in future research so that sarcasm interpretation can be performed in a fully automatic manner.
p17-1155,2017,8 Discussion and Future Work,these include the design of appropriate automatic evaluation measures as well as improving the algorithmic approach so that it can take world knowledge into account and deal with cases where the sentiment of the input tweet is not expressed with a clear sentiment words.
p17-1155,2017,8 Discussion and Future Work,we hope this new resource will help researchers make further progress on this new task.
p17-1158,2017,6 Conclusion and Future Work,"in future, we will strive to implement cane on a wider variety of information networks with multi-modal data, such as labels, images and so on.(2) cane encodes latent relations between vertices into their context-aware embeddings."
p17-1158,2017,6 Conclusion and Future Work,"thus, we want to explore how to incorporate and predict these explicit relations between vertices in ne."
p17-1158,2017,6 Conclusion and Future Work,we will explore the following directions in future: (1) we have investigated the effectiveness of cane on text-based information networks.
p17-1159,2017,7 Conclusion,possible future work include expanding the investigation to other regional languages such as malay and indonesian.
p17-1160,2017,Future work,an interesting avenue for future work is to explore higher order factorizations for noncrossing digraphs and the related inference.
p17-1160,2017,Future work,"we are planning to extend the coverage of the approach by exploring 1-endpointcrossing and mhk trees (pitler et al., 2013; gomez-rodr 麓 麓谋guez, 2016), and related digraphs 鈥 see (yli-jyra篓, 2004; gomez-rodr 麓 麓谋guez et al., 2011)."
p17-1160,2017,Future work,we would also like to have more insight on the transformation of mso definable properties to the current framework and to logspace algorithms.
p17-1163,2017,7 Conclusion,"in future work, we intend to explore applications of the nbt for multi-domain dialogue systems, as well as in languages other than english that require handling of complex morphological variation."
p17-1164,2017,6 Conclusions,"moreover, we also use events from fn to augment the performance of the proposed approach."
p17-1166,2017,5 Conclusion and Future Works,"in the future, we will focus on two aspects: (1) our method in this paper considers pairwise intersections between labels, so to better exploit class ties, we will extend our method to exploit all other labels鈥 influences on each relation for relation extraction, transferring second-order to high-order (zhang and zhou, 2014); (2) we will focus on other problems by leveraging class ties between labels, specially on multi-label learning problems (zhou et al., 2012) such as multi-category text categorization (rousu et al., 2005) and multi-label image categorization (zha et al., 2008)."
p17-1167,2017,6 Conclusion & Future Work,"because of the dynamic nature of our framework, it is not trivial to leverage the computational capabilities of gpus using minibatched training; we plan to investigate ways to take full advantage of modern computing machinery in the near future."
p17-1167,2017,6 Conclusion & Future Work,"for instance, although our current formal language design covers most question types in sqa, it is nevertheless important to extend it further to make the semantic parser more robust (e.g., by including union or allowing comparison of multiple previous answers)."
p17-1167,2017,6 Conclusion & Future Work,"in the future, we plan to investigate several interesting research questions triggered by this work."
p17-1169,2017,6 Conclusions and Future Work,it would also be interesting to investigate several possible extensions to the current clustering work.
p17-1169,2017,6 Conclusions and Future Work,one direction is to learn a proper ground distance for word embeddings such that the final document clustering performance can be improved with labeled data.
p17-1169,2017,6 Conclusions and Future Work,"the work by (huang et al., 2016; cuturi and avis, 2014) have partly touched this goal with an emphasis on document proximities."
p17-1170,2017,7 Conclusion and Future Work,"also, given the promising results observed for supersenses, we plan to investigate taskspecific coarsening of sense inventories, particularly wikipedia, or the use of sentiwordnet (baccianella et al., 2010), which could be more suitable for polarity detection."
p17-1170,2017,7 Conclusion and Future Work,"as future work, we plan to investigate the extension of the approach to other languages and applications."
p17-1170,2017,7 Conclusion and Future Work,we hope that our work will foster future research on the integration of senselevel knowledge into downstream applications.
p17-1171,2017,6 Conclusion,future work should aim to improve over our drqa system.
p17-1174,2017,6 Conclusion,"in addition, we plan to combine our decoder with other encoders that capture language structure, such as a hierarchical rnn (luong and manning, 2016), a tree-lstm (eriguchi et al., 2016b), or an order-free encoder, such as a cnn (kalchbrenner and blunsom, 2013)."
p17-1174,2017,6 Conclusion,"in future work, we will explore the optimal structures of chunk-based decoder for other free word-order languages such as czech, german, and turkish."
p17-1175,2017,7 Conclusions and Future Work,"in the future, we will incorporate coverage into our model and study how to apply it to other natural language processing tasks."
p17-1176,2017,6 Conclusion,"in the future, we plan to test our approach on more diverse language pairs, e.g., zero-resource uyghur-english translation using chinese as a pivot."
p17-1176,2017,6 Conclusion,it is also interesting to extend the teacherstudent framework to other cross-lingual nlp applications as our method is transparent to architectures.
p17-1177,2017,6 Conclusion,"for future work, it will be interesting to make use of node labels from the tree, or to use syntactic information on the target side, as well."
p17-1178,2017,6 Conclusions and Future Work,"in the future, we will explore the topological structure of related languages and exploit cross-lingual knowledge transfer to enhance the quality of extraction and linking."
p17-1178,2017,6 Conclusions and Future Work,the general idea of deriving noisy annotations from kb properties can also be extended to other ie tasks such as relation extraction.
p17-1179,2017,6 Conclusion,"future work also includes investigating other divergences that adversarial training can minimize (nowozin et al., 2016), and broader mathematical tools that match distributions (mohamed and lakshminarayanan, 2016)."
p17-1179,2017,6 Conclusion,our work is likely to benefit from advances in techniques that further stabilize adversarial training.
p17-1180,2017,8 Conclusion and Future Work,"as future directions, we plan to extend gwld to several other languages and conduct similar sociolinguistic studies on cs patterns including not only more languages and geographies, but also other aspects like topic and sentiment."
p17-1181,2017,5 Conclusions,cognates detection is an interesting and challenging task.
p17-1182,2017,8 Future Work,"in the future, we want to develop methods to make better use of languages with different alphabets or morphosyntactic features, in order to increase the applicability of our knowledge transfer method."
p17-1183,2017,7 Conclusion,"future work may include applying our model to other nearly-monotonic alignand-transduce tasks like abstractive summarization, transliteration or machine translation."
p17-1184,2017,6 Conclusion,we plan to explore these effects in future work.
p17-1185,2017,7 Conclusions,possible direction of future work is to apply more advanced optimization techniques to the step 1 of the scheme proposed in section 1 and to explore the step 2 鈥 obtaining embeddings with a given low-rank matrix.
p17-1186,2017,6 Conclusion,"because our techniques apply to labeled directed graphs in general, they can easily be extended to incorporate more formalisms, semantic or otherwise."
p17-1186,2017,6 Conclusion,in future work we hope to explore cross-task scoring and inference for tasks where parallel annotations are not available.
p17-1187,2017,5 Conclusion and Future Work,we will explore the effectiveness of sememe information for wrl in other languages.
p17-1187,2017,5 Conclusion and Future Work,"we will explore the following research directions in future: (1) the sememe information in hownet is annotated with hierarchical structure and relations, which have not been considered in our framework."
p17-1187,2017,5 Conclusion and Future Work,we will explore to utilize these annotations for better wrl.(2) we believe the idea of sememes is universal and could be wellfunctioned beyond languages.
p17-1188,2017,7 Conclusion and Future Work,"in the future, we hope to further generalize this method to other tasks such as pronunciation estimation, which can take advantage of the fact that pronunciation information is encoded in parts of the characters as demonstrated in fig.1, or machine translation, which could benefit from a wholistic view that considers both semantics and pronunciation."
p17-1188,2017,7 Conclusion and Future Work,"we also hope to apply the model to other languages with complicated compositional writing systems, potentially including historical texts such as hieroglyphics or cuneiform."
p17-1189,2017,7 Conclusion and Future Work,"so in the future, we might try to combine more heterogeneous semantic data for other tasks like event extraction and relation classification, etc."
p17-1189,2017,7 Conclusion and Future Work,we hope that it will be helpful in providing common benchmarks for future work on chinese srl tasks.
p17-1190,2017,6 Conclusion,"future work will explore additional data sources, including from aligning different translations of novels (barzilay and mckeown, 2001), aligning new articles of the same topic (dolan et al., 2004), or even possibly using machine translation systems to translate bilingual text into paraphrastic sentence pairs."
p17-1191,2017,6 Conclusion,"moreover, the methods described in this paper can be extended to other kinds of structured knowledge sources like freebase which may be more suitable for tasks like question answering."
p17-1191,2017,6 Conclusion,this approach may be extended to other nlp tasks that can benefit from using encoders that can access wordnet information.
p17-1193,2017,6 Conclusion and Future Work,"in particular, we enhance the maximum subgraph model with new parsing algorithms for 1ec/p2 graphs."
p17-1193,2017,6 Conclusion and Future Work,we leave this for future investigation.
p17-1194,2017,9 Conclusion,future work could investigate the extension of this architecture to additional unannotated resources.
p17-1194,2017,9 Conclusion,the model is incentivised to discover useful features in order to learn the language distribution and composition patterns in the training data.
p18-1001,2018,6 Conclusion and Future Work,"future work includes an investigation into the trade-off between learning full covariance matrices for each word distribution, computational complexity, and performance."
p18-1001,2018,6 Conclusion and Future Work,other future work involves co-training pft on many languages.
p18-1002,2018,6 Conclusion,"both in this area and for n-grams there is great scope for combining our approach with compositional approaches (bojanowski et al., 2016; poliak et al., 2017) that can handle settings such as zero-shot learning."
p18-1002,2018,6 Conclusion,"extensions of the mathematical formulation, such as the use of word weighting when building context vectors as in arora et al.(2018b) or of spectral information along the lines of mu and viswanath (2018), are also worthy of further study."
p18-1002,2018,6 Conclusion,"more work is needed to understand the usefulness of our method for representing (potentially cross-lingual) entities such as synsets, whose embeddings have found use in enhancing wordnet and related knowledge bases (camachocollados et al., 2016; khodak et al., 2017)."
p18-1002,2018,6 Conclusion,"of particular interest is the replacement of simple window contexts by other structures, such as dependency parses, that could yield results in domains such as question answering or semantic role labeling."
p18-1004,2018,6 Conclusion,"in future work, we will investigate explicit retrofitting methods for asymmetric relations like hypernymy and meronymy."
p18-1004,2018,6 Conclusion,we also intend to apply the method to other downstream tasks and to investigate the zero-shot language transfer of the specialization function for more language pairs.
p18-1005,2018,5 Conclusion and Future work,"besides, we decide to make more efforts to explore how to reinforce the temporal order information for the proposed model."
p18-1005,2018,5 Conclusion and Future work,"in the future, we would like to investigate how to utilize the monolingual data more effectively, such as incorporating the language model and syntactic information into unsupervised nmt."
p18-1005,2018,5 Conclusion and Future work,unsupervised nmt opens exciting opportunities for the future research.
p18-1006,2018,5 Conclusion,"by introducing another rich language, our method can better exploit the additional language pairs to enrich the original low-resource pair."
p18-1006,2018,5 Conclusion,"in the future, we may extend our architecture to other scenarios, such as totally unsupervised training with no bilingual data for the rare language."
p18-1007,2018,6 Conclusion,"additionally, we would like to explore the application of subword regularization for machine learning, including denoising auto encoder (vincent et al., 2008) and adversarial training (goodfellow et al., 2015)"
p18-1007,2018,6 Conclusion,"promising avenues for future work are to apply subword regularization to other nlp tasks based on encoder-decoder architectures, e.g., dialog generation (vinyals and le, 2015) and automatic summarization (rush et al., 2015)."
p18-1008,2018,7 Conclusion,"our focus on a standard single-language-pair translation task leaves important open questions to be answered: how do our new architectures compare in multilingual settings, i.e., modeling an interlingua?"
p18-1008,2018,7 Conclusion,"we hope that our work will motivate nmt researchers to further investigate generally applicable training and optimization techniques, and that our exploration of hybrid architectures will open paths for new architecture search efforts for nmt."
p18-1010,2018,8 Conclusion,"while this work already demonstrates considerable improvement over non-hierarchical modeling, future work will explore techniques such as box embeddings (vilnis et al., 2018) and poincare麓 embeddings (nickel and kiela, 2017) to represent the hierarchical embedding space, as well as methods to improve recall in the candidate generation process for entity linking."
p18-1015,2018,5 Conclusion and Future Work,"on the other hand, we plan to test our system on the other tasks such as document-level summarization and short text conversation."
p18-1015,2018,5 Conclusion and Future Work,we believe our work can be extended in various aspects.
p18-1016,2018,8 Conclusion,future work will leverage ucca鈥檚 cross-linguistic applicability to support multi-lingual ts and ts pre-processing for mt.
p18-1018,2018,7 Conclusion,"we expect that future work will develop methods to scale the annotation process beyond requiring highly trained experts; bring this scheme to bear on other languages; and investigate the relationship of our scheme to more structured semantic representations, which could lead to more robust models."
p18-1019,2018,6 Conclusions,"this dataset fills a need for larger scale corpora to facilitate research on nlp methods for processing the biomedical literature, which have the potential to aid the conduct of ebm."
p18-1020,2018,6 Conclusions,the significant gains demonstrated suggests easl as a promising approach for future dataset curation and system evaluation in the community.
p18-1023,2018,7 Conclusion,"to obtain further insights into the nature of counterarguments, deeper linguistic analysis along with supervised learning may be needed, though."
p18-1023,2018,7 Conclusion,"we did not aim to engineer the best approach to this retrieval task, but to study whether we can model the simultaneous similarity and dissimilarity of a counterargument to an argument computationally."
p18-1023,2018,7 Conclusion,"we provide a corpus to train respective approaches, but leave the according research to future work."
p18-1023,2018,7 Conclusion,"while the model can be considered open-topic, a next step will be to study counterargument retrieval open-source."
p18-1024,2018,7 Concluding Remarks and Future Work,"for future work, we would like to extend the current evaluation of our work from a two-graph setting to multiple graphs."
p18-1024,2018,7 Concluding Remarks and Future Work,"in real-world setting, we envision our method to be integrated in a large scale system that would include various other components for tasks like conflict resolution, active learning and human-in-loop learning to ensure quality of constructed super-graph."
p18-1024,2018,7 Concluding Remarks and Future Work,we believe that this work opens a new research direction in joint representation learning over multiple knowledge graphs.
p18-1025,2018,6 Conclusion and Future Work,an exciting direction is the incorporation of multi-relational data for general knowledge representation and inference.
p18-1026,2018,7 Discussion and Conclusion,incorporating both ideas to our architecture is an research direction we plan for future work.
p18-1026,2018,7 Discussion and Conclusion,we believe this is an interesting research direction in terms of applications.
p18-1027,2018,8 Conclusion,"these findings not only help us better understand these models but also suggest ways for improving them, as discussed in section 7."
p18-1027,2018,8 Conclusion,"while observations in this paper are reported at the token level, deeper understanding of sentence-level interactions warrants further investigation, which we leave to future work."
p18-1029,2018,6 Discussion and Future Work,"future work can model how these parameters can be adapted in a task specific way (e.g., cases such as cancer prediction where base rates are small), and provide better models of quantifier semantics.e.g., as distributions, rather than point values."
p18-1030,2018,5 Conclusion,"next directions also include the investigation of s-lstm to more nlp tasks, such as machine translation."
p18-1030,2018,5 Conclusion,we leave such investigation to future work.
p18-1031,2018,6 Discussion and future directions,another direction is to apply the method to novel tasks and models.
p18-1031,2018,6 Discussion and future directions,"given that transfer learning and particularly fine-tuning for nlp is under-explored, many future directions are possible."
p18-1031,2018,6 Discussion and future directions,"language modeling can also be augmented with additional tasks in a multi-task learning fashion (caruana, 1993) or enriched with additional supervision, e.g.syntax-sensitive dependencies (linzen et al., 2016) to create a model that is more general or better suited for certain downstream tasks, ideally in a weakly-supervised manner to retain its universal properties."
p18-1033,2018,6 Conclusion,developers of future large-scale datasets should incorporate joins and nesting to create more human-like data.
p18-1033,2018,6 Conclusion,our analysis has clear implications for future work.
p18-1034,2018,6 Conclusion and Future Work,"in the future, we plan to improve the accuracy of the column prediction component."
p18-1034,2018,6 Conclusion and Future Work,we also plan to build a large-scale dataset that considers more sophisticated sql queries.
p18-1034,2018,6 Conclusion and Future Work,"we also plan to extend the approach to low-resource scenarios (feng et al., 2018)."
p18-1035,2018,10 Conclusion,"future work will investigate whether a single algorithm and architecture can be competitive on all of these parsing tasks, an important step towards a joint many-task model for semantic parsing."
p18-1037,2018,6 Conclusions,another promising direction is integrating character seq2seq to substitute the copy function.
p18-1037,2018,6 Conclusions,this should also improve the handling of negation and rare words.
p18-1039,2018,8 Conclusion,"in addition, we plan to expand the coverage of our meaning representation to support more mathematic concepts."
p18-1039,2018,8 Conclusion,"in the future, we will focus on tackling this challenge."
p18-1040,2018,8 Conclusions,"in the future, we plan to model document-level representations which are more in line with drt and the gmb annotations."
p18-1042,2018,8 Conclusion,"we are actively investigating ways to apply these two new resources to downstream applications, including machine translation, question answering, and additional paraphrase generation tasks."
p18-1044,2018,6 Conclusion,"in the future, we will apply this semi-supervised training method to other nlp tasks."
p18-1045,2018,6 Conclusions and the Future Work,"in the future, we will explore the use of additional discourse structures that correlate highly with event coreference chains."
p18-1045,2018,6 Conclusions and the Future Work,"moreover, we will extend this work to other domains such as biomedical domains."
p18-1047,2018,Conclusions and Future Work,another future work is test our model in other nlp tasks like event extraction.
p18-1047,2018,Conclusions and Future Work,our future work will concentrate on how to improve the performance further.
p18-1048,2018,7 Conclusion,"considering this possibility, we suggest to drive the two discriminators in our self-regulation framework to cooperate with each other."
p18-1048,2018,7 Conclusion,"therefore, in the future, we will encode the global information by neural networks and use the self-regulation strategy to reduce the negative influence of noises."
p18-1050,2018,7 Conclusions,"for the future work, we plan to expand event temporal knowledge acquisition by dealing with event sense disambiguation and event synonym identification (e.g., drag, pull and haul)."
p18-1051,2018,5 Conclusion,"in future work, we intend to thoroughly study the impact of tds given morphosyntactic information."
p18-1053,2018,5 Conclusion,"in the future, we plan to explore neural network models for efficaciously resolving anaphoric zero pronoun documents and research on some specific components which might influence the performance of the model, such as the embedding."
p18-1053,2018,5 Conclusion,"meanwhile, we plan to research on the possibility of applying adversarial learning (goodfellow et al., 2014) to generate better rewards than the human-defined reward functions."
p18-1056,2018,5 Conclusion,"therefore, we suggest that future work on social power and language use should consider other (maybe higher-level) linguistic elements."
p18-1056,2018,5 Conclusion,"we call for the inclusion of a wider range of factors in future studies of social influences on language use, especially low-level but interpretable cognitive factors."
p18-1057,2018,7 Conclusion and Future Work,for future work we plan to continue the collection and annotation of resources and to separately explore each of the above research tasks.
p18-1059,2018,5 Conclusion,"indeed, if outputs all similarly under-correct, correlation studies will not be affected by whether an rbm is sensitive to undercorrection."
p18-1060,2018,7 Discussion,"alternatively, we could give up on measuring absolute scores, and seek instead to find techniques stably rank methods and thus improve them."
p18-1060,2018,7 Discussion,we hope our work provides some clarity on to how to make it more cost effective.
p18-1065,2018,5 Conclusions and Recommendations,"consequently, we encourage researchers to evaluate the usefulness of these features and study these moderating factors in different domains, platforms, and languages, possibly identifying new features and moderating factors."
p18-1065,2018,5 Conclusions and Recommendations,"however, there have been exciting developments in helpfulness prediction: systems that have attempted to exploit user and reviewer information, along with those based on sophisticated models (e.g., probabilistic matrix factorization, hmm-lda) and neural network architectures, are promising prospects for future work."
p18-1065,2018,5 Conclusions and Recommendations,there are multiple ways to approach this task.
p18-1065,2018,5 Conclusions and Recommendations,we conclude our survey with several recommendations for future work on computational modeling and prediction of review helpfulness.
p18-1066,2018,7 Conclusion,"future directions include: 1) mining cross-cultural differences in general concepts other than names and slang, 2) merging the mined knowledge into existing knowledge bases, and 3) applying the socvec in downstream tasks like machine translation."
p18-1067,2018,7 Conclusion,"in future works, we will exploit the interesting connections between moral foundations and frames for the analysis of more detailed ideological leanings and stance prediction."
p18-1068,2018,6 Conclusions,"in the future, we would like to apply the framework in a weakly supervised setting, i.e., to learn semantic parsers from question-answer pairs and to explore alternative ways of defining meaning sketches."
p18-1069,2018,7 Conclusions,directions for future work are many and varied.
p18-1069,2018,7 Conclusions,"the proposed framework could be applied to a variety of tasks (bahdanau et al., 2015; schmaltz et al., 2017) employing sequence-to-sequence architectures."
p18-1072,2018,6 Conclusion,we hope that this work will guide further developments in this new and exciting field.
p18-1073,2018,6 Conclusions,"in the future, we would like to extend the method from the bilingual to the multilingual scenario, and go beyond the word level by incorporating embeddings of longer phrases."
p18-1074,2018,5 Conclusions and Future Work,"the next step of this research is to apply this architecture to other types of tasks, such as event extract and semantic role labeling that involve structure prediction."
p18-1074,2018,5 Conclusions and Future Work,we also plan to explore the possibility of integrating incremental learning into this architecture to adapt a trained model for new tasks rapidly.
p18-1076,2018,7 Conclusion and Future Work,"in future work, we will investigate even tighter integration of the attended knowledge and stronger reasoning methods."
p18-1079,2018,7 Limitations and Future Work,"developing more effective ways of leveraging the expert鈥檚 time to close the loop, and facilitating more interactive collaboration between humans and sears are exciting areas for future work."
p18-1079,2018,7 Limitations and Future Work,"table 5b), which recent work on paraphrasing (iyyer et al., 2018) or generation using gans (zhao et al., 2018) may address."
p18-1079,2018,8 Conclusion,"we demonstrated that seas and sears can be an invaluable tool for debugging nlp models, while indicating their current limitations and avenues for future work."
p18-1080,2018,7 Conclusion,in future work we intend to apply this technique to debiasing sentences and anonymization of author traits such as gender and age.
p18-1080,2018,7 Conclusion,"in the future work, we will also explore whether an enhanced back-translation by pivoting through several languages will learn better grounded latent meaning representations."
p18-1081,2018,5 Conclusion,"in future work, we hope to explore the potential of this architecture on further kinds of data, including multimodal data (long et al., 2018), from which one can extract structured signals."
p18-1083,2018,5 Conclusion,"we believe there are still lots of improvement space in the narrative paragraph generation tasks, like how to better simulate human imagination to create more vivid and diversified stories."
p18-1084,2018,8 Conclusion and Future Work,in future work we plan to improve our methods by exploiting the internal structure of images and sentences as well as by effectively integrating signals from more than two languages.
p18-1085,2018,5 Conclusion,"in future work, we would like to explore other aspects of search engines for language grounding as well as the effect these embeddings may have on learning generic sentence representations (kiros et al., 2015b; hill et al., 2016; conneau et al., 2017a; logeswaran and lee, 2018)."
p18-1085,2018,5 Conclusion,"recently, contextualized word representations have shown promising improvements when combined with existing embeddings (melamud et al., 2016; peters et al., 2017; mccann et al., 2017; peters et al., 2018)."
p18-1085,2018,5 Conclusion,we expect that integrating picturebook with these embeddings to lead to further performance improvements as well.
p18-1086,2018,6 Discussion and Conclusion,"we also plan to incorporate action-effect prediction to humanrobot collaboration, for example, to bridge the gap of commonsense knowledge about the physical world between humans and robots."
p18-1086,2018,6 Discussion and Conclusion,"we plan to apply more advanced approaches in the future, for example, attention models that jointly capture actions, image states, and effect descriptions."
p18-1088,2018,7 Conclusion and Future Work,we believe that there will be more effective solutions coming in the near future.
p18-1090,2018,5 Conclusions and Future Work,"for future work, we would like to explore a fine-grained version of sentiment-to-sentiment translation that not only reverses sentiment, but also changes the strength of sentiment."
p18-1091,2018,7 Conclusion,future works involve the choice of discourse markers and some other transfer learning sources.
p18-1092,2018,5 Conclusion,it might be interesting to analyze how other reasoning modules can improve different weaknesses of the model.
p18-1092,2018,5 Conclusion,"while other models have focused on different parts of these components, we think that is important to find ways to combine these different mechanisms if we want to build models capable of complex reasoning."
p18-1095,2018,6 Conclusions,"in the future we want to apply our marginal utility based framework to other metrics, such as mean average precision (map)."
p18-1098,2018,6 Conclusions and Discussions,"we will address these two issues in future by investigating diversity-promoting regularization (xie et al., 2017) and leveraging an external knowledge base that maps medical abbreviations into their full names."
p18-1101,2018,5 Conclusion and Future Work,"our findings also suggest promising future research directions, including learning better context-based latent actions and using reinforcement learning to adapt policy networks."
p18-1103,2018,6 Conclusion,we would like to explore in depth how attention can help improve neural dialogue modeling for both chatbots and taskoriented dialogue systems in our future work.
p18-1105,2018,6 Conclusion,another direction will be to apply fluctuation analysis in formulating a statistical test to evaluate the structural complexity underlying a sequence.
p18-1105,2018,6 Conclusion,"our future work will include an analysis using other kinds of data, such as twitter data and adult utterances, and a study of how taylor鈥檚 law relates to grammatical complexity for different sequences."
p18-1105,2018,6 Conclusion,taylor鈥檚 law and its exponent can also be applied to evaluate machine-generated text.
p18-1106,2018,5 Discussion,"in our investigation of kauhanen鈥檚 basic assumptions, we discover how seemingly innocuous decisions about population size and learning conspire to drive simulation results."
p18-1107,2018,8 Conclusion and Future Work,we further plan to empirically evaluate our lexicalization on an alignment task and to offer a comparison against the lexicalization due to zhang and gildea (2005).
p18-1107,2018,8 Conclusion and Future Work,"we plan to verify whether rank-k pl-rstag is more powerful than rank-k scfg in future work, and to reduce the rank of the transformed grammar if possible."
p18-1108,2018,6 Conclusion,"since the architecture of model is no more than a stack of standard recurrent and convolution layers, which are essential components in most academic and industrial deep learning frameworks, the deployment of this method would be straightforward."
p18-1111,2018,7 Conclusion,"in the future, we plan to take generalization one step further, and explore the possibility to use the bilstm for generating completely new paraphrase templates unseen during training."
p18-1113,2018,7 Conclusion,future work will introduce weighted cbow and skip-gram to learn positional information within sentences.
p18-1114,2018,6 Conclusion,"in the future, we intend to evaluate our models for some morpheme-rich languages like russian, german and so on."
p18-1115,2018,7 Conclusion and Future Work,"a latent factor model such as darn (gregor et al., 2014) would consider several sources simultaneously."
p18-1115,2018,7 Conclusion and Future Work,"extension to other architectures: introducing latent variables into non-autoregressive translation models such as the transformer (vaswani et al., 2017) should increase their translation ability further."
p18-1116,2018,6 Conclusion and future work,"as future work, we plan to design some more elaborate structures to incorporate the score layer into the encoder."
p18-1116,2018,6 Conclusion and future work,we will also apply the proposed linearization method to other tasks.
p18-1117,2018,7 Conclusions,"future work could also investigate whether context-aware nmt systems learn other discourse phenomena, for example whether they improve the translation of elliptical constructions, and markers of discourse relations and information structure."
p18-1118,2018,7 Conclusion,"for future work, we intend to investigate models which incorporate specific discourse-level phenomena."
p18-1119,2018,6 Conclusions and Future Work,future work may see our methods applied to document geolocation to assess the effectiveness of scaling geodesic vectors from paragraphs to entire documents.
p18-1120,2018,5 Conclusions and Future Work,challenges also remain in how to evaluate the accuracy of goal knowledge extracted from text corpora.
p18-1120,2018,5 Conclusions and Future Work,"in future work, we hope to see if we can take advantage of more contextual information as well as other external knowledge to improve the recognition of goalacts."
p18-1120,2018,5 Conclusions and Future Work,"nevertheless, our work represents a first step toward learning goal knowledge about locations, and we believe that learning knowledge about plans and goals is an important direction for natural language understanding research."
p18-1123,2018,6 Conclusions,in our future work we plan to extend the proposed method to these other applications.
p18-1123,2018,6 Conclusions,"while in this work, we apply the exemplar encoder decoder network on conversational task, the method is generic and could be used with other tasks such as question answering and machine translation."
p18-1124,2018,6 Conclusion,we further investigate the usability of dialsql in a real life setting by conducting human evaluations.
p18-1125,2018,7 Conclusions and Future Work,a promising line of future work could consider the complementary problem of identifying pragmatic strategies that can help bring uncivil conversations back on track.
p18-1125,2018,7 Conclusions and Future Work,our approach has several limitations which open avenues for future work.
p18-1125,2018,7 Conclusions and Future Work,"while our analysis focused on the very first exchange in a conversation for the sake of generality, more complex modeling could extend its scope to account for conversational features that more comprehensively span the interaction."
p18-1127,2018,9 Conclusion,"the difficulties in basing validation on system outputs may be applicable to other text-to-text generation tasks, a question we will explore in future work."
p18-1130,2018,5 Conclusion,"another interesting direction is to further improve our model by exploring reinforcement learning approaches to learn an optimal order for the children of head words, instead of using a predefined fixed order."
p18-1130,2018,5 Conclusion,"first, we intend to consider how to conduct experiments to improve the analysis of parsing errors qualitatively and quantitatively."
p18-1131,2018,6 Conclusion,"while the cross-domain strategies we presented can greatly increase accurate parsing of these features, narrowing the performance gap between aae- and mae-like tweets, much work remains to be done for accurate parsing of even linguistically well-documented features."
p18-1133,2018,6 Conclusion,"for our future work, we will consider advanced instantiations for sequicity, and extend sequicity to handle unsupervised cases where information and requested slots values are not annotated."
p18-1137,2018,6 Conclusion,"in future work, we plan to further investigate the impact of risksensitive objective functions, including the relations between model robustness and diverse generations."
p18-1137,2018,6 Conclusion,"while if we want to generate diverse responses, a risk-sensitive objective functions is helpful."
p18-1138,2018,5 Conclusion,"in future work, we plan to introduce reinforcement learning and knowledge base reasoning mechanisms to improve the performance."
p18-1139,2018,5 Conclusion,"as for future work, we will investigate how to apply the technique to multi-turn conversational systems, provided that the most proper sentence function can be predicted under a given conversation context."
p18-1140,2018,9 Conclusion,we believe this approach can be easily generalized to other domains given its end-to-end training procedure and task independence.
p18-1142,2018,7 Conclusions and Future Work,"another possible research direction is learning the mapping between structures from parallel texts jointly with a main task, in the spirit of quasi-synchronous grammars (smith and eisner, 2009)."
p18-1142,2018,7 Conclusions and Future Work,future work will look into automating the tree processing procedure.
p18-1143,2018,7 Conclusion,"further, we would like to study sampling techniques motivated by natural distributions of linguistic structures."
p18-1143,2018,7 Conclusion,"hence, as a future work, we would like to compare the usefulness of different linguistic theories and different constraints within each theory in our proposed lm framework."
p18-1145,2018,6 Conclusions and Future Work,"because the mismatch between words and extraction units is a common problem in information extraction, we believe our method can also be applied to many other languages and tasks for exploiting inner composition structure during extraction, such as named entity recognition."
p18-1147,2018,8 Conclusions,"first of all, we plan to explore the use of more advanced forms of entity detection and linking, including propagating features from the edl system forward for both unary and binary deep models."
p18-1147,2018,8 Conclusions,"in addition we plan to exploit unary and binary relations as source of evidence to bootstrap a probabilistic reasoning approach, with the goal of leveraging constraints from the kb schema such as domain, range and taxonomies."
p18-1148,2018,5 Conclusion and Future work,"besides, we would like to examine whether the induced latent relations could be helpful for relation extract."
p18-1148,2018,5 Conclusion and Future work,"in future work, we would like to use syntactic and discourse structures (e.g., syntactic dependency paths between mentions) to encourage the models to discover a richer set of relations."
p18-1148,2018,5 Conclusion and Future work,"in this way we also hope it will lead to interesting follow-up work, as individual relations can be informed by injecting prior knowledge (e.g., by training jointly with relation extraction models)."
p18-1148,2018,5 Conclusion and Future work,we also would like to combine ment-norm and relnorm.
p18-1153,2018,5 Conclusion and Future Work,"for future work, we hope to improve the results by using the pun data and design a more proper way to select candidates from associative words."
p18-1154,2018,6 Conclusions,an interesting point to explore is whether such pragmatically trained game state representations can be leveraged for the task of game commentary generation.
p18-1154,2018,6 Conclusions,generating commentary for such multi-moves is a potential direction for future work.
p18-1155,2018,7 Discussion,"given the numerous potential applications of such an oracle, we believe improving its accuracy will be a promising future direction."
p18-1157,2018,7 Conclusion,"further, we also would like to explore san on other tasks, such as text classification and natural language inference for its generalization in the future."
p18-1161,2018,5 Conclusion and Future Work,"in the future, we will explore the following directions: (1) an additional answer re-ranking step can further improve our model."
p18-1161,2018,5 Conclusion and Future Work,"we will explore how to effectively re-rank our extracted answers to further enhance the performance.(2) background knowledge such as factual knowledge, common sense knowledge can effectively help us in paragraph selection and answer extraction."
p18-1161,2018,5 Conclusion and Future Work,we will incorporate external knowledge bases into our ds-qa model to improve its performance.
p18-1162,2018,8 Conclusion and Future Work,"in future work, we will try to incorporate more hand-crafted features in our model."
p18-1163,2018,6 Conclusion,"it is also necessary to further validate our approach on more advanced nmt architectures, such as cnn-based nmt (gehring et al., 2017) and transformer (vaswani et al., 2017)."
p18-1164,2018,6 Conclusion,"additionally, we also intend to apply these methods to other sequence-to-sequence tasks, including natural language conversation."
p18-1164,2018,6 Conclusion,"in future work, we will explore further strategies to bridge the source and target side for sequence-to-sequence and tree-based nmt."
p18-1166,2018,6 Conclusion and Future Work,"in the future, we plan to apply our model on other sequence to sequence learning tasks."
p18-1166,2018,6 Conclusion and Future Work,we will also attempt to improve our model to enhance its modeling ability so as to consistently outperform the original neural transformer.
p18-1168,2018,8 Discussion,in future work we plan to extend this work and automatically learn such a lexicon.
p18-1169,2018,7 Conclusion,"finally, our approach to collecting feedback can also be transferred to other domains."
p18-1169,2018,7 Conclusion,"for example, (yih et al., 2016) designed a user interface to help freebase experts to efficiently create queries."
p18-1170,2018,8 Conclusion,"in future work, we will speed it up through the use of pruning techniques."
p18-1170,2018,8 Conclusion,"in particular, advanced methods for alignments, as in lyu and titov (2018), seem promising."
p18-1170,2018,8 Conclusion,we will also look into more principled methods for splitting the amrs into elementary as-graphs to replace our hand-crafted heuristics.
p18-1171,2018,6 Conclusion,"while we are focused on amr parsing in this paper, in future work our cache transition system and the presented sequence-to-sequence models can be potentially applied to other semantic graph parsing tasks (oepen et al., 2015; du et al., 2015; zhang et al., 2016; cao et al., 2017)."
p18-1172,2018,6 Conclusion,"in future, we will extend our proposed all-sample learning scheme to deep learning methods, which are more expressive than the shallow embedding model."
p18-1172,2018,6 Conclusion,"lastly, we are interested in exploring the recent adversarial learning techniques to enhance the robustness of word embeddings."
p18-1172,2018,6 Conclusion,"moreover, we will integrate prior knowledge, such as the words that are synonyms and antonyms, into the word embedding process."
p18-1176,2018,7 Conclusion,we also believe that other qa models may share these weaknesses.
p18-1177,2018,7 Conclusion,it would also be interesting to apply our approach to incorporating coreference knowledge to other text generation tasks.
p18-1179,2018,7 Conclusion,we extend the work on dag automata in chiang et al.(2018) and propose a general method to build flexible dag transducer.
p18-1184,2018,6 Conclusions and Future Work,"in our future work, we plan to integrate other types of information such as user properties into the structured neural models to further enhance representation learning and detect rumor spreaders at the same time."
p18-1184,2018,6 Conclusions and Future Work,we also plan to use unsupervised models for the task by exploiting structural information.
p18-1185,2018,6 Conclusions and Future Work,we hope this work will encourage more research on multimodal social media in the future and we plan on making our benchmark available upon request.
p18-1185,2018,6 Conclusions and Future Work,we plan to expand our model to tasks such as fine-grained name tagging or entity liking in the future.
p18-1187,2018,6 Conclusion,"in future work, we are interested in modelling the extent to which a social interaction is caused by geographical proximity (e.g.using user鈥搖ser gates)."
p18-1193,2018,9 Discussion,"one possible direction for future work is to use an estimator to predict rewards for all actions, rather than observing them."
p18-1196,2018,7 Conclusion,"our approaches in modelling and evaluation can be used in future work in tasks such as approximate information extraction, knowledge base completion, numerical fact checking, numerical question answering, and fraud detection."
p18-1198,2018,6 Conclusion,"in future work, we would like to extend the probing tasks to other languages (which should be relatively easy, given that they are automatically generated), investigate how multi-task training affects probing task performance and leverage our probing tasks to find more linguistically-aware universal encoders."
p18-1201,2018,8 Conclusions and Future Work,"in the future, we will extend this framework to other information extraction problems."
p18-1203,2018,4 Conclusion,one interesting topic for future research is exploration in planning.
p18-1203,2018,4 Conclusion,"to this end, we want the agent to explore in the environment, but not so much that the performance would be greatly degraded."
p18-1203,2018,4 Conclusion,"we need to deal with the challenge of adapting the world model in a changing environment, as exemplified by the domain extension problem (lipton et al., 2016)."
p18-1204,2018,5 Conclusion and Future Work,the work can be extended to multi-turn conversation generation by including an additional detector predicting when to ask a question.
p18-1205,2018,6 Conclusion & Discussion,we believe persona-chat will be a useful resource for training components of future dialogue systems.
p18-1206,2018,6 Conclusions,"in future work, we plan to incorporate various types of context (e.g.anaphora, device-specific capabilities) and dialogue history into a large-scale nlu system."
p18-1209,2018,6 Conclusion,"future work on similar topics could explore the applications of using low-rank tensors for attention models over tensor representations, as they can be even more memory and computationally intensive."
p18-1211,2018,5 Conclusion,"more generally, similar approaches can explore a wider range of scenarios involving sequences of text."
p18-1213,2018,9 Conclusion,"while our work only use information present in our dataset, we view our dataset as a future testbed for evaluating models trained on any number of resources for learning common sense about emotional reactions and motivations."
p18-1214,2018,5 Conclusion,"in the future, we plan to enrich the architecture of dazer to allow few-shot document filtering by incorporating several labeled examples."
p18-1217,2018,5 Conclusion,"for future work, we are interested in various extensions, including combining stc with autoencoding variational bayes (avb)."
p18-1219,2018,7 Conclusion and Future Work,"in future, we plan to use use approaches, like multi-task learning (mishra et al., 2018), in estimating gaze features and using those estimated features for text quality prediction."
p18-1223,2018,7 Conclusions,"though mainly fouced on search, we hope our findings shed some lights on a potential path towards more intelligent neural systems and will motivate more explorations in this direction."
p18-1225,2018,6 Conclusion,"our rule-based generators can be expanded to cover more patterns and phenomena, and the seq2seq generator extended to incorporate per-example loss for adversarial training."
p18-1225,2018,6 Conclusion,"our seq2seq and knowledge-guided example generators, trained in an end-to-end fashion, can be used to make any base entailment model more robust."
p18-1226,2018,6 Conclusion and Discussions,"lastly, since our method can capture korean syntactic features through jamo and character n-grams, we can apply the same idea to other tasks such as pos tagging and parsing."
p18-1226,2018,6 Conclusion and Discussions,"meanwhile, we will further train our model over noisy data and investigate how it is dealing with noisy words."
p18-1226,2018,6 Conclusion and Discussions,"since korean words are divisible once more into grapheme level, resulting in longer sequence of jamos for a given word, we plan to explore potential applicability of deeper level of subword information in korean."
p18-1226,2018,6 Conclusion and Discussions,"we plan to apply these vectors for various neural network based nlp models, such as conversation modeling."
p18-1227,2018,6 Conclusion and Future Work,"in the future, we will explore methods of exploiting internal information in other languages.(4) we believe that sememes are universal for all human languages."
p18-1227,2018,6 Conclusion and Future Work,"in the future, we will take structured annotations into account.(2) it would be meaningful to take more information into account for blending external and internal information and design more sophisticated methods.(3) besides chinese, many other languages have rich subword-level information."
p18-1227,2018,6 Conclusion and Future Work,we will explore a general framework to recommend and utilize sememes for other nlp tasks.
p18-1227,2018,6 Conclusion and Future Work,"we will explore the following research directions in the future: (1) concepts in hownet are annotated with hierarchical structures of senses and sememes, but those are not considered in this paper."
p18-1228,2018,6 Discussion and Conclusion,our study may facilitate further investigations on context-dependent text analysis techniques and applications.
p18-1228,2018,6 Discussion and Conclusion,we hope that semaxis can facilitate research on other semantic axes so that we will have labeled datasets for other axes as well.
p18-1229,2018,7 Conclusion and Future Work,"in addition, study on how to effectively encode induction history will be interesting."
p18-1229,2018,7 Conclusion and Future Work,"in the future, we will explore more strategies towards term pair selection (e.g., allow the rl agent to remove terms from the taxonomy) and reward function design."
p18-1230,2018,5 Conclusions and Future Work,"in the next step, we will consider integrating the rich structural information into the neural network for word sense disambiguation."
p18-1230,2018,5 Conclusions and Future Work,there is still one challenge left for the future.
p18-1231,2018,8 Conclusion,"in the future, we will extend our model so that it can project multi-word phrases, as well as single words, which could help with negations and modifiers."
p18-1234,2018,8 Conclusions and Future Work,how to leverage large-scale sentiment lexicons in neural networks would be our future work.
p18-1237,2018,5 Discussion and Conclusion,"in future work, we plan to study how to distinguish effective from ineffective discussions based on our model as well as how to learn from the strategies used in successful discussions, in order to predict the best next deliberative move in an ongoing discussion."
p18-1238,2018,6 Conclusions,we hope that the availability of the conceptual captions dataset will foster considerable progress on the automatic image-captioning task.
p18-1243,2018,6 Discussion,"although it might be non-trivial to extend the proposed approach to real natural language directly, we regard this work as an initial step towards this ultimate ambitious goal and our game might shed some light on designing more advanced games or performing real-world data collection."
p18-1243,2018,6 Discussion,we plan to investigate the generalization and application of the proposed approach to more realistic environments with more diverse tasks in future work.
p18-1247,2018,7 Conclusion and Future Work,we believe this framework can be extended to other sequence labeling tasks in nlp such as semantic role labeling.
p18-1249,2018,7 Conclusion,our results suggest that further research into different ways of encoding utterances can lead to additional improvements in both parsing and other natural language processing tasks.
p18-1249,2018,7 Conclusion,"the gains we see come not only from incorporating more information (such as subword features or externally-trained word representations), but also from structuring the architecture to separate different kinds of information from each other."
p18-1251,2018,5 Future Work,"for future work, other potential bottlenecks could be addressed."
p18-1251,2018,5 Future Work,the queue will also require a mechanism to avoid inserting duplicate tuples into the queue.
p18-1252,2018,6 Conclusions and Future Work,"in future, we would like to advance this work in two directions: 1) proposing more effective conversion approaches, especially by exploring the potential of treelstms; 2) constructing bi-tree aligned data for other treebanks and exploiting all available single-tree and bi-tree labeled data for better conversion."
p18-1255,2018,7 Conclusion,"in order to move to a full system that can help users like terry write better posts, there are three interesting lines of future work."
p18-1255,2018,7 Conclusion,"such pragmatic principles have recently been shown to be useful in other tasks as well (golland et al., 2010; smith et al., 2013; orita et al., 2015; andreas and klein, 2016)."
p18-1256,2018,8 Conclusion,"in future work, we would like to focus more on designing models that can deal with and be optimized for scenarios with severe data imbalance."
p18-1256,2018,8 Conclusion,"we would like to also explore various applications of presupposition trigger prediction in language generation applications, as well as additional attention-based neural network architectures."
P19-1001,2019,7 Conclusions and Future Work,"in the future, we plan to integrate our ioi model with models like elmo (peters et al., 2018) and bert (devlin et al., 2018) to study if the performance of ioi can be further improved."
P19-1002,2019,5 Conclusion and Future Work,"in the future, we plan to apply reinforcement learning to further improve the performance."
P19-1003,2019,6 Conclusion,we hope the collected dataset and proposed model can benefit future related research.
P19-1004,2019,5 Conclusion,we also foresee this paradigm being useful when building new dialog datasets to understand the kinds of information models use to solve them.
P19-1006,2019,4 Conclusion and Future Work,"in the future, we would like to explore the effectiveness of various attention methods to solve indefinite choices task with interpretive features."
P19-1007,2019,6 Conclusions and Future Work,"in the future, we want to incorporate this framework with much refined primal and dual models, and design more informative reward signals to make the training more efficient."
P19-1007,2019,6 Conclusions and Future Work,"it would be appealing to apply graph neural networks (chen et al., 2018b, 2019) to model structured logical forms."
P19-1009,2019,8 Conclusion,"for future work, we would like to extend our model to other semantic parsing tasks (oepen et al., 2014; abend and rappoport, 2013)."
P19-1010,2019,6 Conclusions,"for future direction, we are interested in exploring constrained decoding, better incorporating pre-trained language representations within our architecture, conditioning on additional relations between entities, and different gnn formulations."
P19-1011,2019,6 Conclusion,experiments on nearest-neighbor sentence retrieval further validate the effectiveness of proposed framework.
P19-1012,2019,7 Discussion and Conclusion,we leave this question for future work.
P19-1014,2019,8 Conclusions,"in future work, we’d like to improve this integration in order to gain from training on examples from different domains for tags like ‘name’ and ‘location’."
P19-1016,2019,5 Conclusions and Future Work,"our future work includes integrating advanced word representation methods (e.g., elmo and bert) and extending the proposed model to other tasks, such as event extraction and co-reference resolution."
P19-1016,2019,5 Conclusions and Future Work,we also plan to incorporate external knowledge and common sense as additional signals into our architecture as they are important for human readers to recognize names but still absent from the current model.
P19-1017,2019,6 Conclusions and Future Work,"for further works, we will leverage more supervised translation hops to improve the performance of unsupervised translation for distant languages."
P19-1017,2019,6 Conclusions and Future Work,we will extend our method to more distant languages.
P19-1019,2019,6 Conclusions and future work,"finally, we would like to adapt our approach to more relaxed scenarios with multiple languages and/or small parallel corpora."
P19-1019,2019,6 Conclusions and future work,"in addition to that, we would like to incorporate a language modeling loss during nmt training similar to he et al.(2016)."
P19-1019,2019,6 Conclusions and future work,"in the future, we would like to explore learnable similarity functions like the one proposed by (mccallum et al., 2005) to compute the characterlevel scores in our initial phrase-table."
P19-1023,2019,5 Conclusions,"in the future, we plan to explore contextbased similarity to complement the lexical similarity to improve the overall performance."
P19-1024,2019,5 Conclusion,"one natural question we would like to ask is how to make use of the proposed framework to perform improved graph representation learning for graph related tasks (bastings et al., 2017)."
P19-1024,2019,5 Conclusion,there are multiple venues for future work.
P19-1027,2019,4 Limitations and Conclusions,"our evaluation did not include other subword representations, most notably elmo (peters et al., 2018) and contextual string embeddings (akbik et al., 2018), since, even though they are languageagnostic in principle, pretrained models are only available in a few languages.conclusions."
P19-1029,2019,5 Conclusion,"future research directions will involve the development of reinforcement learning model with multi-dimensional rewards, and modeling explicit credit assignment for improving the capabilities of the regulator to make context-sensitive decisions in mini-batch learning."
P19-1029,2019,5 Conclusion,"the proposed framework can naturally be expanded to integrate more feedback modes suitable for the interaction with humans, e.g., pairwise comparisons or output rankings."
P19-1034,2019,6 Conclusion,another interesting topic for future research is the comparison of domain adaptation based on our domain-specific word embeddings vs. based on word embeddings trained on much larger corpora.
P19-1034,2019,6 Conclusion,"in the future, we plan to investigate whether there are changes over time that significantly impact the linguistic characteristics of the data, in the simplest case changes in the meaning of a word."
P19-1035,2019,8 Conclusion,another strand of research will be combining both strategies and deploying the manipulation strategies in a large scale testing platform that allows the system to adapt to an individual learner over time.
P19-1035,2019,8 Conclusion,future manipulation strategies that take the competencies into account have the potential to train particular skills and to better control the competencies required for a placement test.
P19-1035,2019,8 Conclusion,our error analysis points out important directions for future work on detecting ambiguous gaps and modeling gap interdependencies for c-tests deviating from the default generation scheme.
P19-1036,2019,6 Conclusion,this is certainly an avenue that we seek to explore.
P19-1036,2019,6 Conclusion,"we have not explored whether recent advances in word embeddings from instance elmo (peters et al., 2018) and bert (devlin et al., 2018) could add further benefits."
P19-1037,2019,9 Conclusion + Future Work,doctors and patients speak a different language and we hope that our work will help them communicate.
P19-1037,2019,9 Conclusion + Future Work,"finally, we will explore adaptations of our methodology for general (non-medical) domains, e.g., simplified search interfaces (ananiadou et al., 2013) for semantically annotated news (thompson et al., 2017)."
P19-1037,2019,9 Conclusion + Future Work,one clear avenue of future work is to apply this system in a clinical setting and to test the results with actual patients.
P19-1037,2019,9 Conclusion + Future Work,we could also look to use parallel simplified medical text to augment the general language parallel text used in the nts system.
P19-1037,2019,9 Conclusion + Future Work,we will look to develop software that uses nts to identify possible simplifications for a clinician when they are writing a letter for a patient.
P19-1038,2019,8 Conclusion,"even though our work is an application of financial domain, we hope our multimodal learning model can also be useful in other areas (such as social media and customer service) where multimodality data is available."
P19-1039,2019,"6 Conclusions, Limitations, and FutureDirections","lastly, we look forward to further exploring this line of research by investigating: 1. the individual differences in both detecting concealed information and concealing information, by analyzing the features across groups defined by individual personality traits (fornaciari et al., 2013, an et al., 2018), ethnics, native languages, and different dimensions of professional skills; 2. the result and model robustness by collecting and testing other field data such as board games; 3. the predictive power of phonotactic variation features; 4. the relationship between perceived information concealment and concealing information; 5. how soon can we detect concealed information; 6. how to conduct domain adaptation with regards to detecting concealed information; 7. efficient ways to make the multi-task learning framework scalable."
P19-1039,2019,"6 Conclusions, Limitations, and FutureDirections","third, more analyses of acoustics such as pitch and tonal contour, phonotactic variations could be incorporated to further explore the space of information concealment in speech."
P19-1040,2019,5 Conclusions and Future Work,"our framework can be extended to deal with sources that generate a spectrum of perspectives, each with a stance relative to claim and with evidence supporting it."
P19-1040,2019,5 Conclusions and Future Work,we leave this for future work.
P19-1043,2019,7 Conclusions and Future Work,"in the future, we would like to generalize it to multiple domains and datasets."
P19-1044,2019,7 Conclusions and future work,"in the future, we plan to evaluate temporal referencing against the related dynamic embedding models on an annotated empirical lexical change dataset with multiple languages."
P19-1045,2019,5 Conclusion,"in our future work, we would like to improve the model structure and the adversarial learning algorithm."
P19-1045,2019,5 Conclusion,"last but not least, we would like to apply our approach to other heterogeneous texts-concerned nlp tasks."
P19-1046,2019,5 Conclusion,"in future work, we intend to explore multiple local fusion methods within our framework."
P19-1047,2019,6 Conclusions and future work,"promising directions for future research include examining additional features and feature representations: pragmatic features such as formality (pavlick and tetreault, 2016) or politeness (danescu-niculescu-mizil et al., 2013); acousticprosodic features from earnings call audio; more sophisticated semantic representations such as claims (lim et al., 2016), automatically induced entity-relation graphs (bansal et al., 2017) or question-answer motifs (zhang et al., 2017) (these representations are non-trivial to construct because a single turn may contain many questions or answers); or even discourse structures."
P19-1047,2019,6 Conclusions and future work,"the models used in this work aim to be just complex enough to determine whether useful signals exist for this task; future modeling work could include training a complete end-to-end system such as a hierarchical attention network (yang et al., 2016), or building industry-specific models."
P19-1048,2019,5 Conclusion,"the proposed architecture can potentially be applied to similar tasks such as relation extraction, semantic role labeling, etc."
P19-1049,2019,5 Conclusion,it would also be nice to explore about more fine-grained functional components and grammatical entities in the future works.
P19-1050,2019,7 Conclusion,"building upon this dataset, future research can explore the design of efficient multimodal fusion algorithms, novel erc frameworks, as well as the extraction of new features from the audio, visual, and textual modalities."
P19-1050,2019,7 Conclusion,we believe this dataset will also be useful as a training corpus for both conversational emotion recognition and multimodal empathetic response generation.
P19-1052,2019,6 Conclusion,"in order to solve the problem of lacking aspect-level labeled data, we wish to utilize the abundant document-level labeled data."
P19-1053,2019,6 Conclusion and Future Work,these mined information can be further used to refine the model training via a regularization term.
P19-1053,2019,6 Conclusion and Future Work,"thus, we plan to extend our approach to other neural nlp tasks with attention mechanisms, such as neural document classification (yang et al., 2016) and neural machine translation (zhang et al., 2018)."
P19-1054,2019,6 Conclusion,future work should thus study the overlapping nature of argument clustering.
P19-1058,2019,6 Conclusion,"in the future work, we will focus on how to mine different representations for different discourse relation types and apply the topic information to other languages."
P19-1059,2019,5 Conclusion,future work should provide further exploration of the data regime in which pragmatic learning is most beneficial and its correspondence to realworld language use.
P19-1060,2019,6 Conclusion,"as part of future work, we would like to investigate the use of contextualized embeddings (e.g., bert, devlin et al.(2018)) for coherence assessment – as such representations have been shown to carry syntactic information of words (tenney et al., 2019) – and whether they allow multi-task learning frameworks to learn complementary aspects of language."
P19-1061,2019,6 Conclusions and Future Work,"in future work, we will enrich our weak supervision system by giving the lfs access to more sophisticated contexts that take into account global structuring constraints in order to see how they compare to exogenous decoding constraints applied in (muller et al., 2012; perret et al., 2016)."
P19-1064,2019,5 Conclusion,another interesting direction would be introducing intermediate step rewards for each action to better guide the behaviour of the rl agent.
P19-1064,2019,5 Conclusion,"there are several potential improvements to our model as future work, such as incorporating mention detection result as a part of the reward."
P19-1065,2019,6 Conclusion and Future Work,"since implicit discourse relation identification is a key task for dialogue systems, there are still many approaches worth investigating in future work."
P19-1066,2019,8 Conclusion,in the future we plan to further enrich these representations by considering information from across the document.
P19-1068,2019,7 Conclusion,"in future work, we aim to determine if the same level of accuracy can be obtained when single sentences will be used as samples for training and testing."
P19-1068,2019,7 Conclusion,we leave these ideas for future exploration.
P19-1069,2019,6 Conclusions,"as future work we plan to exploit a subset of the wikipedia categories as coarse-grained sense inventory and enrich our dataset with coarser labels, hence enabling wsd at different granularities."
P19-1070,2019,5 Conclusion,we hope that this study will encourage future work on cle evaluation and analysis and help guide the development of new cle models.
P19-1077,2019,9 Discussion and Conclusions,the integration takes place by using the automatic mention detectors as ‘players’ in the game.
P19-1078,2019,7 Conclusion,"in future work, transferring knowledge from other resources can be applied to further improve zero-shot performance, and collecting a dataset with a large number of domains is able to facilitate the application and study of metalearning techniques within multi-domain dst."
P19-1079,2019,7 Conclusions,we further explored learning these features in parallel and serial mtl architectures.
P19-1084,2019,8 Conclusion,"our methodology can be extended to handle biases in other tasks where one is concerned with finding relationships between two objects, such as visual question answering, story cloze completion, and reading comprehension."
P19-1084,2019,8 Conclusion,we hope to encourage such investigation in the broader community.
P19-1085,2019,6 Conclusion,"in the future, we would like to design a multi-step evidence extractor and incorporate external knowledge into our framework."
P19-1086,2019,8 Conclusion,we hope that sherliic will foster better modeling of lexical inference in context as well as progress in nli in general.
P19-1087,2019,7 Conclusions,we plan to investigate the impact of combining the two models.
P19-1088,2019,8 Conclusions,"in the future, we plan to build upon the acquired knowledge and the developed classifiers to generate systems able to provide actionable feedback on how to achieve high-quality counseling."
P19-1088,2019,8 Conclusions,we presented an extensive analysis of linguistic aspects of the collaboration process during counseling conversations in relation to counseling quality.
P19-1089,2019,7 Discussion and Future Work,future work could adapt our framework to examine more complex forms of conversational development.
P19-1089,2019,7 Discussion and Future Work,"future work could more directly model how counselors respond to texter behaviors, hence gauging the extent to which counselors evolve in their interactional practices."
P19-1089,2019,7 Discussion and Future Work,"other approaches, such as qualitative labeling by domain experts, could examine whether such changes in language use also result in better conversations."
P19-1089,2019,7 Discussion and Future Work,"our methodology could also be extended to examine other conversational contexts such as academic advising or business interactions, where individuals are expected to learn from experience."
P19-1090,2019,6 Conclusion and Future Work,"a next step would be comprehensive error analysis for a better understanding of where the models succeed or fail in capturing semantic information, particularly for the low-resource languages."
P19-1090,2019,6 Conclusion and Future Work,we are also working to include into our models the long tail in the distribution of template answers.
P19-1090,2019,6 Conclusion and Future Work,"we further intend to explore transfer learning techniques (zhang et al., 2017) as well as deep architectures designed specifically for answer selection (lai et al., 2018)."
P19-1091,2019,5 Conclusion,"for future work, we plan to investigate the model on other related tasks such as relation extraction, normalization as well as the use of advanced conditional models."
P19-1092,2019,5 Conclusion,we hope this work will encourage research on designing more powerful qa systems that can carry out effective information extraction and reasoning.
P19-1093,2019,7 Discussion and future work,"a possibility that we did not expand on in this paper is to pre-train one leg of the network on an argument detection data set, like the one of shnarch et al.(2018)."
P19-1093,2019,7 Discussion and future work,"in addition, more careful design of the architecture details, which was not the focus of this work, will probably yield better results yet, e.g., contextualized word embeddings (peters et al., 2018), batch normalization (ioffe and szegedy, 2015; cooijmans et al., 2017), deeper networks and other architecture practical heuristics."
P19-1093,2019,7 Discussion and future work,"in the future we aim to test and adapt other improvements in the learning to rank field to our task, hoping for further improvement by those models (burges, 2010; severyn and moschitti, 2015)."
P19-1094,2019,8 Conclusion,we plan to pursue these research directions in future work.
P19-1096,2019,6 Conclusions and Future Work,"in the future work, we will try to build a one-step model that directly extract the emotion-cause pairs in an end-to-end fashion."
P19-1100,2019,5 Conclusion,our detailed observations can provide more hints for the follow-up researchers to design more powerful learning frameworks.
P19-1101,2019,4 Conclusion and Future Work,"by aggregating over different people but in one domain, one can uncover a domain-specific k. similarly, by aggregating over many topics for one person, one would find a personalized k. these consistute promising research directions for future works."
P19-1101,2019,4 Conclusion and Future Work,"for the background knowledge k, a promising direction would be to use the framework to actually learn it from data."
P19-1101,2019,4 Conclusion and Future Work,"then, interesting research questions arise like which granularity offers a good approximation of semantic units?"
P19-1102,2019,8 Conclusion,in the future we plan to explore interactions among documents beyond concatenation and experiment with summarizing longer input documents.
P19-1103,2019,6 Conclusion,"in the future, we would like to evaluate the attacking effectiveness and efficiency of our methods on more datasets and models, and do elaborate human evaluation on the similarity between clean texts and the corresponding adversarial examples."
P19-1104,2019,6 Conclusion,"our study opens up interesting avenues for future research: obfuscation by addition instead of by reduction, development of more powerful, targeted paraphrasing operators, and, theoretical analysis of the search space properties."
P19-1105,2019,5 Conclusion,"future work will include: (i) incorporating lexical semantics such as named entities for further improvement, (ii) comparing our model to other deep contextualized word representation such as elmo and bert, and (iii) applying the method to other domains for quantitative evaluation."
P19-1106,2019,7 Conclusion,we aim to include review reliability prediction in the pipeline of our future work.
P19-1106,2019,7 Conclusion,we intend to work upon those and also explore more sophisticated techniques for sentiment polarity encoding.
P19-1106,2019,7 Conclusion,"with further exploration, we aim to mould the ongoing research to an efficient ai-enabled system that would assist the journal editors or conference chairs in making informed decisions."
P19-1108,2019,8 Conclusions,"other ways of combining the two modules, more sophisticated classifiers for both phm detection and figurative usage detection, are possible directions of future work."
P19-1109,2019,5 Conclusions,"finally, we plan to investigate alternative methods to modelling phrase and multi-word expression complexity."
P19-1109,2019,5 Conclusions,our future research will focus on the relative nature of complexity judgements and will use the seq model to predict complexity on a scale.
P19-1109,2019,5 Conclusions,we will also investigate whether the seq model may benefit from sources of information other than word embeddings and character-level morphology.
P19-1112,2019,7 Conclusion,"as future work, we plan to investigate emphasis selection on a larger and more diverse dataset."
P19-1112,2019,7 Conclusion,"we also plan to investigate the role of word sentiment and emotion intensity as well as more advanced language models such as bert (devlin et al., 2018) in modeling emphasis."
P19-1114,2019,6 Conclusions and Future Work,"after collecting more labeled data, and tuning our model using anomaly detection techniques like isolation forests (liu et al., 2008), we hope to expand this study to the stage where we are able to use unbalanced data sets."
P19-1114,2019,6 Conclusions and Future Work,"given that, in our future work, we will be investigating those false positive cases with our collaborators to assess what the correct label for these ads should be."
P19-1114,2019,6 Conclusions and Future Work,"moreover, since the proposed full feature set involves hundreds of features we plan to increase our sample size to have a better estimation of the performance of our final predictor."
P19-1115,2019,7 Conclusion,promising future work includes extension to tree-structured inputs and application to other tasks.
P19-1117,2019,7 Conclusion,we first introduce a representor for replacing both encoder and decoder so as to fully explore the commonality among languages.
P19-1120,2019,7 Conclusion,"5 as for future work, we will test our methods in the nmt transfer where the target language is switched."
P19-1120,2019,7 Conclusion,"we also plan to compare different algorithms for learning the cross-lingual mapping (artetxe et al., 2018a; xu et al., 2018; joulin et al., 2018) to optimize the transfer performance."
P19-1122,2019,7 Conclusions & Future Work,"finally, we hope that future work in this area will follow our lead in using carefully-controlled experiments to enable meaningful comparisons."
P19-1122,2019,7 Conclusions & Future Work,"while our method is currently restricted to languages that have reliable constituency parsers, an exciting future direction is to explore unsupervised tree induction methods for low-resource target languages (drozdov et al., 2019)."
P19-1123,2019,6 Conclusion,"in future, we would like to extend the method to handle more than two curricula objectives."
P19-1124,2019,6 Conclusions and Future Work,"in the future, we believe more work on improving cfs alignment is potential to improve translation quality, and we will investigate on using source context and target history context in a more robust manner for better predicting cfs and cft words."
P19-1125,2019,7 Conclusion,"as a future work, we can try to improve the performance of the nmt by introducing more powerful demonstrator with different structure (e.g.right to left)."
P19-1128,2019,6 Conclusion and Future Work,"our model can also be considered as a more generic framework for graph generation problem with unstructured input other than text, e.g.image, video, audio."
P19-1132,2019,5 Conclusion,"in the future work, we will explore the usage of this method with other applications."
P19-1132,2019,5 Conclusion,"our idea of encoding a passage regarding multiple entities has potentially broader applications beyond relation extraction, e.g., entity-centric passage encoding in question answering (song et al., 2018a)."
P19-1133,2019,5 Conclusion,"future work will investigate more complex and recent neural network models such as devlin et al.(2018), as well as alternative losses."
P19-1134,2019,7 Conclusion,"in future work, we want to further investigate the extent of syntactic structure captured in deep language language representations."
P19-1135,2019,6 Conclusion,"in the future, we hope to improve our work by the utilization of better model-based pattern extractor, and resorting to latent variable model (kim et al., 2018) for jointly modeling instance selector."
P19-1135,2019,6 Conclusion,"what is more, we also hope to verify the effectiveness of our method on more tasks, including open information extraction and event extraction, and also overlapping relation extraction models (dai et al., 2019)."
P19-1137,2019,5 Conclusion and Future Work,"for the future work, we plan to extend diagnre to other ds-based applications, such as question answering (lin et al., 2018), event extraction (chen et al., 2017), etc."
P19-1139,2019,5 Conclusion,"there are three important directions remain for future research: (1) inject knowledge into feature-based pre-training models such as elmo (peters et al., 2018); (2) introduce diverse structured knowledge into language representation models such as conceptnet (speer and havasi, 2012) which is different from the world knowledge database wikidata; (3) annotate more real-world corpora heuristically for building larger pre-training data."
P19-1140,2019,7 Conclusions,"in future, we are interested in introducing text information of entities for alignment by considering word ambiguity (cao et al., 2017b); and meanwhile, through cross-kg entity proximity (cao et al., 2015)."
P19-1141,2019,4 Conclusion and Future Work,"although we specifically investigated the ner task for chinese in this work, we believe the proposed model can be extended and applied to other languages, for which we leave as future work."
P19-1142,2019,8 Conclusion,"also, it will be worthwhile to ascertain the efficacy of pdr for language models using transformers and in combination with frage embeddings."
P19-1142,2019,8 Conclusion,future work includes exploring the application of pdr to other seq2seq models that have a similar input-output symmetry.
P19-1144,2019,7 Conclusion,"in future work, we aim to capture better structural information and possible connections to unsupervised grammar induction."
P19-1146,2019,6 Conclusion and Future Work,"a natural next step is to apply entmax to self-attention (vaswani et al.,2017)."
P19-1147,2019,9 Conclusions,future work includes developing a adversarial training scheme as well as devising a more robust architecture based on our findings.
P19-1149,2019,7 Conclusion and Future Work,"in the future, we are interested in testing lowlevel optimizations of lrn, which are orthogonal to this work, such as dedicated cudnn kernels."
P19-1150,2019,5 Conclusion,"in the future, we plan to apply capsule networks to even more challenging nlp problems such as language modeling and text generation."
P19-1150,2019,5 Conclusion,making computers perform more like humans is a major issue in nlp and machine learning.
P19-1153,2019,4 Conclusion & Future Work,"continuing in the direction of training our model on different nlp tasks, we would like our representations to generalize well on downstream tasks while maintaining their reconstruction property."
P19-1153,2019,4 Conclusion & Future Work,"finally, we would like to learn our sentence embeddings’ latent space, similarly to subramanian et al.(2018)’s method, so as to leverage our autoencoder’s strong reconstruction ability and generate very long sequences of text."
P19-1153,2019,4 Conclusion & Future Work,we would also like to further explore the usage of sub-sentence representations in natural language processing.
P19-1159,2019,5 Conclusion and Future Directions,"a few interdisciplinary studies (herbelot et al., 2012; avin et al., 2015; fu et al., 2016; schluter, 2018) have emerged, and we urge more interdisciplinary discussions in terms of gender bias."
P19-1159,2019,5 Conclusion and Future Directions,"approaches from other technical fields may improve current debiasing methods in nlp or inspire the development of new, more effective methods even if the properties of the data or problem are different across fields."
P19-1159,2019,5 Conclusion and Future Directions,"as mentioned in section 1, gender bias is not a problem that is unique to nlp; other fields in computer science such as data mining, machine learning, and security also study gender bias (calders and verwer, 2010; feldman et al., 2015; hardt et al., 2016; misra et al., 2016; kleinberg et al., 2016; pleiss et al., 2017; beutel et al., 2017; kilbertus et al., 2017)."
P19-1159,2019,5 Conclusion and Future Directions,future work can look to apply existing methods or devise new techniques towards mitigating gender bias in other languages as well.
P19-1159,2019,5 Conclusion and Future Directions,"non-binary genders (richards et al., 2016) as well as racial biases have largely been ignored in nlp and should be considered in future work."
P19-1159,2019,5 Conclusion and Future Directions,"to completely debias effectively, it is important to understand how machine learning methods encode biases and how humans perceive biases."
P19-1159,2019,5 Conclusion and Future Directions,"to perform gender-swapping in such languages, besides swapping those gendered nouns, we also need to change the modifiers.non-binary gender bias."
P19-1160,2019,5 Conclusion,"in future, we plan to extend the proposed method to debias other types of demographic biases such as ethnic, age or religious biases."
P19-1161,2019,7 Conclusion,"finally, we also identified avenues for future work, such as the inclusion of co-reference information."
P19-1165,2019,6 Conclusions,trying more complex networks is also within our scope and is left as future work.
P19-1167,2019,5 Conclusion and Limitations,"in future work, we intend to conduct a diachronic analysis in english using the same corpus, in addition to a cross-linguistic study of gendered language."
P19-1168,2019,5 Conclusion,it may be of interest to extend these techniques to embed knowledge graph elements.
P19-1169,2019,5 Conclusion and Future Work,"additionally, the mapping technique used for relation-aware semantic projection can be further improved to model different linguistic properties of lexical relations (e.g., the “one-to-many” mappings for meronymy)."
P19-1169,2019,5 Conclusion and Future Work,"in the future, we will improve our model to deal with datasets containing a relatively large number of lexical relation types and random term pairs."
P19-1170,2019,5 Conclusion,"finally, the preliminary results we have shown on aligning more than two languages at the same time provide an exciting path for future research."
P19-1171,2019,6 Conclusion,"an avenue for future work is connecting our discovered phonesthemes to putative meanings, as done by abramova et al.(2013) and abramova and fernandez ′ (2016)."
P19-1173,2019,8 Conclusions and Future Work,"future work includes joint and cross-dialectal lemmatization models, in addition to further extension to other dialects."
P19-1174,2019,7 Conclusion and Future Work,"in future work, we will further explore the effectiveness of the reordering mechanism and apply it to other natural language processing tasks, such dependency parsing (zhang et al., 2016; li et al., 2018), and semantic role labeling (he et al., 2018; li et al., 2019)."
P19-1175,2019,7 Conclusion,"in a next step, we plan to compare the performance of nfr to other approaches to tm-nmt integration, for example by carrying out evaluations on the jrc-acquis corpus (gu et al., 2018; koehn and senellart, 2010a; zhang et al., 2018)."
P19-1175,2019,7 Conclusion,"we also intend to carry out further tests to potentially improve the quality of the output, for example by testing different match metrics and retrieval methods, nmt architectures (e.g.transformer), ways to include alignment information and by applying additional morphological preprocessing."
P19-1178,2019,5 Conclusions and Future Work,"as future work, we will apply our methodology to domain adaptation."
P19-1178,2019,5 Conclusions and Future Work,"in the same vain as unsupervised mt, we want to continue our research by using back translation for rejected pairs and dealing with phrases instead of full sentences."
P19-1178,2019,5 Conclusions and Future Work,our architecture is also useful for data selection in data rich language pairs and we will perform experiments on cleaning noisy parallel corpora.
P19-1178,2019,5 Conclusions and Future Work,that will allow us to extract more parallel text from a corpus and facilitate using these approaches for low-resourced languages.
P19-1179,2019,6 Conclusion,"our method does not introduce additional parameters: we hope to motivate future work on learning speech representations, with continued performance on lowerresource settings if additional parameters are introduced."
P19-1180,2019,5 Discussion,"finally, it may be possible to extend our approach to other linguistic tasks such as dependency parsing (christie et al., 2016b), coreference resolution (kottur et al., 2018), and learning pragmatics beyond semantics (andreas and klein, 2016)."
P19-1180,2019,5 Discussion,"its applicability could be extended by learning shared representations across multiple modalities (castrejon et al., 2016) or integrating with pure text-domain models (such as prpn, shen et al., 2018a)."
P19-1180,2019,5 Discussion,"its performance may be boosted by considering structured representations of both images (e.g., lu et al., 2016; wu et al., 2019) and texts (steedman, 2000)."
P19-1180,2019,5 Discussion,the results suggest multiple future research directions.
P19-1181,2019,6 Conclusion,"for example, future extensions of vln will likely involve games (baldridge et al., 2018) where the instructions being given take the agent around a trap or help it avoid opponents."
P19-1182,2019,6 Conclusion,"for future work, we are going to further explore the possibility to merge the three datasets by either learning a joint image representation or by transferring domain-specific knowledge."
P19-1184,2019,8 Conclusion,"in future work, the data can be used to further investigate common ground and conceptual pacts; be extended through manual annotations for a more thorough linguistic analysis of co-reference chains; exploit the combination of vision and language to develop computational models for referring expression generation; or use the photobook task in the parlai framework for turing-test-like evaluation of dialogue agents."
P19-1187,2019,6 Conclusions,in the future work we would like to consider setups where human-annotated data is combined with naturally occurring one (i.e.distantly-supervised one).
P19-1187,2019,6 Conclusions,it would also be interesting to see if mistakes made by fully-supervised systems differ from the ones made by our system and other wikipediabased linkers.
P19-1190,2019,6 Conclusion,"as future work, we will consider integrating more kinds of syntactic features from linguistic analysis such as dependency parsing."
P19-1191,2019,5 Conclusions and Future Work,"in the future, we plan to develop techniques for extracting entities of more fine-grained entity types, and extend paperrobot to write related work, predict authors, their affiliations and publication venues."
P19-1191,2019,5 Conclusions and Future Work,"we build a paperrobot who can predict related entities for an input title and write some key elements of a new paper (abstract, conclusion and future work) and predict a new title."
P19-1192,2019,5 Conclusion and Future work,"in the future, we will investigate the possibility of incorporating additional forms of rhetoric, such as parallelism and exaggeration, to further enhance the model and generate more diverse poems."
P19-1193,2019,6 Conclusion,the proposed model integrates commonsense from the external knowledge base into the generator through a dynamic memory mechanism to enrich the source information.
P19-1196,2019,5 Conclusion and Future Work,"aside from such syntax templates, in the future, we aim to explore how semantic templates contribute to type description generation."
P19-1198,2019,7 Conclusion,"in future, we would like to improve the system further by incorporating better architectural designs and training schemes to tackle complex simplification operations."
P19-1201,2019,6 Conclusion,"conala (yin et al., 2018a) and staqc (yao et al., 2018)."
P19-1201,2019,6 Conclusion,"in the future, we will further apply dim to learn semantic parser and nl generator from the noisy datasets."
P19-1201,2019,6 Conclusion,we further extend supervised dim to semi-supervised scenario (semidim).
P19-1203,2019,6 Conclusion,"besides, it would also be interesting to consider using linguistic knowledge such as named entities or part-of-speech tags to improve the coherence of the conversation."
P19-1203,2019,6 Conclusion,"in the future, we would like to explore how to better select the rationale for each question."
P19-1204,2019,5 Conclusion,"in the future, we plan to study the effect of other video modalities on the alignment algorithm."
P19-1206,2019,6 Conclusion,"in future work, we will make comparisons with those of a humanannotated dataset."
P19-1206,2019,6 Conclusion,"our model can also be applied to other applications, such as argument mining, because arguments typically have the same discourse structure as reviews."
P19-1208,2019,7 Conclusion and Future Work,"another interesting direction is to apply our rl approach on the microblog hashtag annotation problem (wang et al., 2019; gong and zhang, 2016; zhang et al., 2018b)."
P19-1208,2019,7 Conclusion and Future Work,"one potential future direction is to investigate the performance of other encoder-decoder architectures on keyphrase generation such as transformer (vaswani et al., 2017) with multi-head attention module (li et al., 2018; zhang et al., 2018a)."
P19-1210,2019,5 Conclusions and Future Work,"in the future, we plan to further integrate higher level participant interactions, such as gestures, face expressions, etc."
P19-1210,2019,5 Conclusions and Future Work,"we also plan to construct a larger multimedia meeting summarization corpus to cover more diverse scenarios, building on our previous work (bhattacharya et al., 2019)."
P19-1211,2019,6 Summary,"in the future, we would like to understand the usefulness of artificial titles for training the decoder relative to other factors that may impact performance, e.g., how similar the true titles or summaries are in the different domains."
P19-1212,2019,6 Conclusion,bigpatent can enable future research to build robust systems that generate abstractive and coherent summaries.
P19-1215,2019,6 Conclusion,"in future work, we intend to examine multitask approaches combining question summarization and question understanding."
P19-1216,2019,6 Conclusion,"in the future, we will consider extending the current approach to the single document or multiple document summarization."
P19-1217,2019,5 Conclusions and Future Works,"in the future, we will expand it to support the questions on text span selection by using the relation type rather than the option as the terminated condition."
P19-1219,2019,7 Conclusion,"in the future, we plan to use some larger knowledge bases, such as conceptnet and freebase, to improve the quality and scope of the general knowledge."
P19-1220,2019,7 Conclusion,our future work will involve exploring the potential of our multi-style learning towards natural language understanding.
P19-1221,2019,6 Conclusion,"future work will concentrate on designing a fast neural pruner to replace the ir-based pruning component, developing better end-to-end training strategies, and adapting our approach to other datasets such as natural questions (kwiatkowski et al., 2019)."
P19-1222,2019,6 Concluding Remarks,an interesting improvement to our approach would be to allow the retriever to automatically determine whether or not more retrieval iterations are needed.
P19-1222,2019,6 Concluding Remarks,we hope to tackle this problem in future work to allow learning more than two retrieval iterations.
P19-1226,2019,6 Conclusion,"this work demonstrates the feasibility of further enhancing advanced lms with knowledge from kbs, which indicates a potential direction for future research."
P19-1227,2019,7 Conclusion,we hope our work could contribute to the development of cross-lingual openqa systems and further promote the research of overall cross-lingual language understanding.
P19-1228,2019,7 Conclusion,learning deep generative models which exhibit such conditional markov properties is an interesting direction for future work.
P19-1233,2019,7 Conclusion,"in the future, we would like to extend gcdt to other analogous sequence labeling tasks and explore its effectiveness on other languages."
P19-1234,2019,7 Conclusion,"this work proposes a neural pcfg inducer which employs context embeddings (peters et al., 2018) in a normalizing flow model (dinh et al., 2015) to extend pcfg induction to use semantic and morphological information."
P19-1238,2019,6 Conclusion,the results of this initial attempt are optimistic.
P19-1239,2019,7 Conclusion and Future Work,"in future work, we will incorporate other modality such as audio into the sarcasm detection task and we will also investigate to make use of common sense knowledge in our model."
P19-1240,2019,6 Conclusion and Future Work,"also, our topic-aware neural keyphrase generation model can be investigated in a broader range of text generation tasks."
P19-1240,2019,6 Conclusion and Future Work,"in the future, we will explore how to explicitly leverage the topic-word distribution to further improve the performance."
P19-1241,2019,8 Conclusion and Future Work,our future agenda includes exploring the applicability of our analysis and system for identifying patterns and potential prevention.
P19-1241,2019,8 Conclusion and Future Work,we also plan to use this model to solve other downstream medium-specific tasks pertaining to mental health and welfare.
P19-1242,2019,8 Conclusion,"although we focused on english hashtags, our pairwise ranking approach is language-independent and we intend to extend our toolkit to languages other than english as future work."
P19-1243,2019,7 Conclusions and Future Work,"we leave alternative solutions for future work, including training embeddings from scratch or fine-tuning on the target corpus (however, these ideas are only feasible with a large target corpus, and the need for fine-tuning reduces the usefulness of pre-trained embeddings)."
P19-1244,2019,6 Conclusions and Future Work,"for the future work, beyond what we have mentioned, we plan to examine our model on different information sources."
P19-1244,2019,6 Conclusions and Future Work,"we will also try to incorporate relevant metadata into it, e.g., author profile, website credibility, etc."
P19-1246,2019,9 Conclusion,"first, we wish to investigate the role of additional variables (such as age and gender)."
P19-1246,2019,9 Conclusion,"nevertheless, there are limitations of distant labelling and social media data — with issues related specifically to the language of food (askalidis and malthouse, 2016) — that we will take into account in future work."
P19-1247,2019,7 Conclusion,"we intend to study fine-grained political perspectives, capturing how different events are framed."
P19-1248,2019,6 Conclusions and Future Work,"for example, revising spoiler contents in a ‘non-spoiler’ way would be an interesting language generation task."
P19-1248,2019,6 Conclusions and Future Work,"our new dataset, analysis of spoiler language, and positive results facilitate several directions for future work."
P19-1249,2019,5 Conclusion,"in future work, we plan on improving the corpus by incorporating verified accounts from other social networks, and, by inferring new labels for as of yet unlabeled celebrities through link prediction."
P19-1250,2019,5 Conclusion,our future work will include efficiently labeling promising comments via active learning.
P19-1252,2019,5 Conclusion and Future Work,"directions of future research include adaptation of our methods to a large scale, sparsely connected social network."
P19-1252,2019,5 Conclusion and Future Work,"one might also want to investigate the inductive settings of gcn (hamilton et al., 2017) to predict demographic information of a user from outside the black network."
P19-1253,2019,7 Conclusion and Future Work,we also plan to adapt daml to multi-domain dialog tasks.
P19-1256,2019,4 Discussion,"we are still on the way to find out solutions for cases with huge noise (perezbeltrachini and lapata, 2018; wiseman et al., 2017), where heavy manual intervention or external knowledge should be desperately needed."
P19-1258,2019,4 Conclusion,our future work will focus on how to transfer the long-term memory across different tasks.
P19-1259,2019,6 Discussion and Conclusion,"finally, we believe that our framework can generalize to other cognitive tasks, such as conversational ai and sequential recommendation."
P19-1259,2019,6 Discussion and Conclusion,"moreover, we expect that prospective architectures combining attention and recurrent mechanisms will largely improve the capacity of system 1 by optimizing the interaction between systems."
P19-1259,2019,6 Discussion and Conclusion,multiple future research directions may be envisioned.
P19-1260,2019,5 Conclusion,"in the future, we would like to investigate explainable gnn for this task, such as explicit reasoning path in (kundu et al., 2018), and work on other data sets such as hotpotqa."
P19-1265,2019,6 Conclusion,"to create them on the fly, annotation aggregation methods for sequence labelling (simpson and gurevych, 2018) can be used."
P19-1268,2019,6 Conclusion,we also recommend publishing prediction files along with models to facilitate error analysis.
P19-1271,2019,5 Conclusion,"as future work, we intend to continue collecting more data for islam and to include other hate targets such as migrants or lgbt+, in order to put the dataset at the service of other organizations and further research."
P19-1271,2019,5 Conclusion,"moreover, as a future direction, we want to utilize conan dataset to develop a counter-narrative generation tool that can support ngos in fighting hate speech online, considering counter-narrative type as an input feature."
P19-1272,2019,8 Conclusions,"future work will look deeper into using the similarity between the content of the text and image (leong and mihalcea, 2011), as the text task results showed room for improvements."
P19-1272,2019,8 Conclusions,"we envision that our data, task and classifiers will be useful as a preprocessing step in collecting data for training large scale models for image captioning (feng and lapata, 2010) or tagging (mahajan et al., 2018) or for improving recommendations (chen et al., 2016) by filtering out tweets where the text and image have no semantic overlap or can enable new tasks such as identifying tweets that contain creative descriptions for images."
P19-1274,2019,7 Conclusions,"future work could study other types of accounts with similar posting behaviors such as organizational accounts, explore other sources for ground truth tweet identity information (robinson, 2016) or study the effects of user traits such as gender or political affiliation in tweeting signed content."
P19-1275,2019,6 Conclusion,we suggest a future research direction in sarcasm detection where the two types of sarcasm are treated as separate phenomena and socio-cultural differences are taken into account.
P19-1277,2019,6 Conclusions,studying few-shot relation classification with data generated by distant supervision and extending our mlman model to zero-shot learning will be the tasks of our future work.
P19-1278,2019,11 Conclusion and Future Work,"we note that there are a wide range of future directions: (1) human prior knowledge could be incorporated into the similarity quantification; (2) similarity between relations could also be considered in multi-modal settings, e.g., extracting relations from images, videos, or even from audios; (3) by analyzing the distributions corresponding to different relations, one can also find some “meta-relations” between relations, such as hypernymy and hyponymy."
P19-1279,2019,6 Conclusion and Future Work,"in future work, we plan to work on relation discovery by clustering relation statements that have similar representations according to bertem+mtb."
P19-1279,2019,6 Conclusion and Future Work,we will also study representations of relations and entities that can be used to store relation triples in a distributed knowledge base.
P19-1281,2019,6 Conclusions,"future research directions include making selection routines more robust to evaluation outliers, relaxing our gaussian assumptions and developing more effective batch strategies."
P19-1282,2019,7 Related and Future Work,"finally, another direction for future work would be to extend the importance-ranking comparisons that we deploy here for evaluation purposes into a method for deriving better, more informative rankings, which in turn could be useful for the development of new, more interpretable models."
P19-1282,2019,7 Related and Future Work,"yet another line of work focuses on aligning models with human feedback for what is interpretable (fyshe et al., 2015; subramanian et al., 2017), which could refine our idea of what defines a highquality explanation derived from attention."
P19-1283,2019,6 Conclusion,"we apply the same techniques to english sentence embeddings, and show where and to what extent each representation encodes syntactic information."
P19-1283,2019,6 Conclusion,we plan to explore these options in future work.
P19-1284,2019,8 Conclusions,we leave further explorations for future work.
P19-1287,2019,6 Conclusion and Future Work,"in the future, we would like to investigate the feasibility of our methods on non-recurrent nmt models such as transformer (vaswani et al., 2017)."
P19-1287,2019,6 Conclusion and Future Work,"moreover, we are also interested in incorporating discourse-level relations into our models."
P19-1288,2019,6 Conclusion,"in the future, we plan to investigate better methods to leverage the sequential information."
P19-1288,2019,6 Conclusion,we believe that the following two directions are worth study.
P19-1289,2019,8 Conclusions,"we leave many open questions to future work, e.g., adaptive policy using a single model (zheng et al., 2019)."
P19-1290,2019,6 Conclusion,we train our model by designing an rl algorithm with the reward shaping strategy.
P19-1292,2019,5 Conclusion and Future Work,"in future work, we would like to do an extensive analysis on the capabilities of bert and transfer learning in general for different domains and language pairs in ape."
P19-1293,2019,5 Conclusion,we use this system to translate test texts from 14 languages into english.
P19-1296,2019,5 Conclusion,"in future work, we intend to apply these methods to other natural language tasks."
P19-1297,2019,7 Conclusion,"in future, we would like to explore other languages with diverse linguistic characteristics."
P19-1299,2019,5 Conclusion,"for future work, we plan to apply man-moe to more challenging languages for tasks such as syntactic parsing, where multilingual data exists (nivre et al., 2017)."
P19-1299,2019,5 Conclusion,"furthermore, we would like to experiment with multilingual contextualized embeddings such as the multilingual bert (devlin et al., 2018)."
P19-1300,2019,6 Conclusion,it would be also interesting to investigate how our approach compares to the baselines given a large amount of data such as wikipedia.
P19-1300,2019,6 Conclusion,our future work is to exploit character and subword information in our model and see how those information affect the performance in each language pair.
P19-1304,2019,5 Conclusions,"in the future, we will explore more applications of the proposed idea of attentive graph matching."
P19-1307,2019,6 Conclusion,"in the future, we will investigate whether our method helps other downstream tasks."
P19-1312,2019,4 Conclusion and future work,"as a future work, we would like to study, for training bwe, the impact of the use of synthetic parallel data generated by unsupervised nmt, or of a different nature, such as translation pairs extracted from monolingual corpora without supervision."
P19-1312,2019,4 Conclusion and future work,"since our approach works on top of unsupervised mapping for bwe and uses synthetic data generated by unsupervised mt, it will directly benefit from any future advances in these two types of techniques."
P19-1316,2019,6 Conclusion,4 two directions for future work are (i) to extend our approach to other languages by using multilingual resources or translation data; and (ii) to explore various compositionality functions to combine the words’ representation on the basis of their grammatical function within a phrase.
P19-1317,2019,5 Conclusion,"alternatively, sequence encoding models such as gru, cnn, transformer, or even encoders with contextualized word embeddings like bert (devlin et al., 2018), or elmo (peters et al., 2018) can be used to replace this bilstm, however, with additional computation cost."
P19-1318,2019,7 Conclusions,"for future work, we would be interested in further exploring the behavior of neural architectures for nlp tasks which intuitively would benefit from having access to relational information, e.g., text classification (espinosa anke and schockaert, 2018; camacho-collados et al., 2019) and other language understanding tasks such as natural language inference or reading comprehension, in the line of joshi et al.(2019)."
P19-1322,2019,5 Conclusion,it is an interesting future work and will make learning word embeddings more like human learning a language.
P19-1323,2019,6 Conclusion and future work,"future work will offer a more principled account of aspectual classification for specific verb classes, among them speech act and communication verbs (e.g., promise or call) that occur frequently in corpora but have hitherto been neglected in aspectual analyses."
P19-1323,2019,6 Conclusion and future work,"we also intend to develop a more principled treatment for the aspectual classification of metaphors, which are frequent in other corpora."
P19-1324,2019,4 Future work,"though we focused on lstm lms for english, this method can be applied to other architectures, objective tasks, and languages; possibilities to explore in future work."
P19-1324,2019,4 Future work,"we also plan to carry out further analyses aimed at individuating factors that challenge the resolution of lexical ambiguity (e.g., morphosyntactic vs. semantic ambiguity, frequency of a word or sense, figurative uses), as well as clarifying the interaction between prediction and processing of words within neural lms."
P19-1326,2019,6 Conclusion and Future Work,future research directions include a theoretical explanation of kg2vec and applications to downstream nlp tasks.
P19-1329,2019,6 Conclusion,this work also raises important questions about other categories of word-like tokens that need to be treated like special cases.
P19-1330,2019,7 Conclusion and Future Work,"also, we will explore ways of automating parts of the process, e.g.the highlight annotation."
P19-1330,2019,7 Conclusion and Future Work,"in future work, we would like to extend our framework to other variants of summarization e.g.multi-document."
P19-1332,2019,5 Conclusion,"in the future, we plan to investigate more efficient methods of unsupervised domain adaptation with decomposition mechanism on other nlp tasks."
P19-1333,2019,7 Conclusion,"in the future, we plan to investigate the constituency type classification and rhetorical relation identification steps and port this approach to languages other than english."
P19-1335,2019,8 Conclusion,future variations of the task could incorporate nil recognition and mention detection (instead of mention boundaries being provided).
P19-1335,2019,8 Conclusion,we also expect models that jointly resolve mentions in a document would perform better than resolving them in isolation.
P19-1337,2019,6 Conclusion,"our approach is a general one that can be applied to other student model architectures, such as transformers (vaswani et al., 2017)."
P19-1338,2019,5 Conclusion,"in future work, we would like to combine more potential parsers—including chartstyle parsing and shift-reduce parsing—and transfer knowledge from one to another in a co-training setting."
P19-1340,2019,6 Conclusion,the remarkable effectiveness of unsupervised pretraining of vector representations of language suggests that future advances in this area can continue improving the ability of machine learning methods to model syntax (as well as other aspects of language).
P19-1344,2019,5 Conclusion and Future Work,"in our future work, we plan to apply our approach to other sequence labeling tasks, such as named entity recognition, word segmentation and so on."
P19-1345,2019,7 Conclusion,"furthermore, we would like to explore the effectiveness of our approach to asc-qa in other languages."
P19-1345,2019,7 Conclusion,"in our future work, we would like to solve other challenges in asc-qa such as data imbalance and negation detection to improve the performance."
P19-1346,2019,7 Conclusion,"we hope eli5 will inspire future work in all aspects of long-form qa, from the information extraction problem of obtaining information from long, multi-document input to generating more coherent and accurate paragraph-length answers."
P19-1350,2019,6 Conclusion,we reserve a deeper investigation of this aspect to future research.
P19-1351,2019,6 Conclusion,"in future work, we are investigating whether the vtqa model can be jointly trained with the paragraph captioning model."
P19-1353,2019,7 Conclusion,we hope this initial application inspires further research by literary scholars and computational humanists in the future.
P19-1354,2019,6 Conclusion,"as our approach is not limited to the nmt encoders, it is also interesting to explore how do the models trained on other nlp tasks learn word order information."
P19-1356,2019,8 Conclusion,it would be interesting to see if our results transfer to other domains with higher variability in syntactic structures (such as noisy user generated content) and with higher word order flexibility as experienced in some morphologically-rich languages.
P19-1358,2019,6 Future Work,"in this way, a dialogue agent could both improve its dialogue ability and its potential to improve further."
P19-1358,2019,6 Future Work,we leave exploration of this metalearning theme to future work.
P19-1360,2019,7 Conclusion and Future Work,"in the future, we intend to infer the dialog acts from the annotated responses and use such noisy data to guide the response generation."
P19-1362,2019,5 Conclusion,"in addition, the detailed content information can be considered in the relevant contexts to further improve the quality of generated response."
P19-1362,2019,5 Conclusion,"in future work, we plan to further investigate the proposed recosa model."
P19-1363,2019,6 Conclusion,"future work may also incorporate contradiction information into the dialogue model itself, and extend to generic contradictions."
P19-1364,2019,4 Conclusion,another interesting direction is to design a trainable budget scheduler.
P19-1364,2019,4 Conclusion,"in future, we plan to investigate the effectiveness of our method on more complex task-oriented dialogue datasets."
P19-1364,2019,4 Conclusion,"one possible solution is transfer learning, in which we train the budget scheduler on some welldefined dialogue tasks, then leverage this scheduler to guide the policy learning on other complex dialogue tasks."
P19-1366,2019,6 Conclusion and Future Work,"in addition, we will also explore how to integrate external knowledge in other formats, like the knowledge graph, into adversarial training so that the quality could be further improved."
P19-1366,2019,6 Conclusion and Future Work,"in future research, we will further investigate how to better leverage larger training data to improve the reat method."
P19-1367,2019,6 Conclusion and Future Work,"in the future, there are some promising explorations in vocabulary pyramid networks.1) we will further study how to obtain multi-level vocabularies, such as employing other clustering methods and incorporating semantic lexicons like wordnet; 2) we also plan to design deep-pass encoding and decoding for vpn; 3) we will investigate how to apply vpn to other natural language generation tasks such as machine translation and generative text summarization."
P19-1372,2019,6 Conclusion and future work,directions of future work may be pursuing betterdefined features and easier training strategies.
P19-1373,2019,7 Conclusion and Future Work,"finally, the addition of word-level pretraining methods to improve the dialog context representations should be explored."
P19-1373,2019,7 Conclusion and Future Work,"in this paper, unsupervised pretraining has been shown to learn effective representations of dialog context, making this an important research direction for future dialog systems."
P19-1373,2019,7 Conclusion and Future Work,"second, it would be interesting to test the representations learned using unsupervised pretraining on less-related downstream tasks such as sentiment analysis."
P19-1373,2019,7 Conclusion and Future Work,these results open three future research directions.
P19-1375,2019,5 Conclusion,"theoretically, we believe this self-supervision can be generalized to other types of temporal order in different nlp tasks."
P19-1376,2019,6 General discussion and conclusions,"although it learns some of the relevant features anyway, it would be interesting to see whether its behaviour becomes more human-like if the correct features are provided in the input."
P19-1376,2019,6 General discussion and conclusions,"there are many other potential architectures and modelling decisions that could be explored, as well as other behavioural data such as developmental patterns (blything et al., 2018; ambridge, 2010) and inflection in other languages (e.g., clahsen et al., 1992; ernestus and baayen, 2004)."
P19-1378,2019,6 Conclusion,"another interesting direction is to explore combining different semantic similarity measures (lin et al., 2015) for our task."
P19-1378,2019,6 Conclusion,"in future work, we will explore ensemble learning."
P19-1379,2019,5 Conclusion and Future Work,"in addition to tracking the language evolvement in the history, we believe it is promising future work to use deep contextual embeddings in predicting the future change or trend, as well as detecting novel senses that are not included in existing dictionaries."
P19-1381,2019,5 Conclusion,"in future work, we would like to further our insights on the cnn aspects that are crucial for the task, our preliminary analyses of kernel width and attention."
P19-1381,2019,5 Conclusion,we leave a proper formulation of a tighter comparison to future work.
P19-1384,2019,4 Conclusions,"in future work, we will extend our set of languages, aiming at more typological variety (indoeuropean languages are greatly over-represented in our current data)."
P19-1385,2019,6 Conclusions & Future work,"in the future, we aim to incorporate more elaborate linguistic resources (e.g.knowledge bases) and to investigate the performance of our methods on more complex nlp tasks, such as named entity recognition and sequence labelling, where prior knowledge integration is an active area of research."
P19-1385,2019,6 Conclusions & Future work,our approach can be applied to any rnn-based architecture as a extra module to further improve performance with minimal computational overhead.
P19-1386,2019,5 Conclusion,"in the future, we wish to study the use of knowref to improve performance on general coreference resolution tasks (e.g., the conll 2012 shared tasks)."
P19-1386,2019,5 Conclusion,we also plan to develop new models on knowref and transfer them to difficult common sense reasoning tasks.
P19-1388,2019,6 Conclusion and Discussion,"below, we discuss a few interesting issues brought up by the data collection process that should be addressed in future work."
P19-1392,2019,5 Conclusions and Further Work,in further work we plan to carry out a multilingual alignment of the collocations in each language.
P19-1394,2019,6 Conclusion and Future Work,"in the future, we aim to analyze how changes in the training procedure and hyperparameters of ulmfit affect resulting model performance."
P19-1394,2019,6 Conclusion and Future Work,"on top of that, we hope to improve model generalization by augmenting negative examples with a split of jokes into setups and punchlines, as they should not be funny by themselves."
P19-1394,2019,6 Conclusion and Future Work,we also plan to reproduce the experiment on english data.
P19-1396,2019,5 Conclusion,"for future study, we would like to investigate how to automatically learn the number of latent clusters with nonparametric bayesian methods."
P19-1397,2019,6 Conclusion,future work could potentially expand our work into an end-to-end invertible model that is able to produce high-quality representations by omnidirectional computations.
P19-1399,2019,6 Conclusion,"future works include applying the method to more language pairs and more domain-specific lexicon induction, e.g., terminologies."
P19-1400,2019,7 Conclusions,"besides, we will consider additional knowledge bases (e.g.yago and wikidata)."
P19-1400,2019,7 Conclusions,"in future work we will aim to improve candidate selection (including different strategies to select candidate lists e+, e?)."
P19-1400,2019,7 Conclusions,we will also use extra document information and jointly predict entities for different mentions in the document.
P19-1408,2019,8 Conclusions,a future direction is to investigate the effect of using mina spans not only in evaluation but also for training existing coreference resolvers.
P19-1408,2019,8 Conclusions,"investigating the use of mina in other nlp areas, e.g., evaluating spans in named entity recognition or reading comprehension, is another future line of work."
P19-1409,2019,8 Conclusion,"future directions include investigating ways to minimize the pipeline errors from the extraction of predicate-argument structures, and incorporating a mention prediction component, rather than relying on gold mentions."
P19-1410,2019,6 Conclusions,"also, in the future, we would like to investigate how our approach can be generalized to other discourse frameworks such as the penn discourse treebank (pdtb)."
P19-1411,2019,5 Conclusion,"in the future work, we plan to extend the idea of multi-task learning/transfer learning with label embeddings to the problems in information extraction (e.g., event detection, relation extraction, entity mention detection) (nguyen and grishman, 2015a,b, 2016d; nguyen et al., 2016a,b,c; nguyen and nguyen, 2018b, 2019)."
P19-1412,2019,6 Conclusion,"in the long run, to perform robust language understanding, models will need to incorporate more linguistic foreknowledge and be able to generalize to a wider range of linguistic constructions."
P19-1413,2019,5 Summary,"in the future we would like to expand this direction, and find ways to connect event and relation representation, learning and inference in a unified framework."
P19-1414,2019,6 Conclusion and Future Work,"as a future work, we plan to introduce a wider range of background knowledge including another type of event causality (hashimoto et al., 2012, 2014, 2015; kruengkrai et al., 2017)."
P19-1415,2019,5 Conclusions,"as for future work, we would like to enhance the ability to utilize antonyms for unanswerable question generation by leveraging external resources."
P19-1416,2019,6 Conclusions,"in this context, we suggest that future work can explore better retrieval methods for multi-hop questions."
P19-1417,2019,5 Conclusion,"in future work, we will extend the proposed idea to other qa tasks with evidence of multimodality, e.g.combining with symbolic approaches for visual qa (gan et al., 2017; mao et al., 2019; hu et al., 2019)."
P19-1421,2019,6 Conclusions and Future Work,"promising future directions would be to investigate how to utilize user interaction in moocs more adequately, as well as how attributes of course concepts can help expanding."
P19-1423,2019,6 Conclusion,"as future work, we plan to incorporate joint named entity recognition training as well as sub-word embeddings in order to further improve the performance of the proposed model."
P19-1424,2019,7 Limitations and Future Work,"finally, we plan to adapt bespoke models proposed for the chinese criminal court (luo et al., 2017; zhong et al., 2018; hu et al., 2018) to data from other courts and explore multitask learning."
P19-1424,2019,7 Limitations and Future Work,"providing valid justifications is an important priority for future work and an emerging topic in the nlp community.8 in this direction, we plan to expand the scope of this study by exploring the automated analysis of additional resources (e.g., relevant case law, dockets, prior judgments) that could be then utilized in a multi-input fashion to further improve performance and justify system decisions."
P19-1424,2019,7 Limitations and Future Work,"we also plan to apply neural methods to data from other courts, e.g., the european court of justice, the us supreme court, and multiple languages, to gain a broader perspective of their potential in legal justice prediction."
P19-1425,2019,6 Conclusion,"in future work, we plan to explore the direction to generate more natural adversarial examples dispensing with word replacements and more advanced defense approaches such as curriculum learning (jiang et al., 2018, 2015)."
P19-1428,2019,7 Conclusions and future work,"another issue to research will be the use of regression models to estimate the expected fitness of a pipeline given its features, as illustrated in section 6."
P19-1428,2019,7 Conclusions and future work,"as future work, we plan to study the introduction of high-level knowledge to deal with the issue of invalid pipelines and improve the performance of the optimization process."
P19-1428,2019,7 Conclusions and future work,"therefore, we plan to explore this line of research in the future, to compare our proposal with other automl frameworks in standard benchmarks."
P19-1428,2019,7 Conclusions and future work,"this addition would support meta-learning algorithms, allowing to reduce the optimization time and increase its performance by learning from past executions."
P19-1429,2019,6 Conclusions,"for future work, we plan to investigate new auxiliary ?-learning algorithms using our ?-learning framework."
P19-1430,2019,5 Conclusion and Future Work,"although we have used word, sense and character information in our work, more level of information can be incorporated into the mg lattice."
P19-1430,2019,5 Conclusion and Future Work,"in the future, we will attempt to improve the ability of the mg lattice to utilize multi-grained information."
P19-1431,2019,5 Conclusion,"future research will look into applying such methods to reason jointly about text and kg, by attending to textual mentions of entities in addition to graph (verga et al., 2016)."
P19-1432,2019,5 Conclusion & Future Work,"consequently, in the future work, we plan to develop methods that can automatically induce the sentence structures for efp."
P19-1436,2019,7 Conclusion,future work includes better phrase representation learning to close its accuracy gap with qa models with query-dependent document encoding.
P19-1436,2019,7 Conclusion,"our phrase representations leverage sparse and dense vectors to capture lexical, semantic, and syntactic information."
P19-1436,2019,7 Conclusion,utilizing the phrase index as an external memory for an interaction with text-based knowledge is also an interesting direction.
P19-1436,2019,7 Conclusion,we believe that even further speedup and larger coverage of documents can be done with a similarity search package for dense+sparse vectors.
P19-1437,2019,7 Conclusion,"in this work, we aim to improve language modeling with shared grammar."
P19-1438,2019,7 Conclusion,promising future directions include experimenting with our zero-shot adaptation methods in the context of neural semantic parsing (after increasing the number of examples per domain) and extending the dataset to include more complicated applications and multi-utterance instructions.
P19-1438,2019,7 Conclusion,we hope this work will inspire readers to use our framework for collecting a larger dataset and experimenting with more approaches.
P19-1441,2019,5 Conclusion,"at last, we also would like to verify whether mt-dnn is resilience against adversarial attacks (glockner et al., 2018; talman and chatzikyriakidis, 2018; liu et al., 2019)."
P19-1441,2019,5 Conclusion,"there are many future areas to explore to improve mt-dnn, including a deeper understanding of model structure sharing in mtl, a more effective training method that leverages relatedness among multiple tasks, for both fine-tuning and pre-training (dong et al., 2019), and ways of incorporating the linguistic structure of text in a more explicit and controllable manner."
P19-1447,2019,4 Conclusion,in the future we plan to apply the reranker to other parsers and more benchmark datasets.
P19-1447,2019,4 Conclusion,we will also attempt to jointly train the base semantic parser and the reranker by using the reranker’s output as supervision to fine tune the base parser.
P19-1450,2019,5 Conclusion,"in the future, we would like to extend our approach to sembanks which are annotated with different types of semantic representation, e.g."
P19-1450,2019,5 Conclusion,"sql (yu et al., 2018) or drt (abzianidze et al., 2017)."
P19-1450,2019,5 Conclusion,we will explore latent-variable models to learn the dependency trees automatically.
P19-1452,2019,5 Conclusion,we employ the edge probing task suite to explore how the different layers of the bert network can resolve syntactic and semantic structure within a sentence.
P19-1455,2019,7 Conclusion and Future Work,another direction could be to create fusion strategies that can better model incongruity among modalities to identify sarcasm.
P19-1455,2019,7 Conclusion and Future Work,"future work could investigate advanced spatiotemporal fusion strategies (e.g., tensor-fusion (zadeh et al., 2017), cca (hotelling, 1936)) to better encode the correspondence between modalities."
P19-1455,2019,7 Conclusion and Future Work,future work should try to leverage these factors to improve the baseline scores reported in this paper.
P19-1455,2019,7 Conclusion and Future Work,"future work should try to overcome this issue with solutions involving pre-training, transfer learning, domain adaption, or low-parameter models."
P19-1455,2019,7 Conclusion and Future Work,"moreover, while conducting this research, we identified several challenges that we believe are important to address in future research work on multimodal sarcasm detection."
P19-1456,2019,6 Conclusion,"besides, developing techniques to incorporate the claim stance and specificity detection models in argument generation to generate more coherent and consistent arguments is another interesting research direction to be explored."
P19-1456,2019,6 Conclusion,"for future work, it may be interesting to understand which other models would be effective in claim specificity and stance detection tasks."
P19-1458,2019,9 Discussion and Future Considerations,"in the future, we will investigate other nlp tasks such as named entity recognition (ner), question answering (qa) and aspect-based sentiment analysis (absa) (pontiki et al., 2016) to see whether results we saw in sentiment analysis is consistent across these tasks."
P19-1458,2019,9 Discussion and Future Considerations,we hope that our experimental results inspire future research dedicated to japanese.
P19-1459,2019,8 Conclusion,the adversarial dataset should be adopted as the standard in future work on arct.
P19-1460,2019,5 Conclusion and Future Work,"in the future, this work can be progressed in several ways."
P19-1462,2019,4 Conclusion,"in future works, we will explore the extension of this approach for other tasks."
P19-1463,2019,5 Conclusion,"for future work, we plan to i) automatically predict relations between argument components in the uselecdeb60to16 dataset, and ii) propose a new task, i.e., fallacy detection so that common fallacies in political argumentation (zurloni and anolli, 2010) can be automatically identified, in line with the work of (habernal et al., 2018)."
P19-1464,2019,5 Conclusion and Future work,"another direction is to explore span representations in several related tasks such as rst-style discourse parsing or new span-related argumentation mining tasks (trautmann et al., 2019)."
P19-1464,2019,5 Conclusion and Future work,one interesting line of our future work is to investigate the performance of our model in an endto-end setting (including ac segmentation).
P19-1466,2019,5 Conclusion and Future Work,"in the future, we intend to extend our method to better perform on hierarchical graphs and capture higher-order relations between entities (like motifs) in our graph attention model."
P19-1466,2019,5 Conclusion and Future Work,"the proposed model can be extended to learn embeddings for various tasks using kgs such as dialogue generation (he et al., 2017; keizer et al., 2017), and question answering (zhang et al., 2016; diefenbach et al., 2018)."
P19-1467,2019,7 Conclusion,"in future work, we intend to refine these alignment boundaries and to optimize the alignment procedure for speed."
P19-1467,2019,7 Conclusion,we hope that this work will raise more interest in developing alignment systems for longer paraphras.
P19-1469,2019,6 Conclusion,"for future work, we plan to focus on generating more realistic text and use the generated text in other tasks e.g.data augmentation, addressing adversarial attack."
P19-1469,2019,6 Conclusion,we expect the model could perform more natural generation via applying recent advancements on deep generative models.
P19-1470,2019,7 Conclusion,"these positive results point to future work in extending the approach to a variety of other types of knowledge bases, as well as investigating whether comet can learn to produce openie-style knowledge tuples for arbitrary knowledge seeds."
P19-1473,2019,7 Conclusions and Future Work,an interesting direction would be to enable domain experts to identify and actively request for program annotations given the knowledge shared by other domains.
P19-1473,2019,7 Conclusions and Future Work,this would further make it feasible to perform transfer learning on a new domain.
P19-1473,2019,7 Conclusions and Future Work,we also plan to investigate the possibility of augmenting the parallel corpus by bootstrapping from shared templates across domains.
P19-1473,2019,7 Conclusions and Future Work,we plan to explore if further fine-tuning using denotations based training on the distilled model can lead to improvements in the unified parser.
P19-1473,2019,7 Conclusions and Future Work,we would also like to explore if guiding the decoder through syntactical and domain-specific constraints helps in reducing the search space for the weakly supervised unified parser.
P19-1474,2019,6 Conclusion,a prominent direction for future work is using the hyperbolic embeddings as the sole signal for taxonomy extraction.
P19-1474,2019,6 Conclusion,"since distributional and hyperbolic embeddings cover different relations between terms, it may be interesting to combine them."
P19-1476,2019,4 Conclusion,"we next plan to evaluate glen on multilingual and cross-lingual graded le datasets (vulic et al.′ , 2019) and release a large multilingual repository of le-specialized embeddings."
P19-1477,2019,5 Conclusion,"future work will entail adaption of the attentions, to further improve the performance."
P19-1478,2019,6 Summary and Outlook,"in future work, other uses and the statistical significance of maskedwiki’s impact and its applications to different tasks will be investigated."
P19-1479,2019,5 Conclusion,"in the future, we would like to explore how to introduce external knowledge into the graph to make the generated comments more logical."
P19-1480,2019,7 Conclusion and Future Work,there are several future directions for this setting.
P19-1481,2019,7 Conclusion,"in future work, we will explore the use of cross-lingual embeddings to further improve performance on this task."
P19-1487,2019,7 Conclusion and Future Work,"with deferral of explanation to neural models, it will be crucial in the future to study the ethical implications of biases that are accumulated during pretraining or fine-tuning."
P19-1489,2019,7 Discussion: What Modularity Can andCannot Do,"future work should investigate how to combine techniques that use both word meaning and nearest neighbors for a more robust, semisupervised cross-lingual evaluation."
P19-1490,2019,5 Conclusion and Future Work,"in the future, we will work on cross-lingual extensions of monolingual hyperbolic embedding models (nickel and kiela, 2017; ganea et al., 2018)."
P19-1490,2019,5 Conclusion and Future Work,"we will also experiment with other sources of bilingual information (e.g., cross-lingual word embeddings) and port the transfer approach to more language pairs, with a particular focus on resource-poor languages."
P19-1491,2019,7 Conclusion,"our mixed-effects approach could be used to assess other nlp systems via parallel texts, separating out the influences on performance of language, sentence, model architecture, and training procedure."
P19-1491,2019,7 Conclusion,"we reevaluated the conclusions of cotterell et al.(2018) on a larger set of languages, requiring new methods to select fully parallel data (§4.2) or handle missing data."
P19-1492,2019,6 Conclusions and future work,"in particular, we would like to explore new methods to jointly learn cross-lingual embeddings on monolingual corpora."
P19-1493,2019,6 Conclusion,it is our hope that these kinds of probing experiments will help steer researchers toward the most promising lines of inquiry by encouraging them to focus on the places where current contextualized word representation approaches fall short.
P19-1494,2019,6 Conclusions and future work,"in addition to that, we would like to integrate our induced dictionaries in other downstream tasks like unsupervised cross-lingual information retrieval (litschko et al., 2018)."
P19-1494,2019,6 Conclusions and future work,"in the future, we would like to further improve our method by incorporating additional ideas from unsupervised machine translation such as joint refinement and neural hybridization (artetxe et al., 2019)."
P19-1495,2019,7 Conclusions & Future Work,a predictive model for identification of complaints is useful to companies that wish to automatically gather and analyze complaints about a particular event or product.
P19-1495,2019,7 Conclusions & Future Work,"another research direction is to study the role of complaints in personal conversation or in the political domain, e.g., predicting political stance in elections (tsakalidis et al., 2018)."
P19-1495,2019,7 Conclusions & Future Work,"in the future, we plan to identify the target of the complaint in a similar way to aspect-based sentiment analysis (pontiki et al., 2016)."
P19-1495,2019,7 Conclusions & Future Work,we plan to use additional context and conversational structure to improve performance and identify the sociodemographic covariates of expressing and phrasing complaints.
P19-1497,2019,5 Conclusion and Future Work,"finally, combining openqg with its reverse task, openqa, is also worth exploration."
P19-1497,2019,5 Conclusion and Future Work,"first, we will explore more powerful qg structure to deal with the huge difference between the length of input and output texts."
P19-1497,2019,5 Conclusion and Future Work,there are many future works to be done.
P19-1499,2019,5 Conclusions,"in the future, we plan to apply models to other tasks that also require hierarchical document encodings (e.g., document question answering)."
P19-1499,2019,5 Conclusions,we are also interested in improving the architectures of hierarchical document encoders and designing other objectives to train hierarchical transformers.
P19-1500,2019,6 Conclusions,in the future we would like to apply our hierarchical transformer to question answering and related textual inference tasks.
P19-1501,2019,7 Conclusion and Future Work,"finally, the distinct semantic representation of each word could further enhance the performance of the deep learning model."
P19-1503,2019,5 Conclusion,future work could be comparing language models of different types and scales in this direction.
P19-1508,2019,8 Conclusion,our work paves the way towards understanding the extent to which human meaning representation is impacted by negation.
P19-1509,2019,5 Discussion,"a better understanding of what are the “innate” biases of standard models in highly controlled setups, such as the one studied here, should complement large-scale simulations, as part of the effort to develop new methods to encourage the emergence of more human-like language."
P19-1509,2019,5 Discussion,how to incorporate “effort”-based pressures in neural networks is an exciting direction for future work.
P19-1509,2019,5 Discussion,the research direction we introduced might lead to a better understanding of the biases that affect the linguistic behaviour of lstms and similar models.
P19-1510,2019,5 Summary,"we are optimistic that nne will encourage the development of new ner models that recognize structural information within entities, and therefore understand finegrained semantic information captured."
P19-1511,2019,6 Conclusions and Future Work,"as the head-driven structures are widely spread in natural language, the solution proposed in this paper can also be used for modeling and exploiting this structure in many other nlp tasks, such as semantic role labeling and event extraction."
P19-1512,2019,6 Conclusion,"looking forward, our attention mechanism can also be applied to tasks such as relational networks (santoro et al., 2017), natural language inference (maccartney and manning, 2009), and qa systems (zhou et al., 2015)."
P19-1513,2019,8 Conclusions,"also the work reported in this paper is based on a small tdm taxonomy, we plan to construct a tdm knowledge base and provide an applicable system for a wide range of nlp papers."
P19-1513,2019,8 Conclusions,we created two datasets in the nlp domain to test our system.
P19-1516,2019,7 Conclusion,"in future, we would like to assist the model with multiple linguistic aspects of social media text like figurative languages."
P19-1517,2019,4 Conclusion and Future Work,"in the future, we would also like to investigate better models that are capable to address general arithmetic word problems, including addition, subtraction, multiplication and division."
P19-1520,2019,5 Conclusion and Future Work,"for future work, we plan to apply the main idea of our approach to other tasks."
P19-1522,2019,6 Conclusion and Discussion,"such features are ignored in our model, but they deserve more investigation for improving the extraction model."
P19-1522,2019,6 Conclusion and Discussion,"therefore, for the future work, we will incorporate relation between events and relation between arguments into pre-trained language models, and take effective measures to overcome the deviation problem of roles in the generation."
P19-1523,2019,5 Conclusion,an error analysis is performed to shed light on possible future directions.
P19-1524,2019,4 Discussion,"also, we would like to further explore the possibility to use domain-specific gazetteers or dictionaries to boost the performance of ner in various domains (shang et al., 2018), beyond the standard corpora."
P19-1524,2019,4 Discussion,"future directions will include trying similarly enhanced modules on other different types of segmental models (kong et al., 2016; liu et al., 2016; zhuo et al., 2016; zhai et al., 2017; sato et al., 2017), along with richer representations for further gain."
P19-1528,2019,7 Conclusion,the problem of adapting this approach to higher order statistical language models (such as pcfgs) remains open.
P19-1529,2019,6 Conclusion and Future Work,in future work we will explore how external information is best used in ensembles of models for srl and other tasks.
P19-1529,2019,6 Conclusion and Future Work,we will also investigate whether the choice of method for injecting external information has the same impact on other nlp tasks as it does on srl.
P19-1533,2019,7 Conclusion,future work will consider problems where more challenging forms of neighbor manipulation are necessary for prediction.
P19-1567,2019,7 Conclusion,we hope that this work will raise more interest in developing alignment systems for longer paraphrase.
P19-1568,2019,7 Conclusion and Future Work,another potential future work would be to explore other ways of providing rich supervision from textual descriptions as targets.
P19-1569,2019,7 Future Work,in future work we plan to use multilingual resources (i.e.embeddings and glosses) for improving our sense embeddings and evaluating on multilingual wsd.
P19-1569,2019,7 Future Work,"we expect our sense embeddings to be particularly useful in downstream tasks that may benefit from relational knowledge made accessible through linking words (or spans) to commonsense-level concepts in wordnet, such as natural language inference."
P19-1570,2019,8 Conclusion and future work,a natural extension to this work would be to capture the sense distribution of sentences using the same framework.
P19-1571,2019,6 Conclusion and Future Work,"in the future, we will explore the following directions: (1) context information is also essential to mwe representation learning, and we will try to combine both internal information and external context information to learn better mwe representations; (2) many mwes lack sememe annotation and we will seek to calculate an mwe’s scd when we only know the sememes of the mwe’s constituents; (3) our proposed models are also applicable to the mwes with more than two constituents and we will extend our models to longer mwes; (4) sememe is universal linguistic knowledge and we will explore to generalize our methods to other languages."
P19-1572,2019,6 Conclusion,"finally, we plan to further extend and use the humour dataset to investigate open questions on the linguistics of humour, such as what relationships hold between a pun’s phonology and its “successfulness” or humorousness (lagerquist, 1980; hempelmann and miller, 2017)."
P19-1572,2019,6 Conclusion,"given that our model achieves good results with rudimentary, task-agnostic linguistic features, in future work we plan to investigate the use of humourand metaphor-specific features, including some of those used in past work (see §2) as well as those inspired by the prevailing linguistic theories of humour (attardo, 1994) and metaphor (black, 1955; lakoff and johnson, 1980)."
P19-1575,2019,6 Conclusions,future work can improve this method further by examining the effects of different dimensionality reduction methods with varying properties on extracting the most informative pathways from the activations.
P19-1576,2019,5 Conclusions and Future Work,"another exciting avenue would involve exploring cross-lingual transfer of lfs, taking advantage of recent development in unsupervised cross-lingual embedding learning (artetxe et al., 2017; conneau et al., 2017)."
P19-1576,2019,5 Conclusions and Future Work,"in the future, we would like to experiment with more data, so that enough training data can be obtained for less frequent lfs."
P19-1576,2019,5 Conclusions and Future Work,"to this end, we could benefit from the supervised approach proposed in (rodr′?guez-fernandez et al.′ , 2016), and then filter by pairwise correlation strength metrics such as pmi."
P19-1577,2019,5 Conclusion,"in future, we plan to develop an automatic procedure of finding thesaurus regularities in dbag of problematic words, which can make more evident what kind of relations or senses are missed in the thesaurus."
P19-1578,2019,4 Discussion and Future Work,"for the future work, we hope to extend this idea proposed in this paper to train a model capable of handling different types of errors through the generative model since it can generate different lengths of results."
P19-1579,2019,7 Conclusion,"in future work, we will explore methods for controlling the induced dictionary quality to improve word substitution as well as m-umt."
P19-1579,2019,7 Conclusion,we will also attempt to create an end-toend framework by jointly training m-umt pivoting system and low-resource translation system in an iterative fashion in order to leverage more versions of augmented data.
P19-1580,2019,8 Conclusions,"important heads have one or more interpretable functions in the model, including attending to adjacent words and tracking specific syntactic relations."
P19-1580,2019,8 Conclusions,"in future work, we would like to investigate how our pruning method compares to alternative methods of model compression in nmt."
P19-1584,2019,10 Conclusions & Future Work,future work: the automatic pseudonymization approach could serve as a data augmentation scheme to be used as a regularizer for deidentification models.
P19-1584,2019,10 Conclusions & Future Work,private character embeddings that are learned from a perturbed source could be an interesting extension to our models.
P19-1584,2019,10 Conclusions & Future Work,"when more training data from multiple sources become available in the future, it will be possible to evaluate our adversarially learned representation against unseen data."
P19-1585,2019,7 Conclusion,"despite being trained only for ner, the architecture provides intuitive embeddings for a variety of multi-word entities, a step which we suggest could prove useful for a variety of downstream tasks, including entity linking and coreference resolution."
P19-1586,2019,7 Conclusion,our frameworks of transfer and active learning for deep learning models are potentially applicable to low-resource settings beyond entity resolution.
P19-1587,2019,5 Conclusion,the proposed model offers promising future extensions in terms of directly optimizing other metrics such as recall and fβ.
P19-1587,2019,5 Conclusion,this work also opens up a range of questions from modeling to evaluation methodology
P19-1588,2019,6 Conclusion,"in the future, first, we try to utilize more eyetracking corpus and estimate more features of reading behavior."
P19-1588,2019,6 Conclusion,"then, we will attempt to analyze real human reading behavior on social media and thereby explore more specific human attention features on social media."
P19-1589,2019,5 Conclusion and Future Work,"in future work, we want to extend this approach to other natural language processing tasks."
P19-1591,2019,6 Conclusions,"in future work we would like to develop more sophisticated trl algorithms, for both in-domain and domain adaptation nlp setups."
P19-1591,2019,6 Conclusions,"moreover, we would like to establish the theoretical groundings to the improved stability achieved by trl, and to explore this effect beyond domain adaptation."
P19-1592,2019,6 Conclusion,"additionally, we hope to apply stance to a wider-range of entity resolution tasks, for which string similarity is a component of model that considers additional features such as the natural language context of the entity mention."
P19-1592,2019,6 Conclusion,"in future work, we hope to further study the connections between our optimal transport-based alignment method and methods based on attention."
P19-1593,2019,5 Conclusion,"a key question for future work is the performance on longer texts, such as the full-length news articles encountered in ontonotes."
P19-1593,2019,5 Conclusion,"another direction is to further explore semi-supervised learning, by reducing the amount of training data and incorporating linguistically-motivated constraints based on morphosyntactic features."
P19-1594,2019,5 Conclusions,the ability of the spectral method for pnfa to estimate substring expectations can be exploited in other contexts.
P19-1595,2019,6 Discussion and Conclusion,"achieving robust multi-task gains across many tasks has remained elusive in previous research, so we hope our work will make multi-task learning more broadly useful within nlp."
P19-1596,2019,6 Conclusions,"finally, we are interested in reproducing our corpus generation method on various other domains to allow for the creation of numerous useful datasets for the nlg community."
P19-1596,2019,6 Conclusions,"for future work, we plan on exploring other models for nlg, and on providing models with a more detailed input representation in order to help preserve more dependency information, as well as to encode more information on syntactic structures we want to realize in the output."
P19-1596,2019,6 Conclusions,"we are also interested in including richer, more semanticallygrounded information in our mrs, for example using abstract meaning representations (amrs) (dorr et al., 1998; banarescu et al., 2013; flanigan et al., 2014)."
P19-1597,2019,5 Conclusion and Future Work,and unsupervised approaches to leverage massive chess comments in social media is also worth exploring.
P19-1597,2019,5 Conclusion and Future Work,another interesting direction is to extend our models to multimove commentary generation tasks.
P19-1598,2019,7 Conclusions and Future Work,"our distantly supervised approach to dataset creation can be used with other knowledge graphs and other kinds of text as well, providing opportunities for accurate language modeling in new domains."
P19-1598,2019,7 Conclusions and Future Work,this work lays the groundwork for future research into knowledge-aware language modeling.
P19-1600,2019,6 Conclusion and Future Work,"in the future, we will explore the representation for the implicit information like whether a man is retired or not or how long a sportsman’s career is given starting and ending years, in the table by including some inference strategies."
P19-1601,2019,5 Conclusions and Future W,"in the future, we are planning to adapt our style transformer to the multiple-attribute setting like lample et al.(2019)."
P19-1603,2019,6 Conclusion and Future Work,"future work can combine the analyzer and generator via joint training, hopefully to achieve better results."
P19-1604,2019,5 Conclusions and Future Work,"we are extending the proposed approach to other qa datasets, and adapting it to use pretrained language models such as bert (devlin et al., 2018), to evaluate the consistency of the mechanisms introduced."
P19-1606,2019,6 Conclusions,we plan on exploring backpropable variants as a scaffold for structure and also extend the techniques to other how-to domains in future.
P19-1606,2019,6 Conclusions,we plan to explore explicit evaluation of the latent structure learnt.
P19-1608,2019,6 Conclusion,further work on these inductive biases could help understand how a pretrained transfer learning model can be adapted in the most optimal fashion to a given target task.
P19-1610,2019,6 Conclusion,there are several possible future directions stemming from this work.
P19-1610,2019,6 Conclusion,"there is also scope for better techniques to generate more coherent question paraphrasing when significant question re-writing is required, such as for the situation when we want to paraphrase the question using context words."
P19-1616,2019,7 Conclusion,"similar problems may exist in other nlp tasks, which will be interesting to investigate in the future."
P19-1617,2019,5 Conclusion,"in the future, we may incorporate new advances in building entity graphs from texts, and solve more difficult reasoning problems, e.g.the cases of comparison query type in hotpotqa."
P19-1618,2019,6 Discussion and Future Work,"additionally, it would be interesting to study the behavior of nlprolog in the presence of multiple wikihop query predicates."
P19-1618,2019,6 Discussion and Future Work,"we are also interested in incorporating future improvements of symbolic provers, triple extraction systems and pretrained sentence representations to further enhance the performance of nlprolog."
P19-1620,2019,5 Conclusion,"we additionally proposed a possible direction for formal grounding of this method, which we hope to develop more thoroughly in future work."
P19-1622,2019,4 Conclusion,we will study further the capability of our approaches on other datasets and tasks in the future work.
P19-1626,2019,6 Conclusion,3 future work includes the fusion of additional operations in neural models.
P19-1628,2019,6 Conclusions,"in the future, we would like to investigate whether some of the ideas introduced in this paper can improve the performance of supervised systems as well as sentence selection in multi-document summarization."
P19-1629,2019,8 Conclusions,"in the future, we would like to more faithfully capture the semantics of documents by explicitly modeling entities and their linking."
P19-1630,2019,6 Conclusions,"an interesting challenge, and open research question, concerns the extent to which synthetic training impacts the overall model generalizability."
P19-1630,2019,6 Conclusions,"we believe that training models on heuristic, but inexpensive data sets is a valuable approach which opens up exciting opportunities for future research."
P19-1631,2019,6 Conclusion and Future Work,"another avenue is to incorporate different model attribution strategies such as deeplrp (bach et al., 2015) into the objective function."
P19-1631,2019,6 Conclusion and Future Work,there are several avenues we can explore as future research.
P19-1631,2019,6 Conclusion and Future Work,we apply this technique to model fairness in toxic comment classification.
P19-1633,2019,5 Conclusions and Future Work,"as future work, we will investigate approaches to hyperparameter tuning to find better model architectures for hierarchical multi-label text classification tasks."
P19-1634,2019,6 Conclusion,"in particular, we strongly suggest that future evaluations should adopt a stateless one-case-at-a-time test policy."
P19-1635,2019,6 Conclusion,"in future work, we plan to extend our work to further applications such as detecting exaggerated statements by investors in social media data."
P19-1636,2019,6 Limitations and Future Work,"finally, we plan to investigate generalized zero-shot learning (liu et al., 2018)."
P19-1636,2019,6 Limitations and Future Work,"furthermore, experimenting with more datasets e.g., rcv1, amazon-13k, wiki-30k, mimic-iii will allow us to confirm our conclusions in different domains."
P19-1636,2019,6 Limitations and Future Work,we also plan to experiment with hierarchical flavors of bert to surpass its length limitations.
P19-1636,2019,6 Limitations and Future Work,we leave the investigation of methods for extremely large label sets for future work.
P19-1636,2019,6 Limitations and Future Work,"we plan to investigate more computationally efficient methods, e.g., dilated cnns (kalchbrenner et al., 2017) and transformers (vaswani et al., 2017; dai et al., 2019)."
P19-1637,2019,6 Conclusion,"an important question for future models, especially interactive ones, is how to signal to the user when their desires do not comport with reality."
P19-1638,2019,5 Conclusions and Future work,"future work can investigate other embedding methods with a richer set of probe tasks, or explore a wider range of downstream tasks."
P19-1639,2019,4 Conclusion,"in the future, we hope to learn models that are able to sample search queries or information needs from sentences and use the output of that model to get relevant documents."
P19-1640,2019,7 Conclusion and Future Work,a future direction is to improve the gan-based training of w-lda.
P19-1640,2019,7 Conclusion and Future Work,another future direction is to experiment with more complex priors than the dirichlet prior.
P19-1642,2019,5 Conclusions and Future work,"in future work we will explore other generative models for multi-modal mt, as well as different ways to directly incorporate images into these models."
P19-1643,2019,7 Conclusion,"in future work, we plan to explore additional representations and architectures to improve the accuracy of our model, and to identify finer-grained alignments between visual actions and their verbal descriptions."
P19-1647,2019,6 Conclusion,we are also planning to investigate how sensitive our approach is to amount of data for the auxiliary task.
P19-1652,2019,5 Conclusion and Future Work,"furthermore, it is also interesting to design a mutual reinforcement mechanisms between the vocabulary constructor and the text generator to improve both components simultaneously."
P19-1652,2019,5 Conclusion and Future Work,"in future, we plan to study more effective ways to construct the image-grounded vocabulary."
P19-1656,2019,5 Discussion,"we hope the emergence of mult could encourage further explorations on tasks where alignment used to be considered necessary, but where crossmodal attention might be an equally (if not more) competitive alternative."
P19-1659,2019,6 Conclusions,"in the future, we would like to extend this work to generate multidocument (multi-video) summaries and also build end-to-end models directly from audio in the video instead of text-based output from pretrained asr."
P19-1660,2019,6 Conclusion,"an orthogonal line of future work might involve using a visual question answering (vqa) task (such as in krishna et al.(2017)), either on its own replacing the captioning task, or in conjunction with the captioning task with a multi-task learning objective."
P19-1660,2019,6 Conclusion,there are several interesting avenues for future work.
