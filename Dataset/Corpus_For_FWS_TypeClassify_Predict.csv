id,chapter,year,text
2020.acl-demos.1.txt,7 Conclusion,2020,another direction for improvement is to further enhance the ability to interact with users via a conversation interface.
2020.acl-demos.1.txt,7 Conclusion,2020,"one such important direction for future improvement is the expansion of areas that it can work in, which can be achieved through a promising approach of adopting model based technologies together with rule/template based ones."
2020.acl-demos.1.txt,7 Conclusion,2020,"so far, xiaomingbot has been deployed online and is serving users."
2020.acl-demos.10.txt,6 Conclusion,2020,"in particular, we plan to incorporate human performance as a reference metric, integrating psycholinguistic experimental results and supporting easy experimental design starting from the test suite format."
2020.acl-demos.10.txt,6 Conclusion,2020,"syntaxgym is continually evolving: we plan to add new features to the site, and to develop further in response to user feedback."
2020.acl-demos.10.txt,6 Conclusion,2020,"we also plan to further incorporate language models into the lm-zoo tool, allowing broader access to state-of-the-art language models in general."
2020.acl-demos.10.txt,6 Conclusion,2020,"we welcome open-source contributions to the website and to the general framework, and especially encourage the nlp community to contribute their models to the lm-zoo repository."
2020.acl-demos.14.txt,5 Conclusion and Future Work,2020,"for future work, we consider the following areas of improvement in the near term: • models downloadable in sta n z a are largely trained on a single dataset."
2020.acl-demos.14.txt,5 Conclusion and Future Work,2020,"simultaneously, sta n z a ’s corenlp client extends its functionality with additional nlp tools."
2020.acl-demos.14.txt,5 Conclusion and Future Work,2020,"to make models robust to many different genres of text, we would like to investigate the possibility of pooling various sources of compatible data to train “default” models for each language; • the amount of computation and resources available to us is limited."
2020.acl-demos.14.txt,5 Conclusion and Future Work,2020,"we would like to further investigate reducing model sizes and speeding up computation in the toolkit, while still maintaining the same level of accuracy.• we would also like to expand sta n z a ’s functionality by adding other processors such as neural coreference resolution or relation extraction for richer text analytics."
2020.acl-demos.16.txt,4 Conclusion,2020,"we will extend mt-dnn to support natural language generation tasks, e.g. question generation, and incorporate more pre-trained encoders, e.g. t5 (raffel et al., 2019) in future."
2020.acl-demos.17.txt,6 Conclusion and Future Work,2020,"an interesting direction to explore is re-ranking corrective suggestions, so that the suggestion more relevant to the original sentence goes to the top."
2020.acl-demos.17.txt,6 Conclusion and Future Work,2020,"for example, the method for introducing additional training data or generating artificial training data could be implemented to improve the performance."
2020.acl-demos.17.txt,6 Conclusion and Future Work,2020,many avenues exist for future research and improvement of our system.
2020.acl-demos.17.txt,6 Conclusion and Future Work,2020,yet another direction of research would be to detect fine-grained error types.
2020.acl-demos.18.txt,5 Conclusions,2020,"our case study on the wmt2019 metrics shared task further highlights the potential of clir as a proxy task for mt evaluation, and we hope that clireval can facilitate future research in this area."
2020.acl-demos.18.txt,5 Conclusions,2020,"the aim of this project is not to replace current automatic evaluation metrics or fix the limitations in those metrics, but to bridge the gap between machine translation and cross-lingual information retrieval and to show that clir is a feasible proxy task for mt evaluation."
2020.acl-demos.19.txt,5 Conclusion,2020,"based on convlab (lee et al., 2019b), convlab-2 integrates more powerful models, supports more datasets, and develops an analysis tool and an interactive tool for comprehensive end-to-end evaluation."
2020.acl-demos.19.txt,5 Conclusion,2020,we hope that convlab-2 is instrumental in promoting the research on task-oriented dialogue.
2020.acl-demos.2.txt,6 Conclusion and Future Work,2020,"for example, its usability in generation tasks such as machine translation has not been tested."
2020.acl-demos.2.txt,6 Conclusion and Future Work,2020,"in the future, we aim to integrate neural architecture search into the toolkit to automate the searching for model structures."
2020.acl-demos.2.txt,6 Conclusion and Future Work,2020,we will keep adding more examples and tests to expand textbrewer’s scope of application.
2020.acl-demos.20.txt,4 Conclusions and Future Work,2020,"additionally, we would like to explore opusfilter’s use in different scenarios and for other language pairs."
2020.acl-demos.20.txt,4 Conclusions and Future Work,2020,especially interesting would be the application in low-resource settings and various levels of noise in the original data.
2020.acl-demos.20.txt,4 Conclusions and Future Work,2020,"furthermore, the use for domain adaptation and data selection should be further explored."
2020.acl-demos.20.txt,4 Conclusions and Future Work,2020,"in future work, we would like to extend the toolbox with additional filters and classification options."
2020.acl-demos.22.txt,6 Discussion,2020,"however, exbert can effectively narrow the scope and refine hypotheses through quick testing iterations."
2020.acl-demos.24.txt,6 Conclusion and Future Work,2020,"we also plan to improve the performance of core models in photon, such as semantic parsing (text-to-sql), response generation (table-to-text) and context-aware user interaction (text-to-text)."
2020.acl-demos.24.txt,6 Conclusion and Future Work,2020,"we will continue to add more features to photon, such as voice input, spelling checking, and visualizing the output when appropriate to inspect the translation process."
2020.acl-demos.25.txt,5 Discussion and Future Work,2020,5.1 using guis for language grounding sugilite illustrates the great promise of using guis as a resource for grounding and understanding natural language instructions in itl.
2020.acl-demos.25.txt,5 Discussion and Future Work,2020,"a promising direction is to use gui references to help with repairing conversational breakdowns (beneteau et al., 2019; ashktorab et al., 2019; myers et al., 2018) caused by incorrect semantic parsing, intent classification, or entity recognition."
2020.acl-demos.25.txt,5 Discussion and Future Work,2020,"another promising approach to enable more robust natural language understanding is to leverage the pre-trained generalpurpose language models (e.g., bert (devlin et al., 2018)) to encode the user instructions and the information extracted from app guis.5.3 extracting task semantics from guis an interesting future direction is to better extract semantics from app guis so that the user can focus on high-level task specifications and personal preferences without dealing with low-level mundane details (e.g., “buy 2 burgers” means setting the value of the textbox below the text “quantity” and next to the text “burger” to be “2”)."
2020.acl-demos.25.txt,5 Discussion and Future Work,2020,"collecting and aggregating personal task instructions across many users also introduce important concerns on user privacy, as discussed in (li et al., 2020).5.4 multi-modal interactions in conversational learning sugilite combines speech and direct manipulation to enable a “speak and point” interaction style, which has been studied since early interactive systems like put-that-there (bolt, 1980)."
2020.acl-demos.25.txt,5 Discussion and Future Work,2020,"for itl, an interesting future challenge is to combine these user-independent domain-agnostic machine-learned models with the user’s personalized instructions for a specific task."
2020.acl-demos.25.txt,5 Discussion and Future Work,2020,"this could be supported by improved background knowledge and task models, and more flexible dialog frameworks that can handle the continuous refinement and uncertainty inherent in natural language interaction, and the variations in user goals."
2020.acl-demos.25.txt,5 Discussion and Future Work,2020,"this will likely require a new kind of mixedinitiative instruction (horvitz, 1999) where the agent is more proactive in guiding the user and takes more initiative in the dialog."
2020.acl-demos.25.txt,5 Discussion and Future Work,2020,we are currently exploring other ways of using multi-modal interactions to supplement natural language instructions in itl.
2020.acl-demos.25.txt,5 Discussion and Future Work,2020,we are looking at alternative approaches for parsing natural language instructions into our domainspecific language (dsl) for representing data description queries and task execution procedures.
2020.acl-demos.26.txt,7 Conclusion,2020,"for future work, we plan to keep adding the state-of-the-art algorithms, reduce latency and fine-tune the implemented models on larger and/or more comprehensive corpus to improve performance."
2020.acl-demos.26.txt,7 Conclusion,2020,"we unified these nlp algorithms in a single codebase, implemented demos as top-level managers to access different models, and provide strategies to allow more organic integration across the models."
2020.acl-demos.28.txt,6 Conclusion and Future Work,2020,"in future, we would like to extend the funlines data collection setup to a more general crowdsourcing framework, for example, to collect style transfer data."
2020.acl-demos.29.txt,6 Discussion,2020,interactive fiction is an art form with an uncertain future that is connected in no small way to its proximity to games and the social norms separating games and fine art.
2020.acl-demos.3.txt,9 Conclusions,2020,"we intend to release the code as open source, as well as providing hosted open access to a pubmed-based corpus."
2020.acl-demos.30.txt,7 Conclusion,2020,detection and control of toxic output will be a major focus of future investigation.
2020.acl-demos.30.txt,7 Conclusion,2020,"dialogpt is fully opensourced and easy to deploy, allowing users to extend the pre-trained conversational system to bootstrap training using various datasets."
2020.acl-demos.30.txt,7 Conclusion,2020,we will investigate leveraging reinforcement learning to further improve the relevance of the generated responses and prevent the model from generating egregious responses.
2020.acl-demos.31.txt,7 Conclusions,2020,"we provide a large variety of functionalities, ranging from speech processing to core dialog system capabilities and social signal processing."
2020.acl-demos.32.txt,5 Conclusion and Future Work,2020,"in future work, we plan to add more media sources, especially from non-english media and regions."
2020.acl-demos.32.txt,5 Conclusion and Future Work,2020,"we further want to extend the tool to support other propaganda techniques such as cherrypicking and omission, among others, which would require analysis beyond the text of a single article."
2020.acl-demos.32.txt,5 Conclusion and Future Work,2020,"we have made publicly available our data and models, as well as an api to the live system."
2020.acl-demos.33.txt,5 Conclusion,2020,"the dilated cnn, which is first applied to the icd coding task, aims to capture semantic information for non-continuous words, and the n-gram matching mechanism aims to capture the continuous semantic."
2020.acl-demos.33.txt,5 Conclusion,2020,we will try to utilize external resources to solve the few-shot and zero-shot problem in the future.
2020.acl-demos.34.txt,5 Conclusion,2020,"in the future, we will support more corpora and implement novel techniques to bridge the gap between end-to-end and cascaded approaches."
2020.acl-demos.35.txt,6 Conclusion,2020,"existing work on amr has targeted the penman string, the parsed tree, or the interpreted graph, and penman accommodates all of these use cases by allowing users to work with the tree or graph data structures or to encode them back to strings."
2020.acl-demos.35.txt,6 Conclusion,2020,"transformations defined at both the graph and tree level make it applicable for pre- and postprocessing steps for corpus creation, evaluation, machine learning projects, and more."
2020.acl-demos.36.txt,5 Conclusion,2020,"finally, we are aware that keyword-based boolean filtering might be prone to the same biases and challenges inherent in the traditional search queries, as discussed above."
2020.acl-demos.36.txt,5 Conclusion,2020,"in future work, we will focus on expanding the database to include additional domains and article sources."
2020.acl-demos.36.txt,5 Conclusion,2020,"we will also seek to improve discovery performance by testing more recent text embedding methods (e.g., bert (devlin et al., 2018)) and by optimizing the search for different input text lengths, such as a whole document, a paragraph, or even a single sentence."
2020.acl-demos.36.txt,5 Conclusion,2020,"we will investigate whether query expansion techniques (azad and deepak, 2019) could mitigate this issue by suggesting or automatically appending semantically related keywords to the boolean filters."
2020.acl-demos.36.txt,5 Conclusion,2020,"we will work on augmenting the workflow with automated tasks, such as suggesting references as the author writes a manuscript, or notifying users about the latest publications relevant to their work."
2020.acl-demos.37.txt,4 Conclusion,2020,"as a next step, we will improve the prototype based on the participants’ valuable feedback."
2020.acl-demos.37.txt,4 Conclusion,2020,"furthermore, an eye tracker will be integrated into the prototype that can be used in combination with speech for cursor placement, thereby simplifying multi-modal pe."
2020.acl-demos.37.txt,4 Conclusion,2020,"last, we will investigate whether using the different modalities has an impact on cognitive load during pe (herbig et al., 2019b)."
2020.acl-demos.37.txt,4 Conclusion,2020,our study with professional translators shows a high level of interest and enthusiasm about using these new modalities.
2020.acl-demos.37.txt,4 Conclusion,2020,"users can directly cross out or hand-write new text, drag and drop words for reordering, or use spoken commands to update the text in place."
2020.acl-demos.38.txt,7 Conclusion and Future Work,2020,"finally, we hope to explore further optimizations to make core algorithms competitive with highly-optimized neural network components."
2020.acl-demos.38.txt,7 Conclusion and Future Work,2020,"in addition to the problems discussed so far, torch-struct also includes several other example implementations including supervised dependency parsing with bert, unsupervised tagging, structured attention, and connectionist temporal classification (ctc) for speech."
2020.acl-demos.38.txt,7 Conclusion and Future Work,2020,"in the future, we hope to support research and production applications employing structured models."
2020.acl-demos.39.txt,5 Conclusion,2020,"using the cl machine teaching ui, the dialog author can provide corrections to the logged user-system dialogs and further improve the cl’s dm performance."
2020.acl-demos.39.txt,5 Conclusion,2020,"we are planning to extend this work by looking into following problems: 1) investigating effectiveness of different ranking algorithms for log correction recommendation, 2) optimizing number of training samples and action masks generated from the rule-based dm, and 3) improving predictions of hcn-based dm by looking into alternative network architectures."
2020.acl-demos.4.txt,4 Conclusion,2020,"as suggested by an anonymous reviewer, another possible addition to the game could then be to predict the age appropriateness of a given topic, allowing for cards to be filtered on the basis of an age setting."
2020.acl-demos.4.txt,4 Conclusion,2020,"in addition to improving the banned words selection process, future work on tabouid includes generating specific lists of cards based on school programs to use the game as an educational tool, using the category system of wikipedia to let users select more or less specific categories to play with, and adapting the algorithms to leverage the wide variety of languages wikipedia is available in beyond english and french."
2020.acl-demos.40.txt,6 Conclusion,2020,"there are many open questions which we intend to research, such as whether autoregressivity in neural sentence compression can be exploited and how to compose themes over longer time periods."
2020.acl-demos.41.txt,Conclusion,2020,we hope to encourage additional research to improve the safety and benefits of dietary supplements for their consumers.
2020.acl-demos.42.txt,7 Conclusion,2020,we hope that our work on lean-life will allow for researches and practitioners alike to more easily obtain useful labeled datasets and models for the various nlp tasks they face.
2020.acl-demos.5.txt,6 Conclusion,2020,future work include (1) expanding the database to more papers (2) improving the qa model using the collected data to better handle question answering in the context of research domain.
2020.acl-demos.8.txt,6 Conclusion,2020,the retrieved evidence sentences can be easily located in the background corpora for better visualization.
2020.acl-demos.8.txt,6 Conclusion,2020,we are further developing evidenceminer to be a more intelligent system that can assist in more efficient and in-depth scientific discoveries.
2020.acl-demos.9.txt,5 Conclusions,2020,"moving forward, we hope to refine the linking of extracted snippets to structured vocabularies to run a more comprehensive user-study to evaluate the use of the system in practice by different types of users."
2020.acl-demos.9.txt,5 Conclusions,2020,"we also hope to develop a joint extraction and inference model, rather than relying on the current pipelined approach."
2020.acl-main.1.txt,7 Discussion,2020,"in future work, we intend to curate a test set with data from separate sources, which can serve as a benchmark for the models we study."
2020.acl-main.1.txt,7 Discussion,2020,"the combined test set scores are more directly comparable, but ideally, we would like to compare the generalization of both models on an independent test set."
2020.acl-main.1.txt,7 Discussion,2020,we also intend to use tools for interpreting the knowledge encoded in neural networks (such as diagnostic classifiers and representational similarity analysis) to investigate the emergent representation of linguistic units such as phonemes and words.
2020.acl-main.1.txt,7 Discussion,2020,"we find indications that learning to extract meaning from speech is initially faster when learning from child-directed speech, but learning from adultdirected speech eventually leads to similar task performance on the training register, and better generalization to the other register."
2020.acl-main.1.txt,7 Discussion,2020,we intend to explore how a curriculum of cds followed by ads affects learning trajectories and outcomes.
2020.acl-main.100.txt,5 Conclusion,2020,"in the future, we are interested in injecting knowledge into text representation learning (cao et al., 2017, 2018b) for deeply understanding expert language, and will help to generate knowledgeenhanced questions (pan et al., 2019) for laymen."
2020.acl-main.102.txt,5 Conclusion,2020,"since dynamic memory can be a learning mechanism more general than what we have used here for fewshot learning, we will investigate this type of models in other learning problems."
2020.acl-main.103.txt,7 Conclusion and Future Work,2020,one interesting future direction is to explore whether the beam search is helpful to our model.
2020.acl-main.104.txt,6 Conclusion,2020,"furthermore, our framework is extended into a parallel variant based on multi-label attention and a serial variant of text feature propagation."
2020.acl-main.105.txt,4 Conclusion,2020,other retrieval tasks may also benefit from using keyphrase information and we expect our results to serve as a basis for further improvements.
2020.acl-main.105.txt,4 Conclusion,2020,we presented the first study of the usefulness of keyphrase generation for scientific document retrieval.
2020.acl-main.107.txt,10 Conclusion,2020,"annotated with 4 standard morphosyntactic layers, two of them following the universal dependency annotation scheme, and provided with translation to french as well as glosses and word language identification, we believe that this corpus will be useful for the community at large, both for linguistic purposes and as training data for resource-scarce nlp in a high-variability scenario."
2020.acl-main.107.txt,10 Conclusion,2020,"in addition to the annotated data, we provide around 1 million tokens (over 46k sentences) of unlabeled narabizi content, resulting in the largest dataset available for this dialect."
2020.acl-main.107.txt,10 Conclusion,2020,"more over, being made of user-generated content, this treebank covers a large variety of language variation among native speakers and displays a high level of codeswitching."
2020.acl-main.108.txt,6 Summary,2020,"we gave an overview of the topics discussed in the corpus, demonstrating that it is a valuable source for several nlp tasks, such as argument mining."
2020.acl-main.109.txt,9 Discussion,2020,"indeed, since function words in ud tend to be dependents of content words, we may analyze the former by considering the distribution of function word types that each type of content word has."
2020.acl-main.109.txt,9 Discussion,2020,"moreover, sub-typing dependency paths based on their linear direction can allow investigating word-order differences.11 other than informing the development of cross-lingual transfer learning, our analysis directly supports the validation of ud annotation."
2020.acl-main.109.txt,9 Discussion,2020,our method can be used to detect and bridge such inconsistencies.