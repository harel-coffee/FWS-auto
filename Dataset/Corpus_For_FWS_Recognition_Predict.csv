id,year,chapter,text
2020.acl-demos.1.txt,2020,7 Conclusion,another direction for improvement is to further enhance the ability to interact with users via a conversation interface.
2020.acl-demos.1.txt,2020,7 Conclusion,"finally, the system produces the video of an animated avatar reading the news with synthesized voice."
2020.acl-demos.1.txt,2020,7 Conclusion,"first, it learns how to write news articles based on a template based table2text technology, and summarize the news through an extraction based method."
2020.acl-demos.1.txt,2020,7 Conclusion,"in this paper, we present xiaomingbot, a multilingual and multi-modal system for news reporting."
2020.acl-demos.1.txt,2020,7 Conclusion,"next, its system translates the summarization into multiple languages."
2020.acl-demos.1.txt,2020,7 Conclusion,"one such important direction for future improvement is the expansion of areas that it can work in, which can be achieved through a promising approach of adopting model based technologies together with rule/template based ones."
2020.acl-demos.1.txt,2020,7 Conclusion,"owing to the voice cloning model that can learn from a few chinese audio samples, xiaomingbot can maintain consistency in intonation and voice projection across different languages."
2020.acl-demos.1.txt,2020,7 Conclusion,"so far, xiaomingbot has been deployed online and is serving users."
2020.acl-demos.1.txt,2020,7 Conclusion,the entire process of xiaomingbot’s news reporting can be condensed as follows.
2020.acl-demos.1.txt,2020,7 Conclusion,"the system is but a first attempt to build a fully functional robot reporter capable of writing, speaking, and expressing with motion."
2020.acl-demos.1.txt,2020,7 Conclusion,"xiaomingbot is not yet perfect, and has limitations and room for improvement."
2020.acl-demos.10.txt,2020,6 Conclusion,"in particular, we plan to incorporate human performance as a reference metric, integrating psycholinguistic experimental results and supporting easy experimental design starting from the test suite format."
2020.acl-demos.10.txt,2020,6 Conclusion,"syntaxgym is continually evolving: we plan to add new features to the site, and to develop further in response to user feedback."
2020.acl-demos.10.txt,2020,6 Conclusion,syntaxgym promises to advance the progress of language model evaluation by uniting the theoretical expertise of linguists with the technical skills of nlp researchers.
2020.acl-demos.10.txt,2020,6 Conclusion,"the site is fully functional at syntaxgym.org, and the entire framework is available as open-source code."
2020.acl-demos.10.txt,2020,6 Conclusion,"this paper presented syntaxgym, an online platform and open-source framework for targeted syntactic evaluation of neural network language models."
2020.acl-demos.10.txt,2020,6 Conclusion,"we also plan to further incorporate language models into the lm-zoo tool, allowing broader access to state-of-the-art language models in general."
2020.acl-demos.10.txt,2020,6 Conclusion,"we welcome open-source contributions to the website and to the general framework, and especially encourage the nlp community to contribute their models to the lm-zoo repository."
2020.acl-demos.11.txt,2020,8 Conclusion,"this system enables the user to readily search a knowledge network of extracted, linked, and summarized complex events from multimedia, multilingual sources (e.g., text, images, videos, speech and ocr)."
2020.acl-demos.11.txt,2020,8 Conclusion,we demonstrate a state-of-the-art multimedia multilingual knowledge extraction and event recommendation system.
2020.acl-demos.12.txt,2020,8 Conclusion,easy-to-use retrieval focused multilingual models for embedding sentence-length text are made available on tensorflow hub.
2020.acl-demos.12.txt,2020,8 Conclusion,"monolingual transfer task performance approaches, and in some cases exceeds, english only sentence embedding models."
2020.acl-demos.12.txt,2020,8 Conclusion,our models are freely available under an apache license with additional documentation and tutorial colaboratory notebooks at: https://tfhub.dev/s?q=universalsentence-encoder-multilingual
2020.acl-demos.12.txt,2020,8 Conclusion,our models embed text from 16 languages into a shared semantic embedding space and achieve a new state-of-the-art in performance on monolingual and cross-lingual semantic retrieval (sr).
2020.acl-demos.12.txt,2020,8 Conclusion,the models achieve good performance on the related tasks of translation pair bitext retrieval (br) and retrieval question answering (reqa).
2020.acl-demos.13.txt,2020,5 Conclusion,a demo of our platform is available at bio-nlp.org/bentodemo/.
2020.acl-demos.13.txt,2020,5 Conclusion,bento includes a number of clinical nlp tools to facilitate the process of ehr notes.
2020.acl-demos.13.txt,2020,5 Conclusion,computational pipelines can be configured through a web-based user-interface and then automatically executed on codalab.
2020.acl-demos.13.txt,2020,5 Conclusion,"in this paper, we have described the design of the workflow management platform bento."
2020.acl-demos.13.txt,2020,5 Conclusion,"to the best of our knowledge, bento represents the first web-based workflow management platform for nlp research."
2020.acl-demos.13.txt,2020,5 Conclusion,"using bento, researchers can make use of existing tools or define their own tools."
2020.acl-demos.14.txt,2020,5 Conclusion and Future Work,"for future work, we consider the following areas of improvement in the near term: • models downloadable in sta n z a are largely trained on a single dataset."
2020.acl-demos.14.txt,2020,5 Conclusion and Future Work,"simultaneously, sta n z a ’s corenlp client extends its functionality with additional nlp tools."
2020.acl-demos.14.txt,2020,5 Conclusion and Future Work,"to make models robust to many different genres of text, we would like to investigate the possibility of pooling various sources of compatible data to train “default” models for each language; • the amount of computation and resources available to us is limited."
2020.acl-demos.14.txt,2020,5 Conclusion and Future Work,"we have showed that sta n z a ’s neural pipeline not only has wide coverage of human languages, but also is accurate on all tasks, thanks to its language-agnostic, fully neural architectural design."
2020.acl-demos.14.txt,2020,5 Conclusion and Future Work,"we introduced sta n z a , a python natural language processing toolkit supporting many human languages."
2020.acl-demos.14.txt,2020,5 Conclusion and Future Work,"we would like to further investigate reducing model sizes and speeding up computation in the toolkit, while still maintaining the same level of accuracy.• we would also like to expand sta n z a ’s functionality by adding other processors such as neural coreference resolution or relation extraction for richer text analytics."
2020.acl-demos.14.txt,2020,5 Conclusion and Future Work,"we would therefore like to build an open “model zoo” for sta n z a , so that researchers from outside our group can also contribute their models and benefit from models released by others; • sta n z a was designed to optimize for accuracy of its predictions, but this sometimes comes at the cost of computational efficiency and limits the toolkit’s use."
2020.acl-demos.15.txt,2020,5 Conclusion,"further, jiant is shown to be able to replicate published performance on various nlu tasks.jiant’s modular design of task and sentence encoder components make it possible for users to quickly and easily experiment with a large number of tasks, models, and parameter configurations, without editing source code.jiant’s design also makes it easy to add new tasks, and jiant’s architecture makes it convenient to extend jiant to support new sentence encoders.jiant code is open source, and jiant invites contributors to open issues or submit pull request to the jiant project repository: https://github.com/nyu-mll/jiant."
2020.acl-demos.15.txt,2020,5 Conclusion,"jiant provides a configuration-driven interface for defining transfer learning and representation learning experiments using a bank of over 50 nlu tasks, cutting-edge sentence encoder models, and multi-task and multi-stage training procedures."
2020.acl-demos.16.txt,2020,4 Conclusion,its key features are: 1) support for robust and transferable learning using adversarial multi-task learning paradigm; 2) enable knowledge distillation under the multi-task learning setting which can be leveraged to derive lighter models for efficient online deployment.
2020.acl-demos.16.txt,2020,4 Conclusion,microsoft mt-dnn is an open-source natural language understanding toolkit which facilitates researchers and developers to build customized deep learning models.
2020.acl-demos.16.txt,2020,4 Conclusion,"we will extend mt-dnn to support natural language generation tasks, e.g. question generation, and incorporate more pre-trained encoders, e.g. t5 (raffel et al., 2019) in future."
2020.acl-demos.17.txt,2020,6 Conclusion and Future Work,"an interesting direction to explore is re-ranking corrective suggestions, so that the suggestion more relevant to the original sentence goes to the top."
2020.acl-demos.17.txt,2020,6 Conclusion and Future Work,experiments show that the proposed label schema achieves comparable performance (on binary task) while providing more informative feedback.
2020.acl-demos.17.txt,2020,6 Conclusion and Future Work,"finally, our system currently providing additional chinese translations for english examples."
2020.acl-demos.17.txt,2020,6 Conclusion and Future Work,"for example, the method for introducing additional training data or generating artificial training data could be implemented to improve the performance."
2020.acl-demos.17.txt,2020,6 Conclusion and Future Work,"for the ged task, we proposed a new label schema, dirc."
2020.acl-demos.17.txt,2020,6 Conclusion and Future Work,"for the interactive writing task, we provide grammatical suggestions, collocations, and bilingual examples, to guide the user towards writing fluently."
2020.acl-demos.17.txt,2020,6 Conclusion and Future Work,"in addition, we leverage an existing linguistic search engine to provide corrective suggestions for each error type."
2020.acl-demos.17.txt,2020,6 Conclusion and Future Work,"in summary, we have presented an writing environment that supports interactive writing suggestions, scoring, error detection and corrective feedback."
2020.acl-demos.17.txt,2020,6 Conclusion and Future Work,many avenues exist for future research and improvement of our system.
2020.acl-demos.17.txt,2020,6 Conclusion and Future Work,obviously we could easily provide languages translations by changing a bilingual dictionary.
2020.acl-demos.17.txt,2020,6 Conclusion and Future Work,yet another direction of research would be to detect fine-grained error types.
2020.acl-demos.18.txt,2020,5 Conclusions,"our case study on the wmt2019 metrics shared task further highlights the potential of clir as a proxy task for mt evaluation, and we hope that clireval can facilitate future research in this area."
2020.acl-demos.18.txt,2020,5 Conclusions,"rather than directly evaluating translated sentences against reference sentences, clireval transforms the inputs into the closely related task of clir, without the need for annotated clir dataset."
2020.acl-demos.18.txt,2020,5 Conclusions,"the aim of this project is not to replace current automatic evaluation metrics or fix the limitations in those metrics, but to bridge the gap between machine translation and cross-lingual information retrieval and to show that clir is a feasible proxy task for mt evaluation."
2020.acl-demos.18.txt,2020,5 Conclusions,"we present clireval, an open-source python-based evaluation toolkit for machine translation."
2020.acl-demos.19.txt,2020,5 Conclusion,"based on convlab (lee et al., 2019b), convlab-2 integrates more powerful models, supports more datasets, and develops an analysis tool and an interactive tool for comprehensive end-to-end evaluation."
2020.acl-demos.19.txt,2020,5 Conclusion,"for demonstration, we give an example of using convlab-2 to build, evaluate, and diagnose a system on the multiwoz dataset."
2020.acl-demos.19.txt,2020,5 Conclusion,we hope that convlab-2 is instrumental in promoting the research on task-oriented dialogue.
2020.acl-demos.19.txt,2020,5 Conclusion,"we present convlab-2, an open-source toolkit for building, evaluating, and diagnosing a taskoriented dialogue system."
2020.acl-demos.2.txt,2020,6 Conclusion and Future Work,"apart from the distillation strategies, the model structure also affects the performance."
2020.acl-demos.2.txt,2020,6 Conclusion and Future Work,"for example, its usability in generation tasks such as machine translation has not been tested."
2020.acl-demos.2.txt,2020,6 Conclusion and Future Work,"in the future, we aim to integrate neural architecture search into the toolkit to automate the searching for model structures."
2020.acl-demos.2.txt,2020,6 Conclusion and Future Work,"in this paper, we present textbrewer, a flexible pytorch-based distillation toolkit for nlp research and applications."
2020.acl-demos.2.txt,2020,6 Conclusion and Future Work,textbrewer also has its limitations.
2020.acl-demos.2.txt,2020,6 Conclusion and Future Work,textbrewer provides rich customization options for users to compare different distillation methods and build their strategies.
2020.acl-demos.2.txt,2020,6 Conclusion and Future Work,the results show that the distilled models can achieve state-of-the-art results with simple settings.
2020.acl-demos.2.txt,2020,6 Conclusion and Future Work,we have conducted a series of experiments.
2020.acl-demos.2.txt,2020,6 Conclusion and Future Work,we will keep adding more examples and tests to expand textbrewer’s scope of application.
2020.acl-demos.20.txt,2020,4 Conclusions and Future Work,"additionally, we would like to explore opusfilter’s use in different scenarios and for other language pairs."
2020.acl-demos.20.txt,2020,4 Conclusions and Future Work,especially interesting would be the application in low-resource settings and various levels of noise in the original data.
2020.acl-demos.20.txt,2020,4 Conclusions and Future Work,"furthermore, the use for domain adaptation and data selection should be further explored."
2020.acl-demos.20.txt,2020,4 Conclusions and Future Work,"in future work, we would like to extend the toolbox with additional filters and classification options."
2020.acl-demos.20.txt,2020,4 Conclusions and Future Work,"one option could be the inclusion of sentence embedding based filtering (guo et al., 2018)."
2020.acl-demos.20.txt,2020,4 Conclusions and Future Work,opusfilter can easily be configured to work with opus data and various filters to train effective classifiers in order to rank bitext segments.
2020.acl-demos.20.txt,2020,4 Conclusions and Future Work,opusfilter is open source and distributed with a permissive license to make it widely applicable.
2020.acl-demos.20.txt,2020,4 Conclusions and Future Work,"the classifiers can be trained without human annotation, and the automatic model selection methods implemented in the toolbox lead to a similar performance compared to classifiers based on small manually labeled validation data."
2020.acl-demos.20.txt,2020,4 Conclusions and Future Work,"this paper introduces opusfilter, a modular tool for parallel data selection and ranking."
2020.acl-demos.20.txt,2020,4 Conclusions and Future Work,we demonstrate its use in a finnish-english translation task based on the noisy paracrawl data used for training.
2020.acl-demos.21.txt,2020,7 Conclusion,"although nlp practitioners know that label noise harms performance, and noise detection algorithms have long been available, this technology is not being applied in practice, perhaps because human review of detected errors is difﬁcult and time consuming."
2020.acl-demos.21.txt,2020,7 Conclusion,"and by providing an explanation of why the model ﬂagged an example as suspicious, it makes the output of label noise detectors understandable and actionable."
2020.acl-demos.21.txt,2020,7 Conclusion,"it reduces the number of false positive examples that the reviewer must look at, providing state-of-the-art precision and f0.5 across several short text datasets."
2020.acl-demos.21.txt,2020,7 Conclusion,lnic makes human review of possible label noise easier and more efﬁcient.
2020.acl-demos.22.txt,2020,6 Discussion,"however, exbert can effectively narrow the scope and refine hypotheses through quick testing iterations."
2020.acl-demos.22.txt,2020,6 Discussion,"in this paper, we introduced an interactive visualization tool, exbert, that can reveal an intelligible structure in the learned representations of transformer models."
2020.acl-demos.22.txt,2020,6 Discussion,"these hypotheses about the model behavior can, in a later step, be evaluated by robust statistical tests on a global level."
2020.acl-demos.22.txt,2020,6 Discussion,these neighbors do not necessarily reveal a head’s or an embedding’s global behavior.
2020.acl-demos.22.txt,2020,6 Discussion,"to assist researchers with their model investigations, we host a demo of the tool with multiple models at exbert.net."
2020.acl-demos.22.txt,2020,6 Discussion,we acknowledge that exbert is limited compared to more global analyses since it only presents statistics across a small number of neighbors for a single token at a time.
2020.acl-demos.22.txt,2020,6 Discussion,"we demonstrated, through an attention visualization and nearest neighbor searching techniques, that exbert can replicate research that explores what attentions and representations learn and detect biases in text inputs."
2020.acl-demos.23.txt,2020,9 Conclusion,"it is powered by a combination of advanced machine learning and manually curated linguistic resources, and thus succeeds in setting a new state of the art for hebrew diacritization."
2020.acl-demos.23.txt,2020,9 Conclusion,we are pleased to release our hebrew diacritization system for free unrestricted use.
2020.acl-demos.23.txt,2020,9 Conclusion,we have released also our diacritized test corpora for benchmarking.
2020.acl-demos.24.txt,2020,6 Conclusion and Future Work,a comprehensive evaluation will also be conducted among the users of our system.
2020.acl-demos.24.txt,2020,6 Conclusion and Future Work,"it is the first cross-domain text-to-sql system designed towards industrial applications with rich features, and bridges the demand of sophisticated database analysis and people without any sql background knowledge."
2020.acl-demos.24.txt,2020,6 Conclusion and Future Work,photon has the potential to scale up to hundreds of different domains.
2020.acl-demos.24.txt,2020,6 Conclusion and Future Work,"the current photon system is still a prototype, with very limited user interactions and functions."
2020.acl-demos.24.txt,2020,6 Conclusion and Future Work,"we also plan to improve the performance of core models in photon, such as semantic parsing (text-to-sql), response generation (table-to-text) and context-aware user interaction (text-to-text)."
2020.acl-demos.24.txt,2020,6 Conclusion and Future Work,"we present photon, a robust modular cross-domain text-to-sql system, consisting of semantic parser, untranslatable question detector, human-in-the-loop question corrector, and natural language response generator."
2020.acl-demos.24.txt,2020,6 Conclusion and Future Work,"we will continue to add more features to photon, such as voice input, spelling checking, and visualizing the output when appropriate to inspect the translation process."
2020.acl-demos.25.txt,2020,5 Discussion and Future Work,5.1 using guis for language grounding sugilite illustrates the great promise of using guis as a resource for grounding and understanding natural language instructions in itl.
2020.acl-demos.25.txt,2020,5 Discussion and Future Work,"a major challenge in natural language instruction is that the users do not know what concepts or knowledge the agent already knows so that they can use it in their instructions (li et al., 2019)."
2020.acl-demos.25.txt,2020,5 Discussion and Future Work,"a promising direction is to use gui references to help with repairing conversational breakdowns (beneteau et al., 2019; ashktorab et al., 2019; myers et al., 2018) caused by incorrect semantic parsing, intent classification, or entity recognition."
2020.acl-demos.25.txt,2020,5 Discussion and Future Work,"a promising strategy is to take advantage of the abstract syntax tree (ast) structure in our dsl for constructing a neural parser (xu et al., 2020; yin and neubig, 2017), which reduces the amount of training data needed and enforces the well-formedness of the output code."
2020.acl-demos.25.txt,2020,5 Discussion and Future Work,"another promising approach to enable more robust natural language understanding is to leverage the pre-trained generalpurpose language models (e.g., bert (devlin et al., 2018)) to encode the user instructions and the information extracted from app guis.5.3 extracting task semantics from guis an interesting future direction is to better extract semantics from app guis so that the user can focus on high-level task specifications and personal preferences without dealing with low-level mundane details (e.g., “buy 2 burgers” means setting the value of the textbox below the text “quantity” and next to the text “burger” to be “2”)."
2020.acl-demos.25.txt,2020,5 Discussion and Future Work,"as described in section 3.2, a key pattern used in sugilite’s multi-modal interface is mutual disambiguation (oviatt, 1999) where it utilizes inputs in complementary modalities to infer robust and generalizable scripts that can accurately represent user intentions."
2020.acl-demos.25.txt,2020,5 Discussion and Future Work,"by using the app guis as the medium, the system can effectively constrain the users to refer to things that can be found out from some app guis (e.g., “hot” can mean “the temperature is high”), which mostly overlaps with the “capability ceiling” of smartphone-based agents, and allows the users to define new concepts for the agent by referring to app guis (li et al., 2017a, 2019).5.2 more robust natural language understanding the current version of sugilite uses a grammar-based executable semantic parser to understand the users’ natural language explanations of their intents for the demonstrated actions."
2020.acl-demos.25.txt,2020,5 Discussion and Future Work,"collecting and aggregating personal task instructions across many users also introduce important concerns on user privacy, as discussed in (li et al., 2020).5.4 multi-modal interactions in conversational learning sugilite combines speech and direct manipulation to enable a “speak and point” interaction style, which has been studied since early interactive systems like put-that-there (bolt, 1980)."
2020.acl-demos.25.txt,2020,5 Discussion and Future Work,"for itl, an interesting future challenge is to combine these user-independent domain-agnostic machine-learned models with the user’s personalized instructions for a specific task."
2020.acl-demos.25.txt,2020,5 Discussion and Future Work,"recent reinforcement learning-based approaches and semantic parsing techniques have also shown promising results in learning models for navigating through guis for user-specified task objectives (liu et al., 2018a; pasupat et al., 2018)."
2020.acl-demos.25.txt,2020,5 Discussion and Future Work,"since guis encapsulate rich semantic information about the users’ intents, the task flows, and the task constraints, we can potentially ask the users to point to the relevant gui screens as a part of the error handling process, explaining the errors with references to the guis, and helping the system recover from the breakdowns."
2020.acl-demos.25.txt,2020,5 Discussion and Future Work,"some works have made early progress in this domain (liu et al., 2018b; deka et al., 2016; chen et al., 2020) thanks to the availability of large datasets of guis like rico (deka et al., 2017)."
2020.acl-demos.25.txt,2020,5 Discussion and Future Work,the current model also only uses the semantic information from the local user instructions and their corresponding app guis.
2020.acl-demos.25.txt,2020,5 Discussion and Future Work,"the guis encapsulate rich knowledge about the flows of the underlying tasks and the properties and relations of relevant entities, so they can be used to bootstrap the domain-specific knowledge needed by itl systems that rely on natural language instructions for learning."
2020.acl-demos.25.txt,2020,5 Discussion and Future Work,"therefore, they often introduce additional unknown concepts that are either unnecessary or entirely beyond the capability of the agent (e.g., explaining “hot” as “when i’m sweating” when teaching the agent to “open the window when it is hot”)."
2020.acl-demos.25.txt,2020,5 Discussion and Future Work,"this could be supported by improved background knowledge and task models, and more flexible dialog frameworks that can handle the continuous refinement and uncertainty inherent in natural language interaction, and the variations in user goals."
2020.acl-demos.25.txt,2020,5 Discussion and Future Work,"this will likely require a new kind of mixedinitiative instruction (horvitz, 1999) where the agent is more proactive in guiding the user and takes more initiative in the dialog."
2020.acl-demos.25.txt,2020,5 Discussion and Future Work,"users are also familiar with guis, which makes guis the ideal medium to which users can refer during task instructions."
2020.acl-demos.25.txt,2020,5 Discussion and Future Work,we are currently exploring other ways of using multi-modal interactions to supplement natural language instructions in itl.
2020.acl-demos.25.txt,2020,5 Discussion and Future Work,we are looking at alternative approaches for parsing natural language instructions into our domainspecific language (dsl) for representing data description queries and task execution procedures.
2020.acl-demos.25.txt,2020,5 Discussion and Future Work,"while this approach comes with many benefits, such as only requiring a small amount of training data and not relying on any domain knowledge, it has rigid patterns and therefore sometimes encounters problems with the flexible structures and varied expressions in the user utterances."
2020.acl-demos.26.txt,2020,7 Conclusion,"for future work, we plan to keep adding the state-of-the-art algorithms, reduce latency and fine-tune the implemented models on larger and/or more comprehensive corpus to improve performance."
2020.acl-demos.26.txt,2020,7 Conclusion,mixingboard is a new open-source platform to organically integrate multiple state-of-the-art nlp algorithms to build demo quickly with user friendly interface.
2020.acl-demos.26.txt,2020,7 Conclusion,we provide the component to retrieve knowledge passage on-the-fly from web or customized document for grounded text generation.
2020.acl-demos.26.txt,2020,7 Conclusion,"we unified these nlp algorithms in a single codebase, implemented demos as top-level managers to access different models, and provide strategies to allow more organic integration across the models."
2020.acl-demos.27.txt,2020,6 Conclusions and Future Work,"all of the data and interactive visualizations associated with this work are freely available through the project homepage.13 future work will provide additional functionalities such as search within abstracts and whole texts, document clustering, and automatically identifying related papers."
2020.acl-demos.27.txt,2020,6 Conclusions and Future Work,it includes several interconnected interactive visualizations (dashboards) that allow users to quickly and efficiently search for relevant related work by clicking on items within a visualization or through search boxes.
2020.acl-demos.27.txt,2020,6 Conclusions and Future Work,"notably, the tool also has access to citation information from google scholar."
2020.acl-demos.27.txt,2020,6 Conclusions and Future Work,we also note that the approach presented here is not required to be applied only to the acl anthology or nlp papers; it can be used to display papers from other sources too such as pre-print archives and anthologies of papers from other fields of study.
2020.acl-demos.27.txt,2020,6 Conclusions and Future Work,we presented nlp scholar—an interactive visual explorer for the acl anthology.
2020.acl-demos.27.txt,2020,6 Conclusions and Future Work,"we see nlp scholar, with its dedicated visual search capabilities for nlp papers, as a useful complementary tool to existing resources such as google scholar."
2020.acl-demos.28.txt,2020,6 Conclusion and Future Work,"compared to our earlier work for gathering the same type of data from only turkers, funlines produced funnier headlines with better rating agreement and at nearly 60% lower cost per headline."
2020.acl-demos.28.txt,2020,6 Conclusion and Future Work,funlines is an online game for generating funny headlines.
2020.acl-demos.28.txt,2020,6 Conclusion and Future Work,"furthermore, this data has already found use in a recent semeval shared task on humor recognition (hossain et al., 2020)3."
2020.acl-demos.28.txt,2020,6 Conclusion and Future Work,"having built models for detecting humor, we see this data as the foundation of the automatic creation of humorous headlines in a generate-and-test approach."
2020.acl-demos.28.txt,2020,6 Conclusion and Future Work,"in future, we would like to extend the funlines data collection setup to a more general crowdsourcing framework, for example, to collect style transfer data."
2020.acl-demos.28.txt,2020,6 Conclusion and Future Work,"more generally, funlines is a prototype for gathering datasets of creative artifacts from people that is engaging, interactive, competitive, rewarding, educational and inexpensive."
2020.acl-demos.28.txt,2020,6 Conclusion and Future Work,"our deployment attracted 290 players for a total cost of us$ 1,000."
2020.acl-demos.28.txt,2020,6 Conclusion and Future Work,"we described the game, including a rich set of feedback for players to assess their own performance along with controls and incentives for them to create funny headlines."
2020.acl-demos.28.txt,2020,6 Conclusion and Future Work,"we showed how players’ performance improves over time, both in terms of their headline quality and rating consistency."
2020.acl-demos.28.txt,2020,6 Conclusion and Future Work,we showed how the funlines data is effective for training machine learning models to detect and to rate humorous headlines.
2020.acl-demos.28.txt,2020,6 Conclusion and Future Work,"while creative data can be difficult to obtain, funlines makes it easier by taking advantage of the inherent fun of creativity and competition."
2020.acl-demos.29.txt,2020,6 Discussion,"furthermore, its use of text based exemplars in a non-parametric model with a pretrained semantic kernel permits the iterative tuning of the semantic parsing system by an author with no programming or machine learning background."
2020.acl-demos.29.txt,2020,6 Discussion,interactive fiction is an art form with an uncertain future that is connected in no small way to its proximity to games and the social norms separating games and fine art.
2020.acl-demos.29.txt,2020,6 Discussion,"our model specification makes use of semantic matching based predicates to traverse a directed graph, tracing out a “reading” of the piece."
2020.acl-demos.29.txt,2020,6 Discussion,"our tool is deployed as an appengine app, is written in angular, and is open source 6."
2020.acl-demos.29.txt,2020,6 Discussion,the use of semantic matching allows active reader ideation of their role while remaining within guard rails that maintain narrative cohesion.
2020.acl-demos.29.txt,2020,6 Discussion,they perhaps did not expect that the question would remain unanswered for thirty years.
2020.acl-demos.29.txt,2020,6 Discussion,"we hope foremost that authors will enjoy using our tool to create something they care about, and that readers will enjoy their creations."
2020.acl-demos.29.txt,2020,6 Discussion,"we present a flexible model specification for a new flavor of interactive fiction inspired by recent trends in retrieval based dialog systems, and provide an accompanying authorship tool."
2020.acl-demos.29.txt,2020,6 Discussion,"ziegfeld (1989) muses that if may either be like american poetry waiting for its walt whitman, or like the cutup poetry fad of the beat poets, bound for obscurity."
2020.acl-demos.3.txt,2020,9 Conclusions,a search interface over wikipedia sentences is available at https://allenai.github.io/ spike/.
2020.acl-demos.3.txt,2020,9 Conclusions,"we intend to release the code as open source, as well as providing hosted open access to a pubmed-based corpus."
2020.acl-demos.3.txt,2020,9 Conclusions,"we introduce a simple query language that allows to pose complex syntax-based queries, and obtain results in an interactive speed."
2020.acl-demos.30.txt,2020,7 Conclusion,detection and control of toxic output will be a major focus of future investigation.
2020.acl-demos.30.txt,2020,7 Conclusion,"dialogpt is fully opensourced and easy to deploy, allowing users to extend the pre-trained conversational system to bootstrap training using various datasets."
2020.acl-demos.30.txt,2020,7 Conclusion,it serves as a building block to novel applications and methodologies.
2020.acl-demos.30.txt,2020,7 Conclusion,the package consists of a distributed training pipeline and several pre-trained models that can be fine-tuned to obtain a conversation model on a moderately-sized customized dataset in few hours.
2020.acl-demos.30.txt,2020,7 Conclusion,"we have released an open-domain pre-trained model, dialogpt, trained on massive real-world reddit dataset."
2020.acl-demos.30.txt,2020,7 Conclusion,we will investigate leveraging reinforcement learning to further improve the relevance of the generated responses and prevent the model from generating egregious responses.
2020.acl-demos.31.txt,2020,7 Conclusions,"we introduce adviser – an open-source, multi-domain dialog system toolkit that allows users to easily develop multi-modal and socially-engaged conversational agents."
2020.acl-demos.31.txt,2020,7 Conclusions,"we provide a large variety of functionalities, ranging from speech processing to core dialog system capabilities and social signal processing."
2020.acl-demos.31.txt,2020,7 Conclusions,"with this toolkit, we hope to provide a flexible platform for collaborative research in multi-domain, multi-modal, socially-engaged conversational agents."
2020.acl-demos.32.txt,2020,5 Conclusion and Future Work,"in future work, we plan to add more media sources, especially from non-english media and regions."
2020.acl-demos.32.txt,2020,5 Conclusion and Future Work,the system also allows users to analyze their own text or the contents of a url of interest.
2020.acl-demos.32.txt,2020,5 Conclusion and Future Work,"the system further shows aggregated statistics about the use of such techniques in articles filtered according to several criteria, including date ranges, media sources, bias of the sources, and keyword searches."
2020.acl-demos.32.txt,2020,5 Conclusion and Future Work,"we further want to extend the tool to support other propaganda techniques such as cherrypicking and omission, among others, which would require analysis beyond the text of a single article."
2020.acl-demos.32.txt,2020,5 Conclusion and Future Work,"we have made publicly available our data and models, as well as an api to the live system."
2020.acl-demos.32.txt,2020,5 Conclusion and Future Work,we have presented the prta system for detecting and highlighting the use of propaganda techniques in online news.
2020.acl-demos.32.txt,2020,5 Conclusion and Future Work,"we hope that the prta system would help raise awareness about the use of propaganda techniques in the news, thus promoting media literacy and critical thinking, which are arguably the best longterm answer to “fake news” and disinformation."
2020.acl-demos.33.txt,2020,5 Conclusion,"in this paper, we propose a dilated convolutional attention network with n-gram matching mechanism (dcanm) for automatic icd coding."
2020.acl-demos.33.txt,2020,5 Conclusion,"moreover, we develop an openaccess system to help users assign icd codes."
2020.acl-demos.33.txt,2020,5 Conclusion,"the dilated cnn, which is first applied to the icd coding task, aims to capture semantic information for non-continuous words, and the n-gram matching mechanism aims to capture the continuous semantic."
2020.acl-demos.33.txt,2020,5 Conclusion,they both provide a pretty good interpretability for prediction.
2020.acl-demos.33.txt,2020,5 Conclusion,we will try to utilize external resources to solve the few-shot and zero-shot problem in the future.
2020.acl-demos.34.txt,2020,5 Conclusion,"in the future, we will support more corpora and implement novel techniques to bridge the gap between end-to-end and cascaded approaches."
2020.acl-demos.34.txt,2020,5 Conclusion,we presented espnet-st for the fast development of end-to-end and cascaded st systems.
2020.acl-demos.34.txt,2020,5 Conclusion,"we provide various all-in-one example scripts containing corpus-dependent pre-processing, feature extraction, training, and inference."
2020.acl-demos.35.txt,2020,6 Conclusion,"existing work on amr has targeted the penman string, the parsed tree, or the interpreted graph, and penman accommodates all of these use cases by allowing users to work with the tree or graph data structures or to encode them back to strings."
2020.acl-demos.35.txt,2020,6 Conclusion,"in this paper i have presented penman, a python library and command-line tool for working with amr and other graphs serialized in the penman format."
2020.acl-demos.35.txt,2020,6 Conclusion,interactive notebook demonstrations and informational videos are available at https://github.com/goodmami/penman#demo.
2020.acl-demos.35.txt,2020,6 Conclusion,penman is available under the mit open-source license at https://github.com/goodmami/penman.
2020.acl-demos.35.txt,2020,6 Conclusion,"transformations defined at both the graph and tree level make it applicable for pre- and postprocessing steps for corpus creation, evaluation, machine learning projects, and more."
2020.acl-demos.36.txt,2020,5 Conclusion,"finally, we are aware that keyword-based boolean filtering might be prone to the same biases and challenges inherent in the traditional search queries, as discussed above."
2020.acl-demos.36.txt,2020,5 Conclusion,"in future work, we will focus on expanding the database to include additional domains and article sources."
2020.acl-demos.36.txt,2020,5 Conclusion,our application combines the processes of reading papers and of writing scientific manuscripts into a single user interface and links them using nlp algorithms.
2020.acl-demos.36.txt,2020,5 Conclusion,we have described an application that aims to reduce the manual workload involved in exploring the scientific literature.
2020.acl-demos.36.txt,2020,5 Conclusion,"we will also seek to improve discovery performance by testing more recent text embedding methods (e.g., bert (devlin et al., 2018)) and by optimizing the search for different input text lengths, such as a whole document, a paragraph, or even a single sentence."
2020.acl-demos.36.txt,2020,5 Conclusion,"we will investigate whether query expansion techniques (azad and deepak, 2019) could mitigate this issue by suggesting or automatically appending semantically related keywords to the boolean filters."
2020.acl-demos.36.txt,2020,5 Conclusion,"we will work on augmenting the workflow with automated tasks, such as suggesting references as the author writes a manuscript, or notifying users about the latest publications relevant to their work."
2020.acl-demos.37.txt,2020,4 Conclusion,"as a next step, we will improve the prototype based on the participants’ valuable feedback."
2020.acl-demos.37.txt,2020,4 Conclusion,"for insertions and replacements, speech and multi-modal interaction were seen as suitable interaction modes; however, mouse & keyboard were still favored and faster."
2020.acl-demos.37.txt,2020,4 Conclusion,"furthermore, an eye tracker will be integrated into the prototype that can be used in combination with speech for cursor placement, thereby simplifying multi-modal pe."
2020.acl-demos.37.txt,2020,4 Conclusion,"last, we will investigate whether using the different modalities has an impact on cognitive load during pe (herbig et al., 2019b)."
2020.acl-demos.37.txt,2020,4 Conclusion,our study with professional translators shows a high level of interest and enthusiasm about using these new modalities.
2020.acl-demos.37.txt,2020,4 Conclusion,"this paper therefore presents mmpe, a cat prototype combining pen, touch, speech, and multi-modal interaction together with common mouse and keyboard input possibilities."
2020.acl-demos.37.txt,2020,4 Conclusion,"users can directly cross out or hand-write new text, drag and drop words for reordering, or use spoken commands to update the text in place."
2020.acl-demos.37.txt,2020,4 Conclusion,"while more and more professional translators are switching to the use of pe to increase productivity and reduce errors, current cat interfaces still heavily focus on traditional mouse and keyboard input."
2020.acl-demos.38.txt,2020,7 Conclusion and Future Work,"code demonstrates that the model is able to replicate standard deep learning results, although we focus here on the fidelity and implementation approach of the core library."
2020.acl-demos.38.txt,2020,7 Conclusion and Future Work,"finally, we hope to explore further optimizations to make core algorithms competitive with highly-optimized neural network components."
2020.acl-demos.38.txt,2020,7 Conclusion and Future Work,"in addition to the problems discussed so far, torch-struct also includes several other example implementations including supervised dependency parsing with bert, unsupervised tagging, structured attention, and connectionist temporal classification (ctc) for speech."
2020.acl-demos.38.txt,2020,7 Conclusion and Future Work,"in the future, we hope to support research and production applications employing structured models."
2020.acl-demos.38.txt,2020,7 Conclusion and Future Work,the full library is available at https: //github.com/harvardnlp/pytorch-struct.
2020.acl-demos.38.txt,2020,7 Conclusion and Future Work,"the library achieves modularity through its adoption of a generic distributional api, completeness by utilizing crfs and semirings to make it easy to add new algorithms, and efficiency through core optimizations to vectorize important dynamic programming steps."
2020.acl-demos.38.txt,2020,7 Conclusion and Future Work,these approaches provide a benchmark for improving autodifferentiation systems and extending their functionality to higher-order properties.
2020.acl-demos.38.txt,2020,7 Conclusion and Future Work,"we also believe the library provides a strong foundation for building generic tools for interpretablity, control, and visualization through its probabilistic api."
2020.acl-demos.38.txt,2020,7 Conclusion and Future Work,"we present torch-struct, a library for deep structured prediction."
2020.acl-demos.39.txt,2020,5 Conclusion,"in this paper, we presented conversation learner, a machine teaching tool for building dialog policy managers."
2020.acl-demos.39.txt,2020,5 Conclusion,"using the cl machine teaching ui, the dialog author can provide corrections to the logged user-system dialogs and further improve the cl’s dm performance."
2020.acl-demos.39.txt,2020,5 Conclusion,"we are planning to extend this work by looking into following problems: 1) investigating effectiveness of different ranking algorithms for log correction recommendation, 2) optimizing number of training samples and action masks generated from the rule-based dm, and 3) improving predictions of hcn-based dm by looking into alternative network architectures."
2020.acl-demos.39.txt,2020,5 Conclusion,we demonstrated this through a case study based on dialog transcripts from microsoft’s text-based customer support system where the gains were approximately 13%.
2020.acl-demos.39.txt,2020,5 Conclusion,we have shown that the cl hcn-based dm can be bootstrapped from a rule-based dm preserving the same behavior expected from the rule-based system.
2020.acl-demos.4.txt,2020,4 Conclusion,"although easy to understand and implement, these techniques can be developed and improved on in many ways."
2020.acl-demos.4.txt,2020,4 Conclusion,"as suggested by an anonymous reviewer, another possible addition to the game could then be to predict the age appropriateness of a given topic, allowing for cards to be filtered on the basis of an age setting."
2020.acl-demos.4.txt,2020,4 Conclusion,"concerning the game itself, we believe that tabouid is more than just a fun game and can develop and help reinforce general knowledge for players of all backgrounds."
2020.acl-demos.4.txt,2020,4 Conclusion,"currently, the content of tabouid aims to reflect the diversity of wikipedia’s encyclopaedic knowledge.” as a consequence, some cards include words related to topics that might be deemed inappropriate for children."
2020.acl-demos.4.txt,2020,4 Conclusion,"in addition to improving the banned words selection process, future work on tabouid includes generating specific lists of cards based on school programs to use the game as an educational tool, using the category system of wikipedia to let users select more or less specific categories to play with, and adapting the algorithms to leverage the wide variety of languages wikipedia is available in beyond english and french."
2020.acl-demos.4.txt,2020,4 Conclusion,"in this paper, we have shown how a range of relatively simple nlp and machine-learning techniques can be integrated effectively to automatically generate the content of tabouid, a wordguessing game freely available on android and ios devices."
2020.acl-demos.4.txt,2020,4 Conclusion,"in this respect, this work could inspire implementation projects in nlp or computational linguistic programs."
2020.acl-demos.4.txt,2020,4 Conclusion,it also appears to be an engaging way to practice speaking for language learners.
2020.acl-demos.4.txt,2020,4 Conclusion,"they also naturally lead to a wide range of practical and theoretical questions relevant to nlp (e.g., data collection and annotation, and model interpretability)."
2020.acl-demos.40.txt,2020,6 Conclusion,"nstm is the first of its kind; it is query-driven, it offers unique news overviews which leverage clustering and succinct summarization, and it has been released to hundreds of thousands of users."
2020.acl-demos.40.txt,2020,6 Conclusion,"there are many open questions which we intend to research, such as whether autoregressivity in neural sentence compression can be exploited and how to compose themes over longer time periods."
2020.acl-demos.40.txt,2020,6 Conclusion,"we also demonstrated effective adoption of modern nlp techniques and advances in the design and implementation of the system, which we believe will be of interest to the community."
2020.acl-demos.40.txt,2020,6 Conclusion,"we presented nstm, a novel and production-ready system that composes concise and human-readable news overviews given arbitrary user search queries."
2020.acl-demos.41.txt,2020,Conclusion,claims of interactions are difficult to validate without links to source evidence.
2020.acl-demos.41.txt,2020,Conclusion,insufficient regulation in the supplement space introduces dangers for the many users of these supplements.
2020.acl-demos.41.txt,2020,Conclusion,"our dataset and web interface can be leveraged by researchers, clinicians, and curious individuals to increase understanding about supplement interactions."
2020.acl-demos.41.txt,2020,Conclusion,"the extracted sdi/ssi evidence are made search-able through a public web interface, supp.ai, where we integrate additional metadata about source papers to help users make decisions about the reliability of evidence."
2020.acl-demos.41.txt,2020,Conclusion,"we create an nlp pipeline to detect sdi/ssi evidence from scientific literature, leveraging umls identifiers, scispacy for ner and entity linking, bert-based language models for classification, and labeled data from a related domain for training."
2020.acl-demos.41.txt,2020,Conclusion,we hope to encourage additional research to improve the safety and benefits of dietary supplements for their consumers.
2020.acl-demos.41.txt,2020,Conclusion,we use this pipeline to extract evidence from 22m biomedical and clinical articles with high precision.
2020.acl-demos.42.txt,2020,7 Conclusion,better training methods also allow us to fight the potential generation of noisy data due to inaccurate annotation recommendations.
2020.acl-demos.42.txt,2020,7 Conclusion,"in this paper, we propose an open-source web-based annotation framework lean-life that not only allows an annotator to provide the needed labels for a task, but can also capture explanation for each labeling decision."
2020.acl-demos.42.txt,2020,7 Conclusion,such explanations enable a significant improvement in model training while only doubling per instance annotation time.
2020.acl-demos.42.txt,2020,7 Conclusion,"this increase in per instance annotation time is greatly outweighed by the benefits in model training, especially in a low resource settings, as proven by our experiments."
2020.acl-demos.42.txt,2020,7 Conclusion,"this is an important consideration for any annotation framework, as the quicker the framework is able to train annotation recommendation models to reach high performance, the sooner the user receives useful annotation recommendations, which in turn cut down on the annotation time required per instance."
2020.acl-demos.42.txt,2020,7 Conclusion,we hope that our work on lean-life will allow for researches and practitioners alike to more easily obtain useful labeled datasets and models for the various nlp tasks they face.
2020.acl-demos.43.txt,2020,7 Conclusion,"a usability study reveals that when the chatbot recommends questions, news readers tend to have longer conversations, with an average of 24 messages exchanged."
2020.acl-demos.43.txt,2020,7 Conclusion,"in each room, the system takes note of generated questions that have already been answered, to minimize repetition of information to the news reader."
2020.acl-demos.43.txt,2020,7 Conclusion,these conversation consist of combination of recommended and user-created questions.
2020.acl-demos.43.txt,2020,7 Conclusion,"we presented a fully automated news chatbot system, which leverages an average of 2,000 news articles a day from a diverse set of sources to build chatrooms for important news stories."
2020.acl-demos.5.txt,2020,6 Conclusion,experiments confirm the effectiveness of our proposed approach and show superior search experience compared to traditional search engine.
2020.acl-demos.5.txt,2020,6 Conclusion,future work include (1) expanding the database to more papers (2) improving the qa model using the collected data to better handle question answering in the context of research domain.
2020.acl-demos.5.txt,2020,6 Conclusion,"we present talk to paper, a qa system for nlp papers powered by soco-qa."
2020.acl-demos.5.txt,2020,6 Conclusion,we welcome contributions from the research community to curate useful resources together for the further research.
2020.acl-demos.6.txt,2020,6 Conclusion,"in this paper we presented and described the architecture of syntagrank, our state-of-the-art knowledge-based system for multilingual word sense disambiguation using syntagmatic information."
2020.acl-demos.6.txt,2020,6 Conclusion,"we also provided details concerning the use of syntagrank’s web interface and restful api, accessible at http://syntagnet.org/ and http://api.syntagnet.org, respectively."
2020.acl-demos.7.txt,2020,6 Conclusion,"we propose a syntax-based representation that aims to make the event structure and as many lexical relations as possible explicit, for the benefit of downstream information-seeking applications."
2020.acl-demos.7.txt,2020,6 Conclusion,"we provide a python api that converts ud trees to this representation, and demonstrate its empirical benefits on a relation extraction task."
2020.acl-demos.8.txt,2020,6 Conclusion,evidenceminer also includes analytic functionalities such as the most frequent entity and relation summarization.
2020.acl-demos.8.txt,2020,6 Conclusion,the retrieved evidence sentences can be easily located in the background corpora for better visualization.
2020.acl-demos.8.txt,2020,6 Conclusion,we are further developing evidenceminer to be a more intelligent system that can assist in more efficient and in-depth scientific discoveries.
2020.acl-demos.8.txt,2020,6 Conclusion,"we build evidenceminer, a web-based system for textual evidence discovery for life sciences."
2020.acl-demos.8.txt,2020,6 Conclusion,"we incorporated another corpus on covid-19 in evidenceminer to help boost the scientific discoveries (wang et al., 2020b,a)."
2020.acl-demos.9.txt,2020,5 Conclusions,"moving forward, we hope to refine the linking of extracted snippets to structured vocabularies to run a more comprehensive user-study to evaluate the use of the system in practice by different types of users."
2020.acl-demos.9.txt,2020,5 Conclusions,"we also hope to develop a joint extraction and inference model, rather than relying on the current pipelined approach."
2020.acl-demos.9.txt,2020,5 Conclusions,"we have presented the evidence extraction component of trialstreamer, an open-source prototype that performs end-to-end identification of published rct reports, extracts key elements from the texts (intervention and outcomes descriptions), and performs relation extraction between these, i.e., attempts to determine which intervention was reported to work for which outcomes."
2020.acl-demos.9.txt,2020,5 Conclusions,"we use this pipeline to provide fast, on-demand overviews of all published evidence pertaining to a condition of interest."
2020.acl-main.1.txt,2020,7 Discussion,"although all metrics are based on a rank of the same number of competitors, the distribution of similarities and differences between the semantic representations of these competitors may differ across datasets."
2020.acl-main.1.txt,2020,7 Discussion,"in future work, we intend to curate a test set with data from separate sources, which can serve as a benchmark for the models we study."
2020.acl-main.1.txt,2020,7 Discussion,"it is also possible that sbert is better suited to encode the semantic content of ads, as ads uterrances are likely to be more similar to the sentences sbert was trained on than cds utterances are."
2020.acl-main.1.txt,2020,7 Discussion,our finding that learning is initially faster for cds is more in line with the idea of learnability as ‘easy to learn’.
2020.acl-main.1.txt,2020,7 Discussion,our finding that models trained on ads generalize better to cds than the other way around contrasts with the findings of kirchhoff and schimmel (2005).
2020.acl-main.1.txt,2020,7 Discussion,"our results are in contrast to the idea that cds is optimized for leading to the most valuable knowledge, as it is the models trained on ads that lead to better generalization."
2020.acl-main.1.txt,2020,7 Discussion,"since there is simply more to learn, learning to perform the task is more difficult on ads, but it leads to more valuable knowledge."
2020.acl-main.1.txt,2020,7 Discussion,"the better generalization of models trained on ads may be due to ads having higher lexical and semantic variability, reflected in the larger vocabulary and higher number of words per utterance."
2020.acl-main.1.txt,2020,7 Discussion,"the combined test set scores are more directly comparable, but ideally, we would like to compare the generalization of both models on an independent test set."
2020.acl-main.1.txt,2020,7 Discussion,"the effect is present both in models trained on natural speech and in models trained on synthetic speech, suggesting that it is at least partly due to differences in the language itself, rather than acoustic properties of the speech register."
2020.acl-main.1.txt,2020,7 Discussion,we also intend to use tools for interpreting the knowledge encoded in neural networks (such as diagnostic classifiers and representational similarity analysis) to investigate the emergent representation of linguistic units such as phonemes and words.
2020.acl-main.1.txt,2020,7 Discussion,"we find indications that learning to extract meaning from speech is initially faster when learning from child-directed speech, but learning from adultdirected speech eventually leads to similar task performance on the training register, and better generalization to the other register."
2020.acl-main.1.txt,2020,7 Discussion,we intend to explore how a curriculum of cds followed by ads affects learning trajectories and outcomes.
2020.acl-main.1.txt,2020,7 Discussion,"we must be prudent in drawing conclusions from the apparent effects we see in this study, as the results on different datasets cannot be interpreted as being on the same scale."
2020.acl-main.10.txt,2020,8 Conclusion,extensive experiments show that the proposed model significantly outperforms previous methods.
2020.acl-main.10.txt,2020,8 Conclusion,human evaluation and case study also confirm the effectiveness of the proposed method.
2020.acl-main.10.txt,2020,8 Conclusion,"in this method, a retrieval-based bootstrapping is introduced to sample pseudo mistaken cases from training corpus to enrich the original training data."
2020.acl-main.10.txt,2020,8 Conclusion,these improvements include both of correctness measured with slot error rates and naturalness measured with bleu scores.
2020.acl-main.10.txt,2020,8 Conclusion,we also employ policy-based reinforcement learning to enable training the models with discrete rewards that are consistent to evaluation metrics.
2020.acl-main.10.txt,2020,8 Conclusion,we have proposed iterative rectification network (irn) to improve slot consistency of general nlg systems.
2020.acl-main.100.txt,2020,5 Conclusion,"in the future, we are interested in injecting knowledge into text representation learning (cao et al., 2017, 2018b) for deeply understanding expert language, and will help to generate knowledgeenhanced questions (pan et al., 2019) for laymen."
2020.acl-main.100.txt,2020,5 Conclusion,it is of high quality and also challenging due to the presence of knowledge gap and the need of structural modifications.
2020.acl-main.100.txt,2020,5 Conclusion,our further discussion analyzed the challenges of existing metrics.
2020.acl-main.100.txt,2020,5 Conclusion,the results shown a significant gap between machine and human performance.
2020.acl-main.100.txt,2020,5 Conclusion,we established benchmark performance of five sota models.
2020.acl-main.100.txt,2020,5 Conclusion,"we proposed a practical task of expertise style transfer and constructed a high-quality dataset, msd."
2020.acl-main.101.txt,2020,5 Conclusion,both automatic and human evaluations show that our framework can significantly outperform the state-of-the-art methods.
2020.acl-main.101.txt,2020,5 Conclusion,extensive experiments are conducted to verify the proposed method.
2020.acl-main.101.txt,2020,5 Conclusion,"in this paper, we propose a novel transformer-based table-to-text generation framework to address the faithful text-generation problem."
2020.acl-main.101.txt,2020,5 Conclusion,"to enforce faithful generation, we propose a new tabletext optimal-transport matching loss and a tabletext embedding similarity loss."
2020.acl-main.101.txt,2020,5 Conclusion,"to evaluate the faithfulness of the generated texts, we further propose a new automatic evaluation metric specialized to the table-to-text generation problem."
2020.acl-main.102.txt,2020,5 Conclusion,"since dynamic memory can be a learning mechanism more general than what we have used here for fewshot learning, we will investigate this type of models in other learning problems."
2020.acl-main.102.txt,2020,5 Conclusion,the model achieves new state-of-the-art results on the minircv1 and odic datasets.
2020.acl-main.102.txt,2020,5 Conclusion,"we propose dynamic memory induction networks (dmin) for few-shot text classification, which builds on external working memory with dynamic routing, leveraging the latter to track previous learning experience and the former to adapt and generalize better to support sets and hence to unseen classes."
2020.acl-main.103.txt,2020,7 Conclusion and Future Work,"besides, we also propose a soft and a hard exclusion mechanisms to enhance the diversity of the generated keyphrases."
2020.acl-main.103.txt,2020,7 Conclusion and Future Work,extensive experimental results demonstrate the effectiveness of our methods.
2020.acl-main.103.txt,2020,7 Conclusion and Future Work,"in this paper, we propose an exclusive hierarchical decoding framework for keyphrase generation."
2020.acl-main.103.txt,2020,7 Conclusion and Future Work,one interesting future direction is to explore whether the beam search is helpful to our model.
2020.acl-main.103.txt,2020,7 Conclusion and Future Work,"unlike previous sequential decoding methods, our hierarchical decoding consists of a phrase-level decoding process to capture the current aspect to summarize and a word-level decoding process to generate keyphrases based on the captured aspect."
2020.acl-main.104.txt,2020,6 Conclusion,and our best model obtains a macro-f1 score of 63.35% and a micro-f1 score of 83.96%.
2020.acl-main.104.txt,2020,6 Conclusion,"furthermore, our framework is extended into a parallel variant based on multi-label attention and a serial variant of text feature propagation."
2020.acl-main.104.txt,2020,6 Conclusion,"in this paper, we propose a novel end-to-end hierarchy-aware global model that extracts the label structural information for aggregating label-wise text features."
2020.acl-main.104.txt,2020,6 Conclusion,"our approaches empirically achieve significant and consistent improvement on three distinct datasets, especially on the low-frequency labels."
2020.acl-main.104.txt,2020,6 Conclusion,"specifically, both variants outperform the state-of-the-art model on the rcv1-v2 benchmark dataset."
2020.acl-main.104.txt,2020,6 Conclusion,we present a bidirectional treel-stm and a hierarchy-gcn as the hierarchy-aware structure encoder.
2020.acl-main.105.txt,2020,4 Conclusion,other retrieval tasks may also benefit from using keyphrase information and we expect our results to serve as a basis for further improvements.
2020.acl-main.105.txt,2020,4 Conclusion,"our results show that keyphrases can significantly improve retrieval effectiveness, and also highlight the importance of evaluating keyphrase generation models from an extrinsic perspective."
2020.acl-main.105.txt,2020,4 Conclusion,we presented the first study of the usefulness of keyphrase generation for scientific document retrieval.
2020.acl-main.106.txt,2020,8 Conclusion,"furthermore, the model learns embeddings capturing information about the compatibility of affixes and stems in derivation.acknowledgements."
2020.acl-main.106.txt,2020,8 Conclusion,the model achieves good results and performs on par with a character-based lstm at a fraction of the number of trainable parameters (less than 10%).
2020.acl-main.106.txt,2020,8 Conclusion,this research was also supported by the european research council (grant no.740516).
2020.acl-main.106.txt,2020,8 Conclusion,valentin hofmann was funded by the arts and humanities research council and the german academic scholarship foundation.
2020.acl-main.106.txt,2020,8 Conclusion,"we have introduced a derivational graph autoencoder (dga) that combines syntactic and semantic information with associative information from the mental lexicon to predict morphological well-formedness (mwf), a task that has not been addressed before."
2020.acl-main.106.txt,2020,8 Conclusion,we thank the reviewers for their helpful and very constructive comments.
2020.acl-main.107.txt,2020,10 Conclusion,"annotated with 4 standard morphosyntactic layers, two of them following the universal dependency annotation scheme, and provided with translation to french as well as glosses and word language identification, we believe that this corpus will be useful for the community at large, both for linguistic purposes and as training data for resource-scarce nlp in a high-variability scenario."
2020.acl-main.107.txt,2020,10 Conclusion,"in addition to the annotated data, we provide around 1 million tokens (over 46k sentences) of unlabeled narabizi content, resulting in the largest dataset available for this dialect."
2020.acl-main.107.txt,2020,10 Conclusion,"more over, being made of user-generated content, this treebank covers a large variety of language variation among native speakers and displays a high level of codeswitching."
2020.acl-main.107.txt,2020,10 Conclusion,our corpora are freely available14 under the cc-by-sa license and the narabizi treebank is also released as part of the universal dependencies project.
2020.acl-main.107.txt,2020,10 Conclusion,"we introduced the first treebank for an arabic dialect spoken in north-africa and written in romanized form, narabizi."
2020.acl-main.108.txt,2020,6 Summary,all the resources developed in this paper are freely available.3
2020.acl-main.108.txt,2020,6 Summary,"despite the prevalence of technical conversations, various important and controversial societal issues are covered in the corpus as well."
2020.acl-main.108.txt,2020,6 Summary,the corpus is targeted mainly at discussion and dialog-based research in nlp.
2020.acl-main.108.txt,2020,6 Summary,this paper contributes the largest email corpus to date.
2020.acl-main.108.txt,2020,6 Summary,"to minimize user overhead, we developed a new neural model for segmenting emails with high precision and recall, which achieves state-of-the-art performance, allowing for fine-grained extraction of structural elements from emails."
2020.acl-main.108.txt,2020,6 Summary,"we gave an overview of the topics discussed in the corpus, demonstrating that it is a valuable source for several nlp tasks, such as argument mining."
2020.acl-main.109.txt,2020,9 Discussion,"at the same time, the methodology and datasets provide means to investigate essentially any kind of well-defined clmd."
2020.acl-main.109.txt,2020,9 Discussion,"for example, we reveal inconsistencies in the treatment of multi-word expressions across languages."
2020.acl-main.109.txt,2020,9 Discussion,"in conclusion we note that our analysis suggests that considerable entropy in the mapping between the syntactic relations of the source and target sides can be reduced by removing inconsistencies in the application of ud, and perhaps more importantly by refining ud with semantic distinctions that will normalize corresponding constructions across languages to have a similar annotation."
2020.acl-main.109.txt,2020,9 Discussion,"indeed, since function words in ud tend to be dependents of content words, we may analyze the former by considering the distribution of function word types that each type of content word has."
2020.acl-main.109.txt,2020,9 Discussion,"languages such as ru, lacking a truly productive nominal-compound relation, carve this class up based mostly on the pos of the dependent element (e.g., episcopal corresponds to a ru amod), its semantic class (e.g., compounds with cardinal directions are ru amods), and whether the dependent element itself has dependents (these mostly correspond to ru nmods)."
2020.acl-main.109.txt,2020,9 Discussion,"moreover, sub-typing dependency paths based on their linear direction can allow investigating word-order differences.11 other than informing the development of cross-lingual transfer learning, our analysis directly supports the validation of ud annotation."
2020.acl-main.109.txt,2020,9 Discussion,"on one hand, by focusing on content words, the approach abstracts away from much local-syntactic detail (such as reordering or adding/removing function words)."
2020.acl-main.109.txt,2020,9 Discussion,our method can be used to detect and bridge such inconsistencies.
2020.acl-main.109.txt,2020,9 Discussion,the presented methodology gives easy access to different levels of analysis.
2020.acl-main.109.txt,2020,9 Discussion,"this will simultaneously advance ud’s stated goal of “bringing out cross-linguistic parallelism across languages” and, as our results on zero-shot parsing suggest, make it more useful for cross-linguistic transfer."
2020.acl-main.109.txt,2020,9 Discussion,"thus, the translation of many nps with adjectival modifiers, such as celtic sea or episcopal church, are analyzed as compounds."
2020.acl-main.11.txt,2020,5 Conclusion and Future Work,"in future work, we plan to experiment with multi-domain span extraction architectures."
2020.acl-main.11.txt,2020,5 Conclusion and Future Work,the formulation allows the model to effectively leverage representations available from large-scale conversational pretraining.
2020.acl-main.11.txt,2020,5 Conclusion and Future Work,"we have also introduced restaurants8k, a new challenging data set that will hopefully encourage further work on span extraction for dialogue."
2020.acl-main.11.txt,2020,5 Conclusion and Future Work,"we have introduced span-convert, a light-weight model for dialog slot-filling that approaches the problem as a turn-based span extraction task."
2020.acl-main.11.txt,2020,5 Conclusion and Future Work,"we have shown that, due to pretrained representations, span-convert is especially useful in few-shot learning setups on small data sets."
2020.acl-main.110.txt,2020,9 Conclusions,"as a final remark, we believe that the proposed framework can be useful for other nlg tasks such as paraphrase generation or text simplification."
2020.acl-main.110.txt,2020,9 Conclusions,"considering the aforementioned limitations, we presented a study on how to reduce data collection effort, using a mix of several strategies."
2020.acl-main.110.txt,2020,9 Conclusions,"however, these automation approaches are not mature yet, since they suffer from the lack of sufficient amount of quality data and tend to produce generic/repetitive responses."
2020.acl-main.110.txt,2020,9 Conclusions,"in this scenario, automation strategies, such as natural language generation, are necessary to help ngo operators in their countering effort."
2020.acl-main.110.txt,2020,9 Conclusions,"to counter hatred online and avoid the undesired effects that come with content moderation, intervening in the discussion directly with textual responses is considered as a viable solution."
2020.acl-main.110.txt,2020,9 Conclusions,"to effectively and efficiently obtain varied and novel data, we first propose the generation of silver counter-narratives – using large scale unsupervised language models – then a filtering stage by crowd-workers and finally an expert validation/post-editing."
2020.acl-main.110.txt,2020,9 Conclusions,we also show promising results obtained by replacing crowd-filtering with an automatic classifier.
2020.acl-main.111.txt,2020,6 Conclusion,"as a result, klej benchmark proves to be both challenging and diverse, as there is no single model that outperforms others on all tasks."
2020.acl-main.111.txt,2020,6 Conclusion,"for that purpose, many existing resources had to be adapted, either automatically (nkjp-ner, psc) or manually (czy wiesz?), to make it easier to use."
2020.acl-main.111.txt,2020,6 Conclusion,it seems reasonable to derive additional benchmarks by requiring a given level of efficiency from participating models.
2020.acl-main.111.txt,2020,6 Conclusion,"it’s worth mentioning that the main weakness of creating such benchmarks is focusing only on the model performance and not the model efficiency, e.g.in terms of training data, speed or a number of parameters."
2020.acl-main.111.txt,2020,6 Conclusion,"its goal is to drive the development of better nlu models, so careful selection of tasks was crucial."
2020.acl-main.111.txt,2020,6 Conclusion,"we also present herbert, a transformer-based model trained specifically for polish and compare it with other lstm- and transformer-based models."
2020.acl-main.111.txt,2020,6 Conclusion,we find it equally important to provide a common evaluation interface for all the tasks.
2020.acl-main.111.txt,2020,6 Conclusion,we find that it is the best on average and achieves highest scores on three tasks.
2020.acl-main.111.txt,2020,6 Conclusion,"we introduce the klej benchmark, a comprehensive set of evaluation tasks for the polish language understanding."
2020.acl-main.111.txt,2020,6 Conclusion,we leave it as future work.
2020.acl-main.111.txt,2020,6 Conclusion,"we mainly focused on a variety of text genres, objectives, text lengths, and difficulties, which allows us to assess the models across different axes."
2020.acl-main.111.txt,2020,6 Conclusion,we plan to continue the work on herbert and use the klej benchmark to guide its development.
2020.acl-main.112.txt,2020,7 Conclusion,"a promising direction would be to combine a multilingual sense inventory such as babelnet (navigli and ponzetto, 2012) with sense embeddings (camacho-collados and pilehvar, 2018)."
2020.acl-main.112.txt,2020,7 Conclusion,"emotion lexicons are at the core of sentiment analysis, a rapidly flourishing field of nlp."
2020.acl-main.112.txt,2020,7 Conclusion,"firstly, the evaluation procedure, which is integrated into the generation methodology, allows us to reliably estimate the quality of resulting lexicons, even without target language gold standard."
2020.acl-main.112.txt,2020,7 Conclusion,future work will have to deepen the way we deal with word sense ambiguity by way of exchanging the simplifying type-level approach our current work is based on with a semantically more informed sense-level approach.
2020.acl-main.112.txt,2020,7 Conclusion,our study is “large-scale” in many respects.
2020.acl-main.112.txt,2020,7 Conclusion,"secondly, our data suggests that embedding-based word emotion models can be used as a repair mechanism, mitigating poor target-language emotion estimates acquired by simple word-to-word translation."
2020.acl-main.112.txt,2020,7 Conclusion,the evaluation of the generated lexicons featured 26 manually annotated datasets spanning 12 diverse languages.
2020.acl-main.112.txt,2020,7 Conclusion,"the predicted ratings showed consistently high correlation with human judgment, compared favorably with state-of-the-art monolingual approaches to lexicon expansion and even surpassed human inter-study reliability in some cases."
2020.acl-main.112.txt,2020,7 Conclusion,the sheer number of test sets we used allowed us to validate fundamental methodological assumptions underlying our approach.
2020.acl-main.112.txt,2020,7 Conclusion,we created representationally complex lexicons— comprising 8 distinct emotion variables—for 91 languages with up to 2 million entries each.
2020.acl-main.112.txt,2020,7 Conclusion,"while there are techniques to tackle these three forms of sparsity in isolation, we introduced a methodology which allows us to cope with them simultaneously by jointly combining emotion representation mapping, machine translation, and embedding-based lexicon expansion."
2020.acl-main.112.txt,2020,7 Conclusion,"yet, despite large community efforts, the coverage of existing lexicons is still limited in terms of languages, size, and types of emotion variables."
2020.acl-main.113.txt,2020,7 Conclusions,"finally, we would like to test this approach for comparing different mt systems."
2020.acl-main.113.txt,2020,7 Conclusions,"first, we plan to test whether similar observations will hold for more language pairs and text domains."
2020.acl-main.113.txt,2020,7 Conclusions,"second, the score combination strategies could be improved by learning weights for each component."
2020.acl-main.113.txt,2020,7 Conclusions,this suggests that model uncertainty alone can be more reliable for assessing mt quality than standard reference-based evaluation.
2020.acl-main.113.txt,2020,7 Conclusions,this work can be extended in numerous ways.
2020.acl-main.113.txt,2020,7 Conclusions,we proposed to explore nmt model uncertainty to generate additional hypotheses for mt evaluation.
2020.acl-main.113.txt,2020,7 Conclusions,"we showed that by exploiting similarities in the space of translation hypotheses generated by the model, along with methods to effectively combine information from these multiple hypotheses, we can achieve more accurate estimation on the quality of mt output than standard reference-based comparison, including cases with multiple references."
2020.acl-main.114.txt,2020,5 Conclusions,"the use of visual features extracted from images has led to significant improvements in the results of state-of-the-art qe approaches, especially at sentence level."
2020.acl-main.114.txt,2020,5 Conclusions,the version of deepquest for multimodal qe and scripts to convert document into sentencelevel data are available on https://github.com/ sheffieldnlp/deepquest.
2020.acl-main.114.txt,2020,5 Conclusions,"we introduced multimodal quality estimation for machine translation, where an external modality – visual information – is incorporated to feature-based and neural-based qe approaches, on sentence and document levels."
2020.acl-main.115.txt,2020,8 Conclusion and Future Work,"still, the field lacks models that can perform human-like reasoning and generalization."
2020.acl-main.115.txt,2020,8 Conclusion and Future Work,the field of nlp has developed deep neural models that can exploit large amounts of data to achieve high scores on downstream tasks.
2020.acl-main.115.txt,2020,8 Conclusion and Future Work,"to mitigate this gap, we draw inspiration from the linguistic olympiads that challenge the meta-linguistic and reasoning abilities of high-school students."
2020.acl-main.115.txt,2020,8 Conclusion and Future Work,"we create a new benchmark dataset from available linguistic puzzles that spans over 81 languages from 39 language families, which is released at https:// ukplab.github.io/puzzling-machines/."
2020.acl-main.115.txt,2020,8 Conclusion and Future Work,"we implement and evaluate simple baselines such as alignment, and state-of-the-art machine translation models with integrated a pretrained english language model."
2020.acl-main.115.txt,2020,8 Conclusion and Future Work,"we show that none of the models can perform well on the puzzles, suggesting that we are still far from having systems with meta-linguistic awareness and reasoning capabilities."
2020.acl-main.116.txt,2020,8 Conclusion,a natural next step is to combine the datasets in a multi-task setting to investigate to what extent models can profit from combining the information annotated in the respective datasets.
2020.acl-main.116.txt,2020,8 Conclusion,"along with this paper, we make the annotation guidelines and the annotated data freely available.outlook."
2020.acl-main.116.txt,2020,8 Conclusion,"based on the annotated structures, we suggest three information extraction tasks: the detection of experiment-describing sentences, entity mention recognition and typing, and experiment slot filling."
2020.acl-main.116.txt,2020,8 Conclusion,"further research will investigate the joint modeling of entity extraction, typing and experiment frame recognition."
2020.acl-main.116.txt,2020,8 Conclusion,"in addition, there are also further natural language processing tasks that can be researched using our dataset."
2020.acl-main.116.txt,2020,8 Conclusion,"in section 7.1, we have shown that our findings generalize well by applying model architectures developed on our corpus to another dataset."
2020.acl-main.116.txt,2020,8 Conclusion,our detailed corpus and interannotator agreement studies highlight the complexity of the task and verify the high annotation quality.
2020.acl-main.116.txt,2020,8 Conclusion,"they include the detection of events and sub-events when regarding the experiment-descriptions as events, and a more linguistically motivated evaluation of the framesemantic approach to experiment descriptions in text, e.g., moving away from the one-experimentper-sentence and one-sentence-per-experiment assumptions and modeling the graph-based structures as annotated."
2020.acl-main.116.txt,2020,8 Conclusion,we have presented a new dataset for information extraction in the materials science domain consisting of 45 open-access scientific articles related to solid oxide fuel cells.
2020.acl-main.116.txt,2020,8 Conclusion,"we have presented various strong baselines for them, generally finding that bert-based models outperform other model variants."
2020.acl-main.116.txt,2020,8 Conclusion,"while some categories remain challenging, overall, our models show solid performance and thus prove that this type of data modeling is feasible and can lead to systems that are applicable in production settings."
2020.acl-main.117.txt,2020,7 Discussion and Future Work,"many answers cannot be obtained by extracting portions of a document based on language alone: in many cases, domain knowledge is needed and often a question cannot be answered from the data collection without reasoning steps."
2020.acl-main.117.txt,2020,7 Discussion and Future Work,techqa is a challenging dataset for models developed for existing open-domain mrc systems.
2020.acl-main.117.txt,2020,7 Discussion and Future Work,the initial version of the dataset was created by selecting questions and answers that are relevant to the it technical support domain but at the same time do not diverge excessively from the spirit of other existing mrc datasets.
2020.acl-main.117.txt,2020,7 Discussion and Future Work,the leaderboard also reports the f1 at the top result and at the top 5 results averaged over the answerable questions.
2020.acl-main.117.txt,2020,7 Discussion and Future Work,the leaderboard ranks submissions according to a metric consisting of the character overlap f1 measure for answerable questions and the zero-one metric for non-answerable questions.
2020.acl-main.117.txt,2020,7 Discussion and Future Work,the obvious approach of fine-tuning these models using the techqa training set yields systems that barely beat the baseline.
2020.acl-main.117.txt,2020,7 Discussion and Future Work,"the overall size of the released data (600 training questions) is in line with real-world scenarios, where the high cost of domain expert time limits the amount of quality data that can reasonably be collected."
2020.acl-main.117.txt,2020,7 Discussion and Future Work,"their out-of-the box performance is very low, especially considering that a system that declares every question as unanswerable achieves f1=48.4% on the development set."
2020.acl-main.117.txt,2020,7 Discussion and Future Work,"thus, the dataset is meant to stimulate research in domain adaptation, in addition to developing algorithms for longer questions and answers than the current leaderboards."
2020.acl-main.117.txt,2020,7 Discussion and Future Work,we consider techqa to be a stepping stone on which to build future data collections and leaderboards.
2020.acl-main.117.txt,2020,7 Discussion and Future Work,"we envision a roadmap where future releases of techqa will require synergy between multiple ai disciplines, from deep-learning based mrc to reasoning, knowledge base acquisition, and causality detection."
2020.acl-main.117.txt,2020,7 Discussion and Future Work,we have created a leaderboard to evaluate systems against a blind dataset of 490 questions with a ratio of answerable to unanswerable questions similar to that of the development set.
2020.acl-main.117.txt,2020,7 Discussion and Future Work,"we have introduced techqa, a questionanswering dataset for the it technical support domain."
2020.acl-main.117.txt,2020,7 Discussion and Future Work,we plan on releasing questions with answers in a broader and more diverse collection that will include documents with a less formulaic structure than the technotes.
2020.acl-main.117.txt,2020,7 Discussion and Future Work,"we will also relax the length limitations to include questions rich in details, and answers that include complex procedures; in the same spirit, we will allow answers consisting of multiple spans from a single document."
2020.acl-main.118.txt,2020,7 Conclusion and Future Work,"in this paper, we presented isarcasm, a dataset of intended sarcasm consisting of 4,484 tweets labeled and explained by their authors, and further revised and categorised by an expert linguistic."
2020.acl-main.118.txt,2020,7 Conclusion and Future Work,"we aim to promote research in sarcasm detection, and to encourage future investigations into sarcasm in general and how it is perceived across cultures."
2020.acl-main.118.txt,2020,7 Conclusion and Future Work,we believe this dataset will allow future work in sarcasm detection to progress in a setting free of the noise found in existing datasets.
2020.acl-main.118.txt,2020,7 Conclusion and Future Work,"we saw that computational models perform poorly in detecting sarcasm in the new dataset, indicating that the sarcasm detection task might be more challenging compared to how it was seen in earlier research."
2020.acl-main.119.txt,2020,7 Conclusion,"also, the idea proposed in this paper may be applied to a broad range of structured prediction tasks (not only restricted to other semantic parsing tasks) where the complex output space can be divided into two interdependent parts with a similar iterative inference process to achieve harmonious predictions and better performance."
2020.acl-main.119.txt,2020,7 Conclusion,an interesting future work is to make the number of inference steps adaptive to input sentences.
2020.acl-main.119.txt,2020,7 Conclusion,"each spanning step is explicitly characterized as answering two questions: which parts of the sequence to abstract, and where in the graph to construct."
2020.acl-main.119.txt,2020,7 Conclusion,our method constructs an amr graph incrementally in a nodeby-node fashion.
2020.acl-main.119.txt,2020,7 Conclusion,our model significantly advances the state-of-the-art results on two amr corpora.
2020.acl-main.119.txt,2020,7 Conclusion,we leverage the mutual causalities between the two and design an iterative inference algorithm.
2020.acl-main.119.txt,2020,7 Conclusion,we presented the dual graph-sequence iterative inference method for amr parsing.
2020.acl-main.12.txt,2020,6 Conclusion,"for transaction dialogues, our technique can bootstrap new domains with less than 100 templates per domain, which can be built in a few person-hours."
2020.acl-main.12.txt,2020,6 Conclusion,"furthermore, this method is general and can be extended to dialogue state tracking beyond transactions, by building new dialogue models."
2020.acl-main.12.txt,2020,6 Conclusion,our technique using the sumbt model improves the zero-shot state of the art by 21% on average across the different domains.
2020.acl-main.12.txt,2020,6 Conclusion,"this suggests that pretraining complements the use of synthesized data to learn the domain, and can be a general technique to bootstrap new dialogue systems."
2020.acl-main.12.txt,2020,6 Conclusion,"we have released our algorithm and dialogue model as part of the open-source genie toolkit, which is available on github3."
2020.acl-main.12.txt,2020,6 Conclusion,"we propose a method to synthesize dialogues for a new domain using an abstract dialogue model, combined with a small number of domain templates derived from observing a small dataset."
2020.acl-main.12.txt,2020,6 Conclusion,we show improvements in joint accuracy in zero-shot and few-shot transfer learning for both the trade and bert-based sumbt models.
2020.acl-main.12.txt,2020,6 Conclusion,"with this little effort, it is already possible to achieve about 2/3 of the accuracy obtained with a large-scale human annotated dataset."
2020.acl-main.120.txt,2020,6 Conclusion,important challenges for future work include how to scale deep learning methods to such large amounts of source documents and how to close the gap to the oracle methods.
2020.acl-main.120.txt,2020,6 Conclusion,"we conducted extensive experiments to establish baseline results, and we hope that future work on mds will use this dataset as a benchmark."
2020.acl-main.120.txt,2020,6 Conclusion,we hope this dataset will facilitate the creation of real-world mds systems for use cases such as summarizing news clusters or search results.
2020.acl-main.120.txt,2020,6 Conclusion,"we present a new large-scale mds dataset for the news domain, consisting of large clusters of news articles, associated with short summaries about news events."
2020.acl-main.121.txt,2020,6 Conclusion and Future Work,(2) our model has a much smaller size and a much faster training efficiency.
2020.acl-main.121.txt,2020,6 Conclusion and Future Work,"besides, we will also explore the influence of probabilistic bilingual lexicon obtained by learning only from monolingual data on our method."
2020.acl-main.121.txt,2020,6 Conclusion and Future Work,experimental results have shown that our method can significantly outperform the baseline and achieve comparable performance with the state-of-the-art.
2020.acl-main.121.txt,2020,6 Conclusion and Future Work,"furthermore, our method has two advantages over the state-of-the-art: (1) our model requires only an additional probabilistic bilingual lexicon rather than largescale parallel datasets of other tasks, thus reducing the model’s dependence on data and making it easier for the model to migrate to other domains or other language pairs."
2020.acl-main.121.txt,2020,6 Conclusion and Future Work,"in our future work, we consider incorporating our method into the multi-task method."
2020.acl-main.121.txt,2020,6 Conclusion and Future Work,"in this paper, we present a novel method consistent with the translation pattern in the process of obtaining a cross-lingual summary."
2020.acl-main.121.txt,2020,6 Conclusion and Future Work,"this method first attends to the source words, then obtains the translation candidates, and incorporates them into the generation of the final summary."
2020.acl-main.122.txt,2020,7 Conclusion,"by exploiting temporal expressions, we have improved the date-wise approach and yielded new state-of-the-art results on all tested datasets."
2020.acl-main.122.txt,2020,7 Conclusion,"for a more robust and diverse evaluation, we have constructed a new tls dataset with a much larger number of topics and with longer time-spans than in previous datasets."
2020.acl-main.122.txt,2020,7 Conclusion,"hence, we showed that an expensive combinatorial search over all sentences in a document collection is not necessary to achieve good results for news tls."
2020.acl-main.122.txt,2020,7 Conclusion,"in this study, we have compared and proposed different strategies to construct timeline summaries of long-ranging news topics: the previous state-of-the-art method based on direct summarization, a date-wise approach, and a clustering-based approach."
2020.acl-main.122.txt,2020,7 Conclusion,most of the generated timelines are still far from oracle timeline extractors and leave large gaps for improvements.
2020.acl-main.122.txt,2020,7 Conclusion,"potential future directions include a more principled use of our proposed heuristic for detecting content relevant to specific dates, the use of abstractive techniques, a more effective treatment of the redundancy challenge, and extending the new dataset with multiple sources."
2020.acl-main.123.txt,2020,7 Conclusion and future work,experimental results demonstrated that the headline generation model trained on filtered supervision data showed no clear difference in rouge scores but remarkable improvements in automatic and manual evaluations of the truthfulness of the generated headlines.
2020.acl-main.123.txt,2020,7 Conclusion and future work,"in the future, we explore a more sophisticated method to improve the relevance and truthfulness of generated headlines, for example, removing only deviated spans in untruthful headlines rather than removing untruthful headlines entirely from the supervision data."
2020.acl-main.123.txt,2020,7 Conclusion and future work,"in this paper, we showed that the current headline generation model yields unexpected words."
2020.acl-main.123.txt,2020,7 Conclusion and future work,"moreover, it will be also interesting to see whether the same issue occurs in other related tasks such as data-to-text generation."
2020.acl-main.123.txt,2020,7 Conclusion and future work,other directions include an extensive evaluation of relevance and truthfulness of abstractive summarization and an establishment of an automatic evaluation metric for truthfulness.
2020.acl-main.123.txt,2020,7 Conclusion and future work,we also presented the importance of evaluating truthfulness in abstractive summarization.
2020.acl-main.123.txt,2020,7 Conclusion and future work,we believe that the concern raised in this paper is beneficial to other tasks.
2020.acl-main.123.txt,2020,7 Conclusion and future work,"we conjectured that one of the reasons lies in the defect in the task setting and data set, where generating a headline from the source document is impossible because of the insufficiency of the source information."
2020.acl-main.123.txt,2020,7 Conclusion and future work,we presented an approach for removing from the supervision data headlines that are not entailed by their source documents.
2020.acl-main.124.txt,2020,7 Conclusion,"furthermore, we use supert as rewards to train a neural-rl-based summarizer, which leads to up to 17% quality improvement (in rouge-2) compared to the state-of-the-art unsupervised summarizers."
2020.acl-main.124.txt,2020,7 Conclusion,"the resulting method, supert, correlates with human ratings substantially better than the state-of-the-art unsupervised metrics."
2020.acl-main.124.txt,2020,7 Conclusion,"this result not only shows the effectiveness of supert in a downstream task, but also promises a new way to train rl-based summarizers: an infinite number of summary-reward pairs can be created from infintely many documents, and their supert scores can be used as rewards to train rl-based summarizers, fundamentally relieving the data-hungriness problem faced by existing rl-based summarization systems."
2020.acl-main.124.txt,2020,7 Conclusion,"we explored unsupervised multi-document summary evaluation methods, which require neither reference summaries nor human annotations."
2020.acl-main.124.txt,2020,7 Conclusion,"we find that vectorizing the summary and the top sentences in the source documents using contextualized embeddings, and measuring their semantic overlap with soft token alignment techniques is a simple yet effective method to rate the summary’s quality."
2020.acl-main.125.txt,2020,6 Conclusion,"for future work, we intend to apply our method to other transformer-based summarization models."
2020.acl-main.125.txt,2020,6 Conclusion,"in this paper, we propose the sagcopy summarization model that acquires guidance signals for the copy mechanism from the encoder self-attention graph."
2020.acl-main.125.txt,2020,6 Conclusion,the experimental results show the effectiveness of our model.
2020.acl-main.125.txt,2020,6 Conclusion,"then, we incorporate the importance score into the copy module."
2020.acl-main.125.txt,2020,6 Conclusion,we first calculate the centrality score for each source word.
2020.acl-main.126.txt,2020,7 Conclusion,"cmade’s results highly correlate with expert judgments on pair-wise dialog comparison ratings (89.2% agreement, 0.787 kappa)."
2020.acl-main.126.txt,2020,7 Conclusion,"however, our analysis indicates that self-reported dialog ratings are skewed (j-shape), noisy and insensitive due to bias and variance among different users."
2020.acl-main.126.txt,2020,7 Conclusion,previously likert-score based self-reported rating is the de-facto standard for current dialog evaluation .
2020.acl-main.126.txt,2020,7 Conclusion,"the ultimate chatbot evaluation metric should be user-centric, as chatbots are there to provide human with an enjoyable experiences."
2020.acl-main.126.txt,2020,7 Conclusion,"we propose a three-stage denoising pipeline cmade to reduce self-reported ratings and, at the same time, build an automatic comparison-based automatic dialog quality predictor."
2020.acl-main.127.txt,2020,5 Conclusions,"in the future, we will adapt the method to more neural models especially the generation-based methods for the dialog system."
2020.acl-main.127.txt,2020,5 Conclusions,"in this paper, we have proposed a conversational word embedding method named pr-embedding, which is learned from conversation pairs for retrieval-based dialog system."
2020.acl-main.127.txt,2020,5 Conclusions,we find that pr-embedding can help the models select the better response both in single-turn and multi-turn conversation by catching the information among the pairs.
2020.acl-main.127.txt,2020,5 Conclusions,we use the word alignment model from machine translation to calculate the cross-sentence co-occurrence and train the embedding on word and sentence level.
2020.acl-main.128.txt,2020,6 Conclusion,"and we propose l-tapnet to calculate emission score, which improves label representation with label name semantics."
2020.acl-main.128.txt,2020,6 Conclusion,experiment results validate that both the collapsed dependency transfer and l-tapnet can improve the tagging accuracy.
2020.acl-main.128.txt,2020,6 Conclusion,"in this paper, we propose a few-shot crf model for slot tagging of task-oriented dialogue."
2020.acl-main.128.txt,2020,6 Conclusion,"to compute transition score under few-shot setting, we propose the collapsed dependency transfer mechanism, which transfers the prior knowledge of the label dependencies across domains with different label sets."
2020.acl-main.129.txt,2020,7 Conclusions and Future Work,"evaluation has shown that all experts exceeded the performance of reinforcement and supervised learning baselines, and in some cases even approached the results of a hand-crafted rule-based dialog manager."
2020.acl-main.129.txt,2020,7 Conclusions and Future Work,"furthermore, we introduced reinforced finetune learning (rofl) a dagger-inspired extension to dqfd which allows a pre-trained expert to adapt to an rl environment on-the-fly, bridging the domain-gap."
2020.acl-main.129.txt,2020,7 Conclusions and Future Work,"in future, we want to continue to investigate the possibility of using even weaker demonstrations."
2020.acl-main.129.txt,2020,7 Conclusions and Future Work,"in this paper, we have shown that weak demonstrations can be leveraged to learn an accurate dialog manager with deep q-learning from demonstrations in a challenging multi-domain environment."
2020.acl-main.129.txt,2020,7 Conclusions and Future Work,it even enables an expert trained on unannotated out-of-domain data to guide an rl dialog manager in a challenging environment.
2020.acl-main.129.txt,2020,7 Conclusions and Future Work,"our experiments show that rofl training is beneficial across different sources of demonstration data, boosting both the rate of convergence and final system performance."
2020.acl-main.129.txt,2020,7 Conclusions and Future Work,"since our no label expert is trained on unannotated data, it would be interesting to leverage large and noisy conversational datasets drawn from message boards or movie subtitles, and to see how rofl training fares with such a significant domain gap between the data and the rl environment."
2020.acl-main.129.txt,2020,7 Conclusions and Future Work,"we established that expert demonstrators can be trained on labeled, reduced-labeled, and unlabeled data and still guide the rl agent by means of their respective auxiliary losses."
2020.acl-main.13.txt,2020,4 Conclusion,"our model is compared with a number of previous models, and experimental results show that our model achieves the state-of-the-art performance and is highly competitive with different setups."
2020.acl-main.13.txt,2020,4 Conclusion,"this work proposes a standalone, complete chinese discourse parser."
2020.acl-main.13.txt,2020,4 Conclusion,"we integrate bert, uda, and a revised training procedure for constructing a robust shift-reduce parser."
2020.acl-main.13.txt,2020,4 Conclusion,we will explore cross-lingual transfer learning for supporting more languages.
2020.acl-main.130.txt,2020,5 Conclusion,the best model roberta only obtains 71.3% r@1.
2020.acl-main.130.txt,2020,5 Conclusion,there is a large gap between the model performance and human performance.
2020.acl-main.130.txt,2020,5 Conclusion,"we describe the process for generating mutual, and perform a detailed analysis."
2020.acl-main.130.txt,2020,5 Conclusion,we find that various state-of-the-art models show poor performance in mutual.
2020.acl-main.130.txt,2020,5 Conclusion,we hope that this dataset facilitates future research on multi-turn conversation reasoning problem.
2020.acl-main.130.txt,2020,5 Conclusion,"we introduced mutual, a high-quality manually annotated multi-turn dialogue reasoning dataset, which contains 8,860 dialogues and aims to test reasoning ability of dialogue models."
2020.acl-main.131.txt,2020,7 Conclusion & Future Work,"after turns of chatting, the agent should be able to infer the user’s persona, based on which personalized contents can be recommended."
2020.acl-main.131.txt,2020,7 Conclusion & Future Work,experiments on a large public dataset persona-chat demonstrate the effectiveness of our approach.
2020.acl-main.131.txt,2020,7 Conclusion & Future Work,"for future work, we would like to extend receiver to conversational recommender systems."
2020.acl-main.131.txt,2020,7 Conclusion & Future Work,"under this framework, mutual persona perception is incorporated as a reward signal to achieve the personalized dialogue generation."
2020.acl-main.131.txt,2020,7 Conclusion & Future Work,"we propose p2 bot, a transmitter-receiver framework which explicitly models understanding between interlocutors."
2020.acl-main.132.txt,2020,7 Conclusions,"compared to previous systems, our model is simple and more realistic in practice: it does not require any gold annotations to construct the list of antecedent candidates."
2020.acl-main.132.txt,2020,7 Conclusions,"finally, we will release our experimental qa datasets (in the squad json format) for bridging anaphora resolution on isnotes and bashi."
2020.acl-main.132.txt,2020,7 Conclusions,"in this paper, we model bridging anaphora resolution as a question answering problem and propose a qa system (barqa) to solve the task."
2020.acl-main.132.txt,2020,7 Conclusions,"moreover, under the proposed qa formulation, our model can be easily strengthened by adding other span-based text understanding qa corpora as pre-training datasets."
2020.acl-main.132.txt,2020,7 Conclusions,they can be used to test a qa model’s ability to understand a text in terms of bridging inference.
2020.acl-main.132.txt,2020,7 Conclusions,we also propose a new method to automatically generate a large scale of “quasi-bridging” training data.
2020.acl-main.132.txt,2020,7 Conclusions,"we show that our qa system, when trained on this “quasi-bridging” training dataset and fine-tuned on a small amount of in-domain dataset, achieves the new state-of-the-art results on two bridging corpora."
2020.acl-main.133.txt,2020,5 Conclusions,"for future work, we would like to deeply study the impacts of our perturbations on the coherence of the examined dialogues."
2020.acl-main.133.txt,2020,5 Conclusions,"our coherence method outperforms its counterparts for discriminating dialogues from their various perturbations on dailydialog, and (mostly) performs on par with them on switchboard."
2020.acl-main.133.txt,2020,5 Conclusions,"our model (a) benefits from dialogue act prediction task during training to obtain informative utterance vectors, and (b) alleviates the need for gold dialogue act labels during evaluations."
2020.acl-main.133.txt,2020,5 Conclusions,these properties holistically make our model suitable for comparing different dialogue agents in terms of coherence and naturalness.
2020.acl-main.133.txt,2020,5 Conclusions,"unlike previous approaches that utilize these two models in a pipeline, we use them in a multi-task learning scenario where dialogue act prediction is an auxiliary task."
2020.acl-main.133.txt,2020,5 Conclusions,we propose a novel dialogue coherence model whose utterance encoder layers are shared with a dialogue act prediction model.
2020.acl-main.133.txt,2020,5 Conclusions,we will also investigate to what extent the rankings of dialogues obtained by our model correlate with human-provided rankings.
2020.acl-main.134.txt,2020,4 Conclusion,"in this paper, we revisit the idea of treating word ordering as a tsp, but unlike the common bag-of-words scenario, the words have an underlying syntactic structure."
2020.acl-main.134.txt,2020,4 Conclusion,one possibility to capitalize on these synergies is to explore data augmentation methods to select beneficial extra training data in an unsupervised fashion.
2020.acl-main.134.txt,2020,4 Conclusion,our work emphasizes the importance of syntax in the word ordering task.
2020.acl-main.134.txt,2020,4 Conclusion,"we believe that quite generally, systems for solving one task can benefit from the other task’s view on syntactic structure."
2020.acl-main.134.txt,2020,4 Conclusion,"we demonstrate that with the tree-lstm encoder, the biaffine scoring model, the divide-and-conquer strategy, and a transition-based sorting system, we can linearize a dependency tree with high speed and quality and without the projectivity restriction."
2020.acl-main.134.txt,2020,4 Conclusion,we discussed many connections and similarities between linearization and parsing.
2020.acl-main.134.txt,2020,4 Conclusion,we show with various ablation experiments that all of the components are crucial for the success of the tsp-based linearizer.
2020.acl-main.135.txt,2020,5 Conclusion and Future Works,"although dp-based and srl-based semantic parsing are widely used, more advanced semantic representations could also be explored, such as discourse structure representation (van noord et al., 2018; liu et al., 2019b) and knowledge graph-enhanced text representations (cao et al., 2017; yang et al., 2019)."
2020.acl-main.135.txt,2020,5 Conclusion and Future Works,"experiments on the hotpotqa dataset demonstrate that introducing semantic graph significantly reduces the semantic errors, and content selection benefits the selection and reasoning over disjoint relevant contents, leading to questions with better quality."
2020.acl-main.135.txt,2020,5 Conclusion and Future Works,"first, graph structure that can accurately represent the semantic meaning of the document is crucial for our model."
2020.acl-main.135.txt,2020,5 Conclusion and Future Works,"second, our method can be improved by explicitly modeling the reasoning chains in generation of deep questions, inspired by related methods (lin et al., 2018; jiang and bansal, 2019) in multi-hop question answering."
2020.acl-main.135.txt,2020,5 Conclusion and Future Works,there are at least two potential future directions.
2020.acl-main.135.txt,2020,5 Conclusion and Future Works,"to this end, we propose a novel framework which incorporates semantic graphs to enhance the input document representations and generate questions by jointly training with the task of content selection."
2020.acl-main.135.txt,2020,5 Conclusion and Future Works,we propose the problem of dqg to generate questions that requires reasoning over multiple disjoint pieces of information.
2020.acl-main.136.txt,2020,5 Conclusion,"as a consequent, our model can simultaneously extract multiple relational triples from sentences, without suffering from the overlapping problem."
2020.acl-main.136.txt,2020,5 Conclusion,"experimental results show that our model overwhelmingly outperforms state-of-the-art baselines over different scenarios, especially on the extraction of overlapping relational triples."
2020.acl-main.136.txt,2020,5 Conclusion,"in this paper, we introduce a novel cascade binary tagging framework (casrel) derived from a principled problem formulation for relational triple extraction."
2020.acl-main.136.txt,2020,5 Conclusion,"instead of modeling relations as discrete labels of entity pairs, we model the relations as functions that map subjects to objects, which provides a fresh perspective to revisit the relational triple extraction task."
2020.acl-main.136.txt,2020,5 Conclusion,we conduct extensive experiments on two widely used datasets to validate the effectiveness of the proposed casrel framework.
2020.acl-main.137.txt,2020,6 Conclusions,"a human reader may, nevertheless, find many incorrect extractions informative."
2020.acl-main.137.txt,2020,6 Conclusions,"a strong scientific ie system is used as a baseline, and its output is used to filter the relations found by a state-of-the-art oie system."
2020.acl-main.137.txt,2020,6 Conclusions,an issue for humans is the sheer amount of oie extractions and the high proportion of uninformative extractions.
2020.acl-main.137.txt,2020,6 Conclusions,"as a result, sore improves the ability for a reader to quickly skim through the remaining extractions, or sentences that they are sourced from, and analyze how central concepts are related in a scientific text."
2020.acl-main.137.txt,2020,6 Conclusions,oie from scientific text is a hard task.
2020.acl-main.137.txt,2020,6 Conclusions,"similarly, sore may aid the collection of a dataset for scientific oie."
2020.acl-main.137.txt,2020,6 Conclusions,the large number of errors that we find in oie extractions from scientific texts render them near-useless to downstream computing tasks.
2020.acl-main.137.txt,2020,6 Conclusions,"the presented approach is currently limited to the domain of biology and the use of trade-off relations, but we expect that central relations can be identified for other scientific domains that enable sore."
2020.acl-main.137.txt,2020,6 Conclusions,"we adapt off-the-shelf ie systems to show that sore is feasible, and that our approach is worth improving upon – both in terms of performance, as well as reducing the system’s complexity."
2020.acl-main.137.txt,2020,6 Conclusions,we introduce the task of semi-open relation extraction (sore) on scientific texts and the focused open biological information extraction (fobie) dataset.
2020.acl-main.137.txt,2020,6 Conclusions,we show that creating a dataset for narrow re can be done relatively cheaply by re-annotating the output of a simple rbs.
2020.acl-main.137.txt,2020,6 Conclusions,"we show that our approach to sore reduces the number of oie extractions by 65%, while increasing the relative amount of informative extractions by 5.75%."
2020.acl-main.138.txt,2020,6 Conclusions,"another application is data anonymization (mamede et al., 2016)."
2020.acl-main.138.txt,2020,6 Conclusions,future work will involve improvements in the proposed noise model to study the importance of fidelity to real-world error patterns.
2020.acl-main.138.txt,2020,6 Conclusions,"in this paper, we investigated the difference in accuracy between sequence labeling performed on clean and noisy text (§2.3)."
2020.acl-main.138.txt,2020,6 Conclusions,it may support an automatic correction method that uses recognized entity types to narrow the list of feasible correction candidates.
2020.acl-main.138.txt,2020,6 Conclusions,"moreover, we avoided expensive re-training of embeddings on noisy data sources by employing existing text representations."
2020.acl-main.138.txt,2020,6 Conclusions,"moreover, we plan to evaluate nat on other real noise distributions (e.g., from asr) and other sequence labeling tasks to support our claims further."
2020.acl-main.138.txt,2020,6 Conclusions,"our experiments confirmed that nat consistently improved efficiency of popular sequence labeling models on data perturbed with different error distributions, preserving accuracy on the original input (§4)."
2020.acl-main.138.txt,2020,6 Conclusions,we conclude that nat makes existing models applicable beyond the idealized scenarios.
2020.acl-main.138.txt,2020,6 Conclusions,we developed the noise induction procedure that simulates the real noisy input (§3.2).
2020.acl-main.138.txt,2020,6 Conclusions,we formulated the noisy sequence labeling problem (§2.2) and introduced a model that can be used to estimate the real noise distribution (§3.1).
2020.acl-main.138.txt,2020,6 Conclusions,"we proposed two noise-aware training methods that boost sequence labeling accuracy on the perturbed text: (i) our data augmentation approach uses a mixture of clean and noisy examples during training to make the model resistant to erroneous input (§3.3).(ii) our stability training algorithm encourages output similarity for the original and the perturbed input, which helps the model to build a noise invariant latent representation (§3.4)."
2020.acl-main.139.txt,2020,5 Conclusion,a neural sequence labelling model can finally be learned on the basis of these unified predictions.
2020.acl-main.139.txt,2020,5 Conclusion,evaluation results on two datasets (conll 2003 and news articles from reuters and bloomberg) show that the method can boost ner performance by about 7 percentage points on entity-level f1.
2020.acl-main.139.txt,2020,5 Conclusion,"furthermore, some of the labelling functions can be rather noisy and model selection of the optimal subset of the labelling functions might well improve the performance of our model."
2020.acl-main.139.txt,2020,5 Conclusion,"furthermore, unlike previous weak supervision approaches, labelling functions may produce probabilistic predictions."
2020.acl-main.139.txt,2020,5 Conclusion,"future work will investigate how to take into account potential correlations between labelling functions in the aggregation model, as done in e.g.(bach et al., 2017)."
2020.acl-main.139.txt,2020,5 Conclusion,"in particular, the proposed model outperforms the unsupervised domain adaptation approach through contextualised embeddings of han and eisenstein (2019)."
2020.acl-main.139.txt,2020,5 Conclusion,labelling functions may be specialised to recognise specific labels while ignoring others.
2020.acl-main.139.txt,2020,5 Conclusion,model selection approaches that can be adapted are discussed in adams and beling (2019); hubin (2019).
2020.acl-main.139.txt,2020,5 Conclusion,"of specific linguistic interest is the contribution of document-level labelling functions, which take advantage of the internal coherence and narrative structure of the texts."
2020.acl-main.139.txt,2020,5 Conclusion,the outputs of these labelling functions are then merged together using a hidden markov model whose parameters are estimated with the baum-welch algorithm.
2020.acl-main.139.txt,2020,5 Conclusion,this paper presented a weak supervision model for sequence labelling tasks such as named entity recognition.
2020.acl-main.139.txt,2020,5 Conclusion,"to leverage all possible knowledge sources available for the task, the approach uses a broad spectrum of labelling functions, including data-driven ner models, gazetteers, heuristic functions, and document-level relations between entities."
2020.acl-main.139.txt,2020,5 Conclusion,we also wish to evaluate the approach on other types of sequence labelling tasks beyond named entity recognition.
2020.acl-main.14.txt,2020,5 Conclusion,"different from the conventional approaches only considering the semantic features, we jointly leverage the latent geometric structure information and the semantic features to optimize the argument representations, which could improve the semantic understanding of discourse."
2020.acl-main.14.txt,2020,5 Conclusion,experimental results on the pdtb show the effectiveness of our model.
2020.acl-main.14.txt,2020,5 Conclusion,"in this paper, we propose a novel transs-driven joint learning neural network framework by optimizing the discourse argument representations to improve implicit discourse relation recognition."
2020.acl-main.14.txt,2020,5 Conclusion,"we interpret the discourse relations as translation in low-dimensional embedding space, which reflects the geometric structure of argument-relation, and also can obtain the richer argument representations based on the multi-level encoder."
2020.acl-main.140.txt,2020,6 Conclusion,"for example, we found self-attentive encoders to be well suited for the re on sentences of different complexity, though they consistently perform lower on probing tasks; hinting that these architectures capture “deeper” linguistic features."
2020.acl-main.140.txt,2020,6 Conclusion,"in future work, we want to extend the probing tasks to also cover specific linguistic patterns such as appositions, and also investigate a model’s ability of generalizing to specific entity types, e.g.company and person names."
2020.acl-main.140.txt,2020,6 Conclusion,"we also showed that the bias induced by different architectures clearly affects the learned properties, as suggested by probing task performance, e.g.for distance and dependency related probing tasks."
2020.acl-main.140.txt,2020,6 Conclusion,"we conducted a comprehensive evaluation of common re encoder architectures, and studied the effect of explicitly and implicitly provided semantic and syntactic knowledge, uncovering interesting properties about the architecture and input features."
2020.acl-main.140.txt,2020,6 Conclusion,we introduced a set of probing tasks to study the linguistic features captured in sentence encoder representations trained on relation extraction.
2020.acl-main.141.txt,2020,5 Conclusion,one possible direction is to extend the scope of structure induction for constructions of nodes without relying on an external parser.
2020.acl-main.141.txt,2020,5 Conclusion,there are multiple avenues for future work.
2020.acl-main.141.txt,2020,5 Conclusion,"unlike previous approaches that rely on syntactic trees, co-references or heuristics, lsr dynamically learns a documentlevel structure and makes predictions in an end-to-end fashion."
2020.acl-main.141.txt,2020,5 Conclusion,we introduce a novel latent structure refinement (lsr) model for better reasoning in the documentlevel relation extraction task.
2020.acl-main.142.txt,2020,7 Conclusion and Future Work,"in addition, we categorized model misclassifications into 9 common re error categories and observed that models are often unable to predict a relation, even if it is expressed explicitly."
2020.acl-main.142.txt,2020,7 Conclusion and Future Work,in an automated evaluation we verified our error hypotheses on the whole test split and showed that two groups of ambiguous relations are responsible for most of the remaining errors.
2020.acl-main.142.txt,2020,7 Conclusion and Future Work,"in this paper, we conducted a thorough evaluation of the tacred re task."
2020.acl-main.142.txt,2020,7 Conclusion and Future Work,"models also frequently do not recognize argument roles correctly, or ignore the sentential context."
2020.acl-main.142.txt,2020,7 Conclusion and Future Work,this clearly highlights the need for careful evaluation of development and test splits when creating datasets via crowdsourcing.
2020.acl-main.142.txt,2020,7 Conclusion and Future Work,"to improve the evaluation accuracy and reliability of future re methods, we provide a revised, extensively relabeled tacred."
2020.acl-main.142.txt,2020,7 Conclusion and Future Work,we also showed that models adopt heuristics when entities are unmasked and proposed that evaluation metrics should consider an instance’s difficulty.
2020.acl-main.142.txt,2020,7 Conclusion and Future Work,"we validated the 5k most challenging examples in development and test set and showed that labeling is a major error source, accounting for 8% absolute f1 error on the test set."
2020.acl-main.143.txt,2020,6 Conclusion,"experiments show that, on both close language pairs and distant language pairs, our proposed approach effectively reduces the gap between the source language space and the target language space, leading to significant improvement of translation quality over the mt approaches that do not use the dictionary and the approaches that use the dictionary to supervise the cross-lingual word embedding transformation."
2020.acl-main.143.txt,2020,6 Conclusion,"in this paper, we explore how much potential an mt system can achieve when only using a bilingual dictionary and large-scale monolingual corpora."
2020.acl-main.143.txt,2020,6 Conclusion,this task simulates people acquiring translation ability via looking up the dictionary and depending on no parallel sentence examples.
2020.acl-main.143.txt,2020,6 Conclusion,we propose to tackle the task by injecting the bilingual dictionary into mt via anchored training that drives both language spaces closer so that the translation becomes easier.
2020.acl-main.144.txt,2020,6 Conclusions and Further Work,"as regards distributed representations, we plan to study alternative networks to more accurately model the identification and incorporation of additional context."
2020.acl-main.144.txt,2020,6 Conclusions and Further Work,"based on “neural fuzzy repair” technique, we introduce tighter integration of fuzzy matches informing neural network of source and target and propose extension to similar translations retrieved from their distributed representations."
2020.acl-main.144.txt,2020,6 Conclusions and Further Work,"in our future work, we plan to optimise the thresholds used with the retrieval algorithms in order to more intelligently select those translations providing richest information to the nmt model and generalize the use of edit distance on the target side."
2020.acl-main.144.txt,2020,6 Conclusions and Further Work,this paper explores augmentation methods for boosting neural machine translation performance by using similar translations.
2020.acl-main.144.txt,2020,6 Conclusions and Further Work,we perform data augmentation at inference time with negligible speed overhead and release an open-source toolkit with an efficient and flexible fuzzy-match implementation.
2020.acl-main.144.txt,2020,6 Conclusions and Further Work,we show that the different types of similar translations and model fine-tuning provide complementary information to the neural model outperforming consistently and significantly previous work.
2020.acl-main.144.txt,2020,6 Conclusions and Further Work,"we would also like to explore better techniques to inject information of small-size n-grams with possible convergence with terminology injection techniques, unifying framework where target clues are mixed with source sentence during translation."
2020.acl-main.145.txt,2020,6 Conclusion,"in future work, we will extend our analysis to include additional source and target languages from different language families, such as more asian languages."
2020.acl-main.145.txt,2020,6 Conclusion,"our experiments show that self-attention performs very well on characterlevel translation, with character-level architectures performing competitively when compared to equivalent subword-level architectures while requiring fewer parameters."
2020.acl-main.145.txt,2020,6 Conclusion,training on multiple input languages is also effective and leads to improvements across all languages when the source and target languages are similar.
2020.acl-main.145.txt,2020,6 Conclusion,we performed a detailed investigation of the utility of self-attention models for character-level translation.
2020.acl-main.145.txt,2020,6 Conclusion,"we test the standard transformer architecture, as well as introduce a novel variant which augments the transformer encoder with convolutions, to facilitate information propagation across nearby characters."
2020.acl-main.145.txt,2020,6 Conclusion,"we will also work towards improving the training efficiency of character-level models, which is one of their main bottlenecks, as well as towards improving their effectiveness in multilingual training."
2020.acl-main.145.txt,2020,6 Conclusion,"when the languages are different, we observe a drop in performance, in particular for the distant language."
2020.acl-main.146.txt,2020,7 Conclusion,"as the resulting model repurposes a pre-trained translation model without changing its parameters, it can directly benefit from improvements in translation quality, e.g.by adaptation via fine-tuning."
2020.acl-main.146.txt,2020,7 Conclusion,in a final step the model is re-trained to leverage full target context with a guided alignment loss.
2020.acl-main.146.txt,2020,7 Conclusion,"our approach extends a pre-trained state-of-the-art neural translation model with an additional alignment layer, which is trained in isolation without changing the parameters used for the translation task."
2020.acl-main.146.txt,2020,7 Conclusion,our results on three language pairs are consistently superior to both giza++ and prior work on end-to-end neural alignment.
2020.acl-main.146.txt,2020,7 Conclusion,this work presents the first end-to-end neural approach to the word alignment task which consistently outperforms giza++ in terms of alignment error rate.
2020.acl-main.146.txt,2020,7 Conclusion,we introduce a novel auxiliary loss function to encourage contiguity in the alignment matrix and a symmetrization algorithm that jointly optimizes the alignment matrix within two models which are trained in opposite directions.
2020.acl-main.147.txt,2020,4 Conclusion,"conversely, dependency-aware self-attention mechanisms (lisa and pascal) best embed syntax, for all corpus sizes, with pascal consistently outperforming other all approaches."
2020.acl-main.147.txt,2020,4 Conclusion,our results show that exploiting core components of the transformer to embed linguistic knowledge leads to higher and consistent gains than previous approaches.
2020.acl-main.147.txt,2020,4 Conclusion,this study provides a thorough investigation of approaches to induce syntactic knowledge into self-attention networks.
2020.acl-main.147.txt,2020,4 Conclusion,"through extensive evaluations on various translation tasks, we find that approaches effective for rnns do not necessarily transfer to transformers (e.g.+s&h)."
2020.acl-main.148.txt,2020,7 Conclusion and Future Work,"in the future, we will develop lightweight alternatives to lalt to reduce the number of model parameters."
2020.acl-main.148.txt,2020,7 Conclusion and Future Work,"our experiments on this dataset show that the proposed approaches substantially increase translation performance, narrowing the performance gap with bilingual nmt models and pivot-based methods."
2020.acl-main.148.txt,2020,7 Conclusion and Future Work,"this paper explores approaches to improve massively multilingual nmt, especially on zero-shot translation."
2020.acl-main.148.txt,2020,7 Conclusion and Future Work,we empirically demonstrate the feasibility of backtranslation in massively multilingual settings to allow for massively zero-shot translation for the first time.
2020.acl-main.148.txt,2020,7 Conclusion and Future Work,"we find that multi-lingual nmt often generates off-target translations on zero-shot directions, and propose to correct it with a random online backtranslation algorithm."
2020.acl-main.148.txt,2020,7 Conclusion and Future Work,"we release opus-100, a multilingual dataset from opus including 100 languages with around 55m sentence pairs for future study."
2020.acl-main.148.txt,2020,7 Conclusion and Future Work,"we show that multilingual nmt suffers from weak capacity, and propose to enhance it by deepening the transformer and devising language-aware neural models."
2020.acl-main.148.txt,2020,7 Conclusion and Future Work,"we will also exploit novel strategies to break the upper bound of robt and obtain larger zero-shot improvements, such as generative modeling (zhang et al., 2016; su et al., 2018; garcía et al., 2020; zheng et al., 2020)."
2020.acl-main.149.txt,2020,6 Conclusion,"differently from bleu and other metrics, ours is language- and tokenization-agnostic, enabling the first systematic and controlled study of cross-lingual translation difficulties."
2020.acl-main.149.txt,2020,6 Conclusion,"in future work, we plan to extend this analysis across more translation pairs, more diverse languages and multiple domains, as well as investigating the effect of translationese or source-side grammatical errors (anastasopoulos, 2019)."
2020.acl-main.149.txt,2020,6 Conclusion,"in this work, we propose a novel informationtheoretic approach, xmi, to measure the translation difficulty of probabilistic mt models."
2020.acl-main.149.txt,2020,6 Conclusion,"our results show that xmi correlates well with bleu scores when translating into the same language (where they are comparable), and that higher bleu scores in different languages do not necessarily imply easier translations."
2020.acl-main.15.txt,2020,7 Conclusion,"in this paper, we conducted a comprehensive study on nar models in nmt, asr and tts tasks to analyze several research questions, including the difficulty of nar generation and why knowledge distillation and alignment constraint can help nar models."
2020.acl-main.15.txt,2020,7 Conclusion,"through a series of empirical studies, we demonstrate that the difficulty of nar generation correlates on the target token dependency, and knowledge distillation as well as alignment constraint reduces the dependency of target tokens and encourages the model to rely more on source context for target token prediction, which improves the accuracy of nar models."
2020.acl-main.15.txt,2020,7 Conclusion,we believe our analyses can shed light on the understandings and further improvements on nar models.
2020.acl-main.15.txt,2020,7 Conclusion,"we design a novel comma and a metric called attention density ratio to measure the dependency on target context when predicting a target token, which can analyze these questions in a unified method."
2020.acl-main.150.txt,2020,5 Conclusion,experiments show that our method achieves remarkable improvements over state-of-the-art multilingual nmt baselines and produces comparable performance with strong individual models.
2020.acl-main.150.txt,2020,5 Conclusion,we have introduced a language-aware interlingua module to tackle the language diversity problem for multilingual nmt.
2020.acl-main.151.txt,2020,6 Conclusion,existing semantically-motivated metrics for reference-free evaluation of mt systems have so far displayed rather poor correlation with human estimates of translation quality.
2020.acl-main.151.txt,2020,6 Conclusion,"first, they portray the viability of referencefree mt evaluation and warrant wider research efforts in this direction."
2020.acl-main.151.txt,2020,6 Conclusion,"in this work, we investigate a range of reference-free metrics based on cutting-edge models for inducing cross-lingual semantic representations: cross-lingual (contextualized) word embeddings and cross-lingual sentence embeddings."
2020.acl-main.151.txt,2020,6 Conclusion,"second, they indicate that reference-free mt evaluation may be the most challenging (“adversarial”) evaluation task for multi-lingual text encoders as it uncovers some of their shortcomings—prominently, the inability to capture semantically non-sensical word-by-word translations or paraphrases—which remain hidden in their common evaluation scenarios."
2020.acl-main.151.txt,2020,6 Conclusion,we believe our results have two relevant implications.
2020.acl-main.151.txt,2020,6 Conclusion,"we have identified some scenarios in which these metrics fail, prominently their inability to punish literal word-by-word translations (the so-called “translationese”)."
2020.acl-main.151.txt,2020,6 Conclusion,"we have investigated two different mechanisms for mitigating this undesired phenomenon: (1) an additional (weakly-supervised) cross-lingual alignment step, reducing the mismatch between representations of mutual translations, and (2) language modeling (lm) on the target side, which is inherently equipped to punish “unnatural” sentences in the target language."
2020.acl-main.151.txt,2020,6 Conclusion,we release our metrics under the name xmover-score publicly: https://github.com/aiphes/ acl20-reference-free-mt-evaluation.
2020.acl-main.151.txt,2020,6 Conclusion,we show that the reference-free coupling of cross-lingual similarity scores with the target-side language model surpasses the reference-based bleu in segment-level mt evaluation.
2020.acl-main.152.txt,2020,6 Conclusion and Future Work,"by constraining on a target side trie during decoding, beam search can approximate pairwise comparison between source and target sentences."
2020.acl-main.152.txt,2020,6 Conclusion and Future Work,"finally, scalability is a key issue in large-scale mining of parallel corpora, where both quantity and quality are of concern."
2020.acl-main.152.txt,2020,6 Conclusion and Future Work,maximising machine translation scores is biased towards finding machine translated text produced by a similar model.
2020.acl-main.152.txt,2020,6 Conclusion and Future Work,more research is needed on this problem given the prevalent usage of nmt.
2020.acl-main.152.txt,2020,6 Conclusion and Future Work,our method achieves a comparable f1 score to existing systems with a vanilla architecture and data.
2020.acl-main.152.txt,2020,6 Conclusion and Future Work,"the scalability of direct sentence alignment without a document aligner has not been thoroughly investigated in our work, as well as other related work."
2020.acl-main.152.txt,2020,6 Conclusion and Future Work,"thus, overall we present an interesting way of finding parallel sentences through trie-constrained decoding."
2020.acl-main.152.txt,2020,6 Conclusion and Future Work,we bring a new insight into using nmt as a similarity scorer for sentences in different languages.
2020.acl-main.152.txt,2020,6 Conclusion and Future Work,"we hypothesise that part of the success of dual conditional cross-entropy filtering (junczysdowmunt, 2018) is checking that scores in both directions are approximately equal, whereas a machine translation would be characterised by a high score in one direction."
2020.acl-main.153.txt,2020,6 Conclusions and Future Work,experiments indicated that the proposed strategies consistently improve the translation performance.
2020.acl-main.153.txt,2020,6 Conclusions and Future Work,"in the future, we plan to extend the cross-lingual position encoding to non-autoregressive mt (gu et al., 2018) and unsupervised nmt (lample et al., 2018)."
2020.acl-main.153.txt,2020,6 Conclusions and Future Work,"in this paper, we presented a novel cross-lingual position encoding to augment sans by considering cross-lingual information (i.e., reordering indices) for the input sentence."
2020.acl-main.153.txt,2020,6 Conclusions and Future Work,we designed two strategies to integrate it into sans.
2020.acl-main.154.txt,2020,6 Conclusion,"on average, translations make the author seem substantially older and more male."
2020.acl-main.154.txt,2020,6 Conclusion,"translating from english into any of the other languages shows more mixed results, but similar tendencies."
2020.acl-main.154.txt,2020,6 Conclusion,"we find that independent of the mt system and the translation quality, the predicted demographics differ systematically when translating into english."
2020.acl-main.154.txt,2020,6 Conclusion,we test what demographic profiles author attribute tools predict for the translations from various commercially available machine translation tools.
2020.acl-main.155.txt,2020,6 Conclusion,"an exploration of multi-modal measuring approaches (herbig et al., 2019b) shows the feasibility of this, so we will try to combine explicit multi-modal input, as done in this work, with implicit multi-modal sensor input to better model and support the user during pe."
2020.acl-main.155.txt,2020,6 Conclusion,"as a next step, we will integrate the participants’ valuable feedback to improve the prototype."
2020.acl-main.155.txt,2020,6 Conclusion,"for deletions and reorderings, pen and touch both received high subjective ratings, with pen being even better than mouse & keyboard."
2020.acl-main.155.txt,2020,6 Conclusion,"for insertions and replacements, speech and multi-modal interaction were seen as suitable interaction modes; however, mouse & keyboard were still favored and faster here."
2020.acl-main.155.txt,2020,6 Conclusion,"furthermore, participants in herbig et al.(2019a) were positive regarding the idea of a user interface that adapts to measured cognitive load, especially if it automatically provides additional resources like tm matches or mt proposals."
2020.acl-main.155.txt,2020,6 Conclusion,"in terms of timings, they were also among the fastest for these two operations."
2020.acl-main.155.txt,2020,6 Conclusion,the study shows a high level of interest and enthusiasm for using these new modalities.
2020.acl-main.155.txt,2020,6 Conclusion,"this paper therefore presents mmpe, a cat prototype combining pen, touch, speech, and multi-modal interaction together with common mouse and keyboard input possibilities, and explores the use of these modalities by professional translators."
2020.acl-main.155.txt,2020,6 Conclusion,"while more and more professional translators are switching to the use of pe to increase productivity and reduce errors, current cat interfaces still heavily focus on traditional mouse and keyboard input, even though the literature suggests that other modalities could support pe operations well."
2020.acl-main.155.txt,2020,6 Conclusion,"while the presented study provided interesting first insights regarding participants’ use of and preferences for the implemented modalities, it did not allow us to see how they would use the modalities over a longer time period in day-to-day work, which we also want to investigate in the future."
2020.acl-main.156.txt,2020,6 Conclusions,"firstly, we found that when properly filtered, common crawl data is not massively noisier than wikipedia."
2020.acl-main.156.txt,2020,6 Conclusions,"in this paper, we have explored the use of the common-crawl-based oscar corpora to train elmo contextualized embeddings for five typologically diverse mid-resource languages."
2020.acl-main.156.txt,2020,6 Conclusions,"our experiments show that common-crawl-based data such as the oscar corpus can be used to train high-quality contextualized embeddings, even for languages for which more standard textual resources lack volume or genre variety."
2020.acl-main.156.txt,2020,6 Conclusions,"our goal was to explore whether the noisiness level of common crawl data, often invoked to criticize the use of such data, could be compensated by its larger size; for some languages, the oscar corpus is several orders of magnitude larger than the corresponding wikipedia."
2020.acl-main.156.txt,2020,6 Conclusions,"secondly, we show that embeddings trained using oscar data consistently outperform wikipedia-based embeddings, to the extent that they allow us to improve the state of the art in pos tagging and dependency parsing for all the 6 chosen treebanks."
2020.acl-main.156.txt,2020,6 Conclusions,"thirdly, we observe that more training epochs generally results in better embeddings even when the training data is relatively small, as is the case for wikipedia."
2020.acl-main.156.txt,2020,6 Conclusions,this could result in better performances in a number of nlp tasks for many non highly resourced languages.
2020.acl-main.156.txt,2020,6 Conclusions,"we have compared them with wikipedia-based elmo embeddings on two classical nlp tasks, pos tagging and parsing, using state-of-the-art neural architectures."
2020.acl-main.157.txt,2020,4 Conclusions,"also, the multi-domain nature of the dataset enables future research in cross-target and cross-domain adaptation, a clear weak point of current models according to our evaluations."
2020.acl-main.157.txt,2020,4 Conclusions,"future research directions might explore the usage of transformer-based models, as well as of models which exploit not only linguistic but also network features, which have been proven to work well for existing stance detection datasets (aldayel and magdy, 2019)."
2020.acl-main.157.txt,2020,4 Conclusions,"our experiments with 11 strong models indicated a consistent (>10%) performance gap between the state-of-the-art and human upperbound, which proves that wt–wt constitutes a strong challenge for current models."
2020.acl-main.157.txt,2020,4 Conclusions,"we presented wt–wt, a large expert-annotated dataset for stance detection with over 50k labeled tweets."
2020.acl-main.158.txt,2020,5 Discussion,"although this paper, together with warstadt et al.(2020), report what is to our knowledge the largestscale targeted syntactic evaluations to date, we emphasize that they are only first steps toward a comprehensive understanding of the syntactic capabilities of contemporary language models."
2020.acl-main.158.txt,2020,5 Discussion,"as each task brings an explicit set of assumptions, complementary assessment methods can collectively provide greater insight into models’ learning outcomes."
2020.acl-main.158.txt,2020,5 Discussion,humans develop extraordinary grammatical capabilities through exposure to natural linguistic input.
2020.acl-main.158.txt,2020,5 Discussion,"in a controlled evaluation of different model classes and datasets, we find model architecture plays a more important role than training data scale in yielding correct syntactic generalizations."
2020.acl-main.158.txt,2020,5 Discussion,"in addition to the insight these results provide about neural nlp systems, they also bear on questions central to cognitive science and linguistics, putting lower bounds on what syntactic knowledge can be acquired from string input alone."
2020.acl-main.158.txt,2020,5 Discussion,it remains to be seen to just what extent contemporary artificial systems do the same.
2020.acl-main.158.txt,2020,5 Discussion,"other methods include classifying sentences as grammatical or ungrammatical (warstadt et al., 2019b), decoding syntactic features from a model’s internal state (belinkov et al., 2017; giulianelli et al., 2018), or transfer learning to a strictly syntactic task such as parsing or pos tagging (hewitt and manning, 2019)."
2020.acl-main.158.txt,2020,5 Discussion,"our circuit-level analysis reveals consistent failure on licensing but inconsistent behavior on other circuits, suggesting that different syntactic circuits make use of different underlying processing capacities."
2020.acl-main.158.txt,2020,5 Discussion,"our results dissociate model perplexity and performance in syntactic generalization tests, suggesting that the two metrics capture complementary features of language model knowledge."
2020.acl-main.158.txt,2020,5 Discussion,targeted syntactic evaluation is just one in a series of complementary methods being developed to assess the learning outcomes of neural language processing models.
2020.acl-main.158.txt,2020,5 Discussion,"this understanding will be further advanced by new targeted-evaluation test suites covering a still wider variety of syntactic phenomena, additional trained models with more varied hyperparameters and randomization seeds, and new architectural innovations."
2020.acl-main.158.txt,2020,5 Discussion,this work addresses multiple open questions about syntactic evaluations and their relationship to other language model assessments.
2020.acl-main.159.txt,2020,5 Conclusions,german number inflection has been claimed to have distributional properties which make it difficult for neural networks to model.
2020.acl-main.159.txt,2020,5 Conclusions,"nonetheless, the german plural system continues to challenge ed architectures."
2020.acl-main.159.txt,2020,5 Conclusions,"on novel nouns, it generalizes the contextually most frequent plural marker /-e/; its predictions are less variable than speaker productions, and show different patterns of response to words which are phonologically typical (rhymes) as opposed to atypical (non-rhymes)."
2020.acl-main.159.txt,2020,5 Conclusions,"our experimental speaker data does not necessarily support all of these claims; in particular, /-s/ does not appear to be the only plural suffix which speakers treat as a ‘default’ for phonologically unfamiliar words, as the more frequent marker /-(e)n/ shows similar trends."
2020.acl-main.159.txt,2020,5 Conclusions,our neural model struggles to accurately predict the distribution of /-s/ for existing german nouns.
2020.acl-main.159.txt,2020,5 Conclusions,"regardless of the minority-default question, it seems that ed models do not necessarily function as good cognitive approximations for inflectional systems like german number, in which no class holds the majority."
2020.acl-main.16.txt,2020,7 Conclusions,"overall, our results can be understood as a successful instance of transfer learning from a unimodal task (text-to-text translation) to a cross-modal task (image-to-text generation), which allows us to indirectly leverage the abundance of text-only parallel data annotations across many languages to improve the quality of an annotation-poor cross-modal setup."
2020.acl-main.16.txt,2020,7 Conclusions,"surprisingly, by considering the generated outputs in the original language of the annotation (stabilizer outputs), we find that the quality of the stabilizers is higher compared to the outputs of a model trained on the original annotated data."
2020.acl-main.16.txt,2020,7 Conclusions,"the result is a multilingual engine capable of generating high-quality outputs in the target languages, with no gold annotations needed for these languages."
2020.acl-main.16.txt,2020,7 Conclusions,"we present a cross-modal language generation approach called plugs, which successfully combines the availability of an existing gold annotation (usually in english) with the availability of translation engines that automatically produce silver-data annotations."
2020.acl-main.16.txt,2020,7 Conclusions,"we show that, for image captioning, the plugs approach out-performs other alternatives, while also providing the ability to pack multiple languages in a single model for increased performance."
2020.acl-main.160.txt,2020,5 Discussion,a model that solves problems that only one capable of inducing syntactic structure can solve may as well have induced syntactic structure from a practical standpoint.
2020.acl-main.160.txt,2020,5 Discussion,"both alternative sources of information are well known in the community and have been tested in the past (bernardy and lappin, 2017; gulordava et al., 2018)."
2020.acl-main.160.txt,2020,5 Discussion,"chomsky, 1957; rimell et al., 2009; nivre et al., 2010; bender et al., 2011; everaert et al., 2015)."
2020.acl-main.160.txt,2020,5 Discussion,consequence-based analysis would be implemented over naturalistic data rather than templates by embedding it in higher level tasks like question answering to mitigate the unnaturalness problem and demonstrate a model’s practical utility.
2020.acl-main.160.txt,2020,5 Discussion,"even though the effect sizes of both our baseline replications were smaller, pvsl could have reported the results from our baseline models instead of their actual model and drawn the same conclusions."
2020.acl-main.160.txt,2020,5 Discussion,"first, success in the priming paradigm is measured by whether or not adaptation reduces surprisal, but not by how much, so even though both baseline models tested here reduce surprisal by less than pvsl’s models on average, they still pass the success criterion."
2020.acl-main.160.txt,2020,5 Discussion,"given the well-known observation that neural models will “take the easy way out” given the presence of this unintended surface information (jia and liang, 2017; naik et al., 2018; sennhauser and berwick, 2018), and other work suggesting that lstms do not necessarily induce syntactic structure (gupta and lewis, 2018; mccoy et al., 2018; warstadt et al., 2019), one must take successes in template-based probing studies with a grain of salt."
2020.acl-main.160.txt,2020,5 Discussion,"gulordava et al., 2018), which might decrease the lexical similarity problem."
2020.acl-main.160.txt,2020,5 Discussion,it follows then that no-explicit- and explicit-syntax models should exhibit quantitatively different behavior on tasks that require parsing such sentences.
2020.acl-main.160.txt,2020,5 Discussion,"marcus (1984) discusses in a theory-independent way which kinds of sentences a model capturing syntax should be able to parse but a “no-explicit-syntax” model (in the modern context, probably a baseline rnn) should not (cf."
2020.acl-main.160.txt,2020,5 Discussion,one possibility would be to include infelicitous “colorless green ideas” sentences with grammatical syntax (cf.
2020.acl-main.160.txt,2020,5 Discussion,"removing the issue altogether could require enforcing completely lexically disjoint training, adaptation, and test sets, but we cannot reasonably expect a model to function when it has no generalizations to work with, and demanding lexically distinct sets (including function words) greatly limits the set of phenomena that could be studied.5.1 an alternative approach as a more radical alternative, we suggest extending behavioral analysis into “consequence-based” analysis."
2020.acl-main.160.txt,2020,5 Discussion,"second, the fact that our surface word order ngram model and lexical similarity-only scrambled lstm lms also show surprisal effects draws into question the basic claim that only a syntactic model would respond to adaptation: it is our hypothesis that the combined effect of word order and lexical similarity are what drive the lstm models’ larger effect."
2020.acl-main.160.txt,2020,5 Discussion,the evaluation of non-syntactic baselines is an easy-to-implement way to combat the tendency of these behavioral probes to overestimate language models’ abilities.
2020.acl-main.160.txt,2020,5 Discussion,"the possibility of side-channel information is already known in relation to these higher-level tasks (e.g., poliak et al., 2018; geva et al., 2019), and various challenge data sets have been constructed to mitigate it in different ways (levesque et al., 2011; chao et al., 2017; dua et al., 2019; lin et al., 2019; dasigi et al., 2019)."
2020.acl-main.160.txt,2020,5 Discussion,"the two have similar reasoning: from an engineering perspective, a family of models that is capable of inducing syntax is useful because it may be expected to improve performance on downstream tasks."
2020.acl-main.160.txt,2020,5 Discussion,these results call into question the van schijndel and linzen (2018) and prasad et al.(2019) syntactic priming paradigm’s ability to distinguish models which represent syntax from those which rely on shallow phenomena by achieving a positive result with two non-syntactic baseline models.
2020.acl-main.160.txt,2020,5 Discussion,"this highlights a more general problem with template-based probing, namely, that the unnatural lack of sentence diversity imposed by the templates imposes unintended regularity for models to latch onto."
2020.acl-main.160.txt,2020,5 Discussion,"this is upheld, especially when it is noted that the adaptation effects of both baselines complement each other."
2020.acl-main.160.txt,2020,5 Discussion,this reiterates the need for proper baseline testing in computational linguistics and for informative evaluations.
2020.acl-main.160.txt,2020,5 Discussion,"to improve the priming paradigm in particular, one would need to establish a success metric that discriminates between baselines and alter the experimental setup to mitigate information side channels."
2020.acl-main.160.txt,2020,5 Discussion,"to put it another way, pvsl report quantitative results but do not actually establish what would constitute a meaningful effect size."
2020.acl-main.160.txt,2020,5 Discussion,"uniting these with a collection of hard sentence types (e.g., marvin and linzen, 2018; warstadt et al., 2019) in something like a syntax-focused qa challenge set would provide new insights into which families of models capture the practical benefits of true hierarchical syntactic representation."
2020.acl-main.161.txt,2020,7 Conclusions,a more sophisticated model would incorporate similar ideas.
2020.acl-main.161.txt,2020,7 Conclusions,"additional annotations, such as how certain readers are about the outcome of the story, may also be helpful in better understanding the relationship between suspense and uncertainty."
2020.acl-main.161.txt,2020,7 Conclusions,an analogue of this would be adversarial examples used in computer vision.
2020.acl-main.161.txt,2020,7 Conclusions,"automated interpretability methods as proposed by sundararajan et al.(2017), could shed further light on models’ predictions."
2020.acl-main.161.txt,2020,7 Conclusions,autoregressive models that generate step by step alternatives for future continuations are computationally impractical for longer rollouts and are not cognitively plausible.
2020.acl-main.161.txt,2020,7 Conclusions,"however, generating plausible future continuations is an essential part of the model."
2020.acl-main.161.txt,2020,7 Conclusions,"in principle, these would avoid the brute-force calculation of a rollout and conceptually, anticipating longer-term states aligns with theories of suspense."
2020.acl-main.161.txt,2020,7 Conclusions,"in text generation, fan et al.(2019) have found that explicitly incorporating coreference and structured event representations into generation produces more coherent generated text."
2020.acl-main.161.txt,2020,7 Conclusions,it provides a springboard to further interesting applications and research on suspense in storytelling.
2020.acl-main.161.txt,2020,7 Conclusions,"one strand of further work is therefore analysis: text could be artificially manipulated using structural changes, for example by switching the order of sentences, mixing multi-ple stories, including a summary at the beginning that foreshadows the work, masking key suspenseful words, or paraphrasing."
2020.acl-main.161.txt,2020,7 Conclusions,"our overall findings suggest that by implementing concepts from psycholinguistic and economic theory, we can predict human judgements of suspense in storytelling."
2020.acl-main.161.txt,2020,7 Conclusions,related tasks such as inverting the understanding of suspense to utilise the models in generating more suspenseful stories may also prove fruitful.
2020.acl-main.161.txt,2020,7 Conclusions,"strong psycholinguistic claims about suspense are difficult to make due to several weaknesses in our approach, which highlight directions for future research: the proposed model does not have a higher-level understanding of event structure; most likely it picks up the textual cues that accompany dramatic changes in the text."
2020.acl-main.161.txt,2020,7 Conclusions,"that uncertainty reduction (uely) outperforms probability-only (shale) and state-only (sely) surprise suggests that, while consequential state change is of primary importance for suspense, the probability distribution over the states is also a necessary factor."
2020.acl-main.161.txt,2020,7 Conclusions,"the recent success of language models in wideranging nlp tasks (e.g., radford et al., 2019) has shown that language models are capable of learning semantically rich information implicitly."
2020.acl-main.161.txt,2020,7 Conclusions,"there is much recent work (e.g., ha and schmidhuber (2018); gregor et al.(2019)), on state-space approaches that model beliefs as latent states using variational methods."
2020.acl-main.161.txt,2020,7 Conclusions,"they also differ from the ely et al.(2015) conception of suspense, which is in terms of bayesian beliefs over a longer-term future state, not step by step."
2020.acl-main.161.txt,2020,7 Conclusions,"this is more in line with the smuts (2008) desire-frustration view of suspense, where uncertainty is secondary."
2020.acl-main.161.txt,2020,7 Conclusions,this paper is a baseline that demonstrates how modern neural network models can implicitly represent text meaning and be useful in a narrative context without recourse to supervision.
2020.acl-main.161.txt,2020,7 Conclusions,"uncertainty reduction therefore captures the view of suspense as reducing paths to a desired outcome, with more consequential shifts as the story progresses (o’neill and riedl, 2014; ely et al., 2015; perreault, 2018)."
2020.acl-main.162.txt,2020,6 Conclusion,"given previous work in single word reading time prediction, we conducted a large novel study to test whether document level reading time could be predicted."
2020.acl-main.162.txt,2020,6 Conclusion,"we carefully designed an experiment containing a myriad of potential factors to measure reading time, distributed the survey to more than a thousand people, and collected the results into the first dataset of its kind."
2020.acl-main.162.txt,2020,6 Conclusion,we hope this resource can benefit future research into developing techniques to model and understand human responses to document sized text.
2020.acl-main.162.txt,2020,6 Conclusion,"we then employed machine learning techniques to predict the time to read, finding that simpler models were the most competitive, with the number of words as the sole critical factor in predicting reading time."
2020.acl-main.163.txt,2020,6 Conclusion,"as a final remark, natural language is richer and more informal."
2020.acl-main.163.txt,2020,6 Conclusion,"however, formal representations utilised by an nlg system are more precisely-defined."
2020.acl-main.163.txt,2020,6 Conclusion,"in future, we aim to refine our generative model to better emphasise this difference of the two tasks."
2020.acl-main.163.txt,2020,6 Conclusion,nlu needs to handle ambiguous or erroneous user inputs.
2020.acl-main.163.txt,2020,6 Conclusion,"since the two space is coupled, we gain the luxury of exploiting each unpaired data source and transfer the acquired knowledge to the shared meaning space."
2020.acl-main.163.txt,2020,6 Conclusion,the proposed model is also suitable for other translation tasks between two modalities.
2020.acl-main.163.txt,2020,6 Conclusion,"this eventually benefits both nlu and nlg, especially in a low-resource scenario."
2020.acl-main.163.txt,2020,6 Conclusion,we proposed a generative model which couples natural language and formal representations via a shared latent variable.
2020.acl-main.164.txt,2020,8 Conclusion,"as the p in nucleus sampling is set increasingly lower to achieve more fluent text (some systems are already using p as low as 0.5 (miculicich et al., 2019)), the distributional deviations that plague top-k text will surface in nucleus sampling as well."
2020.acl-main.164.txt,2020,8 Conclusion,building better world understanding into automatic discriminators so that they are more capable of detecting the types of errors that humans notice.3.
2020.acl-main.164.txt,2020,8 Conclusion,developing tools and educational materials to improve humans’ ability to detect machine-generated text.
2020.acl-main.164.txt,2020,8 Conclusion,"finally, we would like to note that all of our experiments were performed with english language models, and it remains an open question how the trade-off between ease of human detection and ease of automatic detection might differ for languages that are very different from english."
2020.acl-main.164.txt,2020,8 Conclusion,generation systems often optimize for fooling humans without acknowledging the trade-off that exists between human perception of quality and ease of automatic detection.
2020.acl-main.164.txt,2020,8 Conclusion,holtzman et al.(2020) explain how a unique attribute of human language is that it dips in and out of low probability zones.
2020.acl-main.164.txt,2020,8 Conclusion,"however, any strategy that oversamples high-likelihood words is susceptible."
2020.acl-main.164.txt,2020,8 Conclusion,identifying ways to improve the language models and decoding strategies we use in order to generate text that is both exciting (ie.unlikely) and semantically plausible.2.
2020.acl-main.164.txt,2020,8 Conclusion,"in our experiments, these artifacts are most prominent with top-k sampling."
2020.acl-main.164.txt,2020,8 Conclusion,"in this work, we study the behavior of automated discriminators and their ability to identify machine-generated and human-written texts."
2020.acl-main.164.txt,2020,8 Conclusion,"most interestingly, we find that human raters and discriminators make decisions based on different qualities, with humans more easily noticing semantic errors and discriminators picking up on statistical artifacts."
2020.acl-main.164.txt,2020,8 Conclusion,these may include automatic detectors with components that explain their predictions.
2020.acl-main.164.txt,2020,8 Conclusion,this variance in likelihood is what makes human-written text interesting and exciting to read.
2020.acl-main.164.txt,2020,8 Conclusion,today’s generation systems have not yet solved the problem of mimicking the human cadence without introducing poor word choices that are easy for humans to detect.
2020.acl-main.164.txt,2020,8 Conclusion,we also show the rate at which discriminator accuracy increases as excerpts are lengthened.
2020.acl-main.164.txt,2020,8 Conclusion,"we find that rater accuracy varies wildly, but has a median of 74%, which is less than the accuracy of our best-performing discriminator."
2020.acl-main.164.txt,2020,8 Conclusion,"we find that, in general, discriminators transfer poorly between decoding strategies, but that training on a mix of data from methods can help."
2020.acl-main.164.txt,2020,8 Conclusion,we further study the ability of expert human raters to perform the same task.
2020.acl-main.164.txt,2020,8 Conclusion,we therefore suggest three prongs for future research: 1.
2020.acl-main.164.txt,2020,8 Conclusion,we train these discriminators on balanced binary classification datasets where all machinegenerated excerpts are drawn from the same generative model but with different decoding strategies.
2020.acl-main.165.txt,2020,6 Conclusions,"moreover, we remark that our approach can be extended to other multi-domain or multi-task nlp problems."
2020.acl-main.165.txt,2020,6 Conclusions,the proposed method outperforms the existing embedding based methods.
2020.acl-main.165.txt,2020,6 Conclusions,"unlike the existing work, we construct multi-head dot-product modules for each domain and then combine them by the layer-wise domain proportion of every word."
2020.acl-main.165.txt,2020,6 Conclusions,we also show mixing method can be combined with embedding based methods to make further improvement.
2020.acl-main.165.txt,2020,6 Conclusions,"we present a novel multi-domain nmt with wordlevel layer-wise domain mixing, which can adaptively exploit the domain knowledge."
2020.acl-main.166.txt,2020,5 Conclusion,"experimental results demonstrate the effectiveness of this framework in terms of local appropriateness, global coherence and dialog-target success rate."
2020.acl-main.166.txt,2020,5 Conclusion,"in the future, we will investigate how to extend the cg to support hierarchical topic management in conversational systems."
2020.acl-main.166.txt,2020,5 Conclusion,"in this paper we present a novel graph grounded policy learning framework for open-domain multi-turn conversation, which can effectively leverage prior information about dialog transitions to foster a more coherent and controllable dialog."
2020.acl-main.167.txt,2020,5 Conclusions,"comparison with state-of-the-art models in blue, chrf++, meteor as well as semsim and human evaluation metrics show that while simple, this approach can outperform existing methods including methods training transformers from scratch."
2020.acl-main.167.txt,2020,5 Conclusions,future work will focus on incorporating better encoding of the amr graph into the current system and exploring data augmentation techniques leveraging the proposed approach.
2020.acl-main.167.txt,2020,5 Conclusions,"in this work, we present a language model-based approach for the amr-to-text generation task."
2020.acl-main.167.txt,2020,5 Conclusions,we also show that cycle consistency-based re-scoring using a conventional amr parser and the smatch metric can notably improve the results.
2020.acl-main.167.txt,2020,5 Conclusions,we show that a strong pre-trained transformer language model (gpt-2) can be fine-tuned to generate text directly from the penman notation of an amr graph.
2020.acl-main.168.txt,2020,13 Conclusion,we designed a new approach for this task which aims to correlate cross-modal edits in order to generate a sequence of edit actions specifying how the comment should be updated.
2020.acl-main.168.txt,2020,13 Conclusion,"we find that our model outperforms multiple rule-based baselines and comment generation models, with respect to several automatic metrics and human evaluation."
2020.acl-main.168.txt,2020,13 Conclusion,we have addressed the novel task of automatically updating an existing programming comment based on changes to the related code.
2020.acl-main.169.txt,2020,6 Conclusion,automatic and human evaluation shows that our approach outperforms other state-of-the-art models on content preservation metrics while retaining (or in some cases improving) the transfer accuracies.
2020.acl-main.169.txt,2020,6 Conclusion,"we believe our approach is the first to be robust in cases when the source is style neutral, like the “non-polite” class in the case of politeness transfer."
2020.acl-main.169.txt,2020,6 Conclusion,"we extend prior works (li et al., 2018; sudhakar et al., 2019) on attribute transfer by introducing a simple pipeline – tag & generate which is an interpretable two-staged approach for content preserving style transfer."
2020.acl-main.169.txt,2020,6 Conclusion,we introduce the task of politeness transfer for which we provide a dataset comprised of sentences curated from email exchanges present in the enron corpus.
2020.acl-main.17.txt,2020,6 Conclusion,"experimental results show that the proposed model factedi-tor performs better and faster than the baselines, including an encoder-decoder model."
2020.acl-main.17.txt,2020,6 Conclusion,"first, we have proposed a data construction method for fact-based text editing and created two datasets."
2020.acl-main.17.txt,2020,6 Conclusion,"in this paper, we have defined a new task referred to as fact-based text editing and made two contributions to research on the problem."
2020.acl-main.17.txt,2020,6 Conclusion,"second, we have proposed a model for fact-based text editing, named facteditor, which performs the task by generating a sequence of actions."
2020.acl-main.170.txt,2020,8 Conclusions,future research directions include adaptive dropout rates for different merges and an in-depth analysis of other pathologies in learned token embeddings for different segmentations.
2020.acl-main.170.txt,2020,8 Conclusions,"models trained with bpe-dropout (1) outperform bpe and the previous subword regularization on a wide range of translation tasks, (2) have better quality of learned embeddings, (3) are more robust to noisy input."
2020.acl-main.170.txt,2020,8 Conclusions,"the only difference from bpe is how a word is segmented during model training: bpe-dropout randomly drops some merges from the bpe merge table, which results in different segmentations for the same word."
2020.acl-main.170.txt,2020,8 Conclusions,"we introduce bpe-dropout – simple and effective subword regularization, which operates within the standard bpe framework."
2020.acl-main.171.txt,2020,6 Discussion,are the observed improvements language-dependent?
2020.acl-main.171.txt,2020,6 Discussion,can the nar model perfectly recover the ar model’s performance with much larger monolingual datasets?
2020.acl-main.171.txt,2020,6 Discussion,"other work in nmt has examined this issue in the context of backtranslation (e.g., edunov et al.(2018)), and we expect the conclusions to be similar in the nar-mt case."
2020.acl-main.171.txt,2020,6 Discussion,there are several open questions to investigate: are the benefits of monolingual data orthogonal to other techniques like iterative refinement?
2020.acl-main.171.txt,2020,6 Discussion,we found that monolingual data augmentation reduces overfitting and improves the translation quality of nar-mt models.
2020.acl-main.171.txt,2020,6 Discussion,"we note that the monolingual corpora are derived from domains which may be different from those of the parallel training data or evaluation sets, and a mismatch can affect nar translation performance."
2020.acl-main.171.txt,2020,6 Discussion,we will consider these research directions in future work.
2020.acl-main.172.txt,2020,6 Conclusion,content selection problem is framed as a word-level sequence-tagging task.
2020.acl-main.172.txt,2020,6 Conclusion,"furthermore, the extrinsic evaluation by domain experts further reveals the qualities of our system-generated summaries in comparison with gold summaries."
2020.acl-main.172.txt,2020,6 Conclusion,the intrinsic evaluations on two publicly available real-life clinical datasets show the efficacy of our model in terms of rouge metrics.
2020.acl-main.172.txt,2020,6 Conclusion,we introduced our novel approach to augment standard summarization model with significant ontological terms within the source.
2020.acl-main.172.txt,2020,6 Conclusion,we proposed an approach to content selection for abstractive text summarization in clinical notes.
2020.acl-main.173.txt,2020,7 Conclusion,we conducted a large-scale study of hallucinations in abstractive document summarization.
2020.acl-main.173.txt,2020,7 Conclusion,"we found that (i) tackling hallucination is a critical challenge for abstractive summarization, perhaps the most critical, (ii) nlu-driven pretraining in neural text generators is key to generate informative, coherent, faithful and factual abstracts, but it is still far from solving the problem; and (iii) measures such as rouge or bertscore will not be sufficient when studying the problem; semantic inference-based automatic measures are better representations of true summarization quality."
2020.acl-main.174.txt,2020,6 Conclusions,"although currently our approach relies solely on textual information, it would be interesting to incorporate additional modalities such as video or audio."
2020.acl-main.174.txt,2020,6 Conclusions,"an often integral part of a compelling story is the emotional experience that is evoked in the reader or viewer (e.g., somebody gets into trouble and then out of it, somebody finds something wonderful, loses it, and then finds it again)."
2020.acl-main.174.txt,2020,6 Conclusions,analysis of model output further revealed that latent events encapsulated by turning points correlate with important aspects of a csi summary.
2020.acl-main.174.txt,2020,6 Conclusions,audiovisual information could facilitate the identification of key events and scenes.
2020.acl-main.174.txt,2020,6 Conclusions,"besides narrative structure, we would also like to examine the role of emotional arcs (vonnegut, 1981; reagan et al., 2016) in a screenplay."
2020.acl-main.174.txt,2020,6 Conclusions,"experiments on the csi corpus showed that this scheme transfers well to a different genre (crime investigation) and that utilizing narrative structure boosts summarization performance, leading to more complete and diverse summaries."
2020.acl-main.174.txt,2020,6 Conclusions,in this paper we argued that the underlying structure of narratives is beneficial for long-form summarization.
2020.acl-main.174.txt,2020,6 Conclusions,"understanding emotional arcs may be useful to revealing a story’s shape, high-lighting important scenes, and tracking how the story develops for different characters over time."
2020.acl-main.174.txt,2020,6 Conclusions,"we adapted a scheme for identifying narrative structure (i.e., turning points) in hollywood movies and showed how this information can be integrated with supervised and unsupervised extractive summarization algorithms."
2020.acl-main.175.txt,2020,6 Conclusions,"in the future, we would like to model aspects and sentiment more explicitly as well as apply some of the techniques presented here to unsupervised single-document summarization."
2020.acl-main.175.txt,2020,6 Conclusions,our key insight is to enable the use of supervised techniques by creating synthetic review-summary pairs using noise generation methods.
2020.acl-main.175.txt,2020,6 Conclusions,"our summarization model, denoisesum, introduces explicit denoising, partial copy, and discrimination modules which improve overall summary quality, outperforming competitive systems by a wide margin."
2020.acl-main.175.txt,2020,6 Conclusions,we consider an unsupervised learning setting for opinion summarization where there are only reviews available without corresponding summaries.
2020.acl-main.176.txt,2020,5 Conclusion,"interrogation of control- and dementia-based lms using synthetic transcripts and interpolation of parameters reveals inconsistencies harmful to model performance that can be remediated by incorporating interpolated models and pre-trained embeddings, with significant performance improvements."
2020.acl-main.176.txt,2020,5 Conclusion,"we offer an empirical explanation for the success of the difference between neural lm perplexities in discriminating between dat patients and controls, involving lexical frequency effects."
2020.acl-main.177.txt,2020,9 Discussion and Conclusion,"first, two of our probes (known word perturbation and consistency) are based on the idea of starting from a test item that is classified correctly, and applying a transformation that should result in a classifiable item (for a model that represents word meaning systematically)."
2020.acl-main.177.txt,2020,9 Discussion and Conclusion,our analyses contain two ideas that may be useful for future studies of systematicity.
2020.acl-main.177.txt,2020,9 Discussion and Conclusion,our probes test whether deep learning systems learn to represent linguistic units systematically in the natural language inference task.
2020.acl-main.177.txt,2020,9 Discussion and Conclusion,"our results indicate that despite their high overall performance, these models tend to generalize in ways that allow the meanings of individual words to vary in different contexts, even in an artificial language where a totally systematic solution is available."
2020.acl-main.177.txt,2020,9 Discussion and Conclusion,"second, our analyses made critical use of differential sensitivity (i.e., variance) of the models across test blocks with different novel words but otherwise identical information content."
2020.acl-main.177.txt,2020,9 Discussion and Conclusion,systematicity refers to the property of natural language representations whereby words (and other units or grammatical operations) have consistent meanings across different contexts.
2020.acl-main.177.txt,2020,9 Discussion and Conclusion,"this suggests the networks lack a sufficient inductive bias to learn systematic representations of words like quantifiers, which even in natural language exhibit very little meaning variation."
2020.acl-main.177.txt,2020,9 Discussion and Conclusion,we believe these are a novel ideas that can be employed in future studies.
2020.acl-main.178.txt,2020,6 Conclusion,"additionally, we show that our measures can uncover the effect in language of narrativization of memories over time."
2020.acl-main.178.txt,2020,6 Conclusion,"to investigate the use of nlp tools for studying the cognitive traces of recollection versus imagination in stories, we collect and release hip-pocorpus, a dataset of imagined and recalled stories."
2020.acl-main.178.txt,2020,6 Conclusion,we hope these findings bring attention to the feasibility of employing statistical natural language processing machinery as tools for exploring human cognition.
2020.acl-main.178.txt,2020,6 Conclusion,we introduce measures to characterize narrative flow and influence of semantic vs. episodic knowledge in stories.
2020.acl-main.178.txt,2020,6 Conclusion,"we show that imagined stories have a more linear flow and contain more commonsense knowledge, whereas recalled stories are less connected and contain more specific concrete events."
2020.acl-main.179.txt,2020,8 Discussion,"although our work raises questions about mismatches between human syntactic knowledge and the linguistic representations acquired by neural language models, it also shows that researchers can fruitfully use sentences with multiple interpretations to probe the linguistic representations acquired by those models."
2020.acl-main.179.txt,2020,8 Discussion,"as it stands now, rnn lms are typically trained on production data (that is, the produced text in wikipedia).14 thus, they will have access to only a subset of the biases needed to learn human-like attachment preferences."
2020.acl-main.179.txt,2020,8 Discussion,"as such, the biases in language production are a proper subset of those of language comprehension."
2020.acl-main.179.txt,2020,8 Discussion,"before now, evaluations have focused on cases of unambiguous grammaticality (i.e.ungrammatical vs. grammatical)."
2020.acl-main.179.txt,2020,8 Discussion,"by using stimuli with multiple simultaneous valid interpretations, we found that evaluating models on single-interpretation sentences overestimates their ability to comprehend abstract syntax."
2020.acl-main.179.txt,2020,8 Discussion,experimental findings from psycholinguistics suggest that this issue could follow from a more general mismatch between language production and language comprehension.
2020.acl-main.179.txt,2020,8 Discussion,future work needs to be done to understand more fully what biases are present in the data and learned by language models.
2020.acl-main.179.txt,2020,8 Discussion,"given the recent body of literature suggesting that rnn lms have learned abstract syntactic representations, we tested the hypothesis that these models acquire human-like attachment preferences."
2020.acl-main.179.txt,2020,8 Discussion,"however, it is clear that whatever attachment biases exist in the data are insufficient for rnns to learn a human-like attachment preference in spanish."
2020.acl-main.179.txt,2020,8 Discussion,"in english, the rnns exhibited a human-like low bias, but this preference persisted even in cases where low attachment was ungrammatical."
2020.acl-main.179.txt,2020,8 Discussion,"in its strongest form, this hypothesis suggests that no amount of production data (i.e.raw text) will ever be sufficient for these models to generalizably pattern like humans during comprehension tasks."
2020.acl-main.179.txt,2020,8 Discussion,"in other words, human syntactic representations of attachment overlap, with prepositional attachment influencing relative clause attachment, etc."
2020.acl-main.179.txt,2020,8 Discussion,"in particular, kehler and rohde (2015, 2018) have provided empirical evidence that the production and comprehension of these structures are guided by different biases in humans."
2020.acl-main.179.txt,2020,8 Discussion,"in post-hoc analyses of the spanish wikipedia training corpus and the ancora spanish newswire corpus (taule´ et al., 2008), we find a consistent production bias towards low attachment among the rcs with unambiguous attachment."
2020.acl-main.179.txt,2020,8 Discussion,"in spanish wikipedia, low attachment is 69% more frequent than high attachment, and in spanish newswire data, low attachment is 21% more frequent than high attachment.13 this distributional bias in favor of low attachment does not rule out a subsequent high rc bias in the models."
2020.acl-main.179.txt,2020,8 Discussion,"in this work, we explored the ability of rnn lms to prioritize multiple simultaneous valid interpretations in a human-like way (as in john met the student of the teacher that was happy)."
2020.acl-main.179.txt,2020,8 Discussion,"it has been established in the psycholinguistic literature that attachment is learned by humans as a general abstract feature of language (see scheepers, 2003)."
2020.acl-main.179.txt,2020,8 Discussion,"once again, rnn lms favored low attachment over high attachment."
2020.acl-main.179.txt,2020,8 Discussion,"production is guided by syntactic and information structural considerations (e.g., topic), while comprehension is influenced by those considerations plus pragmatic and discourse factors (e.g., coherence relations)."
2020.acl-main.179.txt,2020,8 Discussion,the inability of rnn lms to learn the spanish high attachment preference suggests that the spanish data may not contain enough high examples to learn human-like attachment preferences.
2020.acl-main.179.txt,2020,8 Discussion,"the mismatch between human interpretation biases and production biases suggested by this work invalidates the tacit assumption in much of the natural language processing literature that standard, production-based training data (e.g., web text) are representative of the linguistic biases needed for natural language understanding and generation."
2020.acl-main.179.txt,2020,8 Discussion,"there are phenomena, like agreement, that seem to have robust manifestations in a production signal, but the present work demonstrates that there are others, like attachment preferences, that do not."
2020.acl-main.179.txt,2020,8 Discussion,these relationships could coalesce during training and result in an attachment preference that differs from any one structure individually.
2020.acl-main.179.txt,2020,8 Discussion,these results suggest that any recency bias in rnn lms is weak enough to be easily overcome by sufficient evidence of high attachment.
2020.acl-main.179.txt,2020,8 Discussion,"this discrepancy is likely the reason that simply adding more data doesn’t improve model quality (e.g., van schijndel et al., 2019; bisk et al., 2020)."
2020.acl-main.179.txt,2020,8 Discussion,this provides compelling evidence that standard training data itself may systematically lack aspects of syntax relevant to performing linguistic comprehension tasks.
2020.acl-main.179.txt,2020,8 Discussion,"to test whether the rnns were over-learning a general low bias of english, we tested whether spanish rnns learned the general high bias in that language."
2020.acl-main.179.txt,2020,8 Discussion,we first used a synthetic language experiment to demonstrate that rnn lms are capable of learning a high bias when high attachment is at least as frequent as low attachment in the training data.
2020.acl-main.179.txt,2020,8 Discussion,we found that they do not.
2020.acl-main.179.txt,2020,8 Discussion,"we speculate that the difference may lie in the inherent ambiguity in attachment, while agreement explicitly disambiguates a relation between two syntactic units."
2020.acl-main.179.txt,2020,8 Discussion,we suspect that there are deep systematic issues leading to this mismatch between the expected distribution of human attachment preferences and the actual distribution of attachment in the spanish training corpus.
2020.acl-main.179.txt,2020,8 Discussion,"while both low attachment (i.e.the teacher was happy) and high attachment (i.e.the student was happy) are equally semantically plausible without a disambiguating context, humans have interpretation preferences for one attachment over the other (e.g., english speakers prefer low attachment and spanish speakers prefer high attachment)."
2020.acl-main.18.txt,2020,5 Conclusion,"in this paper, we propose the new research problem of few-shot natural language generation."
2020.acl-main.18.txt,2020,5 Conclusion,"our approach is simple, easy to implement, while achieving strong performance on various domains."
2020.acl-main.18.txt,2020,5 Conclusion,"our basic idea of acquiring language modeling prior can be potentially extended to a broader scope of generation tasks, based on various input structured data, such as knowledge graphs, sql queries, etc."
2020.acl-main.18.txt,2020,5 Conclusion,the deduction of manual data curation efforts for such tasks is of great potential and importance for many real-world applications.
2020.acl-main.180.txt,2020,5 Discussion,a second hypothesis is that the discrepancy arises from the role of top-down expectations in confusability.
2020.acl-main.180.txt,2020,5 Discussion,because speakers enhance or reduce their speech in ways other than changing duration (see e.g.
2020.acl-main.180.txt,2020,5 Discussion,"confusability, on the other hand, is a token-level phenomenon: contextual expectations will change the confusability of a word."
2020.acl-main.180.txt,2020,5 Discussion,conversational context may make a speaker’s intended message easier or harder to recover from ambiguous acoustics.
2020.acl-main.180.txt,2020,5 Discussion,"first, we provide evidence that speakers lengthen words that are more confusable."
2020.acl-main.180.txt,2020,5 Discussion,"first, while there are advantages of using naturalistic speech data (gahl et al.2012), it would be desirable to have experimental validation of the confusability measure and its relationship to speaker reduction."
2020.acl-main.180.txt,2020,5 Discussion,"gahl and strand (2016), chen and mirman (2012), dell (1986), vitevitch and luce (2016))."
2020.acl-main.180.txt,2020,5 Discussion,"gahl et al.2012, gahl and strand 2016)."
2020.acl-main.180.txt,2020,5 Discussion,"in contrast, confusability is sensitive to fine-grained perceptual structure."
2020.acl-main.180.txt,2020,5 Discussion,"kirov and wilson 2012, schertz 2013, seyfarth et al.2016, buz et al.2016), such a model would permit investigation of targeted enhancement and reduction in naturalistic data."
2020.acl-main.180.txt,2020,5 Discussion,"kirov and wilson 2012, schertz 2013, seyfarth et al.2016, buz et al.2016)."
2020.acl-main.180.txt,2020,5 Discussion,neighborhood eﬀects are type-level phenomena: a word has the same neighbors no matter what context it appears in.
2020.acl-main.180.txt,2020,5 Discussion,"second, a lower-perplexity neural language model would provide better estimates of a word’s confusability, but would first need to be validated on speech data."
2020.acl-main.180.txt,2020,5 Discussion,"second, we provide large scale, naturalistic evidence for reduction and enhancement driven by contextual confusability."
2020.acl-main.180.txt,2020,5 Discussion,"stable properties of the lexicon may determine which segment sequences undergo frequent articulatory rehearsal, and are reduced as a consequence."
2020.acl-main.180.txt,2020,5 Discussion,that work found negative or null associations between word duration and neighborhood density and related measures (e.g.
2020.acl-main.180.txt,2020,5 Discussion,"the confusability measure picks up on context-dependent variation, which rehearsal processes in the articulatory systemmay not be sensitive to."
2020.acl-main.180.txt,2020,5 Discussion,"the proposed confusability measure diﬀers from neighborhood density in three ways: it is sensitive to edit type, words greater than two edits away, and top-down eﬀects."
2020.acl-main.180.txt,2020,5 Discussion,the results complement previous work which demonstrates reduction and enhancement driven by contextual predictability (see e.g.seyfarth 2014).
2020.acl-main.180.txt,2020,5 Discussion,"the results suggest that speakers modulate their utterances in a manner that is sensitive to this eﬀect of context, increasing duration when context makes the intended utterance harder to recover."
2020.acl-main.180.txt,2020,5 Discussion,the study may help to resolve questions raised by previous work examining the eﬀects of neighborhood density.
2020.acl-main.180.txt,2020,5 Discussion,the study suggests several directions for future work.
2020.acl-main.180.txt,2020,5 Discussion,these diﬀerences may account for the discrepancy in the eﬀects of neighborhood density and confusability.
2020.acl-main.180.txt,2020,5 Discussion,they also complement work which shows confusability-driven reduction and enhancement in targeted experimental manipulations (see e.g.
2020.acl-main.180.txt,2020,5 Discussion,"third, a more sophisticated channel model would allow for insertions and deletions, and better capture transitional coarticulatory cues (wright 2004)."
2020.acl-main.180.txt,2020,5 Discussion,this spillover is potentially sensitive only to levenshtein distance.
2020.acl-main.180.txt,2020,5 Discussion,"this supports the hypothesis that variation and structure in natural languages are shaped not only by pressures for eﬃcient signals, but also pressures for eﬀective communication of the speaker’s intended message in the face of noise and uncertainty (lindblom 1990, lindblom et al.1995, hall et al.2018)."
2020.acl-main.180.txt,2020,5 Discussion,"under one hypothesis, neighborhood density eﬀects reflect spillover of activation between words with overlapping subsequences of speech sounds (e.g."
2020.acl-main.180.txt,2020,5 Discussion,we draw two main conclusions from our results.
2020.acl-main.180.txt,2020,5 Discussion,"when lexical neighbors diﬀer in perceptually distinct segments, they will typically be non-confusable."
2020.acl-main.181.txt,2020,6 Conclusion,generalizing to larger datasets of english is straightforward.
2020.acl-main.181.txt,2020,6 Conclusion,"in that case, the predictions about post-nominal order could differ substantially from the predictions about pre-nominal order."
2020.acl-main.181.txt,2020,6 Conclusion,"information locality, subjectivity, and integration cost make precisely that prediction, though none addresses mixed-type languages in which adjectives can precede or follow nouns."
2020.acl-main.181.txt,2020,6 Conclusion,it is an open question how to implement ig for these postor mixed-placement adjectives; one possibility is to measure the information gained when the set of adjectives associated to a noun an is partitioned by an adjective a.
2020.acl-main.181.txt,2020,6 Conclusion,"more excitingly, we now have the opportunity to bring new languages into the fold."
2020.acl-main.181.txt,2020,6 Conclusion,"our dependency-treebank-based methods can be applied to any other corpus of any language, provided it has enough data in the form of adjective–noun pairs to get reliable estimates of the information-theoretic predictors."
2020.acl-main.181.txt,2020,6 Conclusion,studying other typologically-distinct languages provides an opportunity to disentangle the theories that we studied here in a way that cannot be done in english.
2020.acl-main.181.txt,2020,6 Conclusion,such studies will be crucial to achieve a complete computational understanding of natural language syntax.
2020.acl-main.181.txt,2020,6 Conclusion,"the available behavioral evidence suggests that mirror-image preferences (e.g., “box blue big”) may be the norm in post-nominal adjective languages (martin, 1969; scontras et al., 2020)."
2020.acl-main.181.txt,2020,6 Conclusion,"the vast majority of research on adjective ordering, and all the corpus work to our knowledge, has been done on english, where adjectives almost always come before the noun."
2020.acl-main.181.txt,2020,6 Conclusion,"this study provides a framework for evaluating further theories of adjective order, and for evaluating the theories given here against new data from dependency treebanks."
2020.acl-main.181.txt,2020,6 Conclusion,we examined a number of theoretically-motivated predictors of adjective order in dependency tree-bank corpora of english.
2020.acl-main.181.txt,2020,6 Conclusion,"we found that the predictors have comparable accuracy, but that it is possible to identify two broad factors: a semantic factor variously captured by subjectivity scores and information gain based on word clusters, and a wordform-based factor captured by pmi."
2020.acl-main.182.txt,2020,7 Conclusions,"we have created a new nota task on the ubuntu dialog corpus, and have proposed to solve the problem by learning the response set representation with a binary classification model."
2020.acl-main.182.txt,2020,7 Conclusions,we hope the dataset we release will be used to benchmark future dialog system uncertainty research.
2020.acl-main.183.txt,2020,5 Discussion and Conclusion,"one natural extension would be to generalize these findings to other skills than the three addressed here, such as humor/wit, eloquence, image commenting, etc."
2020.acl-main.183.txt,2020,5 Discussion and Conclusion,"this paper focuses on the goal of creating an open-domain conversational agent that can display many skills, and blend them in a seamless and engaging way."
2020.acl-main.183.txt,2020,5 Discussion and Conclusion,this would in principle be straightforward to do as long as these additional skills have a corresponding “single-skill” dataset to train on and are sufficiently distinguishable from each other.
2020.acl-main.183.txt,2020,5 Discussion and Conclusion,"we compared the performance of these schemes on blendedskilltalk, a new english-language dataset blending three conversation skills in balanced proportions (demonstrating knowledge, empathy, or ability to talk about oneself)."
2020.acl-main.183.txt,2020,5 Discussion and Conclusion,"we have shown several ways to leverage previous work focusing on individual conversational skills, either by combining trained singleskill models in a two-stage way, by re-using the datasets for simultaneous multi-task training, and by fine-tuning on the overall blended task."
2020.acl-main.183.txt,2020,5 Discussion and Conclusion,"we showed that multiple multi-task approaches can be effective on this task, however careful construction of the training scheme is important to mitigate biases when blending and selecting skills, while fine-tuning on the overall blended task improves models further."
2020.acl-main.184.txt,2020,6 Conclusion and Future Work,"conceptflow models conversation structure explicitly as transitions in the latent concept space, in order to generate more informative and meaningful responses."
2020.acl-main.184.txt,2020,6 Conclusion and Future Work,"gpt-2, and how to effectively and efficiently introduce more concepts in generation models."
2020.acl-main.184.txt,2020,6 Conclusion and Future Work,"in future, we plan to explore how to combine knowledge with pre-trained language models, e.g."
2020.acl-main.184.txt,2020,6 Conclusion and Future Work,our experiments on reddit conversations illustrate the advantages of conceptflow over previous conversational systems.
2020.acl-main.184.txt,2020,6 Conclusion and Future Work,our human evaluation demonstrates that conceptflow generates more appropriate and informative responses while using much fewer parameters.
2020.acl-main.184.txt,2020,6 Conclusion and Future Work,"our studies confirm that conceptflow’s advantages come from the high coverage latent concept flow, as well as its graph attention mechanism that effectively guides the flow to highly related concepts."
2020.acl-main.185.txt,2020,6 Conclusion,"in our experiments, we apply negative training to the malicious response problem and the frequent response problem and get significant improvement for both problems."
2020.acl-main.185.txt,2020,6 Conclusion,"in this work, we propose the negative training framework to correct undesirable behaviors of a trained neural dialogue response generator."
2020.acl-main.185.txt,2020,6 Conclusion,"the algorithm involves two major steps, first input-output pairs that exhibit bad behavior are identified, and then are used for fine-tuning the model as negative training examples."
2020.acl-main.185.txt,2020,6 Conclusion,we also show that negative training can be derived from an overall objective (eq.(2)) to minimize the expected risk of undesirable behaviors.
2020.acl-main.186.txt,2020,5 Conclusion and Future Work:,"as future work, we are extending the proposed approach and test its efficacy on real human conversations."
2020.acl-main.186.txt,2020,5 Conclusion and Future Work:,"more broadly, we continue to explore strategies that combine semantic parsing and neural networks for frame generation."
2020.acl-main.186.txt,2020,5 Conclusion and Future Work:,"the proposed recursive, hierarchical frame-based representation captures complex relationships between slots labels."
2020.acl-main.186.txt,2020,5 Conclusion and Future Work:,"we also introduced global supervision by extending the tree-based loss function, and showed that it is possible to learn all this end-to-end."
2020.acl-main.186.txt,2020,5 Conclusion and Future Work:,we also proposed an approach using a template-based tree decoder to generate these hierarchical representations from users’ utterances.
2020.acl-main.186.txt,2020,5 Conclusion and Future Work:,"with this preliminary work, we showed cases where traditional flat semantic representations fail to capture slot label dependencies and we high-lighted the need for deep hierarchical semantic representations for dialog frames."
2020.acl-main.187.txt,2020,7 Conclusions and Future Work,"future work can explore improving the correction models, leveraging logs of natural language feedback to improve text-to-sql parsers, and expanding the dataset to include multiple turns of correction."
2020.acl-main.187.txt,2020,7 Conclusions and Future Work,"we compare baseline models and show that natural language feedback is effective for correcting parses, but still state-of-the-art models struggle to solve the task."
2020.acl-main.187.txt,2020,7 Conclusions and Future Work,we introduce the task of sql parse correction using natural language feedback together with a dataset of human-authored feedback paired with mispredicted and gold parses.
2020.acl-main.188.txt,2020,5 Conclusion,"lastly, we show that our calibration does not over-fit on in-domain data and is capable of generalizing the calibration to out-of-domain datasets."
2020.acl-main.188.txt,2020,5 Conclusion,our method is compatible with modern nlp architectures like bert.
2020.acl-main.188.txt,2020,5 Conclusion,our task-agnostic methods can provide calibrated model outputs of specific entities instead of the entire label sequence prediction.
2020.acl-main.188.txt,2020,5 Conclusion,we also show that our calibration method can provide improvements to the trained model’s accuracy at no additional training or data cost.
2020.acl-main.188.txt,2020,5 Conclusion,we show a new calibration and confidence based re-scoring scheme for structured output entities in nlp.
2020.acl-main.188.txt,2020,5 Conclusion,we show that our calibration methods outperform competitive baselines on several nlp tasks.
2020.acl-main.189.txt,2020,5 Discussion and Limitations,"despite these limitations, we hope that leaqi provides a useful (and relatively simple) bridge that can enable using rule-based systems, heuristics, and unsupervised models as building blocks for more complex supervised learning systems."
2020.acl-main.189.txt,2020,5 Discussion and Limitations,"in some settings, learning a difference classifier may be as hard or harder than learning the structured predictor; for instance if the task is binary sequence labeling (e.g., word segmentation), minimizing its usefulness.2."
2020.acl-main.189.txt,2020,5 Discussion and Limitations,"in this paper, we considered the problem of reducing the number of queries to an expert labeler for structured prediction problems."
2020.acl-main.189.txt,2020,5 Discussion and Limitations,the true labeling cost is likely more complicated than simply the number of individual actions queried to the expert.
2020.acl-main.189.txt,2020,5 Discussion and Limitations,"this is particularly attractive in settings where we have very strong rule-based systems, ones which often outperform the best statistical systems, like coreference resolution (lee et al., 2011), information extraction (riloff and wiebe, 2003), and morphological segmentation and analysis (smit et al., 2014)."
2020.acl-main.189.txt,2020,5 Discussion and Limitations,"to use this heuristic as a policy, we learn a difference classifier that effectively tells leaqi when it is safe to treat the heuristic’s action as if it were optimal."
2020.acl-main.189.txt,2020,5 Discussion and Limitations,"we showed empirically— across named entity recognition, keyphrase extraction and part of speech tagging tasks—that the active learning approach improves significantly on passive learning, and that leveraging a difference classifier improves on that.1."
2020.acl-main.189.txt,2020,5 Discussion and Limitations,"we took an imitation learning approach and developed an algorithm, leaqi, which leverages a source that has low-quality labels: a heuristic policy that is suboptimal but free."
2020.acl-main.19.txt,2020,5 Conclusion,"furthermore, the d-gpt model with oracle answers is able to generate conversational responses on the coqa dev set 77 % of the time showcasing the model’s scalability."
2020.acl-main.19.txt,2020,5 Conclusion,human evaluations on squad-dev-test show that our models generate significantly better conversational responses compared to the baseline coqa and quac models.
2020.acl-main.19.txt,2020,5 Conclusion,"in this work, we study the problem of generating fluent qa responses in the context of building fluent conversational agents."
2020.acl-main.19.txt,2020,5 Conclusion,"this method is used to modify the squad 2.0 dataset such that it includes conversational answers, which is used to train seq2seq based generation models."
2020.acl-main.19.txt,2020,5 Conclusion,"to this end, we propose an over-generate and rank data augmentation procedure based on syntactic transformations and a best response classifier."
2020.acl-main.190.txt,2020,6 Discussion,extending expbert to other natural language tasks where this relationship might not hold is an open problem that would entail finding different ways of interpreting an explanation with respect to the input.
2020.acl-main.190.txt,2020,6 Discussion,"first, combining our expbert approach with more complex state-of-the-art models can be conceptually straightforward (e.g., we could swap out bert-base for a larger model) but can sometimes also require overcoming technical hurdles."
2020.acl-main.190.txt,2020,6 Discussion,"for example, we do not fine-tune expbert in this paper; doing so might boost performance, but fine-tuning through all of the explanations on each example is computationally intensive."
2020.acl-main.190.txt,2020,6 Discussion,"however, more work will need to be done to make this approach more broadly applicable."
2020.acl-main.190.txt,2020,6 Discussion,"in this work, we show how using these models with natural language explanations can allow us to leverage a richer set of explanations than if we were constrained to only use explanations that can be programmatically evaluated, e.g., through ngram matching (bert+patterns) or semantic parsing (bert+semparser)."
2020.acl-main.190.txt,2020,6 Discussion,recent progress in general-purpose language representation models like bert open up new opportunities to incorporate language into learning.
2020.acl-main.190.txt,2020,6 Discussion,"second, in this paper we provided a proof-of-concept for several relation extraction tasks, relying on the fact that models trained on existing natural language inference datasets (like multinli) could be applied directly to the input sentence and explanation pair."
2020.acl-main.190.txt,2020,6 Discussion,the ability to incorporate prior knowledge of the “right” inductive biases into model representations dangles the prospect of building models that are more robust.
2020.acl-main.190.txt,2020,6 Discussion,we outline two such avenues of future work.
2020.acl-main.191.txt,2020,5 Conclusion,experiments confirm that fine-tuning such architectures with few labeled examples lead to unstable models whose performances are not acceptable.
2020.acl-main.191.txt,2020,5 Conclusion,"from a linguistic perspective, it is worth investigating what the generator encodes in the produced representations."
2020.acl-main.191.txt,2020,5 Conclusion,"in fact, the generator network is only used in training, while at inference time only the discriminator is necessary."
2020.acl-main.191.txt,2020,5 Conclusion,"in this paper, we extended the limits of transformed-based architectures (i.e., bert) in poor training conditions."
2020.acl-main.191.txt,2020,5 Conclusion,"moreover, we will investigate the potential impact of the adversarial training directly in the bert pre-training."
2020.acl-main.191.txt,2020,5 Conclusion,"the evaluations show that the proposed variant of bert, namely gan-bert, systematically improves the robustness of such architectures, while not introducing additional costs to the inference."
2020.acl-main.191.txt,2020,5 Conclusion,"this first investigation paves the way to several extensions including adopting other architectures, such as gpt-2 (radford et al., 2019) or distilbert (sanh et al., 2019) or other tasks, e.g., sequence labeling or question answering."
2020.acl-main.191.txt,2020,5 Conclusion,we suggest here to adopt adversarial training to enable semisupervised learning transformer-based architectures.
2020.acl-main.192.txt,2020,6 Conclusion,"as a result, these tasks can be solved in a single modeling framework that first extracts spans and predicts their labels, then predicts relations between spans."
2020.acl-main.192.txt,2020,6 Conclusion,future directions include (1) devising hierarchical span representations that can handle spans of different length and diverse content more effectively and efficiently; (2) robust multitask learning or meta-learning algorithms that can reconcile very different tasks.
2020.acl-main.192.txt,2020,6 Conclusion,we attempted 10 tasks with this spanrel model and show that this generic task-independent model can achieve competitive performance as state-of-the-art methods tailored for each tasks.
2020.acl-main.192.txt,2020,6 Conclusion,we merge 8 datasets into our glad benchmark for evaluating future models for natural language analysis.
2020.acl-main.192.txt,2020,6 Conclusion,we provide the simple insight that a large number of natural language analysis tasks can be represented in a single format consisting of spans and relations between spans.
2020.acl-main.193.txt,2020,6 Conclusion,experiments show that our model is superior to previous crowd-sourcing and unsupervised domain adaptation sequence labeling models.
2020.acl-main.193.txt,2020,6 Conclusion,"in contrast to prior works, connet learns fine-grained representations of each source which are further dynamically aggregated for every unseen sentence in the target data."
2020.acl-main.193.txt,2020,6 Conclusion,"in this paper, we present connet for learning a sequence tagger from multi-source supervision."
2020.acl-main.193.txt,2020,6 Conclusion,it could be applied in two practical scenarios: learning with crowd annotations and cross-domain adaptation.
2020.acl-main.193.txt,2020,6 Conclusion,the proposed learning framework also shows promising results on other nlp tasks like text classification.
2020.acl-main.194.txt,2020,6 Conclusion,"for future direction, we plan to explore the effectiveness of mixtext in other nlp tasks such as sequential labeling tasks and other real-world scenarios with limited labeled data."
2020.acl-main.194.txt,2020,6 Conclusion,"through experiments on four benchmark text classification datasets, we demonstrated the effectiveness of our proposed tmix technique and the mixup model, which have better testing accuracy and more stable loss trend, compared with current pre-training and fine-tuning models and other state-of-the-art semi-supervised learning methods."
2020.acl-main.194.txt,2020,6 Conclusion,"to alleviate the dependencies of supervised models on labeled data, this work presented a simple but effective semi-supervised learning method, mix-text, for text classification, in which we also introduced tmix, an interpolation-based augmentation and regularization technique."
2020.acl-main.195.txt,2020,5 Conclusion,empirical results on popular nlp benchmarks show that mobilebert is comparable with bertbase while being much smaller and faster.
2020.acl-main.195.txt,2020,5 Conclusion,"in this paper, we show that 1) it is crucial to keep mobilebert deep and thin, 2) bottleneck/invertedbottleneck structures enable effective layer-wise knowledge transfer, and 3) progressive knowledge transfer can efficiently train mobilebert."
2020.acl-main.195.txt,2020,5 Conclusion,mobilebert can enable various nlp applications7 to be easily deployed on mobile devices.
2020.acl-main.195.txt,2020,5 Conclusion,we believe our findings are generic and can be applied to other model compression problems.
2020.acl-main.195.txt,2020,5 Conclusion,we have presented mobilebert which is a taskagnostic compact variant of bert.
2020.acl-main.196.txt,2020,5 Conclusion,"our contributions include: (1) showing that importance sampling produces stochastic upper bounds of perplexity, thereby justifying the use of such estimates for comparing language model performance, (2) a concise description of (sometimes unstated) common practices used in applying this technique, (3) a simple direct marginalization-based alternative to importance sampling, and (4) experimental results demonstrating the effect of sample size, sampling distribution, and granularity on estimates."
2020.acl-main.196.txt,2020,5 Conclusion,"thus, we encourage future research into obtaining tighter bounds on latent lm perplexity, possibly by using more powerful proposal distributions that consider entire documents as context, or by considering methods such as annealed importance sampling."
2020.acl-main.196.txt,2020,5 Conclusion,we investigate the application of importance sampling to evaluating latent language models.
2020.acl-main.196.txt,2020,5 Conclusion,"while this work helps clarify and validate existing results, we also observe that none of the estimates appear to converge even after drawing large numbers of samples."
2020.acl-main.197.txt,2020,6 Conclusion,"our empirical results suggest that smart improves the performance on many nlp benchmarks (e.g., glue, snli, scitail, anli) with the state-of-the-art pre-trained models (e.g., bert, mt-dnn, roberta)."
2020.acl-main.197.txt,2020,6 Conclusion,our proposed fine-tuning framework can be generalized to solve other transfer learning problems.
2020.acl-main.197.txt,2020,6 Conclusion,smart includes two important ingredients: 1) smooth-inducing adversarial regularization; 2) bregman proximal point optimization.
2020.acl-main.197.txt,2020,6 Conclusion,the framework effectively alleviates the overfitting and aggressive updating issues in the fine-tuning stage.
2020.acl-main.197.txt,2020,6 Conclusion,we also demonstrate that the proposed framework is applicable to domain adaptation and results in a significant performance improvement.
2020.acl-main.197.txt,2020,6 Conclusion,"we propose a robust and efficient computation framework, smart, for fine-tuning large scale pre-trained natural language models in a principled manner."
2020.acl-main.197.txt,2020,6 Conclusion,we will explore this direction as future work.
2020.acl-main.198.txt,2020,6 Conclusion,alternative architectures that can overcome the stolen probability effect are an item of future work.
2020.acl-main.198.txt,2020,6 Conclusion,our experiments show that the effect is relatively common in smaller neural language models.
2020.acl-main.198.txt,2020,6 Conclusion,"this is structural weakness of nnlms with dot-product softmax output layers, which we call the stolen probability effect."
2020.acl-main.198.txt,2020,6 Conclusion,"we present numerical, theoretical and empirical analyses showing that the dot-product softmax limits a nnlm’s expressiveness for words on the interior of a convex hull of the embedding space."
2020.acl-main.199.txt,2020,5 Conclusion,"furthermore, our proposed model encodes acyclicity as a soft constraint and shows that the overall model outperforms state of the art."
2020.acl-main.199.txt,2020,5 Conclusion,"in the future, we would like to figure out different strategies to merge individual gains, obtained by separate application of the dag constraint, into a setup that can take the best of both precision and recall improvements, and put forth a better performing system."
2020.acl-main.199.txt,2020,5 Conclusion,we also plan on looking into strategies to improve recall of the constructed taxonomy.
2020.acl-main.199.txt,2020,5 Conclusion,"we have introduced a gnn-based cross-domain knowledge transfer framework graph2taxo, which makes use of a cross-domain graph structure, in conjunction with an acyclicity constraint-based dag learning for taxonomy construction."
2020.acl-main.2.txt,2020,6 Conclusion,"depression prediction is a difficult task which requires especially trained experts to conduct interviews and do their detailed analysis (lakhan et al., 2010)."
2020.acl-main.2.txt,2020,6 Conclusion,"for example, our findings show how language of depressed individuals changes when interviewers use backchannels to encourage continued speech."
2020.acl-main.2.txt,2020,6 Conclusion,future work can further investigate temporal patterns in how language used by depressed people evolves over the course of an interaction.
2020.acl-main.2.txt,2020,6 Conclusion,the model jointly learns these prompt categories while identifying depression.
2020.acl-main.2.txt,2020,6 Conclusion,the proposed model analyzes the participant’s responses in light of various categories of prompts provided by the interviewer.
2020.acl-main.2.txt,2020,6 Conclusion,this paper addressed the problem of identifying depression from interview transcripts.
2020.acl-main.2.txt,2020,6 Conclusion,we hope that this combination will encourage the research community to make more progress in this direction.
2020.acl-main.2.txt,2020,6 Conclusion,we show that the model outperforms competitive baselines and we use the prompt categorization to investigate various psycholinguistic hypotheses.
2020.acl-main.2.txt,2020,6 Conclusion,"while the absolute performance of our model is low for immediate practical deployment, it improves upon existing methods and at the same time, unlike modern methods, provides insight about the model’s workflow."
2020.acl-main.20.txt,2020,5 Conclusion,"as future work, we plan to extend our qag model to a meta-learning framework, for generalization over diverse datasets."
2020.acl-main.20.txt,2020,5 Conclusion,"specifically, our model learns the joint distribution of question and answer given context with a hierarchically conditional variational autoencoder, while enforcing consistency between generated qa pairs by maximizing their mutual information with a novel infomax regularizer."
2020.acl-main.20.txt,2020,5 Conclusion,"to our knowledge, ours is the first successful probabilistic qag model."
2020.acl-main.20.txt,2020,5 Conclusion,"we evaluated the qag performance of our model by the accuracy of the bert-base qa model trained using the generated questions on multiple datasets, on which it largely outperformed the state-of-the-art qag baseline (+6.59-10.69 in em), even with a smaller number of qa pairs."
2020.acl-main.20.txt,2020,5 Conclusion,"we further validated our model for semi-supervised qa, where it improved the performance of the bert-base qa model on the squad by 2.12 in em, significantly outperforming the state-of-the-art model."
2020.acl-main.20.txt,2020,5 Conclusion,we proposed a novel probabilistic generative framework for generating diverse and consistent questionanswer (qa) pairs from given texts.
2020.acl-main.200.txt,2020,6 Conclusion,finetuning bert-style models on resource-rich downstream tasks is not well studied.
2020.acl-main.200.txt,2020,6 Conclusion,"for industrial applications where there is a trade-off typically between accuracy and latency, our findings suggest it might be feasible to gain accuracy for faster models by collecting more training examples."
2020.acl-main.200.txt,2020,6 Conclusion,"in this paper, we reported that, when the downstream task has sufficiently large amount of training exampes, i.e., millions, competitive accuracy results can be achieved by training a simple lstm, at least for text classification tasks."
2020.acl-main.200.txt,2020,6 Conclusion,the findings of this work have significant implications on both the practical aspect as well as the research on pretraining.
2020.acl-main.200.txt,2020,6 Conclusion,we further discover that reusing the token embeddings learned during bert pretraining in an lstm model leads to significant improvements.
2020.acl-main.201.txt,2020,6 Conclusion and Discussion,an obvious fix is adding training words to the bli test set.
2020.acl-main.201.txt,2020,6 Conclusion and Discussion,"bli accuracy assumes that all test words are equally important, but the importance of a word depends on the downstream task; e.g., “the” is irrelevant in document classification but important in dependency parsing."
2020.acl-main.201.txt,2020,6 Conclusion and Discussion,bli test accuracy does not always correlate with downstream task accuracy because words from the training dictionary are ignored.
2020.acl-main.201.txt,2020,6 Conclusion and Discussion,"however, it is unclear how to balance between training and test words."
2020.acl-main.201.txt,2020,6 Conclusion and Discussion,popular clwe methods are optimized for bli test accuracy.
2020.acl-main.201.txt,2020,6 Conclusion and Discussion,"there are other ways to fit the dictionary better; e.g., using a non-linear projection such as a neural network."
2020.acl-main.201.txt,2020,6 Conclusion and Discussion,"therefore, future work should focus on downstream tasks instead of bli."
2020.acl-main.201.txt,2020,6 Conclusion and Discussion,"they underfit the training dictionary, which hurts downstream models."
2020.acl-main.201.txt,2020,6 Conclusion and Discussion,this post-processing step improves downstream task accuracy despite lowering bli test accuracy.
2020.acl-main.201.txt,2020,6 Conclusion and Discussion,we focus on retrofitting due to its simplicity.
2020.acl-main.201.txt,2020,6 Conclusion and Discussion,we leave the exploration of non-linear projections to future work.
2020.acl-main.201.txt,2020,6 Conclusion and Discussion,"we then add a synthetic dictionary to balance bli test and training accuracy, which further helps downstream models on average."
2020.acl-main.201.txt,2020,6 Conclusion and Discussion,we use retrofitting to fully exploit the training dictionary.
2020.acl-main.202.txt,2020,8 Conclusions,our distillation strategy leveraging teacher representations agnostic of its architecture and stage-wise optimization schedule outperforms existing ones.
2020.acl-main.202.txt,2020,8 Conclusions,we develop xtremedistil for massive multi-lingual ner and classification that performs close to huge pre-trained models like mbert but with massive compression and inference speedup.
2020.acl-main.202.txt,2020,8 Conclusions,"we perform extensive study of several distillation dimensions like the impact of unlabeled transfer set, embeddings and student architectures, and make interesting observations outlined in summary."
2020.acl-main.203.txt,2020,6 Conclusion,"in this paper, we showed that the state-of-the-art authorship obfuscation methods are not stealthy."
2020.acl-main.203.txt,2020,6 Conclusion,our findings point to future research opportunities to build stealthy authorship obfuscation methods.
2020.acl-main.203.txt,2020,6 Conclusion,"our proposed obfuscation detectors were effective at classifying obfuscated and evaded documents (f1 score as high as 0.92 and 0.95, respectively)."
2020.acl-main.203.txt,2020,6 Conclusion,we showed that the degradation in text smoothness caused by authorship obfuscators allow a detector to distinguish between obfuscated documents and original documents.
2020.acl-main.203.txt,2020,6 Conclusion,we suggest that obfuscation methods should strive to preserve text smoothness in addition to semantics.
2020.acl-main.204.txt,2020,5 Conclusions and Future Work,"experiments demonstrate its ability to accelerate bert’s and roberta’s inference by up to ∼40%, and also reveal interesting patterns of different transformer layers in bert models."
2020.acl-main.204.txt,2020,5 Conclusions and Future Work,"there are a few interesting questions left unanswered in this paper, which would provide interesting future research directions: (1) deebert’s training method, while maintaining good quality in the last off-ramp, reduces model capacity available for intermediate off-ramps; it would be important to look for a method that achieves a better balance between all off-ramps.(2) the reasons why some transformer layers appear redundant2 and why dee-bert considers some samples easier than others remain unknown; it would be interesting to further explore relationships between pre-training and layer redundancy, sample complexity and exit layer, and related characteristics."
2020.acl-main.204.txt,2020,5 Conclusions and Future Work,"we propose deebert, an effective method that exploits redundancy in bert models to achieve better quality–efficiency trade-offs."
2020.acl-main.205.txt,2020,6 Conclusion,"finally, we also open a path to study integration of knowledge into the decoding phase, which can benefit other tasks such as neural machine translation."
2020.acl-main.205.txt,2020,6 Conclusion,our proposed methods established new state-of-the-art results with class hierarchies on the wos and dbpedia datasets in english.
2020.acl-main.205.txt,2020,6 Conclusion,we presented a bag of tricks to efficiently improve hierarchical text classification by adding an auxiliary task of reverse hierarchy prediction and integrating external knowledge (vectorized textual definitions of classes in a parent node conditioning scheme and in the beam search).
2020.acl-main.206.txt,2020,6 Conclusion,"additional experiments indicated that the removal of filled pauses from the transcriptions was beneficial to the scoring models, and that scoring performance is best for the middle grades of the scoring range."
2020.acl-main.206.txt,2020,6 Conclusion,"finally, we acknowledge that speaking proficiency in a second language is a multi-faceted construct made up of more than the features which can be drawn from transcriptions (galaczi et al., 2011; lim, 2018)."
2020.acl-main.206.txt,2020,6 Conclusion,"for instance, the speaker’s prosody, pronunciations and disfluencies are also contributing factors."
2020.acl-main.206.txt,2020,6 Conclusion,"further research is needed to improve machine assessment at the upper and lower ends of the scoring scale, although these are the scores for which the least training data exists."
2020.acl-main.206.txt,2020,6 Conclusion,"further work should be undertaken in terms of scoring validity and the robustness of such an approach, before such models are applied to any ‘high stakes’ (i.e.exam) scenario, as opposed to the kind of at-home practice apps we have discussed in this paper."
2020.acl-main.206.txt,2020,6 Conclusion,furthermore such applications may become increasingly relevant as organisations reduce the types of data they collect from the end user due to privacy concerns.
2020.acl-main.206.txt,2020,6 Conclusion,"however, given the text-only constraints faced by third-party application developers for home assistants, the proficiency assessment models we present in this work allow for progress in providing low-stakes assessment and continuous practice for language learners, with the caveat that fuller speaking skills should be taught and assessed with the complete construct in mind."
2020.acl-main.206.txt,2020,6 Conclusion,"its error is on average less than 1, and 76% of its predictions are within 1 grade of the examiners’ gold scores."
2020.acl-main.206.txt,2020,6 Conclusion,our best performing model involves a bert encoder with first language prediction as an auxiliary task.
2020.acl-main.206.txt,2020,6 Conclusion,"therefore future work could include different sampling methods, generation of synthetic data, or training objectives which reward models which are less conservatively drawn to the middle of the scoring scale."
2020.acl-main.206.txt,2020,6 Conclusion,"we also showed that the models improve as they are trained on increasingly accurate asr transcriptions, though performance deteriorates when they are evaluated on manual transcriptions."
2020.acl-main.206.txt,2020,6 Conclusion,"we presented an effective approach to grading spontaneous speech based on asr transcriptions only, without direct access to the audio recording or features derived from it."
2020.acl-main.206.txt,2020,6 Conclusion,"we recognise that without the audio signal, some information is lost that would be useful for speech assessment – namely prosodic and phonemic features – but that assessment on transcriptions alone has a use case in educational technology for home assistants."
2020.acl-main.206.txt,2020,6 Conclusion,"we showed that this model improves on alternative lstm-based models, and over a feature-rich baseline, by better predicting scores at the edges of the proficiency scale, while also offering (smaller) gains at the central points on the scale."
2020.acl-main.206.txt,2020,6 Conclusion,"we surmise that this is because of stylistic differences in the machine and human transcriptions, and that adaptation of the models to manual transcriptions will help mitigate the drop in performance."
2020.acl-main.207.txt,2020,8 Conclusions and Future Work,another item of future work is to develop better multitask approaches to leverage multiple signals of relatedness information during training.
2020.acl-main.207.txt,2020,8 Conclusions and Future Work,including other information such as outgoing citations as additional input to the model would be yet another area to explore in future.
2020.acl-main.207.txt,2020,8 Conclusions and Future Work,it would be interesting to initialize our model weights from more recent transformer models to investigate if additional gains are possible.
2020.acl-main.207.txt,2020,8 Conclusions and Future Work,the landscape of transformer language models is rapidly changing and newer and larger models are frequently introduced.
2020.acl-main.207.txt,2020,8 Conclusions and Future Work,"we achieve substantial improvements over the strongest of a wide variety of baselines, demonstrating the effectiveness of our model."
2020.acl-main.207.txt,2020,8 Conclusions and Future Work,"we additionally introduce scidocs, a new evaluation suite consisting of seven document-level tasks and release the corresponding datasets to foster further research in this area."
2020.acl-main.207.txt,2020,8 Conclusions and Future Work,"we present specter, a model for learning representations of scientific papers, based on a transformer language model that is pretrained on citations."
2020.acl-main.207.txt,2020,8 Conclusions and Future Work,"we used citations to build triplets for our loss function, however there are other metrics that have good support from the bibliometrics literature (klavans and boyack, 2006) that warrant exploring as a way to create relatedness graphs."
2020.acl-main.208.txt,2020,7.3 Code Piece Error Analysis,"however, in 32% of the programs at least one “hard” line has no generated code piece that is functionally equivalent to the solution, thus indicating plenty of room for improvement."
2020.acl-main.208.txt,2020,7.3 Code Piece Error Analysis,so far we have focused on combining independent candidates from each line together to search for the target program.
2020.acl-main.208.txt,2020,7.3 Code Piece Error Analysis,the error analysis is available on our github.
2020.acl-main.208.txt,2020,7.3 Code Piece Error Analysis,this heavily depends on the underlying model to generate potentially correct code pieces.
2020.acl-main.208.txt,2020,7.3 Code Piece Error Analysis,"this requires incorporating contextual information of the program into the code piece generation process.(d, e) the pseudocode either (d) consists of variable name typos or (e) is completely wrong."
2020.acl-main.208.txt,2020,7.3 Code Piece Error Analysis,"to help the readers understand the bottleneck for code piece generation and point out important future directions, we randomly sampled 200 “hard” lines and manually analyzed why the generation fails by looking at the top 1 candidate of the model."
2020.acl-main.208.txt,2020,7.3 Code Piece Error Analysis,"we group the failures into the following categories, giving a detailed breakdown and examples in figure 7.(a) the model generation is wrong despite clear pseudocode; this typically happens when the gold code piece is long or highly compositional.(b, c) the pseudocode contains ambiguity; the model generation is reasonable but either needs (b) variable type clarification or (c) syntactic context."
2020.acl-main.209.txt,2020,7 Conclusion,our results indicate that most predicted true facts are genuinely new.
2020.acl-main.209.txt,2020,7 Conclusion,"we created the large olp benchmark olpbench, which will be made publicly available4."
2020.acl-main.209.txt,2020,7 Conclusion,"we investigated the effect of leakage of evaluation facts, non-relational information, and entity-knowledge during model selection using a prototypical open link prediction model."
2020.acl-main.209.txt,2020,7 Conclusion,we proposed the olp task and a method to create an olp benchmark.
2020.acl-main.21.txt,2020,7 Conclusion,"besides, more powerful question clustering and coarse-to-fine generation scenarios are also worth exploration."
2020.acl-main.21.txt,2020,7 Conclusion,"different from prior works regarding sqg as a dialog generation task, we propose the first semi-autoregressive sqg model, which divides questions into different groups and further generates each group of closely-related questions in parallel."
2020.acl-main.21.txt,2020,7 Conclusion,"during this process, we first build a passage-info graph, an answer-info graph, and then perform dual-graph interaction to get representations capturing the context dependencies between passages and questions."
2020.acl-main.21.txt,2020,7 Conclusion,experimental results show that our model outperforms previous works by a substantial margin.
2020.acl-main.21.txt,2020,7 Conclusion,"finally, performing sqg on other types of inputs, e.g., images and knowledge graphs, is an interesting topic."
2020.acl-main.21.txt,2020,7 Conclusion,"for future works, the major challenge is generating more meaningful, informative but concise questions."
2020.acl-main.21.txt,2020,7 Conclusion,"in this paper, we focus on sqg which is an important yet challenging task."
2020.acl-main.21.txt,2020,7 Conclusion,these representations are further used during our coarse-to-fine generation process.
2020.acl-main.21.txt,2020,7 Conclusion,"to perform experiments, we analyze the limitation of existing datasets and create the first dataset specially used for sqg containing 81.9k questions."
2020.acl-main.210.txt,2020,8 Conclusion,"infotabs has multiple test sets that are designed to pose difficulties to models that only learn superficial correlations between inputs and the labels, rather than reasoning about the information."
2020.acl-main.210.txt,2020,8 Conclusion,our analysis showed that our data encompasses several different kinds of inferences.
2020.acl-main.210.txt,2020,8 Conclusion,"via extensive experiments, we showed that derivatives of several popular classes of models find this new inference task challenging."
2020.acl-main.210.txt,2020,8 Conclusion,we expect that the dataset can serve as a testbed for developing new kinds of models and representations that can handle semistructured information as first class citizens.
2020.acl-main.210.txt,2020,8 Conclusion,"we presented a new high quality natural language inference dataset, infotabs, with heterogeneous semi-structured premises and natural language hypotheses."
2020.acl-main.211.txt,2020,6 Discussion and Future Work,"as a concrete example, in real world environments such as the internet, different pieces of knowledge are interconnected by hyperlinks."
2020.acl-main.211.txt,2020,6 Discussion and Future Work,"despite being necessary, our preliminary experiments do not seem sufficient."
2020.acl-main.211.txt,2020,6 Discussion and Future Work,"for example, a multi-word query with fuzzy matching is more realistic."
2020.acl-main.211.txt,2020,6 Discussion and Future Work,"for our baseline, we adopted off-the-shelf, top-performing mrc and rl methods, and applied a memory mechanism which serves as an inductive bias."
2020.acl-main.211.txt,2020,6 Discussion and Future Work,"however, a host of other options could be considered in future work."
2020.acl-main.211.txt,2020,6 Discussion and Future Work,"in this work, we propose and explore the direction of converting mrc datasets into interactive, partially observable environments."
2020.acl-main.211.txt,2020,6 Discussion and Future Work,"it is complementary to existing mrc dataset and models, meaning almost all mrc datasets can be used to study interactive, information-seeking behavior through similar modifications."
2020.acl-main.211.txt,2020,6 Discussion and Future Work,"it lies at the intersection of nlp and rl, which is arguably less studied in existing literature."
2020.acl-main.211.txt,2020,6 Discussion and Future Work,it would also be interesting for an agent to generate a vector representation of the query in some latent space and modify it during the dynamic reasoning process.
2020.acl-main.211.txt,2020,6 Discussion and Future Work,our idea for reformulating existing mrc datasets as partially observable and interactive environments is straightforward and general.
2020.acl-main.211.txt,2020,6 Discussion and Future Work,our proposed setup presently uses only a single word as query in the ctrl+f command in an abstractive manner.
2020.acl-main.211.txt,2020,6 Discussion and Future Work,"this could further be used to retrieve different contents by comparing with pre-computed document representations (e.g., in an open-domain qa dataset), with such behavior tantamount to learning to do ir."
2020.acl-main.211.txt,2020,6 Discussion and Future Work,this extends traditional query reformulation for open-domain qa by allowing to drastically change the queries without strictly keeping the semantic meaning of the original queries.
2020.acl-main.211.txt,2020,6 Discussion and Future Work,"we believe information-seeking behavior is desirable for neural mrc systems when knowledge sources are partially observable and/or too large to encode in their entirety, where knowledge is by design easily accessible to humans through interaction."
2020.acl-main.211.txt,2020,6 Discussion and Future Work,"we could equip the agent with an action to “click” a hyperlink, which returns another webpage as new observations, thus allowing it to navigate through a large number of web information to answer difficult questions.imrc is difficult and cannot yet be solved, however it clearly matches a human’s informationseeking behavior compared to most static and fullyobservable laboratory mrc benchmarks."
2020.acl-main.211.txt,2020,6 Discussion and Future Work,"we encourage work on this task to determine what inductive biases, architectural components, or pretraining recipes are necessary or sufficient for mrc based on information-seeking."
2020.acl-main.211.txt,2020,6 Discussion and Future Work,"we hypothesize that such behavior can, in turn, help in solving real-world mrc problems involving search."
2020.acl-main.212.txt,2020,6 Discussion,"an alternative would be to create training sets that adequately represent a diverse range of linguistic phenomena; crowdworkers’ (rational) preferences for using the simplest generation strategies possible could be counteracted by approaches such as adversarial filtering (nie et al., 2019)."
2020.acl-main.212.txt,2020,6 Discussion,"at the same time, the inversion transformation did not completely counteract the heuristic; in particular, the models showed poor performance on passive sentences."
2020.acl-main.212.txt,2020,6 Discussion,"for these constructions, then, bert’s pretraining may not yield strong syntactic representations that can be tapped into with a small nudge from augmentation; in other words, this may be a case where our representational inadequacy hypothesis holds."
2020.acl-main.212.txt,2020,6 Discussion,"in the interim, however, we conclude that data augmentation is a simple and effective strategy to mitigate known inference heuristics in models such as bert."
2020.acl-main.212.txt,2020,6 Discussion,our best-performing strategy involved augmenting the mnli training set with a small number of instances generated by applying the subject/object inversion transformation to mnli examples.
2020.acl-main.212.txt,2020,6 Discussion,the best-performing augmentation strategy involved generating premise/hypothesis pairs from a single source sentence—meaning that this strategy does not rely on an nli corpus.
2020.acl-main.212.txt,2020,6 Discussion,"the fact that we can generate augmentation examples from any corpus makes it possible to test if very large augmentation sets are effective (with the caveat, of course, that augmentation sentences from a different domain may hurt performance on mnli itself)."
2020.acl-main.212.txt,2020,6 Discussion,"this hypothesis predicts that pretrained bert, as a word prediction model, struggles with passives, and may need to learn the properties of this construction specifically for the nli task; this would likely require a much larger number of augmentation examples."
2020.acl-main.212.txt,2020,6 Discussion,"this supports the missed connection hypothesis: a small amount of augmentation with one construction induced abstract syntactic sensitivity, instead of just “inoculating” the model against failing on the challenge set by providing it with a sample of cases from the same distribution (liu et al., 2019)."
2020.acl-main.212.txt,2020,6 Discussion,"this yielded considerable generalization: both to another domain (the hans challenge set), and, more importantly, to additional constructions, such as relative clauses and prepositional phrases."
2020.acl-main.212.txt,2020,6 Discussion,"ultimately, it would be desirable to have a model with a strong inductive bias for using syntax across language understanding tasks, even when overlap heuristics lead to high accuracy on the training set; indeed, it is hard to imagine that a human would ignore syntax entirely when understanding a sentence."
2020.acl-main.213.txt,2020,5 Conclusions,"through phonetic classification, we find the representations learned with our approach contain richer phonetic content than the original representations, and achieve better performance on speech recognition and speech translation."
2020.acl-main.213.txt,2020,5 Conclusions,"we improve the generalization of autoregressive predictive coding by multi-target training of future prediction lf and past memory reconstruction lr, where the latter serves as a regularization."
2020.acl-main.214.txt,2020,7 Conclusion,"a unique characteristic of mag is that it makes no change to the original structure of bert or xlnet, but rather comes as an attachment to both models."
2020.acl-main.214.txt,2020,7 Conclusion,"in this paper, we introduced a method for efficiently finetuning large pre-trained transformer models for multimodal language."
2020.acl-main.214.txt,2020,7 Conclusion,"mag essentially poses the nonverbal behavior as a vector with a trajectory and magnitude, which is subsequently used to shift lexical representations within the pre-trained transformer model."
2020.acl-main.214.txt,2020,7 Conclusion,our experiments demonstrated the superior performance of mag-bert and mag-xlnet.
2020.acl-main.214.txt,2020,7 Conclusion,the code for both mag-bert and mag-xlnet are publicly available here 2
2020.acl-main.214.txt,2020,7 Conclusion,"using a proposed multimodal adaptation gate (mag), bert and xlnet were successfully fine-tuned in presence of vision and acoustic modalities."
2020.acl-main.215.txt,2020,7 Conclusions,"our findings generalize to a medical symptoms labeling task, suggesting that our model is applicable as a general-purpose speech tagger wherever the speech modality is coupled in real time to asr output."
2020.acl-main.215.txt,2020,7 Conclusions,this way we learn a model to identify questions in real time with a high accuracy while trained on a small annotated dataset.
2020.acl-main.215.txt,2020,7 Conclusions,we proposed a novel approach to speech sequence labeling by learning a multimodal representation from the temporal binding of the audio signal and its automatic transcription.
2020.acl-main.215.txt,2020,7 Conclusions,we show the multimodal representation to be more accurate and more robust to noise than the unimodal approaches.
2020.acl-main.216.txt,2020,5 Conclusions,audio and visual features are then combined with a scalar additive fusion and used to predict character as well as subword transcriptions.
2020.acl-main.216.txt,2020,5 Conclusions,"due to large memory requirements of the attention mechanism, we apply aggressive preprocessing to shorten the input sequences, which may hurt model performance."
2020.acl-main.216.txt,2020,5 Conclusions,"finally, we will evaluate the multiresolution loss on larger datasets to analyze it’s regularizing effects."
2020.acl-main.216.txt,2020,5 Conclusions,"furthermore, we will experiment with more ellaborate, attention-based fusion mechanisms."
2020.acl-main.216.txt,2020,5 Conclusions,"in the future, we plan to alleviate this by incorporating ideas from sparse transformer variants (kitaev et al., 2020; child et al., 2019)."
2020.acl-main.216.txt,2020,5 Conclusions,our proposed framework uses a crossmodal dot-product attention to map visual features to audio feature space.
2020.acl-main.216.txt,2020,5 Conclusions,results on the how2 database show that a) multiresolution losses regularizes our model producing significant gains in wer over character level and subword level losses individually b) adding visual information results in relative gains of 3.76% over audio model’s results validating our model.
2020.acl-main.216.txt,2020,5 Conclusions,this paper explores the applicability of the transformer architecture for multimodal grounding in asr.
2020.acl-main.216.txt,2020,5 Conclusions,we employ a novel multitask loss that combines the subword level and character losses.
2020.acl-main.217.txt,2020,10 Conclusion,"generating phone features uses the same data as auxiliary speech recognition tasks from prior work; our experiments suggest these features are a more effective use of this data, with our models matching the performance from previous works’ performance without additional training data."
2020.acl-main.217.txt,2020,10 Conclusion,"our greatest improvements are seen in our lowest-resource settings (20 hours), where our end-to-end model outperforms a strong baseline cascade by ≈5 bleu, and our cascade outperforms prior work by ≈9 bleu."
2020.acl-main.217.txt,2020,10 Conclusion,"our improvements hold across high, medium, and low-resource conditions."
2020.acl-main.217.txt,2020,10 Conclusion,"we hope that these model comparisons and results inform development of more robust end-to-end models, and provide a stronger benchmark for performance on low-resource settings."
2020.acl-main.217.txt,2020,10 Conclusion,we show that phone features significantly improve the performance and data efficiency of neural speech translation models.
2020.acl-main.217.txt,2020,10 Conclusion,"we study the existing performance gap between cascaded and end-to-end models, and introduce two methods to use phoneme-level features in both architectures."
2020.acl-main.218.txt,2020,7 Conclusion,from human evaluations of dialogue models trained with various data configurations we find that spolin is useful—when including it we are able to build models that can generate yes-ands more consistently than when we leave it out.
2020.acl-main.218.txt,2020,7 Conclusion,"inspired by yes-ands in improv, we carefully construct spolin, a collection of dialogue pairs with responses that are not only coherent with dialogue context but also initiate the next relevant contribution."
2020.acl-main.218.txt,2020,7 Conclusion,"nevertheless, our models are still inferior at producing good yes-ands when compared to professional improvisers."
2020.acl-main.218.txt,2020,7 Conclusion,"we extract high-quality yes-ands from spontaneanation and build a classifier with them, which is then used to mine additional yes-ands from the cornell movie-dialogs corpus."
2020.acl-main.218.txt,2020,7 Conclusion,"we further use our mining technique to elicit a corpus of more than 68,000 yes-and turn pairs, easily the largest collection of this dialogue act known to exist."
2020.acl-main.218.txt,2020,7 Conclusion,we plan to continue our data-driven approach for grounded conversations by expanding our dataset through our iterative data collection process with other larger text-based open-domain dialogue corpora and extend our work to model and collect longer conversations exhibiting more complex improv-backed turns.
2020.acl-main.219.txt,2020,6 Conclusion,"focusing on the case of chit-chatting about a given image, a naturally useful application for end-users of social dialogue agents, this work shows that our best proposed model can generate grounded dialogues that humans prefer over dialogues with other fellow humans almost half of the time (47.7%)."
2020.acl-main.219.txt,2020,6 Conclusion,"however, our retrieval models outperformed their generative versions; closing that gap is an important challenge for the community."
2020.acl-main.219.txt,2020,6 Conclusion,"our work shows that we are close to having models that humans can relate to in chit-chat conversations, which could set new ground for social dialogue agents."
2020.acl-main.219.txt,2020,6 Conclusion,"the next challenge will also be to combine this engagingness with other skills, such as world knowledge (antol et al., 2015) relation to personal interests (zhang et al., 2018), and task proficiency."
2020.acl-main.219.txt,2020,6 Conclusion,this paper presents an approach for improving the way machines can generate grounded conversations that humans find engaging.
2020.acl-main.219.txt,2020,6 Conclusion,this result is made possible by the creation of a new dataset image-chat3.
2020.acl-main.219.txt,2020,6 Conclusion,"while our human evaluations were on short conversations, initial investigations indicate the model as is can extend to longer chats, see appendix g, which should be studied in future work."
2020.acl-main.22.txt,2020,6 Conclusion,"in this work, we propose a two-step framework for paraphrase generation: construction of diverse syntactic guides in the form of target reorderings followed by actual paraphrase generation that respects these reorderings."
2020.acl-main.22.txt,2020,6 Conclusion,our experiments show that this approach can be used to produce paraphrases that achieve a better quality-diversity trade-off compared to previous methods and strong baselines.
2020.acl-main.220.txt,2020,5 Conclusion,"in this work, we explore the feasibility of learning an automatic dialogue evaluation metric by leveraging pre-trained language models and the temporal structure of dialogue."
2020.acl-main.220.txt,2020,5 Conclusion,"maude also learns a recurrent neural network to model the transition between the utterances in a dialogue, allowing it to correlate better with human annotations."
2020.acl-main.220.txt,2020,5 Conclusion,"since it provides immediate continuous rewards and at the singlestep level, maude can be also be used to optimize and train better dialogue generation models, which we want to pursue as future work."
2020.acl-main.220.txt,2020,5 Conclusion,this is a good indication that maude can be used to evaluate online dialogue conversations.
2020.acl-main.220.txt,2020,5 Conclusion,"we propose maude, which is an unreferenced dialogue evaluation metric that leverages sentence representations from large pretrained language models, and is trained via noise contrastive estimation."
2020.acl-main.221.txt,2020,5 Conclusion,"in this paper, we have presented models that can be used to generate the turn switch offset distributions of sds system responses."
2020.acl-main.221.txt,2020,5 Conclusion,"it has been shown in prior studies (e.g.(bo¨gels et al., 2019)) that humans are sensitive to these timings and that they can impact how responses are perceived by a listener."
2020.acl-main.221.txt,2020,5 Conclusion,we would argue that they are an important element of producing naturalistic interactions that is often overlooked.
2020.acl-main.221.txt,2020,5 Conclusion,"with the advent of commercial sds systems that attempt to engage users over extended multi-turn interactions (e.g.(zhou et al., 2018)) generating realistic response behaviors is a potentially desirable addition to the overall experience."
2020.acl-main.222.txt,2020,6 Discussion,"for example, longer conversations involving memory (moon et al., 2019), or mixing open-domain conversation with task oriented goals."
2020.acl-main.222.txt,2020,6 Discussion,"future work should consider adding these tasks to the ones used here, while continuing the quest for improved models."
2020.acl-main.222.txt,2020,6 Discussion,"recently reported results show systems can be reasonably competitive compared to humans in particular domains for short conversations (li et al., 2019b; shuster et al., 2018)."
2020.acl-main.222.txt,2020,6 Discussion,"still, despite leveraging 12 tasks, there are many skills not included in our set."
2020.acl-main.222.txt,2020,6 Discussion,"the goal of introducing this task is not just as another challenge dataset, but to further motivate building and evaluating conversational agents capable of multiple skills – one of the core goals of ai."
2020.acl-main.222.txt,2020,6 Discussion,"this work tries to bridge the gap to avoid agents with niche skills, to move towards evaluating an open-domain set of skills."
2020.acl-main.222.txt,2020,6 Discussion,we believe current systems are closer to that goal than ever before – but we also still have a long way to go.
2020.acl-main.222.txt,2020,6 Discussion,"we have introduced the dodecadialogue task, and provide strong baseline results leveraging multi-modal image+seq2seq transformers trained across all tasks."
2020.acl-main.223.txt,2020,5 Conclusion,a successful incorporation of such devices would mean a significant step towards truly inspired poetry generation.
2020.acl-main.223.txt,2020,5 Conclusion,"and finally, we would like to adapt the model for automatic poetry translation—as we feel that the constraint-based approach lends itself perfectly to a poetry translation model that is able to adhere to an original poem in both form and meaning."
2020.acl-main.223.txt,2020,5 Conclusion,"compared to previous systems, our model achieves state of the art performance, even though it is trained on standard, non-poetic text."
2020.acl-main.223.txt,2020,5 Conclusion,"first of all, we would like to experiment with different neural network architectures."
2020.acl-main.223.txt,2020,5 Conclusion,"gripping poetry often relies on figurative language use, such as symbolism and metaphor."
2020.acl-main.223.txt,2020,5 Conclusion,"in order to facilitate reproduction of the results and encourage further research, the poetry generation system is made available as open source software."
2020.acl-main.223.txt,2020,5 Conclusion,"in our best setup, about half of the generated poems are judged to be written by a human."
2020.acl-main.223.txt,2020,5 Conclusion,"secondly, we would like to incorporate further poetic devices, especially those based on meaning."
2020.acl-main.223.txt,2020,5 Conclusion,"specifically, we believe hierarchical approaches (serban et al., 2017) as well as the transformer network (vaswani et al., 2017) would be particularly suitable to poetry generation."
2020.acl-main.223.txt,2020,5 Conclusion,"the best verse is then selected for inclusion in the poem, using a global optimization framework."
2020.acl-main.223.txt,2020,5 Conclusion,the current version can be downloaded at https://github.com/timvdc/poetry.
2020.acl-main.223.txt,2020,5 Conclusion,"the results indicate that the system is able to generate credible poetry, that scores well with regard to fluency and coherence, as well as meaningfulness and poeticness."
2020.acl-main.223.txt,2020,5 Conclusion,"the system uses a recurrent neural encoder-decoder architecture in order to generate candidate verses, incorporating poetic and topical constraints by modifying the output probability distribution of the neural network."
2020.acl-main.223.txt,2020,5 Conclusion,we conclude with a number of future research avenues.
2020.acl-main.223.txt,2020,5 Conclusion,"we presented a system for automatic poetry generation that is trained exclusively on standard, non-poetic text."
2020.acl-main.223.txt,2020,5 Conclusion,"we trained the system on both english and french, and equally carried out a human evaluation for both languages."
2020.acl-main.224.txt,2020,6 Conclusion,experiments on webnlg dataset demonstrate the effectiveness of our planner and generator by outperforming the previous state-of-the-art by a large margin.
2020.acl-main.224.txt,2020,6 Conclusion,future work will validate the effectiveness of this method on more varied data-to-text generation tasks.
2020.acl-main.224.txt,2020,6 Conclusion,"this paper proposes dualenc, a dual encoding method to bridge the structural gap between encoder and decoder for data-to-text generation."
2020.acl-main.224.txt,2020,6 Conclusion,"this serialized plan is more compatible with the output sequence, making the information alignment between the input and output easier."
2020.acl-main.224.txt,2020,6 Conclusion,we also introduce an intermediate content planning stage to serialize the data and then encode it with an lstm network.
2020.acl-main.224.txt,2020,6 Conclusion,"we use gcn encoders to capture the structural information of the data, which is essential for accurate planning and faithful generation."
2020.acl-main.225.txt,2020,8 Conclusion,"furthermore, we demonstrated that our infilling framework is effective when starting from large-scale pre-trained lms, which may be useful in limited data settings."
2020.acl-main.225.txt,2020,8 Conclusion,"in future work, we plan to incorporate these features into co-creation systems which assist humans in the writing process."
2020.acl-main.225.txt,2020,8 Conclusion,our approach is capable of infilling sentences which humans have difficulty recognizing as machinegenerated.
2020.acl-main.225.txt,2020,8 Conclusion,"we hope that our work encourages more investigation of infilling, which may be a key missing element of current writing assistance tools."
2020.acl-main.225.txt,2020,8 Conclusion,we presented a simple strategy for the task of infilling which leverages language models.
2020.acl-main.226.txt,2020,5 Conclusions and Outlook,"a large-scale pre-trained variational autoencoder (li et al., 2020) could possibly improve the smoothness of sentence embeddings.(ii) our model predicts a feature vector for the missing sentence."
2020.acl-main.226.txt,2020,5 Conclusions and Outlook,"a post-editor to fix the issue (voita et al., 2019) should be able to understand inter-sentential relationship and to generate fluent sentences in the surrounding context, both of which can be learned from sentence infilling.note."
2020.acl-main.226.txt,2020,5 Conclusions and Outlook,"after this paper was posted on arxiv, some related works appeared.(shen et al., 2020) proposes blank language model for text infilling and other tasks.(donahue et al., 2020) trains (finetunes) a language model (gpt-2) for text and sentence infilling.(li et al., 2020) pre-trains a largescale variational autoencoder with a pair of bert and gpt-2.(ippolito et al., 2020) uses a sentencelevel language model, which operates on sentence embeddings obtained from bert, to predict story endings."
2020.acl-main.226.txt,2020,5 Conclusions and Outlook,"for example, in machine translation of long texts, it is often the case that sentences are translated independently from each other."
2020.acl-main.226.txt,2020,5 Conclusions and Outlook,"it is complementary to (token-level) masked language modeling, which focuses more on syntactic appropriateness and short-range correlation."
2020.acl-main.226.txt,2020,5 Conclusions and Outlook,"one can try to use a variational autoencoder (kingma and welling, 2014) instead."
2020.acl-main.226.txt,2020,5 Conclusions and Outlook,our approach can be modified or extended in some ways.(i) we use a denoising autoencoder to obtain sentence embeddings.
2020.acl-main.226.txt,2020,5 Conclusions and Outlook,sentence infilling requires the model to handle long-range inter-sentential correlation and to process high-level semantic information.
2020.acl-main.226.txt,2020,5 Conclusions and Outlook,"since sentence infilling is analogous to masked language modeling, we expect that it can also be used as a pre-training task."
2020.acl-main.226.txt,2020,5 Conclusions and Outlook,this sometimes leads to incoherence or even inconsistency between the translated sentences.
2020.acl-main.226.txt,2020,5 Conclusions and Outlook,"this vector can be fed into and serve as a guide to token-level models including the baseline (zhu et al., 2019)."
2020.acl-main.226.txt,2020,5 Conclusions and Outlook,we demonstrate the effectiveness of our approach using automatic and human evaluation.
2020.acl-main.226.txt,2020,5 Conclusions and Outlook,"we propose a framework called inset to decouple three aspects of the task (understanding, planning, and generation) and address them in a unified manner."
2020.acl-main.226.txt,2020,5 Conclusions and Outlook,"we study the task of sentence infilling, which is analogous to the masked language modeling task for (pre-)training bert, but now it is at the sentence level."
2020.acl-main.227.txt,2020,7 Conclusions,"furthermore, this framework can alleviate the sparse-reward issue, as the intermediate rewards are used to optimize the generator."
2020.acl-main.227.txt,2020,7 Conclusions,"our proposed models are validated on both unconditional and conditional text generation, including adversarial text generation and non-parallel style transfer."
2020.acl-main.227.txt,2020,7 Conclusions,the guider network provides a plan-ahead mechanism for next-word selection.
2020.acl-main.227.txt,2020,7 Conclusions,we achieve improved performance in terms of generation quality and diversity for unconditional and conditional generation tasks.
2020.acl-main.227.txt,2020,7 Conclusions,"we have proposed a model-based imitationlearning framework for adversarial text generation, by introducing a guider network to model the generation environment."
2020.acl-main.228.txt,2020,6 Conclusion and Future Work,"by performing analysis on gigaword, we find that there exists room to improve summarization performance with better post-ranking algorithms, a promising direction for future research."
2020.acl-main.228.txt,2020,6 Conclusion and Future Work,"in this paper, we presented a retrieve-edit-rerank framework for seq2seq text generation."
2020.acl-main.228.txt,2020,6 Conclusion and Future Work,"moving forward, we would like to apply this framework to other retrieve-and-edit based generation scenarios such as dialogue, conversation, and code generation."
2020.acl-main.228.txt,2020,6 Conclusion and Future Work,our results show that our simple ranking functions are effective in helping our model outperform the comparable retrieve-and-edit based methods for these datasets.
2020.acl-main.228.txt,2020,6 Conclusion and Future Work,"this is in line with our overall goal, which is not to find the best possible way to do the postranking, but only to show that gains are possible by editing multiple candidates and then comparing the results."
2020.acl-main.228.txt,2020,6 Conclusion and Future Work,we applied the framework on two mt datasets and the gigaword summarization dataset.
2020.acl-main.228.txt,2020,6 Conclusion and Future Work,"we used lucene for retrieval, a transformer model for editing, and simple task-specific post-generation ranking techniques."
2020.acl-main.229.txt,2020,6 Discussion,"first, despite the signiﬁcant improvement, the gap between short and long tasks is still large and needs to be further reduced."
2020.acl-main.229.txt,2020,6 Discussion,"for instance, developing agents that are robust to variations in both visual appearance and instruction descriptions is an important next step."
2020.acl-main.229.txt,2020,6 Discussion,"secondly, richer and more complicated variations between the learning setting and the real physical world need to be tackled."
2020.acl-main.229.txt,2020,6 Discussion,there are a few future directions to pursue.
2020.acl-main.23.txt,2020,8 Conclusion,"in this paper, we present a novel ppvae framework for flexible conditional text generation, which decouples the text generation module from the condition representation module."
2020.acl-main.23.txt,2020,8 Conclusion,the extensive experiments demonstrate the superiority of the proposed ppvae against the existing alternatives on conditionality and diversity while allowing new conditions to be added without a full retraining.
2020.acl-main.230.txt,2020,6 Conclusions and Future Work,experiments demonstrate its effectiveness as a new step towards semantic understanding of events in multimedia data.
2020.acl-main.230.txt,2020,6 Conclusions and Future Work,"in the future, we aim to extend our framework to extract events from videos, and make it scalable to new event types."
2020.acl-main.230.txt,2020,6 Conclusions and Future Work,in this paper we propose a new task of multimedia event extraction and setup a new benchmark.
2020.acl-main.230.txt,2020,6 Conclusions and Future Work,we also develop a novel multimedia structured common space construction method to take advantage of the existing image-caption pairs and singlemodal annotated data for weakly supervised training.
2020.acl-main.230.txt,2020,6 Conclusions and Future Work,"we plan to expand our annotations by including event types from other text event ontologies, as well as new event types not in existing text ontologies."
2020.acl-main.230.txt,2020,6 Conclusions and Future Work,"we will also apply our extraction results to downstream applications including cross-media event inference, timeline generation, etc."
2020.acl-main.231.txt,2020,8 Discussion,"future work might explore methods for incorporating richer learned representations both of the diverse visual observations in videos, and the narration that describes them, into such models."
2020.acl-main.231.txt,2020,8 Discussion,"our results also illustrate the importance of strong baselines: without weak supervision from step orderings and narrative language, even state-of-the-art unsupervised action segmentation models operating on rich video features underperform feature-agnostic baselines."
2020.acl-main.231.txt,2020,8 Discussion,"we find that unsupervised action segmentation in naturalistic instructional videos is greatly aided by the inductive bias given by typical step orderings within a task, and narrative language describing the actions being done."
2020.acl-main.231.txt,2020,8 Discussion,we hope that future work will continue to evaluate broadly.
2020.acl-main.231.txt,2020,8 Discussion,"while action segmentation in videos from diverse domains remains challenging – videos contain both a large variety of types of depicted actions, and high visual variety in how the actions are portrayed – we find that structured generative models provide a strong benchmark for the task due to their abilities to capture the full diversity of action types (by directly modeling distributions over action occurrences), and to benefit from weak supervision."
2020.acl-main.231.txt,2020,8 Discussion,"while some results are more mixed (with the same supervision, different models are better on different metrics), we do observe that across settings and metrics, step ordering and narration increase performance."
2020.acl-main.232.txt,2020,7 Conclusion and Future Work,"and, finally, an approach like the speaker-follower models of fried et al.(2018) could be used to train our builder model and the architect model of narayan-chen et al.(2019) jointly."
2020.acl-main.232.txt,2020,7 Conclusion and Future Work,"for true interactivity, the builder must be augmented with the capability to determine when and how to respond when it is too uncertain to act."
2020.acl-main.232.txt,2020,7 Conclusion and Future Work,"in the future, richer representations of the dialogue history (e.g.by using bert (devlin et al., 2019) or of past builder actions) combined with de-noising of the human data and perhaps more exhaustive data augmentation should produce better output sequences."
2020.acl-main.232.txt,2020,7 Conclusion and Future Work,"in the minecraft collaborative building task, builders must be able to comprehend complex instructions in order to achieve their primary goal of building 3d structures."
2020.acl-main.232.txt,2020,7 Conclusion and Future Work,our models process the game history along with a 3d representation of the evolving world to predict actions in a sequence-to-sequence fashion.
2020.acl-main.232.txt,2020,7 Conclusion and Future Work,"to this end, we define the challenging subtask of builder action prediction, tasking models with generating appropriate action sequences learned from the actions of human builders."
2020.acl-main.232.txt,2020,7 Conclusion and Future Work,"we show that these models, especially when conditioned on a suitable amount of game history and trained on larger amounts of synthetically generated data, improve over naive baselines."
2020.acl-main.233.txt,2020,5 Conclusion,experimental results on two standard datasets show that mart has better overall performance than the baseline methods.
2020.acl-main.233.txt,2020,5 Conclusion,"in particular, mart can generate more coherent, less redundant paragraphs without any degradation in relevance."
2020.acl-main.233.txt,2020,5 Conclusion,"in this work, we present a new approach – memory-augmented recurrent transformer (mart) for video paragraph captioning, where we designed an auxiliary memory module to enable recurrence in transformers."
2020.acl-main.234.txt,2020,6 Conclusion and Related Work,"for example, this can be achieved by distilling the original model to the less expressive variants, and observing both the agreement between the models and their performance."
2020.acl-main.234.txt,2020,6 Conclusion and Related Work,"in our case, this requires further development of distillation methods for the type of reinforcement learning setup vg-nsl uses, an effort that is beyond the scope of this paper."
2020.acl-main.234.txt,2020,6 Conclusion and Related Work,"our analysis shows that the visual signal leads vg-nsl to rely mostly on estimates of noun concreteness, in contrast to more complex syntactic reasoning."
2020.acl-main.234.txt,2020,6 Conclusion and Related Work,"our conclusions are similar: multi-modal models often rely on simple signals, and do not exhibit the complex reasoning we would like them to acquire."
2020.acl-main.234.txt,2020,6 Conclusion and Related Work,"our empirical analysis is related to htut et al.(2018), who methodologically, and successfully replicate the results of shen et al.(2018a) to study their performance."
2020.acl-main.234.txt,2020,6 Conclusion and Related Work,our work is related to the recent inference procedure analysis of dyer et al.(2019).
2020.acl-main.234.txt,2020,6 Conclusion and Related Work,"studying this type of difference between expressive models and their less expressive, restricted variants remains an important direction for future work."
2020.acl-main.234.txt,2020,6 Conclusion and Related Work,the issues we study generalize beyond the parsing task.
2020.acl-main.234.txt,2020,6 Conclusion and Related Work,"the question of what is captured by vision and language models has been studied before, including for visual question answering (agrawal et al., 2016, 2017; goyal et al., 2017), referring expression resolution (cirik et al., 2018), and visual navigation (jain et al., 2019)."
2020.acl-main.234.txt,2020,6 Conclusion and Related Work,"we ask this question in the setting of syntactic parsing, which allows to ground the analysis in the underlying formalism."
2020.acl-main.234.txt,2020,6 Conclusion and Related Work,"we studied the vg-nsl model by introducing several significantly less expressive variants, analyzing their outputs, and showing they maintain, and even improve performance."
2020.acl-main.234.txt,2020,6 Conclusion and Related Work,"while our model variants are very similar to the original vg-nsl, they are not completely identical, as reflected by the self-f1 scores in table 2."
2020.acl-main.234.txt,2020,6 Conclusion and Related Work,"while they study what biases a specific inference algorithm introduces to the unsupervised parsing problem, we focus on the representation induced in a grounded version of the task."
2020.acl-main.235.txt,2020,6 Conclusions and Future Work,experiments show that our approach outperforms strong baselines and is competitive with more complex methods which keeping substantially faster.
2020.acl-main.235.txt,2020,6 Conclusions and Future Work,"for these distributions, we can approximate them by the gaussian distributions (such as in srivastava and sutton (2017))."
2020.acl-main.235.txt,2020,6 Conclusions and Future Work,further study in this direction may be interesting.
2020.acl-main.235.txt,2020,6 Conclusions and Future Work,"however, it is hard to get the same formula for some more strong or sophisticated priors, e.g., the dirichlet prior."
2020.acl-main.235.txt,2020,6 Conclusions and Future Work,"in this paper, we tackle the posterior collapse problem when vae is paired with autoregressive decoders."
2020.acl-main.235.txt,2020,6 Conclusions and Future Work,"in this way, we can batch normalize the corresponding parameters."
2020.acl-main.235.txt,2020,6 Conclusions and Future Work,"instead of considering the kl individually, we make it follow a distribution dkl and show that keeping the expectation of dkl positive is sufficient to prevent posterior collapse."
2020.acl-main.235.txt,2020,6 Conclusions and Future Work,our approach can also avoid the recently proposed lagging problem efficiently without additional training efforts.
2020.acl-main.235.txt,2020,6 Conclusions and Future Work,the key to our approach to be applicable is that we can get a formula for the expectation of the kl.
2020.acl-main.235.txt,2020,6 Conclusions and Future Work,we leverage the gaussian prior as the example to introduce our method in this work.
2020.acl-main.235.txt,2020,6 Conclusions and Future Work,"we propose batch normalized vae (bn-vae), a simple but effective approach to set a lower bound ofdkl by regularization the approximate posterior’s parameters."
2020.acl-main.235.txt,2020,6 Conclusions and Future Work,we show that our approach can be easily extended to cvae.
2020.acl-main.235.txt,2020,6 Conclusions and Future Work,"we test our approach on three real applications, language modeling, text classification and dialogue generation."
2020.acl-main.236.txt,2020,5 Conclusion,we compared the performance of contextual embeddings with non-contextual pretrained embeddings and with an even simpler baseline—random embeddings.
2020.acl-main.236.txt,2020,5 Conclusion,"we hope this work inspires future research on better understanding the differences between embedding methods, and on designing simpler and more efficient models."
2020.acl-main.236.txt,2020,5 Conclusion,we showed that these non-contextual embeddings perform surprisingly well relative to the contextual embeddings on tasks with plentiful labeled data and simple language.
2020.acl-main.236.txt,2020,5 Conclusion,"while much recent and impressive effort in academia and industry has focused on improving state-of-the-art performance through more sophisticated, and thus increasingly expensive, embedding methods, this work offers an alternative perspective focused on realizing the trade-offs involved when choosing or designing embedding methods."
2020.acl-main.237.txt,2020,8 Conclusion,a potential future research direction is to bridge the gap between this simple bootstrapping paradigm and the incorporation of user free-form responses to allow the system to handle free-text responses.
2020.acl-main.237.txt,2020,8 Conclusion,"our expertguided, incremental design of questions and answers enables easy extension to add new classes, striking the balance between simplicity and extendability."
2020.acl-main.237.txt,2020,8 Conclusion,"our method uses information gain to select the best question to ask at every turn, and a lightweight policy to efficiently control the interaction."
2020.acl-main.237.txt,2020,8 Conclusion,our modeling choices enable the system to perform zero-shot generalization to unseen classification targets and questions.
2020.acl-main.237.txt,2020,8 Conclusion,we demonstrate that the system can be bootstrapped without any interaction data and show effectiveness on two tasks.
2020.acl-main.237.txt,2020,8 Conclusion,"we hope our work will encourage more research on different possibilities of building interactive systems that do not necessarily require handling full-fledged dialogue, but still benefit from user interaction."
2020.acl-main.237.txt,2020,8 Conclusion,"we propose an approach for interactive classification, where the system can inquire missing information through a sequence of simple binary or multi-choice questions when users provide underspecified natural language queries."
2020.acl-main.238.txt,2020,6 Conclusion,"at test time, the discrete kg representation can be cheaply and efficiently converted to a dense embedding and then used in any downstream application requiring the use of a knowledge graph."
2020.acl-main.238.txt,2020,6 Conclusion,"in the future, we would like to jointly learn discrete representations of entities as well as relations."
2020.acl-main.238.txt,2020,6 Conclusion,"in this work, we only considered the problem of learning discrete entity representations."
2020.acl-main.238.txt,2020,6 Conclusion,"in this work, we proposed novel and general approaches for kg embedding compression."
2020.acl-main.238.txt,2020,6 Conclusion,our approaches learn to represent entities in a kg as a vector of discrete codes in an end-to-end fashion.
2020.acl-main.238.txt,2020,6 Conclusion,the embedding layer contains majority of the parameters in any representation learning approach on knowledge graphs.
2020.acl-main.238.txt,2020,6 Conclusion,this is a barrier in successful deployment of models using knowledge graphs at scale on user-facing computing devices.
2020.acl-main.238.txt,2020,6 Conclusion,we evaluated our proposed methods on different link prediction and kg inference tasks and show that the proposed methods for kg embedding compression can effectively compress the kg embedding table without suffering any significant loss in performance.
2020.acl-main.239.txt,2020,5 Conclusion,"also, we have demonstrated our model with muse, since it provides word alignment across languages."
2020.acl-main.239.txt,2020,5 Conclusion,it achieves competitive results with a relatively simple baseline.
2020.acl-main.239.txt,2020,5 Conclusion,this work demonstrates the power of sentence reconstruction for transferring knowledge from a rich dataset to a sparse one.
2020.acl-main.239.txt,2020,5 Conclusion,we also show its strength in few-shot and one-shot learning.
2020.acl-main.239.txt,2020,5 Conclusion,we believe that using the proposed sentence `2 reconstruction may contribute as an auxiliary loss for other tasks.
2020.acl-main.239.txt,2020,5 Conclusion,"yet, our approach can be applied also with other more recent language models that have stronger context-based embeddings."
2020.acl-main.24.txt,2020,6 Conclusion,"besides, the proposed probabilistic masking scheme also improves the nlu capability of a masked language model."
2020.acl-main.24.txt,2020,6 Conclusion,the experiments show that the text generated in arbitrary order has comparable quality with gpt.
2020.acl-main.24.txt,2020,6 Conclusion,we have proposed a probabilistically masked language model for autoregressive generation in arbitrary word order.
2020.acl-main.240.txt,2020,6 Conclusion,"future work could find additional modular uses of mlms, simplify maskless pll computations, and use plls to devise better sentence- or document-level scoring metrics."
2020.acl-main.240.txt,2020,6 Conclusion,"we attributed this to pll’s promotion of fluency via self-consistency, as demonstrated by improvement on unsupervised acceptability judgements and by qualitative analysis."
2020.acl-main.240.txt,2020,6 Conclusion,"we examined the numerical properties of plls, proposed maskless scoring for speed, and proposed pseudo-perplexities for intrinsic evaluation of mlms, releasing a codebase implementing our work."
2020.acl-main.240.txt,2020,6 Conclusion,we found rescoring with plls can match or outperform comparable scores from large unidirectional language models (gpt-2).
2020.acl-main.240.txt,2020,6 Conclusion,"we showed the effectiveness of n -best rescoring with plls from pretrained mlms in modern sequence-to-sequence models, for both asr and low- to medium-resource nmt."
2020.acl-main.240.txt,2020,6 Conclusion,we studied scoring with mlm pseudo-loglikelihood scores in a variety of settings.
2020.acl-main.241.txt,2020,5 Conclusions,"experimental results on standard benchmark fb15k-237 and wn18rr show that ote improves consistently over rotate, the state-of-the-art distance-based embedding model, especially on fb15k-237 with many high in-degree nodes."
2020.acl-main.241.txt,2020,5 Conclusions,"first, ote extends the modeling of rotate from 2d complex domain to high dimensional space with orthogonal relation transforms."
2020.acl-main.241.txt,2020,5 Conclusions,in this paper we propose a new distance-based knowledge graph embedding for link prediction.it includes two-folds.
2020.acl-main.241.txt,2020,5 Conclusions,on wn18rr our model achieves the new state-of-the-art results.
2020.acl-main.241.txt,2020,5 Conclusions,"second, graph context is proposed to integrate graph structure information into the distance scoring function to measure the plausibility of the triples during training and inference."
2020.acl-main.241.txt,2020,5 Conclusions,"the proposed approach effectively improves prediction accuracy on the difficult n-to-1, 1-to-n and n-to-n link predictions."
2020.acl-main.241.txt,2020,5 Conclusions,this work is partially supported by beijing academy of artificial intelligence (baai).
2020.acl-main.242.txt,2020,5 Conclusion and Future Directions,"currently, we fix the bin size at 10 and then estimate q by calculating accuracy of p per bin."
2020.acl-main.242.txt,2020,5 Conclusion and Future Directions,estimating q with adaptive binning can be a potential alternative for the fixed binning.
2020.acl-main.242.txt,2020,5 Conclusion and Future Directions,our experiments empirically show that poscal can improve both the performance of classifiers and the quality of predicted posterior output compared to mle-based classifiers.
2020.acl-main.242.txt,2020,5 Conclusion and Future Directions,"the theoretical underpinnings of our poscal idea are not explored in detail here, but developing formal statistical support for these ideas constitutes interesting future work."
2020.acl-main.242.txt,2020,5 Conclusion and Future Directions,we propose a simple yet effective training technique called poscal for better posterior calibration.
2020.acl-main.243.txt,2020,10 Conclusion,another direction is to improve the sources of weak supervision and such as interactive new constraints provided by users.
2020.acl-main.243.txt,2020,10 Conclusion,"finally, it would be interesting to explore alternative training methods for these models, such as reducing reliance on hard sampling through better relaxations of structured models."
2020.acl-main.243.txt,2020,10 Conclusion,induction of grounded control states opens up many possible future directions for this work.
2020.acl-main.243.txt,2020,10 Conclusion,one could also explore alternative posterior constraints based on pre-trained models for summarization or paraphrase tasks to induce semantically grounded latent variables.
2020.acl-main.243.txt,2020,10 Conclusion,the methodology utilizes posterior regularization within a structured variational framework.
2020.acl-main.243.txt,2020,10 Conclusion,these states can be used to provide integration with external rule-based systems such as hard constraints at inference time.
2020.acl-main.243.txt,2020,10 Conclusion,they also can be used to provide tools for human-assisted generation.
2020.acl-main.243.txt,2020,10 Conclusion,this work introduces a method for controlling the output of a blackbox neural decoder model to follow weak supervision.
2020.acl-main.243.txt,2020,10 Conclusion,we show that this approach can induce a fully autoregressive neural model that is as expressive as standard neural decoders but also utilizes meaningful discrete control states.
2020.acl-main.243.txt,2020,10 Conclusion,we show this decoder is effective for text generation while inducing meaningful discrete representations.
2020.acl-main.244.txt,2020,6 Conclusion,"overall, our extensive evaluation shows that while pretrained transformers are moderately robust, there remains room for future research on robustness."
2020.acl-main.244.txt,2020,6 Conclusion,"to accomplish this, we carefully restructured and matched previous datasets to induce numerous realistic distribution shifts."
2020.acl-main.244.txt,2020,6 Conclusion,we created an expansive benchmark across several nlp tasks to evaluate out-of-distribution robustness.
2020.acl-main.244.txt,2020,6 Conclusion,"we first showed that pretrained transformers generalize to ood examples far better than previous models, so that the iid/ood generalization gap is often markedly reduced."
2020.acl-main.244.txt,2020,6 Conclusion,we then showed that pretrained transformers detect ood examples surprisingly well.
2020.acl-main.245.txt,2020,6 Discussion,additional related work.
2020.acl-main.245.txt,2020,6 Discussion,"besides typos, other perturbations can also be applied to text."
2020.acl-main.245.txt,2020,6 Discussion,"edizel et al.(2019) attempt to learn typo-resistant word embeddings, but focus on common typos, rather than worst-case typos."
2020.acl-main.245.txt,2020,6 Discussion,"garg et al.(2018) generate functions that map to robust features, while enforcing variation in outputs.incorporating context."
2020.acl-main.245.txt,2020,6 Discussion,"gong et al.(2019) apply a spellcorrector to correct typos chosen to create ambiguity as to the original word, but these typos are not adversarially chosen to fool a model."
2020.acl-main.245.txt,2020,6 Discussion,"however, existing typo correctors are far from perfect, and a choosing an incorrect unperturbed word from a perturbed input leads to errors in predictions of the downstream model."
2020.acl-main.245.txt,2020,6 Discussion,"in computer vision, chen et al.(2019) discretizes pixels to compute exact robust accuracy on mnist, but their approach generalizes poorly to other tasks like cifar-10."
2020.acl-main.245.txt,2020,6 Discussion,"in principle, an oracle that maps every word with a typo to the correct unperturbed word seems to have higher fidelity than our encodings, without compromising stability."
2020.acl-main.245.txt,2020,6 Discussion,"in this work, we introduce roben, a framework to construct systems that are robust to adversarial perturbations."
2020.acl-main.245.txt,2020,6 Discussion,"many recent advances in nlp have been fueled by the rise of task-agnostic representations, such as bert, that facilitate the creation of accurate models for many tasks."
2020.acl-main.245.txt,2020,6 Discussion,"other attack surfaces involving insertion of sentences (jia and liang, 2017) or syntactic rearrangements (iyyer et al., 2018) are harder to pair with roben, and are interesting directions for future work."
2020.acl-main.245.txt,2020,6 Discussion,other defenses are based on various forms of preprocessing.
2020.acl-main.245.txt,2020,6 Discussion,our framework extends easily to these perturbations.
2020.acl-main.245.txt,2020,6 Discussion,"our token-level robust encodings lead to strong performance, despite ignoring useful contextual information."
2020.acl-main.245.txt,2020,6 Discussion,our work shows that even simple robust encodings generalize across tasks and are more robust than existing defenses.
2020.acl-main.245.txt,2020,6 Discussion,"prior attacks consider semantic operations, such as replacing a word with a synonym (alzantot et al., 2018; ribeiro et al., 2018)."
2020.acl-main.245.txt,2020,6 Discussion,"robustness to typos should similarly be achieved in a task-agnostic manner, as it is a shared goal across many nlp tasks."
2020.acl-main.245.txt,2020,6 Discussion,this mandates an intractable search over all perturbations to compute the robust accuracy.task-agnosticity.
2020.acl-main.245.txt,2020,6 Discussion,"using context is not fundamentally at odds with the idea of robust encodings, and making contextual encodings stable is an interesting technical challenge and a promising direction for future work."
2020.acl-main.245.txt,2020,6 Discussion,we hope our work inspires new task-agnostic robust encodings that lead to more robust and more accurate models.
2020.acl-main.245.txt,2020,6 Discussion,we then use roben to achieve state-of-the-art robust accuracy when defending against adversarial typos.
2020.acl-main.246.txt,2020,5 Conclusions,"in this work, we provide a dual-pronged theoretical and empirical analysis of dodge et al.(2019)."
2020.acl-main.246.txt,2020,5 Conclusions,we demonstrate that it prefers negative errors and that bootstrapping leads to poorly controlled confidence intervals.
2020.acl-main.246.txt,2020,5 Conclusions,we empirically study its practical effects on tasks in document classification and sentiment analysis.
2020.acl-main.246.txt,2020,5 Conclusions,"we find unspoken caveats in their work—namely, that the estimator is statistically biased under weak conditions and uses an ecdf assumption that is subject to large errors."
2020.acl-main.247.txt,2020,7 Conclusion and Future Work,"in future work, we will address end-to-end question answering with pre-training for both the answer selection and retrieval components."
2020.acl-main.247.txt,2020,7 Conclusion and Future Work,"span selection pre-training is effective in improving reading comprehension across four diverse datasets, including both generated and natural questions, and with provided contexts of passages, documents and even passage sets."
2020.acl-main.247.txt,2020,7 Conclusion and Future Work,"the span selection task is suitable for pre-training on any domain, since it makes no assumptions about document structure or availability of summary/article pairs."
2020.acl-main.247.txt,2020,7 Conclusion and Future Work,this allows pre-training of language understanding models in a very generalizable way.
2020.acl-main.247.txt,2020,7 Conclusion and Future Work,"this style of pretraining focuses the model on finding semantic connections between two sequences, and supports a style of cloze that can train deep semantic understanding without demanding memorization of specific knowledge in the model."
2020.acl-main.247.txt,2020,7 Conclusion and Future Work,"we hope to progress to a model of general purpose language modeling that uses an indexed long term memory to retrieve world knowledge, rather than holding it in the densely activated transformer encoder layers."
2020.acl-main.248.txt,2020,4 Conclusion and Future Work,our future work is to include the paragraph representation in the constraint prediction model.
2020.acl-main.248.txt,2020,4 Conclusion and Future Work,this will help our methodology to have the benefit of making informed decision while also solving constraints.
2020.acl-main.248.txt,2020,4 Conclusion and Future Work,we acknowledge that our current model has the limitation of not including the entire context of the paragraph while making the decision of the relative order of the pairs.
2020.acl-main.248.txt,2020,4 Conclusion and Future Work,we have shown a new way to design the task of sentence ordering.
2020.acl-main.248.txt,2020,4 Conclusion and Future Work,we provide a simple yet efficient method to solve the task which outperforms the state of the art technique on all metrics.
2020.acl-main.249.txt,2020,7 Conclusion,"in this paper, we identify the potential for “weight poisoning” attacks where pre-trained models are “poisoned” such that they expose backdoors when fine-tuned."
2020.acl-main.249.txt,2020,7 Conclusion,"the most effective method — rip-ples — is capable of creating backdoors with success rates as high as 100%, even without access to the training dataset or hyperparameter settings."
2020.acl-main.249.txt,2020,7 Conclusion,"we hope that this work makes clear the necessity for asserting the genuineness of pre-trained weights, just like there exist similar mechanisms for establishing the veracity of other pieces of software."
2020.acl-main.249.txt,2020,7 Conclusion,we outline a practical defense against this attack that examines possible trigger keywords based on their frequency and relationship with the output class.
2020.acl-main.25.txt,2020,6 Conclusion,we believe our work paves the way for better understanding of neural text generation models and understanding that modeling choices reveals the model configurations is a first crucial step.
2020.acl-main.25.txt,2020,6 Conclusion,we proposed the novel task of machine configuration detection (mcd) which aided in the discovery of these artifacts.
2020.acl-main.25.txt,2020,6 Conclusion,"we studied machine generated text and found that modeling choices leave artifacts, i.e., it is possible to predict modeling choices such as parameterization/sampling choices by looking at generated text alone."
2020.acl-main.251.txt,2020,6 Conclusion,"in the future, we seek to expand upon energy-based translation using our method."
2020.acl-main.251.txt,2020,6 Conclusion,we proposed a new method to train non-autoregressive neural machine translation systems via minimizing pretrained energy functions with inference networks.
2020.acl-main.252.txt,2020,7 Conclusion and Future Directions,"future work should explore techniques like iterative back-translation (hoang et al., 2018) for further improvement and scaling to larger model capacities and more languages (arivazhagan et al., 2019b; huang et al., 2019) to maximize transfer across languages and across data sources."
2020.acl-main.252.txt,2020,7 Conclusion and Future Directions,"we demonstrate that combining multilingual nmt with monolingual data and self-supervision (i) improves the translation quality for both low and high-resource languages in a multilingual setting, (ii) leads to on-par zero-shot capability compared with competitive bridging-based approaches and (iii) is an effective way to extend multilingual models to new unseen languages."
2020.acl-main.252.txt,2020,7 Conclusion and Future Directions,"we present a simple framework to combine multi-lingual nmt with self-supervised learning, in an effort to jointly exploit the learning signals from multilingual parallel data and monolingual data."
2020.acl-main.253.txt,2020,6 Conclusions,"according to our findings, back-translation improves translation accuracy, for both source and target original sentences."
2020.acl-main.253.txt,2020,6 Conclusions,"however, automatic metrics like bleu fail to capture human preference for source original sentences (direct mode)."
2020.acl-main.253.txt,2020,6 Conclusions,"if human evaluation is not feasible, complementing standard metrics like bleu with a language model (§5) may help assessing the overall translation quality."
2020.acl-main.253.txt,2020,6 Conclusions,"in the future, we plan to investigate more thoroughly the use of language models for evaluating fluency, the effect of domain mismatch in the choice of monolingual data, and ways to generalize this study to other applications beyond mt."
2020.acl-main.253.txt,2020,6 Conclusions,"we find that bt produces outputs that are closer to natural text than the output of op, which may explain human preference for bt."
2020.acl-main.253.txt,2020,6 Conclusions,"we recommend distinguishing between direct and reverse translations for automatic evaluation, and to make final judgements based on human evaluation."
2020.acl-main.254.txt,2020,6 Conclusions,"experiments show that our method not only outperforms the original wait-k method with relatively large gap, but also surpasses greedy full-sentence translation with much lower latency."
2020.acl-main.254.txt,2020,6 Conclusions,"we have designed a simple heuristic algorithm to obtain an adaptive policy based on a set of wait-k policies, and applied ensemble in our method to improve the translation quality while maintaining low latency."
2020.acl-main.255.txt,2020,6 Conclusion,"most notably, our model is the first to break through the 80 f1 ceiling on the overall evaluation, the estimated upper bound on the task."
2020.acl-main.255.txt,2020,6 Conclusion,"on almost all the evaluation settings, our system beats the previous state of the art."
2020.acl-main.255.txt,2020,6 Conclusion,"on the multilingual setting, even with no training data besides the english corpora, ewiser sets the new state of the art."
2020.acl-main.255.txt,2020,6 Conclusion,"thanks to the joint exploitation of the wordnet graph and to the use of pretrained synset embeddings, ewiser is able to predict meanings which are not found in the training set, thus mitigating the knowledge acquisition bottleneck."
2020.acl-main.255.txt,2020,6 Conclusion,we leave it as future work to explore ways to raise accuracy on unseen synsets without harming performances on frequent synsets.
2020.acl-main.255.txt,2020,6 Conclusion,"we presented ewiser, a new neural wsd architecture that, by embedding information from the wordnet graph within the neural architecture, can also make use of the relational information that is usually only exploited by knowledge-based systems."
2020.acl-main.255.txt,2020,6 Conclusion,"we release the code used in the experiments, as well as pretrained models at github.com/sapienzanlp/ewiser."
2020.acl-main.256.txt,2020,6 Conclusion,"in this work, we propose a multi-modal framework that expand pre-trained embedding space to include oov words using character visual features such as cangjie feature and chinese character glyphs."
2020.acl-main.256.txt,2020,6 Conclusion,"we have demonstrated the effectiveness of glyph2vec on traditional chinese, and we believe glyph2vec can also be applied to other ideographic languages to handle oov words as well."
2020.acl-main.257.txt,2020,7 Conclusion and Future Work,in future work we will investigate more sophisticated neural (sub-)networks within the proposed framework.
2020.acl-main.257.txt,2020,7 Conclusion and Future Work,the pre-trained word vectors used in this work are available online at: https://github.com/cambridgeltl/fs-wrep.
2020.acl-main.257.txt,2020,7 Conclusion and Future Work,"we found that resulting function-specific vectors yield state-of-the-art results on established benchmarks for the tasks of estimating event similarity and evaluating thematic fit, previously held by task-specific methods."
2020.acl-main.257.txt,2020,7 Conclusion and Future Work,"we induced a joint vector space 6with separate parameters we merge vectors from “duplicate” vector spaces by non-weighted averaging.in which several groups of words (e.g., s, v, and o words forming the svo structures) are represented while taking into account the mutual associations between the groups."
2020.acl-main.257.txt,2020,7 Conclusion and Future Work,"we presented a novel multidirectional neural framework for learning function-specific word representations, which can be easily composed into multi-word representations to reason over event similarity and thematic fit."
2020.acl-main.257.txt,2020,7 Conclusion and Future Work,"we will also apply the idea of functionspecific training to other interrelated linguistic phenomena and other languages, probe the usefulness of function-specific vectors in other language tasks, and explore how to integrate the methodology with sequential models."
2020.acl-main.258.txt,2020,7 Conclusion,"an adapted siamese multi-channel network model performed best, and centering has an overall beneficial effect on pre-processing the vector spaces."
2020.acl-main.258.txt,2020,7 Conclusion,the models integrate general- vs. domain-specific word embedding information in different ways.
2020.acl-main.258.txt,2020,7 Conclusion,we semi-automatically created the first large-scale gold standard for technicality prediction across domains and proposed two novel neural network models to fine-tune automatic terminology extraction by distinguishing between degrees of technicality.
2020.acl-main.259.txt,2020,7 Conclusions and Future Work,"directionality of edges did not result in improvement in our models in this work, however for future, we plan to develop gcns that incorporate edge typing, which would enable us to differentiate between different mwe types and dependency relations while comparing them against the current models."
2020.acl-main.259.txt,2020,7 Conclusions and Future Work,experiments showed that the resulting system sets a new state-of-the-art in several criteria across two benchmark metaphor datasets.
2020.acl-main.259.txt,2020,7 Conclusions and Future Work,"for future work, we plan to add vmwe annotations to the vu amsterdam corpus (steen, 2010) which is the largest metaphor dataset and extend our experiments using that resource."
2020.acl-main.259.txt,2020,7 Conclusions and Future Work,"in this work, we presented a neural model to classify metaphorical verbs in their sentential context using information from the dependency parse tree and annotations for verbal multiword expressions."
2020.acl-main.259.txt,2020,7 Conclusions and Future Work,the code used in the experiments will be made publicly available 6.
2020.acl-main.259.txt,2020,7 Conclusions and Future Work,"to the best of our knowledge, this is the first mweaware metaphor identification system, that demonstrates how the knowledge of mwes can enhance the performance of a metaphor classification model."
2020.acl-main.26.txt,2020,5 Conclusions,an adaptive instance transfer and augmentation framework is designed for handling the task via an iterative learning algorithm.
2020.acl-main.26.txt,2020,5 Conclusions,experiments on real-world e-commerce data demonstrate the effectiveness of the training instance manipulation in our framework and the potentials of the review-based question generation task.
2020.acl-main.26.txt,2020,5 Conclusions,unsupervised aspect extraction is integrated for aspect-aware question generation.
2020.acl-main.26.txt,2020,5 Conclusions,"we propose a practical task of question generation from reviews, whose major challenge is the lack of training instances."
2020.acl-main.260.txt,2020,5 Conclusion,"however, due to the limitation of available resources, this study is limited to the european languages."
2020.acl-main.260.txt,2020,5 Conclusion,"however, most of the work only focuses on english corpora and little is known about the bias in multilingual embeddings."
2020.acl-main.260.txt,2020,5 Conclusion,"in practice, we can choose the embeddings aligned to a gender-rich language to reduce the bias."
2020.acl-main.260.txt,2020,5 Conclusion,"in this work, we build different metrics and datasets to analyze gender bias in the multilingual embeddings from both the intrinsic and extrinsic perspectives."
2020.acl-main.260.txt,2020,5 Conclusion,recently bias in embeddings has attracted much attention.
2020.acl-main.260.txt,2020,5 Conclusion,we encourage researchers to look at languages with different grammatical gender (such as czech and slovak) and propose new methods to reduce the bias in multilingual embeddings as well as in cross-lingual transfer learning.
2020.acl-main.260.txt,2020,5 Conclusion,we hope this study can work as a foundation to motivate future research about the analysis and mitigation of bias in multilingual embeddings.
2020.acl-main.260.txt,2020,5 Conclusion,we show that gender bias commonly exists across different languages and the alignment target for generating multilingual word embeddings also affects such bias.
2020.acl-main.261.txt,2020,5 Moving Forward,"a recent innovation in this direction has been the adoption of the acm code of ethics by the association for computational linguistics, and explicit requirement in the emnlp 2020 calls for papers for conformance with the code:10 where a paper may raise ethical issues, we ask that you include in the paper an explicit discussion of these issues, which will be taken into account in the review process."
2020.acl-main.261.txt,2020,5 Moving Forward,"and even if acl could do so, and wanted to do so, the efficacy of ethics to answer complex political and societal challenges needs to be questioned (mittelstadt, 2019)."
2020.acl-main.261.txt,2020,5 Moving Forward,"as argued above, it is not clear that acl should attempt to position itself as ethical gatekeeper, or has the resources to do so."
2020.acl-main.261.txt,2020,5 Moving Forward,"as noted above, there are certainly cases where even if there are no potential issues with the dataset, the resulting model can potentially be used for harm (e.g.gpt2)."
2020.acl-main.261.txt,2020,5 Moving Forward,"finally, while we have used one particular paper as a case study throughout this paper, our intent was in no way to name and shame the authors, but rather to use it as a case study to explore different ethical dimensions of research publications, and attempt to foster much broader debate on this critical issue for nlp research."
2020.acl-main.261.txt,2020,5 Moving Forward,"further, the term “anonymised” is often a misnomer as even data that is classified by governments and other actors as “anonymous” can often easily be reidentified (culnane and leins, 2020)."
2020.acl-main.261.txt,2020,5 Moving Forward,"given all of the above, what should have been the course of action for the paper in question?"
2020.acl-main.261.txt,2020,5 Moving Forward,"having said this, the acm code of ethics is (deliberately) abstract in its terms, with relevant principles which would guide an assessment of the paper in question including: 1.2 avoid harm; 1.4 be fair and take action not to discriminate; 1.6 respect privacy; 2.6 perform work only in areas of competence; and 3.1 ensure that the public good is the central concern during all professional computing work."
2020.acl-main.261.txt,2020,5 Moving Forward,"in each of these cases, the introspection present in a clearlyarticulated data statement would help ameliorate potential concerns."
2020.acl-main.261.txt,2020,5 Moving Forward,"it is important to note that the only mentions of research integrity/ethics in the call for papers relate to author anonymisation, dual submissions, originality, and the veracity of the research, meaning that there was no relevant mechanism for reviewers or pc chairs to draw on in ruling on the ethics of this or any other submission."
2020.acl-main.261.txt,2020,5 Moving Forward,looking to other scientific disciplines that have faced similar issues in the past may provide some guidance for our future.
2020.acl-main.261.txt,2020,5 Moving Forward,"one could consider this as part of an extension of data statements, in requiring that all code/model releases associated with acl papers be accompanied with a structured risk assessment of some description, and if risk is found to exist, some management plan be put in place."
2020.acl-main.261.txt,2020,5 Moving Forward,"should there be a requirement that code/model releases also be subject to scrutiny for possible misuse, e.g.via a central database/registry?"
2020.acl-main.261.txt,2020,5 Moving Forward,"the gdpr provides some protection for the use of data, but its scope and geographic reach are limited."
2020.acl-main.261.txt,2020,5 Moving Forward,"there certainly seems to be an argument for a requirement that papers describing new datasets are accompanied by a data statement or datasheet of some form (e.g.as part of the supplementary material, to avoid concerns over this using up valuable space in the body of the paper)."
2020.acl-main.261.txt,2020,5 Moving Forward,this still leaves the question of what to do with pre-existing datasets: should they all be given a free pass; or should there be a requirement for a data statement to be retrospectively completed?
2020.acl-main.261.txt,2020,5 Moving Forward,"we reserve the right to reject papers on ethical grounds, where the authors are judged to have operated counter to the code of ethics, or have inadequately addressed legitimate ethical concerns with their work this is an important first step, in providing a structure for the program committee to assess a paper for ethical compliance, and potentially reject it in cases of significant concerns."
2020.acl-main.261.txt,2020,5 Moving Forward,what about code and model releases?
2020.acl-main.261.txt,2020,5 Moving Forward,what could an ethics assessment for acl look like?
2020.acl-main.261.txt,2020,5 Moving Forward,would an ethics statement for acl be enough to address all concerns?
2020.acl-main.262.txt,2020,4 Conclusion,"although datasets currently used to estimate classification bias (e.g., winobias) are undoubtedly a step in the right direction, our findings suggest that they need to be much larger in order to be a useful diagnostic."
2020.acl-main.262.txt,2020,4 Conclusion,"given that most bias estimates are made using small samples, we proposed bernstein-bounded unfairness (bbu) for quantifying the uncertainty about a bias estimate using a confidence interval."
2020.acl-main.262.txt,2020,4 Conclusion,"in quantifying this uncertainty, bbu helps prevent classifiers from being deemed biased or unbiased when there is insufficient evidence to make either claim."
2020.acl-main.262.txt,2020,4 Conclusion,"using mnli, we provided empirical evidence that 95% bbu confidence intervals consistently bound the true populationlevel bias."
2020.acl-main.262.txt,2020,4 Conclusion,"we first showed that many standard measures of fairness (e.g., equal opportunity) can be expressed as the difference in expected cost incurred by protected and unprotected groups."
2020.acl-main.263.txt,2020,8 Conclusion,"ensuring that nlp technologies are inclusive, in the sense of working for users with diverse linguistic backgrounds (e.g., speakers of world englishes such as aave, as well as l2 speakers), is especially important since natural language user interfaces are becoming increasingly ubiquitous."
2020.acl-main.263.txt,2020,8 Conclusion,"finally, we show that, instead of retraining the model, fine-tuning it on a representative adversarial training set for a single epoch is sufficient to achieve significant robustness to inflectional adversaries while preserving performance on the clean dataset."
2020.acl-main.263.txt,2020,8 Conclusion,"next, we demonstrate the adversaries’ effectiveness using qa and mt, two tasks with direct and wide-ranging applications, before validating their plausibility and semantic content with real humans."
2020.acl-main.263.txt,2020,8 Conclusion,"to find these adversarial examples, we propose mor-pheus, which crafts plausible and semantically similar adversaries by perturbing an example’s inflectional morphology in a constrained fashion, without needing access to the model’s gradients."
2020.acl-main.263.txt,2020,8 Conclusion,we also present a method of generating this adversarial training set in linear time by making use of the adversarial examples’ inflectional distribution to perform weighted random sampling.
2020.acl-main.263.txt,2020,8 Conclusion,"we take a step in this direction by revealing the existence of linguistic bias in current english nlp models—e.g., bert and transformer—through the use of inflectional adversaries, before using adversarial fine-tuning to significantly reduce it."
2020.acl-main.264.txt,2020,7 Conclusion,"due to the limitation of the data, we only analyze the bias over binary gender."
2020.acl-main.264.txt,2020,7 Conclusion,"however, a comprehensive study is required to prove the conjecture and we leave this as future work."
2020.acl-main.264.txt,2020,7 Conclusion,"however, our analysis and the mitigation framework is general and can be adopted to other applications and other types of bias."
2020.acl-main.264.txt,2020,7 Conclusion,one remaining open question is why the gender bias in the posterior distribution is amplified.
2020.acl-main.264.txt,2020,7 Conclusion,"we analyzed the bias amplification from the posterior distribution perspective, which provides a better view to understanding the bias amplification issue in natural language models as these models are trained with the maximum likelihood objective."
2020.acl-main.264.txt,2020,7 Conclusion,we further proposed a bias mitigation technique based on posterior regularization and show that it effectively reduces the bias amplification in the distribution.
2020.acl-main.264.txt,2020,7 Conclusion,we posit that the regularization and the over-fitting nature of deep learning models might contribute to the bias amplification.
2020.acl-main.265.txt,2020,6 Conclusion,"additionally, future work should further probe the source of gender bias in the model’s predictions, perhaps by visualizing attention or looking more closely at the model’s outputs."
2020.acl-main.265.txt,2020,6 Conclusion,"in our study, we create and publicly release wiki-genderbias: the first dataset aimed at evaluating bias in nre models."
2020.acl-main.265.txt,2020,6 Conclusion,it is an open and difficult research question to build unbiased neural relation extractors.
2020.acl-main.265.txt,2020,6 Conclusion,one possibility is that some bias mitigation methods that add noise to the dataset encourage neural relation extraction models to learn spurious correlations and unwanted biases.
2020.acl-main.265.txt,2020,6 Conclusion,we also examine existing bias mitigation techniques and find that naive data augmentation causes a significant performance drop.
2020.acl-main.265.txt,2020,6 Conclusion,we encourage future work to dive deeper into this problem.
2020.acl-main.265.txt,2020,6 Conclusion,we find a difference in f1 scores for the spouse relation between predictions on male sentences and female for the model’s predictions.
2020.acl-main.265.txt,2020,6 Conclusion,"we only consider binary gender, but future work should consider non-binary genders."
2020.acl-main.265.txt,2020,6 Conclusion,we train nre models on the wikigenderbias dataset and test them on genderseparated test sets.
2020.acl-main.265.txt,2020,6 Conclusion,"while these findings will help future work avoid gender biases, this study is preliminary."
2020.acl-main.266.txt,2020,4 Conclusion,"beyond applications to typeface clustering, the general approach we take might apply more broadly to other clustering problems, and the model we developed might be incorporated into ocr models for historical text."
2020.acl-main.267.txt,2020,6 Conclusion and Future Work,extensive experiments on four benchmark datasets validate the effectiveness of our approach.
2020.acl-main.267.txt,2020,6 Conclusion and Future Work,"first, we plan to explore why the model prefers “soft” attentions rather than “hard” ones, which is different from the findings in several prior works based on hard attention."
2020.acl-main.267.txt,2020,6 Conclusion and Future Work,"in addition, we propose two methods to ensure the numerical stability of the model training."
2020.acl-main.267.txt,2020,6 Conclusion and Future Work,"in our future work, we will explore several potential directions."
2020.acl-main.267.txt,2020,6 Conclusion and Future Work,"in this paper, we propose an attentive pooling with learnable norms (apln) approach for text representation."
2020.acl-main.267.txt,2020,6 Conclusion and Future Work,"instead of using a fixed pooling norm for universal text representation learning, we propose to learn the norm in an end-to-end framework to automatically find the optimal ones for learning text representations in different tasks."
2020.acl-main.267.txt,2020,6 Conclusion and Future Work,"second, we plan to study how to model the differences on the characteristics of different samples and use different pooling norms, which may have the potential to further improve our approach."
2020.acl-main.267.txt,2020,6 Conclusion and Future Work,"the first one is scale limiting, which limits the scale of input representations to ensure their non-negativity and avoid potential exponential explosion."
2020.acl-main.267.txt,2020,6 Conclusion and Future Work,"the second one is re-formulation, which decomposes the exponent operation into several safe atomic operations to avoid computing the real-valued powers of input features with less computational cost."
2020.acl-main.267.txt,2020,6 Conclusion and Future Work,"third, we will explore how to generalize our approach to other modalities, such as images, audios and videos, to see whether it can facilitate more attention-based methods."
2020.acl-main.268.txt,2020,6 Conclusion,"in contrast to previous methods, our measures do not require the label sets to be the same and do not require automatic tagging."
2020.acl-main.268.txt,2020,6 Conclusion,"in future work, apart from improving the similarity measures, it could be examined to predict mtl scores or estimate the right amount of auxiliary data or shared parameters in the neural network."
2020.acl-main.268.txt,2020,6 Conclusion,our experimental findings are also supported from a theoretical point of view.
2020.acl-main.268.txt,2020,6 Conclusion,the developed methods working on both words and their labels have a substantial advantage over approaches that are based only on words or the label distributions.
2020.acl-main.268.txt,2020,6 Conclusion,the experiments show that similarity measures allow ordering the effects of auxiliary datasets by direction and intensity for an individual training dataset.
2020.acl-main.268.txt,2020,6 Conclusion,the quick similarity calculation can improve the main task performance when better datasets are used as auxiliary data that would never have made it through the otherwise purely manual preselection process.
2020.acl-main.268.txt,2020,6 Conclusion,the similarity measures allow distinguishing good from bad candidates for usage as auxiliary data.
2020.acl-main.268.txt,2020,6 Conclusion,this is an immensely valuable information as the number of expensive neural network training runs can be reduced to a fraction while still finding the best auxiliary dataset(s) to increase performance on the main task.
2020.acl-main.269.txt,2020,6 Conclusion,"future directions include validating our findings on other san architectures (e.g., bert (devlin et al., 2019)) and more general attention models (bahdanau et al., 2015; luong et al., 2015)."
2020.acl-main.269.txt,2020,6 Conclusion,"in this work, we make an early attempt to assess the strengths of the selective mechanism for sans, which is implemented with a flexible gumbel-softmax approach."
2020.acl-main.269.txt,2020,6 Conclusion,"through several well-designed experiments, we empirically reveal that the selective mechanism migrates two major weaknesses of sans, namely word order encoding and structure modeling, which are essential for natural language understanding and generation."
2020.acl-main.27.txt,2020,8 Conclusion,"as future work, we will extend our framework to more complex contexts by devising efficient learning algorithms."
2020.acl-main.27.txt,2020,8 Conclusion,"in addition, a hierarchical reinforcement learning method is provided for the training of our framework."
2020.acl-main.27.txt,2020,8 Conclusion,"in this paper, we present a type auxiliary guiding encoder-decoder framework for the code comment generation task."
2020.acl-main.27.txt,2020,8 Conclusion,our proposed framework also verifies the necessity of the type information in the code translation related tasks with a practical framework and good results.
2020.acl-main.27.txt,2020,8 Conclusion,our proposed framework takes full advantage of the type information associated with the code through the well designed type-associated encoder and type-restricted decoder.
2020.acl-main.27.txt,2020,8 Conclusion,the experimental results demonstrate significant improvements over state-of-the-art approaches and strong applicable potential in software development.
2020.acl-main.270.txt,2020,8 Conclusion,"although sandwich ordering does not improve translation models, we show that they are robust to layer order changes, and that even extreme reorderings (all attention sublayers at the bottom, and all the feedforward sublayers at the top) perform as well as the baseline."
2020.acl-main.270.txt,2020,8 Conclusion,"by showing that sublayer ordering can improve models at no extra cost, we hope that future research continues this line of work by looking into optimal sublayer ordering for other tasks, such as translation, question answering, and classification."
2020.acl-main.270.txt,2020,8 Conclusion,"sublayer reordering can improve the performance of transformer models, but an ordering that improves models on one group of tasks (word/character-level language modeling) might not improve the performance on another task."
2020.acl-main.270.txt,2020,8 Conclusion,"this leads us to design a new transformer stack, the sandwich transformer, which significantly improves performance over the baseline at no cost in parameters, memory, or runtime."
2020.acl-main.270.txt,2020,8 Conclusion,"we observe that, on average, better models contain more self-attention sublayers at the bottom and more feedforward sublayer at the top."
2020.acl-main.270.txt,2020,8 Conclusion,"we then show that the sandwich ordering also improves language modeling performance on a different word-level language modeling benchmark, and that the sandwich pattern can be used to achieve state of the art results on character-level language modeling."
2020.acl-main.270.txt,2020,8 Conclusion,"we train random transformer models with reordered sublayers, and find that some perform better than the baseline interleaved transformer in language modeling."
2020.acl-main.271.txt,2020,7 Conclusion,further theoretical analysis can also be performed to elucidate the mechanisms of the proposed method.
2020.acl-main.271.txt,2020,7 Conclusion,"in this paper, we propose a single model ensemble technique called singleens."
2020.acl-main.271.txt,2020,7 Conclusion,"moreover, our method with tfm:bert surpassed the normal ensemble on the imdb and rotten datasets, while its parameter size was 1/k-times smaller."
2020.acl-main.271.txt,2020,7 Conclusion,our experiments demonstrated that the proposed method outperformed single models in both text classification and sequence labeling tasks.
2020.acl-main.271.txt,2020,7 Conclusion,the principle of singleens is to explicitly create multiple virtual models in a single model.
2020.acl-main.271.txt,2020,7 Conclusion,"the proposed method is not limited to the two aforementioned tasks, but can be applied to any nlp as well as other tasks such as machine translation and image recognition."
2020.acl-main.271.txt,2020,7 Conclusion,the results thus indicate that explicitly creating virtual models within a single model improves performance.
2020.acl-main.272.txt,2020,5 Conclusion,"beyond that, we use reinforcement learning to learn data selection policy automatically, thus obviating the need to manual adjustment."
2020.acl-main.272.txt,2020,5 Conclusion,experimental results on both benchmarks and real-world e-commerce dataset demonstrate the effectiveness of the integration of unlabeled data and the reinforced data selection policy.
2020.acl-main.272.txt,2020,5 Conclusion,"in this paper, we propose a reinforced self-training framework for zero-shot text classification."
2020.acl-main.272.txt,2020,5 Conclusion,"in this way, our approach could leverage unlabeled data and alleviate the domain shift between seen classes and unseen classes."
2020.acl-main.272.txt,2020,5 Conclusion,"to realize the transferring between classes with low similarity, our method essentially turns a zero-shot learning problem into a semi-supervised learning problem."
2020.acl-main.273.txt,2020,5 Conclusion,"besides, how to introduce scene graphs into multi-modal nmt is a worthy problem to explore."
2020.acl-main.273.txt,2020,5 Conclusion,experiment results and analysis on the multi30k dataset demonstrate the effectiveness of our model.
2020.acl-main.273.txt,2020,5 Conclusion,"finally, we will apply our model into other multi-modal tasks such as multi-modal sentiment analysis."
2020.acl-main.273.txt,2020,5 Conclusion,"in the future, we plan to incorporate attributes of visual objects and dependency trees to enrich the multi-modal graphs."
2020.acl-main.273.txt,2020,5 Conclusion,"in this paper, we have proposed a novel graph-based multi-modal fusion encoder, which exploits various semantic relationships between multi-modal semantic units for nmt."
2020.acl-main.274.txt,2020,5 Conclusion,"moreover, our approach also optimizes the translation mapping q in a bi-directional way, and has been shown better than all other unsupervised sota models with the refinement in table 1."
2020.acl-main.274.txt,2020,5 Conclusion,"our key insight is that the relaxed matching mitigates the counter-intuitive pairs by polysemy and obscure words, which is supported by comparing w.proc.-rmp with w.proc in table 1."
2020.acl-main.274.txt,2020,5 Conclusion,the optimal transport constraint considered by w.proc.is not proper for bli tasks.
2020.acl-main.274.txt,2020,5 Conclusion,this paper focuses on the matching procedure of bli task.
2020.acl-main.275.txt,2020,6 Conclusion,"our approach utilizes dynamic programming for marginalizing the latent segmentations when training, and inferring the highest probability segmentation when tokenizing."
2020.acl-main.275.txt,2020,6 Conclusion,"our comprehensive experiments show impressive improvements compared to state-of-the-art segmentation methods in nmt, i.e., bpe and its stochastic variant bpe dropout."
2020.acl-main.275.txt,2020,6 Conclusion,this paper introduces dynamic programming encoding in order to incorporate the information of the source language into subword segmentation of the target language.
2020.acl-main.276.txt,2020,5 Conclusion,"a popular approach in literature is the gromov-wasserstein (gw) alignment approach (me´moli, 2011; peyre´ et al., 2016; alvarez-melis and jaakkola, 2018), which constructs a transport map by viewing the two embedding spaces as distributions."
2020.acl-main.276.txt,2020,5 Conclusion,aligning the metric spaces of languages has a wide usage in cross-lingual applications.
2020.acl-main.276.txt,2020,5 Conclusion,both are motivated solely from the language perspective.
2020.acl-main.276.txt,2020,5 Conclusion,"empirically, we observe that the proposed algorithm mba outperforms the gw algorithm for learning bilingual mapping (alvarez-melis and jaakkola, 2018), demonstrating the benefit of geometric optimization modeling."
2020.acl-main.276.txt,2020,5 Conclusion,"in contrast, we have viewed unsupervised bilingual word alignment as an instance of the more general unsupervised domain adaptation problem."
2020.acl-main.276.txt,2020,5 Conclusion,"in particular, our formulation allows search over the space of doubly stochastic matrices and induces bi-directional mapping between the source and target words."
2020.acl-main.276.txt,2020,5 Conclusion,the riemannian framework allows to exploit the geometry of the doubly stochastic manifold.
2020.acl-main.277.txt,2020,6 Conclusion,"by determining segment length dynamically, recoversat is capable of recovering from missing token errors and reducing repetitive token errors."
2020.acl-main.277.txt,2020,6 Conclusion,"by explicitly detecting and deleting repetitive segments, recoversat is able to recover from repetitive token errors."
2020.acl-main.277.txt,2020,6 Conclusion,experiments on three widely-used benchmark datasets show that our recoversat model maintains comparable performance with more than 4× decoding speedup compared with the at model.
2020.acl-main.277.txt,2020,6 Conclusion,"in this work, we propose a novel semiautoregressive model recoversat to alleviate the multi-modality problem, which performs translation by generating segments non-autoregressively and predicts the tokens in a segment autoregressively."
2020.acl-main.278.txt,2020,7 Conclusion,"although nmt models are well-calibrated in training, we observe that they still suffer from miscalibration during inference because of the discrepancy between training and inference."
2020.acl-main.278.txt,2020,7 Conclusion,"as well-calibrated confidence estimation is more likely to establish trustworthiness with users, we plan to apply our work to interactive machine translation scenarios in the future."
2020.acl-main.278.txt,2020,7 Conclusion,"finally, we find that increasing model size can negatively affect the calibration of nmt models and this can be alleviated by only enlarging the encoder."
2020.acl-main.278.txt,2020,7 Conclusion,"through a series of in-depth analyses, we report several interesting findings which may help to analyze, understand and improve nmt models."
2020.acl-main.278.txt,2020,7 Conclusion,we further propose graduated label smoothing that can reduce the inference calibration error effectively.
2020.acl-main.278.txt,2020,7 Conclusion,we revisit recent advances and find that label smoothing and dropout play key roles in calibrating modern nmt models.
2020.acl-main.279.txt,2020,4 Conclusion,"as an exploration study for this newly proposed problem, the preliminary results have revealed the potential of signal to address the critical problems in the proposed task."
2020.acl-main.279.txt,2020,4 Conclusion,"for instance, figure 2 (a) proves that signal can be more sensitive to spam samples (imbalance challenge); case study (figure 3) shows the generation capacity of signal to simulate the phonetic or glyph variation mutations (camouflage challenge); comparing to classical diversity-based approach, we integrate self-diversity based active learning and generative learning which can greatly reduce the computational complexity (o (n) → o (n), efficiency challenge)."
2020.acl-main.279.txt,2020,4 Conclusion,"in the future, we plan to enable the glyph and phonetic variation detection by integrating the variation graph representation learning, which may improve signal’s performance."
2020.acl-main.279.txt,2020,4 Conclusion,"in this paper, we propose a signal model for chinese text spam detection."
2020.acl-main.279.txt,2020,4 Conclusion,signal integrates active learning and semi-supervised generative learning into a unified framework.
2020.acl-main.28.txt,2020,5 Conclusion and Future Work,experiments on four datasets show that upsa outperforms previous state-of-the-art unsupervised methods to a large extent.
2020.acl-main.28.txt,2020,5 Conclusion and Future Work,"in the future, we plan to apply the sa framework on syntactic parse trees in hopes of generating more syntactically different sentences (motivated by our case study)."
2020.acl-main.28.txt,2020,5 Conclusion and Future Work,"in this paper, we proposed a novel unsupervised approach upsa that generates paraphrases by simulated annealing."
2020.acl-main.280.txt,2020,6 Conclusion,"in ladan, a novel attention mechanism is proposed to extract the key features for distinguishing confusing law articles attentively."
2020.acl-main.280.txt,2020,6 Conclusion,"in the future, we plan to study complicated situations such as a law case with multiple defendants and charges."
2020.acl-main.280.txt,2020,6 Conclusion,"in this paper, we present an end-to-end model, ladan, to solve the issue of confusing charges in ljp."
2020.acl-main.280.txt,2020,6 Conclusion,"our attention mechanism not only considers the interaction between fact description and law articles but also the differences among similar law articles, which are effectively extracted by a graph neural network gdl proposed in this paper."
2020.acl-main.280.txt,2020,6 Conclusion,the experimental results on real-world datasets show that our ladan raises the f1-score of state-of-the-art by up to 5.79%.
2020.acl-main.281.txt,2020,6 Conclusion,"besides, we proposed a novel model, sama, for this task."
2020.acl-main.281.txt,2020,6 Conclusion,extensive experiments conducted on real-world job posting data demonstrated the effectiveness and superiority of sama.
2020.acl-main.281.txt,2020,6 Conclusion,"firstly, it decomposed the long text generation into two stages, including an mlc task and a multiple skills guided text generation task."
2020.acl-main.281.txt,2020,6 Conclusion,"in this paper, we proposed the job posting generation (jpg) task and formalized it to a conditional text generation problem."
2020.acl-main.281.txt,2020,6 Conclusion,"last but not least, the learned mapping relationships can be applied to various downstream tasks, such as automatic resume, and person-job fit."
2020.acl-main.281.txt,2020,6 Conclusion,"secondly, it considered both the local and the global information to generate accurate and rich skills."
2020.acl-main.281.txt,2020,6 Conclusion,the merits of sama come from three aspects.
2020.acl-main.282.txt,2020,5 Conclusion,experimental results on two widely used datasets indicate that our proposed model outperforms previous state-of-the-art methods.
2020.acl-main.282.txt,2020,5 Conclusion,"in this paper, we propose a novel hyperbolic and cograph representation framework for the automatic icd coding task, which can jointly exploit code hierarchy and code co-occurrence."
2020.acl-main.282.txt,2020,5 Conclusion,"moreover, we use the graph convolutional network to capture the co-occurrence correlation."
2020.acl-main.282.txt,2020,5 Conclusion,"we believe our method can also be applied to other tasks that need to exploit hierarchical label structure and label co-occurrence, such as fine-grained entity typing and hierarchical multi-label classification."
2020.acl-main.282.txt,2020,5 Conclusion,we exploit the hyperbolic representation learning method to leverage the code hierarchy in the hyperbolic space.
2020.acl-main.283.txt,2020,7 Conclusion,adaptive routing is additionally applied to improve the scalability of hypercaps by controlling the number of capsules during the routing procedure.
2020.acl-main.283.txt,2020,7 Conclusion,"as recent works explore the superiority of hyperbolic space to euclidean space for serval natural language processing tasks, we intend to couple with the hyperbolic neural networks (ganea et al., 2018b) and the hyperbolic word embedding method such as poincare´glove (tifrea et al., 2019) in the future."
2020.acl-main.283.txt,2020,7 Conclusion,extensive experiments are carried out on four benchmark datasets.
2020.acl-main.283.txt,2020,7 Conclusion,"results compared with the state-of-the-art methods demonstrate the superiority of hypercaps, especially on tail labels."
2020.acl-main.283.txt,2020,7 Conclusion,the proposed hypercaps takes advantage of the parallel combination of finegrained local and global contextual information and label-aware feature aggregation method hdr to dynamically construct label-aware hyperbolic capsules for tail and head labels.
2020.acl-main.283.txt,2020,7 Conclusion,we present the hyperbolic capsule networks (hypercaps) with hyperbolic dynamic routing (hdr) and adaptive routing for multi-label classification (mlc).
2020.acl-main.284.txt,2020,7 Conclusion,"in the end, we propose the novel idea of combining pre-trained embeddings from language models trained on different data sources, which substantially improves performance."
2020.acl-main.284.txt,2020,7 Conclusion,"in this paper, we introduce and address an important problem towards a better understanding of support tickets - segmentation of various non-natural language segments."
2020.acl-main.284.txt,2020,7 Conclusion,"it is still valuable to study models on open datasets, however, as these are readily available to the community."
2020.acl-main.284.txt,2020,7 Conclusion,"our future research direction includes a thorough study of differences in this dataset with actual tickets, and potential for transfer."
2020.acl-main.284.txt,2020,7 Conclusion,we also demonstrate the usefulness of the task with improvements in retrieval of the correct answer.
2020.acl-main.284.txt,2020,7 Conclusion,"we also study the performance of the most recent recurrent neural network-based approaches to sequence labelling, on this task."
2020.acl-main.284.txt,2020,7 Conclusion,"we create an annotated dataset for the task, on questions from the publicly available website, ask ubuntu."
2020.acl-main.285.txt,2020,6 Conclusion and Future Work,"obtaining large-scale data in all dimensions, mooccube can support new models and diverse nlp applications in moocs."
2020.acl-main.285.txt,2020,6 Conclusion and Future Work,promising future directions include: 1) utilize more types of data from mooccube to facilitate existing topics; 2) employ advanced models in existing tasks; 3) more innovative nlp application tasks in online education domain.
2020.acl-main.285.txt,2020,6 Conclusion and Future Work,"we also conduct prerequisite relation extraction as an example application, and experimental results show the potential of such a repository."
2020.acl-main.285.txt,2020,6 Conclusion and Future Work,"we present mooccube, a multi-dimensional data repository containing courses, concepts, and student activities from real mooc websites."
2020.acl-main.286.txt,2020,5 Conclusion,"in this paper, we investigate the problem of automatic diagnosis with emr documents for clinical decision support."
2020.acl-main.286.txt,2020,5 Conclusion,the evaluation conducted on the real emr documents from hospitals validates the effectiveness of the proposed framework compared to the baselines in automatic diagnosis with emr.
2020.acl-main.286.txt,2020,5 Conclusion,"the proposed design brings interpretability into the predictions, which is very important for the ai-empowered healthcare, without compromising the accuracy of convolutional networks."
2020.acl-main.286.txt,2020,5 Conclusion,we propose a novel framework that stacks the bayesian network ensembles on top of the entity-aware convolutional neural networks.
2020.acl-main.287.txt,2020,7 Conclusion,"in related work, el baff et al.(2019) revealed the impact of style features on generating pathos- and logos-oriented short argumentative texts based on the rhetorical strategies discussed by wachsmuth et al.(2018)."
2020.acl-main.287.txt,2020,7 Conclusion,"inspired by the theory of the high importance of the lead and ending in writing editorials (rich, 2015), we also reveal common effective and ineffective style sequences (lead-body-ending) statistically."
2020.acl-main.287.txt,2020,7 Conclusion,our findings help to understand how effective argumentation works in the political sphere of editorial argumentation — and how to generate such argumentation.
2020.acl-main.287.txt,2020,7 Conclusion,this paper analyzes the importance of news editorials style in achieving persuasive effects on readers with different political ideologies.
2020.acl-main.287.txt,2020,7 Conclusion,we find evidence that style has a significant influence on how a (liberal) editorial affects a (liberal) reader.
2020.acl-main.287.txt,2020,7 Conclusion,"with the findings of this paper, we go beyond, defining the basis of a styledependent generation model for more sophisticated argumentation, as found in news editorials."
2020.acl-main.288.txt,2020,5 Conclusions,however the previous approach employed a two-step pipeline framework and has some inherent flaws.
2020.acl-main.288.txt,2020,5 Conclusions,"in this paper, instead of a pipeline of two steps, we propose a joint end-to-end framework, called ecpe-2d, to represent the emotion-cause pairs by a 2d representation scheme, and integrate the 2d emotion-cause pair representation, interaction, and prediction into a joint a framework."
2020.acl-main.288.txt,2020,5 Conclusions,the emotion-cause pair extraction (ecpe) task has drawn attention recently.
2020.acl-main.288.txt,2020,5 Conclusions,"the experimental results on the benchmark emotion cause corpus demonstrate that in addition to the advantages of joint modeling, our approach outperforms the state-of-the-art method by 7.6 percentage points in terms of the f1 score on the ecpe task."
2020.acl-main.288.txt,2020,5 Conclusions,"we also develop two kinds of 2d transformers, i.e., window-constrained and cross-road 2d transformers, to further model the interaction of different emotion-cause pairs."
2020.acl-main.289.txt,2020,6 Conclusion and Future Work,"experimental results on the benchmark dataset demonstrate that rankcp significantly outperforms previous systems, and further analysis verifies the effectiveness of each component in our model."
2020.acl-main.289.txt,2020,6 Conclusion and Future Work,"finally, it would be interesting to study the semantic roles of emotion (bostan et al., 2020), which considers the full structure of an emotion expression and is more challenging."
2020.acl-main.289.txt,2020,6 Conclusion and Future Work,"first, current studies on emotion cause analysis mainly focus on clause-level extraction which is relatively coarse-grained, and it is desirable to further design fine-grained methods that can extract span-level or phrase-level emotion expressions and causes."
2020.acl-main.289.txt,2020,6 Conclusion and Future Work,"in future work, we shall explore the following directions."
2020.acl-main.289.txt,2020,6 Conclusion and Future Work,"in this paper, we propose the first one-step neural approach rankcp to tackle the problem of emotion-cause pair extraction, which emphasizes inter-clause modeling from a ranking perspective."
2020.acl-main.289.txt,2020,6 Conclusion and Future Work,"our approach effectively models inter-clause relationships to learn clause representations, and integrates relative position enhanced clause pair ranking into a unified neural network to extract emotioncause pairs in an end-to-end fashion."
2020.acl-main.289.txt,2020,6 Conclusion and Future Work,"second, designing effective methods to inject appropriate linguistic knowledge into neural models is valuable to emotion analysis tasks (ke et al., 2019; zhong et al., 2019)."
2020.acl-main.29.txt,2020,6 Conclusion and Future Work,"experiments demonstrate that jointly modeling the segmentation and segment labeling, segmentation alignment and exploration, and segment pooling each contribute to s-lstm’s improved performance."
2020.acl-main.29.txt,2020,6 Conclusion and Future Work,"having shown that segment bounds contain useful supervisory signal, it would be interesting to examine if segment hierarchies might also contain useful signal."
2020.acl-main.29.txt,2020,6 Conclusion and Future Work,in this paper we introduce the segment pooling lstm (s-lstm) model for joint segmentation and segment labeling tasks.
2020.acl-main.29.txt,2020,6 Conclusion and Future Work,"like segment bounds and headers, this structure is naturally available in wikipedia."
2020.acl-main.29.txt,2020,6 Conclusion and Future Work,"s-lstm is agnostic as to the sentence encoder used, so we would like to investigate the potential usefulness of transformer-based language models as sentence encoders."
2020.acl-main.29.txt,2020,6 Conclusion and Future Work,"there are additional engineering challenges associated with using models such as bert as sentence encoders, since encoding entire documents can be too expensive to fit on a gpu without model parallelism."
2020.acl-main.29.txt,2020,6 Conclusion and Future Work,we find that the model dramatically reduces segmentation error (by 30% on average across four datasets) while improving segment labeling accuracy compared to previous neural and non-neural baselines for both singlelabel and multi-label tasks.
2020.acl-main.29.txt,2020,6 Conclusion and Future Work,we would also like to investigate the usefulness of an unconsidered source of document structure: the hierarchical nature of sections and subsections.
2020.acl-main.290.txt,2020,6 Conclusion,"we present a simple model of aspect extraction that uses a frequency threshold for candidate selection together with a novel attention mechanism based on rbf kernels, together with an automated aspect assignment method."
2020.acl-main.290.txt,2020,6 Conclusion,"we show that for the task of assigning aspects to sentences in the restaurant domain, the rbf kernel attention mechanism outperforms a regular attention mechanism, as well as more complex models based on auto-encoders and topic models."
2020.acl-main.291.txt,2020,6 Conclusion,"in this paper, we proposed a semantic-emotion knowledge transferring (sekt) model for cross-target stance classification, which used the external knowledge from semantic and emotion lexicons as commonsense knowledge to bridge the gap across different targets."
2020.acl-main.291.txt,2020,6 Conclusion,"second, the gcn was employed to learn the graph representation that captured multi-hop semantic connections between words or emotion tags."
2020.acl-main.291.txt,2020,6 Conclusion,"specifically, we first built a se-graph from semantic and emotion lexicons, which leveraged external knowledge from both word-level and concept-level."
2020.acl-main.291.txt,2020,6 Conclusion,the experimental results demonstrated that the sekt model significantly outperformed the state-of-the-art methods for cross-target stance detection.
2020.acl-main.291.txt,2020,6 Conclusion,"third, we extend the standard bilstm classifier to fully integrate the external knowledge by adding a novel knowledge-aware memory unit to the bilstm cell."
2020.acl-main.292.txt,2020,7 Conclusion,"in this paper, we explored the role of external commonsense knowledge for domain adaptation."
2020.acl-main.292.txt,2020,7 Conclusion,our code is publicly available at https://github.com/declare-lab/kingdom.
2020.acl-main.292.txt,2020,7 Conclusion,our experiments demonstrate the usefulness of external knowledge for the task of cross-domain sentiment analysis.
2020.acl-main.292.txt,2020,7 Conclusion,"using the standard amazon benchmark for domain adaption in sentiment analysis, we showed that our framework exceeds the performance of previously proposed methods for the same task."
2020.acl-main.292.txt,2020,7 Conclusion,"we introduced a domain-adversarial framework called kingdom, which relies on an external commonsense kb (conceptnet) to perform unsupervised domain adaptation."
2020.acl-main.292.txt,2020,7 Conclusion,we showed that we can learn domain-invariant features for the concepts in the kb by using a graph convolutional autoencoder.
2020.acl-main.293.txt,2020,7 Conclusion and Future work,"one can try to adapt our proposed csae architecture for an integrated approach by applying the unified tagging scheme; thereby, aspect extraction and sentiment classification can be achieved simultaneously."
2020.acl-main.293.txt,2020,7 Conclusion and Future work,our proposed aspect sentiment classifier outperformed post-training asc model and enabled the creation of a domain-independent solution.
2020.acl-main.293.txt,2020,7 Conclusion and Future work,the proposed srd allows the aspect sentiment classifier to focus on critical sentiment words which modify the target aspect term through dependency-based structure.
2020.acl-main.293.txt,2020,7 Conclusion and Future work,the results indicate that exploitation of syntactical structures of sentences empowers the contextualized models to improve on current works in both asc and ae tasks.
2020.acl-main.293.txt,2020,7 Conclusion and Future work,the substantial improvements highlight the under-performance of recent contextualized embedding models in “understanding” syntactical features and suggests future directions in developing more syntax-learning contextualized embeddings.
2020.acl-main.293.txt,2020,7 Conclusion and Future work,we proposed an end-to-end absa solution which pipelined an aspect extractor and an aspect sentiment classifier.
2020.acl-main.294.txt,2020,5 Conclusion,"in this paper, we propose novel data augmentation methods for formality style transfer."
2020.acl-main.294.txt,2020,5 Conclusion,our proposed data augmentation methods can effectively generate diverse augmented data with various formality style transfer knowledge.
2020.acl-main.294.txt,2020,5 Conclusion,the augmented data can significantly help improve the performance when it is used for pre-training the model and leads to the state-of-the-art results in the formality style transfer benchmark dataset.
2020.acl-main.295.txt,2020,6 Conclusion,"experimental results on three public datasets showed that the connections between aspects and opinion words can be better established with r-gat, and the performance of gat and bert are signiﬁcantly improved as a result."
2020.acl-main.295.txt,2020,6 Conclusion,"finally, an error analysis was performed on incorrectly-predicted examples, leading to some insights into this task."
2020.acl-main.295.txt,2020,6 Conclusion,"in this paper, we have proposed an effective approach to encoding comprehensive syntax information for aspect-based sentiment analysis."
2020.acl-main.295.txt,2020,6 Conclusion,we also conducted an ablation study to validate the role of the new tree structure and the relational heads.
2020.acl-main.295.txt,2020,6 Conclusion,we ﬁrst deﬁned a novel aspect-oriented dependency tree structure by reshaping and pruning an ordinary dependency parse tree to root it at a target aspect.
2020.acl-main.295.txt,2020,6 Conclusion,we then demonstrated how to encode the new dependency trees with our relational graph attention network (r-gat) for sentiment classiﬁcation.
2020.acl-main.296.txt,2020,5 Conclusion,"for future works, we will explore pair-wise at and ot extraction together with aspect category and sentiment polarity classification."
2020.acl-main.296.txt,2020,5 Conclusion,"for joint optimizing the objectives of term extraction and pair-wise relation extraction, the two subtasks share the span representations and the losses are combined."
2020.acl-main.296.txt,2020,5 Conclusion,"in this paper, we study a novel task pair-wise aspect and opinion terms extraction (paote)."
2020.acl-main.296.txt,2020,5 Conclusion,our framework can effectively learn contextualized information with various base encoders.
2020.acl-main.296.txt,2020,5 Conclusion,"specifically, we try two different encoders (bilstm encoder and bert encoder)."
2020.acl-main.296.txt,2020,5 Conclusion,the experimental results demonstrate that our spanmlt significantly outperforms all the compared methods.
2020.acl-main.296.txt,2020,5 Conclusion,then a span generator enumerates all possible spans and each span is represented based on the outputs of the encoders.
2020.acl-main.296.txt,2020,5 Conclusion,we treat this task as a joint term and relation extraction problem and develop a span-based multi-task learning framework (spanmlt).
2020.acl-main.297.txt,2020,7 Conclusions,"in this paper, we present a syntax-aware opinion role labeling approach based on dependency gcn and mtl."
2020.acl-main.297.txt,2020,7 Conclusions,"overall, our syntax-aware model brings in about 9.29 improvement of exact f1 score compared with the syntax-agnostic baseline."
2020.acl-main.297.txt,2020,7 Conclusions,"the mtl framework further boosts the performance, and together with bert, our best model achieves a new state-of-the-art result on the widely-used orl benchmark mpqa corpus."
2020.acl-main.297.txt,2020,7 Conclusions,we compare different representations of syntactic dependency information and propose dependency gcn to encode richer structural information from different processing levels of the parser.
2020.acl-main.298.txt,2020,8 Conclusion,experimental results showed that our proposed model outperforms baselines.
2020.acl-main.298.txt,2020,8 Conclusion,"for future work, we aim to develop a universal model to handle both tree and non-tree arguments."
2020.acl-main.298.txt,2020,8 Conclusion,this paper demonstrated that we could successfully analyze more flexible structures in arguments.
2020.acl-main.298.txt,2020,8 Conclusion,this paper focused on non-tree argument mining.
2020.acl-main.298.txt,2020,8 Conclusion,we provided an approach to effectively encode a proposition sequence and to predict non-tree edges.
2020.acl-main.299.txt,2020,6 Conclusion,"compared with previous local normalization methods, our method is more accurate for considering more span information, and reserves the fast running speed due to the parallelizable linearization model."
2020.acl-main.299.txt,2020,6 Conclusion,"in addition, we build a new normalization method, which can add constraints on all the spans with the same right boundary."
2020.acl-main.299.txt,2020,6 Conclusion,"in this work, we propose a novel linearization of constituent trees tied on the spans tightly."
2020.acl-main.299.txt,2020,6 Conclusion,the experiments show that our model significantly outperforms existing local models and achieves competitive results with global models.
2020.acl-main.3.txt,2020,6 Conclusion,"experiments show that our model significantly outperforms existing cross-domain slot filling approaches, and it also achieves better performance for the cross-domain ner task, where there is no unseen label type in the target domain."
2020.acl-main.3.txt,2020,6 Conclusion,"moreover, template regularization is proposed to improve the adaptation robustness further."
2020.acl-main.3.txt,2020,6 Conclusion,our model shares its parameters across all slot types and learns to predict whether input tokens are slot entities or not.
2020.acl-main.3.txt,2020,6 Conclusion,"then, it detects concrete slot types for these slot entity tokens based on the slot type descriptions."
2020.acl-main.3.txt,2020,6 Conclusion,we introduce a new cross-domain slot filling framework to handle the unseen slot type issue.
2020.acl-main.30.txt,2020,8 Conclusions and Future Work,"also, extending our method for other types of textual data, such as short texts, multi-lingual data, and code-switched data is a potential direction."
2020.acl-main.30.txt,2020,8 Conclusions and Future Work,"currently, we perform coarse- and fine-grained classifications separately."
2020.acl-main.30.txt,2020,8 Conclusions and Future Work,"experimental results demonstrate that our model outperforms previous methods significantly, thereby signifying the superiority of contextualized weak supervision, especially when labels are fine-grained."
2020.acl-main.30.txt,2020,8 Conclusions and Future Work,"in the future, we are interested in generalizing contextualized weak supervision to hierarchical text classification problems."
2020.acl-main.30.txt,2020,8 Conclusions and Future Work,"in this paper, we proposed conwea, a novel contextualized weakly supervised classification framework."
2020.acl-main.30.txt,2020,8 Conclusions and Future Work,our method leverages contextualized representation techniques and initial user-provided seed words to contextualize the corpus.
2020.acl-main.30.txt,2020,8 Conclusions and Future Work,there should be more useful information embedded in the tree-structure of the label hierarchy.
2020.acl-main.30.txt,2020,8 Conclusions and Future Work,this contextualized corpus is further used to resolve the interpretation of seed words through iterative seed word expansion and document classifier training.
2020.acl-main.300.txt,2020,4 Discussion,"for models sensitive to random seeds, we recommend reporting means and standard deviations from multiple runs."
2020.acl-main.300.txt,2020,4 Discussion,"for the evaluation metrics, since small details in implementing micro and macro average will lead to nontrivial differences, we suggest using parseval which has publicly available implementation."
2020.acl-main.300.txt,2020,4 Discussion,"for the sentence length limit, we think one can set any limit on the training data, but should report evaluation results on both length ≤ 10 and alllength test data."
2020.acl-main.300.txt,2020,4 Discussion,"we also recommend evaluation on treebanks of both leftbranching and right-branching languages, such as ptb and ktb."
2020.acl-main.300.txt,2020,4 Discussion,we make the following recommendations for future experiments on unsupervised constituency parsing.
2020.acl-main.301.txt,2020,5 Conclusion,"on multilingual constituency parsing, it also establishes new state-of-the-art in basque and swedish."
2020.acl-main.301.txt,2020,5 Conclusion,our method utilizes an efficient top-down decoding algorithm that uses pointing functions for scoring possible spans.
2020.acl-main.301.txt,2020,5 Conclusion,"our method with pre-training rivals the state-of-the-art method, while being faster than it."
2020.acl-main.301.txt,2020,5 Conclusion,the pointing formulation inherently captures global structural properties and allows efficient training with cross entropy loss.
2020.acl-main.301.txt,2020,5 Conclusion,we have presented a novel constituency parsing method that is based on a pointing mechanism.
2020.acl-main.301.txt,2020,5 Conclusion,with experiments we have shown that our method outperforms all existing top-down methods on the english penn treebank parsing task.
2020.acl-main.302.txt,2020,6 Conclusions,this paper for the first time presents second-order treecrf for neural dependency parsing using triaffine for explicitly scoring second-order subtrees.
2020.acl-main.302.txt,2020,6 Conclusions,"we also empirically verify that the complex outside algorithm can be implicitly performed via efficient back-propagation, which naturally produces gradients and marginal probabilities."
2020.acl-main.302.txt,2020,6 Conclusions,"we conduct experiments and detailed analysis on 27 datasets from 13 languages, and find that structural learning and high-order modeling can further enhance the state-of-the-art biaffine parser in various aspects: 1) better convergence behavior; 2) higher performance on sub- and full-tree levels; 3) better utilization of partially annotated data."
2020.acl-main.302.txt,2020,6 Conclusions,we propose to batchify the inside algorithm to accommodate gpus.
2020.acl-main.303.txt,2020,7 Discussion,"another effective approach for improving the syntactic robustness of neural models is data augmentation, as demonstrated in experiment 2."
2020.acl-main.303.txt,2020,7 Discussion,"at the same time, as demonstrated by the constructed sentence test, the syntactic capabilities of the dependency lstm are inherently limited."
2020.acl-main.303.txt,2020,7 Discussion,"finally, future work should investigate whether data augmentation can fully bridge the gap between low-bias learners and structured tree lstms, and whether our conclusions apply to other syntactic phenomena besides agreement."
2020.acl-main.303.txt,2020,7 Discussion,future work should further explore both of these approaches.
2020.acl-main.303.txt,2020,7 Discussion,"however, it performs no better than the constituency model when trained on natural language, suggesting that there is little benefit to incorporating dependency structure into a constituency lstm."
2020.acl-main.303.txt,2020,7 Discussion,"in experiment 1, the network provided with a dependency parse did the best on most of the natural language test sets."
2020.acl-main.303.txt,2020,7 Discussion,"in future work, these syntactic limitations may be overcome by giving the model typed dependencies (which would distinguish between a subject-verb dependency and a verb-object dependency)."
2020.acl-main.303.txt,2020,7 Discussion,"in particular, our results suggest that the most important aspect of syntactic structure to include is constituency structure, as constituency models appear to implicitly learn dependency structure as well."
2020.acl-main.303.txt,2020,7 Discussion,"in some cases, the headlexicalized model without fine-tuning even performs worse than the constituency lstm."
2020.acl-main.303.txt,2020,7 Discussion,it seems particularly promising to explore alternative formulations of the dependency lstm (as mentioned above) and the effect of learning embeddings of non-terminal symbols for the constituency lstm.
2020.acl-main.303.txt,2020,7 Discussion,"one might expect the head-lexicalized model to perform the best, since it can leverage both syntactic formalisms."
2020.acl-main.303.txt,2020,7 Discussion,our conclusions about the importance of explicit mechanisms for representing syntactic structure can be strengthened by developing different formulations of the tree lstms.
2020.acl-main.303.txt,2020,7 Discussion,our results point to two possible approaches for improving how models handle syntax.
2020.acl-main.303.txt,2020,7 Discussion,"overall, we found that neural models trained on natural language achieve much more robust performance on syntactic tasks when syntax is explicitly built into the model."
2020.acl-main.303.txt,2020,7 Discussion,the first approach is to use models that have explicit mechanisms for representing syntactic structure.
2020.acl-main.303.txt,2020,7 Discussion,"this is unsurprising, as the task is largely about a particular dependency (i.e., the dependency between a verb and its subject)."
2020.acl-main.303.txt,2020,7 Discussion,this suggests that the information we provided to our tree-based models is unlikely to be learned from natural language by models with only general inductive biases.
2020.acl-main.303.txt,2020,7 Discussion,"though the models we used require parse trees to be provided, it is possible that models can learn to induce tree structure in an unsupervised or weakly-supervised manner (bowman et al., 2016; choi et al., 2018; shen et al., 2019)."
2020.acl-main.303.txt,2020,7 Discussion,"thus, it must default to non-robust heuristics in cases where the unlabeled dependency information is ambiguous."
2020.acl-main.303.txt,2020,7 Discussion,"when fine-tuned on more challenging constructed examples, the head-lexicalized model performed similarly to the constituency lstm, suggesting that there is not enough signal in the natural language training set to teach this model what to do with the heads it has been given."
2020.acl-main.303.txt,2020,7 Discussion,"with this approach, it is possible to bring the syntactic performance of less-structured models closer to that of models with explicit tree structure, even with an augmentation set generated simply and easily using a pcfg."
2020.acl-main.304.txt,2020,7 Conclusion,in this paper our major contributions are the two structure-level methods to distill the knowledge of monolingual models to a single multilingual model in sequence labeling: top-k knowledge distillation and posterior distillation.
2020.acl-main.304.txt,2020,7 Conclusion,our code is publicly available at https://github.com/alibaba-nlp/multilangstructurekd.
2020.acl-main.304.txt,2020,7 Conclusion,the analysis also shows that our model has stronger zero-shot transfer ability on unseen languages on the ner and pos tagging task.
2020.acl-main.304.txt,2020,7 Conclusion,the experimental results show that our approach improves the performance of multilingual models over 4 tasks on 25 datasets.
2020.acl-main.305.txt,2020,6 Conclusion,"experimental results on three new datasets from reddit show that our model significantly outperforms all comparisons, including previous state of the arts."
2020.acl-main.305.txt,2020,6 Conclusion,further discussion demonstrates the robustness of our model against history sparsity and cold start.
2020.acl-main.305.txt,2020,6 Conclusion,this paper presents a dynamic conversation recommendation model learned from the change of content and user interactions over time.
2020.acl-main.305.txt,2020,6 Conclusion,we also analyze our model’s outputs to get more insights into user interest dynamics.
2020.acl-main.306.txt,2020,5 Conclusion,experimental results show that our umt approach can consistently achieve the best performance on two benchmark datasets.
2020.acl-main.306.txt,2020,5 Conclusion,"in this paper, we first presented a multimodal transformer architecture for the task of mner, which captures the inter-modal interactions with a multi-modal interaction module."
2020.acl-main.306.txt,2020,5 Conclusion,"moreover, to alleviate the bias of the visual context, we further proposed a unified multimodal transformer (umt), which incorporates an entity span detection module to guide the final predictions for mner."
2020.acl-main.306.txt,2020,5 Conclusion,"on the one hand, despite bringing performance improvements over existing mner methods, our umt approach still fails to perform well on social media posts with unmatched text and images, as analyzed in section 3.5."
2020.acl-main.306.txt,2020,5 Conclusion,"on the other hand, since the size of existing mner datasets is relatively small, we plan to leverage the large amount of unlabeled social media posts in different platforms, and propose an effective framework to combine them with the small amount of annotated data to obtain a more robust mner model."
2020.acl-main.306.txt,2020,5 Conclusion,there are several future directions for this work.
2020.acl-main.306.txt,2020,5 Conclusion,"therefore, our next step is to enhance umt so as to dynamically filter out the potential noise from images."
2020.acl-main.307.txt,2020,8 Conclusion,a method was formulated to acquire such vectors from stock price history and news articles by using a neural network framework.
2020.acl-main.307.txt,2020,8 Conclusion,"as an example, we showed the use of stock embeddings in a portfolio optimization task by replacing the risk matrix in the portfolio objective function with a cosine matrix of stock embeddings."
2020.acl-main.307.txt,2020,8 Conclusion,"because the stock embedding is a vector that can be separated from the other components of the classification model, it can be applied to other tasks besides price movement classification."
2020.acl-main.307.txt,2020,8 Conclusion,"in investment simulations on the r&b dataset, our stock embedding method generated 2.80 times the annual return obtained using the covariance matrix of the historic return series."
2020.acl-main.307.txt,2020,8 Conclusion,"in the framework, the stock embedding detects news articles that are related to the stock, which is the essence of the proposed method."
2020.acl-main.307.txt,2020,8 Conclusion,"the improvements in classification accuracy with our framework, due to the classifier sharing and dual-vector text representation proposed in this paper, implied that the stock embeddings successfully incorporated market knowledge from both the news articles and price history."
2020.acl-main.307.txt,2020,8 Conclusion,"this paper has proposed the idea of a stock embedding, a vector representation of a stock in a financial market."
2020.acl-main.307.txt,2020,8 Conclusion,"this significant gain suggests further potential of our stock embedding for modeling the correlation among stocks in a financial market, and for further applications, such as risk control and asset pricing."
2020.acl-main.307.txt,2020,8 Conclusion,"we trained stock embeddings for the task of binary classification of stock price movements on two different datasets, the wsj and r&b."
2020.acl-main.308.txt,2020,5 Conclusion and Future Work,"another research direction is to profile media based on their stance with respect to previously fact-checked claims (mohtarami et al., 2018; shaar et al., 2020), or by the proportion and type of propaganda techniques they use (da san martino et al., 2019, 2020)."
2020.acl-main.308.txt,2020,5 Conclusion and Future Work,"finally, we plan to experiment with other languages."
2020.acl-main.308.txt,2020,5 Conclusion and Future Work,"in future work, we plan to perform user profiling with respect to polarizing topics such as gun control (darwish et al., 2020), which can then be propagated from users to media (atanasov et al., 2019; stefanov et al., 2020)."
2020.acl-main.308.txt,2020,5 Conclusion and Future Work,"the evaluation results have shown that while what was written matters most, the social media context is also important as it is complementary, and putting them all together yields sizable improvements over the state of the art."
2020.acl-main.308.txt,2020,5 Conclusion and Future Work,"we compared the textual content of what media publish vs. who read it on social media, i.e., on twitter, facebook, and youtube."
2020.acl-main.308.txt,2020,5 Conclusion and Future Work,"we further modeled different modalities: text, metadata, and speech signal."
2020.acl-main.308.txt,2020,5 Conclusion and Future Work,we further modeled what was written about the target medium in wikipedia.
2020.acl-main.308.txt,2020,5 Conclusion and Future Work,"we further want to model the network structure, e.g., using graph embeddings (darwish et al., 2020)."
2020.acl-main.308.txt,2020,5 Conclusion and Future Work,"we have combined a variety of information sources, many of which were not explored for at least one of the target tasks, e.g., youtube channels, political bias of the facebook audience, and information from the profiles of the media followers on twitter."
2020.acl-main.308.txt,2020,5 Conclusion and Future Work,"we have presented experiments in predicting the political ideology, i.e., left/center/right bias, and the factuality of reporting, i.e., high/mixed/low, of news media."
2020.acl-main.309.txt,2020,7 Discussion and Conclusion,a new insight provided by our study is that this difficulty holds even after alleviating the frequency effects by augmenting the target structures along with direct supervision signals.
2020.acl-main.309.txt,2020,7 Discussion and Conclusion,alleviating this restriction is an important future direction.
2020.acl-main.309.txt,2020,7 Discussion and Conclusion,"in terms of language acquisition, the supervision provided in our approach can be seen as direct negative evidence (marcus, 1993)."
2020.acl-main.309.txt,2020,7 Discussion and Conclusion,"object relative clauses are known to be harder for a human as well, and our results may indicate some similarities in the sentence processing behaviors by a human and rnn, though other studies also find some dissimilarities between them (linzen and leonard, 2018; wilcox et al., 2019a)."
2020.acl-main.309.txt,2020,7 Discussion and Conclusion,one limitation of our approach is that the scope of negative examples has to be predetermined and fixed.
2020.acl-main.309.txt,2020,7 Discussion and Conclusion,our results with explicit negative examples are overall positive.
2020.acl-main.309.txt,2020,7 Discussion and Conclusion,since human learners are known to acquire syntax without such direct feedback we do not claim that our proposed learning method itself is cognitively plausible.
2020.acl-main.309.txt,2020,7 Discussion and Conclusion,"the difficulty of object relative clauses for rnn-lms has also been observed in the prior work (marvin and linzen, 2018; van schijndel et al., 2019)."
2020.acl-main.309.txt,2020,7 Discussion and Conclusion,"this indicates that rnns might inherently suffer from some memory limitation like a human subject, for which the difficulty of particular constructions, including center-embedded object relative clauses, are known to be incurred due to memory limitation (gibson, 1998; demberg and keller, 2008) rather than purely frequencies of the phenomena."
2020.acl-main.309.txt,2020,7 Discussion and Conclusion,"though it is challenging, we believe that our final analysis for transferability, which indicates that the negative examples do not have to be complete and can be noisy, suggests a possibility of a mechanism to induce negative examples themselves during training, perhaps relying on other linguistic cues or external knowledge."
2020.acl-main.309.txt,2020,7 Discussion and Conclusion,we found that our new token-level margin loss is superior to the other approaches and the remaining challenging cases are dependencies across an object relative clause.
2020.acl-main.309.txt,2020,7 Discussion and Conclusion,we have demonstrated that models exposed to these examples at training time in an appropriate way will be capable of handling the targeted constructions at near perfect level except a few cases.
2020.acl-main.31.txt,2020,4 Conclusion,experiments proved the effectiveness of our approach in modelling local word-word relations and word significances in the text.
2020.acl-main.31.txt,2020,4 Conclusion,"we proposed a novel graph-based method for inductive text classification, where each text owns its structural graph and text level word interactions can be learned."
2020.acl-main.310.txt,2020,8 Conclusion,"in this paper, we conducted a thorough study to evaluate the robustness of language encoders against grammatical errors."
2020.acl-main.310.txt,2020,8 Conclusion,this study shed light on understanding the behaviors of language encoders against grammatical errors and encouraged future work to enhance the robustness of these models.
2020.acl-main.310.txt,2020,8 Conclusion,we proposed a novel method to simulating grammatical errors and facilitating our evaluations.
2020.acl-main.310.txt,2020,8 Conclusion,"we studied three pre-trained language encoders, elmo, bert, and roberta and concentrated on three aspects of their abilities against grammatical errors: performance on downstream tasks when confronted with noised texts, ability in identifying errors and capturing the interaction between tokens in the presence of errors."
2020.acl-main.311.txt,2020,7 Conclusion,"furthermore, little is known for whether and how we can utilize them for better capturing linguistic properties and eventually improving the performance of downstream tasks for which the embeddings are constructed."
2020.acl-main.311.txt,2020,7 Conclusion,"furthermore, we explored how the hundreds of attention heads underwent performance variation during the fine-tuning process on the downstream tasks, revealing the internal behaviors with the proposed analysis method."
2020.acl-main.311.txt,2020,7 Conclusion,immediate attention should be paid to the investigation of how heat maps would vary during the extensive pre-training so that we have a better understanding of the dynamics of the learning processes.
2020.acl-main.311.txt,2020,7 Conclusion,"it also showed that the amount of both syntactic and semantic information handled by the heads vary from layer to layer, sometimes showing that the last layer contributes much less especially with large models."
2020.acl-main.311.txt,2020,7 Conclusion,one of the major contributions of this paper is to fill the void by inspecting where and how the attention heads are “trained” internally for classification tasks corresponding to different linguistic properties and for the downstream tasks.
2020.acl-main.311.txt,2020,7 Conclusion,the additional performance gains obtained by the simple method show that this approach of using the anatomy of the transformer models and the attention heads is promising in utilizing expensive pre-trained transformer models to their maximal extent.
2020.acl-main.311.txt,2020,7 Conclusion,the analysis of syntacticsemantic score distributions revealed that individual attention heads capture both syntactic and semantic information.
2020.acl-main.311.txt,2020,7 Conclusion,the analysis results clearly show a tendency through the comprehensive heat maps that syntactic and semantic information is mainly handled from the lower layers to the upper layers.
2020.acl-main.311.txt,2020,7 Conclusion,"we also showed that understanding the roles of attention heads in handling task-specific information can help to develop adaptive sentence representations, by selecting influential attention heads and testing them for the three downstream tasks."
2020.acl-main.311.txt,2020,7 Conclusion,"while recent research demonstrated the capability of the transformer-based encoders for generating rich sentence representations, the roles of individual self-attention heads were hardly unknown."
2020.acl-main.311.txt,2020,7 Conclusion,"while the empirical results are strong, additional work remains to further our understanding of the internal workings of the transformer architecture and its role in building such strong language models for a variety of tasks."
2020.acl-main.312.txt,2020,6 Conclusions,"in this work, we focused on understanding the underlying factors that may influence the attention mechanism, and proposed to examine attention scores – a global measurement of significance of word tokens."
2020.acl-main.312.txt,2020,6 Conclusions,"moreover, we can also examine the influence of using deep contextualized input encoders such as elmo (peters et al., 2018) or bert (devlin et al., 2018)."
2020.acl-main.312.txt,2020,6 Conclusions,"our analysis also revealed factors that may impact the interpretability of the attention mechanism, providing understandings on why the model may still be robust even under scenarios where the attention scores appear to be less interpretable."
2020.acl-main.312.txt,2020,6 Conclusions,"specifically, we can further examine the influence of using pre-trained word embeddings –whether similar words can help each other boost their polarity and attention scores."
2020.acl-main.312.txt,2020,6 Conclusions,the empirical results of experiments on various real datasets supported our analysis.
2020.acl-main.312.txt,2020,6 Conclusions,there are some future directions that are worth exploring.
2020.acl-main.312.txt,2020,6 Conclusions,through the analysis we found that both quantities play important roles in the learning and prediction process and examining both of them in an integrated manner allows us to better understand the underlying workings of an attention based model.
2020.acl-main.312.txt,2020,6 Conclusions,"we also extended to and empirically examined additive attention, multi-label classification and models with an affine input layer, and observed similar behaviors."
2020.acl-main.312.txt,2020,6 Conclusions,"we focused on binary classification models with dot-product attention, and analyzed through a gradient descent based learning framework the behavior of attention scores and polarity scores – another quantity that we defined and proposed to examine."
2020.acl-main.313.txt,2020,5 Conclusion,experimental results show that our proposed r-men obtains the new state-of-the-art performances for both the triple classification and search personalization tasks.
2020.acl-main.313.txt,2020,5 Conclusion,"in future work, we plan to extend r-men for multi-hop knowledge graph reasoning."
2020.acl-main.313.txt,2020,5 Conclusion,our code is available at: https://github.com/daiquocnguyen/ r-men.
2020.acl-main.313.txt,2020,5 Conclusion,"we propose a new kg embedding model, named r-men, where we integrate transformer self-attention mechanism-based memory interactions with a cnn decoder to capture the potential dependencies in the kg triples effectively."
2020.acl-main.314.txt,2020,5 Conclusion,experiments on various datasets show the effectiveness and efficiency of mc-tailor.
2020.acl-main.314.txt,2020,5 Conclusion,"in this paper, we propose mc-tailor to alleviate the over- and under-estimation problem between true and model distributions."
2020.acl-main.314.txt,2020,5 Conclusion,"mc-tailor is composed of a ratio estimator, which adjusts the probabilities of mle fine-tuned plms to approximate true distributions, and the ers to accelerate sampling while ensuring sample quality."
2020.acl-main.315.txt,2020,4 Conclusion,"furthermore, the word-aligned attention can also be applied to english plms to bridge the semantic gap between the whole word and the segmented word-piece tokens, which we leave for future work."
2020.acl-main.315.txt,2020,4 Conclusion,"in this paper, we develop a novel multi-source word aligned attention model (referred as mwa), which integrates word segmentation information into character-level self-attention mechanism to enhance the fine-tuning performance of chinese plms."
2020.acl-main.315.txt,2020,4 Conclusion,"the proposed approach yields substantial improvements compared to bert, bert-wwm and ernie, demonstrating its effectiveness and universality."
2020.acl-main.315.txt,2020,4 Conclusion,we conduct extensive experiments on five nlp tasks with six public datasets.
2020.acl-main.316.txt,2020,7 Conclusions,"experiments on benchmark datasets, i.e., ptb, yelp, and yahoo, show that our approach improves various vae models in terms of probability estimation and the richness of the latent space."
2020.acl-main.316.txt,2020,7 Conclusions,"in this paper, we observe the encode-decoder incompatibility of vae for text modeling."
2020.acl-main.316.txt,2020,7 Conclusions,our approach is model-agnostic and can be applied to a wide range of models in the vae family.
2020.acl-main.316.txt,2020,7 Conclusions,results on switchboard show that coupled-cvae largely improves diversity in dialogue generation.
2020.acl-main.316.txt,2020,7 Conclusions,we also generalize coupled-vae to conditional language modeling and propose coupled-cvae.
2020.acl-main.316.txt,2020,7 Conclusions,"we bridge the incompatibility and the posterior collapse problem by viewing the encoder and the decoder as two inadequately learned chart maps from the data manifold to the parameterizations, and the posterior network as a part of the transition map between them."
2020.acl-main.316.txt,2020,7 Conclusions,we couple the vae model with a deterministic network and improve the parameterizations via encoder weight sharing and decoder signal matching.
2020.acl-main.317.txt,2020,5 Conclusion,"an naı¨ve way is to add the “oov” token into the synonyms set of every word, but potentially better procedures can be further explored."
2020.acl-main.317.txt,2020,5 Conclusion,"compared with previous work such as jia et al.(2019); huang et al.(2019), our method is structure-free and thus can be easily applied to any pre-trained models (such as bert) and character-level models (such as char-cnn)."
2020.acl-main.317.txt,2020,5 Conclusion,"in further work, we will explore more efficient ways for constructing the perturbation set."
2020.acl-main.317.txt,2020,5 Conclusion,"in this paper, we used a heuristic way based on the synonym network to construct the perturbation set, which may not be optimal."
2020.acl-main.317.txt,2020,5 Conclusion,the construction of the perturbation set is of critical importance to our method.
2020.acl-main.317.txt,2020,5 Conclusion,"we also plan to generalize our approach to achieve certified robustness against other types of adversarial attacks in nlp, such as the out-of-list attack."
2020.acl-main.317.txt,2020,5 Conclusion,"we proposed a robustness certification method, which provably guarantees that all the possible perturbations cannot break down the system."
2020.acl-main.318.txt,2020,7 Conclusion,"in the future, we will consider combining our method with graph neural networks to update the word graphs we build."
2020.acl-main.318.txt,2020,7 Conclusion,"in this paper, we propose a novel graph-based coarse-to-fine paradigm for unsupervised bilingual lexicon induction."
2020.acl-main.318.txt,2020,7 Conclusion,our method uses clique-level information and reduces the bad effect of noise in the pre-trained embeddings.
2020.acl-main.318.txt,2020,7 Conclusion,"the experiments show that our method can significantly improve the bilingual word induction performance after several iterations compared with strong baselines, even for distant language pairs."
2020.acl-main.319.txt,2020,7 Conclusion,"as a result, the mass production of adversarial examples for the victim model’s analysis and further improvement of robustness become convenient."
2020.acl-main.319.txt,2020,7 Conclusion,experiments show that our method achieves stable degradation with meaning preserving adversarial examples over different victim models.
2020.acl-main.319.txt,2020,7 Conclusion,"furthermore, we notice some exceptional cases which we call as “reinforced samples”, which we leave as the future work."
2020.acl-main.319.txt,2020,7 Conclusion,it is noticeable that our method can generate adversarial examples efficiently from monolingual data.
2020.acl-main.319.txt,2020,7 Conclusion,"we propose a new paradigm to generate adversarial examples for neural machine translation, which is capable of exposing translation pitfalls without handcrafted error features."
2020.acl-main.32.txt,2020,5 Conclusion,bat models topics with the dirichlet prior and builds a two-way transformation between document-topic distribution and document-word distribution via bidirectional adversarial training.
2020.acl-main.32.txt,2020,5 Conclusion,"besides, developing correlated topic modelsis another promising direction."
2020.acl-main.32.txt,2020,5 Conclusion,"gaussian-bat extends from bat by incorporating word embeddings into the modeling process, thereby naturally considers the word relatedness information captured in word embeddings."
2020.acl-main.32.txt,2020,5 Conclusion,"in the future, we would like to devise a nonparametric neural topic model based on adversarial training."
2020.acl-main.32.txt,2020,5 Conclusion,"in this paper, we have explored the use of bidirectional adversarial training in neural topic models and proposed two novel approaches: the bidirectional adversarial topic (bat) model and the bidirectional adversarial topic model with gaussian (gaussian-bat)."
2020.acl-main.32.txt,2020,5 Conclusion,the experimental comparison on three widely used benchmark text corpus with the existing neural topic models shows that bat and gaussian-bat achieve improved topic coherence results.
2020.acl-main.320.txt,2020,5 Conclusion,"in this paper, we propose a novel method for unsupervised machine translation with a retrieve-andrewrite schema."
2020.acl-main.320.txt,2020,5 Conclusion,we first retrieve similar sentences from monolingual corpora and then rewrite the targets with a rewriting model.
2020.acl-main.320.txt,2020,5 Conclusion,"with the pseudo parallel data, we better initialize pbsmt models and significantly improve the final iteration performance as the experiments show."
2020.acl-main.321.txt,2020,5 Conclusions,"besides, our flat-transformer is compatible with the pretraining model, yielding a better performance than both the existing uni-encoder models and the dualencoder models on two datasets."
2020.acl-main.321.txt,2020,5 Conclusions,"in this work, we explore the solutions to improve the uni-encoder structures for document-level machine translation."
2020.acl-main.321.txt,2020,5 Conclusions,"we propose a flat-transformer model with a unified encoder, which is simple and can model the bi-directional relationship between the contexts and the source sentences."
2020.acl-main.322.txt,2020,6 Conclusions,another observation is that the nmt models can even achieve better translation quality without the context encoder.
2020.acl-main.322.txt,2020,6 Conclusions,"even though we feed the incorrect context into training, the nmt system can still obtain substantial bleu improvements on several small datasets."
2020.acl-main.322.txt,2020,6 Conclusions,experiments on large-scale training data demonstrate the effectiveness of this method.
2020.acl-main.322.txt,2020,6 Conclusions,"motivated by this, we significantly improve the sentence-level systems with a gaussian noise imposed on the encoder output."
2020.acl-main.322.txt,2020,6 Conclusions,"this gives us an interesting finding that the context-encoder acts more like a noise generator, which provides rich supervised training signals for robust training."
2020.acl-main.322.txt,2020,6 Conclusions,"we have shown that, in multi-encoder context-aware nmt, the bleu improvement is not attributed to the leverage of contextual information."
2020.acl-main.323.txt,2020,5 Conclusion,"in this paper, we analyze the effects of accumulated batches on the gradient direction, and propose to achieve efficient automated batch sizes by monitoring change in gradient accumulation and performing an optimization step when the accumulated gradient direction is almost stable."
2020.acl-main.323.txt,2020,5 Conclusion,our approach improves the transformer with a fixed 25k batch size by +0.73 and +0.82 bleu on the wmt 14 english to german and english to french tasks respectively while preserving efficiency.
2020.acl-main.323.txt,2020,5 Conclusion,"to improve the efficiency of our approach with large models, we propose a sampling approach to select gradients of parameters sensitive to the batch size."
2020.acl-main.324.txt,2020,8 Conclusion and Future Work,"in the future, we intend to extend the work to include language types such as asian languages."
2020.acl-main.324.txt,2020,8 Conclusion and Future Work,"in this paper, we have introduced a unified framework, using a single encoder and decoder, for munmt training on a large scale of european languages."
2020.acl-main.324.txt,2020,8 Conclusion and Future Work,our extensive experiments and analysis demonstrate the effectiveness of our proposed methods.
2020.acl-main.324.txt,2020,8 Conclusion and Future Work,"to further enhance munmt performance, we have proposed two knowledge distillation methods."
2020.acl-main.324.txt,2020,8 Conclusion and Future Work,we will also introduce other effective methods to improve zero-shot translation quality.
2020.acl-main.325.txt,2020,6 Conclusion,our constraint insertion step is simple and we have empirically validated its effectiveness.
2020.acl-main.325.txt,2020,6 Conclusion,"the approach demonstrated control over constraint terms in target translations while being able to decode as fast as a baseline levenshtein transformer model, which achieves significantly higher decoding speed than traditional beam search.6 in addition to the terminological lexical constraints discussed in this work, future work can potentially modify insertion or selection operations to handle target translations of multiple forms; this can potentially disambiguate the morphological variants of the lexical constraints."
2020.acl-main.325.txt,2020,6 Conclusion,we proposed a non-autoregressive decoding approach to integrate lexical constraints for nmt.
2020.acl-main.326.txt,2020,5 Conclusions,"furthermore, we believe that a better understanding of the links between exposure bias and well-known translation problems will help practitioners decide when sequence-level training objectives are especially promising, for example in settings where the test domain is unknown, or where hallucinations are a common problem."
2020.acl-main.326.txt,2020,5 Conclusions,"our findings are pertinent to the academic debate how big of a problem exposure bias is in practice – we find that this can vary substantially depending on the dataset –, and they provide a new justification for sequence-level training objectives that reduce or eliminate exposure bias."
2020.acl-main.326.txt,2020,5 Conclusions,"our results and analysis show a connection between the exposure bias due to mle training with teacher forcing and several well-known problems in neural machine translation, namely poor performance under domain shift, hallucinated translations, and deteriorating performance with increasing beam size."
2020.acl-main.326.txt,2020,5 Conclusions,"we find that minimum risk training, which does not suffer from exposure bias, can be useful even when it does not increase performance on an in-domain test set: it increases performance under domain shift, reduces the number of hallucinations substantially, and makes beam search with large beams more stable."
2020.acl-main.327.txt,2020,6 Conclusion,"in future work, we will work around the problem of evaluation errors in the low da range."
2020.acl-main.327.txt,2020,6 Conclusion,"in this paper, we proposed an mte framework that utilizes source sentences using xlm."
2020.acl-main.327.txt,2020,6 Conclusion,we also investigated why our proposed method worked poorly in the other conditions and found the importance of tlm training.
2020.acl-main.327.txt,2020,6 Conclusion,we show that the proposed method with tlm-trained xlm showed a higher correlation with human judgments than the baseline method in the small corpus condition and stabilize the evaluation performance regardless of the quality of translation sentences by using additional source sentences.
2020.acl-main.328.txt,2020,9 Conclusions,a practical line of future work is embedding our plotting agent in interactive environments such as jupyter lab.
2020.acl-main.328.txt,2020,9 Conclusions,future work includes methods that get closer to human performance on the dataset.
2020.acl-main.328.txt,2020,9 Conclusions,"in this paper, we defined the problem of conversational plotting agents, which is of great practical importance considering the large volume of questions online about plotting library usage."
2020.acl-main.328.txt,2020,9 Conclusions,"our experiments have demonstrated the feasibility of seq2seq-based methods to produce working models for dataset; however, there is still a large gap between our best performing methods and human performance."
2020.acl-main.328.txt,2020,9 Conclusions,"we also presented a dataset, chartdialogs, to facilitate the development of such agents."
2020.acl-main.329.txt,2020,6 Conclusion,"all the datasets used in the gluecos benchmark are publicly available, and we plan to make the nli dataset available for research use."
2020.acl-main.329.txt,2020,6 Conclusion,"in future work, we would like to experiment with a multi-task setup wherein tasks with less training data can significantly benefit from those having abundant labelled data, since most code-switched datasets are often small and difficult to annotate."
2020.acl-main.329.txt,2020,6 Conclusion,"in this paper, we introduce the first evaluation benchmark for code-switching, gluecos."
2020.acl-main.329.txt,2020,6 Conclusion,"in this work, we use standard architectures to solve each nlp task individually and vary the embeddings used."
2020.acl-main.329.txt,2020,6 Conclusion,"the benchmark contains datasets in english-hindi and english-spanish for six nlp tasks - lid, pos tagging, ner, sentiment analysis, question answering and a new code-switched natural language inference task."
2020.acl-main.329.txt,2020,6 Conclusion,"this indicates that while multilingual models do go a long way in solving code-switched nlp, they can be improved further by using real and synthetic code-switched data, since the distributions in code-switched languages differ from the two languages being mixed."
2020.acl-main.329.txt,2020,6 Conclusion,"we also find that for most datasets, a modified version of mbert that has been fine-tuned on synthetically generated code-switched data with a small amount of real code-switched data performs best."
2020.acl-main.329.txt,2020,6 Conclusion,"we experiment with datasets having varied amounts of code-switching and from different domains and show that some tasks, such as lid and pos tagging are relatively easier to solve, while tasks such as qa and nli have low accuracies."
2020.acl-main.329.txt,2020,6 Conclusion,"we hope that this will encourage researchers to test multilingual, cross-lingual and code-switched embedding techniques and models on this benchmark."
2020.acl-main.329.txt,2020,6 Conclusion,we test various embedding techniques across all tasks and datasets and find that multilingual bert outperforms cross-lingual embedding techniques on all tasks.
2020.acl-main.329.txt,2020,6 Conclusion,we would like to add more diverse tasks and language pairs to the gluecos benchmark in a future version.
2020.acl-main.33.txt,2020,5 Conclusion,"comprehensive evaluation empirically confirmed that our model consistently outperformed bert and han models on single- and multi-label classifications, sentence and document classifications, and classifications in different languages."
2020.acl-main.33.txt,2020,5 Conclusion,"in future work, we will examine semantic relations between class labels in the auxiliary task."
2020.acl-main.33.txt,2020,5 Conclusion,"in this paper, we proposed a simple multitask learning model that uses negative supervision to generate distinct representations for texts with different labels."
2020.acl-main.33.txt,2020,5 Conclusion,"moreover, we will adapt our model to text generation tasks."
2020.acl-main.33.txt,2020,5 Conclusion,our multitask learning model provides a general framework that is easily applicable to existing text classification models.
2020.acl-main.33.txt,2020,5 Conclusion,"we expect that our model will encourage a generation model to generate texts with different labels, such as styles, have distinct representations, which will result in class specific expressions."
2020.acl-main.330.txt,2020,7 Conclusion,our extensive experiments reveal the potential of the proposed dataset for accelerating the innovations in the three tasks and multi-task learning.
2020.acl-main.330.txt,2020,7 Conclusion,"to conclude, in this paper, we present matinf, a jointly labeled large-scale dataset for classification, question answering and summarization."
2020.acl-main.330.txt,2020,7 Conclusion,we benchmark existing methods and a straightforward baseline with a novel multi-task paradigm on mat-inf and analyze their performance on these three tasks.
2020.acl-main.331.txt,2020,6 Conclusion and Discussion,"besides news recommendation, the mind dataset can also be used in other natural language processing tasks such as topic classification, text summarization and news headline generation."
2020.acl-main.331.txt,2020,6 Conclusion and Discussion,"in addition, besides the click behaviors, we plan to incorporate other user behaviors such as read and engagement to support more accurate user modeling and performance evaluation."
2020.acl-main.331.txt,2020,6 Conclusion and Discussion,"in the future, we plan to extend the mind dataset by incorporating image and video information in news as well as news in different languages, which can support the research of multi-modal and multi-lingual news recommendation."
2020.acl-main.331.txt,2020,6 Conclusion and Discussion,"in this paper we present the mind dataset for news recommendation research, which is constructed from user behavior logs of microsoft news."
2020.acl-main.331.txt,2020,6 Conclusion and Discussion,"it contains 1 million users and more than 160k english news articles with rich textual content such as title, abstract and body."
2020.acl-main.331.txt,2020,6 Conclusion and Discussion,"many interesting researches can be conducted on the mind dataset, such as designing better news and user modeling methods, improving the diversity, fairness and explainability of news recommendation results, and exploring privacy-preserving news recommendation."
2020.acl-main.331.txt,2020,6 Conclusion and Discussion,"many natural language processing and machine learning techniques such as text modeling, attention mechanism and pre-trained language models can contribute to the performance improvement of news recommendation."
2020.acl-main.331.txt,2020,6 Conclusion and Discussion,the results show the importance of accurate news content understanding and user interest modeling for news recommendation.
2020.acl-main.331.txt,2020,6 Conclusion and Discussion,we conduct extensive experiments on this dataset.
2020.acl-main.332.txt,2020,8 Conclusions and Future Work,"finally, we have presented learning-to-rank experiments, demonstrating sizable improvements over state-of-the-art retrieval and textual similarity approaches."
2020.acl-main.332.txt,2020,8 Conclusions and Future Work,"in future work, we plan to extend this work to more datasets and to more languages."
2020.acl-main.332.txt,2020,8 Conclusions and Future Work,"we further want to go beyond textual claims, and to take claimimage and claim-video pairs as an input."
2020.acl-main.332.txt,2020,8 Conclusions and Future Work,"we have argued for the need to address detecting previously fact-checked claims as a task of its own right, which could be an integral part of automatic fact-checking, or a tool to help human fact-checkers or journalists."
2020.acl-main.332.txt,2020,8 Conclusions and Future Work,"we have created specialized datasets, which we have released, together with our code, to the research community in order to enable further research."
2020.acl-main.333.txt,2020,6 Conclusion,"for response diversity and logical self-consistency, we propose to measure these two aspects under augmented utterances with controlled paraphrasing."
2020.acl-main.333.txt,2020,6 Conclusion,"in contrast to prior art, our means of evaluation captures not only the quality of generation, but also the diversity and logical consistency of responses."
2020.acl-main.333.txt,2020,6 Conclusion,it is our hope the proposed holistic metrics may pave the way towards the comparability of open-domain dialogue models.
2020.acl-main.333.txt,2020,6 Conclusion,"moreover, we utilize n-gram based entropy to capture response diversity and entailment based approach to measure logical self-consistency."
2020.acl-main.333.txt,2020,6 Conclusion,the proposed metrics show a strong correlation with human judgments.
2020.acl-main.333.txt,2020,6 Conclusion,this paper provides a holistic and automatic evaluation method for open-domain dialogue models.
2020.acl-main.333.txt,2020,6 Conclusion,we leverage two effective approaches to generate augmented utterances: word substitution and text generator with k-best decoder.
2020.acl-main.333.txt,2020,6 Conclusion,we recruit gpt-2 as a strong language model to evaluate the context coherency and response fluency.
2020.acl-main.334.txt,2020,5 Conclusion and Future Work,experimental results show that birre outperforms state-of-the-arts over various benchmark datasets.
2020.acl-main.334.txt,2020,5 Conclusion and Future Work,"future work includes i) improving projection learning to model complicated linguistic properties of hypernymy; ii) extending our model to address other tasks, such as graded lexical entailment (vulic et al., 2017) and cross-lingual graded lexical entailment (vulic et al., 2019); and iii) exploring how deep neural language models (such as bert (devlin et al., 2019), transformer-xl (dai et al., 2019), xlnet (yang et al., 2019)) can improve the performance of hypernymy detection."
2020.acl-main.334.txt,2020,5 Conclusion and Future Work,"in this paper, we present the birre model for supervised hypernymy detection."
2020.acl-main.334.txt,2020,5 Conclusion and Future Work,it employs two projection-based hypernym and hyponym generation modules based on word embeddings to learn birre vectors for hypernymy classification.
2020.acl-main.335.txt,2020,7 Conclusion,"although the datasets used in our experiments are in english, we expect that our methodology would work in any language as long as there is a synonym dictionary for the language."
2020.acl-main.335.txt,2020,7 Conclusion,"for future work, an extrinsic evaluation of our methods is needed to prove the effectiveness of learned biomedical entity representations and to prove the quality of the entity normalization in downstream tasks."
2020.acl-main.335.txt,2020,7 Conclusion,"in this study, we introduce biosyn that utilizes the synonym marginalization technique and the iterative candidate retrieval for learning biomedical entity representations."
2020.acl-main.335.txt,2020,7 Conclusion,"on four biomedical entity normalization datasets, our experiment shows that our model achieves state-of-the-art performance on all datasets, improving previous scores up to 2.6%."
2020.acl-main.336.txt,2020,5 Conclusion,"in this paper, we investigate three joint training paradigms for detecting hypernymy in low-resource languages."
2020.acl-main.336.txt,2020,5 Conclusion,our study demonstrates the feasibility and effectiveness to combine high- and low-resource data to jointly train hypernymy detection models.
2020.acl-main.336.txt,2020,5 Conclusion,transferring lexical knowledge across languages are important especially for low-resource cases.
2020.acl-main.336.txt,2020,5 Conclusion,we show that simple multilingual training is not helpful for all tasks and we significantly improve the performance using meta learning.
2020.acl-main.337.txt,2020,6 Conclusion and Future Work,"furthermore, gong et al.(2018) reported that word embeddings learned in several tasks are biased towards word frequency: the embeddings of high-frequency and low-frequency words lie in different subregions of the embedding space."
2020.acl-main.337.txt,2020,6 Conclusion and Future Work,"furthermore, we investigated the relationship between the score calculated by each model and the degree of membership and demonstrated that, while discriminative learning-based models can distinguish a possible member of a word class from others, the offset-based model achieves higher correlations with the degree of membership."
2020.acl-main.337.txt,2020,6 Conclusion and Future Work,"however, several studies have expanded the method for acquiring a word vector to account for the uncertainty of word meanings and word polysemy (e.g., athiwaratkun et al.2018)."
2020.acl-main.337.txt,2020,6 Conclusion and Future Work,"however, the impact is limited, and the negative instances must be taken into account for adequate modeling."
2020.acl-main.337.txt,2020,6 Conclusion and Future Work,"in addition, contextualized word embeddings have been shown to be very effective on a range of nlp tasks (peters et al., 2018; devlin et al., 2019)."
2020.acl-main.337.txt,2020,6 Conclusion and Future Work,the experimental results show that a centroid-based approach cannot provide a reasonably good model and considering the geometry of the distribution and the existence of subgroups is useful for modeling the distribution in some cases.
2020.acl-main.337.txt,2020,6 Conclusion and Future Work,the investigation in this study leveraged only general-purpose word vectors to represent the meaning of a word.
2020.acl-main.337.txt,2020,6 Conclusion and Future Work,the results indicate that just observing the distribution of positive instances is not enough to understand the geometry of word embedding spaces.
2020.acl-main.337.txt,2020,6 Conclusion and Future Work,"thus, in the future, we will take the uncertainty, polysemy, and context sensitivity of the word meanings and the frequency of words into account and explore better ways of modeling the word-class distributions in semantic vector spaces."
2020.acl-main.337.txt,2020,6 Conclusion and Future Work,we investigated the distribution of words that belong to a certain word class in a pre-trained generalpurpose word vector space.
2020.acl-main.338.txt,2020,6 Conclusion,"experimental results on four datasets from semeval-2015 and 2016 demonstrate that our approach signiﬁcantly outperforms a number of competitive baselines, including all the three best-performed systems in the shared tasks of both semeval-2015 and 2016."
2020.acl-main.338.txt,2020,6 Conclusion,"in our future work, we would like to improve the performance of the asc task by using unlabeled data since our graph-based neural network approach is easy to add unlabeled data."
2020.acl-main.338.txt,2020,6 Conclusion,"in this paper, we propose a novel cooperative graph attention networks (cogan) approach to aspect sentiment classiﬁcation (asc)."
2020.acl-main.338.txt,2020,6 Conclusion,"moreover, we would like to apply our approach to other sentiment analysis tasks, e.g., aspect-oriented opinion summarization and multi-label emotion detection."
2020.acl-main.338.txt,2020,6 Conclusion,"the main idea of the proposed approach is to incorporate two kinds of sentiment preference information (i.e., the intra-aspect consistency and inter-aspect tendency) in a document for remedying the information deﬁciency problem in asc."
2020.acl-main.339.txt,2020,6 Conclusion,extracting these aspects suffers more severely from boundary errors.
2020.acl-main.339.txt,2020,6 Conclusion,"in the future, we will develop a syntax-based multi-scale graph convolutional network to deal with both short and long aspects."
2020.acl-main.339.txt,2020,6 Conclusion,our experimental results demonstrate that boundary repositioning can be used as a simple and robust post-processing method to improve aspect extraction.
2020.acl-main.339.txt,2020,6 Conclusion,our findings reveal that illustrative aspects in scientific literature are generally long-winded.
2020.acl-main.34.txt,2020,6 Conclusion and Future Works,"in future work, we will investigate the impact of fine-grained word categories (such as nouns, verbs, and adjectives) on the translation performance and design specific methods according to these categories."
2020.acl-main.34.txt,2020,6 Conclusion and Future Works,"our proposed nmt models, that are easy to implement and not much time and space cost, are introduced to the training and inference, and can improve the representation and translation of content words."
2020.acl-main.34.txt,2020,6 Conclusion and Future Works,this paper explored the importance of word for nmt.
2020.acl-main.34.txt,2020,6 Conclusion and Future Works,we divided words of one sentence into content and function words through word frequency-related information.
2020.acl-main.340.txt,2020,6 Conclusion,experiments on three realworld datasets demonstrate that our racl framework with its two implementations outperforms the state-of-the-art pipeline and unified baselines for the complete absa task.
2020.acl-main.340.txt,2020,6 Conclusion,"in order to exploit these relations, we propose a relation-aware collaborative learning (racl) framework with multi-task learning and relation propagation techniques."
2020.acl-main.340.txt,2020,6 Conclusion,"in this paper, we highlight the importance of interactive relations in the complete absa task."
2020.acl-main.341.txt,2020,6 Conclusion,experiments show the effectiveness and transferability of sentibert.
2020.acl-main.341.txt,2020,6 Conclusion,"for future work, we will extend sentibert to other applications involving phrase-level annotations."
2020.acl-main.341.txt,2020,6 Conclusion,further analysis demonstrates its interpretability and potential with less supervision.
2020.acl-main.341.txt,2020,6 Conclusion,sentibert considers the necessity of contextual information and explicit syntactic guidelines for modeling semantic composition.
2020.acl-main.341.txt,2020,6 Conclusion,"we proposed sentibert, an architecture designed for capturing better compositional sentiment semantics."
2020.acl-main.342.txt,2020,7 Conclusion,"besides, graph neural network-based (kipf and welling, 2016) methods are also worth investigating to model the relations among nodes for this task."
2020.acl-main.342.txt,2020,7 Conclusion,experimental results on a standard benchmark demonstrate the superiority and robustness of the proposed model compared to a number of competitive methods.
2020.acl-main.342.txt,2020,7 Conclusion,"in the future, one possible direction is creating complete graphs with their nodes being input clauses to achieve full coverage."
2020.acl-main.342.txt,2020,7 Conclusion,"in this paper, we present a novel transition-based framework to extract emotion-cause pairs as a procedure of directed graph construction."
2020.acl-main.342.txt,2020,7 Conclusion,"instead of previous pipelined approaches, the proposed framework incrementally outputs the emotion-cause pairs as a single task, thereby the interdependence between emotions and causes can be exploited more effectively."
2020.acl-main.343.txt,2020,7 Conclusion,"furthermore, we conduct extensive experiments on discussing unimodal, multimodal, and multi-task learning."
2020.acl-main.343.txt,2020,7 Conclusion,"in the future, we will further explore the connection between multimodal analysis and multi-task learning and incorporate more fusion strategy, including early- and middle-fusion."
2020.acl-main.343.txt,2020,7 Conclusion,"in this paper, we propose a novel chinese multi-modal sentiment analysis dataset with independent unimodal annotations and a multimodal multi-task learning framework based on late-fusion methods."
2020.acl-main.343.txt,2020,7 Conclusion,"lastly, we summarize our overall findings as follows: • multimodal labels cannot reflect unimodal sentimental states always."
2020.acl-main.343.txt,2020,7 Conclusion,"the unified multimodal annotations may mislead the model to learn inherent characteristics of unimodal representations.• with the help of unimodal annotations, models can learn more differentiated information and improve the complementarity between modalities.• when performing multi-task learning, the asynchrony of learning in different subtasks may cause an adverse effect on multimodal sentiment analysis."
2020.acl-main.343.txt,2020,7 Conclusion,we hope that the introduction of ch-sims will provide a new perspective for researches on multi-modal analysis.
2020.acl-main.344.txt,2020,5 Conclusion and Future Work,"besides, we expect the idea of curriculum pre-training can be adopted on other nlp tasks."
2020.acl-main.344.txt,2020,5 Conclusion and Future Work,empirical studies have demonstrated that our model significantly outperforms baselines.
2020.acl-main.344.txt,2020,5 Conclusion and Future Work,"in the future, we will explore how to leverage unlabeled speech data and large bilingual text data to further improve the performance."
2020.acl-main.344.txt,2020,5 Conclusion and Future Work,"this paper investigates the end-to-end method for st. we propose a curriculum pre-training method, consisting of an elementary course with an as-r loss, and two advanced courses with a frame-based masked language model loss and a bilingual lexicon translation loss, in order to teach the model syntactic and semantic knowledge in the pre-training stage."
2020.acl-main.345.txt,2020,8 Summary,"the insights we gleaned from this investigation provide hints on how we could potentially adapt such end-to-end asr models, using auxiliary losses, to be robust to variations across accents."
2020.acl-main.345.txt,2020,8 Summary,this work presents a thorough analysis of how accent information manifests within an end-to-end asr system.
2020.acl-main.345.txt,2020,8 Summary,we will investigate this direction in future work.
2020.acl-main.346.txt,2020,6 Conclusion,"a qualitative analysis of the results also indicated that self-training is helpful for detecting complicated types of disfluencies, including corrections and restarts."
2020.acl-main.346.txt,2020,6 Conclusion,"in future work, we intend to explore the idea of self-training for parsing written texts."
2020.acl-main.346.txt,2020,6 Conclusion,"the first step is to develop parsing models that parse asr output, rather than speech transcripts."
2020.acl-main.346.txt,2020,6 Conclusion,we also aim at integrating syntactic parsing and self-training more closely with automatic speech recognition.
2020.acl-main.346.txt,2020,6 Conclusion,we introduced a new state-of-the-art for joint disfluency detection and constituency parsing of transcribed speech.
2020.acl-main.346.txt,2020,6 Conclusion,we showed that self-training and ensembling are effective methods for improving disfluency detection.
2020.acl-main.347.txt,2020,4 Conclusion,"in this paper, we propose a spoken language representation learning framework that learns contextualized representation of lattices."
2020.acl-main.347.txt,2020,4 Conclusion,"the experiments show that our proposed framework is capable of providing high-quality representations of lattices, yielding consistent improvement on slu tasks."
2020.acl-main.347.txt,2020,4 Conclusion,we introduce the lattice language modeling objective and a two-stage pre-training method that efficiently trains a neural lattice language model to provide the downstream tasks with contextualized lattice representations.
2020.acl-main.348.txt,2020,6 Conclusion,"based on experimental results, our training strategy outperforms joint training even without adding a fine-tuning step, and it requires less iterations to converge."
2020.acl-main.348.txt,2020,6 Conclusion,"finally, we will explore further the generability of our meta-transfer learning approach to more downstream multilingual tasks in our future work."
2020.acl-main.348.txt,2020,6 Conclusion,"in this paper, we have shown that our approach can be effectively applied to both speech processing and language modeling tasks."
2020.acl-main.348.txt,2020,6 Conclusion,our model recognizes individual languages and transfers them so as to better recognize mixed-language speech by conditioning the optimization objective to the code-switching domain.
2020.acl-main.348.txt,2020,6 Conclusion,"we propose a novel method, meta-transfer learning, to transfer learn on a code-switched speech recognition system in a low-resource setting by judiciously extracting information from high-resource monolingual datasets."
2020.acl-main.349.txt,2020,5 Conclusion,experimental results on a public dataset show that our model achieves the state-of-the-art performance compared with the existing models.
2020.acl-main.349.txt,2020,5 Conclusion,"in this paper, we identify the essential research issue in multimodal sarcasm detection."
2020.acl-main.349.txt,2020,5 Conclusion,our model is capable of reasoning multimodal sarcastic tweets with word-level interpretation.
2020.acl-main.349.txt,2020,5 Conclusion,"to model the cross-modality contrast in the associated context of multimodal sarcastic tweets, we propose the d&r net to represent the commonality and discrepancy between image and text and multi-view semantic associations in cross-modality context."
2020.acl-main.35.txt,2020,6 Conclusions,"experiments reveal that pd, ngrad, and attn are all good explanation methods that are able to construct the nmt model’s predictions with relatively low perplexity and pd shows the best fidelity among them."
2020.acl-main.35.txt,2020,6 Conclusions,it has presented a principled metric based on fidelity in regard to the predictive behavior of the nmt model.
2020.acl-main.35.txt,2020,6 Conclusions,"on six standard translation tasks, the metric quantitatively evaluates and compares four different explanation methods for two popular translation models."
2020.acl-main.35.txt,2020,6 Conclusions,"since it is intractable to exactly calculate the principled metric for a given explanation method, it thereby proposes an approximate approach to address the minimization problem."
2020.acl-main.35.txt,2020,6 Conclusions,the proposed approach does not rely on human annotation and can be used to evaluate explanation methods on all target words.
2020.acl-main.35.txt,2020,6 Conclusions,this paper has made an initial attempt to evaluate explanation methods from a new viewpoint.
2020.acl-main.350.txt,2020,7 Conclusion,experiments on must-c spoken language translation datasets demonstrate the advantages of simul-speech in terms of both translation accuracy and delay.
2020.acl-main.350.txt,2020,7 Conclusion,"for future work, we will design more flexible policies to achieve better translation quality and lower delay in simultaneous spoken language translation."
2020.acl-main.350.txt,2020,7 Conclusion,"in this work, we developed simulspeech, an end-to-end simultaneous speech to text translation system that directly translates source speech into target text concurrently."
2020.acl-main.350.txt,2020,7 Conclusion,"simulspeech consists of a speech encoder, a speech segmenter, and a text decoder with wait-k strategy for simultaneous translation."
2020.acl-main.350.txt,2020,7 Conclusion,we further introduced several techniques including data-level and attention-level knowledge distillation to boost the accuracy of simulspeech.
2020.acl-main.350.txt,2020,7 Conclusion,we will also investigate simultaneous translation from the speech in a source language to the speech in a target.
2020.acl-main.351.txt,2020,7 Future work,the results still vary and are worse compared to using human annotations.
2020.acl-main.351.txt,2020,7 Future work,"we believe our approach can benefit from some straightforward modifications to the architecture, such as using convolutional neural networks which have shown to perform better at handling timecontinuous data like speech."
2020.acl-main.351.txt,2020,7 Future work,"we plan to do a detailed analysis along two lines: 1) comparing if the proposed modeling technique can help bridge gap between predicted and human annotations, and 2) effect of environment variables e.g., background noise, speaker features, different languages etc."
2020.acl-main.352.txt,2020,5 Conclusion,experimental results verify the effectiveness of the model.
2020.acl-main.352.txt,2020,5 Conclusion,"furthermore, visualisation of the topics and attention signals shows that ntom captures the dynamics in the focused topics and contextual attention."
2020.acl-main.352.txt,2020,5 Conclusion,"in this paper, we propose a novel neural temporal opinion model (ntom) to address users’ changing interest and dynamic social context."
2020.acl-main.352.txt,2020,5 Conclusion,we model users’ tweet posting behaviour based on a temporal point process for the joint prediction of the posting time and stance label of the next tweet.
2020.acl-main.353.txt,2020,7 Conclusion,an exciting synthesis would incorporate deception and language generation into an agent’s policy; our data would help train such agents.
2020.acl-main.353.txt,2020,7 Conclusion,"beyond a silly board game, humans often need help verifying claims are true when evaluating health information (xie and bugg, 2009), knowing when to take an e-mail at face value (jagatic et al., 2007), or evaluating breaking news (hassan et al., 2017)."
2020.acl-main.353.txt,2020,7 Conclusion,"beyond playing against humans, playing with a human in the loop (hitl) resembles designs for cybersecurity threats (cranor, 2008), annotation (branson et al., 2010), and language alteration (wallace et al., 2019)."
2020.acl-main.353.txt,2020,7 Conclusion,building systems to help information consumers become more discerning and suspicious in low-stakes settings like online diplomacy are the seeds that will bear the fruits of interfaces and machine learning tools necessary for a safer and more robust internet ecosystem.
2020.acl-main.353.txt,2020,7 Conclusion,computers can meld their attention to detail and nigh infinite memory to humans’ grasp of social interactions and nuance to forge a more discerning player.
2020.acl-main.353.txt,2020,7 Conclusion,"dante asks count ugolino to name his betrayer, which leads him to say: but if my words can be the seed to bear the fruit of infamy for this betrayer who feeds my hunger, then i shall speak—in tears (alighieri and musa, 1995, canto xxxiii) similarly, we ask victims to expose their betrayers in the game of diplomacy."
2020.acl-main.353.txt,2020,7 Conclusion,"in dante’s inferno, the ninth circle of hell—a fate worse even than that reserved for murderers—is for betrayers."
2020.acl-main.353.txt,2020,7 Conclusion,"likewise, our lie-detection models can help a user in the moment better decide whether they are being deceived (lai et al., 2020)."
2020.acl-main.353.txt,2020,7 Conclusion,"the seeds of players’ negotiations and deceit could, we hope, yield fruit to help others: understanding multi-party negotiation and protecting internet users."
2020.acl-main.353.txt,2020,7 Conclusion,"while we ignore nuances of the game board to keep our work general, diplomacy is also a rich, multi-agent strategic environment; paquette et al.(2019) ignore diplomacy’s rich language to build bots that only move pieces around the board."
2020.acl-main.354.txt,2020,5 Conclusion,our results demonstrated that backpropagating through discrete data is not an issue for the training via matching distributions at the token level.
2020.acl-main.354.txt,2020,5 Conclusion,seqgfmn can be trained from scratch without the need for rl or gumbel softmax.
2020.acl-main.354.txt,2020,5 Conclusion,"this approach has allowed us to create effective models for unconditional generation, class-conditional generation, and unsupervised text style transfer."
2020.acl-main.354.txt,2020,5 Conclusion,we believe this work opens a new competitive avenue in the area of implicit generative models for sequential data.
2020.acl-main.354.txt,2020,5 Conclusion,we presented new implicit generative models based on feature matching loss that are suitable for unconditional and conditional text generation.
2020.acl-main.355.txt,2020,6 Conclusions and Future Work,"as a potential direction for future work, it would be interesting to investigate the use of the ema technique on transformer models as well and conduct similar studies to examine needless architectural complexity in other nlp tasks."
2020.acl-main.355.txt,2020,6 Conclusions and Future Work,"empirically, our model achieves the state of the art in both of the tasks."
2020.acl-main.355.txt,2020,6 Conclusions and Future Work,"in this paper, we question the necessity of complex neural architectures for text generation from structured data (neural table-to-text generation) and unstructured data (nqg)."
2020.acl-main.355.txt,2020,6 Conclusions and Future Work,"our results highlight the importance of thoroughly exploring simple models before introducing complex neural architectures, so that we can properly attribute the source of performance gains."
2020.acl-main.355.txt,2020,6 Conclusions and Future Work,we then propose a simple yet effective seq2seq model trained with the ema technique.
2020.acl-main.356.txt,2020,5 Conclusion and Future Work,"finally, when combining bhwr with the post-processing from (faruqui et al., 2015), further improvement is observed."
2020.acl-main.356.txt,2020,5 Conclusion and Future Work,"importantly, a remarkable improvement is obtained for rare words."
2020.acl-main.356.txt,2020,5 Conclusion and Future Work,"in the future, we plan to extend the applicability of the presented model to other linguistics tasks as well as recommendations and medical inference tasks."
2020.acl-main.356.txt,2020,5 Conclusion and Future Work,"moreover, bhwr outperforms all other baselines even when the latter are enhanced with the post-processing taxonomy refinement procedure from (faruqui et al., 2015)."
2020.acl-main.356.txt,2020,5 Conclusion and Future Work,"we presented bhwr - a word representation learning model, facilitating bayesian learning of co-occurrences relations together with word taxonomy via hierarchical priors."
2020.acl-main.356.txt,2020,5 Conclusion and Future Work,"when trained on a small corpus, bhwr exhibits a significant performance gain over other word embedding methods across various word similarity datasets."
2020.acl-main.357.txt,2020,6 Conclusions,future works include applying such scoring method on broader classification tasks like natural language inference and sentiment analysis.
2020.acl-main.357.txt,2020,6 Conclusions,"in this work, we presented a new method for plausibility ranking tasks, specifically targeting commonsense ranking problem."
2020.acl-main.357.txt,2020,6 Conclusions,our study demonstrates that the direct use of mlm over custom head yields increasingly superior performance gain when decreasing training data size.
2020.acl-main.357.txt,2020,6 Conclusions,the proposed approach outperforms state-of-the-art training methods in terms of both test accuracy and training stability.
2020.acl-main.357.txt,2020,6 Conclusions,"we also think that our token-level scoring method could be used during the self-supervised pretraining phase to extend traditional next sentence prediction and sequence ordering tasks, bringing more commonsense knowledge in the model."
2020.acl-main.357.txt,2020,6 Conclusions,we define a scoring function that leverages the mlm head of large pre-trained bidirectional transformer models.
2020.acl-main.357.txt,2020,6 Conclusions,"we establish strong results in a zero-shot setting on four commonsense reasoning datasets, comparable to supervised approaches."
2020.acl-main.357.txt,2020,6 Conclusions,"we then fine-tune such model using a margin-based loss on the proposed scoring function, and provide a comparative study with state of the art randomly initialized head methods."
2020.acl-main.358.txt,2020,5 Conclusion and Future Work,"besides, as a general framework, seek can incorporate many existing models, such as dist-mult, complex, and hole, as special cases."
2020.acl-main.358.txt,2020,5 Conclusion and Future Work,"in the future, we plan to extend the key insights of segmenting features and facilitating interactions to other representation learning problems."
2020.acl-main.358.txt,2020,5 Conclusion and Future Work,"in this paper, we propose a lightweight kge framework (seek) that can improve the expressiveness of embeddings without increasing the model complexity."
2020.acl-main.358.txt,2020,5 Conclusion and Future Work,"our extensive experiments on widely used public benchmarks demonstrate the efficiency, the effectiveness, and the robustness of seek."
2020.acl-main.358.txt,2020,5 Conclusion and Future Work,"to this end, our framework focuses on designing scoring functions and highlights two critical characteristics: 1) facilitating sufficient feature interactions and 2) preserving various relation properties."
2020.acl-main.359.txt,2020,7 Conclusions and Future Work,"by doing so, we managed to improve the results for the de-en system, while for eu-es we obtained similar performance to the other mt systems; this allows us to use just 25% of the data."
2020.acl-main.359.txt,2020,7 Conclusions and Future Work,"finally, we intend to analyse the effect of these measures in a wider range of language pairs and settings, in order to propose a more general solution."
2020.acl-main.359.txt,2020,7 Conclusions and Future Work,"further investigation is required to study under which conditions our proposed rescoring method is beneficial, but our experiments with both low- and high-resource language pairs suggest that if the systems used for backtranslation are poor, then this technique will be of little value; clearly this is closely related to the amount of resources available for the language pair under study."
2020.acl-main.359.txt,2020,7 Conclusions and Future Work,"furthermore, in terms of the two target languages, english is a morphologically less rich language than spanish, which creates a different setting again in which to evaluate our methodology."
2020.acl-main.359.txt,2020,7 Conclusions and Future Work,"in the future, we plan to investigate ways to directly incorporate the rescoring metrics into the data selection process itself, so that penalising similar sentences can also be taken into account."
2020.acl-main.359.txt,2020,7 Conclusions and Future Work,"the former is a low-resource language pair, and the latter a well researched, high-resource language pair."
2020.acl-main.359.txt,2020,7 Conclusions and Future Work,"under the assumption that fda’s performance is hindered by the fact that the data originates from mt systems, and as such contains errors and is of lower lexical richness, we rescored the data selection scores for each sentence by a factor depending on the bleu, ter and mtld values of the system used to backtranslate it."
2020.acl-main.359.txt,2020,7 Conclusions and Future Work,we also aim to conduct a human evaluation of the translated sentences in order to obtain a better understanding of the effects of data selection and backtranslation on the overall quality.
2020.acl-main.359.txt,2020,7 Conclusions and Future Work,"we evaluated several approaches to data selection over the data backtranslated by rbmt, pb-smt, lstm and transformer systems for two language pairs (eu-es and de-en) from the clinical/biomedical domain."
2020.acl-main.359.txt,2020,7 Conclusions and Future Work,"we show how the different fda data selection configurations tend to select different numbers of sentences coming from different systems, resulting in mt systems with different performance."
2020.acl-main.359.txt,2020,7 Conclusions and Future Work,we use these two different use-cases to better understand both data selection and backtranslation.
2020.acl-main.36.txt,2020,6 Conclusion,"for the decoder, we propose to utilize consecutive masking and introduce an n-gram based loss function to alleviate the problem of repetitive translations."
2020.acl-main.36.txt,2020,6 Conclusion,"in the future, we will extend the investigation on the functionalities of the encoder and decoder to other sequence-to-sequence tasks such as text summarization and text style transfer to explore more applications of our model."
2020.acl-main.36.txt,2020,6 Conclusion,"in this paper, we propose a jointly masked sequence-to-sequence model for non-autoregressive neural machine translation."
2020.acl-main.36.txt,2020,6 Conclusion,our model outperforms all compared nat baselines and achieves comparable performance with autoregressive models on five benchmark tasks with 5+ times speed up on the inference latency.
2020.acl-main.36.txt,2020,6 Conclusion,"we first empirically investigate the functionalities of the non-autoregressive translation model, and improve the training of the encoder by masking its input and introducing a prediction based loss function."
2020.acl-main.360.txt,2020,6 Conclusion,"by combining both slt pruning and mp, we can improve the sparsity-accuracy tradeoff."
2020.acl-main.360.txt,2020,6 Conclusion,"especially for very high sparsities of 90% or more, mp has proven to perform reasonably well while being easy to implement and having no additional training overhead."
2020.acl-main.360.txt,2020,6 Conclusion,"finally, we show that mp cannot be used to determine winning lottery tickets."
2020.acl-main.360.txt,2020,6 Conclusion,"in conclusion, we have shown that the stabilized lottery ticket (slt) hypothesis performs similar to magnitude pruning (mp) on the complex transformer architecture up to a sparsity of about 85%."
2020.acl-main.360.txt,2020,6 Conclusion,"in future work, we suggest performing a hyperparameter search over possible values for t in slt pruning (i.e., the number of training steps that are not discarded during model reset), and over si for the switch from slt to mp in slt-mp."
2020.acl-main.360.txt,2020,6 Conclusion,"in slt-mp, slt pruning first discards 60% of all parameters, so mp can focus on fine-tuning the model for maximum accuracy."
2020.acl-main.360.txt,2020,6 Conclusion,the specific initial parameter values do not significantly influence the training.
2020.acl-main.360.txt,2020,6 Conclusion,"we also have successfully verified that even for the transformer architecture, only the signs of the parameters are important when applying the slt pruning technique."
2020.acl-main.360.txt,2020,6 Conclusion,"we also recommend looking into why clt pruning works in our setup, while frankle et al.(2020) present opposing results."
2020.acl-main.361.txt,2020,5 Conclusion and Future Work,"as future work, we plan to extend our method to other nlp tasks which rely on evidence finding, such as natural language inference."
2020.acl-main.361.txt,2020,5 Conclusion and Future Work,"experiment results show that our proposed method consistently improves the base models in seven datasets for three mrc tasks, and that better evidence extraction indeed enhances the final performance of mrc."
2020.acl-main.361.txt,2020,5 Conclusion and Future Work,"in this iterative method, we train the base model with golden answers and pseudo evidence labels."
2020.acl-main.361.txt,2020,5 Conclusion and Future Work,"the updated model then generates new pseudo evidence labels, which can be used as additional supervision in the next iteration."
2020.acl-main.361.txt,2020,5 Conclusion and Future Work,"we present an iterative self-training method (stm) to improve mrc models with soft evidence extraction, when golden evidence labels are unavailable."
2020.acl-main.362.txt,2020,6 Conclusion,"for future work, we aim to consider more complex relationships among the quantities and other attributes to enrich quantity representations further."
2020.acl-main.362.txt,2020,6 Conclusion,"in this paper, we proposed a novel mwp solver, graph2tree, which improves the task performance by enriching the quantity representations in the problem."
2020.acl-main.362.txt,2020,6 Conclusion,our experiments shown that graph2tree is able to outperform the baselines on the mwp task.
2020.acl-main.362.txt,2020,6 Conclusion,we conducted extensive experiments to evaluate our model against state-of-the-art baselines.
2020.acl-main.362.txt,2020,6 Conclusion,we will also explore adding heuristic in the tree-based decoder to guide and improve the generation of solution expression.
2020.acl-main.363.txt,2020,6 Conclusions,code to compute cem will be available at github.com/evallteam/evalltoolkit.
2020.acl-main.363.txt,2020,6 Conclusions,"from a methodological perspective, the evidence that we have presented covers the four approaches pointed out in amigó et al.(2018): we have compared metrics in terms of desirable formal properties to be satisfied (theoretic top-down), we have generalized existing approaches (theoretic bottomup), and we have compared effectiveness on human assessed and on synthetic data (empirical bottomup and top-down)."
2020.acl-main.363.txt,2020,6 Conclusions,future work includes the application of cem at scales other than the ordinal.
2020.acl-main.363.txt,2020,6 Conclusions,"our findings can be summarized as follows: (i) metrics commonly used for ordinal classification problems are highly heterogeneous and, in general, inconsistent with the notion of ordinal scale in measurement theory; (ii) the notion of closeness between classes can be modelled in terms of measurement theory and information theory and particularized for different scales; and (iii) our proposed ordinal closeness evaluation measure (cemord) is the only one that satisfies all desirable formal properties, it is as robust as the best state-of-the-art metrics, and it is the one that better captures the different quality aspects of oc problems in our experimentation, with both synthetic and naturalistic datasets."
2020.acl-main.364.txt,2020,6 Conclusion,"although we have focused on compressing the embeddings by learning task-specific features, adacomp could be used at nlp tasks without fine-tuning."
2020.acl-main.364.txt,2020,6 Conclusion,evaluation results have clearly shown that adacomp could obtain better results than other methods in terms of both accuracy and memory requirement.
2020.acl-main.364.txt,2020,6 Conclusion,"in this paper, we have described adacomp that adaptively compresses word embeddings by using different lengths of code-books."
2020.acl-main.364.txt,2020,6 Conclusion,"to showcase the general applicability of adacomp, we conduct four different nlp tasks, which are sentence classification, chunking, natural language inference, and language modeling."
2020.acl-main.364.txt,2020,6 Conclusion,"to this end, we have used the gumbel-softmax tricks and the binary-constraint networks to learn the code-book selection and its discrete codes in an end-to-end manner."
2020.acl-main.364.txt,2020,6 Conclusion,we also found that adacomp assigns each word to different code-books by considering the significance of tasks.
2020.acl-main.364.txt,2020,6 Conclusion,"we believe that our method can benefit simultaneously from other compression techniques, such as pruning (han et al., 2016) and low-precision representation (ling et al., 2016)."
2020.acl-main.364.txt,2020,6 Conclusion,we leave this as an avenue for future work.
2020.acl-main.365.txt,2020,7 Conclusion,"finally, through extensive qualitative analysis, we have demonstrated that our method allows us to capture a variety of synchronic and diachronic linguistic phenomena."
2020.acl-main.365.txt,2020,7 Conclusion,"in recent work, we have experimented with alternative ways of obtaining usage representations (using a different language model, fine-tuning, and various layer selection strategies) and we have obtained very promising results in detecting semantic change across four languages (kutuzov and giulianelli, 2020)."
2020.acl-main.365.txt,2020,7 Conclusion,"in the future, we plan to investigate whether usage representations can provide an even finer grained account of lexical meaning and its dynamics, e.g., to automatically discriminate between different types of meaning change."
2020.acl-main.365.txt,2020,7 Conclusion,"our approach offers several advantages over previous methods: (1) it does not rely on a fixed number of word senses, (2) it captures morphosyntactic properties of word usage, and (3) it offers a more effective interpretation of lexical meaning by enabling the inspection of particular example sentences."
2020.acl-main.365.txt,2020,7 Conclusion,"to our knowledge, this is the first work that tackles this problem using neural contextualised word representations and no lexicographic supervision."
2020.acl-main.365.txt,2020,7 Conclusion,we expect our work to inspire further analyses of variation and change which exploit the expressiveness of contextualised word representations.
2020.acl-main.365.txt,2020,7 Conclusion,we have introduced a novel approach to the analysis of lexical semantic change.
2020.acl-main.365.txt,2020,7 Conclusion,"we have shown that the representations and the detected semantic shifts are aligned to human interpretation, and presented a new dataset of human similarity judgements which can be used to measure said alignment."
2020.acl-main.366.txt,2020,6 Conclusion,"experiments show that our model achieves better performance than the sequencelevel representations, and we conduct a series of analyses to further understand the reasons behind such a performance gain."
2020.acl-main.366.txt,2020,6 Conclusion,"in this paper, we propose a document clustering model based on features induced unsupervisedly from a gae and kcg."
2020.acl-main.366.txt,2020,6 Conclusion,"our model offers an elegant way to learn features directly from large corpora, bypassing the dependence on mature nlp pipelines."
2020.acl-main.366.txt,2020,6 Conclusion,"thus, it is not limited to resource-rich languages and can be used by any applications that operate on text."
2020.acl-main.367.txt,2020,5 Conclusion,"for both datasets, it outperforms bert, despite being a shallower model with fewer parameters, trained on less data."
2020.acl-main.367.txt,2020,5 Conclusion,"for gs2011, the pixie autoencoder achieves state-of-the-art results."
2020.acl-main.367.txt,2020,5 Conclusion,"for relpron, it learns information not captured by a vector space model."
2020.acl-main.367.txt,2020,5 Conclusion,"i have presented the pixie autoencoder, a novel encoder architecture and training algorithm for functional distributional semantics, improving on previous results in this framework."
2020.acl-main.367.txt,2020,5 Conclusion,"it is also easy to apply to these datasets (with no need to tune query strings), as it has a clear logical interpretation."
2020.acl-main.367.txt,2020,5 Conclusion,this points to the usefulness of building semantic structure into the model.
2020.acl-main.368.txt,2020,6 Conclusion,"by replacing important words with rare synonyms, we created downstream task datasets that are more challenging and support the evaluation of nlp models on the task of understanding rare words, a capability that human speakers have."
2020.acl-main.368.txt,2020,6 Conclusion,"furthermore, it would be interesting to explore more complex ways of incorporating surface-form information – e.g., by using a character-level cnn similar to the one of kim et al.(2016) – to balance out the potency of bertram’s form and context parts."
2020.acl-main.368.txt,2020,6 Conclusion,"in future work, we want to investigate bertram’s potential benefits for such frequent words."
2020.acl-main.368.txt,2020,6 Conclusion,"on all of these datasets, bertram improves over standard bert and roberta, demonstrating the usefulness of our method."
2020.acl-main.368.txt,2020,6 Conclusion,"our analysis showed that bertram is beneficial not only for rare words (our main target in this paper), but also for frequent words."
2020.acl-main.368.txt,2020,6 Conclusion,this is achieved by employing a powerful pretrained language model and deeply integrating surface-form and context information.
2020.acl-main.368.txt,2020,6 Conclusion,"we have introduced bertram, a novel architecture for inducing high-quality representations for rare words in bert’s and roberta’s embedding spaces."
2020.acl-main.369.txt,2020,7 Conclusions,"as future work, we plan to refine our approach by exploiting other strategies for weighting the words in the clusters and to leverage them for automatically building multilingual sense-tagged corpora."
2020.acl-main.369.txt,2020,7 Conclusions,"clubert attains state-of-the-art results on both intrinsic and extrinsic evaluations, also beating the widely-used and manually-curated wordnet mfs."
2020.acl-main.369.txt,2020,7 Conclusions,"finally, when injecting clubert mfs into off-the-shelf wsd models, we showed that it brings greater benefits than the wordnet mfs."
2020.acl-main.369.txt,2020,7 Conclusions,"in this paper we presented clubert, an automatic multilingual approach which induces the distribution of word senses in an arbitrary input corpus by exploiting the contextual information coming from bert and the lexical-semantic knowledge available in babelnet."
2020.acl-main.369.txt,2020,7 Conclusions,"similarly, our approach demonstrated its ability to scale well on different languages, attaining state-of-the-art results on the multilingual wsd tasks."
2020.acl-main.369.txt,2020,7 Conclusions,we release the sense distributions in five different languages at https://github.com/sapienzanlp/clubert.
2020.acl-main.369.txt,2020,7 Conclusions,"when considering input corpora that come from specific domains, clubert showed an unmatched nimbleness in shaping the distributions accordingly, hence outperforming its manual and automatic competitors on most domains."
2020.acl-main.37.txt,2020,5 Conclusion,"considering that the strong transformer translation model still has difficulty in fully capturing long-distance dependencies (tang et al., 2018), and that using a shorter phrase sequence (in addition to the original token sequence) is an intuitive approach to help the model capture long-distance features, in this paper, we first propose an attention mechanism to generate phrase representations by merging corresponding token representations."
2020.acl-main.37.txt,2020,5 Conclusion,"in addition, we incorporate the generated phrase representations into the transformer translation model to help it capture long-distance relationships."
2020.acl-main.37.txt,2020,5 Conclusion,our further analysis shows that the transformer with phrase representation empirically improves its performance especially in long-distance dependency learning.
2020.acl-main.37.txt,2020,5 Conclusion,"we obtained statistically significant improvements on the wmt 14 english-german and english-french tasks over the strong transformer baseline, which demonstrates the effectiveness of our approach."
2020.acl-main.370.txt,2020,5 Conclusion and Future Work,a novel domain-distinguish pre-training task is designed to distill the domain-specific features in a self-supervised.
2020.acl-main.370.txt,2020,5 Conclusion and Future Work,"experimental results on amazon dataset demonstrate the effectiveness of our model, which remarkably outperforms state-of-the-art methods."
2020.acl-main.370.txt,2020,5 Conclusion and Future Work,"in the future, we would like to investigate the application of our theory in these domain adaptation tasks."
2020.acl-main.370.txt,2020,5 Conclusion and Future Work,"in this paper, we propose the bert-daat model for cross-domain sentiment analysis."
2020.acl-main.370.txt,2020,5 Conclusion and Future Work,our purpose is to inject the target domain knowledge to bert and encourage bert to be domain-aware.
2020.acl-main.370.txt,2020,5 Conclusion and Future Work,"specifically, we conduct post-training and adversarial training."
2020.acl-main.370.txt,2020,5 Conclusion and Future Work,"the proposed post-training procedure could also be applied to other domain adaptation scenarios such as named entity recognition, question answering, and reading comprehension."
2020.acl-main.371.txt,2020,5 Conclusion,"finally, detecting the more implicit relations between the argument and the key point, as seen in our error analysis, is another intriguing direction for future work."
2020.acl-main.371.txt,2020,5 Conclusion,"however, by using state of the art supervised learning methods for match scoring, together with an appropriate key point selection policy for match classification, we were able to achieve promising results on this task."
2020.acl-main.371.txt,2020,5 Conclusion,"in addition, we plan to apply the methods presented in this work also to automatically-mined arguments."
2020.acl-main.371.txt,2020,5 Conclusion,"our experimental results demonstrate that the problem is far from trivial, and cannot be effectively solved using unsupervised methods based on word or sentence-level embedding."
2020.acl-main.371.txt,2020,5 Conclusion,such summary aims to provide both textual and quantitative views of the argument data in a concise form.
2020.acl-main.371.txt,2020,5 Conclusion,the natural next step for this work is the challenging task of automatic key point generation.
2020.acl-main.371.txt,2020,5 Conclusion,this work addressed the practical problem of summarizing a large collection of arguments on a given topic.
2020.acl-main.371.txt,2020,5 Conclusion,we demonstrated the feasibility and effectiveness of the proposed approach through extensive data annotation and analysis.
2020.acl-main.371.txt,2020,5 Conclusion,we proposed to represent such summaries as a set of key points scored according to their relative salience.
2020.acl-main.371.txt,2020,5 Conclusion,"we showed that a domain expert can quickly come up with a short list of pro and con key points per topic, that would capture the gist of crowd-contributed arguments, even without being exposed to the arguments themselves."
2020.acl-main.371.txt,2020,5 Conclusion,"we studied the problem of automatically matching arguments to key points, and developed the first large-scale dataset for this task, which we make publicly available."
2020.acl-main.372.txt,2020,7 Conclusion,"all these likely affect labeling, precision, and recall for a trained model."
2020.acl-main.372.txt,2020,7 Conclusion,anyone using this dataset should be aware of these limitations of the dataset.
2020.acl-main.372.txt,2020,7 Conclusion,data disclaimer: we are aware that the dataset contains biases and is not representative of global diversity.
2020.acl-main.372.txt,2020,7 Conclusion,"future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
2020.acl-main.372.txt,2020,7 Conclusion,"potential biases in the data include: inherent biases in reddit and user base biases, the offensive/vulgar word lists used for data filtering, inherent or unconscious bias in assessment of offensive identity labels, annotators were all native english speakers from india."
2020.acl-main.372.txt,2020,7 Conclusion,"the emotion pilot model used for sentiment labeling, was trained on examples reviewed by the research team."
2020.acl-main.372.txt,2020,7 Conclusion,we are aware that the dataset contains potentially problematic content.
2020.acl-main.372.txt,2020,7 Conclusion,"we build a strong baseline by fine-tuning a bert model, however, the results suggest much room for future improvement."
2020.acl-main.372.txt,2020,7 Conclusion,"we present goemotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction."
2020.acl-main.372.txt,2020,7 Conclusion,"we provide a detailed data analysis, demonstrating the reliability of the annotations for the full taxonomy."
2020.acl-main.372.txt,2020,7 Conclusion,we show the generalizability of the data across domains and taxonomies via transfer learning experiments.
2020.acl-main.373.txt,2020,7 Conclusion,"in the future, we plan to develop more complex models to be added in the next stages of the cascade classifier as well as automatically identify irony, gender stereotypes and sexist vocabulary."
2020.acl-main.373.txt,2020,7 Conclusion,"in this paper, we have presented the first approach to detect reports/denunciations of sexism from real sexist content that are directly addressed to a target or describes a target."
2020.acl-main.373.txt,2020,7 Conclusion,"these results are encouraging and demonstrate that detecting reporting assertions of sexism is possible, which is a first step towards automatic offensive content moderation."
2020.acl-main.373.txt,2020,7 Conclusion,"we proposed a new dataset of about 12, 000 french tweets annotated according to a new characterization of sexist content inspired from both speech act theory and discourse studies in gender."
2020.acl-main.373.txt,2020,7 Conclusion,"we then experimented with several deep learning models in binary, three classes and a cascade classifier configurations, showing that bert trained on word embeddings, linguistic features and generalization strategies (i.e., place and hypernym replacements) achieved the best results for all the configurations, and that cascade classification allows to successfully correct misclassified non-sexist messages."
2020.acl-main.374.txt,2020,7 Conclusion,"in the future, we hope to apply skep on more sentiment analysis tasks, to further see the generalization of skep, and we are also interested in exploiting more types of sentiment knowledge and more fine-grained sentiment mining methods."
2020.acl-main.374.txt,2020,7 Conclusion,"in this paper, we propose sentiment knowledge enhanced pre-training for sentiment analysis."
2020.acl-main.374.txt,2020,7 Conclusion,"our work verifies the necessity of utilizing sentiment knowledge for pre-training models, and provides a unified sentiment representation for a wide range of sentiment analysis tasks."
2020.acl-main.374.txt,2020,7 Conclusion,sentiment masking and three sentiment pre-training objectives are designed to incorporate various types of knowledge for pre-training model.
2020.acl-main.374.txt,2020,7 Conclusion,"skep significantly outperforms strong pre-training baseline roberta, and achieves new state-of-the-art on most datasets of three typical specific sentiment analysis tasks."
2020.acl-main.374.txt,2020,7 Conclusion,"thought conceptually simple, skep is empirically highly effective."
2020.acl-main.375.txt,2020,7 Conclusion and Future Work,"for example, while the chinese, hebrew, hindi, italian, and japanese models proved to be overwhelmingly better-fit for ud, basque aligned more with sud, and finnish, korean and turkish did not exhibit a clear preference."
2020.acl-main.375.txt,2020,7 Conclusion and Future Work,"for future work, besides seeking a deeper understanding of the interplay of linguistic factors and tree shape, we want to explore probes that combine the distance and depth assumptions into a single transformation, rather than learning separate probes and combining them post-hoc, as well as methods for alleviating treebank supervision altogether."
2020.acl-main.375.txt,2020,7 Conclusion and Future Work,"furthermore, an error analysis revealed that, when attaching words of various part-of-speech tags to their heads, ud fared better across the vast majority of categories, most notably adpositions and determiners."
2020.acl-main.375.txt,2020,7 Conclusion and Future Work,"lastly, given recent criticisms of probing approaches in nlp, it will be vital to revisit the insights produced here within a non-probing framework, for example, using representational similarity analysis (rsa) (chrupała and alishahi, 2019) over symbolic representations from treebanks and their encoded representations."
2020.acl-main.375.txt,2020,7 Conclusion and Future Work,"related to this, we found a strong correlation between differences in average tree height and the tendency to prefer one framework over the other."
2020.acl-main.375.txt,2020,7 Conclusion and Future Work,this suggested a tradeoff between morphological complexity — where differences in tree height between ud and sud are minimal and probing accuracy similar — and a high proportion of function words — where sud trees are significantly higher and probing accuracy favors ud.
2020.acl-main.375.txt,2020,7 Conclusion and Future Work,"ultimately, we observed a better overall fit for the ud-style formalism across models, layers, and languages, with some notable exceptions."
2020.acl-main.375.txt,2020,7 Conclusion and Future Work,"we have extended the structural probe of hewitt and manning (2019) to extract directed, rooted trees and fit it on pretrained bert and elmo representations for 13 languages."
2020.acl-main.375.txt,2020,7 Conclusion and Future Work,"we have investigated the extent to which the syntactic structure captured by neural language models aligns with different styles of analysis, using ud treebanks and their sud conversions as proxies."
2020.acl-main.376.txt,2020,5 Conclusion,"in addition, any advances in seq2seq neural architectures or pretrained transformer-based language models (devlin et al., 2019) can be directly used to enhance our approach."
2020.acl-main.376.txt,2020,5 Conclusion,the proposed linearization techniques can be used by any off-the-self seq2seq model without building a specific algorithm or structure.
2020.acl-main.376.txt,2020,5 Conclusion,we present significant accuracy and speed improvements in seq2seq constituent parsing.
2020.acl-main.377.txt,2020,5 Conclusion,we introduce several locality-centric refinements to advance graph parsing and empirically evaluate their effectiveness.
2020.acl-main.377.txt,2020,5 Conclusion,we show that exact graph parsing can be efficient even for large graphs and with large graph grammars.
2020.acl-main.378.txt,2020,7 Conclusion and Future work,"in principle, they could be applied to any task involving sequential structure prediction."
2020.acl-main.378.txt,2020,7 Conclusion and Future work,"such parsers can potentially make much more informed decisions about the next word, compared to the models based on mere sequence of words prefix, by including semantic and referential meaning (altmann and steedman, 1988), as well as syntax."
2020.acl-main.378.txt,2020,7 Conclusion and Future work,"the methods discussed here have been applied to the task of incremental ccg parsing, but they are not limited to ccg or even to parsing as a task."
2020.acl-main.378.txt,2020,7 Conclusion and Future work,we see this as the most interesting use case not only for the bso*-all training method but also for having an incremental ccg parser.
2020.acl-main.379.txt,2020,5 Conclusion,"here the gold tree ratio in the k-best list plays an important role, as the discriminative rerankers are very well able to distinguish the gold trees from other trees in the list, but their performance drops notably when we remove the gold trees from the list."
2020.acl-main.379.txt,2020,5 Conclusion,"however, none of the rerankers works well on the two morphologically rich(er) languages."
2020.acl-main.379.txt,2020,5 Conclusion,"in addition, we observe a higher diversity in the english k-best list, as compared to german and czech, which helps the rerankers to learn the differences between high- and low-quality trees."
2020.acl-main.379.txt,2020,5 Conclusion,our analysis gave some insights into this issue.
2020.acl-main.379.txt,2020,5 Conclusion,"the latter is much harder to achieve for mrls where the freer word order and high amount of non-projectivity result in a larger number of tree candidates, reflected by a lower gold tree ratio."
2020.acl-main.379.txt,2020,5 Conclusion,we conclude that the prerequisite for improving dependency parsing with neural reranking is a diverse k-best list with a high gold-tree ratio.
2020.acl-main.379.txt,2020,5 Conclusion,"we have evaluated recent neural techniques for reranking dependency parser output for english, german and czech and presented a novel reranking model, based on graph convolutional networks (gcns)."
2020.acl-main.379.txt,2020,5 Conclusion,we showed that the failure of the rerankers to improve results for german and czech over the baseline is due to the lower quality of the k-best lists.
2020.acl-main.379.txt,2020,5 Conclusion,"we were able to reproduce results for english, using existing rerankers, and showed that our novel gcn-based reranker even outperformed them."
2020.acl-main.38.txt,2020,5 Conclusion,"in contrast to previous works (bapna et al., 2018; wang et al., 2019; wu et al., 2019) which show that deep transformers with the computation order as in vaswani et al.(2017) have difficulty in convergence, we show that deep transformers with the original computation order can converge as long as proper parameter initialization is performed."
2020.acl-main.38.txt,2020,5 Conclusion,"our experiments show the effectiveness of our simple approach on the convergence of deep transformers, which achieves significant improvements on the wmt 14 english to german and the wmt 15 czech to english news translation tasks."
2020.acl-main.38.txt,2020,5 Conclusion,we also study the effects of deep decoders in addition to deep encoders extending previous works.
2020.acl-main.38.txt,2020,5 Conclusion,"we conjecture that the convergence issue of deep transformers is because layer normalization sometimes shrinks residual connections, we support our conjecture with a theoretical analysis (table 2), and propose a lipschitz constrained parameter initialization approach for solving this problem."
2020.acl-main.38.txt,2020,5 Conclusion,"we first investigate convergence differences between the published transformer (vaswani et al., 2017) and its official implementation (vaswani et al., 2018), and compare the differences of computation orders between them."
2020.acl-main.380.txt,2020,6 Conclusion,experiments show that our method can effectively alleviate discrimination.
2020.acl-main.380.txt,2020,6 Conclusion,"in this paper, we focus on the unintended discrimination bias in existing text classification datasets."
2020.acl-main.380.txt,2020,6 Conclusion,"it’s worth mentioning that our method is general enough to be applied to other tasks, as the key idea is to obtain the loss on the non-discrimination distribution, and we leave this to future works."
2020.acl-main.380.txt,2020,6 Conclusion,we formalize the problem as a kind of selection bias from the non-discrimination distribution to the discrimination distribution and propose a debiasing training framework that does not require any extra resources or annotations.
2020.acl-main.381.txt,2020,5 Conclusion,in future we will sample target models with a larger number of plausible combinations of factors.
2020.acl-main.381.txt,2020,5 Conclusion,in future work we hope to further disentangle these differences.
2020.acl-main.381.txt,2020,5 Conclusion,in this systematic study of analysis methods for neural models of spoken language we offered some suggestions on best practices in this endeavor.
2020.acl-main.381.txt,2020,5 Conclusion,"likewise, a choice of an analytical method may often entail changes in other aspects of the analysis: for example, unlike a global diagnostic classifier, global rsa captures the sequential order of phonemes."
2020.acl-main.381.txt,2020,5 Conclusion,"nevertheless our work is only a first step, and several limitations remain."
2020.acl-main.381.txt,2020,5 Conclusion,"the main challenge is that it is often difficult to completely control for the many factors of variation in the target models, due to the fact that a particular objective function, or even a dataset, may require relatively important architectural modifications."
2020.acl-main.382.txt,2020,5 Summary and Outlook,"for example, users would likely not accept a self-driving car if its explanation module is prone to state that “the car accelerates because there are people cross-ing the intersection.”."
2020.acl-main.382.txt,2020,5 Summary and Outlook,"future work will focus on developing more advanced procedures for detecting inconsistencies, and on building robust models that do not generate inconsistencies."
2020.acl-main.382.txt,2020,5 Summary and Outlook,this concern is general and can have a large practical impact.
2020.acl-main.382.txt,2020,5 Summary and Outlook,we drew attention that models generating natural language explanations are prone to producing inconsistent explanations.
2020.acl-main.382.txt,2020,5 Summary and Outlook,we introduced a generic framework for identifying such inconsistencies and showed that the best existing model on e-snli can generate a significant number of inconsistencies.
2020.acl-main.383.txt,2020,8 Discussion & Conclusion,"by improving this lower-bound, we could uncover more “accurate” information to support supervised probes’ findings."
2020.acl-main.383.txt,2020,8 Discussion & Conclusion,"finally, we find that feeding the empirically induced dependency structures into a downstream system (zhang et al., 2019) can further improve its accuracy."
2020.acl-main.383.txt,2020,8 Discussion & Conclusion,"first, it provides a lowerbound (on the unsupervised syntactic parsing ability of bert)."
2020.acl-main.383.txt,2020,8 Discussion & Conclusion,"however, this does not mean that supervised probes are wrong or that bert captures less syntax than we thought."
2020.acl-main.383.txt,2020,8 Discussion & Conclusion,"in fact, there is actually no guarantee that our probe will find a strong correlation with human-designed syntax, since we do not introduce the human-designed syntax as supervision."
2020.acl-main.383.txt,2020,8 Discussion & Conclusion,"in summary, we propose a parameter-free probing technique to complement current line of work on interpreting bert through probes."
2020.acl-main.383.txt,2020,8 Discussion & Conclusion,"one concern shared by our reviewers is that performance of our probes are underwhelming: the induced trees are barely closer to linguist-defined trees than simple baselines (e.g., rightbranching) and are even worse in the case of discourse parsing."
2020.acl-main.383.txt,2020,8 Discussion & Conclusion,"our results reinforce those of (hewitt and manning, 2019; liu et al., 2019; jawahar et al., 2019; tenney et al., 2019b,a) who demonstrated that bert encodes rich syntactic properties."
2020.acl-main.383.txt,2020,8 Discussion & Conclusion,"second, we show that when combined with a down-stream application (sec 6), the syntax learned by bert might be empirically helpful despite not totally identical to the human design."
2020.acl-main.383.txt,2020,8 Discussion & Conclusion,the improvement is compatible with or even superior to a human-designed dependency schema.
2020.acl-main.383.txt,2020,8 Discussion & Conclusion,"this matrix mirrors the function of attention mechanism that captures inter-word correlations, except that it emerges through the output of bert model, instead of from intermediate representations."
2020.acl-main.383.txt,2020,8 Discussion & Conclusion,this offers an insight into bert’s success in downstream tasks.
2020.acl-main.383.txt,2020,8 Discussion & Conclusion,"we also extend our method to probe document structure, which sheds lights on bert’s effectiveness in modeling long sequences."
2020.acl-main.383.txt,2020,8 Discussion & Conclusion,we devise algorithms to extract syntactic trees from this matrix.
2020.acl-main.383.txt,2020,8 Discussion & Conclusion,"we leave it for future work to use our technique to test other linguistic properties (e.g., coreference) and to extend our study to more downstream tasks and systems."
2020.acl-main.383.txt,2020,8 Discussion & Conclusion,we would rather say our probe complements the supervised probing findings in two ways.
2020.acl-main.383.txt,2020,8 Discussion & Conclusion,"what we found is the “natural” syntax inherent in bert, which is acquired from selfsupervised learning on plain text."
2020.acl-main.383.txt,2020,8 Discussion & Conclusion,"with carefully designed two-stage perturbation, we obtain impact matrices from bert."
2020.acl-main.384.txt,2020,5 Conclusion,"although it is much harder for models to induce a more global notion of entity (what we have called semantico-referential aspects), models seem to encode entity-specific information to some extent."
2020.acl-main.384.txt,2020,5 Conclusion,"as expected, our results show that language models capture morphosyntactic facts about anaphora: based on the information in the hidden layers, a simple linear transformation learns to link pronouns to other pronouns or noun phrases, and to do so largely respecting agreement constraints in gender and number."
2020.acl-main.384.txt,2020,5 Conclusion,"future work should investigate where these primitive referential abilities stem from and how they can be fostered in future architectures and training setups for language modeling, and neural models more generally."
2020.acl-main.384.txt,2020,5 Conclusion,"in this paper, we have analyzed to what extent they learn referential aspects of language, focusing on anaphora."
2020.acl-main.384.txt,2020,5 Conclusion,"models get confused when there are other mentions in the context, especially if they match in some morphosyntactic feature, but less than could be expected; and they show some limited ability to distinguish mentions that have the same form but are in different coreference chains, though hampered by their heavy recency bias."
2020.acl-main.384.txt,2020,5 Conclusion,"our results thus suggest that language models are more successful at learning grammatical constraints than they are at learning truly referential information, in the sense of capturing the fact that we use language to refer to entities in the world; however, they still do surprisingly well at referential aspects, given that they are trained on text alone."
2020.acl-main.384.txt,2020,5 Conclusion,"previous work has provided robust evidence that language models capture grammatical information without being explicitly trained to do so (linzen et al., 2016; gulordava et al., 2018)."
2020.acl-main.384.txt,2020,5 Conclusion,"the recency bias affects lstms more, but is also found in transformers, consistent with previous work on syntax (van schijndel et al., 2019)."
2020.acl-main.384.txt,2020,5 Conclusion,"we find that the two models behave similarly, but the transformer performs consistently better (around 10% higher accuracy in the probe tasks).8 future work should test other architectures, like cnn-based lms and lstms with attention, to provide additional insights into the linguistic capabilities of language models."
2020.acl-main.384.txt,2020,5 Conclusion,"we have tested two models representative of the prevailing architectures (transformer, lstm), and our methodology can be extended to any other architecture."
2020.acl-main.385.txt,2020,5 Conclusion,"following this work, we can build the attention graph with effective attention weights (brunner et al., 2020) instead of raw attentions."
2020.acl-main.385.txt,2020,5 Conclusion,"furthermore, we can come up with a new method that adjusts the attention weights using gradient-based attribution methods (ancona et al., 2019)."
2020.acl-main.385.txt,2020,5 Conclusion,"hence, to apply these methods on a transformer decoder, we should first normalize based on the receptive field of attention."
2020.acl-main.385.txt,2020,5 Conclusion,"in this paper, we insisted on sticking with simple ideas that only require attention weights and can be easily employed in any task or architecture that uses self-attention."
2020.acl-main.385.txt,2020,5 Conclusion,our ideas are simple and task/architecture agnostic.
2020.acl-main.385.txt,2020,5 Conclusion,"since in transformer decoder, future tokens are masked, naturally there is more attention toward initial tokens in the input sequence, and both attention rollout and attention flow will be biased toward these tokens."
2020.acl-main.385.txt,2020,5 Conclusion,translating embedding attentions to token attentions can provide us with better explanations about models’ internals.
2020.acl-main.385.txt,2020,5 Conclusion,"we should note that all our analysis in this paper is for a transformer encoder, with no casual masking."
2020.acl-main.385.txt,2020,5 Conclusion,"yet, we should be cautious about our interpretation of these weights, because, we are making many simplifying assumptions when we approximate information flow in a model with the attention weights."
2020.acl-main.386.txt,2020,9 Conclusion,"second, faithfulness is often evaluated in a binary “faithful or not faithful” manner, and we believe strictly faithful interpretation is a “unicorn” which will likely never be found."
2020.acl-main.386.txt,2020,9 Conclusion,"the opinion proposed in this paper is two-fold: first, interpretability evaluation often conflates evaluating faithfulness and plausibility together."
2020.acl-main.386.txt,2020,9 Conclusion,we should instead evaluate faithfulness on a more nuanced “grayscale” that allows interpretations to be useful even if they are not globally and definitively faithful.
2020.acl-main.386.txt,2020,9 Conclusion,we should tease apart the two definitions and focus solely on evaluating faithfulness without any influence of the convincing power of the interpretation.
2020.acl-main.387.txt,2020,7 Conclusion & Future work,"as future work, we would like to extend our analysis and proposed techniques to more complex models and downstream tasks."
2020.acl-main.387.txt,2020,7 Conclusion & Future work,"in this work, we have analyzed why existing attention distributions can neither provide a faithful nor a plausible explanation for the model’s predictions."
2020.acl-main.387.txt,2020,7 Conclusion & Future work,we proposed two techniques to effectively overcome this shortcoming and showed that attention distributions in the resulting models provide more faithful and plausible explanations.
2020.acl-main.387.txt,2020,7 Conclusion & Future work,"we showed that hidden representations learned by lstm encoders tend to be highly similar across different timesteps, thereby affecting the interpretability of attention weights."
2020.acl-main.388.txt,2020,6 Conclusion,"however, existing methods ignore the non-convexity and solve the problems using convex optimization methods."
2020.acl-main.388.txt,2020,6 Conclusion,"in the future, it would be fruitful to develop a novel weighting strategy for the tchebycheff procedure."
2020.acl-main.388.txt,2020,6 Conclusion,"in the tchebycheff procedure, we choose the weight for each task according to the empirical risk of learning the corresponding task independently."
2020.acl-main.388.txt,2020,6 Conclusion,most of multi-task text classiﬁcation problems are non-convex multi-objective optimization problems.
2020.acl-main.388.txt,2020,6 Conclusion,numerical experiments show that our proposed methods can converge and outperform state-of-the-art methods.
2020.acl-main.388.txt,2020,6 Conclusion,obtaining the empirical risk is a little laborious.
2020.acl-main.388.txt,2020,6 Conclusion,"to address this issue, this paper presents an (adversarial) tchebycheff procedure for multi-task text classiﬁcation without any convex assumption."
2020.acl-main.389.txt,2020,7 Conclusion,"the findings of our experiments provide important insights for translating morphologically rich languages, and are particularly important for low-resource settings."
2020.acl-main.389.txt,2020,7 Conclusion,we showed that morphologically sound segmentation that considers non-concatenative processes in order to obtain a consistent representation of subwords improves translation.
2020.acl-main.39.txt,2020,8 Discussion,"as current methods that memorize and learn superficial cues are unable to extrapolate while humans are, we believe that such a setting might help (and force) the field to come up with more human-like computational models that are capable of abstract reasoning."
2020.acl-main.39.txt,2020,8 Discussion,"by analyzing its behavior, we uncovered an interesting heuristic used by seq2seq models, namely that they keep track of a decoding “counter” to know when to output the <eos> token."
2020.acl-main.39.txt,2020,8 Discussion,"despite promising initial results, our model is still unable to extrapolate perfectly for harder tasks."
2020.acl-main.39.txt,2020,8 Discussion,"finally, as the location attender is not model dependent, it could be pretrained on complex location patterns and incorporated as a plug-and-play module to get extrapolatable position attention."
2020.acl-main.39.txt,2020,8 Discussion,"in this paper, we focused on one type of extrapolation, which is especially important in nlp: generalization to longer sequences."
2020.acl-main.39.txt,2020,8 Discussion,it would also be interesting to test such attention mechanisms in self-attentive seq2seq models without recurrence.
2020.acl-main.39.txt,2020,8 Discussion,"once the <eos> problem is solved, we could test the model on real-world datasets."
2020.acl-main.39.txt,2020,8 Discussion,"taking a step back, we have shown that current deep learning models with common attention mechanisms are unable to extrapolate well on seemingly straightforward tasks."
2020.acl-main.39.txt,2020,8 Discussion,"this is a bottleneck for extrapolation, suggesting that removing this heuristic is key to reaching perfect extrapolation and should be investigated in future work."
2020.acl-main.39.txt,2020,8 Discussion,this tends to be overlooked by the field due to standard benchmarks that can be solved using only interpolation.
2020.acl-main.39.txt,2020,8 Discussion,we hope that this paper acts as a reminder that extrapolation is a hard setting that has not been much investigated by the machine learning community.
2020.acl-main.39.txt,2020,8 Discussion,"we propose a new location-based attention, and show that it can extrapolate better than previous models while learning various attention patterns."
2020.acl-main.390.txt,2020,7 Conclusion,additional experiments with an adaptive λ for our trade-off sampling strategy show that properly balancing system and user objective can lead to considerable improvements in performance for both objectives.
2020.acl-main.390.txt,2020,7 Conclusion,additional use cases like the training of personalized recommendation models as well as the use of reinforcement learning to find a good trade-off between system and user objective remain to be investigated in future work.
2020.acl-main.390.txt,2020,7 Conclusion,"although system and user objective at first seem counteracting, our experiments indicate that they complement each other as jointly optimizing them outperforms optimizing only one of the goals."
2020.acl-main.390.txt,2020,7 Conclusion,"in this work, we investigated how we can incorporate user feedback into existing active learning approaches without hurting the user’s actual needs."
2020.acl-main.390.txt,2020,7 Conclusion,our experiments show that both our novel sampling strategies are successfully selecting instances which lead to a better model training while not hurting a learner’s progress by selecting too easy or too difficult c-tests.
2020.acl-main.390.txt,2020,7 Conclusion,our findings open up new opportunities for training models on low-resource scenarios with implicitly collected user feedback while jointly serving the user’s actual needs.
2020.acl-main.390.txt,2020,7 Conclusion,we create simulated learners for five different proficiency levels from real-world data and use them to define different learning behaviors.
2020.acl-main.390.txt,2020,7 Conclusion,"we evaluate our sampling strategies for the task of selecting suited c-tests, a type of fill-the-gap exercise, which fit the current proficiency of a human learner."
2020.acl-main.390.txt,2020,7 Conclusion,we formalize both system (active learning) and user objectives and propose two novel sampling strategies which aim to maximize both objectives jointly.
2020.acl-main.391.txt,2020,7 Conclusion,"in future work, we will investigate whether bert-init can be used effectively by using methods to deal with catastrophic forgetting."
2020.acl-main.391.txt,2020,7 Conclusion,"in this paper, we investigated how to effectively use mlms for training gec models."
2020.acl-main.391.txt,2020,7 Conclusion,our results show that bert-fuse ged was one of the most effective techniques when it was fine-tuned with gec corpora.
2020.acl-main.392.txt,2020,6 Conclusion,"a preference regularizer is also designed to force each disentangled subspace to independently reflect an isolated preference, further improving the quality of user and news embeddings."
2020.acl-main.392.txt,2020,6 Conclusion,"experimental results on realworld news datasets demonstrate that our model achieves significant performance gains compared to state-of-the-art methods, supporting the importance of exploiting the high-order connectivity and disentangling the latent preference factors in user and news representations."
2020.acl-main.392.txt,2020,6 Conclusion,"furthermore, the learned representations are disentangled with different latent preference factors by a neighborhood routing mechanism, enhancing expressiveness and interpretability."
2020.acl-main.392.txt,2020,6 Conclusion,"in this paper, we consider the high-order connectivity as well as the latent preference factors underlying the user-news interactions, and propose a novel graph neural news recommendation model gnud with unsupervised preference disentanglement."
2020.acl-main.392.txt,2020,6 Conclusion,our model regards the user-news interactions as a bipartite graph and encode high-order relationships among users and news by graph convolution.
2020.acl-main.393.txt,2020,7 Conclusion,"in this paper, we study the task of identifying principals and accessories from the fact description in a complex case."
2020.acl-main.393.txt,2020,7 Conclusion,we find a set of effective features for role modeling.and evaluate that the behavioral semantic information is most worthy of attention.
2020.acl-main.393.txt,2020,7 Conclusion,we hope to address this problem with a completely semantic-based approach in the future.
2020.acl-main.394.txt,2020,7 Conclusion,"for instance, we expect that abuse detection may also benefit from joint learning with complex semantic tasks, such as figurative language processing and inference."
2020.acl-main.394.txt,2020,7 Conclusion,"in this paper, we proposed a new approach to abuse detection, which takes advantage of the affective features to gain auxiliary knowledge through an mtl framework."
2020.acl-main.394.txt,2020,7 Conclusion,our experiments demonstrate that mtl with emotion detection is beneficial for the abuse detection task in the twitter domain.
2020.acl-main.394.txt,2020,7 Conclusion,"overall, our results also suggest the superiority of mtl over stl for abuse detection."
2020.acl-main.394.txt,2020,7 Conclusion,"the mutually beneficial relationship that exists between these two tasks opens new research avenues for improvement of abuse detection systems in other domains as well, where emotion would equally play a role."
2020.acl-main.394.txt,2020,7 Conclusion,"with this new approach, one can build more complex models introducing new auxiliary tasks for abuse detection."
2020.acl-main.395.txt,2020,5 Conclusion,another improvement concerns the analysis of verb references.
2020.acl-main.395.txt,2020,5 Conclusion,"finally, we synthesize method signatures from the declarative and method bodies from the specifying parts."
2020.acl-main.395.txt,2020,5 Conclusion,"first, we classify whether a natural language description entails an explicitly stated intent to teach new functionality."
2020.acl-main.395.txt,2020,5 Conclusion,future adoptions to fuse will include the integration of a dialog component.
2020.acl-main.395.txt,2020,5 Conclusion,giving feedback to newly learned method definitions that may be lengthy and therefore unhandy to repeat as a whole is an interesting challenge.
2020.acl-main.395.txt,2020,5 Conclusion,"however, we still have to figure out, how to query users properly if an api mapping is ambiguous or parameters are missing."
2020.acl-main.395.txt,2020,5 Conclusion,"humans often refer to previous actions, which may cause superfluous instructions."
2020.acl-main.395.txt,2020,5 Conclusion,"if an intent is spotted, we use a second classifier to separate the input into semantically disjoint parts; we identify declarative and specifying parts and filter out superfluous information."
2020.acl-main.395.txt,2020,5 Conclusion,in a second evaluation on a speech corpus the f1-score for api calls is 79.2%.
2020.acl-main.395.txt,2020,5 Conclusion,instructions are mapped to api calls.
2020.acl-main.395.txt,2020,5 Conclusion,"it will be interesting to see, if we can reuse (or transfer) the machine learning models as well as the rest of the approach."
2020.acl-main.395.txt,2020,5 Conclusion,method bodies contain instructions and control structures.
2020.acl-main.395.txt,2020,5 Conclusion,"more precisely, we aim to enable laypersons to teach an intelligent system new functionality with nothing but spoken instructions.our approach is three-tiered."
2020.acl-main.395.txt,2020,5 Conclusion,teaching intents are identified with an accuracy of 97.7% (using bert).
2020.acl-main.395.txt,2020,5 Conclusion,the classification of the semantics is correct in 97.6% of the cases (using a bilstm).
2020.acl-main.395.txt,2020,5 Conclusion,the latter may involve a feedback mechanism via the dialog component.
2020.acl-main.395.txt,2020,5 Conclusion,the mapping of instructions in the body to api calls achieved an f1-score of 66.9%.
2020.acl-main.395.txt,2020,5 Conclusion,the results are promising; fuse correctly synthesized 84.6% of the method signatures.
2020.acl-main.395.txt,2020,5 Conclusion,we evaluated fuse on 100 descriptions obtained from a user study.
2020.acl-main.395.txt,2020,5 Conclusion,"we have implemented an extensible dialog module and shown that it can be used to resolve ambiguous references, word recognition errors, and missing conditions (weigelt et al., 2018a)."
2020.acl-main.395.txt,2020,5 Conclusion,"we have presented fuse, a system for programming in natural language."
2020.acl-main.395.txt,2020,5 Conclusion,we implemented the first two steps using classical machine learning and neural networks.
2020.acl-main.395.txt,2020,5 Conclusion,we may query the user in case of ambiguous statements or missing parameters.
2020.acl-main.395.txt,2020,5 Conclusion,we plan to evaluate fuse in other domains.
2020.acl-main.395.txt,2020,5 Conclusion,we will also implement a sanity check that considers feasibility and meaningfulness of the sequence of actions in the method body.
2020.acl-main.396.txt,2020,4 Conclusions and Future Work,"a limitation of our work is that we considered a narrow contextual context, comprising only the previous comment and the discussion title.11 it would be interesting to investigate in future work ways to improve the annotation quality when more comments in the discussion thread are provided, and also if our findings hold when broader context is considered (e.g., all previous comments in the thread, or the topic of the thread as represented by a topic model)."
2020.acl-main.396.txt,2020,4 Conclusions and Future Work,another limitation of our work is that we used randomly sampled comments.
2020.acl-main.396.txt,2020,4 Conclusions and Future Work,our experiments and datasets provide an initial foundation to investigate these important directions.
2020.acl-main.396.txt,2020,4 Conclusions and Future Work,"the effect of context may be more significant in conversations about particular topics, or for particular conversational tones (e.g.sarcasm), or when they reference communities that are frequently the target of online abuse."
2020.acl-main.396.txt,2020,4 Conclusions and Future Work,"the lack of improvement in system performance seems to be related to the fact that context-sensitive comments are infrequent, at least in the data we collected."
2020.acl-main.396.txt,2020,4 Conclusions and Future Work,"we also found no evidence that context actually improves the performance of toxicity classifiers, having tried both simple and more powerful classifiers, having experimented with several methods to make the classifiers context aware, and having also considered the effect of gold labels obtained out of context vs. gold labels obtained by showing context to the annotators."
2020.acl-main.396.txt,2020,4 Conclusions and Future Work,we collected and share two datasets for investigating our research questions around the effect of context on the annotation of toxic comments (rq1) and its detection by automated systems (rq2).
2020.acl-main.396.txt,2020,4 Conclusions and Future Work,we investigated the role of context in detecting toxicity in online comments.
2020.acl-main.396.txt,2020,4 Conclusions and Future Work,"we showed that context does have a statistically significant effect on toxicity annotation, but this effect is seen in only a narrow slice (5.2%) of the (first) dataset."
2020.acl-main.397.txt,2020,6 Conclusion,experiment results show that the latent structural information improve the best reported parsing performance on both amr 2.0 (ldc2017t10) and amr 1.0 (ldc2014t12).
2020.acl-main.397.txt,2020,6 Conclusion,"we also propose to incorporate the latent graph into other multi-task learning problems (chen et al., 2019; kurita and søgaard, 2019)."
2020.acl-main.397.txt,2020,6 Conclusion,"we investigate latent structure for amr parsing, and we denote that the inferred latent graph can interpret the connection probabilities between input words."
2020.acl-main.398.txt,2020,7 Conclusion,"in future work we aim to extend the model to represent a database with multiple tables as context, and to effectively handle large tables."
2020.acl-main.398.txt,2020,7 Conclusion,"in this paper we presented tapas, a model for question answering over tables that avoids generating logical forms."
2020.acl-main.398.txt,2020,7 Conclusion,results show that tapas achieves better or competitive results in comparison to state-of-the-art semantic parsers.
2020.acl-main.398.txt,2020,7 Conclusion,"we additionally showed that the model can fine-tune on semantic parsing datasets, only using weak supervision, with an end-to-end differentiable recipe."
2020.acl-main.398.txt,2020,7 Conclusion,we showed that tapas effectively pre-trains over large scale data of text-table pairs and successfully restores masked words and table cells.
2020.acl-main.399.txt,2020,7 Conclusion,a more elaborate phrasing approach may take over context information from the premises.
2020.acl-main.399.txt,2020,7 Conclusion,an argument’s conclusion comprises its stance towards the target it discusses.
2020.acl-main.399.txt,2020,7 Conclusion,"combining target inference with stance classification in future work, we can already generate basic conclusions, say, “raising the school leaving age is good”."
2020.acl-main.399.txt,2020,7 Conclusion,"hypothesizing that the conclusion target depends on the premise targets, we have developed two new and complementary target inference approaches: premise targets (ranking) returns the premise target that is most likely adequate for the conclusion, while target embedding (learning) generates a conclusion target embedding from the premises and matches it against a target knowledge base."
2020.acl-main.399.txt,2020,7 Conclusion,"in terms of bleu, meteor, and accuracy, target embedding (learning) and a hybrid of both approaches turned out particularly strong, whereas premise targets (ranking) was best in a manual evaluation."
2020.acl-main.399.txt,2020,7 Conclusion,"on three datasets from two domains (debate portals and student essays), our approaches outperform several baselines, including a state-of-the-art neural sequence-to-sequence summarizer."
2020.acl-main.399.txt,2020,7 Conclusion,"overall, we manage to infer an at least somewhat adequate conclusion target in 89% of all cases, indicating the practical applicability of our approaches."
2020.acl-main.399.txt,2020,7 Conclusion,"still, the conclusion is often left implicit in real life, because it is clear for humans or hidden for rhetorical reasons."
2020.acl-main.399.txt,2020,7 Conclusion,"the latter also benefits from modeling premise targets, additionally supporting our hypothesis."
2020.acl-main.399.txt,2020,7 Conclusion,then we have focused on the first step in which we infer the conclusion target given a set of premises.
2020.acl-main.399.txt,2020,7 Conclusion,"we have conceptualized the task of reconstructing the conclusion from the argument’s premises as (1) inferring the conclusion’s target, (2) inferring its stance, and (3) phrasing its actual text."
2020.acl-main.4.txt,2020,7 Conclusion,automatic dialogue response evaluators have problems in robustness and correlation with human judgement.
2020.acl-main.4.txt,2020,7 Conclusion,experimental results demonstrated that our proposed evaluator achieved strong correlation (> 0.6) with human judgement and showed robustness in dealing with diverse responses and a new domain.
2020.acl-main.4.txt,2020,7 Conclusion,it can also be trained efficiently with less than 100 annotated samples.
2020.acl-main.4.txt,2020,7 Conclusion,"we investigated three methods to alleviate them: 1) using reference-free metrics, 2) applying semi-supervised training, and 3) exploiting powerful pretrained text encoders."
2020.acl-main.40.txt,2020,6 Conclusion and Future Work,"and the deeper msc model results in high computational overhead, to address this issue, we would like to apply the average attention network (zhang et al., 2018a) to our deep msc models."
2020.acl-main.40.txt,2020,6 Conclusion and Future Work,experiments on various language pairs show that the msc achieves prominent improvements over strong baselines as well as previous deep models.
2020.acl-main.40.txt,2020,6 Conclusion and Future Work,"in the future, we would like to extend our model to extremely large datasets, such as wmt’14 english-to-french with about 36m sentence-pairs."
2020.acl-main.40.txt,2020,6 Conclusion and Future Work,"in this paper, we propose a multisacle collaborative framework to ease the training of extremely deep nmt models."
2020.acl-main.40.txt,2020,6 Conclusion and Future Work,"specifically, instead of the top-most representation of the encoder stack, we attend the decoder to multi-granular source information with different space-scales."
2020.acl-main.40.txt,2020,6 Conclusion and Future Work,we have shown that the proposed approach boosts the training of very deep models and can bring improvements on translation quality from greatly increased depth.
2020.acl-main.400.txt,2020,4 Conclusion,"in future work, we would like to investigate the effectiveness of our model in these tasks."
2020.acl-main.400.txt,2020,4 Conclusion,"in this paper, we propose the multimodal selfattention to consider the relative importance between different modalities in the mmt task."
2020.acl-main.400.txt,2020,4 Conclusion,the experiments and visualization show that our model can make good use of multimodal information and get better performance than previous works.
2020.acl-main.400.txt,2020,4 Conclusion,the hidden representations of less important modality (image) are induced from the important modality (text) under the guide of image-aware attention.
2020.acl-main.400.txt,2020,4 Conclusion,there are various multimodal tasks where multiple modalities have different relative importance.
2020.acl-main.401.txt,2020,6 Conclusion,"along with investigating new techniques, we hope that assembling a bigger curated dataset with quality annotations will help in better performance."
2020.acl-main.401.txt,2020,6 Conclusion,"as there was no suitable labeled data available for this problem, we have created the dataset by manually annotating an existing dataset of sarcasm with sentiment and emotion labels.we have introduced two attention mechanisms (i.e., ie-attention and ia-attention), and incorporated the significance of context and speaker information w.r.t.sarcasm."
2020.acl-main.401.txt,2020,6 Conclusion,"during our analysis, we found that the dataset is not big enough for a complex framework to learn from."
2020.acl-main.401.txt,2020,6 Conclusion,empirical evaluation results on the extended version of the mustard dataset suggests the efficacy of the proposed model for sarcasm analysis over the existing state-of-the-art systems.
2020.acl-main.401.txt,2020,6 Conclusion,"in this paper, we have proposed an effective deep learning-based multi-task model to simultaneously solve all the three problems, viz.sentiment analysis, emotion analysis and sarcasm detection."
2020.acl-main.401.txt,2020,6 Conclusion,"the evaluation also showed that the proposed multi-tasking framework achieves better performance for the primary task, i.e.sarcasm detection, with the help of emotion analysis and sentiment analysis, the two secondary tasks in our setting."
2020.acl-main.402.txt,2020,6 Conclusion and Future Work,"consequently, we also propose an attention based (self, inter-modal, inter-task) multi-modal, multi-task framework for joint optimization of das and emotions."
2020.acl-main.402.txt,2020,6 Conclusion and Future Work,"in future, conversation history, speaker information, fine-grained modality encodings can be incorporated to predict da with more accuracy and precision."
2020.acl-main.402.txt,2020,6 Conclusion and Future Work,"in this paper, we investigate the role of emotion and multi-modality in determining das of an utterance."
2020.acl-main.402.txt,2020,6 Conclusion and Future Work,results show that multi-modality and multi-tasking boosted the performance of da identification compared to its unimodal and single task dac variants.
2020.acl-main.402.txt,2020,6 Conclusion and Future Work,"to enable research with these aspects, we create a novel dataset, emotyda, that contains emotion-rich videos of dialogues collected from various open-source datasets manually annotated with das."
2020.acl-main.403.txt,2020,7 Conclusion,"focusing on political parody in social media, we introduced a freely available large-scale data set containing a total of 131,666 english tweets from 184 real and corresponding parody accounts."
2020.acl-main.403.txt,2020,7 Conclusion,"in the future, we plan to study more in depth the stylistic and figurative devices used for parody, extend the data set beyond the political case study and explore human behavior regarding parody, including how this is detected and diffused through social media."
2020.acl-main.403.txt,2020,7 Conclusion,we defined parody prediction as a new binary classification task at a tweet level and evaluated a battery of feature-based and neural models achieving high predictive accuracy of up to 89.7% f1 on tweets from people unseen in training.
2020.acl-main.403.txt,2020,7 Conclusion,"we presented the first study of parody using methods from computational linguistics and machine learning, a related but distinct linguistic phenomenon to irony and sarcasm."
2020.acl-main.404.txt,2020,6 Conclusion,"actor masking improves recall for infrequent actors without affecting overall performance, and, as a side benefit, also improves out-of-domain generalization."
2020.acl-main.404.txt,2020,6 Conclusion,"clearly, actor frequency is only one of a large number of potential frequency-related biases."
2020.acl-main.404.txt,2020,6 Conclusion,"it warrants the same kind of attention as other bias types: lower recall for infrequent actors is inherently unfair, hitting those who can afford least to have their contribution overlooked."
2020.acl-main.404.txt,2020,6 Conclusion,"since frequency is known to be strongly correlated with performance in machine learning-based nlp, such biases should be investigated more systematically in areas building on nlp such as computational social sciences."
2020.acl-main.404.txt,2020,6 Conclusion,"the nlp community has mostly focused on biases grounded in extralinguistic reality, e.g., gender (bolukbasi et al., 2016; rudinger et al., 2018; stanovsky et al., 2019), race (kiritchenko and mohammad, 2018), or age (hovy and søgaard, 2015)."
2020.acl-main.404.txt,2020,6 Conclusion,this paper has discussed the task of political claims analysis as an example of computational social science where nlp methods are finding adoption to scale analysis to large data sets.
2020.acl-main.404.txt,2020,6 Conclusion,"to remove these biases, however, presumably more sophisticated methods will be necessarily in the general case."
2020.acl-main.404.txt,2020,6 Conclusion,we compared two approaches to mitigating frequency bias in political claims detection and tested them on in-domain and out-of-domain settings.
2020.acl-main.404.txt,2020,6 Conclusion,we found that a simple data modification strategy does as good as or better than modifying the model objective.
2020.acl-main.404.txt,2020,6 Conclusion,we have argued that this scenario must be aware of systematic biases in the output of the nlp methods.
2020.acl-main.404.txt,2020,6 Conclusion,we identified frequency as a language-internal bias present in a current neural model in political claims analysis.
2020.acl-main.404.txt,2020,6 Conclusion,"while we only evaluated the strategy on one model, we believe its benefits carry over to other model architectures and similar tasks."
2020.acl-main.405.txt,2020,6 Conclusion,"again, two different perspectives on this are needed."
2020.acl-main.405.txt,2020,6 Conclusion,but gonen and goldberg’s 2019 argument relies on a technical deficiency of existing approaches.
2020.acl-main.405.txt,2020,6 Conclusion,"currently, upstream evaluations of debiasing are centered almost exclusively on occupational identities on gender, where some of the most salient social biases we know of exist (ridgeway, 2011)."
2020.acl-main.405.txt,2020,6 Conclusion,"depending on one’s perspective, this could be good or bad."
2020.acl-main.405.txt,2020,6 Conclusion,"for example, the correlation between gender beliefs and the gender direction in the hard-debiased embeddings of bolukbasi et al.(2016) is 0.05 (p = .84) using identities in their data, and 0.4 (p <.05) using the identities in our data."
2020.acl-main.405.txt,2020,6 Conclusion,"for example, while sweeney and najafian (2019) show that the numberbatch embeddings harbor the least gender bias, we find that they are the only embedding to show consistently high correlations with age, leading to the potential for ageism downstream."
2020.acl-main.405.txt,2020,6 Conclusion,"from a cultural studies/social psychological perspective, this positive correlation further validates efforts to use word embeddings to study perceptions of people historically, at scale, and in context."
2020.acl-main.405.txt,2020,6 Conclusion,"from the bias perspective, given the rash of recent work on debiasing word embeddings, our results suggest that much more attention needs to be paid to how we are evaluating these approaches."
2020.acl-main.405.txt,2020,6 Conclusion,"however, we also find that some beliefs— specifically, extreme beliefs on salient dimensions — are easier to measure than others."
2020.acl-main.405.txt,2020,6 Conclusion,"in this paper, we asked, can we trust measures of beliefs about people derived from word embeddings?"
2020.acl-main.405.txt,2020,6 Conclusion,"more generally, across four datasets, we find that what we measure is more important than how we measure it."
2020.acl-main.405.txt,2020,6 Conclusion,"more generally, stereotypes exist along a network of beliefs (freeman and ambady, 2011) reflecting unwarranted correlations between many dimensions (ridgeway, 2011); we must therefore be careful not to expect that removing meaning along one dimension will expel social biases from our models."
2020.acl-main.405.txt,2020,6 Conclusion,"on the other hand, from the “bias” perspective, this suggests that a vast array of social biases are encoded in embeddings."
2020.acl-main.405.txt,2020,6 Conclusion,"others have argued that removing these salient beliefs may not remove gender information from embeddings (gonen and goldberg, 2019)."
2020.acl-main.405.txt,2020,6 Conclusion,"similarly, removing gender bias does not remove bias on other dimensions."
2020.acl-main.405.txt,2020,6 Conclusion,we can make a similar critique by simply changing what is being measured.
2020.acl-main.405.txt,2020,6 Conclusion,"we find the answer to be yes, at least on average."
2020.acl-main.405.txt,2020,6 Conclusion,"with respect to the study of culture and human stereotypes, we may be safest in studying only the most extreme results from embedding models, as has been done by, e.g., spirling and rodriguez (2019)."
2020.acl-main.406.txt,2020,7 Conclusion and Future Work,"for example, inferring the current version of the provenance graph depends on the ability to identify authors."
2020.acl-main.406.txt,2020,7 Conclusion and Future Work,"for example, the edge labels, indicating the evolution operators of a claim should also be useful."
2020.acl-main.406.txt,2020,7 Conclusion and Future Work,"from the application perspective, it is clear that the graph contains more information than we have exploited so far."
2020.acl-main.406.txt,2020,7 Conclusion and Future Work,"in particular, this will support a more informed study of influence of specific sources and of trustworthiness, and possibly other aspects of information spread."
2020.acl-main.406.txt,2020,7 Conclusion and Future Work,the framework introduces a range of important questions both from the inference and the application perspectives.
2020.acl-main.406.txt,2020,7 Conclusion and Future Work,"this could be difficult when the authors are not mentioned in the text, which might require a deeper understanding of sources’ writing style and positions."
2020.acl-main.406.txt,2020,7 Conclusion and Future Work,"we argue that this notion of provenance is essential if we are to understand how claims evolve over time, and what sources contributed to earlier versions of the claims."
2020.acl-main.406.txt,2020,7 Conclusion and Future Work,we introduce a formal definition and a computational framework for the provenance of a natural language claim given a corpus.
2020.acl-main.406.txt,2020,7 Conclusion and Future Work,"we provide initial results exhibiting that our framework can be used successfully to infer the provenance graph and, that it can be applied to boost the performance of claim verification."
2020.acl-main.407.txt,2020,7 Discussion,a thorough investigation of neural network codes that can generalize while being partially entangled might shed light on similar phenomena in human languages.
2020.acl-main.407.txt,2020,7 Discussion,"as we argued that compositionality has, after all, desirable properties, future work could adapt methods for learning disentangled representations (e.g., higgins et al., 2017; kim and mnih, 2018) to let (more) compositional languages emerge."
2020.acl-main.407.txt,2020,7 Discussion,"before concluding that garden-variety neural networks do not generalize, the simple strategy of exposing them to a richer input should always be tried."
2020.acl-main.407.txt,2020,7 Discussion,compositionality and disentanglement language is a way to represent meaning through discrete symbols.
2020.acl-main.407.txt,2020,7 Discussion,"compositionality might act like a “dominant” genetic feature: it might arise by a random mutation but, once present, it will survive and thrive, as it guarantees that languages possessing it will generalize and will be easier to learn."
2020.acl-main.407.txt,2020,7 Discussion,"finally, and perhaps most importantly, recent interest in compositionality among ai researchers stems from the assumption that compositionality is crucial to achieve good generalization through language (e.g., lake and baroni, 2018; lazaridou et al., 2018; baan et al., 2019)."
2020.acl-main.407.txt,2020,7 Discussion,"from an ai perspective, this suggests that trying to enforce compositionality during language emergence will increase the odds of developing languages that are quickly usable by wide communities of artificial agents, that might be endowed with different architectures."
2020.acl-main.407.txt,2020,7 Discussion,"from the linguistic perspective, our results suggest an alternative view of the relation between compositionality and language transmission–one in which the former might arise by chance or due to other factors, but then makes the resulting language much easier to be spread."
2020.acl-main.407.txt,2020,7 Discussion,"generalization without compositionality our most important result is that there is virtually no correlation between whether emergent languages are able to generalize to novel composite inputs and the presence of compositionality in their messages (andreas (2019) noted in passing the emergence of non-compositional generalizing languages, but did not explore this phenomenon systematically)."
2020.acl-main.407.txt,2020,7 Discussion,"here, we reversed the arrow of causality and showed that, if compositionality emerges (due to chance during initial language development), it will make a language easier to transmit to new agents."
2020.acl-main.407.txt,2020,7 Discussion,"if agents develop a compositional language, they are then very likely to be able to use it correctly to refer to novel inputs."
2020.acl-main.407.txt,2020,7 Discussion,"in our setting, the emergence of generalization is very strongly correlated with variety of the input environment."
2020.acl-main.407.txt,2020,7 Discussion,"indeed, even studies of the origin of human language conjecture that the latter did not develop sophisticated generalization mechanisms until pressures from an increasingly complex environment forced it to evolve in that direction (bickerton, 2014; hurford, 2014)."
2020.acl-main.407.txt,2020,7 Discussion,"indeed, when training new agents on emerged languages that generalize, it is much more likely that the new agents will learn them fast and thoroughly (i.e., they will be able to understand expressions referring to novel inputs) if the languages are already compositional according to our measures."
2020.acl-main.407.txt,2020,7 Discussion,"interestingly one of the ways in which they did differ is that, when a language is positionally disentangled, (and, to a lesser extent, bag-of-symbols disentangled), it is very likely that the language will be able to generalize–a guarantee we don’t have from less informative topographic similarity."
2020.acl-main.407.txt,2020,7 Discussion,"it is thus worth exploring the link between the area of language emergence and that of representation learning (bengio et al., 2013)."
2020.acl-main.407.txt,2020,7 Discussion,"on the one hand, given that topographic similarity is an established way to quantify compositionality, this serves as a sanity check on the new measures."
2020.acl-main.407.txt,2020,7 Discussion,"on the other, we are disappointed that we did not find more significant differences between the three measures."
2020.acl-main.407.txt,2020,7 Discussion,"our result might also speak to those linguists who are exploring the non-fully-compositional corners of natural language (e.g., goldberg, 2019)."
2020.acl-main.407.txt,2020,7 Discussion,"our results suggest that the pursuit of generalization might be separated from that of compositionality, a point also recently made by kharitonov and baroni (2020) through hand-crafted simulations."
2020.acl-main.407.txt,2020,7 Discussion,"supporting generalization to new composite inputs is seen as one of the core purposes of compositionality in natural language (e.g., pagin and westersta˚hl, 2010)."
2020.acl-main.407.txt,2020,7 Discussion,"that language transmission increases pressure for structured representations is an established fact (e.g., kirby et al., 2015; cornish et al., 2017)."
2020.acl-main.407.txt,2020,7 Discussion,"the natural emergence of generalization there has been much discussion on the generalization capabilities of neural networks, particularly in linguistic tasks where humans rely on compositionality (e.g., fodor and lepore, 2002; marcus, 2003; van der velde et al., 2004; brakel and frank, 2009; kottur et al., 2017; lake and baroni, 2018; andreas, 2019; hupkes et al., 2019; resnick et al., 2019)."
2020.acl-main.407.txt,2020,7 Discussion,"the representation learning literature is not only proposing disentanglement measures, but also ways to encourage emergence of disentanglement in learned representations."
2020.acl-main.407.txt,2020,7 Discussion,"this has implications for the ongoing debate on the origins of compositionality in natural language, (e.g., townsend et al., 2018, and references there), as it suggests that the need to generalize alone might not constitute a sufficient pressure to develop a fully compositional language."
2020.acl-main.407.txt,2020,7 Discussion,this supports the intuition that compositional languages are easier to fully understand.
2020.acl-main.407.txt,2020,7 Discussion,"we focused in particular on the intuition that, if emergent languages must denote ensembles of primitive input elements, they are compositional when they use symbols to univocally denote input elements independently of each other."
2020.acl-main.407.txt,2020,7 Discussion,"we observed that positional disentanglement, while not necessary, is sufficient for generalization."
2020.acl-main.407.txt,2020,7 Discussion,"we took this route, borrowing ideas from research on disentangled representations to craft our compositionality measures."
2020.acl-main.407.txt,2020,7 Discussion,what is compositionality good for?
2020.acl-main.407.txt,2020,7 Discussion,"while the new measures we proposed are not highly correlated with topographic similarity, in most of our experiments they did not behave significantly differently from the latter."
2020.acl-main.407.txt,2020,7 Discussion,"while there is no doubt that compositional languages do support generalization, we also found other systems spontaneously arising that generalize without being compositional, at least according to our intuitive measures of compositionality."
2020.acl-main.407.txt,2020,7 Discussion,"while this result should be replicated in different conditions, it suggests that it is dangerous to study the generalization abilities of neural networks in “thought experiment” setups where they are only exposed to a small pool of carefully-crafted examples."
2020.acl-main.408.txt,2020,7 Conclusions and Future Directions,eraser is intended to facilitate progress on explainable models for nlp.
2020.acl-main.408.txt,2020,7 Conclusions and Future Directions,"it also serves as an ideal starting point for several future directions such as better evaluation metrics for interpretability, causal analysis of nlp models and datasets of rationales in other languages."
2020.acl-main.408.txt,2020,7 Conclusions and Future Directions,"our hope is that eraser enables future work on designing more interpretable nlp models, and comparing their relative strengths across a variety of tasks, datasets, and desired criteria."
2020.acl-main.408.txt,2020,7 Conclusions and Future Directions,"this comprises seven datasets, all of which include both instance level labels and corresponding supporting snippets (‘rationales’) marked by human annotators."
2020.acl-main.408.txt,2020,7 Conclusions and Future Directions,"we believe these metrics provide reasonable means of comparison of specific aspects of interpretability, but we view the problem of measuring faithfulness, in particular, a topic ripe for additional research (which eraser can facilitate)."
2020.acl-main.408.txt,2020,7 Conclusions and Future Directions,"we have augmented many of these datasets with additional annotations, and converted them into a standard format comprising inputs, rationales, and outputs."
2020.acl-main.408.txt,2020,7 Conclusions and Future Directions,we have introduced a new publicly available resource: the evaluating rationales and simple english reasoning (eraser) benchmark.
2020.acl-main.408.txt,2020,7 Conclusions and Future Directions,"we proposed several metrics intended to measure the quality of rationales extracted by models, both in terms of agreement with human annotations, and in terms of ‘faithfulness’."
2020.acl-main.409.txt,2020,9 Conclusions,"additionally, we found fresh rationales to be at least as plausible to human users as comparable end-to-end methods."
2020.acl-main.409.txt,2020,9 Conclusions,fresh performs discrete rationale selection and ensures the faithfulness of provided explanations — regardless of the complexity of the individual components — by using independent extraction and prediction modules.
2020.acl-main.409.txt,2020,9 Conclusions,"further, we accomplish this without recourse to explicit rationale-level supervision such as reinforce or the reparameterization trick; this greatly simplifies training."
2020.acl-main.409.txt,2020,9 Conclusions,here we have considered explainability as an instance-specific procedure.
2020.acl-main.409.txt,2020,9 Conclusions,"in addition, while we do have a guarantee under our model about which part of the document was used to inform a given classification, this approach cannot readily say why this specific rationale was selected in the first place."
2020.acl-main.409.txt,2020,9 Conclusions,nor do we clearly understand how the pred uses extracted rationale to perform its classification.
2020.acl-main.409.txt,2020,9 Conclusions,our framework does not currently support further pruning (or expanding) this token set once the rationale has been selected.
2020.acl-main.409.txt,2020,9 Conclusions,"our method can be used with any feature importance metric, is very simple to implement and train, and empirically often outperforms more complex rationalized models."
2020.acl-main.409.txt,2020,9 Conclusions,the final explanation provided by the model is limited to the tokens provided by the extraction method.
2020.acl-main.409.txt,2020,9 Conclusions,"this allows for contextualized models such as transformers to be used, without sacrificing explainability (at least at the level of rationales)."
2020.acl-main.409.txt,2020,9 Conclusions,we acknowledge some important limitations of this work.
2020.acl-main.409.txt,2020,9 Conclusions,"we have proposed faithful rationale extraction from saliency thresholding (fresh), a simple, flexible, and effective method to learn explainable neural models for nlp."
2020.acl-main.409.txt,2020,9 Conclusions,"we showed empirically that fresh outperforms existing models, recovering most of the performance of the original ‘black-box’ model."
2020.acl-main.409.txt,2020,9 Conclusions,we view these as interesting directions for future work.
2020.acl-main.41.txt,2020,6 Conclusion,"empirical results on the medium- and large-scale benchmarks confirm the generalizability and usability of the proposed method, which provides a significant performance boost and training speedup for nmt."
2020.acl-main.41.txt,2020,6 Conclusion,"we have proposed a novel norm-based curriculum learning method for nmt by: 1) a novel sentence difficulty criterion, consisting of linguistically motivated features and learning-dependent features; 2) a novel model competence criterion enabling a fully automatic learning framework without the need for a task-dependent setting of a feature; and 3) a novel sentence weight, alleviating any bias in the objective function and further improving the representation learning."
2020.acl-main.410.txt,2020,7 Conclusion,"moreover, questions that involve complex relations and are across different domains should be included, and then more advanced external knowledge incorporation methods as well as domain adaptation methods can be carefully designed and systematically evaluated."
2020.acl-main.410.txt,2020,7 Conclusion,our qualitative and quantitative analysis as well as exploration of the two desired aspects of clinirc systems show that future clinical qa datasets should not only be large-scale but also less noisy and more diverse.
2020.acl-main.410.txt,2020,7 Conclusion,we study the clinical reading comprehension (clinirc) task with the recently created emrqa dataset.
2020.acl-main.411.txt,2020,6 Conclusion,a key benefit of the model is that its architecture remains largely the same as the original model which allows us to avoid repeating pretraining and use the original model weights for finetuning.
2020.acl-main.411.txt,2020,6 Conclusion,"in this work, we showed that modeling such large contexts may not always be necessary."
2020.acl-main.411.txt,2020,6 Conclusion,the distillation techniques further reduce the performance gap with respect to the original model.
2020.acl-main.411.txt,2020,6 Conclusion,this decomposition model provides a simple yet strong starting point for efficient qa models as nlp moves towards increasingly larger models handling wider contexts.
2020.acl-main.411.txt,2020,6 Conclusion,this however imposes a significant complexity cost.
2020.acl-main.411.txt,2020,6 Conclusion,transformers have improved the effectiveness of nlp tools by their ability to incorporate large contexts effectively in multiple layers.
2020.acl-main.411.txt,2020,6 Conclusion,"we build a decomposition of the transformer model that provides substantial improvements in inference speed, memory reduction, while retaining most of the original model’s accuracy."
2020.acl-main.412.txt,2020,6 Conclusion,"embedkgqa achieves state-of-the-art performance in multiple kgqa settings, suggesting that the link prediction properties of kg embeddings can be utilized to mitigate the kg incompleteness problem in multi-hop kgqa."
2020.acl-main.412.txt,2020,6 Conclusion,embedkgqa also overcomes the shortcomings due to limited neighborhood size constraint imposed by existing multi-hop kgqa methods.
2020.acl-main.412.txt,2020,6 Conclusion,embedkgqa utilizes the link prediction properties of kg embeddings to mitigate the kg incompleteness problem without using any additional data.
2020.acl-main.412.txt,2020,6 Conclusion,"however, the availability of a relevant text corpus is often limited, thereby reducing broad-coverage applicability of such methods."
2020.acl-main.412.txt,2020,6 Conclusion,"in a separate line of research, kg embedding methods have been proposed to reduce kg sparsity by performing missing link prediction."
2020.acl-main.412.txt,2020,6 Conclusion,"in this paper, we propose embedkgqa, a novel method for multi-hop kgqa."
2020.acl-main.412.txt,2020,6 Conclusion,"it trains the kg entity embeddings and uses it to learn question embeddings, and during the evaluation, it scores (head entity, question) pair again all entities, and the highest-scoring entity is selected as an answer."
2020.acl-main.412.txt,2020,6 Conclusion,kgs are often incomplete and sparse which poses additional challenges for multi-hop kgqa methods.
2020.acl-main.412.txt,2020,6 Conclusion,recent recent for this problem have tried to address the incompleteness problem by utilizing an additional text corpus.
2020.acl-main.413.txt,2020,4 Conclusion,"a simple template-based approach achieves state-of-the-art results for unsupervised methods on the squad dataset of 64.04 f1, and 77.55 f1 when the answer is a named entity."
2020.acl-main.413.txt,2020,4 Conclusion,in this paper we introduce a retrieval-based approach to unsupervised extractive question answering.
2020.acl-main.413.txt,2020,4 Conclusion,"we aim to experiment with other datasets and other domains, incorporate our synthetic data in a semi-supervised setting and test the feasibility of our framework in a multi-lingual setting."
2020.acl-main.413.txt,2020,4 Conclusion,we analyze the effect of several components in our template-based approaches through ablation studies.
2020.acl-main.414.txt,2020,6 Conclusion,"further, we show that considerable improvements can be obtained by aggregating knowledge from parallel evidence chains retrieved by our method."
2020.acl-main.414.txt,2020,6 Conclusion,"in addition of improving qa, we hypothesize that these simple unsupervised components of air will benefit future work on supervised neural iterative retrieval approaches by improving their query reformulation algorithms and termination criteria."
2020.acl-main.414.txt,2020,6 Conclusion,"our approach combines three ideas: (a) an unsupervised alignment approach to soft-align questions and answers with justification sentences using glove embeddings, (b) an iterative process that reformulates queries focusing on terms that are not covered by existing justifications, and (c) a simple stopping condition that concludes the iterative process when all terms in the given question and candidate answers are covered by the retrieved justifications."
2020.acl-main.414.txt,2020,6 Conclusion,"overall, despite its simplicity, unsupervised nature, and its sole reliance on glove embeddings, our approach outperforms all previous methods (including supervised ones) on the evidence selection task on two datasets: multirc and qasc."
2020.acl-main.414.txt,2020,6 Conclusion,"we introduced a simple, unsupervised approach for evidence retrieval for question answering."
2020.acl-main.414.txt,2020,6 Conclusion,"when these evidence sentences are fed into a roberta answer classification component, we achieve the best qa performance on these two datasets."
2020.acl-main.415.txt,2020,6 Conclusion,"nonetheless, we hope that directly releasing our alignments and token-level features enables greater research accessibility in this area."
2020.acl-main.415.txt,2020,6 Conclusion,"voxclamantis v1.0 is the first large-scale corpus for phonetic typology, with extracted phonetic features for 635 typologically diverse languages."
2020.acl-main.415.txt,2020,6 Conclusion,we discuss several caveats for the use of this corpus and areas for substantial improvement.
2020.acl-main.415.txt,2020,6 Conclusion,we hope this corpus will motivate and enable further developments in both phonetic typology and methodology for working with cross-linguistic speech corpora.
2020.acl-main.415.txt,2020,6 Conclusion,we present two case studies illustrating both the research potential and limitations of this corpus for investigation of phonetic typology at a large scale.
2020.acl-main.416.txt,2020,5 Conclusions,dscorer allows to speed up model selection and development removing the bottleneck of evaluation time.
2020.acl-main.416.txt,2020,5 Conclusions,"in this work we proposed dscorer, as a drs evaluation metric alternative to counter."
2020.acl-main.416.txt,2020,5 Conclusions,our metric is significantly more efficient than counter and considers high-order drss.
2020.acl-main.417.txt,2020,11 Conclusions,"each of the processing steps we describe here still have great potential for improvement, and we hope that our work contributes to the development of novel methods both in terms of better processing of raw parallel data sources, but also increasing the robustness of neural machine translation training when faced with noisy data."
2020.acl-main.417.txt,2020,11 Conclusions,"going beyond providing data, the goals of this project include the creation of publicly available infrastructure to explore new research directions on parallel corpus mining by releasing open source code for the entire pipeline and public benchmarks for individual processing steps."
2020.acl-main.417.txt,2020,11 Conclusions,we are especially interested in further extending this work into low resource languages where resources tend to be noisier and underlying models to support data mining less reliable.
2020.acl-main.417.txt,2020,11 Conclusions,we released the largest publicly available parallel corpora for many language pairs and demonstrated their benefit to train machine translation systems.
2020.acl-main.418.txt,2020,6 Discussion and Moving Forward,"additionally, our ability to collect naturally occurring data was limited because many sources simply do not yet permit (or have only recently permitted) the use of gender inclusive language in their articles."
2020.acl-main.418.txt,2020,6 Discussion and Moving Forward,"both datasets show significant gaps in system performance, but perhaps moreso, show that taking crowdworker judgments as “gold standard” can be problematic."
2020.acl-main.418.txt,2020,6 Discussion and Moving Forward,"finally, because the social construct of gender is fundamentally contested, some of our results may apply only under some frameworks."
2020.acl-main.418.txt,2020,6 Discussion and Moving Forward,"in particular, the gender taxonomy we presented, while not novel, is (to our knowledge) previously unattested in discussions around gender bias in nlp systems; we hope future work in this area can draw on these ideas."
2020.acl-main.418.txt,2020,6 Discussion and Moving Forward,"it may be the case that to truly build gender inclusive datasets and systems, we need to hire or consult experiential experts (patton et al., 2019; young et al., 2019)."
2020.acl-main.418.txt,2020,6 Discussion and Moving Forward,"more broadly, we found that trans-exclusionary assumptions around gender in nlp papers is made commonly (and implicitly), a practice that we hope to see change in the future because it fundamentally limits the applicability of nlp systems."
2020.acl-main.418.txt,2020,6 Discussion and Moving Forward,"moreover, although we studied crowdworkers on mechanical turk (because they are often employed as annotators for nlp resources), if other populations are used for annotation, it becomes important to consider their positionality and how that may impact annotations."
2020.acl-main.418.txt,2020,6 Discussion and Moving Forward,"moreover, our ability to evaluate coreference systems with data that includes incorrect references was limited as well, because current systems do not mark any forms of misgendering or deadnaming explicitly, and current metrics do not take this into account."
2020.acl-main.418.txt,2020,6 Discussion and Moving Forward,"one in particular is a largely western bias, both in terms of what models of gender we use and also in terms of the data we annotated."
2020.acl-main.418.txt,2020,6 Discussion and Moving Forward,"our goal in this paper was to analyze how gender bias exist in coreference resolution annotations and models, with a particular focus on how it may fail to adequately process text involving binary and non-binary trans referents."
2020.acl-main.418.txt,2020,6 Discussion and Moving Forward,the primary limitation of our study and analysis is that it is limited to english.
2020.acl-main.418.txt,2020,6 Discussion and Moving Forward,"this echoes a related finding in annotation of hate-speech that annotator positionality matters (olteanu et al., 2019)."
2020.acl-main.418.txt,2020,6 Discussion and Moving Forward,"this is particularly limiting because english lacks a grammatical gender system, and some extensions of our work to languages with grammatical gender are non-trivial."
2020.acl-main.418.txt,2020,6 Discussion and Moving Forward,"this led us to counterfactual text manipulation, which, while useful, is essentially impossible to do flawlessly."
2020.acl-main.418.txt,2020,6 Discussion and Moving Forward,"we also emphasize that while we endeavored to be inclusive, our own positionality has undoubtedly led to other biases."
2020.acl-main.418.txt,2020,6 Discussion and Moving Forward,we also hope that developers of datasets or systems can use some of our analysis as inspiration for how one can attempt to measure—and then root out—different forms of bias in coreference resolution systems and nlp systems more broadly.
2020.acl-main.418.txt,2020,6 Discussion and Moving Forward,"we have attempted to partially compensate for this bias by intentionally including documents with non-western non-binary expressions of gender in the gicoref dataset16, but the dataset nonetheless remains western-dominant."
2020.acl-main.418.txt,2020,6 Discussion and Moving Forward,we hope this paper can serve as a roadmap for future studies.
2020.acl-main.418.txt,2020,6 Discussion and Moving Forward,we thus created two datasets: map and gicoref.
2020.acl-main.419.txt,2020,8 Conclusion,"our research not only results in insights into significant similarities between bidirectional rnns and human attention, but also opens the avenue for promising future research directions."
2020.acl-main.419.txt,2020,8 Conclusion,this human attention dataset represents a valuable community resource that we then leverage for quantifying similarities between human and attention-based neural network models using novel attention-map similarity metrics.
2020.acl-main.419.txt,2020,8 Conclusion,"to gain a deeper understanding of the relationships between human and attention-based neural network models, we conduct a large crowd-sourcing study to collect human attention maps for text classification."
2020.acl-main.42.txt,2020,6 Conclusions,we also defined two metrics for revision-enabled simultaneous translation for the first time.
2020.acl-main.42.txt,2020,6 Conclusions,we have proposed an opportunistic decoding timely correction technique which improves the latency and quality for simultaneous translation.
2020.acl-main.420.txt,2020,7 Conclusion,"although it does encode a large amount of information about syntax—more than 76% and 65%, respectively, about pos and dependency labels in all languages9—bert only encodes at most 12% more information than a simple baseline (a type-level representation)."
2020.acl-main.420.txt,2020,7 Conclusion,"keeping this in mind, we suggest a change of focus—instead of concentrating on probe size or information, we should pursue ease of extraction going forward."
2020.acl-main.420.txt,2020,7 Conclusion,"on a final note, we apply our formalization to evaluate multilingual bert’s syntactic knowledge on a set of eleven typologically diverse languages."
2020.acl-main.420.txt,2020,7 Conclusion,"on pos labeling, more specifically, our mi estimates based on bert are higher than the control in less than half of the analyzed languages."
2020.acl-main.420.txt,2020,7 Conclusion,this indicates that word-level pos labeling may not be ideal for contemplating the syntax contained in contextual word embeddings.
2020.acl-main.420.txt,2020,7 Conclusion,"we further explored our operationalization and showed that, given perfect probes, probing can only yield insights into the language itself and cannot tell us anything about the representations under investigation."
2020.acl-main.420.txt,2020,7 Conclusion,"we introduce control functions, which put in context our mutual information estimates—how much more informative are contextual representations than some knowledge judged to be trivial?"
2020.acl-main.420.txt,2020,7 Conclusion,we propose an information-theoretic operationalization of probing that defines it as the task of estimating conditional mutual information.
2020.acl-main.421.txt,2020,7 Conclusions,our results and analysis contradict previous theories and provide new insights into the basis of the generalization abilities of multi-lingual models.
2020.acl-main.421.txt,2020,7 Conclusions,"to provide a more comprehensive benchmark to evaluate cross-lingual models, we also released the cross-lingual question answering dataset (xquad)."
2020.acl-main.421.txt,2020,7 Conclusions,we also showed that a monolingual model trained on a particular language learns some semantic abstractions that are generalizable to other languages in a series of probing experiments.
2020.acl-main.421.txt,2020,7 Conclusions,we compared state-of-the-art multilingual representation learning models and a monolingual model that is transferred to new languages at the lexical level.
2020.acl-main.421.txt,2020,7 Conclusions,"we demonstrated that these models perform comparably on standard zero-shot cross-lingual transfer benchmarks, indicating that neither a shared vocabulary nor joint pre-training are necessary in multilingual models."
2020.acl-main.422.txt,2020,7 Conclusion,"an exciting direction for future work is to combine the two approaches in order to identify which linguistic properties are captured in model components that are similar to one another, or explicate how localization of information contributes to the learnability of particular properties."
2020.acl-main.422.txt,2020,7 Conclusion,"comparing finetuned and pre-trained models, we found that higher layers are more affected by fine-tuning in their representations and attention weights, and become less localized."
2020.acl-main.422.txt,2020,7 Conclusion,"finally, the similarity analysis may also help improve model efficiency, for instance by pointing to components that do not change much during fine-tuning and can thus be pruned."
2020.acl-main.422.txt,2020,7 Conclusion,"in this work, we analyzed various prominent contextual word representations from the perspective of similarity analysis."
2020.acl-main.422.txt,2020,7 Conclusion,"it may be insightful to compare the results of our analysis to the loss surfaces of the same models, especially before and after fine-tuning (hao et al., 2019)."
2020.acl-main.422.txt,2020,7 Conclusion,one could also study whether a high similarity entail that two models converged to a similar solution.
2020.acl-main.422.txt,2020,7 Conclusion,our approach is complementary to the linguistic analysis of models via probing classifiers.
2020.acl-main.422.txt,2020,7 Conclusion,"our localization score can also be compared to other aspects of neural representations, such as gradient distributions and their relation to memorization/generalization (arpit et al., 2017)."
2020.acl-main.422.txt,2020,7 Conclusion,"these findings motivated experimenting with layer-selective fine-tuning, where we were able to obtain good performance while freezing the lower layers and only fine-tuning the top ones."
2020.acl-main.422.txt,2020,7 Conclusion,we also observed that higher layers are more localized than lower ones.
2020.acl-main.422.txt,2020,7 Conclusion,"we compared different layers of pre-trained models using both localized and distributed measures of similarity, at neuron, representation, and attention levels."
2020.acl-main.422.txt,2020,7 Conclusion,"we found that different architectures often have similar internal representations, but differ at the level of individual neurons."
2020.acl-main.423.txt,2020,6 Conclusion,"our work indicates that semantic signals extending beyond the lexical level can be similarly introduced at the pre-training stage, allowing the network to elicit further insight without human supervision."
2020.acl-main.423.txt,2020,6 Conclusion,"this improvement was obtained without human annotation, but rather by harnessing an external linguistic knowledge source."
2020.acl-main.423.txt,2020,6 Conclusion,"this results in a boosted word-level semantic awareness of the resultant model, named sensebert, which considerably outperforms a vanilla bert on a semeval based supersense disambiguation task and achieves state of the art results on the word in context task."
2020.acl-main.423.txt,2020,6 Conclusion,we introduce lexical semantic information into a neural language model’s pre-training objective.
2020.acl-main.424.txt,2020,7 Conclusion,"finally, we hope that asset’s multi-transformation features will motivate the development of ss models that benefit a variety of target audiences according to their specific needs such as people with low literacy or cognitive disabilities."
2020.acl-main.424.txt,2020,7 Conclusion,"furthermore, we have motivated the need to develop new metrics for automatic evaluation of ss models, especially when evaluating simplifications with multiple rewriting operations."
2020.acl-main.424.txt,2020,7 Conclusion,"simplifications in asset were crowdsourced, and annotators were instructed to apply multiple rewriting transformations."
2020.acl-main.424.txt,2020,7 Conclusion,"this improves current publicly-available evaluation datasets, which are focused on only one type of transformation."
2020.acl-main.424.txt,2020,7 Conclusion,"through several experiments, we have shown that asset contains simplifications that are more abstractive, and that are consider simpler than those in other evaluation corpora."
2020.acl-main.424.txt,2020,7 Conclusion,"we have introduced asset, a new dataset for tuning and evaluation of ss models."
2020.acl-main.425.txt,2020,6 Conclusions,babelpic is innovative in being the first dataset with a focus on nominal and verbal non-concrete concepts linked to the wordnet and babelnet lexical knowledge bases.
2020.acl-main.425.txt,2020,6 Conclusions,"furthermore, we presented a methodology to extend the resource by fine-tuning vlp, a state-of-the-art pre-trained language-vision architecture."
2020.acl-main.425.txt,2020,6 Conclusions,"in our approach, we automatically verify the synset-image associations by exploiting the natural language definitions in wordnet, showing strong results on zero-shot classification as well."
2020.acl-main.425.txt,2020,6 Conclusions,"in this work we introduced babelpic, a new resource for language-vision tasks, built by validating the existing image-to-synset associations in the babelnet resource."
2020.acl-main.425.txt,2020,6 Conclusions,"we exploited our method for the automatic generation of a widecoverage silver dataset containing around 10,013 synsets."
2020.acl-main.425.txt,2020,6 Conclusions,we make babelpic (both gold and silver data) available to the community for download at http://babelpic.org.
2020.acl-main.426.txt,2020,8 Conclusions,"as with many tasks, bert provided additional context, but our integration of these label semantics showed significant improvements."
2020.acl-main.426.txt,2020,8 Conclusions,"further, we showed that modeling the class labels as semantic embeddings helped to learn better representations with more meaningful predictions."
2020.acl-main.426.txt,2020,8 Conclusions,the multi-label nature of emotion prediction lends itself naturally to use the correlations between the labels themselves.
2020.acl-main.426.txt,2020,8 Conclusions,we believe these models can improve many other nlp tasks where the class labels carry inherent semantic meaning in their names.
2020.acl-main.426.txt,2020,8 Conclusions,"we present new results for the multi-label emotion classification task of rashkin et al.(2018), extending previous reported results by 10.7 f1 points (55.1 to 65.8)."
2020.acl-main.427.txt,2020,7 Conclusion,"consistent with recent works, we find that bert pre-trained models do better than models trained from scratch, but there is much space for improvement."
2020.acl-main.427.txt,2020,7 Conclusion,"finally, we showed the results of using this new dataset to train several neural models for parsing natural language instructions."
2020.acl-main.427.txt,2020,7 Conclusion,"in this work, we have described a grammar over a mid-level interface for a minecraft assistant."
2020.acl-main.427.txt,2020,7 Conclusion,"the code, dataset and annotation tools described in the paper have been open-sourced 5."
2020.acl-main.427.txt,2020,7 Conclusion,"we believe this data will be useful to researchers studying semantic parsing, especially interactive semantic parsing, human-robot interaction, and even imitation and reinforcement learning."
2020.acl-main.427.txt,2020,7 Conclusion,we then discussed the creation of a dataset of natural language utterances with associated logical forms over this grammar that can be executed in-game.
2020.acl-main.428.txt,2020,5 Conclusion,"further, utilizing supervised datasets with labeled coherent and incoherent utterances and applying unlikelihood yields measurably improved levels of coherence with respect to the aspect measured, in this case contradiction."
2020.acl-main.428.txt,2020,5 Conclusion,"future work could apply this same technique with other supervised data, e.g.correcting causal or commonsense reasoning errors (zellers et al., 2019; qin et al., 2019)."
2020.acl-main.428.txt,2020,5 Conclusion,generating consistent and coherent human-like dialogue is a core goal of natural language research.
2020.acl-main.428.txt,2020,5 Conclusion,"our method defines objective functions under the umbrella of unlikelihood: during training, we wish to make inconsistent dialogue unlikely by lowering the probability of such events occurring."
2020.acl-main.428.txt,2020,5 Conclusion,"this makes generative models repeat themselves less, copy the context less, and use more rare words from the vocabulary – closer to matching human statistics."
2020.acl-main.428.txt,2020,5 Conclusion,"we studied several aspects that contribute to that goal, defined metrics to measure them, and proposed algorithms that improve them, mitigating some of the failings of maximum likelihood training, the current dominant approach."
2020.acl-main.429.txt,2020,7 Conclusion,"in this paper, we propose a basic procedure and analysis methods that take a hypothesis of how a transformer-based model might encode a linguistic phenomenon, and test the validity of that hypothesis based on unsupervised probes, downstream control tasks, and measurement of cross-dataset consistency."
2020.acl-main.429.txt,2020,7 Conclusion,"we hypothesize an interpretable encoding of negation scope, where in-scope words attend to the negation cue, and find evidence of such an encoding in bert-base and roberta-base."
2020.acl-main.43.txt,2020,6 Conclusion,"based on the hierarchy, we formally distinguish the state expressiveness of the non-rational s-lstm and its rational counterpart, the s-qrnn."
2020.acl-main.43.txt,2020,6 Conclusion,"moreover, the hierarchy translates to differences in language recognition capabilities."
2020.acl-main.43.txt,2020,6 Conclusion,"strengthening the decoder alleviates some, but not all, of these differences."
2020.acl-main.43.txt,2020,6 Conclusion,these results further our understanding of the inner working of nlp systems.
2020.acl-main.43.txt,2020,6 Conclusion,we conclude with an analysis that shows that an rnn architecture’s strength must also take into account its space complexity.
2020.acl-main.43.txt,2020,6 Conclusion,"we develop a hierarchy of saturated rnn encoders, considering two angles: space complexity and rational recurrence."
2020.acl-main.43.txt,2020,6 Conclusion,we hope they will guide the development of more expressive rational rnns.
2020.acl-main.43.txt,2020,6 Conclusion,"we present two languages, both recognizable by an lstm."
2020.acl-main.43.txt,2020,6 Conclusion,we show further distinctions in state expressiveness based on encoder space complexity.
2020.acl-main.43.txt,2020,6 Conclusion,"we show that one can be recognized by an s-qrnn only with the help of a decoder, and that the other cannot be recognized by an s-qrnn with the help of any decoder."
2020.acl-main.43.txt,2020,6 Conclusion,"while this means existing rational rnns are fundamentally limited compared to lstms, we find that it is not necessarily being rationally recurrent that limits them: in fact, we prove that a wfa can perfectly encode its input—something no saturated rnn can do."
2020.acl-main.430.txt,2020,5 Conclusions,acknowledgement this work was developed with the support of nsf grant cns-1704845 as well as by darpa and the air force research laboratory under agreement number fa8750-152-0277.
2020.acl-main.430.txt,2020,5 Conclusions,"investigating longer contexts, the diminishing dominance of the primary path, and the requisite algorithmic scalability requirements are elements of our ongoing work."
2020.acl-main.430.txt,2020,5 Conclusions,node-based compression results further corroborate these conclusions.
2020.acl-main.430.txt,2020,5 Conclusions,the combination of finely-tuned attribution and gradient decomposition lets us investigate the handling of the grammatical number agreement concept attributed to paths across lstm components.
2020.acl-main.430.txt,2020,5 Conclusions,"the concentration of attribution to a primary path and two primary cell state neurons and its persistence in a variety of short-term and long-term contexts, even with confounding attractors, demonstrates that the concept’s handling is, to a large degree, general and localized."
2020.acl-main.430.txt,2020,5 Conclusions,the u.s. government is authorized to reproduce and distribute reprints for governmental purposes not withstanding any copyright notation thereon.
2020.acl-main.430.txt,2020,5 Conclusions,"the views, opinions, and/or findings expressed are those of the author(s) and should not be interpreted as representing the official views or policies of darpa, the air force research laboratory, the national science foundation, or the u.s. government."
2020.acl-main.430.txt,2020,5 Conclusions,"though our present datasets do not pose computational problems, the number of paths, at both the neuron and the gate level, is exponential with respect to context size."
2020.acl-main.430.txt,2020,5 Conclusions,"though the heuristic decisioning aspect of an lstm is present in the large quantities of paths with non-zero influence, their overall contribution to the concept is insignificant as compared to the primary path."
2020.acl-main.430.txt,2020,5 Conclusions,"we also note that our method can be expanded to explore number agreement in more complicated sentences with clausal structures, or other syntactic/semantic signals such as coreference or gender agreement."
2020.acl-main.430.txt,2020,5 Conclusions,we gratefully acknowledge the support of nvidia corporation with the donation of the titan v gpu used for this work.
2020.acl-main.430.txt,2020,5 Conclusions,"we note, however, that our results are based on datasets exercising the agreement concept in contexts of a limited size."
2020.acl-main.430.txt,2020,5 Conclusions,"we speculate that the primary path’s attribution diminishes with the length of the context, which would suggest that at some context size, the handling of number will devolve to be mostly heuristic-like with no significant primary paths."
2020.acl-main.431.txt,2020,9 Conclusion,"in this work, we consider how methods developed for analyzing static embeddings can be re-purposed for understanding contextualized representations."
2020.acl-main.431.txt,2020,9 Conclusion,our findings also have significant implications with respect to the reliability of existing protocols for estimating bias in word embeddings.
2020.acl-main.431.txt,2020,9 Conclusion,our large-scale analysis reveals that bias is encoded disparately across different popular pretrained models and different model layers.
2020.acl-main.431.txt,2020,9 Conclusion,"we further study the extent to which various social biases (gender, race, religion) are encoded, employing several different quantification schemas."
2020.acl-main.431.txt,2020,9 Conclusion,we introduce simple and effective procedures for converting from contextualized representations to static word embeddings.
2020.acl-main.431.txt,2020,9 Conclusion,"when applied to pretrained models like bert, we find the resulting embeddings are useful proxies that provide insights into the pretrained model while simultaneously outperforming word2vec and glove substantially under intrinsic evaluation."
2020.acl-main.432.txt,2020,6 Conclusion,"amidst practices that perceive attention scores to be an indication of what the model focuses on, we characterize the manipulability of attention mechanism and the (surprisingly small) cost to be paid for it in accuracy."
2020.acl-main.432.txt,2020,6 Conclusion,"further analysis reveals how the manipulated models cheat, and raises concerns about the potential use of attention as a tool to audit models."
2020.acl-main.432.txt,2020,6 Conclusion,"our simple training scheme produces models with significantly reduced attention mass over tokens known a priori to be useful for prediction, while continuing to use them."
2020.acl-main.433.txt,2020,6 Conclusion,"future work will explore other measures and alternative game settings for the emergence of compositionality, as well as more subtle psychological effects (categeorical perception) of continuous biological systems exhibiting discrete structure, like the auditory system."
2020.acl-main.433.txt,2020,6 Conclusion,we find no evidence of compositional structure using vector analogies and a generalization thereof but do find sharp boundaries between the discrete message clusters.
2020.acl-main.433.txt,2020,6 Conclusion,we propose a general signaling game framework in which fewer a priori assumptions are imposed on the conversational situations.
2020.acl-main.433.txt,2020,6 Conclusion,"we use both production and perception analyses, and find that under appropriate conditions, which are met by most studies involving neural signaling games, messages become discrete without the analyst having to force this property into the language (and having to deal with non-differentiability issues)."
2020.acl-main.434.txt,2020,10 Conclusion,"bidirectional context appears to impact distribution patterns more than depth or architecture, though the transformer models show more robustness to distance."
2020.acl-main.434.txt,2020,10 Conclusion,"distance manipulations indicate that the observed rich contextual encoding is not an artifact of proximity between words, and probing for information about context word identities suggests a weaker encoding of identity information than of more abstract word feature information."
2020.acl-main.434.txt,2020,10 Conclusion,"however, we see substantial variation across encoders in how robustly each information type is distributed to which tokens."
2020.acl-main.434.txt,2020,10 Conclusion,in this paper we have begun to tackle a key question in our understanding of the contextual embeddings on which most current state-of-the-art nlp models are founded: what is it that contextual embeddings pick up about the words in their contexts?
2020.acl-main.434.txt,2020,10 Conclusion,"overall, these results help to clarify the patterns of distribution of context information within contextual embeddings— future work can further clarify the impact of more diverse syntactic relations between words, and of additional types of word features."
2020.acl-main.434.txt,2020,10 Conclusion,"we apply these tests to examine the distribution of contextual information across sentence tokens for popular contextual encoders bert, elmo, and gpt."
2020.acl-main.434.txt,2020,10 Conclusion,"we find that each of the tested word features can be encoded in contextual embeddings for other words of the sentence, often with perfect or nearperfect recoverability."
2020.acl-main.434.txt,2020,10 Conclusion,"we have introduced a novel probing approach and a suite of tasks through which we have performed systematic, fine-grained probing of contextual token embeddings for information about features of their surrounding words."
2020.acl-main.434.txt,2020,10 Conclusion,we make all datasets and code available for additional testing.
2020.acl-main.435.txt,2020,7 Conclusion,each component added to our base model architecture (proposed loss functions and the adoption of dense captions) significantly improves the model’s performance.
2020.acl-main.435.txt,2020,7 Conclusion,"furthermore, we employed dense captions to help the model better find clues from salient regions for answering questions."
2020.acl-main.435.txt,2020,7 Conclusion,"overall, our model outperforms the state-of-the-art models on the tvqa leaderboard, while showing more balanced scores on the diverse tv show genres."
2020.acl-main.435.txt,2020,7 Conclusion,we presented our dual-level attention and frameselection gates model and novel losses for more effective frame-selection.
2020.acl-main.436.txt,2020,6 Discussion,"however, unlike attributes, language is (1) a more natural medium for annotators, (2) does not require preconceived restrictions on the kinds of features relevant to the task, and (3) is abundant in unsupervised forms."
2020.acl-main.436.txt,2020,6 Discussion,lsl outperforms baselines across two tasks and uses language supervision more efficiently than l3.
2020.acl-main.436.txt,2020,6 Discussion,"the line between language and sufficiently rich attributes and rationales is blurry, and recent work (tokmakov et al., 2019) suggests that similar performance gains can likely be observed by regularizing with attributes."
2020.acl-main.436.txt,2020,6 Discussion,this makes shaping representations with language a promising and easily accessible way to improve the generalization of vision models in low-data settings.
2020.acl-main.436.txt,2020,6 Discussion,"we find that if a model is trained to expose the features and abstractions in language, a linguistic bottleneck on top of these language-shaped representations is unnecessary, at least for the kinds of visual tasks explored here."
2020.acl-main.436.txt,2020,6 Discussion,"we presented lsl, a few-shot visual recognition model that is regularized with language descriptions during training."
2020.acl-main.437.txt,2020,9 Conclusion,"our best classification models are able to outperform previous work, and this remains so even when we reembed discrete latents from scratch in the learned classifier."
2020.acl-main.437.txt,2020,9 Conclusion,"we find that amortized hard em is particularly effective in low-resource regimes when reembedding from scratch, and that vq-vae struggles in these settings."
2020.acl-main.437.txt,2020,9 Conclusion,"we have presented experiments comparing the discrete representations learned by a categorical vae, a vq-vae, and hard em in terms of their ability to improve a low-resource text classification system, and to allow for nearest neighbor-based document retrieval."
2020.acl-main.438.txt,2020,8 Conclusions,"instead, it only uses general constraint features as inputs to rectifier networks."
2020.acl-main.438.txt,2020,8 Conclusions,"our approach is particularly suited to tasks where designing constraints manually is hard, and/or the number of training examples is small."
2020.acl-main.438.txt,2020,8 Conclusions,"the learned constraints can be used for structured prediction problems in two ways: (1) combining them with an existing model to improve prediction performance, or (2) incorporating them into the training process to train a better model."
2020.acl-main.438.txt,2020,8 Conclusions,the proposed approach is built upon a novel transformation from two layer rectifier networks to linear inequality constraints and does not rely on domain expertise for any specific problem.
2020.acl-main.438.txt,2020,8 Conclusions,"we demonstrated the effectiveness of our approach on three nlp tasks, each with different original models."
2020.acl-main.438.txt,2020,8 Conclusions,we presented a systematic way for discovering constraints as linear inequalities for structured prediction problems.
2020.acl-main.439.txt,2020,5 Discussion,future work should explore the extent to which our model could further benefit from initializing with stronger models and what computational challenges may arise.
2020.acl-main.439.txt,2020,5 Discussion,"in future work, we hope to better understand how a discourse model can also learn fine-grained relationship types between sentences from unlabeled data."
2020.acl-main.439.txt,2020,5 Discussion,in this paper we present a novel approach to encoding discourse and fine-grained sentence ordering in text with an inter-sentence objective.
2020.acl-main.439.txt,2020,5 Discussion,"our ablation analysis shows that the key architectural aspects of our model are cross attention, an auxiliary mlm objective and a window size that is two or greater."
2020.acl-main.439.txt,2020,5 Discussion,we achieve a new state-of-the-art on the discoeval benchmark and outperform bert-large with a model that has the same number of parameters as bert-base.
2020.acl-main.439.txt,2020,5 Discussion,"we also observe that, on discoeval, our model benefits the most on ordering tasks rather than discourse relation classification tasks."
2020.acl-main.44.txt,2020,4 Conclusion and Future Work,"a more general form, f ∝ ∏k(r + γk)−βk , can be considered for further investigation."
2020.acl-main.44.txt,2020,4 Conclusion and Future Work,the k terms can depict k different proportional coefficients.
2020.acl-main.44.txt,2020,4 Conclusion and Future Work,"this is an explainable extension of several related formulations, with α related to the analytic features of syntax and β + γ to that of morphology."
2020.acl-main.44.txt,2020,4 Conclusion and Future Work,we have shown that f ∝ r−α(r + γ)−β for the rank-frequency relation in natural languages.
2020.acl-main.440.txt,2020,8 Conclusion,"although our dataset focuses on the cooking domain, our framework should generalize to any domain with abundant volumes of unstructured-butalignable multi-modal data."
2020.acl-main.440.txt,2020,8 Conclusion,"diy (do-it-yourself) videos and websites, for instance, are an obvious next target."
2020.acl-main.440.txt,2020,8 Conclusion,"ultimately, we believe this work will further the goal of building agents that can work with human collaborators to carry out complex tasks in the real world."
2020.acl-main.440.txt,2020,8 Conclusion,we also envision extending this work by including audio and video features to enhance the quality of our alignment algorithm.
2020.acl-main.440.txt,2020,8 Conclusion,we introduce a novel two-stage unsupervised algorithm for aligning multiple text and multiple video recipes.
2020.acl-main.440.txt,2020,8 Conclusion,we release a large-scale dataset constructed using this algorithm consisting of joint alignments between multiple text and video recipes along with useful commonsense information such as textual and visual paraphrases; and single-step to multi-step breakdown.
2020.acl-main.440.txt,2020,8 Conclusion,we use an existing algorithm to first learn pairwise alignments and then use a graph-based algorithm to derive the joint alignments across multiple recipes describing the same dish.
2020.acl-main.441.txt,2020,7 Discussion & Conclusion,"a concern might be that the static approach is probably cheaper, since dynamic adversarial data collection requires a verification step to ensure examples are correct."
2020.acl-main.441.txt,2020,7 Discussion & Conclusion,"adversarial nli is meant to be a challenge for measuring nlu progress, even for as yet undiscovered models and architectures."
2020.acl-main.441.txt,2020,7 Discussion & Conclusion,"annotators were employed to act as adversaries, and encouraged to find vulnerabilities that fool the model into misclassifying, but that another person would correctly classify."
2020.acl-main.441.txt,2020,7 Discussion & Conclusion,"for instance, we collected annotatorprovided explanations for each example that the model got wrong."
2020.acl-main.441.txt,2020,7 Discussion & Conclusion,future work could explore a detailed cost and time trade-off between adversarial and static collection.
2020.acl-main.441.txt,2020,7 Discussion & Conclusion,"hamlet was applied against an ensemble of models in rounds 2 and 3, and it would be straightforward to put more diverse ensembles in the loop to examine what happens when annotators are confronted with a wider variety of architectures."
2020.acl-main.441.txt,2020,7 Discussion & Conclusion,"however, verifying examples is probably also a good idea in the static case, and adversarially collected examples can still prove useful even if they didn’t fool the model and weren’t verified."
2020.acl-main.441.txt,2020,7 Discussion & Conclusion,"in this work, we used a human-and-model-in-the-loop training method to collect a new benchmark for natural language understanding."
2020.acl-main.441.txt,2020,7 Discussion & Conclusion,it is important to note that our approach is modelagnostic.
2020.acl-main.441.txt,2020,7 Discussion & Conclusion,it is less clear how the method can be applied in generative cases.
2020.acl-main.441.txt,2020,7 Discussion & Conclusion,"it was carefully constructed to mitigate issues with previous datasets, and was designed from first principles to last longer."
2020.acl-main.441.txt,2020,7 Discussion & Conclusion,"luckily, if the benchmark does turn out to saturate quickly, we will always be able to collect a new round."
2020.acl-main.441.txt,2020,7 Discussion & Conclusion,"moreover, annotators were better incentivized to do a good job in the adversarial setting."
2020.acl-main.441.txt,2020,7 Discussion & Conclusion,our finding that adversarial data is more data-efficient corroborates this theory.
2020.acl-main.441.txt,2020,7 Discussion & Conclusion,the anli benchmark presents a new challenge to the community.
2020.acl-main.441.txt,2020,7 Discussion & Conclusion,the benchmark is designed to be challenging to current state-of-the-art models.
2020.acl-main.441.txt,2020,7 Discussion & Conclusion,the dataset also presents many opportunities for further study.
2020.acl-main.441.txt,2020,7 Discussion & Conclusion,"the proposed procedure can be extended to other classification tasks, as well as to ranking with hard negatives either generated (by adversarial models) or retrieved and verified by humans."
2020.acl-main.441.txt,2020,7 Discussion & Conclusion,training on this new data yielded the state of the art on existing nli benchmarks.
2020.acl-main.441.txt,2020,7 Discussion & Conclusion,"we collected three rounds, and as the rounds progressed, the models became more robust and the test sets for each round became more difficult."
2020.acl-main.441.txt,2020,7 Discussion & Conclusion,"we found that non-expert annotators, in this gamified setting and with appropriate incentives, are remarkably creative at finding and exploiting weaknesses."
2020.acl-main.441.txt,2020,7 Discussion & Conclusion,"we provided inference labels for the development set, opening up possibilities for interesting more fine-grained studies of nli model performance."
2020.acl-main.441.txt,2020,7 Discussion & Conclusion,"while we verified the development and test examples, we did not verify the correctness of each training example, which means there is probably some room for improvement there."
2020.acl-main.442.txt,2020,6 Conclusion,"adopting principles from behavioral testing in software engineering, we propose checklist, a model-agnostic and task-agnostic testing methodology that tests individual capabilities of the model using three different test types."
2020.acl-main.442.txt,2020,6 Conclusion,"checklist is open source, and available at https://github.com/marcotcr/checklist."
2020.acl-main.442.txt,2020,6 Conclusion,"further, checklist reveals critical bugs in commercial systems developed by large software companies, indicating that it complements current practices well."
2020.acl-main.442.txt,2020,6 Conclusion,"more importantly, the abstractions and tools in checklist can be used to collectively create more exhaustive test suites for a variety of tasks."
2020.acl-main.442.txt,2020,6 Conclusion,"our user studies indicate that checklist is easy to learn and use, and helpful both for expert users who have tested their models at length as well as for practitioners with little experience in a task."
2020.acl-main.442.txt,2020,6 Conclusion,"since many tests can be applied across tasks as is (e.g.typos) or with minor variations (e.g.changing names), we expect that collaborative test creation will result in evaluation of nlp models that is much more robust and detailed, beyond just accuracy on held-out data."
2020.acl-main.442.txt,2020,6 Conclusion,"tests created with checklist can be applied to any model, making it easy to incorporate in current benchmarks or evaluation pipelines."
2020.acl-main.442.txt,2020,6 Conclusion,"the tests presented in this paper are part of check-list’s open source release, and can easily be incorporated into existing benchmarks."
2020.acl-main.442.txt,2020,6 Conclusion,"to illustrate its utility, we highlight significant problems at multiple levels in the conceptual nlp pipeline for models that have “solved” existing benchmarks on three different tasks."
2020.acl-main.442.txt,2020,6 Conclusion,"while useful, accuracy on benchmarks is not sufficient for evaluating nlp models."
2020.acl-main.443.txt,2020,7 Conclusion,"furthermore, we investigated the important sub-task of code recognition."
2020.acl-main.443.txt,2020,7 Conclusion,"in this work, we investigated the task of named entity recognition in the social computer programming domain."
2020.acl-main.443.txt,2020,7 Conclusion,our code recognition model captures additional spelling information beyond then contextual word representations and consistently helps to improve the ner performance.
2020.acl-main.443.txt,2020,7 Conclusion,"we also proposed a novel attention based model, named soft-ner, that outperforms the state-of-the-art ner models on this dataset."
2020.acl-main.443.txt,2020,7 Conclusion,"we believe our corpus, stackoverflow-specific bert embeddings and named entity tagger will be useful for various language-and-code tasks, such as code retrieval, software knowledge base extraction and automated question-answering."
2020.acl-main.443.txt,2020,7 Conclusion,"we demonstrate that this new corpus is an ideal benchmark dataset for contextual word representations, as there are many challenging ambiguities that often require long-distance context to resolve."
2020.acl-main.443.txt,2020,7 Conclusion,"we developed a new ner corpus of 15,372 sentences from stackoverflow and 6,510 sentences from github annotated with 20 fine-grained named entities."
2020.acl-main.444.txt,2020,7 Conclusions,"in the future, we are interested in investigating the generality of our defined schema for other comedies and different conversational registers, identifying the temporal intervals when relations are valid (surdeanu, 2013) in a dialogue, and joint dialogue-based information extraction as well as its potential combinations with multimodal signals from images, speech, and videos."
2020.acl-main.444.txt,2020,7 Conclusions,we also design a new metric to evaluate the performance of re methods in a conversational setting and argue that tracking speakers play a critical role in this task.
2020.acl-main.444.txt,2020,7 Conclusions,"we investigate the performance of several re methods, and experimental results demonstrate that a speaker-aware extension on the best-performing model leads to substantial gains in both the standard and conversational settings."
2020.acl-main.444.txt,2020,7 Conclusions,we present the first human-annotated dialogue-based re dataset dialogre.
2020.acl-main.445.txt,2020,6 Conclusion and Future Work,"in the future, we will investigate multi-document summarization datasets such as duc (paul and james, 2004) and tac (dang and owczarzak, 2008) to see whether our findings coincide when multiple references are provided."
2020.acl-main.445.txt,2020,6 Conclusion and Future Work,we also evaluate sentence regression approaches and explore the feasibility of fully-automatic evaluation without any human annotation.
2020.acl-main.445.txt,2020,6 Conclusion and Future Work,"we construct an extractive summarization dataset and demonstrate the effectiveness of facet-aware evaluation on this newly constructed dataset, including better human correlation on the assessment of information coverage, and the support for fine-grained evaluation as well as comparative analysis."
2020.acl-main.445.txt,2020,6 Conclusion and Future Work,we propose a facet-aware evaluation setup for better assessment of information coverage in extractive summarization.
2020.acl-main.445.txt,2020,6 Conclusion and Future Work,we will also explore better sentence regression approaches for the use of both extractive summarization methods and automatic fam creation.
2020.acl-main.446.txt,2020,6 Conclusion,this allows for measurement of the impact of adding each utterance to the corpus.
2020.acl-main.446.txt,2020,6 Conclusion,we define diversity of an utterance compared to the other utterances in a corpus.
2020.acl-main.446.txt,2020,6 Conclusion,"we propose a method, diversity-informed data collection, which leverages this to produce more diverse datasets than the standard approach, and which performs better on downstream tasks."
2020.acl-main.446.txt,2020,6 Conclusion,"working under the same assumption that a subset of participants produce diverse data compared to the corpus, our method can be extended to other diversity measures and can be modified to work with other corpus-level metrics."
2020.acl-main.447.txt,2020,8 Conclusion,cord-19 is aimed at assisting biomedical experts and policy makers process large amounts of covid-19 literature in the search for effective treatments and management policies.
2020.acl-main.447.txt,2020,8 Conclusion,"full text is augmented with sections, citation mentions, and references to tables and figures."
2020.acl-main.447.txt,2020,8 Conclusion,our hope with the release of s2orc is to ensure such text mining resources are available to researchers even beyond periods of global crisis.
2020.acl-main.447.txt,2020,8 Conclusion,"s2orc consists of 81.1m papers, 380.5m resolved citation links, and structured full text from 8.1m open-access pdfs and 1.5m latex source files."
2020.acl-main.447.txt,2020,8 Conclusion,"the pipeline for creating s2orc was used to construct the cord-19 corpus (wang et al., 2020), which saw fervent adoption as the canonical resource for covid-19 text mining."
2020.acl-main.447.txt,2020,8 Conclusion,we aggregate metadata and abstracts from hundreds of trusted sources.
2020.acl-main.447.txt,2020,8 Conclusion,we demonstrate that s2orc can be used effectively for downstream nlp tasks in academic paper analysis.
2020.acl-main.447.txt,2020,8 Conclusion,"we introduce s2orc, the largest publiclyavailable corpus of english-language academic papers covering dozens of academic disciplines."
2020.acl-main.447.txt,2020,8 Conclusion,"with over 75k dataset downloads, dozens of search and question-answering systems, and hundreds of participating teams across two shared tasks19 in the first month of its release, there is little doubt of the resource’s impact."
2020.acl-main.448.txt,2020,6 Conclusion,"a more serious problem, however, is outlier systems, i.e.those systems whose quality is much higher or lower than the rest of the systems."
2020.acl-main.448.txt,2020,6 Conclusion,"although our analysis assumes the direct assessment human evaluation method to be a gold standard despite its shortcomings, our analysis does suggest that the current rule of thumb for publishing empirical improvements based on small bleu differences has little meaning."
2020.acl-main.448.txt,2020,6 Conclusion,"any single number is not adequate to describe the data, and visualising metric scores against human scores is the best way to gain insights into metric reliability."
2020.acl-main.448.txt,2020,6 Conclusion,"chrf, yisi-1 and esim) becomes wider."
2020.acl-main.448.txt,2020,6 Conclusion,"figure 3a) for each language pair, or figure 5, which compresses this information into one graph."
2020.acl-main.448.txt,2020,6 Conclusion,"finally, the same value of correlation coefficient can describe different patterns of errors."
2020.acl-main.448.txt,2020,6 Conclusion,"however, human evaluation must always be the gold standard, and for continuing improvement in translation, to establish significant improvements over prior work, all automatic metrics make for inadequate substitutes."
2020.acl-main.448.txt,2020,6 Conclusion,"in the worst case scenario, outliers introduce a high correlation when there is no association between metric and human scores for the rest of the systems."
2020.acl-main.448.txt,2020,6 Conclusion,"in this paper, we revisited the findings of the metrics task at wmt 2019, which flagged potential problems in the current best practises for assessment of evaluation metrics."
2020.acl-main.448.txt,2020,6 Conclusion,"metrics are commonly used to compare two systems, and accordingly we have also investigated the real meaning encoded by a difference in metric score, in terms of what this indicates about human judgements of the two systems."
2020.acl-main.448.txt,2020,6 Conclusion,"most published work report bleu differences of 1-2 points, however at this level we show this magnitude of difference only corresponds to true improvements in quality as judged by humans about half the time."
2020.acl-main.448.txt,2020,6 Conclusion,"once the outliers are removed, the gap between correlation of bleu and other metrics (e.g."
2020.acl-main.448.txt,2020,6 Conclusion,"overall, this paper adds to the case for retiring bleu as the de facto standard metric, and instead using other metrics such as chrf, yisi-1, or esim in its place."
2020.acl-main.448.txt,2020,6 Conclusion,"pearson’s correlation coefficient is known to be unstable for small sample sizes, particularly when the systems in consideration are very close in quality."
2020.acl-main.448.txt,2020,6 Conclusion,the resulting high values of correlation can then lead to to false confidence in the reliability of metrics.
2020.acl-main.448.txt,2020,6 Conclusion,they are more powerful in assessing empirical improvements.
2020.acl-main.448.txt,2020,6 Conclusion,this could be done with scatter plots (e.g.
2020.acl-main.448.txt,2020,6 Conclusion,"this effect can partly be attributed to noise due to the small sample size, rather than true shortcomings in the metrics themselves."
2020.acl-main.448.txt,2020,6 Conclusion,this goes some way to explaining the findings whereby strong correlations between metric scores and human judgements evaporate when considering small numbers of strong systems.
2020.acl-main.448.txt,2020,6 Conclusion,"thus, future evaluations should also measure correlations after removing outlier systems."
2020.acl-main.448.txt,2020,6 Conclusion,"to summarise, our key recommendations are: • when evaluating metrics, use the technique outlined in section 4.2 to remove outliers before computing pearson’s r. • when evaluating mt systems, stop using bleu or ter for evaluation of mt, and instead use chrf, yisi-1, or esim; • stop using small changes in evaluation metrics as the sole basis to draw important empirical conclusions, and make sure these are supported by manual evaluation."
2020.acl-main.448.txt,2020,6 Conclusion,we found that such systems can have a disproportionate effect on the computed correlation of metrics.
2020.acl-main.448.txt,2020,6 Conclusion,we need better methods to empirically test whether our metrics are less reliable when evaluating high quality mt systems.
2020.acl-main.448.txt,2020,6 Conclusion,"we show that the same can be true for any small set of similar quality systems, not just the top systems."
2020.acl-main.449.txt,2020,5 Conclusion,"in our future work, we want to study the effective incorporation of code structure into the transformer and apply the techniques in other software engineering sequence generation tasks (e.g., commit message generation for source code changes)."
2020.acl-main.449.txt,2020,5 Conclusion,this paper empirically investigates the advantage of using the transformer model for the source code summarization task.
2020.acl-main.449.txt,2020,5 Conclusion,we demonstrate that the transformer with relative position representations and copy attention outperforms state-of-the-art approaches by a large margin.
2020.acl-main.45.txt,2020,6 Conclusion,experimental results show that the proposed loss function help to achieve significant performance boost without changing model architectures.
2020.acl-main.45.txt,2020,6 Conclusion,"in this paper, we propose the dice-based loss to narrow down the gap between training objective and evaluation metrics (f1 score)."
2020.acl-main.450.txt,2020,9 Conclusion,"additionally, incorporating a content selection mechanism to focus the generated questions on salient facts is a promising direction."
2020.acl-main.450.txt,2020,9 Conclusion,"inspecting the generated questions and answers, we identify the transfer ability of qa models and the rigidity of f1 score as a measure of answer similarity as two key performance bottlenecks."
2020.acl-main.450.txt,2020,9 Conclusion,"overall, we believe qags demonstrates the potential of this framework to quantify and incentivize factually consistent text generation."
2020.acl-main.450.txt,2020,9 Conclusion,"qags correlates with human judgments of factuality significantly better than standard automatic evaluation metrics for summarization, and outperforms related nli-based approaches to factual consistency checking."
2020.acl-main.450.txt,2020,9 Conclusion,qags is naturally interpretable: the questions and answers produced in computing qags indicate which tokens in a generated summary are inconsistent and why.
2020.acl-main.450.txt,2020,9 Conclusion,"the framework we present is general, and extending it to other conditional text generation tasks such as image captioning or machine translation is a promising directions."
2020.acl-main.450.txt,2020,9 Conclusion,we expect improvements in either would straightforwardly improve the quality of qags evaluation.
2020.acl-main.450.txt,2020,9 Conclusion,"we introduce a framework for automatically detecting factual inconsistencies in conditionally generated texts and use this framework to develop qags, a metric for measuring inconsistencies in abstractive summarization."
2020.acl-main.451.txt,2020,6 Conclusion,"for future work, we will explore better graph encoding methods, and apply discourse graphs to other tasks that require long document encoding."
2020.acl-main.451.txt,2020,6 Conclusion,"in this paper, we present discobert, which uses discourse unit as the minimal selection basis to reduce summarization redundancy and leverages two types of discourse graphs as inductive bias to capture long-range dependencies among discourse units."
2020.acl-main.451.txt,2020,6 Conclusion,"we validate the proposed approach on two popular summarization datasets, and observe consistent improvement over baseline models."
2020.acl-main.452.txt,2020,6 Conclusion,a hard length constraint is also imposed in our objective function.
2020.acl-main.452.txt,2020,6 Conclusion,"in a controlled experiment, our model achieves better performance than strong baselines on headline generation and duc2004 datasets."
2020.acl-main.452.txt,2020,6 Conclusion,we proposed a novel word-extraction model for sentence summarization that generates summaries by optimizing an objective function of language fluency and semantic similarity.
2020.acl-main.453.txt,2020,6 Discussion and Conclusion,"another issue is that the pre-trained language models are very large and take up a substantial amount of gpu memory, which limits how long the input document can be."
2020.acl-main.453.txt,2020,6 Discussion and Conclusion,"in contrast, we do include sentence context in the pyramid evaluation in order to make the summaries readable for humans and thus, fewer constituents make it into the generated summary for the human evaluation."
2020.acl-main.453.txt,2020,6 Discussion and Conclusion,"in future work, we plan to experiment more with this, examining how we can combine constituents to make fluent sentences without including potentially irrelevant context."
2020.acl-main.453.txt,2020,6 Discussion and Conclusion,"it is also possible that smaller constituents can be matched to phrases within the summary with metrics such as rouge, when they actually should not have counted."
2020.acl-main.453.txt,2020,6 Discussion and Conclusion,the resulting system is the first step towards addressing this task.
2020.acl-main.453.txt,2020,6 Discussion and Conclusion,this could account for the increased score on automated metrics.
2020.acl-main.453.txt,2020,6 Discussion and Conclusion,"we hypothesize that because we use rouge to score summaries of extracted constituents without context, the selected content is packed into the word budget; there is no potentially irrelevant context to count against the system."
2020.acl-main.453.txt,2020,6 Discussion and Conclusion,we present a new challenging task for summarization of novel chapters.
2020.acl-main.453.txt,2020,6 Discussion and Conclusion,"we show that sentencelevel, stable-matched alignment is better than the summary-level alignment used in previous work and our proposed r-wtd method for creating gold extracts is shown to be better than other similarity metrics."
2020.acl-main.453.txt,2020,6 Discussion and Conclusion,"we suspect these models are problematic for our documents because they are, on average, an order of magnitude larger than what was used for pretraining the language model (512 tokens)."
2020.acl-main.453.txt,2020,6 Discussion and Conclusion,"we would also like to further experiment with abstractive summarization to re-examine whether large, pre-trained language models (liu and lapata, 2019) can be improved for our domain."
2020.acl-main.453.txt,2020,6 Discussion and Conclusion,"while both human evaluation and automated metrics concur that summaries produced with our new alignment approach outperform previous approaches, they contradict on the question of whether extraction is better at the constituent or the sentence level."
2020.acl-main.453.txt,2020,6 Discussion and Conclusion,"while truncation of a document may not hurt performance in the news domain due to the heavy lede bias, in our domain, truncation can hurt the performance of the summarizer."
2020.acl-main.454.txt,2020,6 Conclusion,a new inductive bias or additional supervision is needed for learning reliable models.
2020.acl-main.454.txt,2020,6 Conclusion,"the final evaluation should still rely on human annotation or human-in-the-loop methods (chaganty et al., 2018)."
2020.acl-main.454.txt,2020,6 Conclusion,"they are good at copying important source content, but tend to concatenate unrelated spans and hallucinate details when generating more abstractive sentences."
2020.acl-main.454.txt,2020,6 Conclusion,we investigate the faithfulness problem in neural abstractive summarization and propose a qa-based metric for evaluating summary faithfulness.
2020.acl-main.454.txt,2020,6 Conclusion,we show that current models suffer from an inherent trade-off between abstractiveness and faithfulness.
2020.acl-main.454.txt,2020,6 Conclusion,"while our qa-based metric correlates better with human judgment and is useful for model development, it is limited by the quality of the qa model."
2020.acl-main.455.txt,2020,7 Conclusions and Future Work,"in the future, we intend to investigate different meaning representation formalisms, such as amr (banarescu et al., 2013) and dynamic syntax (kempson et al., 2001) and extend to other datasets (e.g.multiplereference summarization) and tasks (e.g.response generation in dialogue)."
2020.acl-main.455.txt,2020,7 Conclusions and Future Work,"our metric is more sensitive to perturbations of the facts in the target summary, which resemble common hallucination phenomena of neural decoders (see figure 2-3 in appendix a for examples)."
2020.acl-main.455.txt,2020,7 Conclusions and Future Work,"using fact representations, we are able to capture semantically similar, but at the same time distant in surface form, content in the summary that aligns with arbitrarily far-apart parts of the input document, casting our metric to be directly interpretable."
2020.acl-main.455.txt,2020,7 Conclusions and Future Work,"we present an automatic evaluation framework for abstractive summarisation, which is low-cost and robust, as it does not rely on expert annotators nor is susceptible to crowdsourcing noise."
2020.acl-main.456.txt,2020,6 Conclusion,"through experiments, we validated our proposed titlestylist can generate more attractive headlines than state-of-the-art hg models."
2020.acl-main.456.txt,2020,6 Conclusion,"to this end, we presented a multitask framework to induce styles into summarization, and proposed the parameters sharing scheme to enhance both summarization and stylization capabilities."
2020.acl-main.456.txt,2020,6 Conclusion,we have proposed a new task of stylistic headline generation (shg) to emphasize explicit control of styles in headline generation for improved attraction.
2020.acl-main.457.txt,2020,8 Conclusion,human evaluation further confirms that our graphaugmented models trained with the cloze reward produce more informative summaries and significantly reduces unfaithful errors.
2020.acl-main.457.txt,2020,8 Conclusion,"in tandem with the graph representation, our cloze reward further improves summary content."
2020.acl-main.457.txt,2020,8 Conclusion,"our models capture both local characteristics and global interactions of entities from the input, thus generating summaries of higher quality."
2020.acl-main.457.txt,2020,8 Conclusion,"we presented a novel knowledge graph-augmented abstractive summarization framework, along with a novel multiple choice cloze reward for reinforcement learning."
2020.acl-main.458.txt,2020,8 Conclusion,in this work we presented a general framework and a training strategy to improve the factual correctness of neural abstractive summarization models.
2020.acl-main.458.txt,2020,8 Conclusion,"our general takeaways include: (1) in a domain with a limited space of facts such as radiology reports, a carefully implemented ie system can be used to improve the factual correctness of neural summarization models via rl; (2) even in the absence of a reliable ie system, optimizing the rouge metrics via rl can substantially improve the factual correctness of the generated summaries."
2020.acl-main.458.txt,2020,8 Conclusion,"we applied this approach to the summarization of radiology reports, and showed its success via both automatic and human evaluation on two separate datasets collected from hospitals."
2020.acl-main.458.txt,2020,8 Conclusion,we hope that our work draws the community’s attention to the factual correctness issue of abstractive summarization models and inspires future work in this direction.
2020.acl-main.459.txt,2020,6 Conclusion and Future Work,dialogue understanding and abstractive summarization remain both important and challenging problems for computational linguistics.
2020.acl-main.459.txt,2020,6 Conclusion and Future Work,"in this paper, we contribute the critical role dungeons and dragons dataset (crd3), a linguistically rich dataset with dialogue extracted from the unscripted, livestreamed show critical role and long, abstractive summaries extracted from the critical role fandom wiki."
2020.acl-main.459.txt,2020,6 Conclusion and Future Work,"we also hope that the dataset can be added to in the future with multi-modal extractions, more granular annotations, and deeper mining of the wiki."
2020.acl-main.459.txt,2020,6 Conclusion and Future Work,"we find current paradigms in summarization modeling to have specific failures in capturing semantics and pragmatics, content selection, rewriting, and evaluation in the domain of long, story-telling dialogue."
2020.acl-main.459.txt,2020,6 Conclusion and Future Work,"we hope crd3 offers useful, unique data for the community to further explore dialogue modeling and summarization."
2020.acl-main.459.txt,2020,6 Conclusion and Future Work,we provide a data augmentation method to help the community start modeling and evaluation for the dialogue summarization task and discuss the initial modeling benchmark results.
2020.acl-main.46.txt,2020,5 Conclusion and Future Work,"an agglutinating morphology probably allows a more straightforward application of the method, by treating affixes as individual elements of the vocabulary."
2020.acl-main.46.txt,2020,5 Conclusion and Future Work,"at present, we are working on an incremental construction of the space of categories."
2020.acl-main.46.txt,2020,5 Conclusion and Future Work,current language models have difficulty acquiring syntactically relevant generalizations for diverse reasons.
2020.acl-main.46.txt,2020,5 Conclusion and Future Work,"in that case, two types of surface realization need to be considered: word order and morphological markers."
2020.acl-main.46.txt,2020,5 Conclusion and Future Work,"in this paper, we proposed a theoretical reformulation for the problem of learning syntactic information from a corpus."
2020.acl-main.46.txt,2020,5 Conclusion and Future Work,it is however possible for linguistic competence - syntax - to emerge from data if we prompt models to establish a distinction between syntactic and contextual (semantic/pragmatic) information.
2020.acl-main.46.txt,2020,5 Conclusion and Future Work,"on the one hand, we observe a natural tendency to lean towards shallow contextual generalizations, likely due to the maximum likelihood training objective."
2020.acl-main.46.txt,2020,5 Conclusion and Future Work,"on the other hand, a corpus is not representative of human linguistic competence but of performance."
2020.acl-main.46.txt,2020,5 Conclusion and Future Work,the adaptation to other types of morphological markers will necessitate more elaborate linguistic reflection.
2020.acl-main.46.txt,2020,5 Conclusion and Future Work,the current formulation of our syntax learning scheme needs adjustments in order to be applicable to real natural language corpora.
2020.acl-main.46.txt,2020,5 Conclusion and Future Work,the immediate one is experimentation.
2020.acl-main.46.txt,2020,5 Conclusion and Future Work,the second direction is towards extending the approach to morphologically rich languages.
2020.acl-main.46.txt,2020,5 Conclusion and Future Work,two orientations can be identified for future work.
2020.acl-main.460.txt,2020,7 Conclusion,in this work we present a new approach to unsupervised abstractive summarization based on maximizing a combination of coverage and fluency for a given length constraint.
2020.acl-main.460.txt,2020,7 Conclusion,"our models attain levels of abstraction closer to human-written summaries, although with more abstraction, more potential for factual inaccuracies arise."
2020.acl-main.460.txt,2020,7 Conclusion,"when tested on common news summarization datasets, our method significantly outperforms previous unsupervised methods, and gets within the range of competitive supervised methods."
2020.acl-main.461.txt,2020,8 Conclusions,"furthermore, human evaluation of the generated summaries (by considering their alignment with the reviews) shows that those created by our model better reflect the content of the input."
2020.acl-main.461.txt,2020,8 Conclusions,"in this work, we presented an abstractive summarizer of opinions, which does not use any summaries in training and is trained end-to-end on a large collection of reviews."
2020.acl-main.461.txt,2020,8 Conclusions,"the model compares favorably to the competitors, especially to the only other unsupervised abstractive multi-review summarization system."
2020.acl-main.462.txt,2020,7 Conclusion,"as the saying goes, the camera doesn’t lie—but it may tell us only a version of the truth."
2020.acl-main.462.txt,2020,7 Conclusion,"but we believe it is both timely and necessary, as language technologies grow in scope and prominence, to seek a more robust treatment of meaning."
2020.acl-main.462.txt,2020,7 Conclusion,"some of the phenomena we have described may seem, at first glance, either too subtle to bother with or too daunting to tackle."
2020.acl-main.462.txt,2020,7 Conclusion,the same goes for language.
2020.acl-main.462.txt,2020,7 Conclusion,we hope that a deeper appreciation of the role of construal in language use will spur progress toward systems that more closely approximate human linguistic intelligence.
2020.acl-main.463.txt,2020,10 Conclusion,"in particular, this paper can be seen as a call for precise language use when talking about the success of current models and for humility in dealing with natural language."
2020.acl-main.463.txt,2020,10 Conclusion,"in this paper, we have argued that in contrast to some current hype, meaning cannot be learned from form alone."
2020.acl-main.463.txt,2020,10 Conclusion,this means that even large language models such as bert do not learn “meaning”; they learn some reflection of meaning into the linguistic form which is very useful in applications.
2020.acl-main.463.txt,2020,10 Conclusion,"we have offered some thoughts on how to maintain a healthy, but not exaggerated, optimism with respect to research that builds upon these lms."
2020.acl-main.463.txt,2020,10 Conclusion,with this we hope to encourage a top-down perspective on our field which we think will help us select the right hill to climb towards human-analogous nlu.
2020.acl-main.464.txt,2020,6 Conclusions,a crucial direction of future work is to develop richer ways of capturing scholarly impact.
2020.acl-main.464.txt,2020,6 Conclusions,"cl journal has the most cited papers, but the citation gap between cl journal and top-tier conferences has reduced in recent years."
2020.acl-main.464.txt,2020,6 Conclusions,"finally, we note again that citations are not an accurate reflection of the quality or importance of individual pieces of work."
2020.acl-main.464.txt,2020,6 Conclusions,"in case of popular shared tasks, the task-description papers and competition-winning system-description papers often receive a considerable number of citations."
2020.acl-main.464.txt,2020,6 Conclusions,"in separate work, we explored the use of the dataset to detect gender disparities in authorship and citations (mohammad, 2020a)."
2020.acl-main.464.txt,2020,6 Conclusions,"on average, long papers get almost three times as many citations as short papers."
2020.acl-main.464.txt,2020,6 Conclusions,so much so that the average number of citations for the shared task papers is higher than the average for non-top-tier conferences.
2020.acl-main.464.txt,2020,6 Conclusions,"the analyses presented here, and the associated dataset of papers mapped to citations, have a number of uses including, understanding how the field is growing and quantifying the impact of different types of papers."
2020.acl-main.464.txt,2020,6 Conclusions,the dataset can potentially also be used to compare patterns of citations in nlp with those in other fields.
2020.acl-main.464.txt,2020,6 Conclusions,"the papers on sentiment classification, anaphora resolution, and entity recognition have the highest median citations."
2020.acl-main.464.txt,2020,6 Conclusions,we extracted citation information for ∼1.1m papers from google scholar profiles of researchers who published at least three papers in the acl anthology.
2020.acl-main.464.txt,2020,6 Conclusions,we showed that only about 56% of the papers are cited ten or more times.
2020.acl-main.464.txt,2020,6 Conclusions,"we used the citation counts of a subset (∼27k papers) to examine patterns of citation across paper types, venues, over time, and across areas of research within nlp."
2020.acl-main.464.txt,2020,6 Conclusions,workshop papers and the shared task papers have higher median and average citations than the non-top-tier conferences.
2020.acl-main.465.txt,2020,6 Conclusion,"human-like inductive biases will improve our models’ ability to learn language structure and new tasks from limited data, and will align the models’ generalization behavior more closely with human expectations, reducing the allure of superficial heuristics that do not follow linguistic structure, and the prevalence of adversarial examples, where changes to the input that are insignificant from a human perspective turn out to affect the network’s behavior in an undesirable way."
2020.acl-main.465.txt,2020,6 Conclusion,"i have described the currently popular pretraining-agnostic identically distributed paradigm, which selects for models that can be trained easily on an unlimited amount of data, and that excel in capturing arbitrary statistical patterns in a fine-tuning data set."
2020.acl-main.465.txt,2020,6 Conclusion,"while such models have considerable value in applications, i have advocated for a parallel evaluation ecosystem—complete with a leaderboard, if one will motivate progress—that will reward models for their ability to generalize in a human-like way."
2020.acl-main.466.txt,2020,5 Conclusion,applying the technology of legalai directly to the legal system will bring ethical issues like gender bias and racial discrimination.
2020.acl-main.466.txt,2020,5 Conclusion,"as a result, we should regard the results of the models only as a reference."
2020.acl-main.466.txt,2020,5 Conclusion,"besides, the three main challenges of legal tasks remain to be solved."
2020.acl-main.466.txt,2020,5 Conclusion,"for example, professionals can spend more time on complex cases and leave the simple cases for the model."
2020.acl-main.466.txt,2020,5 Conclusion,"for tasks that do not yet have a dataset or the datasets are not large enough, we can try to build a large-scale and high-quality dataset or use few-shot or zero-shot methods to solve these problems."
2020.acl-main.466.txt,2020,5 Conclusion,"furthermore, we need to take the ethical issues of legalai seriously."
2020.acl-main.466.txt,2020,5 Conclusion,"however, for safety, these simple cases must still be reviewed."
2020.acl-main.466.txt,2020,5 Conclusion,"in addition to these applications and tasks we have mentioned, there are many other tasks in legalai like legal text summarization and information extraction from legal contracts."
2020.acl-main.466.txt,2020,5 Conclusion,"in general, legalai should play as a supporting role to help the legal system."
2020.acl-main.466.txt,2020,5 Conclusion,"in the future, for these existing tasks, researchers can focus on solving the three most pressing challenges of legalai combining embedding-based and symbol-based methods."
2020.acl-main.466.txt,2020,5 Conclusion,"in this paper, we describe the development status of various legalai tasks and discuss what we can do in the future."
2020.acl-main.466.txt,2020,5 Conclusion,"knowledge modelling, legal reasoning, and interpretability are the foundations on which legalai can reliably serve the legal domain."
2020.acl-main.466.txt,2020,5 Conclusion,"nevertheless, no matter what kind application is, we can apply embedding-based methods for better performance, together with symbol-based methods for more interpretability."
2020.acl-main.466.txt,2020,5 Conclusion,"otherwise, the legal system will no longer be reliable."
2020.acl-main.466.txt,2020,5 Conclusion,"some existing methods are trying to solve these problems, but there is still a long way for researchers to go."
2020.acl-main.466.txt,2020,5 Conclusion,the results given by these methods cannot convince people.
2020.acl-main.466.txt,2020,5 Conclusion,"to address this issue, we must note that the goal of legalai is not replacing the legal professionals but helping their work."
2020.acl-main.467.txt,2020,6 Conclusion and Future Work,future work in this area would benefit greatly from improvements to both the breadth and depth of available probing tasks.
2020.acl-main.467.txt,2020,6 Conclusion and Future Work,"however, it is difficult to draw definite conclusions about the specific skills that drive positive transfer."
2020.acl-main.467.txt,2020,6 Conclusion and Future Work,"intermediate-task training may help improve the handling of syntax, but there is little to no correlation between target-task and probing-task performance for these skills."
2020.acl-main.467.txt,2020,6 Conclusion and Future Work,"looking to our probing analysis, intermediate tasks that help roberta improve across the board show the most positive transfer in downstream tasks."
2020.acl-main.467.txt,2020,6 Conclusion and Future Work,"most directly, we observe that tasks like cosmos qa and hellaswag, which require complex reasoning and inference, tend to work best as intermediate tasks."
2020.acl-main.467.txt,2020,6 Conclusion and Future Work,our results therefore suggest a need for further work on efficient transfer learning mechanisms.
2020.acl-main.467.txt,2020,6 Conclusion and Future Work,"probes for higherlevel semantic abilities tend to have a higher correlation with the target-task performance, but these results are too diffuse to yield more specific conclusions."
2020.acl-main.467.txt,2020,6 Conclusion and Future Work,this paper presents a large-scale study on when and why intermediate-task training works with pretrained models.
2020.acl-main.467.txt,2020,6 Conclusion and Future Work,"thus, the results of our intermediate-task training analysis may be driven in part by forgetting of knowledge acquired during pretraining."
2020.acl-main.467.txt,2020,6 Conclusion and Future Work,we also observe a worryingly high correlation between target-task performance and the two probing tasks which most closely resemble roberta’s masked language modeling pretraining objective.
2020.acl-main.467.txt,2020,6 Conclusion and Future Work,"we perform experiments on roberta with a total of 110 pairs of intermediate and target tasks, and perform an analysis using 25 probing tasks, covering different semantic and syntactic phenomena."
2020.acl-main.468.txt,2020,5 Conclusion,"based on this survey, we develop a unifying conceptual framework to describe bias sources and their effects (rather than just their effects)."
2020.acl-main.468.txt,2020,5 Conclusion,framework application steps (tl;dr) 1.
2020.acl-main.468.txt,2020,5 Conclusion,having a formal framework of the causes can help us achieve this.
2020.acl-main.468.txt,2020,5 Conclusion,"if outcome disparity or error disparity, check for potential origins: (a) if label bias: use post-stratification or retrain annotators.(b) if selection bias: use stratified sampling to match source to target populations, or use post-stratification, re-weighting techniques.(c) if overamplification: synthetically match distributions or add outcome disparity to cost function.(d) if semantic bias: retrain or retrofit embeddings considering approaches above, but with attributed (e.g., gendered) words (rather than people) as the population."
2020.acl-main.468.txt,2020,5 Conclusion,"rather than giving the impression that bias is a growing problem, we would like to point out that bias is not necessarily something gone awry, but rather something nearly inevitable in statistical models."
2020.acl-main.468.txt,2020,5 Conclusion,specify target population and an ideal distribution of the attribute (a) to be investigated for bias; consult datasheets and data statements5 if available for the model source; 2.
2020.acl-main.468.txt,2020,5 Conclusion,this framework allows us to group and compare works on countermeasures.
2020.acl-main.468.txt,2020,5 Conclusion,"we do, however, stress that we need to acknowledge and address bias with proactive measures."
2020.acl-main.468.txt,2020,5 Conclusion,"we hope it inspires further work in both identifying and countering bias, as well as conceptually and mathematically defining bias in nlp."
2020.acl-main.468.txt,2020,5 Conclusion,we present a comprehensive overview of the recent literature on predictive bias in nlp.
2020.acl-main.468.txt,2020,5 Conclusion,we see this paper as a step toward a unified understanding of bias in nlp.
2020.acl-main.468.txt,2020,5 Conclusion,"we would like to leave the reader with these main points: (1) every predictive model with errors is bound to have disparities over human attributes (even those not directly integrating human attributes); (2) disparities can result from a variety of origins — the embedding model, the feature sample, the fitting process, and the outcome sample — within the standard predictive pipeline; (3) selection of “protected attributes” (or human attributes along which to avoid biases) is necessary for measuring bias, and often helpful for mitigating bias and increasing the generalization ability of the models."
2020.acl-main.469.txt,2020,5 Conclusion and Future Work,"for future work, it would be interesting to see if more linguistically-inspired phenomena can be systematically found in cross-modal models."
2020.acl-main.469.txt,2020,5 Conclusion and Future Work,"moreover, visualbert exhibits a hint of cross-modal pronoun resolution, as in the bottom image of figure 5, the word “her” is resolved to the woman."
2020.acl-main.469.txt,2020,5 Conclusion and Future Work,"we have presented an analysis on the attention maps of visualbert, a proposed visually grounded language model."
2020.acl-main.469.txt,2020,5 Conclusion and Future Work,"we note that the grounding behaviour we have found is linguistically inspired, as entity grounding can be regarded as cross-modal entity coref resolution while syntactic grounding can be regarded as cross-modal parsing."
2020.acl-main.47.txt,2020,7 Conclusion and Future work,"from an engineering view, this study supports the use of lms for scoring japanese word order automatically."
2020.acl-main.47.txt,2020,7 Conclusion and Future work,"from the viewpoint of the linguistic field, we provide additional empirical evidence to various word order hypotheses as well as demonstrate the validity of the lm-based method."
2020.acl-main.47.txt,2020,7 Conclusion and Future work,"furthermore, we would like to extend a comparison between machine and human language processing beyond the perspective of word order."
2020.acl-main.47.txt,2020,7 Conclusion and Future work,"our experimental results support the validity of using japanese lms for canonical word order analysis, which has the potential to broaden the possibilities of linguistic research."
2020.acl-main.47.txt,2020,7 Conclusion and Future work,"since lms are language-agnostic, analyzing word order in another language with the lm-based method would also be an interesting direction to investigate."
2020.acl-main.47.txt,2020,7 Conclusion and Future work,we have proposed to use lms as a tool for analyzing word order in japanese.
2020.acl-main.47.txt,2020,7 Conclusion and Future work,"we plan to further explore the capability of lms on other linguistic phenomena related to word order, such as “given new ordering” (nakagawa, 2016; asahara et al., 2018)."
2020.acl-main.470.txt,2020,6 Discussion and Future Work,"for instance, our method could assist human supervisors in monitoring the progress of ongoing conversations to detect instances of rushing or stalling, or enable largerscale analyses of conversational behaviors to inform how counselors are trained."
2020.acl-main.470.txt,2020,6 Discussion and Future Work,future work could bolster the measure’s usefulness in several ways.
2020.acl-main.470.txt,2020,6 Discussion and Future Work,"however, the method’s efficacy in the present setting is likely boosted by the relative uniformity of crisis counseling conversations; and future work could aim to better accomodate settings with less structure and more linguistic variability."
2020.acl-main.470.txt,2020,6 Discussion and Future Work,"in these settings, individuals also make potentially consequential choices that span the backwardsforwards orientation axis, such as addressing previous arguments (tan et al., 2016; zhang et al., 2016) or asking leading questions (leech, 2002)."
2020.acl-main.470.txt,2020,6 Discussion and Future Work,"in this work, we sought to examine a key balance in crisis counseling conversations between advancing forwards and addressing what has already been said."
2020.acl-main.470.txt,2020,6 Discussion and Future Work,"our measure is designed to be broadly applicable, requiring no domain-specific annotations; we provide exploratory output on justice utterances from the supreme court’s oral arguments in the appendix and release code implementing our approach at http://convokit.cornell.edu to encourage experiments in other domains."
2020.acl-main.470.txt,2020,6 Discussion and Future Work,"realizing this balance is one of the many challenges that crisis counselors must manage, and modeling the actions they take in light of such challenges could point to policies to better support them."
2020.acl-main.470.txt,2020,6 Discussion and Future Work,technical improvements like richer utterance representations could improve the measure’s fidelity; more sophisticated analyses could better capture the dynamic ways in which the balance of objectives is negotiated across many turns.
2020.acl-main.470.txt,2020,6 Discussion and Future Work,"the preliminary explorations in section 5.4 could also be extended to gauge the causal effects of counselors’ behaviors (kazdin, 2007)."
2020.acl-main.470.txt,2020,6 Discussion and Future Work,the unsupervised approach we propose could circumvent difficulties in getting large-scale annotations of such sensitive content.
2020.acl-main.470.txt,2020,6 Discussion and Future Work,"we expect balancing problems to recur in conversational settings beyond crisis counseling, such as court proceedings, interviews, debates and other mental health contexts like long-term therapy."
2020.acl-main.470.txt,2020,6 Discussion and Future Work,"with such improvements, it would be interesting to study other domains where interlocutors are faced with conversational challenges."
2020.acl-main.471.txt,2020,8 Conclusion,"comprehensive experiments demonstrate our dataset is a challenging benchmark, even for large-scale pre-trained language models."
2020.acl-main.471.txt,2020,8 Conclusion,"tweets are annotated with finegrained plutchik-24 emotions, from which we analyze implicit and explicit emotions and construct plutchik-8 binary classification tasks."
2020.acl-main.471.txt,2020,8 Conclusion,"we present hurricaneemo, an annotated dataset of perceived emotions spanning 15,000 tweets from multiple hurricanes."
2020.acl-main.471.txt,2020,8 Conclusion,we release our code and datasets as a step towards facilitating research in disastercentric domains.
2020.acl-main.472.txt,2020,9 Conclusion,"finally, this work highlights the critical role of document relevance as we progress with further human-centered natural language processing."
2020.acl-main.472.txt,2020,9 Conclusion,language-based personality prediction is an important task with many applications in social science and natural language processing.
2020.acl-main.472.txt,2020,9 Conclusion,our analysis demonstrates that the level of abstraction at which attention is applied can have a significant impact on a model’s overall performance.
2020.acl-main.472.txt,2020,9 Conclusion,"our approach, which novelly models the idea that all messages are not equally valuable for psychological regression tasks, achieves new state-of-the-art results for personality prediction and provides insight into the relationship between language and personality."
2020.acl-main.472.txt,2020,9 Conclusion,we presented a hierarchical sequence model with message- and word-level attention that learns to differentiate high-and low-signal messages.
2020.acl-main.473.txt,2020,7 Conclusion,"for example, we observe that skilled forecasters are more open-minded and exhibit a higher level of uncertainty about future events."
2020.acl-main.473.txt,2020,7 Conclusion,"in this work, we presented the first study of connections between people’s forecasting skill and language used to justify their predictions."
2020.acl-main.473.txt,2020,7 Conclusion,our experimental results support several findings from the psychology literature.
2020.acl-main.473.txt,2020,7 Conclusion,we analyzed people’s forecasts in two domains: geopolitical forecasts from an online prediction forum and a corpus of company earning forecasts made by financial analysts.
2020.acl-main.473.txt,2020,7 Conclusion,we further demonstrated that it is possible to identify skilled forecasters and accurate predictions based solely on language.
2020.acl-main.473.txt,2020,7 Conclusion,"we investigated a number of linguistic metrics that are related to people’s cognitive processes while making predictions, including: uncertainty, readability and emotion."
2020.acl-main.474.txt,2020,8 Discussion and Conclusion,"computational social science is an exciting, rapidly expanding discipline."
2020.acl-main.474.txt,2020,8 Discussion and Conclusion,"in particular, we caution against using all available text in causal adjustment methods without any human validation or supervision, since one cannot diagnose any potential errors."
2020.acl-main.474.txt,2020,8 Discussion and Conclusion,"solving these open problems, along with the others presented in this paper, would be a major advance for nlp as a social science methodology."
2020.acl-main.474.txt,2020,8 Discussion and Conclusion,"unlike predictive applications, causal applications have no ground truth and so it is difficult distinguish modeling errors and forking paths from the true causal effects."
2020.acl-main.474.txt,2020,8 Discussion and Conclusion,"while text data ought to be as useful for measurement and inference as “traditional” low-dimensional social-scientific variables, combining nlp with causal inference methods requires tackling major open research questions."
2020.acl-main.474.txt,2020,8 Discussion and Conclusion,"with greater availability of text data, alongside improved natural language processing models, there is enormous opportunity to conduct new and more accurate causal observational studies by controlling for latent confounders in text."
2020.acl-main.475.txt,2020,6 Summary,"it estimates the latent topics of the texts, the ideal points of their authors, and how each author’s political position affects her choice of words within each topic."
2020.acl-main.475.txt,2020,6 Summary,"moreover, the tbip can estimate ideal points of anyone who authors political texts, including non-voting actors."
2020.acl-main.475.txt,2020,6 Summary,"we developed the text-based ideal point model (tbip), an ideal point model that analyzes texts to quantify the political positions of their authors."
2020.acl-main.475.txt,2020,6 Summary,we used the tbip to analyze u.s. senate speeches and tweets.
2020.acl-main.475.txt,2020,6 Summary,"when used to study tweets from 2020 democratic presidential candidates, the tbip identifies them along a progressive-to-moderate spectrum."
2020.acl-main.475.txt,2020,6 Summary,"without analyzing the votes themselves, the tbip separates lawmakers by party, learns interpretable politicized topics, and infers ideal points close to the classical vote-based ideal points."
2020.acl-main.476.txt,2020,6 Summary,"in this paper, we take the first step towards understanding the dynamics of state-level legislative processes in the us through a data-driven approach."
2020.acl-main.476.txt,2020,6 Summary,"to fully realize the potential of graph-based modeling, we created a new dataset, used to characterize the real-world context in which the legislative process takes place, consisting of bills, donors, and legislators and their behavior."
2020.acl-main.476.txt,2020,6 Summary,we approach these problems by formulating them as aggregate roll-call prediction.
2020.acl-main.476.txt,2020,6 Summary,"we model the rich relationship between these entities and the content of the bills using a joint text and graph prediction model on top of bert and rgcn, outperforming each one of the models in isolation."
2020.acl-main.476.txt,2020,6 Summary,"we proposed to collapse the legislative process into a heterogeneous multi-relational graph and suggest several tasks for capturing disagreement over several ideological and demographic cleavages, as well as predicting the outcome of the legislative process."
2020.acl-main.477.txt,2020,4 Conclusion,it is designed to be challenging where state-of-the-art nlp models still struggle at ≈ 60%.
2020.acl-main.477.txt,2020,4 Conclusion,macs encompasses and requires social and cultural reasoning to solve and an overall holistic understanding of humanity.
2020.acl-main.477.txt,2020,4 Conclusion,"we propose macs (machine alignment with cultural and social preferences), a new benchmark dataset for learning machine alignment with human cultural and social preferences."
2020.acl-main.478.txt,2020,8 Conclusion,"in the future, we will further improve the performance of news discourse profiling by investigating subgenres of news articles, and extensively explore its usage for various other nlp tasks and applications."
2020.acl-main.478.txt,2020,8 Conclusion,our initial experiments using neural models ascertain the feasibility of this task.
2020.acl-main.478.txt,2020,8 Conclusion,we conducted experiments and demonstrated the usefulness of news discourse profiling for event coreference resolution.
2020.acl-main.478.txt,2020,8 Conclusion,we have created the first broad-coverage corpus of news articles annotated with a theoretically grounded functional discourse structure.
2020.acl-main.479.txt,2020,7 Conclusion and future work,"considering the importance of context in drawing both scalar and other inferences in communication (grice, 1975; clark, 1992; bonnefon et al., 2009; zondervan, 2010; bergen and grodner, 2012; goodman and stuhlmu¨ller, 2013; degen et al., 2015), the development of appropriate representations of larger context is an exciting avenue for future research."
2020.acl-main.479.txt,2020,7 Conclusion and future work,"further, several model behavior analyses provided consistent evidence that the model learned associations between previously established linguistic features and the strength of scalar inferences."
2020.acl-main.479.txt,2020,7 Conclusion and future work,"in an analysis of the contribution of the conversational context, we found that humans make use of the preceding context whereas the models we considered failed to do so adequately."
2020.acl-main.479.txt,2020,7 Conclusion and future work,"it would be interesting to investigate how much supervision is necessary and, for example, to what extent a model trained to perform another task such as predicting natural language inferences is able to predict scalar inferences (see jiang and de marneffe (2019b) for such an evaluation of predicting speaker commitment, and jereticˇ et al.(2020) for an evaluation of different nli models for predicting lexically triggered scalar inferences)."
2020.acl-main.479.txt,2020,7 Conclusion and future work,it would be straightforward to train similar models for other types of inferences.
2020.acl-main.479.txt,2020,7 Conclusion and future work,"lastly, the fact that the attention weights provided insights into the model’s decisions suggests possibilities for using neural network models for developing more precise theories of pragmatic language use."
2020.acl-main.479.txt,2020,7 Conclusion and future work,one further interesting line of research would be to extend this work to other pragmatic inferences.
2020.acl-main.479.txt,2020,7 Conclusion and future work,"our goal here was to investigate whether neural networks can learn associations for already established linguistic features but it would be equally interesting to investigate whether such models could be used to discover new features, which could then be verified in experimental and corpus work, potentially providing a model-driven approach to experimental and formal pragmatics."
2020.acl-main.479.txt,2020,7 Conclusion and future work,"recent experimental work has shown that inference strength is variable across scale and inference type (doran et al., 2012; van tiel et al., 2016)."
2020.acl-main.479.txt,2020,7 Conclusion and future work,we also only considered the supervised setting in which the model was trained to predict inference strength.
2020.acl-main.479.txt,2020,7 Conclusion and future work,"we showed that despite lacking specific pragmatic reasoning abilities, neural network-based sentence encoders are capable of harnessing the linguistic signal to learn to predict human inference strength ratings from some to not all with high accuracy."
2020.acl-main.479.txt,2020,7 Conclusion and future work,"we treated some as a case study in this work, but none of our modeling decisions are specific to some."
2020.acl-main.48.txt,2020,6 Conclusion,"besides, gcan can also provide early detection of fake news with satisfying performance."
2020.acl-main.48.txt,2020,6 Conclusion,"besides, while fake news usually targets at some events, we will also extend gcan to study how to remove eventspecific features to further boost the performance and explainability."
2020.acl-main.48.txt,2020,6 Conclusion,evaluation results show the powerful effectiveness and the reasonable explainability of gcan.
2020.acl-main.48.txt,2020,6 Conclusion,"gcan is able to predict whether a short-text tweet is fake, given the sequence of its retweeters."
2020.acl-main.48.txt,2020,6 Conclusion,"in this study, we propose a novel fake news detection method, graph-aware co-attention networks (gcan)."
2020.acl-main.48.txt,2020,6 Conclusion,the problem scenario is more realistic and challenging than existing studies.
2020.acl-main.48.txt,2020,6 Conclusion,"we believe gcan can be used for not only fake news detection, but also other short-text classification tasks on social media, such as sentiment detection, hate speech detection, and tweet popularity prediction."
2020.acl-main.48.txt,2020,6 Conclusion,we will explore model generalization in the future work.
2020.acl-main.480.txt,2020,5 Conclusion,"we discussed several future directions, including data augmentation for downstream transferability, applicability of pretrained encoders to discourse, and utilizing larger discourse contexts."
2020.acl-main.480.txt,2020,5 Conclusion,"we have surveyed the literature to highlight experimental inconsistencies in implicit discourse relation classification, and suggested an improved protocol using section-level cross-validation."
2020.acl-main.480.txt,2020,5 Conclusion,"we provided a set of strong baselines for pdtb 2.0 and 3.0 following this protocol, as well as results on a range of existing setups to maintain comparability."
2020.acl-main.481.txt,2020,6 Conclusion and Future Work,"in future work, we plan to extend this work to longer documents such as the recently released dataset of bamman et al.(2019)."
2020.acl-main.481.txt,2020,6 Conclusion and Future Work,the proposed model outperforms a previous approach with far fewer parameters and a simpler architecture.
2020.acl-main.481.txt,2020,6 Conclusion and Future Work,"we propose a new diagnostic evaluation and conduct a human evaluation to test the interpretability of the model, and find that our model again does better on this evaluation."
2020.acl-main.481.txt,2020,6 Conclusion and Future Work,"we propose a new memory model for entity tracking, which is trained using sparse coreference resolution supervision."
2020.acl-main.482.txt,2020,5 Conclusion,"experiments on two benchmarks show that our model is consistently better than previous results under various settings, and that the auxiliary zp detection sub-task can make the training process more robust."
2020.acl-main.482.txt,2020,5 Conclusion,"to alleviate the data magnitude imbalance problem, we introduce zp detection as a common auxiliary sub-task for extra supervision."
2020.acl-main.482.txt,2020,5 Conclusion,we studied the effectiveness of jointly modeling zp recovery and resolution using the recently introduced multi-task learning + bert framework.
2020.acl-main.483.txt,2020,7 Conclusion & Future Work,also motivated by the present work is the more general pursuit of integrating structure into neural models like bert.
2020.acl-main.483.txt,2020,7 Conclusion & Future Work,"as such, it is an encouraging technique towards directing models’ internal representation of target phenomena via lexical anchors."
2020.acl-main.483.txt,2020,7 Conclusion & Future Work,"complementing prior work in bias detection and removal in the context of hate speech and in other settings, our method is directly integrated into transformer-based models and does not rely on data augmentation."
2020.acl-main.483.txt,2020,7 Conclusion & Future Work,"explanation algorithms offer a window into complex predictive models, and regularization as performed in this work can improve models’ internal representations of target phenomena."
2020.acl-main.483.txt,2020,7 Conclusion & Future Work,"for example, this post from the ghc requires background information and reasoning across sentences in order to classify as offensive or prejudiced: “donald trump received much criticism for referring to haiti, el salvador and africa as ‘shitholes’."
2020.acl-main.483.txt,2020,7 Conclusion & Future Work,"future work includes direct extension and validation of this technique with other language models such as gpt-2 (radford et al., 2019); experimenting with other hate speech or offensive language datasets; and experimenting with these and other sets of identity terms."
2020.acl-main.483.txt,2020,7 Conclusion & Future Work,"he was simply speaking the truth.” the examples we presented (see appendix 4 and 5) show that regularization leads to models that are context-sensitive to a degree, but not to the extent of reasoning over sentences like those above."
2020.acl-main.483.txt,2020,7 Conclusion & Future Work,"in this work, we effectively applied this technique to hate speech classifiers biased towards group identifiers; future work can determine the effectiveness and further potential for this technique in other tasks and contexts."
2020.acl-main.483.txt,2020,7 Conclusion & Future Work,"regularized hate speech classifiers increases sensitivity to the compositionality of hate speech, but the phenomena remain highly complex rhetorically and difficult to learn through supervision."
2020.acl-main.483.txt,2020,7 Conclusion & Future Work,regularizing soc explanations of group identifiers tunes hate speech classifiers to be more context-sensitive and less reliant on high-frequency words in imbalanced training sets.
2020.acl-main.483.txt,2020,7 Conclusion & Future Work,we hope that the present work can motivate more attempts to inject more structure into hate speech classification.
2020.acl-main.484.txt,2020,6 Conclusion,"though word frequency statistics have until now been neglected in previous gender bias reduction work, we propose double-hard debias, which mitigates the negative effects that word frequency features can have on debiasing algorithms."
2020.acl-main.484.txt,2020,6 Conclusion,we experiment on several benchmarks and demonstrate that our double-hard debias is more effective on gender bias reduction than other methods while also preserving the quality of word embeddings suitable for the downstream applications and embedding-based word analogy tasks.
2020.acl-main.484.txt,2020,6 Conclusion,we have discovered that simple changes in word frequency statistics can have an undesirable impact on the debiasing methods used to remove gender bias from word embeddings.
2020.acl-main.484.txt,2020,6 Conclusion,"while we have shown that this method significantly reduces gender bias while preserving quality, we hope that this work encourages further research into debiasing along other dimensions of word embeddings in the future."
2020.acl-main.485.txt,2020,6 Conclusion,"by surveying 146 papers analyzing “bias” in nlp systems, we found that (a) their motivations are often vague, inconsistent, and lacking in normative reasoning; and (b) their proposed quantitative techniques for measuring or mitigating “bias” are poorly matched to their motivations and do not engage with the relevant literature outside of nlp."
2020.acl-main.485.txt,2020,6 Conclusion,these recommendations rest on a greater recognition of the relationships between language and social hierarchies—a step that we see as paramount to establishing a path forward.
2020.acl-main.485.txt,2020,6 Conclusion,"to help researchers and practitioners avoid these pitfalls, we proposed three recommendations that should guide work analyzing “bias” in nlp systems, and, for each, provided several concrete research questions."
2020.acl-main.486.txt,2020,8 Conclusion,"our frames combine categorical knowledge about the offensiveness, intent, and targets of statements, as well as free-text inferences about which groups are targeted and biased implications or stereotypes."
2020.acl-main.486.txt,2020,8 Conclusion,this indicates that more sophisticated models are required for social bias frames inferences.
2020.acl-main.486.txt,2020,8 Conclusion,"to help machines reason about and account for societal biases, we introduce social bias frames, a new structured commonsense formalism that distills knowledge about the biased implications of language."
2020.acl-main.486.txt,2020,8 Conclusion,we collect a new dataset of 150k annotations on social media posts using a new crowdsourcing framework and establish baseline performance of models built on top of large pretrained language models.
2020.acl-main.486.txt,2020,8 Conclusion,"we show that while classifying the offensiveness of statements is easier, current models struggle to generate relevant social bias inferences, especially when implications have low lexical overlap with posts."
2020.acl-main.487.txt,2020,7 Discussion and Conclusion,"as such, when addressing ableism in ml models, it is particularly critical to involve disability communities and other impacted stakeholders in deﬁning appropriate mitigation objectives."
2020.acl-main.487.txt,2020,7 Discussion and Conclusion,"both phrases and ontological definitions around disability are themselves contested, and not all people who would describe themselves with the language we analyze would identify as disabled."
2020.acl-main.487.txt,2020,7 Discussion and Conclusion,future work is required to study if our ﬁndings carry over to other languages and cultural contexts.
2020.acl-main.487.txt,2020,7 Discussion and Conclusion,"it is important to recognize that social norms around language are contextual and differ across groups (castelle, 2018; davidson et al., 2019; vidgen et al., 2019)."
2020.acl-main.487.txt,2020,7 Discussion and Conclusion,one limitation of this paper is its restriction to the english language and us sociolinguistic norms.
2020.acl-main.487.txt,2020,7 Discussion and Conclusion,"social biases in nlp models are deserving of concern, due to their ability to moderate how people engage with technology and to perpetuate negative stereotypes."
2020.acl-main.487.txt,2020,7 Discussion and Conclusion,"we have presented evidence that these concerns extend to biases around disability, by demonstrating bias in three readily available nlp models that are increasingly being deployed in a wide variety of applications."
2020.acl-main.487.txt,2020,7 Discussion and Conclusion,"we have shown that models are sensitive to various types of disabilities being referenced, as well as to the prescriptive status of referring expressions."
2020.acl-main.488.txt,2020,5 Conclusion,leveraging these developments will allow researchers to further characterize and remove social biases from sentence representations for fairer nlp.
2020.acl-main.488.txt,2020,5 Conclusion,our experiments show that we can remove biases that occur in bert and elmo while preserving performance on downstream tasks.
2020.acl-main.488.txt,2020,5 Conclusion,this paper investigated the post-hoc removal of social biases from pretrained sentence representations.
2020.acl-main.488.txt,2020,5 Conclusion,we also demonstrate the importance of using a large number of diverse sentence templates when estimating bias subspaces.
2020.acl-main.488.txt,2020,5 Conclusion,we proposed the sent-debias method that accurately captures the bias subspace of sentence representations by using a diverse set of templates from naturally occurring text corpora.
2020.acl-main.489.txt,2020,6 Conclusion,"based on our observations, we propose random evaluation protocol that can clearly distinguish between these affected methods from others."
2020.acl-main.489.txt,2020,6 Conclusion,"combined with inappropriate evaluation protocol, such methods reported inflated performance."
2020.acl-main.489.txt,2020,6 Conclusion,"in this paper, we performed an extensive reexamination study of recent neural network based kgc techniques."
2020.acl-main.489.txt,2020,6 Conclusion,we also strongly encourage the research community to follow the random evaluation protocol for all kgc evaluation purposes.
2020.acl-main.489.txt,2020,6 Conclusion,we find that many such models have issues with their score functions.
2020.acl-main.49.txt,2020,5 Conclusion,extensive experiments conducted on two datasets demonstrate that our proposed models outperform the compared methods and prove that our models can integrate both semantic and structural information with significant genaralizablity.
2020.acl-main.49.txt,2020,5 Conclusion,"in this paper, we propose a novel method tpc-gcn to integrate the information from the graph structure and content of topics, posts, and comments for post-level controversy detection on social media."
2020.acl-main.49.txt,2020,5 Conclusion,"to improve the performance of our model for inter-topic detection, we propose an extension of tpc-gcn named dtpc-gcn, to disentangle the topic-related and topic-unrelated features and then dynamically fuse them."
2020.acl-main.49.txt,2020,5 Conclusion,"unlike the existing works, we exploit the information from related posts in the same topic and the reply structure for more effective detection."
2020.acl-main.490.txt,2020,7 Conclusions,"conversely, mbert demonstrated moderately better syntactic knowledge in morphologically simpler languages."
2020.acl-main.490.txt,2020,7 Conclusions,"in future work, we intend to expand the coverage of clams by incorporating language-specific and non-binary phenomena (e.g., french subjunctive vs. indicative and different person/number combinations, respectively), and by expanding the typological diversity of our languages."
2020.acl-main.490.txt,2020,7 Conclusions,"in this work, we have introduced the clams data set for cross-linguistic syntactic evaluation of word prediction models, and used it to to evaluate monolingual and multilingual versions of lstms and bert."
2020.acl-main.490.txt,2020,7 Conclusions,it is possible that its performance drop in hebrew and russian could be mitigated with fine-tuning on more data in these languages.
2020.acl-main.490.txt,2020,7 Conclusions,"our experiments on bert and mbert suggest (1) that mbert shows signs of learning syntactic generalizations in multiple languages, (2) that it learns these generalizations better in some languages than others, and (3) that its sensitivity to syntax is lower than that of monolingual bert."
2020.acl-main.490.txt,2020,7 Conclusions,"since clams currently includes only five languages, this correlation should be taken as very preliminary."
2020.acl-main.490.txt,2020,7 Conclusions,the design conditions of marvin and linzen (2018) and our cross-linguistic replications rule out the possibility of memorizing the training data or relying on statistical correlations/token collocations.
2020.acl-main.490.txt,2020,7 Conclusions,"this issue could be mitigated in the future with architectural changes to neural lms (such as better handling of morphology), more principled combinations of languages (as in dhar and bisazza 2020), or through explicit separation between languages during training (e.g., using explicit language ids)."
2020.acl-main.490.txt,2020,7 Conclusions,"thus, our findings indicate that lstm language models can distinguish grammatical from ungrammatical subject-verb agreement dependencies with considerable overall accuracy across languages, but their accuracy declines on some constructions (in particular, center-embedded clauses)."
2020.acl-main.490.txt,2020,7 Conclusions,"we also find that multilingual neural lms in their current form do not show signs of transfer across languages, but rather harmful interference."
2020.acl-main.490.txt,2020,7 Conclusions,"when evaluating the effect of the morphological complexity of a language on the lms’ syntactic prediction accuracy, we found that recurrent neural lms demonstrate better hierarchical syntactic knowledge in morphologically richer languages."
2020.acl-main.491.txt,2020,8 Conclusion,"in this paper, we evaluated five explanation methods through simulation tests with text and tabular data."
2020.acl-main.491.txt,2020,8 Conclusion,it also appears that subjective user ratings of explanation quality are not predictive of explanation effectiveness in simulation tests.
2020.acl-main.491.txt,2020,8 Conclusion,"simulatability metrics give a quantitative measure of interpretability, capturing the intuition that explanations should improve a person’s understanding of why a model produces its outputs."
2020.acl-main.491.txt,2020,8 Conclusion,these are the first experiments to fully isolate the effect of algorithmic explanations on simulatability.
2020.acl-main.491.txt,2020,8 Conclusion,"these results suggest that we must be careful about the metrics we use to evaluate explanation methods, and that there is significant room for improvement in current methods."
2020.acl-main.491.txt,2020,8 Conclusion,we find clear improvements in simulatability only with lime for tabular data and our prototype method in counterfactual tests.
2020.acl-main.492.txt,2020,7 Conclusion,"finally, we are interested in exploring how these types of explanations are actually interpreted by users, and whether providing them actually establishes trust in predictive systems."
2020.acl-main.492.txt,2020,7 Conclusion,"future work might explore how rankings induced over training instances by influence functions can be systematically analyzed in a stand-alone manner (rather than in comparison with interpretations from other methods), and how these might be used to improve model performance."
2020.acl-main.492.txt,2020,7 Conclusion,"they are not consistent, however, on the task of nli."
2020.acl-main.492.txt,2020,7 Conclusion,we compared two complementary interpretation methods—gradient-based saliency maps and influence functions—in two text classification tasks: sentiment analysis and nli.
2020.acl-main.492.txt,2020,7 Conclusion,we first validated the reliability of influence functions when used with deep transformer-based models.
2020.acl-main.492.txt,2020,7 Conclusion,"we found that in a lexicon-driven sentiment analysis task, saliency maps and influence functions are largely consistent with each other."
2020.acl-main.492.txt,2020,7 Conclusion,"we introduced a new potential use of influence functions: revealing and quantifying the effect of data artifacts on model predictions, which have been shown to be very common in nli."
2020.acl-main.492.txt,2020,7 Conclusion,we posit that influence functions may be a more suitable approach to interpreting models for such relatively complex natural language ‘understanding‘ tasks (while simpler attribution methods like gradients may be sufficient for tasks like sentiment analysis).
2020.acl-main.493.txt,2020,7 Discussion,"by evaluating spearman correlation between all pairs of words, one directly evaluates the extent to which the ordering of words j by distance to each word i is correctly predicted, a key notion of the geometric interpretation of the structural probe."
2020.acl-main.493.txt,2020,7 Discussion,"even though our method for identifying these distinctions lacks dependency label supervision, we still identify that mbert has a cross-linguistic clustering of grammatical relations that qualitatively overlaps considerably with the universal dependencies formalism."
2020.acl-main.493.txt,2020,7 Discussion,"for example, it would be a reasonable strategy for mbert to share little representation space between languages, effectively learning a private model for each language and avoiding destructive interference."
2020.acl-main.493.txt,2020,7 Discussion,future work could extend this analysis to include quantitative results on the extent of agreement with ud.
2020.acl-main.493.txt,2020,7 Discussion,"future work should explore other multilingual models like xlm and xlm-roberta (lample and conneau, 2019) and attempt to come to an understanding of the extent to which the properties we’ve discovered have causal implications for the decisions made by the model, a claim our methods cannot support."
2020.acl-main.493.txt,2020,7 Discussion,"instead, our transfer experiments provide evidence that at a syntactic level, mbert shares portions of its representation space between languages."
2020.acl-main.493.txt,2020,7 Discussion,"language models trained on large amounts of text have been shown to develop surprising emergent properties; of particular interest is the emergence of non-trivial, easily accessible linguistic properties seemingly far removed from the training objective."
2020.acl-main.493.txt,2020,7 Discussion,"limitations our methods are unable to tease apart, for all pairs of languages, whether transfer performance is caused by subword overlap (singh et al., 2019) or by a more fundamental sharing of parameters, though we do note that language pairs with minimal subword overlap do exhibit non-zero transfer, both in our experiments and in others (k et al., 2020)."
2020.acl-main.493.txt,2020,7 Discussion,"moreover, while we quantitatively evaluate cross-lingual transfer in recovering dependency distances, we only conduct a qualitative study in the unsupervised emergence of dependency labels via t-sne."
2020.acl-main.493.txt,2020,7 Discussion,"perhaps more surprisingly, we find evidence for fine-grained, cross-lingual syntactic distinctions in these representations."
2020.acl-main.493.txt,2020,7 Discussion,see maudslay et al.(2020) for further discussion.
2020.acl-main.493.txt,2020,7 Discussion,the uuas metric we note that the uuas metric alone is insufficient for evaluating the accuracy of the structural probe.
2020.acl-main.493.txt,2020,7 Discussion,"we acknowledge as well issues in interpreting t-sne plots (wattenberg et al., 2016), and include multiple plots with various hyperparameter settings to hedge against this confounder in figure 11."
2020.acl-main.493.txt,2020,7 Discussion,"while the probe is optimized to directly recreate parse distances, (that is, db(h`i ,h`j) ≈ d`t (w`i , w`j)) a perfect uuas score under the minimum spanning tree construction can be achieved by ensuring that db(h`i ,h`j) is small if there is an edge between w`i and w`j , and large otherwise, instead of accurately recreating distances between words connected by longer paths."
2020.acl-main.494.txt,2020,5 Conclusion,"in this paper, we proposed an effective method, hedge, building model-agnostic hierarchical interpretations via detecting feature interactions."
2020.acl-main.494.txt,2020,5 Conclusion,"in this work, we mainly focus on sentiment classification task."
2020.acl-main.494.txt,2020,5 Conclusion,the superiority of hedge is approved by both automatic and human evaluations.
2020.acl-main.494.txt,2020,5 Conclusion,"we test hedge with three different neural network models on two benchmark datasets, and compare it with several competitive baseline methods."
2020.acl-main.495.txt,2020,7 Conclusion,"we encourage future work to judge model interpretability using the proposed evaluation and publicly published annotations, and explore techniques for improving faithfulness and interpretability in compositional models."
2020.acl-main.495.txt,2020,7 Conclusion,"we introduce the concept of module-wise faithfulness, a systematic evaluation of faithfulness in neural module networks (nmns) for visual and textual reasoning."
2020.acl-main.495.txt,2020,7 Conclusion,we show how our approach leads to much higher module-wise faithfulness at a low cost to performance.
2020.acl-main.495.txt,2020,7 Conclusion,we show that naı¨ve training of nmns does not produce faithful modules and propose several techniques to improve module-wise faithfulness in nmns.
2020.acl-main.496.txt,2020,8 Conclusion,"as an added benefit, our method is very general in nature and can be used as a differentiable hard-alignment module in larger nlp models that compare two pieces of text, such as sequence-to-sequence models."
2020.acl-main.496.txt,2020,8 Conclusion,balancing performance and interpretability in deep learning models has become an increasingly important aspect of model design.
2020.acl-main.496.txt,2020,8 Conclusion,"furthermore, our method is agnostic to the underlying nature of the two objects being aligned and can therefore align disparate objects such as images and captions, enabling a wide range of future applications within nlp and beyond."
2020.acl-main.496.txt,2020,8 Conclusion,"in this work, we propose jointly learning interpretable alignments as part of the downstream prediction to reveal how neural network models operate for text matching applications."
2020.acl-main.496.txt,2020,8 Conclusion,"our method extends vanilla optimal transport by adding various constraints that produce alignments with highly controllable sparsity patterns, making them particularly interpretable."
2020.acl-main.496.txt,2020,8 Conclusion,our models show superiority by selecting very few alignments while achieving text matching performance on par with alternative methods.
2020.acl-main.497.txt,2020,5 Conclusion,"however, it may be unnecessary to collect these for entire dataset and there is a sweet-spot that works best depending on the task."
2020.acl-main.497.txt,2020,5 Conclusion,we believe that in future they can be used more directly to yield better performance gains.
2020.acl-main.497.txt,2020,5 Conclusion,we have also released these annotations for the research community at https: //github.com/ddua/intermediate_annotations.
2020.acl-main.497.txt,2020,5 Conclusion,we proposed a simple semi-supervision technique to expose the model to these annotations.
2020.acl-main.497.txt,2020,5 Conclusion,we show that intermediate annotations are a costeffective way to not only boost model performance but also alleviate certain unanticipated biases introduced during the dataset collection.
2020.acl-main.498.txt,2020,5 Conclusion,experiments show that our method improves mrr and r@1 over the best baseline by 1.06% and 2.44% on squad.
2020.acl-main.498.txt,2020,5 Conclusion,"given a candidate question, answer retrieval aims to find the most similar answer text between candidate answer texts."
2020.acl-main.498.txt,2020,5 Conclusion,"in this paper, we proposed to cross variational autoencoders by generating questions with aligned answers and generating answers with aligned questions."
2020.acl-main.499.txt,2020,5 Conclusion,"notably, our approach advances the state-of-the-art on quarel and wiqa, two standard benchmarks requiring rich logical and language understanding."
2020.acl-main.499.txt,2020,5 Conclusion,our approach significantly improves the state-of-the-art models across three substantially different qa datasets.
2020.acl-main.499.txt,2020,5 Conclusion,we further show that our approach can effectively learn from extremely limited training data.
2020.acl-main.499.txt,2020,5 Conclusion,"we introduce a logic guided data augmentation and consistency-based regularization framework for accurate and globally consistent qa, especially under limited training data setting."
2020.acl-main.5.txt,2020,6 Conclusion,annotations complement for multiwoz dataset in the future might enable dst-sc to handle the related-slot problem more effectively and further improve the joint accuracy.
2020.acl-main.5.txt,2020,6 Conclusion,"in this paper, we highlight a regularly appeared yet rarely discussed problem in multi-domain dst, namely the related-slot problem."
2020.acl-main.5.txt,2020,6 Conclusion,our model achieves significant improvements on two public datasets and shows effectiveness on relatedslot problem tests.
2020.acl-main.5.txt,2020,6 Conclusion,"we propose a novel dialogue state tracking model dst-sc, which equips with the slot connecting mechanism to build slot connections across domains."
2020.acl-main.50.txt,2020,7 Conclusion and Future Work,doing so would enable us to easily retarget this work to new countries and languages.
2020.acl-main.50.txt,2020,7 Conclusion and Future Work,"ideally, we would like to automatically identify such polarizing topics."
2020.acl-main.50.txt,2020,7 Conclusion and Future Work,"in future work, we plan to increase the number of topics that we use to characterize media."
2020.acl-main.50.txt,2020,7 Conclusion and Future Work,"next, we expand the discovered sets using supervised learning that is trained on the automatically discovered user clusters."
2020.acl-main.50.txt,2020,7 Conclusion and Future Work,"our method uses retweeted accounts, and a combination of dimensionality reduction and clustering algorithms, namely umap and mean shift, in order to produce sets of users that have opposing opinions on specific topics."
2020.acl-main.50.txt,2020,7 Conclusion and Future Work,"the main advantage of our method is that it does not require manual labeling of entity stances, which requires both topical expertise and time."
2020.acl-main.50.txt,2020,7 Conclusion and Future Work,"the projection allows us to tag a large number of influencers with their stances on specific issues and with their political leaning in general (i.e., left vs. right) with high accuracy and with minimal human effort."
2020.acl-main.50.txt,2020,7 Conclusion and Future Work,users’ stances are then projected to the influencers that are being cited in the tweets for each of the topics using the so-called valence score.
2020.acl-main.50.txt,2020,7 Conclusion and Future Work,"we also investigated the quality of the valence features, and we found that valence scores help to predict media bias with high accuracy."
2020.acl-main.50.txt,2020,7 Conclusion and Future Work,we are able to automatically tag large sets of users according to their stance of preset topics.
2020.acl-main.50.txt,2020,7 Conclusion and Future Work,"we have presented a method for predicting the general political leaning of media sources and popular twitter users, as well as their stances on specific polarizing topics."
2020.acl-main.500.txt,2020,5 Conclusion,"possible future directions include a systematic study of different aspects of qg diversity (e.g., lexical and factual) and controlled diversification of individual aspects in generation."
2020.acl-main.500.txt,2020,5 Conclusion,we hope that our work will encourage further exploration of diversity-promoting qg and its evaluation.
2020.acl-main.500.txt,2020,5 Conclusion,"while diversity of generation has received significant attention in other text generation problems (e.g., dialog), we show in this paper that it is also an important and measurable dimension of quality in question generation for qa."
2020.acl-main.501.txt,2020,7 Conclusions,"a future direction is to extend this work to question answering tasks that require reasoning over multiple documents, e.g., open-domain qa."
2020.acl-main.501.txt,2020,7 Conclusions,"depending on the properties of the data, different configurations are best, and a combined multi-objective formulation can reap the benefits of its constituents."
2020.acl-main.501.txt,2020,7 Conclusions,"in addition, the findings may generalize to other tasks, e.g., corpus-level distantly-supervised relation extraction."
2020.acl-main.501.txt,2020,7 Conclusions,"in this paper, we demonstrated that the choice of probability space and interpretation of the distant supervision signal for document-level qa have a large impact, and that they interact."
2020.acl-main.502.txt,2020,6 Conclusion,human performance is found to exceed advanced contextual embedding and language models by a significant margin.
2020.acl-main.502.txt,2020,6 Conclusion,"more importantly, the high quality distractors make this task more challenging."
2020.acl-main.502.txt,2020,6 Conclusion,scde requires use of discourselevel context and different reasoning types.
2020.acl-main.502.txt,2020,6 Conclusion,"through scde, we aim to encourage the development of more advanced language understanding models."
2020.acl-main.502.txt,2020,6 Conclusion,"we introduce scde, a sentence cloze dataset with high quality distractors carefully designed by english teachers."
2020.acl-main.503.txt,2020,6 Discussion,"achieving high accuracy on out-of-domain data may not even be possible if the test data requires abilities that are not learnable from the training data (geiger et al., 2019)."
2020.acl-main.503.txt,2020,6 Discussion,"across many tasks, nlp models struggle on out-of-domain inputs."
2020.acl-main.503.txt,2020,6 Discussion,"adversarially chosen ungrammatical text can also cause catastrophic errors (wallace et al., 2019; cheng et al., 2020)."
2020.acl-main.503.txt,2020,6 Discussion,"in all these cases, a more intelligent model would recognize that it should abstain on these inputs."
2020.acl-main.503.txt,2020,6 Discussion,"in this paper, we propose the setting of selective question answering under domain shift, in which systems must know when to abstain on a mixture of in-domain and unknown ood examples."
2020.acl-main.503.txt,2020,6 Discussion,"models trained on standard natural language inference datasets (bowman et al., 2015) generalize poorly to other distributions (thorne et al., 2018; naik et al., 2018)."
2020.acl-main.503.txt,2020,6 Discussion,nlp systems deployed in real-world settings inevitably encounter a mixture of familiar and unfamiliar inputs.
2020.acl-main.503.txt,2020,6 Discussion,"our setting combines two important goals for real-world systems: knowing when to abstain, and handling distribution shift at test time."
2020.acl-main.503.txt,2020,6 Discussion,our work provides a framework to study how models can more judiciously abstain in these challenging environments.
2020.acl-main.503.txt,2020,6 Discussion,"qualm answers reading comprehension questions by constructing reasoning chains, and abstains if it cannot find one that supports an answer (lehnert, 1977)."
2020.acl-main.503.txt,2020,6 Discussion,"shrdlu recognizes statements that it cannot parse, or that it finds ambiguous (winograd, 1972)."
2020.acl-main.503.txt,2020,6 Discussion,traditional nlu systems typically have a natural ability to abstain.
2020.acl-main.503.txt,2020,6 Discussion,"we show that models are overconfident on ood examples, leading to poor performance in the our setting, but training a calibrator using other ood data can help correct for this problem."
2020.acl-main.503.txt,2020,6 Discussion,"while we focus on question answering, our framework is general and extends to any prediction task for which graceful handling of out-of-domain inputs is necessary."
2020.acl-main.504.txt,2020,6 Conclusions and Future Work,"compared to a traditional monolithic stacked transformer model, our approach leverages classifiers placed at different encoding stages to prune candidates in a batch and improve model throughput."
2020.acl-main.504.txt,2020,6 Conclusions and Future Work,"in future work, we plan to explore techniques to automatically learn where to place intermediate classifiers, and what drop ratio to use for each one of them."
2020.acl-main.504.txt,2020,6 Conclusions and Future Work,"our experiments show that a ct model not only achieves comparable performance to a traditional transformer model while reducing computational cost per batch by over 37%, but also that our training strategy is stable and jointly produces smaller transformer models that are suitable for classification when higher throughput and lower latency goals must be met."
2020.acl-main.504.txt,2020,6 Conclusions and Future Work,"this work introduces ct, a variant of the traditional transformer model designed to improve inference throughput."
2020.acl-main.505.txt,2020,4 Conclusion,"coupled with the joint inference between token span prediction and utterance id prediction, these two language models significantly outperform two of the state-of-the-art transformer approaches, bert and roberta, on a span-based qa task called friendsqa ."
2020.acl-main.505.txt,2020,4 Conclusion,this paper introduces a novel transformer approach that effectively interprets hierarchical contexts in multiparty dialogue by learning utterance embeddings.
2020.acl-main.505.txt,2020,4 Conclusion,"two language modeling approaches are proposed, utterance-level masked lm and utterance order prediction."
2020.acl-main.505.txt,2020,4 Conclusion,we will evaluate our approach on other machine comprehension tasks using dialogues as evidence documents to further verify the generalizability of this work.
2020.acl-main.506.txt,2020,6 Conclusion,every technique has its own weaknesses.
2020.acl-main.506.txt,2020,6 Conclusion,"hence, a researcher should pick the right approach according to their needs and intentions, with a proper understanding of the techniques."
2020.acl-main.506.txt,2020,6 Conclusion,incorrect use of any technique can result in misleading conclusions.
2020.acl-main.506.txt,2020,6 Conclusion,"our goal was to review different alternatives, especially a few often ignored in nlp."
2020.acl-main.506.txt,2020,6 Conclusion,"our survey indicates that the nlp community is not fully utilizing scientific methods geared towards such assessment, with only a relatively small number of papers using such methods, and most of them relying on p-value."
2020.acl-main.506.txt,2020,6 Conclusion,using well-founded mechanisms for assessing the validity of hypotheses is crucial for any field that relies on empirical work.
2020.acl-main.506.txt,2020,6 Conclusion,"we contribute a new toolkit, hybayes, to make it easy for nlp practitioners to use bayesian assessment in their efforts."
2020.acl-main.506.txt,2020,6 Conclusion,we do not recommend a particular approach.
2020.acl-main.506.txt,2020,6 Conclusion,we hope that this work provides a complementary picture of hypothesis assessment techniques for the field and encourages more rigorous reporting trends.
2020.acl-main.506.txt,2020,6 Conclusion,we surfaced various issues and potential dangers of careless use and interpretations of different approaches.
2020.acl-main.507.txt,2020,6 Discussion,"our experiments further demonstrate substantial quality assurance issues with race, which are alleviated in our new dataset."
2020.acl-main.507.txt,2020,6 Discussion,"our results demonstrate the promise of our annotation framework and dataset in supporting a wide range of reading behavior analyses, as well as the feasibility of developing automated question validation tools for reading comprehension examinations for humans as exciting directions for future work."
2020.acl-main.507.txt,2020,6 Discussion,we introduce a new annotation framework for reading comprehension and an accompanying high-quality dataset.
2020.acl-main.507.txt,2020,6 Discussion,we leverage the novel structure of our annotations to develop a methodology for automatic validation of annotations and to perform detailed comparisons between human and machine reading comprehension.
2020.acl-main.508.txt,2020,7 Conclusion,"by doing so, we better understand the strengths and limitations of current commonsense reasoning models."
2020.acl-main.508.txt,2020,7 Conclusion,"experiments show that even though current models have gained significant improvement over the original wsc task, they still cannot fully understand the reasons behind."
2020.acl-main.508.txt,2020,7 Conclusion,"in this paper, we presented the first deep diagnosis of essential commonsense knowledge for answering winograd schema challenge questions."
2020.acl-main.508.txt,2020,7 Conclusion,"more importantly, we better know about what kinds of commonsense knowledge are required to be acquired for better commonsense reasoning."
2020.acl-main.508.txt,2020,7 Conclusion,"on top of the collected reasons, we develop a new task called winowhy, which requires models to select the plausible reasons for answering wsc questions."
2020.acl-main.509.txt,2020,9 Conclusion,our empirical results demonstrate that the ensemble model svr-rf-r performed the best for agreement prediction and models trained for agreement prediction learn to differentiate between intensity values without degrading their performance for determining stance polarity.
2020.acl-main.509.txt,2020,9 Conclusion,research into this new problem of agreement prediction will allow for a more nuanced annotation and analysis of online debate.
2020.acl-main.509.txt,2020,9 Conclusion,this problem encapsulates stance detection and adds the additional difficulty of detecting subtle differences in intensity found in the text.
2020.acl-main.509.txt,2020,9 Conclusion,"we implemented five models, adapted from top-performing stance detection models, for evaluation on the new dataset for agreement prediction."
2020.acl-main.509.txt,2020,9 Conclusion,"we introduce a new research problem called stance polarity and intensity prediction in a responsive relationship between posts, which predicts both an online post’s stance polarity and intensity value toward another post."
2020.acl-main.509.txt,2020,9 Conclusion,"we introduced a new large empirical dataset for agreement prediction, collected using a cyber argumentation platform."
2020.acl-main.51.txt,2020,9 Conclusion,detecting words that are used differently in different corpora is an important use-case in corpus-based research.
2020.acl-main.51.txt,2020,9 Conclusion,"we present a simple and effective method for this task, demonstrating its applicability in multiple different settings."
2020.acl-main.51.txt,2020,9 Conclusion,"we show that the method is considerably more stable than the popular alignment-based method popularized by hamilton et al.(2016b), and requires less tuning and word filtering."
2020.acl-main.51.txt,2020,9 Conclusion,"we suggest researchers to adopt this method, and provide an accompanying software toolkit."
2020.acl-main.510.txt,2020,5 Conclusion,"built upon pretrained language models, our method utilizes the encoder-decoder component with a language discriminator from an unsupervised machine translation system to learn a language-invariant feature space."
2020.acl-main.510.txt,2020,5 Conclusion,"by constructing the finetuned latent feature space and two views of input from the encoder-decoder of umt, our model significantly outperforms previous methods for 8/11 zero-shot sentiment classification tasks."
2020.acl-main.510.txt,2020,5 Conclusion,"in this paper, we propose a cross-lingual multi-view encoder-classifier (mvec) that requires neither labeled data in the target language nor cross-lingual resources with the source language."
2020.acl-main.510.txt,2020,5 Conclusion,our approach departs from previous models that could only make use of the shared language-invariant features or depend on parallel resources.
2020.acl-main.511.txt,2020,6 Conclusion,"a novel approach for annotating argument quality based on stochastic transitivity modeling has been proposed, outperforming existing approaches in terms of annotation effort and annotation detail, while maintaining a high annotation quality."
2020.acl-main.511.txt,2020,6 Conclusion,"a procedure to derive a scalar value for overall quality was introduced, proposing euclidean vector length to combine the different dimension scores."
2020.acl-main.511.txt,2020,6 Conclusion,"a second field of application is debate systems, where a dataset can be of use for training a system to formulate new arguments."
2020.acl-main.511.txt,2020,6 Conclusion,"even though the annotation cost can be slightly higher compared to the traditional absolute rating approach, the derived data is much more detailed and allows for conclusions with higher statistical power."
2020.acl-main.511.txt,2020,6 Conclusion,insight into argument quality was derived on a larger scale than in previous studies.
2020.acl-main.511.txt,2020,6 Conclusion,it has been shown that the three quality dimensions can be successfully annotated by laymen when using the described annotation procedure.
2020.acl-main.511.txt,2020,6 Conclusion,"sampling at even higher rates is possible, resulting in the new framework operating at the same cost as the traditional approach relying on graded scales."
2020.acl-main.511.txt,2020,6 Conclusion,"the collected corpus can be used for a multitude of purposes—especially in the emerging field of argument retrieval, it is suitable as basis for retrieval evaluation, or to train new learning to rank models."
2020.acl-main.511.txt,2020,6 Conclusion,"the collected data and a reference implementation of our model are made available in form of the webis-argquality-20 corpus, one of the largest and most detailed corpora for pairwise argument quality."
2020.acl-main.511.txt,2020,6 Conclusion,"the correlation patterns found in previous studies were reproduced, showing the quality dimensions to be equally correlating with each other."
2020.acl-main.511.txt,2020,6 Conclusion,the developed annotation approach is also not only limited to rate argument quality: it can easily be transferred to other questions or criteria that can be rated by comparison.
2020.acl-main.511.txt,2020,6 Conclusion,the overall workload in comparison to previous approaches within the same class of approaches was reduced by 93.17% through an efficient sampling method.
2020.acl-main.511.txt,2020,6 Conclusion,"this is likely due to them being dependent on a latent overall quality, a hypothesis that was supported using a pca analysis of derived quality vectors."
2020.acl-main.512.txt,2020,5 Conclusion,"apart from that, we also plan to design novel models to perform the related tasks of entity extraction and aspect extraction from comparative sentences."
2020.acl-main.512.txt,2020,5 Conclusion,experimental results show that it outperforms all strong baselines and even bert pretrained using a huge corpus.
2020.acl-main.512.txt,2020,5 Conclusion,it naturally leverages dependency graph features and word embeddings to capture the comparison and to classify the preference direction between two given entities.
2020.acl-main.512.txt,2020,5 Conclusion,our future work aims to improve the cpc performance further.
2020.acl-main.512.txt,2020,5 Conclusion,performing all these tasks jointly in a multitask learning framework is a promising direction as well because it can exploit the shared features and the inherent relationships of these tasks to perform all tasks better.
2020.acl-main.512.txt,2020,5 Conclusion,this paper proposes a novel model called ed-gat for comparative preference classification.
2020.acl-main.513.txt,2020,4 Conclusion,opiniondigest is a combination of existing absa and seq2seq models and does not require any gold-standard summaries for training.
2020.acl-main.513.txt,2020,4 Conclusion,"our experiments on the yelp dataset showed that opiniondigest outperforms baseline methods, including a state-of-the-art unsupervised abstractive summarization technique."
2020.acl-main.513.txt,2020,4 Conclusion,"our user study and qualitative analysis confirmed that our method can generate controllable high-quality summaries, and can summarize large numbers of input reviews."
2020.acl-main.513.txt,2020,4 Conclusion,"we described opiniondigest, a simple yet powerful framework for abstractive opinion summarization."
2020.acl-main.514.txt,2020,6 Conclusions,exploring the space of subsets of our preprocessing factors might yield more interesting combinations; we leave this for future work.
2020.acl-main.514.txt,2020,6 Conclusions,"interestingly, incorporating preprocessing into word representations appears to be far more beneficial than applying it in a downstream task to classification datasets."
2020.acl-main.514.txt,2020,6 Conclusions,"moreover, while all the three affective tasks (sentiment analysis, sarcasm detection and emotion classification) benefit from our proposed preprocessing framework, our analysis reveals that the multiclass emotion classification task benefits the most."
2020.acl-main.514.txt,2020,6 Conclusions,"the overall best performance is achieved by applying all the preprocessing techniques, except stopwords removal (all-stop)."
2020.acl-main.514.txt,2020,6 Conclusions,we systematically examined the role of preprocessing training corpora used to induce word representations for affect analysis.
2020.acl-main.514.txt,2020,6 Conclusions,"while all preprocessing techniques improved performance to a certain extent, our analysis suggests that the most noticeable increase is obtained through negation processing (neg)."
2020.acl-main.515.txt,2020,5 Conclusion and Future Work,"although conkadi has achieved a notable performance, there is still much room to improve.1) while ats2smmi is behind our conkadi, we find mmi can effectively enhance the ats2s; hence, in the future, we plan to verify the feasibility of the re-ranking technique for knowledge-aware models.2) we will continue to promote the integration of high-quality knowledge, including more types of knowledge and a more natural integration method."
2020.acl-main.515.txt,2020,5 Conclusion and Future Work,"besides, the proposed context-knowledge fusion and flexible mode fusion can facilitate the integration of the knowledge in the conkadi."
2020.acl-main.515.txt,2020,5 Conclusion and Future Work,extensive evaluations over both an open-released english dataset and our constructed chinese dataset demonstrate our conkadi can significantly outperform the state-of-the-art model ccm and other baselines in most experiments.
2020.acl-main.515.txt,2020,5 Conclusion and Future Work,"the proposed felicitous fact mechanism can help the conkadi focus on the facts that are highly relevant to the dialogue context, by generating a felicitous fact probability distribution over the retrieved facts."
2020.acl-main.515.txt,2020,5 Conclusion and Future Work,"to bridge the gap of the knowledge between machines and human beings in the dialogue generation, this paper proposes a novel knowledge-aware model conkadi."
2020.acl-main.516.txt,2020,5 Conclusion and Future Work,both human evaluations and automatic metrics show that our method achieves remarkably good performance.
2020.acl-main.516.txt,2020,5 Conclusion and Future Work,experiments are carried out on public-available datasets.
2020.acl-main.516.txt,2020,5 Conclusion and Future Work,"in the future, we plan to extend our approach to improve the consistency of multi-turn dialogues."
2020.acl-main.516.txt,2020,5 Conclusion and Future Work,"in this paper, we presented a three-stage framework, generate-delete-rewrite, for persona consistent dialogue generation."
2020.acl-main.516.txt,2020,5 Conclusion and Future Work,our method adopts transformer architecture and integrates a matching model to delete the inconsistent words.
2020.acl-main.517.txt,2020,6 Conclusion,"cmaml introduces a private network for each task’s dialogue model, whose structure will evolve during the training to better fit the characteristics of this task."
2020.acl-main.517.txt,2020,6 Conclusion,"in this paper, we address the problem of the fewshot dialogue generation."
2020.acl-main.517.txt,2020,6 Conclusion,"the experiment results show that cmaml achieves the best performance in terms of response quality, diversity and task consistency."
2020.acl-main.517.txt,2020,6 Conclusion,the private module will only be trained on the corpora of the corresponding task and its similar tasks.
2020.acl-main.517.txt,2020,6 Conclusion,"we also measure the model differences among tasks, and the results prove that cmaml produces diverse dialogue models for different tasks."
2020.acl-main.517.txt,2020,6 Conclusion,"we propose cmaml, which is able to customize unique dialogue models for different tasks."
2020.acl-main.518.txt,2020,5 Conclusions,"despite using gpt-2 models, our framework can be extended with other language models and similarly adopted to improve other multi-modal dialogues."
2020.acl-main.518.txt,2020,5 Conclusions,"in this work, we leverage pre-trained language models for a video-grounded dialogue task."
2020.acl-main.518.txt,2020,5 Conclusions,our early fusion strategy effectively unifies different levels of features in both dialogues and video without complicating the network architecture
2020.acl-main.518.txt,2020,5 Conclusions,we propose a sequence-to-sequence framework and a multi-task fine-tuning approach to adapt the pre-trained models to the video dialogue domain.
2020.acl-main.519.txt,2020,6 Conclusion,"in the future, we would like to explore variants of the model architecture."
2020.acl-main.519.txt,2020,6 Conclusion,"in this paper, we reformalize the ner task as a mrc question answering task."
2020.acl-main.519.txt,2020,6 Conclusion,"the proposed method obtains sota results on both nested and flat ner datasets, which indicates its effectiveness."
2020.acl-main.519.txt,2020,6 Conclusion,this formalization comes with two key advantages: (1) being capable of addressing overlapping or nested entities; (2) the query encodes significant prior knowledge about the entity category to extract.
2020.acl-main.52.txt,2020,6 Conclusion,"besides, with the support of curriculum learning, it can be more efficient."
2020.acl-main.52.txt,2020,6 Conclusion,cdl utilizes two kinds of rewards to enhance emotion and content simultaneously via dual learning.
2020.acl-main.52.txt,2020,6 Conclusion,"experimental results show that cdl can generate fluent, coherent, informative as well as emotional responses."
2020.acl-main.52.txt,2020,6 Conclusion,"in this paper, we propose a new framework curriculum dual learning (cdl) for generating emotional responses in a controlled manner."
2020.acl-main.52.txt,2020,6 Conclusion,"since existing methods in this field only focus on the emotion expression of target label but fail to consider the emotion of queries, the safe response problem deteriorates and hurts the content consistency."
2020.acl-main.520.txt,2020,8 Summary,"comparing against two existing discontinuous ner models, our model is more effective, especially in terms of recall."
2020.acl-main.520.txt,2020,8 Summary,we evaluate our model on three biomedical data sets with a substantial number of discontinuous mentions.
2020.acl-main.520.txt,2020,8 Summary,"we propose a simple, effective transition-based model that can recognize discontinuous mentions without sacrificing the accuracy on continuous mentions."
2020.acl-main.521.txt,2020,8 Conclusions and Discussion,"additionally, we also contribute a novel technique to combine multiple openie datasets to create a high-quality dataset in a completely unsupervised manner."
2020.acl-main.521.txt,2020,8 Conclusions and Discussion,bahdanau et al.(2015) showed that attending over the input words is important for text generation.
2020.acl-main.521.txt,2020,8 Conclusions and Discussion,"imo-jie significantly improves upon the existing openie systems in all three metrics, optimal f1, auc, and last f1, establishing a new state of the art system."
2020.acl-main.521.txt,2020,8 Conclusions and Discussion,see et al.(2017) showed that using a coverage loss to track the attention over the decoded words improves the quality of the generated output.
2020.acl-main.521.txt,2020,8 Conclusions and Discussion,"this general observation may be of independent interest beyond openie, such as in text summarization."
2020.acl-main.521.txt,2020,8 Conclusions and Discussion,"unlike existing neural openie systems, imo-jie produces non-redundant as well as a variable number of openie tuples depending on the sentence, by iteratively generating them conditioned on the previous tuples."
2020.acl-main.521.txt,2020,8 Conclusions and Discussion,we add to this narrative by showing that deep inter-attention between the input and the partially-decoded words (achieved by adding previous output in the input) creates a better representation for iterative generation of triples.
2020.acl-main.521.txt,2020,8 Conclusions and Discussion,we propose imojie for the task of openie.
2020.acl-main.521.txt,2020,8 Conclusions and Discussion,"we release the training data, code, and the pretrained models.9 imojie presents a novel way of using attention for text generation."
2020.acl-main.522.txt,2020,5 Conclusion,ekd forces the student model to learn open-domain trigger knowledge from teacher model by mimicking the predicted results of the teacher model.
2020.acl-main.522.txt,2020,5 Conclusion,"experiments show that our method surpasses seven strong knowledge-enhanced baselines, and is especially efficient for unseen/sparsely triggers identification."
2020.acl-main.522.txt,2020,5 Conclusion,"specifically, we adopt a wordnet-based pipeline for efficient knowledge collection, and then we propose a teacher-student model, ekd, to distill open-domain trigger knowledge from both labeled and abundant unlabeled data."
2020.acl-main.522.txt,2020,5 Conclusion,we leverage the wealth of the open-domain trigger knowledge to address the long-tail issue in ace2005.
2020.acl-main.523.txt,2020,4 Conclusion,"our model supports multi-class classification where the sentence and token labels can be weakly related, which indicates the potential of our model for many other real-world applications."
2020.acl-main.523.txt,2020,4 Conclusion,"using a larger amount of general domain texts to build pre-trained representations (peters et al., 2018; radford et al., 2018; devlin et al., 2019; clark et al., 2020) can complement with our model and is one of the directions that we plan to take in future work."
2020.acl-main.523.txt,2020,4 Conclusion,"we have shown that the proposed joint sentence and token labeling model is remarkably effective for low-resource ner in three different languages: vietnamese, thai, and indonesian."
2020.acl-main.524.txt,2020,5 Conclusion,results on a range of cross-domain datasets show that multi-cell compositional lstm outperforms bil-stm under the multi-task learning strategy.
2020.acl-main.524.txt,2020,5 Conclusion,"theoretically, our method benefits from the distinct feature distributions for each entity type across domains."
2020.acl-main.524.txt,2020,5 Conclusion,we have investigated a multi-cell compositional lstm structure for cross-domain ner under the multi-task learning strategy.
2020.acl-main.525.txt,2020,5 Conclusion,"our model relies on a layer-wise bidirectional decoding process (with both normal and inverse pyramids), allowing each decoding layer to take into account the global information from lower and upper layers."
2020.acl-main.525.txt,2020,5 Conclusion,"pyramid does not suffer from layer disorientation or error propagation, and is applicable for the more general overlapping ner."
2020.acl-main.525.txt,2020,5 Conclusion,"the proposed method obtained state-of-the-art results on four different nested ner datasets, confirming its effectiveness."
2020.acl-main.525.txt,2020,5 Conclusion,"this paper presented pyramid, a novel layered neural model for nested entity recognition."
2020.acl-main.526.txt,2020,4 Conclusions,empirical studies demonstrate that our proposed method obtains comparative performance compared with the state-of-the-art performance on two widely used benchmark datasets wn18rr and fb15k-237.
2020.acl-main.526.txt,2020,4 Conclusions,"in this paper, we propose a novel relation-aware inception network for knowledge graph embedding, called reinceptione."
2020.acl-main.526.txt,2020,4 Conclusions,reinceptione takes the benefits of conve and kbgat together.
2020.acl-main.526.txt,2020,4 Conclusions,"the proposed method first employs inception network to learn the query embedding, with the aim of increasing the interaction between head and relation embeddings, while at the same time to keep the parameter efficient."
2020.acl-main.526.txt,2020,4 Conclusions,"then, we gather the relation-aware local neighborhood and global entity information with an attention mechanism and enrich the query embedding with the joint local-global structural information."
2020.acl-main.527.txt,2020,5 Conclusions,"thanks to the exploration of suitable label distribution by rl agents, the confidences are further used to adjust the training losses of extractors and the potential harm caused by noisy instances can be alleviated."
2020.acl-main.527.txt,2020,5 Conclusions,"to deal with the noise labels and accompanying shifted label distribution problem in distant supervision, in this paper, we propose a novel method to jointly extract entity and relation through a group of cooperative multiagents."
2020.acl-main.527.txt,2020,5 Conclusions,"to demonstrate the effectiveness of the proposed method, we evaluate it on two real-world datasets and the results confirm that the proposed method can significantly improve extractor performance and achieve effective learning."
2020.acl-main.527.txt,2020,5 Conclusions,"to make full use of each instance, each agent evaluates the instance confidence from different views, and then a confidence consensus module is designed to re-label noisy instances with confidences."
2020.acl-main.528.txt,2020,5 Conclusion,experimental studies on four benchmark chinese ner datasets reveal that our method can achieve a much faster inference speed and better performance than the compared state-of-the-art methods.
2020.acl-main.528.txt,2020,5 Conclusion,"in this work, we addressed the computational efficiency of utilizing word lexicons in chinese ner."
2020.acl-main.528.txt,2020,5 Conclusion,"to obtain a high-performing chinese ner system with a fast inference speed, we proposed a novel method to incorporate the lexicon information into the character representations."
2020.acl-main.529.txt,2020,6 Conclusion,"experimental results on chinese-english, english-french and english-german translation tasks demonstrate the capability of our approach to improving both translation performance and robustness."
2020.acl-main.529.txt,2020,6 Conclusion,"to further improve the translation quality, we also incorporate an existing vicinity distribution, similar to mixup for observed examples in the training set."
2020.acl-main.529.txt,2020,6 Conclusion,we design an augmentation algorithm over the virtual sentences sampled from both of the vicinity distributions in sequence-to-sequence nmt model training.
2020.acl-main.529.txt,2020,6 Conclusion,we have presented an approach to augment the training data of nmt models by introducing a new vicinity distribution defined over the interpolated embeddings of adversarial examples.
2020.acl-main.53.txt,2020,7 Conclusion,"from this result, we propose that tackling dst with our proposed problem definition is a promising future research direction."
2020.acl-main.53.txt,2020,7 Conclusion,further analysis shows that improving state operation prediction has the potential to increase the overall dst performance dramatically.
2020.acl-main.53.txt,2020,7 Conclusion,som-dst achieves state-of-the-art joint goal accuracy on both multiwoz 2.0 and multiwoz 2.1 datasets in an open vocabulary-based setting.
2020.acl-main.53.txt,2020,7 Conclusion,som-dst decomposes dialogue state tracking into state operation prediction and slot value generation.
2020.acl-main.53.txt,2020,7 Conclusion,som-dst effectively makes use of the explicit dialogue state and discrete operations to perform relatively robust dst even in complicated conversations.
2020.acl-main.53.txt,2020,7 Conclusion,this setup makes the generation process efficient because the values of only a minimal subset of the slots are generated at each dialogue turn.
2020.acl-main.53.txt,2020,7 Conclusion,"we propose som-dst, an open vocabulary-based dialogue state tracker that regards dialogue state as an explicit memory that can be selectively overwritten."
2020.acl-main.530.txt,2020,5 Conclusions,"further, it shows comparable and in some cases better performance as compared to using the previous sentence in terms of both generic and pronounfocused evaluation."
2020.acl-main.530.txt,2020,5 Conclusions,"in future work, we plan to investigate translation of other discourse phenomena that may benefit from the use of future context."
2020.acl-main.530.txt,2020,5 Conclusions,"in this paper, we have investigated the use of future context for nmt and particularly for pronoun translation."
2020.acl-main.530.txt,2020,5 Conclusions,"while previous works have focused on the use of past context, we demonstrate through rigorous experiments that using future context does not deteriorate translation performance over a baseline."
2020.acl-main.531.txt,2020,5 Conclusion,experimental results show that our st-nmt significantly improves performance on these datasets.
2020.acl-main.531.txt,2020,5 Conclusion,"in this work, we propose a novel approach that utilizes source text and additional soft templates."
2020.acl-main.531.txt,2020,5 Conclusion,"more specifically, our approach can extract the templates from the sub-tree, which derives from the specific depth of the constituency-based parse tree."
2020.acl-main.531.txt,2020,5 Conclusion,"on top of soft templates and source text, we incorporate the template information to guide the translation procedure."
2020.acl-main.531.txt,2020,5 Conclusion,"then, we use a transformer model to predict the soft target templates conditioned on the source text."
2020.acl-main.531.txt,2020,5 Conclusion,we compare our soft-template neural machine translation (st-nmt) with other baselines on four benchmarks and multiple language pairs.
2020.acl-main.532.txt,2020,4 Discussions,adding a tag to back-translations prevents a large drop of translation quality on original texts while improvements of translation quality for translationese texts remain and may be further boosted by tagging test sentences at decoding time.
2020.acl-main.532.txt,2020,4 Discussions,"for future work, following the work on automatic identification of translationese (rabinovich and wintner, 2015; rubino et al., 2016), we plan to investigate the impact of tagging translationese texts inside parallel training data, such as parallel sentences collected from the web."
2020.acl-main.532.txt,2020,4 Discussions,"if the user does not know it a priori, a tag should be added to back-translations during training to prevent a possible large drop of translation quality."
2020.acl-main.532.txt,2020,4 Discussions,"moreover, in low-resource conditions, we show that the overall tendency is significantly different from the high-resource conditions: backtranslation improves translation quality for both translationese and original texts while adding a tag to back-translations has only a little impact."
2020.acl-main.532.txt,2020,4 Discussions,"our results show that this is partly due to the use of back-translations which is also confirmed by concurrent and independent work (bogoychev and sennrich, 2019; edunov et al., 2019)."
2020.acl-main.532.txt,2020,4 Discussions,"previous work (graham et al., 2019; zhang and toral, 2019) showed that state-of-the-art nmt systems are better in translating translationese than original texts."
2020.acl-main.532.txt,2020,4 Discussions,using back-translation improves translation quality for translationese texts but worsens it for original texts.
2020.acl-main.532.txt,2020,4 Discussions,"we conclude from this study that training nmt on back-translated data, in high-resource conditions, remains reasonable when the user knows in advance that the system will be used to translate translationese texts."
2020.acl-main.532.txt,2020,4 Discussions,we empirically demonstrated that training nmt on back-translated data overfits some of its characteristics that are partly similar to those of translationese.
2020.acl-main.533.txt,2020,5 Conclusions,"for example, cross-lingual word embedding mapping methods can be considered within the st model to shorten the distance between mt and st tasks."
2020.acl-main.533.txt,2020,5 Conclusions,"our proposals showed that utilizing word embedding as intermediate helps with the st task, and it is possible to map speech to the semantic space."
2020.acl-main.533.txt,2020,5 Conclusions,"this work is the first attempt to utilize word embedding in the st task, and further techniques can be applied upon this idea."
2020.acl-main.533.txt,2020,5 Conclusions,we also observed that lower wer in source language recognition not imply higher bleu in target language translation.
2020.acl-main.534.txt,2020,5 Conclusion,"besides, we propose an unsupervised method to address the alignment problem."
2020.acl-main.534.txt,2020,5 Conclusion,"for our purpose, we develop neural-dinf which measures document influence from the texts of documents."
2020.acl-main.534.txt,2020,5 Conclusion,"in this paper, we aim to evaluate document influence from a fine-grained level by additionally considering word semantic shifts."
2020.acl-main.534.txt,2020,5 Conclusion,our experimental results show that our model performs better than the dim on acl anthology.
2020.acl-main.534.txt,2020,5 Conclusion,the document receives an influence score based on how it explains the word frequency change and the word semantic shift.
2020.acl-main.535.txt,2020,5 Conclusion,"in future work, we intend to adapt our editor module for other learning tasks with both the structured input and structured output."
2020.acl-main.535.txt,2020,5 Conclusion,"in this paper, we proposed a retrieval-based paraphrase generation model which includes a novel fully-attentional editor."
2020.acl-main.535.txt,2020,5 Conclusion,"moreover, the outputs show that our model is able to produce paraphrases by editing sentences in a fine-grained manner using the idea of mevs."
2020.acl-main.535.txt,2020,5 Conclusion,the proposed model outperforms the previous state-of-the-art paraphrase generation models in terms of both automatic metrics and human evaluation.
2020.acl-main.535.txt,2020,5 Conclusion,this editor learns how to extract edits from a paraphrase pair and also when and how to apply these edits to a new input sentence.
2020.acl-main.535.txt,2020,5 Conclusion,"we also introduced the new idea of micro edit vectors, where each one of these vectors represents a small edit that should be applied to the source sentence to get its paraphrase."
2020.acl-main.535.txt,2020,5 Conclusion,we incorporated transformer modules in our editor and augmented them with attention over micro edit vectors.
2020.acl-main.536.txt,2020,6 Discussion,"all of these effects were stronger for more closely related languages, suggesting there is room for significant improvements on more distant language pairs."
2020.acl-main.536.txt,2020,6 Discussion,"by using a linear mapping, we are able to align the embedding layers and the contextual representations of transformers trained in different languages."
2020.acl-main.536.txt,2020,6 Discussion,"even without any anchor points, the model can still learn to map representations coming from different languages in a single shared embedding space."
2020.acl-main.536.txt,2020,6 Discussion,"in this paper, we show that multilingual representations can emerge from unsupervised multilingual masked language models with only parameter sharing of some transformer layers."
2020.acl-main.536.txt,2020,6 Discussion,"we also show that isomorphic embedding spaces emerge from monolingual masked language models in different languages, similar to word2vec embedding spaces (mikolov et al., 2013)."
2020.acl-main.536.txt,2020,6 Discussion,we also use the cka neural network similarity index to probe the similarity between bert models and show that the early layers of the transformers are more similar across languages than the last layers.
2020.acl-main.537.txt,2020,6 Future work,"these promising results point to future works in (1) linearizing the speed-speedup curve; (2) extending this approach to other pre-training architectures such as xlnet (yang et al., 2019) and elmo (peters et al., 2018); (3) applying fastbert on a wider range of nlp tasks, such as named entity recognition and machine translation."
2020.acl-main.538.txt,2020,4 Conclusion and Future Work,"for example, developers may provide relative estimates of each documented api usages to guide the re-sampling; or we could find nearest neighbors to each api call in terms of semantics and use existing usage statistics as estimates to guide the re-sampling."
2020.acl-main.538.txt,2020,4 Conclusion and Future Work,"in the future, evaluation by automatically executing generated code with test cases could be a better way to assess code generation results."
2020.acl-main.538.txt,2020,4 Conclusion and Future Work,"it will also likely be useful to generalize our re-sampling procedures to zero-shot scenarios, where a programmer writes a library and documents it, but nobody has used it yet."
2020.acl-main.538.txt,2020,4 Conclusion and Future Work,"we proposed a model-agnostic approach based on data augmentation, retrieval and data re-sampling, to incorporate external knowledge into code generation models, which achieved state-of-the-art results on the conala open-domain code generation task."
2020.acl-main.539.txt,2020,6 Conclusion,"in this paper, we present logicalfactchecker, a neural network based approach that considers logical operations for fact checking."
2020.acl-main.539.txt,2020,6 Conclusion,"logicalfactchecker has a sequence-to-action semantic parser for generating programs, and builds a heterogeneous graph to capture the connections among statements, tables, and programs."
2020.acl-main.539.txt,2020,6 Conclusion,"we evaluate our system on tabfact, a large-scale benchmark dataset for verifying textual statements over semi-structured tables, and demonstrate that our approach achieves the state-of-the-art performance."
2020.acl-main.539.txt,2020,6 Conclusion,"we find that both graph-based mechanisms are beneficial to the performance, and our sequence-to-action semantic parser is capable of generating semantic-consistent programs."
2020.acl-main.539.txt,2020,6 Conclusion,"we utilize the graph information with two mechanisms, including a mechanism to learn graph-enhanced contextual representations of tokens with graph-based attention mask matrix, and a neural module network which learns semantic compositionality in a bottom-up manner with a fixed set of modules."
2020.acl-main.54.txt,2020,7 Conclusion,"in this paper, we presented an end-to-end monolithic neural model for goal-oriented dialogues that learns to follow the core steps in the dialogue management pipeline."
2020.acl-main.54.txt,2020,7 Conclusion,"since our model outputs all the intermediate results in the dialogue management pipeline, it is easy to integrate with external systems and to interpret why the system generates a particular response."
2020.acl-main.54.txt,2020,7 Conclusion,"the experimental results from human evaluation show evidence that our approach can provide very natural human-level interaction for goal-oriented dialogues, advancing the state-of-the-art in conversational ai agents."
2020.acl-main.54.txt,2020,7 Conclusion,this also demonstrates the power of large-scale pre-trained language models to be adopted for building end-to-end goal-oriented dialogue systems.
2020.acl-main.540.txt,2020,6 Conclusion and Future Work,"in the future, we will try to increase the robustness gains of adversarial training and consider utilizing sememes in adversarial defense model."
2020.acl-main.540.txt,2020,6 Conclusion and Future Work,"in this paper, we propose a novel word-level attack model comprising the sememe-based word substitution method and particle swarm optimization-based search algorithm."
2020.acl-main.540.txt,2020,6 Conclusion and Future Work,"we conduct extensive experiments to demonstrate the superiority of our model in terms of attack success rate, adversarial example quality, transferability and robustness improvement to victim models by adversarial training."
2020.acl-main.541.txt,2020,8 Conclusion,better methods are needed to solve this dataset; we show that such methods might generalize well to real-world settings.
2020.acl-main.541.txt,2020,8 Conclusion,"our dataset contains compositionally structured regexes paired with linguistically diverse language, and organically includes distinguishing examples."
2020.acl-main.541.txt,2020,8 Conclusion,"we introduce structuredregex, a new dataset for regex synthesis from natural language and examples."
2020.acl-main.542.txt,2020,6 Conclusion,"in the future, we look forward to extend cl strategy to the pretraining stage, and guide deep models like transformer from a language beginner to a language expert."
2020.acl-main.542.txt,2020,6 Conclusion,in this work we proposed a novel curriculum learning approach which does not rely on human heuristics and is simple to implement.
2020.acl-main.542.txt,2020,6 Conclusion,"with the help of such a curriculum, language models can significantly and universally perform better on a wide range of downstream nlu tasks."
2020.acl-main.543.txt,2020,6 Conclusion,a series of experiments showed that the capability of three models to capture systematicity of predicate replacements was limited to cases where the positions of the constituents were similar between the training and test sets.
2020.acl-main.543.txt,2020,6 Conclusion,"for embedding monotonicity, no models consistently drew inferences involving embedded clauses whose depths were two levels deeper than those in the training set."
2020.acl-main.543.txt,2020,6 Conclusion,"this indicates that though current dnn-based models do not systematically interpret monotonicity inference, some models might have sufﬁcient ability to memorize different types of reasoning."
2020.acl-main.543.txt,2020,6 Conclusion,this suggests that models fail to capture inferential systematicity of monotonicity and its productivity.
2020.acl-main.543.txt,2020,6 Conclusion,we also found that bert trained with our synthetic dataset mixed with multinli maintained performance on multinli while improving the performance on monotonicity.
2020.acl-main.543.txt,2020,6 Conclusion,we hope that our work will be useful in future research for realizing more advanced models that are capable of appropriately performing arbitrary inferences.
2020.acl-main.543.txt,2020,6 Conclusion,we introduced a method for evaluating whether dnn-based models can learn systematicity of monotonicity inference under four aspects.
2020.acl-main.544.txt,2020,6 Conclusion,experimental results show that our approach achieves state-of-the-art performance on event2mind and atomic datasets.
2020.acl-main.544.txt,2020,6 Conclusion,further analysis shows that our approach selectively uses evidence to generate different inferential texts from multiple perspectives.
2020.acl-main.544.txt,2020,6 Conclusion,"in this paper, we present an evidence-aware generative model based on vq-vae, which utilizes discrete semantic latent variables to select evidence as background knowledge to guide the generation."
2020.acl-main.545.txt,2020,6 Conclusion and Future Work,"besides, human evaluation results demonstrate that the paraphrase knowledge benefits our model to ask more human-like questions of high quality."
2020.acl-main.545.txt,2020,6 Conclusion and Future Work,"in the future, we will explore more diverse and advanced paraphrase expanding methods for both sentence and paragraph level qg."
2020.acl-main.545.txt,2020,6 Conclusion and Future Work,"in this paper, we propose a two-hand hybrid model leveraging paraphrase knowledge for qg."
2020.acl-main.545.txt,2020,6 Conclusion and Future Work,"moreover, we will apply our methods to other similar tasks, such as sentence simplification."
2020.acl-main.545.txt,2020,6 Conclusion and Future Work,the experimental results of independent modules and hybrid models prove that our models are effective and transferable.
2020.acl-main.546.txt,2020,6 Conclusions,experimental results manifest the merits and superiority of neuinfer.
2020.acl-main.546.txt,2020,6 Conclusions,"for future works, to further improve the method, we will explore the introduction of additional information, such as rules and external texts."
2020.acl-main.546.txt,2020,6 Conclusions,"furthermore, neuinfer is capable of dealing with the newly proposed flexible knowledge inference, which tackles the inference on partial facts consisting of a primary triple coupled with any number of its auxiliary descriptive attributevalue pair(s)."
2020.acl-main.546.txt,2020,6 Conclusions,"in this paper, we distinguished the information in the same n-ary fact and represented each n-ary fact as a primary triple coupled with a set of its auxiliary description(s)."
2020.acl-main.546.txt,2020,6 Conclusions,"in this paper, we use only n-ary facts in the datasets to conduct knowledge inference."
2020.acl-main.546.txt,2020,6 Conclusions,"in this way, neuinfer has the ability of well handling simple knowledge inference, which copes with the inference on whole facts."
2020.acl-main.546.txt,2020,6 Conclusions,neuinfer combines the validity evaluation of the primary triple and the compatibility evaluation of the n-ary fact to obtain the validity score of the n-ary fact.
2020.acl-main.546.txt,2020,6 Conclusions,neuinfer improves the performance of hits@3 even by 16.2% on jf17k.
2020.acl-main.546.txt,2020,6 Conclusions,"particularly, on simple entity inference, neuinfer outperforms the state-of-the-art method significantly in terms of all the metrics."
2020.acl-main.546.txt,2020,6 Conclusions,"we then proposed a neural network model, neuinfer, for knowledge inference on n-ary facts."
2020.acl-main.547.txt,2020,5 Conclusion,"additionally, our model and the pre-training model are complementary."
2020.acl-main.547.txt,2020,5 Conclusion,"in this paper, we propose a neural graph matching model for chinese short text matching."
2020.acl-main.547.txt,2020,5 Conclusion,it can be regarded as a flexible method to introduce word information into bert during the fine-tuning phase.
2020.acl-main.547.txt,2020,5 Conclusion,it takes a pair of word lattices as input instead of word or character sequences.
2020.acl-main.547.txt,2020,5 Conclusion,the experimental results show that our model outperforms the state-of-the-art text matching models as well as some bert-based models.
2020.acl-main.547.txt,2020,5 Conclusion,the utilization of word lattice can provide more multi-granularity information and avoid the error propagation issue of word segmentation.
2020.acl-main.548.txt,2020,6 Conclusion,"different from the current time consuming bayesian methods, our models apply to large-scale datasets through the efficient back-propagation algorithm and gpu acceleration."
2020.acl-main.548.txt,2020,6 Conclusion,"extensive experiments on real-world datasets validate the effectiveness of our models in terms of perplexity, topic coherence, and producing explainable intermediate variables by generating dispersed proportions of document topics."
2020.acl-main.548.txt,2020,6 Conclusion,i also have a sound blaster pro and a 3com ethernet card (3c507) installed.
2020.acl-main.548.txt,2020,6 Conclusion,"if windows does come up, i get general protection faults and divide by zero system errors."
2020.acl-main.548.txt,2020,6 Conclusion,"in this paper, we present two neural mixed counting models named nb-ntm and gnb-ntm."
2020.acl-main.548.txt,2020,6 Conclusion,"in turbo mode, windows for workgroups crashes or won't come up at all."
2020.acl-main.548.txt,2020,6 Conclusion,is there a problem with memory keeping up with the speed of the cpu on these machines?over-dispersed and hierarchically dependent characteristics.
2020.acl-main.548.txt,2020,6 Conclusion,the machine is completely stable in non-turbo mode.
2020.acl-main.548.txt,2020,6 Conclusion,"the results also indicate that nb distribution families can characterize text data aptly, which is essentially due to their conformity with the over-dispersed and sparse properties of natural language."
2020.acl-main.548.txt,2020,6 Conclusion,"when compared to the existing neural topic models, both nb-ntm and gnb-ntm can well model the random variables with i have a standard computer 486dx2/66mhz eisa tower with 16mb ram, a quantum 240mb hard drive, 1.2 and 1.44 mb floppies and a colorado 250mb tape drive."
2020.acl-main.549.txt,2020,7 Conclusion,"a potential solution is to jointly learn evidence selection and claim verification model, which we leave as a future work."
2020.acl-main.549.txt,2020,7 Conclusion,evidence selection is an important component of fact checking as finding irrelevant evidence may lead to different predictions.
2020.acl-main.549.txt,2020,7 Conclusion,experiments show that both graph-based modules bring improvements and our final system is the state-of-the-art on the public leaderboard by the time our paper is submitted.
2020.acl-main.549.txt,2020,7 Conclusion,"in this work, we present a graph-based approach for fact checking."
2020.acl-main.549.txt,2020,7 Conclusion,"to better exploit the graph information, we propose two graph-based modules, one for calculating contextual word embeddings using graph-based distance in xlnet, and the other for learning representations of graph components and reasoning over the graph."
2020.acl-main.549.txt,2020,7 Conclusion,"when assessing the veracity of a claim giving multiple evidence sentences, our approach is built upon an automatically constructed graph, which is derived based on semantic role labeling."
2020.acl-main.55.txt,2020,5 Conclusion,"in the future, we will provide labels that indicate “why this candidate is false” for false candidates in our test set, so that one can easily detect weak points of systems through error analysis."
2020.acl-main.55.txt,2020,5 Conclusion,"in this paper, we focused on evaluating response generation systems via response selection."
2020.acl-main.55.txt,2020,5 Conclusion,"specifically, we proposed to construct test sets filtering out some types of false candidates: (i) those unrelated to the ground-truth response and (ii) those acceptable as appropriate responses."
2020.acl-main.55.txt,2020,5 Conclusion,"to evaluate systems properly via response selection, we proposed a method to construct response selection test sets with well-chosen false candidates."
2020.acl-main.55.txt,2020,5 Conclusion,"we demonstrated that evaluating systems via response selection with the test sets developed by our method correlates more strongly with human evaluation, compared with that of widely used metrics such as bleu."
2020.acl-main.550.txt,2020,8 Conclusion and Future Work,empirical evaluation results on three datasets verify the efficacy of our proposed method.
2020.acl-main.550.txt,2020,8 Conclusion and Future Work,"in future work, we will consider introducing more information like the citation texts to the cited paper in other papers to help the generation."
2020.acl-main.550.txt,2020,8 Conclusion and Future Work,in this paper we investigate the challenging task of automatic generation of citation texts in scholarly papers.
2020.acl-main.550.txt,2020,8 Conclusion and Future Work,we annotate a dataset and train an implicit citation extraction model to automatically enlarge the training data.we then propose the multi-source pointer-generation network with cross attention mechanism to deal with this task.
2020.acl-main.551.txt,2020,5 Conclusions,"compared to previous work, this work has provided a feasible and effective method which makes full use of edus in summarization."
2020.acl-main.551.txt,2020,5 Conclusions,"in our model, the module of edu selection is designed to extract and group salient edus and the module of edu fusion to convert groups of edus into summary sentences."
2020.acl-main.551.txt,2020,5 Conclusions,"in this paper, we choose edu as the basic summary unit and propose a novel edu based summarization model edusum."
2020.acl-main.551.txt,2020,5 Conclusions,we also apply reinforcement learning to leverage edu selection and edu fusion for improving summarization performance.
2020.acl-main.551.txt,2020,5 Conclusions,"with such a design, edusum can fuse cross-sentence information and remedy the poor readability problem brought by edus."
2020.acl-main.552.txt,2020,6 Conclusion,"experimental results show matchsum outperforms the current state-of-the-art extractive model on six benchmark datasets, which demonstrates the effectiveness of our method."
2020.acl-main.552.txt,2020,6 Conclusion,we conduct an analysis to show how our model could better fit the characteristic of the data.
2020.acl-main.552.txt,2020,6 Conclusion,we formulate the extractive summarization task as a semantic text matching problem and propose a novel summary-level framework to match the source document and candidate summaries in the semantic space.
2020.acl-main.553.txt,2020,6 Conclusion,"furthermore, our models have achieved the best results on cnn/dailymail compared with non-bert-based models, and we will take the pretrained language models into account for better encoding representations of nodes in the future."
2020.acl-main.553.txt,2020,6 Conclusion,"in this paper, we propose a heterogeneous graph-based neural network for extractive summarization."
2020.acl-main.553.txt,2020,6 Conclusion,it is also convenient to adapt our singledocument graph to multi-document with document nodes.
2020.acl-main.553.txt,2020,6 Conclusion,the introduction of more fine-grained semantic units in the summarization graph helps our model to build more complex relationships between sentences .
2020.acl-main.554.txt,2020,9 Conclusions,experimental results show that our model outperforms supervised baselines in most cases and outperforms unsupervised baselines in all cases.
2020.acl-main.554.txt,2020,9 Conclusions,"in this paper, we propose a framework that jointly learns to align and summarize for neural cross-lingual summarization."
2020.acl-main.554.txt,2020,9 Conclusions,we also propose methods to enhance the isomorphism and cross-lingual transfer between languages.
2020.acl-main.554.txt,2020,9 Conclusions,"we design training objectives for supervised and unsupervised cross-lingual summarizations, respectively."
2020.acl-main.555.txt,2020,5 Conclusion,experimental results show that our model outperforms several strong baselines by a wide margin.
2020.acl-main.555.txt,2020,5 Conclusion,"in the future we would like to explore other more informative graph representations such as knowledge graphs, and apply them to further improve the summary quality."
2020.acl-main.555.txt,2020,5 Conclusion,in this paper we explore the importance of graph representations in mds and propose to leverage graphs to improve the performance of neural abstractive mds.
2020.acl-main.555.txt,2020,5 Conclusion,"our proposed model is able to incorporate explicit graph representations into the document encoding process to capture richer relations within long inputs, and utilize explicit graph structure to guide the summary decoding process to generate more informative, fluent and concise summaries."
2020.acl-main.555.txt,2020,5 Conclusion,"we also propose an effective method to combine our model with pre-trained lms, which further improves the performance of mds significantly."
2020.acl-main.556.txt,2020,5 Conclusion and Future Work,experiment results show that the proposed method significantly outperforms all strong baseline methods and achieves the best result on the multi-news dataset.
2020.acl-main.556.txt,2020,5 Conclusion and Future Work,"in the future, we will introduce more tasks like document ranking to supervise the learning of the multi-granularity representations for further improvement."
2020.acl-main.556.txt,2020,5 Conclusion and Future Work,"in this work, we propose a novel multi-granularity interaction network to encode semantic representations for documents, sentences, and words."
2020.acl-main.556.txt,2020,5 Conclusion and Future Work,it can unify the extractive and abstractive summarization by utilizing the word representations to generate the abstractive summary and the sentence representations to extract sentences.
2020.acl-main.557.txt,2020,5 Conclusion,"by reducing the task-specific architecture components to a minimum, our method can be rapidly adapted as new modeling techniques, efficiency optimizations, and hardware accelerators become available."
2020.acl-main.557.txt,2020,5 Conclusion,code for our approach is available at github.com/nikitakit/tetra-tagging.
2020.acl-main.557.txt,2020,5 Conclusion,"remarkably, probabilities for these tags can be estimated fully in parallel by a simple classification layer on top of a neural network architecture such as bert."
2020.acl-main.557.txt,2020,5 Conclusion,"we hope that this formulation can be useful as a simple and low-overhead way of integrating syntax into any neural nlp model, including for multi-task training and to predict syntactic annotations during inference."
2020.acl-main.557.txt,2020,5 Conclusion,we present a reduction from constituency parsing to a tagging task with two binary structural decisions and two labeling decisions per word.
2020.acl-main.558.txt,2020,5 Conclusions and future work,"although recent advances in pre-trained multilingual language models significantly improve performances on these benchmark qe datasets, we highlight several instances of sampling bias embedded in the qe datasets which undermine the apparent successes of modern qe models."
2020.acl-main.558.txt,2020,5 Conclusions and future work,"each language pair contains 10,000 sentences extracted from wikipedia and translated by state-of-the-art neural models, manually annotated for quality with direct assessment (0-100) by multiple annotators following industry standards for quality control.improving label diversity."
2020.acl-main.558.txt,2020,5 Conclusions and future work,"for each of these problems, we proposed recommendations."
2020.acl-main.558.txt,2020,5 Conclusions and future work,"hopefully, this will mitigate the problems associated with partialinputs by having more instances with high fluency but low adequacy."
2020.acl-main.558.txt,2020,5 Conclusions and future work,"in figure 3, we show one of such examples."
2020.acl-main.558.txt,2020,5 Conclusions and future work,"in this work, we presented our analysis of qe datasets used in recent evaluation campaigns."
2020.acl-main.558.txt,2020,5 Conclusions and future work,"now, the average type-token ratio (ttr) for the english sentences in this set is 0.166, which is a 417% increase from the average ttr of the qe dataset from wmt18 and a 259% increase from the average ttr of the qe dataset from wmt19.improving representatation."
2020.acl-main.558.txt,2020,5 Conclusions and future work,"this dataset is based on direct assessment, which balances between adequacy and fluency."
2020.acl-main.558.txt,2020,5 Conclusions and future work,"this dataset, named mlqe, has been released to the research community3 and will be used for the wmt20 shared task on quality estimation.4 in future work, we will test the partial input hypothesis on this data."
2020.acl-main.558.txt,2020,5 Conclusions and future work,"upon the submission of this paper, we implemented the proposed recommendations by creating a new dataset for quality estimation that addresses the limitations in current datasets."
2020.acl-main.558.txt,2020,5 Conclusions and future work,"we collected data for six language pairs, namely two high-resource languages (english–german and english–chinese), two medium–resource languages (romanian–english and estonian–english), and two low-resource languages (sinhala–english and nepali–english)."
2020.acl-main.558.txt,2020,5 Conclusions and future work,we hope it will be useful for general research in qe towards more reliable models.
2020.acl-main.558.txt,2020,5 Conclusions and future work,we identified (i) issues with the balance between high-and low- quality instances (ii) issues with the lexical variety of the test sets and (iii) the lack of robustness to partial input.
2020.acl-main.558.txt,2020,5 Conclusions and future work,"we sampled sentences from a diverse set of topics from wikipedia, which led to a more diverse vocabulary."
2020.acl-main.558.txt,2020,5 Conclusions and future work,"we selected language pairs with varying degrees of resource availability, which led to more diverse translation quality distributions (particularly for the mediumresource languages), mitigating the issue of imbalanced datasets, as shown in figure 2.improving lexical diversity."
2020.acl-main.559.txt,2020,8 Conclusions,"accordingly, this work suggests a symbiotic view of cognitive science, embodied ai, and computational linguistics."
2020.acl-main.559.txt,2020,8 Conclusions,"by sharing common foundational problems, these fields may better share and co-evolve common solutions."
2020.acl-main.559.txt,2020,8 Conclusions,"finally, we believe that attaining deeper language understanding must be a large scale effort, beyond the scope of any one research group."
2020.acl-main.559.txt,2020,8 Conclusions,growing empirical evidence shows that language is intricately intertwined with a vast range of other neural processes.
2020.acl-main.559.txt,2020,8 Conclusions,"one of our main goals was to stimulate a discussion; moving forward, we welcome comments, feedback, and suggestions."
2020.acl-main.559.txt,2020,8 Conclusions,"the proposed architecture, drawing on contemporary cognitive science, aims to address key limitations of current nlu systems through mental simulation and grounded metaphoric inference."
2020.acl-main.559.txt,2020,8 Conclusions,this position paper has proposed an approach to representation and learning based on the tenets of ecl.
2020.acl-main.559.txt,2020,8 Conclusions,we hope that the paradigm presented here will help provide coherence to such efforts.
2020.acl-main.559.txt,2020,8 Conclusions,we outlined major challenges and suggested a roadmap towards realizing the proposed vision.
2020.acl-main.56.txt,2020,5 Conclusion,"finally, a novel negative sampling augmentation method was introduced to augment off-topic training data."
2020.acl-main.56.txt,2020,5 Conclusion,"first of all, a model framework of five major layers was proposed, within which bi-attention mechanism and convolutions were used to well capture the topic words of prompts and key-phrase of responses, and gated unit as relevance layer was applied to better obtaining semantic matching representation, as well as residual connections with each major layer."
2020.acl-main.56.txt,2020,5 Conclusion,"in this paper, we conducted a series of work around the task of off-topic response detection."
2020.acl-main.56.txt,2020,5 Conclusion,"moreover, the visualization analysis of the off-topic model was given to study the essence of the model."
2020.acl-main.56.txt,2020,5 Conclusion,we verified the effectiveness of our approach and achieved significant improvements on both seen and unseen test data.
2020.acl-main.560.txt,2020,6 Conclusion,are your findings and contributions contributing to the inclusivity of various languages?
2020.acl-main.560.txt,2020,6 Conclusion,"as a result, we uncover a set of interesting insights and also yield consistent findings about language disparity: — the taxonomical hierarchy is repeatedly evident from individual resource availabilities (ldc, lre, wikipedia, web), entropy calculations for conferences, and the embeddings analysis.— lrec and workshops(ws) have been more inclusive across different classes of languages, seen through the inverse mrr statistics, entropy plots and the embeddings projection.— there are typological features (such as 144e), existing in languages over spread out regions, represented in many resource-poor languages but not sufficiently in resource-rich languages."
2020.acl-main.560.txt,2020,6 Conclusion,"finally, in case you’re still itching to know, language x is dutch, and y is somali."
2020.acl-main.560.txt,2020,6 Conclusion,"instead, a way to promote change could be the addition of d&i (diversity and inclusion) clauses involving language-related questions in the submission and reviewer forms: do your methods and experiments apply (or scale) to a range of languages?"
2020.acl-main.560.txt,2020,6 Conclusion,pertinent questions should be posed to authors of future publications about whether their proposed language technologies extend to other languages.
2020.acl-main.560.txt,2020,6 Conclusion,"special tracks could be initiated for low-resource, language-specific tasks, although we believe that in doing so, we risk further marginalization of those languages."
2020.acl-main.560.txt,2020,6 Conclusion,there are ways to improve the inclusivity of acl conferences.
2020.acl-main.560.txt,2020,6 Conclusion,"this could potentially reduce the performance of language tools relying on transfer learning.— newer conferences have been more language-inclusive, whereas older ones have maintained interests in certain themes of research which don’t necessarily favour multilingual systems.— there is a possible indication of a time progression or even a technological shift in nlp, which can be visualized in the embeddings projection.— there is hope for low-resource languages, with mrr figures indicating that there are focused communities working on these languages and publishing works on them, but there are still plenty of languages, such as javanese and igbo, which do not have any such support."
2020.acl-main.560.txt,2020,6 Conclusion,we believe these findings will play a strong role in making the community aware of the gap that needs to be filled before we can truly claim state-of-the-art technologies to be language agnostic.
2020.acl-main.560.txt,2020,6 Conclusion,we do so by conducting a series of quantitative analyses through the lens of a defined taxonomy.
2020.acl-main.560.txt,2020,6 Conclusion,we set out to answer some critical questions about the state of language resource availability and research.
2020.acl-main.561.txt,2020,6 Conclusions,and deep learning allows many aspects of these structured representations to be learned from data.
2020.acl-main.561.txt,2020,6 Conclusions,attention-based models are fundamentally different because they use bag-of-vector representations.
2020.acl-main.561.txt,2020,6 Conclusions,"bov representations are nonparametric representations, in that the number of vectors in the bag can grow arbitrarily large, and these vectors are exchangeable."
2020.acl-main.561.txt,2020,6 Conclusions,"but of one thing we can be certain: the immense success of adapting deep learning architectures to fit with our computational-linguistic understanding of the nature of language will doubtless continue, with greater insights for both natural language processing and machine learning."
2020.acl-main.561.txt,2020,6 Conclusions,"even within a given level, the set of entities is a pre-defined function of the text."
2020.acl-main.561.txt,2020,6 Conclusions,"however, successful deep learning architectures for natural language currently still have many handcoded aspects."
2020.acl-main.561.txt,2020,6 Conclusions,"if we can induce the entities at a given level, a more challenging task will be the induction of the levels themselves."
2020.acl-main.561.txt,2020,6 Conclusions,"it is not clear what advances in deep learning methods will be necessary to improve over our current fixed entity definitions, nor whether the resulting entities will be any different from the ones postulated by linguistic theory."
2020.acl-main.561.txt,2020,6 Conclusions,"often deep learning models only address one level at a time, whereas a full model would involve levels ranging from the perceptual input to logical reasoning."
2020.acl-main.561.txt,2020,6 Conclusions,"the levels of representation are hand-coded, based on linguistic theory or available resources."
2020.acl-main.561.txt,2020,6 Conclusions,the presumably-innate nature of linguistic levels suggests that this might not even be possible.
2020.acl-main.561.txt,2020,6 Conclusions,this analysis suggests that an important next step in deep learning architectures for natural language understanding will be the induction of entities.
2020.acl-main.561.txt,2020,6 Conclusions,"vector space representations (as in mlps) are not adequate, nor are vector spaces which evolve over time (as in lstms)."
2020.acl-main.561.txt,2020,6 Conclusions,we conclude that the nature of language has influenced the design of deep learning architectures in fundamental ways.
2020.acl-main.561.txt,2020,6 Conclusions,"with bov representations, attention-based neural network models like transformer can model the kinds of unbounded structured representations that computational linguists have found to be necessary to capture the generalisations in natural language."
2020.acl-main.562.txt,2020,7 The Final Frontier – An Outlook,"a fully self-contained and human-readable query protocol that can embed any existing query language and augment it with (boilerplate) statements to bind the query content to actual corpora and annotation layers, provide information about the query dialect and its version and store configuration and result preparation instructions, would go a long way towards unification and potential interoperability of corpus query systems.7.2 towards a hybrid architecture?"
2020.acl-main.562.txt,2020,7 The Final Frontier – An Outlook,"a strong dependency on indexing to access large corpora also presupposes a priori knowledge of what information is meant to be searchable, frequently confining corpus query tools to the role of being mere finding aids within a research process."
2020.acl-main.562.txt,2020,7 The Final Frontier – An Outlook,choices in technology or algorithms for (i) through (iii) definitively dictate the basic nature and structure of the information that can be queried.
2020.acl-main.562.txt,2020,7 The Final Frontier – An Outlook,"fortunately the cqlf standardization initiative aims at providing developers with the means of locating their tools on a map of query features, so that prospective users may find them without an odyssey."
2020.acl-main.562.txt,2020,7 The Final Frontier – An Outlook,"however, many questions regarding the future of corpus querying still remain, two of which we consider of particular importance and will discuss in the following sections.7.1 one language to query them all?"
2020.acl-main.562.txt,2020,7 The Final Frontier – An Outlook,"instead we refer to (ga¨rtner, to appear) for an overview of our ongoing efforts to design and implement a hybrid corpus query architecture and associated query protocol."
2020.acl-main.562.txt,2020,7 The Final Frontier – An Outlook,"interestingly they all share the pros and cons of being designed as formal languages with the goal of taciturnity, meaning that for the untrained eye they usually represent just a weird salad of letters and special characters .51 this is particularly noteworthy, as all modern corpus query tools feature a rich gui and could easily employ a more verbose query language while at the same time shield users from the time overhead when creating queries by clever auto-completion or recommendation functions."
2020.acl-main.562.txt,2020,7 The Final Frontier – An Outlook,"likewise, today’s corpus queries are not selfcontained to the level of for instance sql queries, which are composed of dedicated parts for scope selection, actual constraints and result preparation."
2020.acl-main.562.txt,2020,7 The Final Frontier – An Outlook,naturally all of these proposed features (and especially the last one) require a drastically different and quite heterogeneous architecture.
2020.acl-main.562.txt,2020,7 The Final Frontier – An Outlook,navigating this ocean in order to find the right tool for the job and then learn to use it can already be as much effort as manually investigating the data at hand.
2020.acl-main.562.txt,2020,7 The Final Frontier – An Outlook,space does not permit we provide a detailed description of such a hybrid approach.
2020.acl-main.562.txt,2020,7 The Final Frontier – An Outlook,"taking the microservices approach of korap as an example, it is easy to imagine a hierarchically organized architecture of query translation and evaluation services working together (by partially answering queries, filtering the results or otherwise post-process them) to provide the optimal combination of freedom in expressiveness and performance guarantees."
2020.acl-main.562.txt,2020,7 The Final Frontier – An Outlook,"the typical architecture of corpus query systems today is a monolithic one and contains from bottom to top (i) a backend storage or custom data model, (ii) a custom query evaluator or query interface to said backend and (iii) a query parser or translator to process the raw user query."
2020.acl-main.562.txt,2020,7 The Final Frontier – An Outlook,"they usually make it very difficult, if not impossible, to implement changes or extensions retrospectively or from the outside."
2020.acl-main.562.txt,2020,7 The Final Frontier – An Outlook,today we have a cluttered buffet of corpus query languages to pick from depending on our information needs.
2020.acl-main.562.txt,2020,7 The Final Frontier – An Outlook,"twenty years ago this might have seemed utterly unrealistic, but advances in information management systems and distributed computing certainly put this vision within technical reach."
2020.acl-main.562.txt,2020,7 The Final Frontier – An Outlook,"usually only the constraint part is present in corpus query languages, with only a few exceptions 52, leaving additional configurations (result size limit, search direction, case sensitivity) exclusively to external components, such as the gui, hampering the reproducibility of search results severely."
2020.acl-main.562.txt,2020,7 The Final Frontier – An Outlook,"we would like to see them become true enablers instead, allowing queries to go far beyond of what a corpus has to offer with its bare annotations alone and for example include the following extensions to create more informed search solutions: • use knowledge bases and similar external resources to allow more generalized queries, e.g.“find verbal constructions containing a preposition in combination with some sort of furniture”.• add (semantic) similarity measures (e.g.word embeddings) and other approaches for increased fuzziness to improve example-based search.• offer true scripting support for users to extent or customize the ability provided by a system."
2020.acl-main.562.txt,2020,7 The Final Frontier – An Outlook,"while this effort is still in an early stage, we are looking forward to having catalogs available in the not too distant future, allowing us to browse for query languages based on our individual information needs."
2020.acl-main.562.txt,2020,7 The Final Frontier – An Outlook,"while this might affect performance in unpredictable and detrimental ways, raw (distributed) computing power and clever use of pre-filtering can offset the impacts on performance."
2020.acl-main.562.txt,2020,7 The Final Frontier – An Outlook,"with several dozens of systems contributing their individual variations, the pool of available corpus query tools and languages has become quite large."
2020.acl-main.563.txt,2020,6 Conclusion,"although our model is based on predefined ontology, it is universal and scalable to unseen domains, slots and values."
2020.acl-main.563.txt,2020,6 Conclusion,"experimental results show that our model achieves state-of-the-art performance of 52.68% and 58.55% joint accuracy with considerable improvements (+1.24% and +5.98%) over previous best results on multiwoz 2.0 and multiwoz2.1 datasets, respectively."
2020.acl-main.563.txt,2020,6 Conclusion,"the main contributions of our model, chan and adaptive objective, can also be applied to open-vocabulary models."
2020.acl-main.563.txt,2020,6 Conclusion,we introduce an effective model that consists of a contextual hierarchical attention network to fully exploit relevant context from dialogue history and an adaptive objective to alleviate the slot imbalance problem in dialogue state tracking.
2020.acl-main.563.txt,2020,6 Conclusion,we will explore it in the future.
2020.acl-main.564.txt,2020,5 Conclusion,experiments conducted on two public conversation datasets show that our proposed framework is able to boost the performance of existing dialogue systems.
2020.acl-main.564.txt,2020,5 Conclusion,"future work will investigate other data manipulation techniques (e.g., data synthesis), which can be further integrated to improve the performance."
2020.acl-main.564.txt,2020,5 Conclusion,"in this work, we consider the automated data manipulation for open-domain dialogue systems."
2020.acl-main.564.txt,2020,5 Conclusion,our learning-to-manipulate framework for neural dialogue generation is not limited to the elaborately designed manipulation skills in this paper.
2020.acl-main.564.txt,2020,5 Conclusion,the resulting data manipulation model is fully end-to-end and can be trained jointly with the dialogue generation model.
2020.acl-main.564.txt,2020,5 Conclusion,"to induce the model learning from effective instances, we propose a learnable data manipulation model to augment effective training samples and reduce the weights of inefficient samples."
2020.acl-main.565.txt,2020,5 Conclusion,"besides, our model can quickly adapt to a new domain with little annotated data."
2020.acl-main.565.txt,2020,5 Conclusion,experiments on two datasets show the effectiveness of the proposed models.
2020.acl-main.565.txt,2020,5 Conclusion,"in addition, a dynamic fusion layer is proposed to dynamically capture the correlation between a target domain and all source domains."
2020.acl-main.565.txt,2020,5 Conclusion,"in this paper, we propose to use a shared-private model to investigate explicit modeling domain knowledge for multi-domain dialog."
2020.acl-main.566.txt,2020,5 Conclusion,"by using policy shaping and reward shaping, s2agent can leverage knowledge distilled from the demonstrations to calibrate actions from underlying rl agents for better trajectories, and obtains extra rewards for these state-actions similar to demonstrations alleviating reward sparsity for better exploration."
2020.acl-main.566.txt,2020,5 Conclusion,"compared with previous work, our proposed s2agent is capable of learning in a more efficient manner."
2020.acl-main.566.txt,2020,5 Conclusion,"in this paper, we present a new strategy for learning dialogue policy with human demonstrations."
2020.acl-main.566.txt,2020,5 Conclusion,the results of simulation and human evaluation show that our proposed agent is efficient and effective in both single domain and a challenging domain adaptation setting.
2020.acl-main.567.txt,2020,8 Conclusions and Future Work,"besides that, we also notice that it is hard for sas to correctly extract names of hotel or attraction which have rich variations."
2020.acl-main.567.txt,2020,8 Conclusions and Future Work,designing a new model to address these problems may be our future work.
2020.acl-main.567.txt,2020,8 Conclusions and Future Work,our model reaches the state-of-the-art performance compared with previous models.
2020.acl-main.567.txt,2020,8 Conclusions and Future Work,the sharing allows sas to generalize on rare slot-value pairs with few training data.
2020.acl-main.567.txt,2020,8 Conclusions and Future Work,"the slot attention of sas enables it to isolate the key information for each slot, while the slot information sharing enhances the expressiveness of the information passed to each slot by integrating the information from similar slots."
2020.acl-main.567.txt,2020,8 Conclusions and Future Work,"we believe that sas provides promising potential extensions, such as adapting our model on other tasks where are troubled by excessive information."
2020.acl-main.567.txt,2020,8 Conclusions and Future Work,"we present sas, an effective dst model which successfully extracts the key feature from the original information excessive dialogue."
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,all existing automatic evaluation methods including ruber that compare the context and the response can be cheated by the copy mechanism.ssrem is also susceptible.
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,"although the classification loss is simple, ssrem outperforms all existing automatic evaluation models."
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,"first, we can make ssrem more robust on adversarial attacks."
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,"however, as table 2 and figure 3 are shown, each negative samples has different correlation with the context."
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,"however, contextual coherence between the input context and the generated text is important in multi-turn conversations."
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,"however, ssrem is fooled less than other existing models because ssrem learns with negative samples from the set of utterances in the same conversation."
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,"in this paper, we presented ssrem, an automatic evaluation model for conversational response generation."
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,"recently, zhang et al.(2020) uses bert (devlin et al., 2019) to evaluate generated candidate sentences by comparing reference sentence."
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,sai et al.(2019) shows limitations of adem on adversarial attacks such as removing stopwords and replacing words with synonyms.
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,"second, we can improve ssrem for a higher correlation with human judgement."
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,"ssrem does not give lower scores for the context utterances than gt, but it is not as bad as ruber."
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,ss-rem learns to differentiate among utterances in the same context.
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,ssrem looks at the context of the conversation and the ground-truth response together.
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,there are several future directions to improve ssrem.
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,"third, we can extend using ssrem to various conversation corpora such as task-oriented dialogues."
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,we also showed that ssrem is effective in evaluating a movie conversation corpus even when it is trained with twitter conversations.
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,we chose to approach ssrem with a classification loss because it is simple and widely used to estimate the models using negative sampling.
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,we investigated another type of the adversarial attack named copy mechanism that copies one of the utterances in the context as the generated response.
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,we proposed negative sampling with speaker sensitive samples to train ssrem.
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,we show this empirically with an experiment to identify true and false responses (sec 6.2).
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,we showed that ssrem outperforms the other metrics including rsrem that uses random negative samples only.
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,we trained and tested ssrem on opendomain conversation corpora.
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,"we used word embeddings to represent an utterance to the vector for the simplicity, but contextual embeddings are much better since it generates more context-related representation than word embeddings."
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,we will apply ssrem to various conversation tasks for evaluating the generated text automatically.
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,we will explore these directions in our future work.
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,we will make ssrem more robust on the attacks.
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,"we will use ranking loss (wang et al., 2014; schroff et al., 2015) to learn the difference among samples."
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,we will use the contextual embedding to represent utterances.
2020.acl-main.568.txt,2020,8 Conclusion and Future Work,"when we look at the mean score for the context utterances that shows this copy mechanism compared to the mean score of the ground-truth response (gt), the mean score of context utterances is 0.07 higher by ruber, but only 0.01 higher by ssrem."
2020.acl-main.569.txt,2020,5 Conclusion,experimentation on the english rst-dt corpus and the chinese cdtb corpus shows the great effectiveness of our proposed approach.
2020.acl-main.569.txt,2020,5 Conclusion,"in particular, we cast the discourse parsing task as a edu split point ranking task, where a split point is classified to different levels according to its rank, and the edus associated with the split point are arranged accordingly."
2020.acl-main.569.txt,2020,5 Conclusion,"in the future work, we will focus on more effective discourse parsing with additional carefully designed features and joint learning with edu segmentation."
2020.acl-main.569.txt,2020,5 Conclusion,"in this paper, we propose a top-down neural architecture to text-level discourse parsing."
2020.acl-main.569.txt,2020,5 Conclusion,"in this way, we can determine the complete discourse rhetorical structure as a hierarchical tree structure."
2020.acl-main.569.txt,2020,5 Conclusion,"specifically, after encoding the edus and edu split points, a encoder-decoder with an internal stack is employed to generate discourse tree recursively."
2020.acl-main.57.txt,2020,5 Conclusion and Future Work,"in the future, we plan to use more powerful encoders and evaluate our methods on real dialog data."
2020.acl-main.57.txt,2020,5 Conclusion and Future Work,"in this paper, we leverage the maml algorithm to optimize a human-machine collaborative dialog system, which shows good results for both fast adaptability and reliable performance."
2020.acl-main.570.txt,2020,7 Conclusion and Future Work,a detailed comparative analysis proves that the proposed multi-modal architecture outperforms other strong baselines and existing models.
2020.acl-main.570.txt,2020,7 Conclusion and Future Work,"besides, we have proposed a novel deep multi-modal architecture for managing the multi-modal scenario for ppis."
2020.acl-main.570.txt,2020,7 Conclusion and Future Work,"for each modality (textual, protein sequence and protein atomic structure), we have developed different deep learning models for efficient feature extractions."
2020.acl-main.570.txt,2020,7 Conclusion and Future Work,further there are plenty of options for improving the fusion technique to enhance the overall performance of the model.
2020.acl-main.570.txt,2020,7 Conclusion and Future Work,future work aims at enhancing sequence feature extraction methods to improve the classification performance as those suffer from low accuracy.
2020.acl-main.570.txt,2020,7 Conclusion and Future Work,"in this work, we have generated some multi-modal protein-protein interaction databases by amalgamating protein structures and sequences with existing text information available in the biomedical literature."
2020.acl-main.570.txt,2020,7 Conclusion and Future Work,the process of generating multi-modal datasets from ppi corpora is illustrated with some examples.
2020.acl-main.571.txt,2020,6 Conclusion,"applying the divideand-conquer policy, the flat module is in charge of outermost entities, while the graph module focuses on inner entities."
2020.acl-main.571.txt,2020,6 Conclusion,"as a general model, our biflag model can also handle non-nested structures by simply removing the graph module."
2020.acl-main.571.txt,2020,6 Conclusion,"in terms of the same strict setting, empirical results show that our model generally outperforms previous state-of-the-art models."
2020.acl-main.571.txt,2020,6 Conclusion,"our biflag model also facilitates a full bidirectional interaction between the two modules, which let the nested ne structures jointly learned at most degree."
2020.acl-main.571.txt,2020,6 Conclusion,this paper proposes a new bipartite flat-graph (biflag) model for nested ner which consists of two interacting subgraph modules.
2020.acl-main.572.txt,2020,5 Conclusion and Future Work,"in this paper, we described a framework for leveraging global triple knowledge to improve kg entity typing by training not only on (entity, entity type) assertions but also using newly generated (head type, relationship, tail type) type triples."
2020.acl-main.572.txt,2020,5 Conclusion and Future Work,"next, we are considering to use this framework to conduct kg entity type noise detection."
2020.acl-main.572.txt,2020,5 Conclusion and Future Work,our modeling method is general and should apply to other typeoriented tasks.
2020.acl-main.572.txt,2020,5 Conclusion and Future Work,"specifically, we propose two novel embedding-based models to encode entity type instances and entity type triples respectively."
2020.acl-main.572.txt,2020,5 Conclusion and Future Work,the connection of both models is utilized to infer missing entity type instances.
2020.acl-main.572.txt,2020,5 Conclusion and Future Work,the empirical experiments demonstrate the effectiveness of our proposed model.
2020.acl-main.573.txt,2020,5 Conclusion and Future Work,"compared with existing memory-based methods, emar requires models to understand the prototypes of old relations rather than to overfit a few specific memorized examples, which can keep better distinction among relations after long-term training."
2020.acl-main.573.txt,2020,5 Conclusion and Future Work,"for future work, how to combine open relation learning and continual relation learning together to complete the pipeline for emerging relations still remains a problem, and we will continue to work on it."
2020.acl-main.573.txt,2020,5 Conclusion and Future Work,"to alleviate catastrophically forgetting old relations in continual relation learning, we introduce episodic memory activation and reconsolidation (emar), inspired by the mechanism in human long-term memory formation."
2020.acl-main.573.txt,2020,5 Conclusion and Future Work,"we conduct experiments on three benchmarks in relation extraction and carry out extensive experimental results as well as empirical analyses, showing the effectiveness of emar on utilizing memorized examples."
2020.acl-main.574.txt,2020,8 Conclusion,"extensive experiments have been conducted on both slot filling and ner tasks on three benchmark datasets, showing that sequence labeling using the proposed methods achieve new state-of-the-art performances."
2020.acl-main.574.txt,2020,8 Conclusion,"importantly, without using external knowledge nor fine tuning of large pretrained models, our methods enable a sequence labeling model to outperform models fine-tuned on bert."
2020.acl-main.574.txt,2020,8 Conclusion,our analysis also indicates large potential of further performance improvements by exploiting oov and lf entities.
2020.acl-main.574.txt,2020,8 Conclusion,we adopt variational autoencoder to learn a stochastic reconstructor for the reconstruction and adversarial training to extract frequency-agnostic and entity-type-specific features.
2020.acl-main.574.txt,2020,8 Conclusion,we have presented local context reconstruction for oov entities and delexicalized entity identification for low-frequency entities to address the rare entity problem.
2020.acl-main.575.txt,2020,5 Conclusion,"through ner experiments, we demonstrated that the models build by our method have (i) competitive performance with a classifier-based span model and (ii) interpretable inference process where it is easy to understand how much each training instance contributes to the predictions."
2020.acl-main.575.txt,2020,5 Conclusion,we presented and investigated an instance-based learning method that learns similarity between spans.
2020.acl-main.576.txt,2020,6 Conclusion and Future Work,"in the future, we should further leverage the internal relations in the candidate end, and try to introduce rich medical background knowledge into our work."
2020.acl-main.576.txt,2020,6 Conclusion and Future Work,"in this paper, we first describe a new constructed corpus for the medical information extraction task, including the annotation methods and the evaluation metrics."
2020.acl-main.576.txt,2020,6 Conclusion and Future Work,mie is able to capture the interaction information between the dialogue turns.
2020.acl-main.576.txt,2020,6 Conclusion and Future Work,the experimental results indicate that mie is a promising solution for medical information extraction towards medical dialogues.
2020.acl-main.576.txt,2020,6 Conclusion and Future Work,"then we propose mie, a deep neural matching model tailored for the task."
2020.acl-main.576.txt,2020,6 Conclusion and Future Work,"to show the advantage of mie, we develop several competitive baselines for comparison."
2020.acl-main.577.txt,2020,8 Conclusion,further constraints are used to predict nested or flat named entities.
2020.acl-main.577.txt,2020,8 Conclusion,"in this paper, we reformulate ner as a structured prediction task and adopted a sota dependency parsing approach for nested and flat ner."
2020.acl-main.577.txt,2020,8 Conclusion,our system uses contextual embeddings as input to a multi-layer bilstm.
2020.acl-main.577.txt,2020,8 Conclusion,the results show that our system achieves sota on all of the eight corpora.
2020.acl-main.577.txt,2020,8 Conclusion,we demonstrate that advanced structured prediction techniques lead to substantial improvements for both nested and flat ner.
2020.acl-main.577.txt,2020,8 Conclusion,we employ a biaffine model to assign scores for all spans in a sentence.
2020.acl-main.577.txt,2020,8 Conclusion,we evaluated our system on eight named entity corpora.
2020.acl-main.578.txt,2020,6 Conclusion,"acknowledgments this work is supported in part by the national hi-tech r&d program of china (no.2018yfb1005100), the nsfc under grant agreements 61672057, 61672058 and 61872294, and a uk royal society international collaboration grant."
2020.acl-main.578.txt,2020,6 Conclusion,"as a departure from prior works, nmn simultaneously estimates the similarity of two entities, by considering both topological structure and neighborhood similarity."
2020.acl-main.578.txt,2020,6 Conclusion,"experimental results show that nmn achieves the best and more robust performance, consistently outperforming competitive methods across datasets and evaluation metrics."
2020.acl-main.578.txt,2020,6 Conclusion,"for any correspondence, please contact yansong feng."
2020.acl-main.578.txt,2020,6 Conclusion,nmn tackles the ubiquitous neighborhood heterogeneity in kgs.
2020.acl-main.578.txt,2020,6 Conclusion,we achieve this by using a new sampling-based approach to choose the most informative neighbors for each entity.
2020.acl-main.578.txt,2020,6 Conclusion,"we have presented nmn, a novel embedded-based framework for entity alignment."
2020.acl-main.578.txt,2020,6 Conclusion,we perform extensive experiments on real-world datasets and compare nmn against 12 recent embedded-based methods.
2020.acl-main.579.txt,2020,7 Conclusion,in this work we provided an annotated test set with ground-truth sentence-level explanations to evaluate the explanation quality of relation extraction models with distant supervision.
2020.acl-main.579.txt,2020,7 Conclusion,our evaluation on the widely used fb-nyt dataset show the effectiveness of our method in achieving state-of-the art performance in both accuracy and explanation quality.
2020.acl-main.579.txt,2020,7 Conclusion,our examination of two baselines show that a model with lower relation extraction accuracy could have higher explanation quality.
2020.acl-main.579.txt,2020,7 Conclusion,our proposed methods are based on changing the representation of the sentences and learning from distractor to teach the model to ignore irrelevant information in a bag.
2020.acl-main.579.txt,2020,7 Conclusion,we proposed methods to improve both the accuracy and explainability.
2020.acl-main.58.txt,2020,5 Conclusion,"experiments confirm the effectiveness of modeling explicit lexical relations, which has not yet been explored by previous works."
2020.acl-main.58.txt,2020,5 Conclusion,"moreover, we find that our method delivers more benefits to data scarcity scenarios."
2020.acl-main.58.txt,2020,5 Conclusion,we hope to provide new guidance for the future slot tagging work.
2020.acl-main.58.txt,2020,5 Conclusion,we present a novel knowledge integration mechanism of incorporating background kb and deep contextual representations to facilitate the few-shot slot tagging task.
2020.acl-main.580.txt,2020,8 Conclusion and Future Work,"in future work, we hope to tackle repeated fields and learn domainspecific candidate generators."
2020.acl-main.580.txt,2020,8 Conclusion and Future Work,"in this initial foray into this challenging problem, we limited our scope to fields with domain-agnostic types like dates and numbers, and which have only one true value in a document."
2020.acl-main.580.txt,2020,8 Conclusion and Future Work,"in this paper, we presented a novel approach to the task of extracting structured information from templatic documents using representation learning."
2020.acl-main.580.txt,2020,8 Conclusion and Future Work,"we are also actively investigating how our learned candidate representations can be used for transfer learning to a new domain and, ultimately, in a few-shot setting."
2020.acl-main.580.txt,2020,8 Conclusion and Future Work,"we showed that our extraction system using this approach not only has promising accuracy on unseen templates in two different domains, but also that the learned representations lend themselves to interpretation of loss cases."
2020.acl-main.581.txt,2020,5 Conclusion,extensive experiments on benchmark datasets show that our method outperforms the existing state-of-the-art approaches.
2020.acl-main.581.txt,2020,5 Conclusion,"in this paper, we propose a teacher-student learning method for single-/multi-source cross-lingual ner, via using source-language models as teachers to train a student model on unlabeled data in the target language."
2020.acl-main.581.txt,2020,5 Conclusion,"the proposed method does not rely on labelled data in the source languages and is capable of leveraging extra information in the unlabelled target-language data, which addresses the limitations of previous label-projection based and model-transfer based methods."
2020.acl-main.581.txt,2020,5 Conclusion,"we also propose a language similarity measuring method based on language identification, to better weight different teacher models."
2020.acl-main.582.txt,2020,5 Conclusion,extensive experiments showed that our model achieves state-of-the-art performances.
2020.acl-main.582.txt,2020,5 Conclusion,"in this paper, we explored aspect-opinion pair extraction (aope) task and proposed synchronous double-channel recurrent network (sdrn)."
2020.acl-main.582.txt,2020,5 Conclusion,"meanwhile, the synchronization unit is devised to integrate high-level interaction information and enable the mutual benefit on opinion entity extraction and relation detection."
2020.acl-main.582.txt,2020,5 Conclusion,"specifically, the opinion entity extraction unit and the relation detection unit are designed to extract aspects, opinion expressions and their relations simultaneously."
2020.acl-main.582.txt,2020,5 Conclusion,"the two units update themselves in a recurrent manner and form two channels, respectively."
2020.acl-main.583.txt,2020,7 Conclusions and Future Work,"according to the description of the conceptual captions dataset, its captions have been hypernymized."
2020.acl-main.583.txt,2020,7 Conclusions and Future Work,examples of this relation involves a caption that mentions the brand of a product or the name of the person in the image.
2020.acl-main.583.txt,2020,7 Conclusions and Future Work,future work should study this additional relation in the context of caption annotation and generation.
2020.acl-main.583.txt,2020,7 Conclusions and Future Work,"however, by studying the examples in the other category, we discovered an additional coherence relation that exists between an image and caption, in which the caption identifies an object or entity in the image–identification."
2020.acl-main.583.txt,2020,7 Conclusions and Future Work,identification is easy to annotate but missing from this work due to the properties of the corpus we annotated.
2020.acl-main.583.txt,2020,7 Conclusions and Future Work,"in addition, the presented dataset, clue, provides opportunities for further theoretical and computational explorations."
2020.acl-main.583.txt,2020,7 Conclusions and Future Work,"representing coherence in image–text presentations can provide a scaffold for organizing, disambiguating and integrating the interpretation of communication across modalities."
2020.acl-main.583.txt,2020,7 Conclusions and Future Work,the experiments described for the coherence relation prediction task set the stage for designing better models for inferring coherence for images–text pairs.
2020.acl-main.583.txt,2020,7 Conclusions and Future Work,the presented work has limitations that can be addressed in future research.
2020.acl-main.583.txt,2020,7 Conclusions and Future Work,this is a step forward towards designing systems that learn commonsense inferences in images and text and use that to communicate naturally and effectively with the users.
2020.acl-main.583.txt,2020,7 Conclusions and Future Work,we show that cross-modal coherence modeling significantly improves the consistency and quality of the generated text with respect to information needs.
2020.acl-main.584.txt,2020,5 Discussion and Conclusion,"as in human perception, knowledge about typical object properties seems to be a valuable source of information for visual language grounding."
2020.acl-main.584.txt,2020,5 Discussion and Conclusion,"future work should look at more complex fusion strategies, possibly coupled with bottom-up recalibration mechanisms (zarrieß and schlangen, 2016; mojsilovic, 2005) to further enhance colour classification under difficult illumination conditions."
2020.acl-main.584.txt,2020,5 Discussion and Conclusion,"however, even early fusion does not yet achieve a perfect balance between top-down and bottom-up processing."
2020.acl-main.584.txt,2020,5 Discussion and Conclusion,our experiment on objects unseen during training looks promising but can be extended towards a more general approach that interfaces colour prediction with object recognition.
2020.acl-main.584.txt,2020,5 Discussion and Conclusion,our fusion models clearly outperform a bottom-up baseline that relies solely on visual input.
2020.acl-main.584.txt,2020,5 Discussion and Conclusion,we also showed that the fusion architecture matters: the early integration of visual and conceptual information and their shared processing appears to be beneficial when colour diagnostic objects have atypical colours.
2020.acl-main.585.txt,2020,5 Conclusion,"by considering a video as a text passage, we solve the nlvl task with a multimodal span-based qa framework."
2020.acl-main.585.txt,2020,5 Conclusion,"however, there are two major differences between video and text."
2020.acl-main.585.txt,2020,5 Conclusion,the effectiveness of vslnet (and even vslbase) suggest that it is promising to explore span-based qa framework to address nlvl problems.
2020.acl-main.585.txt,2020,5 Conclusion,"through experiments, we show that adopting a standard span-based qa framework, vslbase, effectively addresses nlvl problem."
2020.acl-main.585.txt,2020,5 Conclusion,"we further propose vslnet, which introduces a simple and effective strategy named query-guided highlighting, on top of vslbase."
2020.acl-main.585.txt,2020,5 Conclusion,"with qgh, vslnet is guided to search for answers within a predicted coarse region."
2020.acl-main.586.txt,2020,6 Conclusion,"although our proposed models are slightly more robust than existing models, there is still significant scope for improvement."
2020.acl-main.586.txt,2020,6 Conclusion,our work shows that current datasets and models for visual referring expressions fail to make effective use of linguistic structure.
2020.acl-main.586.txt,2020,6 Conclusion,we hope that ref-hard and ref-adv will foster more research in this area.
2020.acl-main.587.txt,2020,7 Conclusion,it is inspired by a mixture-of-experts perspective of multi-head attention.
2020.acl-main.587.txt,2020,7 Conclusion,"mae is trained using a block coordinate descent algorithm, which alternates between updating the responsibilities of the experts and their parameters."
2020.acl-main.587.txt,2020,7 Conclusion,our experiments show that mae outperforms the transformer baselines on machine translation and language modeling benchmarks.
2020.acl-main.587.txt,2020,7 Conclusion,the analysis shows that mae learns to activate different experts.
2020.acl-main.587.txt,2020,7 Conclusion,the code is publicly available at https://github.com/ noahs-ark/mae.
2020.acl-main.587.txt,2020,7 Conclusion,we presented mae.
2020.acl-main.587.txt,2020,7 Conclusion,"with a learned gating function, mae activates different experts on different inputs."
2020.acl-main.588.txt,2020,6 Conclusion,"first, the edge information of the dependency trees needs to be exploited in later work."
2020.acl-main.588.txt,2020,6 Conclusion,"in future work, we can further improve our method in the following aspects."
2020.acl-main.588.txt,2020,6 Conclusion,recently neural structures with syntactical information such as semantic dependency tree and constituent tree are widely employed to enhance the word-level representation of traditional neural networks.
2020.acl-main.588.txt,2020,6 Conclusion,"second and last, domain-specific knowledge can be incorporated into our method as an external learning source."
2020.acl-main.588.txt,2020,6 Conclusion,the results on five datasets demonstrate that dependency tree indeed promotes the final performance when utilized as a sub-module for dual-transformer structure.
2020.acl-main.588.txt,2020,6 Conclusion,these structures are often modeled and described by treelstms or gcns.
2020.acl-main.588.txt,2020,6 Conclusion,"to introduce transformer into our task and diminish the error induced by incorrect dependency trees, we propose a dual-transformer structure which considers the connections in dependency tree as a supplementary gcn module and a transformer-like structure for self alignment in traditional transformer."
2020.acl-main.588.txt,2020,6 Conclusion,we plan to employ an edgeaware graph neural network considering the edge labels.
2020.acl-main.589.txt,2020,6 Conclusion,"all in all, we demonstrate the benefit of incorporating the differentiable window in the attention."
2020.acl-main.589.txt,2020,6 Conclusion,"in the future, we would like to extend our work to make a syntactically-aware window that can automatically learn tree (or phrase) structures."
2020.acl-main.589.txt,2020,6 Conclusion,our experiments show that our proposed methods outperform the baselines significantly across all the tasks.
2020.acl-main.589.txt,2020,6 Conclusion,"specifically, we proposed trainable soft masking and segment-based masking, which can be applied to encoder/decoder self-attentions and cross attention."
2020.acl-main.589.txt,2020,6 Conclusion,"we evaluated our models on four nlp tasks including machine translation, sentiment analysis, subject verb agreement and language modeling."
2020.acl-main.589.txt,2020,6 Conclusion,"we have presented a novel differential window method for dynamic window selection, and used it to improve the standard attention modules by enabling more focused attentions."
2020.acl-main.59.txt,2020,7 Conclusion,"as future work, we will apply madpl in the more complex dialogs and verify the role-aware reward decomposition in other dialog scenarios."
2020.acl-main.59.txt,2020,7 Conclusion,"extensive experiments1 demonstrate the effectiveness, reasonableness and scalability of madpl."
2020.acl-main.59.txt,2020,7 Conclusion,it only requires the annotation of dialog acts in the corpus for pretraining and does not need to build a user simulator explicitly beforehand.
2020.acl-main.59.txt,2020,7 Conclusion,it uses the actor-critic framework to facilitate pretraining and bootstrap rl training in multi-domain task-oriented dialog.
2020.acl-main.59.txt,2020,7 Conclusion,madpl enables the developers to set up a dialog system rapidly from scratch.
2020.acl-main.59.txt,2020,7 Conclusion,we also introduce role-aware reward decomposition to integrate the task knowledge into the algorithm.
2020.acl-main.59.txt,2020,7 Conclusion,"we present a multi-agent dialog policy algorithm, madpl, that trains the user policy and the system policy simultaneously."
2020.acl-main.590.txt,2020,6 Conclusion,"furthermore, by applying adversarial training using the proposed attacks, we are able to significantly improve the robustness of dependency parsers without sacrificing their performance on clean data."
2020.acl-main.590.txt,2020,6 Conclusion,"in this paper, we study the robustness of neural network-based dependency parsing models."
2020.acl-main.590.txt,2020,6 Conclusion,"to the best of our knowledge, adversarial examples to syntactic tasks, such as dependency parsing, have not been explored in the literature."
2020.acl-main.590.txt,2020,6 Conclusion,we develop the first adversarial attack algorithms for this task to successfully find the blind spots of parsers with high success rates.
2020.acl-main.591.txt,2020,7 Conclusion,"apart from the explicit observations in achieving strong perplexity scores, our model reveals several interesting aspects of the quality of the trees learned by the model."
2020.acl-main.591.txt,2020,7 Conclusion,"as a byproduct of our investigation, we release a version of ptb-concat, which contains syntactic structures while at the same time the same pre-processing steps adopted by most previous work on neural language models."
2020.acl-main.591.txt,2020,7 Conclusion,"we investigated linguistic supervision for distance-based structure-aware language models, showing its strengths over transition-based counterparts in language modeling."
2020.acl-main.592.txt,2020,6 Conclusions,experiments on two language modeling tasks show that ess yields improvements of 4.5 and 2.4 perplexity scores over a strong rnn-based baseline.
2020.acl-main.592.txt,2020,6 Conclusions,it learns intra-cell and inter-cell architectures simultaneously.
2020.acl-main.592.txt,2020,6 Conclusions,"meanwhile, the high-level and low-level sub-networks can be learned in a joint fashion."
2020.acl-main.592.txt,2020,6 Conclusions,"more interestingly, it is observed that transferring the pre-learned architectures to other tasks also obtains a promising performance improvement."
2020.acl-main.592.txt,2020,6 Conclusions,"moreover, we present a general model of differentiable architecture search to handle the arbitrary search space."
2020.acl-main.592.txt,2020,6 Conclusions,we have proposed the extended search space (ess) method of nas.
2020.acl-main.593.txt,2020,8 Conclusion,"experiments with bert-large on five text classification and nli datasets yield substantially faster inference compared to the standard approach, up to 80% faster while maintaining similar performance."
2020.acl-main.593.txt,2020,8 Conclusion,"it also allows for controlling the speed/accuracy tradeoff using a single model, without retraining it for any point along the curve."
2020.acl-main.593.txt,2020,8 Conclusion,our approach requires neither additional training time nor significant number of additional parameters compared to the standard approach.
2020.acl-main.593.txt,2020,8 Conclusion,"our method makes early exits for simple instances that require less processing, and thereby avoids running many of the layers of the model."
2020.acl-main.593.txt,2020,8 Conclusion,we presented a method that improves the speed/accuracy tradeoff for inference using pretrained language models.
2020.acl-main.594.txt,2020,7 Conclusion,"for example, once the bootstrapped model has been used to tag verbs containing reduplication, we can confirm the model’s high-confidence predictions and retrain."
2020.acl-main.594.txt,2020,7 Conclusion,"for researchers developing robust morphological analyzers for low resource, morphologically complex languages, this work represents a template of model development which is well-suited for the context."
2020.acl-main.594.txt,2020,7 Conclusion,"in particular, we showed that a robust neural model can be bootstrapped in a relatively short space of time from an incomplete fst."
2020.acl-main.594.txt,2020,7 Conclusion,"in this second iteration, we may find that we no longer need to hallucinate reduplication because it is sufficiently represented in the new training set."
2020.acl-main.594.txt,2020,7 Conclusion,"indeed, the concept of bootstrapping a model implies an iterative development story where much of the scaffolding used in early efforts will eventually fall away."
2020.acl-main.594.txt,2020,7 Conclusion,"producing a viable morphological analyzer is the first step towards building improved dictionary search interfaces, spell-checking tools, and computer-assisted language learning applications for communities who speak low-resource languages."
2020.acl-main.594.txt,2020,7 Conclusion,"similarly, once we have applied the complete neural model to a corpus of natural text, we will no longer need to approximate distributional information."
2020.acl-main.594.txt,2020,7 Conclusion,"the pattern of training robust systems on data that has been augmented by the knowledge captured in symbolic systems could be applied to areas outside of morphological analysis, and is a promising avenue of future exploration."
2020.acl-main.594.txt,2020,7 Conclusion,this work represents a successful first iteration of a process whereby the morphological model can be continually improved.
2020.acl-main.594.txt,2020,7 Conclusion,"we have shown that complex features of polysynthetic morphology, such as reduplication and distributional morphotactic information, can be simulated in the dataset and used to train a robust neural morphological analyzer for a polysynthetic language."
2020.acl-main.595.txt,2020,5 Conclusion,"empirical results show that our framework significantly outperforms other proposed methods, achieving the state-of-the-art result on all five datasets across different domains."
2020.acl-main.595.txt,2020,5 Conclusion,"further, an adversarial training procedure is designed to capture information from both the source and target domains."
2020.acl-main.595.txt,2020,5 Conclusion,"in our method, we investigate an automatic distant annotator to build the labeled target domain dataset, effectively address the oov issue."
2020.acl-main.595.txt,2020,5 Conclusion,"in this paper, we intuitively propose a unified framework via coupling distant annotation and adversarial training for the cross-domain cws task."
2020.acl-main.596.txt,2020,7 Conclusion and Future Work,"although the evaluation is based on segmentation points, our model outputs much richer structure."
2020.acl-main.596.txt,2020,7 Conclusion and Future Work,"combined with the ability to process infixation and reduplication, our system improves access for geographically diverse low-resource languages."
2020.acl-main.596.txt,2020,7 Conclusion and Future Work,future work will aim to extend the current model to capture particularly challenging morphological patterns such as templatic non-concatenative morphology and polysynthetic composition.
2020.acl-main.596.txt,2020,7 Conclusion and Future Work,"in this paper, we develop a model for morphological analysis that exploits typological features to achieve the best performance on a wide range of languages."
2020.acl-main.596.txt,2020,7 Conclusion and Future Work,it can also tell us the productivity of each morphological process and thus can obtain much deeper knowledge in terms of morphological structures of languages.
2020.acl-main.596.txt,2020,7 Conclusion and Future Work,"our next step will be to attempt to automate the determination of language typology, yielding somewhat better performance with a system requiring no human intervention per language at all."
2020.acl-main.596.txt,2020,7 Conclusion and Future Work,the tool is publicly available here: https://github.com/xuhongzhi/parama2.
2020.acl-main.596.txt,2020,7 Conclusion and Future Work,this unsupervised model can be quickly and easily extended to novel languages without data annotation or expert input.
2020.acl-main.597.txt,2020,8 Conclusion,"despite their relatively small magnitudes, our estimates of mutual information between class and form accounted for between 25% and 60% of the class’ entropy, even after relevant controls, and mi between class and meaning accounted for between 13% and nearly 40%."
2020.acl-main.597.txt,2020,8 Conclusion,"in sum, this paper has proposed a new information-theoretic method for quantifying the strength of morphological relationships, and applied it to declension class."
2020.acl-main.597.txt,2020,8 Conclusion,we adduce new evidence that declension class membership is not wholly idiosyncratic nor fully deterministic based on form or meaning in czech and german.
2020.acl-main.597.txt,2020,8 Conclusion,"we also observe that classes that have high mi(c = c;v | g) often have high mi(c = c;w | g), with a few noted exceptions that have specific orthographic (e.g., german umlauted plurals), or semantic (e.g., czech masculine animacy) properties."
2020.acl-main.597.txt,2020,8 Conclusion,"we analyze results per-class, and find that classes vary in how much information they share with meaning and form."
2020.acl-main.597.txt,2020,8 Conclusion,we quantify mutual information and find estimates which range from 0.2 bits to nearly one bit.
2020.acl-main.597.txt,2020,8 Conclusion,"we verify and build on existing linguistic findings, by showing that the mutual information quantities between declension class, orthographic form, and lexical semantics are statistically significant."
2020.acl-main.598.txt,2020,6 Conclusion,"by substituting the current transducers in our pipeline, we expect that we will be able to improve the overall performance of our system."
2020.acl-main.598.txt,2020,6 Conclusion,"further analysis showed the importance of our individual components and detected possible sources of errors, like wrongly identified edit trees early in the pipeline or syncretism."
2020.acl-main.598.txt,2020,6 Conclusion,"in the future, we will explore the following directions: (i) a difficult challenge for our proposed system is to correctly determine the paradigm size."
2020.acl-main.598.txt,2020,6 Conclusion,"introducing best-match accuracy, a metric for the task, we evaluated our system on a typologically diverse set of 14 languages."
2020.acl-main.598.txt,2020,6 Conclusion,"our system obtained promising results for most of our languages and even outperformed a minimally supervised baseline on basque, english, and navajo."
2020.acl-main.598.txt,2020,6 Conclusion,"since transfer across related languages has shown to be beneficial for morphological tasks (jin and kann, 2017; mccarthy et al., 2019; anastasopoulos and neubig, 2019, inter alia), future work could use typologically aware priors to guide the number of paradigm slots based on the relationships between languages.(ii) we plan to explore other methods, like word embeddings, to incorporate context information into our feature function.(iii) we aim at developing better performing string transduction models for the morphological inflection step."
2020.acl-main.598.txt,2020,6 Conclusion,"we further developed a system for the task, which performs the following steps: (i) edit tree retrieval, (ii) additional lemma retrieval, (iii) paradigm size discovery, and (iv) inflection generation."
2020.acl-main.598.txt,2020,6 Conclusion,"we proposed unsupervised morphological paradigm completion, a novel morphological generation task."
2020.acl-main.599.txt,2020,6 Conclusion,"besides, the currently existing methods actually cannot process a long document without truncating or slicing it into fragments."
2020.acl-main.599.txt,2020,6 Conclusion,how to model long documents is still a problem that needs to be solved.
2020.acl-main.599.txt,2020,6 Conclusion,improving our graph structure of representing the document as well as the document-level pretraining tasks is our future research goals.
2020.acl-main.599.txt,2020,6 Conclusion,"in this work, we present a novel multi-grained mrc framework based on graph attention networks and bert."
2020.acl-main.599.txt,2020,6 Conclusion,"on the natural questions dataset, which contains two sub-tasks predicting a paragraph-level long answer and a token-level short answer, our method jointly trains the two sub-tasks to consider the dependencies of the twograined answers."
2020.acl-main.599.txt,2020,6 Conclusion,the experiments show that our proposed methods are effective and outperform the previously existing methods by a large margin.
2020.acl-main.599.txt,2020,6 Conclusion,we model documents at different levels of granularity to learn the hierarchical nature of the document.
2020.acl-main.6.txt,2020,5 Conclusion,experimental results demonstrate that our model is powerful to generate much more informative and coherent responses than the competitive baseline models.
2020.acl-main.6.txt,2020,5 Conclusion,"in future work, we plan to analyze each turn of dialogue with reinforcement learning architecture, and to enhance the diversity of the whole dialogue by avoiding knowledge reuse."
2020.acl-main.6.txt,2020,5 Conclusion,"meanwhile, the knowledge-aware pointer networks we designed allow copying important words, usually oov words, from knowledge."
2020.acl-main.6.txt,2020,5 Conclusion,we propose a knowledge grounded conversational model with a recurrent knowledge interactive generator that effectively exploits multiple relevant knowledge to produce appropriate responses.
2020.acl-main.60.txt,2020,9 Conclusion,"in this paper, we propose to use dialog paraphrase as data augmentation to improve the response generation quality of task-oriented dialog systems."
2020.acl-main.60.txt,2020,9 Conclusion,"it also beats other data augmentation methods, especially under the low-resource settings."
2020.acl-main.60.txt,2020,9 Conclusion,our framework achieves significant improvements when it is applied to state-of-the-art response generation models on two datasets.
2020.acl-main.60.txt,2020,9 Conclusion,we give out the definition of the paraphrase for a dialog utterance and design an approach to construct paraphrase dataset from a dialog corpus.
2020.acl-main.60.txt,2020,9 Conclusion,"we propose a paraphrase augmented response generation (parg) framework which consists of a paraphrase generation model, an utterance filter and a response generation model, where the models are trained jointly to take fully advantage of the paraphrase data for better response generation performance."
2020.acl-main.600.txt,2020,6 Conclusion,"in this paper, we present two approaches to improve the quality of synthetic qa data for unsupervised question answering."
2020.acl-main.600.txt,2020,6 Conclusion,"our method outperforms the previous unsupervised state-of-the-art models on squad 1.1, and newsqa, and achieves the best performance in the few-shot learning setting."
2020.acl-main.600.txt,2020,6 Conclusion,we first use the wikipedia paragraphs and its references to construct a synthetic qa data refqa and then use the qa model to iteratively refine data over re-fqa.
2020.acl-main.601.txt,2020,6 Conclusions and Future Works,experimental results on the hotpotqa data set demonstrated the effectiveness of our approach.
2020.acl-main.601.txt,2020,6 Conclusions and Future Works,"in order to tackle the labeled data shortage problem, we learned the structural patterns from the unlabeled data by the hidden semi-markov model."
2020.acl-main.601.txt,2020,6 Conclusions and Future Works,"moreover, we explored the generated results to facilitate the real-world application of machine reading comprehension."
2020.acl-main.601.txt,2020,6 Conclusions and Future Works,we first built a multi-hop qg model and guided it to satisfy the logical rationality by the reasoning chain extracted from a given text.
2020.acl-main.601.txt,2020,6 Conclusions and Future Works,we have proposed an approach to generate the questions required multi-hop reasoning in low-resource conditions.
2020.acl-main.601.txt,2020,6 Conclusions and Future Works,we will investigate the robustness and scalability of the model.
2020.acl-main.601.txt,2020,6 Conclusions and Future Works,"with the patterns as a prior, we transferred this fundamental knowledge into the generation model to produce the optimal results."
2020.acl-main.602.txt,2020,6 Conclusions,"for modeling, we plan to explore recent advances in conditional language models for jointly modeling qa with generating their derivations."
2020.acl-main.602.txt,2020,6 Conclusions,"for scalability, we have carefully developed a crowdsourced framework for annotating existing rc datasets with derivations."
2020.acl-main.602.txt,2020,6 Conclusions,one immediate future work is to evaluate state-of-the-art rc systems’ internal reasoning on our dataset.
2020.acl-main.602.txt,2020,6 Conclusions,"our experiments have demonstrated that our framework produces high-quality derivations, and that automatic evaluation metrics using multiple reference derivations can reliably capture oracle derivations."
2020.acl-main.602.txt,2020,6 Conclusions,"the experiments using two simple baseline models high-light the nature of r4c, namely that the derivation generation task is not simply the sf detection task."
2020.acl-main.602.txt,2020,6 Conclusions,"towards evaluating rc systems’ internal reasoning, we have proposedr4c that requires systems not only to output answers but also to give their derivations."
2020.acl-main.602.txt,2020,6 Conclusions,"we make the dataset, automatic evaluation script, and baseline systems publicly available at https://naoya-i.github.io/r4c/."
2020.acl-main.603.txt,2020,6 Conclusion,"in this paper, we propose a chunking policy network for machine reading comprehension, which enables a model learn to chunk lengthy documents in a more flexible way via reinforcement learning."
2020.acl-main.603.txt,2020,6 Conclusion,our approach outperforms benchmark models across different datasets.
2020.acl-main.603.txt,2020,6 Conclusion,we also add a recurrent mechanism to allow the information to flow across segments so that the model could have knowledge beyond the current segment when selecting answers.
2020.acl-main.603.txt,2020,6 Conclusion,"we have performed extensive experiments on three public datasets of machine reading comprehension: coqa, quac, and triviaqa."
2020.acl-main.604.txt,2020,6 Conclusion,"furthermore, the rikinet ensemble achieves the new state-of-the-art results at 76.1 f1 on long-answer and 61.3 f1 on shortanswer tasks, which significantly outperforms all the other models on both criteria."
2020.acl-main.604.txt,2020,6 Conclusion,"on the natural questions dataset, the rikinet is the first single model that outperforms the single human performance."
2020.acl-main.604.txt,2020,6 Conclusion,"the rikinet consists of a dynamic paragraph dual-attention reader which learns the token-level, paragraphlevel and question representations, and a multi-level cascaded answer predictor which jointly predicts the long and short answers in a cascade manner."
2020.acl-main.604.txt,2020,6 Conclusion,"we propose the rikinet, which reads the wikipedia pages to answer the natural question."
2020.acl-main.605.txt,2020,7 Conclusion,graph-structured meaning representations provide an effective way to encode rich semantic information of natural language sentences and have been extensively studied recently.
2020.acl-main.605.txt,2020,7 Conclusion,"in particular, we introduced a novel neural graph rewriting system and developed a new state-of-the-art semantic parser for variable-in-situ graphs."
2020.acl-main.605.txt,2020,7 Conclusion,we enriched the discussion by studying an alternative graph-based representation for underspecified logical forms.
2020.acl-main.606.txt,2020,7 Conclusion and Future Work,"for intended meaning, we investigate how grammatical errors affect the understanding of sentences as well as how grammatical error correction (gec) can contribute to the parsing."
2020.acl-main.606.txt,2020,7 Conclusion and Future Work,"for literal meaning, we probe the semantic parsing of multiple state-of-the-art neural parsers and give detailed analysis of effects from grammatical errors."
2020.acl-main.606.txt,2020,7 Conclusion and Future Work,"future research may involve tailoring existing parsers to learner data, combining literal and intended meanings in a unified framework, evaluating gec models in terms of speakers’ intention and parsing for other languages."
2020.acl-main.606.txt,2020,7 Conclusion and Future Work,"in this paper, we formulate the esl semantic parsing task based on the divergence on literal and intended meanings."
2020.acl-main.606.txt,2020,7 Conclusion and Future Work,"results reveal three facts: 1) semantic parsing is sensitive to non-canonical expressions, and the parsing performance varies with regard to the distribution as well as types of grammatical errors; 2) factorization-based parser is the most promising parser to process learner english; and 3) gec has a positive, but limited influence on the parsing of intended meaning."
2020.acl-main.606.txt,2020,7 Conclusion and Future Work,this paper shows a pilot study on the semantic parsing for learner language.
2020.acl-main.606.txt,2020,7 Conclusion and Future Work,we establish parallel meaning representations by combining the complementary strengths of knowledge-intensive erg-licensed analysis and dependency tree annotations through a new reranking model.
2020.acl-main.607.txt,2020,6 Conclusion,"by adding unlabeled data, our model exhibits further performance improvements."
2020.acl-main.607.txt,2020,6 Conclusion,"in particular, our semi-supervised model performs well in the low resource setting and on the out-of-domain test set."
2020.acl-main.607.txt,2020,6 Conclusion,"in this work, we proposed a semi-supervised learning model for semantic dependency parsing using crf autoencoders."
2020.acl-main.607.txt,2020,6 Conclusion,our code is publicly available at https://github.com/jzxxx/semi-sdp.
2020.acl-main.607.txt,2020,6 Conclusion,"our model is composed of a discriminative neural encoder producing a dependency graph conditioned on an input sentence, and a generative neural decoder for input reconstruction based on the dependency graph."
2020.acl-main.607.txt,2020,6 Conclusion,our model outperforms the baseline on multiple target representations.
2020.acl-main.607.txt,2020,6 Conclusion,"the model works in an arc-factored fashion, promising end-to-end learning and efficient parsing."
2020.acl-main.607.txt,2020,6 Conclusion,this points to future directions of applying our model to low-resource languages and cross-domain settings.
2020.acl-main.607.txt,2020,6 Conclusion,we evaluated our model under both fullsupervision settings and semi-supervision settings.
2020.acl-main.608.txt,2020,6 Conclusion,"experimental results show that our framework is effective, and compatible with supervised training."
2020.acl-main.608.txt,2020,6 Conclusion,"in this work, aiming to reduce annotation, we propose a two-stage semantic parsing framework."
2020.acl-main.608.txt,2020,6 Conclusion,the first stage utilizes the dual structure of an unsupervised paraphrase model to rewrite the input natural language utterance into canonical utterance.
2020.acl-main.608.txt,2020,6 Conclusion,"three self-supervised tasks, namely denoising auto-encoder, back-translation and dual reinforcement learning, are introduced to iteratively improve our model through pre-training and cycle learning phases."
2020.acl-main.609.txt,2020,7 Conclusion,"experiments on the standard gmb dataset show that our method is high effective, achieving the best results in the literature."
2020.acl-main.609.txt,2020,7 Conclusion,"in particular, we use gat for representing syntax in encoding, and representing a structural backbone for decoding."
2020.acl-main.609.txt,2020,7 Conclusion,"we investigated the representation of structural information for discourse representation tree structure parsing, showing that a graph neural network can bring significant improvements."
2020.acl-main.61.txt,2020,6 Conclusion,"focusing on the cbr task, we propose a novel response-anticipated document memory to exploit and memorize the document information that is important in response generation."
2020.acl-main.61.txt,2020,6 Conclusion,the teacher accesses the response and learns a response-aware weight matrix; the student learns to estimate the weight matrix in the teacher model and construct the response-anticipated document memory.
2020.acl-main.61.txt,2020,6 Conclusion,we construct the response-anticipated memory by a teacher-student framework.
2020.acl-main.61.txt,2020,6 Conclusion,we verify our model on both automatic and human evaluations and experimental results show our model obtains the state-of-the-art performance on the cbr task.
2020.acl-main.610.txt,2020,4 Conclusions,"beyond strong tse results, our method demonstrates a novel use of pre-trained mlms, using their predictions directly rather than relying on their states for fine-tuning."
2020.acl-main.610.txt,2020,4 Conclusions,"the method uses the power of lm predictions to locate indicative patterns for the concept class indicated by the seed terms, and then to generalize these patterns to other corpus locations."
2020.acl-main.610.txt,2020,4 Conclusions,"we introduce an lm-based tse method, reaching state-of-the-art results."
2020.acl-main.611.txt,2020,6 Conclusion and Future Work,experimental results show our model outperforms other lexicon-based models in the performance and efficiency.
2020.acl-main.611.txt,2020,6 Conclusion and Future Work,"in this paper, we introduce a flat-lattice transformer to incorporate lexicon information for chinese ner."
2020.acl-main.611.txt,2020,6 Conclusion and Future Work,the core of our model is converting lattice structure into a set of spans and introducing the specific position encoding.
2020.acl-main.611.txt,2020,6 Conclusion and Future Work,we leave adjusting our model to different kinds of lattice or graph as our future work.
2020.acl-main.612.txt,2020,6 Conclusion,"fgs2ee first uses the word embeddings of semantic type words to generate semantic embeddings, and then combines them with existing entity embeddings through linear aggregation."
2020.acl-main.612.txt,2020,6 Conclusion,"for the future work, we are planning to extract fine-grained semantic types from unlabelled documents and use the relatedness between the finegrained types and contexts as distant supervision for entity linking."
2020.acl-main.612.txt,2020,6 Conclusion,"in this paper, we presented a simple yet effective method, fgs2ee, to inject fine-grained semantic information into entity embeddings to reduce the distinctiveness and facilitate the learning of contextual commonality."
2020.acl-main.612.txt,2020,6 Conclusion,"our entity embeddings draw entities of similar types closer, while entities of different types are drawn further."
2020.acl-main.612.txt,2020,6 Conclusion,thus can facilitate the learning of semantic commonalities about entity-context and entity-entity relations.
2020.acl-main.612.txt,2020,6 Conclusion,we have achieved new state-of-the-art performance using our entity embeddings.
2020.acl-main.613.txt,2020,7 Conclusions,"for dt, we trained two smt systems: the first one was built to produce word forms, and the second one to produce word lemmas."
2020.acl-main.613.txt,2020,7 Conclusions,"for example, the word ”development” probably in most cases means in medicine the growth or spread of a disease (or a tumor), while in the general domain we can not say without a context, and in that case, the need for linguistics information in the queries will be more important to solve the translation ambiguity."
2020.acl-main.613.txt,2020,7 Conclusions,"furthermore, we performed lemmatization and stemming on the collection that was translated using the smt system that produces word forms."
2020.acl-main.613.txt,2020,7 Conclusions,"however, we emphasize that the way we trained our mt systems is very domain-specific (medical domain), and we made use of a vast amount of medical data (monolingual and parallel)."
2020.acl-main.613.txt,2020,7 Conclusions,"in our experiments, nmt improved retrieval results in both qt and dt, but the qt approach is still superior, so the results are consistent with the findings from the smt experiments."
2020.acl-main.613.txt,2020,7 Conclusions,our experiments suggest that this approach is even more effective (better retrieval results).
2020.acl-main.613.txt,2020,7 Conclusions,"so far, the qt approach has been preferred mainly for efficiency reasons (less space and computation needed)."
2020.acl-main.613.txt,2020,7 Conclusions,"the results showed that a well-tuned qt system outperforms dt, which is a positive result with an important impact on practical applications."
2020.acl-main.613.txt,2020,7 Conclusions,the smt systems for qt were specifically trained and tuned to translate medical search queries.
2020.acl-main.613.txt,2020,7 Conclusions,"this completely new paradigm in machine translation tends to improve the fluency of generated output (which is appreciated by humans), but often mismatches content and adequacy (which might hurt the performance in ir)."
2020.acl-main.613.txt,2020,7 Conclusions,this makes our comparative study very task-oriented.
2020.acl-main.613.txt,2020,7 Conclusions,"this should be considered when comparing qt and dt approaches; thus, the reader should be careful when drawing the same conclusion of this work while working on a different domain."
2020.acl-main.613.txt,2020,7 Conclusions,"to conduct this study, we investigated various mt systems and their configurations and performed a thorough large-scale evaluation based on the test collection produced within the clef ehealth tasks on patient-centered information retrieval during 2013–2015, and extended with additional relevance assessments."
2020.acl-main.613.txt,2020,7 Conclusions,"we also investigated the effect of using neural machine translation, which is now considered the state-of-the-art in many domains."
2020.acl-main.613.txt,2020,7 Conclusions,we experimented with both statistical and neural mt paradigms.
2020.acl-main.613.txt,2020,7 Conclusions,"we presented a comparative study between querytranslation (qt), and document translation (dt) approaches in the cross-lingual information retrieval (clir) task."
2020.acl-main.613.txt,2020,7 Conclusions,we then used these two systems to translate the test collection into seven european languages.
2020.acl-main.613.txt,2020,7 Conclusions,"when dealing with general domain test collection, some search terms might have a different meaning in different domains."
2020.acl-main.614.txt,2020,4 Conclusion and Future Work,experiments on an offline dataset and live product search traffic showed that our method improves significantly over baselines.
2020.acl-main.614.txt,2020,4 Conclusion and Future Work,"this allows us to address the class imbalance of our datasets, while also generating samples that help robustly train the classifier."
2020.acl-main.614.txt,2020,4 Conclusion and Future Work,"to train the model end to end, we modified the cross-entropy loss, allowing us to avoid optimizing a minimax objective."
2020.acl-main.614.txt,2020,4 Conclusion and Future Work,"we built upon ideas for textual entailment, and used a word by word attention layer to help create item representations conditioned on an input query."
2020.acl-main.614.txt,2020,4 Conclusion and Future Work,we developed an end-to-end model with hard to classify query generation for retrieval in ecommerce product search.
2020.acl-main.614.txt,2020,4 Conclusion and Future Work,"we trained a generator that yields representations of queries that are mismatched to a source item, while at the same time being “realistic”."
2020.acl-main.615.txt,2020,7 Conclusion,theoretical and empirical evidence show label smoothing adds undesirable constraints to the model and is the hardest to tune of the regularizers tested.
2020.acl-main.615.txt,2020,7 Conclusion,we discuss the properties of generalized entropy regularization and provide empirical results on two language generation tasks.
2020.acl-main.615.txt,2020,7 Conclusion,we find entropy regularization leads to improvements over baseline systems on evaluation metrics for all values of the parameter α with our regularizer djα .
2020.acl-main.615.txt,2020,7 Conclusion,we therefore advocate the use of alternate forms of entropy regularization for language generation tasks.
2020.acl-main.616.txt,2020,6 Conclusion and Future Work,"gating-enhanced architecture enjoys both the advantage of mhdpa and self-regulated gating mechanism, allowing for the pseudo-highway information flow for better convergence by elastically introducing a few trainable parameters."
2020.acl-main.616.txt,2020,6 Conclusion and Future Work,"in the future, it is necessary to interpret the semantics that transformer layers in different depths can convey, which is beneficial for the computing-efficiency."
2020.acl-main.616.txt,2020,6 Conclusion and Future Work,it is empirically proved that self-gating units on shallow layers could provide more internal representations of importance and significantly benefit for convergence.
2020.acl-main.616.txt,2020,6 Conclusion and Future Work,it outperforms or matches the performance of common transformer variants without hyperparameter tuning.
2020.acl-main.616.txt,2020,6 Conclusion and Future Work,this also supports the argument that different levels of transformer components attend to different semantic aspects while lower levels pay more attention to local regions.
2020.acl-main.617.txt,2020,6 Conclusion,"atth achieves new sota on wn18rr and yago3-10, real-world kgs which exhibit hierarchical structures."
2020.acl-main.617.txt,2020,6 Conclusion,"atth learns embeddings with trainable hyperbolic curvatures, allowing it to learn the right geometry for each relationship and generalize across multiple embedding dimensions."
2020.acl-main.617.txt,2020,6 Conclusion,"future directions for this work include exploring other tasks that might benefit from hyperbolic geometry, such as hypernym detection."
2020.acl-main.617.txt,2020,6 Conclusion,the proposed attention-based transformations can also be extended to other geometric operations.
2020.acl-main.617.txt,2020,6 Conclusion,"we introduce atth, a hyperbolic kg embedding model that leverages the expressiveness of hyperbolic space and attention-based geometric transformations to learn improved kg representations in low-dimensions."
2020.acl-main.618.txt,2020,4 Conclusion and Future Work,"one particularly exciting direction is the application of our classification-based self-learning framework on top of the most recent methods that induce bilingual spaces via non-linear alignments (glavasˇ and vulic´, 2020; mohiuddin and joty, 2020)."
2020.acl-main.618.txt,2020,4 Conclusion and Future Work,the code is available online at: https://github.com/mladenk42/classymap.
2020.acl-main.618.txt,2020,4 Conclusion and Future Work,"this proof-of-concept work opens up a wide spectrum of interesting avenues for future research, including the use of more powerful classifiers, more sophisticated features (e.g., character-level transformers), and fine-grained linguistic analyses on the importance of disparate features over different language pairs."
2020.acl-main.618.txt,2020,4 Conclusion and Future Work,"we introduced classymap, a novel classification-based approach to self-learning, which is a crucial component of projection-based cross-lingual word embedding induction models in low-data regimes."
2020.acl-main.618.txt,2020,4 Conclusion and Future Work,"we reported its usefulness and robustness across a wide spectrum of diverse language pairs in the bli task, confirming the usefulness of learning classifiers both as part of the self-learning procedure as well as for the final word retrieval in the bli task."
2020.acl-main.619.txt,2020,6 Conclusion,"back to our title: if, in st, gender is still in danger, we encourage our community to start its rescue from must-she and the findings discussed in this paper."
2020.acl-main.619.txt,2020,6 Conclusion,"by eating (audio, text) pairs, st has a potential advantage: the possibility to infer speakers’ gender from input audio signals."
2020.acl-main.619.txt,2020,6 Conclusion,"if, like human beings, “machine learning is what it eats”, the different “diet” of mt and st models can help them to develop different skills."
2020.acl-main.619.txt,2020,6 Conclusion,"one is the proper treatment of gender, a problem when translating from languages without productive grammatical gender into gender-marked ones."
2020.acl-main.619.txt,2020,6 Conclusion,"our evaluation shows that, in spite of lower overall performance, the direct approach can actually exploit audio information to better handle speaker-dependent gender phenomena."
2020.acl-main.619.txt,2020,6 Conclusion,"these are out of reach for cascade solutions, unless the mt step is supplied with external (not always accessible) knowledge about the speaker."
2020.acl-main.619.txt,2020,6 Conclusion,"to this aim, we created must-she, a benchmark annotated with different types of gender-related phenomena in two language directions."
2020.acl-main.619.txt,2020,6 Conclusion,"we investigated for the first time the importance of this information in st, analysing the behaviour of cascade (the state of the art in the field) and end-to-end st technology (the emerging approach)."
2020.acl-main.619.txt,2020,6 Conclusion,"with respect to this problem, by eating parallel texts during training, mt performance is bounded by the statistical patterns learned from written material."
2020.acl-main.62.txt,2020,6 Conclusions,"for future work, we will explore the scenarios that annotations are absent for all expert dialogues."
2020.acl-main.62.txt,2020,6 Conclusions,the experimental results confirm that act-vrnn achieves better task completion compared with the state-of-the-art in two settings that consider partially labeled or unlabeled dialogues.
2020.acl-main.62.txt,2020,6 Conclusions,"we design a novel reward function to first model dialogue progress, and estimate action rewards by determining whether the action leads to similar progress as expert dialogues."
2020.acl-main.62.txt,2020,6 Conclusions,we formulate a generative model to jointly infer action labels and learn action embeddings.
2020.acl-main.62.txt,2020,6 Conclusions,we study the problem of semi-supervised policy learning and propose act-vrnn to provide more effective and stable rewards estimations.
2020.acl-main.620.txt,2020,6 Conclusion and Future Work,another promising direction is to design more powerful training strategies to replace the baby step.
2020.acl-main.620.txt,2020,6 Conclusion and Future Work,"as our model is not limited to machine translation, it is interesting to validate the proposed framework into other nlp tasks that need to exploit cl."
2020.acl-main.620.txt,2020,6 Conclusion and Future Work,it surprisingly draws a similar changing curve as human confidence.
2020.acl-main.620.txt,2020,6 Conclusion and Future Work,"our contributions are mainly in: • we propose to estimate the data uncertainty of each training example as its difficulty, which is more explainable and comprehensive.• we introduce a self-adaptive cl strategy that evaluates the model uncertainty to govern the curriculum by the model itself.• the extensive experiments on various translation tasks and model settings demonstrate the universal-effectiveness of the proposed framework."
2020.acl-main.620.txt,2020,6 Conclusion and Future Work,our method is able to achieve over 50% accelerate rate on model convergence.• quantitative and qualitative analyses indicate that the model confidence is fluctuant at the training time.
2020.acl-main.620.txt,2020,6 Conclusion and Future Work,"we propose a novel uncertainty-aware framework to improve the two key components in cl for nmt, i.e.data difficulty measurement and curriculum arrangement."
2020.acl-main.621.txt,2020,5 Conclusion,"in this paper, we close the gap and consider deidentification of clinical text together with concept extraction, a possible downstream application."
2020.acl-main.621.txt,2020,5 Conclusion,we investigate the effects of de-identification on concept extraction and show that it positively influences the concept extraction performance.
2020.acl-main.621.txt,2020,5 Conclusion,"we propose two models to learn both tasks jointly, a multitask model and a stacked model, and set the new state of the art on medical concept extraction benchmark datasets for english and spanish."
2020.acl-main.622.txt,2020,6 Conclusion,empirical results on two widelyused coreference datasets demonstrate the effectiveness of our model.
2020.acl-main.622.txt,2020,6 Conclusion,"furthermore, a new speaker modeling strategy can also boost the performance in dialogue settings."
2020.acl-main.622.txt,2020,6 Conclusion,"in future work, we will explore novel approaches to generate the questions based on each mention, and evaluate the influence of different question generation methods on the coreference resolution task."
2020.acl-main.622.txt,2020,6 Conclusion,"in this paper, we present corefqa, a coreference resolution model that casts anaphora identification as the task of query-based span prediction in question answering."
2020.acl-main.622.txt,2020,6 Conclusion,it also makes data augmentation using a plethora of existing question answering datasets possible.
2020.acl-main.622.txt,2020,6 Conclusion,we showed that the proposed formalization can successfully retrieve mentions left out at the mention proposal stage.
2020.acl-main.623.txt,2020,8 Conclusions and Future Work,"for best results, one should use a combination of aleatoric and epistemic uncertainty estimates and tune the parameters of uncertainty estimation methods using a development set."
2020.acl-main.623.txt,2020,8 Conclusions and Future Work,"future work would include a comparison with other, more complex, methods for uncertainty estimation, incorporating uncertainty to affect model decisions over time, and further investigating links between uncertainty values and linguistic features of the input."
2020.acl-main.623.txt,2020,8 Conclusions and Future Work,our results indicate that the effect of data uncertainty and model uncertainty varies across datasets due to differences in their respective properties.
2020.acl-main.623.txt,2020,8 Conclusions and Future Work,"the methods presented here can be selected based on knowledge of the properties of the data at hand, for example prioritising the use of aleatoric uncertainty estimates on imbalanced and heterogeneous datasets such as pheme."
2020.acl-main.623.txt,2020,8 Conclusions and Future Work,"using uncertainty estimation methods can help identify which instances are hard for the model to classify, thus highlighting the areas where one should focus during model development."
2020.acl-main.623.txt,2020,8 Conclusions and Future Work,we have also shown how uncertainty estimates can be used to interpret model decisions over time.
2020.acl-main.623.txt,2020,8 Conclusions and Future Work,"we have demonstrated two ways in which uncertainty estimates can be leveraged to remove instances that are likely to be incorrectly predicted, so that making a decision concerning those instances can be prioritised by a human."
2020.acl-main.623.txt,2020,8 Conclusions and Future Work,we have presented a method for obtaining model and data uncertainty estimates on the task of rumour verification in twitter conversations.
2020.acl-main.624.txt,2020,6 Conclusion,"both systems are retrained whenever new annotations are made, forming the human-in-the-loop."
2020.acl-main.624.txt,2020,6 Conclusion,"in a user study, results show that users prefer our approach compared to the typical annotation process; annotation speed improves by around 35% when using our system relative to using no reranking support."
2020.acl-main.624.txt,2020,6 Conclusion,"in the future, we want to investigate more powerful recommenders, combine interactive entity linking with knowledge base completion and use online learning to leverage deep models, despite their long training time."
2020.acl-main.624.txt,2020,6 Conclusion,"in this paper, we evaluate on three datasets: aida, which is often used to validate state-of-the-art entity linking systems as well as wwo and 1641 from the humanities."
2020.acl-main.624.txt,2020,6 Conclusion,"it can be applied to any domain, only requiring a knowledge base whose entities have a label and a description."
2020.acl-main.624.txt,2020,6 Conclusion,"it consists of two main components: recommenders that are algorithms that suggest potential annotations to users and a ranker that, given a mention span, ranks potential entity candidates so that they show up higher in the candidate list, making it easier to find for users."
2020.acl-main.624.txt,2020,6 Conclusion,"our approach does not require the existence of external resources like labeled data, tools like named entity recognizers or large-scale resources like wikipedia."
2020.acl-main.624.txt,2020,6 Conclusion,we presented a domain-agnostic annotation approach for annotating entity linking for low-resource domains.
2020.acl-main.624.txt,2020,6 Conclusion,"we show that in simulation, only a very small subset needs to be annotated (fewer than 100) for the ranker to reach high accuracy."
2020.acl-main.625.txt,2020,6 Conclusion,"in addition, we presented in-depth analysis and extensive ablation studies on various aspects of the model functioning mechanism and architecture design, showing the necessity of our design contribution in achieving good results."
2020.acl-main.625.txt,2020,6 Conclusion,"in this paper, we have demonstrated that small amount of unstructured natural language descriptions for object classes can provide enough information to fine-tune an entire pretrained neural network to perform classification task on class labels unseen during training."
2020.acl-main.625.txt,2020,6 Conclusion,we have achieved state-of-the-art performance for natural language guided zero-shot image classification tasks on 4 public datasets with practical metadata requirements.
2020.acl-main.626.txt,2020,6 Conclusion,"applying our proposed controlled crowdsourcing protocol to qa-srl successfully attains truly scalable high-quality annotation by laymen, facilitating future research of this paradigm."
2020.acl-main.626.txt,2020,6 Conclusion,"exploiting the open nature of the qa-srl schema, our non-expert annotators produce rich argument sets with many valuable implicit arguments."
2020.acl-main.626.txt,2020,6 Conclusion,"finally, we suggest that our simple yet rigorous controlled crowdsourcing protocol would be effective for other challenging annotation tasks, which often prove to be a hurdle for research projects."
2020.acl-main.626.txt,2020,6 Conclusion,"indeed, thanks to effective and practical training over the crowdsourcing platform, our workers’ annotation quality, and particularly its coverage, are on par with expert annotation."
2020.acl-main.626.txt,2020,6 Conclusion,"we release our data, software and protocol, enabling easy future dataset production and evaluation for qa-srl, as well as possible extensions of the qa-based semantic annotation paradigm."
2020.acl-main.627.txt,2020,6 Conclusion,experiment analysis is offered to understand the proposed method in depth.
2020.acl-main.627.txt,2020,6 Conclusion,experimental results on the upb v1.0 dataset show that the translation-based method is an effective method for cross-lingual srl transferring.
2020.acl-main.627.txt,2020,6 Conclusion,"further, we presented a pgn-bilstm encoder to better exploit the mixture corpora of different languages."
2020.acl-main.627.txt,2020,6 Conclusion,"in addition, we combined the gold-standard source srl corpora and the pseudo translated target corpora together to enhance the cross-lingual srl models."
2020.acl-main.627.txt,2020,6 Conclusion,"significant improvements can be achieved by using the translated datasets for all selected languages, including both single-source and multi-source transfer."
2020.acl-main.627.txt,2020,6 Conclusion,the key idea is to construct high-quality datasets for the target languages by corpus translation from the gold-standard srl annotations of the source languages.
2020.acl-main.627.txt,2020,6 Conclusion,we investigated cross-lingual srl models with different kinds of multilingual word representations.
2020.acl-main.627.txt,2020,6 Conclusion,we proposed a translation-based alternative for cross-lingual srl.
2020.acl-main.628.txt,2020,5 Conclusion,"all sentence meta-embeddings consistently outperform their individual single-source components on the sts benchmark and the sts12–16 datasets, with a new unsupervised sota set by our gcca meta-embeddings."
2020.acl-main.628.txt,2020,5 Conclusion,"because sentence metaembeddings are agnostic to the size and specifics of their ensemble, it should be possible to add new encoders to the ensemble, potentially improving performance further."
2020.acl-main.628.txt,2020,5 Conclusion,"inspired by the success of word meta-embeddings, we have shown how to apply different metaembedding techniques to ensembles of sentence encoders."
2020.acl-main.629.txt,2020,6 Conclusions and Future work,"by adding bert-based embeddings, we significantly improve our model accuracy by marginally affecting computational cost, achieving state-of-the-art f-scores in out-of-domain test sets."
2020.acl-main.629.txt,2020,6 Conclusions and Future work,"despite the promising results, the accuracy of our approach could probably be boosted further by experimenting with new feature information and specifically tuning hyper-parameters for the sdp task, as well as using different enhancements such as implementing the hierarchical decoding recently presented by liu et al.(2019), including contextual string embeddings (akbik et al., 2018) like he and choi (2019), or applying multi-task learning across the three formalisms like peng et al.(2017)."
2020.acl-main.629.txt,2020,6 Conclusions and Future work,our multi-head transition system can accurately parse a sentence in quadratic worst-case runtime thanks to pointer networks.
2020.acl-main.629.txt,2020,6 Conclusions and Future work,"while being more efficient, our approach outperforms the previous state-of-the-art parser by dozat and manning (2018) and matches the accuracy of the best model to date (wang et al., 2019), proving that, with a state-of-the-art neural architecture, transition-based sdp parsers are a competitive alternative."
2020.acl-main.63.txt,2020,6 Conclusion,"the experiments on the benchmark dataset demonstrate that the proposed approach is capable of boosting the performance of both nlu and nlg models, motivating the potential research directions in this area."
2020.acl-main.63.txt,2020,6 Conclusion,the proposed framework provides a potential method towards unsupervised learning of both language understanding and generation models by considering their data distribution.
2020.acl-main.63.txt,2020,6 Conclusion,"this paper proposes a general learning framework leveraging the duality between language understanding and generation, providing the flexibility of incorporating supervised and unsupervised learning algorithms to jointly train two models."
2020.acl-main.630.txt,2020,6 Conclusion,"another research direction is to investigate if introducing more sophisticated topic models, such as named entity promoting topic models (krasnashchok and jouili, 2018) into the proposed framework can further improve results."
2020.acl-main.630.txt,2020,6 Conclusion,future work may focus on how to directly induce topic information into bert without corrupting pretrained information and whether combining topics with other pretrained contextual models can lead to similar gains.
2020.acl-main.630.txt,2020,6 Conclusion,"in our qualitative analysis, we showed that these improvements were mainly achieved on examples involving domain-specific words."
2020.acl-main.630.txt,2020,6 Conclusion,"in this work, we proposed a flexible framework for combining topic models with bert."
2020.acl-main.630.txt,2020,6 Conclusion,we demonstrated that adding lda topics to bert consistently improved performance across a range of semantic similarity prediction datasets.
2020.acl-main.631.txt,2020,6 Conclusion,experimental results on two review datasets conﬁrm its effectiveness in this conditional augmentation scenario.
2020.acl-main.631.txt,2020,6 Conclusion,"in this paper, we have presented a conditional data augmentation approach for aspect term extraction."
2020.acl-main.631.txt,2020,6 Conclusion,"moreover, the proposed augmentation method tends not to be unique to the current task and could be applied to other low-resource sequence labeling tasks such as chunking and named entity recognition."
2020.acl-main.631.txt,2020,6 Conclusion,"unlike existing augmentation approaches, ours is controllable to generate qualiﬁed sentences, and allows more diversiﬁed new sentences."
2020.acl-main.631.txt,2020,6 Conclusion,"we also conducted qualitative studies to analyze how this augmentation approach works, and tested other language models to explain why our masked sequence-to-sequence generation framework is favored."
2020.acl-main.631.txt,2020,6 Conclusion,we formulated it as a conditional generation problem and proposed a masked sequence-to-sequence generation model to implement it.
2020.acl-main.632.txt,2020,6 Conclusion,"applying these features on persuasiveness corpora derived from the subreddit r/changemyview, we accomplish a fair improvement on the effectiveness of tackling the studied persuasiveness tasks, particularly in predicting the debaters’ resistance to persuasion."
2020.acl-main.632.txt,2020,6 Conclusion,"based on this hypothesis, we develop a set of various features to capture debaters characteristics using the reddit.com platform."
2020.acl-main.632.txt,2020,6 Conclusion,"in the future, we plan to consider the ethos mode of persuasion by exploring how debaters strengthen their credibility in debates."
2020.acl-main.632.txt,2020,6 Conclusion,"this paper proposes a new approach for modeling the personal characteristics of debaters including interests, prior beliefs, and personality traits for predicting both argument persuasiveness and debaters’ resistance to persuasion."
2020.acl-main.632.txt,2020,6 Conclusion,we hypothesize that these characteristics can be induced automatically from the history of debaters’ activity such as their earlier texts.
2020.acl-main.633.txt,2020,7 Conclusions,"as previous studies have shown, and consistent with our own findings, obtaining data for such a task is difficult, especially considering that labeling at scale of full speeches is an arduous effort."
2020.acl-main.633.txt,2020,7 Conclusions,"future research may focus on the motivation we described, but may also utilize the large speeches corpus we release as part of this work to a variety of additional different endeavors."
2020.acl-main.633.txt,2020,7 Conclusions,"noteworthy is that some of the automatic methods outperform the results achieved by the crowd, suggesting that the task is difficult, and may require a level of expertise beyond layman-level."
2020.acl-main.633.txt,2020,7 Conclusions,"the experiments suggest that the best results are achieved using jensen–shannon similarity, for speeches that contain explicit responses (accuracy of 80%) and using conditional mutual-information on speeches that respond to the input speech in an implicit way (accuracy of 43%)."
2020.acl-main.633.txt,2020,7 Conclusions,the reported gap between the performance of expert humans and the results achieved by nlp models demonstrate room for further research.
2020.acl-main.633.txt,2020,7 Conclusions,"to facilitate research of this problem, we recast the proposed general task in a defined debate setup and construct a corresponding benchmark data."
2020.acl-main.633.txt,2020,7 Conclusions,"we collected, and release as part of this work, more than 3,600 debate speeches annotated for the proposed task."
2020.acl-main.633.txt,2020,7 Conclusions,"we established the performance of humans on this task, showing that expert humans currently outperform automatic methods by a significant margin — attaining an accuracy of 92% on speeches with an explicit true counter, and 76% on speeches with an implicit one."
2020.acl-main.633.txt,2020,7 Conclusions,"we presented a novel nlu task of identifying a counter speech, which best counters an input speech, within a set of candidate counter speeches."
2020.acl-main.633.txt,2020,7 Conclusions,"we presented baselines for the task, considering a variety of contemporary nlp models."
2020.acl-main.634.txt,2020,7 Conclusion and Future Work,"by training the model through iterative back translation, it is able to significantly improve the diversity of generated responses both semantically and syntactically."
2020.acl-main.634.txt,2020,7 Conclusion and Future Work,"the model can be potentially improved by filtering the corpus according to different domains, or augmenting with a retrieve-and-rewrite mechanism, which we leave for future work."
2020.acl-main.634.txt,2020,7 Conclusion and Future Work,"to do so, we collect a large-scale corpus from forum comments, idioms and book snippets."
2020.acl-main.634.txt,2020,7 Conclusion and Future Work,we compare it with several strong baselines and find it achieved the best overall performance.
2020.acl-main.634.txt,2020,7 Conclusion and Future Work,we propose a novel way of diversifying dialogue generation by leveraging non-conversational text.
2020.acl-main.635.txt,2020,5 Conclusion and Future Work,each dialogue contains various topics and sentence-level annotations that map each utterance with the related knowledge triples.
2020.acl-main.635.txt,2020,5 Conclusion and Future Work,"extensive experiments demonstrate that these models can be enhanced by introducing knowledge, whereas there is still much room in knowledge-grounded conversation modeling for future work."
2020.acl-main.635.txt,2020,5 Conclusion and Future Work,"in addition, kd-conv covers three domains, including film, music, and travel, that can be used to explore domain adaptation or transfer learning for further research."
2020.acl-main.635.txt,2020,5 Conclusion and Future Work,"in this paper, we propose a chinese multi-domain corpus for knowledge-driven conversation generation, kdconv."
2020.acl-main.635.txt,2020,5 Conclusion and Future Work,"it contains 86k utterances and 4.5k dialogues, with an average number of 19.0 turns."
2020.acl-main.635.txt,2020,5 Conclusion and Future Work,the dataset provides a benchmark to evaluate the ability to model knowledge-driven conversations.
2020.acl-main.635.txt,2020,5 Conclusion and Future Work,we provide generation- and retrieval-based benchmark models to facilitate further research.
2020.acl-main.636.txt,2020,6 Conclusion,"empirical results on multiwoz datasets indicate that our solution outperforms non-meta-learning baselines training from scratch, adapting to new few-shot domains with less data and faster convergence rate."
2020.acl-main.636.txt,2020,6 Conclusion,"experiments on multi-domain dataset show that our proposed model achieves state-of-the-art performance on the dst task, exceeding current best result by over 2%."
2020.acl-main.636.txt,2020,6 Conclusion,"in addition, we train the dialogue state tracker using multiple single-domain dialogue data with richresource by using the maml."
2020.acl-main.636.txt,2020,6 Conclusion,"in future work, we intend to explore more with the combination of rl and dst on the basis of reward designing, trying to explore more in the internal mechanism."
2020.acl-main.636.txt,2020,6 Conclusion,"in the long run, we are interested in combing many tasks into one learning process with meta-learning."
2020.acl-main.636.txt,2020,6 Conclusion,the model is capable of learning a competitive and scalable dst on a new domain with only a few training examples in an efficient manner.
2020.acl-main.636.txt,2020,6 Conclusion,"we introduce an end-to-end generative framework with pre-trained language model and copymechanism, using rl-based generator to encourage higher semantic relevance in greater exploration space for dst."
2020.acl-main.637.txt,2020,5 Conclusion,experiments on the multiwoz 2.0 dataset show that our model significantly outperforms the baselines and achieves new state-of-the-art results.
2020.acl-main.637.txt,2020,5 Conclusion,"in this paper, we have presented the utterance tagging and auxiliary bi-directional language modeling in a multi-task learning framework to model long dialogue context for open vocabulary-based dst."
2020.acl-main.638.txt,2020,8 Conclusion,"extensive studies were conducted on a large-scale task-oriented dataset to evaluate the proposed model, and the results conﬁrm its effectiveness with very favorable performance over several state-of-the-art methods."
2020.acl-main.638.txt,2020,8 Conclusion,"in this paper, we presented a novel co-generation model for dialogue act prediction and response generation in task-oriented dialogue systems."
2020.acl-main.638.txt,2020,8 Conclusion,"to train this joint model, we applied an uncertainty loss for adaptive weighting of the two tasks."
2020.acl-main.638.txt,2020,8 Conclusion,"unlike previous approaches, we modeled act prediction as a sequence generation problem to exploit the semantic structures of acts and trained it jointly with response generation via dynamic attention from response generation to act prediction."
2020.acl-main.639.txt,2020,5 Conclusion,experiments on two benchmark datasets prove the superiority of our model over several competitive baselines.
2020.acl-main.639.txt,2020,5 Conclusion,"in the future, we plan to adapt variational neural network to refine our style transfer model, which has shown effectiveness in other conditional text generation tasks, such as machine translation (zhang et al., 2016; su et al., 2018)."
2020.acl-main.639.txt,2020,5 Conclusion,"then, equipped with the style component, our model can exploit the word-level predicted style relevance for better style transfer."
2020.acl-main.639.txt,2020,5 Conclusion,this paper has proposed a novel attentional seq2seq model equipped with a neural style component for unsupervised style transfer.
2020.acl-main.639.txt,2020,5 Conclusion,"using the quantified style relevance from a pre-trained style classifier as supervision information, our model is first trained to reconstruct input sentences and repredict the word-level style relevance simultane ously."
2020.acl-main.64.txt,2020,7 Conclusions,"this paper presents usr, an unsupervised and reference-free evaluation metric for dialog."
2020.acl-main.64.txt,2020,7 Conclusions,thus the metric may be adapted to different tasks and datasets.
2020.acl-main.64.txt,2020,7 Conclusions,"to address the shortcomings of standard metrics for language generation, usr (1) is reference-free, (2) is composed of multiple sub-metrics that evaluate specific qualities of dialog, (3) has a definition of good dialog that is configurable."
2020.acl-main.64.txt,2020,7 Conclusions,"usr is shown to strongly correlate with human judgment on topical-chat (turn-level: 0.42, system-level: 1.0) and personachat (turn-level: 0.48, systemlevel: 1.0)."
2020.acl-main.640.txt,2020,6 Conclusion,experimental results show that hetgt strongly outperforms the state of the art performances on four benchmark datasets of amr-to-text generation and syntax-based neural machine translation tasks.
2020.acl-main.640.txt,2020,6 Conclusion,"in this paper, we propose the heterogeneous graph transformer (hetgt) for graph2seq learning."
2020.acl-main.640.txt,2020,6 Conclusion,"on the other hand, we would also like to investigate how to make use of our proposed model to solve sequence-to-sequence tasks."
2020.acl-main.640.txt,2020,6 Conclusion,one is to investigate how the other graph models can benefit from our proposed heterogeneous mechanism.
2020.acl-main.640.txt,2020,6 Conclusion,our proposed heterogeneous mechanism can adaptively model the different representation subgraphs.
2020.acl-main.640.txt,2020,6 Conclusion,there are two directions for future works.
2020.acl-main.641.txt,2020,7 Conclusion,"as our model is interpretable in the correspondence between segments and input records, it can be easily combined with hand-engineered heuristics or user-specific requirements to further improve the performance."
2020.acl-main.641.txt,2020,7 Conclusion,"in this work, we exploit the segmental structure in data-to-text generation."
2020.acl-main.641.txt,2020,7 Conclusion,"it is end-to-end trainable, domain-independent and allows explicit control over the structure of generated text."
2020.acl-main.641.txt,2020,7 Conclusion,"the proposed model significantly alleviates the information hallucination, repetition and missing problems without sacrificing the fluency and diversity."
2020.acl-main.642.txt,2020,5 Conclusion,extensive experiments on the vqa-v2 and vqa-cp-v2 datasets demonstrate that our model achieves comparable performance with the state-of-the-art approaches.
2020.acl-main.642.txt,2020,5 Conclusion,"furthermore, we explicitly construct the relations between words by dependency tree and align the image and question representations by an attention alignment module to reduce the gaps between vision and language."
2020.acl-main.642.txt,2020,5 Conclusion,"in this paper, we propose a dual channel graph convolutional network to explore the relations between objects in an image and the syntactic dependency relations between words in a question."
2020.acl-main.642.txt,2020,5 Conclusion,we will explore more complicated object relation modeling in future work.
2020.acl-main.643.txt,2020,6 Conclusions,a future research direction is to combine rgcs with distant supervision by an external knowledge base to answer the visual questions that need external knowledge; for example which animal in this photo can climb a tree?
2020.acl-main.643.txt,2020,6 Conclusions,"in experiments on three datasets, the mn-gmn showed superior quantitative and qualitative performance compared to the lesion approaches and rivals the state-of-the-art models."
2020.acl-main.643.txt,2020,6 Conclusions,"it leverages a graph neural network model, graph network, to reason about objects and their interactions in a scene."
2020.acl-main.643.txt,2020,6 Conclusions,multi-modal neural graph memory networks are a new architecture for the vqa task.
2020.acl-main.643.txt,2020,6 Conclusions,the mn-gmn represents bimodal local features as node attributes in a graph.
2020.acl-main.644.txt,2020,6 Conclusion,"however, one can also study a much larger number of scenarios and modeling tasks using refer360° ."
2020.acl-main.644.txt,2020,6 Conclusion,"in our experiments, we presented one of these scenarios (single instruction, static, and pixellevel) since it was the closest to the pre-existing touchdown-sdr system."
2020.acl-main.644.txt,2020,6 Conclusion,"refer360° is a versatile dataset and enables investigation along three axes: • language: refer360° enables modeling tasks that study single instruction, multiple instructions, or interactive language where the next instruction is revealed only after reaching an intermediate milestone.• vision: refer360° enables modeling tasks that try to predict targets at different granularities: at the object level if trying to identify the closest object to the target, at the region level in a similar style to touchdown-sdr, and finally, at the pixel level.• action: refer360° enables modeling tasks where the action space is static with the whole 360 image given upfront, where the action space consists of a sequence of discrete choices between fixed views, and when the action space is continuous, consisting of angles for rotation."
2020.acl-main.644.txt,2020,6 Conclusion,we collected a fine-grained set of annotations that support study at many levels of language grounding.
2020.acl-main.644.txt,2020,6 Conclusion,we designed refer360° to study 3d spatial language understanding for real scenes.
2020.acl-main.645.txt,2020,8 Conclusion,"in addition we showed that our models could reach surprisingly high performances with as low as 4gb of pretraining data, questioning thus the need for large scale pretraining corpora."
2020.acl-main.645.txt,2020,8 Conclusion,"in this work, we investigated the feasibility of training a transformer-based language model for languages other than english."
2020.acl-main.645.txt,2020,8 Conclusion,our experiments demonstrate that using web crawled data with high variability is preferable to using wikipedia-based data.
2020.acl-main.645.txt,2020,8 Conclusion,"pretrained on pure open-source corpora, camem-bert is freely available and distributed with the mit license via popular nlp libraries (fairseq and huggingface) as well as on our website camembert-model.fr."
2020.acl-main.645.txt,2020,8 Conclusion,the question of knowing whether pretraining on small domain specific content will be a better option than transfer learning techniques such as fine-tuning remains open and we leave it for future work.
2020.acl-main.645.txt,2020,8 Conclusion,this paves the way for the rise of monolingual contextual pre-trained language-models for under-resourced languages.
2020.acl-main.645.txt,2020,8 Conclusion,"this shows that state-of-the-art transformer-based language models can be trained on languages with far fewer resources than english, whenever a few gigabytes of data are available."
2020.acl-main.645.txt,2020,8 Conclusion,"using french as an example, we trained camembert, a language model based on roberta."
2020.acl-main.645.txt,2020,8 Conclusion,"we evaluated camem-bert on four downstream tasks (part-of-speech tagging, dependency parsing, named entity recognition and natural language inference) in which our best model reached or improved the state of the art in all tasks considered, even when compared to strong multilingual models such as mbert, xlm and xlm-r, while also having fewer parameters."
2020.acl-main.646.txt,2020,8 Discussion,"amongst these, our proposed optimisation subject to a minimum rate constraint is simple enough to tune (as fb it only takes a pre-specified rate and unlike fb it does not suffer from gradient discontinuities), superior to annealing and word dropout, and require less resources than strategies based on multiple annealing schedules and/or aggressive optimisation of the inference model."
2020.acl-main.646.txt,2020,8 Discussion,"fb, sfb, β-vae, infovae), though they may require a considerable hyperparameter search (e.g.sfb and infovae)."
2020.acl-main.646.txt,2020,8 Discussion,"in this paper, we have introduced and compared techniques for effective estimation of such a model."
2020.acl-main.646.txt,2020,8 Discussion,"other ways to lower-bound rate, such as by imposing a multimodal prior, though promising, still require a minimum desired rate."
2020.acl-main.646.txt,2020,8 Discussion,"our interest in latent variable models stems from the desire to obtain generative stories that are less opaque than that of an rnnlm, for example, in that they may expose knobs that we can use to control generation and a hierarchy of steps that may award a degree of interpretability to the model."
2020.acl-main.646.txt,2020,8 Discussion,"senvae is a deep generative model whose generative story is rather shallow, yet, due to its strong generator component, it is hard to make effective use of the extra knob it offers."
2020.acl-main.646.txt,2020,8 Discussion,"the sen-vae is not that model, but it is a crucial building block in the pursue for hierarchical probabilistic models of language."
2020.acl-main.646.txt,2020,8 Discussion,"the typical rnnlm is built upon an exact factorisation of the joint distribution, thus a well-trained architecture is hard to improve upon in terms of log-likelihood of gold-standard data."
2020.acl-main.646.txt,2020,8 Discussion,"we hope this work, i.e.the organised review it contributes and the techniques it introduces, will pave the way to deeper—in statistical hierarchy—generative models of language."
2020.acl-main.646.txt,2020,8 Discussion,we show that many techniques in the literature perform reasonably similarly (i.e.
2020.acl-main.647.txt,2020,8 Conclusion,"this method can be applicable for other end goals, such as style-transfer, disentanglement of neural representations and increasing their interpretability."
2020.acl-main.647.txt,2020,8 Conclusion,we aim to explore those directions in a future work.
2020.acl-main.647.txt,2020,8 Conclusion,"we focus on bias and fairness as case studies, and demonstrate that across increasingly complex settings, our method is capable of attenuating societal biases that are expressed in representations learned from data."
2020.acl-main.647.txt,2020,8 Conclusion,we present a novel method for removing linearlyrepresented information from neural representations.
2020.acl-main.647.txt,2020,8 Conclusion,"while we focused on bias, iterative nullspace projection has broader possible use-cases, and can be utilized to remove specific components from a representation, in a controlled manner."
2020.acl-main.648.txt,2020,5 Takeaways and Open Questions,supervised models are prone to anaphora and unseen entities related errors.• a simple lm-based viterbi segmentation model outperforms other subword tokenizers on topic classification tasks and reduces skewness of token distribution on a noisy dataset.
2020.acl-main.648.txt,2020,5 Takeaways and Open Questions,"the contributions of our work are: • 2kenize, a subword segmentation model, which jointly segments source sentence and its corresponding approximate target conversions.• an unsupervised script converter based on 2kenize which shows a significant improvement over existing script converters and supervised models.• 1kenize, a variant of 2kenize which performs tokenization on only traditional chinese sentences which improves accuracy on topic classification tasks.• character conversion evaluation datasets: spanning hong kong and taiwanese literature and news genres.• traditional chinese topic classification datasets: formal (scraped from singtao) and informal (scraped from lihkg) styles spanning genres like news, social media discussions, and memes."
2020.acl-main.648.txt,2020,5 Takeaways and Open Questions,the key findings of our work are: • our script converter shows a strong performance when dealing with code mixing and named entities.
2020.acl-main.648.txt,2020,5 Takeaways and Open Questions,"we anticipate that this study would be useful to tc nlp practitioners, as we address several research gaps, namely script conversion and a lack of benchmark datasets."
2020.acl-main.648.txt,2020,5 Takeaways and Open Questions,we leave some open questions to explore: • how can we exploit subword variations to reduce skewness in the nlu tasks?• would subword-segmentation-transfer be helpful for other nmt-nlu task pairs like we did for 2kenize (script conversion) to 1kenize (classification)?
2020.acl-main.649.txt,2020,8 Conclusion,"furthermore, we have presented a random forest model for mfep that achieves very good accuracies, particularly in predicting extreme growth in morphological family size."
2020.acl-main.649.txt,2020,8 Conclusion,"however, the initial growth of small families is mainly driven by the trending behavior of the parent, an exogenous factor."
2020.acl-main.649.txt,2020,8 Conclusion,"in future work, we intend to further fine-tune our methodological apparatus for tackling mfep."
2020.acl-main.649.txt,2020,8 Conclusion,"in this paper, we have proposed mfep (morphological family expansion prediction), a new task that aims at predicting how morphological families evolve over time."
2020.acl-main.649.txt,2020,8 Conclusion,"overall, we see our study as an exciting step in the direction of bringing together computational social science and derivational morphology."
2020.acl-main.649.txt,2020,8 Conclusion,"the strongest predictor of growth is the morphological family size itself, an endogenous factor."
2020.acl-main.649.txt,2020,8 Conclusion,"this reflection of external events makes morphological families a promising tools for various fields drawing upon nlp techniques for tracing temporal dynamics in text (e.g., virality detection)."
2020.acl-main.649.txt,2020,8 Conclusion,"we have shown that changes in morphological family size provide a fresh look at topical dynamics in social media, thus complementing token frequency as a metric."
2020.acl-main.65.txt,2020,7 Conclusion,"as a result, esd leads to significant improvements over the previous strong baselines on two established definition datasets."
2020.acl-main.65.txt,2020,7 Conclusion,"in future work, we plan to seek better ways to guide the learning of latent variables, such as using dynamic routing (sabour et al., 2017) method to align the latent variables and sememes, and learn more explainable latent codes."
2020.acl-main.65.txt,2020,7 Conclusion,"quantitative and qualitative analysis showed that our model could generate more meaningful, specific and accurate definitions."
2020.acl-main.65.txt,2020,7 Conclusion,"specifically, we model the decomposed semantics as discrete latent variables, and training with auxiliary losses to ensure that the model learns informative latent codes for definition modeling."
2020.acl-main.65.txt,2020,7 Conclusion,"we proposed esd, a context-aware definition generation model that explicitly models the decomposed semantics of words."
2020.acl-main.650.txt,2020,8 Conclusion,"by leveraging unlabeled data and accessing context at training time, we train accurate models with fewer manually normalized training samples."
2020.acl-main.650.txt,2020,8 Conclusion,no labeled training data are necessary to achieve 88.4% of the best published performance that uses full training sets.
2020.acl-main.650.txt,2020,8 Conclusion,strong gains are observed for most of the considered languages across realistic low-resource settings (up to 5k labeled training tokens).
2020.acl-main.650.txt,2020,8 Conclusion,"the techniques developed here readily apply to other types of normalization data (e.g.informal, dialectal)."
2020.acl-main.650.txt,2020,8 Conclusion,this paper proposes semi-supervised contextual normalization of non-standard text.
2020.acl-main.650.txt,2020,8 Conclusion,we develop simple contextualized generative neural models that we train with expectation–maximization.
2020.acl-main.650.txt,2020,8 Conclusion,"we focus on historical data, which has gained attention in the digital humanities community over the past years."
2020.acl-main.650.txt,2020,8 Conclusion,we will make our implementation publicly available.2
2020.acl-main.651.txt,2020,5 Conclusion and Future Work,clarq consists of∼2m post-question tuples spanning 173 different domains.
2020.acl-main.651.txt,2020,5 Conclusion and Future Work,"in this paper, we present a diverse, large-scale dataset (clarq) for the task of clarification question generation."
2020.acl-main.651.txt,2020,5 Conclusion and Future Work,it is created by a two-step iterative bootstrapping framework based on selfsupervision.
2020.acl-main.651.txt,2020,5 Conclusion and Future Work,"we hope that this dataset will encourage research into clarification question generation and, in the long run, enhance dialog and question-answering systems."
2020.acl-main.652.txt,2020,8 Conclusion and Future Work,doqa introduces a more realistic scenario where the passage with the answer needs to be retrieved.
2020.acl-main.652.txt,2020,8 Conclusion and Future Work,"for the future, we would like to exploit the abstractive answers in our dataset, explore more sophisticated systems in both scenarios and perform user studies to study how real users interact with a conversational qa system when accessing faqs."
2020.acl-main.652.txt,2020,8 Conclusion and Future Work,"in contrast to previous conversational qa datasets, our dataset responds to a real information need, is multi-domain, more natural and coherent."
2020.acl-main.652.txt,2020,8 Conclusion and Future Work,"our dataset and experiments show that it is possible to access domain-specific faqs using conversational qa systems with little or no in-domain training data, yielding quality which is comparable to those reported in quac."
2020.acl-main.652.txt,2020,8 Conclusion and Future Work,"the expert can rephrase the selected span, in order to make it look more natural."
2020.acl-main.652.txt,2020,8 Conclusion and Future Work,the goal of this work is to access the large body of domain-specific information in the form of frequently asked question sites via conversational qa systems.
2020.acl-main.652.txt,2020,8 Conclusion and Future Work,"these dialogues are created by crowdworkers that play the following two roles: the user asks questions about a certain topic posted in stack exchange, and the domain expert who replies to the questions by selecting a short span of text from the long textual reply in the original post."
2020.acl-main.652.txt,2020,8 Conclusion and Future Work,"together with the dataset, we presented results of a strong conversational model, including transfer learning from wikipedia qa datasets to our faq dataset."
2020.acl-main.652.txt,2020,8 Conclusion and Future Work,"we have presented doqa, a dataset for accessing domain specific faqs via conversational qa that contains 2,437 information-seeking dialogues on the cooking, travel and movies domain (10,917 questions in total)."
2020.acl-main.653.txt,2020,7 Conclusion,"we developed several baselines on two cross-lingual understanding tasks on mlqa with state-of-the-art methods, and demonstrate significant room for improvement."
2020.acl-main.653.txt,2020,7 Conclusion,"we have introduced mlqa, a highly-parallel multilingual qa benchmark in seven languages."
2020.acl-main.653.txt,2020,7 Conclusion,we hope that mlqa will help to catalyse work in cross-lingual qa to close the gap between training and testing language performance.
2020.acl-main.654.txt,2020,6 Conclusion,"in future work, we explore to extend this approach for other low resource tasks in nlp."
2020.acl-main.654.txt,2020,6 Conclusion,"in this work, we propose a novel method named multi-source meta transfer for multiple-choice question answering on low resource setting."
2020.acl-main.654.txt,2020,6 Conclusion,"our method considers multiple sources meta learning and target fine-tuning into a unified framework, which is able to learn a general representation from multiple sources and alleviate the discrepancy between source and target."
2020.acl-main.654.txt,2020,6 Conclusion,we demonstrate the superiority of our methods on both supervised setting and unsupervised domain adaptation settings over the state-of-the-arts.
2020.acl-main.655.txt,2020,7 Conclusion,"in the future, we will further study this properties of kernel-based attentions in neural networks, both in the effectiveness front and also the explainability front."
2020.acl-main.655.txt,2020,7 Conclusion,our experiments show that kernels lead to the more accurate fact verification.
2020.acl-main.655.txt,2020,7 Conclusion,our studies illustrate the two kernels play different roles and contribute to different aspects crucial for fact verification.
2020.acl-main.655.txt,2020,7 Conclusion,"this paper presents kgat, which uses kernels in graph neural networks to conduct more accurate evidence selection and fine-grained joint reasoning."
2020.acl-main.655.txt,2020,7 Conclusion,"while the dot-product attentions are rather scattered and hard to explain, the kernel-based attentions show intuitive and effective attention patterns: the node kernels focus more on the correct evidence pieces; the edge kernels accurately gather the necessary information from one node to the other to complete the reasoning chain."
2020.acl-main.656.txt,2020,7 Conclusions,a manual evaluation shows that the coverage and the overall quality of the explanation system is also improved in the multi-task set-up.
2020.acl-main.656.txt,2020,7 Conclusions,"for future work, an obvious next step is to investigate the possibility of generating veracity explanations from evidence pages crawled from the web."
2020.acl-main.656.txt,2020,7 Conclusions,"furthermore, other approaches of generating veracity explanations should be investigated, especially as they could improve fluency or decrease the redundancy of the generated text."
2020.acl-main.656.txt,2020,7 Conclusions,"we presented the first study on generating veracity explanations, and we showed that veracity prediction can be combined with veracity explanation generation and that the multi-task set-up improves the performance of the veracity system."
2020.acl-main.657.txt,2020,7 Conclusion & Future Work,"as future work, we will explore different heuristics for navigating in the premises graph, as researched before for textual entailment (silva et al., 2019, 2018) and selective reasoning (freitas et al., 2014)."
2020.acl-main.657.txt,2020,7 Conclusion & Future Work,"in this work, we introduced an approach for natural language premise selection (finding relevant theorems, axioms and definitions) in large natural language mathematical texts."
2020.acl-main.657.txt,2020,7 Conclusion & Future Work,"moreover, the proposed model shows significantly lower f1-score degradation concerning class imbalance, a fundamental desirable scalability property for the problem of premise selection."
2020.acl-main.657.txt,2020,7 Conclusion & Future Work,our approach is also able to obtain better performance when we consider the transitivity of premises.
2020.acl-main.657.txt,2020,7 Conclusion & Future Work,results show that the approach outperforms a bert-based baseline by 41% in f1-score.
2020.acl-main.657.txt,2020,7 Conclusion & Future Work,"the proposed approach, which uses deep graph convolutional neural networks (dgcnns) combines both structural and content elements of mathematical statements for addressing the premise selection problem as a link prediction classification problem."
2020.acl-main.657.txt,2020,7 Conclusion & Future Work,the qualitative analysis indicates that there is the demand to design principled embeddings for better capturing the semantics of proofs which are denser in mathematical formulae.
2020.acl-main.658.txt,2020,8 Conclusions,"finally, we discuss connections between cross-lingual word embeddings, deep multilingual pre-training, and unsupervised machine translation, calling for an evaluation on an equal footing."
2020.acl-main.658.txt,2020,8 Conclusions,"in addition, we describe methodological issues related to the unsupervised setting and propose measures to ameliorate them."
2020.acl-main.658.txt,2020,8 Conclusions,"in light of the unprecedented growth of our field in recent times, we believe that it is essential to establish a rigorous foundation connecting past and present research, and an evaluation protocol that carefully controls for the use of parallel data and assesses models in diverse, challenging settings."
2020.acl-main.658.txt,2020,8 Conclusions,"in this position paper, we review the status quo of unsupervised cross-lingual learning—a relatively recent field."
2020.acl-main.658.txt,2020,8 Conclusions,"instead, we advocate for the importance of ucl for scientific reasons."
2020.acl-main.658.txt,2020,8 Conclusions,"ucl is typically motivated by the lack of cross-lingual signal for many of the world’s languages, but available resources indicate that a scenario with no parallel data and sufficient monolingual data is not realistic."
2020.acl-main.658.txt,2020,8 Conclusions,"we also discuss different monolingual and cross-lingual training signals that have been used in the past, and advocate for carefully reporting them to enable a meaningful comparison across different approaches."
2020.acl-main.658.txt,2020,8 Conclusions,"we hope that this position paper will serve to strengthen research in ucl, providing a more rigorous look at the motivation, definition, and methodology."
2020.acl-main.659.txt,2020,7 Conclusion,"however, the structural probe outperformed the parser on a novel metric proposed in hewitt and manning (2019), bringing to attention a broader question: how should one choose metrics for probing?"
2020.acl-main.659.txt,2020,7 Conclusion,"in our discussion, we argued that if one is to propose a new metric, they should clearly justify its usage."
2020.acl-main.659.txt,2020,7 Conclusion,"we advocate for the position that, beyond some notion of model complexity, there should be no inherent difference between the design of a probe and a model designed for a corresponding task."
2020.acl-main.659.txt,2020,7 Conclusion,"we analysed the structural probe (hewitt and manning, 2019), and showed that a simple parser with an identical lightweight parameterisation was able to identify more syntax in bert in seven of nine compared languages under uuas."
2020.acl-main.66.txt,2020,7 Conclusion,"in response, we propose loss truncation, a robust training method that optimizes for distinguishability of generated samples."
2020.acl-main.66.txt,2020,7 Conclusion,"in this work, we show that log loss is not robust to noise, which can in turn cause undesired behavior, such as hallucinating facts in summarization."
2020.acl-main.66.txt,2020,7 Conclusion,these results suggest that robust learning in the form of truncating the log loss can complement model-based approaches to faithful generation by ignoring invalid and undesired references.
2020.acl-main.66.txt,2020,7 Conclusion,we additionally propose a sequence-level rejection sampling scheme to generate high quality sequences.
2020.acl-main.66.txt,2020,7 Conclusion,"we additionally show that rejection sampling outperforms all baselines, including beam search, on generating factual summaries."
2020.acl-main.66.txt,2020,7 Conclusion,"we show that loss truncation outperforms a range of baselines (including beam search, top-p, top-k, and full sampling) on distinguishability."
2020.acl-main.660.txt,2020,7 Discussion and Conclusion,"our preliminary experiments on hebrew multi-tagging confirmed that relying on lessons learned for mrls in the pre-neural era and incorporating similar theoretical constructs into the neural architecture indeed improves the empirical results on multi-tagging of hebrew, on the very basic form of analysis of modern hebrew — a morphologically rich and highly-fusional language."
2020.acl-main.660.txt,2020,7 Discussion and Conclusion,"this paper proposes nmrl, a new (or rather, redefined) research theme aiming to develop neural models, benchmarks, and modeling strategies for mrls."
2020.acl-main.660.txt,2020,7 Discussion and Conclusion,"this type of research needs to be extended to the investigation of multiple tasks, multiple languages, and multiple possible pre-training regimes (words, chars, morphemes, lattices) in order to investigate whether this trend extends to other languages and tasks."
2020.acl-main.660.txt,2020,7 Discussion and Conclusion,"we proceeded to define the three deep counterparts to the challenges proposed in tsarfaty et al.(2010), namely, the deep architectural challenge, deep modeling challenge and deep lexical challenge, and sketched plausible research avenues that the nmrl community might wish to explore towards their resolution."
2020.acl-main.660.txt,2020,7 Discussion and Conclusion,"we surveyed current research practices in neural nlp, characterized the particular challenges associated with mrls, and demonstrated that some of the neural modeling practices are incompatible with the accumulated wisdom concerning mrls in the spmrl literature."
2020.acl-main.660.txt,2020,7 Discussion and Conclusion,"whether adopting solution strategies for mrls proposed herein or devising new ones, it is high time to bring the linguistic and moprhological complexity of mrls back to the forefront of nlp research, both for the purpose of getting a better grasp of the abilities, as well as limitations, of neural models for nlp, and towards serving the exciting nlp/ai advances to the understudied, less-privileged, languages."
2020.acl-main.661.txt,2020,7 Conclusion,our hope is to encourage meaningful and generalizable comparisons on our quest toward overcoming the long-standing issues found in st models.
2020.acl-main.661.txt,2020,7 Conclusion,we exposed a significant space of both modeling ideas and application-specific requirements left to be addressed in future research.
2020.acl-main.661.txt,2020,7 Conclusion,"we started this paper with a chronological survey of three decades of st research, focusing on carving out the key concepts."
2020.acl-main.661.txt,2020,7 Conclusion,"we then provided definitions of the central challenges, techniques, and requirements, motivated by the observation that recent work does not sufficiently analyze these challenges."
2020.acl-main.662.txt,2020,5 A Call to Action,"a simple model that requires less training data or runs in under ten milliseconds may be objectively more useful than a bloated, brittle monster of a system that has a slightly higher f1 (dodge et al., 2019)."
2020.acl-main.662.txt,2020,5 A Call to Action,"after qa datasets are released, there should also be deeper, more frequent discussion of actual questions within the nlp community."
2020.acl-main.662.txt,2020,5 A Call to Action,"again, we emphasize that human and computer skills are not identical, but this is a benefit: humans’ natural aversion to unfairness will help you create a better task, while computers will blindly optimize an objective function (bostrom, 2003)."
2020.acl-main.662.txt,2020,5 A Call to Action,"appreciate ambiguity if your intended qa application has to handle ambiguous questions, do justice to the ambiguity by making it part of your task—for example, recognize the original ambiguity and resolve it (“did you mean...”) instead of giving credit for happening to ‘fit the data’."
2020.acl-main.662.txt,2020,5 A Call to Action,"as machine learning algorithms improve, the “good enough” crowdsourcing that got us this far may not be enough for continued progress."
2020.acl-main.662.txt,2020,5 A Call to Action,"as you go through the process of playing on your question–answer dataset, you can see where you might have fallen short on the goals we outline in section 3."
2020.acl-main.662.txt,2020,5 A Call to Action,be honest in crowning qa champions leaderboards are a ranking over entrants based on a ranking over numbers.
2020.acl-main.662.txt,2020,5 A Call to Action,but you can use some quizbowl intuitions to improve discrimination.
2020.acl-main.662.txt,2020,5 A Call to Action,"creators want people to invest time and sometimes money (e.g., gpu hours) in using their data and submitting to their leaderboards."
2020.acl-main.662.txt,2020,5 A Call to Action,"eat your own dog food as you develop new question answering tasks, you should feel comfortable playing the task as a human."
2020.acl-main.662.txt,2020,5 A Call to Action,"embrace multiple answers or specify specificity as qa moves to more complicated formats and answer candidates, what constitutes a correct answer becomes more complicated."
2020.acl-main.662.txt,2020,5 A Call to Action,"finally, if you want to make human–computer comparisons, pick the right humans."
2020.acl-main.662.txt,2020,5 A Call to Action,"for example, the quora query “is there a nuclear control room on nuclear aircraft carriers?” is purportedly answered by someone who worked in such a room (humphries, 2017)."
2020.acl-main.662.txt,2020,5 A Call to Action,"for more traditional qa tasks, you can maximize the usefulness of your dataset by ensuring as many questions as possible are challenging (but not impossible) for today’s qa systems."
2020.acl-main.662.txt,2020,5 A Call to Action,"for other settings, create pyramidality by adding metadata: coreference, disambiguation, or alignment to a knowledge base."
2020.acl-main.662.txt,2020,5 A Call to Action,"for this to feel real, you will need to keep score; have all of your coauthors participate and compare scores."
2020.acl-main.662.txt,2020,5 A Call to Action,fully automatic evaluations are valuable for both training and quick-turnaround evaluation.
2020.acl-main.662.txt,2020,5 A Call to Action,here are our recommendations if you want to have an effective leaderboard.
2020.acl-main.662.txt,2020,5 A Call to Action,"however, we should not forget that these metrics were introduced as ‘understudies’—good enough when quick evaluations are needed for system building but no substitute for a proper evaluation."
2020.acl-main.662.txt,2020,5 A Call to Action,"if your task is realistic, fun, and challenging, you will find experts to play against your computer."
2020.acl-main.662.txt,2020,5 A Call to Action,"importantly, this is not just to replicate what crowdworkers are doing (also important) but to remove hidden assumptions, institute fair metrics, and define the task well."
2020.acl-main.662.txt,2020,5 A Call to Action,"in between full automation and expensive humans in the loop are automatic metrics that mimic the flexibility of human raters, inspired by machine translation evaluations (papineni et al., 2002; specia and farzindar, 2010) or summarization (lin, 2004)."
2020.acl-main.662.txt,2020,5 A Call to Action,"in contrast to low-paid crowdworkers, public platforms for question answering and citizen science (bowser et al., 2013) are brimming with free expertise if you can engage the relevant communities."
2020.acl-main.662.txt,2020,5 A Call to Action,"in machine translation, laubli et al.(2020) reveal that crowdworkers cannot spot the errors that neural mt systems make—fortunately, trivia nerds are cheaper than professional translators."
2020.acl-main.662.txt,2020,5 A Call to Action,"in short, consider multiple versions/views of your data that progress from difficult to easy."
2020.acl-main.662.txt,2020,5 A Call to Action,"in the case annotators disagree, the question should explicitly state what level of specificity is required (e.g., september 1, 1939 vs. 1939 or leninism vs. socialism)."
2020.acl-main.662.txt,2020,5 A Call to Action,"in this final section, we hope to distill our advice into a call to action regardless of your question format or source."
2020.acl-main.662.txt,2020,5 A Call to Action,"in visual qa, you can offer increasing resolutions of the image."
2020.acl-main.662.txt,2020,5 A Call to Action,it is “good business” to build a reputation for quality questions and discussing individual questions.
2020.acl-main.662.txt,2020,5 A Call to Action,"just like trivia tournaments, qa datasets resemble a product for sale."
2020.acl-main.662.txt,2020,5 A Call to Action,"likewise, beating a distracted crowdworker on qa is not qa’s endgame."
2020.acl-main.662.txt,2020,5 A Call to Action,"likewise, the broader public has unique knowledge and skills."
2020.acl-main.662.txt,2020,5 A Call to Action,"make questions discriminative we argue that questions should be discriminative (section 2.3), and while quizbowl is one solution (section 4), not everyone is crazy enough to adopt this (beautiful) format."
2020.acl-main.662.txt,2020,5 A Call to Action,"moreover, another lesson the qa community could learn from trivia games is to turn it into a spectacle: exciting games with a telegenic host."
2020.acl-main.662.txt,2020,5 A Call to Action,"not only will this give you human baselines worth reporting—they can also tell you how to fix your qa dataset...after all, they’ve been at it longer than you have."
2020.acl-main.662.txt,2020,5 A Call to Action,"or, if not all questions have a single answer, link answers to a knowledge base with multiple surface forms or explicitly enumerate which answers are acceptable."
2020.acl-main.662.txt,2020,5 A Call to Action,"paraphrasing a participant of the 2019 mrqa workshop (fisch et al., 2019), a system better than the average human at brain surgery does not imply superhuman performance in brain surgery."
2020.acl-main.662.txt,2020,5 A Call to Action,"part of every post-mortem of trivia tournaments is a detailed discussion of the questions, where good questions are praised and bad questions are excoriated."
2020.acl-main.662.txt,2020,5 A Call to Action,"revel in spectacle however, with more complicated systems and evaluations, a return to the yearly evaluations of trecqa may be the best option."
2020.acl-main.662.txt,2020,5 A Call to Action,"similarly, discussing and comparing the actual predictions made by the competing systems should be part of any competition culture—without it, it is hard to tell what a couple of points on some leaderboard mean."
2020.acl-main.662.txt,2020,5 A Call to Action,talk to trivia nerds you should talk to trivia nerds because they have useful information (not just about the election of 1876).
2020.acl-main.662.txt,2020,5 A Call to Action,the first is that single numbers have some variance; it’s better to communicate estimates with error bars.
2020.acl-main.662.txt,2020,5 A Call to Action,"the trivia community benefits from tools that make their job easier: show related questions, link to wikipedia, or predict where humans will answer."
2020.acl-main.662.txt,2020,5 A Call to Action,these skills are exactly those we want computers to develop.
2020.acl-main.662.txt,2020,5 A Call to Action,this can be problematic for several reasons.
2020.acl-main.662.txt,2020,5 A Call to Action,"this has a benefit to the public, who see how qa systems fail on difficult questions and to qa researchers, who have a spoonful of fun sugar to inspect their systems’ output and their competitors’."
2020.acl-main.662.txt,2020,5 A Call to Action,"this improves not only the quality of evaluation (we can have real-time human judging) but also lets the test set reflect the build it/break it cycle (ruef et al., 2016), as attempted by the 2019 iteration of fever (thorne et al., 2019)."
2020.acl-main.662.txt,2020,5 A Call to Action,"this is not meant to shame the writers but rather to help build and reinforce cultural norms: questions should be well-written, precise, and fulfill the creator’s goals."
2020.acl-main.662.txt,2020,5 A Call to Action,this not only makes more of your dataset discriminative but also reveals what makes a question answerable.
2020.acl-main.662.txt,2020,5 A Call to Action,"to ensure that our datasets properly “isolate the property that motivated [the dataset] in the first place” (zaenen, 2006), we need to explicitly appreciate the unavoidable ambiguity instead of silently glossing over it.14 this is already an active area of research, with conversational qa being a new setting actively explored by several datasets (reddy et al., 2018; choi et al., 2018); and other work explicitly focusing on identifying useful clarification questions (rao and daumé iii), thematically linked questions (elgohary et al., 2018) or resolving ambiguities that arise from coreference or pragmatic constraints by rewriting underspecified question strings (elgohary et al., 2019; min et al., 2020)."
2020.acl-main.662.txt,2020,5 A Call to Action,"to make this possible, we recommend that leaderboards include an easy way for anyone to download a system’s development predictions for qualitative analyses."
2020.acl-main.662.txt,2020,5 A Call to Action,"trivia is not just the accumulation of information but also connecting disparate facts (jennings, 2006)."
2020.acl-main.662.txt,2020,5 A Call to Action,"trivia nerds are writing questions anyway; we can save money and time if we pool resources.13 computer scientists benefit if the trivia community writes questions that aren’t trivial for computers to solve (e.g., avoiding quotes and named entities)."
2020.acl-main.662.txt,2020,5 A Call to Action,"while you may only rank by a single metric (this is what trivia tournaments do too), you may want to recognize the highestscoring model that was built by undergrads, took no more than one second per example, was trained only on wikipedia, etc."
2020.acl-main.662.txt,2020,5 A Call to Action,"while—particularly for leaderboards—it is tempting to turn everything into a single number, there are often different sub-tasks and systems who deserve recognition."
2020.acl-main.662.txt,2020,5 A Call to Action,won’t somebody look at the data?
2020.acl-main.662.txt,2020,5 A Call to Action,you may disagree with the superiority of quizbowl as a qa framework (de gustibus non est disputandum).
2020.acl-main.663.txt,2020,6 Conclusion,a common thread among all of the above sections is that reaching our semantic goals requires structure beyond representing meaning as a point in space.
2020.acl-main.663.txt,2020,6 Conclusion,"however, there is a trade-off between expressiveness and learnability: the more structure we add, the more difficult it can be to work with our representations."
2020.acl-main.663.txt,2020,6 Conclusion,i hope that this survey paper will help other researchers to develop the field in a way that keeps long-term goals in mind.
2020.acl-main.663.txt,2020,6 Conclusion,"in particular, it seems desirable to represent the meaning of a word as a region of space or as a classifier, and to work with probability logic."
2020.acl-main.663.txt,2020,6 Conclusion,"my own recent work in this direction has been to develop the pixie autoencoder (emerson, 2020a), and i look forward to seeing alternative approaches from other authors, as the field of distributional semantics continues to grow."
2020.acl-main.663.txt,2020,6 Conclusion,"to mitigate computationally expensive calculations in probabilistic models, there are promising new techniques such as amortised variational inference, used in the variational autoencoder (kingma and welling, 2014; rezende et al., 2014; titsias and la´zaro-gredilla, 2014)."
2020.acl-main.663.txt,2020,6 Conclusion,"to this end, there are promising neural architectures for working with structured data, such dependency graphs (for example: marcheggiani and titov, 2017) or logical propositions (for example: rockta¨schel and riedel, 2017; minervini et al., 2018)."
2020.acl-main.664.txt,2020,5 Conclusions,"during generation, the network is regularized to take into account explicit object/predicate constraints with multi-task learning."
2020.acl-main.664.txt,2020,5 Conclusions,"extensive experiments are performed on the mscoco dataset, showing that the proposed framework significantly outperforms the baselines, resulting in the state-of-the-art performance under various evaluation metrics."
2020.acl-main.664.txt,2020,5 Conclusions,in the near future we plan to extend the proposed approach to several other language-vision modeling tasks.
2020.acl-main.664.txt,2020,5 Conclusions,the representation is further enhanced with text and visual features of neighbouring nodes.
2020.acl-main.664.txt,2020,5 Conclusions,this paper presents a novel image captioning architecture that constructs caption-guided visual relationship graphs to introduce beneficial inductive bias to better utilize captions.
2020.acl-main.665.txt,2020,4 Conclusion,another avenue of research would be to investigate the role of synthetic data in surface realization in other languages.
2020.acl-main.665.txt,2020,4 Conclusion,"assuming the use of synthetic data, more needs to be investigated in order to fully maximize its benefit on performance."
2020.acl-main.665.txt,2020,4 Conclusion,"future work will look more closely at the choice of corpus, construction details of the synthetic dataset, as well as the tradeoff between training time and accuracy that comes with larger vocabularies."
2020.acl-main.665.txt,2020,4 Conclusion,the work described in this paper has focused on english.
2020.acl-main.665.txt,2020,4 Conclusion,"we have argued for the use of synthetic data in english surface realization, justified by the fact that its use gives a significant performance boost on the shallow task, from 72.7 bleu up to 80.1."
2020.acl-main.665.txt,2020,4 Conclusion,"while this is not yet at the level of reliability needed for neural nlg systems to be used commercially, it is a step in the right direction."
2020.acl-main.666.txt,2020,5 Conclusions,"as future work, we plan to further evaluate the impact of different sequential architectures, longer contexts, alternative sentence embeddings, and cleverer selection of distractors."
2020.acl-main.666.txt,2020,5 Conclusions,"at train time, our model considers much larger amounts of text per update than typical tokenlevel language models."
2020.acl-main.666.txt,2020,5 Conclusions,"inspired by deliberation networks and automatic post editing methods (xia et al., 2017; freitag et al., 2019), we ultimately want to apply our model to two-step generation, first selecting a sentence from a large set before refining it to fit the context."
2020.acl-main.666.txt,2020,5 Conclusions,"it takes advantage of pretrained bert embeddings to avoid having to learn token-level fluency, allowing the model to focus solely on the coherence of the sentence sequences."
2020.acl-main.666.txt,2020,5 Conclusions,our results on the story cloze task highlight the advantage of this strategy over word-level language models.
2020.acl-main.666.txt,2020,5 Conclusions,this work introduces a sentence-level language model which takes a sequence of sentences as context and predicts a distribution over a finite set of candidate next sentences.
2020.acl-main.666.txt,2020,5 Conclusions,we show that this strategy allows our model to surface appropriate endings to short stories out of a large set of candidates.
2020.acl-main.667.txt,2020,4 Conclusion,"in this work, we propose a flexible two-step approach for implicit event argument detection."
2020.acl-main.667.txt,2020,4 Conclusion,our head-word based approach effectively reduces the candidate size and achieves good results on the rams dataset.
2020.acl-main.667.txt,2020,4 Conclusion,"we further provide detailed error analysis, showing that non-local and non-core arguments are the main difficulties."
2020.acl-main.667.txt,2020,4 Conclusion,we hope that this work can shed some light and inspire future work at this line of research.
2020.acl-main.668.txt,2020,9 Conclusion,future work will explore the use of domain adaptation techniques to enhance performance where the domains of the ci and event text differ substantially.
2020.acl-main.668.txt,2020,9 Conclusion,"in this paper we argued that the task of predicting the time of historical events strikes a balance between being a focused task, with transparent evaluation and interpretable results, and presenting challenges that are not simple to overcome using standard nlp models."
2020.acl-main.668.txt,2020,9 Conclusion,"we outlined a procedure to extract the ci related to an event and compared two approaches for the task, using bag of embeddings and an lstm, showing that the latter achieves the best performance."
2020.acl-main.669.txt,2020,5 Conclusion,"our methods use only entity types, yet they yield higher performance than previous work on both nyt-fb and tacred."
2020.acl-main.669.txt,2020,5 Conclusion,"ure remains challenging, which requires improved methods to deal with silver data."
2020.acl-main.669.txt,2020,5 Conclusion,"we also plan to use different types of labelled data, e.g., domain specific data sets, to ascertain whether entity type information is more discriminative in sub-languages."
2020.acl-main.669.txt,2020,5 Conclusion,"we have investigated the current experimental setting, concluding that a strong inductive bias is required to train a relation extraction model without labelled data."
2020.acl-main.669.txt,2020,5 Conclusion,we have shown the importance of entity types in ure.
2020.acl-main.67.txt,2020,7 Conclusion and Future Work,"also, we plan to use large-scale unlabeled data to improve the performance further."
2020.acl-main.67.txt,2020,7 Conclusion and Future Work,"furthermore, our framework can be efficiently applied to other graph-to-sequence tasks such as webnlg (gardent et al., 2017) and syntax-based neural machine translation (bastings et al., 2017)."
2020.acl-main.67.txt,2020,7 Conclusion and Future Work,in future work we would like to do several experiments on other related tasks to test the versatility of our framework.
2020.acl-main.67.txt,2020,7 Conclusion and Future Work,"in this work, we presented a novel graph-to-sequence approach which uses line graph to model the relationships between labeled edges from the original amr graph."
2020.acl-main.67.txt,2020,7 Conclusion and Future Work,the ablation studies also demonstrate that exploring edge relations brings benefits to graph-to-sequence modeling.
2020.acl-main.67.txt,2020,7 Conclusion and Future Work,the mix-order graph attention networks are found effective when handling indirectly connected nodes.
2020.acl-main.670.txt,2020,6 Conclusion,an analysis of the performance of our model emphasizes the need for better document-level models that can overcome the new challenges posed by our dataset.
2020.acl-main.670.txt,2020,6 Conclusion,"as our research community moves towards document level ie and discourse modeling, we position this dataset as a testing ground to focus on this important and challenging task."
2020.acl-main.670.txt,2020,6 Conclusion,"each of these tasks challenges existing methodologies in the information extraction domain, which, by and large, focus on short text sequences."
2020.acl-main.670.txt,2020,6 Conclusion,"this task poses multiple technical and modeling challenges, including 1. the use of transformer-based models on long documents and related device memory issues, 2. aggregating coreference information from across documents in an end-to-end manner, 3. identifying salient entities in a document and 4. performing n-ary relation extraction of these entities."
2020.acl-main.670.txt,2020,6 Conclusion,"we also develop a baseline model for our dataset, which, to the best of our knowledge, is the first attempt toward a neural document level ie that can perform all the necessary subtasks in an end-to-end manner."
2020.acl-main.670.txt,2020,6 Conclusion,"we introduce scirex, a comprehensive and challenging dataset for information extraction on full documents."
2020.acl-main.670.txt,2020,6 Conclusion,"we show that using a document level model gave a significant improvement in terms of recall, compared to existing paragraphlevel approaches."
2020.acl-main.671.txt,2020,6 Conclusion,"additionally, it is less susceptible to gender and number biases as the performance on knowref suggests."
2020.acl-main.671.txt,2020,6 Conclusion,all this taken together confirms that selfsupervision is possible for commonsense reasoning tasks.
2020.acl-main.671.txt,2020,6 Conclusion,"at the more challenging wsc task, it outperforms all unsupervised approaches while being comparable in performance to the most recent supervised approaches."
2020.acl-main.671.txt,2020,6 Conclusion,"furthermore, we seek to investigate the transferability of the obtained inductive bias to other commonsense-demanding downstream tasks, which are distinct from the winograd-structure."
2020.acl-main.671.txt,2020,6 Conclusion,"possibilities are automatically generating an extensive collection of similar sentences or pre-training in a self-supervised fashion on large-scale winograd-structured datasets, such as the recently published winogrande (sakaguchi et al., 2019)."
2020.acl-main.671.txt,2020,6 Conclusion,the proposed approach outperforms all approaches on pdp and dpr tasks.
2020.acl-main.671.txt,2020,6 Conclusion,"therefore, future work will aim at relaxing the prior of winograd-structured twin-question pairs."
2020.acl-main.671.txt,2020,6 Conclusion,"we believe in order to solve commonsense reasoning truly, algorithms should refrain from using labeled data, instead exploit the structure of the task itself."
2020.acl-main.672.txt,2020,6 Discussion,"comparable performance can be obtained with a fraction (1/6th) of long-range memories if they are spaced equally across the network, or in the latter layers."
2020.acl-main.672.txt,2020,6 Discussion,"for example, the differentiable neural computer (graves et al., 2016) and recent memory-augmented agents for reinforcement learning, which utilise a distinct working memory with a single long-range episodic memory (fortunato et al., 2019)."
2020.acl-main.672.txt,2020,6 Discussion,"here we show that longrange attention does not need to be scaled for every layer, and thus these architectures can be further sped-up with this observation."
2020.acl-main.672.txt,2020,6 Discussion,however these models maintain the use of uniform memory capacity for each layer.
2020.acl-main.672.txt,2020,6 Discussion,"in our set of interventions, we only modify the flow of information within the network, versus the number of trainable parameters."
2020.acl-main.672.txt,2020,6 Discussion,our finding is that we do not need long-range memories at every layer of the network.
2020.acl-main.672.txt,2020,6 Discussion,perhaps performance could be improved by adding additional layers of episodic memories.
2020.acl-main.672.txt,2020,6 Discussion,the practice of storing deep long-range memories is not scalable if we wish for neural networks to have the kinds of large-horizon reasoning that humans possess.
2020.acl-main.672.txt,2020,6 Discussion,"there have been a number of long-range transformer variants published in the past year (lample et al., 2019; rae et al., 2019; roy et al., 2020; kitaev et al., 2020) which aim to extend the range of attention via sparsity or compression."
2020.acl-main.672.txt,2020,6 Discussion,"this study also has implications for researchers using a single long-range memory, which has typically been the approach in traditional rnn + attention systems."
2020.acl-main.672.txt,2020,6 Discussion,this study has implications for practitioners interested in speeding up deep transformer-xl models.
2020.acl-main.672.txt,2020,6 Discussion,thus we do not have confounding factors of varying network capacity.
2020.acl-main.672.txt,2020,6 Discussion,"we also find a real performance drop using a single long-range memory, proving long-range dependency is not superfluous to the task."
2020.acl-main.672.txt,2020,6 Discussion,we believe the solution of maintaining a small number of long-range memories is a step towards tractable lifelong memory.
2020.acl-main.672.txt,2020,6 Discussion,"we explore a set of interventions to the transformer-xl’s architecture that are very simple to implement, i.e.a few lines of code, but shed light on the fundamental workings of the model when modelling long sequences of text."
2020.acl-main.672.txt,2020,6 Discussion,we hypothesise this is because modelling long-range correlations is best done when representations are first formed from short-range correlations.
2020.acl-main.673.txt,2020,6 Conclusions,a sample-based mutual information upper bound is derived to help reduce the dependence between embedding spaces.
2020.acl-main.673.txt,2020,6 Conclusions,"concurrently, the original text information is well preserved by maximizing the mutual information between input sentences and latent representations."
2020.acl-main.673.txt,2020,6 Conclusions,"following the theoretical guidance from information theory, our method separates the textual information into independent spaces, constituting style and content representations."
2020.acl-main.673.txt,2020,6 Conclusions,"for future work, our model can be extended to disentangled representation learning with non-categorical style labels, and applied to zero-shot style transfer with newly-coming unseen styles."
2020.acl-main.673.txt,2020,6 Conclusions,"in experiments, we introduce several two-sample test statistics to measure label-embedding correlation."
2020.acl-main.673.txt,2020,6 Conclusions,the proposed model achieves competitive performance compared with previous methods on both conditional generation and style transfer.
2020.acl-main.673.txt,2020,6 Conclusions,we have proposed a novel information-theoretic disentangled text representation learning framework.
2020.acl-main.674.txt,2020,6 Conclusion and Future work,"although the conversion of visual cues to captions cause a loss of information, the addition of scene-text mitigates most of the loss."
2020.acl-main.674.txt,2020,6 Conclusion and Future work,"better emotion, scene, scene-text, object detection and captions might lead to further improvement of performance."
2020.acl-main.674.txt,2020,6 Conclusion and Future work,"syntax matches play a vital role in achieving the accuracy, but are not entirely the reason behind it."
2020.acl-main.674.txt,2020,6 Conclusion and Future work,the scene-text holds vital information and can be used to achieve good accuracy on this task.
2020.acl-main.674.txt,2020,6 Conclusion and Future work,using attention to associate the arps with the textual and visual cues is helping the task.
2020.acl-main.675.txt,2020,4 Conclusion,"first, we will explore mechanisms for instance-specific translation that are more sophisticated than the aggregation of translation vectors of nearest dictionary neighbours."
2020.acl-main.675.txt,2020,4 Conclusion,our experiments show that (1) in-stamap significantly outperforms four state-of-the-art projection-based clwe models on a benchmark bli dataset with 28 language pairs and (2) that it yields largest improvements for pairs of distant languages with a lower degree of isomorphism between their respective monolingual spaces.
2020.acl-main.675.txt,2020,4 Conclusion,"second, we plan to couple instance-based mapping with other informative features (e.g., character-level features) in classification-based bli frameworks (heyman et al., 2017; karan et al., 2020)."
2020.acl-main.675.txt,2020,4 Conclusion,the instamap code is available at: https://github.com/codogogo/instamap.
2020.acl-main.675.txt,2020,4 Conclusion,"this way, we learn a globally non-linear projection."
2020.acl-main.675.txt,2020,4 Conclusion,"unlike existing projection-based clwe induction models, which learn a global linear projection matrix, instamap couples global rotation with instance-specific translations."
2020.acl-main.675.txt,2020,4 Conclusion,"we have proposed instamap, a simple and effective approach for improving the post-hoc cross-lingual alignment between non-isomorphic monolingual embedding spaces."
2020.acl-main.675.txt,2020,4 Conclusion,we plan to extend this work in two directions.
2020.acl-main.676.txt,2020,7 Discussion,"an alternative line of work on paraphrase-based data augmentation (ganitkevitch et al., 2013; iyyer et al., 2018) uses external, text-only resources to encourage robust interpretation of new inputs corresponding to known outputs."
2020.acl-main.676.txt,2020,7 Discussion,further improvements are likely obtainable by constraining the extracted fragments to respect constituent boundaries when syntactic information is available.
2020.acl-main.676.txt,2020,7 Discussion,"more generally, the present results underline the extent to which current models fail to learn simple, context-independent notions of reuse, but also how easy it is to make progress towards addressing this problem without fundamental changes in model architecture."
2020.acl-main.676.txt,2020,7 Discussion,the experiments presented here focus on rewriting sentences using evidence within a dataset to encourage generalization to new outputs.
2020.acl-main.676.txt,2020,7 Discussion,"the procedure detailed in this paper relies on exact string matching to identify common context; future work might take advantage of learned representations of spans and their environments (mikolov et al., 2013; peters et al., 2018)."
2020.acl-main.676.txt,2020,7 Discussion,"the two lines of work could be combined, e.g.by using geca-identified fragments to indicate productive locations for sub-sentential paraphrasing."
2020.acl-main.676.txt,2020,7 Discussion,"we introduced geca, a simple data augmentation scheme based on identifying local phrase substitutions that are licensed by common contexts, and demonstrated that extra training examples generated with geca lead to substantial improvements on both diagnostic and natural datasets for semantic parsing and language modeling."
2020.acl-main.676.txt,2020,7 Discussion,"while the approach is surprisingly effective in its current form, we view these results primarily as an invitation to consider more carefully the role played by representations of sentence fragments in larger questions about compositionality in blackbox sequence models."
2020.acl-main.677.txt,2020,6 Conclusion,"despite active research in text-to-sql parsing, many contemporary models struggle to learn good representations for a given database schema as well as to properly link column/table references in the question."
2020.acl-main.677.txt,2020,6 Conclusion,"empirically, the rat framework allows us to gain significant state of the art improvement on text-to-sql parsing."
2020.acl-main.677.txt,2020,6 Conclusion,"in this work, we present a unified framework for addressing the schema encoding and linking challenges."
2020.acl-main.677.txt,2020,6 Conclusion,"qualitatively, it provides a way to combine predefined hard schema relations and inferred soft self-attended relations in the same encoder architecture."
2020.acl-main.677.txt,2020,6 Conclusion,"thanks to relation-aware self-attention, it jointly learns schema and question representations based on their alignment with each other and schema relations."
2020.acl-main.677.txt,2020,6 Conclusion,"these problems are related: to encode & use columns/tables from the schema, the model must reason about their role in the context of the question."
2020.acl-main.677.txt,2020,6 Conclusion,"this representation learning will be beneficial in tasks beyond text-to-sql, as long as the input has some predefined structure."
2020.acl-main.678.txt,2020,5 Conclusion,"despite the existence of several prior work on event duration, this is the first attempt to jointly model three key dimensions of tcs—duration, frequency, and typical time— from cheap supervision signals mined from unannotated free text."
2020.acl-main.678.txt,2020,5 Conclusion,temporal common sense (tcs) is an important yet challenging research topic.
2020.acl-main.678.txt,2020,5 Conclusion,the proposed method may be an important module for future applications related to time.
2020.acl-main.678.txt,2020,5 Conclusion,"the proposed sequence modeling framework improves over bert in terms of handling reporting bias, taking into account the ordinal relations and exploiting interactions among multi-ple dimensions of time."
2020.acl-main.678.txt,2020,5 Conclusion,"the success of this model is confirmed by intrinsic evaluations on realnews and uds-t (where we see a 19% improvement), as well as extrinsic evaluations on timebank, hieve and mctaco."
2020.acl-main.679.txt,2020,6 Conclusion,"finally, we find that fine-tuning language models can lead to much-improved accuracy and stability."
2020.acl-main.679.txt,2020,6 Conclusion,"fine-tuning a model on a rather large number of examples similar to the wsc leads to increased robustness, but this stands in stark contrast to humans, who are robust to the perturbations without having been exposed to similar examples in the past."
2020.acl-main.679.txt,2020,6 Conclusion,"in an analysis of the behaviour of language models, we observe that there is a preference for referents in the object role and that the models do not always consider the discriminatory segments of examples."
2020.acl-main.679.txt,2020,6 Conclusion,it remains an open question whether this task-specific approach to generalisation constitutes a true advancement in “reasoning”.
2020.acl-main.679.txt,2020,6 Conclusion,"we found that compared to out-of-the-box models, humans are significantly more stable to the perturbations and that they answer non-associative examples with higher accuracy than associative ones, show sensitivity to wsc pair complementarity, and are more sensitive to sentence-level (as opposed to wordlevel) perturbations."
2020.acl-main.679.txt,2020,6 Conclusion,we presented a detailed investigation of the effect of linguistic perturbations on how language models and humans perform on the winograd schema challenge.
2020.acl-main.68.txt,2020,6 Conclusion,a pre-training and fine-tuning framework songnet is designed to address the problem.
2020.acl-main.68.txt,2020,6 Conclusion,extensive experiments conducted on two collected corpora demonstrate that our framework generates significantly better results in terms of both automatic metrics and human evaluations given arbitrary cold start formats.
2020.acl-main.68.txt,2020,6 Conclusion,"sets of symbols are tailordesigned to improve the modeling performance for format, rhyme, and sentence integrity."
2020.acl-main.68.txt,2020,6 Conclusion,we propose to tackle a challenging task called rigid formats controlled text generation.
2020.acl-main.680.txt,2020,7 Conclusions,"by analyzing the data, we quantify the temporal drift in named entity type and mention usage and identify that, as expected, the data distribution is more similar when drawn from closer time intervals."
2020.acl-main.680.txt,2020,7 Conclusions,this paper studies and models text data drift in the information extraction task of named entity recognition.
2020.acl-main.680.txt,2020,7 Conclusions,"we expect our data, results, and error analysis to inform the design of similar experimental setups for other nlp tasks beyond ner, such as part-of-speech tagging or relation extraction."
2020.acl-main.680.txt,2020,7 Conclusions,"we introduce a new data set of 12,000 english tweets stratified by time, which allows us to study the effects of drift and evaluate named entity recognition models in a realistic scenario of performing inference on temporally unseen data."
2020.acl-main.680.txt,2020,7 Conclusions,"we then use current state-of-the-art approaches for named entity recognition and demonstrated that, through modeling of temporal information, performance can be improved when testing on future data."
2020.acl-main.681.txt,2020,5 Conclusion,"future directions to explore include incorporating noise-robust training procedures (goldberger and ben-reuven, 2017) and example weighting (dehghani et al., 2018) during self-training, and exploring lexical alignment methods from literature on learning cross-lingual embeddings."
2020.acl-main.681.txt,2020,5 Conclusion,"in this work, we tackled the task of building generalizable supervised event trigger identification models using adversarial domain adaptation (ada) to introduce domain-invariance."
2020.acl-main.681.txt,2020,5 Conclusion,our best performing model (bert-a) was able to reach 44-49 f1 across both domains using no labeled target domain data.
2020.acl-main.681.txt,2020,5 Conclusion,"our experiments with two domains (english literature and news) showed that ada made supervised models more robust on out-of-domain data, with an average f1 score improvement of 3.9."
2020.acl-main.681.txt,2020,5 Conclusion,"preliminary experiments showed that finetuning bert-a on 1% labeled data, followed by selftraining led to substantial improvement, reaching 51.5 and 67.2 f1 on literature and news respectively."
2020.acl-main.681.txt,2020,5 Conclusion,"while these results are encouraging, we are yet to match supervised in-domain model performance."
2020.acl-main.682.txt,2020,7 Conclusions & Future Work,"bleu); the attribute-prediction task would use the decoder representation to predict the attributes of the objects in the image (elliott and ka´da´r, 2017, e.g.); and the zero-shot setting could leverage the nocaps dataset (agrawal et al., 2018)."
2020.acl-main.682.txt,2020,7 Conclusions & Future Work,"clearly, even models with high in-domain gameplay success rates still have difficulty generalising to new scenarios."
2020.acl-main.682.txt,2020,7 Conclusions & Future Work,compguesswhat?!requires models to learn to combine the co-grounded information provided for every turn.
2020.acl-main.682.txt,2020,7 Conclusions & Future Work,"finally, from the evaluation presented here, it emerges that these models learn task-specific representations that do not generalise to unseen object categories."
2020.acl-main.682.txt,2020,7 Conclusions & Future Work,"for example, in image captioning, the goal-oriented evaluation would be the textual similarity metrics (e.g."
2020.acl-main.682.txt,2020,7 Conclusions & Future Work,"for instance, in the context of compguesswhat?!, it would be used to learn a representation for the current turn that is influenced by both the language and visual modality."
2020.acl-main.682.txt,2020,7 Conclusions & Future Work,"however, the same framework could be applied to other multi-modal tasks."
2020.acl-main.682.txt,2020,7 Conclusions & Future Work,"in addition, we will use the compguesswhat?!image annotations to design a visual grounding evaluation to assess the ability of the model to attend to the correct objects during the turns of the dialogue."
2020.acl-main.682.txt,2020,7 Conclusions & Future Work,"in the following, we discuss insights gained from the evaluation and new research directions for this task."
2020.acl-main.682.txt,2020,7 Conclusions & Future Work,"in this work, we have shown how our multi-task evaluation framework can be be applied to guess-what?!."
2020.acl-main.682.txt,2020,7 Conclusions & Future Work,"likewise, in the vision-and-dialog navigation task (thomason et al., 2019b), the goal-oriented evaluation is the navigation task; attribute prediction is based on predicting the attributes of the hidden object when the agent decides it is in the correct room, and the zeroshot setting could evaluate model performance on novel combinations of rooms and object types."
2020.acl-main.682.txt,2020,7 Conclusions & Future Work,the attribute prediction task shows that model representations are not able to accurately recover attribute representations.
2020.acl-main.682.txt,2020,7 Conclusions & Future Work,"therefore, we propose that compguesswhat?!represents a benchmark dataset for evaluating the design of such an attribute compositionality operator that would be a possible implementation of compositionality a` la barsalou (2017)."
2020.acl-main.682.txt,2020,7 Conclusions & Future Work,we argue that this result calls for new approaches to exploiting and representing textual and visual data.
2020.acl-main.682.txt,2020,7 Conclusions & Future Work,we believe that models should be equipped with a co-grounding operator that fuses the textual and visual modalities.
2020.acl-main.682.txt,2020,7 Conclusions & Future Work,"we found that the best performing model achieves a grolla score of 50.06%; notably this model’s out-of-domain accuracy is under 30%, as compared to the human performance on the original guesswhat?!dataset of 90.2% (de vries et al., 2017)."
2020.acl-main.682.txt,2020,7 Conclusions & Future Work,we hope that grolla and the compguesswhat?!data will encourage the implementation of learning mechanisms that fuse taskspecific representations with more abstract representations to encode attributes in a more compositional manner.
2020.acl-main.682.txt,2020,7 Conclusions & Future Work,"we proposed compguesswhat?!as an implementation of grolla, a multi-task evaluation framework for grounded language learning with attributes."
2020.acl-main.683.txt,2020,6 Conclusion,"in this paper, we propose a novel cross-modality relevance (cmr) for language and vision reasoning."
2020.acl-main.683.txt,2020,6 Conclusion,"moreover, the model trained on nlvr2 boosts the training of vqa v2.0 dataset."
2020.acl-main.683.txt,2020,6 Conclusion,our approach exceeds the state-of-the-art on nlvr2 and vqa v2.0 datasets.
2020.acl-main.683.txt,2020,6 Conclusion,our proposed architectural component for capturing relevance patterns can be used independently from the full cmr architecture and is potentially applicable for other multi-modal tasks.
2020.acl-main.683.txt,2020,6 Conclusion,"particularly, we argue for the significance of relevance between the components of the two modalities for reasoning, which includes entity relevance and relational relevance."
2020.acl-main.683.txt,2020,6 Conclusion,the experiments and the empirical analysis demonstrate cmr’s capability of modeling relational relevance for reasoning and consequently its better generalizability to unobserved data.
2020.acl-main.683.txt,2020,6 Conclusion,this result indicates the significance of relevance patterns.
2020.acl-main.683.txt,2020,6 Conclusion,we evaluate the proposed cmr on nlvr and vqa tasks.
2020.acl-main.683.txt,2020,6 Conclusion,we propose an end-to-end cross-modality relevance framework that is tailored for language and vision reasoning.
2020.acl-main.684.txt,2020,6 Conclusion,"also, language grounding models that incorporate richer alignments between explanations and demonstrations can lead to more effective learning."
2020.acl-main.684.txt,2020,6 Conclusion,"conversely, this would be complicated in domains with rich composition and nesting in logical forms, which go beyond simple features and relations.e.g., “click the third email from jeanette”, and where modeling inverse semantics is infeasible."
2020.acl-main.684.txt,2020,6 Conclusion,"future work can also explore curriculum learning in this domain, by first learning simpler tasks, which can be compositionally invoked in explanations for complex tasks."
2020.acl-main.684.txt,2020,6 Conclusion,"here, we posed the learning of web-based tasks as similar to instruction-following problem, with no aspect of interactivity or exploration of the environment."
2020.acl-main.684.txt,2020,6 Conclusion,"in future work, the possibility of learning from a mix of explanations, exploration and a limited budget of interaction with the environment can be explored."
2020.acl-main.684.txt,2020,6 Conclusion,"in terms of problem framing, interactive use-cases that enable the agent to ask questions when it is confused may also be realistic."
2020.acl-main.684.txt,2020,6 Conclusion,"in terms of technique, our bottom-up approach to generating logical forms ensures consistency between interpretations and the ambient context during search."
2020.acl-main.684.txt,2020,6 Conclusion,our work here is a step in the direction of teachable ai agents that can learn new behavior from conversational interactions with ordinary users.
2020.acl-main.684.txt,2020,6 Conclusion,"since led only requires tokenization as pre-processing, it can possibly extend to low resource scenarios."
2020.acl-main.685.txt,2020,8 Discussion and Limitations,"finally, we introduced the reward-learned reranker approach which alleviates language drift and achieves the highest human performance, by constraining the functional learning to happen on the level of utterances generated by a pre-trained language model."
2020.acl-main.685.txt,2020,8 Discussion and Limitations,"gpt2 (radford et al., 2019)) whose samples potentially do not fit the functional context."
2020.acl-main.685.txt,2020,8 Discussion and Limitations,"however, since the functional signal is not currently influencing the sampling from the language model, this will lead to poor performance when using more general language models with weaker conditioning (e.g."
2020.acl-main.685.txt,2020,8 Discussion and Limitations,"moving towards integrating our findings into more realistic applications of self-play, e.g., user simulation in dialogue (schatzmann et al., 2006; shah et al., 2008), these shortcomings need to be addressed."
2020.acl-main.685.txt,2020,8 Discussion and Limitations,"self-play between speakers and listeners can result in language drift, the most severe of which being pragmatic drift."
2020.acl-main.685.txt,2020,8 Discussion and Limitations,"since speakers and listeners are learning concurrently, they can co-adapt to pair-specific policies that deviate from the policies that humans learn."
2020.acl-main.685.txt,2020,8 Discussion and Limitations,"this pathological behaviour of self-play is not specific to language and extends to other policies (carroll et al., 2019)."
2020.acl-main.685.txt,2020,8 Discussion and Limitations,"we presented a method for teaching agents to communicate with humans in natural language, by combining two learning signals coming from multi-agent communication and traditional data-driven natural language learning techniques, which adds on recent efforts of blending emergent communication with natural language (lowe et al., 2020; lu et al., 2020)."
2020.acl-main.686.txt,2020,6 Conclusion,"we conduct hardware-aware neural architecture search in an ample design space with an efficient weight-shared supertransformer, consuming four orders of magnitude less cost than the prior evolved transformer, and discover high-performance low-latency models."
2020.acl-main.686.txt,2020,6 Conclusion,we hope hat can open up an avenue towards efficient transformer deployments for real-world applications.
2020.acl-main.686.txt,2020,6 Conclusion,we propose hardware-aware transformers (hat) framework to solve the challenge of efficient deployments of transformer models on various hardware platforms.
2020.acl-main.687.txt,2020,8 Conclusion,"in this paper, we present “hard-coded” gaussian attention, which while lacking any learned parameters can rival multi-headed attention for neural machine translation."
2020.acl-main.687.txt,2020,8 Conclusion,our experiments suggest that encoder and decoder self-attention is not crucial for translation quality compared to cross attention.
2020.acl-main.687.txt,2020,8 Conclusion,our work provides a foundation for future work into simpler and more computationally efficient neural machine translation.
2020.acl-main.687.txt,2020,8 Conclusion,we further find that a model with hard-coded selfattention and just a single cross attention head performs slightly worse than a baseline transformer.
2020.acl-main.688.txt,2020,7 Conclusion,"from a practical perspective, our results indicate that we can initialise models with a pre-trained model regardless of the parent language or vocabulary handling."
2020.acl-main.688.txt,2020,7 Conclusion,"from a theoretical perspective, our results indicate that while transfer learning is effective in our scenario, it performed less “transfer” than previously thought."
2020.acl-main.688.txt,2020,7 Conclusion,"in transfer learning, we can also transfer the alignment."
2020.acl-main.688.txt,2020,7 Conclusion,"the embeddings contain transferable information, as long as the vectors are mapped correctly and the inner layers are also transferred."
2020.acl-main.688.txt,2020,7 Conclusion,"therefore, a promising research direction to investigate would involve the development and assessment of improved initialisation methods that would more efficiently yield the benefits of the model transfer."
2020.acl-main.688.txt,2020,7 Conclusion,"therefore, models can train and converge faster, which is useful in high-resource settings."
2020.acl-main.688.txt,2020,7 Conclusion,this result further highlights the use of transfer learning as a better model initialisation.
2020.acl-main.688.txt,2020,7 Conclusion,transferred parents without fine-tuning will align the input diagonally and copy most of the tokens.
2020.acl-main.688.txt,2020,7 Conclusion,we demonstrate that the internal layers of the network are the most crucial for cross-lingual transfer learning.
2020.acl-main.688.txt,2020,7 Conclusion,"we further demonstrate that transfer learning still functions with a simple copy model, even with an artificial dataset—albeit with a reduced quality."
2020.acl-main.688.txt,2020,7 Conclusion,"while not as optimal, we can still perform transfer learning by excluding the embedding."
2020.acl-main.688.txt,2020,7 Conclusion,"with this perspective in mind, we can use transfer learning as a better initialisation, resulting in the child model having more stable gradients from the onset of training."
2020.acl-main.688.txt,2020,7 Conclusion,"with transfer learning, the model can be trained with more aggressive hyperparameters—such as removing the learning rate warm-up entirely—to further improve the convergence speed."
2020.acl-main.689.txt,2020,6 Conclusion,"end-to-end experiments and ablation studies on large datasets at different noise levels show that the multi-domain curriculum simultaneously reaches or outperforms the individual performance and brings solid gains over no-curriculum training, on in-domain and out-of-domain testsets."
2020.acl-main.689.txt,2020,6 Conclusion,existing curriculum learning research in nmt focuses on a single domain.
2020.acl-main.689.txt,2020,6 Conclusion,we carefully introduce instance-level features and learn a training curriculum to gradually concentrate on multi-domain relevant and noise-reduced data batches.
2020.acl-main.689.txt,2020,6 Conclusion,we present a multi-domain curriculum learning method.
2020.acl-main.69.txt,2020,5 Discussion,automatic and manual evaluations show that syn-qg is able to generate a large number of diverse and highly relevant questions with better fluency.
2020.acl-main.69.txt,2020,5 Discussion,"however, we believe that with an extensible and transparent architecture, it is very much possible to keep improving the system continuously in order to achieve this larger goal."
2020.acl-main.69.txt,2020,5 Discussion,the larger goal of qg is currently far from being solved.
2020.acl-main.69.txt,2020,5 Discussion,"understanding abstract representations, leveraging world knowledge, and reasoning about them is crucial."
2020.acl-main.69.txt,2020,5 Discussion,verb-focused rules help approach long-distance dependencies and reduce the need for explicit sentence simplification by breaking down a sentence into clauses while custom rules like implications serve a purpose similar to a reranker to discard irrelevant questions but with increased determinism.
2020.acl-main.69.txt,2020,5 Discussion,"we introduced syn-qg, a set of broad coverage rules leveraging event-based and sub-event based sentence views along with verb-specific argument descriptions."
2020.acl-main.69.txt,2020,5 Discussion,"while our work focuses on sentence-level qg, it would be interesting to see how questions generated from verbnet predicates would have an impact on multi-sentence or passage level qg, where the verb-agnostic states of the participants would change as a function of multiple verbs."
2020.acl-main.690.txt,2020,4 Conclusions,both allow debiasing while maintaining general translation performance.
2020.acl-main.690.txt,2020,4 Conclusions,"lattice rescoring, although a two-step procedure, allows far more debiasing and potentially no degradation, without requiring access to the original model."
2020.acl-main.690.txt,2020,4 Conclusions,"we demonstrate strong improvements under the winomt challenge set by adapting to tiny, handcrafted gender-balanced datasets for three language pairs."
2020.acl-main.690.txt,2020,4 Conclusions,"we do not claim to fix the bias problem in nmt, but demonstrate that bias can be reduced without degradation in overall translation quality."
2020.acl-main.690.txt,2020,4 Conclusions,we suggest small-domain adaptation as a more effective and efficient approach to debiasing machine translation than counterfactual data augmentation.
2020.acl-main.690.txt,2020,4 Conclusions,we treat the presence of gender bias in nmt systems as a domain adaptation problem.
2020.acl-main.690.txt,2020,4 Conclusions,"while naive domain adaptation leads to catastrophic forgetting, we further demonstrate two approaches to limit this: ewc and a lattice rescoring approach."
2020.acl-main.691.txt,2020,8 Conclusion,"however, this is associated with a drop in bleu score, indicating that better automatic evaluation is needed."
2020.acl-main.691.txt,2020,8 Conclusion,"the resulting model has improved performance in the ideal, zero-shot scenario of original→original translation, as measured by human evaluation of adequacy and fluency."
2020.acl-main.691.txt,2020,8 Conclusion,"we have demonstrated that translationese and original text can be treated as separate target languages in a “multilingual” model, distinguished by a classifier trained using only monolingual and synthetic data."
2020.acl-main.692.txt,2020,6 Conclusions and Future Work,"another interesting avenue is applying this to unsupervised nmt, which is highly sensitive to domain mismatch (marchisio et al., 2020; kim et al., 2020)."
2020.acl-main.692.txt,2020,6 Conclusions and Future Work,our methods perform similarly or better than an established data selection method and oracle in-domain training across all five domains in the benchmark.
2020.acl-main.692.txt,2020,6 Conclusions and Future Work,"this work just scratches the surface with what can be done on the subject; possible avenues for future work include extending this with multilingual data selection and multilingual lms (conneau and lample, 2019; conneau et al., 2019; wu et al., 2019; hu et al., 2020), using such selection methods with domain-curriculum training (zhang et al., 2019; wang et al., 2019b), applying them on noisy, web-crawled data (junczys-dowmunt, 2018) or for additional tasks (gururangan et al., 2020)."
2020.acl-main.692.txt,2020,6 Conclusions and Future Work,"we demonstrated the effectiveness of our methods on a new, improved data split we created for a previously studied multi-domain machine translation benchmark."
2020.acl-main.692.txt,2020,6 Conclusions and Future Work,"we hope this work will encourage more research on finding the right data for the task, towards more efficient and robust nlp."
2020.acl-main.692.txt,2020,6 Conclusions and Future Work,"we proposed new methods to harness this property for domain data selection using distance-based ranking in vector space and pretrained lm finetuning, requiring only a small set of in-domain data."
2020.acl-main.692.txt,2020,6 Conclusions and Future Work,we showed that massive pre-trained language models are highly effective in mapping data to domains in a fully-unsupervised manner using averagepooled sentence representations and gmm-based clustering.
2020.acl-main.692.txt,2020,6 Conclusions and Future Work,"we suggest that such clusters are a more appropriate, data driven approach to domains in natural language than simplistic labels (e.g.“medical text”), and that it will improve over time as better and larger pretrained lms will become available."
2020.acl-main.693.txt,2020,4 Conclusions and future work,"as well as randomly selecting training data, we assess training with mini-batches consisting only of single document contexts."
2020.acl-main.693.txt,2020,4 Conclusions and future work,our approach relies on a procedure for sampling a set of diverse batch-level contexts using n-wise sample ordering.
2020.acl-main.693.txt,2020,4 Conclusions and future work,we demonstrate improvements covering three document-level evaluation metrics: bleu and ter for nmt and gleu for gec.
2020.acl-main.693.txt,2020,4 Conclusions and future work,we finish by noting that the original mert procedure developed for smt optimised document-level bleu and with our procedure we reintroduce this to nmt.
2020.acl-main.693.txt,2020,4 Conclusions and future work,we present a novel approach for structured loss training with document-level objective functions.
2020.acl-main.693.txt,2020,4 Conclusions and future work,"while the scope of this work does not extend to sampling sentences given document context, this would be an interesting direction for future work."
2020.acl-main.694.txt,2020,4 Discussions and Conclusions,"also, we plan to consider more structured latent variables beyond modeling the sentence-level variation as well as to apply our vnmt model to more language pairs."
2020.acl-main.694.txt,2020,4 Discussions and Conclusions,we also demonstrate the robustness of our proposed model in an increased multimodality condition and on a simulated out-of-domain test set.
2020.acl-main.694.txt,2020,4 Discussions and Conclusions,we conjecture that conditioning the posterior on the target sentences would be more beneficial.
2020.acl-main.694.txt,2020,4 Discussions and Conclusions,we plan to conduct a more in-depth investigation into actual multimodality condition with high-coverage sets of plausible translations.
2020.acl-main.694.txt,2020,4 Discussions and Conclusions,we present a variational nmt model that outperforms a strong state-of-the-art non-latent nmt model.
2020.acl-main.694.txt,2020,4 Discussions and Conclusions,we show that the gain modestly comes from the introduction of a family of flexible distribution based on normalizing flows.
2020.acl-main.695.txt,2020,6 Conclusion,"additionally, unseen inflectional variants of seen forms are to be predicted."
2020.acl-main.695.txt,2020,6 Conclusion,"we believe our benchmark system represents a reasonable approach to solving the problem based on past work and highlights many directions for improvement, e.g.joint modeling and making better use of distributional semantic information."
2020.acl-main.695.txt,2020,6 Conclusion,"we discuss the data required to undertake this task, a benchmark for solving it, and multiple evaluation metrics."
2020.acl-main.695.txt,2020,6 Conclusion,"we present a framework for the paradigm discovery problem, in which words attested in an unannotated corpus are analyzed according to the morphosyntactic property set they realize and the paradigm to which they belong."
2020.acl-main.696.txt,2020,6 Conclusion,"furthermore, this research presents the first schwa-deletion model for punjabi, and has contributed several freely-accessible scripts for scraping hindi and punjabi pronunciation data from online sources."
2020.acl-main.696.txt,2020,6 Conclusion,"our system requires no hard-coded phonological rules, instead relying solely on pairs of orthographic and phonetic forms for hindi words at training time."
2020.acl-main.696.txt,2020,6 Conclusion,we have presented the first statistical schwa deletion classifier for hindi achieves state-of-the-art performance.
2020.acl-main.697.txt,2020,5 Summary & Conclusion,"in his visionary paper from 1966, ellis page provided a proof-of-concept demonstration of the possibility of automated grading of essays, as well as outlined some potential challenges to its adoption."
2020.acl-main.697.txt,2020,5 Summary & Conclusion,"in particular, while page imagined the main use case of awe to be in the service of a harried english teacher and his feedback-thirsty students, in reality, the most visible use case has arguably evolved to be automated scoring of essays for standardized testing, which, in turn, has led to new challenges, such as ensuring the validity and fairness of scores."
2020.acl-main.697.txt,2020,5 Summary & Conclusion,"subsequent research and practice have delivered on page’s minimum desiderata for an awe system; current research is working to address the outstanding challenges dealing with a variety of languages, content domains, and writing tasks."
2020.acl-main.697.txt,2020,5 Summary & Conclusion,"the field of awe has thus progressed according to the trajectory charted by page to a large extent, though not completely."
2020.acl-main.697.txt,2020,5 Summary & Conclusion,"the other development that page could not anticipate is the sheer pervasiveness of technology in people’s daily lives; awe software can be made available not only in classrooms to be used under the watchful eye of the english teacher, but (almost) anywhere and at any time, including on mobile devices."
2020.acl-main.697.txt,2020,5 Summary & Conclusion,"we believe that we, as researchers, can help users find value in our technology by considering the goals, engaging partners from other relevant disciplines, and designing the tools as well as their evaluations to focus on specific types of use."
2020.acl-main.697.txt,2020,5 Summary & Conclusion,"while it is difficult to predict specific uses people would find for such software, we outlined a number of types of use, depending on the goal: (a) consequential decision making about the user; (b) delivery of the best possible written product in partnership with the user; and (c) assisting the user in improving her writing skills."
2020.acl-main.698.txt,2020,6 Conclusion,further architectural innovations in deep learning seem necessary to deal with such discrete phenomena.(ii) we found that plms have difficulty distinguishing “informed” best guesses (based on information extracted from training corpora) from “random” best guesses (made in the absence of any evidence in the training corpora).
2020.acl-main.698.txt,2020,6 Conclusion,"implications for future work on pretrained language models.(i) both factual knowledge and logic are discrete phenomena in the sense that sentences with similar representations in current pretrained language models differ sharply in factuality and truth value (e.g., “newton was born in 1641” vs. “newton was born in 1642”)."
2020.acl-main.698.txt,2020,6 Conclusion,our results suggest that pretrained language models address open domain qa in datasets like lama by mechanisms that are more akin to relatively shallow pattern matching than the recall of learned factual knowledge and inference.
2020.acl-main.698.txt,2020,6 Conclusion,this implies that better confidence assessment of plm predictions is needed.(iii) our premise was that we should emulate human language processing and that therefore tasks that are easy for humans are good tests for nlp models.
2020.acl-main.698.txt,2020,6 Conclusion,"to the extent this is true, the two phenomena we have investigated in this paper – that plms seem to ignore negation in many cases and that they are easily confused by simple distractors – seem to be good vehicles for encouraging the development of plms whose performance on nlp tasks is closer to humans."
2020.acl-main.699.txt,2020,4 Conclusions,"a potential confound in our analysis is that some proceedings imposed a page limit for references; e.g., the acl conference gave unlimited space for references in 2010, 2012, and from 2016 onwards, but imposed a page limit in 2011 and 2013–2015."
2020.acl-main.699.txt,2020,4 Conclusions,"finally, since several influential neural network papers have been published in the 1990s (cf."
2020.acl-main.699.txt,2020,4 Conclusions,future work includes a deeper qualitative analysis of which (type of) papers are being cited; a more fine-grained analysis of different research topics in nlp to determine whether changes are more prevalent within certain areas than others; or extending the analysis to a larger set of the papers in the acl anthology.
2020.acl-main.699.txt,2020,4 Conclusions,"in addition, our analysis is limited to studying the age of the papers cited in the acl anthology – it does not make any claims about the complex network effects involved in researchers from particular institutions, countries, or sub-fields, and it does not study other venues that also publish nlp papers."
2020.acl-main.699.txt,2020,4 Conclusions,"some areas of nlp research did also not exist 15 years ago, e.g.social media analysis, potentially making it challenging to cite older related work."
2020.acl-main.699.txt,2020,4 Conclusions,"tab.2), a mostly quantitative analysis is limited in its ability to determine, e.g., to what extent we still engage with older literature outside of this domain."
2020.acl-main.699.txt,2020,4 Conclusions,there is also a marked difference between journal and conference publications in the distribution of citation age: journal articles feature more citations to older papers.
2020.acl-main.699.txt,2020,4 Conclusions,"these findings could be due to the increasing difficulty of keeping up with the literature, given that many more papers are being published now, in addition to the deluge of papers that appear on preprint servers."
2020.acl-main.699.txt,2020,4 Conclusions,"we can still observe an increase in the average number of citations per paper during this latter period, so it seems unlikely that this had an effect."
2020.acl-main.699.txt,2020,4 Conclusions,"we found that recently published papers (0–3 years old) are cited significantly more often in publications from recent years (ca.2015–2019), while papers published 15 or more years ago are being cited at a stable rate."
2020.acl-main.699.txt,2020,4 Conclusions,"we presented an analysis of citations in publications from major acl venues between 2010 and 2019, focusing on the distribution of the age of cited papers."
2020.acl-main.7.txt,2020,7 Conclusions,experimental results show that our approach significantly outperforms other baseline models and the proposed metrics are effective in evaluating the capabilities of models on generating persona-aware responses.
2020.acl-main.7.txt,2020,7 Conclusions,"in addition, to better reflect the persona characteristics of the response generation model, three metrics have been introduced to quantify the level of persona of the generated responses."
2020.acl-main.7.txt,2020,7 Conclusions,"in this paper, we proposed a variational neural network to model the conversation as well as the persona of users."
2020.acl-main.7.txt,2020,7 Conclusions,"on the basis of the network, two regularization terms are designed to guide the model in emphasizing the importance of the hidden user information."
2020.acl-main.70.txt,2020,6 Conclusion,"building upon the semantic embedding and online learning, our method allows finding high-quality evolving clusters."
2020.acl-main.70.txt,2020,6 Conclusion,extensive results further demonstrate that osdm has better performance compared to many state-of-the-art algorithms.
2020.acl-main.70.txt,2020,6 Conclusion,"in contrast to existing approaches, osdm does not require to specify the batch size and the dynamic number evolving clusters."
2020.acl-main.70.txt,2020,6 Conclusion,"in this paper, we propose a new online semanticenhanced dirichlet model for short text stream clustering."
2020.acl-main.70.txt,2020,6 Conclusion,it dynamically assigns each arriving document into an existing cluster or generating a new cluster based on the poly urn scheme.
2020.acl-main.70.txt,2020,6 Conclusion,"more importantly, osdm tried to incorporate semantic information in the proposed graphical representation model to remove the term ambiguity problem in short-text clustering."
2020.acl-main.700.txt,2020,8 Conclusion,"building upon eisenstein (2013); lynn et al.(2017), and hovy (2018), i argue that, following the historical development in areas related to nlp, users are ready also for the personalization of text classification models, enabling more flexible adaptation to truly processing their “natural” language rather than enforcing a uniform nlp treatment for everyone."
2020.acl-main.700.txt,2020,8 Conclusion,"i suggest to also shift the focus of our evaluation strategies towards the individual aims and characteristics of the end users of our labeling models, rather than aggregating all variations into objective truths, which will allow us to pay more attention to present social biases in our models."
2020.acl-main.700.txt,2020,8 Conclusion,"modeling demographic and personal variables as dynamic and social will allow to reflect the variety of ways individuals construct their identity by language, and to conduct novel sociolinguistic experiments to better understand the development in online communities."
2020.acl-main.700.txt,2020,8 Conclusion,"reflecting the current possibilities with available web and mobile data, i propose to expand the existing user modeling approaches in deep learning models with contextual personalization, mirroring different facets of one user in dynamic, socially conditioned vector representations."
2020.acl-main.701.txt,2020,6 Taking the ToU idea forward,"but even beyond our tou, the broader point stands: existing mrc approaches are not satisfactorily testing for a systematic set of content."
2020.acl-main.701.txt,2020,6 Taking the ToU idea forward,"drawing on work in psychology, philosophy, and pedagogy, we have argued for the tou as a minimal standard and a valuable target for mrc."
2020.acl-main.701.txt,2020,6 Taking the ToU idea forward,"if mrc is to achieve its ultimate goals, we—the nlp community—owe it to ourselves to ensure that our reading comprehension tests actually test for the comprehension we desire."
2020.acl-main.701.txt,2020,6 Taking the ToU idea forward,"our efforts demonstrate that it is possible, with a sufficiently interdisciplinary approach, to define a plausible floor for comprehension for a given class of applications."
2020.acl-main.701.txt,2020,6 Taking the ToU idea forward,"our tou for stories is a first attempt at defining what mrc systems should comprehend in a principled, systematic way."
2020.acl-main.701.txt,2020,6 Taking the ToU idea forward,this includes refining and perhaps expanding the questions; better defining the answers and evaluation procedures; building mrc corpora based on the tou; and developing better-performing systems.
2020.acl-main.701.txt,2020,6 Taking the ToU idea forward,we have also shown it to be beyond the reach of current systems.
2020.acl-main.701.txt,2020,6 Taking the ToU idea forward,"we ourselves are working on all four, and we welcome collaboration."
2020.acl-main.701.txt,2020,6 Taking the ToU idea forward,we therefore suggest that the nlp community further build on our tou.
2020.acl-main.702.txt,2020,7 Conclusions,"even though there are some areas where ffa% is close to parity with male first authorship, most areas have a substantial gap in the numbers for male and female authorship."
2020.acl-main.702.txt,2020,7 Conclusions,"however, the inequities that impact the number of women pursuing scientific research (roos, 2008; foschi, 2004; buchmann, 2009) and biases that impact citation patterns unfairly (brouns, 2007; feller, 2004; gupta et al., 2005) are well-documented."
2020.acl-main.702.txt,2020,7 Conclusions,"if anything, past research has shown that self-selection in the face of inequities and adversity leads to more competitive, capable, and confident cohorts (nekby et al., 2008; hardies et al., 2013)."
2020.acl-main.702.txt,2020,7 Conclusions,"strikingly, even though some gains were made in the early years of nlp, overall ffa% has not improved since the mid 2000s."
2020.acl-main.702.txt,2020,7 Conclusions,"these factors play a substantial role in creating the gender gap, as opposed to differences in innate ability or differences in quality of work produced by these two genders."
2020.acl-main.702.txt,2020,7 Conclusions,this paper did not explore the reasons behind the gender gaps.
2020.acl-main.702.txt,2020,7 Conclusions,"thus, in nlp, gender gaps exist both in authorship and citations."
2020.acl-main.702.txt,2020,7 Conclusions,"we also showed how ffa% varied by paper type, venue, academic age, and area of research."
2020.acl-main.702.txt,2020,7 Conclusions,"we analyzed the acl anthology to show that only ∼30% have female authors, ∼29% have female first authors, and ∼25% have female last authors."
2020.acl-main.702.txt,2020,7 Conclusions,we found no correlation between popularity of research area and ffa%.
2020.acl-main.702.txt,2020,7 Conclusions,"we used citation counts extracted from google scholar to show that, on average, male first authors are cited markedly more than female first authors, even when controlling for experience and area of work."
2020.acl-main.703.txt,2020,8 Conclusions,"bart performs comparably to roberta on discriminative tasks, and achieves new state-of-the-art results on several text generation tasks."
2020.acl-main.703.txt,2020,8 Conclusions,"future work should explore new methods for corrupting documents for pretraining, perhaps tailoring them to specific end tasks."
2020.acl-main.703.txt,2020,8 Conclusions,"we introduced bart, a pre-training approach that learns to map corrupted documents to the original."
2020.acl-main.704.txt,2020,7 Conclusion,"because the metric is trained end-to-end, bleurt can model human assessment with superior accuracy."
2020.acl-main.704.txt,2020,7 Conclusion,"furthermore, pre-training makes the metrics robust particularly robust to both domain and quality drifts."
2020.acl-main.704.txt,2020,7 Conclusion,"future research directions include multilingual nlg evaluation, and hybrid methods involving both humans and classifiers."
2020.acl-main.704.txt,2020,7 Conclusion,"we presented bleurt, a reference-based text generation metric for english."
2020.acl-main.705.txt,2020,5 Conclusion,"experiments show that our model improves over strong transformer baselines on multiple text generation tasks such as machine translation and abstractive summarization, and achieves new state-of-the-art on some of the translation tasks."
2020.acl-main.705.txt,2020,5 Conclusion,"for future work, we will explore the extension of conditional mlm to multimodal input such as image captioning."
2020.acl-main.705.txt,2020,5 Conclusion,"in this work, we propose a novel and generic approach to utilizing pre-trained language models to improve text generation without explicit parameter sharing, feature extraction, or augmenting with auxiliary tasks."
2020.acl-main.705.txt,2020,5 Conclusion,"our distillation approach indirectly influences the text generation model by providing soft-label distributions only, hence is model-agnostic."
2020.acl-main.705.txt,2020,5 Conclusion,"our proposed conditional mlm mechanism leverages unsupervised language models pre-trained on large corpus, and then adapts to supervised sequence-to-sequence tasks."
2020.acl-main.706.txt,2020,7 Conclusions and Future Directions,an rl agent that leverages natural language descriptions of physical events to reason about the solution for a given goal (similar to zhong et al.(2020)) or for reward shaping (similar to goyal et al.(2019)) could be a compelling line of future research.
2020.acl-main.706.txt,2020,7 Conclusions and Future Directions,esprit uses a two-step approach for qualitative physical reasoning.
2020.acl-main.706.txt,2020,7 Conclusions and Future Directions,"more importantly, having a model that can meaningfully reason about commonsense qualitative physics could be interpretable and more robust, as they might focus on the parts of physical dynamics that are relevant for generalization to new scenarios."
2020.acl-main.706.txt,2020,7 Conclusions and Future Directions,our results indicate that table-to-text models perform better than language models on generating valid explanations of physical events but there is a lot more room for improvement compared to human annotations.
2020.acl-main.706.txt,2020,7 Conclusions and Future Directions,reinforcement learning (rl) agents may be able to solve physical tasks much more efficiently by leveraging natural language reasoning as opposed to model-free approaches that are often highly sample-inefficient.
2020.acl-main.706.txt,2020,7 Conclusions and Future Directions,"such systems are widely applicable to self-driving cars or tasks that involve human-ai interactions, such as robots performing everyday human tasks like making coffee or even collaboratively helping with rescue operations."
2020.acl-main.706.txt,2020,7 Conclusions and Future Directions,"to train models that can describe physical tasks, we collected open-ended natural language text descriptions of initial states and pivotal physical events in a 2d simulation from human annotators."
2020.acl-main.706.txt,2020,7 Conclusions and Future Directions,we hope that the dataset we collected will facilitate research in using natural language for physical reasoning.
2020.acl-main.706.txt,2020,7 Conclusions and Future Directions,we then trained a model to identify these pivotal events and then fine-tuned on pre-trained table-to-text generation and language models without using the image representations of the actual simulation frames.
2020.acl-main.707.txt,2020,5 Conclusion,"in future work, we plan to add paraphrase generation to generate diverse simple sentences."
2020.acl-main.707.txt,2020,5 Conclusion,our approach works in an unsupervised manner that does not require a parallel corpus for training.
2020.acl-main.707.txt,2020,5 Conclusion,"we proposed an iterative, edit-based approach to text simplification."
2020.acl-main.708.txt,2020,8 Conclusion,"in this paper, we propose logical nlg to study the logical inference problem in generation."
2020.acl-main.708.txt,2020,8 Conclusion,"there are still some unsolved problems for logical nlg, e.g.how to improve the quality of automatic metrics to better help human automatically judge models’ performances."
2020.acl-main.708.txt,2020,8 Conclusion,"to promote the research in this direction, we host a logicnlg challenge2 to help better benchmark the current progress."
2020.acl-main.708.txt,2020,8 Conclusion,we conduct comprehensive experiments to show the existing nlg models are restricted by its monotonic nature and conclude this to be a proper nextstep problem to study nlg systems.
2020.acl-main.709.txt,2020,7 Conclusion,"in this paper, we proposed a novel neural crf model for sentence alignment, which substantially outperformed the existing approaches."
2020.acl-main.709.txt,2020,7 Conclusion,"using the neural crf sentence aligner, we constructed two largest sentencealigned datasets to date (newsela-auto and wiki-auto) for text simplification."
2020.acl-main.709.txt,2020,7 Conclusion,we created two high-quality manually annotated datasets (newsela-manual and wiki-manual) for training and evaluation.
2020.acl-main.709.txt,2020,7 Conclusion,we showed that a bert-initalized transformer trained on our new datasets establishes new state-of-the-art performance for automatic sentence simplification.
2020.acl-main.71.txt,2020,6 Conclusion,"in this paper, by employing the distribution of boltzmann machine as the posterior, we show that correlations can be efficiently introduced into the bits."
2020.acl-main.71.txt,2020,6 Conclusion,significant performance gains are observed in the experiments after introducing correlations into the bits of hash codes.
2020.acl-main.71.txt,2020,6 Conclusion,"then, an asymptoticallyexact lower bound of elbo is further developed to tackle the tricky normalization term in boltzmann machines."
2020.acl-main.71.txt,2020,6 Conclusion,"to facilitate training, we first show that the bm distribution can be augmented as a hierarchical concatenation of a gaussian-like distribution and a bernoulli distribution."
2020.acl-main.710.txt,2020,7 Conclusion and Future Work,"in future work, we plan to investigate how target phrase order affects the generation behavior, and further explore set generation in an order invariant fashion."
2020.acl-main.710.txt,2020,7 Conclusion and Future Work,"our model shows competitive performance on a set of keyphrase generation datasets, including one introduced in this work."
2020.acl-main.710.txt,2020,7 Conclusion and Future Work,"we propose a recurrent generative model that sequentially generates multiple keyphrases, with two extra modules that enhance generation diversity."
2020.acl-main.710.txt,2020,7 Conclusion and Future Work,we propose new metrics to evaluate keyphrase generation.
2020.acl-main.711.txt,2020,8 Conclusion,"a bigger challenge in sarcasm generation and more generally, creative text generation, is to capture the difference between creativity (novel but well-formed material) and nonsense (ill-formed material)."
2020.acl-main.711.txt,2020,8 Conclusion,a human-based evaluation based on four criteria shows that our generation approach significantly outperforms a state-of-the-art model.
2020.acl-main.711.txt,2020,8 Conclusion,"compared with human generated sarcasm, our model shows promise particularly for creativity, humor and sarcasticness, but less for grammaticality."
2020.acl-main.711.txt,2020,8 Conclusion,"language models conflate the two, so developing methods that are nuanced enough to recognize this difference is key to future progress."
2020.acl-main.711.txt,2020,8 Conclusion,the key contribution of our approach is the modeling of commonsense knowledge in a retrieve-and-edit generation framework.
2020.acl-main.711.txt,2020,8 Conclusion,we address the problem of unsupervised sarcasm generation that models several sarcasm factors including reversal of valence and semantic incongruity with the context.
2020.acl-main.712.txt,2020,6 Conclusion,experiments on two benchmarks showed the effectiveness of our framework under both the automatic bleu metric and human judgements.
2020.acl-main.712.txt,2020,6 Conclusion,"in particular, the auxiliary losses for recovering two complementary views (triple relations and linearized graph) of input graphs are introduced, so that our model is trained to retain input structures for better generation."
2020.acl-main.712.txt,2020,6 Conclusion,our training framework is general for different graph types.
2020.acl-main.712.txt,2020,6 Conclusion,we proposed reconstructing input graphs as autoencoding processes to encourage preserving the input semantic information for graph-to-text generation.
2020.acl-main.713.txt,2020,6 Conclusions and Future Work,experiments show that our framework achieves better or comparable performance compared to the state of the art and prove the effectiveness of global features.
2020.acl-main.713.txt,2020,6 Conclusions and Future Work,"in the future, we plan to incorporate more comprehensive event schemas that are automatically induced from multilingual multimedia data and external knowledge to further improve the quality of ie."
2020.acl-main.713.txt,2020,6 Conclusions and Future Work,"our framework is also proved to be language-independent and can be applied to other languages, and it can benefit from multi-lingual training."
2020.acl-main.713.txt,2020,6 Conclusions and Future Work,we also plan to extend our framework to more ie subtasks such as document-level entity coreference resolution and event coreference resolution.
2020.acl-main.713.txt,2020,6 Conclusions and Future Work,we propose a joint end-to-end ie framework that incorporates global features to capture the interdependency between knowledge elements.
2020.acl-main.714.txt,2020,6 Conclusion and Future Work,evaluations on the benchmark dataset and qualitative analysis prove that our model achieves substantial improvement over prior work.
2020.acl-main.714.txt,2020,6 Conclusion and Future Work,"in the future work, it would be interesting to further explore how the model can be adapted to jointly extract role fillers, tackles coreferential mentions and constructing event templates."
2020.acl-main.714.txt,2020,6 Conclusion and Future Work,investigations on how the input context length affects the neural sequence readers’ performance show that context of very long length might be hard for the neural models to capture and results in lower performance.
2020.acl-main.714.txt,2020,6 Conclusion and Future Work,we have demonstrated that document-level event role filler extraction could be successfully tackled with end-to-end neural sequence models.
2020.acl-main.714.txt,2020,6 Conclusion and Future Work,we propose a novel multi-granularity reader to dynamically incorporate paragraph- and sentence-level contextualized representations.
2020.acl-main.715.txt,2020,5 Conclusion,extensive experiments are conducted to demonstrate the benefits of the proposed model.
2020.acl-main.715.txt,2020,5 Conclusion,"finally, we present a novel inductive bias for the deep learning models that exploits the similarity of the representation vectors for the whole input sentences and the shortest dependency paths between the two entity mentions for re."
2020.acl-main.715.txt,2020,5 Conclusion,"first, we represent the dependency trees via the syntax-based importance scores for the words in the input sentences for re."
2020.acl-main.715.txt,2020,5 Conclusion,"in the future, we plan to apply ceon-lstm to other related nlp tasks (e.g., event extraction, semantic role labeling) (nguyen et al., 2016a; nguyen and grishman, 2018a)."
2020.acl-main.715.txt,2020,5 Conclusion,"second, we propose to incorporate the overall sentence representation vectors into the cells of on-lstm, allowing it to compute the model-based importance scores more effectively."
2020.acl-main.715.txt,2020,5 Conclusion,we achieve the state-of-the-art performance on three datasets for re.
2020.acl-main.715.txt,2020,5 Conclusion,we also devise a novel mechanism to project the syntactic information into the computation of on-lstm via promoting the consistency between the syntax-based and model-based importance scores.
2020.acl-main.715.txt,2020,5 Conclusion,"we introduce a new deep learning model for re (i.e., ceon-lstm) that features three major proposals."
2020.acl-main.716.txt,2020,7 Conclusion and Future Work,"additionally, we show the effectiveness of our cs-elmo model by further fine-tuning it for ner and pos tagging."
2020.acl-main.716.txt,2020,7 Conclusion and Future Work,"in our ongoing research, we are investigating the expansion of this technique to language pairs where english may not be involved."
2020.acl-main.716.txt,2020,7 Conclusion and Future Work,"our method enables large pre-trained models, such as elmo, to be adapted to code-switching settings while taking advantage of the pre-trained knowledge."
2020.acl-main.716.txt,2020,7 Conclusion and Future Work,"we establish new state of the art on lid for nepali-english, spanish-english, and hindi-english."
2020.acl-main.716.txt,2020,7 Conclusion and Future Work,we outperform multi-lingual bert and homologous elmo models on spanish-english ner and hindi-enlgish pos tagging.
2020.acl-main.716.txt,2020,7 Conclusion and Future Work,we present a transfer learning method from english to code-switched languages using the lid task.
2020.acl-main.717.txt,2020,5 Conclusion,"experimental results on an english dataset and a chinese dataset reveal that the learned network structures can better identify concepts for entities based on the relations of entities from open domain facts, which will further help building a more complete concept graph."
2020.acl-main.717.txt,2020,5 Conclusion,"in this paper, we investigate the task of learning interpretable relationships between entities, relations and concepts from open domain facts to help enriching and refining concept graphs."
2020.acl-main.717.txt,2020,5 Conclusion,the bayesian network structures are learned from open domain facts as the discovered meaningful dependencies between relations of facts and concepts of entities.
2020.acl-main.718.txt,2020,7 Conclusion,"because of the small amount of existing data for the task, to support training our neural framework we constructed the rams dataset consisting of 9,124 events covering 139 event types."
2020.acl-main.718.txt,2020,7 Conclusion,"our model outperforms strong baselines on rams, and we also illustrated its applicability to a variety of related datasets."
2020.acl-main.718.txt,2020,7 Conclusion,we hope that rams will stimulate further work on multi-sentence argument linking.
2020.acl-main.718.txt,2020,7 Conclusion,we introduced a novel model for document-level argument linking.
2020.acl-main.719.txt,2020,6 Conclusion,"compared with a comprehensive list of baseline models, our model obtains competitive predictive performances."
2020.acl-main.719.txt,2020,6 Conclusion,"essentially, it leverages corpus-level statistics to recall associative contexts and recognizes their relational connections as model rationales."
2020.acl-main.719.txt,2020,6 Conclusion,"in this paper, we propose an interpretable framework to rationalize medical relation prediction based on corpus-level statistics."
2020.acl-main.719.txt,2020,6 Conclusion,"moreover, we demonstrate its interpretability via expert evaluation and case studies."
2020.acl-main.719.txt,2020,6 Conclusion,"our framework is inspired by existing cognitive theories on human memory recall and recognition, and can be easily understood by users as well as provide reasonable explanations to justify its prediction."
2020.acl-main.72.txt,2020,6 Conclusion,"also, our experimental results show the promising performances of our method in concern with real situations of text analytics."
2020.acl-main.72.txt,2020,6 Conclusion,"in response to those issues, we provide the method, the evaluation framework, and the experimental dataset."
2020.acl-main.72.txt,2020,6 Conclusion,our systematic study will pave the way to future research about the effective construction of dictionaries for text analytics.
2020.acl-main.72.txt,2020,6 Conclusion,"to the best of our knowledge, this paper proposes the first formulation of interactive dictionary construction for text analytics, which clarifies the critical issues to resolve."
2020.acl-main.720.txt,2020,6 Conclusions,"finally, regarding the sharing of weights between languages in polyglot models, our key conclusion is that standard training objectives are unable to find an optimum which simultaneously achieves high task performance across all languages."
2020.acl-main.720.txt,2020,6 Conclusions,"in future work, we will explore whether the observed trends hold in much larger polyglot settings, e.g.the wikiann ner corpus (pan et al., 2017b)."
2020.acl-main.720.txt,2020,6 Conclusions,"on the other hand, when the objective is to maximize performance on a single target language it may be possible to improve the proposed fine-tuning approach further using methods such as elastic weight consolidation (kirkpatrick et al., 2016)."
2020.acl-main.720.txt,2020,6 Conclusions,we explore the benefits of polyglot training for ner across a range of models.
2020.acl-main.720.txt,2020,6 Conclusions,"we find that, while not all models can benefit in performance from polyglot training, the parameters learned by those models can be leveraged in a language-specific way to consistently outperform monolingual models."
2020.acl-main.720.txt,2020,6 Conclusions,"we probe properties of polyglot ner models, and find that they are much more efficient than monolingual models in terms of the parameters they require, while generally maintaining a competitive performance across all languages."
2020.acl-main.720.txt,2020,6 Conclusions,"we show that the high amount of parameter sharing in polyglot models partially explains this, and additionally find that language-specific fine-tuning may use a large portion of those shared parameters."
2020.acl-main.720.txt,2020,6 Conclusions,"with this in mind, exploring different training strategies, such as multi-objective optimization, may prove beneficial (sener and koltun, 2018)."
2020.acl-main.721.txt,2020,8 Conclusion,"by representing a webpage as a graph defined by layout relationship between text fields, with text fields associated with both visual and textual features, we attain a 31% improvement over the baseline for new-vertical openie extraction."
2020.acl-main.721.txt,2020,8 Conclusion,future extensions of this work involve a more general pre-training objective allowing for the learned representations to be useful in many tasks as well as distantly or semi-supervised approaches to benefit from more data.
2020.acl-main.721.txt,2020,8 Conclusion,"moreover, this approach enables openie extraction from entirely new subject verticals where no prior knowledge is available."
2020.acl-main.721.txt,2020,8 Conclusion,we have introduced a zero-shot method for learning a model for relation extraction from semistructured documents that generalizes beyond a single document template.
2020.acl-main.722.txt,2020,6 Conclusion,acknowledgements shruti rijhwani is supported by a bloomberg data science ph.d. fellowship.
2020.acl-main.722.txt,2020,6 Conclusion,possible future directions include using more sophisticated feature design and combinations of candidate retrieval methods.
2020.acl-main.722.txt,2020,6 Conclusion,shuyan zhou is supported by the darpa information innovation office (i2o) low resource languages for emergent incidents (lorelei) program under contract no.hr0011-15-c0114.
2020.acl-main.722.txt,2020,6 Conclusion,we also thank samridhi choudhary for help with the model implementation and deepak gopinath for feedback on the paper.
2020.acl-main.722.txt,2020,6 Conclusion,we present a method to create features for low-resource ner and show its effectiveness on four low-resource languages.
2020.acl-main.723.txt,2020,7 Conclusions and Future Work,"in our experiment, a “relevant item” is a person classified by experts as being at risk of attempting suicide in the near future."
2020.acl-main.723.txt,2020,7 Conclusions and Future Work,"like tbg, the htbg score is interpretable as a lower bound on the expected number of relevant items found in a ranking, given a time budget."
2020.acl-main.723.txt,2020,7 Conclusions and Future Work,"measured at an expected reading time budget of about half a day (4hr20min, half-life 3hrs), our joint ranking approach achieved htbg of 12.49 compared with 11.70 for a plausible baseline from prior art: using logistic regression to rank individuals, and then looking at a individual’s posts in backward chronological order."
2020.acl-main.723.txt,2020,7 Conclusions and Future Work,that increase is just a bit short of identifying one more person in need of immediate help in the experiment’s population of 242 individuals.
2020.acl-main.723.txt,2020,7 Conclusions and Future Work,"there are certainly limitations in our study and miles to go before validating our approach in the real world, but our framework should make it easy to integrate and explore other individual rankers, document rankers and explanation mechanisms, and to actually build user interfaces like the schematic in figure 1."
2020.acl-main.723.txt,2020,7 Conclusions and Future Work,"we introduced htbg, a new evaluation measure, as a step toward moving beyond risk classification to a paradigm in which prioritization is the focus, and where time matters."
2020.acl-main.724.txt,2020,6 Conclusion,"as future work, we intend to apply cluhtm in other representative applications on the web, such as hierarchical classification by devising a supervised version of cluhtm."
2020.acl-main.724.txt,2020,6 Conclusion,"cluhtm excelled in terms of effectiveness, being around two times more effective than the strongest state-of-the-art baselines, considering all tested datasets and evaluation metrics."
2020.acl-main.724.txt,2020,6 Conclusion,our new method exploits a more elaborate (global) semantic data representation – cluwords – as well as an original application of a stability measure to define the “shape” of the hierarchy.
2020.acl-main.724.txt,2020,6 Conclusion,the overall gains over some of these strongest baselines are higher than 500% in some datasets.
2020.acl-main.724.txt,2020,6 Conclusion,"we advanced the state-of-the-art in hierarchical topic modeling (htm) by designing, implementing and evaluation a novel unsupervised non-probabilistic method – cluhtm."
2020.acl-main.724.txt,2020,6 Conclusion,we also intend to incorporate some type of attention mechanism into our methods to better understand which cluwords are more important to define certain topics.
2020.acl-main.724.txt,2020,6 Conclusion,"we also showed that cluhtm results are consistent across most datasets, independently of the data characteristics and idiosyncrasies."
2020.acl-main.725.txt,2020,6 Conclusions,another interesting direction is to generate a class name hierarchy via language model probing.
2020.acl-main.725.txt,2020,6 Conclusions,extensive experiments on the wiki and apr datasets demonstrate the effectiveness of our framework on both class name prediction and entity set expansion.
2020.acl-main.725.txt,2020,6 Conclusions,"for example, we may expand the set {“machine translation”, “information extraction”, “syntactic parsing”} to acquire more nlp task concepts."
2020.acl-main.725.txt,2020,6 Conclusions,"in the future, we plan to expand the method scope from expanding concrete entity sets to more abstract concept sets."
2020.acl-main.725.txt,2020,6 Conclusions,"in this paper, we propose a new entity set expansion framework that can use a pre-trained lm to generate candidate class names for the seed set, rank them according to the provided text corpus, and guide the entity selection process with the selected class names."
2020.acl-main.726.txt,2020,5 Conclusion,"in our future work, we will consider extending it to graph-based methods such as gcn for graph data, and to generation-based methods such as gan for adversarial learning."
2020.acl-main.726.txt,2020,5 Conclusion,"in this paper, we proposed a novel feature purification network (fp-net) to improve the representation for text classification."
2020.acl-main.726.txt,2020,5 Conclusion,"our current method is designed only for traditional text classification methods such as lstm, cnn, and transformer."
2020.acl-main.726.txt,2020,5 Conclusion,the method is based on feature projection.
2020.acl-main.726.txt,2020,5 Conclusion,"the proposed model uses two sub-networks, one for identifying common features that are not discriminative for classification, and the other for feature projection that projects the traditional features to the orthogonal direction of the common features."
2020.acl-main.726.txt,2020,5 Conclusion,"through a large number of comparative experiments, we showed the effectiveness of the proposed feature projection method."
2020.acl-main.726.txt,2020,5 Conclusion,"to the best of our knowledge, this is the first method that uses feature projection to improve text classification."
2020.acl-main.727.txt,2020,7 Conclusion,future visual grounding methods should be tested with a more comprehensive experimental setup and datasets for proper evaluation.
2020.acl-main.727.txt,2020,7 Conclusion,"here, we showed that existing visual grounding based bias mitigation methods for vqa are not working as intended."
2020.acl-main.727.txt,2020,7 Conclusion,we found that the accuracy improvements stem from a regularization effect rather than proper visual grounding.
2020.acl-main.727.txt,2020,7 Conclusion,"we proposed a simple regularization scheme which, despite not requiring additional annotations, rivals state-of-the-art accuracy."
2020.acl-main.728.txt,2020,8 Conclusion and Future Work,"however, we also show the limitations of this shared task in terms of dialog phenomena and evaluation metrics."
2020.acl-main.728.txt,2020,8 Conclusion and Future Work,"in sum, this paper shows that we can get sota performance on the visdial task by using transformer-based models with guided-attention (yu et al., 2019b), and by encoding dialog history and finetuning we can improve results even more."
2020.acl-main.728.txt,2020,8 Conclusion and Future Work,"of course, we expect pre-trained visual bert models to show even more improvements on this task, e.g."
2020.acl-main.728.txt,2020,8 Conclusion and Future Work,"vilbert (lu et al., 2019), lxmert (tan and bansal, 2019), uniter (chen et al., 2019) etc."
2020.acl-main.728.txt,2020,8 Conclusion and Future Work,"we, thus, argue that progress needs to be carefully measured by posing the right task in terms of dataset and evaluation procedure."
2020.acl-main.729.txt,2020,7 Conclusion,"for example, action span extraction is related to both semantic role labeling (he et al., 2018) and extraction of multiple facts from text (jiang et al., 2019) and could benefit from innovations in span identification and multitask learning."
2020.acl-main.729.txt,2020,7 Conclusion,"lastly, our work provides a technical foundation for investigating user experiences in language-based human computer interaction."
2020.acl-main.729.txt,2020,7 Conclusion,our decomposition of the problem means that progress on either can improve full task performance.
2020.acl-main.729.txt,2020,7 Conclusion,our work provides an important first step on the challenging problem of grounding natural language instructions to mobile ui actions.
2020.acl-main.729.txt,2020,7 Conclusion,reinforcement learning that has been applied in previous grounding work may help improve out-of-sample prediction for grounding in uis and improve direct grounding from hidden state representations.
2020.acl-main.73.txt,2020,5 Conclusion,"by incorporating our model instead of flat topic models, they can provide multiple information with desirable granularity."
2020.acl-main.73.txt,2020,5 Conclusion,"experimental results demonstrated that the tsntm achieves competitive performance when inducing latent topics and their tree structures, as compared to a prior tree-structured topic model (blei et al., 2010)."
2020.acl-main.73.txt,2020,5 Conclusion,"this allows the tree-structured topic model to be incorporated with recent neural models for downstream tasks, such as aspect-based sentiment analysis (esmaeili et al., 2019) and abstractive summarization (wang et al., 2019)."
2020.acl-main.73.txt,2020,5 Conclusion,"we proposed a novel tree-structured topic model, the tsntm, which parameterizes the topic distribution over an infinite tree by a drnn."
2020.acl-main.73.txt,2020,5 Conclusion,"with the help of aevb, the tsntm can be trained approximately 15 times faster and scales to larger datasets than the ncrp-based model."
2020.acl-main.730.txt,2020,6 Conclusion,"comprehensive experiments show that temporal and spatial predictions help improve qa performance, as well as providing explainable results."
2020.acl-main.730.txt,2020,6 Conclusion,"this task requires systems to jointly localize relevant moments, detect referred objects/people, and answer questions."
2020.acl-main.730.txt,2020,6 Conclusion,"though our stage achieves state-of-the-art performance, there is still a large gap compared with human performance, leaving space for further improvement."
2020.acl-main.730.txt,2020,6 Conclusion,we collected the tvqa+ dataset and proposed the spatio-temporal video qa task.
2020.acl-main.730.txt,2020,6 Conclusion,"we further introduced stage, an end-to-end trainable framework to jointly perform all three tasks."
2020.acl-main.731.txt,2020,6 Conclusion,"besides, it synthesizes image-pivoted pseudo sentences in two languages and pairs them to translate by reconstruction without parallel corpora."
2020.acl-main.731.txt,2020,6 Conclusion,"beyond features, we use visual content to improve the cross-lingual alignments in the shared latent space."
2020.acl-main.731.txt,2020,6 Conclusion,"precisely, our model utilizes the visual space as the approximate pivot for aligning the multilingual multimodal embedding space."
2020.acl-main.731.txt,2020,6 Conclusion,the experiments on multi30k show that the proposed model generalizes well and yields new state-of-the-art performance.
2020.acl-main.731.txt,2020,6 Conclusion,we have presented a novel approach: pseudo visual pivoting for unsupervised multimodal mt.
2020.acl-main.732.txt,2020,7 Discussion & Conclusion,"although we apply our joint model on arabic, this model provides a framework for other languages that include diacritics whenever resources become available."
2020.acl-main.732.txt,2020,7 Discussion & Conclusion,"although we observed improvements in terms of generalizing beyond observed data when using the proposed linguistic features, the oov performance is still an issue for diacritic restoration."
2020.acl-main.732.txt,2020,7 Discussion & Conclusion,including semantic information through pretrained word embeddings within the diacritic restoration model also helped boosting the diacritic restoration performance.
2020.acl-main.732.txt,2020,7 Discussion & Conclusion,our results shows statistically significant improvements across all evaluation metrics.
2020.acl-main.732.txt,2020,7 Discussion & Conclusion,this shows the importance of considering additional linguistic information at morphological and/or sentence levels.
2020.acl-main.732.txt,2020,7 Discussion & Conclusion,we present a diacritic restoration joint model that considers the output distributions for different related tasks to improve the performance of diacritic restoration.
2020.acl-main.733.txt,2020,7 Conclusion,"thus, ssa might be replaced with a less costly architecture while our model might be improved by conditioning on semantics and jointly decoding from a variable number of sources."
2020.acl-main.733.txt,2020,7 Conclusion,"we demonstrated that typologically distinct morphological systems require unique treatment and benefit from our ssa, that learns its strategy from data."
2020.acl-main.733.txt,2020,7 Conclusion,"we found that inducing this strategy is not as challenging as previously suggested (finkel and stump, 2007)."
2020.acl-main.733.txt,2020,7 Conclusion,"we presented frugal paradigm completion, which reduces the manual labor required to expand a morphological lexicon by 16-63% over competitive approaches across 7 languages."
2020.acl-main.734.txt,2020,6 Conclusion,"experimental results on various widely used benchmark datasets illustrate the effectiveness of wmseg, where state-of-the-art performance is achieved on all datasets."
2020.acl-main.734.txt,2020,6 Conclusion,further experiments and analyses also demonstrate the robustness of wm-seg in the cross-domain scenario as well as when using different lexicons and wordhood measures.
2020.acl-main.734.txt,2020,6 Conclusion,"in this paper, we propose wmseg, a neural framework for cws using wordhood memory networks, which maps n-grams and their wordhood information to keys and values in it and appropriately models the values according to the importance of keys in a specific context."
2020.acl-main.734.txt,2020,6 Conclusion,"the framework follows the sequence labeling paradigm, and the encoders and decoders in it can be implemented by various prevailing models."
2020.acl-main.734.txt,2020,6 Conclusion,"to the best of our knowledge, this is the first work using key-value memory networks and utilizing wordhood information for neural models in cws."
2020.acl-main.735.txt,2020,6 Conclusion,"experimental results on five benchmark datasets illustrate the validity and effectiveness of our model, where the two-way attentions can be integrated with different encoders and provide consistent improvements over baseline taggers."
2020.acl-main.735.txt,2020,6 Conclusion,"for future work, we plan to apply the same methodology to other nlp tasks."
2020.acl-main.735.txt,2020,6 Conclusion,"in this paper, we propose neural approach with a two-way attention mechanism to incorporate autoanalyzed knowledge for joint cws and pos tagging, following a character-based sequence labeling paradigm."
2020.acl-main.735.txt,2020,6 Conclusion,our model achieves state-of-the-art performance on all the datasets.
2020.acl-main.735.txt,2020,6 Conclusion,"our proposed attention module learns and weights context features and their corresponding knowledge instances in two separate ways, and use the combined attentions from the two ways to enhance the joint tagging."
2020.acl-main.735.txt,2020,6 Conclusion,"overall, this work presents an elegant way to use autoanalyzed knowledge and enhance neural models with existing nlp tools."
2020.acl-main.736.txt,2020,5 Conclusions and Future Work,"our model achieves a significant improvement over several baselines for arabic, and matches the baseline for msa without having to use an expensive morphological analyzer."
2020.acl-main.736.txt,2020,5 Conclusions and Future Work,"the results highlight the benefits of joint modeling, where diacritization seems to have benefitted the most."
2020.acl-main.736.txt,2020,5 Conclusions and Future Work,"we observe, however, that further research is needed to enhance the overall consistency of the predicted features, without relying on external morphological analyzers."
2020.acl-main.736.txt,2020,5 Conclusions and Future Work,we presented a joint modeling approach for the lexicalized and non-lexicalized features in morphologically rich and semitic languages.
2020.acl-main.737.txt,2020,7 Conclusion,"another research avenue that could be explored is modeling specific user preferences: since each user likely favors a certain set of character substitutions, allowing user-specific parameters could improve decoding and be useful for authorship attribution."
2020.acl-main.737.txt,2020,7 Conclusion,the informative priors used in our experiments are constructed using sets of character mappings compiled for other purposes but using the same underlying principle (phonetic keyboard layouts and the unicode confusable symbol list).
2020.acl-main.737.txt,2020,7 Conclusion,this paper tackles the problem of decoding non-standardized informal romanization used in social media into the original orthography without parallel text.
2020.acl-main.737.txt,2020,7 Conclusion,"we train a wfst noisy-channel model to decode romanized egyptian arabic and russian to their original scripts with the stepwise em algorithm combined with curriculum learning and demonstrate that while the unsupervised model by itself performs poorly, introducing an informative prior that encodes the notion of phonetic or visual character similarity brings its performance substantially closer to that of the supervised model."
2020.acl-main.737.txt,2020,7 Conclusion,"while these mappings provide a convenient way to avoid formalizing the complex notions of the phonetic and visual similarity, they are restrictive and do not capture all the diverse aspects of similarity that idiosyncratic romanization uses, so designing more suitable priors via operationalizing the concept of character similarity could be a promising direction for future work."
2020.acl-main.738.txt,2020,7 Discussion and Conclusion,"by adding a simple question to the annotation interface, we obtained significantly better models per human-annotation hour."
2020.acl-main.738.txt,2020,7 Discussion and Conclusion,"in addition, we introduced a clustering technique which further optimizes sample selection during the annotation process."
2020.acl-main.738.txt,2020,7 Discussion and Conclusion,"more broadly, our work suggests that improvements in annotation interfaces can elicit responses which are more efficient in terms of the obtained performance versus the invested annotation time."
2020.acl-main.738.txt,2020,7 Discussion and Conclusion,"we presented discrete annotation, an attractive alternative to pairwise annotation in active learning of coreference resolution in low-resource domains."
2020.acl-main.739.txt,2020,6 Conclusions,"additionally, as just exemplified, many possessions can be extracted even if prototypical possession verbs (e.g., have, buy, acquire) are missing."
2020.acl-main.739.txt,2020,6 Conclusions,"beyond word embeddings, the lstm benefits from additional embeddings indicating the tokens that are the possessor and possessee."
2020.acl-main.739.txt,2020,6 Conclusions,"from a theoretical perspective, they include having control over something (e.g.flying a plane, impounding a vehicle, eating ice cream) thus most objects are actually possessees of one or more possessors."
2020.acl-main.739.txt,2020,6 Conclusions,"in this paper, we tackle both problems and determine possession durations and co-possessions."
2020.acl-main.739.txt,2020,6 Conclusions,"information extracted from the image, however, is not helpful."
2020.acl-main.739.txt,2020,6 Conclusions,possessions are ubiquitous yet understudied from a computational perspective.
2020.acl-main.739.txt,2020,6 Conclusions,"regarding co-possessions, we obtain slightly better agreement (0.65 cohen’s κ)."
2020.acl-main.739.txt,2020,6 Conclusions,"regarding durations, we collect lower and upper bounds in order to derive sound duration intervals."
2020.acl-main.739.txt,2020,6 Conclusions,standard relation extraction does not provide information about for how long relations hold true or whether relations are one-to-one or one-to-many.
2020.acl-main.739.txt,2020,6 Conclusions,the resulting three intervals obtain substantial agreement (0.63 cohen’s κ).
2020.acl-main.739.txt,2020,6 Conclusions,we have also presented baseline models and a neural network architecture to solve both tasks.
2020.acl-main.739.txt,2020,6 Conclusions,we have presented new annotations on top of existing corpora.
2020.acl-main.739.txt,2020,6 Conclusions,"while the work presented here targets possession relations, we believe that a similar approach could be used to to determine for how long any semantic relation holds true."
2020.acl-main.74.txt,2020,5 Summary and Conclusions,"the first one is based on an ir passage retrieval approach, and the others two are independent bert models that are fine-tuned to predict query-to-answer and query-to-question matching."
2020.acl-main.74.txt,2020,5 Summary and Conclusions,the method is based on an initial retrieval of faq candidates followed by three rerankers.
2020.acl-main.74.txt,2020,5 Summary and Conclusions,we experimentally showed that our unsupervised method is on par and sometimes even outperforms existing supervised methods.
2020.acl-main.74.txt,2020,5 Summary and Conclusions,we presented a fully unsupervised method for faq retrieval.
2020.acl-main.74.txt,2020,5 Summary and Conclusions,we showed that we can overcome the “unsupervised gap” by generating high-quality question paraphrases and use them to fine-tune the query-to-question bert model.
2020.acl-main.740.txt,2020,7 Conclusion,"our experiments reveal that even a model of hundreds of millions of parameters struggles to encode the complexity of a single textual domain, let alone all of language."
2020.acl-main.740.txt,2020,7 Conclusion,our findings suggest it may be valuable to complement work on ever-larger lms with parallel efforts to identify and use domain- and taskrelevant corpora to specialize models.
2020.acl-main.740.txt,2020,7 Conclusion,"our work points to numerous future directions, such as better data selection for tapt, efficient adaptation large pretrained language models to distant domains, and building reusable language models after adaptation."
2020.acl-main.740.txt,2020,7 Conclusion,"we investigate several variations for adapting pretrained lms to domains and tasks within those domains, summarized in table 10."
2020.acl-main.740.txt,2020,7 Conclusion,we show that pretraining the model towards a specific task or small corpus can provide significant benefits.
2020.acl-main.740.txt,2020,7 Conclusion,"while our results demonstrate how these approaches can improve roberta, a powerful lm, the approaches we studied are general enough to be applied to any pretrained lm."
2020.acl-main.741.txt,2020,5 Conclusion,in this work we explored how to apply mutual information (mi) as a semantic similarity measure for continuous dense word embeddings.
2020.acl-main.741.txt,2020,5 Conclusion,we have summarised the vast literature on estimating mi for continuous random variables from the sample and singled out a simple and elegant ksg estimator which is based on elementary nearest-neighbour statistics.
2020.acl-main.741.txt,2020,5 Conclusion,we showed empirically that this estimator and mutual information in general can be an excellent candidate for a similarity measure between dense word embeddings.
2020.acl-main.742.txt,2020,7 Discussion,"future work could also make the stronger assumption that a small number of in-domain training examples are available, and train and evaluate in a few-shot setting."
2020.acl-main.742.txt,2020,7 Discussion,our results and analysis demonstrate the need for developing more holistic evaluation of cross-database semantic parsing using a more diverse set of language and databases.
2020.acl-main.742.txt,2020,7 Discussion,"several significant generalization challenges remaining, including improving commonsense and in-domain reasoning and table schema understanding capabilities."
2020.acl-main.742.txt,2020,7 Discussion,some examples in our filtered evaluation set still require reasoning about dataset conventions that are difficult to acquire without indomain training examples.
2020.acl-main.742.txt,2020,7 Discussion,"using a model that performs well on evaluation data designed for xsp, we are able to move towards addressing some of the generalization challenges on these additional evaluation sets without any indomain training data."
2020.acl-main.742.txt,2020,7 Discussion,"we identify several new generalization challenges that arise when evaluating in our proposed setup, including identifying entities, mapping entities and domainspecific phrases to a database schema, and generalizing to more complex database schemas."
2020.acl-main.742.txt,2020,7 Discussion,"we study the task of cross-database semantic parsing (xsp), where a system that maps natural language utterances to executable sql queries is evaluated on databases unseen at training time."
2020.acl-main.742.txt,2020,7 Discussion,"while this task has been studied through datasets developed specifically for xsp, we propose a more holistic evaluation for xsp, where we also evaluate on datasets originally studied in a setting where in-domain training data is available."
2020.acl-main.743.txt,2020,6 Conclusions,"a quantitative analysis shows that this model is robust across most role labels (table 3), sentence lengths, and verb classes (table 4)."
2020.acl-main.743.txt,2020,6 Conclusions,"experimental results show that incorporating scope of negation information yields better results, despite the fact that we train the scope detector with data in a different domain (short stories vs. news)."
2020.acl-main.743.txt,2020,6 Conclusions,"first, including scope information solves many syntactic errors but introduces semantic errors (recall that scope information is beneficial from a quantitative point of view)."
2020.acl-main.743.txt,2020,6 Conclusions,"in addition to state-of-the-art results, we have presented a detailed qualitative analysis."
2020.acl-main.743.txt,2020,6 Conclusions,"in this paper, we have presented a neural architecture to predict the focus of negation."
2020.acl-main.743.txt,2020,6 Conclusions,"negation is generally understood to carry positive meaning, or in other words, to suggest affirmative alternatives."
2020.acl-main.743.txt,2020,6 Conclusions,our best model (nn + scope) obtains the best focus prediction results to date.
2020.acl-main.743.txt,2020,6 Conclusions,"predicting the focus of negation (i.e., pinpointing the usually few tokens that are actually negated) is key to revealing affirmative alternatives."
2020.acl-main.743.txt,2020,6 Conclusions,"second, the lower results after including context, at least with the current architecture, are largely due to additional semantic errors via distractors in the previous and next sentences."
2020.acl-main.743.txt,2020,6 Conclusions,"the model obtains worse results, however, when the role that is the focus is only one token, or the negated verb has more than 5 roles (table 4)."
2020.acl-main.743.txt,2020,6 Conclusions,these results suggest that scope of negation transfers across domains.
2020.acl-main.743.txt,2020,6 Conclusions,"we discover three main error categories (syntactic, semantic, and other) and 8 error types after manual analysis of the predictions made by the four models with all test instances."
2020.acl-main.743.txt,2020,6 Conclusions,we draw two main insights from the qualitative analysis.
2020.acl-main.743.txt,2020,6 Conclusions,"we work with pb-foc, a corpus of verbal negations (i.e., when a negation cue grammatically modifies a verb) in which one semantic role is annotated as focus."
2020.acl-main.744.txt,2020,5 Discussion & Conclusion,constraints have long been a cornerstone in the srl models.
2020.acl-main.744.txt,2020,5 Discussion & Conclusion,"final words in this work, we have presented a framework that seeks to predict structurally consistent outputs without extensive model redesign, or any expensive decoding at prediction time."
2020.acl-main.744.txt,2020,5 Discussion & Conclusion,"for example, we could revisit the analysis of yi et al.(2007), who showed that the propbank a2 label takes on multiple meanings, but by mapping them to verbnet, they can be disambiguated."
2020.acl-main.744.txt,2020,5 Discussion & Conclusion,"in this paper, we show that even in the world of neural networks with contextual embeddings, there is still room for systematically introducing knowledge in the form of constraints, without sacrificing the benefits of end-to-end learning."
2020.acl-main.744.txt,2020,5 Discussion & Conclusion,"introducing extra regularization terms to a downstream task have been shown to be beneficial in terms of both output structure consistency and prediction accuracy (e.g., minervini and riedel, 2018; hsu et al., 2018; mehta et al., 2018; du et al., 2019; li et al., 2019)."
2020.acl-main.744.txt,2020,5 Discussion & Conclusion,kimmig et al.(2012) used the łukasiewicz t-norm for probabilistic soft logic.
2020.acl-main.744.txt,2020,5 Discussion & Conclusion,"learning with constraints has also been widely adopted in semisupervised srl (e.g., fu¨rstenau and lapata, 2012)."
2020.acl-main.744.txt,2020,5 Discussion & Conclusion,li and srikumar (2019) augment the neural network architecture itself using such soft logic.
2020.acl-main.744.txt,2020,5 Discussion & Conclusion,our experiments on the semantic role labeling task show that such an approach can be especially helpful in scenarios where we do not have the luxury of massive annotated datasets.
2020.acl-main.744.txt,2020,5 Discussion & Conclusion,our work is in the same spirit of training models that attempts to maintain output consistency.
2020.acl-main.744.txt,2020,5 Discussion & Conclusion,"punyakanok et al., 2004, 2008; surdeanu et al., 2007) modeled inference for propbank srl using integer linear programming."
2020.acl-main.744.txt,2020,5 Discussion & Conclusion,riedel and meza-ruiz (2008) used markov logic networks to learn and predict semantic roles with declarative constraints.
2020.acl-main.744.txt,2020,5 Discussion & Conclusion,semantic role labeling & constraints the srl task is inherently knowledge rich; the outputs are defined in terms of an external ontology of frames.
2020.acl-main.744.txt,2020,5 Discussion & Conclusion,several early linear models for srl (e.g.
2020.acl-main.744.txt,2020,5 Discussion & Conclusion,structured losses chang et al.(2012) and ganchev et al.(2010) developed models for structured learning with declarative constraints.
2020.acl-main.744.txt,2020,5 Discussion & Conclusion,such mappings naturally define constraints that link semantic ontologies.
2020.acl-main.744.txt,2020,5 Discussion & Conclusion,"the work of (ta¨ckstro¨m et al., 2015) showed that certain srl constraints admit efficient decoding, leading to a neural model that used this framework (fitzgerald et al., 2015)."
2020.acl-main.744.txt,2020,5 Discussion & Conclusion,"the work presented here can be generalized to several different flavors of the task, and indeed, constraints could be used to model the interplay between them."
2020.acl-main.744.txt,2020,5 Discussion & Conclusion,there are some recent works on the design of models and loss functions by relaxing boolean formulas.
2020.acl-main.744.txt,2020,5 Discussion & Conclusion,"with the increasing influence of neural networks in nlp, however, the role of declarative constraints seem to have decreased in favor of fully end-to-end training (e.g., he et al., 2017; strubell et al., 2018, and others)."
2020.acl-main.744.txt,2020,5 Discussion & Conclusion,xu et al.(2018) present a general framework for loss design that does not rely on soft logic.
2020.acl-main.745.txt,2020,7 Conclusion and Future Work,"finally, to extend tabert to cross-lingual settings with utterances in foreign languages and structured schemas defined in english, we plan to apply more advanced semantic similarity metrics for creating content snapshots."
2020.acl-main.745.txt,2020,7 Conclusion and Future Work,"first, we plan to evaluate tabert on other related tasks involving joint reasoning over textual and tabular data (e.g., table retrieval and table-to-text generation)."
2020.acl-main.745.txt,2020,7 Conclusion and Future Work,"second, following the discussions in § 5, we will explore other table linearization strategies with transformers, improving the quality of pretraining corpora, as well as novel unsupervised objectives."
2020.acl-main.745.txt,2020,7 Conclusion and Future Work,this work also opens up several avenues for future work.
2020.acl-main.745.txt,2020,7 Conclusion and Future Work,"we present tabert, a pretrained encoder for joint understanding of textual and tabular data."
2020.acl-main.745.txt,2020,7 Conclusion and Future Work,we show that semantic parsers using tabert as a general-purpose feature representation layer achieved strong results on two benchmarks.
2020.acl-main.746.txt,2020,8 Conclusion,"the scalar valued, multi-attribute nature of uds provides for a distinct structured prediction problem as compared to other existing representations."
2020.acl-main.746.txt,2020,8 Conclusion,we envision future efforts exploring the interactions between improving the underlying graphstructure prediction and ever-better correlations to human judgements on individual properties.
2020.acl-main.746.txt,2020,8 Conclusion,"we have demonstrated how a transductive parsing paradigm that has achieved state-of-the-art results on other representations can be adapted to uds1.0 structures and attributes, and have provided procedures for analysis, with the fine-grained nature of uds allowing for investigating novel correlations and aspects of meaning."
2020.acl-main.746.txt,2020,8 Conclusion,"while uds structures and various attribute types have been modeled separately (vashishtha et al., 2019; govindarajan et al., 2019; white et al., 2016; rudinger et al., 2018a,b; zhang et al., 2018), this work represents the first time all of these attributes and structures have been modeled jointly, and establishes a baseline for future efforts on uds1.0."
2020.acl-main.747.txt,2020,6 Conclusion,"in this work, we introduced xlm-r, our new state of the art multilingual masked language model trained on 2.5 tb of newly created clean commoncrawl data in 100 languages."
2020.acl-main.747.txt,2020,6 Conclusion,"we also expose the surprising effectiveness of multilingual models over monolingual models, and show strong improvements on low-resource languages."
2020.acl-main.747.txt,2020,6 Conclusion,"we exposed the limitations of multilingual mlms, in particular by uncovering the high-resource versus low-resource trade-off, the curse of multilinguality and the importance of key hyperparameters."
2020.acl-main.747.txt,2020,6 Conclusion,"we show that it provides strong gains over previous multilingual models like mbert and xlm on classification, sequence labeling and question answering."
2020.acl-main.748.txt,2020,8 Conclusion,"because the candidate ranker makes predictions over pairs of concept mentions and candidate concepts, it is able to predict concepts never seen during training."
2020.acl-main.748.txt,2020,8 Conclusion,our proposed semantic type regularizer allows the ranker to incorporate semantic type information into its predictions without requiring semantic types at prediction time.
2020.acl-main.748.txt,2020,8 Conclusion,this generate-and-rank framework achieves state-of-the-art performance on multiple concept normalization datasets.
2020.acl-main.748.txt,2020,8 Conclusion,we propose a concept normalization framework consisting of a candidate generator and a list-wise classifier based on bert.
2020.acl-main.749.txt,2020,6 Conclusions,"additionally, we advocate for careful investigation into partial type paths: their interpretation relies on how the data is annotated, and in turn, influences typing performance."
2020.acl-main.749.txt,2020,6 Conclusions,"our approach achieved state-of-the-art performance across various datasets, and made substantial improvement (4–8%) upon strict accuracy."
2020.acl-main.749.txt,2020,6 Conclusions,"we proposed (i) a novel multi-level learning to rank loss function that operates on a type tree, and (ii) an accompanying coarse-to-fine decoder to fully embrace the ontological structure of the types for hierarchical entity typing."
2020.acl-main.75.txt,2020,5 Conclusions,"in this paper, we propose a novel approach, pcpr, for pun detection and location by leveraging a contextualized word encoder and modeling phonemes as word pronunciations."
2020.acl-main.75.txt,2020,5 Conclusions,"moreover, we would love to apply the proposed model to other problems, such as general humor recognition, irony discovery, and sarcasm detection, as the future work."
2020.acl-main.750.txt,2020,8 Conclusions,"building on past research, we proposed a new neural architecture that achieves substantial improvements of up to 5 f1 points when compared to standard methods."
2020.acl-main.750.txt,2020,8 Conclusions,existing ner approaches are widely faced with limited scalability when applied to data that spans multiple domains.
2020.acl-main.750.txt,2020,8 Conclusions,future work will focus on domain adaptation at the embedding layer.
2020.acl-main.750.txt,2020,8 Conclusions,robustness of nlp models is essential to their wider adoption and usability.
2020.acl-main.750.txt,2020,8 Conclusions,"these include learning from data in multiple domains and testing on all domains, when the domain label of the test point is unknown and when this does not belong to a domain seen in training."
2020.acl-main.750.txt,2020,8 Conclusions,this paper introduced three experimental setups that provide a framework for evaluating the robustness of ner models.
2020.acl-main.751.txt,2020,6 Conclusions and Future Work,"interesting future work includes applying our techniques to different taxonomies (e.g., biomedical) and training a model for different attributes."
2020.acl-main.751.txt,2020,6 Conclusions and Future Work,"our proposed model, txtract, is both efficient and effective: it leverages the taxonomy into a deep neural network to improve extraction quality and can extract attribute values on all categories in parallel."
2020.acl-main.751.txt,2020,6 Conclusions and Future Work,txtract significantly outperforms state-of-the-art approaches and strong baselines under a taxonomy with thousands of product categories.
2020.acl-main.751.txt,2020,6 Conclusions and Future Work,we present a novel method for large-scale attribute value extraction for products from a taxonomy with thousands of product categories.
2020.acl-main.752.txt,2020,5 Conclusion,"we crowdsourced triggers on two mainstream datasets and will release them to the community, and proposed a novel framework tmn which can generalize to unseen sentences easily for tagging named entities."
2020.acl-main.752.txt,2020,5 Conclusion,we introduce “entity trigger” as a complementary annotation.
2020.acl-main.753.txt,2020,8 Conclusion,"as a result, our model has outperformed previous variational nmt models in terms of translation quality, and is comparable to non-latent transformer on standard wmt ro↔en and de↔en datasets."
2020.acl-main.753.txt,2020,8 Conclusion,"furthermore, the proposed method has improved robustness in dealing with uncertainty in data, including exploiting source-side monolingual data as well as training with noisy parallel data."
2020.acl-main.753.txt,2020,8 Conclusion,"instead, by providing a new analysis of the conditional vae objective to improve it in a principled way and incorporating an auxiliary decoding objective, we measurably prevented posterior collapse."
2020.acl-main.753.txt,2020,8 Conclusion,our approach does not require an annealing schedule or a hamstrung decoder to avoid posterior collapse.
2020.acl-main.753.txt,2020,8 Conclusion,"we have presented a conditional generative model with latent variables whose distribution is learned with variation inference, then evaluated it in machine translation."
2020.acl-main.754.txt,2020,8 Conclusion,"in addition, there are other conceivable multilingual optimization objectives than those we explored in § 6.4."
2020.acl-main.754.txt,2020,8 Conclusion,"in this paper, we propose multidds, an algorithm that learns a language scorer to optimize multilingual data usage to achieve good performance on many different languages."
2020.acl-main.754.txt,2020,8 Conclusion,"multidds not only outperforms prior methods in terms of overall performance on all languages, but also provides a flexible framework to prioritize different multilingual objectives."
2020.acl-main.754.txt,2020,8 Conclusion,"notably, multidds is not limited to nmt, and future work may consider applications to other multilingual tasks."
2020.acl-main.754.txt,2020,8 Conclusion,"we extend and improve over previous work on dds (wang et al., 2019b), with a more efficient algorithmic instantiation tailored for the multilingual training problem and a stable reward to optimize multiple objectives."
2020.acl-main.755.txt,2020,5 Conclusion,"furthermore, we identify a strong correlation between robustness and consistency in these models indicating that consistency can be used to estimate robustness on data sets or domains lacking reference translations."
2020.acl-main.755.txt,2020,5 Conclusion,our robustness metrics reveal a clear trend of subword regularization being much more robust to input perturbations than standard bpe.
2020.acl-main.755.txt,2020,5 Conclusion,these measure robustness as relative degradation in quality as well as consistency which quantifies variation in translation output irrespective of reference translations.
2020.acl-main.755.txt,2020,5 Conclusion,we also tested two popular subword regularization techniques and their effect on overall performance and robustness.
2020.acl-main.755.txt,2020,5 Conclusion,we proposed two additional measures for nmt robustness which can be applied when both original and noisy inputs are available.
2020.acl-main.756.txt,2020,5 Conclusions,"because it is artificial to use synthetic data for training a filter classifier, future work can focus on a better objective that models parallelism more smoothly."
2020.acl-main.756.txt,2020,5 Conclusions,future work also includes extending the method to low-resource languages not covered by multilingual bert.
2020.acl-main.756.txt,2020,5 Conclusions,"in this paper, we address the parallel corpus filtering problem in machine translation."
2020.acl-main.756.txt,2020,5 Conclusions,our method outperforms strong baselines and achieves a new state-of-the-art.
2020.acl-main.756.txt,2020,5 Conclusions,we propose a novel filtering method using pre-trained language models.
2020.acl-main.756.txt,2020,5 Conclusions,we release a large japanese-chinese web crawled parallel corpus for the research purposes.
2020.acl-main.757.txt,2020,4 Conclusions,"based on this observation, we propose a regularization method to guide the learning of context gates with an effective way to generate supervision from training data."
2020.acl-main.757.txt,2020,4 Conclusions,experimental results show the regularized context gates can significantly improve translation performances over different translation tasks even though the context control problem is only slightly relieved.
2020.acl-main.757.txt,2020,4 Conclusions,"in the future, we believe more work on alleviating context control problem has the potential to improve translation performance as quantified in table 3."
2020.acl-main.757.txt,2020,4 Conclusions,this paper transplants context gates from the rnn based nmt to the transformer to control the source and target context for translation.
2020.acl-main.757.txt,2020,4 Conclusions,"we find that context gates only modestly improve the translation quality of the transformer, because learning context gates freely from scratch is more challenging for the transformer with the complicated structure than for rnn."
2020.acl-main.758.txt,2020,5 Conclusions,"in contrast to previous approaches, the multi-perspective nature of our model allows it to capture richer similarities between the two sequences."
2020.acl-main.758.txt,2020,5 Conclusions,"in this paper, we consider the task of semantic code search or retrieval using a code–text similarity model."
2020.acl-main.758.txt,2020,5 Conclusions,"we propose mp-cat, a novel multi-perspective deep neural network framework for this task."
2020.acl-main.759.txt,2020,6 Conclusion and Future Work,"although tcattn outperforms all baselines and requires no human effort on tc extraction, annotation of essay evidence scores is still needed."
2020.acl-main.759.txt,2020,6 Conclusion and Future Work,"currently, the tclda are trained on student essays, while the tcpr only works on the source article."
2020.acl-main.759.txt,2020,6 Conclusion and Future Work,evaluations show the potential of tcattn for eliminating expert effort without degrading aesrubric performance or the feature representations themselves.
2020.acl-main.759.txt,2020,6 Conclusion and Future Work,"however, tcattn uses both student essays and the source article for tc generation."
2020.acl-main.759.txt,2020,6 Conclusion and Future Work,it might be hard to say that the superior performance of tcattn is due to the neural architecture and attention scores rather than the richer training resources.
2020.acl-main.759.txt,2020,6 Conclusion and Future Work,"one of our next steps is to investigate the impact of tc extraction methods on a corresponding awe system (zhang et al., 2019), which uses the feature values produced by aesrubric to generate formative feedback to guide essay revision."
2020.acl-main.759.txt,2020,6 Conclusion and Future Work,tcattn outperforms baselines and generates comparable or even better results than a manual approach.
2020.acl-main.759.txt,2020,6 Conclusion and Future Work,"therefore, a comparison between tcattn and a model that uses both student essays and the source article is needed."
2020.acl-main.759.txt,2020,6 Conclusion and Future Work,"this leads to an interesting future investigation direction, which is training the aesneural using the gold standard that can be extracted automatically."
2020.acl-main.759.txt,2020,6 Conclusion and Future Work,"this paper proposes tcattn, a method for using the attention scores in a neural aes model to automatically extract the topical components of a source text."
2020.acl-main.76.txt,2020,6 Conclusion,"experimental results on n -best list reranking and unsupervised sts tasks demonstrate that the proposed t-ta is significantly faster than the bert-like approach, and its encoding ability is competitive with (or even better than) that of bert."
2020.acl-main.76.txt,2020,6 Conclusion,"in this work, we propose a novel deep bidirectional language model, namely, the t-ta, to eliminate the computational overload of applying bert to unsupervised applications."
2020.acl-main.760.txt,2020,6 Discussion,"comparing to similar errors from attention model (n = 161), we find that the number of acronym errors is nearly the same (24) as the better performing model (26)."
2020.acl-main.760.txt,2020,6 Discussion,"considering a smaller subset (e.g., 20) of concepts instead of all would significantly improve the speed."
2020.acl-main.760.txt,2020,6 Discussion,"depending on the application, a less accurate but faster linker might be a better choice (e.g.for all clinical notes at a medical institution)."
2020.acl-main.760.txt,2020,6 Discussion,deploying our system in a large-volume clinical setting would likely require several alterations.
2020.acl-main.760.txt,2020,6 Discussion,"further, if using a consistent portion of the ontology, caching the concept embeddings c as opposed to building them in-model also enhances efficiency."
2020.acl-main.760.txt,2020,6 Discussion,future work will consider additional methods for integrating ontology structure into representation learning.
2020.acl-main.760.txt,2020,6 Discussion,"however, it does not help with acronyms or other abbreviations that are less likely to appear in the ontology or are shorter and more ambiguous (e.g., ’r’ for rhonchus)."
2020.acl-main.760.txt,2020,6 Discussion,"in contrast, a more complex linker, such as ours, maybe a better option for specific subsets of notes that require better accuracy (e.g., the results of specific clinical studies)."
2020.acl-main.760.txt,2020,6 Discussion,"in contrast, the number of non-abbreviation errors drops significantly."
2020.acl-main.760.txt,2020,6 Discussion,"in the case of all entities, we find that the attention models provide a sizable gain in both accuracy and mrr."
2020.acl-main.760.txt,2020,6 Discussion,"in the case of entities with cuis, we find that pre-training the model does provide a gain in ranking accuracy (mrr)."
2020.acl-main.760.txt,2020,6 Discussion,"lbp for lower back pain), and 14% are mentions containing some other abbreviation (a shorted word, e.g.post nasal drip for posterior rhinorrhoea, or a partial acronym, seizure d / o for epilepsy)."
2020.acl-main.760.txt,2020,6 Discussion,"of those errors (n = 110), we find that 26% are mentions that contain only acronyms (e.g."
2020.acl-main.760.txt,2020,6 Discussion,"our neural ranking models with attention outperform all other models, except for cui-only accuracy."
2020.acl-main.760.txt,2020,6 Discussion,"our results demonstrate the advantages of using contextualized embeddings for ranking tasks, and that using information from the knowledge base for training is an essential direction for learning concept representations for sparse kb domains."
2020.acl-main.760.txt,2020,6 Discussion,"the main computational barrier to labeling a large amount of data, the speed of prediction, can be addressed by using an accurate candidate selection system to prune the number of concepts considered."
2020.acl-main.760.txt,2020,6 Discussion,this could be due to multiple possible concepts in the ontology or the presence of closely-related concepts.
2020.acl-main.760.txt,2020,6 Discussion,this suggests that pre-training provides useful signal for mentions that consist of variations appearing in the ontology.
2020.acl-main.760.txt,2020,6 Discussion,"we conducted an error analysis of the best performing mrr model (att.+ pre.)on the development data, looking at errors where the gold standard concept was not highly ranked (assigned a rank of 10 or above)."
2020.acl-main.760.txt,2020,6 Discussion,"we further noticed that in 21% of cases the linker predicted a relevant concept (e.g., mention thrombosed and thrombosis), but is not counted as correct due to annotation decisions."
2020.acl-main.760.txt,2020,6 Discussion,"while the linker often predicted unrelated concepts (40% of errors) for concepts where the correct concept was ranked above 10, many incorrect concept predictions were somewhat related to the gold concept (e.g., for mention atherosclerotic plaque with gold concept atherosclerotic fibrous plaque our model predicted the concept atherosclerosis)."
2020.acl-main.761.txt,2020,7 Conclusion,an end-to-end approach or a query reformulation step (re-writing claims to be similar to fever) might make the model more resilient as new attacks are introduced.
2020.acl-main.761.txt,2020,7 Conclusion,"finally, many verifiable claims are non-experiential (park and cardie, 2014), e.g.personal testimonies, which would require predicting whether a reported event was actually possible."
2020.acl-main.761.txt,2020,7 Conclusion,"finally, our system could be improved in many ways."
2020.acl-main.761.txt,2020,7 Conclusion,"future work in multi-hop reasoning could represent the relation between consecutive pieces of evidence and future work in temporal reasoning could incorporate numerical operations with bert (andor et al., 2019)."
2020.acl-main.761.txt,2020,7 Conclusion,"one limitation of our system is the pipeline nature, which may require addressing each type of attack individually as adversaries adjust their techniques."
2020.acl-main.761.txt,2020,7 Conclusion,"propositions with causal relations (hidey and mckeown, 2016), which are event-based rather than attribute-based as in fever, are also challenging."
2020.acl-main.761.txt,2020,7 Conclusion,"the drop dataset (dua et al., 2019) requires mathematical operations for question answering such as addition or counting."
2020.acl-main.761.txt,2020,7 Conclusion,"the facebook babi tasks (weston et al., 2016) include other types of reasoning (e.g.positional or size-based)."
2020.acl-main.761.txt,2020,7 Conclusion,there are many unaddressed vulnerabilities that are relevant for fact-checking.
2020.acl-main.761.txt,2020,7 Conclusion,we showed weaknesses in approaches to factchecking via novel adversarial claims.
2020.acl-main.761.txt,2020,7 Conclusion,"we took steps towards realistic fact-checking with targeted improvements to multi-hop reasoning (by a document pointer network and a pointer network for sequential joint sentence selection and relation prediction), simple temporal reasoning (by rule-based date handling), and ambiguity and variation (by fine-tuned contextualized representations)."
2020.acl-main.762.txt,2020,6 Conclusion,"for example, we can use font similarity techniques and enable users to pick a group of fonts, or to provide increased flexibility for the fonts available to users."
2020.acl-main.762.txt,2020,6 Conclusion,"in this paper, we associated font with written text and tackle the problem of font recommendation from the input text."
2020.acl-main.762.txt,2020,6 Conclusion,"the current approach covers a fixed number of fonts, but it can be extended to support a larger set of fonts."
2020.acl-main.762.txt,2020,6 Conclusion,"we collected more than 1,300 short written texts and annotated them with ten fonts."
2020.acl-main.762.txt,2020,6 Conclusion,we formulated this task as a ranking problem and compared different models based on emotional and contextual representations that exploit label distribution learning to predict fonts.
2020.acl-main.763.txt,2020,7 Conclusion,"in conclusion, while accounting for multiple frames per sample, we demonstrate how a cross-lingual analysis of news framing is informative and insightful in developing a global view surrounding the gun violence problem in the u.s."
2020.acl-main.763.txt,2020,7 Conclusion,"in this work, we present a novel code-switch model for the task of automatic cross-lingual news frame detection and show that it matches the performance of full translation if not overrides."
2020.acl-main.763.txt,2020,7 Conclusion,"moreover, we leverage an existing dataset by making use of multiple labels, create benchmark news framing test sets for three new languages, and employ a variant of focal loss to account for class imbalance in the data."
2020.acl-main.764.txt,2020,8 Conclusion and Future Work,"experimental records we use are from relatively homogeneous settings, e.g.all datasets in wiki-mt task are sentencepieced to have 5000 subwords, indicating that our predictor may fail for other subword settings."
2020.acl-main.764.txt,2020,8 Conclusion and Future Work,"first, the dataset and language settings covered in our study are still limited."
2020.acl-main.764.txt,2020,8 Conclusion and Future Work,"in reality, a slight change in model hyperparameters (hoos and leyton-brown, 2014; probst et al., 2019), optimization algorithms (kingma and ba, 2014), or even random seeds (madhyastha and jain, 2019) may give rise to a significant variation in performance, which our predictor is not able to capture."
2020.acl-main.764.txt,2020,8 Conclusion and Future Work,"in this work, we investigate whether the experiment setting itself is informative for predicting the evaluation scores of nlp tasks."
2020.acl-main.764.txt,2020,8 Conclusion and Future Work,it turned out that the average sentence length of the arn–spa data set is much lower than that of the training data sets and our predictors fail to generalize to this different setting.
2020.acl-main.764.txt,2020,8 Conclusion and Future Work,"lastly, we assume that the distribution of training and testing data is the same, which does not consider domain shift."
2020.acl-main.764.txt,2020,8 Conclusion and Future Work,"on top of this, there might also be a domain shift between data sets of training and testing experimental records."
2020.acl-main.764.txt,2020,8 Conclusion and Future Work,"our findings promisingly show that given a sufficient number of past training experimental records, our predictor can 1) outperform human experts; 2) make plausible predictions even over new-coming models and languages; 3) extrapolate well on features like dataset size; 4) provide a guide on how we should choose representative datasets for fast iteration."
2020.acl-main.764.txt,2020,8 Conclusion and Future Work,our model also failed to generalize to cases where feature values are out of the range of the training experimental records.
2020.acl-main.764.txt,2020,8 Conclusion and Future Work,"second, using a categorical feature to denote model types constrains its expressive power for modeling performance."
2020.acl-main.764.txt,2020,8 Conclusion and Future Work,"we attempted to apply the predictor of wiki-mt to evaluate on a low-resource mt dataset, translating from mapudungun (arn) to spanish (spa) with the dataset from duan et al.(2019), but ended up with a poor rmse score."
2020.acl-main.764.txt,2020,8 Conclusion and Future Work,we believe that modeling domain shift is a promising future direction to improve performance prediction.
2020.acl-main.764.txt,2020,8 Conclusion and Future Work,"while investigating the systematic implications of model structures or hyperparameters is practically infeasible in this study, we may use additional information such as textual model descriptions for modeling nlp models and training procedures more elaborately in the future."
2020.acl-main.764.txt,2020,8 Conclusion and Future Work,"while this discovery is a promising start, there are still several avenues on improvement in future work."
2020.acl-main.765.txt,2020,6 Conclusion and Future Work,"although story generation has been extensively studied in the literature, no existing work addressed the problem of generating movie scripts following a given storyline or narrative."
2020.acl-main.765.txt,2020,6 Conclusion and Future Work,"as a first investigation on the problem, our study has several limitations."
2020.acl-main.765.txt,2020,6 Conclusion and Future Work,"experimental results on the dataset showed that our proposed approach based on narrative significantly outperforms the baselines that use a narrative as an additional context, and showed the importance of using the narrative in a proper manner."
2020.acl-main.765.txt,2020,6 Conclusion and Future Work,"for example, we have not considered the order in the narrative description, which could be helpful in generating dialogues in correct order."
2020.acl-main.765.txt,2020,6 Conclusion and Future Work,further investigations are thus required to fully understand how narratives can be effectively used in dialogue generation.
2020.acl-main.765.txt,2020,6 Conclusion and Future Work,"in this paper, we addressed this problem in the context of generating dialogues in a movie script."
2020.acl-main.765.txt,2020,6 Conclusion and Future Work,other methods to track the dialogue state and the coverage of narrative can also be designed.
2020.acl-main.765.txt,2020,6 Conclusion and Future Work,"the final selection of the next response is based on multiple matching criteria between context, narrative and response."
2020.acl-main.765.txt,2020,6 Conclusion and Future Work,this is the first public dataset available for testing narrativeguided dialogue generation/selection.
2020.acl-main.765.txt,2020,6 Conclusion and Future Work,we constructed a new large-scale data collection for narrative-guided script generation from movie scripts.
2020.acl-main.765.txt,2020,6 Conclusion and Future Work,we keep track of what in the narrative has already been expressed and what is remaining to select the next line through an updating mechanism.
2020.acl-main.765.txt,2020,6 Conclusion and Future Work,we proposed a model that uses the narrative to guide the dialogue generation/retrieval.
2020.acl-main.766.txt,2020,6 Conclusion,"in the future, we will investigate a hub language ranking/selection model a la lin et al.(2019)."
2020.acl-main.766.txt,2020,6 Conclusion,"more importantly, we hope that by providing new dictionaries and baseline results on several language pairs, we will stir the community towards evaluating all methods in challenging scenarios that include under-represented language pairs."
2020.acl-main.766.txt,2020,6 Conclusion,"the problem of identifying the best hub language, despite our analysis based on the use of typological distance, remains largely unsolved."
2020.acl-main.766.txt,2020,6 Conclusion,"towards this end, our analysis provides insights and general directions for stronger baselines for non-anglocentric cross-lingual word embeddings."
2020.acl-main.766.txt,2020,6 Conclusion,we empirically show that the choice of the hub language is an important parameter that affects lexicon induction performance in both bilingual (between distant languages) and multilingual settings.
2020.acl-main.766.txt,2020,6 Conclusion,with this work we challenge the standard practice in learning cross-lingual word embeddings.
2020.acl-main.767.txt,2020,6 Conclusions,"in this work, we study the problem of automatic to-do item generation from email context and metadata to provide smart contextual assistance in email applications."
2020.acl-main.767.txt,2020,6 Conclusions,there are several directions for future work including better architecture design for utilizing structured meta-data and replacing the two-stage framework with a multi-task generation model that can jointly identify helpful context for the task and perform corresponding text generation.
2020.acl-main.767.txt,2020,6 Conclusions,"to this end, we introduce a new task and dataset for action-focused text intelligence."
2020.acl-main.767.txt,2020,6 Conclusions,we design a two stage framework with deep neural networks for task-focused text generation.
2020.acl-main.768.txt,2020,8 General Discussion & Conclusion,"for the other subdatasets, the models did not perform as expected on the basic unembedded presupposition triggers, again suggesting the model’s lack of knowledge of the basic meaning of these words."
2020.acl-main.768.txt,2020,8 General Discussion & Conclusion,"given that multinli contains few examples of the type found in imppres (see §4), where might our positive results come from?"
2020.acl-main.768.txt,2020,8 General Discussion & Conclusion,"in the case of presuppositions, the bert nli models, and bow to some extent, perform well on a number of our subdatasets (only, cleft existence, possessive existence, questions)."
2020.acl-main.768.txt,2020,8 General Discussion & Conclusion,maybe this induces patterns in the data that make the nature of those assumptions recoverable from the data itself.
2020.acl-main.768.txt,2020,8 General Discussion & Conclusion,nli training provides specific examples of valid (or invalid) inferences constituting an incomplete characterization of what commonsense inference is in general.
2020.acl-main.768.txt,2020,8 General Discussion & Conclusion,"pragmatic or logical reasoning was not diagnosable for the other scales, whose meaning was not fully understood by our models (as most scalar pairs were treated as synonymous)."
2020.acl-main.768.txt,2020,8 General Discussion & Conclusion,"since presuppositions and scalar implicatures triggered by specific lexical items are largely absent from the multinli data used for nli training, any positive results on imppres would likely use prior knowledge from the pretraining stage to make an inductive leap that pragmatic inferences are valid commonsense inferences."
2020.acl-main.768.txt,2020,8 General Discussion & Conclusion,"the natural language text used for pretraining certainly contains pragmatic information, since, like any natural language data, it is produced with the assumption that readers are capable of pragmatic reasoning."
2020.acl-main.768.txt,2020,8 General Discussion & Conclusion,"there are two potential sources of signal for the bert model: nli training, and pretraining (either bert’s masked language modeling objective or its input word embeddings)."
2020.acl-main.768.txt,2020,8 General Discussion & Conclusion,this work is an initial step towards rigorously investigating the extent to which nli models learn semantic versus pragmatic inference types.
2020.acl-main.768.txt,2020,8 General Discussion & Conclusion,"though their behavior is far from systematic, this is suggestive evidence that some nli models can perform in ways that correlate with human-like pragmatic behavior."
2020.acl-main.768.txt,2020,8 General Discussion & Conclusion,we find strong evidence that bert learns scalar implicatures associated with determiners some and all.
2020.acl-main.768.txt,2020,8 General Discussion & Conclusion,"we have introduced a new dataset imppres for probing this question, which can be reused to evaluate pragmatic performance of any nli given model."
2020.acl-main.768.txt,2020,8 General Discussion & Conclusion,we observe some encouraging results in §6–7.
2020.acl-main.769.txt,2020,7 Conclusion,"additionally, we extend our methods to combat multiple bias patterns simultaneously."
2020.acl-main.769.txt,2020,7 Conclusion,"extensive experiments show that our methods substantially improve the model robustness to domainshift, including 9.8 points gain on fever symmetric test set, 7.4 on hans dataset, and 4.8 points on snli hard set."
2020.acl-main.769.txt,2020,7 Conclusion,"furthermore, we show that our debiasing techniques result in better generalization to other nli datasets."
2020.acl-main.769.txt,2020,7 Conclusion,future work may include developing debiasing strategies that do not require prior knowledge of bias patterns and can automatically identify them.
2020.acl-main.769.txt,2020,7 Conclusion,"our debiasing strategies then work by adjusting the cross-entropy loss based on the performance of these bias-only models, to focus learning on the hard examples and downweight the importance of the biased examples."
2020.acl-main.769.txt,2020,7 Conclusion,"our proposed debiasing techniques are model agnostic, simple, and highly effective."
2020.acl-main.769.txt,2020,7 Conclusion,the bias-only models are designed to leverage biases and shortcuts in the datasets.
2020.acl-main.769.txt,2020,7 Conclusion,"we propose two novel techniques, product-of-experts and debiased focal loss, to reduce biases learned by neural models, which are applicable whenever one can specify the biases in the form of one or more bias-only models."
2020.acl-main.77.txt,2020,6 Conclusion and Future Work,"different from previous work that first integrates user’s reading history into a single representation vector and then matches the candidate news representation, our model can capture more fine-grained interest matching signals by performing interactions between each pair of news at multi-level semantic granularities."
2020.acl-main.77.txt,2020,6 Conclusion and Future Work,extensive experiments on a real-world dataset collected from msn news show that our model significantly outperforms the state-of-the-art methods.
2020.acl-main.77.txt,2020,6 Conclusion and Future Work,"in the future, we will do more tests and surveys on the improvement of business objectives such as user experience, user engagement and service revenue."
2020.acl-main.77.txt,2020,6 Conclusion and Future Work,"in this paper, we propose a new architecture for neural news recommendation based on multi-grained representation and matching."
2020.acl-main.770.txt,2020,7 Conclusion,existing debiasing methods improve the performance of nlu models on out-of-distribution datasets.
2020.acl-main.770.txt,2020,7 Conclusion,"however, this improvement comes at the cost of strongly diminishing the training signal from a subset of the original dataset, which in turn reduces the in-distribution accuracy."
2020.acl-main.770.txt,2020,7 Conclusion,"in this paper, we address this issue by introducing a novel method that regularizes models’ confidence on biased examples."
2020.acl-main.770.txt,2020,7 Conclusion,our debiasing framework is general and can be extended to other task setups where the biases leveraged by models are correctly identified.
2020.acl-main.770.txt,2020,7 Conclusion,our experiments on four out-of-distribution datasets across three nlu tasks show that our method provides a competitive out-of-distribution performance while preserves the original accuracy.
2020.acl-main.770.txt,2020,7 Conclusion,several challenges in this direction of research may include extending the debiasing methods to overcome multiple biases at once or to automatically identify the format of those biases which simulate a setting where the prior knowledge is unavailable.
2020.acl-main.770.txt,2020,7 Conclusion,this method allows models to still learn from all training examples without exploiting the biases.
2020.acl-main.771.txt,2020,6 Conclusion,"finally, we demonstrate that task-specific probes are necessary to measure such sensitivity."
2020.acl-main.771.txt,2020,6 Conclusion,"in the paper, we also argue the importance of explicit evaluation of faithfulness of the generated explanations, i.e., how correlated are the explanations to the model’s decision making."
2020.acl-main.771.txt,2020,6 Conclusion,"in this paper we propose nile, a system for natural language inference (nli) capable of generating labels along with natural language explanations for the predicted labels."
2020.acl-main.771.txt,2020,6 Conclusion,nile supports the hypothesis that accurate systems can produce testable natural language explanations of their decisions.
2020.acl-main.771.txt,2020,6 Conclusion,"through extensive experiments, we demonstrate the effectiveness of this approach, in terms of both label and explanation accuracy."
2020.acl-main.771.txt,2020,6 Conclusion,we evaluate faithfulness of nile’s explanations using sensitivity analysis.
2020.acl-main.772.txt,2020,5 Conclusion,"experiments on a wide range of tasks show that the distinction of s-quase and p-quase is highly effective, and quaseqamr has the potential to improve on many tasks, especially in the low-resource setting."
2020.acl-main.772.txt,2020,5 Conclusion,"for tasks with a single-sentence input, such as srl and ner, we propose s-quase that provides latent sentence-level representations; for tasks with a sentence pair input, such as te and mrc we propose p-quase, that generates latent representations related to attentions."
2020.acl-main.772.txt,2020,5 Conclusion,"in this paper, we investigate an important problem in nlp: can we make use of low-cost signals, such as qa data, to help related tasks?"
2020.acl-main.772.txt,2020,5 Conclusion,we retrieve signals from sentence-level qa pairs to help nlp tasks via two types of sentence encoding approaches.
2020.acl-main.773.txt,2020,7 Conclusion,"finally, we would like to point out that our methods and results here do not mean to belittle the importance of collecting clean/unbiased data."
2020.acl-main.773.txt,2020,7 Conclusion,"for model-level changes, we first show the ineffectiveness of embedding-debiasing approaches, thus highlighting the uniqueness of lexical bias against gender bias problems."
2020.acl-main.773.txt,2020,7 Conclusion,"however, some biases are inherent and inevitable in the natural distribution of the task (e.g., for nli, it is natural that sentences with high overlapping are most likely entailment pairs)."
2020.acl-main.773.txt,2020,7 Conclusion,joint efforts are needed for promoting unbiased models that learn true semantics; and we hope our paper can encourage more work towards this important direction.
2020.acl-main.773.txt,2020,7 Conclusion,neither model-level debiasing nor data-level debiasing alone is the conclusive solution for this problem.
2020.acl-main.773.txt,2020,7 Conclusion,"next, we robustify the model by forcing orthogonality between a bow sub-model and the main model and demonstrate its effectiveness through several experiments."
2020.acl-main.773.txt,2020,7 Conclusion,"since none of our methods is biastype specific, we believe these results can also be generalized to other similar lexical biases."
2020.acl-main.773.txt,2020,7 Conclusion,"therefore, our work stresses that it is also very important to encourage the development of models that are unlikely to exploit these inevitable biases/shortcuts in the dataset."
2020.acl-main.773.txt,2020,7 Conclusion,we first showed that lexical dataset biases cannot be solved by simple dataset changes and motivate the importance of directly designing model-level changes to solve this problem.
2020.acl-main.773.txt,2020,7 Conclusion,we strongly believe in the importance of unbiased data for model design and evaluation.
2020.acl-main.773.txt,2020,7 Conclusion,we study the problem of lexical dataset biases using wob and cwb as two examples.
2020.acl-main.774.txt,2020,6 Conclusion,"humans are able to make finer distinctions between meanings than is being captured by current annotation approaches; we advocate the community strives for systems that can do the same, and therefore shift away from categorical nli labels and move to something more fine-grained such as our unli protocol."
2020.acl-main.774.txt,2020,6 Conclusion,"in short, we have shown that not all nli contradictions are created equal, nor neutrals, nor entailments."
2020.acl-main.774.txt,2020,6 Conclusion,"we demonstrated that (1) eliciting supporting data is feasible, and (2) annotations in the data can be used for improving a scalar regression model beyond the information contained in existing categorical labels, using recent contextualized word embeddings, e.g.bert."
2020.acl-main.774.txt,2020,6 Conclusion,"we proposed uncertain natural language inference (unli), a new task of directly predicting human likelihood judgments on nli premisehypothesis pairs."
2020.acl-main.775.txt,2020,6 Conclusion and Further Directions,"an interesting additional use-case for our joint decoder is when a downstream task, e.g., relation extraction, requires output structures from both a parser and a tagger."
2020.acl-main.775.txt,2020,6 Conclusion and Further Directions,another limitation of our current work is that our joint decoder only produces projective dependency parse trees.
2020.acl-main.775.txt,2020,6 Conclusion and Further Directions,"experiments on the mwe-aware english dependency corpus and ud 2.2 across five languages show that tagging, a widely-used methodology for extracting spans from texts, is more accurate than parsing for this task."
2020.acl-main.775.txt,2020,6 Conclusion and Further Directions,future community efforts on a unified representation of flat structures for all languages would facilitate further research on linguistically-motivated treatments of headless structures in “headful” dependency treebanks.
2020.acl-main.775.txt,2020,6 Conclusion and Further Directions,"our joint decoder can find the highest-scoring consistent structures among all candidates, and thus has the potential to provide simpler model designs in downstream applications."
2020.acl-main.775.txt,2020,6 Conclusion and Further Directions,"our paper provides an empirical comparison of different strategies for extracting headless mwes from dependency parse trees: parsing, tagging, and joint modeling."
2020.acl-main.775.txt,2020,6 Conclusion and Further Directions,our study has been limited to a few treebanks in ud partially due to large variations and inconsistencies across different treebanks.
2020.acl-main.775.txt,2020,6 Conclusion and Further Directions,"to handle non-projectivity, one possible solution is pseudo-projective parsing (nivre and nilsson, 2005)."
2020.acl-main.775.txt,2020,6 Conclusion and Further Directions,we also show that most of the gains stem from a multi-task learning strategy that shares common neural representations between the parsers and the taggers.
2020.acl-main.775.txt,2020,6 Conclusion and Further Directions,we leave it to future work to design a non-projective decoder for joint parsing and headless structure extraction.
2020.acl-main.775.txt,2020,6 Conclusion and Further Directions,"when using bi-lstm (but not bert) representations, our proposed joint decoder reaches higher f1 scores than either of the two other strategies, by combining scores of the two different and complementary representations of the same structures."
2020.acl-main.776.txt,2020,4 Conclusion,"another interesting line of research would be to evaluate the contribution of higher-order features in a cross-lingual setting, leveraging structure learned from larger treebanks to underresourced languages."
2020.acl-main.776.txt,2020,4 Conclusion,"considering the exact match of complete parse trees or all modifiers of a word, second-order models exhibit an advantage over first-order ones."
2020.acl-main.776.txt,2020,4 Conclusion,our results indicate that even a powerful encoder as bert can still benefit from explicit output structure modelling; this would be interesting to explore in other nlp tasks as well.
2020.acl-main.776.txt,2020,4 Conclusion,"we compared second-order dependency parsers to their more common, first-order counterparts."
2020.acl-main.776.txt,2020,4 Conclusion,"while their overall performance gain was small, they are distinctively better for longer sentences and long-range dependencies."
2020.acl-main.777.txt,2020,6 Conclusion,"in addition, seqvat is also highly effective in semi-supervised settings and outperforms traditional semi-supervised algorithms (st and em) as well as a state-of-the-art approach (cvt)."
2020.acl-main.777.txt,2020,6 Conclusion,"in this paper, we propose a crf compatible vat training algorithm and demonstrate that sequence labeling tasks can greatly benefit from it."
2020.acl-main.777.txt,2020,6 Conclusion,"our proposed method, seqvat, has strong effects to improve model robustness and accuracy on supervised sequence labeling tasks."
2020.acl-main.777.txt,2020,6 Conclusion,"overall, our approach is highly effective for chunking, ner and slot filling, and can be easily extended to solve other sequence labeling problems in both supervised and semi-supervised settings."
2020.acl-main.778.txt,2020,7 Conclusion,another area for future work is to explore what information treebank vectors encode.
2020.acl-main.778.txt,2020,7 Conclusion,"for individual sentences, las is usually constant in large areas and there are clear, sharp steps to the next las level."
2020.acl-main.778.txt,2020,7 Conclusion,"future work should also test even simpler strategies which do not use the las of previous parses to gauge the best treebank vector, e. g. always picking the largest treebank."
2020.acl-main.778.txt,2020,7 Conclusion,"in experiments with czech, english and french, we investigated treebank embedding vectors, exploring the ideas of interpolated vectors and vector weight prediction."
2020.acl-main.778.txt,2020,7 Conclusion,"interpolating treebank vectors adds a layer of opacity, and, in future work, it would be interesting to carry out experiments with synthetic data, e. g. varying the number of unknown words, to get a better understanding of what they may be capturing."
2020.acl-main.778.txt,2020,7 Conclusion,"on the whole, it seems that our predictor is not yet good enough to find interpolated treebank vectors that are clearly superior to the basic, fixed vectors and that we know to exist from the oracle runs."
2020.acl-main.778.txt,2020,7 Conclusion,our attempts to predict good vector weights using a simple regression model yielded encouraging results.
2020.acl-main.778.txt,2020,7 Conclusion,"still, we think it is encouraging that performance did not drop substantially when the set of candidate vectors was widened (αt ≥ 0 and ‘any’)."
2020.acl-main.778.txt,2020,7 Conclusion,"testing on pud languages, we match the performance of using the best fixed treebank embedding vector in nine of ten cases within the bounds of statistical significance and in five cases exactly match it."
2020.acl-main.778.txt,2020,7 Conclusion,the previous work on the use of treebank vectors in mono- and multi-lingual parsing suggests that treebank vectors encode information that enables the parser to select treebank-specific information where needed while also taking advantage of treebank-independent information available in the training data.
2020.acl-main.778.txt,2020,7 Conclusion,"the type of information will depend on the selection of tree-banks, e. g. in a polyglot setting the vector may simply encode the language, and in a monolingual setting such as ours it may encode annotation or domain differences between the treebanks."
2020.acl-main.778.txt,2020,7 Conclusion,"therefore, we think that there is room for improvement for the predictor to find interpolated vectors which are better than the fixed ones."
2020.acl-main.778.txt,2020,7 Conclusion,"we do not think the superior treebank vectors found by the oracle runs are simply noise, i. e. model fluctuations due to varied inputs, because the las landscape in the weight vector space is not noisy."
2020.acl-main.778.txt,2020,7 Conclusion,"we plan to explore other methods to predict tree-bank vectors, e. g. neural sequence modelling, and to apply our ideas to the related task of language embedding prediction for zero-shot learning."
2020.acl-main.78.txt,2020,4 Conclusion,"in sum, our framework aims to address two common issues in the financial industry: lacking labeled data and the need for transparency in prediction outcomes."
2020.acl-main.78.txt,2020,4 Conclusion,"to conclude, in this paper, we work on a significant practical problem in the financial industry: operational risk prediction."
2020.acl-main.78.txt,2020,4 Conclusion,we design a text classification framework with the multi-head attention mechanism and semivae.
2020.acl-main.79.txt,2020,5 Conclusion,"in this work, we presented a framework for explaining the gnn-based models by extending the influence function to estimate the effect of samples in graph data."
2020.acl-main.79.txt,2020,5 Conclusion,"some interesting observations include the effects of regions and the sensitivity of gnn-based models, which open potentials for further improvements that we plan to address in our future work."
2020.acl-main.79.txt,2020,5 Conclusion,the experiments conducted on a specific task – user geolocation – provided intuitive explanations and enabled quantification of the influence of individual training samples.
2020.acl-main.8.txt,2020,7 Conclusions,"furthermore, when conditioned on prior conversations, the model is able to utilize a speaker’s personality when choosing how to reply in a conversation to allow for greater control and more diverse responses."
2020.acl-main.8.txt,2020,7 Conclusions,"in the future, we aim to leverage these pre-trained models to advance sota on downstream conversational tasks, such as knowledge-grounded conversations or question answering."
2020.acl-main.8.txt,2020,7 Conclusions,recent advancements in learnable information retrieval systems could select contextually relevant references to further strengthen the quality of generated dialogue.
2020.acl-main.8.txt,2020,7 Conclusions,"when a large conversational model is trained on a diverse collection of multi-turn conversations, it is able to generate quality conversations that are engaging, coherent, and plausibly human."
2020.acl-main.80.txt,2020,6 Conclusion,"it can be further extended for l1 → l1l2, or l2 → l1l2 for code switch sentence generation, and l1 → l2, or l2 → l1 for machine translation."
2020.acl-main.80.txt,2020,6 Conclusion,the experiments show that it outperforms all state-of-the-art models in the literature for similar tasks.
2020.acl-main.80.txt,2020,6 Conclusion,the results validate the idea of bilingual attention.
2020.acl-main.80.txt,2020,6 Conclusion,the same balm can be used in l1l2 → l1 or l2 for language normalization.
2020.acl-main.80.txt,2020,6 Conclusion,we note that balm is an implementation of l1l2 → l1l2.
2020.acl-main.81.txt,2020,5 Conclusions,"beyond csc, spellgcn can be generalized to other situations where specific prior knowledge is available, and to other languages by leveraging specific similarity graphs analogously."
2020.acl-main.81.txt,2020,5 Conclusions,"our method can also be adapted to grammar error correction, which needs insertion and deletion, by utilizing more flexible extractors such as levenshtein transformer (gu et al., 2019)."
2020.acl-main.81.txt,2020,5 Conclusions,the empirical comparison and the results of analytical experiments verify its effectiveness.
2020.acl-main.81.txt,2020,5 Conclusions,we leave this direction to future work.
2020.acl-main.81.txt,2020,5 Conclusions,we proposed spellgcn for csc to incorporate both phonological and visual similarities into language models.
2020.acl-main.82.txt,2020,5 Conclusion,"as future work, we plan to extend soft-masked bert to other problems like grammatical error correction and explore other possibilities of implementing the detection network."
2020.acl-main.82.txt,2020,5 Conclusion,experimental results on two datasets show that soft-masked bert significantly outperforms the state-of-art method of solely utilizing bert.
2020.acl-main.82.txt,2020,5 Conclusion,"in this paper, we have proposed a novel neural network architecture for spelling error correction, more specifically chinese spelling error correction (csc)."
2020.acl-main.82.txt,2020,5 Conclusion,our model called soft-masked bert is composed of a detection network and a correction network based on bert.
2020.acl-main.82.txt,2020,5 Conclusion,the correction network takes the soft-masked characters as input and makes correction on the characters.
2020.acl-main.82.txt,2020,5 Conclusion,the detection network identifies likely incorrect characters in the given sentence and soft-masks the characters.
2020.acl-main.82.txt,2020,5 Conclusion,the technique of soft-masking is general and potentially useful in other detection-correction tasks.
2020.acl-main.83.txt,2020,5 Conclusion,our extensive experimental results demonstrate it works very well for the challenging machine reading comprehension task.
2020.acl-main.83.txt,2020,5 Conclusion,"we propose a novel frame-based sentence representation method, which integrates multi-frame semantic information to facilitate sentence modelling."
2020.acl-main.84.txt,2020,8 Conclusion,"based on these results, we will explore ways to leverage the token assignment to domain adaption and few-shot learning."
2020.acl-main.84.txt,2020,8 Conclusion,"finally, we showed that the inclusion of the token alignment results in an increase of precision of up to 7%."
2020.acl-main.84.txt,2020,8 Conclusion,"furthermore, our procedure allows a fine-grained alignment of tokens to operations."
2020.acl-main.84.txt,2020,8 Conclusion,"furthermore, we implemented a baseline system for automatically generating ots from nl queries."
2020.acl-main.84.txt,2020,8 Conclusion,generating this corpus was more time- and costefficient than with previous approaches.
2020.acl-main.84.txt,2020,8 Conclusion,"in this paper, we introduced a fast annotation procedure to create nl queries and corresponding database queries (in our case, operation trees)."
2020.acl-main.84.txt,2020,8 Conclusion,"our procedure more than triples the velocity of annotation in comparison to previous methods, while ensuring a larger variety of different types of queries and covering a larger part of the underlying databases."
2020.acl-main.84.txt,2020,8 Conclusion,our statistical analysis showed that the corpus yields a higher coverage of attributes in the databases and more complex natural language questions than other existing methods.
2020.acl-main.84.txt,2020,8 Conclusion,"this baseline achieves scores of up to 48% precision, which are already reasonable while also leaving large potential for improvement in future research."
2020.acl-main.84.txt,2020,8 Conclusion,we also plan to enhance the annotation process by automatically generating proposals for the nl questions and token assignments and letting the annotators only perform corrections.
2020.acl-main.84.txt,2020,8 Conclusion,we hope that this increases annotation efficiency even more.
2020.acl-main.84.txt,2020,8 Conclusion,"we then used our new method to generate otta, a novel corpus for semantic parsing based on operation trees in combination with token assignments."
2020.acl-main.85.txt,2020,5 Conclusion,"experimental results show that our fast open-domain qa model that augments denspi with sparc outperforms previous open-domain qa models, including recent bert-based pipeline models, with two orders of magnitude faster inference time."
2020.acl-main.85.txt,2020,5 Conclusion,"in this paper, we demonstrate the effectiveness of contextualized sparse representations, sparc, for encoding phrase with rich lexical information in open-domain question answering."
2020.acl-main.85.txt,2020,5 Conclusion,we efficiently train our sparse representations by kernelizing the sparse inner product space.
2020.acl-main.86.txt,2020,4 Conclusions,"it is also worth noting that for the duorc, narrativeqa, squad, and quoref datasets there are cases where the multi-task model outperforms the single-task model."
2020.acl-main.86.txt,2020,4 Conclusions,our goal was to investigate which instance sampling method and epoch scheduling strategy gives optimal performance in a multi-task reading comprehension setting.
2020.acl-main.86.txt,2020,4 Conclusions,the results suggest that dynamic sampling—sampling instances from each task based on their respective metric differentials— is a fruitful direction to explore for improving performance.
2020.acl-main.86.txt,2020,4 Conclusions,"this suggests that for specific cases, we observe an effect similar to data augmentation (like exposure to squad benefitting quoref performance as mentioned above) but this needs to be explored further."
2020.acl-main.86.txt,2020,4 Conclusions,we also show that interleaving instances from different tasks within each epoch and forming heterogeneous batches is crucial for optimizing multi-task performance.
2020.acl-main.86.txt,2020,4 Conclusions,"we hope that future work experiments further with dynamic sampling such as by modifying the metric (e.g., using bleu or rouge score if applicable) and/or modifying other values like number of instances per epoch based on performance metrics (not only does this effectively change learning rate, but it would also allow the model to update the sampling distribution more or less frequently)."
2020.acl-main.87.txt,2020,6 Conclusion,extensive experiments on two multilingual mrc datasets have been conducted to prove the effective of our proposed approach.
2020.acl-main.87.txt,2020,6 Conclusion,"meanwhile, we further analyze the model performance on fine-grained answer types, which shows interesting insights."
2020.acl-main.87.txt,2020,6 Conclusion,this paper proposes two auxiliary tasks (mixmrc and lakm) in the multilingual mrc fine-tuning stage to enhance answer boundary detection especially for low resource languages.
2020.acl-main.88.txt,2020,5 Conclusions,emt achieved a new state-of-the-art result on the sharc cmr challenge.
2020.acl-main.88.txt,2020,5 Conclusions,emt also gives interpretability by showing the entailment-oriented reasoning process as the conversation flows.
2020.acl-main.88.txt,2020,5 Conclusions,"in this paper, we have proposed a new framework for conversational machine reading (cmr) that comprises a novel explicit memory tracker (emt) to track entailment states of the rule sentences explicitly within its memory module."
2020.acl-main.88.txt,2020,5 Conclusions,the updated states are utilized for decision making and coarse-to-fine follow-up question generation in a unified manner.
2020.acl-main.88.txt,2020,5 Conclusions,"while we conducted experiments on the sharc dataset, we believe the proposed methodology could be extended to other kinds of cmr tasks."
2020.acl-main.89.txt,2020,7 Conclusions,"consequently, current models that perform numerical reasoning over a pretrained lm resorted to customized modules with limited flexibility."
2020.acl-main.89.txt,2020,7 Conclusions,"in this work, we propose a general method for injecting additional skills into lms, assuming automatic data generation is possible."
2020.acl-main.89.txt,2020,7 Conclusions,large pre-trained lms lack high-level skills such as numerical reasoning.
2020.acl-main.89.txt,2020,7 Conclusions,"our experiments demonstrate the effectiveness of our method, showing that genbert successfully learns the numerical skills, and performs on par with state-of-the-art nrot models of the same size."
2020.acl-main.89.txt,2020,7 Conclusions,"we apply our approach to the task of numerical reasoning over text, using a general-purpose model called genbert, and a simple framework for generating large amounts of synthetic examples."
2020.acl-main.9.txt,2020,5 Conclusion,"a novel pre-training model for dialogue generation is introduced in this paper, incorporated with latent discrete variables for one-to-many relationship modeling."
2020.acl-main.9.txt,2020,5 Conclusion,and the results demonstrate that our model obtains significant improvements over the other state-of-the-art methods.
2020.acl-main.9.txt,2020,5 Conclusion,extensive and intensive experiments have been carried out on three different kinds of publicly available datasets.
2020.acl-main.9.txt,2020,5 Conclusion,"in the future, we will also explore to boost the latent selection policy with reinforcement learning and extend our pre-training to support dialogue generation in other languages."
2020.acl-main.9.txt,2020,5 Conclusion,our pre-trained model is flexible enough to handle various down-stream tasks of dialogue generation.
2020.acl-main.9.txt,2020,5 Conclusion,our work can be potentially improved with more fine-grained latent variables.
2020.acl-main.9.txt,2020,5 Conclusion,"to pre-train our model, two reciprocal tasks of response generation and latent recognition are carried out simultaneously on large-scale conversation datasets."
2020.acl-main.90.txt,2020,8 Conclusion,"additionally, we developed several strong baseline systems, and showed that our proposed three-way attentive pooling network outperforms all the baseline systems."
2020.acl-main.90.txt,2020,8 Conclusion,"in this paper, we present a new follow-up question identification task in a conversational setting."
2020.acl-main.90.txt,2020,8 Conclusion,incorporating our three-way attentive pooling network into open domain conversational qa systems will be interesting future work.
2020.acl-main.90.txt,2020,8 Conclusion,"notably, the proposed dataset supports automatic evaluation."
2020.acl-main.90.txt,2020,8 Conclusion,"we developed a dataset, namely lif, which is derived from the previously released quac dataset."
2020.acl-main.90.txt,2020,8 Conclusion,we proposed a novel three-way attentive pooling network which identifies whether a follow-up question is valid or invalid by considering the associated knowledge in a passage and the conversation history.
2020.acl-main.91.txt,2020,4 Conclusion,"by incorporating constraints into query graphs early, coupled with the help of beam search, we are able to restrict the search space."
2020.acl-main.91.txt,2020,4 Conclusion,experiments showed our method substantially outperformed existing methods on the complexwebquestions dataset and also outperformed the previous state of the art on two other kbqa datasets.
2020.acl-main.91.txt,2020,4 Conclusion,in this paper we proposed a modified staged query graph generation method to deal with complex questions with both multi-hop relations and constraints.
2020.acl-main.92.txt,2020,5 Conclusion and Future Work,"each mwp is annotated with the corresponding problem type, equation, and grade level, which are useful for machine learning and assessing the difficulty level of each mwp."
2020.acl-main.92.txt,2020,5 Conclusion and Future Work,"in terms of this metric, we show that in comparison with those corpora widely adopted to compare systems, ours is more suitable for assessing the real performance of an mwp solver."
2020.acl-main.92.txt,2020,5 Conclusion and Future Work,"last, we conduct experiments to show that a low-diverse mwp corpora will exaggerate the true performance of sota systems (we are still far behind human-level performance), and that grade level is a useful index for indicating the difficulty of an mwp."
2020.acl-main.92.txt,2020,5 Conclusion and Future Work,we also propose a metric to measure the diversity of lexicon usage of a given corpus.
2020.acl-main.92.txt,2020,5 Conclusion and Future Work,we present an mwp corpus which not only is highly diverse in terms of lexicon usage but also covers most problem types taught in elementary school.
2020.acl-main.93.txt,2020,6 Conclusion,"in this work, we study the intrinsic variance among ground truth captions in image captioning evaluation."
2020.acl-main.93.txt,2020,6 Conclusion,our metric also benefits from stop word removal by reducing the impact of stop words.
2020.acl-main.93.txt,2020,6 Conclusion,the experimental results show that our metric can reach state-of-the-art human correlation in several evaluation tasks.
2020.acl-main.93.txt,2020,6 Conclusion,"we propose an improved matching metrics based on bertscore, which can combine all of the references for taking full advantage of multi-references."
2020.acl-main.94.txt,2020,6 Conclusion and Future Work,"despite their obvious connection, the relation between the choice of context window and the structural similarity of two embedding spaces has not been fully investigated in prior work."
2020.acl-main.94.txt,2020,6 Conclusion and Future Work,"especially for dependency parsing, the smallest context size produces the best result for the source task, but performs the worst for test languages."
2020.acl-main.94.txt,2020,6 Conclusion and Future Work,"in summary, we have shown that: • larger context windows for both the source and target facilitate the alignment of words, especially nouns.• for cross-lingual transfer, the best context window for the source task is often not the best for test languages."
2020.acl-main.94.txt,2020,6 Conclusion and Future Work,"in this study, we have offered the first thorough empirical results on the relation between the context window size and bilingual embeddings, and shed new light on the property of bilingual embeddings."
2020.acl-main.94.txt,2020,6 Conclusion and Future Work,we hope that our study will provide insights into ways to improve cross-lingual embeddings by not only mapping methods but also the properties of monolingual embedding spaces.
2020.acl-main.95.txt,2020,8 Conclusion,"another direction for future work would improve few-shot approaches to wsd, which is both important for moving wsd into new domains and for modeling rare senses that naturally have less support in wsd data."
2020.acl-main.95.txt,2020,8 Conclusion,"however, we still see a large gap in performance between mfs and lfs examples, with our model still performing over 40 points better on the mfs subset."
2020.acl-main.95.txt,2020,8 Conclusion,"in this work, we address the issue of wsd systems underperforming on uncommon senses of words."
2020.acl-main.95.txt,2020,8 Conclusion,most recent wsd systems show a similar trend: even the representations of frozen bertbase that are not fine-tuned on wsd can achieve over 94 f1 on examples labeled with the most frequent sense.
2020.acl-main.95.txt,2020,8 Conclusion,"potential directions include finding ways to obtain more informative training signal from uncommon senses, such as with different approaches to loss reweighting, and exploring the effectiveness of other model architectures on lfs examples."
2020.acl-main.95.txt,2020,8 Conclusion,the bem then disambiguates the sense of each word by assigning it the label of the nearest sense embedding.
2020.acl-main.95.txt,2020,8 Conclusion,this approach leads to a 31.1% error reduction over prior work on the less frequent sense examples.
2020.acl-main.95.txt,2020,8 Conclusion,this leaves better disambiguation of less common senses as the main avenue for future work on wsd.
2020.acl-main.95.txt,2020,8 Conclusion,we present a bi-encoder model (bem) that maps senses and ambiguous words into the same embedding space by jointly optimizing the context and glosses encoders.
2020.acl-main.96.txt,2020,6 Conclusion,"finally, this model can be improved further by pumping the switching features in the final layer of the deep network."
2020.acl-main.96.txt,2020,6 Conclusion,"for instance, we have seen examples of such switching in english-spanish10 and english-telugu11 pairs also."
2020.acl-main.96.txt,2020,6 Conclusion,further we plan to investigate other nlp applications that can benefit from the simple linguistic features introduced here.
2020.acl-main.96.txt,2020,6 Conclusion,"in addition, we exploit the modern deep learning machinery to improve the performance further."
2020.acl-main.96.txt,2020,6 Conclusion,"in future, we would like to extend this work for other language pairs."
2020.acl-main.96.txt,2020,6 Conclusion,"in this paper, we identified how switching patterns can be effective in improving three different nlp applications."
2020.acl-main.96.txt,2020,6 Conclusion,we present a set of nine features that improve upon the state-of-the-art baselines.
2020.acl-main.97.txt,2020,5 Conclusion,"in the future, we will extend the proposed framework by considering more context (meta data) information, such as time, storylines, and comment sentiment, to further enrich our explainability."
2020.acl-main.97.txt,2020,5 Conclusion,results on two public datasets demonstrated the effectiveness and explainability of this framework.
2020.acl-main.97.txt,2020,5 Conclusion,"we proposed a novel framework combining decision tree and neural attention networks to explore a transparent and interpretable way to discover evidence for explainable claim veriﬁcation, which constructed decision tree model to select comments with high credibility as evidence, and then designed co-attention networks to make the evidence and claims interact with each other for unearthing the false parts of claims."
2020.acl-main.98.txt,2020,6 Conclusion,"the complexity in durecdial makes it a great testbed for more tasks such as knowledge grounded conversation (ghazvininejad et al., 2018), domain transfer for dialog modeling, target-guided conversation (tang et al., 2019a) and multi-type dialog modeling (yu et al., 2017)."
2020.acl-main.98.txt,2020,6 Conclusion,the study of these tasks will be left as the future work.
2020.acl-main.98.txt,2020,6 Conclusion,we demonstrate usability of this dataset and provide results of state of the art models for future studies.
2020.acl-main.98.txt,2020,6 Conclusion,"we identify the task of conversational recommendation over multi-type dialogs, and create a dataset durecdial with multiple dialog types and multi-domain use cases."
2020.acl-main.99.txt,2020,6 Conclusion,"furthermore, we successfully applied seg to improve generalized zero-shot intent classification and achieved remarkable performance gain over a most recent competitive method recapsnet."
2020.acl-main.99.txt,2020,6 Conclusion,"in future work, we plan to conduct more empirical studies on seg and further improve its performance on new intent identification."
2020.acl-main.99.txt,2020,6 Conclusion,"in this paper, we have proposed seg, a semanticenhanced gaussian mixture model coupled with a lof outlier detector, for unknown (new) intent detection."
2020.acl-main.99.txt,2020,6 Conclusion,we also plan to conduct more case studies in applying seg to boost the performance of current zero-shot intent classification methods.
2020.acl-main.99.txt,2020,6 Conclusion,we empirically verified the effectiveness of seg for unknown intent detection on real dialogue datasets in english and chinese.
2020.acl-srw.1.txt,2020,6 Conclusion,adaptive methods can significantly reduce the cost incurred to train such models and carbon footprints.
2020.acl-srw.1.txt,2020,6 Conclusion,"in this work, we extend adaptive approaches to visiolinguistic tasks to understand more about attention and adaptive mechanisms."
2020.acl-srw.1.txt,2020,6 Conclusion,"while attention-based approaches are becoming universal, computationally efficient ways must be favored for broader adoption of provided pretrained models on low resource hardware."
2020.acl-srw.1.txt,2020,6 Conclusion,"while the empirical results are encouraging, important future work includes explorations of higher efficient adaptive and sparse mechanisms that can significantly cause flops and parameter reduction with minimal loss in performance."
2020.acl-srw.10.txt,2020,6 Conclusion,"further extensions may include studying the behavior of more powerful subword combination strategies (e.g.convolutions, self-attention) and the application of subword merging to the target side."
2020.acl-srw.10.txt,2020,6 Conclusion,"future extensions to this work may include applying it to character-level instead of subword representations, and using it for morphologically richer languages, especially low-resourced agglutinative ones, where our approach, together with the incorporation of linguistic information, may provide larger improvements in translation quality."
2020.acl-srw.10.txt,2020,6 Conclusion,"in this work, we proposed a modification to the transformer architecture to merge the subword representations from the first layers of the encoder into word-level representations."
2020.acl-srw.10.txt,2020,6 Conclusion,merging wordlevel representations inside the model allows it to use the subword-level representations in the final decoder layers so that it can handle compositional structures and other situations where copying from source is needed.
2020.acl-srw.10.txt,2020,6 Conclusion,this approach provided an appropriate point to incorporate linguistic word-level information and it is superior at doing so compared with the reference approach by sennrich and haddow (2016).
2020.acl-srw.11.txt,2020,7 Conclusions and Future Work,"for example, the meaning of “낙지” is “squid” in south korean, but “octopus” in north korean."
2020.acl-srw.11.txt,2020,7 Conclusions and Future Work,"however, the differences that exist between south korean and north korean are not only grammatical ones."
2020.acl-srw.11.txt,2020,7 Conclusions and Future Work,"in the future, we intend to use the english translation data of north korean news articles to create an evaluation dataset that considers differences in words, and attempt to develop a translation method using a language model with context, such as bert (devlin et al., 2019)."
2020.acl-srw.11.txt,2020,7 Conclusions and Future Work,"in this study, to solve the language resource bottleneck in north korean translation, we proposed a method to tokenize input sentences in south korean and north korean at the character level and decompose them into phonemes."
2020.acl-srw.11.txt,2020,7 Conclusions and Future Work,there are some words that have the same pronunciation and notation but different meanings.
2020.acl-srw.11.txt,2020,7 Conclusions and Future Work,"therefore, the differences in word meanings are a major challenge."
2020.acl-srw.11.txt,2020,7 Conclusions and Future Work,"this method is simple and mitigates the grammatical differences between south korean and north korean; moreover, the method demonstrates improvement in translation accuracy for north korean to english translation."
2020.acl-srw.12.txt,2020,6 Conclusion and Implications,"also, i think that these methods could be integrated into the analysis workflow of content analyses and frame analyses, helping to automate further these currently mostly manual and thus time-consuming analysis concepts prevalent in the social sciences."
2020.acl-srw.12.txt,2020,6 Conclusion and Implications,devising suitable methods to resolve broad coreferences across news articles reporting on the same event and estimating the frames of the found instances of wcl bias are at the heart of this research project.
2020.acl-srw.12.txt,2020,6 Conclusion and Implications,"in summary, both everyday news consumers, as well as researchers in the social sciences, could benefit strongly from the automated identification of bias by word choice and labeling (wcl) in news articles."
2020.acl-srw.12.txt,2020,6 Conclusion and Implications,"my vision is that at a later point in time, such methods might be integrated into popular news aggregators, such as google news, helping news readers to explore and understand media bias through their daily news consumption."
2020.acl-srw.12.txt,2020,6 Conclusion and Implications,one primary result of the project will be the first automated approach capable of identifying instances of bias by wcl in a set of news articles reporting on the same event or topic.
2020.acl-srw.13.txt,2020,5 Conclusion and Future Work,it outperforms the existing state-of-the-art unsupervised models.
2020.acl-srw.13.txt,2020,5 Conclusion and Future Work,scar addresses a significant limitation of the unavailability of labeled data for sentence compression.
2020.acl-srw.13.txt,2020,5 Conclusion and Future Work,"since scar learns to drop inferable components of the input and therefore reduces noise, it can be used as a preprocessing step for machine translation and other information retrieval tasks."
2020.acl-srw.14.txt,2020,5 Conclusion,"as expected, the model that uses low-level convolutional features from the cnn model can convey low-level details, such as contrast, texture, and localized area."
2020.acl-srw.14.txt,2020,5 Conclusion,"based on these experiments, we can conclude that the feature differences between images and semantic tags are crucial elements necessary for training."
2020.acl-srw.14.txt,2020,5 Conclusion,"furthermore, improving the accuracy of multiple tag prediction is crucial to deliver semantic facts accurately."
2020.acl-srw.14.txt,2020,5 Conclusion,"in the future, we will strengthen tags that contain semantic information to extract keywords for more accurate information, such as disease information, location, and size."
2020.acl-srw.14.txt,2020,5 Conclusion,"some of our models outperform the conventional image captioning models in terms of bleu score, rouge-l, and cider."
2020.acl-srw.14.txt,2020,5 Conclusion,the mditag(-) model performs best according to every metric.
2020.acl-srw.14.txt,2020,5 Conclusion,we are also considering obtaining more images from hospitals to reduce the proportion of abnormal images in the datasets.
2020.acl-srw.14.txt,2020,5 Conclusion,we propose models that exploit feature differences and tag information.
2020.acl-srw.15.txt,2020,6 Conclusions,"extensive experimental results show that the proposed model is beneficial for the agglutinative language machine translation, and only a small amount of the agglutinative data can improve the translation performance in both directions."
2020.acl-srw.15.txt,2020,6 Conclusions,"in future work, we plan to utilize other word segmentation methods for model training."
2020.acl-srw.15.txt,2020,6 Conclusions,"in this paper, we propose a multi-task neural model for translation task from and into a low-resource and morphologically-rich agglutinative language."
2020.acl-srw.15.txt,2020,6 Conclusions,"moreover, the proposed approach with external monolingual data is more useful for translating into the agglutinative language, which achieves an improvement of +1.42 bleu points for translation from english into turkish and +1.45 bleu points from chinese into uyghur."
2020.acl-srw.15.txt,2020,6 Conclusions,the model jointly learns to perform bi-directional translation and agglutinative language stemming by utilizing the shared encoder and decoder under standard nmt framework.
2020.acl-srw.15.txt,2020,6 Conclusions,we also plan to combine the proposed multi-task neural model with back-translation method to enhance the ability of the nmt model on target-side language modeling.
2020.acl-srw.16.txt,2020,5 Conclusion,a practical difficulty of olm is the approximation with a language model.
2020.acl-srw.16.txt,2020,5 Conclusion,"first, a language model can create syntactically correct data, that does not make sense for the task."
2020.acl-srw.16.txt,2020,5 Conclusion,"furthermore, we show other axioms that olm satisfies."
2020.acl-srw.16.txt,2020,5 Conclusion,"however, we argue that using a language model is a suitable way for finding reference inputs."
2020.acl-srw.16.txt,2020,5 Conclusion,"in our experiments, we compare our methods to other occlusion and gradient explanation methods."
2020.acl-srw.16.txt,2020,5 Conclusion,"in the future, we want to extend this method to language features other than words."
2020.acl-srw.16.txt,2020,5 Conclusion,it is especially suited for word-level relevance of sentence classification with state-of-the-art nlp models.
2020.acl-srw.16.txt,2020,5 Conclusion,"nlp tasks with longer input are probably not very sensitive to single word occlusion, which could be measured with olm-s."
2020.acl-srw.16.txt,2020,5 Conclusion,"second, even state-of-the-art language models do not always produce syntactically correct data."
2020.acl-srw.16.txt,2020,5 Conclusion,"unfortunately, there is no general evaluation for explanation methods."
2020.acl-srw.16.txt,2020,5 Conclusion,"we also introduce the class zero-sum axiom for explanation methods, compare it with an existing axiom."
2020.acl-srw.16.txt,2020,5 Conclusion,"we argue that current black-box and gradient-based explanation methods do not yet consider the likelihood of data and present olm, a novel explanation method, which uses a language model to resample occluded words."
2020.acl-srw.16.txt,2020,5 Conclusion,we argue that with this more solid theoretical foundation olm can be regarded as an improvement over existing nlp classification explanation methods.
2020.acl-srw.16.txt,2020,5 Conclusion,we do not consider these experiments to be exhaustive.
2020.acl-srw.16.txt,2020,5 Conclusion,we show that our method adds value by showing distinctive results and better founded theory.
2020.acl-srw.17.txt,2020,8 Conclusion and Future Work,"analyzing the interaction between the identified ntus and their discourse context, we discover a set of patterns of dtcs, represented by the ntus."
2020.acl-srw.17.txt,2020,8 Conclusion and Future Work,"based on these patterns, we propose a simple classification of ntus in social talk, yet introducing new sequence-based social intents that traditional taxonomies of speech acts do not capture."
2020.acl-srw.17.txt,2020,8 Conclusion and Future Work,"in this paper, we present a pilot annotation study13 as a first step towards a dialogue model which is capable of rationalizing ntus and conversational coherence in social talk."
2020.acl-srw.17.txt,2020,8 Conclusion and Future Work,"next, we aim to develop an actionable bayesian game-theoretic model for social talk, focusing on decomposing its utility function."
2020.acl-srw.17.txt,2020,8 Conclusion and Future Work,"particularly, we seek to learn from social interaction work such as stevanovic and koski (2018) for designing the goal-directedness aspect of the model."
2020.acl-srw.17.txt,2020,8 Conclusion and Future Work,these intents not only adequately account for non-topical coherence in social talk but also convincingly demonstrate social talk as a sophisticated form of goal-directed rational interactions.
2020.acl-srw.17.txt,2020,8 Conclusion and Future Work,"we hypothesize that the bayesian gametheoretic framework, which explicitly models the interactive and rational aspects of social interaction, is a sensible architecture for handling social talk."
2020.acl-srw.18.txt,2020,6 Conclusion,"furthermore, it can be expected that the methodological insights gained in the simulation experiments can inform other approaches investigating non-transparent embedding representations and yield important insights about the behavior of distributional models."
2020.acl-srw.18.txt,2020,6 Conclusion,"i expect that the corpus and insights gathered in this project can be complementary to resources capturing common-sense knowledge explicitly, such as conceptnet (speer et al., 2017) and common sense challenges (e.g.(talmor et al., 2019))."
2020.acl-srw.18.txt,2020,6 Conclusion,"secondly, i propose to interpret the results against the background of a methodological investigation of model analysis methods and the potential of distributional models."
2020.acl-srw.18.txt,2020,6 Conclusion,the linguistic hypotheses to be tested may be falsified.
2020.acl-srw.18.txt,2020,6 Conclusion,"this proposal presents a framework for investigating the semantic content of distributional word representations from two perspectives: firstly, i propose to test linguistic hypotheses about what aspects of conceptual knowledge are represented in natural language."
2020.acl-srw.18.txt,2020,6 Conclusion,"while this would be a negative result, it is still a relevant insight and can be used as a basis for new predictions."
2020.acl-srw.19.txt,2020,7 Conclusion and Future Work,"future work could include finding a way to incorporate other linguistic features like case-markers, gender, number, person, tense, aspect and verb agreement information into the parser."
2020.acl-srw.19.txt,2020,7 Conclusion and Future Work,"we also show that with good vector representations, a small feature set is more effective for a morphologically rich, agglutinative language like telugu."
2020.acl-srw.19.txt,2020,7 Conclusion and Future Work,"we demonstrate that even with vectors trained on a small corpus of 2.6m sentences, we can reduce the need for explicit linguistic features in deep learning based models."
2020.acl-srw.19.txt,2020,7 Conclusion and Future Work,we present a simple yet effective dependency parser for telugu using contextual word representations.
2020.acl-srw.19.txt,2020,7 Conclusion and Future Work,we show based on the results of the parser that bert vectors effectively capture much of the linguistic information required for parsing.
2020.acl-srw.2.txt,2020,4 Summary,the challenge remains in preserving the main plot and generating consistent and meaningful text in the target style.
2020.acl-srw.2.txt,2020,4 Summary,we plan to focus mostly on studying the possible application of gcn in this task.
2020.acl-srw.2.txt,2020,4 Summary,we propose to explore text style transfer on the story level.
2020.acl-srw.2.txt,2020,4 Summary,we will perform extensive experiments and report results in future work.
2020.acl-srw.20.txt,2020,6 Conclusion,"although the state-of-art paraphrase identification models can achieve impressive performance under the pointwise evaluation method, they cannot handle real-world problems and unseen data well and even have worse results than a bow model on simple tasks."
2020.acl-srw.20.txt,2020,6 Conclusion,"this suggests future work to reconsider how to match the training and evaluation to the actual objective of downstream applications, and thus create more reliable evaluation metrics and benchmarks."
2020.acl-srw.20.txt,2020,6 Conclusion,"we examined the relation of semantic equivalence learned by models trained with pointwise approach, and found that they may consider a random sentence as more similar to the query sentence itself."
2020.acl-srw.20.txt,2020,6 Conclusion,we show that the asymmetry in bert can produce inconsistent prediction results when reversing the order of the two sentences.
2020.acl-srw.22.txt,2020,5 Conclusion & Future Work,"however, the performance of multilingual models can easily be enhanced by fine-tuning them on the low-resource language pairs of interest."
2020.acl-srw.22.txt,2020,5 Conclusion & Future Work,"in future, we would like to work on effective techniques to exploit monolingual data and parallel data from other languages together to improve the translation of low-resource languages."
2020.acl-srw.22.txt,2020,5 Conclusion & Future Work,"in this paper, we explore effective methods to exploit parallel data from multiple related languages to improve the translation between indian languages and english."
2020.acl-srw.22.txt,2020,5 Conclusion & Future Work,our experiments show that using a multilingual nmt model as a parent model (consisting of multiple language pairs with related languages either on the source side or on the target side) and fine-tuning it on the low-resource language pair of interest yields an overall average improvement of 5 bleu points over a standard transformer-based nmt baseline.
2020.acl-srw.22.txt,2020,5 Conclusion & Future Work,our proposed multilingual transfer learning approach also outperforms the simple transfer learning approach by a significant amount.
2020.acl-srw.22.txt,2020,5 Conclusion & Future Work,our results show that multilingual learning for translation between indian languages and english is not very effective given the set of data we have.
2020.acl-srw.23.txt,2020,5 Conclusions,"for example, in the odin language, brackets must be paired to produce syntactically valid rules."
2020.acl-srw.23.txt,2020,5 Conclusions,"further, we plan to use this decoder in an iterative, semi-supervised learning scenario akin to co-training (blum and mitchell, 1998)."
2020.acl-srw.23.txt,2020,5 Conclusions,"in the longer term, we envision a decoder with constraints, which enforces that the generated rules follow correct odin syntax."
2020.acl-srw.23.txt,2020,5 Conclusions,"that is, the newly decoded, executable rules can be applied over large, unannotated texts to generate new training examples for the event classifier."
2020.acl-srw.23.txt,2020,5 Conclusions,"this can be enforced with different strategies in the decoder, ranging from constrained greedy decoding to globally optimal solutions that could be implemented with integer linear programming."
2020.acl-srw.23.txt,2020,5 Conclusions,we also showed that the performance of our approach further improves when trained on automatically-labeled data generated by a rule-based system.
2020.acl-srw.23.txt,2020,5 Conclusions,"we evaluated the proposed approach on three biomedical events and demonstrated that the decoder generates interpretable rules, and that the joint training improves the performance of the event classifier."
2020.acl-srw.23.txt,2020,5 Conclusions,"we implemented this approach using an encoder-decoder architecture, where the decoder jointly optimizes the decoding of extraction rules and event classification."
2020.acl-srw.23.txt,2020,5 Conclusions,we introduced an interpretable approach for event extraction that jointly trains an event classifier with a component that translates the classifier’s decisions into interpretable extraction rules.
2020.acl-srw.23.txt,2020,5 Conclusions,we plan to include constraints as part of decoding to aid in rule synthesis.
2020.acl-srw.23.txt,2020,5 Conclusions,we suspect that including such validity constraints will further improve the quality of the decoded rules.
2020.acl-srw.24.txt,2020,5 Conclusion,future qualitative work could also suggest further variables whose inclusion would enhance our knowledge of humor perception.
2020.acl-srw.24.txt,2020,5 Conclusion,humor detection and rating is a multi-faceted problem.
2020.acl-srw.24.txt,2020,5 Conclusion,"this could set a new standard for shared tasks which aim to model humor in future, and could outline a methodology that can be replicated with other cultures and languages."
2020.acl-srw.24.txt,2020,5 Conclusion,"we hope that the inclusion of demographic information will shift the state of the art away from objective classification, towards a more subjective approach."
2020.acl-srw.25.txt,2020,5 Summary,an unsupervised method that effectively aligns bilingual texts will lower the barrier for building high-quality mt systems for low-resource languages and our first results suggest that it may also play a role in improving mt for morphologically rich languages.
2020.acl-srw.25.txt,2020,5 Summary,"the motivation for this research is to improve the quality of machine translations by making better use of and increasing the quality of parallel training data, especially in regard to sparse data scenarios."
2020.acl-srw.25.txt,2020,5 Summary,we have given an overview of the literature on sentence alignment and parallel corpus filtering.
2020.acl-srw.25.txt,2020,5 Summary,we outlined challenges associated with implementing these methods for low-resource and morphologically rich languages and proposed initial experiments to tackle these challenges.
2020.acl-srw.26.txt,2020,6 Conclusion,"in this paper, we describe a first effort at annotating points of correspondence between disparate sentences."
2020.acl-srw.26.txt,2020,6 Conclusion,"our findings shed light on the importance of modeling points of correspondence, suggesting important future directions for sentence fusion."
2020.acl-srw.26.txt,2020,6 Conclusion,the dataset fills a notable gap of coreference resolution and summarization research.
2020.acl-srw.26.txt,2020,6 Conclusion,"we present a benchmark dataset comprised of the documents, source and fusion sentences, and human annotations of points of correspondence between sentences."
2020.acl-srw.27.txt,2020,7 Conclusions,"experimental results on massive twitter dialogue data revealed that υbleu is comparable to human-aided ∆bleu, and that, by integrating it into ruber, the state of the art method for evaluating open-domain dialogue systems, we can improve the correlation with human judgment."
2020.acl-srw.27.txt,2020,7 Conclusions,"our proposed υbleu rates diverse reference responses retrieved from massive dialogue logs by using a neural network trained with automatically-collected training data, and it uses the responses and the scores to run ∆bleu."
2020.acl-srw.27.txt,2020,7 Conclusions,"we have proposed a method to remove the need for costly human judgment in ∆bleu (galley et al., 2015) and obtain an automatic uncertainty-aware metric for dialogue systems."
2020.acl-srw.27.txt,2020,7 Conclusions,we will release all code and datasets (tweet ids) to promote the reproducibility of our experiments.4 the readers are referred to our code to evaluate their dialogue systems for their native languages.
2020.acl-srw.28.txt,2020,5 Conclusion,"additionally, the process of building an fst proves to be a great way to examine the validity of the linguistic analyses."
2020.acl-srw.28.txt,2020,5 Conclusion,"assuming that either segmentation is linguistically possible, if the size of the transducer is of concern (as a result of the size of lexicon, complexity of rules or sheer number of rules) a ‘chunking’ approach can be taken with no cost to accuracy."
2020.acl-srw.28.txt,2020,5 Conclusion,both models achieve the same accuracy of 80.3%.
2020.acl-srw.28.txt,2020,5 Conclusion,"future work would entail analysing and implementing more detailed underlying morphonological rules, and investigating the cross-over from fsts to neural models."
2020.acl-srw.28.txt,2020,5 Conclusion,"if the user, prefers structural granularity or a one-to-one mapping between the computational implementation and the linguistic grammar then the decomposition approach can be taken."
2020.acl-srw.28.txt,2020,5 Conclusion,"most often, the primary use of fst grammars are to provide morphological glosses, in this case there is no computational motivation for having a high resolution description."
2020.acl-srw.28.txt,2020,5 Conclusion,nen shows distributed exponence; multiple morphs can contribute to the specification of a particular feature value.
2020.acl-srw.28.txt,2020,5 Conclusion,"one of the prime motivations for building an fst, in the era of neural networks is to generate enough labelled data, in the appropriate format to enable testing across architectures."
2020.acl-srw.28.txt,2020,5 Conclusion,the choice of model depends on the primary concern of the user.
2020.acl-srw.28.txt,2020,5 Conclusion,this paper explores options for modeling the low-resource language nen using finite-state transducers.
2020.acl-srw.28.txt,2020,5 Conclusion,"this property motivates the comparison between a ‘chunking’ model, which combines the thematic and desinence segment, to a decomposition model which handles the two separately at the cost of many more parameters."
2020.acl-srw.29.txt,2020,6 Conclusion,"aradic’s image-based character embedding strategy eliminated the need for complicated preprocessing, segmentation and morphological analysis, and achieved much better performance than conventional deep and classical text classification techniques that use word and character-based embeddings."
2020.acl-srw.29.txt,2020,6 Conclusion,"in this paper, we proposed a novel end-to-end arabic text classification framework aradic."
2020.acl-srw.29.txt,2020,6 Conclusion,"we also published two large scale arabic text classification datasets that contain the three types of arabic language, the awt and the arap datasets."
2020.acl-srw.29.txt,2020,6 Conclusion,we have shown also that class-balanced loss is useful for text classification tasks with long tailed distribution datasets.
2020.acl-srw.3.txt,2020,5 Conclusion,our future work will target demonstrating the method on other languages.
2020.acl-srw.3.txt,2020,5 Conclusion,"the work presented in this paper is heavily inspired by (le et al., 2017), but differs and improves it in the following ways."
2020.acl-srw.3.txt,2020,5 Conclusion,"we also hope to address semantic paraphasia in future work and create, deploy aac systems building on the method proposed in this paper."
2020.acl-srw.3.txt,2020,5 Conclusion,we lay the ground-work for paraphasia classification in low-resource languages allowing for development of asr and aac systems for not only english-speaking pwa’s but also pwa’s in developing nations.
2020.acl-srw.3.txt,2020,5 Conclusion,we provide a completely unsupervised method which outperforms previous work in paraphasia classification and detection.
2020.acl-srw.3.txt,2020,5 Conclusion,"while we maintain that our method can be used for all languages, irrespective of aphasic speech data, due to time constraints we could include only english in our evaluations."
2020.acl-srw.30.txt,2020,5 Conclusion,"also, we plan to extend the simple label embedding calculation methods to more sophisticated ones."
2020.acl-srw.30.txt,2020,5 Conclusion,"for future work, we envision to apply our method to other tasks and datasets and investigate the effectiveness."
2020.acl-srw.30.txt,2020,5 Conclusion,"through experiments on english and japanese fine-grained ner, we demonstrated that our proposed method improves the performance, especially for instances with low-frequency labels."
2020.acl-srw.30.txt,2020,5 Conclusion,we proposed a method that shares and learns the embeddings of label components.
2020.acl-srw.31.txt,2020,7 Conclusion,"in this paper, we built a japanese wikipedia typo dataset (jwtd) which contains over half a million typo–correction sentence pairs obtained from wikipedia’s revision history."
2020.acl-srw.31.txt,2020,7 Conclusion,"it is also interesting to apply our methods to other languages requiring word segmentation, most notably, chinese."
2020.acl-srw.31.txt,2020,7 Conclusion,"to the best of our knowledge, jwtd is the first freely available large japanese typo dataset."
2020.acl-srw.31.txt,2020,7 Conclusion,we classified japanese typos into four categories and presented mining procedures for each of them.
2020.acl-srw.31.txt,2020,7 Conclusion,we evaluated jwtd using crowdsourcing and built a baseline typo correction system on top of it.
2020.acl-srw.31.txt,2020,7 Conclusion,"while the focus of this paper was on data construction, developing a higher-quality typo correction system is the future direction to pursue."
2020.acl-srw.32.txt,2020,6 Conclusion and Future Work,"however, some issues remain, for example, how to determine the effective threshold τ that can strictly guarantee zero cse is still unknown."
2020.acl-srw.32.txt,2020,6 Conclusion and Future Work,"in this paper, we introduced a new formulation of the sas task to evaluate the effectiveness of the sas systems in actual usage."
2020.acl-srw.32.txt,2020,6 Conclusion and Future Work,"moreover, we must develop a method for more accurately estimating the confidence scores, which is our primary focus in the next step."
2020.acl-srw.32.txt,2020,6 Conclusion and Future Work,our study revealed some potential for a better task formulation of sas that links to actual usage.
2020.acl-srw.32.txt,2020,6 Conclusion and Future Work,"the experimental results show that by using our proposed procedure of selecting reliable predictions, sas systems can predict scores with zero cse for approximately 50% of test data at maximum."
2020.acl-srw.32.txt,2020,6 Conclusion and Future Work,"then, we formulate the objective of the task to obtain as many predictions without cse as possible."
2020.acl-srw.32.txt,2020,6 Conclusion and Future Work,this is one major challenge regarding our formulation.
2020.acl-srw.32.txt,2020,6 Conclusion and Future Work,"this result directly indicates the possibility of reducing half scoring cost of human raters, which, we believe, is highly preferable for the evaluation of sas systems."
2020.acl-srw.32.txt,2020,6 Conclusion and Future Work,"we defined the concept of a critical scoring error (cse), which represents unacceptable prediction errors."
2020.acl-srw.33.txt,2020,7 Conclusion,"as shown by mccoy et al.(2020); kuncoro et al.(2018), explicitly modeling hierarchical structure helps to discover non-local structural dependencies."
2020.acl-srw.33.txt,2020,7 Conclusion,"from the cognitive neuroscience perspective, it would be interesting to investigate if the proposed decay rnn can capture some aspects of actual neuronal behaviour and language cognition."
2020.acl-srw.33.txt,2020,7 Conclusion,"in this paper, we proposed the decay rnn, a bioinspired recurrent network that emulates the decaying nature of neuronal activations after receiving excitatory and inhibitory impulses from upstream neurons."
2020.acl-srw.33.txt,2020,7 Conclusion,"our results here do at least indicate that the complex gating mechanisms of lstms (whose cognitive plausibility has not been established) may not be essential to their performance on many linguistic tasks, and that simpler and perhaps more cognitively plausible rnn architectures are worth exploring further as psycholinguistic models."
2020.acl-srw.33.txt,2020,7 Conclusion,"recently, maheswaranathan and sussillo (2020) showed the existence of a line attractor in the dynamics of the hidden states for sentiment classification."
2020.acl-srw.33.txt,2020,7 Conclusion,"the contrast in the performance of the language models encourages us to look at the inductive biases, which might have led to better syntactic generalization in certain cases."
2020.acl-srw.33.txt,2020,7 Conclusion,"thus, similar dynamical-system-based analysis can be extended to our settings to further understand the working of the decay rnn."
2020.acl-srw.33.txt,2020,7 Conclusion,we have found that the balance between the free term (h(t)) and the coupled term (wh(t)) enabled the model to capture syntax-level dependencies.
2020.acl-srw.34.txt,2020,5 Conclusion,for the future we would like to apply our model on other cross-lingual nlp tasks such as xnli or cross-lingual semantic textual similarity.
2020.acl-srw.34.txt,2020,5 Conclusion,"interestingly, targeting only one language pair during the fine-tuning phase suffices to propagate the alignment improvement to unrelated languages."
2020.acl-srw.34.txt,2020,5 Conclusion,it is therefore not necessary to build a working mt system for every language pair we wish to mine.
2020.acl-srw.34.txt,2020,5 Conclusion,our sentence embeddings yield significantly better results on the tasks of parallel data mining and parallel sentence matching than our unsupervised baselines.
2020.acl-srw.34.txt,2020,5 Conclusion,"since the synthetic translations were obtained from an unsupervised mt system, the entire procedure requires no authentic parallel sentences for training."
2020.acl-srw.34.txt,2020,5 Conclusion,the average f1 margin across four language pairs on the bucc task is ∼17 points over the original xlm model and ∼7 on the news dataset where only one of the evaluated language pairs was seen during fine-tuning.
2020.acl-srw.34.txt,2020,5 Conclusion,"the gain in accuracy in parallel sentence matching across 8 language pairs is 7.2% absolute, lagging only 7.1% absolute behind supervised methods."
2020.acl-srw.34.txt,2020,5 Conclusion,we proposed a completely unsupervised method to train multilingual sentence embeddings which can be used for building a parallel corpus with no previous translation knowledge.
2020.acl-srw.34.txt,2020,5 Conclusion,we show that fine-tuning an unsupervised multilingual model with a translation objective using as little as 20k synthetic translation pairs can significantly enhance the cross-lingual alignment of its representations.
2020.acl-srw.35.txt,2020,4 Conclusion,"in future work, we will extend our analysis to cover the more complex constructions mentioned in section 3."
2020.acl-srw.35.txt,2020,4 Conclusion,"in this study, we presented an end-to-end logic-based inference system for handling complex inferences with comparatives, quantifiers, and numerals."
2020.acl-srw.35.txt,2020,4 Conclusion,the entire system is transparently composed of several modules and can solve complex inferences for the right reason.
2020.acl-srw.35.txt,2020,4 Conclusion,"we are also considering combining our system with an abduction mechanism that uses large knowledge bases (yoshikawa et al., 2019) for handling commonsense reasoning with external knowledge."
2020.acl-srw.36.txt,2020,6 Conclusion and Future Work,"in the future, we plan to introduce constraints for asymmetric relations as well as extend our proposed method to leverage them."
2020.acl-srw.36.txt,2020,6 Conclusion and Future Work,"in this work, we presented a method to perform semantic specialization of word vectors."
2020.acl-srw.36.txt,2020,6 Conclusion and Future Work,"moreover, we improved a state-of-the-art post-specialization method by incorporating adversarial losses with the wasserstein distance."
2020.acl-srw.36.txt,2020,6 Conclusion and Future Work,"moreover, we plan to experiment with adapting our model to a multilingual scenario, to be able to use it in a neural machine translation task."
2020.acl-srw.36.txt,2020,6 Conclusion and Future Work,"our results obtained in an intrinsic and an extrinsic task, suggest that our method yields performance gains over current methods."
2020.acl-srw.36.txt,2020,6 Conclusion and Future Work,"specifically, we compiled a new set of constraints obtained from babelnet."
2020.acl-srw.36.txt,2020,6 Conclusion and Future Work,we make the code and resources available at: https://github.com/ mbiesialska/wgan-postspec
2020.acl-srw.37.txt,2020,6 Conclusion,"even if monolingual corpora for the languages of interest are unavailable, we can successfully improve translation quality by up to 8.5 bleu, in low-resource settings, using monolingual corpora of assisting languages."
2020.acl-srw.37.txt,2020,6 Conclusion,"in the future, we plan to experiment with even more challenging language pairs such as japanese–russian and attempt to leverage monolingual corpora belonging to diverse language families.we might be able to identify subtle relationships among languages and approaches to better leverage assisting languages for several nlp tasks."
2020.acl-srw.37.txt,2020,6 Conclusion,in this paper we showed that it is possible to leverage monolingual corpora of other languages to pretrain nmt models for language pairs that lack parallel as well as monolingual data.
2020.acl-srw.37.txt,2020,6 Conclusion,we showed that the similarity between the other (assisting) languages and the languages to be translated is crucial and leveraged script mapping wherever possible.
2020.acl-srw.38.txt,2020,6 Conclusions and Future Work,"in this paper, we introduce a method of selecting an n -best list for nmt systems and propose a way of reranking to the generated hypotheses from the system."
2020.acl-srw.38.txt,2020,6 Conclusions and Future Work,introducing language models during reranking could establish a tradeoff between perplexity and the scores to the hypotheses generated.
2020.acl-srw.38.txt,2020,6 Conclusions and Future Work,it is another promising area to be looked upon for reranking.
2020.acl-srw.38.txt,2020,6 Conclusions and Future Work,language models are used for getting the likelihood of sentences and is a widely used concept for reranking hypotheses.
2020.acl-srw.38.txt,2020,6 Conclusions and Future Work,one can investigate our approach with varying beam sizes and analyzing the effect of length penalty wu et al.(2016) and comparing it with methods such as yang et al.(2018).
2020.acl-srw.38.txt,2020,6 Conclusions and Future Work,we also look forward to coming up with better reranking ways that are closer to the oracle scores and investigate the efficacy of the approach in low-resourced data conditions.
2020.acl-srw.38.txt,2020,6 Conclusions and Future Work,we also plan to explore the work by c¸aglar gu¨lc¸ehre et al.(2017) and c¸aglar gu¨lc¸ehre et al.(2015) that introduces language models into the existing neural architecture with methods such as shallow fusion and deep fusion.
2020.acl-srw.38.txt,2020,6 Conclusions and Future Work,we observe that our approach is giving better results over the baseline model by following the proposed reranking method and is also evaluated with the coverage penalty.
2020.acl-srw.39.txt,2020,6 Conclusion,"in the future, it would be interesting to explore weak suervision and other data augmentation techniques to improve models’ robustness further."
2020.acl-srw.39.txt,2020,6 Conclusion,we also motivate the use of manifold mixup for further improvement.
2020.acl-srw.39.txt,2020,6 Conclusion,we motivate the use of m-bert for disaster-related tweet classification and we demonstrate its strong performance on unseen disasters and languages.
2020.acl-srw.39.txt,2020,6 Conclusion,we present a way to aggregate prior disaster-related resources to compile a large scale tweet dataset for multi-label classification utilizing both multi-class classes and binary classes.
2020.acl-srw.4.txt,2020,5 Conclusion,"in the field of biomedicine, the co-occurrence relationship of tags is very common and useful."
2020.acl-srw.4.txt,2020,5 Conclusion,modelling the relationship between mesh terms is a key issue in mesh indexing.
2020.acl-srw.4.txt,2020,5 Conclusion,this paper proposes a model for constructing specifying the relationship between mesh terms based on gcn and a new end-to-end model for mesh indexing.
2020.acl-srw.4.txt,2020,5 Conclusion,"we use the co-occurrence relationship between tags to design the adjacency matrix by the gcn using the data-driven method, which can also be extended to other extreme multi-label classification fields."
2020.acl-srw.40.txt,2020,4 Discussion and Conclusion,"besides, while sentiment analysis requires the understandings of compositionality, models trained on linguistic tasks may better capture syntactic information."
2020.acl-srw.40.txt,2020,4 Discussion and Conclusion,"firstly, as discussed in sec 2.6, the connection between the structure induction through machine interpretation algorithm and construction grammar remains a question — whether what is semantically important for sentiment analysis is necessarily reflected in the syntax and the way the syntactic constituents are formed in the language?"
2020.acl-srw.40.txt,2020,4 Discussion and Conclusion,"for future work, we consider conducting the same experiments on cola, a dataset for judging the grammatical acceptability of a sentence (warstadt et al., 2019)."
2020.acl-srw.40.txt,2020,4 Discussion and Conclusion,"in this work, we extract trees from lstm by an interpretation algorithm — agglomerative contextual decomposition (acd)."
2020.acl-srw.40.txt,2020,4 Discussion and Conclusion,"moreover, it is unclear whether models truly learned compositionality or just overfit to some spurious patterns of the dataset, as recent works have demonstrated that a well-performing natural language inference model completely fails on challenging cases generated by syntactic transformations (mccoy et al., 2019)."
2020.acl-srw.40.txt,2020,4 Discussion and Conclusion,"nonetheless, we conclude with encouragement for the community to look deeper into interpretation-based methods and their connections with semantic and syntactic theory in linguistics."
2020.acl-srw.40.txt,2020,4 Discussion and Conclusion,the generated trees also do not seem to provide more computational improvement when we train a recursive neural network leveraging the structure to predict the final label.
2020.acl-srw.40.txt,2020,4 Discussion and Conclusion,these negative observations can result from several possible reasons.
2020.acl-srw.40.txt,2020,4 Discussion and Conclusion,we show empirically that the generated trees are not similar to the trees produced from formal syntactic theory.
2020.acl-srw.41.txt,2020,7 Conclusions,"a contribution of this work is that we have created a new dataset containing nearly 5,000 question tweets labeled as rhetorical or informationseeking coupled with their prior tweets."
2020.acl-srw.41.txt,2020,7 Conclusions,our hope is that this work will lead to further research on the role of context for recognizing rhetorical and informationseeking questions in social media.
2020.acl-srw.41.txt,2020,7 Conclusions,"to our knowledge, this is the first twitter-based dataset for studying rhetorical questions that has both humangenerated gold labels and includes prior context for each question."
2020.acl-srw.41.txt,2020,7 Conclusions,"we also presented classification models to benchmark performance on this task, and showed that including the tweet prior to a question improves performance."
2020.acl-srw.41.txt,2020,7 Conclusions,"we also showed several ways to capture topic information, and that topic information represented in the preceding context seems to be useful for this task."
2020.acl-srw.42.txt,2020,5 Conclusion,"in this work we attempt to operationalize an intuition from cognitive science, implementing it as inductive bias in the form of a factorization between alignment and translation in the seq2seq setting."
2020.acl-srw.42.txt,2020,5 Conclusion,"we believe this factorization prevents the model from memorizing spurious correlations in the data, and note that similar ideas may be useful in other natural language tasks."
2020.acl-srw.42.txt,2020,5 Conclusion,"we showed that this can improve compositional generalization performance on the scan task, and that it doesn’t degrade performance on a small mt task."
2020.acl-srw.43.txt,2020,6 Conclusion and Future Work,"in this work, we established the need to devise a computationally effective method to identify victim blaming language on twitter."
2020.acl-srw.43.txt,2020,6 Conclusion and Future Work,"on a manually annotated dataset, our proposed approach could achieve significant improvement over existing methods that rely on custom textual features and popular deep learning based methods."
2020.acl-srw.43.txt,2020,6 Conclusion and Future Work,our future agenda includes further bifurcating and exploring the specific types of victim blaming and the efficacy of the proposed approach on such a multi label classification task.
2020.acl-srw.43.txt,2020,6 Conclusion and Future Work,"our work, therefore presents an extensive study of popular text classification methods on a niche’ dataset with victim blaming semantics and further presents the significance of using a simple transfer learning approach to capture twitter semantics on a limited dataset."
2020.acl-srw.43.txt,2020,6 Conclusion and Future Work,the prevalence of rape culture and the subsequent victim blaming on unsolicited social media forums like twitter has not been studied from a computational linguistic perspective before.
2020.acl-srw.43.txt,2020,6 Conclusion and Future Work,"to achieve this, we proposed a single step transfer learning based classification method that effectively captures the unique linguistic structures of twitter data and victim blaming language."
2020.acl-srw.43.txt,2020,6 Conclusion and Future Work,we anticipate that this study encourages further research on how victims of sexual assault are portrayed on social media.
2020.acl-srw.43.txt,2020,6 Conclusion and Future Work,we plan to explore the different weighting factors for the language modelling loss and classification loss described in section 4 to determine if weighting factors can help customize the auxiliary loss for different tasks.
2020.acl-srw.5.txt,2020,6 Conclusions,"based on our experimental results, we observed that the recall for error types considered in our study improved or were comparable."
2020.acl-srw.5.txt,2020,6 Conclusions,"in particular, we conﬁrmed that combining data selection and realistic error injection approaches to obtain pseudo data improved the f0.5 scores."
2020.acl-srw.5.txt,2020,6 Conclusions,"in this study, we studied the effect of pseudo data obtained using two approaches."
2020.acl-srw.5.txt,2020,6 Conclusions,"moreover, we analyzed the recall for each error type."
2020.acl-srw.6.txt,2020,5 Summary,"in this paper, we pointed out the current key challenges and unsolved problems: 1) going beyond the conventional way of hard-sharing in multi-task learning and finding the most useful architecture for a given setting, 2) finding good auxiliary tasks in a multi-task setting for a specific target task, and 3) finding useful pretraining schemes."
2020.acl-srw.6.txt,2020,5 Summary,it helps achieve better generalization and utilization of the training datasets.
2020.acl-srw.6.txt,2020,5 Summary,our research aims to apply the current work on transfer learning to new tasks and also find novel methods to obtain better multi-task learning models.
2020.acl-srw.6.txt,2020,5 Summary,transfer learning is a promising area of research for deep neural network based machine learning models.
2020.acl-srw.7.txt,2020,6 Conclusion,"in this paper, we propose rpd, a metric to quantify the distance between embedding spaces (i.e different sets of word embeddings)."
2020.acl-srw.7.txt,2020,6 Conclusion,"justifying rpd theoretically and empirically, we believe rpd can offer us a new perspective to understand and compare word embeddings."
2020.acl-srw.7.txt,2020,6 Conclusion,"with the help of rpd and its properties, we verify some intuitions and answer some questions."
2020.acl-srw.8.txt,2020,7 Conclusion,the experimental results showed that the proposed method outperforms analogy-based and mlp baselines in transfer accuracy for attribute words and stability for non-attribute words.
2020.acl-srw.8.txt,2020,7 Conclusion,"the proposed method transfers binary word attributes using reflection-based mappings and keeps non-attribute words unchanged, without attribute knowledge in inference time."
2020.acl-srw.8.txt,2020,7 Conclusion,"this research aims to transfer word binary attributes (e.g., gender) for applications such as data augmentation of a sentence."
2020.acl-srw.8.txt,2020,7 Conclusion,"we can transfer the word attribute with analogy of word vectors, but it requires explicit knowledge whether the input word has the attribute or not (e.g., man ∈ gender, woman ∈ gender, person /∈ gender)."
2020.acl-srw.9.txt,2020,5 Discussion and conclusion,"after reviewing the problem, we proposed an approach to building topic models, able to maintain relatively high imbalance degree."
2020.acl-srw.9.txt,2020,5 Discussion and conclusion,in this paper we discussed the problem of training topic models with unbalanced text collections.
2020.acl-srw.9.txt,2020,5 Discussion and conclusion,learning an unbalanced topic model from unbalanced text collection is a non-trivial task for all of the existing modelling methods.
2020.acl-srw.9.txt,2020,5 Discussion and conclusion,no previous research provides a thorough analysis of this problem or an efficient training procedure for unbalanced models.
2020.acl-srw.9.txt,2020,5 Discussion and conclusion,we described our approach in terms of plsa regularization and brought theoretical justification for the rtopicprior regularizer.
2020.emnlp-main.1.txt,2020,7 Conclusion,"based on these findings we demonstrated that machine learning models can automatically detect attackable sentences, comparably well to laypeople."
2020.emnlp-main.1.txt,2020,7 Conclusion,"further, argumentation structure (support relations between sentences or lack thereof) might provide useful information about each sentences attackability."
2020.emnlp-main.1.txt,2020,7 Conclusion,"our work contributes a new application to the growing literature on causal inference from text (egami et al., 2018), in the setting of text as a treatment."
2020.emnlp-main.1.txt,2020,7 Conclusion,"our work could be improved also by including discourse properties (coherence, cohesiveness)."
2020.emnlp-main.1.txt,2020,7 Conclusion,"specifically, our findings in section 5 pave the way towards answering the causal question: would attacking a certain type of sentence (e.g., questions or expressions of confusion) in an argument increase the probability of persuading the opinion holder?"
2020.emnlp-main.1.txt,2020,7 Conclusion,"using online arguments, we demonstrated that a sentences attackability is associated with many of its characteristics regarding its content, proposition types, and tone, and that kialo provides useful information about attackability."
2020.emnlp-main.1.txt,2020,7 Conclusion,we leave this analysis to future work.
2020.emnlp-main.1.txt,2020,7 Conclusion,we studied how to detect attackable sentences in arguments for successful persuasion.
2020.emnlp-main.1.txt,2020,7 Conclusion,"while our findings suggest initial hypotheses about the characteristics of sentences that can be successfully attacked, establishing causality in a credible manner would require addressing confounders, such as the challengers reputation (manzoor et al., 2020) and persuasive skill reflected in their attack (tan et al., 2014)."
2020.emnlp-main.10.txt,2020,6.5 Future Directions,a future extension could explore more robust techniques for identifying abstract chains which do not make such assumptions.
2020.emnlp-main.10.txt,2020,6.5 Future Directions,"additionally, in the current work we have explored only a sequence of two sentences as an explanation for the third."
2020.emnlp-main.10.txt,2020,6.5 Future Directions,"although much of nlp has focused on qa scores, more recent work has targeted explanation as an end-goal in itself, with ultimate benefits for tutoring, validation, and trust."
2020.emnlp-main.10.txt,2020,6.5 Future Directions,extending the proposed approaches for longer chains is an important future direction.
2020.emnlp-main.10.txt,2020,6.5 Future Directions,"nonetheless, a useful future direction is exploring answer prediction and explanation prediction as joint goals, and perhaps they can benefit each other."
2020.emnlp-main.10.txt,2020,6.5 Future Directions,"the main purpose of this dataset is to generate explanations as an end-goal in itself, rather than improve qa scores (we do not make any claims in terms of qa accuracy or ability to improve qa scores)."
2020.emnlp-main.10.txt,2020,6.5 Future Directions,this technique makes assumptions about being able to match overlapping words.
2020.emnlp-main.10.txt,2020,6.5 Future Directions,we have proposed a technique for reducing reasoning chains to abstract chains.
2020.emnlp-main.100.txt,2020,4 Conclusion,"accuracy of bilingual lexicon induction decays for low-frequency words, as indicated by two factors: (1) diminishing margin between cosine similarities, and (2) exacerbated hubness."
2020.emnlp-main.100.txt,2020,4 Conclusion,experimental results validate their effectiveness.
2020.emnlp-main.100.txt,2020,4 Conclusion,two methods are proposed to address each factor.
2020.emnlp-main.101.txt,2020,6 Concluding Remarks,"in fact, the applicability of rrt can be generalized beyond dirichlet distributions."
2020.emnlp-main.101.txt,2020,6 Concluding Remarks,"in this paper, rounded reparameterization trick, or rrt, is shown as an effective and efficient reparameterization method for dirichlet distributions in the context of learning vae based lda models."
2020.emnlp-main.101.txt,2020,6 Concluding Remarks,successes in these investigations will certainly extend the applicability of vae to much broader application domains and model families.
2020.emnlp-main.101.txt,2020,6 Concluding Remarks,this is because any distribution can be reparameterized to an “rrt form” as long as a sampling algorithm exists for that distribution.
2020.emnlp-main.101.txt,2020,6 Concluding Remarks,thus it will be interesting to investigate the performance of rrt in other applications of vae beyond topic modelling.
2020.emnlp-main.102.txt,2020,6 Conclusion,"extensive experiments on six datasets demonstrate that our method outperforms state-of-the-art calibration methods in terms of expected calibration error, misclassification detection and ood detection."
2020.emnlp-main.102.txt,2020,6 Conclusion,our method imposes two new regularizers using generated on- and off- manifold samples to improve both in-distribution and out-of-distribution calibration.
2020.emnlp-main.102.txt,2020,6 Conclusion,we have proposed a regularization method to mitigate miscalibration of fine-tuned language models from a data augmentation perspective.
2020.emnlp-main.103.txt,2020,7 Conclusion,"future work includes using these approaches to induce model structure, develop accurate models with better interpretability, and to apply these approaches in lower data regimes."
2020.emnlp-main.103.txt,2020,7 Conclusion,"hmms are a useful class of probabilistic models with different inductive biases, performance characteristics, and conditional independence structure than rnns."
2020.emnlp-main.103.txt,2020,7 Conclusion,"in order to scale, we introduce three techniques: a blocked emission constraint, a neural parameterization, and state dropout, which lead to an hmm that outperforms n-gram models and prior hmms."
2020.emnlp-main.103.txt,2020,7 Conclusion,"once scaled up to take advantage of modern hardware, very large hmms demonstrate meaningful improvements over smaller hmms."
2020.emnlp-main.103.txt,2020,7 Conclusion,"this work demonstrates methods for effectively scaling hmms to large state spaces on parallel hardware, and shows that this approach results in accuracy gains compared to other hmm models."
2020.emnlp-main.104.txt,2020,6 Conclusion,"in this paper, we conduct a comprehensive study on how to code textual inputs from multiple linguistically-motivated perspectives and how to integrate alternative language representations into nn-nlp systems."
2020.emnlp-main.104.txt,2020,6 Conclusion,"our approach appears to be very useful and achieves up to 20.77%, 20%, and 15.79% relative improvements on state-of-the-art models of mt, lm, and pos, respectively."
2020.emnlp-main.104.txt,2020,6 Conclusion,our paradigm is general for any language and adaptable to various models.
2020.emnlp-main.104.txt,2020,6 Conclusion,we conduct extensive experiments on five languages over six tasks.
2020.emnlp-main.104.txt,2020,6 Conclusion,"we propose to use soundex, nysiis, metaphone, logogram, fixed-outputlength, and huffman codings into nlp and describe how to combine them in state-of-the-art nn architectures, such as transformer, convs2s, bilstm with attentions."
2020.emnlp-main.105.txt,2020,6 Conclusion,"stateof-the-art performance on zest is 12%, leaving much room for future improvement."
2020.emnlp-main.105.txt,2020,6 Conclusion,the dataset is designed to test models’ ability to systematically generalize across four different areas.
2020.emnlp-main.105.txt,2020,6 Conclusion,"this is an interesting avenue for future work, for which zest should also be useful."
2020.emnlp-main.105.txt,2020,6 Conclusion,"to facilitate future work, we make our models, code, and data available at https://allenai.org/data/ zest."
2020.emnlp-main.105.txt,2020,6 Conclusion,"to make progress toward this goal, we create a dataset, zest, that rigorously evaluates how well a model truly understands each task."
2020.emnlp-main.105.txt,2020,6 Conclusion,"we introduced a framework for creating general purpose nlp systems that can solve tasks from natural language descriptions, synthesizing and extending previous work in zero-shot learning."
2020.emnlp-main.105.txt,2020,6 Conclusion,"while we have been focused on zero shot learning from task descriptions, our framework also permits few-shot scenarios where a task description is given along with a handful of examples, making meta-learning approaches applicable."
2020.emnlp-main.106.txt,2020,5 Conclusion,"a large-scale chinese weibo dataset is built containing trending hashtags, emotion votes, and user comments (for context modeling)."
2020.emnlp-main.106.txt,2020,5 Conclusion,"in experiments, we have shown that the prediction of social emotions is challenging and the modeling of user comments may usefully bridge topic descriptions and public emotions."
2020.emnlp-main.106.txt,2020,5 Conclusion,we have investigated social emotions to online discussion topics.
2020.emnlp-main.107.txt,2020,5 Conclusion,experiments conducted on three benchmark datasets in english and chinese show that our model outperforms previous studies and achieves the new state-of-the-art result.
2020.emnlp-main.107.txt,2020,5 Conclusion,"in this paper, we proposed a neural-based approach to enhance social media ner with semantic augmentation to alleviate data sparsity problem."
2020.emnlp-main.107.txt,2020,5 Conclusion,"particularly, an attentive semantic augmentation module is suggested to encode semantic information and a gate module is applied to aggregate such information to tagging process."
2020.emnlp-main.108.txt,2020,5 Conclusion,experiments on two benchmarks show the effectiveness of our single-task and multi-task learning methods.
2020.emnlp-main.108.txt,2020,5 Conclusion,"in this paper, we first examined the limitations of existing approaches to stance classification (sc) and rumor verification (rv)."
2020.emnlp-main.108.txt,2020,5 Conclusion,"to tackle these limitations, we first proposed a single-task model (i.e., hierarchical transformer) for sc and rv, followed by designing a multi-task learning framework with a coupled transformer module to capture intertask interactions and a post-level attention layer to use stance distributions for the rv task."
2020.emnlp-main.109.txt,2020,7 Discussion,(2) focusing on one of the most important crises of the future: water;
2020.emnlp-main.109.txt,2020,7 Discussion,• unseen attribution factor: our model can generalize to unseen attributions factors.
2020.emnlp-main.109.txt,2020,7 Discussion,a human inspection of randomly sampled 200 comments aligns with out classifier’s predictions.
2020.emnlp-main.109.txt,2020,7 Discussion,a human inspection of randomly sampled 200 comments aligns with the classifier predictions.
2020.emnlp-main.109.txt,2020,7 Discussion,"among the attributed comments, we find public water wastage, pollution, and overpopulation are considered as primary causes for this crisis."
2020.emnlp-main.109.txt,2020,7 Discussion,and (3) release an annotated data set on this important domain.
2020.emnlp-main.109.txt,2020,7 Discussion,"as shown in figure 3, we find that nearly 80% of the discussions in our corpus do not contain any attributions."
2020.emnlp-main.109.txt,2020,7 Discussion,figure 3: distribution of number of comments detected by mfinal bertindian model on 40k comments.• the big picture: we finally run our classifier on our initial data set of 40k english comments to obtain a bigger picture.
2020.emnlp-main.109.txt,2020,7 Discussion,"for instance, with a new dummy attribution factor pandemic and input sentence ‘this flu caused the water crisis’, our model is able to predict pandemic with the highest probability."
2020.emnlp-main.109.txt,2020,7 Discussion,"in the expanding reach of social media, we thus (1) present a new approach to collect aggregated opinions on crisis attribution and complement surveys;"
2020.emnlp-main.109.txt,2020,7 Discussion,"on a data set of 5,000 comments randomly sampled from 503 youtube relevant to the flint water crisis (butler et al., 2016), our model predicts government inaction, pollution (subsumes contamination according to table 4), and corruption."
2020.emnlp-main.109.txt,2020,7 Discussion,table 8 presents a random sample of example comments detected by our classifier.
2020.emnlp-main.109.txt,2020,7 Discussion,"these insights, along with sample comments from the detected attributions, may provide a holistic view of people’s opinion around the topic."
2020.emnlp-main.109.txt,2020,7 Discussion,this aligns with our previous annotation experiment that yielded 24.3% positives from randomly sampled comments.
2020.emnlp-main.109.txt,2020,7 Discussion,this merits a deeper exploration with a holdout attribution set we aim to investigate in future.• flint water crisis: we were curious to know how our model performs in the wild on a data set of a different water crisis.
2020.emnlp-main.109.txt,2020,7 Discussion,"to this end, we zero in on the flint water crisis, another major water crisis happening in a completely different part of the globe with predominantly different sets of attribution factors."
2020.emnlp-main.11.txt,2020,8 Conclusion,our framework achieves state-of-the-art in the zero-shot question answering task achieving performance like prior supervised work and sets a strong baseline in the few-shot question answering task.
2020.emnlp-main.11.txt,2020,8 Conclusion,this work proposes a new framework of knowledge triplet learning over knowledge graph entities and relations.
2020.emnlp-main.11.txt,2020,8 Conclusion,we learn from both human-annotated and synthetic knowledge graphs and evaluate our framework on the six question-answering datasets.
2020.emnlp-main.11.txt,2020,8 Conclusion,"we show learning all three possible functions, fr, fh, and ft help the model perform zero-shot multiple-choice question answering, where we do not use question-answering annotations."
2020.emnlp-main.110.txt,2020,6 Discussion,"(ii) even for familiar targets, targeted methods, especially stance detection, have high fluctuations and perform worse than sentiment lexicons for certain datasets;"
2020.emnlp-main.110.txt,2020,6 Discussion,"(iii) finally, stance methods do not have a clear advantage over targeted sentiment in understanding approval due to the latter’s low performance on indirect stance."
2020.emnlp-main.110.txt,2020,6 Discussion,"by comparing the performance of twelve methods over five datasets with approval towards seven targets, we find that targeted ots methods do not perform well across targets or datasets that span over different time periods and have been collected using different collection strategies."
2020.emnlp-main.110.txt,2020,6 Discussion,"concretely, (i) targeted ots methods do not generalize beyond the targets they were trained on."
2020.emnlp-main.110.txt,2020,6 Discussion,"finally, to help practitioners and css researchers interested in measuring the approval of novel and familiar targets beyond a data collection setting familiar to an ots method, we find that minimal in-domain models are preferable.limitations."
2020.emnlp-main.110.txt,2020,6 Discussion,future semeval challenges should consider this when constructing test datasets and mention the hashtags and keywords they use for data collection.
2020.emnlp-main.110.txt,2020,6 Discussion,"in our error analysis, we show that current stance detection methods, which are slated as being capable of measuring indirect opinions expressed via ”pronouns, epithets, honorifics and relationships,” perform poorly on indirect stance."
2020.emnlp-main.110.txt,2020,6 Discussion,"in the future, we hope to explore abstract topics like ‘immigration’ where differentiating between direct and indirect stance is non-trivial and ensemble models that combine the strengths of multiple methods."
2020.emnlp-main.110.txt,2020,6 Discussion,"keeping in mind this vision, we investigate how an important construct in css, political approval, can be operationalized using existing nlp techniques, either through off-the-shelf sentiment and stance detection methods or through custom domain-specific methods."
2020.emnlp-main.110.txt,2020,6 Discussion,"one of the goals of language technology, including nlp methods, is the ability to (re-)use them."
2020.emnlp-main.110.txt,2020,6 Discussion,"second, we only consider approval towards named entities, which we find is already a difficult task, especially for indirect stances."
2020.emnlp-main.110.txt,2020,6 Discussion,"since ots targeted methods do not perform well for unknown targets, authors of papers on stance detection and target-dependent sentiment analysis should clarify if their method works only for certain targets (target-specific) or can be used to measure stance towards any unseen target (general-purpose), i.e., clarify the borders of their method’s applicability."
2020.emnlp-main.110.txt,2020,6 Discussion,"the high performance of sentiment lexicons, especially for unseen targets (table 4), implies that these resources can be used with ml techniques for general-purpose stance detection."
2020.emnlp-main.110.txt,2020,6 Discussion,"the poor performance of dssd on other trump datasets implies that, compared to sentiment analysis methods, stance methods are more susceptible to changes in topic and time."
2020.emnlp-main.110.txt,2020,6 Discussion,they are as good as or even worse than general-purpose lexicons in this case;
2020.emnlp-main.110.txt,2020,6 Discussion,"this suggests that future research should explore approaches like coreference resolution (for pronouns), word sense disambiguation (for epithets), and background knowledge (relationships to other entities)."
2020.emnlp-main.110.txt,2020,6 Discussion,"this work does not capture all methods that have been proposed for assessing political approval but focuses on those that have been popular in the past or are exemplary for different types of methods (untargeted sentiment, targeted sentiment, and stance)."
2020.emnlp-main.110.txt,2020,6 Discussion,"while researchers interested in measuring approval should use targeted constructs like stance or targeted sentiment instead of overall sentiment to avoid conceptual confusion, current targeted methods need to be improved before they can be used in an off-the-shelf manner."
2020.emnlp-main.111.txt,2020,6 Conclusion,"in the future, we will explore how to apply our model to more domains, and enhance the interpretability of the reasoning path when the model answers questions."
2020.emnlp-main.111.txt,2020,6 Conclusion,"in this work, we explore how to solve multi-choice reading comprehension tasks in the medical field based on the examination problems of licensed pharmacists, and propose a novel model kmqa."
2020.emnlp-main.111.txt,2020,6 Conclusion,it explicitly combines knowledge and pre-trained models into a unified framework.
2020.emnlp-main.111.txt,2020,6 Conclusion,"moreover, kmqa implicitly takes advantage of factual information via learning from an intermediate task and also transfers structural knowledge to enhance entity representation."
2020.emnlp-main.111.txt,2020,6 Conclusion,"on the test set from the real world, the kmqa is the single model that outperforms the human pass line."
2020.emnlp-main.112.txt,2020,6 Conclusion,"experimental results on two benchmark datasets illustrate the effectiveness of the memory by either concatenating it with the output or integrating it with different layers of the decoder by mcln, which obtains the state-of-the-art performance."
2020.emnlp-main.112.txt,2020,6 Conclusion,further analyses investigate how memory size affects model performance and show that our model is able to generate long reports with necessary medical terms and meaningful image-text attention mappings.
2020.emnlp-main.112.txt,2020,6 Conclusion,"in this paper, we propose to generate radiology reports with memory-driven transformer, where a relational memory is used to record the information from previous generation processes and a novel layer normalization mechanism is designed to incorporate the memory into transformer."
2020.emnlp-main.113.txt,2020,5 Conclusion,experiments demonstrate that our proposed disfluency generation model outperformed existing baselines; those disfluent sentences generated significantly aided the task of disfluency detection and led to state-of-the-art performances.
2020.emnlp-main.113.txt,2020,5 Conclusion,this work presents a simple two-stage disfluency generation model to generate natural and diverse disfluent texts.
2020.emnlp-main.113.txt,2020,5 Conclusion,we further used them as augmented data for pretraining and aiding the task of disfluency detection.
2020.emnlp-main.114.txt,2020,7 Conclusions,"in this paper, we introduce a novel task, ctrp, to predict clinical trial results without actually doing them."
2020.emnlp-main.114.txt,2020,7 Conclusions,"instead of using structured evidence that is prohibitively expensive to annotate, we heuristically collect 12m unstructured sentences as implicit evidence, and use large-scale clm pretraining to learn the conditional ordering function required for solving the ctrp task."
2020.emnlp-main.114.txt,2020,7 Conclusions,our ebm-net model outperforms other strong baselines on the evidence integration dataset and is also validated on covid-19 clinical trials.
2020.emnlp-main.115.txt,2020,6 Conclusion,language can provide valuable support to improve clinical decision-making.
2020.emnlp-main.115.txt,2020,6 Conclusion,"not only are we able to sufficiently predict cases with performance comparable to models that use structured emr data, but we are also able to provide useful rationales to support the predictions, as validated by medical domain expertise."
2020.emnlp-main.115.txt,2020,6 Conclusion,"the model achieves auroc scores of 0.75 and 0.78 on sepsis and mortality tasks, respectively."
2020.emnlp-main.115.txt,2020,6 Conclusion,this has important implications for real-world application of explainable clinical decision support from text.
2020.emnlp-main.115.txt,2020,6 Conclusion,we also address challenges in extracting medical documents that are representative of a predictive task.
2020.emnlp-main.115.txt,2020,6 Conclusion,"we also address model explainability by experimenting with a simple (yet effective) linear attention mechanism, and emphasize the interaction between models and users in the design of a novel protocol to evaluate explanations."
2020.emnlp-main.115.txt,2020,6 Conclusion,we augment the power of domain-specific bert and build a hierarchical cnn-transformer that can potentially be applied to any long-document processing task.
2020.emnlp-main.115.txt,2020,6 Conclusion,we conduct a diverse set of experiments to explore several aspects of the applicability of deep nlp in the clinical domain.
2020.emnlp-main.116.txt,2020,6 Conclusions and Future Work,"furthermore, the proposed model can be applied to the normalization of both “uniimplication” and “multi-implication” chinese medical procedure mentions."
2020.emnlp-main.116.txt,2020,6 Conclusions and Future Work,"in the future, we plan to focus on how to improve the performance of medical entity normalization when resources are limited."
2020.emnlp-main.116.txt,2020,6 Conclusions and Future Work,"in this paper, we propose a sequence generative learning framework with a category-based constraint decoding and model-refining mechanism for chinese medical procedure normalization."
2020.emnlp-main.116.txt,2020,6 Conclusions and Future Work,"notwithstanding, considering the complexity of the domain specificity and the scarcity of training data, this challenging task is far from being solved."
2020.emnlp-main.116.txt,2020,6 Conclusions and Future Work,our comprehensive experimental results demonstrate that the proposed model significantly outperforms the baseline methods.
2020.emnlp-main.116.txt,2020,6 Conclusions and Future Work,the “generating and re-ranking” strategy is employed to integrate the proposed generative model with a discriminative similarity re-ranking method to further improve normalization performance.
2020.emnlp-main.116.txt,2020,6 Conclusions and Future Work,"the proposed model can achieve the endto-end generation of all corresponding standard entities for all types of input mentions, especially for “multi-implication” mentions."
2020.emnlp-main.117.txt,2020,7 Conclusion,"extracting highly accurate labels from medical reports by taking advantage of both sources can enable many important downstream tasks, including the development of more accurate and robust medical imaging models required for clinical deployment."
2020.emnlp-main.117.txt,2020,7 Conclusion,"fifth, we find that chexbert is 0.007 f1 points from the radiologist performance benchmark, suggesting that the gap to ceiling performance is narrow."
2020.emnlp-main.117.txt,2020,7 Conclusion,"first, we find that chexbert outperforms models trained only on radiologistlabeled reports, or only on the existing labeler’s outputs."
2020.emnlp-main.117.txt,2020,7 Conclusion,"fourth, we find that chexbert outperforms the previous best labeler, chexpert (which was rules-based), with an improvement of 0.055 (95% ci 0.039, 0.070) on the f1 metric; we also find that the best model trained only on manually labeled radiology reports (tblue-rad-bt) performs at least as well as the chexpert labeler."
2020.emnlp-main.117.txt,2020,7 Conclusion,"in this method, a biomedically pretrained bert model is first trained on the outputs of a labeler, and then further fine-tuned on the manual annotations, the set of which is augmented using backtranslation."
2020.emnlp-main.117.txt,2020,7 Conclusion,"in this study, we propose a simple method for combining existing report labelers with handannotations for accurate radiology report labeling."
2020.emnlp-main.117.txt,2020,7 Conclusion,"second, we find that chexbert outperforms the bert-based model not pretrained on biomedical data."
2020.emnlp-main.117.txt,2020,7 Conclusion,"third, we find that chexbert outperforms models which do not use backtranslation."
2020.emnlp-main.117.txt,2020,7 Conclusion,"we expect this method of training medical report labelers is broadly useful within the medical domain, where collection of expert labels can produce a small set of high quality labels, and existing feature engineered labelers can produce labels at scale."
2020.emnlp-main.117.txt,2020,7 Conclusion,"we report five findings on our resulting model, chexbert."
2020.emnlp-main.118.txt,2020,7 Conclusion,"additionally, funql is relatively robust against program alias."
2020.emnlp-main.118.txt,2020,7 Conclusion,"based on unimer, we conduct an empirical study to understand the characteristics of different meaning representations and their impact on neural semantic parsing."
2020.emnlp-main.118.txt,2020,7 Conclusion,"by open-sourcing our source code and benchmark, we believe that our work can facilitate the community to inform the design and development of next-generation mrs.implications."
2020.emnlp-main.118.txt,2020,7 Conclusion,"first, according to our experimental results, funql tends to outperform lambda calculus and prolog in neural semantic parsing."
2020.emnlp-main.118.txt,2020,7 Conclusion,"hence, when developers need to design an mr for a new domain, funql is recommended to be the first choice."
2020.emnlp-main.118.txt,2020,7 Conclusion,"in this work, we propose unimer, a unified benchmark on meaning representations, based on established semantic parsing datasets; unimer covers three domains and four different meaning representations along with their execution engines."
2020.emnlp-main.118.txt,2020,7 Conclusion,our findings have clear implications for future work.
2020.emnlp-main.118.txt,2020,7 Conclusion,"second, to reduce program alias’ negative effect on neural semantic parsing, developers should define a concrete protocol for annotating logical forms to ensure their consistency."
2020.emnlp-main.118.txt,2020,7 Conclusion,"specifically, given an mr, developers should identify as many as possible sources where program alias can occur."
2020.emnlp-main.118.txt,2020,7 Conclusion,take sql as an example.
2020.emnlp-main.118.txt,2020,7 Conclusion,"to express the argmax semantics, one can either use subquery or the orderby clause.8 having identified these sources, developers need to determine using which expression in what context, e.g., argmax is always expressed with subquery, and the unordered expressions in conjunctions are always sorted by characters."
2020.emnlp-main.118.txt,2020,7 Conclusion,unimer allows researchers to comprehensively and fairly evaluate the performance of their approaches.
2020.emnlp-main.119.txt,2020,5 Conclusion,"in this paper, we try to understand events vertically by viewing them as processes and predicting their sub-event sequences."
2020.emnlp-main.119.txt,2020,5 Conclusion,"moreover, the extrinsic evaluation shows that, even with a naive application method, the process knowledge can help better predict missing events."
2020.emnlp-main.119.txt,2020,5 Conclusion,"our apsi framework is motivated by the notion of analogous processes, and attempts to transfer knowledge from (a very small number of) familiar processes to a new one."
2020.emnlp-main.119.txt,2020,5 Conclusion,the intrinsic evaluation demonstrates the effectiveness of apsi and the quality of the predicted sub-event sequences.
2020.emnlp-main.12.txt,2020,6 Conclusion,creating perturbed examples is often cheaper than creating new ones and we empirically observed notable gains even at a moderate cost ratio of 0.6.
2020.emnlp-main.12.txt,2020,6 Conclusion,"for instance, how do the results change as a function of the total dataset budget b or large values of c?"
2020.emnlp-main.12.txt,2020,6 Conclusion,"our results demonstrate that models trained on perturbations of boolq questions are more robust to minor variations and generalize better, while preserving performance on the original boolq test set as long as the natural perturbations are moderately cheap to create."
2020.emnlp-main.12.txt,2020,6 Conclusion,"over-generation of perturbations can result in overly-similar (lessinformative) variations of a seed example, making larger clusters valuable only up to a certain extent."
2020.emnlp-main.12.txt,2020,6 Conclusion,"we proposed an alternative approach for constructing training sets, by expanding seed examples via natural perturbations."
2020.emnlp-main.12.txt,2020,6 Conclusion,"while this is not a dataset paper (since our focus is on more on the value of natural perturbations for robust model design), we provide the natural perturbations resource for boolq constructed during the course of this study.4 this work suggests a number of interesting lines of future investigation."
2020.emnlp-main.12.txt,2020,6 Conclusion,"while we leave a detailed study to future work, we expect general trends regarding the value of perturbations to hold broadly."
2020.emnlp-main.120.txt,2020,7 Conclusions,"in addition, we designed a special sequence reconstructor (sr) module to learn to perform the sentence re-ordering."
2020.emnlp-main.120.txt,2020,7 Conclusions,"in this paper, we proposed the sentence-level language modeling objective for contextualized sentence representation learning."
2020.emnlp-main.120.txt,2020,7 Conclusions,it reconstructs the original order by pointing to the next sentence among encoded sentence representations using a pointer network and a transformer decoder.
2020.emnlp-main.120.txt,2020,7 Conclusions,our approach extends a word-level language modeling strategy to the sentence-level by reconstructing the original order of shuffled sentences.
2020.emnlp-main.120.txt,2020,7 Conclusions,"through a qualitative analysis, we showed that our model can embed not only semantic but also relational features of sentences."
2020.emnlp-main.120.txt,2020,7 Conclusions,we are excited about future work that could extend our motivation and further aim at incorporating stronger hierarchy into the language model architectures and the pre-training tasks.
2020.emnlp-main.120.txt,2020,7 Conclusions,"we evaluated the effect of the proposed idea on three benchmarks, glue, squad, and discoeval, and showed consistent improvements over the previous approaches."
2020.emnlp-main.120.txt,2020,7 Conclusions,"we matched performance with the state-of-the-art model with the same model size using much fewer parameters, computation, and data."
2020.emnlp-main.121.txt,2020,9 Conclusion,"inspired by the rationale-based annotation process, we show that predicting token-level and sentence-level divergences jointly is a promising direction for further distinguishing between coarser and finer-grained divergences."
2020.emnlp-main.121.txt,2020,9 Conclusion,"we contribute refresd, a new dataset of wikimatrix sentences-pairs in english and french, annotated with semantic divergence classes and tokenlevel rationales that justify the sentence level annotation.64% of samples are annotated as divergent, and 40% of samples contain fine-grained meaning divergences, confirming that divergences are too frequent to ignore even in parallel corpora."
2020.emnlp-main.121.txt,2020,9 Conclusion,we show that explicitly considering diverse semantic divergence types benefits both the annotation and prediction of divergences between texts in different languages.
2020.emnlp-main.121.txt,2020,9 Conclusion,"we show that these divergences can be detected by a mbert model fine-tuned without annotated samples, by learning to rank synthetic divergences of varying granularity."
2020.emnlp-main.122.txt,2020,6 Conclusion,"in future work, we will explore generalizing this approach to the multilingual setting, or applying it to the pre-train and fine-tune paradigm used widely in other models such as bert."
2020.emnlp-main.122.txt,2020,6 Conclusion,"we also find our model to be especially effective on unsupervised cross-lingual semantic similarity, due to its stripping away of language-specific information allowing for the underlying semantics to be more directly compared."
2020.emnlp-main.122.txt,2020,6 Conclusion,"we find that our model bests all baselines on unsupervised semantic similarity tasks, with the largest gains coming from a new challenge we propose as hard sts, designed to foil methods approximating semantic similarity as word overlap."
2020.emnlp-main.122.txt,2020,6 Conclusion,"we propose bilingual generative transformers, a model that uses parallel data to learn to perform source separation of common semantic information between two languages from language-specific information."
2020.emnlp-main.122.txt,2020,6 Conclusion,we show that the model is able to accomplish this source separation through probing tasks and text generation in a style-transfer setting.
2020.emnlp-main.123.txt,2020,5 Conclusion,"furthermore, this aligner may help to build or increase semantic resources, using a promising approach as back-translation (sobrevilla cabezudo et al., 2019)."
2020.emnlp-main.123.txt,2020,5 Conclusion,"future work includes adopting multilingual word embeddings (lample et al., 2018) to produce alignments for other languages."
2020.emnlp-main.123.txt,2020,5 Conclusion,"in this paper, we presented an amr alignment method designed for the portuguese language."
2020.emnlp-main.123.txt,2020,5 Conclusion,it is based on pre-trained word embeddings and word mover’s distance to match word tokens in the sentences and nodes in the corresponding amr graphs.
2020.emnlp-main.123.txt,2020,5 Conclusion,more details about amr resources and tools for the portuguese language may be found at the opinando project webpage 8.
2020.emnlp-main.123.txt,2020,5 Conclusion,"this simple approach may be adopted for other languages with few resources, aiming to get tools for natural language understanding tasks."
2020.emnlp-main.124.txt,2020,5 Conclusions,"in addition, we show that sentence bert (sbert), the state-of-the-art supervised method, is problematic to apply to certain unsupervised tasks when the target domain significantly differs from the dataset it was trained on."
2020.emnlp-main.124.txt,2020,5 Conclusions,"in the future, we want to explore semi-supervised methods for sentence embedding and its transferability across domains."
2020.emnlp-main.124.txt,2020,5 Conclusions,"in this paper, we proposed the is-bert model for unsupervised sentence representation learning with a novel mi maximization objective."
2020.emnlp-main.124.txt,2020,5 Conclusions,is-bert achieves substantially better results in this scenario as it has the flexibility to be trained on the taskspecific corpus without label restriction.
2020.emnlp-main.124.txt,2020,5 Conclusions,is-bert outperforms all unsupervised sentence embedding baselines on various tasks and is competitive with supervised sentence embedding methods in certain scenarios.
2020.emnlp-main.125.txt,2020,7 Discussion and Future Work,"further, as our method does not restrict input to syntactic trees but only assumes tree structures with arbitrary numbering (e.g.leftto-right post-order numbering) as input, we intend to try alignments of chunk-based trees, which is desirable for applications that process text fragments, e.g.those that perform information extraction."
2020.emnlp-main.125.txt,2020,7 Discussion and Future Work,"in contrast to previous methods, ours can align phrases not only in paraphrasal sentence pairs but also in partially paraphrasal pairs."
2020.emnlp-main.125.txt,2020,7 Discussion and Future Work,"we intend to expand our method to conduct forest alignments for making it robust against parsing errors, which are inevitable in handling large corpora."
2020.emnlp-main.125.txt,2020,7 Discussion and Future Work,"we plan to apply it to a comparable corpus of partial paraphrases and investigate the performance, with the aim of creating a large-scale syntactic and phrasal paraphrase dataset."
2020.emnlp-main.126.txt,2020,5 Conclusion,"overall, an improvement of 4.93% is achieved compared to the state-of-the-art method."
2020.emnlp-main.126.txt,2020,5 Conclusion,significant improvements on tabfact demonstrate its effectiveness.
2020.emnlp-main.126.txt,2020,5 Conclusion,"the proposed method can further contribute to other semi-structured data (table, graph, etc.)related tasks, e.g."
2020.emnlp-main.126.txt,2020,5 Conclusion,there still exists plenty of potentials that require future studies in this direction.
2020.emnlp-main.126.txt,2020,5 Conclusion,"we further enhance sat by appending a summary row to the table, the results show that it is promising to solve the fact verification that requires both symbolic reasoning and semantic understanding by feeding symbolic reasoning results into sat."
2020.emnlp-main.126.txt,2020,5 Conclusion,we propose sat to enhance the pre-trained transformer’s ability on table representation by injecting structural information into the mask of selfattention layers.
2020.emnlp-main.126.txt,2020,5 Conclusion,"wikitablequestions (pasupat and liang, 2015) and commonsenseqa (talmor et al., 2019)."
2020.emnlp-main.127.txt,2020,6 Conclusion,"experimental results on the large-scale humanannotated dataset, docred, show gain outperforms previous methods, especially in intersentence and inferential relations scenarios."
2020.emnlp-main.127.txt,2020,6 Conclusion,extracting inter-sentence relations and conducting relational reasoning are challenging in documentlevel relation extraction.
2020.emnlp-main.127.txt,2020,6 Conclusion,gain utilizes a heterogeneous mention-level graph to model the interaction among different mentions across the document and capture document-aware features.
2020.emnlp-main.127.txt,2020,6 Conclusion,"in this paper, we introduce graph aggregationand-inference network (gain) to better cope with document-level relation extraction, which features double graphs in different granularity."
2020.emnlp-main.127.txt,2020,6 Conclusion,it also uses an entity-level graph with a proposed path reasoning mechanism to infer relations more explicitly.
2020.emnlp-main.127.txt,2020,6 Conclusion,the ablation study also confirms the effectiveness of different modules in our model.
2020.emnlp-main.128.txt,2020,6 Conclusion and Future Work,"in the future, we would adapt our method to other ie tasks to study its application scope."
2020.emnlp-main.128.txt,2020,6 Conclusion and Future Work,"in this paper, we take a fresh look at ee by casting it as an mrc problem."
2020.emnlp-main.128.txt,2020,6 Conclusion and Future Work,"our method includes an unsupervised question generation process which can generate both relevant and context-related questions, whose effectiveness is verified by empirical results."
2020.emnlp-main.129.txt,2020,7 Conclusion and Future work,"in the future, we will extend maven to more event-related tasks like event argument extraction, event sequencing, etc."
2020.emnlp-main.129.txt,2020,7 Conclusion and Future work,"in this paper, we present a massive general domain event detection dataset (maven), which significantly alleviates the data scarcity and low coverage problems of existing datasets."
2020.emnlp-main.129.txt,2020,7 Conclusion and Future work,the results indicate that general domain ed is still challenging and maven may facilitate further research.
2020.emnlp-main.129.txt,2020,7 Conclusion and Future work,"we also explore some promising directions with analytic experiments, including modeling multiple event correlations (section 5.3), utilizing the hierarchical event schema to distinguish close types (section 5.6), and improving other ed tasks with transfer learning (section 5.5)."
2020.emnlp-main.129.txt,2020,7 Conclusion and Future work,we conduct a thorough evaluation of the state-of-the-art ed models on maven.
2020.emnlp-main.13.txt,2020,6 Conclusion,"an investigation of how, whether, and why formalisms and their implementations affect probing results for tasks beyond role labeling and for frameworks beyond edge probing constitutes an exciting avenue for future research."
2020.emnlp-main.13.txt,2020,6 Conclusion,"finally, assembling corpora with parallel cross-formalism annotations would facilitate further research on the effect of formalism in probing."
2020.emnlp-main.13.txt,2020,6 Conclusion,"first, the formalism and implementation used to prepare the linguistic material underlying a probing study should be always explicitly specified."
2020.emnlp-main.13.txt,2020,6 Conclusion,our findings suggest that linguistic formalism is an important factor to be accounted for in probing studies.
2020.emnlp-main.13.txt,2020,6 Conclusion,our refined implementation of the edge probing framework coupled with the anchor task methodology enabled new insights into the processing of predicate-semantic information within mbert.
2020.emnlp-main.13.txt,2020,6 Conclusion,"second, if possible, results on multiple formalisations of the same task should be reported and validated for several languages."
2020.emnlp-main.13.txt,2020,6 Conclusion,this prompts several recommendations for the follow-up probing studies.
2020.emnlp-main.13.txt,2020,6 Conclusion,"we have demonstrated that the choice of linguistic formalism can have substantial, linguistically meaningful effects on role-semantic probing results."
2020.emnlp-main.13.txt,2020,6 Conclusion,"we have shown how probing classifiers can be used to detect discrepancies between formalism implementations, and presented evidence of semantic proto-role encoding in the pre-trained mbert model."
2020.emnlp-main.13.txt,2020,6 Conclusion,"while our work illustrates the impact of formalism using a single task and a single probing framework, the influence of linguistic formalism per se is likely to be present for any probing setup that builds upon linguistic material."
2020.emnlp-main.130.txt,2020,6 Conclusion,experiments on five real-world datasets show that our approach achieves the stateof-the-art results.
2020.emnlp-main.130.txt,2020,6 Conclusion,"our approach first extracts useful attribute features of entity-pairs by using a convolutional neural network, and then propagates the features among the neighbors of entitypairs, by using a graph neural network with edgeaware attentions."
2020.emnlp-main.130.txt,2020,6 Conclusion,the embeddings are learned with the object of separating equivalent and nonequivalent entity-pairs.
2020.emnlp-main.130.txt,2020,6 Conclusion,this paper presents a new entity-pair embedding approach for kg alignment.
2020.emnlp-main.131.txt,2020,6 Conclusion,experiments on two public datasets demonstrate that our model outperforms current state-of-art methods with different few-shot sizes.
2020.emnlp-main.131.txt,2020,6 Conclusion,"faan proposes to encode entity pairs adaptively, and predict facts by adaptively matching references with queries."
2020.emnlp-main.131.txt,2020,6 Conclusion,"previous studies solve this problem by learning static representations of entities or references, ignoring their dynamic properties."
2020.emnlp-main.131.txt,2020,6 Conclusion,"this paper proposes an adaptive attentional network for few-shot kg completion, termed as faan."
2020.emnlp-main.132.txt,2020,6 Conclusion,"by designning three pre-training objectives, we can learn better pre-trained encoders customized for entity relation extraction task."
2020.emnlp-main.132.txt,2020,6 Conclusion,experiments on two benchmark datasets demonstrate the effectiveness of the proposed pre-training method.
2020.emnlp-main.132.txt,2020,6 Conclusion,"in comparison to universal pre-trained model, we introduce a span encoder and a span pair encoder."
2020.emnlp-main.132.txt,2020,6 Conclusion,"we propose a pre-training network architecture with three objectives, which can incorporate intraspan and inter-span information into pre-trained models."
2020.emnlp-main.133.txt,2020,7 Conclusion,another direction is to generalize the way in which the table and sequence interact to other types of representations.
2020.emnlp-main.133.txt,2020,7 Conclusion,"in the future, we would like to investigate how the table representation may be applied to other tasks."
2020.emnlp-main.133.txt,2020,7 Conclusion,"in this paper, we introduce the novel tablesequence encoders architecture for joint extraction of entities and their relations."
2020.emnlp-main.133.txt,2020,7 Conclusion,it learns two separate encoders rather than one – a sequence encoder and a table encoder where explicit interactions exist between the two encoders.
2020.emnlp-main.133.txt,2020,7 Conclusion,"we achieved state-of-the-art f1 scores for both ner and re tasks across four standard datasets, which confirm the effectiveness of our approach."
2020.emnlp-main.133.txt,2020,7 Conclusion,we also introduce a new method to effectively employ useful information captured by the pre-trained language models for such a joint learning task where a table representation is involved.
2020.emnlp-main.134.txt,2020,4 Conclusion,"additionally, we demonstrated that unlikelihood-based losses are effective for allowing the use of negative examples in generative-based information retrieval."
2020.emnlp-main.134.txt,2020,4 Conclusion,"we believe that our approach can also be effectively used for text classification problems, where the score of a class label c is computed as the likelihood of generating the class label c given the document d, p(c|d)."
2020.emnlp-main.134.txt,2020,4 Conclusion,"we have proposed a new generative approach for ir based on large pretrained neural language models, and demonstrated their effectiveness as rankers by providing robust experimental results on four different datasets."
2020.emnlp-main.135.txt,2020,7 Conclusion,"our experiments suggest that pretrained word embeddings (both contextualized and non-contextualized), combined with tf-weighted k-means and tf-based reranking, provide a viable alternative to traditional topic modeling at lower complexity and runtime."
2020.emnlp-main.135.txt,2020,7 Conclusion,"we outlined a methodology for clustering word embeddings for unsupervised document analysis, and presented a systematic comparison of various influential embedding methods and clustering algorithms."
2020.emnlp-main.136.txt,2020,6 Conclusion,"in the future, we will investigate the feasibility of incorporating classical mds guidance to abstractive models with large-scale pre-training (gu et al., 2020) and more challenging settings where each document set may contain hundreds or even thousands of documents."
2020.emnlp-main.136.txt,2020,6 Conclusion,"the proposed framework leverages the benefits of both neural sequence learning and statistical measures, bridging the gap between sds and mds."
2020.emnlp-main.136.txt,2020,6 Conclusion,"we conduct extensive experiments on benchmark mds datasets and demonstrate the superior performance of the proposed framework, especially in handling the large search space and high redundancy of mds."
2020.emnlp-main.136.txt,2020,6 Conclusion,we present a reinforcement learning framework for mds that unifies neural sds advances and maximal marginal relevance (mmr) through end-toend learning.
2020.emnlp-main.137.txt,2020,7 Conclusions and Future Work,another intriguing direction is exploring the connection between our methods and neural network interpretability.
2020.emnlp-main.137.txt,2020,7 Conclusions and Future Work,"finally, although we are motivated primarily by the widespread use of topic models for identifying interpretable topics (boyd-graber et al., 2017, ch.3), we plan to explore the ideas presented here further in the context of downstream applications like document classification."
2020.emnlp-main.137.txt,2020,7 Conclusions and Future Work,"in future work, we also hope to explore the effects of the pretraining corpus (gururangan et al., 2020) and teachers (besides bert) on the generated topics."
2020.emnlp-main.137.txt,2020,7 Conclusions and Future Work,"in our work, as the weight on the bert autoencoder logits λ goes to one, the topic model begins to describe less the corpus and more the teacher."
2020.emnlp-main.137.txt,2020,7 Conclusions and Future Work,"our adaptable framework does not just produce improvements in the aggregate (as is commonly reported): its effect can be interpreted more specifically as identifying the same space of topics generated by an existing model and, in most cases, improving the coherence of individual topics, thus highlighting the modular value of our approach."
2020.emnlp-main.137.txt,2020,7 Conclusions and Future Work,"our modular method sits atop any neural topic model (ntm) to improve topic quality, which we demonstrate using two ntms of highly disparate architectures (vaes and waes), obtaining state-of-the-art topic coherence across three datasets from different domains."
2020.emnlp-main.137.txt,2020,7 Conclusions and Future Work,"the use of knowledge distillation to facilitate interpretability has also been previously explored, for example, in liu et al.(2018a) to learn interpretable decision trees from neural networks."
2020.emnlp-main.137.txt,2020,7 Conclusions and Future Work,"to our knowledge, we are the first to distill a “blackbox” neural network teacher to guide a probabilistic graphical model."
2020.emnlp-main.137.txt,2020,7 Conclusions and Future Work,"we believe mining this connection can open up further research avenues; for instance, by investigating the differences in such teacher-topics conditioned on the pre-training corpus."
2020.emnlp-main.137.txt,2020,7 Conclusions and Future Work,we do this in order to combine the expressivity of probabilistic topic models with the precision of pretrained transformers.
2020.emnlp-main.138.txt,2020,6 Conclusion,experiments on benchmark datasets quantitatively and qualitatively show our model significantly outperforms baselines to overcome the data sparsity problem of short texts.
2020.emnlp-main.138.txt,2020,6 Conclusion,future works could focus on employing the proposed model in more downstream tasks.
2020.emnlp-main.138.txt,2020,6 Conclusion,"in this paper, for short text topic modeling, we propose the negative sampling and quantization topic model (nqtm) with a novel topic distribution quantization mechanism to yield peakier distributions and a new negative sampling decoder to enrich the learning signals."
2020.emnlp-main.139.txt,2020,5 Conclusion,"in contrast to recent research in ad hoc neural ir, which require large amounts of training data (mitra and craswell, 2018), we present a system that combines term-weighting techniques and neural models across two distinct linguistic genres."
2020.emnlp-main.139.txt,2020,5 Conclusion,"our data collection process also reveals that even in a domain as critically important as medical news, only a small fraction of news articles (24.6%) include a complete citation and a link to the original research."
2020.emnlp-main.139.txt,2020,5 Conclusion,"our results show that while neural models excel at re-ranking a small number of documents when pre-trained contextual embeddings are tuned on domain-specific data, classical token-based approaches remain difficult to beat in a cross-genre retrieval scenario when the search space is larger."
2020.emnlp-main.139.txt,2020,5 Conclusion,"thus, the presented task has utility in medical factchecking, identifying health-related misinformation, and assessing some empirically verifiable aspects of health news reporting."
2020.emnlp-main.139.txt,2020,5 Conclusion,we also provide a novel dataset of medical newswire queries linked to research literature.
2020.emnlp-main.14.txt,2020,6 Conclusions,we explain how to easily measure mdl on top of standard probe-training pipelines.
2020.emnlp-main.14.txt,2020,6 Conclusions,we propose information-theoretic probing which measures minimum description length (mdl) of labels given representations.
2020.emnlp-main.14.txt,2020,6 Conclusions,"we show that mdl naturally characterizes not only probe quality, but also ‘the amount of effort’ needed to achieve it (or, intuitively, strength of the regularity in representations with respect to the labels); this is done in a theoretically justified way without manual search for settings."
2020.emnlp-main.14.txt,2020,6 Conclusions,we show that results of mdl probing are more sensible compared to standard probes.
2020.emnlp-main.140.txt,2020,6 Conclusions and Future Work,"as a first attempt to introduce macro-level meta-features for strategy selection, we believe there is much potential to refine and improve our approach."
2020.emnlp-main.140.txt,2020,6 Conclusions and Future Work,"different from traditional keyphrase extraction models mainly focusing on text, smartkpe illustrates the advantage of incorporating other modalities to help keyphrases location and salience prediction."
2020.emnlp-main.140.txt,2020,6 Conclusions and Future Work,"furthermore, the smart-kpe framework can be easily adapted to other nlp tasks, and we believe there is much potential in combining smart-kpe with different models to further boost performance on opendomain kpe and other web-related tasks."
2020.emnlp-main.140.txt,2020,6 Conclusions and Future Work,in this work we propose a strategy-based multimodal architecture for keyphrase extraction (smart-kpe) as a new state-of-the-art method for multimodal web-page keyphrase extraction.
2020.emnlp-main.140.txt,2020,6 Conclusions and Future Work,"one high-level idea is to add further supervision to the current selector model based on empirical web page clustering, to better train the model to develop a set of more distinct keyphrase prediction strategies, and more effectively adjust the respective selector weights."
2020.emnlp-main.140.txt,2020,6 Conclusions and Future Work,our proposed model outperforms several state-of-the-art baselines with the introduction of multimodal information.
2020.emnlp-main.140.txt,2020,6 Conclusions and Future Work,"through several case studies, we further illustrate how micro and macro-level features lead to the model’s correct or incorrect selections."
2020.emnlp-main.140.txt,2020,6 Conclusions and Future Work,we also plan to add more types of meta-features to generate richer multimodal representations.
2020.emnlp-main.141.txt,2020,5 Conclusion,a state-of-the-art model was trained to establish strong baselines for future studies.
2020.emnlp-main.141.txt,2020,5 Conclusion,"cmumoseas contains 20 annotated labels including sentiment (and subjectivity), emotions, and personality traits."
2020.emnlp-main.141.txt,2020,5 Conclusion,"in this paper, we introduced a new large-scale inthe-wild dataset of multimodal language, called cmu-moseas (cmu multimodal opinion sentiment, emotions and attributes)."
2020.emnlp-main.141.txt,2020,5 Conclusion,"the cmumoseas dataset is the largest of its kind in all four constituent languages (french, german, portuguese, and spanish) with 40,000 total samples spanning 1,645 speakers and 250 topics."
2020.emnlp-main.141.txt,2020,5 Conclusion,"the dataset and accompanied descriptors will be made publicly available, and regularly updated with new feature descriptors as multimodal learning advances."
2020.emnlp-main.141.txt,2020,5 Conclusion,"to protect the privacy of the speakers, the released descriptors will not carry invertible information, and no video or audio can be reconstructed based on the extracted features."
2020.emnlp-main.141.txt,2020,5 Conclusion,"we believe that data of this scale presents a step towards learning human communication at a more fine-grained level, with the long-term goal of building more equitable and inclusive nlp systems across multiple languages."
2020.emnlp-main.142.txt,2020,7 Conclusion,"in this work, we explore unsupervised disfluency detection by combining self-training and selfsupervised learning."
2020.emnlp-main.142.txt,2020,7 Conclusion,we showed that it is possible to completely remove the need of human-annotated data and train a high-performance disfluency detection system in a completely unsupervised manner.
2020.emnlp-main.143.txt,2020,6 Conclusion,"for each specific input, our method dynamically associates an explanatory feature with a prediction if the feature explains the prediction well."
2020.emnlp-main.143.txt,2020,6 Conclusion,"in this paper, we presented multimodal routing to identify the contributions from unimodal, bimodal and trimodal explanatory features to predictions in a locally manner."
2020.emnlp-main.143.txt,2020,6 Conclusion,the advantage of both local and global interpretation is achieved without much loss of performance compared to the sota methods.
2020.emnlp-main.143.txt,2020,6 Conclusion,"then, we interpret our approach by analyzing the routing coefficients, showing great variation of feature importance in different samples."
2020.emnlp-main.143.txt,2020,6 Conclusion,these observations align with prior work in psychological research.
2020.emnlp-main.143.txt,2020,6 Conclusion,"we also conduct global interpretation over the whole datasets, and show that the acoustic features are crucial for predicting negative sentiment or emotions, and the acoustic-visual interactions are crucial for predicting emotion angry."
2020.emnlp-main.143.txt,2020,6 Conclusion,"we believe that this work sheds light on the advantages of understanding human behaviors from a multimodal perspective, and makes a step towards introducing more interpretable multimodal language models."
2020.emnlp-main.144.txt,2020,6 Conclusions,experiments on the how2 dataset show the effectiveness of the proposed models.
2020.emnlp-main.144.txt,2020,6 Conclusions,"furthermore, when using high noise speech recognition transcription, our model still achieves the effect of being close to the groundtruth transcription model, which reduces the manual annotation cost of transcripts."
2020.emnlp-main.144.txt,2020,6 Conclusions,we introduce a multistage fusion network with fusion forget gate for generating text summaries for the open-domain videos.
2020.emnlp-main.144.txt,2020,6 Conclusions,we propose a multistep fusion schema to model fine-grained interactions between multisource modalities and a fusion forget gate module to handle the flow of multimodal noise of multisource long sequences.
2020.emnlp-main.145.txt,2020,5 Conclusion,"our experimental results show that bist can extract relevant, high-resolution visual cues from videos and generate quality dialogue responses/answers."
2020.emnlp-main.145.txt,2020,5 Conclusion,"we proposed bist, a novel deep neural network approach for video-grounded dialogues and video qa, which exploits the complex visual nuances of videos through a bidirectional reasoning framework in both spatial and temporal dimensions."
2020.emnlp-main.146.txt,2020,6 Conclusion,"the promising performance of uniconv on the multiwoz benchmark (including three tasks: dst, context-to-text generation, and end-to-end dialogues) validates the efficacy of our method."
2020.emnlp-main.146.txt,2020,6 Conclusion,"we proposed uniconv, a novel unified neural architecture of conversational agents for multi-domain task-oriented dialogues, which jointly trains (1) a bi-level state tracker to capture dependencies in both domain and slot levels simultaneously, and (2) a joint dialogue act and response generator to model dialogue act latent variable and semantically conditions output responses with contextual cues."
2020.emnlp-main.147.txt,2020,5 Conclusion,"in this work, we present a novel graph-based endto-end model for task-oriented dialogue systems."
2020.emnlp-main.147.txt,2020,5 Conclusion,"our model may also be applied to end-to-end open-domain chatbots since the goal is to generate responses given inputs and external knowledge, which is what our model can do."
2020.emnlp-main.147.txt,2020,5 Conclusion,the model further exploits the relationships between entities in the kb to achieve better reasoning ability by combining the multi-hop reasoning ability with graph.
2020.emnlp-main.147.txt,2020,5 Conclusion,the model leverages the graph structural information in dialogue history via the proposed recurrent cell architecture to capture the semantics of dialogue history.
2020.emnlp-main.147.txt,2020,5 Conclusion,we empirically show that our model outperforms the state-of-the-art models on two real-world taskoriented dialogue datasets.
2020.emnlp-main.147.txt,2020,5 Conclusion,we will explore this direction in future work.
2020.emnlp-main.148.txt,2020,6 Conclusion,both models are proved to have a better structure learning performance over the state of the art algorithms.
2020.emnlp-main.148.txt,2020,6 Conclusion,"in the future, we will further explore how to explicitly incorporate linguistics information, such as named entities into the latent states."
2020.emnlp-main.148.txt,2020,6 Conclusion,this paper proposed to inject structured attention into variational recurrent neural network models for unsupervised dialogue structure learning.
2020.emnlp-main.148.txt,2020,6 Conclusion,we explored two different structure inductive biases: linear crf for utterance-level semantic structure induction in two-party dialogues; and non-projective dependency tree for interactive structure learning in multi-party dialogues.
2020.emnlp-main.149.txt,2020,6 Conclusion and Outlook,"experimentalresults proved ccn's superiority when comparing with a number of existing state-of-art text generation models, which tells the cross copy mechanism can successfully enhance the dialogue generation performance."
2020.emnlp-main.149.txt,2020,6 Conclusion and Outlook,"in future work, we will further investigate othercontent generation problems by leveraging multi-granularity copying mechanism."
2020.emnlp-main.149.txt,2020,6 Conclusion and Outlook,"in this paper, we proposed a novel neural network structure-cross copy networks, enabling both vertical copy (from dialogue context) and horizontal copy (from similar cases)."
2020.emnlp-main.149.txt,2020,6 Conclusion and Outlook,this study serves as the methodological foundation.
2020.emnlp-main.149.txt,2020,6 Conclusion and Outlook,"unlike prior models, the proposed ccn doesn't need additional knowledge input, and it can be easily adopted to other domains."
2020.emnlp-main.149.txt,2020,6 Conclusion and Outlook,we conduct experiments on two different datasets with both quantitative and human evaluation to validate the proposed model.
2020.emnlp-main.15.txt,2020,8 Conclusion,future work will be separated into two strands.
2020.emnlp-main.15.txt,2020,8 Conclusion,"in this paper, we introduce an alternative framework for intrinsic probing, which we term dimension selection."
2020.emnlp-main.15.txt,2020,8 Conclusion,"overall, we find that fasttext is more focal than bert, requiring fewer dimensions to capture most of the information pertaining to a morphosyntactic property.future work."
2020.emnlp-main.15.txt,2020,8 Conclusion,"the first will focus on how to better model the distribution of embeddings given a morphosyntactic attribute; as mentioned above, this should yield a better probe overall."
2020.emnlp-main.15.txt,2020,8 Conclusion,the idea is to use probe performance on different subsets of dimensions as a gauge for how much information about a linguistic property different subsets of dimensions jointly encode.
2020.emnlp-main.15.txt,2020,8 Conclusion,"the second strand of work pertains to a deeper analysis of our results, and expansion to other probing tasks."
2020.emnlp-main.15.txt,2020,8 Conclusion,"therefore, we present a decomposable probe which is based on the gaussian distribution, and evaluate its effectiveness by probing bert and fasttext for morphosyntax across 36 languages."
2020.emnlp-main.15.txt,2020,8 Conclusion,"we show that current probes are unsuitable for intrinsic probing through dimension selection as they are not inherently decomposable, which is required to make the procedure computationally tractable."
2020.emnlp-main.150.txt,2020,6 Conclusion,"as far as we know, we are the first work bringing the dependency information of dialogues into the multi-turn response selection task."
2020.emnlp-main.150.txt,2020,6 Conclusion,"in the future, we will move on to develop a more general dialogue dependency parser and better incorporate dependency information into dialogue context modeling tasks."
2020.emnlp-main.150.txt,2020,6 Conclusion,"we proposed the dialogue extraction algorithm and thread-encoder model, which becomes the stateof-the-art on several well-known ubuntu datasets."
2020.emnlp-main.151.txt,2020,6 Conclusion,empirical studies on two benchmark datasets demonstrate the effectiveness of the pin model.
2020.emnlp-main.151.txt,2020,6 Conclusion,existing generationbased models fail to model the dialogue dependencies and ignore the slot-overlapping problem in mdst.
2020.emnlp-main.151.txt,2020,6 Conclusion,"furthermore, a distributed copy mechanism is introduced to perform a selective copy from either the historical system utterances or the historical user utterances."
2020.emnlp-main.151.txt,2020,6 Conclusion,the design of the pin model is inspired by the interactive nature of the dialogues and the overlapping slots in the ontology.
2020.emnlp-main.151.txt,2020,6 Conclusion,the interactive encoder characterizes the cross-turn dependencies and the in-turn dependencies.
2020.emnlp-main.151.txt,2020,6 Conclusion,the slot-overlapping problem is solved by introducing the slot-level context.
2020.emnlp-main.151.txt,2020,6 Conclusion,this paper studies the problem of state generation for multi-domain dialogues.
2020.emnlp-main.151.txt,2020,6 Conclusion,"to overcome the limitation of existing models, we present novel parallel interactive networks (pin) for more accurate and robust dialogue state generation."
2020.emnlp-main.152.txt,2020,4 Conclusion,further analyses show that our proposed non-autoregressive refiner has great potential to replace crf in at least slot filling task.
2020.emnlp-main.152.txt,2020,4 Conclusion,"in the future, we plan to extend our nonautoregressive refiner to other natural language understanding (nlu) tasks, e.g., named entity recognition (tjong kim sang and de meulder, 2003), semantic role labeling (he et al., 2018), and natural language generation (nlg) tasks, e.g., machine translation (vaswani et al., 2017), summarization (liu and lapata, 2019)."
2020.emnlp-main.152.txt,2020,4 Conclusion,"in this paper, we first reveal an uncoordinated slots problem for a classical language understanding task, i.e., slot filling."
2020.emnlp-main.152.txt,2020,4 Conclusion,"to address this problem, we present a novel non-autoregressive joint model for slot filling and intent detection with two-pass refine mechanism (non-autoregressive refiner), which significantly improves the performance while substantially speeding up the decoding."
2020.emnlp-main.153.txt,2020,7 Conclusion,"in future work, we would like to apply our approach on document-level and multi-document nlu tasks."
2020.emnlp-main.153.txt,2020,7 Conclusion,our approach outperforms existing norm-minimization techniques in task performance and agreement with human rationales for tasks in the eraser benchmark.
2020.emnlp-main.153.txt,2020,7 Conclusion,our objective obtains a better trade off of accuracy vs. sparsity.
2020.emnlp-main.153.txt,2020,7 Conclusion,we are also able to close the gap with models that use the full input with < 25% rationale annotations for a majority of the tasks.
2020.emnlp-main.153.txt,2020,7 Conclusion,we introduce a novel sparsity-inducing objective derived from the information bottleneck principle to extract rationales of desired conciseness.
2020.emnlp-main.154.txt,2020,8 Conclusion,"another possible avenue for future work is to use crows-pairs to help directly debias lms, by in some way minimizing a metric like ours."
2020.emnlp-main.154.txt,2020,8 Conclusion,"doing this in a way that generalizes broadly without overly harming performance on unbiased examples will likely involve further methods work, and may not be possible with the scale of dataset that we present here."
2020.emnlp-main.154.txt,2020,8 Conclusion,"this crowdsourced dataset covers nine categories of social bias, and we show that widely-used mlms exhibit substantial bias in every category."
2020.emnlp-main.154.txt,2020,8 Conclusion,"this highlights the danger of deploying systems built around mlms like these, and we expect crows-pairs to serve as a metric for stereotyping in future work on model debiasing."
2020.emnlp-main.154.txt,2020,8 Conclusion,we introduce the crowdsourced stereotype pairs challenge dataset.
2020.emnlp-main.154.txt,2020,8 Conclusion,"while our evaluation is limited to mlms, we were limited by our metric, a clear next step of this work is to develop metrics that would allow one to test autoregressive language models on crowspairs."
2020.emnlp-main.155.txt,2020,5 Conclusion,"for example, the number of instances in clusters could be uneven (see appendix a.3)."
2020.emnlp-main.155.txt,2020,5 Conclusion,"in practice, many works use the global performance gap between different groups as a metric to detect the bias."
2020.emnlp-main.155.txt,2020,5 Conclusion,"in this work, we revisit the coarse-grained metric for group bias analysis and propose a new method, logan, to detect local group bias by clustering."
2020.emnlp-main.155.txt,2020,5 Conclusion,machine learning models risk inheriting the underlying societal biases from the data.
2020.emnlp-main.155.txt,2020,5 Conclusion,our method can help detect model biases that previously are hidden from the global bias metrics and provide an explanation of such biases.
2020.emnlp-main.155.txt,2020,5 Conclusion,we notice there are some limitations in logan.
2020.emnlp-main.156.txt,2020,9 Discussion and conclusion,"but humans generating natural language have finite memory, and context-free languages are known to be both too expressive and not expressive enough (chomsky, 1959; joshi et al., 1990)."
2020.emnlp-main.156.txt,2020,9 Discussion and conclusion,our constructions provide insight into the mechanisms that rnns and lstms can implement.
2020.emnlp-main.156.txt,2020,9 Discussion and conclusion,our experiments point towards learnability; what class of memory-bounded languages are efficiently learnable?
2020.emnlp-main.156.txt,2020,9 Discussion and conclusion,the chomsky hierarchy puts all finite memory languages in the single category of regular languages.
2020.emnlp-main.156.txt,2020,9 Discussion and conclusion,we hope that answers to these questions will not just demystify the empirical success of rnns but ultimately drive new methodological improvements as well.
2020.emnlp-main.156.txt,2020,9 Discussion and conclusion,"we proved that finite-precision rnns can generate dyck-(k,m), a canonical family of bounded-depth hierarchical languages, in o(m log k) memory, a result we also prove is tight."
2020.emnlp-main.156.txt,2020,9 Discussion and conclusion,"we thus suggest the further study of what structure networks can encode in their memory (here, stack-like) as opposed to (just) their position in the chomsky hierarchy."
2020.emnlp-main.156.txt,2020,9 Discussion and conclusion,"while we have settled the representation question for dyck-(k,m), many open questions still remain: what broader class of bounded hierarchical languages can rnns efficiently generate?"
2020.emnlp-main.157.txt,2020,6 Discussion and Recommendations,"a pretrained model would therefore treat the two forms separately, without one amplifying the other even when a language model can’t capture the syntactic differences."
2020.emnlp-main.157.txt,2020,6 Discussion and Recommendations,"bert’s tokenizer includes all pronouns in this paper except “themself”, thus exhibiting unintended featuredesign bias by needing to be constructed as “thems-elf” or “them-se-lf”, presumably because all other forms with a “-self” suffix are already full tokens."
2020.emnlp-main.157.txt,2020,6 Discussion and Recommendations,"from gonen et al., (2019), it is not clear that debiasing the model itself would solve the problem."
2020.emnlp-main.157.txt,2020,6 Discussion and Recommendations,"however, it might be possible to encode the data with the grammatical categories to mitigate some bias, for example, encoding the two “his” pronouns, like “his{dep}” and “his{ind}”."
2020.emnlp-main.157.txt,2020,6 Discussion and Recommendations,"people who use “hers”, “theirs” and “themself” to align their current social gender(s) with their pronouns’ grammatical gender are marginalized when applications fail to identify those pronouns."
2020.emnlp-main.157.txt,2020,6 Discussion and Recommendations,"that might include words other than pronouns, especially for multilingual models."
2020.emnlp-main.157.txt,2020,6 Discussion and Recommendations,this has significant implications for bias in tasks like co-reference resolution and text generation.
2020.emnlp-main.157.txt,2020,6 Discussion and Recommendations,"this is especially timely with singular “they” as merriamwebster’s 2019 word of the year (dwyer, 2019)."
2020.emnlp-main.157.txt,2020,6 Discussion and Recommendations,this will support applications the rely on technologies like conference resolution.
2020.emnlp-main.157.txt,2020,6 Discussion and Recommendations,we find that pretrained models amplify biases in the data because linguistic differences that are not biases can become biases in the models.
2020.emnlp-main.157.txt,2020,6 Discussion and Recommendations,we recommend that creators of language models include identity words as full tokens.
2020.emnlp-main.157.txt,2020,6 Discussion and Recommendations,we recommend that creators of language models use the methods introduced in this paper for partially-synthetic data generation to diagnose potential bias in their models and that this text generation strategy is explored for other applications.
2020.emnlp-main.157.txt,2020,6 Discussion and Recommendations,"we recommend that creators of widely used english syntactic parsers and part-of-speech taggers ensure that all unambiguous pronouns, including “hers”, “theirs”, and “themself”, are correctly identified as pronouns."
2020.emnlp-main.158.txt,2020,7 Conclusion,"experiments reveal that continual learning algorithms struggle at composing phrases which have a very large search space, and show very limited generalization to novel compositions."
2020.emnlp-main.158.txt,2020,7 Conclusion,future works include looking into models and continual learning algorithms to better address the challenge.
2020.emnlp-main.158.txt,2020,7 Conclusion,"to facilitate study on viscoll, we generate continuously shifting data-stream to construct two datasets, namely coco-shift and flickr-shift, and establish evaluation protocols."
2020.emnlp-main.158.txt,2020,7 Conclusion,viscoll presents two main challenges: continual learning and compositionality generalization.
2020.emnlp-main.158.txt,2020,7 Conclusion,we benchmark our proposed datasets with extensive analysis using state-of-theart continual learning methods.
2020.emnlp-main.158.txt,2020,7 Conclusion,"we propose viscoll, a novel continual learning setup for visually grounded language acquisition."
2020.emnlp-main.159.txt,2020,5 Conclusions,detailed analysis is also provided to help future works investigate other critical feature enrichment and alignment methods for this task.
2020.emnlp-main.159.txt,2020,5 Conclusions,we evaluate our model on flickr30k entities and achieve substantial improvements over the previous state-of-the-art methods with both weakly-supervised and unsupervised training strategies.
2020.emnlp-main.159.txt,2020,5 Conclusions,"we present a multimodal alignment framework, a novel method with fine-grained visual and textual representations for phrase localization, and we train it under a weakly-supervised setting, using a contrastive objective to guide the alignment between visual and textual representations."
2020.emnlp-main.16.txt,2020,7 Future Work & Conclusion,"finally, while our results naturally lead to the conclusion that we should continue to pursue models with ever more pretraining, such as gpt-3 (brown et al., 2020), we do not wish to suggest that this will be the only or best way to build models with stronger inductive biases."
2020.emnlp-main.16.txt,2020,7 Future Work & Conclusion,future work might use msgs as a diagnostic tool to measure how effectively new model architectures and selfsupervised pretraining tasks can more efficiently equip neural networks with better inductive biases.
2020.emnlp-main.16.txt,2020,7 Future Work & Conclusion,our experiments shed light on the relationship between pretraining data and an inductive bias towards linguistic generalization.
2020.emnlp-main.16.txt,2020,7 Future Work & Conclusion,"our results indicate that, although some abstract linguistic features are learnable from relatively small amounts of pretraining data, models require significant pretraining after discovering these features to develop a bias towards using them when generalizing."
2020.emnlp-main.16.txt,2020,7 Future Work & Conclusion,these models could prove to be a helpful resource for future studies looking to study learning curves of various kinds with respect to the quantity of pretraining data.
2020.emnlp-main.16.txt,2020,7 Future Work & Conclusion,this gives some insight into why extensive pretraining helps general-purpose neural networks adapt to downstream tasks with relative ease.
2020.emnlp-main.16.txt,2020,7 Future Work & Conclusion,"we also introduce msgs, a new diagnostic dataset for probing the inductive biases of learning algorithms using the poverty of the stimulus design and inoculation, and also introduce a set of 12 roberta models we pretrain on smaller data quantities."
2020.emnlp-main.160.txt,2020,4 Discussion,"by encouraging words to compete to claim responsibility for images, we “sharpen” the resulting image/word associations."
2020.emnlp-main.160.txt,2020,4 Discussion,"conversely, the word “bright” outside the context of streeteasy may not be “visually concrete” (according to human judgment); nonetheless, it frequently co-occurs with images of sunlit hardwood floors."
2020.emnlp-main.160.txt,2020,4 Discussion,"for example, the word “gristedes” (the name of a super market chain) appears in streeteasy documents, but users rarely post photographs of the supermarkets themselves."
2020.emnlp-main.160.txt,2020,4 Discussion,"given the lexical and visual identifiability issues explored in §1, incorporating prior human concreteness judgments (e.g., nelson et al.(2004)) for vocabulary items might enable entsharp to learn for these sorts of ambiguous lexical items."
2020.emnlp-main.160.txt,2020,4 Discussion,"however, finding an appropriate balance of domain-specific flexibility versus alignment with human priors could pose a significant challenge."
2020.emnlp-main.160.txt,2020,4 Discussion,it is motivated by the unlabeled multimodal data that exists in abundance rather than relying on expensive custom datasets.
2020.emnlp-main.160.txt,2020,4 Discussion,one area for future work would be to better identify and model words that either don’t have a visual grounding or whose identified visual grounding doesn’t align with human expectation.
2020.emnlp-main.160.txt,2020,4 Discussion,"the method is effective at finding contextual lexical groundings of words in unlabeled multi-image, multi-sentence documents even in the presence of high cross-document similarity."
2020.emnlp-main.160.txt,2020,4 Discussion,"we present entsharp, a simple clustering-based algorithm for learning image groundings for words."
2020.emnlp-main.161.txt,2020,5 Conclusion,"in this paper, we present a hierarchical encoder for video+language omni-representation pre-training."
2020.emnlp-main.161.txt,2020,5 Conclusion,novel pre-training tasks are proposed to capture temporal alignment both locally and globally.
2020.emnlp-main.161.txt,2020,5 Conclusion,"our hero model presents a hierarchical architecture, consisting of cross-modal transformer and temporal transformer for multi-modal fusion."
2020.emnlp-main.161.txt,2020,5 Conclusion,"pretrained on two large-scale video datasets, hero exceeds state of the art by a significant margin when transferred to multiple video-and-language tasks."
2020.emnlp-main.161.txt,2020,5 Conclusion,two new datasets on text-based video-moment retrieval and video qa are introduced to serve as additional benchmarks for downstream evaluation.
2020.emnlp-main.161.txt,2020,5 Conclusion,"we consider extension of our model to other videoand-language tasks as future work, as well as developing more well-designed pre-training tasks."
2020.emnlp-main.162.txt,2020,7 Conclusion,"in order to overcome the challenges in grounded language, we develop the vokenizer with contextual token-image matching models and use it to vokenize the language corpus."
2020.emnlp-main.162.txt,2020,7 Conclusion,"in this paper, we explored the possibility of utilizing visual supervision to language encoders."
2020.emnlp-main.162.txt,2020,7 Conclusion,"supervised by these generated vokens, we observe a significant improvement over the purely self-supervised language model on multiple language tasks."
2020.emnlp-main.163.txt,2020,7 Summary Of Exploitable Weaknesses and Defense Directions,adversaries can easily exploit this fact to create misleading disinformation by generating fake articles and combining them with manually sourced images and captions.
2020.emnlp-main.163.txt,2020,7 Summary Of Exploitable Weaknesses and Defense Directions,"based on the findings from the user evaluation, humans may be easily deceived by articles generated by sota models if they are not attuned to noticing possible visualsemantic inconsistencies between the article text and images."
2020.emnlp-main.163.txt,2020,7 Summary Of Exploitable Weaknesses and Defense Directions,"encouragingly, our experimental results suggest that visual-semantic consistency is an important and promising research area in our defense against neural news."
2020.emnlp-main.163.txt,2020,7 Summary Of Exploitable Weaknesses and Defense Directions,"last but not least, didan and neuralnews may be leveraged to supplement fact verification in detecting humanwritten misinformation in general by evaluating visual-semantic consistency."
2020.emnlp-main.163.txt,2020,7 Summary Of Exploitable Weaknesses and Defense Directions,other interesting avenues for future research is to understand the importance of metadata in this multimodal setting and investigating counter-attacks to improved generators that incorporate image-text consistency.
2020.emnlp-main.163.txt,2020,7 Summary Of Exploitable Weaknesses and Defense Directions,"we hope future work will address any potential limitations of this work, such as expanding the dataset to evaluate generalization across different news sources, and a larger variety of neural generators."
2020.emnlp-main.163.txt,2020,7 Summary Of Exploitable Weaknesses and Defense Directions,"while this is not entirely representative of all the future challenges presented by neural fake news, we believe that this comprehensive study will provide an effective initial defense mechanism against articles with images and captions."
2020.emnlp-main.164.txt,2020,6 Conclusion,extensive experiments on four semeval datasets demonstrate that softproto greatly boosts the performance of the typical ate methods and introduces small computational cost.
2020.emnlp-main.164.txt,2020,6 Conclusion,"for this purpose, we resort to the language models for automatically generating soft prototypes and then design a gating conditioner for utilizing them."
2020.emnlp-main.164.txt,2020,6 Conclusion,"in this paper, we present a general softproto framework to enhance the ate task."
2020.emnlp-main.164.txt,2020,6 Conclusion,"rather than designing elaborated sequence taggers, we turn to correlate samples with each other through soft prototypes."
2020.emnlp-main.164.txt,2020,6 Conclusion,the performance of softproto can be further improved after introducing the large-scale external unlabeled data like yelp and amazon reviews.
2020.emnlp-main.165.txt,2020,5 Conclusion and Future Work,"as to future work, we plan to explore how to jointly extract entities and relations in federated settings."
2020.emnlp-main.165.txt,2020,5 Conclusion and Future Work,"in this paper, we propose a privacy-preserving medical relation extraction model based on federated learning, namely feded."
2020.emnlp-main.165.txt,2020,5 Conclusion and Future Work,our experiments on three benchmark datasets illustrate the advantages of our approach over previous federated algorithms.
2020.emnlp-main.165.txt,2020,5 Conclusion and Future Work,"the main obstacle of applying federated learning to medical relation extraction is communication bottleneck, which is caused by the need to upload cumbersome parameters."
2020.emnlp-main.165.txt,2020,5 Conclusion and Future Work,"to overcome this bottleneck, we leverage a knowledge distillation based strategy, which uses the uploaded predictions of ensemble local models to train the central model without requiring uploading cumbersome parameters."
2020.emnlp-main.166.txt,2020,6 Conclusion,"the experimental results on this dataset show that the correlations between product attributes and values are valuable for this task, and visual information should be selectively used."
2020.emnlp-main.166.txt,2020,6 Conclusion,we collect a multimodal e-commerce product attribute value extraction (mepave) dataset2.
2020.emnlp-main.166.txt,2020,6 Conclusion,"we jointly tackle the tasks of e-commerce product attribute prediction and value extraction from multiple aspects towards the relationship between product attributes and values, and we prove that the models can benefit a lot from visual product information by the aid of global and regional visual gates."
2020.emnlp-main.167.txt,2020,7 Conclusions and Future Work,"as the core of the pipeline, the open-domain information expression (oix) task is thoroughly studied, and an open information annotation (oia) is proposed as a solution to the oix task."
2020.emnlp-main.167.txt,2020,7 Conclusions and Future Work,"there are many potential directions for future work on oia, including 1) more labeled data; 2) better learning algorithm; 3) becoming crosslingual by adding support for more natural languages; 4) porting existing oie strategies on oia and evaluating the performance compared with the original ones."
2020.emnlp-main.167.txt,2020,7 Conclusions and Future Work,this paper proposes a reusable and adaptive pipeline to construct oie systems.
2020.emnlp-main.167.txt,2020,7 Conclusions and Future Work,we discuss how to port the strategies of various existing oie algorithms to the oia graph.
2020.emnlp-main.167.txt,2020,7 Conclusions and Future Work,we label data for oia annotation and build a rule-based baseline method to convert sentences into oia graphs.
2020.emnlp-main.168.txt,2020,7 Conclusion,"furthermore, our lm after structure-aware fine-tuning gave significantly improved performance for both semantic-dependent and syntactic-dependent tasks, also yielding most task-related and interpretable syntactic structures."
2020.emnlp-main.168.txt,2020,7 Conclusion,"results showed that structure-aware transformer retrofitted via our proposed method achieved better language perplexity, inducing highquality syntactic phrase."
2020.emnlp-main.168.txt,2020,7 Conclusion,"to relieve the conflict of structure learning and semantic learning in transformer lm, we proposed a middlelayer structure learning strategy under a multitasks scheme."
2020.emnlp-main.168.txt,2020,7 Conclusion,we adopted the syntax distance to encode both the constituency and dependency structure.
2020.emnlp-main.168.txt,2020,7 Conclusion,we presented a retrofitting method for structureaware transformer-based language model.
2020.emnlp-main.169.txt,2020,7 Conclusion,"compared with existing gcns and sans, ldgcns maintain a better balance between parameter efficiency and model capacity."
2020.emnlp-main.169.txt,2020,7 Conclusion,"in future work, we would like to investigate methods to stabilize the training of weight tied models and apply our model on other tasks in natural language generation."
2020.emnlp-main.169.txt,2020,7 Conclusion,"in this paper, we propose ldgcns for amr-totext generation."
2020.emnlp-main.169.txt,2020,7 Conclusion,ldgcns outperform state-of-the-art models on amr-to-text generation.
2020.emnlp-main.17.txt,2020,7 Conclusion,"extensive experimental results on representative attention models demonstrate that our approach can significantly improve the diversity in multi-head attention, resulting in more expressiveness attention models with performance improvement on a wide range of tasks."
2020.emnlp-main.17.txt,2020,7 Conclusion,our bayesian framework explains the long-standing question of why and how multi-head attention affects model performance.
2020.emnlp-main.17.txt,2020,7 Conclusion,we apply particle-optimization sampling to train repulsive multi-head attention with no additional trainable parameters nor explicit regularizers.
2020.emnlp-main.17.txt,2020,7 Conclusion,we propose a principled way of understanding multi-head attention from a bayesian perspective.
2020.emnlp-main.170.txt,2020,8 Conclusion,we analyze beam search as a decoding strategy for text generation models by framing it as the solution to an exact decoding problem.
2020.emnlp-main.170.txt,2020,8 Conclusion,"we hypothesize that beam search has an inductive bias which can be linked to the promotion of uniform information density (uid), a theory from cognitive science regarding even distribution of information in linguistic signals."
2020.emnlp-main.170.txt,2020,8 Conclusion,we observe a strong relationship between variance of surprisals (an operationalization of uid) and bleu in our experiments with nmt models.
2020.emnlp-main.170.txt,2020,8 Conclusion,"with the aim of further exploring decoding strategies for neural text generators in the context of uid, we design a set of objectives to explicitly encourage uniform information density in text generated from neural probabalistic models and find that they alleviate the quality degradation typically seen with increased beam widths."
2020.emnlp-main.171.txt,2020,8 Conclusions,"differentiable relaxation models (using softmax and sparsemax) are the easiest to optimize to high downstream accuracy, but they fail to correctly identify the latent clusters."
2020.emnlp-main.171.txt,2020,8 Conclusions,"however, the lack of gold-truth latent structures makes it impossible to assess recovery performance."
2020.emnlp-main.171.txt,2020,8 Conclusions,"in this work, we provide a novel motivation for straight-through estimator (ste) and spigot, based on pulling back the downstream loss."
2020.emnlp-main.171.txt,2020,8 Conclusions,"on structured nlp experiments, relaxations (sparsemap and marginals) tend to overall perform better and be more stable than straightthrough variants in terms of classification accuracy."
2020.emnlp-main.171.txt,2020,8 Conclusions,"unstructured controlled experiments suggest that our new algorithms, which use the cross-entropy loss instead of the perceptron loss, can be more stable than spigot while accurately disentangling the latent variable."
2020.emnlp-main.171.txt,2020,8 Conclusions,"we derive promising new algorithms, and novel insight into existing ones."
2020.emnlp-main.171.txt,2020,8 Conclusions,"we hope that our insights, including some of our negative results, may encourage future research on learning with latent structures."
2020.emnlp-main.172.txt,2020,7 Conclusions,"by sampling from the posterior distribution of these models, we estimate the likelihood that a given tagger will be better than, worse than, or practically equivalent to other taggers on three different evaluation metrics."
2020.emnlp-main.172.txt,2020,7 Conclusions,it also allows us to incorporate results obtained across multiple data sets and to make populationlevel inferences.
2020.emnlp-main.172.txt,2020,7 Conclusions,"the results provide new insights into the strengths and weaknesses of english part-of-speech tagging models, complementing other approaches to model comparison and interpretation."
2020.emnlp-main.172.txt,2020,7 Conclusions,these estimates are valid insofar as the data sets used to estimate the bayesian models comprise a representative sample of a coherent population of data sets.
2020.emnlp-main.172.txt,2020,7 Conclusions,"this method provides a principled way to perform statistical model comparison using k-fold crossvalidation, a data-efficient evaluation technique."
2020.emnlp-main.172.txt,2020,7 Conclusions,we compare the performance of six part-of-speech taggers on two data sets using twenty repetitions of a ten-fold cross-validation procedure and statistical system comparison performed using hierarchical bayesian models.
2020.emnlp-main.172.txt,2020,7 Conclusions,we finally compare the results obtained with the proposed method to those computed using randomly generated splits and traditional nhst-based model comparison.
2020.emnlp-main.173.txt,2020,6 Conclusion,"in the future, we are interested in social science topics, such as modeling the causal effect between mental health and the suicide decisions reflected through social media, which may help predict and stop the final decisions."
2020.emnlp-main.173.txt,2020,6 Conclusion,"in this paper, we investigated the mtl problem with logically dependent tasks."
2020.emnlp-main.173.txt,2020,6 Conclusion,the model achieves sota results on 6 out of 7 subtasks and improves the consistency of predicted results of different subtasks.
2020.emnlp-main.173.txt,2020,6 Conclusion,we first analyze mtl models from the perspective of causal inference and then propose a model cmtl to utilize task dependencies properly.
2020.emnlp-main.174.txt,2020,7 Conclusion,"also, the binary masks learned by our method have low sparsity such that inference speed is not improved."
2020.emnlp-main.174.txt,2020,7 Conclusion,developing methods improving both memory and inference efficiency without sacrificing task performance can open the possibility of widely deploying the powerful pretrained language models to more nlp applications.
2020.emnlp-main.174.txt,2020,7 Conclusion,extensive experiments show that masking yields performance comparable to finetuning on a series of nlp tasks.
2020.emnlp-main.174.txt,2020,7 Conclusion,"future work may explore the possibility of applying masking to the pretrained multilingual encoders like mbert (devlin et al., 2019) and xlm (conneau and lample, 2019)."
2020.emnlp-main.174.txt,2020,7 Conclusion,"instead of updating the pretrained parameters, we only train one set of binary masks per task to select critical parameters."
2020.emnlp-main.174.txt,2020,7 Conclusion,intrinsic evaluations show that binary masked models extract valid and generalizable representations for downstream tasks.
2020.emnlp-main.174.txt,2020,7 Conclusion,"leaving the pretrained parameters unchanged, masking is much more memory efficient when several tasks need to be solved."
2020.emnlp-main.174.txt,2020,7 Conclusion,"moreover, we demonstrate that the minima obtained by finetuning and masking can be easily connected by a line segment, confirming the effectiveness of applying masking to pretrained language models."
2020.emnlp-main.174.txt,2020,7 Conclusion,our code is available at: https://github.com/ ptlmasking/maskbert.
2020.emnlp-main.174.txt,2020,7 Conclusion,"we have presented masking, an efficient alternative to finetuning for utilizing pretrained language models like bert/roberta/distilbert."
2020.emnlp-main.175.txt,2020,7 Conclusion and Future Work,"in the future, we will select context sentences in larger candidate space, and explore more effective ways to extend our approach to select target-side context sentences."
2020.emnlp-main.175.txt,2020,7 Conclusion and Future Work,the candidate context sentences are scored and selected by two proposed strategies.
2020.emnlp-main.175.txt,2020,7 Conclusion and Future Work,we propose a dynamic selection method to choose variable sizes of context sentences for documentlevel translation.
2020.emnlp-main.175.txt,2020,7 Conclusion and Future Work,"we train the whole model via reinforcement learning, and design a novel reward to encourage the selection of useful context sentences."
2020.emnlp-main.175.txt,2020,7 Conclusion and Future Work,"when applied to existing docnmt models, our approach can improve translation quality significantly."
2020.emnlp-main.176.txt,2020,6 Conclusion,experimental results on different model architectures and language pairs demonstrate the effectiveness and universality of the data rejuvenation approach.
2020.emnlp-main.176.txt,2020,6 Conclusion,"future directions include exploring advanced identification and rejuvenation models that can better reflect the learning abilities of nmt models, as well as validating on other nlp tasks such as dialogue and summarization."
2020.emnlp-main.176.txt,2020,6 Conclusion,"in this study, we propose data rejuvenation to exploit the inactive training examples for neural machine translation on large-scale datasets."
2020.emnlp-main.176.txt,2020,6 Conclusion,"the proposed data rejuvenation scheme is a general framework where one can freely define, for instance, the identification and rejuvenation models."
2020.emnlp-main.177.txt,2020,7 Conclusions and Future Work,"although we focus on pronoun translations, our fine-tuning method is generic and can be used to correct other kinds of errors in machine translations, like named entities or other rare words."
2020.emnlp-main.177.txt,2020,7 Conclusions and Future Work,"in future work, we will explore other such applications of our proposed methods."
2020.emnlp-main.177.txt,2020,7 Conclusions and Future Work,"we demonstrated the effectiveness of our methods on different languages and testsets, also reporting improved pronoun translations."
2020.emnlp-main.177.txt,2020,7 Conclusions and Future Work,"we have proposed a class of conditional generativediscriminative losses to increase the learning potential of nmt models, showing that it is possible to leverage “unlearned” training data to further improve an mt model, by strategically filtering the data and applying additional targeted losses."
2020.emnlp-main.178.txt,2020,5 Conclusions,experimental results on both chineseenglish and german-english show that our model outperforms the previous state-of-the-art.
2020.emnlp-main.178.txt,2020,5 Conclusions,"further, we propose a refined-method to extract fine-grained mus to reduce latency."
2020.emnlp-main.178.txt,2020,5 Conclusions,"in this paper, we propose a novel adaptive segmentation policy for simultaneous translation."
2020.emnlp-main.178.txt,2020,5 Conclusions,"motivated by human interpreters, the model constantly reads streaming text and dynamically segments it into meaning units."
2020.emnlp-main.178.txt,2020,5 Conclusions,the method obtains a good trade-off between translation accuracy and latency and can be easily implemented into a practical simultaneous translation system.
2020.emnlp-main.178.txt,2020,5 Conclusions,"we first generate training data for mu via a translation-prefix based method, keeping consistency between the segmentation model and the translation model."
2020.emnlp-main.179.txt,2020,7 Conclusion and Future Works,extensive evaluations on both the public benchmark and large-scale industrial dataset quantitively and qualitatively demonstrate the effectiveness of the mgl.
2020.emnlp-main.179.txt,2020,7 Conclusion and Future Works,"in the future, the proposed mgl method can potentially applied to more cross-lingual natural language understanding (xlu) tasks (conneau et al., 2018b; wang et al., 2019; lewis et al., 2019; karthikeyan et al., 2020), and be generalized to learn to learn for domain adaptation (blitzer et al., 2007), representation learning (shen et al., 2018), multi-task learning (shen et al., 2019) problems, etc. universal syntactic interpretations are valuable language interpretations, which have been developed in years of study."
2020.emnlp-main.179.txt,2020,7 Conclusion and Future Works,"in this paper, we propose a novel mgl method for task-aware clt adaptation of mplm by leveraging historical clt experiences."
2020.emnlp-main.18.txt,2020,5 Conclusions,"as kermit has a clear description of the used syntactic subtrees and gives the possibility of visualizing how syntactic information is exploited during inference, it opens the possibility of devising models to include explicit syntactic inference rules in the training process."
2020.emnlp-main.18.txt,2020,5 Conclusions,"finally, kermit is in the line of research of human-in-the-loop artificial intelligence (zanzotto, 2019), since it gives the opportunity to track how human knowledge is used by learning algorithms."
2020.emnlp-main.18.txt,2020,5 Conclusions,"in this paper, we introduced kermit to show that these interpretations can be effectively used in combination with universal sentence embeddings produced from scratch."
2020.emnlp-main.18.txt,2020,5 Conclusions,"moreover, kermitviz allows us to explain how syntactic information is used in classification decisions within networks combining kermit, on the one side, and bert or xlnet on the other."
2020.emnlp-main.18.txt,2020,5 Conclusions,we also showed that kermit can be easily used in situations where training large transformers is extremely difficult.
2020.emnlp-main.180.txt,2020,6 Conclusion,"finally, the analyses we performed on the underlying characteristics of our model show that typological features are crucial for zero-shot languages."
2020.emnlp-main.180.txt,2020,6 Conclusion,"in this way, the model retains high per-language performance in the training data and achieves better zero-shot transfer."
2020.emnlp-main.180.txt,2020,6 Conclusion,"udapter, trained on a concatenation of typologically diverse languages (kulmizev et al., 2019), outperforms strong monolingual and multilingual baselines on the majority of both high-resource and low-resource (zero-shot) languages, which reflects its strong balance between per-language capacity and maximum sharing."
2020.emnlp-main.180.txt,2020,6 Conclusion,"we have presented udapter, a multilingual dependency parsing model that learns to adapt languagespecific parameters on the basis of adapter modules (rebuffi et al., 2018; houlsby et al., 2019) and the contextual parameter generation (cpg) method (platanios et al., 2018) which is in principle applicable to a range of multilingual nlp tasks."
2020.emnlp-main.180.txt,2020,6 Conclusion,"while adapters provide a more general tasklevel adaptation, cpg enables language-specific adaptation, defined as a function of language embeddings projected from linguistically curated typological features."
2020.emnlp-main.181.txt,2020,6 Conclusions,experimental results across three sequence labeling datasets demonstrated that the proposed method significantly outperforms the previous methods.
2020.emnlp-main.181.txt,2020,6 Conclusions,"in addition, the proposed model can capture different ranges of label dependencies and word-label interactions in parallel, which can avoid the use of viterbi decoding of the crf for a faster prediction."
2020.emnlp-main.181.txt,2020,6 Conclusions,"in this work, we introduce a novel sequence labeling framework that incorporates bayesian neural networks to estimate model uncertainty."
2020.emnlp-main.181.txt,2020,6 Conclusions,the proposed method can selectively refine the uncertain labels to avoid the side effects of the refinement on correct labels.
2020.emnlp-main.181.txt,2020,6 Conclusions,we find that the model uncertainty can effectively indicate the labels with a high probability of being wrong.
2020.emnlp-main.182.txt,2020,8 Conclusion,building an effective adversarial attacker for structured prediction models is challenging.
2020.emnlp-main.182.txt,2020,8 Conclusion,"in this paper, we propose a novel framework to attack structured prediction models in nlp."
2020.emnlp-main.182.txt,2020,8 Conclusion,our attack experiments on dependency parsing and pos tagging show that our proposed framework can produce high-quality sentences that can effectively attack current state-of-the-art models.
2020.emnlp-main.182.txt,2020,8 Conclusion,our defense experiments show that adversarial training using the adversarial samples generated by our model can be used to improve the original model.
2020.emnlp-main.182.txt,2020,8 Conclusion,our framework consists of a structured-output evaluation criterion based on reference models and a seq2seq sentence generator.
2020.emnlp-main.182.txt,2020,8 Conclusion,the biggest challenge is the sensitivity of the output to small perturbations in the input in structured prediction.
2020.emnlp-main.182.txt,2020,8 Conclusion,"we believe that our framework is general and can be applied to many other structured prediction tasks in nlp, such as neural machine translation, semantic parsing and so on."
2020.emnlp-main.182.txt,2020,8 Conclusion,we propose to utilize reinforcement learning to train the sentence generator based on the evaluation criterion.
2020.emnlp-main.183.txt,2020,6 Conclusion,"based on the position-aware tagging scheme, we propose a novel approach jet that is capable of jointly extracting the aspect sentiment triplets."
2020.emnlp-main.183.txt,2020,6 Conclusion,future work includes finding applications of our novel tagging scheme in other tasks involving extracting triplets as well as extending our approach to support other tasks within sentiment analysis.
2020.emnlp-main.183.txt,2020,6 Conclusion,"in this work, we propose a novel position-aware tagging scheme by enriching label expressiveness to address a limitation associated with existing works."
2020.emnlp-main.183.txt,2020,6 Conclusion,"such a tagging scheme is able to specify the connection among three elements – a target, the target sentiment as well as an opinion span in an aspect sentiment triplet for the aste task."
2020.emnlp-main.183.txt,2020,6 Conclusion,we also design factorized feature representations so as to effectively capture the interaction.
2020.emnlp-main.183.txt,2020,6 Conclusion,we conduct extensive experiments and results show that our models outperform strong baselines significantly with detailed analysis.
2020.emnlp-main.184.txt,2020,6 Conclusion,"from a multimodal perspective, we introduce effective ways of integrating visual features and show that explicit object region information consistently outperforms commonly used global features."
2020.emnlp-main.184.txt,2020,6 Conclusion,our experiments reveal that integrating visual context lowers the latency for heuristic policies while retaining the quality of the translations.
2020.emnlp-main.184.txt,2020,6 Conclusion,"our qualitative analysis illustrates that the models are capable of resolving linguistic particularities, including gender marking and word order handling by exploiting the associated visual cues."
2020.emnlp-main.184.txt,2020,6 Conclusion,"under low-latency wait-k policies, the visual cues are highly impactful with quality improvements of almost 3 bleu points compared to unimodal baselines."
2020.emnlp-main.184.txt,2020,6 Conclusion,"we believe that our work can also benefit research in multimodal speech translation (niehues et al., 2019) where the audio stream is accompanied by a video stream."
2020.emnlp-main.184.txt,2020,6 Conclusion,"we hope that future research continues this line of work, especially by finding novel ways to devise adaptive policies – such as reinforcement learning models with the visual modality."
2020.emnlp-main.184.txt,2020,6 Conclusion,we present the first thorough investigation of the utility of visual context for the task of simultaneous machine translation.
2020.emnlp-main.185.txt,2020,6 Conclusion and Future Work,"all xcopa instances are aligned across 11 languages, which enables cross-lingual comparisons."
2020.emnlp-main.185.txt,2020,6 Conclusion and Future Work,"finally, we investigated resource-lean adaptation of pretrained multilingual models to out-of-sample languages, exploiting only small monolingual corpora and/or bilingual dictionaries, reporting notable gains."
2020.emnlp-main.185.txt,2020,6 Conclusion and Future Work,"the language selection was informed by variety sampling, in order to maximise diversity in terms of typological features, geographical macro-area, and language family."
2020.emnlp-main.185.txt,2020,6 Conclusion and Future Work,this allows us to test the robustness of machine learning models for an array of rare phenomena displayed by the chosen languages.
2020.emnlp-main.185.txt,2020,6 Conclusion and Future Work,"we also ran a series of cross-lingual transfer experiments, evaluating state-of-the-art transfer methods based on multilingual pretraining and finetuning on english."
2020.emnlp-main.185.txt,2020,6 Conclusion and Future Work,we hope that this new challenging evaluation set will foster further research in multilingual commonsense reasoning and cross-lingual transfer.
2020.emnlp-main.185.txt,2020,6 Conclusion and Future Work,"we observed that, although these methods perform better than chance, they still lag significantly behind the monolingual setting where test data are translated into english, due to the ‘curse of multilinguality.’ in addition, we verified that spurious correlations are insufficient to solve this new task by creating ‘adversarial’ variants of the dataset, where premises or prompts are masked, thus showing that robust causal reasoning is indeed required to solve xcopa."
2020.emnlp-main.185.txt,2020,6 Conclusion and Future Work,"we presented the cross-lingual choice of plausible alternatives (xcopa), a multilingual evaluation benchmark for causal commonsense reasoning."
2020.emnlp-main.186.txt,2020,6 Further Discussion and Conclusion,"all these inspired us to replace the standard matrix rank with effective rank when computing the condition number, and to introduce the statistic of effective condition number in §2.1."
2020.emnlp-main.186.txt,2020,6 Further Discussion and Conclusion,"another recent study (yin and shen, 2018) employed perturbation analysis to study the robustness of embedding spaces to noise in monolingual settings, and established that it is also related to effective dimensionality of the embedding space."
2020.emnlp-main.186.txt,2020,6 Further Discussion and Conclusion,"both measures leverage information from singular values in different ways: econd-hm uses the ratio between two singular values, and is grounded in linear algebra and numerical analysis (blum, 2014; roy and vetterli, 2007), while svg directly utilizes the full range of singular values."
2020.emnlp-main.186.txt,2020,6 Further Discussion and Conclusion,"however, previous work has taken an empirical approach by simply tuning embedding dimensionality to evaluation tasks at hand (wang, 2019; raunak et al., 2019; carrington et al., 2019)."
2020.emnlp-main.186.txt,2020,6 Further Discussion and Conclusion,in future research we also plan an in-depth study of these factors and their relation to our spectral analysis.
2020.emnlp-main.186.txt,2020,6 Further Discussion and Conclusion,"our findings, suggesting that it is possible to effectively combine these two types of language distance measures, call for further research that will advance our understanding of: 1) what knowledge is captured in monolingual and cross-lingual embedding spaces (gerz et al., 2018; pires et al., 2019; artetxe et al., 2020); 2) how that knowledge complements or overlaps with linguistic knowledge compiled into lexical-semantic and typological databases (dryer and haspelmath, 2013; wichmann et al., 2018; ponti et al., 2019); and 3) how to use the combined knowledge for more effective transfer in cross-lingual nlp applications (ponti et al., 2018; eisenschlos et al., 2019)."
2020.emnlp-main.186.txt,2020,6 Further Discussion and Conclusion,"our intention, on the other hand, is to extract the true embedding dimensionality directly from the embedding space."
2020.emnlp-main.186.txt,2020,6 Further Discussion and Conclusion,"our study is also the first to compare language distance measures that are based on discrete linguistic information (littell et al., 2017) with measures of isomorphism (i.e., our spectral-based measures, is, gh), which can also be used as proxy language distance measures."
2020.emnlp-main.186.txt,2020,6 Further Discussion and Conclusion,our use of effective rank in improving the condition number (via effective condition number) is also inspired by recent work that aimed to automatically detect true dimensionality of embedding spaces.
2020.emnlp-main.186.txt,2020,6 Further Discussion and Conclusion,"the differences in embedding spaces of different languages do not only depend on linguistic properties of the languages in consideration, but also on other factors such as the chosen training algorithm, underlying training domain, or training data size and quality (søgaard et al., 2018; arora et al., 2019; vulic et al.´ , 2020)."
2020.emnlp-main.186.txt,2020,6 Further Discussion and Conclusion,"these might range from choosing source languages for transfer in low-data regimes, over monolingual word vector induction guided by the spectral statistics, even to more effective hyperparameter search."
2020.emnlp-main.186.txt,2020,6 Further Discussion and Conclusion,"this work introduces two spectral-based measures, svg and econd-hm, that excel in predicting performance on a variety of cross-lingual tasks."
2020.emnlp-main.186.txt,2020,6 Further Discussion and Conclusion,we believe that the main insights from this study will inform and guide different cross-lingual transfer learning methods and scenarios in future work.
2020.emnlp-main.186.txt,2020,6 Further Discussion and Conclusion,"we believe that this discrepancy does not constitute a shortcoming of our analyses, but rather the opposite: spectral-based methods maintain their high correlations in the downstream tasks as well, and this supports the notion that these measures might indeed capture deeper linguistic information than mere similarities between embedding spaces."
2020.emnlp-main.186.txt,2020,6 Further Discussion and Conclusion,"we suspect that the use of the full range of singular values is what makes svg more robust across different tasks and datasets, compared to econd-hm that shows greater variance, as observed in our results above."
2020.emnlp-main.186.txt,2020,6 Further Discussion and Conclusion,"while the spectral methods are computed solely on word vectors from wikipedia, the results in the downstream tasks are computed with different sets of embeddings (e.g., multilingual embeddings for dependency parsing), or the embeddings are learnt during training (for pos tagging and mt)."
2020.emnlp-main.187.txt,2020,10 Conclusion,"moreover, our approach offers important advantages because we can evaluate projected languages with entries in only one of the views and can easily extend the language coverage."
2020.emnlp-main.187.txt,2020,10 Conclusion,"the benefits are noticeable in multilingual nmt tasks, like language clustering and ranking related languages for multilingual transfer."
2020.emnlp-main.187.txt,2020,10 Conclusion,we compute multi-view language representations with svcca using two sources: kb and nmtlearned vectors.
2020.emnlp-main.187.txt,2020,10 Conclusion,"we plan to study how to deeply incorporate our typologically-enriched embeddings in multilingual nmt, where there are promising avenues in parameter selection (sachan and neubig, 2018) and generation (platanios et al., 2018)."
2020.emnlp-main.187.txt,2020,10 Conclusion,"with a typological feature prediction task and the inference of phylogenetic trees, we showed that the knowledge and language relationship encoded in both sources is preserved in the combined representation."
2020.emnlp-main.188.txt,2020,6 Conclusions,extensive experiments show that our proposed method outperforms various established baselines.
2020.emnlp-main.188.txt,2020,6 Conclusions,"further, we propose aver model to predict answer veracity via tailored evidence ranking module."
2020.emnlp-main.188.txt,2020,6 Conclusions,"in this paper, we investigate the fact checking problem in product question answering forums, aiming to predict the answer veracity so as to provide more reliable online shopping environment."
2020.emnlp-main.188.txt,2020,6 Conclusions,"to this end, we introduce answerfact, an evidence-based fact checking datasets in qa settings."
2020.emnlp-main.189.txt,2020,6 Conclusion,"also, we showed the generalizability of blanc and its contextaware performance with the zero-shot supporting fact prediction task on the hotpotqa dataset."
2020.emnlp-main.189.txt,2020,6 Conclusion,block attention models the latent context words with negligible extra parameters and training/inference time.
2020.emnlp-main.189.txt,2020,6 Conclusion,"in this paper, we showed the importance of predicting an answer with the correct context of a given question."
2020.emnlp-main.189.txt,2020,6 Conclusion,the context words prediction task labels latent context words with the labeled answer-span and is used in a multi-task learning manner.
2020.emnlp-main.189.txt,2020,6 Conclusion,we proposed blanc with two novel ideas: context word prediction task and a block attention method that identifies an answer within the context of a given question.
2020.emnlp-main.189.txt,2020,6 Conclusion,"we showed that blanc increases reading comprehension performance, and we verify that the performance gain increases for complex examples (i.e., when the answer occurs two or more times in the passage)."
2020.emnlp-main.19.txt,2020,5 Conclusions,"as future work, we would like to investigate complementary attention mechanisms like those of reformer (kitaev et al., 2020) or routing transformer (roy et al., 2020), push scalability with ideas like those from revnet (gomez et al., 2017), and study the performance of etc in datasets with even richer structure."
2020.emnlp-main.19.txt,2020,5 Conclusions,"etc allows lifting weights from existing bert models, improving results significantly."
2020.emnlp-main.19.txt,2020,5 Conclusions,"notice that although our datasets contain a limited amount of structure (compared to graph datasets), our experiments show that etc was able to exploit this existing structure."
2020.emnlp-main.19.txt,2020,5 Conclusions,the ability to represent dataset structure in etc further improves the model quality.
2020.emnlp-main.19.txt,2020,5 Conclusions,"the key ideas are a new globallocal attention mechanism, coupled with relative position encodings and a cpc pre-training task."
2020.emnlp-main.19.txt,2020,5 Conclusions,"this paper introduced the extended transformer construction (etc), an architecture designed to (1) scale up the input length (linearly with input), and (2) encode structured inputs."
2020.emnlp-main.19.txt,2020,5 Conclusions,"we hypothesize that cpc helps the model train the usage of the higherlevel global input summary tokens, as cpc plays a role akin to mlm, but at the global input level."
2020.emnlp-main.19.txt,2020,5 Conclusions,we showed that significant gains can be obtained thanks to increased input sequence length.
2020.emnlp-main.190.txt,2020,6 Conclusions,"based on our work, we make the following recommendations to researchers who create or evaluate qa datasets: • test for generalizability: models are more valuable to real-world applications if they generalize."
2020.emnlp-main.190.txt,2020,6 Conclusions,discard questions that can be solved trivially by high overlap or extracting the first named entity.• be wary of cheating: good performance does not mean good understanding.
2020.emnlp-main.190.txt,2020,6 Conclusions,"in this work, we probed five qa datasets with six tasks and found that our models did not learn to generalize well, remained suspiciously robust to incorrect or missing data, and failed to handle variations in questions."
2020.emnlp-main.190.txt,2020,6 Conclusions,"include variations such as filler words or negation to existing questions to evaluate how well models have understood a question.• standardize dataset formats: when creating new datasets, consider following a standardized format, such as squad, to make cross-dataset evaluations simpler."
2020.emnlp-main.190.txt,2020,6 Conclusions,new qa model should report performance across multiple relevant datasets.• challenge the models: evaluating on too many easy questions can inflate our judgement of what a model has learned.
2020.emnlp-main.190.txt,2020,6 Conclusions,"probe datasets by adding noise, shuffling contexts, or providing incomplete input to ensure models aren’t taking shortcuts.• include variations: models should be prepared to handle a variety of questions."
2020.emnlp-main.190.txt,2020,6 Conclusions,the shortcomings in datasets and evaluation methods make it difficult to judge if models are learning reading comprehension.
2020.emnlp-main.190.txt,2020,6 Conclusions,these findings show that models learn simple heuristics around question-context overlap or entity types and pick up on underlying patterns in the datasets that allow them to remain robust to corrupt examples but not to valid variations.
2020.emnlp-main.191.txt,2020,5 Conclusion,discern explicitly builds the connection between entailment states of conditions and the final decisions.
2020.emnlp-main.191.txt,2020,5 Conclusion,"in future, we plan to explore how to incorporate discourse parsing into the current decision making model for end-to-end learning."
2020.emnlp-main.191.txt,2020,5 Conclusion,"in this paper, we present discern, a system that does discourse-aware entailment reasoning for conversational machine reading."
2020.emnlp-main.191.txt,2020,5 Conclusion,one possibility would be to frame them as multi-task learning with a common (shared) encoder.
2020.emnlp-main.191.txt,2020,5 Conclusion,results on the sharc benchmark shows that discern outperforms existing methods by a large margin.
2020.emnlp-main.191.txt,2020,5 Conclusion,we also conduct comprehensive analyses to unveil the limitations of discern and challenges for sharc.
2020.emnlp-main.192.txt,2020,7 Conclusion,each problem is a multiple-choice question that asks contingency between basic events.
2020.emnlp-main.192.txt,2020,7 Conclusion,"for commonsense acquisition from text, there is a problem that every commonsense is not written in text because of reporting bias (gordon and van durme, 2013)."
2020.emnlp-main.192.txt,2020,7 Conclusion,"in the future, we will make a model learn commonsense with the obtained dataset and consider applying it to semantic tasks, such as anaphora resolution and discourse parsing."
2020.emnlp-main.192.txt,2020,7 Conclusion,"in this paper, we proposed a scalable, low-bias, and low-cost method for building a commonsense inference dataset that combines automatic extraction from a corpus and crowdsourcing."
2020.emnlp-main.192.txt,2020,7 Conclusion,"to acquire a wider range of commonsense, it is possible to combine our method with other methods based on physical world resources, such as video captions used in swag."
2020.emnlp-main.192.txt,2020,7 Conclusion,"we also confirmed that the dataset contained low bias, and thus it can be used as a good benchmark for language understanding research."
2020.emnlp-main.192.txt,2020,7 Conclusion,"we applied the proposed method to a japanese web corpus and acquired 103,907 problems."
2020.emnlp-main.192.txt,2020,7 Conclusion,"while the human accuracy was high (88.9%), the bertlarge accuracy was reasonably low (76.0%)."
2020.emnlp-main.193.txt,2020,7 Conclusion,experiments show that components of our approach bring improvements and our full model significantly outperforms transformer-based baselines on both datasets.
2020.emnlp-main.193.txt,2020,7 Conclusion,"in this paper, we present fast, a graph-based reasoning approach utilizing fine-grained factual knowledge for deepfake detection of text."
2020.emnlp-main.193.txt,2020,7 Conclusion,model analysis further suggests that our model can distinguish the difference in the factual structure of machine-generated and human-written text.
2020.emnlp-main.193.txt,2020,7 Conclusion,"sentence representations are further composed through document-level aggregation for the final prediction, where the consistency and coherence of continuous sentences are sequentially modeled."
2020.emnlp-main.193.txt,2020,7 Conclusion,"we evaluate our system on a news-style dataset and a webtext-style dataset, whose fake instances are generated by grover and gpt-2 respectively."
2020.emnlp-main.193.txt,2020,7 Conclusion,"we represent the factual structure of a document as a graph, which is utilized to learn graph-enhanced sentence representations."
2020.emnlp-main.194.txt,2020,7 Conclusion,"by investigating such a large number of models, we provided an extensive comparison and fair baselines to combination methods, and were able to extensively analyze a large sample size."
2020.emnlp-main.194.txt,2020,7 Conclusion,"for instance, combining our approach with additional pre-training objectives such as the inverse cloze task (chang et al., 2020) could substantially increase the amount of training data for the large quantity of smaller forums."
2020.emnlp-main.194.txt,2020,7 Conclusion,researchers could also use our 140 domain-specific adapters and investigate further combination techniques to make them even more broadly applicable.
2020.emnlp-main.194.txt,2020,7 Conclusion,we clearly demonstrated the effectiveness and the relevance of zero-shot transfer in many realistic scenarios and believe that our work lays foundations for a wide range of research questions.
2020.emnlp-main.194.txt,2020,7 Conclusion,"we have shown that (1) bert models trained in a self-supervised manner on cqa forum data transfer well to all our benchmarks, even across distant domains; (2) training data size and domain similarity are not suitable for predicting the zeroshot transfer performances, revealing that a broad selection of source domains is crucial; (3) our multicqa approach that combines self-supervised and supervised training data across a large set of source domains outperforms many in-domain baselines and achieves state-of-the-art zero-shot performances on six benchmarks; (4) fine-tuning multicqarba-lg in-domain further improves the performances and achieves state-of-the-art results on all nine benchmarks."
2020.emnlp-main.194.txt,2020,7 Conclusion,"we studied the zero-shot transfer of text matching models on a massive scale, with 140 different source domains and nine benchmark datasets of non-factoid answer selection and question similarity tasks."
2020.emnlp-main.195.txt,2020,6 Conclusion,a qualitative evaluation showed that xl-amr is able to handle most of the structural divergences among languages.
2020.emnlp-main.195.txt,2020,6 Conclusion,"it would therefore be promising to extend this line of our research to exploit larger multilingual semantic resources, in order to further improve the parsing quality."
2020.emnlp-main.195.txt,2020,6 Conclusion,the performance of xl-amr together with the qualitative analysis suggests that carefully modeling cross-lingual amr parsing leads to the production of suitable amr structures across languages.
2020.emnlp-main.195.txt,2020,6 Conclusion,these amr representations could then be integrated into downstream crosslingual tasks to investigate their added value.
2020.emnlp-main.195.txt,2020,6 Conclusion,"we created silver data based on annotation projection through parallel sentences and machine translation, on which we trained xl-amr, a cross-lingual amr parser that achieves the highest results reported to date on chinese, german, italian and spanish."
2020.emnlp-main.195.txt,2020,6 Conclusion,we explored transfer learning techniques to enable high performance cross-lingual amr parsing.
2020.emnlp-main.196.txt,2020,7 Conclusion,detail experimentation shows that both the single and joint pre-trained models substantially improve our baseline and the performance reaches the state of the art.
2020.emnlp-main.196.txt,2020,7 Conclusion,in this paper we proposed a seq2seq-based pretraining approach to improving the performance of seq2seq-based amr parsing.
2020.emnlp-main.196.txt,2020,7 Conclusion,the accomplishment is encouraging since we achieve this simply by using the generic seq2seq framework rather than complex models.
2020.emnlp-main.196.txt,2020,7 Conclusion,then we built seq2seq pre-trained models through either single or joint pre-training tasks.
2020.emnlp-main.196.txt,2020,7 Conclusion,"to this end, we designed three relevant seq2seq learning tasks, including machine translation, syntactic parsing, and amr parsing itself."
2020.emnlp-main.197.txt,2020,7 Conclusion and Future Work,"as a future research, semantically challenging cases at fine-grained level with respect to complexities of abusive/offensive (targeted) and profane (untargeted) language demand further investigation."
2020.emnlp-main.197.txt,2020,7 Conclusion and Future Work,"furthermore, an extensive experimentation with respect to multiple embeddings, their power of transfer learning, and comparison with existing baseline models is carried out."
2020.emnlp-main.197.txt,2020,7 Conclusion and Future Work,"in this work, we presented a dataset in roman urdu for the task of hate-speech detection in social media content, annotated with five fine-grained labels."
2020.emnlp-main.197.txt,2020,7 Conclusion and Future Work,we also make publicly available domain-specific embeddings trained on a parallel corpora of more than 4.7 million tweets.
2020.emnlp-main.198.txt,2020,4 Discussion and Conclusions,"after the dataset is constructed, we fine-tune a pre-trained language models, achieving at most 0.88 f1 score."
2020.emnlp-main.198.txt,2020,4 Discussion and Conclusions,"as our first step, we collect 2,791 militaryrelevant posts in a social q&a platform that are written by at-risk active-duty soldiers and remove any identifying information from the data."
2020.emnlp-main.198.txt,2020,4 Discussion and Conclusions,"in this paper, we tackle the problem of suicidal risk of military personnel from their social media posts."
2020.emnlp-main.198.txt,2020,4 Discussion and Conclusions,our research can be the first step toward proper intervention programs and institutional support for soldiers with mental health issues.
2020.emnlp-main.198.txt,2020,4 Discussion and Conclusions,such follow-up would maximize the value of our model and data.
2020.emnlp-main.198.txt,2020,4 Discussion and Conclusions,then five annotators (three military experts and two clinicians) evaluate the degree of suicidal risk of the posts.
2020.emnlp-main.198.txt,2020,4 Discussion and Conclusions,"we also plan to add domain-specific features to our model, collect more data, integrate existing suicidal risk datasets with various languages to improve performance."
2020.emnlp-main.198.txt,2020,4 Discussion and Conclusions,we focus on the specific population of military personnel in compulsory service because it requires a unique approach to fully understand their suicidal risk.
2020.emnlp-main.199.txt,2020,6 Conclusion,"as unpreventable as selection bias in social data can be, we believe there is a way to mitigate it by incorporating evaluation as a step which directs the construction of a new dataset or when combining existing corpora."
2020.emnlp-main.199.txt,2020,6 Conclusion,"our metrics are extensible to other forms of bias such as user, label, and semantic biases, and could be adapted in cross-lingual contexts using different similarity measures."
2020.emnlp-main.199.txt,2020,6 Conclusion,the results reveal potential similarities across available hate speech datasets which may hurt the classification performance.
2020.emnlp-main.199.txt,2020,6 Conclusion,"we proposed two label-agnostic metrics to evaluate bias in eleven hate speech datasets that differ in language, size, and content."
2020.emnlp-main.2.txt,2020,7 Conclusion,"for reported speech, bert-based models demonstrated high effectiveness in identifying speech content and source by utilizing the rich semantic information in the pretrained model."
2020.emnlp-main.2.txt,2020,7 Conclusion,"for transforming questions to propositions, handcrafted rules were significantly more effective than neural models and provided insights into the regularities in how propositions are implicitly asserted in question form."
2020.emnlp-main.2.txt,2020,7 Conclusion,identifying implicitly asserted propositions in argumentation is key to understanding arguments properly.
2020.emnlp-main.2.txt,2020,7 Conclusion,"lastly, for imperatives, we demonstrated some regularities and irregularities in how propositions are asserted in imperatives."
2020.emnlp-main.2.txt,2020,7 Conclusion,"since rule-based methods do not take context into account, however, more annotated data would be needed for better question transformation based on machine learning."
2020.emnlp-main.2.txt,2020,7 Conclusion,"we find evidence that some verbs may need to be treated specially, while many other verbs could be treated in consistent ways."
2020.emnlp-main.2.txt,2020,7 Conclusion,we presented and tested computational methods for extracting implicit propositions from questions and reported speech in argumentation.
2020.emnlp-main.20.txt,2020,5 Conclusion,"although electric can be derived solely from the cloze task, the resulting pre-training method is closely related to electra’s ganlike pre-training algorithm."
2020.emnlp-main.20.txt,2020,5 Conclusion,"furthermore, it offers a clearer and more principled view of the electra objective as a “negative sampling” version of cloze pre-training."
2020.emnlp-main.20.txt,2020,5 Conclusion,we have developed an energy-based cloze model we call electric and designed an efficient training algorithm for electric based on noise-contrastive estimation.
2020.emnlp-main.20.txt,2020,5 Conclusion,"while slightly underperforming electra on downstream tasks, electric is useful for its ability to quickly produce pseudo-log-likelihood scores for text."
2020.emnlp-main.200.txt,2020,6 Conclusion,cyberbullying detection on social media attracts growing attention in recent years.
2020.emnlp-main.200.txt,2020,6 Conclusion,experimental results exhibit both promising performance and evidential explanation of henin.
2020.emnlp-main.200.txt,2020,6 Conclusion,"in addition, it is worthwhile to further model information propagation and temporal correlation of comments in the future."
2020.emnlp-main.200.txt,2020,6 Conclusion,it is also crucial to understand why a media session is detected as cyberbullying.
2020.emnlp-main.200.txt,2020,6 Conclusion,such results can encourage future studies to develop advanced graph neural networks in better representing the interactions between heterogeneous information.
2020.emnlp-main.200.txt,2020,6 Conclusion,thus we study the novel problem of explainable cyberbullying detection that aims at improving detection performance and highlighting explainable comments.
2020.emnlp-main.200.txt,2020,6 Conclusion,we also find that the learning of graph-based session-session and post-post interactions contributes most to the performance.
2020.emnlp-main.200.txt,2020,6 Conclusion,"we propose a novel deep learning-based model, heterogeneous neural interaction networks (henin), to learn various feature representations from comment encodings, post-comment co-attention, and graph-based interactions between sessions and posts."
2020.emnlp-main.201.txt,2020,5 Conclusion,"by modifying the cue tweet selection criteria, our method can be adapted to related domains such as sentiment analysis and emotion detection, thereby advancing the quality and quantity of data collection and offering new research directions in affective computing."
2020.emnlp-main.201.txt,2020,5 Conclusion,our approach has multiple advantages over all existing methods.
2020.emnlp-main.201.txt,2020,5 Conclusion,"these new features, including labels for sarcasm perspective and unique context (e.g., oblivious texts), offer opportunities for advances in sarcasm detection.reactive supervision is generalizable."
2020.emnlp-main.201.txt,2020,5 Conclusion,we present an innovative method for collecting sarcasm data that exploits the natural dynamics of online conversations.
2020.emnlp-main.201.txt,2020,5 Conclusion,"we used it to create and release spirs, a large sarcasm dataset with multiple novel features."
2020.emnlp-main.202.txt,2020,5 Summary and Conclusions,a tertiary curriculum of increased sample complexity is observed via an analysis of the extracted data’s gunning fog indices.
2020.emnlp-main.202.txt,2020,5 Summary and Conclusions,"analogously, we also analyze the similarity scores of extracted sentences and observe that they also increase over time."
2020.emnlp-main.202.txt,2020,5 Summary and Conclusions,"as extracted pairs are increasingly similar, and precise, the extraction itself instantiates a secondary curriculum of growing task-relevance, where the task at hand is nmt learning with parallel sentences."
2020.emnlp-main.202.txt,2020,5 Summary and Conclusions,"even if the quality of extraction increases over time, lower-similarity sentence pairs used at the beginning of training are still relevant for the development of the translation engine."
2020.emnlp-main.202.txt,2020,5 Summary and Conclusions,"here, the system starts with sentences suitable for initial high school students and quickly moves towards content suitable for first year undergraduate students: an overachiever indeed as the norm over the complete wp is end of high school level."
2020.emnlp-main.202.txt,2020,5 Summary and Conclusions,"lastly, by estimating the perplexity with an external lm trained on wp, we observe a steep decrease in perplexity at the beginning of training with fast convergence."
2020.emnlp-main.202.txt,2020,5 Summary and Conclusions,"their combination for data selection over time instantiates a denoising curriculum in that the percentage of non-matching pairs, i.e.nontranslations, decreases from 18% to 2%, with an especially fast descent at the beginning of training."
2020.emnlp-main.202.txt,2020,5 Summary and Conclusions,these outliers can be accounted for by the importance of homographs at that point.
2020.emnlp-main.202.txt,2020,5 Summary and Conclusions,this association makes the system naturally and internally evolve its own curriculum without it having been externally enforced.
2020.emnlp-main.202.txt,2020,5 Summary and Conclusions,"this indicates that the extracted data quickly starts to resemble the underlying distribution of all wp data, with a larger amount of outliers at the beginning."
2020.emnlp-main.202.txt,2020,5 Summary and Conclusions,this paper explores self-supervised nmt systems which jointly learn the mt model and how to find its supervision signal in comparable data; i.e.how to identify and select similar sentences.
2020.emnlp-main.202.txt,2020,5 Summary and Conclusions,"this raises the question of how ssnmt will perform on really distant languages (less homographs) or when using smaller bpe sizes (more homographs), which is something that we will examine in our future work."
2020.emnlp-main.202.txt,2020,5 Summary and Conclusions,we analyze the translation quality of a supervised nmt system trained on the epoch-wise data extracted by ssnmt and observe a continuous increase in bleu.
2020.emnlp-main.202.txt,2020,5 Summary and Conclusions,"we observe that the dynamics of mutual-supervision of both system internal representations, cw and ch, is imperative to the high recall and precision parallel data extraction of ssnmt."
2020.emnlp-main.203.txt,2020,6 Conclusions,our approach does not require computationally expensive architecture changes and does not require dramatically increased model depth.
2020.emnlp-main.203.txt,2020,6 Conclusions,subword-based models can be finetuned to work on the character level without explicit segmentation with somewhat of a drop in translation quality.
2020.emnlp-main.203.txt,2020,6 Conclusions,the models are robust to input noise and better capture some morphological phenomena.
2020.emnlp-main.203.txt,2020,6 Conclusions,this is important for research groups that need to train and deploy character transformer models without access to very large computational resources.
2020.emnlp-main.203.txt,2020,6 Conclusions,we presented a simple approach for training character-level models by finetuning subword models.
2020.emnlp-main.204.txt,2020,8 Conclusions,"in this work, we evaluated transfer learning and distant supervision on multilingual transformer models, studying realistic low-resource settings for african languages."
2020.emnlp-main.204.txt,2020,8 Conclusions,we hope that our new datasets and our reflections on assumptions in low-resource settings help to foster future research in this area.
2020.emnlp-main.204.txt,2020,8 Conclusions,"we show that even with a small amount of labeled data, reasonable performance can be achieved."
2020.emnlp-main.205.txt,2020,4 Conclusion,multi-task learning of these two closely related tasks enables us to make use of both types of human evaluation data for model training and improve performance compared to learning these two tasks separately.
2020.emnlp-main.205.txt,2020,4 Conclusion,our model obtains new state-of-the-art results on the wmt 2019 qe as a metric task and outperforms sentbleu on the wmt 2019 metrics task.
2020.emnlp-main.205.txt,2020,4 Conclusion,the scoring and ranking results performed by human assessors can be used as training data for learning the scoring and ranking tasks respectively.
2020.emnlp-main.205.txt,2020,4 Conclusion,"this paper presents a multi-task leaning qe model that jointly learns two tasks, score a translation and rank two translations."
2020.emnlp-main.206.txt,2020,6 Conclusions,"firstly, we have confirmed how the preprocessing of the mt training data has a significant effect for st, and how a special preprocessing that is closer to the inference conditions is able to obtain significant improvements."
2020.emnlp-main.206.txt,2020,6 Conclusions,"in contrast to previous works, these models not only consider text, but also acoustic information."
2020.emnlp-main.206.txt,2020,6 Conclusions,"in terms of future work, there are many ways of improving the segmenter system that has been presented here."
2020.emnlp-main.206.txt,2020,6 Conclusions,"secondly, we have shown the importance of including acoustic information in the segmentation process, as the inclusion of these features improves system performance."
2020.emnlp-main.206.txt,2020,6 Conclusions,the experimental results reported provide two key insights.
2020.emnlp-main.206.txt,2020,6 Conclusions,the proposed model improves the results of previous works on the europarl-st test set when evaluated with two training data setups.
2020.emnlp-main.206.txt,2020,6 Conclusions,the segmenter model itself could also benefit from the incorporation of additional text data as well as pre-training procedures.
2020.emnlp-main.206.txt,2020,6 Conclusions,"this work introduces a statistical framework for the problem of asr output segmentation in streaming st, as well as three possible models to instantiate this framework."
2020.emnlp-main.206.txt,2020,6 Conclusions,"we also devise two supplementary research lines, the integration of the segmentation into the translation process, so the system learns how to segment and translate at the same time, and moving from an offline mt system to a streaming mt system to improve response time, but without performance degradation."
2020.emnlp-main.206.txt,2020,6 Conclusions,we plan to look into additional acoustic features as well as possible ways to incorporate asr information to the segmentation process.
2020.emnlp-main.207.txt,2020,8 Conclusion and Future Works,"furthermore, we wish to explore semi-supervised and unsupervised approaches to leverage monolingual data and explore multilingual machine translation for low-resource indic languages."
2020.emnlp-main.207.txt,2020,8 Conclusion and Future Works,"in future, we plan to design segmentation-agnostic aligners or aligners that can jointly segment and align sentences."
2020.emnlp-main.207.txt,2020,8 Conclusion and Future Works,"in this work, we developed a custom sentence segmenter for bengali, showed that aligner ensembling with batch filtering provides better performance than single sentence aligners, collected a total of 2.75 million high-quality parallel sentences for bengali-english from multiple sources, trained nmt models that outperformed previous results, and prepared a new test set; thus elevating bengali from its low-resource status."
2020.emnlp-main.207.txt,2020,8 Conclusion and Future Works,"laser fails to identify one-tomany/many-to-one sentence alignments, we want to address this."
2020.emnlp-main.207.txt,2020,8 Conclusion and Future Works,"we want to experiment more with the laser toolkit: we used laser out-of-the-box, we want to train it with our data, and modify the model architecture to improve it further."
2020.emnlp-main.207.txt,2020,8 Conclusion and Future Works,"we would also like to experiment with bert (devlin et al., 2019) embeddings for similarity search."
2020.emnlp-main.208.txt,2020,6 Conclusions and Future work,"additionally, we show that csp is able to enhance the ability of nmt on handling code-switching inputs."
2020.emnlp-main.208.txt,2020,6 Conclusions and Future work,experimental results show that the proposed approach achieves substantial improvements over strong baselines consistently.
2020.emnlp-main.208.txt,2020,6 Conclusions and Future work,"firstly, we are interested in applying csp to other related nlp areas for code-switching problems."
2020.emnlp-main.208.txt,2020,6 Conclusions and Future work,"secondly, we plan to investigate the pre-training objectives which are more effective in utilizing the cross-lingual alignment information for nmt."
2020.emnlp-main.208.txt,2020,6 Conclusions and Future work,there are two promising directions for the future work.
2020.emnlp-main.208.txt,2020,6 Conclusions and Future work,"this work proposes a simple yet effective pretraining approach, i.e., csp for nmt, which randomly replaces some words in the source sentence with their translation words in the probabilistic translation lexicons extracted from monolingual corpus only."
2020.emnlp-main.208.txt,2020,6 Conclusions and Future work,"to verify the effectiveness of csp, we investigate two downstream tasks, supervised and unsupervised nmt, on english-german, englishfrench and chinese-to-english translation tasks."
2020.emnlp-main.209.txt,2020,7 Conclusion,"additionally, we find that for some tasks, these associations are more in line with national labor market gender statistics than with u.s. statistics, revealing another way that anglocentric biases can prohibit the detection of gender biases in our models."
2020.emnlp-main.209.txt,2020,7 Conclusion,"in this work we have introduced the anti-reflexive bias challenge (abc) dataset for multilingual, multi-task gender bias detection, the first of its kind, including four languages and four tasks: machine translation, natural language inference, coreference resolution and masked language modeling."
2020.emnlp-main.209.txt,2020,7 Conclusion,our evaluations of state-of-the-art models across the four tasks generally reveal significant gender biases leading to false predictions.
2020.emnlp-main.209.txt,2020,7 Conclusion,"the abc dataset focuses on a specific linguistic phenomenon that does not occur in english but is found in languages with type b reflexivization: namely, anti-reflexive gendered pronouns."
2020.emnlp-main.209.txt,2020,7 Conclusion,"the problem of anti-reflexive gendered pronouns has, to the best of our knowledge, not received attention before in the nlp literature, which tends to focus heavily on english (bender and friedman, 2018)."
2020.emnlp-main.209.txt,2020,7 Conclusion,"this phenomenon is shown to be useful for exposing unambiguous gender bias, because it quantifies to what extent gender bias leads to prediction errors, in contrast to just unwarranted disambiguations (’hallucinations’)."
2020.emnlp-main.21.txt,2020,5 Conclusion,"in this work, we examine the calibration of pre-trained transformers in both in-domain and out-of-domain settings."
2020.emnlp-main.21.txt,2020,5 Conclusion,posterior calibration is one lens to understand the trustworthiness of model confidence scores.
2020.emnlp-main.21.txt,2020,5 Conclusion,"results show bert and roberta coupled with temperature scaling achieve low eces in-domain, and when trained with label smoothing, are also competitive out-of-domain."
2020.emnlp-main.210.txt,2020,6 Conclusion,"extensive experiments are conducted on different scenarios, including low/medium/rich resource and exotic corpus, demonstrating the efficacy of mrasp."
2020.emnlp-main.210.txt,2020,6 Conclusion,"in future work, we will pre-train on larger corpus to further boost the performance."
2020.emnlp-main.210.txt,2020,6 Conclusion,"in this paper, we propose a multilingual neural machine translation pre-training model (mrasp)."
2020.emnlp-main.210.txt,2020,6 Conclusion,"to bridge the semantic space between different languages, we incorporate word alignment into the pre-training model."
2020.emnlp-main.210.txt,2020,6 Conclusion,"we also conduct a set of analytical experiments to quantify the model, showing that the alignment information does bridge the gap between languages as well as boost the performance."
2020.emnlp-main.210.txt,2020,6 Conclusion,we leave different alignment approaches to be explored in the future.
2020.emnlp-main.211.txt,2020,11 Conclusions,"in our paper, we have only shown one example of a heuristic approach — there may be yet to be identified more efficient algorithms better suited to specific tasks, which will result in no need to train overly parametrised models."
2020.emnlp-main.211.txt,2020,11 Conclusions,"in the teacher-student regime, the student model with a reduced decoder can be pruned of 75% encoder heads with 0.1–0.2 bleu loss and 10–15% faster translation speed."
2020.emnlp-main.211.txt,2020,11 Conclusions,"no matter how a model is trained like, attention heads can be easily removed from it."
2020.emnlp-main.211.txt,2020,11 Conclusions,our experiments on nmt have proved that it is possible to remove a significant percentage of all heads (50–72%) in a large transformer with no significant damage to translation quality.
2020.emnlp-main.211.txt,2020,11 Conclusions,"since attention mechanism is expensive, especially during inference, reducing the number of heads in a model led to 1.5× speed-up and more if one is willing to sacrifice quality for speed."
2020.emnlp-main.211.txt,2020,11 Conclusions,this paper investigated block-wise pruning of attention heads in the transformer by applying the lottery ticket hypothesis to the problem.
2020.emnlp-main.211.txt,2020,11 Conclusions,this shows that lottery ticket pruning is complementary to other methods that reduce inference workload.
2020.emnlp-main.211.txt,2020,11 Conclusions,we hope our paper will inspire further work on attention-sparse architectures.
2020.emnlp-main.211.txt,2020,11 Conclusions,we used an iterative approach with pruning done in early stages training.
2020.emnlp-main.212.txt,2020,5 Conclusion,both the automatic and human evaluations show that our approach can effectively improve the faithfulness of translations.
2020.emnlp-main.212.txt,2020,5 Conclusion,"in the future, we will continue investigate the learning method for effectively utilizing self-generated samples and expand to other text generation tasks."
2020.emnlp-main.212.txt,2020,5 Conclusion,"in this paper, we address the problem that current nmt can’t generate faithful translations which will observably decrease translation quality."
2020.emnlp-main.212.txt,2020,5 Conclusion,"our work can employ on different text generation tasks, e.g., text summarization and dialogue, to enhance the key phrases (or terms) generation."
2020.emnlp-main.212.txt,2020,5 Conclusion,we implement the proposed method based on the transformer model and evaluate it on three translation tasks.
2020.emnlp-main.212.txt,2020,5 Conclusion,we propose a fenmt to learn the faithful translation from mistranslated parts.
2020.emnlp-main.213.txt,2020,8 Conclusions and Future Work,"a primary avenue for future work on comet will look at the impact of more compact solutions such as distilbert (sanh et al., 2019)."
2020.emnlp-main.213.txt,2020,8 Conclusions and Future Work,"additionally, whilst we outline the potential importance of the source text above, we note that our comet-rank model weighs source and reference differently during inference but equally in its training loss function."
2020.emnlp-main.213.txt,2020,8 Conclusions and Future Work,future work will investigate the optimality of this formulation and further examine the interdependence of the different inputs.
2020.emnlp-main.213.txt,2020,8 Conclusions and Future Work,"in this paper we present comet, a novel neural framework for training mt evaluation models that can serve as automatic metrics and easily be adapted and optimized to different types of human judgements of mt quality."
2020.emnlp-main.213.txt,2020,8 Conclusions and Future Work,one of the challenges of leveraging the power of pretrained models is the burdensome weight of parameters and inference time.
2020.emnlp-main.213.txt,2020,8 Conclusions and Future Work,"to showcase the effectiveness of our framework, we sought to address the challenges reported in the 2019 wmt metrics shared task (ma et al., 2019)."
2020.emnlp-main.213.txt,2020,8 Conclusions and Future Work,"we trained three distinct models which achieve new state-of-the-art results for segment-level correlation with human judgments, and show promising ability to better differentiate high-performing systems."
2020.emnlp-main.214.txt,2020,6 Conclusions,"in future work, we will apply our method to languages with corpora from diverse domains and also to other languages."
2020.emnlp-main.214.txt,2020,6 Conclusions,"re-lm outperformed a strong baseline in unmt, while also improving translations on a lowresource supervised setup."
2020.emnlp-main.214.txt,2020,6 Conclusions,training competitive unsupervised nmt models for hmr-lmr scenarios is important for many real low-resource languages.
2020.emnlp-main.214.txt,2020,6 Conclusions,"we proposed re-lm, a novel approach that fine-tunes a high-resource lm on a low-resource language and initializes an nmt model."
2020.emnlp-main.215.txt,2020,6 Conclusions,"apart from exploiting weak supervision from a small (1k) seed dictionary, our lnmap leverages the information from monolingual word embeddings."
2020.emnlp-main.215.txt,2020,6 Conclusions,"comparison with existing supervised, semi-supervised, and unsupervised baselines show that lnmap learns a better mapping."
2020.emnlp-main.215.txt,2020,6 Conclusions,"extensive experiments with fifteen different language pairs comprising high- and low-resource languages show the efficacy of non-linear transformations, especially for low-resource and distant languages."
2020.emnlp-main.215.txt,2020,6 Conclusions,"in contrast to the existing methods that directly map word embeddings using the isomorphic assumption, our framework is independent of any such strong prior assumptions."
2020.emnlp-main.215.txt,2020,6 Conclusions,lnmap first learns to transform the embeddings into a latent space and then uses a non-linear transformation to learn the mapping.
2020.emnlp-main.215.txt,2020,6 Conclusions,"to guide the non-linear mapping further, we include constraints for back-translation and original embedding reconstruction."
2020.emnlp-main.215.txt,2020,6 Conclusions,we have presented a novel semi-supervised framework lnmap to learn the cross-lingual mapping between two monolingual word embeddings.
2020.emnlp-main.215.txt,2020,6 Conclusions,"with an indepth ablation study, we show that different components of lnmap works in a collaborative nature."
2020.emnlp-main.216.txt,2020,5 Conclusion and Future Work,"experiments on wmt14 english→french, wmt16 english→german, nist chinese→english and wmt18 chinese→english translation tasks show that the proposed method can achieve consistent improvements across different language pairs."
2020.emnlp-main.216.txt,2020,5 Conclusion and Future Work,"in addition, learning universal representations among semantically-equivalent source and target sentences (wei et al., 2020) can complete the proposed method."
2020.emnlp-main.216.txt,2020,5 Conclusion and Future Work,"in particular, we first synthesize a proper number of source sentences to play the role of intrinsic uncertainties via the controllable sampling for each target sentence."
2020.emnlp-main.216.txt,2020,5 Conclusion and Future Work,"then, we develop a semantic constrained network to summarize multiple source inputs into a closed semantic region which is then utilized to augment latent representations."
2020.emnlp-main.216.txt,2020,5 Conclusion and Future Work,we present an uncertainty-aware semantic augmentation method to bridge the discrepancy of the data distribution between the training and the inference phases for dominant nmt models.
2020.emnlp-main.216.txt,2020,5 Conclusion and Future Work,"while we showed that uncertainty-aware semantic augmentation with gaussian priors is effective, more work is required to investigate if such an approach will also be successful for more sophisticated priors."
2020.emnlp-main.217.txt,2020,8 Conclusion,ape has been an effective option to fix systematic mt errors and improve translations from blackbox mt services.
2020.emnlp-main.217.txt,2020,8 Conclusion,"finally, our experiments comparing in and out-domain ape show that domain-specificity of training affects ape performance drastically and a combination of in and out-of-domain data with certain upscaling alleviates the domain-shift problem for ape."
2020.emnlp-main.217.txt,2020,8 Conclusion,"however, on nmt outputs, ape has shown hardly any improvement since training has been done on limited human postedited data."
2020.emnlp-main.217.txt,2020,8 Conclusion,the newly collected subedits corpus is the largest corpus of nmt human post-edits collected so far.
2020.emnlp-main.217.txt,2020,8 Conclusion,we find that ape mostly contributes to improving nmt performance by fixing the poorer-quality outputs that still exist with strong in-domain nmt systems.
2020.emnlp-main.217.txt,2020,8 Conclusion,we reassessed the usefulness of ape on nmt using this corpus.
2020.emnlp-main.217.txt,2020,8 Conclusion,we release the postediting datasets used in this paper (subescape and subedits) along with pre/post-processing scipts at pedra github repository (https://github.com/shamilcm/pedra)
2020.emnlp-main.217.txt,2020,8 Conclusion,"we showed that with a larger human post-edited corpus, a strong neural ape model can substantially improve a strong in-domain nmt system."
2020.emnlp-main.217.txt,2020,8 Conclusion,"while artificial ape corpora help, we showed that the ape model performs better when trained on adequate human post-edited data (subedits) compared to large-scale artificial corpora."
2020.emnlp-main.218.txt,2020,5 Conclusion,"in the future work, we will extend our method by replacing the simple role matching score with grammatical or semantic similaritybased measures to improve the alignment accuracy."
2020.emnlp-main.218.txt,2020,5 Conclusion,our proposed method is simple but effective.
2020.emnlp-main.218.txt,2020,5 Conclusion,this paper has proposed a method of parsing gapping constructions based on tag-annotated constituent trees.
2020.emnlp-main.218.txt,2020,5 Conclusion,we believe that it will serve as a strong baseline for the task of parsing gapping constructions.
2020.emnlp-main.219.txt,2020,5 Conclusion,"despite being based on chart-based algorithms, our approach is fast at test time and we can parse all sentences without pruning or filtering long sentences."
2020.emnlp-main.219.txt,2020,5 Conclusion,future research could explore neural architectures and training losses tailored to our approach.
2020.emnlp-main.219.txt,2020,5 Conclusion,"importantly, we showed that a specific set of discontinuous constituent trees can be parsed in cubic time while covering most of the linguistic structures observed in treebanks."
2020.emnlp-main.219.txt,2020,5 Conclusion,we proposed a novel family of algorithms for discontinuous constituency parsing achieving stateof-the art results.
2020.emnlp-main.22.txt,2020,6 Discussions and Future Work,adapting the current methods to be robust to an active eavesdropper who may alter the cover text is another interesting direction.
2020.emnlp-main.22.txt,2020,6 Discussions and Future Work,"finally, this study assumes a passive eavesdropper who does not modify the transmitted cover text."
2020.emnlp-main.22.txt,2020,6 Discussions and Future Work,"first, we may combine the edit-based steganography with generative steganography method by first transforming the original plaintext in a semantics-preserving way and then encoding the transformed plaintext."
2020.emnlp-main.22.txt,2020,6 Discussions and Future Work,"second, we will study whether this current method is still effective when a small-scale neural lm (e.g., distilgpt-2) is applied."
2020.emnlp-main.22.txt,2020,6 Discussions and Future Work,there are several directions we will further explore in the future.
2020.emnlp-main.22.txt,2020,6 Discussions and Future Work,this work presents a new linguistic steganography method that encodes secret messages using selfadjusting arithmetic coding.
2020.emnlp-main.22.txt,2020,6 Discussions and Future Work,we formally prove this method is near-imperceptible and empirically show it achieves the state-of-the-art results on various text corpora.
2020.emnlp-main.220.txt,2020,6 Discussion and Conclusion,"evaluating only on non-isomorphic trees, i.e., leaving out graphs that have been seen at training time from the test sections of treebanks, would reduce this bias."
2020.emnlp-main.220.txt,2020,6 Discussion and Conclusion,"i compared this factor to previous attempts to explain variance in parser performance across languages through a series of correlation and linear regression experiments; and showed that graph-level train-test leakage, treebank size aside, is the most predictive factor among those proposed, yet complementary."
2020.emnlp-main.220.txt,2020,6 Discussion and Conclusion,"it also suggests a new and improved evaluation methodology: since language is zipfian, not only at the level of words, but at the level of phrases (ha et al., 2002; williams et al., 2015), standard evaluation methodology relying on random samples (gorman and bedrick, 2019; dodge et al., 2019) is biased toward frequent phenomena."
2020.emnlp-main.220.txt,2020,6 Discussion and Conclusion,"it is an open question whether graph-level train-test leakage is predictive of performance in other sentence-level nlp tasks, i.e., whether the ratio of test sentences whose (predicted) syntactic dependency structure is identical to that of one of our training examples, correlates with state-of-the-art performance."
2020.emnlp-main.220.txt,2020,6 Discussion and Conclusion,"the result is perhaps not too surprising, since graph isomorphisms correlate with syntactic constructions, which in turn correlate with the occurrence of linguistic markers and tail linguistic phenomena.6 the observation that treebanks leak, quite dramatically, at the graph level, is not only interesting for explaining variance in parser performance."
2020.emnlp-main.220.txt,2020,6 Discussion and Conclusion,this form of leakage can be quantified by computing graph isomorphisms from training sections and counting the ratio of trees in the test sections that are not isomorphic with any tree in the training data.
2020.emnlp-main.220.txt,2020,6 Discussion and Conclusion,this paper suggested a factor contributing to variance in (universal) dependency parser performance across languages: graph-level train-test leakage in treebanks.
2020.emnlp-main.220.txt,2020,6 Discussion and Conclusion,we hope this is a factor that designers of future syntactic treebanks will take into account.
2020.emnlp-main.221.txt,2020,7 Conclusion,"overall, the models achieve a good trade-off speed/accuracy without the need of any parsing algorithm or auxiliary structures, while being easily parallelizable."
2020.emnlp-main.221.txt,2020,7 Conclusion,"the key contribution consisted in predicting a continuous tree with a rearrangement of the leaf nodes to shape discontinuities, and defining various ways to encode such a rearrangement as a sequence of labels associated to each word, taking advantage of the fact that in practice they are nearly ordered permutations."
2020.emnlp-main.221.txt,2020,7 Conclusion,we reduced discontinuous parsing to sequence labeling.
2020.emnlp-main.221.txt,2020,7 Conclusion,"we tested whether those encodings are learnable by neural models and saw that the choice of permutation encoding is not trivial, and there are interactions between encodings and models (i.e., a given architecture may be better at learning a given encoding than another)."
2020.emnlp-main.222.txt,2020,4 Conclusion,a tree-parallel mini-batch strategy is further designed for efficient running and support for non-binary trees.
2020.emnlp-main.222.txt,2020,4 Conclusion,"in this work, a novel model msnn is proposed to model syntax trees for classification."
2020.emnlp-main.222.txt,2020,4 Conclusion,it uses global context information and syntax category labels to help improve the modeling of sub-trees and thus better sentence representations.
2020.emnlp-main.222.txt,2020,4 Conclusion,our future work will include conducting experiments on dependency trees and more nlp tasks.
2020.emnlp-main.223.txt,2020,7 Conclusion,it is equipped with high-quality annotations and linguistic elements tailored for both chinese and the genre of spoken monologue.
2020.emnlp-main.223.txt,2020,7 Conclusion,"moreover, we display the ability of ted-cdb to help address the issue of insufficient or unbalanced data on other corpora and improve the performance of models for other languages."
2020.emnlp-main.223.txt,2020,7 Conclusion,"the benchmark results of pretrained language models suggest that ted-cdb is a challenging dataset, which can be used to promote further development on discourse relation recognition and discourselevel nlp tasks."
2020.emnlp-main.223.txt,2020,7 Conclusion,"we have presented ted-cdb, a large-scale dataset for discourse relations on spoken monologues in chinese."
2020.emnlp-main.224.txt,2020,8 Conclusion,"in future work, we plan to extend the annotation process to also cover inter-sentential relations."
2020.emnlp-main.224.txt,2020,8 Conclusion,"in this work, we show that discourse relations can be represented as qa pairs."
2020.emnlp-main.224.txt,2020,8 Conclusion,"this intuitive representation enables scalable, high-quality annotation via crowdsourcing, which paves the way for learning robust parsers of informational discourse qa pairs."
2020.emnlp-main.225.txt,2020,7 Conclusion,"experiments on a chinese dataset and an english dataset show that (i) although it is simple, the positional encoding largely improves the performance."
2020.emnlp-main.225.txt,2020,7 Conclusion,"in future, we plan to evaluate disa on other discourse analysis tasks."
2020.emnlp-main.225.txt,2020,7 Conclusion,"moreover, we use inter-sentence attention vectors to capture sentence relations in content and function."
2020.emnlp-main.225.txt,2020,7 Conclusion,the structural positional encoding considers relative positions of the sentence and its paragraph.
2020.emnlp-main.225.txt,2020,7 Conclusion,"this indicates that modeling structural positions is feasible and important to analyze the role of sentences; (ii) discourse elements could be better identified with the help of inter-sentence attention vectors, especially the minority ones and the ones that have distinct relation patterns to other sentences."
2020.emnlp-main.225.txt,2020,7 Conclusion,we presented a method disa to identify discourse elements in argumentative student essays by explicitly modeling structural positions and intersentence relations.
2020.emnlp-main.226.txt,2020,5 Future Work,"for example, how long the control signal would propagate for longer generations?"
2020.emnlp-main.226.txt,2020,5 Future Work,"for example, the generated story of megatroncntrl-8b in table 15 only mentions the keyword “realize” instead of centering around it."
2020.emnlp-main.226.txt,2020,5 Future Work,investigating this issue using longer story datasets (e.g.
2020.emnlp-main.226.txt,2020,5 Future Work,one way to mitigate this issue is to leverage longer context information to identify better keywords which is subject of the future work.
2020.emnlp-main.226.txt,2020,5 Future Work,other interesting direction may include incorporating the structure-level controllability by adding it as either an extra input for the conditional generator or a multitask learning supervision for each sequence.
2020.emnlp-main.226.txt,2020,5 Future Work,short story sentences in roc story dataset limits our exploration from several potential research directions.
2020.emnlp-main.226.txt,2020,5 Future Work,"this is caused by the rake keywords extractor, which does not always extract the keywords that represent the sentence well."
2020.emnlp-main.226.txt,2020,5 Future Work,"we also observed that in some cases during the generation, our model simply mentions the given word in the sentence, and talks about things that are not strictly related to or restricted by the given word."
2020.emnlp-main.226.txt,2020,5 Future Work,"writingprompts (fan et al., 2018)) is a subject for future work."
2020.emnlp-main.227.txt,2020,6 Conclusion & Future Work,"in the future, we will investigate on extending our approach to more areas."
2020.emnlp-main.227.txt,2020,6 Conclusion & Future Work,"in this paper, we present a novel and extensive approach which formulates the incomplete utterance rewriting as a semantic segmentation task."
2020.emnlp-main.227.txt,2020,6 Conclusion & Future Work,"on top of the formulation, we carefully design a ushaped rewritten network, which outperforms existing baselines significantly on several datasets."
2020.emnlp-main.228.txt,2020,5 Conclusion,experimental results demonstrate that the gec models trained with the data augmented by these adversarial examples can substantially improve both generalization and robustness.
2020.emnlp-main.228.txt,2020,5 Conclusion,"in this paper, we proposed a data augmentation method for training a gec model by continually adding to the training set the adversarial examples, particularly generated to compensate the weakness of the current model."
2020.emnlp-main.228.txt,2020,5 Conclusion,"the samples generated by our adversarial attack algorithm are more meaningful and valuable than those produced by recently proposed methods, such as the direct noise and the back translation."
2020.emnlp-main.228.txt,2020,5 Conclusion,"to generate such adversarial examples, we first determine an important position to change and then modify it by introducing specific grammatical issues that maximize the gec model’s prediction error."
2020.emnlp-main.229.txt,2020,4 Conclusion,and then we use constraint selection algorithm to rank candidate sentences and choose support word to imply the semantics of the pun word.
2020.emnlp-main.229.txt,2020,4 Conclusion,finally we rewrite the sentence with constraints.
2020.emnlp-main.229.txt,2020,4 Conclusion,"however,as discussion in error analysis, there are several challenges to solve in the future."
2020.emnlp-main.229.txt,2020,4 Conclusion,"in this work, we propose to generate homophonic puns with lexically constrained rewriting."
2020.emnlp-main.229.txt,2020,4 Conclusion,our model outperforms previous works on the task of homophonic pun generation.
2020.emnlp-main.229.txt,2020,4 Conclusion,we retrieve sentences containing the alternative word as candidate sentences.
2020.emnlp-main.23.txt,2020,7 Conclusion,we annotate eight large existing datasets and create an evaluation dataset for the task of detect bias along each of these dimensions.
2020.emnlp-main.23.txt,2020,7 Conclusion,"we propose a general framework for analyzing gender bias along three dimensions: (1) gender of the person being spoken about (about), (2) gender of the addressee (to), and (3) gender of the speaker (as)."
2020.emnlp-main.23.txt,2020,7 Conclusion,"we train classifiers (single and multitask) that demonstrate their broad utility by displaying strong performance in controlling bias in dialogue, detecting genderedness in text such as wikipedia, and highlighting gender differences in offensive text."
2020.emnlp-main.230.txt,2020,6 Discussion,a method to do this automatically would be convenient.
2020.emnlp-main.230.txt,2020,6 Discussion,"an alternate approach, which we did not compare with, is automatic template generation (biran et al., 2016; wiseman et al., 2018)."
2020.emnlp-main.230.txt,2020,6 Discussion,"by combining neural generation with constraints based on content words included in the input sequence, we aim to achieve both reliability and naturalness."
2020.emnlp-main.230.txt,2020,6 Discussion,diversity can always be increased later.
2020.emnlp-main.230.txt,2020,6 Discussion,however this provides no guarantee as to what will be contained within the brackets.
2020.emnlp-main.230.txt,2020,6 Discussion,"however, as with neural generation, when applied to the e2e task it has issues with reliability."
2020.emnlp-main.230.txt,2020,6 Discussion,"however, the question remains: why pursue this approach when templates perform satisfactorily?"
2020.emnlp-main.230.txt,2020,6 Discussion,"if augmenting an input sequence with surface forms allows us to restrict decoding and generate utterances that are as reliable as templates, then this is an approach worth investigating further."
2020.emnlp-main.230.txt,2020,6 Discussion,"in our proposed approach, we purposefully remove a neural nlg system’s ability to generate diverse text."
2020.emnlp-main.230.txt,2020,6 Discussion,"in our proposed method, surface forms still need to be joined together with function words."
2020.emnlp-main.230.txt,2020,6 Discussion,"in the surface realization shared task (mille et al., 2018), the deep task dataset was created by pruning function words from a dependency tree, leaving only content words remaining."
2020.emnlp-main.230.txt,2020,6 Discussion,"mille and dasiopoulou (2017) used an automated template generation approach on the e2e shared task and their accuracy score was similar to that of a neural system, 92% (dusek et al.ˇ , 2019b), mostly due to missing attributes in templates."
2020.emnlp-main.230.txt,2020,6 Discussion,"some work has already been done by oraby et al.(2019), in which dependency trees were used to find adjectives that describe a specific list of food related nouns."
2020.emnlp-main.230.txt,2020,6 Discussion,the difference between our decoding strategies lies in the guarantees provided.
2020.emnlp-main.230.txt,2020,6 Discussion,"the major weakness of both approaches, however, is the difficulty of extracting surface forms from human-authored text."
2020.emnlp-main.230.txt,2020,6 Discussion,"their approach focuses on an augmented target utterance, as opposed to input sequence, in which special bracket tokens are used to surround surface forms.e.g.[ arg area city centre city centre ]."
2020.emnlp-main.230.txt,2020,6 Discussion,"their constrained decoding strategy guarantees that when an opening bracket is generated, a closing bracket will also be generated."
2020.emnlp-main.230.txt,2020,6 Discussion,this is not the first data-focused approach to improving accuracy; balakrishnan et al.(2019) also proposed a constrained decoding strategy.
2020.emnlp-main.230.txt,2020,6 Discussion,"we believe neural networks are well suited for this task because they are good at generating natural sounding, though sometimes nonsensical, text."
2020.emnlp-main.230.txt,2020,6 Discussion,"we believe that neural nlg systems are easier to maintain, generate more natural text, and, as surface form extraction improves, they also become more scalable: to new domains, languages, and, possibly even, personalization."
2020.emnlp-main.230.txt,2020,6 Discussion,"we were able to avail of the hand-crafted regular expressions of dusek et al in our e2e experiments, but moving to another dataset would entail a similar exercise."
2020.emnlp-main.230.txt,2020,6 Discussion,"what sets our method apart is that: we can guarantee the text will actually be generated as requested, we generate shorter sequences (no bracket tokens in the output) and have the option for a restricted vocabulary, which speeds up decoding."
2020.emnlp-main.230.txt,2020,6 Discussion,"while this may seem perverse, we consider reliability to be the most important starting point."
2020.emnlp-main.231.txt,2020,6 Conclusion,abstract meaning representations were designed to describe the meaning of english sentences.
2020.emnlp-main.231.txt,2020,6 Conclusion,"amr concepts are either english words, propbank framesets (“want-01”) or special, english-based keywords (e.g., “date-entity”)."
2020.emnlp-main.231.txt,2020,6 Conclusion,as such they are heavily biased towards english.
2020.emnlp-main.231.txt,2020,6 Conclusion,"for instance, the main relation of “i like to eat” is the concept associated with its main verb (“like”) whereas given the corresponding german sentence ”ich esse gern” (lit.“i eat willingly”), the main predicate might have been chosen to be “eat” (“essen”)."
2020.emnlp-main.231.txt,2020,6 Conclusion,"in other words, amrs should not necessarily be viewed as an interlingua (banarescu et al., 2013)."
2020.emnlp-main.231.txt,2020,6 Conclusion,"nonetheless, our work suggests that it can be used as one: given an english-centric amr it is possible to generate the corresponding sentence in multiple languages."
2020.emnlp-main.231.txt,2020,6 Conclusion,the structure of amrs is also influenced by english syntax.
2020.emnlp-main.231.txt,2020,6 Conclusion,"this is in line with previous work by (damonte and cohen, 2019) which shows that despite translation divergences, amr parsers can be learned for italian, chinese, german and spanish which all map into an english-centric amr."
2020.emnlp-main.232.txt,2020,7 Conclusion,"a) show that gradually increasing the degree of non-linearity has again no significant change in performance for the weat (caliskan et al., 2017) benchmark."
2020.emnlp-main.232.txt,2020,7 Conclusion,"furthermore, the results in tab.7(app."
2020.emnlp-main.232.txt,2020,7 Conclusion,"there may very well exist a non-linear technique that works better, but we were unable to find one in this work."
2020.emnlp-main.232.txt,2020,7 Conclusion,this allows us to provide equivalent constructions of the neutralize and equalize steps presented.
2020.emnlp-main.232.txt,2020,7 Conclusion,"thus, we provide empirical evidence for the linear subspace hypothesis; our results suggest representing gender bias as a linear subspace is a suitable assumption."
2020.emnlp-main.232.txt,2020,7 Conclusion,we compare the linear bias mitigation technique to our new kernelized non-linear version across a suite of tasks and datasets.
2020.emnlp-main.232.txt,2020,7 Conclusion,we contend our extension is natural in the sense that it reduces to the method of bolukbasi et al.(2016) in the special case when we employ a linear kernel and in the non-linear case it preserves all the desired linear properties in the feature space.
2020.emnlp-main.232.txt,2020,7 Conclusion,we observe that our non-linear extensions of bolukbasi et al.(2016) show no notable performance differences across a set of benchmarks designed to quantify gender bias in word embeddings.
2020.emnlp-main.232.txt,2020,7 Conclusion,we offer a non-linear extension to the method presented in bolukbasi et al.(2016) by connecting its bias space construction to pca and subsequently applying kernel pca.
2020.emnlp-main.232.txt,2020,7 Conclusion,"we would like to highlight that our results are specific to our proposed kernelized extensions and does not imply that all non-linear variants of (bolukbasi et al., 2016) will yield similar results."
2020.emnlp-main.233.txt,2020,6 Conclusion,"in addition, the proposed approach only requires a little extra time for training the teacher without extra memory or capacity needed, showing the potential of being applied to the practical scenarios."
2020.emnlp-main.233.txt,2020,6 Conclusion,"the experiments show the consistent improvement achieved by l2kd for three different settings, indicating the effectiveness of the proposed method to train robust lll models."
2020.emnlp-main.233.txt,2020,6 Conclusion,"this paper presents lifelong language knowledge distillation (l2kd), a simple method that effectively help lifelong language learning models to maintain good performance comparable to its multitask upper bounds."
2020.emnlp-main.234.txt,2020,Conclusion,"by formulating this algorithm using a representation of the hdp which connects it with the well-studied latent dirichlet allocation model, we obtain a parallel algorithm whose per-token sampling complexity is the minima of two sparsity terms."
2020.emnlp-main.234.txt,2020,Conclusion,our algorithm for the hdp scales to a 768m-token corpus (pubmed) on a single multicore machine in under four days.
2020.emnlp-main.234.txt,2020,Conclusion,the ideas used apply to a large array of topic models which possess the same full conditional for the topic indicators z.
2020.emnlp-main.234.txt,2020,Conclusion,the proposed techniques leverage parallelism and sparsity to scale nonparametric topic models to larger datasets than previously considered feasible for mcmc or other methods possessing similar convergence properties.
2020.emnlp-main.234.txt,2020,Conclusion,we hope these contributions enable wider use of bayesian nonparametrics for large collections of text.
2020.emnlp-main.234.txt,2020,Conclusion,we introduce the doubly sparse partially collapsed gibbs sampler for the hierarchical dirichlet process topic model.
2020.emnlp-main.235.txt,2020,5 Conclusion,"as future work, we will further study our method’s ability of extreme multi-label learning (bhatia et al., 2016) and different document encoders."
2020.emnlp-main.235.txt,2020,5 Conclusion,experiments on mimicii/iii and eurlex57k have shown that the classifiers derived from the multi-graph aggregation have achieved substantial performance improvements particularly on few/zero-shot labels.
2020.emnlp-main.235.txt,2020,5 Conclusion,we have proposed a multi-graph aggregation method that can effectively fuse knowledge from multiple label graphs.
2020.emnlp-main.236.txt,2020,9 Conclusion,"based on this finding, wrd was designed so that the norm and angle of word vectors correspond to the probability mass and transportation cost in emd, respectively."
2020.emnlp-main.236.txt,2020,9 Conclusion,"in experiments on multiple sts benchmarks, the proposed methods outperformed not only alignment-based methods such as wmd but also powerful sentence vectors."
2020.emnlp-main.236.txt,2020,9 Conclusion,"moreover, we found that the latest powerful methods for sentence-vector estimation improve the norm and angle of word vectors (via vector converter; vc)."
2020.emnlp-main.236.txt,2020,9 Conclusion,"to solve the performance problem remaining for alignment-based sts methods, we proposed word rotator’s distance (wrd), a new unsupervised, emd-based sts metric."
2020.emnlp-main.236.txt,2020,9 Conclusion,"we first indicated that (1) the norm and angle of word vectors are good proxies for the importance of a word and the dissimilarity between words, respectively, and (2) some previous methods “mix up” the norm and direction vectors."
2020.emnlp-main.237.txt,2020,5 Conclusion and Future Work,"extensive experiments on several typical kge and ne datasets show that dicgrl achieves consistent and significant improvement compared to existing continual learning models, which verifies the effectiveness of our model on alleviating the catastrophic forgetting problem."
2020.emnlp-main.237.txt,2020,5 Conclusion and Future Work,"in the future, we will explore to extend the idea of disentanglement in the continual learning of other nlp tasks."
2020.emnlp-main.237.txt,2020,5 Conclusion and Future Work,"in this paper, we propose to study the problem of continual graph representation learning, aiming to handle the streaming nature of the emerging multi-relational data."
2020.emnlp-main.237.txt,2020,5 Conclusion and Future Work,"to this end, we propose a disentangled-based continual graph representation learning (dicgrl) framework, inspired by human’s ability to learn procedural knowledge."
2020.emnlp-main.238.txt,2020,7 Conclusions,ablation study shows that the two-way interaction by pot and blu is the key to significant improvement.
2020.emnlp-main.238.txt,2020,7 Conclusions,"as css and pss are compatible with any supervised bli and ot-based unsupervised bli approaches, they can also be applied to the latent space optimization."
2020.emnlp-main.238.txt,2020,7 Conclusions,"based on the message passing mechanisms, we design two strategies of semi-supervised bli to integrate supervised and unsupervised approaches, css and pss, which are constructed on cyclic and parallel strategies respectively."
2020.emnlp-main.238.txt,2020,7 Conclusions,blu employs a bidirectional retrieval to enlarge the annotated data and stabilize the training of supervised bli approaches.
2020.emnlp-main.238.txt,2020,7 Conclusions,"in this paper, we introduce the two-way interaction between the supervised signal and unsupervised alignment by proposed pot and blu message passing mechanisms."
2020.emnlp-main.238.txt,2020,7 Conclusions,pot guides the ot-based unsupervised bli by prior bli transformation.
2020.emnlp-main.238.txt,2020,7 Conclusions,the results show that css and pss achieve sota results over two popular datasets.
2020.emnlp-main.239.txt,2020,5 Conclusion and Future Work,empirical analysis showed the effectiveness of the wasserstein distance-based regularizer in text matching.
2020.emnlp-main.239.txt,2020,5 Conclusion and Future Work,experimental results on four benchmarks showed that wd-match can outperform the baselines including its underlying models.
2020.emnlp-main.239.txt,2020,5 Conclusion and Future Work,"in the future, we plan to study different regularizers in the asymmetrical text matching task, for further exploring their effectiveness in bridging the gap between asymmetrical domains."
2020.emnlp-main.239.txt,2020,5 Conclusion and Future Work,"in this paper, we proposed a novel wasserstein distance-based regularizer to improve the sequence representations, for text matching in asymmetrical domains."
2020.emnlp-main.239.txt,2020,5 Conclusion and Future Work,"the method, called wd-match, amounts to adversarial interplay of two branches: estimating the wasserstein distance given the projected features, and minimizing the wasserstein distance regularized matching loss."
2020.emnlp-main.239.txt,2020,5 Conclusion and Future Work,"we show that the regularizer helps wd-match to well distribute the generated feature vectors in the semantic space, and therefore more suitable for matching."
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,find is suitable for any text classification tasks where a model might learn irrelevant or harmful features during training.
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,"first, the word clouds may reveal sensitive contents in the training data to human debuggers."
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,"first, what is an effective way to understand each feature?"
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,"for example, using relu as activation functions in lstm cells (instead of tanh) renders the features non-negative."
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,"for future work, it would be interesting to extend find to other nlp tasks, e.g., question answering and natural language inference."
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,"for instance, bert-base (devlin et al., 2019) has 768 features (before the final dense layer) which require lots of human effort to perform investigation."
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,"hence, find would be more effective when used together with disentangled text representations (cheng et al., 2020)."
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,"in general cases, find is at least useful for model verification."
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,"in general, the principle of find is understanding the features and then disabling the irrelevant ones."
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,"in order to generalize the framework beyond cnns, there are two questions to consider."
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,"in this case, it would be more efficient to use find to disable attention heads rather than individual features (voita et al., 2019)."
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,it is also convenient to use since only the trained model and the training data are required as input.
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,"moreover, it can address many problems simultaneously such as removing religious and racial bias together with gender bias even if we might not be aware of such problems before using find."
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,"nevertheless, find has some limitations."
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,"over the past few years, we have seen rapid growth of scientific research in both topics (visualizations and interpretability) aiming to understand many emerging advanced models including the popular transformer-based models (jo and myaeng, 2020; voita et al., 2019; hoover et al., 2020)."
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,"second, can we make the model features more interpretable?"
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,"second, the more hidden features the model has, the more human effort find needs for debugging."
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,"so, they can be summarized using one word cloud which is more practical for debugging."
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,the process makes visualizations and interpretability more actionable.
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,"third, it is possible that one feature detects several patterns (jacovi et al., 2018) and it will be difficult to disable the feature if some of the detected patterns are useful while the others are harmful."
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,this will require some modifications to understand how the features capture relationships between two input texts.
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,"using the proposed framework on cnn text classifiers, we found that (i) word clouds generated by running lrp on the training data accurately revealed the behaviors of cnn features, (ii) some of the learned features might be more useful to the task than the others and (iii) disabling the irrelevant or harmful features could improve the model predictive performance and reduce unintended biases in the model."
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,we believe that our work will inspire other researchers to foster advances in both topics towards the more tangible goal of model debugging.
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,"we exemplified this with two word clouds representing each bilstm feature in appendix c, and we plan to experiment with advanced visualizations such as lstmvis (strobelt et al., 2018) in the future."
2020.emnlp-main.24.txt,2020,8 Discussion and Conclusions,"we proposed find, a framework which enables humans to debug deep text classifiers by disabling irrelevant or harmful features."
2020.emnlp-main.240.txt,2020,4 Conclusion,"empirical results on several different benchmarks on bilingual lexicon induction, cross-lingual word similarity, multilingual document classification, and multilingual document parsing tasks show remarkably good performance and robustness of the proposed framework."
2020.emnlp-main.240.txt,2020,4 Conclusion,"overall, our results encourage the development of simple multi-stage models for learning multilingual word embeddings."
2020.emnlp-main.240.txt,2020,4 Conclusion,the proposed framework has the flexibility to be easily employed in hybrid setups where supervision is available for a few language pairs but is unavailable for others.
2020.emnlp-main.240.txt,2020,4 Conclusion,the two stages correspond to unsupervised generation of bilingual lexicons for a few language pairs and subsequently learning a shared latent multilingual space.
2020.emnlp-main.240.txt,2020,4 Conclusion,"though the proposed framework seems simple compared to the joint optimization methods (chen and cardie, 2018; alaux et al., 2019; heyman et al., 2019), our main contribution has been to show that it is a strong performer."
2020.emnlp-main.240.txt,2020,4 Conclusion,"we propose to solve each of them with existing techniques (artetxe et al., 2018b; alvarezmelis and jaakkola, 2018; jawanpuria et al., 2019)."
2020.emnlp-main.240.txt,2020,4 Conclusion,we study a two-stage framework for learning unsupervised multilingual word embeddings.
2020.emnlp-main.241.txt,2020,5 Conclusion,"in the future, we will focus our attention on the topic of generalization in the presence of domain differences such as novel objects; and given goal statements in test games that were not seen by the agent during training."
2020.emnlp-main.241.txt,2020,5 Conclusion,"in this paper, we have restricted our analysis to tbgs that feature similar domain distributions in train and test games."
2020.emnlp-main.241.txt,2020,5 Conclusion,our bootstrapped model – trained on the salient observation tokens – obtains generalization performance similar to sota methods – with 10x-20x fewer training games – due to better generalization; and shows accelerated convergence.
2020.emnlp-main.241.txt,2020,5 Conclusion,we present a method for improving generalization in tbgs by removing irrelevant tokens from observation texts.
2020.emnlp-main.242.txt,2020,6 Conclusion,extensive experiments on glue tasks show that bert-emd can achieve competitive performances with the large bertbase model while significantly reducing the model size and inference time.
2020.emnlp-main.242.txt,2020,6 Conclusion,"in addition, a cost attention mechanism is designed to further improve the model’s performance and accelerate convergence time by learning the layer weights used in emd automatically."
2020.emnlp-main.242.txt,2020,6 Conclusion,"in this paper, we propose a novel bert compression method based on many-to-many layer mapping by earth mover’s distance (emd)."
2020.emnlp-main.242.txt,2020,6 Conclusion,"to our knowledge, bert-emd is the first work that allows each intermediate student layer to learn from any intermediate teacher layers adaptively."
2020.emnlp-main.243.txt,2020,7 Conclusion,"and vn can handle ontology flexibly with a simple and effective structure, which is able to work with incomplete ontology."
2020.emnlp-main.243.txt,2020,7 Conclusion,and we also introduce the annotation of supporting span.
2020.emnlp-main.243.txt,2020,7 Conclusion,"combining sa with vn, savn has shown excellent performance on both multiwoz 2.0 and multiwoz 2.1."
2020.emnlp-main.243.txt,2020,7 Conclusion,"furthermore, dst models with supporting span allow for a fairer comparison regardless of whether the ontology is used."
2020.emnlp-main.243.txt,2020,7 Conclusion,"in future work, the supporting span annotation can be added to the datasets of a task-oriented dialog system, for the reason that supporting span serves as a bridge between diverse descriptions of users and the normative values in the system."
2020.emnlp-main.243.txt,2020,7 Conclusion,sa shares parameters not only among all slots but also between slots and utterances.
2020.emnlp-main.243.txt,2020,7 Conclusion,we introduce a new architecture that divides the prediction of slots and the use of ontology.
2020.emnlp-main.244.txt,2020,6 Discussions and Future Works,"in this paper, we focus on reducing the number of layers and operations of odqa models, but the actual latency improvement also depends on the hardware specifications."
2020.emnlp-main.244.txt,2020,6 Discussions and Future Works,it will be interesting to integrate these two approaches to achieve further computation reduction for odqa models.
2020.emnlp-main.244.txt,2020,6 Discussions and Future Works,"on gpus we cannot expect a reduction in the number of operations to translate 1:1 to lower execution times, since they are highly optimised for parallelism.3 we leave the parallelism enhancements of skylinebuilder for future work."
2020.emnlp-main.244.txt,2020,6 Discussions and Future Works,we also notice that the distillation technique is complementary to the adaptive computation methods.
2020.emnlp-main.245.txt,2020,6 Conclusion,we have additionally presented some analysis of ropes that should inform future work on this dataset.
2020.emnlp-main.245.txt,2020,6 Conclusion,"we have demonstrated that our model substantially outperforms prior work on ropes, a challenging new reading comprehension dataset."
2020.emnlp-main.245.txt,2020,6 Conclusion,we propose a multi-step reading comprehension model that performs chained inference over natural language text.
2020.emnlp-main.245.txt,2020,6 Conclusion,"while our model is not a neural module network, as our model uses a single fixed layout instead of different layouts per question, we believe there are enough similarities that future work could explore combining our modules with those used in other neural module networks over text, leading to a single model that could perform the necessary reasoning for multiple different datasets."
2020.emnlp-main.246.txt,2020,7 Discussion and Future Work,"furthermore, it would be worth investigating how inter-annotator agreement or potential human biases manifest in traditional datasets as compared to those generated with our framework."
2020.emnlp-main.246.txt,2020,7 Discussion and Future Work,"in this paper, we want to put the emphasis on the methodological innovation of our framework and the novel annotation scheme itself."
2020.emnlp-main.246.txt,2020,7 Discussion and Future Work,"on the other hand, experiments involving real users would provide valuable insights concerning the annotation costs and the quality of a dataset annotated with our method."
2020.emnlp-main.246.txt,2020,7 Discussion and Future Work,outcomes of such an experiment would be sensitive to the design of the user interface as well as the study design itself.
2020.emnlp-main.246.txt,2020,7 Discussion and Future Work,the robustness of our framework is demonstrated in an extensive set of simulations and experiments.
2020.emnlp-main.246.txt,2020,7 Discussion and Future Work,we assume for the purposes of this study that questions have an answer span contained in a single document and leave an extension to multi-hop questions and unanswerable questions to future research.
2020.emnlp-main.246.txt,2020,7 Discussion and Future Work,we deliberately choose to leave experiments including real human annotators to future research for the following reason.
2020.emnlp-main.247.txt,2020,5 Conclusion,"further experimental investigation shows that (1) compared with concept knowledge, the event knowledge we choose is more suitable for narrative mrc; (2) our proposed graph models the scene more effectively than the unstructured text and the unified plane graph do; (3) our proposed gdin encodes the scene graph efficiently by iterating multiple steps."
2020.emnlp-main.247.txt,2020,5 Conclusion,"in this paper, we focus on narrative machine reading comprehension."
2020.emnlp-main.247.txt,2020,5 Conclusion,"inspired by human behaviors, we propose a novel method to restore the scene for the narrative passage."
2020.emnlp-main.247.txt,2020,5 Conclusion,"specifically, we introduce the event knowledge from atomic and build a three-dimensional graph to describe the scene."
2020.emnlp-main.247.txt,2020,5 Conclusion,the result shows our method achieves state-of-the-art.
2020.emnlp-main.247.txt,2020,5 Conclusion,"to encode the scene graph, we propose graph dimensional-iteration network (gdin)."
2020.emnlp-main.247.txt,2020,5 Conclusion,"we conduct experiments on two relevant datasets, rocstories and cosmosqa."
2020.emnlp-main.248.txt,2020,6 Conclusion,"in addition, integrating our multi-span architecture into existing models further improves performance on drop, as is evident from the leading models on drop’s leaderboard."
2020.emnlp-main.248.txt,2020,6 Conclusion,"in this work, we cast the task of answering multi-span questions as a sequence tagging problem, and present a simple corresponding multispan architecture."
2020.emnlp-main.248.txt,2020,6 Conclusion,our code can be downloaded from https://github.com/eladsegal/tag-based-multispan-extraction.
2020.emnlp-main.248.txt,2020,6 Conclusion,"we show that replacing the standard single-span architecture with our multispan architecture dramatically improves results on multi-span questions, without harming performance on single-span questions, leading to state-of-the-art results on quoref."
2020.emnlp-main.249.txt,2020,5 Conclusion & Future Work,"additional future investigations may include a deeper analysis of the mathematical and statistical properties of the weighted coefficients ρw, τw, as well as a rigorous derivation of the optimal values for the parameters of the data collection approach."
2020.emnlp-main.249.txt,2020,5 Conclusion & Future Work,"as future work, we plan to collect human annotations (i) to test the proposed data collection approach on real data and (ii) to assess the validity and estimate the parameters of the proposed stochastic transitivity model."
2020.emnlp-main.249.txt,2020,5 Conclusion & Future Work,"finally, we defined a stochastic transitivity model to simulate semantic-driven pairwise comparisons, which allows tuning the parameters of the data collection approach and which confirmed a significant increase in the performance metrics ρw and τw of the proposed adaptive approach compared with the uniform approach (see table 2)."
2020.emnlp-main.249.txt,2020,5 Conclusion & Future Work,"in this paper, we provided a protocol for the construction – based on adaptive pairwise comparisons and tailored on the available resources – of a dataset, which can be used to test or validate any relatedness-based domain-specific semantic model and which is optimized to be particularly accurate in top-rank evaluation."
2020.emnlp-main.249.txt,2020,5 Conclusion & Future Work,"moreover, we defined the metrics ρw and τw, extensions of well-known ranking correlation coefficients, to evaluate a semantic model via the aforementioned dataset by taking into account the greater significance of top ranks."
2020.emnlp-main.25.txt,2020,5 Conclusion and Future Work,"in this work, we considered only url documents with content."
2020.emnlp-main.25.txt,2020,5 Conclusion and Future Work,"other potential document types that could be considered are pdfs, doc etc.and urls without content (e.g.login, tracking)."
2020.emnlp-main.25.txt,2020,5 Conclusion and Future Work,we also release a new public twitter dataset on the cdp task.
2020.emnlp-main.25.txt,2020,5 Conclusion and Future Work,we introduced the conversational document prediction (cdp) task and investigated the performance of state-of-the-art dl and ir models.
2020.emnlp-main.25.txt,2020,5 Conclusion and Future Work,we plan to address these challenges in future work.
2020.emnlp-main.250.txt,2020,5 Conclusion and Future Work,experimental results show the effectiveness of mft from various aspects.
2020.emnlp-main.250.txt,2020,5 Conclusion and Future Work,"in the future, we plan to apply mft to other language models (e.g., transformerxl (dai et al., 2019) and albert (lan et al., 2019)) and for other nlp tasks."
2020.emnlp-main.250.txt,2020,5 Conclusion and Future Work,"in this paper, we propose a new training procedure named meta fine-tuning (mft) used in neural language models for multi-domain text mining."
2020.emnlp-main.251.txt,2020,6 Conclusions,"in future work, we will focus on producing novel continuations of the user’s search intent, extending the approach to other domains, and automating the design of behavioral hypotheses."
2020.emnlp-main.251.txt,2020,6 Conclusions,"on a domain-specific search benchmark, our model outperforms all reference methods in aggregate and across varying session properties, demonstrating its effectiveness in a robust way."
2020.emnlp-main.251.txt,2020,6 Conclusions,qualitative evaluation for open-ended generation is also an interesting topic on the roadmap.
2020.emnlp-main.251.txt,2020,6 Conclusions,this paper presents an effective approach for incorporating user-induced interaction patterns as behavioral hypotheses into the query generation process.
2020.emnlp-main.251.txt,2020,6 Conclusions,"under an encoder-decoder transformer framework, the proposed tokenwise attentions demonstrate the desirable modeling working by placing emphasis on different behavioral hypotheses at different occasions."
2020.emnlp-main.252.txt,2020,7 Conclusion and Future Work,"besides, how to enable the existing emotion-cause pair extraction models to consider the effect of context is also a meaningful task."
2020.emnlp-main.252.txt,2020,7 Conclusion and Future Work,experiments demonstrate the effectiveness and generality of our proposed pam.
2020.emnlp-main.252.txt,2020,7 Conclusion and Future Work,"for example, how to quantify the effect of context on the targeted causal relationship is another important task to study this type of causal relationship."
2020.emnlp-main.252.txt,2020,7 Conclusion and Future Work,"furthermore, we propose a prediction aggregation module (pam) with low computational complexity, to enable the models to dynamically adjust the final prediction according to the type of emotion-cause pair contained in a document."
2020.emnlp-main.252.txt,2020,7 Conclusion and Future Work,"in this paper, we articulate the importance of context in determining the causal relationships between emotions and their causes."
2020.emnlp-main.252.txt,2020,7 Conclusion and Future Work,"in view of the importance of context in the conditional causal relationships we define in this work, what we have done is only the first step."
2020.emnlp-main.252.txt,2020,7 Conclusion and Future Work,there remain many important and interesting problems ahead of us.
2020.emnlp-main.252.txt,2020,7 Conclusion and Future Work,"to address this problem, we define a new task of determining whether or not an input emotion-cause pair has a causal relationship under a specific context."
2020.emnlp-main.252.txt,2020,7 Conclusion and Future Work,we construct a dataset for our task through manual annotation and negative sampling based on the ecpe dataset.
2020.emnlp-main.253.txt,2020,6 Conclusion,cometa is available by contacting the last author via e-mail or following the instructions on https://www.siphs.org/.
2020.emnlp-main.253.txt,2020,6 Conclusion,different evaluation scenarios were designed to compare the performance of conventional dictionary/string-matching techniques against the mainstream neural counterparts and revealed that these models complement each other very well and the best performance is achieved by combining these paradigms.
2020.emnlp-main.253.txt,2020,6 Conclusion,"nonetheless, the missing performance of 28-46% (depending on the evaluation scenario) encourages future research on this area to take this corpus as a challenging yet reliable evaluation benchmark for further development of models specific to this domain."
2020.emnlp-main.253.txt,2020,6 Conclusion,"we presented cometa, a unique corpus for its scale and coverage which is curated to maintain high quality annotations of medical terms in layman’s language on reddit with concepts from snomed knowledge graph."
2020.emnlp-main.253.txt,2020,6 Conclusion,we release the pre-trained embeddings and the code to replicate our baselines online at https://github.com/ cambridgeltl/cometa.
2020.emnlp-main.254.txt,2020,9 Conclusion,"for tasks such as posl or dal, which require only shallow notions of syntax, non-contextual representations can do almost as well as contextual ones— pretraining on large amounts of data, or encoding contextual knowledge in the representations, does not help much for these tasks."
2020.emnlp-main.254.txt,2020,9 Conclusion,"in this paper, we argued for a new approach to probing, treating it as a bi-objective optimization problem."
2020.emnlp-main.254.txt,2020,9 Conclusion,"it has no single optimal solution, but can be analyzed—under the lens of pareto efficiency— to arrive on a set of optimal solutions."
2020.emnlp-main.254.txt,2020,9 Conclusion,the second part of our paper argues that we need to select harder tasks for the purpose of probing representations for syntactic knowledge.
2020.emnlp-main.254.txt,2020,9 Conclusion,"these pareto optimal solutions make explicit the trade-off between accuracy and complexity, also permitting for a deeper analysis of the probed representations."
2020.emnlp-main.254.txt,2020,9 Conclusion,we then run a battery of experiments on the harder task of dependency parsing; these show that contextual representations indeed provide much more useable syntactic knowledge than non-contextual ones.
2020.emnlp-main.255.txt,2020,5 Conclusion,"among them, we focused on the ood problem arising from the widely used zero erasure scheme, which results in misleading interpretation."
2020.emnlp-main.255.txt,2020,5 Conclusion,"hence, several interpretation methods have been proposed, and we reviewed their limitations throughout the paper."
2020.emnlp-main.255.txt,2020,5 Conclusion,"in addition, as experimentally analyzed, the interpretation result of our method is affected by the likelihood modeling performance."
2020.emnlp-main.255.txt,2020,5 Conclusion,interpretability is becoming more important owing to the increase in deep learning in nlp.
2020.emnlp-main.255.txt,2020,5 Conclusion,"it will be meaningful to interpret the state-of-the-art models such as xlnet (yang et al., 2019) and electra (clark et al., 2019)."
2020.emnlp-main.255.txt,2020,5 Conclusion,"our proposed input marginalization, which can mitigate the ood problem, can result in a faithful interpretation, thereby enabling better understanding of “black box” dnns."
2020.emnlp-main.255.txt,2020,5 Conclusion,"regarding the model-agnostic and task-agnostic properties of our method, they are applicable to any types of nlp model for various tasks, such as neural machine translation and visual question answering."
2020.emnlp-main.255.txt,2020,5 Conclusion,the scope of this study was primarily focused on interpreting dnns for sentiment analysis and natural language inference.
2020.emnlp-main.255.txt,2020,5 Conclusion,"to the best of our knowledge, neither the ood problem has been raised in interpreting nlp models nor the attempt to resolve it has been undertaken."
2020.emnlp-main.255.txt,2020,5 Conclusion,we can expect even more faithful interpretation if the modeling performance improves.
2020.emnlp-main.256.txt,2020,5 Conclusion,"as fc is only one test bed for adversarial attacks, it would be interesting to test this method on other nlp tasks requiring semantic understanding such as question answering to better understand shortcomings of models."
2020.emnlp-main.256.txt,2020,5 Conclusion,"our method is fully automatic, whereas previous work on generating claims for fact checking is generally rule-based or requires manual intervention."
2020.emnlp-main.256.txt,2020,5 Conclusion,we improve upon previous work on universal adversarial triggers by determining how to construct valid claims containing a trigger word.
2020.emnlp-main.256.txt,2020,5 Conclusion,"we present a method for automatically generating highly potent, well-formed, label cohesive claims for fc."
2020.emnlp-main.257.txt,2020,5 Conclusion,"the study suggests that besides improving our alignment algorithms for distant languages (vulic et al.´ , 2019), we should also focus on improving monolingual word vector spaces, and monolingual training conditions to unlock a true potential of cross-lingual learning."
2020.emnlp-main.257.txt,2020,5 Conclusion,"through controlled experiments in simulated low-resource scenarios, also involving languages with different morphology that are spoken in culturally related regions, we found that such vector spaces mainly arise from poor conditioning during training."
2020.emnlp-main.257.txt,2020,5 Conclusion,"we have provided a series of analyses that demonstrate together that non-isomorphism is not—as previously assumed—primarily a result of typological differences between languages, but in large part due to degenerate vector spaces and discrepancies between monolingual training regimes and data availability."
2020.emnlp-main.258.txt,2020,8 Conclusion and Future Work,"in the future, we plan to apply fa-rnn to other tasks and explore other variants of fa-rnn."
2020.emnlp-main.258.txt,2020,8 Conclusion and Future Work,"it can be initialized from res and can also learn from data, hence applicable to various scenarios including zero-shot, cold-start, low-resource and rich-resource scenarios."
2020.emnlp-main.258.txt,2020,8 Conclusion and Future Work,it is also interpretable and can be converted back into res.
2020.emnlp-main.258.txt,2020,8 Conclusion and Future Work,our experiments on text classification show that it outperforms previous neural approaches in both zero-shot and low-resource scenarios and is very competitive in rich-resource scenarios.
2020.emnlp-main.258.txt,2020,8 Conclusion and Future Work,we propose a type of recurrent neural networks called fa-rnn.
2020.emnlp-main.258.txt,2020,8 Conclusion and Future Work,"we release our data, re rules and code at https://github.com/ jeffchy/re2rnn."
2020.emnlp-main.259.txt,2020,7 Conclusion,"for both methods, we find that the pruned “good” subnetworks alone reach the performance comparable with the full model, while the “bad” ones do not."
2020.emnlp-main.259.txt,2020,7 Conclusion,"however, for structured pruning, even the “bad” subnetworks can be finetuned separately to reach fairly strong performance."
2020.emnlp-main.259.txt,2020,7 Conclusion,"the “good” subnetworks are not stable across finetuning runs, and their success is not attributable exclusively to non-trivial linguistic patterns in individual self-attention heads."
2020.emnlp-main.259.txt,2020,7 Conclusion,this study systematically tested the lottery ticket hypothesis in bert fine-tuning with two pruning methods: magnitude-based weight pruning and importance-based pruning of bert self-attention heads and mlps.
2020.emnlp-main.259.txt,2020,7 Conclusion,"this suggests that most of pre-trained bert is potentially useful in fine-tuning, and its success could have more to do with optimization surfaces rather than specific bits of linguistic knowledge."
2020.emnlp-main.26.txt,2020,7 Discussion and conclusion,a follow-up idea is training a classifier that predicts more precisely how likely it is that the final labels will be accurate based on the development of eo.
2020.emnlp-main.26.txt,2020,7 Discussion and conclusion,a natural extension is training a language model that generates the prophecies together with the encoder.
2020.emnlp-main.26.txt,2020,7 Discussion and conclusion,"bert, having access to the whole sentence at any time step, is less stable because new input can cause it to reassess past labels more easily."
2020.emnlp-main.26.txt,2020,7 Discussion and conclusion,"besides, we also found evidence of different behavior of the instability of partial outputs between correct and incorrect output sequences, which could potentially be a signal of later lower quality."
2020.emnlp-main.26.txt,2020,7 Discussion and conclusion,"even though the training (being done on complete sequences) differs from the testing situation (which exposes the model to partial input), the incremental metrics of most models are, in general, good: in sequence tagging, edit overhead is low, final decisions are taken early, and often partial outputs are a correct prefix of the complete non-incremental output."
2020.emnlp-main.26.txt,2020,7 Discussion and conclusion,"finally, we believe that using attention mechanisms to study the grounding of the edits, similarly to the ideas in kohn ¨ (2018), can be an important step towards understanding how the preliminary representations are built and decoded; we want to test this as well in future work."
2020.emnlp-main.26.txt,2020,7 Discussion and conclusion,"however, our initial experiments on building such classifier were not successful."
2020.emnlp-main.26.txt,2020,7 Discussion and conclusion,"if long-range dependencies are not captured, only neighboring words exert more influence in the choice of a label, so after seeing two words in the right context, the system rarely revises labels further back."
2020.emnlp-main.26.txt,2020,7 Discussion and conclusion,"our experiments show that the deficiencies of bert in the incremental metrics can be mitigated with some adaptations (truncated training or prophecies together with delay), which make its incremental quality become as good as those of other models."
2020.emnlp-main.26.txt,2020,7 Discussion and conclusion,"sequence classification is more unstable because, at initial steps, there is a higher level of uncertainty on what is coming next."
2020.emnlp-main.26.txt,2020,7 Discussion and conclusion,"since the semantic information is only kept encoded for a few steps in rnns (hupkes et al., 2018), this may be a reason why delay causes incremental metrics to be much better."
2020.emnlp-main.26.txt,2020,7 Discussion and conclusion,the use of gpt-2 prophecies led to promising improvements for bert in sequence tagging.
2020.emnlp-main.26.txt,2020,7 Discussion and conclusion,"this could be used, for example, in dialog systems: if edit overhead gets too high, a clarification request should be made."
2020.emnlp-main.26.txt,2020,7 Discussion and conclusion,"we see room for improvement, e.g.resorting to domain adaptation to make prophecies be more related to each genre."
2020.emnlp-main.26.txt,2020,7 Discussion and conclusion,we show that bidirectional encoders can be adapted to work under an incremental interface without a too drastic impact on their performance.
2020.emnlp-main.26.txt,2020,7 Discussion and conclusion,"we suppose this is due to the fact that, in our datasets, incorrect final output sequences still usually have more than 90% correct labels, so the learnable signal may be too weak."
2020.emnlp-main.260.txt,2020,4 Conclusion,for transformers we observed no preference for pruning attention heads which have been identified as important in interpretability studies.
2020.emnlp-main.260.txt,2020,4 Conclusion,"however, pruning middle layers and consecutive layers led to a larger drop in accuracy."
2020.emnlp-main.260.txt,2020,4 Conclusion,"similarly, for bert we found no preference between pruning top and bottom layers."
2020.emnlp-main.260.txt,2020,4 Conclusion,we also observe that the recovery during fine-tuning was uniformly distributed across attention heads.
2020.emnlp-main.260.txt,2020,4 Conclusion,"we conclude that there is often no direct entailment between importance of an attention head as characterised in several recent studies, and low prunability of the respective head using random pruning."
2020.emnlp-main.260.txt,2020,4 Conclusion,we confirmed the general expectation that a large number of attention heads can be pruned with limited impact on performance.
2020.emnlp-main.260.txt,2020,4 Conclusion,we systematically studied the effect of pruning attention heads in transformer and bert models.
2020.emnlp-main.261.txt,2020,6 Conclusion,"in this work, we highlight that the lack of predefined roles for layers adds to the difficulty of interpreting highly complex bert-based models."
2020.emnlp-main.261.txt,2020,6 Conclusion,we first define each layer’s functionality using integrated gradients.
2020.emnlp-main.261.txt,2020,6 Conclusion,"we found the following observations interesting and with a potential to be probed further: (i) why do the question word representations move away from contextual and answer representation in later layers?(ii) if the focus on confusing words increases from the initial to later layers, how does bert still have a high accuracy?"
2020.emnlp-main.261.txt,2020,6 Conclusion,we hope that this work will help the research community interpret bert for other complex tasks and explore the above open-ended questions.
2020.emnlp-main.261.txt,2020,6 Conclusion,we present results and analysis to show that bert is learning some form of passagequery interaction in its initial layers before arriving at the answer.
2020.emnlp-main.262.txt,2020,5 Conclusion,diffmask sheds light on what different layers ‘know’ about the input and where information about the prediction is stored in different layers.
2020.emnlp-main.262.txt,2020,5 Conclusion,faithfulness is validated in a controlled experiment pointing more clearly to some flaws of other attribution methods.
2020.emnlp-main.262.txt,2020,5 Conclusion,"to overcome the hindsight bias problem, we probe the model’s hidden states at different depths and amortize predictions over the training set."
2020.emnlp-main.262.txt,2020,5 Conclusion,we circumvent an intractable search by learning an end-to-end differentiable prediction model.
2020.emnlp-main.262.txt,2020,5 Conclusion,we have introduced a new post hoc interpretation method which learns to completely remove subsets of inputs or hidden states through masking.
2020.emnlp-main.262.txt,2020,5 Conclusion,we used our method to study bert-based models on sentiment classification and question answering.
2020.emnlp-main.263.txt,2020,6 Conclusion,"other explainability techniques, such as shapsampl, lime and occlusion take more time to compute, and are in addition considerably less faithful to the models and less consistent with the rationales of the models and similarities in the datasets."
2020.emnlp-main.263.txt,2020,6 Conclusion,we found that gradient-based explanations are the best for all of the three models and all of the three downstream text classification tasks that we consider in this work.
2020.emnlp-main.263.txt,2020,6 Conclusion,we further used them to compare and contrast different groups of explainability techniques on three downstream tasks and three diverse architectures.
2020.emnlp-main.263.txt,2020,6 Conclusion,we proposed a comprehensive list of diagnostic properties for the evaluation of explainability techniques from different perspectives.
2020.emnlp-main.264.txt,2020,6 Conclusion and Future Works,"in this work, we proposed an extension to the leaf-qa data using the public metadata provided for the charts."
2020.emnlp-main.264.txt,2020,6 Conclusion and Future Works,we also defined and experimented with a set of pre-training tasks and showed the improvement due to pre-training on the problem of cqa.
2020.emnlp-main.264.txt,2020,6 Conclusion and Future Works,"we also proposed a transformersbased framework while emphasizing on the need to exploit the structural properties of chart, and showed its strong effectiveness by achieving stateof-the-art with a significant margin on the recent chart q/a datasets."
2020.emnlp-main.264.txt,2020,6 Conclusion and Future Works,we discussed the current line of cqa work and proposed future directions by outlining the limitations of the current datasets and models.
2020.emnlp-main.264.txt,2020,6 Conclusion and Future Works,we used attention to dissect our model to show how each of its module functions to retrieve the final answer.
2020.emnlp-main.265.txt,2020,5 Conclusion,"in order to fully utilize the supervision information of synthesized counterfactual samples in robust vqa, we introduce a self-supervised contrastive learning mechanism to learn the relationship between factual samples and counterfactual samples."
2020.emnlp-main.265.txt,2020,5 Conclusion,the experimental results demonstrate that our method improves the reasoning ability and robustness of the vqa models.
2020.emnlp-main.266.txt,2020,5 Conclusion,"compared with typical knowledge graph embedding methods, our results show good performance on triple classification and link prediction."
2020.emnlp-main.266.txt,2020,5 Conclusion,"in this paper, we formulate physical commonsense learning as a knowledge graph completion problem."
2020.emnlp-main.266.txt,2020,5 Conclusion,our method also has the potential to be a generic approach to benefit performance on the knowledge graph completion problem.
2020.emnlp-main.266.txt,2020,5 Conclusion,we constrain types to reduce the solution space and add negative relationships to leverage negative training samples.
2020.emnlp-main.266.txt,2020,5 Conclusion,"we first use bert to augment training data of op and oa, and then employ constrained tucker factorization to complete the knowledge graph."
2020.emnlp-main.267.txt,2020,5 Conclusion,"in this paper, we have presented the vfd dataset with verbal and non-verbal responses."
2020.emnlp-main.267.txt,2020,5 Conclusion,"we confirmed the validity of the first-person view in the experiments for the response selection tasks; however, this task (especially, non-verbal response production) remains challenging, and improvements are required."
2020.emnlp-main.267.txt,2020,5 Conclusion,"we manually annotated 308k human utterances and 308k verbal and 81k non-verbal responses of agents, which are grounded in the agents’ first-person images with human eye-gaze locations."
2020.emnlp-main.268.txt,2020,6 Conclusion,experimental results on a large-scale newly-collected twitter corpus show that our model significantly outperforms sota either generation or classification models with traditional attention.
2020.emnlp-main.268.txt,2020,6 Conclusion,"moreover, we propose a novel multi-modality multi-head attention to capture the dense interactions between texts and images, where image wordings explicit in optical characters and implicit in image attributes are further exploited to bridge their semantic gap."
2020.emnlp-main.268.txt,2020,6 Conclusion,this paper studies cross-media keyphrase prediction on social media and presents a unified framework to couple the advantages of generation and classification models for this task.
2020.emnlp-main.269.txt,2020,6 Conclusion,"besides, it can either rank or generate answers seamlessly."
2020.emnlp-main.269.txt,2020,6 Conclusion,vd-bert is capable of modeling all the interactions between an image and a multi-turn dialog within a single-stream transformer encoder and enables the effective fusion of features from both modalities via simple visually grounded training.
2020.emnlp-main.269.txt,2020,6 Conclusion,"we have presented vd-bert, a unified visiondialog transformer model that exploits the pretrained bert language models for visual dialog."
2020.emnlp-main.269.txt,2020,6 Conclusion,"without pretraining on external visionlanguage datasets, our model establishes new stateof-the-art performance in the discriminative setting and shows promising results in the generative setting on the visual dialog benchmarks."
2020.emnlp-main.27.txt,2020,5 Discussion and Future Work,"for instance, we do not learn the concept of “person” from scratch in a new task, but have prior knowledge that “person” likely corresponds to names, and refine this concept through observations."
2020.emnlp-main.27.txt,2020,5 Discussion and Future Work,"moreover, it has the characteristics of a single, lifelong learning model that works well on many levels of data, unlike other approaches that only perform well on few-shot or high-resource tasks."
2020.emnlp-main.27.txt,2020,5 Discussion and Future Work,our approach naturally lends itself to life-long learning.
2020.emnlp-main.27.txt,2020,5 Discussion and Future Work,our experiments consistently show that the generation framework is suitable for sequence labeling and sets a new record for few-shot learning.
2020.emnlp-main.27.txt,2020,5 Discussion and Future Work,"our model adapts to new tasks efficiently with limited samples, while incorporating the label semantics expressed in natural words."
2020.emnlp-main.27.txt,2020,5 Discussion and Future Work,"our simple yet effective approach is also easily extensible to other applications such as multi-label classification, or structured prediction via nested tagging patterns."
2020.emnlp-main.27.txt,2020,5 Discussion and Future Work,"the natural language output space allows us to retain the knowledge from previous tasks through shared embeddings, unlike the tokenlevel model which needs new classifiers for novel tasks, resulting in a broken chain of knowledge."
2020.emnlp-main.27.txt,2020,5 Discussion and Future Work,the unified input-output format allows the model to incorporate new data from any domain.
2020.emnlp-main.27.txt,2020,5 Discussion and Future Work,this is akin to how humans learn.
2020.emnlp-main.270.txt,2020,6 Conclusion,"as a consequence, we still have very little understanding of what kind of information these languages encode."
2020.emnlp-main.270.txt,2020,6 Conclusion,"examples of more sophisticated game scenarios are bidirectional conversations where multi-symbol messages are challenging to analyse (kottur et al., 2017; bouchacourt and baroni, 2019) or games with image sequences as input (santamaría-pang et al., 2019)."
2020.emnlp-main.270.txt,2020,6 Conclusion,"in light of these results, it would be interesting to explore the use of unsupervised tokenisers that work well for languages without spaces (e.g."
2020.emnlp-main.270.txt,2020,6 Conclusion,"in this paper, for the first time, we focus on syntactic analysis of emergent languages."
2020.emnlp-main.270.txt,2020,6 Conclusion,"interestingly, our analysis shows that even these languages do not appear to have a notion of word classes, suggesting that their symbols may in fact be more akin to letters than to words."
2020.emnlp-main.270.txt,2020,6 Conclusion,our results also suggest that more sophisticated game scenarios may be required to obtain more interesting structure.
2020.emnlp-main.270.txt,2020,6 Conclusion,"sentencepiece kudo and richardson, 2018) prior to our approach and to try other word embedding models for diora, such as the character-based elmo embeddings8 (peters et al., 2018) or the more recent bert (devlin et al., 2019)."
2020.emnlp-main.270.txt,2020,6 Conclusion,"to facilitate such analysis, we bundled our tests in a comprehensive and easily usable evaluation framework.9 we hope to have inspired other researchers to apply syntactic analysis techniques and encourage them to use our code to evaluate new emergent languages trained in other scenarios."
2020.emnlp-main.270.txt,2020,6 Conclusion,"ugi could provide an integral part in analysing the languages emerging in such games, especially since it – contrary to most techniques previously used for the analysis of emergent languages – does not require a description of the hypothesised semantic content of the messages."
2020.emnlp-main.270.txt,2020,6 Conclusion,"we argue that while the extent to which syntax develops in different types of referential games is an interesting question in its own right, a better understanding of the syntactic structure of emergent languages could also provide pivotal in better understanding their semantics, especially if this is considered from a compositional point of view."
2020.emnlp-main.270.txt,2020,6 Conclusion,we first confirm that the techniques are capable of inferring interesting grammars for our artificial setup and demonstrate that ccl appears to be a more suitable constituency parser than diora.
2020.emnlp-main.270.txt,2020,6 Conclusion,"we test two different unsupervised grammar induction (ugi) algorithms that have been successful for natural language: a pre-neural statistical one, ccl, and a neural one, diora."
2020.emnlp-main.270.txt,2020,6 Conclusion,"we then find that the shorter languages, with messages up to 5 symbols, do not contain any interesting structure, while languages with longer messages appear to be substantially more structured than the two random baselines we compare them with."
2020.emnlp-main.270.txt,2020,6 Conclusion,"we use them to infer grammars for a variety of languages emerging from a simple referential game and then label those trees with bmm, considering in particular the effect of the message length and vocabulary size on the extent to which structure emerges."
2020.emnlp-main.270.txt,2020,6 Conclusion,"while studying language and communication through referential games with artificial agents has recently regained popularity, there is still a very limited amount of tools available to facilitate the analysis of the resulting emergent languages."
2020.emnlp-main.271.txt,2020,7 Conclusion,in this paper we introduce a novel sub-instruction module and the fine-grained r2r dataset to encourage the learning of correspondences between vision and language.
2020.emnlp-main.271.txt,2020,7 Conclusion,"our experiments show that by implementing the sub-instruction module in state-of-theart agents, most of the agents are able to follow the given instruction more closely and achieve better performance."
2020.emnlp-main.271.txt,2020,7 Conclusion,the sub-instruction module enables the agent to attend to one particular sub-instruction at each time-step and decides whether the agent needs to proceed to the next subinstruction.
2020.emnlp-main.271.txt,2020,7 Conclusion,"we also show that, with the subinstruction annotations, the entire navigation trajectory is trackable."
2020.emnlp-main.271.txt,2020,7 Conclusion,we believe that the idea of subinstruction module and a sub-instruction annotated dataset can benefit future studies in the vln task as well as other vision-and-language problems.
2020.emnlp-main.272.txt,2020,6 Conclusions,evaluation results on two benchmarks indicate that our model can significantly outperform state-of-the-art methods.
2020.emnlp-main.272.txt,2020,6 Conclusions,"to this end, we devise a knowledge selection module, and propose an unsupervised approach to jointly optimizing knowledge selection and response generation."
2020.emnlp-main.272.txt,2020,6 Conclusions,we apply large-scaled pre-trained language models to the task of knowledge-grounded dialogue generation.
2020.emnlp-main.273.txt,2020,5 Conclusion,"experimental results on multiwoz shows that, by using mintl, our systems not only achieve new sota result on both dialogue state tracking and end-to-end response generation but also improves the inference efficiency."
2020.emnlp-main.273.txt,2020,5 Conclusion,"in addition, two pre-trained seq2seq language models: t5 (raffel et al., 2019) and bart (lewis et al., 2019) are incorporated in our framework."
2020.emnlp-main.273.txt,2020,5 Conclusion,"in future work, we plan to explore taskoriented dialogues domain-adaptive pre-training methods (wu et al., 2020; peng et al., 2020) to enhance our language model backbones, and extend the framework for mixed chit-chat and taskoriented dialogue agents (madotto et al., 2020a)."
2020.emnlp-main.273.txt,2020,5 Conclusion,"in this paper, we proposed mintl, a simple and general transfer learning framework that effectively leverages pre-trained language models to jointly learn dst and dialogue response generation."
2020.emnlp-main.273.txt,2020,5 Conclusion,the lev is proposed for reducing the dst complexity and improving inference efficiency.
2020.emnlp-main.274.txt,2020,5 Conclusion,"to reduce the risk of inference collapse while maximizing the generation quality, we directly modified the training objective and devised a technique to scale dropouts along the hierarchy."
2020.emnlp-main.274.txt,2020,5 Conclusion,we proposed a novel hierarchical and recurrent vae-based architecture to capture accurately the semantics of fully annotated goal-oriented dialog corpora.
2020.emnlp-main.274.txt,2020,5 Conclusion,we showed that our proposed model vhda was able to achieve significant improvements for various competitive dialog state trackers in diverse corpora through extensive experiments.
2020.emnlp-main.274.txt,2020,5 Conclusion,"we would also like to explore different implementations in line with recent advances in dialog models, especially using large-scale pre-trained language models."
2020.emnlp-main.274.txt,2020,5 Conclusion,"with recent trends in goal-oriented dialog systems gravitating towards end-to-end approaches (lei et al., 2018), we wish to explore a self-supervised model, which discriminatively generates samples that directly benefit the downstream models for the target task."
2020.emnlp-main.275.txt,2020,7 Conclusion,experiments show that both pipm and kdbts improve the state-of-the-art latent variable model and their combination achieves further improvement.
2020.emnlp-main.275.txt,2020,7 Conclusion,"in the future, we would explore three aspects: (1) more efficient posterior information representation and corresponding prediction module, (2) the interpretability of knowledge selection and (3) knowledge selection without knowledge label."
2020.emnlp-main.275.txt,2020,7 Conclusion,"in this paper, we firstly analyze the gap between prior and posterior knowledge selection for opendomain knowledge-grounded dialogue."
2020.emnlp-main.275.txt,2020,7 Conclusion,"then, we deal with it on two aspects: (1) we enhance the prior selection module with the posterior information obtained by the pipm and we explore several variants of posterior information.(2) we design the kdbts to train the decoder with knowledge selected from the prior distribution, removing the exposure bias of knowledge selection."
2020.emnlp-main.276.txt,2020,5 Conclusion,"experiments show that the copt significantly improves the quality of the generated responses, which demonstrates the effectiveness of this approach."
2020.emnlp-main.276.txt,2020,5 Conclusion,"in contrast to existing approaches, it learns on counterfactual responses inferred from the structural causal model, taking advantage of observed responses."
2020.emnlp-main.276.txt,2020,5 Conclusion,this helps the model to explore the high-reward area of the potential response space.
2020.emnlp-main.276.txt,2020,5 Conclusion,"we propose a model-agnostic approach, copt, that can be applied to any adversarial learning-based dialogue generation models."
2020.emnlp-main.277.txt,2020,6 Conclusion,"automatic and manual evaluation shows that our method can produce high-quality post-response pairs that are both coherent and content-rich, which can be further used to improve the performance of competitive baselines."
2020.emnlp-main.277.txt,2020,6 Conclusion,our method may inspire other research in low-resource nlp tasks.
2020.emnlp-main.277.txt,2020,6 Conclusion,"this paper presents a novel dialogue distillation method that consists of two processes, i.e., 1) a data augmentation process to construct new postresponse pairs from unpaired data and 2) a model distillation process that distills a teacher model trained on the original data to the augmented data."
2020.emnlp-main.278.txt,2020,5 Conclusions,"however, it is reasonable and straightforward to combine them together."
2020.emnlp-main.278.txt,2020,5 Conclusions,"in the movie-ticket booking task, mctsddu agent exceeds recent background planning approaches by a wide margin with extraordinary data efficiency."
2020.emnlp-main.278.txt,2020,5 Conclusions,"in this paper, one main focus is to demonstrate the differences between background and decisiontime planning."
2020.emnlp-main.278.txt,2020,5 Conclusions,our work introduces a novel way to apply deep model-based rl to task-completion dialogue policy learning.
2020.emnlp-main.278.txt,2020,5 Conclusions,this might be an interesting topic for future work.
2020.emnlp-main.278.txt,2020,5 Conclusions,we combine the advanced valuebased methods with mcts as decision-time planning.
2020.emnlp-main.279.txt,2020,5 Conclusions,evaluation results on three benchmarks indicate that our model can significantly outperform state-of-the-art deep generation models in terms of both response quality and decoding speed.
2020.emnlp-main.279.txt,2020,5 Conclusions,we propose a simple generation model with order recovery and masked content recovery as auxiliary tasks.
2020.emnlp-main.28.txt,2020,6 Conclusion,"for the future work, we suggest to integrate the ranking models and generation model, e.g., in beam search stage or reinforcement learning using ranking score as reward signal."
2020.emnlp-main.28.txt,2020,6 Conclusion,human evaluation shows that human preference is improved with our ranking method.
2020.emnlp-main.28.txt,2020,6 Conclusion,"in particular, the conventional dialog perplexity baseline shows little predictive power on reddit human feedback data."
2020.emnlp-main.28.txt,2020,6 Conclusion,we ensemble the feedback prediction models and a humanlike scoring model to rank the machine generated dialog responses.
2020.emnlp-main.28.txt,2020,6 Conclusion,we leverage reddit human feedback data to build and release a large-scale training dataset for feedback prediction.we trained gpt-2 based models on 133m pairs of human feedback data and demonstrate that these models outperform several standard baselines.
2020.emnlp-main.280.txt,2020,5 Conclusion,attnio can also be trained to generate proper paths even in a more affordable setting of target supervision.
2020.emnlp-main.280.txt,2020,5 Conclusion,"in this work, we suggest attnio, a novel path traversal model that reasons over kg based on two directions of attention flows."
2020.emnlp-main.280.txt,2020,5 Conclusion,"lastly, we show through case study that our model enjoys from transparent interpretation of path reasoning process, and is capable of intuitively modeling knowledge exploration depending on the dialog characteristics."
2020.emnlp-main.280.txt,2020,5 Conclusion,the empirical evaluations on opendialkg dataset show the strength of attnio in knowledge retrieval compared to baselines.
2020.emnlp-main.281.txt,2020,6 Conclusion,"in this paper, we propose a novel “two-teacher one-student” learning framework (ttos) for taskoriented dialogue, which aims to improve the performance of the task-oriented dialogue system in retrieving accurate entries from kb and generating human-like responses simultaneously."
2020.emnlp-main.281.txt,2020,6 Conclusion,the experimental results on two benchmark datasets demonstrated that our model achieves impressive results compared to the state-of-the-art task-oriented dialogue systems.
2020.emnlp-main.281.txt,2020,6 Conclusion,"with adversarial learning, we train the student network to amalgamate expert knowledge naturally from the two teacher networks for the above two goals."
2020.emnlp-main.282.txt,2020,5 Conclusion,"in this paper, we propose a meta-embedding learning approach called task-oriented domain-specific autoencoded meta-embedding (tdaeme), which leverages task-oriented supervision to improve the combination of general and domain embeddings."
2020.emnlp-main.282.txt,2020,5 Conclusion,we conducted experiments on four text classification datasets and the results show the effectiveness of our proposed method.
2020.emnlp-main.283.txt,2020,6 Conclusion,"at the same time, their coverage in existing sense-annotated corpora is very limited."
2020.emnlp-main.283.txt,2020,6 Conclusion,"by leveraging a state-of-the-art bert-based wsd system that propagates sense embeddings across wordnet, we have shown that these unambiguous words provide an excellent bridge to reach a wider range of oov senses."
2020.emnlp-main.283.txt,2020,6 Conclusion,"finally, we openly release uwa, a large corpus annotated with unambiguous words, together improved bert and roberta-based sense embeddings, model predictions and visualizations at http://danlou.github.io/uwa."
2020.emnlp-main.283.txt,2020,6 Conclusion,for future work it would be interesting to test these sense embeddings in a wider range of applications outside wsd.
2020.emnlp-main.283.txt,2020,6 Conclusion,"in this paper, we proposed a simple method which exploits sense annotations of unambiguous words from unlabeled corpora, thereby effectively extending existing sense-annotated corpora with low-effort."
2020.emnlp-main.283.txt,2020,6 Conclusion,"moreover, one of the most surprising findings from this paper is that a single occurrence of oov unambiguous words is enough to improve the performance of wsd models."
2020.emnlp-main.283.txt,2020,6 Conclusion,"since the embedding space is clearly more diversified, as shown in figure 2, this may lead to improvements in other downstream tasks."
2020.emnlp-main.283.txt,2020,6 Conclusion,"this is relevant because (1) it is not always easy to retrieve a large number of examples for unambiguous words, and (2) it facilitates a cheaper manual verification, if required."
2020.emnlp-main.283.txt,2020,6 Conclusion,"this translates, in turn, into improving results for wsd."
2020.emnlp-main.283.txt,2020,6 Conclusion,unambiguous words are a surprisingly large portion of existing knowledge resources like wordnet.
2020.emnlp-main.284.txt,2020,5 Conclusions,"wbr facilitates the novel between-within relation loss, enabling the exploitation of distributional information."
2020.emnlp-main.284.txt,2020,5 Conclusions,"wbr is evaluated on four different datasets, where it is shown to outperform various baselines across all evaluation metrics."
2020.emnlp-main.284.txt,2020,5 Conclusions,we presented wbr - a novel model for lexical relation classification.
2020.emnlp-main.285.txt,2020,10 Conclusion,ares can couple the information within senseannotated corpora with that automatically created by means of a cluster-based algorithm so as to produce high-quality latent representations for the concepts within a lexical knowledge base.
2020.emnlp-main.285.txt,2020,10 Conclusion,"as future work, we plan to exploit the information brought by our embeddings to other downstream tasks, such as multilingual semantic role labeling (di fabio et al., 2019; conia et al., 2020) and cross-lingual semantic parsing (blloshmi et al., 2020)."
2020.emnlp-main.285.txt,2020,10 Conclusion,"in this paper we presented ares, a semisupervised approach for producing embeddings of senses in english and across different languages."
2020.emnlp-main.285.txt,2020,10 Conclusion,"it achieves state-of-the-art results on both english and multilingual wsd benchmarks, leveraging bert large and mbert, respectively, as underlying pre-trained language models."
2020.emnlp-main.285.txt,2020,10 Conclusion,our embeddings computed with bert large and mbert and the automatically-extracted contexts are available at http://sensembert.org/ares.
2020.emnlp-main.285.txt,2020,10 Conclusion,our experiments showed that despite relying on english data only ares outperforms all its alternatives.
2020.emnlp-main.285.txt,2020,10 Conclusion,we further tested our embeddings in the wic task where they lead a baseline neural model to outperform its closest competitors that rely on larger architectures or dedicated pre-training routines.
2020.emnlp-main.286.txt,2020,6 Conclusions,"besides the ordinary syntactic graph, we employ a lexical graph to capture the global word co-occurrence information in the training corpus."
2020.emnlp-main.286.txt,2020,6 Conclusions,"finally, we design a hieragg module to let the lexical and syntactic graphs work in a cooperative way."
2020.emnlp-main.286.txt,2020,6 Conclusions,"furthermore, we build a concept hierarchy on each of the lexical and syntactic graphs, such that the functionally different types of relations in the graph can be treated separately."
2020.emnlp-main.286.txt,2020,6 Conclusions,"in this paper, we propose a novel framework bigcn to leverage the graph based methods for aspect level sentiment classification tasks."
2020.emnlp-main.286.txt,2020,6 Conclusions,the results prove that our model achieves the state-of-the-art performance.
2020.emnlp-main.286.txt,2020,6 Conclusions,we conduct a set of experiments on five real world datasets.
2020.emnlp-main.287.txt,2020,5 Conclusion,ac-mimlln predicts the sentiment of an aspect category mentioned in a sentence by aggregating the sentiments of the words indicating the aspect category in the sentence.
2020.emnlp-main.287.txt,2020,5 Conclusion,experimental results demonstrate the effectiveness of ac-mimlln.
2020.emnlp-main.287.txt,2020,5 Conclusion,"in some sentences, phrases or clauses rather than words indicate the given aspect category, future work could consider multi-grained instances, including words, phrases and clauses."
2020.emnlp-main.287.txt,2020,5 Conclusion,"in this paper, we propose a multi-instance multilabel learning network for aspect-category sentiment analysis (ac-mimlln)."
2020.emnlp-main.287.txt,2020,5 Conclusion,"since ac-mimlln finds the key instances for the given aspect category and predicts the sentiments of the key instances, it is more interpretable."
2020.emnlp-main.287.txt,2020,5 Conclusion,"since directly finding the key instances for some aspect categories is ineffective, we will try to first recognize all opinion snippets in a sentence, then assign these snippets to the aspect categories mentioned in the sentence."
2020.emnlp-main.288.txt,2020,5 Conclusion,"in particular, with the proposed multi-crf structured attention layer and the effective position decay function, our model is capable of extracting various aspect-specific opinion span features from different representation sub-spaces."
2020.emnlp-main.288.txt,2020,5 Conclusion,"one future direction is to investigate how to integrate the two different attention mechanisms, namely the standard attention and structured attention for nlp applications."
2020.emnlp-main.288.txt,2020,5 Conclusion,the experimental results demonstrate that our method effectively exploits the corresponding opinion features for sentiment classification.
2020.emnlp-main.288.txt,2020,5 Conclusion,we propose a simple and effective mcrf-sa model to extract aspect-specific opinion span features.
2020.emnlp-main.289.txt,2020,4 Conclusion,experimental results demonstrate the effectiveness and robustness of the proposed method on a benchmark dataset.
2020.emnlp-main.289.txt,2020,4 Conclusion,"in the future, we will explore the extension of this approach to achieve full coverage."
2020.emnlp-main.289.txt,2020,4 Conclusion,"in this paper, we consider the emotion-cause pair extraction as a sequence labeling problem and propose an end-to-end model based on a novel tagging scheme with multiple labels."
2020.emnlp-main.289.txt,2020,4 Conclusion,"moreover, the proposed model parses the input texts in order from left to right, greatly reducing the search space, leading to a speed up."
2020.emnlp-main.289.txt,2020,4 Conclusion,"the proposed model is capable of integrating the emotion-cause structure into a unified framework, so that emotions with the related causes can be extracted simultaneously."
2020.emnlp-main.29.txt,2020,8 Discussion and Conclusion,"additionally, some answers might be pragmatically acceptable but semantically wrong."
2020.emnlp-main.29.txt,2020,8 Discussion and Conclusion,"adversarially, we can always construct a query that differs from the gold only under extreme cases and fools our metric."
2020.emnlp-main.29.txt,2020,8 Discussion and Conclusion,"automatic constraint induction from database content and schema descriptions might also be possible, which is on its own an open research problem."
2020.emnlp-main.29.txt,2020,8 Discussion and Conclusion,"beyond semantic evaluation although test suite evaluation provably never creates false negatives in a strict programming language sense, it might still consider “acceptable answers” to be wrong and result in false negatives in a broader sense."
2020.emnlp-main.29.txt,2020,8 Discussion and Conclusion,collecting multiple gold sql query references for evaluation (like machine translation) might be a potential solution.
2020.emnlp-main.29.txt,2020,8 Discussion and Conclusion,"depending on the goal of the evaluation, other metrics such as memory/time efficiency and readability are also desirable and complementary to test suite accuracy."
2020.emnlp-main.29.txt,2020,8 Discussion and Conclusion,"finally, as discussed in section 7, there might be other crucial aspects of a model-predicted query beyond semantic correctness."
2020.emnlp-main.29.txt,2020,8 Discussion and Conclusion,"for example, if a user asks “who is the oldest person?”, the correct answer is a person’s name."
2020.emnlp-main.29.txt,2020,8 Discussion and Conclusion,"for example, in a database of basketball game results, the predicate “a wins” is equivalent to “scorea > scoreb” according to common sense."
2020.emnlp-main.29.txt,2020,8 Discussion and Conclusion,"fortunately, this issue is mitigated by current models."
2020.emnlp-main.29.txt,2020,8 Discussion and Conclusion,"fortunately, we never observe models making such pathological mistakes."
2020.emnlp-main.29.txt,2020,8 Discussion and Conclusion,"however, it also makes sense to return both the name and age columns, with the age column sorted in descending order."
2020.emnlp-main.29.txt,2020,8 Discussion and Conclusion,"however, it is necessary to revisit and verify this hypothesis some time later due to goodhardt’s law, since researchers will optimize over our metric."
2020.emnlp-main.29.txt,2020,8 Discussion and Conclusion,"however, such a relation is not explicitly reflected in the database schema, and our procedure might generate an “unnatural” database where “scorea > scoreb” but not “a wins”, hence distinguishing the model-predicted query from the gold."
2020.emnlp-main.29.txt,2020,8 Discussion and Conclusion,"if “a wins” is mentioned in the text, the model would prefer predicting “a wins” rather than “scorea > scoreb”."
2020.emnlp-main.29.txt,2020,8 Discussion and Conclusion,"nevertheless, to completely solve this issue, we recommend future dataset builders to explicitly define the database generation procedure."
2020.emnlp-main.29.txt,2020,8 Discussion and Conclusion,"our framework for creating test suites is general and only has two requirements: (1) the input is strongly typed so that the fuzzing distribution ig can be defined and the sample input can be meaningfully executed, and (2) there exist neighbor queries ng that are semantically close but different from the gold g. since these two conditions hold in many execution environments, our framework might potentially be applied to other logical forms, such as λ-dcs (liang, 2013), knowledge graphs (lin et al., 2018), and python code snippets (yin et al., 2018; oda et al., 2015) if variable types can future work that evaluates approximate semantic accuracy on the existing benchmarks and formulates new tasks amenable to test suite evaluation."
2020.emnlp-main.29.txt,2020,8 Discussion and Conclusion,our test suites will be released for eleven datasets so that future works can conveniently evaluate test suite accuracy.
2020.emnlp-main.29.txt,2020,8 Discussion and Conclusion,"semantic evaluation via test suites we propose test suite accuracy to approximate the semantic accuracy of a text-to-sql model, by automatically distilling a small test suite with comprehensive code coverage from a large number of random inputs."
2020.emnlp-main.29.txt,2020,8 Discussion and Conclusion,"this metric better reflects semantic accuracy, and we hope that it can inspire novel model designs and training objectives."
2020.emnlp-main.29.txt,2020,8 Discussion and Conclusion,we assure test suite quality by requiring it to distinguish neighbor queries and manually examining its judgments on model-predicted queries.
2020.emnlp-main.29.txt,2020,8 Discussion and Conclusion,we do not attempt to solve sql equivalence testing in general.
2020.emnlp-main.29.txt,2020,8 Discussion and Conclusion,"while our test suite achieves comprehensive code coverage of the gold query, it might not cover all the branches of model-predicted queries."
2020.emnlp-main.290.txt,2020,5 Conclusion,"at the same time, we propose the dual form of cmll, i.e., emll, which uses the cause clauses as the pivot to extract the corresponding emotion clauses."
2020.emnlp-main.290.txt,2020,5 Conclusion,"specifically, we assume that all clauses in the document are emotion clauses, and build an emotion-oriented sliding window centered on each of them."
2020.emnlp-main.290.txt,2020,5 Conclusion,the emotion-cause pair extraction (ecpe) task is a new direction in emotion analysis.
2020.emnlp-main.290.txt,2020,5 Conclusion,the final predictions are obtained by integrating the results of cmll and emll.
2020.emnlp-main.290.txt,2020,5 Conclusion,"then, in each window, we use the emotion clause as a pivot to extract the corresponding one or more cause clauses based on multi-label learning (cmll)."
2020.emnlp-main.290.txt,2020,5 Conclusion,"to overcome the shortcomings of the existing two-step approach, we propose a sliding window multi-label learning scheme."
2020.emnlp-main.290.txt,2020,5 Conclusion,"we evaluated our model on a benchmark emotion cause dataset, and the experimental results show that our method has achieved a substantial improvement over the state-of-the-art method."
2020.emnlp-main.291.txt,2020,6 Conclusion,"furthermore, we would like to investigate other approaches (e.g., graph-based neural network) to better model the modality and label dependence in multi-modal multi-label emotion detection."
2020.emnlp-main.291.txt,2020,6 Conclusion,"in our future work, we will extend our approach to more multi-modal multi-label scenarios, such as intention detection in video conversations and aspect analysis in multi-modal reviews."
2020.emnlp-main.291.txt,2020,6 Conclusion,"in this paper, we propose a multi-modal sequenceto-set approach to simultaneously handle the modality and label dependence in multi-modal multi-label emotion detection."
2020.emnlp-main.291.txt,2020,6 Conclusion,"our approach can not only model the dependence between each label and different modalities, but also model the dependence among multiple labels of a sample."
2020.emnlp-main.291.txt,2020,6 Conclusion,the detailed evaluation demonstrates that our proposed model significantly outperforms several state-ofthe-art baselines.
2020.emnlp-main.292.txt,2020,8 Conclusion,"in this paper, we proposed a simple but effective mechanism to generate test instances to probe the aspect robustness of the models."
2020.emnlp-main.292.txt,2020,8 Conclusion,"using our new test set, we probed the aspect robustness of nine absa models, and discussed model designs and better training that can improve the robustness."
2020.emnlp-main.292.txt,2020,8 Conclusion,we enhanced the original semeval 2014 test sets by 294% and 315% in laptop and restaurant domains.
2020.emnlp-main.293.txt,2020,6 Conclusion,"we propose to use large pre-trained language models to estimate the information amount of given text units, by filtering out the background knowledge as encoded in the large models."
2020.emnlp-main.293.txt,2020,6 Conclusion,"we show that the large pre-trained models can be used as unsupervised methods for content importance estimation, where significant improvement over nontrivial baselines is achieved on both keyphrase extraction and sentence-level extractive summarization tasks."
2020.emnlp-main.294.txt,2020,5 Conclusion,"although our experiments are only on single-document summarization datasets, our method can also be also extended to evaluation of multi-document summarization with slight changes, especially in the part of semantic quality evaluation."
2020.emnlp-main.294.txt,2020,5 Conclusion,"in this paper, we propose a new evaluation method in the field of text summarization."
2020.emnlp-main.294.txt,2020,5 Conclusion,"leveraging powerful representations of bert, our methods achieve the highest performance on two datasets."
2020.emnlp-main.294.txt,2020,5 Conclusion,"since human-authored references used in most of the existing metrics are costly, we investigate automatic evaluation metrics in an unsupervised reference-free setting."
2020.emnlp-main.294.txt,2020,5 Conclusion,we found that the quality of a summary can be evaluated in two separate dimensions: semantic quality and linguistic quality.
2020.emnlp-main.295.txt,2020,6 Conclusion,"as a result, hahsum produces more focused summaries with fewer superfluous and the performance improvements are more pronounced on more extractive datasets."
2020.emnlp-main.295.txt,2020,6 Conclusion,"in this paper, we propose hierarchical attentive heterogeneous graph, aiming to advance text summarization by measuring salience and redundancy simultaneously."
2020.emnlp-main.295.txt,2020,6 Conclusion,our approach model redundancy information by iteratively update the sentence information with message passing in redundancy-aware graph.
2020.emnlp-main.296.txt,2020,6 Conclusions,experimental results across datasets show that the proposed model yields results superior to competitive baselines contributing to summaries which are more relevant and less redundant.
2020.emnlp-main.296.txt,2020,6 Conclusions,"in the future, we would like to generate abstractive summaries following an unsupervised approach (baziotis et al., 2019; chu and liu, 2019) and investigate how recent advances in open domain qa (wang et al., 2019; qi et al., 2019) can be adapted for query focused summarization."
2020.emnlp-main.296.txt,2020,6 Conclusions,"in this work, we proposed a coarse-to-fine estimation framework for query focused multi-document summarization."
2020.emnlp-main.296.txt,2020,6 Conclusions,we explored the potential of leveraging distant supervision signals from question answering to better capture the semantic relations between queries and document segments.
2020.emnlp-main.296.txt,2020,6 Conclusions,"we have also shown that disentangling the tasks of relevance, evidence, and centrality estimation is beneficial allowing us to progressively specialize the summaries to the semantics of the query."
2020.emnlp-main.297.txt,2020,6 Conclusion,a seq2seq model for abstractive document summarization can be pre-trained using such objectives and then fine-tuned on the summarization dataset.
2020.emnlp-main.297.txt,2020,6 Conclusion,all those objectives have relations with abstractive summarization task and are designed based on reinstating the source text.
2020.emnlp-main.297.txt,2020,6 Conclusion,"compared to models pre-training on the even larger corpora (≥160gb), our method, with only 19gb for pre-training, can still achieve comparable and even better performance."
2020.emnlp-main.297.txt,2020,6 Conclusion,"in the future, we would like to investigate other objectives to pre-train seq2seq models for abstractive summarization."
2020.emnlp-main.297.txt,2020,6 Conclusion,"we proposed three sequence-to-sequence pretraining objectives, including sentence reordering, next sentence generation, and masked document generation."
2020.emnlp-main.298.txt,2020,6 Conclusion,"experiments and case studies prove that (i) both context and entity mentions (mainly as type information) provide critical information for relation extraction, and (ii) existing re datasets may leak superficial cues through entity mentions and models may not have the strong abilities to understand context as we expect."
2020.emnlp-main.298.txt,2020,6 Conclusion,"from these points, we propose an entity-masked contrastive pre-training framework for re to better understand textual context and entity types, and experimental results prove the effectiveness of our method."
2020.emnlp-main.298.txt,2020,6 Conclusion,"in the future, we will continue to explore better re pre-training techniques, especially with a focus on open relation extraction and relation discovery."
2020.emnlp-main.298.txt,2020,6 Conclusion,"in this paper, we thoroughly study how textual context and entity mentions affect re models respectively."
2020.emnlp-main.298.txt,2020,6 Conclusion,"these problems require models to encode good relational representation with limited or even zero annotations, and we believe that our pre-trained re models will make a good impact in the area."
2020.emnlp-main.299.txt,2020,5 Conclusions,"comparing with unsupervised models, our model exploits the advantages of supervised models and bootstraps the discriminative power using self-supervised signals via learning improved contextualized relational features."
2020.emnlp-main.299.txt,2020,5 Conclusions,"different from conventional distant-supervised models which require labeled instances for relation extraction in a closed-world setting, our model does not require annotations and is able to work on open-domain scenarios when target relation number and relation distributions are not known in advance."
2020.emnlp-main.299.txt,2020,5 Conclusions,experiments on three real-world datasets show effectiveness and robustness of selfore over competitive baselines.
2020.emnlp-main.299.txt,2020,5 Conclusions,we propose a self-supervised learning model selfore for open-domain relation extraction.
2020.emnlp-main.3.txt,2020,6 Conclusion,"first, we present an automatic method for key point extraction, which is shown to perform on par with a human expert."
2020.emnlp-main.3.txt,2020,6 Conclusion,"furthermore, we show that the necessary knowledge for key point analysis, once acquired by supervised learning from argumentation data, can be successfully applied cross-domain, making it unnecessary to collect domain specific labeled data for each target domain."
2020.emnlp-main.3.txt,2020,6 Conclusion,"in future work, we would like to improve comment matching, e.g., by making it stance-aware."
2020.emnlp-main.3.txt,2020,6 Conclusion,"it provides both textual and quantitative view of the main points in the summarized data, and allows the user to interactively drill down from points to the actual sentences they cover."
2020.emnlp-main.3.txt,2020,6 Conclusion,"key point analysis is a novel framework for summarizing arguments, opinions and views."
2020.emnlp-main.3.txt,2020,6 Conclusion,"previous work only applied key point analysis in the context of argumentation data, and required a domain expert for writing the key points."
2020.emnlp-main.3.txt,2020,6 Conclusion,"second, our work demonstrates the potential of key point analysis in multiple domains besides argumentation."
2020.emnlp-main.3.txt,2020,6 Conclusion,the current work addresses both of the above limitations.
2020.emnlp-main.3.txt,2020,6 Conclusion,we also plan to experiment with sequence-tosequence neural models for generating key point candidates from comments.
2020.emnlp-main.30.txt,2020,5 Conclusion,experiments demonstrate that using cross-thought trained with short sequences can effectively improve sentence embedding.
2020.emnlp-main.30.txt,2020,5 Conclusion,our pre-trained sentence encoder with further finetuning can beat several strong baselines on many nlp tasks.
2020.emnlp-main.30.txt,2020,5 Conclusion,"we propose a novel approach, cross-thought, to pre-train sentence encoder."
2020.emnlp-main.300.txt,2020,5 Conclusion,experiment results verify the effectiveness of our model.
2020.emnlp-main.300.txt,2020,5 Conclusion,"in the future, we will explore how to improve the efficiency of our pre-training."
2020.emnlp-main.300.txt,2020,5 Conclusion,"in this work, we propose to denoise distantly supervised data in docre by multiple pre-training tasks."
2020.emnlp-main.301.txt,2020,7 Conclusion,and we call for a unified end-to-end re evaluation setting to prevent future mistakes and enable more meaningful cross-domain comparisons.
2020.emnlp-main.301.txt,2020,7 Conclusion,"furthermore, this fragmentation of the community complicates the emergence of new proposals."
2020.emnlp-main.301.txt,2020,7 Conclusion,"indeed, in this confusion, numerous articles present unfair comparisons, often overestimating the performance of their proposed model."
2020.emnlp-main.301.txt,2020,7 Conclusion,"our critical literature review epitomizes the need for more rigorous reports of evaluation settings, including detailed datasets statistics."
2020.emnlp-main.301.txt,2020,7 Conclusion,the multiplication of settings in the evaluation of end-to-end relation extraction makes the comparison to previous work difficult.
2020.emnlp-main.302.txt,2020,7 Discussion and Conclusion,"continuing gardner et al.(2020), we conclude that challenge sets are an effective tool of benchmarking against shallow heuristics, not only of models and systems, but also of data collection methodologies."
2020.emnlp-main.302.txt,2020,7 Discussion and Conclusion,"ideally, the same should apply also to training sets."
2020.emnlp-main.302.txt,2020,7 Discussion and Conclusion,"if impractical, the data should at least attempt to exhaustively annotate confusion-sets: if a certain entity-pair is annotated in a sentence, all other pairs of the same entity-types in the sentence should also be annotated."
2020.emnlp-main.302.txt,2020,7 Discussion and Conclusion,qa-trained models are less susceptible to this behavior.
2020.emnlp-main.302.txt,2020,7 Discussion and Conclusion,we created a challenge dataset demonstrating the tendency of tacred-trained models to classify using an event+type heuristic that fails to connect the relation and its arguments.
2020.emnlp-main.302.txt,2020,7 Discussion and Conclusion,"we suggest the following recommendation for future re data collection: evaluation sets should be exhaustive, and contain all relevant entity pairs."
2020.emnlp-main.303.txt,2020,5 Conclusion,"entity global representations model the semantic information of an entire document with r-gcn, and entity local representations aggregate the contextual information of mentions selectively using multi-head attention."
2020.emnlp-main.303.txt,2020,5 Conclusion,"in future work, we plan to integrate knowledge graphs and explore other document graph modeling ways (e.g., hierarchical graphs) to improve the performance."
2020.emnlp-main.303.txt,2020,5 Conclusion,"in this paper, we proposed glre, a global-to-local neural network for document-level re."
2020.emnlp-main.303.txt,2020,5 Conclusion,"moreover, context relation representations encode the topic information of other relations using self-attention."
2020.emnlp-main.303.txt,2020,5 Conclusion,"our experiments demonstrated the superiority of glre over many comparative models, especially the big leads in extracting relations between entities of long distance and with multiple mentions."
2020.emnlp-main.304.txt,2020,6 Conclusion,"as future work, we intend to explore its application in those fields."
2020.emnlp-main.304.txt,2020,6 Conclusion,"in this paper, we tackle the weakness of existing mtl-based methods proposed for the joint extraction of entities and relation in unstructured text."
2020.emnlp-main.304.txt,2020,6 Conclusion,"instead, we show that dynamically learning the interactions between the tasks may capture complex correlations which improves the taskspecific feature for classification."
2020.emnlp-main.304.txt,2020,6 Conclusion,our experiments on benchmark datasets validates clear advantage over the existing proposed methods.
2020.emnlp-main.304.txt,2020,6 Conclusion,"specifically, these methods assume that a shared network is sufficient to capture the correlations between the entity recognition task and the relation classification task, and that the shared features derived from this network can be passed into models for the task-specific tasks to make predictions independently."
2020.emnlp-main.304.txt,2020,6 Conclusion,"we note that our model can be adapted for other nlp tasks, including aspect level sentiment classification and slot filling."
2020.emnlp-main.304.txt,2020,6 Conclusion,we proposed multi-task learning model which allows explicit interactions to be dynamically learned among the subtasks.
2020.emnlp-main.305.txt,2020,7 Conclusion,"it also learns soft temporal consistency constraints, which allow knowledge of one temporal fact to influence belief in another fact."
2020.emnlp-main.305.txt,2020,7 Conclusion,"time embeddings are temporally meaningful, and timeplex makes fewer temporal consistency and ordering mistakes."
2020.emnlp-main.305.txt,2020,7 Conclusion,timeplex exceeds the performance of existing tkbc systems.
2020.emnlp-main.305.txt,2020,7 Conclusion,"we also argue that current evaluation schemes for both link and time prediction have limitations, and propose more meaningful schemes."
2020.emnlp-main.305.txt,2020,7 Conclusion,"we presented timeplex, a new tkbc framework, which combines representations of time with representations of entities and relations."
2020.emnlp-main.306.txt,2020,9 Conclusion,"openie6 is 10x faster, handles conjunctive sentences and establishes a new state of art for openie."
2020.emnlp-main.306.txt,2020,9 Conclusion,openie6 is available at https://github.com/dair-iitd/ openie6 for further research.
2020.emnlp-main.306.txt,2020,9 Conclusion,"using the same architecture, we achieve a new state of the art for coordination parsing, with a 12.3 pts improvement in f1 over previous analyzers."
2020.emnlp-main.306.txt,2020,9 Conclusion,we highlight the role of constraints in training for openie.
2020.emnlp-main.306.txt,2020,9 Conclusion,we plan to explore the utility of this architecture in other nlp problems.
2020.emnlp-main.306.txt,2020,9 Conclusion,"we propose a new openie system – openie6, based on the novel iterative grid labeling architecture, which models sequence labeling tasks with overlapping spans as a 2-d grid labeling problem."
2020.emnlp-main.307.txt,2020,4 Conclusions,"different from existing classifier and threshold based models, the proposed method directly measures the distribution differences and thus is more effective."
2020.emnlp-main.307.txt,2020,4 Conclusions,"finally, extensive experimental results demonstrate that hvae performs significantly better than three state-of-the-art techniques across two benchmark datasets, indicating that it can be effectively used for real-world public sentiment drift analysis."
2020.emnlp-main.307.txt,2020,4 Conclusions,"in addition, a new drift measure is designed to effectively measure distribution difference between historical and newly arrived data."
2020.emnlp-main.307.txt,2020,4 Conclusions,"to tackle challenges in sentiment drift detection, we have proposed a novel hvae model, which is 3-level meta-distributions to extend vae over hierarchical structure, leading to effective learning latent distribution representations of input sentiments."
2020.emnlp-main.308.txt,2020,6 Conclusion,"as future work, we plan to generalize the ept to other datasets, including non-english word problems or non-algebraic domains in math, to extend our model."
2020.emnlp-main.308.txt,2020,6 Conclusion,"in addition, the ept resolves the operand-context separation issue by applying operand-context pointers."
2020.emnlp-main.308.txt,2020,6 Conclusion,"in this study, we proposed a neural algebraic word problem solver, expression-pointer transformer (ept), and examined its characteristics."
2020.emnlp-main.308.txt,2020,6 Conclusion,our work is meaningful in that we demonstrated a possibility for alleviating the costly procedure of devising hand-crafted features in the domain of solving algebraic word problems.
2020.emnlp-main.308.txt,2020,6 Conclusion,"the ept resolves the expression fragmentation issue by generating ‘expression’ tokens, which simultaneously generate an operator and required operands."
2020.emnlp-main.308.txt,2020,6 Conclusion,we designed ept to address two issues: expression fragmentation and operand-context separation.
2020.emnlp-main.309.txt,2020,6 Conclusion,"besides, we also propose a subtree-level semantically-aligned regularization to improve subtree semantic representation."
2020.emnlp-main.309.txt,2020,6 Conclusion,experimental results show the superiority of our approach.
2020.emnlp-main.309.txt,2020,6 Conclusion,"finally, we introduce a new mwps datasets, called hmwp, to validate our solver’s universality and push the research boundary of mwps to math real-world applications better."
2020.emnlp-main.309.txt,2020,6 Conclusion,"we propose an sau-solver, which is able to solve multiple types of mwps, to generate the universal express tree explicitly in a semantically-aligned manner."
2020.emnlp-main.31.txt,2020,7 Discussion,autoqa relies on a neural paraphraser trained with an out-of-domain dataset to generate training data.
2020.emnlp-main.31.txt,2020,7 Discussion,"even for common domains, autoqa sometimes failed to generate some common phrases."
2020.emnlp-main.31.txt,2020,7 Discussion,further improvement on neural paraphraser is needed to generate more diverse outputs.
2020.emnlp-main.31.txt,2020,7 Discussion,"future work is also needed to handle attributes containing long free-form text, as autoqa currently only supports database operations without reading comprehension."
2020.emnlp-main.31.txt,2020,7 Discussion,"in this work, we propose autoqa, a methodology and a toolkit to automatically create a semantic parser given a database."
2020.emnlp-main.31.txt,2020,7 Discussion,"on both datasets, autoqa achieves comparable accuracy to state-ofthe-art qa systems trained with manual attribute annotation and human paraphrases."
2020.emnlp-main.31.txt,2020,7 Discussion,we suspect the methodology to be less effective for domains full of jargon.
2020.emnlp-main.31.txt,2020,7 Discussion,we test autoqa on two different datasets with different target logical forms and data synthesis templates.
2020.emnlp-main.310.txt,2020,4 Conclusion,both quantitative and qualitative results are presented in the experiments to demonstrate the effectiveness of the proposed approach.
2020.emnlp-main.310.txt,2020,4 Conclusion,"in the future, we would like to extend gtm to corpora with explicit doc-doc interactions, e.g., scientific documents with citations or social media posts with user relationships."
2020.emnlp-main.310.txt,2020,4 Conclusion,replacing gcn in gtm with more advanced graph neural networks is another promising research direction.
2020.emnlp-main.310.txt,2020,4 Conclusion,"we have introduced graph topic model, a neural topic model that incorporates corpus-level neighboring context using graph convolutions to enrich document representations and facilitate the topic inference."
2020.emnlp-main.311.txt,2020,5 Conclusion and Future Work,"extensive experiments shows that the generated recipes are not only fluent and feasible, but also creative."
2020.emnlp-main.311.txt,2020,5 Conclusion and Future Work,"for example, the clustering ability of routing algorithm can be used to control the style of generated texts."
2020.emnlp-main.311.txt,2020,5 Conclusion and Future Work,"given ingredients with noises, our model selects reasonable ingredient collocations and generates recipes based on user demands."
2020.emnlp-main.311.txt,2020,5 Conclusion and Future Work,"in this paper, we make an effort on introducing routing algorithm to enforce the recipe generation model."
2020.emnlp-main.311.txt,2020,5 Conclusion and Future Work,there are several directions to explore in the future.
2020.emnlp-main.311.txt,2020,5 Conclusion and Future Work,we model the internal relationships between ingredients by selective routing algorithm.
2020.emnlp-main.312.txt,2020,6 Conclusion,"context modeling in the learnerlike agent relies upon additional perturbed examples to mimic human behavior, whereas emla already has this ability."
2020.emnlp-main.312.txt,2020,6 Conclusion,"entailment modeling, unlike common context-based nearsynonymous word disambiguation, makes inferences to learn the relationship between the example sentences and the question, similar to human behavior."
2020.emnlp-main.312.txt,2020,6 Conclusion,"in the future, we would like to explore if the learner-like agent can be extended to materials and data beyond the example sentences for near-synonyms."
2020.emnlp-main.312.txt,2020,6 Conclusion,"the agent can be used to evaluate the helpfulness of learning materials, or—more interestingly—to select the best materials from a large candidate pool."
2020.emnlp-main.312.txt,2020,6 Conclusion,these demonstrate the usefulness of modeling learner behavior using an inference approach.
2020.emnlp-main.312.txt,2020,6 Conclusion,"using the emla learner-like agent, we find more helpful learning material for learners, as demonstrated by the learner study."
2020.emnlp-main.312.txt,2020,6 Conclusion,"we introduce the learner-like agent, in particular emla, which differentiates the helpfulness of learning materials using inference."
2020.emnlp-main.312.txt,2020,6 Conclusion,"we select good example sentences in practice, which confirms the usefulness of modeling learner behavior."
2020.emnlp-main.313.txt,2020,7 Conclusion and Future Work,"after stitching the generated copywriting together, we can obtain a well-formed multi-product ad post."
2020.emnlp-main.313.txt,2020,7 Conclusion and Future Work,"due to the characteristics of such a new task, most existing classical text generation schemes do not work well."
2020.emnlp-main.313.txt,2020,7 Conclusion and Future Work,experiments conducted a large-scale real-world product description dataset demonstrate that our proposed model achieves promising and impressive performance.
2020.emnlp-main.313.txt,2020,7 Conclusion and Future Work,"in this paper, we explore the multi-product ad post generation which is meaningful for both academic researches and industrial applications."
2020.emnlp-main.313.txt,2020,7 Conclusion and Future Work,"moreover, we will conduct further exploration of the multiproduct ad post form, including more vivid multimedia information, such as pictures and videos."
2020.emnlp-main.313.txt,2020,7 Conclusion and Future Work,"owing to the promising results of the proposed two-stage model, we could degenerate this process into more specific sub-processes."
2020.emnlp-main.313.txt,2020,7 Conclusion and Future Work,"to fill this gap, we propose an end-to-end s-mg net model that uses the selectnet to select the combination of products associated with each other and uses the mgennet to generate the copywriting for each product."
2020.emnlp-main.314.txt,2020,5 Conclusion,further we also perform table structure recognition (grouping texts present in a table into rows and columns) achieving state-of-the-art results.
2020.emnlp-main.314.txt,2020,5 Conclusion,our proposed model uses only lower level elements - textblocks & widgets without using visual modality.
2020.emnlp-main.314.txt,2020,5 Conclusion,we are also releasing a part of our forms dataset to aid further research in this direction.
2020.emnlp-main.314.txt,2020,5 Conclusion,we discuss two tasks - element type classification and grouping into larger constructs.
2020.emnlp-main.314.txt,2020,5 Conclusion,we establish improvement in performance through text info and joint training of two tasks.
2020.emnlp-main.314.txt,2020,5 Conclusion,we present an nlp based form2seq framework for form document structure extraction.
2020.emnlp-main.314.txt,2020,5 Conclusion,we show that our model performs better compared to current semantic segmentation approaches.
2020.emnlp-main.315.txt,2020,4 Conclusion,"for thai word segmentation, the results showed that our method is an effective domain adaptation method and has similar performance as the transfer learning method."
2020.emnlp-main.315.txt,2020,4 Conclusion,the results from japanese and chinese word segmentation experiments showed that our method could improve the performance of japanese and chinese black-box models.
2020.emnlp-main.315.txt,2020,4 Conclusion,we conducted extensive experimental studies using nine benchmark corpora from three languages.
2020.emnlp-main.315.txt,2020,4 Conclusion,we proposed a novel solution for adapting a blackbox model to a new domain by formulating it as an ensemble learning problem.
2020.emnlp-main.316.txt,2020,7 Conclusion,dagobert’s errors are mainly due to syntactic and semantic overlap between affixes.
2020.emnlp-main.316.txt,2020,7 Conclusion,"furthermore, we demonstrate that the input segmentation impacts how much derivational knowledge is available to bert."
2020.emnlp-main.316.txt,2020,7 Conclusion,"our best model, dagobert, clearly beats an lstm-based model, the previous state of the art in dg."
2020.emnlp-main.316.txt,2020,7 Conclusion,this suggests that the performance of plms could be further improved if a morphologically informed vocabulary of units were used.
2020.emnlp-main.316.txt,2020,7 Conclusion,"we show that a plm, specifically bert, can generate derivationally complex words."
2020.emnlp-main.317.txt,2020,5 Conclusion,"for chinese word segmentation, upholding the belief that a better representation is all we need and thus taking a greedy decoder for fast segmentation as the basis, we only focus on the encoder design and propose an attention mechanism only based cws model."
2020.emnlp-main.317.txt,2020,5 Conclusion,"our model is evaluated on standard benchmark sighan bakeoff datasets, which shows not only our model performs segmentation faster than any previous models but also gives new higher or comparable segmentation performance against previous state-of-the-art models."
2020.emnlp-main.317.txt,2020,5 Conclusion,our model uses the proposed gdtransformer encoder to take sequence input and biaffine attention scorer to directly predict the word boundaries.
2020.emnlp-main.317.txt,2020,5 Conclusion,"to improve the ability of capturing the localness and directional information, gaussianmasked directional multi-head attention in the gdtransformer replaces the standard self-attention in the original transformer."
2020.emnlp-main.317.txt,2020,5 Conclusion,"with powerful enough encoding ability, our model only needs unigram features for scoring instead of various n-gram features in previous work."
2020.emnlp-main.318.txt,2020,5 Conclusion,extensive experiments on seven in-domain and four cross-domain datasets for chinese word segmentation confirm the superiority of our model over all other advanced methods.
2020.emnlp-main.318.txt,2020,5 Conclusion,"first, the model has a stronger robustness with a straightforward transfer learning method."
2020.emnlp-main.318.txt,2020,5 Conclusion,"in particular, we will enhance the practicability of chinese word segmentation to improve the effectiveness of other downstream chinese nlp tasks."
2020.emnlp-main.318.txt,2020,5 Conclusion,"in summary, the advantages of our model are twofold."
2020.emnlp-main.318.txt,2020,5 Conclusion,"in the future, we will continue studying the efficiency of the neural architecture, and pay attention to improving the speed of both training and testing steps on an ever-increasing dataset."
2020.emnlp-main.318.txt,2020,5 Conclusion,"in this paper, we construct a transfer layer structure that leverages the pre-trained feature information for cws and exploit a transfer tag to boost joint multiple criteria learning."
2020.emnlp-main.318.txt,2020,5 Conclusion,"nevertheless, there is still a gap in a real-word situation."
2020.emnlp-main.318.txt,2020,5 Conclusion,"second, our model effectively solves the parameters exploding due to different segmentation criteria."
2020.emnlp-main.318.txt,2020,5 Conclusion,the model could relieve the oov problem for chinese word segmentation and achieves the best performance in comparison with state-of-the-art techniques for both in-domain and out-of-domain chinese word segmentation.
2020.emnlp-main.318.txt,2020,5 Conclusion,"the performance of our model is better, especially when dealing with high oov rate data."
2020.emnlp-main.318.txt,2020,5 Conclusion,we do not need to design any redundant structures.
2020.emnlp-main.319.txt,2020,5 Conclusions,"although our focus has been on dependency-based srl, our model can be easily adapted to span-based annotations (carreras and marquez ` , 2005; pradhan et al., 2013)."
2020.emnlp-main.319.txt,2020,5 Conclusions,"in this case, the semantic role compressor could be modified to represent entire spans rather than just head words while decompression would remain unchanged (it would still output a probability distribution for each word over all semantic roles)."
2020.emnlp-main.319.txt,2020,5 Conclusions,in this paper we developed a cross-lingual srl model and demonstrated it can effectively leverage unlabeled parallel data without relying on word alignments or any other external tools.
2020.emnlp-main.319.txt,2020,5 Conclusions,"we also plan to extend our framework to semi-supervised learning, where a small number of annotations might also be available in the target language."
2020.emnlp-main.319.txt,2020,5 Conclusions,we have also contributed two quality controlled datasets (compatible with propbank-style guidelines) which we hope will be useful for the development of crosslingual models.
2020.emnlp-main.32.txt,2020,5 Conclusion,extending our method to an abstractive setting is meaningful future work.
2020.emnlp-main.32.txt,2020,5 Conclusion,various mds tasks of different sizes and domains show a promising result of our method.
2020.emnlp-main.32.txt,2020,5 Conclusion,"we propose a novel hypothesis-driven method for unsupervised mds, where the goodness of any summary candidate can be determined from a spectral perspective when dropping it from the document cluster."
2020.emnlp-main.320.txt,2020,6 Conclusion,extensive experiments on the ptc dataset demonstrate that our knowledge-augmented method achieves superior performance with more consistent between sentence-level and token-level predictions.
2020.emnlp-main.320.txt,2020,6 Conclusion,"in this paper, we propose a fine-grained multitask learning approach, which leverages declarative knowledge to detect propaganda techniques in news articles."
2020.emnlp-main.320.txt,2020,6 Conclusion,"specifically, the declarative knowledge is expressed in both first-order logic and natural language, which are used as regularizers to obtain better propaganda representations and improve logical consistency between coarse- and finegrained predictions, respectively."
2020.emnlp-main.321.txt,2020,5 Conclusions,"finally, we included an extrinsic evaluation where we train srl models using our data and obtain consistent results that showcase the generalization capacities emerging from our new 4-way multilingual dataset."
2020.emnlp-main.321.txt,2020,5 Conclusions,future work should address the application of our method to more and typologically more divergent languages.
2020.emnlp-main.321.txt,2020,5 Conclusions,"in this paper, we present the first fully parallel srl dataset with homogeneous annotations for four different languages."
2020.emnlp-main.321.txt,2020,5 Conclusions,we included human-validated test sets where we address the linguistic difficulties that emerge when transferring labels across languages – despite transferring gold labels from the source.
2020.emnlp-main.321.txt,2020,5 Conclusions,we introduce and evaluate a novel effective and portable automatic method to transfer srl labels that relies on the robustness of machine translation and multilingual bert and therefore could be straightforwardly applied to produce srl data in other languages.
2020.emnlp-main.322.txt,2020,7 Conclusions,"by comparing to dependency gcn, we observed that for srl constituent structures yield more informative features that the dependency ones."
2020.emnlp-main.322.txt,2020,7 Conclusions,"given that gcns over dependency and constituency structure have access to very different information, it would be interesting to see in future work if combining two types of representations can lead to further improvements."
2020.emnlp-main.322.txt,2020,7 Conclusions,"in this paper, we introduced spangcn, a novel neural architecture for encoding constituency syntax at the word level."
2020.emnlp-main.322.txt,2020,7 Conclusions,"we applied spangcn to srl, on propbank and framenet."
2020.emnlp-main.322.txt,2020,7 Conclusions,"we observed substantial improvements from using constituent syntax on both datasets, and also in the realistic out-of-domain setting."
2020.emnlp-main.322.txt,2020,7 Conclusions,"while we experimented only with constituency syntax, spangcn may be able to encode any kind of span structure, for example, co-reference graphs, and can be used to produce linguistically-informed encoders for other nlp tasks rather than only srl."
2020.emnlp-main.323.txt,2020,7 Conclusion,"in contrast, the parsing speed with the transition systems is less sensitive to the graphbank and faster overall."
2020.emnlp-main.323.txt,2020,7 Conclusion,"in future work, one could make the a* parser more accurate by extending it to non-projective dependency trees, especially on dm, eds and amr."
2020.emnlp-main.323.txt,2020,7 Conclusion,it would also be interesting to see if our method for avoiding dead ends can be applied to other formalisms with complex symbolic restrictions.
2020.emnlp-main.323.txt,2020,7 Conclusion,the parsing speed of the a* parser differs dramatically for the different graphbanks.
2020.emnlp-main.323.txt,2020,7 Conclusion,the transition systems also achieve higher accuracy.
2020.emnlp-main.323.txt,2020,7 Conclusion,"the transition-based parser could be made more accurate by making bottom-up information available to its top-down choices, e.g.with cai and lam’s (2020) “iterative inference” method."
2020.emnlp-main.323.txt,2020,7 Conclusion,"we have presented two fast and accurate algorithms for am dependency parsing: an a* parser which optimizes groschwitz et al.’s projective parser, and a novel transition-based parser which builds an am dependency top-down while avoiding dead ends."
2020.emnlp-main.324.txt,2020,5 Final Remarks,"by connecting the intents to lower-level entities, i.e.the words associated to the intents, and therefore establishing inter-connections between the classes, the word graph space enriches the traditional representation of classes by means of classifier parameters which are learned solely from input examples."
2020.emnlp-main.324.txt,2020,5 Final Remarks,"in addition, the proposed word graph method can be improved by exploiting combinations of the proposed expanded class representation with the traditional softmaxbased method, what may also provide better accuracy for in-scope samples in some situations."
2020.emnlp-main.324.txt,2020,5 Final Remarks,"in image classification problems, for instance, word graphs related to visual words could be computed."
2020.emnlp-main.324.txt,2020,5 Final Remarks,"in our view, the improved results are due to a better representation of the higher-level concepts associated to the classes."
2020.emnlp-main.324.txt,2020,5 Final Remarks,"in the publicly-available larson dataset, the proposed approach beats the previouslypublished results by a high margin, and particularly enhancing the false acceptance rate (far) from 41.1% to 9.9%."
2020.emnlp-main.324.txt,2020,5 Final Remarks,"in this paper we propose the use of information from word graphs to enhance intent classification, more specifically, for the detection of out-of-scope examples."
2020.emnlp-main.324.txt,2020,5 Final Remarks,"instead of working on the representation of the input text, we enhance the representation of the outputs, i.e.how classes and their corresponding labels are represented."
2020.emnlp-main.324.txt,2020,5 Final Remarks,the results demonstrate the approach has a considerable positive impact for the detection of out-of-scope examples when an appropriate sentence embedding such as lstm and bert is used.
2020.emnlp-main.324.txt,2020,5 Final Remarks,"we believe that the approach is general enough to be applied to others areas and presents ideas to develop more accurate classifiers in general, across multiple areas, particularly in contexts where outof-scope samples are common."
2020.emnlp-main.325.txt,2020,Conclusion,"s2p experiences a late-stage collapse on the grounding score, whereas sil has a higher negative likelihood on human corpus."
2020.emnlp-main.325.txt,2020,Conclusion,we further show the correlation between s2p late-stage collapse and conflicting gradients.
2020.emnlp-main.325.txt,2020,Conclusion,we introduce ssil to combine these two methods effectively.
2020.emnlp-main.325.txt,2020,Conclusion,we investigate two general methods to counter language drift: s2p and sil.
2020.emnlp-main.326.txt,2020,6 Conclusion,a team of researchers who would like to benchmark their system against four competing chatbots could do that for the cost of fewer than 3 hours of crowdsourced annotations.
2020.emnlp-main.326.txt,2020,6 Conclusion,"in this work, we introduced spot the bot, a robust and time-efficient approach for evaluating conversational dialogue systems."
2020.emnlp-main.326.txt,2020,6 Conclusion,it is based on conversations between bots rated by humans with respect to the bots’ ability to mimic human behavior.
2020.emnlp-main.326.txt,2020,6 Conclusion,"spot the bot facilitates developers making real progress based on frequent manual evaluations data, avoiding the use of noisy automatic metrics or once-in-a-year costly manual evaluations."
2020.emnlp-main.326.txt,2020,6 Conclusion,we make the framework as well as the data publicly available.
2020.emnlp-main.326.txt,2020,6 Conclusion,we show that spot the bot yields robust and significant results while reducing the evaluation time compared to other evaluation frameworks.
2020.emnlp-main.327.txt,2020,7 Discussion,a limitation of our study is that the question of what to optimize with rl to improve overall qualitative ratings remains open.
2020.emnlp-main.327.txt,2020,7 Discussion,"compared to prior work in offline rl, the novel wop offline rl algorithm achieves higher performance in traditional rl tasks, elicits more positive feedback in conversations with novel humans at test time, and earns overall higher human ratings."
2020.emnlp-main.327.txt,2020,7 Discussion,"considering the broader impacts of our work, a representative and diverse set of conversations and annotations should be collected before real world systems are trained and deployed using our algorithms."
2020.emnlp-main.327.txt,2020,7 Discussion,"for many practical applications, we may have specific requirements for the language generated by a model—for example, that it is appropriate, positive, and polite—even if this leads to a lower perception of conversation quality for some users."
2020.emnlp-main.327.txt,2020,7 Discussion,"further, rl currently remains the only option for maximizing user feedback over the course of a conversation."
2020.emnlp-main.327.txt,2020,7 Discussion,future work should investigate more rewards for training an open-domain dialog model such as long term conversation rewards that may need to be computed over many conversation turns.
2020.emnlp-main.327.txt,2020,7 Discussion,"however, our reward set proved insufficient to achieve higher human quality ratings, at least with the limited offline training data we were able to collect."
2020.emnlp-main.327.txt,2020,7 Discussion,"in this work, we present novel techniques that enable successful offline reinforcement learning on any base language model from real human conversations."
2020.emnlp-main.327.txt,2020,7 Discussion,it is unlikely the rewards proposed here fully cover what it means to have a high quality openended conversation.
2020.emnlp-main.327.txt,2020,7 Discussion,our work computes conversational rewards based on dialog data and annotations from online task workers in the united states.
2020.emnlp-main.327.txt,2020,7 Discussion,"this allows the dialog systems practitioner to train models that learn language structure from vast, readily-available corpora, then fine-tune for specific desirable behaviors post-hoc through rl rewards."
2020.emnlp-main.327.txt,2020,7 Discussion,"we have shown that manual ratings are too sparse to optimize effectively, and instead suggest using implicit rewards."
2020.emnlp-main.327.txt,2020,7 Discussion,we have shown that the proposed techniques can be useful for shaping dialog model behavior towards a desired objective.
2020.emnlp-main.327.txt,2020,7 Discussion,we have shown that the way off-policy algorithm provides a more effective way to teach a language model specific behaviors from offline data than previously proposed rl or regularization techniques.
2020.emnlp-main.327.txt,2020,7 Discussion,we observe that the new offline rl method successfully optimizes both generated bot rewards and elicited human responses.
2020.emnlp-main.327.txt,2020,7 Discussion,we show that it presents a better option than using regularization in training a specific bot behavior.
2020.emnlp-main.328.txt,2020,9 Conclusion,"in this paper we hypothesised that, were a language economical in its expressions and clear, then the contextual uncertainty of a word should negatively correlate with its lexical ambiguity—suggesting speakers compensate for lexical ambiguity by making contexts more informative."
2020.emnlp-main.328.txt,2020,9 Conclusion,"to investigate this, we proposed an information-theoretic operationalisation of lexical ambiguity, together with two methods of approximating it, one using wordnet and one using bert."
2020.emnlp-main.328.txt,2020,9 Conclusion,"we discuss the relative advantages of each, and provide experiments using both."
2020.emnlp-main.328.txt,2020,9 Conclusion,"with our bert approximation, we then expanded our analysis to a larger set of 18 typologically diverse languages and found significant negative correlations between lexical ambiguity and contextual uncertainty in all of them, further supporting our hypothesis that contextual uncertainty negatively correlates with lexical ambiguity."
2020.emnlp-main.328.txt,2020,9 Conclusion,"with our wordnet approximation, we found significant negative correlations between lexical ambiguity and contextual uncertainty in five out of six high-resource languages analysed, supporting our hypothesis in this restricted setting."
2020.emnlp-main.329.txt,2020,8 Conclusion,"french, spanish), despite having only trained on languages predominantly featuring prenominal ones (czech, english, german, russian), by simply reversing the prenominal adjective ordering rules for postnominal ones."
2020.emnlp-main.329.txt,2020,8 Conclusion,"in summary, our work presents converging evidence that adjectives exhibit universal hierarchical ordering tendencies, with the added observations that individual languages feature additional unique tendencies not shared by others, and that adjective ordering is symmetric with respect to the noun."
2020.emnlp-main.329.txt,2020,8 Conclusion,"interestingly, we were able to achieve high predictive accuracy on languages predominantly featuring postnominal adjectives (e.g."
2020.emnlp-main.329.txt,2020,8 Conclusion,"we also found that our model is able to accurately order adjectives from 24 different languages, regardless of whether it was directly trained on them, although it does benefit from having been trained on the language on which it is tested."
2020.emnlp-main.329.txt,2020,8 Conclusion,"we built an interpretable, multi-lingual latentvariable model of hierarchical adjective ordering that directly enforces a hierarchy of semantic classes and is trained entirely using corpus data."
2020.emnlp-main.329.txt,2020,8 Conclusion,"we found that our fixed-w variants, which enforce total orderings of semantic classes, perform similarly to our learned-w variants, suggesting that adjective ordering preferences naturally tend towards total orderings."
2020.emnlp-main.33.txt,2020,7 Conclusion,"our observations suggest that extractive summarizers generally outperform abstractive summarizers by human evaluation, and more details are also found about the unique advantages gained by copy, coverage, hybrid and especially pre-training technologies."
2020.emnlp-main.33.txt,2020,7 Conclusion,"the overall conclusions are largely in line with existing research, while we provide more details in an error diagnostics aspect."
2020.emnlp-main.33.txt,2020,7 Conclusion,"we empirically compared 10 representative text summarizers using a fine-grained set of human evaluation metrics designed according to mqm for human writing, aiming to achieve a better understanding on neural text summarization systems and the effect of milestone techniques investigated recently."
2020.emnlp-main.330.txt,2020,6 Conclusion,a corpus of text containing chinese-english conversations was collected and its code-switched phrases were translated back into their context language.
2020.emnlp-main.330.txt,2020,6 Conclusion,"since surprisal has been associated with cognitive effort during language production (e.g., kello and plaut, 2000) and comprehension (hale, 2001), we can relate cs to situations in which the speaker faces difficulties; and/or similar to mysl´ın and levy (2015), situations where the speaker uses cs as a strategy to emphasize highly informative content to the comprehender in order to facilitate communication."
2020.emnlp-main.330.txt,2020,6 Conclusion,"the first one is the finding that cs in written language is reliably affected by sentence length, word length, arguably word frequency, and, most importantly word surprisal."
2020.emnlp-main.330.txt,2020,6 Conclusion,"the second contribution is a new chinese-english cs dataset, which includes translations to the dominant language, which we hope will be used in further models of cs."
2020.emnlp-main.330.txt,2020,6 Conclusion,"the translations of the code-switched sentences were compared to sentences with similar syntactic structure but without code-switches, in order to see what factors affected the propensity to cs.surprisal predicts cs."
2020.emnlp-main.330.txt,2020,6 Conclusion,"this may be due to the formulation of entropy that we used, which does not consider the semantics the speaker tries to convey."
2020.emnlp-main.330.txt,2020,6 Conclusion,this paper makes two specific contributions.
2020.emnlp-main.330.txt,2020,6 Conclusion,"we found no evidence showing that entropy, as opposed to surprisal, predicts cs."
2020.emnlp-main.330.txt,2020,6 Conclusion,we investigated the effect of word surprisal and word entropy on the probability of code-switching (cs) in chinese-english written communication.
2020.emnlp-main.331.txt,2020,6 Discussion,"a potential explanation of the results is that they are caused by catastrophic forgetting (ratcliff, 1990; french, 1999): although a sufficient number of fsl samples are observed for a noun, these samples are forgotten during training, causing the performance of the noun to degrade.this explanation is implausible."
2020.emnlp-main.331.txt,2020,6 Discussion,"if catastrophic forgetting is occurring, then the problem should be more severe for infrequent nouns than for frequent nouns, as the interval between training samples will be longer for infrequent nouns."
2020.emnlp-main.331.txt,2020,6 Discussion,"if number agreement can be correctly learned from a few samples (fsl samples), then one would expect model performance to either a) improve with more data, as more fsl samples are observed, or b) improve with more data up to some threshold, and then asymptote after learning has saturated."
2020.emnlp-main.331.txt,2020,6 Discussion,"in either case, for high frequency nouns, a sufficient number of fsl samples should be observed for these nouns to be learned very accurately."
2020.emnlp-main.331.txt,2020,6 Discussion,it is relatively easy to learn the number agreement properties of a noun.
2020.emnlp-main.331.txt,2020,6 Discussion,many nouns that occur with high frequency are not learned accurately.
2020.emnlp-main.331.txt,2020,6 Discussion,"moreover, when one language model exhibits this knowledge, other language models are more likely to as well."
2020.emnlp-main.331.txt,2020,6 Discussion,nouns that occur more frequently during training are not learned more accurately.
2020.emnlp-main.331.txt,2020,6 Discussion,subsequent analyses demonstrate a pair of empirical phenomena: 1.
2020.emnlp-main.331.txt,2020,6 Discussion,"the models learn the agreement properties of a novel noun from just a few samples, and the data supporting few-shot learning appears to be densely distributed; nearly all types of syntactic and semantic data examined lead to improvements on the reflexive pronoun or subject-verb agreement tasks.2."
2020.emnlp-main.331.txt,2020,6 Discussion,"the study found two latent dimensions of variation between nouns: one corresponding to how well the models understood its behavior with reflexive pronouns, and the other corresponding to subject-verb agreement."
2020.emnlp-main.331.txt,2020,6 Discussion,these results suggest that nouns should vary less in their grammatical performance than is actually observed; the study finds excess variation in grammatical performance.
2020.emnlp-main.331.txt,2020,6 Discussion,this would predict better performance for frequent nouns.
2020.emnlp-main.331.txt,2020,6 Discussion,"we found that there are systematic differences between nouns: when a language model exhibits knowledge of a noun’s grammatical properties in one task, it is more likely to do so in other tasks."
2020.emnlp-main.331.txt,2020,6 Discussion,we have investigated the sources of variation in neural language models’ grammatical judgments.
2020.emnlp-main.332.txt,2020,6 Conclusion,"we demonstrated experimentally that softconstraint can consistently improve wsd performance even when no manual translations are available, leading to state-of-the-art results on multilingual and english all-words wsd."
2020.emnlp-main.332.txt,2020,6 Conclusion,we make the source code available at https:// github.com/yixingluan/translations4wsd.
2020.emnlp-main.332.txt,2020,6 Conclusion,"we proposed a novel approach that improves wsd by leveraging translations from multiple languages, which incorporates a knowledge-based bitext alignment."
2020.emnlp-main.332.txt,2020,6 Conclusion,we tested our methods with several base wsd systems.
2020.emnlp-main.333.txt,2020,6 Conclusion,"in the future, we plan to apply the transformed representations on more lexical semantics tasks such as word sense disambiguation within an application (navigli and vannella, 2013)."
2020.emnlp-main.333.txt,2020,6 Conclusion,"we also perform controlled analysis to highlight, in isolation, the improvement from the transformation on both contextualization and on an overall interword semantic space."
2020.emnlp-main.333.txt,2020,6 Conclusion,we present an effective post-processing method that transforms and enhances contextual word representations through static anchors with guidance from other contextualized/static embeddings.
2020.emnlp-main.333.txt,2020,6 Conclusion,"we show leveraging static embeddings, with no labeled data, consistently improves (across almost all configurations) on both inter word and surprisingly within word context-aware lexical semantic tasks."
2020.emnlp-main.334.txt,2020,8 Conclusions,our data is not available due to licensing restrictions but can be redownloaded and processed with our scripts.
2020.emnlp-main.334.txt,2020,8 Conclusions,"through experiments on two core language processing tasks, language modeling and word associations, we show that demographicaware word representations outperform generic embeddings."
2020.emnlp-main.334.txt,2020,8 Conclusions,through several ablation analyses we show that word embeddings that leverage multiple demographic attributes give better performance than those using single attributes.
2020.emnlp-main.334.txt,2020,8 Conclusions,"to support future work that can help model individuals and demographics, our code is available at http://lit.eecs.umich.edu."
2020.emnlp-main.334.txt,2020,8 Conclusions,we also find that demographic matrices perform much better than demographic vectors.
2020.emnlp-main.334.txt,2020,8 Conclusions,we hope this will support work on solutions for nlp applications and resources that can better serve minorities and underrepresented groups.
2020.emnlp-main.334.txt,2020,8 Conclusions,we proposed a novel method of generating word representations by composing demographicspecific word vectors.
2020.emnlp-main.335.txt,2020,8 Summary,"in this paper, we describe the problem of pretrained word embeddings conflating denotation and connotation."
2020.emnlp-main.335.txt,2020,8 Summary,"lastly, we show that our decomposed spaces are capable of improving the diversity of document rankings in an information retrieval task."
2020.emnlp-main.335.txt,2020,8 Summary,we address this issue by introducing an adversarial network that explicitly represents the two properties as two different vector spaces.
2020.emnlp-main.335.txt,2020,8 Summary,we confirm that our decomposed spaces encode the desired structure of denotation or connotation by both quantitatively measuring their homogeneity and qualitatively evaluating their clusters and their representation of well-known political euphemisms.
2020.emnlp-main.336.txt,2020,6 Conclusion,"due to the lack of annotations, we only adopted simple unsupervised segmentation methods to extract different views."
2020.emnlp-main.336.txt,2020,6 Conclusion,experiments conducted demonstrated the effectiveness of our proposed models in terms of both quantitative and qualitative evaluations.
2020.emnlp-main.336.txt,2020,6 Conclusion,"in order to strategically combine these different views for better summary generations, we propose a multiview sequence-to-sequence model."
2020.emnlp-main.336.txt,2020,6 Conclusion,"in the future, we plan to annotate some of the data, explore supervised segmentation models (li et al., 2018) and introduce more conversation structures like dialogue acts (oya and carenini, 2014; joty and hoque, 2016) into abstractive dialogue summarization."
2020.emnlp-main.336.txt,2020,6 Conclusion,"in this work, we proposed a multi-view sequenceto-sequence model that leveraged multiple conversational structures (topic view and stage view) and generic views (global view and discrete view) to generate summaries for conversations."
2020.emnlp-main.336.txt,2020,6 Conclusion,"via thorough error analyses, we concluded a set of challenges that current models struggled with, which can further facilitate future research on conversation summarization."
2020.emnlp-main.337.txt,2020,8 Conclusions,"finally, we show that it also allows for successful cross-domain adaptation."
2020.emnlp-main.337.txt,2020,8 Conclusions,"in this work, we introduce the first to our knowledge few-shot framework for abstractive opinion summarization."
2020.emnlp-main.337.txt,2020,8 Conclusions,then we train a tiny plug-in network that learns to switch the generator to the summarization regime.
2020.emnlp-main.337.txt,2020,8 Conclusions,"we demonstrate that our approach substantially outperforms competitive ones, both abstractive and extractive, in human and automatic evaluation."
2020.emnlp-main.337.txt,2020,8 Conclusions,we propose to exploit summary related properties in unannotated reviews that are used for unsupervised training of a generator.
2020.emnlp-main.337.txt,2020,8 Conclusions,"we show that it can efficiently utilize even a handful of annotated reviews-summary pairs to train models that generate fluent, informative, and overall sentiment reflecting summaries."
2020.emnlp-main.338.txt,2020,4 Conclusion,future work may explore the use of points of correspondence and sentence fusion in the standard setting of document summarization.
2020.emnlp-main.338.txt,2020,4 Conclusion,"our findings suggest that modeling points of correspondence is crucial for effective sentence fusion, and sentence fusion remains a challenging direction of research."
2020.emnlp-main.338.txt,2020,4 Conclusion,"performing sentence fusion accurately and succinctly is especially important for summarizing long documents and book chapters (ladhak et al., 2020)."
2020.emnlp-main.338.txt,2020,4 Conclusion,"these domains may contain more entities and events to potentially confuse a summarizer, making our method of explicitly marking these entities beneficial."
2020.emnlp-main.338.txt,2020,4 Conclusion,we address the challenge of information fusion in the context of neural abstractive summarization by making crucial use of points of correspondence between sentences.
2020.emnlp-main.338.txt,2020,4 Conclusion,we enrich transformers with poc information and report model performance on a new test bed for information fusion.
2020.emnlp-main.339.txt,2020,8 Conclusion,"future work will focus on extending our models to generate extractive plans for better abstractive summarization of long or multiple documents (liu et al., 2018)."
2020.emnlp-main.339.txt,2020,8 Conclusion,"stepwise etcsum, in particular, sets a new standard for both tasks."
2020.emnlp-main.339.txt,2020,8 Conclusion,"the stepwise structured transformer paradigm, exemplified by hibert and etcsum, can be easily adapted both to extractive document summarization or content planning for table-to-text generation."
2020.emnlp-main.34.txt,2020,6 Conclusion,"however, in qualitative analysis, we found that the quality of the generated summaries of any unsupervised model was not sufficient, and there are individual limitations for each model."
2020.emnlp-main.34.txt,2020,6 Conclusion,"in particular for ealm, there is room for improvement by importing the latest techniques in rl."
2020.emnlp-main.34.txt,2020,6 Conclusion,our work paves the way for further research on bridging q-learning and unsupervised text summarization.
2020.emnlp-main.34.txt,2020,6 Conclusion,the experments showed that ealm performed competitively with the previous encoder-decoder-based methods.
2020.emnlp-main.34.txt,2020,6 Conclusion,these issue must be overcome as the step forward to generating practically available summaries without paired data.
2020.emnlp-main.34.txt,2020,6 Conclusion,we brought the q-learning framework into unsupervised text summarization and proposed a new method ealm that is an edit-based unsupervised summarizer leveraging a q-learning agent and a language model.
2020.emnlp-main.340.txt,2020,5 Conclusion and future work,"for future work, we think it will be interesting to look at: 1. zero-shot clir models for low-resource languages, 2. comparison of end-to-end neural rankers with traditional translation+ir pipelines in terms of both scalability, cost, and retrieval accuracy, 3. advanced neural architectures and training algorithms that can exploit our large training data, 4. building universal models for multilingual ir."
2020.emnlp-main.340.txt,2020,5 Conclusion and future work,our mixlanguage retrieval experiments on multi-8 show that a single multilingual model can significantly outperform the combination of multiple bilingual models.
2020.emnlp-main.340.txt,2020,5 Conclusion and future work,"the bi-139 dataset supports clir in 139×138 language pairs, whereas the multi-8 dataset enables mix-language retrieval in 8 languages."
2020.emnlp-main.340.txt,2020,5 Conclusion and future work,"the large number of supported language directions allows the research community to explore and build new models for many more languages, especially the low-resource ones."
2020.emnlp-main.340.txt,2020,5 Conclusion and future work,we document baseline ndcg results using a neural ranker based on multilingual bert.
2020.emnlp-main.340.txt,2020,5 Conclusion and future work,"we present clirmatrix, the largest and the most comprehensive collection of bilingual and multilingual clir datasets to date."
2020.emnlp-main.341.txt,2020,6 Conclusion,"in this work, we present sledge-z, an adaptation of a neural ranking pipeline for covid-19 scientific literature search."
2020.emnlp-main.341.txt,2020,6 Conclusion,"the approach is zeroshot and adapts to medical literature by filtering the training data, using a contextualized language model based trained on scientific text, and by filtering the document collection."
2020.emnlp-main.341.txt,2020,6 Conclusion,the zero-shot setting is important because it suggests that the approach can be generally applied to similar problems—even when no training data are available (which can be expensive to collect).
2020.emnlp-main.341.txt,2020,6 Conclusion,these observations underscore the importance of properly considering the domain when building medical search engines.
2020.emnlp-main.341.txt,2020,6 Conclusion,"through experiments and analysis on trec-covid, we find that each component of our approach is beneficial, and it outperforms or is comparable to approaches that are trained or tuned on trec-covid judgments."
2020.emnlp-main.341.txt,2020,6 Conclusion,we hope that techniques like sledge-z can help overcome the global covid-19 crisis.
2020.emnlp-main.342.txt,2020,7 Conclusion,"complex full query-document attention in stateof-the-art transformer rankers can be factored into independent document and query representation, and shallow light-weight interaction."
2020.emnlp-main.342.txt,2020,7 Conclusion,decoupling representation and interaction provides new understanding of transformer rankers.
2020.emnlp-main.342.txt,2020,7 Conclusion,"domain adaptation experiments show that mores’ modular design does not hurt transfer ability, indicating that mores can be adapted to low-resource domains with simple techniques."
2020.emnlp-main.342.txt,2020,7 Conclusion,experiments on a large supervised ranking task show that mores is as effective as a state-of-theart bert ranker.
2020.emnlp-main.342.txt,2020,7 Conclusion,"moreover, we found that the interaction in ranking is less domain-specific, while the representations need more domain adaptation."
2020.emnlp-main.342.txt,2020,7 Conclusion,mores is effective while being efficient and interpretable.
2020.emnlp-main.342.txt,2020,7 Conclusion,state-of-the-art neural rankers based on the transformer architecture consider all token pairs in a concatenated query and document sequence.
2020.emnlp-main.342.txt,2020,7 Conclusion,these findings provide opportunities for future work towards more efficient and interpretable neural ir.
2020.emnlp-main.342.txt,2020,7 Conclusion,"this paper proposes mores, a modular transformer ranking framework that decouples ranking into document representation, query representation, and interaction."
2020.emnlp-main.342.txt,2020,7 Conclusion,"though effective, they are slow and challenging to interpret."
2020.emnlp-main.342.txt,2020,7 Conclusion,"we further discovered two types of interaction: further query understanding based on the document, and the query to document tokens matching for relevance."
2020.emnlp-main.342.txt,2020,7 Conclusion,"with our proposed document representation pre-compute and re-use methods, mores can achieve 120x speedup in online ranking while retaining accuracy."
2020.emnlp-main.343.txt,2020,5 Conclusions and Future work,"as a future work, we plan to utilize automatic summarization for missing abstracts, instead of taking the first 512 content tokens."
2020.emnlp-main.343.txt,2020,5 Conclusions and Future work,"coupled with a second bert model that was trained to match queries to abstracts, we have experimentally shown on three different benchmarks that our proposed method outperformed state-of-the-art alternatives."
2020.emnlp-main.343.txt,2020,5 Conclusions and Future work,"we have cast a solution for faq retrieval to a solution for ad-hoc document retrieval, where titles and abstracts took the role of questions and answers in faqs."
2020.emnlp-main.343.txt,2020,5 Conclusions and Future work,"we have shown that, using the corpus itself, we could generate weakly-supervised title paraphrases for training a bert model that matches queries to titles."
2020.emnlp-main.344.txt,2020,7 Conclusion,"we also demonstrated that simple perplexity-based filtering is not sufficient to mitigate our attacks, motivating future research on more effective defenses."
2020.emnlp-main.344.txt,2020,7 Conclusion,"we demonstrated a new class of vulnerabilities in nlp applications: semantic collisions, i.e., input pairs that are unrelated to each other but perceived by the application as semantically similar."
2020.emnlp-main.344.txt,2020,7 Conclusion,we developed gradient-based search algorithms for generating collisions and showed how to incorporate constraints that help generate more “natural” collisions.
2020.emnlp-main.344.txt,2020,7 Conclusion,"we evaluated the effectiveness of our attacks on state-of-the-art models for paraphrase identification, document and sentence retrieval, and extractive summarization."
2020.emnlp-main.345.txt,2020,6 Conclusion and Future Work,allowing domain experts to instill their expertise into les can also enable humanmachine co-creation of explainable models.
2020.emnlp-main.345.txt,2020,6 Conclusion and Future Work,"another is to learn rules and dictionaries jointly, which may also aid sentiment analysis (wilson et al., 2005)."
2020.emnlp-main.345.txt,2020,6 Conclusion and Future Work,"as an extension, it may even be possible to determine the number of les k from the data using recurrent neural networks (yang et al., 2017)."
2020.emnlp-main.345.txt,2020,6 Conclusion and Future Work,"as opposed to learning an explainable model (e.g., rulenn), one may also choose to explain a black-box model."
2020.emnlp-main.345.txt,2020,6 Conclusion and Future Work,ideas presented here are general enough to enable other applications.
2020.emnlp-main.345.txt,2020,6 Conclusion and Future Work,"note that, rulenn’s philosophy is distinct from other explainable ai approaches (ribeiro et al., 2016; serrano and smith, 2019)."
2020.emnlp-main.345.txt,2020,6 Conclusion and Future Work,one avenue of future work is to learn explainable rules that domain experts can interact with on top of such embeddings.
2020.emnlp-main.345.txt,2020,6 Conclusion and Future Work,our experiments indicate that neuro-symbolic rulenn outperforms other rule induction techniques in terms of efficiency and quality of rules learned even in the presence of challenging conditions such as class skew.
2020.emnlp-main.345.txt,2020,6 Conclusion and Future Work,rulenn can be used for any mil task assuming predicates are given and pgms can be used to learn combinations of base predicates p even if the structure of the rule differs from les.
2020.emnlp-main.345.txt,2020,6 Conclusion and Future Work,"rulenn can learn rules combining any previously built classifier’s output probabilities (assuming such probabilities lie within [0, 1])."
2020.emnlp-main.345.txt,2020,6 Conclusion and Future Work,"such efforts are usually restricted to explaining outcomes and only provide a shallow understanding of the overall model, if at all (guidotti et al., 2018)."
2020.emnlp-main.345.txt,2020,6 Conclusion and Future Work,we show that it is possible to learn human-interpretable models by designing neural networks keeping explainability in mind.
2020.emnlp-main.345.txt,2020,6 Conclusion and Future Work,"while recent embeddings (devlin et al., 2019) may lead to improved accuracy, these remain poorly understood (moradshahi et al., 2019)."
2020.emnlp-main.346.txt,2020,8 Conclusion,"although we focus only on masked language models in this paper, our method can be trivially extended to standard language models, and thus maybe useful for constructing inputs for models like gpt-3 (brown et al., 2020)."
2020.emnlp-main.346.txt,2020,8 Conclusion,"furthermore, the results for sentiment analysis and textual entailment suggest that, in some data-scarce settings, it may be more effective to prompt language models than to finetune them for the task."
2020.emnlp-main.346.txt,2020,8 Conclusion,"in this paper, we introduce autoprompt, an approach to develop automatically-constructed prompts that elicit knowledge from pretrained mlms for a variety of tasks."
2020.emnlp-main.346.txt,2020,8 Conclusion,source code and datasets to reproduce the results in this paper is available at http://ucinlp.github.io/autoprompt.
2020.emnlp-main.346.txt,2020,8 Conclusion,we show that these prompts outperform manual prompts while requiring less human effort.
2020.emnlp-main.347.txt,2020,6 Conclusion,"in this paper, we proposed an effective method, vmask, learning global task-specific important features to improve both model interpretability and prediction accuracy."
2020.emnlp-main.347.txt,2020,6 Conclusion,"we tested vmask with three different neural text classifiers on seven benchmark datasets, and assessed its effectiveness via both quantitative and qualitative evaluations."
2020.emnlp-main.348.txt,2020,6 Conclusions,"additionally, we proposed new metrics for evaluating language models that produce sparse and truncated probability distributions: _x000f_-perplexity, sparsemax score, and jensen-shannon divergence."
2020.emnlp-main.348.txt,2020,6 Conclusions,"experiments show that entmax sampling leads to higher n-gram diversity, fewer repetitions, and similar or improved results in automatic metrics."
2020.emnlp-main.348.txt,2020,6 Conclusions,"human evaluation confirms that entmax outperforms greedy decoding, top-k, and nucleus sampling in coherence/consistency and engagement, and is similar or better in terms of fluency."
2020.emnlp-main.348.txt,2020,6 Conclusions,"it provides three main advantages: (i) it offers a natural way of sampling directly from the output probability distribution; (ii) the distribution sparsity is modeled during training, avoiding a sparsity mismatch between training and run time; (iii) when sampling with entmax, the number of words to be considered varies with the context, as in nucleus sampling and in contrast to top-k sampling."
2020.emnlp-main.348.txt,2020,6 Conclusions,we proposed entmax sampling as a new strategy for generating text from a sparse probability distribution.
2020.emnlp-main.349.txt,2020,6 Conclusion,"in order to keep track of plot elements, we create plotmachines which generates paragraphs using a high-level discourse structure and a dynamic plot memory keeping track of both the outline and story."
2020.emnlp-main.349.txt,2020,6 Conclusion,quantitative analysis shows that plotmachines is effective in composing tighter narratives based on outlines compared to competitive baselines.
2020.emnlp-main.349.txt,2020,6 Conclusion,we facilitate training by altering three datasets to include plot outlines as input for long story generation.
2020.emnlp-main.349.txt,2020,6 Conclusion,"we present outline-conditioned story generation, a new task for generating stories from outlines representing key plot elements."
2020.emnlp-main.35.txt,2020,6 Conclusion and future work,"by introducing a small number of parameters, ta is able to further improve the performance of these models, especially under a long-document scenario."
2020.emnlp-main.35.txt,2020,6 Conclusion and future work,"in the future, we will study the effectiveness of ta on other nlp tasks, such as the document-level translation, and investigate whether ta is useful for transformer pre-training."
2020.emnlp-main.35.txt,2020,6 Conclusion and future work,"in this paper, we explore and rearrange semantics of a topic model and then propose a friendly plug-and-play ta for transformer-based abstractive summarization models."
2020.emnlp-main.350.txt,2020,7 Conclusion,a promising research direction is to investigate the root cause behind memorization.
2020.emnlp-main.350.txt,2020,7 Conclusion,"a simple reason for the memorization of the first few words could be that, in the beginning of training, the reconstruction loss is higher on these words (see lstm-lm in figures 1, 2, 3, 4)."
2020.emnlp-main.350.txt,2020,7 Conclusion,"although recent incarnations of the seq2seq vae fix the posterior collapse, they partially memorize the first few words and the document lengths."
2020.emnlp-main.350.txt,2020,7 Conclusion,"depending on the data, these local features are sometimes not very correlated with global aspects like topic or sentiment."
2020.emnlp-main.350.txt,2020,7 Conclusion,"eliminating posterior collapse is necessary to get useful vae models, but not sufficient."
2020.emnlp-main.350.txt,2020,7 Conclusion,"if that is correct, the left-to-right factorization of the decoder could be at fault, which would explain the successes of the unigram decoders."
2020.emnlp-main.350.txt,2020,7 Conclusion,"in contrast, models in the bert family (devlin et al., 2018) are given corrupted inputs and are penalized only on these corrupted inputs, thereby avoid memorization altogether."
2020.emnlp-main.350.txt,2020,7 Conclusion,"methodologically, we introduced a simple way to examine the content of latent variables by looking at the reconstruction loss per position."
2020.emnlp-main.350.txt,2020,7 Conclusion,"more powerful decoders with alternative factorizations could avoid this issue, for example, non-autoregressive transformers (gu et al., 2017) or transformers with flexible word orders (gu et al., 2019)."
2020.emnlp-main.350.txt,2020,7 Conclusion,"moreover, the agreement metric is another complementary evaluation that is automatic and focused on generation."
2020.emnlp-main.350.txt,2020,7 Conclusion,the latent variable is more predictive of global features and memorisation of the first words and sentence length is decreased.
2020.emnlp-main.350.txt,2020,7 Conclusion,"therefore, another research avenue would be to blend the two frameworks (im et al., 2017)."
2020.emnlp-main.350.txt,2020,7 Conclusion,"therefore, they are of limited use for controllable and diverse text generation."
2020.emnlp-main.350.txt,2020,7 Conclusion,these early errors should therefore account for a proportionally large part of the gradients and pressure the encoder to store information about the first words.
2020.emnlp-main.350.txt,2020,7 Conclusion,"these variants are all effective, in particular, the unigram decoder used as an auxiliary decoder (de fauw et al., 2019)."
2020.emnlp-main.350.txt,2020,7 Conclusion,"thus, these models are more suitable for diverse and controllable generation."
2020.emnlp-main.350.txt,2020,7 Conclusion,"to learn to infer more global features, we explored alternative architectures based on bag-ofword assumptions on the encoder or decoder side, as well as a pretraining procedure."
2020.emnlp-main.350.txt,2020,7 Conclusion,vaes operate on uncorrupted inputs and learn a corruption process in the latent space.
2020.emnlp-main.350.txt,2020,7 Conclusion,"we also presented a reliable way to perform semisupervised learning experiments to analyze the content of the variable, free of the problems that one can find in past work (incorrect model selection for small data-regimes, use of samples instead of variational parameters as inputs)."
2020.emnlp-main.350.txt,2020,7 Conclusion,we hope that these methods will see widespread adoption for measuring progress more reliably.
2020.emnlp-main.350.txt,2020,7 Conclusion,"we showed that there are particularly difficult datasets for which the first words are not very predictive of their labels, and therefore, these datasets should be systematically used in evaluations."
2020.emnlp-main.351.txt,2020,8 Conclusion and Future Work,"although our aristotelian plots improved over the naive plot, there remains gaps in quality between generated and gold plot structures."
2020.emnlp-main.351.txt,2020,8 Conclusion and Future Work,our findings also suggest future work on additional ways to incorporate story principles into plot generation.
2020.emnlp-main.351.txt,2020,8 Conclusion and Future Work,"there is also further work to be done in investigating what models are best able to incorporate plots, which would enable plot improvements to be even more effective."
2020.emnlp-main.351.txt,2020,8 Conclusion and Future Work,we found that this results in stories that are both more relevant and higher quality than stories that are generated directly from prompts or that use plots without aristotelian rescoring.
2020.emnlp-main.351.txt,2020,8 Conclusion and Future Work,we have shown that content planning via an interim plot structure representation can be combined with the use of rescoring models to inject aristotelian story-writing principles into the plot.
2020.emnlp-main.352.txt,2020,5 Conclusion,"human evaluation results concretely demonstrate that our latent space method significantly outperforms baselines using end-to-end cross entropy training, in terms of relevance and informativeness."
2020.emnlp-main.352.txt,2020,5 Conclusion,"in this work, we pointed out that end-to-end cross entropy classification used in most previous methods is not able to integrate information from different semantically similar words responses, and designed a substitute method that is able to do so."
2020.emnlp-main.352.txt,2020,5 Conclusion,"our method learns the pair relationship between prompts and responses as a regression task on a latent space, which is more suitable for the openended nature of this task."
2020.emnlp-main.352.txt,2020,5 Conclusion,we performed ablation study to validate the components of our model.
2020.emnlp-main.353.txt,2020,7 Conclusion,generating subsequent references with such properties has the potential to enhance user adaptation and successful communication in dialogue systems.
2020.emnlp-main.353.txt,2020,7 Conclusion,"in the present work, we simplified the interactive aspects of reference by extracting referring utterances from the photobook dialogues and framing the problem as that of generating the next referring utterance given the previous mention."
2020.emnlp-main.353.txt,2020,7 Conclusion,our analysis revealed that the generated utterances exhibit linguistic properties that are similar to those observed in the human utterances regarding reuse of words and reduction.
2020.emnlp-main.353.txt,2020,7 Conclusion,"to our knowledge, this is the first attempt at tackling this problem at the level of surface realisation within a multimodal dialogue context."
2020.emnlp-main.353.txt,2020,7 Conclusion,"we believe that the resulting dataset of referring utterance chains can be a useful resource to analyse and model other dialogue phenomena, such as saliency or partner specificity, both on language alone or on the interaction of language and vision."
2020.emnlp-main.353.txt,2020,7 Conclusion,we have addressed the generation of descriptions that are (1) discriminative with respect to the visual context and (2) grounded in the linguistic common ground established in previous mentions.
2020.emnlp-main.353.txt,2020,7 Conclusion,we proposed an encoder-decoder model that is able to generate both first mentions and subsequent references by encoding the dialogue context in a multimodal fashion and dynamically attending over it.
2020.emnlp-main.353.txt,2020,7 Conclusion,"we showed that our best performing model is able to produce better, more effective referring utterances than a variant that is solely grounded in the visual context."
2020.emnlp-main.353.txt,2020,7 Conclusion,"yet, in our approach we abstracted away from important interactive aspects such as the collaborative nature of referring in dialogue (clark and wilkes-gibbs, 1986), which was considered by shore and skantze (2018) for the task of reference resolution."
2020.emnlp-main.354.txt,2020,6 Conclusion,"to tackle the issues of misleading and insufficient learning signals from purely agreement-based learning, we propose to complement the image-text alignment loss with a loss defined on unlabeled text."
2020.emnlp-main.354.txt,2020,6 Conclusion,"vc-pcfgs exploit visual groundings via contrastive learning, with learning signals derived from minimizing an image-text alignment loss."
2020.emnlp-main.354.txt,2020,6 Conclusion,we empirically show that our vc-pcfgs are superior to models that are trained only through visually grounded learning or only relying on text.
2020.emnlp-main.354.txt,2020,6 Conclusion,we have presented visually-grounded compound pcfgs (vc-pcfgs) that use compound pcfgs and generalize the visually grounded grammar learning framework.
2020.emnlp-main.354.txt,2020,6 Conclusion,"we resort to using compound pcfgs which enables us to complement the alignment loss with a language modeling objective, resulting in a fully-differentiable end-to-end visually grounded learning."
2020.emnlp-main.355.txt,2020,7 Conclusion,"in the future, we plan to examine the hierarchical classification architecture’s potential for reducing computational runtime."
2020.emnlp-main.355.txt,2020,7 Conclusion,our experiments on two visual recognition tasks show that incorporating natural language explanations is far more data-efficient than adding extra training data.
2020.emnlp-main.355.txt,2020,7 Conclusion,we extend the concept of active learning to class-based active learning for choosing the most informative query pair.
2020.emnlp-main.355.txt,2020,7 Conclusion,we incorporate the extracted knowledge from expert natural language explanation by changing our algorithm’s neural network structure.
2020.emnlp-main.355.txt,2020,7 Conclusion,we propose an expert-in-the-loop training framework alice to utilize contrastive natural language explanations to improve a learning algorithm’s data efficiency.
2020.emnlp-main.356.txt,2020,6 Conclusion,evaluating on three typologically diverse languages will help the community avoid overfitting to a particular language and dataset.
2020.emnlp-main.356.txt,2020,6 Conclusion,"finally, every instruction is accompanied by a follower demonstration, including a perspective camera pose trace that shows a play-by-play account of how a human interpreted the instructions given their position and progress through the path."
2020.emnlp-main.356.txt,2020,6 Conclusion,"rxr represents a significant evolution in the scale, scope and possibilities for research on embodied language agents in simulated, photo-realistic 3d environments."
2020.emnlp-main.356.txt,2020,6 Conclusion,rxr’s paths better ensure that language itself will play a fundamental role in better agents.
2020.emnlp-main.356.txt,2020,6 Conclusion,this is obtained as a by-product of significant work on the annotation tooling itself and designing the process to be more natural for guides.
2020.emnlp-main.356.txt,2020,6 Conclusion,we have only begun to explore the possibilities opened up by pose traces.
2020.emnlp-main.356.txt,2020,6 Conclusion,"we have shown that these can help with agent training, but they also open up new possibilities for studying grounded language pragmatics in the vln setting, and for training vln agents with perspective cameras – either in the graph-based simulator or by lifting rxr into a continuous simulator (krantz et al., 2020)."
2020.emnlp-main.356.txt,2020,6 Conclusion,"whereas others have retro-actively refined r2r’s annotations to get alignments between sub-instructions and panorama sequences (hong et al., 2020), rxr provides wordlevel alignments to specific pixels in panoramas."
2020.emnlp-main.357.txt,2020,5 Conclusion and Future Work,"also, a stronger explainer that explains not only token-level error but also global editing operation between two images can provide robust self-supervised loss.acknowledgments."
2020.emnlp-main.357.txt,2020,5 Conclusion and Future Work,"currently, the interactive explainer and counterfactual intervention in sscr both improve the editing quality in the token-level."
2020.emnlp-main.357.txt,2020,5 Conclusion and Future Work,"despite without groundtruth resulting images, we propose cross-task consistency (ctc) to provide a more explicit training signal and train these counterfactual instructions in a self-supervised scenario."
2020.emnlp-main.357.txt,2020,5 Conclusion and Future Work,"experimental results show that our counterfactual framework not only trains the image editor better but also improves the generalizability, even under data scarcity."
2020.emnlp-main.357.txt,2020,5 Conclusion and Future Work,"for the real world, both visual and linguistic will be more complicated."
2020.emnlp-main.357.txt,2020,5 Conclusion and Future Work,"from a theoretical perspective, our sscr is a model-agnostic framework that can incorporate with any image generator, for gan or non-gan architecture, to perform real-world image editing."
2020.emnlp-main.357.txt,2020,5 Conclusion and Future Work,research was sponsored by the u.s. army research office and was accomplished under contract number w911nf19-d-0001 for the institute for collaborative biotechnologies.
2020.emnlp-main.357.txt,2020,5 Conclusion and Future Work,sscr allows the model to consider new instruction-image pairs.
2020.emnlp-main.357.txt,2020,5 Conclusion and Future Work,the u.s. government is authorized to reproduce and distribute reprints for government purposes notwithstanding any copyright notation herein.
2020.emnlp-main.357.txt,2020,5 Conclusion and Future Work,"the views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the u.s. government."
2020.emnlp-main.357.txt,2020,5 Conclusion and Future Work,"to accomplish real-world image editing, large-scale pre-trained language encoders and image generators should be applied to understand the diverse instructions and model the interaction for editing."
2020.emnlp-main.357.txt,2020,5 Conclusion and Future Work,"to make it more suitable for real-world images, semantic-level intervention for the diverse natural instructions can support better counterfactual reasoning."
2020.emnlp-main.357.txt,2020,5 Conclusion and Future Work,we present a self-supervised counterfactual reasoning (sscr) framework that introduces counterfactual thinking to cope with the data scarcity limitation for iterative language-based image editing.
2020.emnlp-main.358.txt,2020,7 Conclusion,in future work we plan to incorporate crosslingual signals as vulic et al.´ (2019) argue that a fully unsupervised setting is hard to motivate.
2020.emnlp-main.358.txt,2020,7 Conclusion,"the main takeaways are: i) shared position embeddings, shared special tokens, replacing masked tokens with random tokens and a limited amount of parameters are necessary elements for multilinguality.ii) word order is relevant: bert is not multilingual with one language having an inverted word order.iii) the comparability of training corpora contributes to multilinguality."
2020.emnlp-main.358.txt,2020,7 Conclusion,we considered a fully unsupervised setting without any crosslingual signals.
2020.emnlp-main.358.txt,2020,7 Conclusion,we experimented with a simple modification to obtain stronger multilinguality in bert models and demonstrate its effectiveness on xnli.
2020.emnlp-main.358.txt,2020,7 Conclusion,we investigated which architectural and linguistic properties are essential for bert to yield crosslingual representations.
2020.emnlp-main.358.txt,2020,7 Conclusion,we show that our findings transfer to larger scale settings.
2020.emnlp-main.359.txt,2020,6 Conclusion,we further propose a method and show it can improve cross-lingual transferability by mitigating negative interference.
2020.emnlp-main.359.txt,2020,6 Conclusion,we present the first systematic study of negative interference in multilingual models and shed light on its causes.
2020.emnlp-main.359.txt,2020,6 Conclusion,"while prior efforts focus on improving sharing and cross-lingual alignment, we provide new insights and a different perspective on unsharing and resolving language conflicts."
2020.emnlp-main.36.txt,2020,5 Conclusion,all existing work either use bertbase or roberta-base as teacher.
2020.emnlp-main.36.txt,2020,5 Conclusion,codir utilizes information from both teacher’s output layer and its intermediate layers for student model training.
2020.emnlp-main.36.txt,2020,5 Conclusion,"extensive experiments demonstrate that codir is highly effective in both finetuning and pre-training stages, and achieves state-of-the-art performance on glue benchmark compared to existing models with a similar size."
2020.emnlp-main.36.txt,2020,5 Conclusion,"for future work, we plan to investigate the use of a more powerful language model, such as megatron-lm (shoeybi et al., 2019), as the teacher; and different strategies for choosing hard negatives to further boost the performance."
2020.emnlp-main.36.txt,2020,5 Conclusion,"in this paper, we present codir, a novel approach to large-scale language model compression via the use of contrastive loss."
2020.emnlp-main.360.txt,2020,6 Conclusion,we believe our results will motivate researchers to pay more attention to the existence of mwes and how they are aligned across languages.
2020.emnlp-main.360.txt,2020,6 Conclusion,we found that simple lexicon-based tokenizations can align embeddings of mwes at a high precision without breaking alignments of single-words.
2020.emnlp-main.360.txt,2020,6 Conclusion,we studied the impact of pre-tokenizing mwes on cross-lingual alignments of word embeddings.
2020.emnlp-main.361.txt,2020,7 Conclusion,"future work will investigate the compositional capability of these adapters, and combine domain and monolingual adapters for nmt."
2020.emnlp-main.361.txt,2020,7 Conclusion,"in a zero-shot setting, we naturally compose our monolingual adapters and obtain a median improvement of +2.77 bleu points over a strong mnmt model."
2020.emnlp-main.361.txt,2020,7 Conclusion,"more generally, this work adds to the growing evidence of the flexibility of adapter layers (pfeiffer et al., 2020a), and their potential for lightweight fine-tuning, including in zero-shot scenarios (pfeiffer et al., 2020b) and in a variety of tasks (üstün et al., 2020)."
2020.emnlp-main.361.txt,2020,7 Conclusion,"our adaptation experiments show the potential of the proposed monolingual adapters, which outperform bilingual adapters while having fewer parameters."
2020.emnlp-main.361.txt,2020,7 Conclusion,"this work investigated adapter modules and their compositionality for mnmt, in particular in the zero-shot setting."
2020.emnlp-main.361.txt,2020,7 Conclusion,"we introduced monolingual adapters and compared them to bilingual adapters, which we also applied to zero-shot translation."
2020.emnlp-main.362.txt,2020,5 Discussion,"additionally, no alignment methods improve xlmr and larger xlmrlarge performs much better, and raw text is easier to obtain than bitext."
2020.emnlp-main.362.txt,2020,5 Discussion,"however, to our surprise, previously proposed methods do not show consistent improvement over no alignment in this setting."
2020.emnlp-main.362.txt,2020,5 Discussion,"more broadly, the community must establish a robust evaluation scheme for zero-shot crosslingual transfer as a single run with one random seed does not reflect the variance of the method (especially in a zero-shot or few-shot setting).5 while keung et al.(2020) advocate using oracle for model selection, we instead argue reporting the variance of test performance, following the few-shot learning literature."
2020.emnlp-main.362.txt,2020,5 Discussion,"our proposed contrastive alignment objective outperforms l2 alignment (cao et al., 2020) and consistently performs as well as or better than no alignment using various quality bitext on 4 nlp tasks under a comprehensive evaluation with multiple seeds."
2020.emnlp-main.362.txt,2020,5 Discussion,"therefore, scaling models to more raw text and larger capacity models may be more beneficial for producing better cross-lingual models."
2020.emnlp-main.362.txt,2020,5 Discussion,"therefore, we make the following recommendations for future work on cross-lingual alignment or multilingual representations: 1) evaluations should consider average quality data, not exclusively high-quality bitext.2) evaluation must consider multiple nlp tasks or datasets.3) evaluation should report mean and variance over multiple seeds, not a single run."
2020.emnlp-main.363.txt,2020,5 Conclusion,"finally and most importantly, we have shown that the mmt potential on distant and low-resource target languages can be quickly unlocked if they are provided a handful of annotated instances in the target language."
2020.emnlp-main.363.txt,2020,5 Conclusion,"in this work, we have demonstrated that, similar to earlier language transfer paradigms, mmts perform poorly in zero-shot transfer to distant target languages, and to languages with smaller monolingual corpora available for exploitation in mmt pretraining."
2020.emnlp-main.363.txt,2020,5 Conclusion,massively multilingual transformers (mmts) have recently been praised for their zero-shot transfer capabilities that mitigate the data scarcity issue.
2020.emnlp-main.363.txt,2020,5 Conclusion,"our results have revealed that structural language similarity determines the transfer success for lower-level tasks like pos-tagging and dependency parsing; on the other hand, the pretraining corpora size of the target language is crucial for explaining transfer results for higher-level language understanding tasks, such as question answering and natural language inference."
2020.emnlp-main.363.txt,2020,5 Conclusion,research on zero-shot language transfer in nlp is motivated by inherent data scarcity: the fact that most languages have no annotated data for most nlp tasks.
2020.emnlp-main.363.txt,2020,5 Conclusion,"this finding provides a strong incentive for intensifying future research efforts that focus on cheap or naturally occurring supervision (vulic et al.´ , 2019; artetxe et al., 2020c; marchisio et al., 2020), quick and simple annotation procedure, and the more effective few-shot transfer learning setups."
2020.emnlp-main.363.txt,2020,5 Conclusion,we have presented a detailed empirical analysis of factors affecting zero-shot transfer performance of mmts across diverse tasks and languages.
2020.emnlp-main.364.txt,2020,8 Conclusions,a second set of experiments has revealed a dataset for which the domains were not clearly separable and some domains exhibited train/test mismatches.
2020.emnlp-main.364.txt,2020,8 Conclusions,"despite using distillation from these experts, mdkd was very robust to this noisy setting: not only was there no quality degradation, but we even observed modest improvements over the baselines."
2020.emnlp-main.364.txt,2020,8 Conclusions,"finally, we performed experiments in which we assumed that the domain labels are unknown and are obtained through clustering of the train data."
2020.emnlp-main.364.txt,2020,8 Conclusions,"for future work, we plan to expand the automatic domain induction methods and test the mdkd framework on generic mt with data exhibiting varying degrees of heterogeneity: as mdkd distills domain-specific models to create multiple simpler data distributions, we want to investigate if inducing train-time specializations and using them for distillation through mdkd can lead to better quality."
2020.emnlp-main.364.txt,2020,8 Conclusions,"in this paper, we have fixed the depth and the architecture of the teachers; however, improving the teachers using different architectures may also lead to better empirical results."
2020.emnlp-main.364.txt,2020,8 Conclusions,"in this setting, training domain-specific expert models is not a robust strategy, as the expert models performed significantly worse that the baselines."
2020.emnlp-main.364.txt,2020,8 Conclusions,"our experiments have covered two data quality conditions: when the domains are well-defined and separable, individually trained deep domain experts outperform all the multi-domain baselines and mdkd bridges a large portion of the gap between these baselines and the deep experts."
2020.emnlp-main.364.txt,2020,8 Conclusions,"since the approach is architecture-independent, it is easy to combine with other multi-domain nmt models."
2020.emnlp-main.364.txt,2020,8 Conclusions,"the resulting mdkd models outperformed the baselines again, showing that gold domain labels are not strictly needed."
2020.emnlp-main.364.txt,2020,8 Conclusions,"we have introduced multi-domain knowledge distillation, a new method for multi-domain nmt that distills multiple expert models into a single student that shows high quality across all domains."
2020.emnlp-main.364.txt,2020,8 Conclusions,we have kept both model architecture and capacity fixed and shown that mdkd leads to significantly better multi-domain models without any increase in translation time or memory usage.
2020.emnlp-main.365.txt,2020,9 Conclusion,"further, it minimizes the potential language bias of the resulting model."
2020.emnlp-main.365.txt,2020,9 Conclusion,"further, we noticed that laser and labse show a language bias, preferring some language combinations over other."
2020.emnlp-main.365.txt,2020,9 Conclusion,models can be extended to multiple languages in the same training process.
2020.emnlp-main.365.txt,2020,9 Conclusion,"then, in an independent step, it can be extended to support further languages."
2020.emnlp-main.365.txt,2020,9 Conclusion,this decoupling significantly simplifies the training procedure compared to previous approaches.
2020.emnlp-main.365.txt,2020,9 Conclusion,"this stepwise training approach has the advantage that an embedding model with desired properties, for example for clustering, can first be created for a high-resource language."
2020.emnlp-main.365.txt,2020,9 Conclusion,this was achieved by using multilingual knowledge distillation.
2020.emnlp-main.365.txt,2020,9 Conclusion,we demonstrated that this approach successfully transfers properties from the source language vector space (in our case english) to various target languages.
2020.emnlp-main.365.txt,2020,9 Conclusion,we extensively tested the approach for various languages from different language families.
2020.emnlp-main.365.txt,2020,9 Conclusion,"we observe that laser and labse work well for retrieving exact translations, however, they work less well assessing the similarity of sentence pairs that are not exact translations."
2020.emnlp-main.365.txt,2020,9 Conclusion,we presented a method to make monolingual sentence embeddings multilingual with aligned vector spaces between the languages.
2020.emnlp-main.366.txt,2020,5 Discussion,"applied to a state-of-the-art machine translation model, our method yields substantial speed improvements compared to traditionally-batched variable-width beam search."
2020.emnlp-main.366.txt,2020,5 Discussion,"in this work, we have proposed a streaming method for variable-length decoding to improve gpu utilization, resulting in cheaper inference."
2020.emnlp-main.366.txt,2020,5 Discussion,"we also apply our method to both semantic and syntactic parsing, demonstrating our method’s broader applicability to tasks that process variable-output-length data in a sequential manner."
2020.emnlp-main.367.txt,2020,5 Conclusion,we describe a novel clustering-based multilingual vocabulary generation algorithm.
2020.emnlp-main.367.txt,2020,5 Conclusion,we showed that this empirically motivated clustering-based method consistently outperforms the standard vocabulary generation recipe used by most multilingual pretrained language modeling work.
2020.emnlp-main.368.txt,2020,7 Conclusion,"furthermore, we show in a typological analysis that languages which share certain morphosyntactic features tend to benefit from this type of transfer."
2020.emnlp-main.368.txt,2020,7 Conclusion,future studies will extend this work to other crosslingual nlp tasks and more languages.
2020.emnlp-main.368.txt,2020,7 Conclusion,"in this work, we show that meta-learning can be used to effectively leverage training data from an auxiliary language for zero-shot and few-shot crosslingual transfer."
2020.emnlp-main.368.txt,2020,7 Conclusion,"we are able to improve the performance of state-of-the-art baseline models for (i) zero-shot xnli, and (ii) zero-shot qa on the mlqa dataset."
2020.emnlp-main.368.txt,2020,7 Conclusion,"we evaluated this on two challenging nlu tasks (nli and qa), and on a total of 15 languages."
2020.emnlp-main.369.txt,2020,5 Conclusion,"to the best of our knowledge, this is the largest public benchmark dataset for the training and evaluation of multilingual text classification models."
2020.emnlp-main.369.txt,2020,5 Conclusion,"we discuss the data preparation steps, analyze the distribution of the important characteristics of the corpus, and present baseline results for supervised and zero-shot cross-lingual text classification."
2020.emnlp-main.369.txt,2020,5 Conclusion,we present a curated subset of amazon reviews specifically designed to aid research in multilingual text classification.
2020.emnlp-main.369.txt,2020,5 Conclusion,"with these contributions, we hope that this corpus will be an important resource to the research community."
2020.emnlp-main.369.txt,2020,5 Conclusion,"with this work, we systematically address various gaps that we identified in existing multilingual corpora: we apply careful sampling, filtering, and text processing to the documents to minimize noise in the dataset, and we provide a large number of samples for training models in six languages with well-defined training, development, and test splits."
2020.emnlp-main.37.txt,2020,5 Conclusion,distillation is also used to reduce the accuracy drop caused by lower capacity due to quantization.
2020.emnlp-main.37.txt,2020,5 Conclusion,empirical experiments show that the proposed ternarybert outperforms stateof-the-art bert quantization methods and even performs comparably as the full-precision bert.
2020.emnlp-main.37.txt,2020,5 Conclusion,"in this paper, we proposed to use approximationbased and loss-aware ternarization to ternarize the weights in the bert model, with different granularities for word embedding and weights in the transformer layers."
2020.emnlp-main.370.txt,2020,8 Conclusions,"first, our analysis showed that glucose rules capture knowledge not available in existing resources or pre-trained models."
2020.emnlp-main.370.txt,2020,8 Conclusions,"our results validate our hypothesis that a promising approach for imbuing machines with commonsense is to use carefully-crafted data, as in glucose, to train neural architectures that have a wide range of lexical and conceptual knowledge encoded, as in models pretrained on large corpora."
2020.emnlp-main.370.txt,2020,8 Conclusions,"second, in order to evaluate how well ai models can predict glucose knowledge on novel inputs, the ultimate value of such a dataset, we defined a standalone evaluation task for predicting specific and general inference rules given a story/sentence pair and a dimension."
2020.emnlp-main.370.txt,2020,8 Conclusions,"the theories are categorized into ten causal dimensions, inspired by cognitive psychology."
2020.emnlp-main.370.txt,2020,8 Conclusions,"together with this paper, we release our dataset16 and models17, which we hope will enable the ai research community to explore effective approaches to incorporate commonsense reasoning capabilities into various downstream tasks."
2020.emnlp-main.370.txt,2020,8 Conclusions,"we curated a doubly-vetted test set, developed a platform to facilitate human judgment of system outputs, and validated bleu as a strong automated evaluation metric."
2020.emnlp-main.370.txt,2020,8 Conclusions,we demonstrated the utility of glucose data in two ways.
2020.emnlp-main.370.txt,2020,8 Conclusions,"we introduced glucose, a large-scale dataset of implicit commonsense knowledge, encoded as explanatory mini-theories grounded in a narrative context."
2020.emnlp-main.370.txt,2020,8 Conclusions,"we presented our multi-stage pipeline for acquiring semi-structured causal explanations at scale from lay workers, resulting in ˜670k annotations in the context of everyday children’s stories."
2020.emnlp-main.370.txt,2020,8 Conclusions,we show that training on glucose data improves model performances significantly on unseen stories.
2020.emnlp-main.371.txt,2020,6 Conclusion,"however, characters can still be used to improve performance, in both a single encoder and two encoders (rq2)."
2020.emnlp-main.371.txt,2020,6 Conclusion,"the improvements are also robust across different languages models, languages and data sets (rq4) and improve performance across a range of semantic phenomena (rq5)."
2020.emnlp-main.371.txt,2020,6 Conclusion,"the improvements are larger than using individual sources of linguistic information, and performance still improves in combination with these sources (rq3)."
2020.emnlp-main.371.txt,2020,6 Conclusion,these methods should be applicable to other semantic parsing and perhaps other natural language analysis tasks.
2020.emnlp-main.371.txt,2020,6 Conclusion,"we performed a range of experiments on discourse representation structure parsing using neural sequence-to-sequence models, in which we vary the neural representation of the input documents."
2020.emnlp-main.371.txt,2020,6 Conclusion,"we show that, not surprisingly, using pretrained contextual language models is better than simply using characters as input (rq1)."
2020.emnlp-main.372.txt,2020,5 Conclusions,experimental results show that these models can be enhanced by this disease infusion method in nearly all cases.
2020.emnlp-main.372.txt,2020,5 Conclusions,"in this paper, we propose a new disease infusion training procedure to augment bert-like pre-trained language models with disease knowledge."
2020.emnlp-main.372.txt,2020,5 Conclusions,we conduct this training procedure on a suite of bert models and evaluate them over diseaserelated tasks.
2020.emnlp-main.373.txt,2020,7 Discussion and Conclusion,a more structured knowledge generation might also make the combination of various knowledge sources more successful.
2020.emnlp-main.373.txt,2020,7 Discussion and Conclusion,a preliminary experiment in which we incorporated clarification pairs to facilitate two hops got mixed results.
2020.emnlp-main.373.txt,2020,7 Discussion and Conclusion,"among the clarifications used in practice by the answer scorer, about 60% of those that yielded a correct prediction and 12% of those that yielded an incorrect prediction were judged by humans as factually correct."
2020.emnlp-main.373.txt,2020,7 Discussion and Conclusion,"an interesting future direction is to generate each clarification in response to the previous ones, in a dialogue setup (saeidi et al., 2018)."
2020.emnlp-main.373.txt,2020,7 Discussion and Conclusion,"another challenge is the “needle in a haystack” problem of the clarifications, and one way to address it is to develop a model that is capable of “introspection”, specifically knowing what it doesn’t know."
2020.emnlp-main.373.txt,2020,7 Discussion and Conclusion,"by design, our model makes a single additional reasoning step explicit, aiming to facilitate reasoning about implicit inferences."
2020.emnlp-main.373.txt,2020,7 Discussion and Conclusion,"despite their insufficient precision compared to a kb like conceptnet, we showed that clarifications generated by lms resulted in similar or superior empirical gains."
2020.emnlp-main.373.txt,2020,7 Discussion and Conclusion,filling in knowledge gaps and making implicit intermediate reasoning steps explicit is imperative going forward.
2020.emnlp-main.373.txt,2020,7 Discussion and Conclusion,"on most tasks, it performs substantially better than the baseline and similarly to a model that had access to external knowledge resources."
2020.emnlp-main.373.txt,2020,7 Discussion and Conclusion,our code and data is available at github.com/vered1986/self talk.
2020.emnlp-main.373.txt,2020,7 Discussion and Conclusion,our code and data will be made available upon publication.
2020.emnlp-main.373.txt,2020,7 Discussion and Conclusion,"we have listed several shortcomings of using pre-trained lms as knowledge providers: (i) insufficient coverage, (ii) insufficient precision, and (iii) limited reasoning capabilities."
2020.emnlp-main.373.txt,2020,7 Discussion and Conclusion,we hope that our framework will facilitate future research in this area.
2020.emnlp-main.373.txt,2020,7 Discussion and Conclusion,we presented an unsupervised framework for multiple choice commonsense tasks that generates and integrates background knowledge from pre-trained lms.
2020.emnlp-main.374.txt,2020,7 Summary,"our test sets serve as a reliable benchmark for commonsense inference, and more importantly, our dataset is an effective transfer learning resource, improving transformer models’ performance on various tasks in zero- or few-shot settings."
2020.emnlp-main.374.txt,2020,7 Summary,this implies a strong potential for pre-training models to better generalize in low-data scenarios.
2020.emnlp-main.374.txt,2020,7 Summary,we propose 3 goal-step inference tasks using wikihow to complement research of event relation reasoning.
2020.emnlp-main.375.txt,2020,5 Discussion,"a summary of our results can be seen in table 1, with few-shot learning outcomes in colored cells on the left, and the effect of structural supervision on the right."
2020.emnlp-main.375.txt,2020,5 Discussion,"although model accuracy is reduced for tests that assess syntactic invariance, all neural models show at least a moderate ability to generalize across syntactic transformations."
2020.emnlp-main.375.txt,2020,5 Discussion,"because these experiments require careful and robust syntactic analysis of the training data, we evaluated models trained on a relatively small, human-annotated corpus."
2020.emnlp-main.375.txt,2020,5 Discussion,"first, we addressed neural models’ ability to learn nominal number, introducing a novel testing paradigm that leveraged polar questions to assess subject/verb number agreement learning in syntactically transformed settings."
2020.emnlp-main.375.txt,2020,5 Discussion,"furthermore, table 1 shows that syntactic invariance is enhanced in structurally supervised models."
2020.emnlp-main.375.txt,2020,5 Discussion,"however, even with the relatively small models tested here, the results support a growing body of evidence that incremental statistical models of language are able to induce many key features of human linguistic competence."
2020.emnlp-main.375.txt,2020,5 Discussion,in each experiment we assessed the effect of syntactic supervision on learning outcomes by comparing two supervised models to one purely sequence model.
2020.emnlp-main.375.txt,2020,5 Discussion,"in this paper, we have tested the few-shot learning capabilities of neural language models, as well as whether these models can learn grammatical representations that are invariant to syntactic transformation."
2020.emnlp-main.375.txt,2020,5 Discussion,"interestingly, both actionlstm and rnng have access to syntactic information, but the comparison in table 1 indicates that rnng can leverage that information more effectively to produce syntactic invariance."
2020.emnlp-main.375.txt,2020,5 Discussion,it may be that larger amounts of training data support even better few-shot learning and syntactic invariance outcomes.
2020.emnlp-main.375.txt,2020,5 Discussion,scaling these carefully-controlled methods to the larger data setting will be an important next step.
2020.emnlp-main.375.txt,2020,5 Discussion,"second, we turned to neural models’ ability to represent verbal argument structure, developing two novel suites of tests that assessed preference for themes—either realized as direct objects or passive subjects—in both active contexts and passive contexts."
2020.emnlp-main.375.txt,2020,5 Discussion,"the results from experiments that assess syntactic invariance are on the bottom, below the line break."
2020.emnlp-main.375.txt,2020,5 Discussion,"therefore we suggest that rnng’s improved performance does not come from the mere presence of syntactic information in the training and test data, but rather from the fact that it uses syntactic information to structure its computation in a non-sequential way."
2020.emnlp-main.375.txt,2020,5 Discussion,this table makes it clear that all neural models are capable of making syntactic generalizations about a token from minimal exposure during training.
2020.emnlp-main.375.txt,2020,5 Discussion,"thus, robust fewshot generalization is still an important problem in these environments."
2020.emnlp-main.375.txt,2020,5 Discussion,"while sub-word tokenization schemes such as byte-pair encoding (sennrich et al., 2015) have helped reduce the number of individual lexical items that need to learned, they do not completely eliminate the long tail of sub-word units."
2020.emnlp-main.375.txt,2020,5 Discussion,"while the small training data poses some limitations when interpreting the results, it makes them more relevant to lowresource nlp applications and suggests that using structurally supervised models can lead to better generalization in a sparse data environment."
2020.emnlp-main.376.txt,2020,6 Conclusions,"another line of future research is to compare the incremental predictions of neural models to finergrained eye-tracking evidence during sentence processing of double-object sentences (e.g.filik et al., 2004)."
2020.emnlp-main.376.txt,2020,6 Conclusions,"as neural language models become more complex, subtler phenomena like verb bias may yield new insights into how lexical and grammatical representations are jointly learned and successfully integrated for language understanding."
2020.emnlp-main.376.txt,2020,6 Conclusions,further work is needed to more precisely determine the source of the architectural differences we observed.
2020.emnlp-main.376.txt,2020,6 Conclusions,"however, it is also possible that differences are attributable to training data."
2020.emnlp-main.376.txt,2020,6 Conclusions,"in natural languages, speakers routinely select one alternative over others to express their intended message."
2020.emnlp-main.376.txt,2020,6 Conclusions,one possibility is that the transformer’s self-attention mechanism and layer-wise organization improves its ability to represent lexicallyspecific structures.
2020.emnlp-main.376.txt,2020,6 Conclusions,"our new dataset, dais, not only offers a higherresolution window into the richness of human preferences, it also provides a newly powerful benchmark for evaluating and understanding the corresponding sensitivity of language models."
2020.emnlp-main.376.txt,2020,6 Conclusions,"these choices are sensitive to many interacting factors, including the choice of the main verb and the length and definiteness of arguments."
2020.emnlp-main.376.txt,2020,6 Conclusions,we found that transformer architectures corresponded especially well with human verb bias judgments.
2020.emnlp-main.377.txt,2020,9 Conclusions,"comparing different ways of aligning the gaze modality with language production, as we have done in the present work, can shed light on how these processes unfold in human cognition."
2020.emnlp-main.377.txt,2020,9 Conclusions,"despite the challenges mentioned above, our experiments show that a state-of-art image captioning model can be effectively extended to encode cognitive information present in human gaze behaviour."
2020.emnlp-main.377.txt,2020,9 Conclusions,"in the future, our approach and new evaluation measure could be applied to larger eyetracking datasets, such as the english dataset by he et al.(2019)."
2020.emnlp-main.377.txt,2020,9 Conclusions,"moreover, this trend is more pronounced when gaze data is fed sequentially, in line with cognitive theories of sequential cross-modal alignment (e.g., coco and keller, 2012)."
2020.emnlp-main.377.txt,2020,9 Conclusions,"our study shows that better descriptions—i.e., more aligned with speakers’ productions in terms of content and order of words—can be obtained by equipping models with human gaze data."
2020.emnlp-main.377.txt,2020,9 Conclusions,"our study was conducted using the dutch language dataset didec (van miltenburg et al., 2018), which posed the additional challenges of dealing with a small amount of data and a low resource language."
2020.emnlp-main.377.txt,2020,9 Conclusions,"since different eye-tracking datasets tend to make use of different gaze encodings and formats, the amount of pre-processing and analysis steps required to apply our method to other resources was beyond the scope of this paper."
2020.emnlp-main.377.txt,2020,9 Conclusions,"taken together, our results open the door to further work in this direction and support the case for computational approaches leveraging cognitive data."
2020.emnlp-main.377.txt,2020,9 Conclusions,"this type of computational modelling could help, for example, study the interaction between gaze and the production of filler words and repetitions, which we have not investigated in detail."
2020.emnlp-main.377.txt,2020,9 Conclusions,"we believe, however, that there is value in conducting research with languages other than english."
2020.emnlp-main.377.txt,2020,9 Conclusions,we leave testing whether the reported pattern of results holds across different languages to future work.
2020.emnlp-main.377.txt,2020,9 Conclusions,"we tackled the problem of automatically generating an image description from a novel perspective, by modelling the sequential visual processing of a speaker concurrently with language production."
2020.emnlp-main.378.txt,2020,6 Discussion,"experimental results on a wide range of tasks and datasets have demonstrated the strong performance of optimus, including new state-of-the-art for language vaes."
2020.emnlp-main.378.txt,2020,6 Discussion,"first, our pre-trained language vae is still under-trained due to limited compute resource, as the training reconstruction loss can still decrease."
2020.emnlp-main.378.txt,2020,6 Discussion,"hence, we deliberately keep a simple model, believing that the first pretrained big vae model itself and its implications are novel: it helps the community to recognize the importance of dgms in the pre-training era, and revisit dgms to make it more practical."
2020.emnlp-main.378.txt,2020,6 Discussion,"indeed, optimus is uniquely positioned to learn a smooth latent space to organize sentences, which can enable guided language generation compared with gpt-2, and yield better generalization in lowresource language understanding tasks than bert."
2020.emnlp-main.378.txt,2020,6 Discussion,"it introduces a smooth and universal latent space, by combining the advantages of vaes, bert and gpt-2 in one model."
2020.emnlp-main.378.txt,2020,6 Discussion,one future direction is to consider more sophisticated mechanisms to gain stronger controlability over longer sentences while maintaining the compactness of latent representations.
2020.emnlp-main.378.txt,2020,6 Discussion,one may further train the models with higher latent dimension and longer time to fully release the power of pre-trained latent spaces.
2020.emnlp-main.378.txt,2020,6 Discussion,"second, the current model can only control sentences of moderate length."
2020.emnlp-main.378.txt,2020,6 Discussion,that’s why this paper makes a timely contribution to making dgms practical for nlp.
2020.emnlp-main.378.txt,2020,6 Discussion,there are several limitations in current optimus.
2020.emnlp-main.378.txt,2020,6 Discussion,we hope that this paper will help renew interest in dgms for this purpose.
2020.emnlp-main.378.txt,2020,6 Discussion,"we present optimus, a large-scale pre-trained deep latent variable model for natural language."
2020.emnlp-main.378.txt,2020,6 Discussion,"while deep generative models (dgms) such as vaes are theoretically attractive due to its principle nature, it is now rarely used by practitioners in the modern pre-trained language modeling era where bert/gpt dominate with strong empirical performance."
2020.emnlp-main.379.txt,2020,6 Conclusion,"for example, model size is a secondary factor to vocabulary set for token classification task."
2020.emnlp-main.379.txt,2020,6 Conclusion,larger model size does not necessarily translate to better performance on a cross-domain benchmark task.
2020.emnlp-main.379.txt,2020,6 Conclusion,the model size is a secondary factor; larger model size can probably further improve the performance of a a domain- and applicationspecific language model.
2020.emnlp-main.379.txt,2020,6 Conclusion,"this probably indicates that there is no master model that can “do it all”, at least well enough as a targeted one."
2020.emnlp-main.379.txt,2020,6 Conclusion,we find that a language model targeted for a domain and application performs best.
2020.emnlp-main.379.txt,2020,6 Conclusion,we review and test several factors that can affect the performance of domain language models.
2020.emnlp-main.38.txt,2020,7 Conclusion,"this enables better representation learning, learning key hyper-parameters like learning rates, demonstrates data-efficient fine-tuning, and ameliorates metaoverfitting when combined with supervised tasks."
2020.emnlp-main.38.txt,2020,7 Conclusion,"this opens up the possibility of exploring large-scale meta-learning in nlp for various meta problems, including neural architecture search, continual learning, hyperparameter learning, and more."
2020.emnlp-main.38.txt,2020,7 Conclusion,"through extensive experiments, we evaluated the proposed approach on few-shot generalization to novel tasks and domains and found that leveraging unlabelled data has significant benefits to enabling data-efficient generalization."
2020.emnlp-main.38.txt,2020,7 Conclusion,we introduced an approach to leverage unlabeled data to crate meta-learning tasks for nlp.
2020.emnlp-main.380.txt,2020,7 Conclusion,"consequently, we showed that distillation is an effective technique to build much more compact models to use in practical settings."
2020.emnlp-main.380.txt,2020,7 Conclusion,"due to its simplicity, we suggest at least trying it as a baseline when tackling other segmentation problems and datasets."
2020.emnlp-main.380.txt,2020,7 Conclusion,"finally, we performed ablation studies to better understand the role of context and model size."
2020.emnlp-main.380.txt,2020,7 Conclusion,"in future work, we plan to further investigate how different techniques apply to the problem of text segmentation, including data augmentation (wei and zou, 2019; lukasik et al., 2020b) and methods for regularization and mitigating labeling noise (jiang et al., 2020; lukasik et al., 2020a)."
2020.emnlp-main.380.txt,2020,7 Conclusion,"in particular, we found that a cross-segment bert model is extremely competitive with hierarchical models which have been the focus of recent research efforts (chalkidis et al., 2019; zhang et al., 2019)."
2020.emnlp-main.380.txt,2020,7 Conclusion,"in this paper, we introduce three new model architectures for text segmentation tasks: a crosssegment bert model that uses only local context around candidate breaks, as well as two hierarchical models, bert+bi-lstm and hierarchical bert."
2020.emnlp-main.380.txt,2020,7 Conclusion,"moreover, rnns in general may also be useful for very long documents as they are able to deal with very long input sequences."
2020.emnlp-main.380.txt,2020,7 Conclusion,naturally these results do not imply that hierarchical models should be disregarded.
2020.emnlp-main.380.txt,2020,7 Conclusion,our experiments showed that all of our models improve the current state-of-theart.
2020.emnlp-main.380.txt,2020,7 Conclusion,"our experiments suggest that deep transformer encoders are useful for encoding long and complex inputs, e.g., documents for document segmentation applications, while bi-lstms proved useful for discourse segmentation."
2020.emnlp-main.380.txt,2020,7 Conclusion,this is surprising as it suggests that local context is sufficient in many cases.
2020.emnlp-main.380.txt,2020,7 Conclusion,"we evaluated these three models on document and discourse segmentation using three standard datasets, and compared them with other recent neural approaches."
2020.emnlp-main.380.txt,2020,7 Conclusion,we showed they are strong contenders and we are convinced there are applications where local context is not sufficient.
2020.emnlp-main.380.txt,2020,7 Conclusion,we tried several encoders at the upperlevel of the hierarchy.
2020.emnlp-main.381.txt,2020,7 Conclusion,further development of the benchmark includes areas such as evaluation of industrial performance of models on the leaderboard and multilingual diagnostics.
2020.emnlp-main.381.txt,2020,7 Conclusion,in this paper we present the first benchmark on general language understanding evaluation for the russian language.
2020.emnlp-main.381.txt,2020,7 Conclusion,"the benchmark including nine task sets is aimed to test bert-like models for their ability to perform entailment recognition, commonsense reasoning and machine reading while denoising various linguistic features added on the level of semantics, logical and syntactic structure."
2020.emnlp-main.381.txt,2020,7 Conclusion,"we invite developers, researchers, and ai experts to join our project."
2020.emnlp-main.382.txt,2020,5 Conclusions,"in this paper, we show that the performance of zero-shot cross-lingual transfer can be improved by training customized bilingual bert for a given language pair and text domain."
2020.emnlp-main.382.txt,2020,5 Conclusions,"the experiments show that our gigabert model outperforms multilingual bert, xlm-roberta, and the monolingual arabert on ner, pos, arl and re tasks."
2020.emnlp-main.382.txt,2020,5 Conclusions,we additionally studied code-switched pre-training for gigabert and domain-adapted pre-training for xlm-roberta.
2020.emnlp-main.382.txt,2020,5 Conclusions,we also achieve the new state-of-the-art performance fro zero-shot transfer learning from english to arabic.
2020.emnlp-main.382.txt,2020,5 Conclusions,we pre-trained several masked language models (gigaberts) for arabicenglish and conducted a focused study on information extraction tasks in the newswire domain.
2020.emnlp-main.383.txt,2020,6 Discussions and future work,"as far as we know, no data available for misspelled words in the context of sentences, we thus have to generate the evaluation set by ourselves."
2020.emnlp-main.383.txt,2020,6 Discussions and future work,"the data size for yahoo finance and news, while being one of the largest in the context of hatespeech classification, is nevertheless small in the context of language modeling."
2020.emnlp-main.383.txt,2020,6 Discussions and future work,"the main goal here is not to develop a more powerful misspelling corrector, but rather to propose a new and stronger language modeling approach."
2020.emnlp-main.383.txt,2020,6 Discussions and future work,this work targets sentence level language understanding.
2020.emnlp-main.383.txt,2020,6 Discussions and future work,we conducted experiments on three classification tasks.
2020.emnlp-main.383.txt,2020,6 Discussions and future work,we plan to perform large-scale pre-training and evaluation on glue datasets for the comprehensive analysis.
2020.emnlp-main.383.txt,2020,6 Discussions and future work,we thus don’t set up the strict and comprehensive evaluation for apples-to-apples comparison on spelling correction.
2020.emnlp-main.383.txt,2020,6 Discussions and future work,we will continue to explore this line in the future.
2020.emnlp-main.384.txt,2020,6 Conclusion,"finally, we introduced an approach to jointly learn embeddings of numbers and words that preserve numerical properties and evaluated them on a contextual word embedding based model."
2020.emnlp-main.384.txt,2020,6 Conclusion,"in our future work, we would like to extend this idea to unseen numbers in vocabulary as a function of seen ones."
2020.emnlp-main.384.txt,2020,6 Conclusion,"in this work, we methodologically assign and learn embeddings for numbers to reflect their numerical properties."
2020.emnlp-main.384.txt,2020,6 Conclusion,the tests that evaluate the numeral embeddings are fundamentally applicable to all real numbers.
2020.emnlp-main.384.txt,2020,6 Conclusion,we validate our proposed approach with several experiments that test number embeddings.
2020.emnlp-main.385.txt,2020,6 Conclusion,future work might explore combining more expressive flows with discrete latent variables.
2020.emnlp-main.385.txt,2020,6 Conclusion,in this work we carried out a large scale empirical investigation of masked number prediction and numerical anomaly detection in text.
2020.emnlp-main.385.txt,2020,6 Conclusion,"specifically, we found that learning the exponent representation using pretrained transformers that can incorporate left and right contexts, combined with discrete latent variable output distributions, results is the most effective way to model masked number quantities in text."
2020.emnlp-main.385.txt,2020,6 Conclusion,we showed that using the base-10 exponent as a discrete latent variable outperformed all other competitive models.
2020.emnlp-main.386.txt,2020,6 Conclusion,"finally, the models to generate cross-lingual annotations should be thoroughly evaluated in downstream music retrieval and recommendation tasks."
2020.emnlp-main.386.txt,2020,6 Conclusion,"hence, the effectiveness of language-specific concept representations to model the culturally diverse perception could be further probed."
2020.emnlp-main.386.txt,2020,6 Conclusion,"however, inspired by paraphrastic sentence embedding learning, one can also consider the music genre relations as paraphrasing forms with different strengths (wieting et al., 2016)."
2020.emnlp-main.386.txt,2020,6 Conclusion,our work provides a methodological framework to study the annotation behavior across languagebound cultures in other domains too.
2020.emnlp-main.386.txt,2020,6 Conclusion,"then, we combined the semantic representations only with retrofitting."
2020.emnlp-main.386.txt,2020,6 Conclusion,"we have presented an extensive investigation on cross-lingual modeling of music genre annotation, focused on six languages, and two common approaches to semantically represent concepts: ontologies and distributed embeddings9 ."
2020.emnlp-main.387.txt,2020,8.1 Future Work,our overarching goal is to identify when (and how often) are characters being portrayed as targets of risk behaviors—especially in the case where characters are women and minorities.
2020.emnlp-main.387.txt,2020,8.1 Future Work,"the next step towards this goal would be to recognize when characters refer to one another, and how this contributes to the movie-level risk behavior rating."
2020.emnlp-main.387.txt,2020,8.1 Future Work,"we hope this leads to tools that can be helpful during the creative process, rather than after the fact."
2020.emnlp-main.388.txt,2020,6 Conclusion,"experiments validate that the joint morphosyntacticparsing hypothesis, i.e., morphological information can benefit syntactic disambiguation and vice versa (tsarfaty, 2006), holds true for sanskrit."
2020.emnlp-main.388.txt,2020,6 Conclusion,"further, all the joint morphological parsing and dp variants of ebm, we experimented here, result in a superior performance than the pipeline morphological parsing and dp ebm model."
2020.emnlp-main.388.txt,2020,6 Conclusion,"in this work, we proposed mg-ebm, a model for joint morphological parsing and dp in sanskrit."
2020.emnlp-main.388.txt,2020,6 Conclusion,"it extends the ebm framework from krishna et al.(2020) by 1) incorporating a linguistically motivated pruning approach resulting in a substantial reduction in the input search space, and 2) modifying the input graph formation to a multigraph resulting in the use of edmonds-chu-liu algorithm (edmonds, 1967), an exact search algorithm, as inference."
2020.emnlp-main.388.txt,2020,6 Conclusion,"we also establish sota results in sanskrit for dp, both in standalone and joint setting."
2020.emnlp-main.388.txt,2020,6 Conclusion,"we find that the mg-ebm reports state of the art results (sota) for morphological parsing, outperforming standalone morphological parsing models, similar to what is observed for hebrew (more et al., 2019)."
2020.emnlp-main.388.txt,2020,6 Conclusion,while the multigraph formulation is language agnostic the linguistically motivated pruning is rooted on the grammatical tradition of sanskrit.
2020.emnlp-main.389.txt,2020,9 Conclusion,"furthermore, we used the interpretability of constituency tests to highlight and explain the parser’s strengths and shortcomings, like the “[ subject verb ]” and “adverb [ adjective noun ]” misbracketings, revealing potential next steps for improvement."
2020.emnlp-main.389.txt,2020,9 Conclusion,"in this paper, we showed that using constituency tests to parse sentences is an effective approach, achieving strong performance for unsupervised parsing."
2020.emnlp-main.389.txt,2020,9 Conclusion,"therefore, we see parsing via constituency tests as a promising new approach with both strong results and many open questions."
2020.emnlp-main.39.txt,2020,5 Conclusion,"in this work, we identify three principles underlying different lifelong language learning methods and show how to unify them in a meta-lifelong framework."
2020.emnlp-main.39.txt,2020,5 Conclusion,"our analysis also shows that negative transfer is an overlooked factor that could cause sub-optimal performance, and we highlight the importance of balancing the trade-off tween catastrophic forgetting and negative transfer for future work."
2020.emnlp-main.39.txt,2020,5 Conclusion,our experiments demonstrate the effectiveness of the proposed framework on text classification and question answering tasks.
2020.emnlp-main.39.txt,2020,5 Conclusion,these results illustrate that it is possible to achieve efficient lifelong learning by establishing complementary learning systems.
2020.emnlp-main.39.txt,2020,5 Conclusion,we report new state-of-the-art results while using 100 times less memory space.
2020.emnlp-main.390.txt,2020,4 Conclusion,"furthermore, we highlighted an important distinction between dependency trees and arborescences, namely that dependency trees are arborescences subject to a root constraint."
2020.emnlp-main.390.txt,2020,4 Conclusion,"in this paper, we have bridged the gap between the graph-theory and dependency parsing literature."
2020.emnlp-main.390.txt,2020,4 Conclusion,our hope is that this paper will remind future research in dependency parsing to please mind the root.
2020.emnlp-main.390.txt,2020,4 Conclusion,previous work uses inefficient algorithms to enforce this constraint.
2020.emnlp-main.390.txt,2020,4 Conclusion,we presented an efficient o(n 2 ) for finding the maximum arborescence of a graph.
2020.emnlp-main.390.txt,2020,4 Conclusion,we provide a solution which runs in o(n 2 ).
2020.emnlp-main.391.txt,2020,5 Conclusion and Future Work,"in addition, stem and morpheme information can be utilized as additional signals in training."
2020.emnlp-main.391.txt,2020,5 Conclusion and Future Work,"in the future, we plan to enhance the system for handling morphologically complex languages trough unsupervised morphological segmentation."
2020.emnlp-main.391.txt,2020,5 Conclusion and Future Work,one approach is to perform the alignment and projection on the stem and morpheme levels.
2020.emnlp-main.391.txt,2020,5 Conclusion and Future Work,we also showed that the robust selection of training instances and the rich word representation in our neural architecture are more efficient than utilizing some labeled data or external linguistic resources.
2020.emnlp-main.391.txt,2020,5 Conclusion and Future Work,we presented a fully unsupervised cross-lingual pos tagger that does annotation projection by utilizing translation from one or more source languages into the target one.
2020.emnlp-main.391.txt,2020,5 Conclusion and Future Work,"we showed that despite the use of limited and out-of-domain parallel data, our models outperform the state-of-the-art systems."
2020.emnlp-main.392.txt,2020,7 Conclusion,our experiments in supervised parsing verify s-diora improves upon the representational power of diora.
2020.emnlp-main.392.txt,2020,7 Conclusion,"unsupervised fine-tuning with s-diora leads to new impressive results in unsupervised constituency parsing, improving upon the previous state of the art by 2.2  6% f1, depending on the data used."
2020.emnlp-main.392.txt,2020,7 Conclusion,"we introduce s-diora, an extension to diora that enables for easy recovery from local errors and is not subject to wash out from vector averaging."
2020.emnlp-main.393.txt,2020,5 Conclusion,"equipped with these statistics, each user could then estimate the utility that each model provides to them and then re-rank accordingly, effectively creating a custom leaderboard for everyone."
2020.emnlp-main.393.txt,2020,5 Conclusion,"given the diversity of nlp practitioners, there is no one-size-fits-all solution; rather, leaderboards should demand transparency, requiring the reporting of statistics that may be of practical concern."
2020.emnlp-main.393.txt,2020,5 Conclusion,"in this work, we offered several criticisms of leaderboard design in nlp."
2020.emnlp-main.393.txt,2020,5 Conclusion,"we were not the first to criticize nlp leaderboards (rogers, 2019; crane, 2018; linzen, 2020), but we were the first to do so under a framework of utility, which we used to study the divergence between what is incentivized by leaderboards and what is valued by practitioners."
2020.emnlp-main.393.txt,2020,5 Conclusion,"while it has helped create more accurate models, we argued that this has been at the expense of fairness, efficiency, and robustness, among other desiderata."
2020.emnlp-main.394.txt,2020,9 Conclusion,"due to practical considerations, we would suggest using latent representation for data selection when working with a data dependent model such as er or ewc."
2020.emnlp-main.394.txt,2020,9 Conclusion,"in the future work wish to explore more data independent methods such as lrc, for both speed and lack of data dependency, as well as manipulation of the decay w.r.t.what we have discovered from our layer-wise analysis."
2020.emnlp-main.394.txt,2020,9 Conclusion,"in this work, we empirically investigated the existence of catastrophic forgetting in large language model pre-training."
2020.emnlp-main.394.txt,2020,9 Conclusion,we find that training a single model across multiple domains is possible.
2020.emnlp-main.394.txt,2020,9 Conclusion,we further explored constraint and replay based mitigation techniques to close the performance gap between general and domain specific natural language tasks.
2020.emnlp-main.394.txt,2020,9 Conclusion,when no previous data is available lrc provides a simple yet powerful solution for retaining prior domain knowledge.
2020.emnlp-main.395.txt,2020,7 Conclusion,"a direct application of our analysis is efficient feature-based transfer learning from largescale neural language models: i) identifying that most relevant features for a task are contained in layer x reduces the forward-pass to that layer, ii) reducing the feature set decreases the time to train a classifier and also its inference."
2020.emnlp-main.395.txt,2020,7 Conclusion,"our results reinforce previous findings and also illuminate further insights: i) while the information in neural language models is massively distributed, it is possible to extract a small number of features to carry out a downstream nlp task, ii) the number of extracted features varies based on the complexity of the task, iii) the neurons that learn word morphology and lexical semantics are predominantly found in the lower layers of the network, whereas the ones that learn syntax are at the higher layers, with the exception of bert, where neurons were spread across the entire network, iv) closed-class words (for example interjections) are handled using fewer neurons compared to polysemous words (such as nouns and adjectives), v) features in xlnet are more localized towards individual properties as opposed to other architectures where neurons are distributed across many properties."
2020.emnlp-main.395.txt,2020,7 Conclusion,"we analyzed individual neurons across a variety of neural language models using linguistic correlation analysis on the task of predicting core linguistic properties (morphology, syntax and semantics)."
2020.emnlp-main.395.txt,2020,7 Conclusion,we refer interested readers to see our work presented in dalvi et al.(2020) for more details.
2020.emnlp-main.396.txt,2020,8 Conclusion,"first, the approach could apply the same meta-learning approach to other classes of tasks beyond span id."
2020.emnlp-main.396.txt,2020,8 Conclusion,"in this work, we considered the class of span identification tasks."
2020.emnlp-main.396.txt,2020,8 Conclusion,"notably, even though bert-based architectures expectedly perform very well, we find that different variants are optimal for different tasks."
2020.emnlp-main.396.txt,2020,8 Conclusion,our current study could be extended in various directions.
2020.emnlp-main.396.txt,2020,8 Conclusion,"second, a larger range of span type metrics could presumably improve model fit, albeit at the cost of interpretability."
2020.emnlp-main.396.txt,2020,8 Conclusion,"such patterns can be used for manual fine-grained model selection, but our meta-learning model could also be incorporated directly into automl systems."
2020.emnlp-main.396.txt,2020,8 Conclusion,"third, we only predict within-corpus performance, and corpus-level similarity metrics could be added to make predictions about performance in transfer learning."
2020.emnlp-main.396.txt,2020,8 Conclusion,"this class contains a number of widely used nlp tasks, but no comprehensive analysis beyond the level of individual tasks is available."
2020.emnlp-main.396.txt,2020,8 Conclusion,"using a number of ‘key metrics’ that we developed to characterize the span id tasks, a simple linear regression model was able to do so at a reasonable accuracy."
2020.emnlp-main.396.txt,2020,8 Conclusion,"we explain such patterns by interpreting the parameters of the regression model, which yields insights into how the properties of span id tasks interact with properties of neural model architectures."
2020.emnlp-main.396.txt,2020,8 Conclusion,"we took a meta-learning perspective, predicting the performance of various architectures on various span id tasks in an unseen-task setup."
2020.emnlp-main.397.txt,2020,9 Conclusions and future directions,"future work can apply these tests to a broader range of models, and continue to develop controlled tests that target encoding of complex compositional meanings, both for two-word phrases and for larger meaning units."
2020.emnlp-main.397.txt,2020,9 Conclusions and future directions,"teasing apart sensitivity to word content versus phrase meaning composition, we find strong sensitivity across models when it comes to word content encoding, but little evidence of sophisticated phrase composition."
2020.emnlp-main.397.txt,2020,9 Conclusions and future directions,"the observed sensitivity patterns across models, layers, and representation types shed light on practical considerations for extracting phrase representations from these models."
2020.emnlp-main.397.txt,2020,9 Conclusions and future directions,we have systematically investigated the nature of phrase representations in state-of-the-art transformers.
2020.emnlp-main.397.txt,2020,9 Conclusions and future directions,we hope that our findings will stimulate further work on leveraging the power of these generalized transformers while improving their capacity to capture compositional meaning.
2020.emnlp-main.398.txt,2020,8 Conclusion and Future Directions,"based on our analysis, we proposed an efficient transfer learning procedure that directly targets layer-level and neuron-level redundancy to achieve efficiency in feature-based transfer learning."
2020.emnlp-main.398.txt,2020,8 Conclusion and Future Directions,"for example, a high amount of neuron-level redundancy in the same layer suggests that layer-size compression might be more effective in reducing the pretrained model size while preserving oracle performance."
2020.emnlp-main.398.txt,2020,8 Conclusion and Future Directions,"our analysis on general redundancy showed that i) adjacent layers are most redundant in the network with an exception of final layers which are close to the objective function, and ii) up to 85% and 92% neurons are redundant in bert and xlnet respectively."
2020.emnlp-main.398.txt,2020,8 Conclusion and Future Directions,"similarly, our finding that core-linguistic tasks are learned at lower-layers and require a higher number of neurons, while sequence-level tasks are learned at higher-layers and require fewer neurons, suggests pyramid-style architectures that have wide lower layers and compact higher layers and may result in smaller models with performance competitive with large models."
2020.emnlp-main.398.txt,2020,8 Conclusion and Future Directions,we defined a notion of redundancy and analyzed pre-trained models for general redundancy and task-specific redundancy exhibited at layer-level and at individual neuron-level.
2020.emnlp-main.398.txt,2020,8 Conclusion and Future Directions,we found that at least 92% of the neurons are redundant with respect to a downstream task.
2020.emnlp-main.398.txt,2020,8 Conclusion and Future Directions,we further showed that networks exhibit varying amount of task-specific redundancy; higher layerlevel redundancy for core language tasks compared to sequence-level tasks.
2020.emnlp-main.398.txt,2020,8 Conclusion and Future Directions,"while our analysis is helpful in understanding pretrained models, it suggests interesting research directions towards building compact models and models with better architectural choices."
2020.emnlp-main.399.txt,2020,5 Conclusion,"apart from the existing efforts, we propose to model text documents with document-level hypergraphs and further develop a new family of gnn model named hypergat for learning discriminative text representations."
2020.emnlp-main.399.txt,2020,5 Conclusion,"by conducting extensive experiments, the results demonstrate the superiority of the proposed model over the state-of-the-art methods."
2020.emnlp-main.399.txt,2020,5 Conclusion,"in this study, we propose a new graph-based method for solving the problem of inductive text classification."
2020.emnlp-main.399.txt,2020,5 Conclusion,"specifically, our method is able to acquire more expressive power with less computational consumption for text representation learning."
2020.emnlp-main.4.txt,2020,6 Conclusion and Future Work,"besides the future extensions of this approach that we mentioned in our results discussion and error analysis, this work opens several interesting research paths."
2020.emnlp-main.4.txt,2020,6 Conclusion and Future Work,"for our evaluation, we annotated arguments from debatepedia regarding their stance and whether they involve consequences or not."
2020.emnlp-main.4.txt,2020,6 Conclusion and Future Work,"mainly, its good performance on the claims that refer to consequences reinforces our intuition that designing systems tailored for particular argumentation schemes might be a good alternative to topic-specific models."
2020.emnlp-main.4.txt,2020,6 Conclusion and Future Work,our method is comparable to bert while being more robust.
2020.emnlp-main.4.txt,2020,6 Conclusion and Future Work,the method exploits grammatical dependencies and lexicons to identify effect words and their impact.
2020.emnlp-main.4.txt,2020,6 Conclusion and Future Work,the results we obtained are motivating.
2020.emnlp-main.4.txt,2020,6 Conclusion and Future Work,"therefore, we plan to complement this work with approaches for other frequently applied schemes such as arguments by expert opinion and arguments by example."
2020.emnlp-main.4.txt,2020,6 Conclusion and Future Work,we propose a fully unsupervised method to detect the stance of arguments from consequences in online debates.
2020.emnlp-main.40.txt,2020,7 Recommendations and discussion,"currently, it is standard practice to use the en dev accuracy for checkpoint selection in the zero-shot cross-lingual setting."
2020.emnlp-main.40.txt,2020,7 Recommendations and discussion,"however, we showed that using en dev accuracy for checkpoint selection leads to somewhat arbitrary zero-shot results."
2020.emnlp-main.40.txt,2020,7 Recommendations and discussion,note that we do not use target dev for hyperparameter tuning; we are using target dev to avoid selecting bad checkpoints within each fine-tuning experiment.
2020.emnlp-main.40.txt,2020,7 Recommendations and discussion,reporting this upper bound makes results more consistent by avoiding arbitrarily bad checkpoints.
2020.emnlp-main.40.txt,2020,7 Recommendations and discussion,table 5 shows our oracle results on mldoc and xnli.
2020.emnlp-main.40.txt,2020,7 Recommendations and discussion,"the problem arises when the variability is high, which we have seen experimentally (table 2) and in the wild (table 1)."
2020.emnlp-main.40.txt,2020,7 Recommendations and discussion,"therefore, we propose reporting oracle accuracies, where one still fine-tunes using english data, but selects a checkpoint using target dev."
2020.emnlp-main.40.txt,2020,7 Recommendations and discussion,this represents the maximum achievable zero-shot accuracy.
2020.emnlp-main.40.txt,2020,7 Recommendations and discussion,"this would not be a major issue if the variance between different training runs were low; the test performance would, in that case, be consistently mediocre."
2020.emnlp-main.40.txt,2020,7 Recommendations and discussion,"to avoid unexpected variation in future cross-lingual publications, we recommend that authors report oracle accuracies alongside their zero-shot results."
2020.emnlp-main.40.txt,2020,7 Recommendations and discussion,using a poor metric like en dev accuracy to select a model checkpoint is similar to picking a checkpoint at random.
2020.emnlp-main.40.txt,2020,7 Recommendations and discussion,"we showed that independent experiments can report very different results, which prevents us from making meaningful comparisons between different baselines and methods."
2020.emnlp-main.400.txt,2020,8 Conclusion,"furthermore, integrating information from knowledge-bases can further improve the quality of entity representation."
2020.emnlp-main.400.txt,2020,8 Conclusion,"future work can explore representations for rare or unseen entities, as well as developing less memory-intensive ways to learn and integrate entity representations."
2020.emnlp-main.400.txt,2020,8 Conclusion,"our entity representations influence the answer predictions for open-domain question answering system and are of high quality, compared to prior work such as knowbert."
2020.emnlp-main.400.txt,2020,8 Conclusion,"our evaluation shows that eae is effective at capturing declarative knowledge and can be used for a wide variety of tasks – including open domain question answering, relation extraction, entity typing and knowledge probing tasks."
2020.emnlp-main.400.txt,2020,8 Conclusion,"our model learns representations for a pre-fixed vocabulary of entities, and cannot handle unseen entities."
2020.emnlp-main.400.txt,2020,8 Conclusion,"we introduced a new transformer architecture, eae, which learns entity representations from text along with other model parameters."
2020.emnlp-main.402.txt,2020,5 Summary and Outlook,"also, one could analyze the strengths and weaknesses of each objective, and evaluate other variations of these objectives."
2020.emnlp-main.402.txt,2020,5 Summary and Outlook,future work may investigate whether these results translate to other language models besides roberta as well as other training datasets besides winogrande.
2020.emnlp-main.402.txt,2020,5 Summary and Outlook,"in this work, we categorized four existing objectives for pronoun resolution, and compared their performance and seed-wise stability on equal grounds."
2020.emnlp-main.402.txt,2020,5 Summary and Outlook,"on out-of-domain testing, the objective of semantic similarity between the pronoun and each candidate outperforms the other three objectives."
2020.emnlp-main.402.txt,2020,5 Summary and Outlook,"our experiments showed that, on indomain testing, the objective of sequence ranking based on the first token in roberta outperforms the other three objectives, but can exhibit convergence problems."
2020.emnlp-main.403.txt,2020,6 Conclusion,"we also demonstrate the benefit of multi-task learning in bert pre-training, and identify key factors on how to best combine tasks."
2020.emnlp-main.403.txt,2020,6 Conclusion,we hope the insights provided here will help guide the development of better language models in the future.
2020.emnlp-main.403.txt,2020,6 Conclusion,"we investigate and support several reasons why next-sentence prediction is ill-suited for bert pretraining, we provide better inference-based alternatives, and we develop other novel auxiliary tasks based on word importance and soft clustering that provide substantial benefits to bert pre-training."
2020.emnlp-main.404.txt,2020,6 Conclusion and Future Work,"finally, our experimental results have shown very sizable improvements over using state-of-the-art pre-trained transformers."
2020.emnlp-main.404.txt,2020,6 Conclusion and Future Work,"in future work, we plan to explore topic-level bias prediction as well as going beyond left-centerright bias."
2020.emnlp-main.404.txt,2020,6 Conclusion and Future Work,"in particular, we created a new large dataset for this task, which features article-level annotations and is well-balanced across topics and media."
2020.emnlp-main.404.txt,2020,6 Conclusion and Future Work,"last but not least, we plan to experiment with other languages, and to explore to what extent a model for one language is transferable to another one given that the left-center-right division is not universal and does not align perfectly across countries and cultures, even when staying within the western political world."
2020.emnlp-main.404.txt,2020,6 Conclusion and Future Work,"we further proposed an adversarial media adaptation approach, as well as a special triplet loss in order to prevent modeling the source instead of the political bias in the news article, which is a common pitfall for approaches dealing with data that exhibit high correlation between the source of a news article and its class, as is the case with our task here."
2020.emnlp-main.404.txt,2020,6 Conclusion and Future Work,"we further want to develop models that would be able to detect specific fragments in an article where the bias occurs, thus enabling explainability."
2020.emnlp-main.404.txt,2020,6 Conclusion and Future Work,we have explored the task of predicting the leading political ideology of news articles.
2020.emnlp-main.405.txt,2020,5 Conclusion,"in future work, we plan to apply our semantic label smoothing technique to various sequence to sequence problems, including text summarization (zhang et al., 2019) and text segmentation (lukasik et al., 2020b)."
2020.emnlp-main.405.txt,2020,5 Conclusion,our method shows a consistent and significant improvement over state-of-the-art techniques across different datasets.
2020.emnlp-main.405.txt,2020,5 Conclusion,"we achieve this by using a pre-trained model to find semantically similar sequences from the training corpus, and then we use bleu score to rerank the closest targets."
2020.emnlp-main.405.txt,2020,5 Conclusion,we also plan to study the relation between pretraining and data augmentation techniques.
2020.emnlp-main.405.txt,2020,5 Conclusion,"we propose a novel label smoothing approach for sequence to sequence problems that selects a subset of sequences that are not only semantically similar to the target sequences, but are also well formed."
2020.emnlp-main.406.txt,2020,6 Conclusion,"graphical models with task specific inductive bias have been successful for tasks like ner, coreference resolution, relation extraction, and parsing."
2020.emnlp-main.406.txt,2020,6 Conclusion,"in this work, we have proposed neural samplerank (nsr), an efficient algorithm for approximate inference and training for crf models with neural network factors."
2020.emnlp-main.406.txt,2020,6 Conclusion,"nsr is computationally efficient for arbitrarily complex graphical models, thus applicable to a wide range of structured prediction tasks."
2020.emnlp-main.406.txt,2020,6 Conclusion,our proposed method paves the way for new neural graphical models to be designed for these tasks.
2020.emnlp-main.406.txt,2020,6 Conclusion,"with a novel skip-chain crf model that models long range context dependencies, nsr can significantly improve ner performance over the linear-chain crf on multiple datasets."
2020.emnlp-main.407.txt,2020,7 Conclusion,"in controlled experiments, we observe a benefit from cross-document attention on three out of the four document-to-document tasks and two out of two sentence-to-document tasks."
2020.emnlp-main.407.txt,2020,7 Conclusion,"we augment hierarchical attention networks with cross-document attention, allowing their use in document-to-document and sentence-to-document alignment tasks."
2020.emnlp-main.407.txt,2020,7 Conclusion,"we introduce benchmarks, based on existing datasets, to evaluate model performance on such tasks."
2020.emnlp-main.408.txt,2020,6 Conclusions,"due to the constraints it imposes, it cannot represent certain utterances with long-term dependencies, and it is unsuitable for semantic parsing at the session (multi-utterance) level."
2020.emnlp-main.408.txt,2020,6 Conclusions,"further, to advance session-based task-oriented semantic parsing, we release to the public a new dataset of roughly 20k sessions (over 60k utterances)."
2020.emnlp-main.408.txt,2020,6 Conclusions,"to overcome these limitations we propose an extension of this representation, the decoupled representation."
2020.emnlp-main.408.txt,2020,6 Conclusions,we propose a family of sequenceto-sequence models based on the pointergenerator architecture – using both recurrent neural network and transformer architectures – and show that they achieve top performance on several semantic parsing tasks.
2020.emnlp-main.408.txt,2020,6 Conclusions,we started this paper by exploring the limitations of compositional intent-slot representations for semantic parsing.
2020.emnlp-main.409.txt,2020,6 Conclusion,"from the ranking results of two different probings, we show a list of interesting observations to provide model selection guidelines and shed light on future research towards a more advanced language modeling learning for dialogue applications."
2020.emnlp-main.409.txt,2020,6 Conclusion,"we investigate representations from pre-trained language models for task-oriented dialogue tasks, including domain identification, intent detection, slot tagging, and dialogue act prediction."
2020.emnlp-main.409.txt,2020,6 Conclusion,we use a supervised classifier probe and a proposed unsupervised mutual information probe.
2020.emnlp-main.41.txt,2020,6 Conclusion,"future works include using other multilingual pretraining models such as xlm-roberta (conneau et al., 2019) for a more accurate model and distilmbert (sanh et al., 2019) for a more compact model."
2020.emnlp-main.41.txt,2020,6 Conclusion,one significant merit of framing word alignment as a squad-style span prediction task is that we can easily import the progress of the latest question answering and multilingual language modeling technologies.
2020.emnlp-main.41.txt,2020,6 Conclusion,our cross-language span prediction method can be used for any alignments between two sequences.
2020.emnlp-main.41.txt,2020,6 Conclusion,"we have already applied it to bilingual sentence alignment (chousa et al., 2020) and we plan to extend it to other related problems."
2020.emnlp-main.41.txt,2020,6 Conclusion,"we made supervised word alignment practical because our method does not require any bitexts for pretraining, and it can be fine-tuned to a specific guideline using fewer gold word alignments."
2020.emnlp-main.41.txt,2020,6 Conclusion,"we presented a novel supervised word alignment method using the multilingual bert, which requires as few as 300 training sentences to outperform previous supervised and unsupervised methods."
2020.emnlp-main.410.txt,2020,7 Conclusion,experiments show that multilingual bert brings substantial improvements on multilingual training and cross-lingual transfer tasks.
2020.emnlp-main.410.txt,2020,7 Conclusion,"furthermore, our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time."
2020.emnlp-main.410.txt,2020,7 Conclusion,we further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.
2020.emnlp-main.410.txt,2020,7 Conclusion,"we introduce multiatis++, a multilingual nlu corpus that extends the multilingual atis corpus to nine languages across four language families."
2020.emnlp-main.410.txt,2020,7 Conclusion,we release our multiatis++ corpus to facilitate future research on cross-lingual nlu to bridge the gap between cross-lingual transfer and supervised methods.
2020.emnlp-main.410.txt,2020,7 Conclusion,"we use our corpus to evaluate various cross-lingual transfer methods including the use of multilingual bert encoder, machine translation, and label projection."
2020.emnlp-main.411.txt,2020,7 Conclusion,experimental results show superior performance of our method on a large-scale multi-domain intent detection dataset with oos.
2020.emnlp-main.411.txt,2020,7 Conclusion,future work includes its cross-lingual transfer and cross-dataset (or cross-task) generalization.
2020.emnlp-main.411.txt,2020,7 Conclusion,"in this paper, we have presented a simple yet efficient nearest-neighbor classification model to detect user intents and oos intents."
2020.emnlp-main.411.txt,2020,7 Conclusion,it includes paired encoding and discriminative training to model relations between the input and example utterances.
2020.emnlp-main.411.txt,2020,7 Conclusion,"moreover, a seamless transfer from nli and a joint approach with fast retrieval are designed to improve the performance in terms of the accuracy and inference speed."
2020.emnlp-main.412.txt,2020,4 Conclusion,"in the future, we plan to explore maskaugment for other tasks in nlp domain."
2020.emnlp-main.412.txt,2020,4 Conclusion,our empirical results show that the proposed training scheme leads to significant improvements on domain adaptation for dialog act taggers.
2020.emnlp-main.412.txt,2020,4 Conclusion,"to combat this shortcoming, we investigate domain adaptation through the proposed unsupervised teacher-student training that leverages the maskaugment method for data augmentation."
2020.emnlp-main.412.txt,2020,4 Conclusion,we study cross-domain generalization of pretrained language models for da tagging.
2020.emnlp-main.412.txt,2020,4 Conclusion,"while the fine-tuned bert model performs well on indomain da tagging, its cross-domain generalization is still not satisfactory."
2020.emnlp-main.413.txt,2020,7 Conclusion,"even when compared with supervised models trained with 500 spis, a 10-fold data increase, our best performing model remains competitive, and outperforms a state-of-the-art lstmbased pointer generator network (rongali et al., 2020)."
2020.emnlp-main.413.txt,2020,7 Conclusion,"first of all, we argue the encoder-only pre-trained representations used in existing work are not ideal for the seq2seq model employed in task-oriented semantic parsing, and instead propose to use bart, a pre-trained model with an encoder-decoder architecture."
2020.emnlp-main.413.txt,2020,7 Conclusion,"in particular, we focus on the 25 spis setting to investigate whether a model can effectively adapt to new domains with a very limited amount of training data."
2020.emnlp-main.413.txt,2020,7 Conclusion,"in this work, we study the low-resource domain scaling problem for task-oriented semantic parsing."
2020.emnlp-main.413.txt,2020,7 Conclusion,"last but not least, we collect the topv2 dataset, a large-scale multi-domain task-oriented semantic parsing dataset with 8 domains and more than 180k annotated samples to evaluate our models, which we release to the research community."
2020.emnlp-main.413.txt,2020,7 Conclusion,"more importantly, we adopt optimization-based meta-learning to improve the model’s generalization to new target domains with very few training samples."
2020.emnlp-main.413.txt,2020,7 Conclusion,our approach distinguishes itself from previous methods on two fronts.
2020.emnlp-main.413.txt,2020,7 Conclusion,our experiments show that our proposed method significantly outperforms all competing methods and achieves the best performance in the lowresource setting.
2020.emnlp-main.414.txt,2020,6 Conclusion,"in this paper, we introduce a new task of message rephrasing in task-oriented dialog."
2020.emnlp-main.414.txt,2020,6 Conclusion,"we also show that by distilling the pre-trained model into a much smaller lstm seq2seq model with copy pointer, we can significantly improve the lstm seq2seq model’s language model capability while still keeping its accurate copying."
2020.emnlp-main.414.txt,2020,6 Conclusion,"we release a dataset, mcr, for this task and propose a new model (bart with copy)."
2020.emnlp-main.414.txt,2020,6 Conclusion,we show that adding an explicit loss to a pre-trained generative model during fine-tuning can improve the copying performance without hurting its generation power.
2020.emnlp-main.415.txt,2020,6 Conclusions,experimental results on transferring simplification knowledge from english to german showed that our model was able to produce significantly better german simplifications than unsupervised and pivot-based approaches.
2020.emnlp-main.415.txt,2020,6 Conclusions,"in addition to zero-shot simplification, we showed that our model can generate german simplifications given english input, without any additional training."
2020.emnlp-main.415.txt,2020,6 Conclusions,"in the future, we plan to explore this approach with other language pairs and other generation tasks."
2020.emnlp-main.415.txt,2020,6 Conclusions,in this paper we developed a general approach for transferring generation data from high- to lowresource languages.
2020.emnlp-main.416.txt,2020,6 Discussion,"as our generation model may not (yet) match the quality of human rewrites, there can be a potential trade-off."
2020.emnlp-main.416.txt,2020,6 Discussion,"as we are provided with more opportunities to interact with people across cultural and language barriers, the risk of misunderstandings in communication also grows (chang et al., 2020a)."
2020.emnlp-main.416.txt,2020,6 Discussion,bridging the gaps in perceptions.
2020.emnlp-main.416.txt,2020,6 Discussion,"first, we focus exclusively on a gradable stylistic aspect that is mostly decoupled from the content (kang and hovy, 2019), reducing the complexity required from both the perception and the generation models."
2020.emnlp-main.416.txt,2020,6 Discussion,"furthermore, to move towards more practical applications, we would also need to conduct communication-based evaluation (newman et al., 2020) in addition to annotating individual utterances."
2020.emnlp-main.416.txt,2020,6 Discussion,future work can aim to propose more general formulations that encapsulate more properties of the circumstance.forms of assistance.
2020.emnlp-main.416.txt,2020,6 Discussion,"future work can consider adapting experiment designs from prior work (gao et al., 2015; hohenstein and jung, 2018) to establish the impact of offering such intention-preserving paraphrases in real conversations, potentially by considering downstream outcomes."
2020.emnlp-main.416.txt,2020,6 Discussion,"future work may consider more complex stylistic aspects and strategies that are more tied to the content, such as switching from active to passive voice."
2020.emnlp-main.416.txt,2020,6 Discussion,"future work may consider more comprehensive modeling of how people form politeness perceptions or obtain more reliable causal estimates for strategy strength (wang and culotta, 2019).task formulation."
2020.emnlp-main.416.txt,2020,6 Discussion,"given the fine-grained and individualized nature of the task, using humans to ascertain the politeness of the outputs would require an extensive and relatively complex annotation setup (e.g., collecting finegrained labels from annotators with known backgrounds for training and evaluating individualized perception models)."
2020.emnlp-main.416.txt,2020,6 Discussion,"hence, while we work towards providing fully automated suggestions, we might also want to utilize the language ability humans possess and consider assistance approaches in the form of interpretable (partial) suggestions.evaluation."
2020.emnlp-main.416.txt,2020,6 Discussion,"in our experiments, we have relied exclusively on model predictions to estimate the level of misalignment in politeness perceptions."
2020.emnlp-main.416.txt,2020,6 Discussion,"in this work, we motivate and formulate the task of circumstance-sensitive intention-preserving paraphrasing and develop a methodology that shows promise in helping people more accurately communicate politeness under different communication settings."
2020.emnlp-main.416.txt,2020,6 Discussion,"other examples commonly observed in communication include, but are not limited to, formality (rao and tetreault, 2018) and emotional tones (chhaya et al., 2018; raji and de melo, 2020)."
2020.emnlp-main.416.txt,2020,6 Discussion,"second, we consider binary channel constraints, but in reality, the channel behavior is often less clear-cut."
2020.emnlp-main.416.txt,2020,6 Discussion,the results and limitations of our method open up several natural directions for future work.modeling politeness perceptions.
2020.emnlp-main.416.txt,2020,6 Discussion,"thus, it is all the more important to develop tools to mitigate such risk and help foster mutual understandings."
2020.emnlp-main.416.txt,2020,6 Discussion,we make several simplifying assumptions in our task formulation.
2020.emnlp-main.416.txt,2020,6 Discussion,we use a simple linear regression model to approximate how people internally interpret politeness and restrict our attention to only the set of local politeness strategies.
2020.emnlp-main.416.txt,2020,6 Discussion,"while an entirely automatic assistance option may put the least cognitive load on the user, it may not produce the most natural and effective rewrite, which may be possible if humans are more involved."
2020.emnlp-main.416.txt,2020,6 Discussion,"while we focus on politeness strategies, they are not the only circumstance-sensitive linguistic signals that may be lost or altered during transmission, nor the only type that are subject to individual or culturalspecific perceptions."
2020.emnlp-main.416.txt,2020,6 Discussion,"while we have focused on offering paraphrasing options as the form of assistance, it is not the only type of assistance possible."
2020.emnlp-main.417.txt,2020,5 Conclusion and Discussion,"as for future directions, one natural extension is how we can automatically identify those attributes."
2020.emnlp-main.417.txt,2020,5 Conclusion and Discussion,"in this paper, we propose a controlled adversarial text generation model that can generate more diverse and fluent adversarial texts."
2020.emnlp-main.417.txt,2020,5 Conclusion and Discussion,one benefit of our framework is that it is flexible enough to incorporate multiple task-irrelevant attributes and our optimization allows the model to figure out which attributes are more susceptible to attacks.
2020.emnlp-main.417.txt,2020,5 Conclusion and Discussion,our current generation is controlled by a few pre-specified attributes that are label-invariant by definition.
2020.emnlp-main.417.txt,2020,5 Conclusion and Discussion,the hope is that the model can pick up attributes implicitly and automatically identify regions where the task model is not robust on.
2020.emnlp-main.417.txt,2020,5 Conclusion and Discussion,the number of different values the attributes can take determines the space where we search for adversarial examples.
2020.emnlp-main.417.txt,2020,5 Conclusion and Discussion,we argue that our model creates more natural and meaningful attacks to real-world tasks by demonstrating our attacks are more robust against model re-training and across model architectures.
2020.emnlp-main.418.txt,2020,5 Discussion,"alternatively, it may be useful to explore generation in a non left-to-right order to improve the efficiency of inference."
2020.emnlp-main.418.txt,2020,5 Discussion,"another line of future work is to extend our model to sequence rewriting tasks, such as machine translation post-editing, that do not have existing error-tag dictionaries."
2020.emnlp-main.418.txt,2020,5 Discussion,"even though our approach is open-vocabulary, future work will explore task specific restrictions."
2020.emnlp-main.418.txt,2020,5 Discussion,"for example, in a model for dialog applications, we may want to restrict the set of response tokens to a predefined list."
2020.emnlp-main.418.txt,2020,5 Discussion,"however, we do not make any claim that seq2edits can provide insights into the internal mechanics of the neural model."
2020.emnlp-main.418.txt,2020,5 Discussion,"notably, the model uses a tailored architecture (figure 2) that would require some engineering effort to implement efficiently."
2020.emnlp-main.418.txt,2020,5 Discussion,our model can predict labels that explain each edit to improve the interpretability for the end-user.
2020.emnlp-main.418.txt,2020,5 Discussion,"second, the output of the model tends to be less fluent than a regular full sequence model, as can be seen from the examples in table 19."
2020.emnlp-main.418.txt,2020,5 Discussion,the underlying neural model in seq2edits is as much of a black-box as a regular full sequence model.
2020.emnlp-main.418.txt,2020,5 Discussion,this is not an issue for localized edit tasks such as text normalization but may be a drawback for tasks involving substantial rewrites (e.g.gec for non-native speakers).
2020.emnlp-main.418.txt,2020,5 Discussion,this research would require induction of error tag inventories using either linguistic insights or unsupervised methods.
2020.emnlp-main.418.txt,2020,5 Discussion,we have presented a neural model that represents sequence transduction using span-based edit operations.
2020.emnlp-main.418.txt,2020,5 Discussion,"we reported competitive results on five different nlp problems, improving the state of the art on text normalization, sentence splitting, and the jfleg test set for grammatical error correction."
2020.emnlp-main.418.txt,2020,5 Discussion,we showed that our approach is 2.0-5.2 times faster than a full sequence model for grammatical error correction.
2020.emnlp-main.418.txt,2020,5 Discussion,"while our model is advantageous in terms of speed and explainability, it does have some weaknesses."
2020.emnlp-main.419.txt,2020,10 Conclusion,"additionally, we identify limitations to this ability, specifically in the small data, random permutation setting, and will focus on this going forward."
2020.emnlp-main.419.txt,2020,10 Conclusion,our findings support the importance of aligned linearization and phrase training for improving model control.
2020.emnlp-main.419.txt,2020,10 Conclusion,we present an empirical study on the effects of linearization order and phrase based data augmentation on controllable mr-to-text generation.
2020.emnlp-main.42.txt,2020,6 Conclusion,both methods induce alignments at the step when the to-be-aligned target token is the decoder input rather than the decoder output as in previous work.
2020.emnlp-main.42.txt,2020,6 Conclusion,experiments on three public alignment datasets and a downstream task prove the effectiveness of these two methods.
2020.emnlp-main.42.txt,2020,6 Conclusion,"in this paper, we have presented two novel methods shift-att and shift-aet for word alignment induction."
2020.emnlp-main.42.txt,2020,6 Conclusion,"shiftaet further extends transformer with an additional alignment module, which consistently outperforms prior neural aligners and giza++, without influencing the translation quality."
2020.emnlp-main.42.txt,2020,6 Conclusion,"to the best of our knowledge, it reaches the new state-of-theart performance among all neural alignment induction methods."
2020.emnlp-main.42.txt,2020,6 Conclusion,we leave it for future work to extend our study to more downstream tasks and systems.
2020.emnlp-main.420.txt,2020,5 Conclusion,appendix c shows examples generated by blm along with their trajectories.
2020.emnlp-main.420.txt,2020,5 Conclusion,"blm has plenty of future applications, including template filling, information fusion, assisting human writing, etc."
2020.emnlp-main.420.txt,2020,5 Conclusion,"depending on the application, we could also train the model to generate in specific orders by placing higher weights on the corresponding trajectories."
2020.emnlp-main.420.txt,2020,5 Conclusion,"given partially specified text with one or more blanks, blm will fill in the blanks with a variable number of tokens consistent with the context."
2020.emnlp-main.420.txt,2020,5 Conclusion,"in this paper, we proposed the blank language model for flexible text generation."
2020.emnlp-main.420.txt,2020,5 Conclusion,"in this way, we encourage the model to realize a sentence equally well in all orders, which is suitable for filling arbitrary blanks."
2020.emnlp-main.420.txt,2020,5 Conclusion,"moreover, we can extend our formulation to a conditional generative model."
2020.emnlp-main.420.txt,2020,5 Conclusion,"such models can be used in machine translation to support editing and refining translation, as well as in dialogue systems to compose a complete sentence with given elements."
2020.emnlp-main.420.txt,2020,5 Conclusion,the action of blm consists of selecting a blank and replacing it with a word and possibly adjoining blanks.
2020.emnlp-main.420.txt,2020,5 Conclusion,"we demonstrate the effectiveness of our model on various text rewriting tasks, including text infilling, ancient text restoration and style transfer."
2020.emnlp-main.420.txt,2020,5 Conclusion,we train blm by optimizing a lower bound on the marginal data likelihood that sums over all possible generation trajectories.
2020.emnlp-main.420.txt,2020,5 Conclusion,"while we proposed blm for language generation, it would also be interesting to compare the representations learned by blm with those produced by other pre-training methods."
2020.emnlp-main.421.txt,2020,5 Conclusion,"cod3s leads to more diverse outputs in a multi-target generation task in a controllable and interpretable manner, suggesting the potential of semantically guided diverse decoding for a variety of text generation tasks in the future."
2020.emnlp-main.421.txt,2020,5 Conclusion,we design sentence lsh signatures that encode bitwise the semantic similarity of underlying statements; conditioning generation on different signatures yields outputs that are semantically heterogeneous.
2020.emnlp-main.421.txt,2020,5 Conclusion,"we have outlined cod3s, a method for producing semantically diverse statements in open-ended generation tasks."
2020.emnlp-main.422.txt,2020,7 Future Work,"also, deciding agreement based on only pos tags is insufficient to capture all phenomena that may influence agreement for e.g.mass nouns such as ‘rice’ do not follow the standard number agreement rules in english."
2020.emnlp-main.422.txt,2020,7 Future Work,consider this simple english example: “john and mary love their dog”.
2020.emnlp-main.422.txt,2020,7 Future Work,"under both ud and sud formalisms, the coordinating conjunction “and"" is a dependent, hence the verb will not agree with either of the (singular) nouns (“john"" or “mary"")."
2020.emnlp-main.422.txt,2020,7 Future Work,"we also plan to expand our methodology for extracting grammar rules from raw text to other aspects of morphosyntax, such as argument structure and word order phenomena."
2020.emnlp-main.422.txt,2020,7 Future Work,we leave a more expressive model and evaluation on more languages as future work.
2020.emnlp-main.422.txt,2020,7 Future Work,"while we have demonstrated that our approach is effective in extracting a first-pass set of agreement rules directly from raw text, it focuses only on agreement between a pair of words and hence might fail to capture more complex phenomena that require broader context or operate at the phrase level."
2020.emnlp-main.423.txt,2020,7 Conclusion,"for large training sets, our systems performed close to the state of the art."
2020.emnlp-main.423.txt,2020,7 Conclusion,"however, we find a large gap between the emulated and the real low-resource scenarios: while accuracy is above 50% for all high-resource languages even with reduced amounts of training data, for popoluca and tepehua, our best model only obtains 37.4% and 28.4% accuracy, respectively."
2020.emnlp-main.423.txt,2020,7 Conclusion,"in emulated low-resource settings with up to 600 training examples, our best proposed model outperformed all baselines in all but one setting."
2020.emnlp-main.423.txt,2020,7 Conclusion,"we evaluated the performance of both models against multiple state-of-the-art baselines on five languages of different morphological typology: english, german, indonesian, tepehua, and popoluca."
2020.emnlp-main.423.txt,2020,7 Conclusion,we obtained a similar picture for experiments on the truly lowresource languages popoluca and tepehua: our best approach outperformed the best baseline by 11.4% and 6.5% accuracy.
2020.emnlp-main.423.txt,2020,7 Conclusion,we proposed two new models for the task of canonical segmentation in the low-resource setting: an lstm pointer-generator model and a neural transducer trained with imitation learning.
2020.emnlp-main.424.txt,2020,6 Conclusion,an elicitation step with native speakers could be added to strategically augment data.
2020.emnlp-main.424.txt,2020,6 Conclusion,another approach would be to compare improvements between manual-only cleaning and cleaning done by a linguist working with someone who can write scripts to automatically correct repeated patterns of noise.
2020.emnlp-main.424.txt,2020,6 Conclusion,better techniques for further cleaning might be useful since accuracy seems to have close related to data quality.
2020.emnlp-main.424.txt,2020,6 Conclusion,"better use of experts’ time might involve identification of lemmata that could be used to train a lemma-to-form model, rather than the form-to-form mapping used here."
2020.emnlp-main.424.txt,2020,6 Conclusion,"finally, since field data does not often include pos annotation, we investigated the usefulness of pos tags for morphological reinflection and find that, surprisingly and in contrast to common assumptions, they are not beneficial to recent state-of-the-art systems."
2020.emnlp-main.424.txt,2020,6 Conclusion,"for example, the generated inflected forms could be used for automated glossing of raw text."
2020.emnlp-main.424.txt,2020,6 Conclusion,"however, at some point more cleaning will return less improvement."
2020.emnlp-main.424.txt,2020,6 Conclusion,igt2p also has implications for the documentation of endangered languages and addressing digital inequity of speakers of marginalized languages.
2020.emnlp-main.424.txt,2020,6 Conclusion,igt2p could speed the discovery and description of a language’s entire morphological structure.
2020.emnlp-main.424.txt,2020,6 Conclusion,igt2p results could serve as to prompt speakers for forms that are rare in natural speech.
2020.emnlp-main.424.txt,2020,6 Conclusion,in languages with the noisiest data performance is improved even further by data augmentation techniques.
2020.emnlp-main.424.txt,2020,6 Conclusion,it could be integrated into linguists’ workflow in order to improve the study of inflection and increase igt data.
2020.emnlp-main.424.txt,2020,6 Conclusion,it might also be integrated into linguistic software such as flex.
2020.emnlp-main.424.txt,2020,6 Conclusion,our experiments show that igt2p is a promising method for creating new morphological resources in a wide range of low-resource languages.
2020.emnlp-main.424.txt,2020,6 Conclusion,the inherent noisiness in igt and other linguistic field data can be overcome with limited input from domain experts.
2020.emnlp-main.424.txt,2020,6 Conclusion,there is room for future improvement.
2020.emnlp-main.424.txt,2020,6 Conclusion,this is a significant contribution considering the extensive effort—on the order of months and years—to produce the curated structured data normally used to train nlp models.
2020.emnlp-main.424.txt,2020,6 Conclusion,this is a useful discovery for researchers who wish to optimize their inflection systems.
2020.emnlp-main.424.txt,2020,6 Conclusion,this would integrate well with linguists’ workflow.
2020.emnlp-main.424.txt,2020,6 Conclusion,"upper bounds could be established by comparing results on languages with gold standard inflection tables, although polysynthetic languages like arapaho would make this difficult since their tables do not always include noun incorporation."
2020.emnlp-main.424.txt,2020,6 Conclusion,we experimented with neural models that have been used for morphological reinflection and new preprocessing steps as baselines for the task.
2020.emnlp-main.424.txt,2020,6 Conclusion,we investigated the effect of manual cleaning on model performance and showed that even very limited cleaning effort (2-7 hours) drastically improves results.
2020.emnlp-main.424.txt,2020,6 Conclusion,"we proposed a new morphological generation task called igt2p, which aims to learn inflectional paradigmatic patterns from interlinear gloss texts (igt) produced in linguistics fieldwork."
2020.emnlp-main.424.txt,2020,6 Conclusion,"with sufficient igt annotations, igt2p obtains reasonable performance from noisy data."
2020.emnlp-main.425.txt,2020,9 Conclusion,"moreover, the identified components are found to be important to mental health platforms and helpful in improving peer-to-peer support through model-based feedback."
2020.emnlp-main.425.txt,2020,9 Conclusion,our computational approach effectively identifies empathy with underlying rationales.
2020.emnlp-main.425.txt,2020,9 Conclusion,"we developed a new framework, dataset, and computational method for understanding expressed empathy in text-based, asynchronous conversations on mental health platforms."
2020.emnlp-main.426.txt,2020,6 Conclusion,experiments demonstrated that our approach improved both content quality and emotion faithfulness of the generated stories.
2020.emnlp-main.426.txt,2020,6 Conclusion,"for example, comet is a discourse-agnostic model, and separately extracting emotional reactions for each sentence may fail to maintain emotional consistency with the rest of the narrative."
2020.emnlp-main.426.txt,2020,6 Conclusion,"in general, such models can have educational applications by enabling children to explore creative writing at an early age and addressing the literary learning needs of learners with disabilities."
2020.emnlp-main.426.txt,2020,6 Conclusion,"in this paper, we proposed the emotion-aware storytelling task for modeling the emotion arc of the protagonist."
2020.emnlp-main.426.txt,2020,6 Conclusion,"in this work, we focused only on the protagonist, but future works can explore modeling motivations, goals, achievements, and emotional trajectory of all characters."
2020.emnlp-main.426.txt,2020,6 Conclusion,"our approach is general and provides a blueprint for similar works going forward and can be used outside emotion-aware storytelling, e.g., for generating other emotional content or text with other attributes or properties."
2020.emnlp-main.426.txt,2020,6 Conclusion,such sources of errors and biases need further systematic investigation.
2020.emnlp-main.426.txt,2020,6 Conclusion,the various assumptions and choices made in this paper and the specific characteristics of the dataset we chose can introduce biases and errors.
2020.emnlp-main.426.txt,2020,6 Conclusion,this paper is a step towards future research directions on planning emotional trajectory while generating stories.
2020.emnlp-main.426.txt,2020,6 Conclusion,"to this goal, we designed two emotionconsistency rewards using a commonsense transformer and an emotion classifier."
2020.emnlp-main.426.txt,2020,6 Conclusion,"using commonsense inferences about the effect of the events on emotional states of various characters of the story has the potential of generating more coherent, realistic, and engaging stories."
2020.emnlp-main.426.txt,2020,6 Conclusion,"we also presented two case studies, which show interesting use cases of our model."
2020.emnlp-main.427.txt,2020,8 Conclusion,future work needs to look into how question and reply context can improve automatic identification of advice.
2020.emnlp-main.427.txt,2020,8 Conclusion,we find that advice language consists of various pragmatic strategies and discourse structures.
2020.emnlp-main.427.txt,2020,8 Conclusion,"we find that fine-tuned bert discovers certain surface-level features indicative of advice, but struggles to disambiguate instances of implicit advice conveyed through personal narrative."
2020.emnlp-main.427.txt,2020,8 Conclusion,"we introduce a new dataset on advice given on the online platform reddit, specifically r/askparents and r/needadvice that differ in audience and level of moderation."
2020.emnlp-main.428.txt,2020,10 Conclusion,"by developing a high-quality dataset of questions rated for their intimacy and a corresponding model that closely correlates with human judgments, we study 80.5m questions across social media, books, and movies to reveal how individuals shape and react to their social setting through selecting the intimacy of their language."
2020.emnlp-main.428.txt,2020,10 Conclusion,"in four studies, we show that the intimacy of language is not only a personal choice, where people may use different linguistic strategies for the expressions of intimacy but reflects constraints from social norms, including gender and social distance."
2020.emnlp-main.428.txt,2020,10 Conclusion,our study provides strong evidence for existing findings in social psychology and also enriches the study of computational sociolinguistics in nlp community.
2020.emnlp-main.428.txt,2020,10 Conclusion,this paper represents a step towards a full understanding of the social information in language through new data and models for studying intimacy in language.
2020.emnlp-main.429.txt,2020,9 Conclusion,"for example, blog sites, which target researchers, use more jargon and focus on the main findings of a paper, while magazine articles, which target a much broader audience of readers, tell more stories and use more active voice."
2020.emnlp-main.429.txt,2020,9 Conclusion,"in this paper we compile writing strategies from theory and practical advice, collect a large corpus and annotate a subset of it to measure strategies’ use."
2020.emnlp-main.429.txt,2020,9 Conclusion,our findings also suggest that science newspaper articles judged by experts to have higher quality use more metaphorical language and tell more stories.
2020.emnlp-main.429.txt,2020,9 Conclusion,"we expect that our strategy formulations, classifiers, annotations and dataset will enable nlppowered tools to support effective science communication for different audiences."
2020.emnlp-main.429.txt,2020,9 Conclusion,we observe how strategies covary with intended audience.
2020.emnlp-main.43.txt,2020,6 Conclusion and Future Work,"besides, we propose our chr-en and en-chr baselines, including both smt and nmt models, using both supervised and semi-supervised methods, and exploring both transfer learning and multilingual joint training methods with 4 other languages."
2020.emnlp-main.43.txt,2020,6 Conclusion and Future Work,"experiments show that smt is significantly better and nmt under out-of-domain condition while nmt is better for in-domain evaluation; and the semi-supervised learning, transfer learning, and multilingual joint training can improve simply supervised baselines."
2020.emnlp-main.43.txt,2020,6 Conclusion and Future Work,"in this paper, we make our effort to revitalize the cherokee language by introducing a clean cherokee-english parallel dataset, chren, with 14k sentence pairs; and 5k cherokee monolingual sentences."
2020.emnlp-main.43.txt,2020,6 Conclusion and Future Work,it not only can be another resource for low-resource machine translation research but also will help to attract attention from the nlp community to save this dying language.
2020.emnlp-main.43.txt,2020,6 Conclusion and Future Work,our future work involves converting the monolingual data to parallel and collecting more data from the news domain.
2020.emnlp-main.43.txt,2020,6 Conclusion and Future Work,"overall, our best models achieve 15.8/12.7 bleu for in-domain chren/en-chr translations and 6.5/5.0 bleu for outof-domain chr-en/en-chr translations."
2020.emnlp-main.43.txt,2020,6 Conclusion and Future Work,we hope these diverse baselines will serve as useful strong starting points for future work by the community.
2020.emnlp-main.430.txt,2020,9 Conclusions,evaluation showed that the acquired subevent pairs are of high quality (90.1% of accuracy) and cover a wide range of event types.
2020.emnlp-main.430.txt,2020,9 Conclusions,"in the future, we would like to explore uses of the subevent knowledge base for other eventoriented applications such as event tracking."
2020.emnlp-main.430.txt,2020,9 Conclusions,we have presented a novel weakly supervised learning framework for acquiring subevent knowledge and built the first large scale subevent knowledge base containing 239k subevent tuples.
2020.emnlp-main.430.txt,2020,9 Conclusions,"we performed extensive evaluations showing that the harvested subevent knowledge not only improves subevent relation extraction, but also improve a wide range of nlp tasks such as causal and temporal relation extraction and discourse parsing."
2020.emnlp-main.431.txt,2020,8 Conclusion,beesl is broadly applicable to event extraction and other tasks that can be recast as sequence labeling.
2020.emnlp-main.431.txt,2020,8 Conclusion,"beesl is fast, and achieves stateof-the-art performance on the genia 2011 event extraction benchmark without the need of external tools for features and resources such as knowledge bases."
2020.emnlp-main.431.txt,2020,8 Conclusion,our analysis shows that beesl works very well across event types.
2020.emnlp-main.431.txt,2020,8 Conclusion,"the system’s strength comes from the joint multi-task modeling paired with multi-label decoding, which aids interdependencies between the tasks and is superior to alternative decoders based on strong contextualized bert embeddings."
2020.emnlp-main.431.txt,2020,8 Conclusion,"this paper proposes beesl, a new end-to-end biomedical event extraction system which is both efficient and accurate."
2020.emnlp-main.431.txt,2020,8 Conclusion,"we release the code freely, to foster research on using beesl for other nlp tasks as well, e.g., enhanced dependency parsing, fine-grained named entity recognition, and semantic parsing."
2020.emnlp-main.432.txt,2020,8 Conclusion,"in this paper, we proposed a temporal annotation scheme called temporal dependency graphs which extend previous research on temporal dependency trees."
2020.emnlp-main.432.txt,2020,8 Conclusion,"the temporal dependency graphs, like temporal dependency trees, draw inspiration from previous research on narrative times and temporal anaphora, allow a good trade-off between completeness and practicality in temporal annotation."
2020.emnlp-main.432.txt,2020,8 Conclusion,"we also demonstrated the utility of the data set by training a neural ranking model on this data set, and the data set is publicly available."
2020.emnlp-main.432.txt,2020,8 Conclusion,we proposed a crowdsourcing strategy and demonstrated its feasibility with a comparative analysis of the quality of the annotation.
2020.emnlp-main.433.txt,2020,7 Conclusion,"in the future, we plan to extend our annotation to include event arguments and other properties of events."
2020.emnlp-main.433.txt,2020,7 Conclusion,our dataset is manually annotated for 30 event types and provides sufficient data to develop deep learning models for this task.
2020.emnlp-main.433.txt,2020,7 Conclusion,our experiments also suggest that document-level information is necessary to perform ed for cybersecurity domain.
2020.emnlp-main.433.txt,2020,7 Conclusion,"we extensively evaluate state-of-the-art models for ed on the proposed dataset, showing that the performance of these models is still much worse than the human performance."
2020.emnlp-main.433.txt,2020,7 Conclusion,we present a new dataset cyseced for event detection in the cybersecurity domain.
2020.emnlp-main.434.txt,2020,7 Conclusion,charm differs from prior work by its zero-shot ability to predict attribute values that are not present in the training samples at all.
2020.emnlp-main.434.txt,2020,7 Conclusion,"we demonstrated the viability of charm for inferring users’ unseen attribute values by comprehensive experiments with reddit conversations, leveraging document collections from wikipedia and web search results for charm’s retrieval component."
2020.emnlp-main.434.txt,2020,7 Conclusion,we presented the charm method for inferring personal traits from conversations.
2020.emnlp-main.435.txt,2020,5 Conclusion,"in the future, we plan to apply the proposed model for the related tasks and other settings of ed, including new type extension (nguyen et al., 2016b; lai and nguyen, 2019), and few-shot learning (lai et al., 2020a,b)."
2020.emnlp-main.435.txt,2020,5 Conclusion,the proposed model achieves state-of-the-art performance on two ed datasets.
2020.emnlp-main.435.txt,2020,5 Conclusion,"we demonstrate how gating mechanisms, gate diversity, and graph structure can be used to integrating syntactic information and improve the hidden vectors for ed models."
2020.emnlp-main.436.txt,2020,5 Conclusions and Future Work,"for the future, instead of using the roberta baseline model for the self-training experiments, we could run several iterations by retraining on the data produced by our best self-trained model(s); this could be a good avenue for further improvements."
2020.emnlp-main.436.txt,2020,5 Conclusions and Future Work,"in addition we plan to extend our work by moving to other languages beyond english (we currently have not tried this due to lack of data) using cross-lingual models, (subburathinam et al., 2019), applying other architectures like cnns (nguyen and grishman, 2015), incorporating tree structure in our models (miwa and bansal, 2016) and/or by handling jointly performing event recognition and temporal ordering (li and ji, 2014; katiyar and cardie, 2017)."
2020.emnlp-main.436.txt,2020,5 Conclusions and Future Work,"it establishes a new state-ofthe-art on the task through pretraining, leveraging complementary tasks through smtl and selftraining techniques."
2020.emnlp-main.436.txt,2020,5 Conclusions and Future Work,this paper presents neural architectures for ordering events in time.
2020.emnlp-main.437.txt,2020,4 Conclusion,"finally, the tasks we used in this paper mainly measure “trivia”-style knowledge."
2020.emnlp-main.437.txt,2020,4 Conclusion,"in contrast, our model distributes knowledge in its parameters in an inexplicable way and hallucinates realistic-looking answers when it is unsure."
2020.emnlp-main.437.txt,2020,4 Conclusion,"in this short paper, we have shown that large language models pre-trained on unstructured text can attain competitive results on open-domain question answering benchmarks without any access to external knowledge."
2020.emnlp-main.437.txt,2020,4 Conclusion,"second, “open-book” models typically provide some indication of what information they accessed when answering a question."
2020.emnlp-main.437.txt,2020,4 Conclusion,"third, the maximum-likelihood objective used to train our model provides no guarantees as to whether a model will learn a fact or not."
2020.emnlp-main.437.txt,2020,4 Conclusion,this can provide a useful form of interpretability.
2020.emnlp-main.437.txt,2020,4 Conclusion,this makes it difficult to ensure that the model obtains specific knowledge over the course of pre-training and prevents us from explicitly updating or removing knowledge from a pre-trained model.
2020.emnlp-main.437.txt,2020,4 Conclusion,"this model size can be prohibitively expensive in resource-constrained settings, prompting future work on more efficient language models."
2020.emnlp-main.437.txt,2020,4 Conclusion,"this suggests a fundamentally different approach to designing question answering systems, motivating many threads for future work: first, we obtained state-of-the-art results only with the largest models which had around 11 billion parameters."
2020.emnlp-main.437.txt,2020,4 Conclusion,"we are therefore interested in measuring performance on question answering tasks that require reasoning capabilities such as drop (dua et al., 2019)."
2020.emnlp-main.438.txt,2020,7 Conclusion and Future Work,"in future work, we plan to extend the dataset with more questions, more subjects, and more languages."
2020.emnlp-main.438.txt,2020,7 Conclusion and Future Work,we further plan to develop new models to address the specific challenges we identified.
2020.emnlp-main.438.txt,2020,7 Conclusion and Future Work,we further proposed new fine-grained evaluation that allows precise comparison across different languages and school subjects.
2020.emnlp-main.438.txt,2020,7 Conclusion and Future Work,we hope that our publicly available data and code will enable work on multilingual models that can reason about question answering in the challenging science domain.
2020.emnlp-main.438.txt,2020,7 Conclusion and Future Work,"we performed various experiments and analysis with pre-trained multilingual models (xlm-r, mbert), and we demonstrated that there is a need for better reasoning and knowledge transfer in order to solve some of the questions from eχαµs."
2020.emnlp-main.438.txt,2020,7 Conclusion and Future Work,"we presented eχαµs, a new challenging crosslingual and multilingual benchmark for science qa in 16 languages and 24 subjects from high school examinations."
2020.emnlp-main.439.txt,2020,5 Conclusions,improving lm-score-based filtering is a future direction of our work.
2020.emnlp-main.439.txt,2020,5 Conclusions,it would be interesting to explore how one can adapt the generative models to the type of target domain questions.
2020.emnlp-main.439.txt,2020,5 Conclusions,"our experiments showed that by proper decoding, significant improvements in domain adaptation of rc models can be achieved."
2020.emnlp-main.439.txt,2020,5 Conclusions,"we concluded that using lm filtering improves the quality of synthetic question-answer pairs; however, there is still a gap with round-trip filtering with some of the target domains."
2020.emnlp-main.439.txt,2020,5 Conclusions,we presented a novel end-to-end approach to generate question-answer pairs by using a single transformer-based model.
2020.emnlp-main.439.txt,2020,5 Conclusions,"while we were able to generate diverse, high quality and challenging synthetic samples on the target domains, the types of the questions produced still were limited to those of squad, since the generative models were trained on squad."
2020.emnlp-main.44.txt,2020,8 Limitations and Future Work,"additionally, we assume that all comments are directly addressed to ow, but some comments may be addressed to other commenters."
2020.emnlp-main.44.txt,2020,8 Limitations and Future Work,"finally, our assumption that human judgements are not reliable for this task makes evaluation difficult, and this task would benefit from the development of additional evaluation metrics."
2020.emnlp-main.44.txt,2020,8 Limitations and Future Work,"first, while our results in §5 suggest that adversarial training does help reduce the influence of latent confounding variables, the analysis in §6 suggests that there is scope for improvement."
2020.emnlp-main.44.txt,2020,8 Limitations and Future Work,"furthermore, while we focus on some confounds in the data, there may be additional ones that our model does not account for, such as the impact of videos, photos, or links shared with o txt."
2020.emnlp-main.44.txt,2020,8 Limitations and Future Work,"nevertheless, our work stands without this perspective: biased comments are harmful to the recipient, regardless of who wrote them."
2020.emnlp-main.44.txt,2020,8 Limitations and Future Work,"notably, we focus on the perspective of ow and examine what bias social media users may be exposed to, i.e.what comments men and women might expect to receive in response to their posts."
2020.emnlp-main.44.txt,2020,8 Limitations and Future Work,"similarly, while our model uses o txt for propensity matching in the training data, thus encouraging the model to encode indicators of bias, a model to classify comments as biased or unbiased should also incorporate o txt when assessing test data."
2020.emnlp-main.44.txt,2020,8 Limitations and Future Work,there are additional avenues for future work beyond our proposed framework.
2020.emnlp-main.44.txt,2020,8 Limitations and Future Work,"this perspective would require controlling for traits of the commenter, such as gender, age, and occupation."
2020.emnlp-main.44.txt,2020,8 Limitations and Future Work,"we do not examine why comments addressed toward men and women may differ, whether because the same commenters write different comments to men and women, or because men and women attract comments from different types of people."
2020.emnlp-main.44.txt,2020,8 Limitations and Future Work,we first focus on limitations within our proposed framework.
2020.emnlp-main.44.txt,2020,8 Limitations and Future Work,"while our work serves as an initial approach toward unsupervised detection of comment-level gender bias, we identify several limitations and areas for future work."
2020.emnlp-main.440.txt,2020,6 Conclusion,"in future work, we aim to extend our approach to more domains and explore more generalizable approaches for unsupervised domain adaptation."
2020.emnlp-main.440.txt,2020,6 Conclusion,"in our preliminary experiments, we empirically show considerable improvements in performance over a standard roberta-large lm on multiple tasks."
2020.emnlp-main.440.txt,2020,6 Conclusion,"in this work, we show that it is beneficial to extend the vocabulary of the lm while fine-tuning it on the target domain language."
2020.emnlp-main.440.txt,2020,6 Conclusion,we empirically demonstrate that structure in the unsupervised domain data can be used to formulate auxillary pre-training tasks that can help downstream low-resource tasks like question answering and document ranking.
2020.emnlp-main.440.txt,2020,6 Conclusion,we show that extending the pre-training with task-specific synthetic data is an effective domain adaptation strategy.
2020.emnlp-main.441.txt,2020,7 Conclusion,"additional effort will also be needed in activities like the development of large diagram datasets, including the semantic annotation of diagram constituents and connectors, and annotating diagram questions with the reasoning and knowledge types required to answer them."
2020.emnlp-main.441.txt,2020,7 Conclusion,"isaaq demonstrates that it is possible to master the grand ai challenge of machine textbook understanding based on modern methods for language and visual understanding, with modest infrastructure requirements."
2020.emnlp-main.441.txt,2020,7 Conclusion,"key to this success are transformers, butd attention, pre-training on related datasets, and the selection of complementary background information to train and ensemble different solvers."
2020.emnlp-main.441.txt,2020,7 Conclusion,our approach allowed overcoming critical challenges like the complexity and relatively small size of the tqa dataset or the scarcity of large diagram datasets.
2020.emnlp-main.441.txt,2020,7 Conclusion,"still, further research is necessary to keep pushing the boundaries of textbook understanding, e.g.by charting and expanding the reasoning skills of transformers, making model outcomes more interpretable by humans, and further exploiting diagrams."
2020.emnlp-main.441.txt,2020,7 Conclusion,"this paper reports on isaaq, the first system to achieve accuracies above 80%, 70% and 55% on tqa true/false, text and diagram mc questions."
2020.emnlp-main.442.txt,2020,7 Conclusion,in this paper we investigate subjectivity in qa by leveraging end-to-end architectures.
2020.emnlp-main.442.txt,2020,7 Conclusion,"the dataset allows i) evaluation and development of architectures for subjective content, and ii) investigation of subjectivity and its interactions in broad and diverse contexts."
2020.emnlp-main.442.txt,2020,7 Conclusion,"we further implement a subjectivity-aware model and evaluate it, along with 4 strong baseline models."
2020.emnlp-main.442.txt,2020,7 Conclusion,"we hope this dataset opens new avenues for research on querying subjective content, and into subjectivity in general."
2020.emnlp-main.442.txt,2020,7 Conclusion,"we release subjqa, a question-answering dataset which contains subjectivity labels for both questions and answers."
2020.emnlp-main.443.txt,2020,6 Conclusion,the task is important because missing captions is a major issue for mobile accessibility and addressing the issue can improve accessibility and empower language-based mobile interaction in general.
2020.emnlp-main.443.txt,2020,6 Conclusion,"the winner configuration—a transformer structural encoder coupled with a resnet cnn—can generate semantically meaningful captions for sparsely labeled elements on the screen, which shows the feasibility of this task and opportunities for future research."
2020.emnlp-main.443.txt,2020,6 Conclusion,we created a large-scale dataset for this novel task by asking human annotators to create widget captions for a mobile ui corpus via crowdsourcing.
2020.emnlp-main.443.txt,2020,6 Conclusion,we experimented with a set of models based on the dataset.
2020.emnlp-main.443.txt,2020,6 Conclusion,we formulate widget captioning as a multimodal captioning task where both structural and image input are available.
2020.emnlp-main.443.txt,2020,6 Conclusion,"we present widget captioning, a novel task for automatically generating language description for ui elements."
2020.emnlp-main.444.txt,2020,5 Conclusion,"in the experiments, our proposed approach steadily surpassed other methods by a large margin."
2020.emnlp-main.444.txt,2020,5 Conclusion,"in this paper, we study the multimodal selfsupervised learning for unsupervised nli."
2020.emnlp-main.444.txt,2020,5 Conclusion,the major flaw of previous multimodal ssl methods is that they use a joint encoder for representing the cross-modal correlations.
2020.emnlp-main.444.txt,2020,5 Conclusion,this prevents us from integrating visual knowledge into the text encoder.
2020.emnlp-main.444.txt,2020,5 Conclusion,"we propose the multimodal aligned contrastive decoupled learning (macd), which learns to represent visual knowledge while using only texts as inputs."
2020.emnlp-main.445.txt,2020,5 Conclusion,"on silent emg recordings from closed vocabulary data our speech outputs achieve high intelligibility, with a 3.6% transcription word error rate and relative error reduction of 95% from our baseline."
2020.emnlp-main.445.txt,2020,5 Conclusion,"our results show that digital voicing of silent speech, while still challenging in open domain settings, shows promise as an achievable technology."
2020.emnlp-main.445.txt,2020,5 Conclusion,"we also significantly improve intelligibility in an open vocabulary condition, with a relative error reduction over 20%."
2020.emnlp-main.445.txt,2020,5 Conclusion,we hope that our public release of data will encourage others to further improve models for this task.6 acknowledgments this material is based upon work supported by the national science foundation under grant no.1618460.
2020.emnlp-main.445.txt,2020,5 Conclusion,we show that it is important to account for differences in emg signals between silent and vocalized speaking modes and demonstrate an effective method of doing so.
2020.emnlp-main.446.txt,2020,6 Conclusion,addressing potential ethical concerns the goal of our work is to help to make nlp models more robust.
2020.emnlp-main.446.txt,2020,6 Conclusion,"for instance, it may shine a negative light on production systems (by exposing their egregious errors) or provide useful information to adversaries."
2020.emnlp-main.446.txt,2020,6 Conclusion,"furthermore, we contacted the three companies (google, bing, and systran) to report the vulnerabilities."
2020.emnlp-main.446.txt,2020,6 Conclusion,"however, in the long-term these developments have led to better mt systems (johnson, 2020)."
2020.emnlp-main.446.txt,2020,6 Conclusion,"however, in the long-term, our work can help to improve mt systems."
2020.emnlp-main.446.txt,2020,6 Conclusion,"in performing our work, we used the acm code of ethics as a guide to minimize harm and ensure our research was ethically sound."
2020.emnlp-main.446.txt,2020,6 Conclusion,"model stealing is not merely hypothetical: companies have been caught stealing models in nlp settings, e.g., bing copied google’s search outputs using browser toolbars (singhal, 2011)."
2020.emnlp-main.446.txt,2020,6 Conclusion,"moving forward, we hope to improve and deploy defenses against adversarial attacks in nlp, and more broadly, we hope to make security and privacy a more prominent focus of nlp research."
2020.emnlp-main.446.txt,2020,6 Conclusion,providing long-term benefit our work has the potential to cause negative short-term impacts.
2020.emnlp-main.446.txt,2020,6 Conclusion,"then, after discovering models have unintended flaws, we take action to secure them by developing a novel defense algorithm."
2020.emnlp-main.446.txt,2020,6 Conclusion,this line of work was published (and received awards and was heavily publicized) in *acl conferences and led to short-term damage due to the bad publicity it caused.
2020.emnlp-main.446.txt,2020,6 Conclusion,"to do so, we first explore new model vulnerabilities (i.e., threat modeling in computer security)."
2020.emnlp-main.446.txt,2020,6 Conclusion,"to draw an analogy, we compare our work to the initial papers which show that production mt systems are systematically biased against women (alvarez-melis and jaakkola, 2017; stanovsky et al., 2019)."
2020.emnlp-main.446.txt,2020,6 Conclusion,we also provided these companies with our proposed defense.
2020.emnlp-main.446.txt,2020,6 Conclusion,we demonstrate that model stealing and adversarial examples are practical concerns for production nlp systems.
2020.emnlp-main.446.txt,2020,6 Conclusion,"we minimize real-world harm we minimized harm by (1) not causing damage to any real users, (2) designing our attacks to be somewhat ludicrous rather than expose any real-world failure modes, and (3) deleting the data and models from our imitation experiments."
2020.emnlp-main.447.txt,2020,6 Conclusion,"despite being a crude approximation to compositional phenomena in language, we found seqmix to be effective on three different sequenceto-sequence tasks, including the challenging scan dataset which is designed to test for compositional generalization."
2020.emnlp-main.447.txt,2020,6 Conclusion,"seqmix is efficient and easy to implement, and as a secondary contribution, we provide a framework that unifies several data augmentation strategies for compositionality, which naturally suggests avenue for future research (e.g., a relaxed variant of geca)."
2020.emnlp-main.447.txt,2020,6 Conclusion,"this paper presents seqmix, a simple data augmentation strategy for sequence-to-sequence applications."
2020.emnlp-main.448.txt,2020,7 Conclusion,"using a sequence completion task, we confirmed that empirical inconsistency occurs in practice, and that each method prevents inconsistency while maintaining the quality of generated sequences."
2020.emnlp-main.448.txt,2020,7 Conclusion,"we extended the notion of consistency of a recurrent language model put forward by chen et al.(2017) to incorporate a decoding algorithm, and used it to analyze the discrepancy between a model and the distribution induced by a decoding algorithm."
2020.emnlp-main.448.txt,2020,7 Conclusion,"we proved that incomplete decoding is inconsistent, and proposed two methods to prevent this: consistent decoding and the self-terminating recurrent language model."
2020.emnlp-main.448.txt,2020,7 Conclusion,"we suspect the absence of decoding in maximum likelihood estimation as a cause behind this inconsistency, and suggest investigating sequence-level learning as an alternative."
2020.emnlp-main.449.txt,2020,9 Conclusion,"this approach achieve substantial improvement using high-order energy terms, especially in noisy data conditions, while having same decoding speed as simple local classifiers."
2020.emnlp-main.449.txt,2020,9 Conclusion,we explore arbitrary-order models with different neural parameterizations on sequence labeling tasks via energy-based inference networks.
2020.emnlp-main.45.txt,2020,9 Conclusion,"applying those models, we examine the dynamics of distress and condolence, showing that not all distress is treated equally online, and there exist regular structures within condolence."
2020.emnlp-main.45.txt,2020,9 Conclusion,"distress is an omnipresent part of life, and individuals turn to their social circle and social platforms for support when experiencing it."
2020.emnlp-main.45.txt,2020,9 Conclusion,"in this paper, we have developed new computational models for recognizing distress, condolences to that distress, and empathy within condolence."
2020.emnlp-main.45.txt,2020,9 Conclusion,models and reproducible code are available at https://blablablab.si.umich.edu/projects/condolence/ and data is made available upon request.
2020.emnlp-main.45.txt,2020,9 Conclusion,"our results have important implications for (i) individuals by providing concrete suggestions of how to express one’s distress to make it more likely to receive support, (ii) site operators by allowing them to observe the emotional health and responsiveness of their community, potentially reaching out to underserved individuals who have yet to receive support, and (iii) the general public for authoring more effective supportive messages."
2020.emnlp-main.45.txt,2020,9 Conclusion,"through analyzing millions of condolence responses, we test what makes for effective condolence online, showing that while some features predicted from observation studies hold true online, e.g., increasing person-centeredness of the message (high and dillard, 2012), distressed individuals did not find empathetic comments more helpful, suggesting different goals from online support."
2020.emnlp-main.450.txt,2020,7 Conclusion,"finally, snoek et al.(2019) find that deep ensembles can significantly improve outof-domain performance over single models, and we are interested in exploring whether our distillation techniques retain these benefits."
2020.emnlp-main.450.txt,2020,7 Conclusion,"furthermore, we show that calibration of the single student models can be further improved by other, orthogonal, re-calibration methods."
2020.emnlp-main.450.txt,2020,7 Conclusion,"in future work, we are interested in exploring nat using distilled ensembles with truncated distributions, and assessing how improved calibration impacts non-sequential decoding performance."
2020.emnlp-main.450.txt,2020,7 Conclusion,"most knowledge distillation for nat is performed at the sequence level, and ignores distributional information at the token level."
2020.emnlp-main.450.txt,2020,7 Conclusion,"non-autoregressive translation (nat) is an active area of research for nmt (gu et al., 2017; stern et al., 2019; ghazvininejad et al., 2019)."
2020.emnlp-main.450.txt,2020,7 Conclusion,our key finding is that ensemble distillation may be used to produce a single model that preserves much of the improved calibration and performance of the ensemble while being as efficient as single models at inference time.
2020.emnlp-main.450.txt,2020,7 Conclusion,summary of contributions.
2020.emnlp-main.450.txt,2020,7 Conclusion,"we present a systematic study of the effect of ensembles on the calibration of structured prediction models, which consistently improve calibration and performance relative to single models."
2020.emnlp-main.450.txt,2020,7 Conclusion,we release all code and scripts.17 open research questions.
2020.emnlp-main.451.txt,2020,5 Conclusion,"compared with dependency tree gcn baselines, the model does not introduce additional model parameters, yet significantly enhances the representation power."
2020.emnlp-main.451.txt,2020,5 Conclusion,experiments on five benchmarks show effectiveness of our model.
2020.emnlp-main.451.txt,2020,5 Conclusion,"to our knowledge, we are the first to investigate latent structures for aspect level sentiment classification, achieving the best-reported accuracy on five benchmark datasets."
2020.emnlp-main.451.txt,2020,5 Conclusion,"we considered latent graph structures for aspect sentiment classification by investigating a variety of neural networks for structure induction, and novel gated mechanisms to dynamically combine different structures."
2020.emnlp-main.452.txt,2020,8 Conclusion,"by combining both the affective event classifier’s prediction and the polarities of coreferent sentiment expressions, our algorithm substantially improved upon the supervised learning results."
2020.emnlp-main.452.txt,2020,8 Conclusion,"in this work, we proposed a bert-based supervised classifier for affective event recognition and showed that it substantially outperforms a large affective event knowledge base."
2020.emnlp-main.452.txt,2020,8 Conclusion,the resulting classification model is substantially more effective for affective event recognition than previous methods.
2020.emnlp-main.452.txt,2020,8 Conclusion,we also believe that the general idea behind our discourse-enhanced self-training approach could be useful for many other types of problems where additional information can be extracted from larger contexts to serve as a secondary signal to help confirm or disconfirm a classifier’s predictions.
2020.emnlp-main.452.txt,2020,8 Conclusion,we also designed a novel discourse-enhanced self-training algorithm to leverage unlabeled data iteratively.
2020.emnlp-main.453.txt,2020,6 Conclusion,extensive experiments are conducted to verify both quantitatively and qualitatively the effectiveness of the proposed model.
2020.emnlp-main.453.txt,2020,6 Conclusion,the attention weights control the information flow between deep neural networks and the maxsat layer which automatically weigh the relevance of each rule towards the data given.
2020.emnlp-main.453.txt,2020,6 Conclusion,"to adapt logic knowledge with noisy real applications, we introduce an attention mechanism to generate an adaptive weight corresponding to each data instance for each logic rule."
2020.emnlp-main.453.txt,2020,6 Conclusion,"we propose a novel joint model that inherits the advantage of high-level feature learning, logic reasoning and structured learning which can be trained smoothly in an end-to-end manner."
2020.emnlp-main.454.txt,2020,7 Conclusion,"finally, we developed an unsupervised technique to extract tags that identify complementary attributes of movies from user reviews."
2020.emnlp-main.454.txt,2020,7 Conclusion,"in this paper, we focused on characterizing stories by generating tags from synopses and reviews."
2020.emnlp-main.454.txt,2020,7 Conclusion,our model learns to predict tags by identifying salient sentences and words from synopses and reviews.
2020.emnlp-main.454.txt,2020,7 Conclusion,"we believe that this coarse story understanding approach can be extended to longer stories, i.e., entire books, and are currently exploring this path in our ongoing work."
2020.emnlp-main.454.txt,2020,7 Conclusion,we demonstrated that exploiting user reviews can further improve performance and experimented with several methods for combining user reviews and synopses.
2020.emnlp-main.454.txt,2020,7 Conclusion,we modeled the problem from the perspective of multiple instance learning and developed a multi-view architecture.
2020.emnlp-main.455.txt,2020,7 Conclusion,"a more general, bite-like algorithm should enable further gains on morphologically rich languages."
2020.emnlp-main.455.txt,2020,7 Conclusion,"as a first step, we propose to evaluate an encoding scheme’s efficacy by measuring its vocabulary coverage and symbol complexity (which may have interesting connections to informationtheoretic limits (ziv and lempel, 1978))."
2020.emnlp-main.455.txt,2020,7 Conclusion,"by encoding raw text into operable symbols, we can improve the generalization and adversarial robustness of resulting systems."
2020.emnlp-main.455.txt,2020,7 Conclusion,"finally, given the effectiveness of the common task framework for spurring progress in nlp (varshney et al., 2019), we hope to do the same for tokenization."
2020.emnlp-main.455.txt,2020,7 Conclusion,"hence, we guide the data-driven tokenizer by incorporating linguistic information to learn a more efficient vocabulary and generate symbol sequences that increase the network’s robustness to inflectional variation."
2020.emnlp-main.455.txt,2020,7 Conclusion,"since dialectal data is often scarce or even nonexistent, an nlp system’s ability to generalize across dialects in a zero-shot manner is crucial for it to work well for diverse linguistic communities."
2020.emnlp-main.455.txt,2020,7 Conclusion,"the tokenization stage of the modern deep learning nlp pipeline has not received as much attention as the modeling stage, with researchers often defaulting to common subword tokenizers like bpe.we can do better."
2020.emnlp-main.455.txt,2020,7 Conclusion,this improves its generalization to l2 and world englishes without requiring explicit training on such data.
2020.emnlp-main.455.txt,2020,7 Conclusion,we have already shown that base-inflection encoding helps a data-driven tokenizer use its limited vocabulary more efficiently by reducing its symbol complexity when the combination is trained from scratch.
2020.emnlp-main.456.txt,2020,8 Conclusion,"a related challenge is east and southeast asian numeral classifier systems, which associate nouns with classifiers based largely on the semantic properties of the nouns (kuo and sera, 2009; zhan and levy, 2018; liu et al., 2019)."
2020.emnlp-main.456.txt,2020,8 Conclusion,"one could apply them more broadly other aspects of the lexicon, e.g.to indo-european verb classes, bantu noun classes, or diachronic time slices of a single language’s gender system, data permitting."
2020.emnlp-main.456.txt,2020,8 Conclusion,separate indo-european branches are no more similar than chance.
2020.emnlp-main.456.txt,2020,8 Conclusion,"they display more idiolectal variation, and often more than one classifier can accompany a given noun (hu, 1993), unlike for gender (where this is rare)."
2020.emnlp-main.456.txt,2020,8 Conclusion,we emphasize that our methods are not specifically tailored to gender systems.
2020.emnlp-main.456.txt,2020,8 Conclusion,"we have presented a clean method for comparing grammatical gender systems across languages: by defining gender classes extensionally, we reduced the problem to cluster evaluation from community detection."
2020.emnlp-main.456.txt,2020,8 Conclusion,"we note that we could further extend our measures to fuzzy partitions, which remain less explored in community detection, but are a promising avenue for future work."
2020.emnlp-main.456.txt,2020,8 Conclusion,"we validate three metrics by recovering known phylogenic relationships in our languages, with measurable success."
2020.emnlp-main.457.txt,2020,5 Discussion,"although this question can be answered relatively easily by simply looking at the overall results of different systems in diverse data sets (sec.2), we take a step further to how to make choices of them (bert v.s elmo, lstm v.s cnn)) by conducting dataset bias-aware aided-diagnosis (sec.3.6)."
2020.emnlp-main.457.txt,2020,5 Discussion,"based on this, we design a measure to quantify the distance between different datasets, which correlates well with the cross-dataset performance and can be used for multi-source transfer learning, help us avoid the negative transfer."
2020.emnlp-main.457.txt,2020,5 Discussion,"beyond giving this unsurprising answer, we present an interpretable evaluation method to help us diagnose the weaknesses of existing top-performing systems and relative merits between two systems."
2020.emnlp-main.457.txt,2020,5 Discussion,can we design a measure to quantify the discrepancies among different criteria?yes.
2020.emnlp-main.457.txt,2020,5 Discussion,"for example, we find even top-scoring bert-based models still cannot deal with the words with low label consistency or long words well, and bert is inferior to elmo as an encoder in dealing with long sentences."
2020.emnlp-main.457.txt,2020,5 Discussion,is there a one-size-fits-all system?
2020.emnlp-main.457.txt,2020,5 Discussion,no (bestperforming systems on different datasets frequently involve diverse neural architectures).
2020.emnlp-main.457.txt,2020,5 Discussion,we first verify that the label consistency of words and word length have a more consistent impact on cws performance.
2020.emnlp-main.457.txt,2020,5 Discussion,we summarize the main observations from our experiments and try to give preliminary answers to our proposed research questions: does existing excellent performance imply a perfect cws system?no.
2020.emnlp-main.458.txt,2020,9 Conclusion,"by combining the two methods, we obtain 89% accuracy, which significantly exceeds that of prior work."
2020.emnlp-main.458.txt,2020,9 Conclusion,"in the presence of one-to-many mappings between pinyin and characters, the mapping accuracy is severely downgraded, leaving open an opportunity to design more robust unsupervised vector mapping systems."
2020.emnlp-main.458.txt,2020,9 Conclusion,"the em method achieves a test-set accuracy of 71%, while the vector-based method achieves 81%."
2020.emnlp-main.458.txt,2020,9 Conclusion,we also demonstrate that current methods for unsupervised matching of vector spaces are sensitive to the structure of the spaces.
2020.emnlp-main.458.txt,2020,9 Conclusion,"we implement and evaluate techniques to pronounce chinese text in mandarin, without the use of a pronunciation dictionary or parallel resource."
2020.emnlp-main.459.txt,2020,6 Conclusion,experimental results show that our model can alleviate the sparsity of kgs and achieve better results than previous multihop reasoning models.
2020.emnlp-main.459.txt,2020,6 Conclusion,"however, there is still some noise in the additional actions given by our model."
2020.emnlp-main.459.txt,2020,6 Conclusion,"in experiments, we verify the effectiveness of dackgr on five datasets."
2020.emnlp-main.459.txt,2020,6 Conclusion,"in future work, we plan to improve the quality of the additional actions."
2020.emnlp-main.459.txt,2020,6 Conclusion,"in order to solve this problem, we propose a reinforcement learning model named dackgr with two strategies (i.e., dynamic anticipation and dynamic completion) designed for sparse kgs."
2020.emnlp-main.459.txt,2020,6 Conclusion,"in this paper, we study the task that multi-hop reasoning over sparse knowledge graphs."
2020.emnlp-main.459.txt,2020,6 Conclusion,the performance of previous multi-hop reasoning models on sparse kgs will drop significantly due to the lack of evidential paths.
2020.emnlp-main.459.txt,2020,6 Conclusion,these strategies can ease the sparsity of kgs.
2020.emnlp-main.46.txt,2020,7 Conclusion,"for one, as is the case for rheault and cochrane (2020), our model is not able to produce uncertainty bounds, as deriving uncertainty measures from neural networks remains an open area of research without a clear solution within the field of machine learning.7 an avenue for improving the model is to allow it to capture legislators’ dynamic attitudes toward trump over time."
2020.emnlp-main.46.txt,2020,7 Conclusion,from this model we obtain representations of legislators that capture their attitudes toward the president.
2020.emnlp-main.46.txt,2020,7 Conclusion,"in this paper, we modeled legislator tweeting behavior towards donald trump, predicting the frequency and sentiment of their tweets."
2020.emnlp-main.46.txt,2020,7 Conclusion,"legislator embeddings can be used to explore how legislators appeal to different audiences, such as party leaders and constituents."
2020.emnlp-main.46.txt,2020,7 Conclusion,legislator vectors and trump vectors interact to produce predictions of both the sentiment of legislator tweets about donald trump and the number of tweets produced each day.
2020.emnlp-main.46.txt,2020,7 Conclusion,"moreover, because our model does not rely on roll call votes, it can also be used to model attitudes by any of the growing number of political elites using twitter, such as non-incumbent political candidates, state legislators, and pundits."
2020.emnlp-main.46.txt,2020,7 Conclusion,our application suggests that ideal points estimated from roll call votes can miss this critical aspect of political preferences for members of congress.
2020.emnlp-main.46.txt,2020,7 Conclusion,"our model’s predictive performance is robust to a variety of settings and achieves sentiment predictive performance of 0.127 mean-absolute-error and 89.3% accuracy, demonstrating its capability to predict legislator tweeting behavior."
2020.emnlp-main.46.txt,2020,7 Conclusion,"possible extensions of this work could investigate enriching trump vectors by incorporating other sources of text, such as white house press releases and speeches."
2020.emnlp-main.46.txt,2020,7 Conclusion,the method presented here could similarly be used to evaluate how members of congress are punished and rewarded in elections for their criticism of praise of the president.
2020.emnlp-main.46.txt,2020,7 Conclusion,the proposed model yields embedding representations for legislators that we interpret as measures of legislator attitudes towards trump.
2020.emnlp-main.46.txt,2020,7 Conclusion,"though our model demonstrates the capability of representing legislators’ attitudes toward trump and performs well with respect to predicting tweet counts and sentiment based upon donald trump’s tweets, our method has some limitations."
2020.emnlp-main.46.txt,2020,7 Conclusion,"to address this shortcoming and obtain representations of legislators’ attitudes toward trump, we have proposed a model that assigns a vector to each legislator based on the content of their tweets about trump."
2020.emnlp-main.46.txt,2020,7 Conclusion,"we similarly represent donald trump with a vector for each day he tweets, constructed using the text of his daily tweets."
2020.emnlp-main.46.txt,2020,7 Conclusion,when visualizing the two dimensions of learned legislator embeddings we find that the model separates legislators across party lines (despite not being trained on the party of legislators) and groups together republican senators who are well-known critics of trump (despite overwhelmingly voting with him on legislation).
2020.emnlp-main.46.txt,2020,7 Conclusion,"whereas legislative voting might recover ideological similarities and differences with the president, it is not well suited to measure attitudes toward the president orthogonal to policy preferences, such as criticisms of his rhetoric and tone."
2020.emnlp-main.46.txt,2020,7 Conclusion,"while legislator attitudes are currently modeled as static embeddings, allowing each legislator’s embedding to change over time would enable the exploration of temporal dynamics and hypothesis testing about when legislators are more likely to tweet negatively about trump, what factors contribute to a legislator’s decision to tweet about trump, and how the trump’s tweets interact with legislator’s tweets over time."
2020.emnlp-main.46.txt,2020,7 Conclusion,"while our aims in this paper were to develop a method of modeling attitudes toward trump beyond legislative policy preferences, this method can be used to test a wide range of hypotheses about modern u.s. politics."
2020.emnlp-main.46.txt,2020,7 Conclusion,"while we restrict ourselves to twitter data in this paper to maintain consistency across the sources of data for vectors representing trump and legislators, the incorporation of auxiliary text data could provide additional context."
2020.emnlp-main.460.txt,2020,6 Conclusion and Future Work,"another meaningful direction is to use hyperka to infer the associations between snapshots in temporally dynamic kgs (xu et al., 2020)."
2020.emnlp-main.460.txt,2020,6 Conclusion and Future Work,"for future work, we plan to incorporate hyperbolic rnns (ganea et al., 2018) to encode auxiliary information for zero-shot entity and concept representations."
2020.emnlp-main.460.txt,2020,6 Conclusion and Future Work,our method outperforms sota baselines using lower embedding dimensions on both entity alignment and type inference.
2020.emnlp-main.460.txt,2020,6 Conclusion and Future Work,"the proposed hyperka method extends translational and gnn-based techniques to hyperbolic spaces, and captures associations by a hyperbolic transformation."
2020.emnlp-main.460.txt,2020,6 Conclusion and Future Work,"we also seek to investigate the use of hyperka for cross-domain representations of biological and medical knowledge (hao et al., 2020)."
2020.emnlp-main.460.txt,2020,6 Conclusion and Future Work,we propose a method to capture knowledge associations with a new hyperbolic gnn-based representation learning model.
2020.emnlp-main.461.txt,2020,9 Conclusion,"in conclusion, we propose a general framework that augments deep neural networks with distributional constraints constructed using probabilistic domain knowledge."
2020.emnlp-main.461.txt,2020,9 Conclusion,we apply it in the setting of end-to-end temporal relation extraction task with event-type and relation constraints and show that the map inference with distributional constraints can significantly improve the final results.
2020.emnlp-main.461.txt,2020,9 Conclusion,"we plan to apply the proposed framework on various event reasoning tasks and construct novel distributional constraints that could leverage domain knowledge beyond corpus statistics, such as the larger unlabeled data and rich information contained in knowledge bases."
2020.emnlp-main.462.txt,2020,5 Conclusion,"additionally, we introduce novel frequencybased gating and data imputation techniques to address the temporal variability and sparsity problems in tkgc."
2020.emnlp-main.462.txt,2020,5 Conclusion,future work involves exploring the generalization of temp to continuous tkgc and better imputation techniques to induce representations for infrequent and inactive entities.
2020.emnlp-main.462.txt,2020,5 Conclusion,"in this work, we present a novel framework named temp for temporal knowledge graph completion (tkgc)."
2020.emnlp-main.462.txt,2020,5 Conclusion,"our work is potentially beneficial to other tasks such as temporal information extraction and temporal question answering, by providing beliefs about the likelihood of facts at particular points in time."
2020.emnlp-main.462.txt,2020,5 Conclusion,temp computes entity representation by jointly modelling multi-hop structural information and temporal facts from nearby time-steps.
2020.emnlp-main.462.txt,2020,5 Conclusion,we show that our model is able to achieve superior performance (10.7% relative improvement) over the state-of-the-arts on three benchmark datasets.
2020.emnlp-main.463.txt,2020,7 Conclusion,"also, the unbalanced gradient distribution issue is mostly addressed by adaptive optimizers."
2020.emnlp-main.463.txt,2020,7 Conclusion,"extensive experiments verify our intuitions and show that, without introducing additional hyper-parameters, admin achieves more stable training, faster convergence, and better performance."
2020.emnlp-main.463.txt,2020,7 Conclusion,"in light of our analysis, we propose admin, an adaptive initialization method to stabilize transformers training."
2020.emnlp-main.463.txt,2020,7 Conclusion,"in section 4, we reveal the root cause of the instability to be the strong dependency on residual branches, which amplifies the fluctuation caused by parameter changes and destabilizes model training."
2020.emnlp-main.463.txt,2020,7 Conclusion,"in this paper, we study the difficulties of training transformers in theoretical and empirical manners."
2020.emnlp-main.463.txt,2020,7 Conclusion,it controls the dependency at the beginning of training and maintains the flexibility to capture those dependencies once training stabilizes.
2020.emnlp-main.463.txt,2020,7 Conclusion,"it leads to many interesting future works, including generalizing theorem 2 to other models, designing new algorithms to automatically adapt deep networks to different training configurations, upgrading the transformer architecture, and applying our proposed admin to conduct training in a larger scale."
2020.emnlp-main.463.txt,2020,7 Conclusion,our study in section 3 suggests that the gradient vanishing problem is not the root cause of unstable transformer training.
2020.emnlp-main.463.txt,2020,7 Conclusion,our work opens up new possibilities to not only further push the state-of-the-art but understand deep network training better.
2020.emnlp-main.464.txt,2020,6 Conclusion,"in this work, we investigated a broad array of generation orders for machine translation using an insertion-based sequence generation model, the insertion transformer."
2020.emnlp-main.464.txt,2020,6 Conclusion,"this opens a wide range of possibilities for generation tasks where monotonic orderings are not the most natural choice, and we would be excited to explore some of these areas in future work."
2020.emnlp-main.464.txt,2020,6 Conclusion,"we found that regardless of the type of strategy selected, be it locationbased, frequency-based, length-based, alphabetical, model-based, or even random, the insertion transformer is able to learn it with high fidelity and produce high-quality output in the selected order."
2020.emnlp-main.465.txt,2020,5 Conclusion,for future work we would like to explore if their success transfers to other generation tasks with mlms where inference efficiency is a concern.
2020.emnlp-main.465.txt,2020,5 Conclusion,"our new heuristics achieve better speed/quality balance by flexibly adjusting the number of total iterations, and by taking the probabilities of sets of tokens into account."
2020.emnlp-main.465.txt,2020,5 Conclusion,"we introduce a perspective which views generation sequences as probabilistic factorizations of the final output sequence, and use it to analyze and extend previous heuristics."
2020.emnlp-main.465.txt,2020,5 Conclusion,we investigated inference strategies for machine translation based on cmlm with a focus on the trade-off between generation speed and quality.
2020.emnlp-main.466.txt,2020,7 Conclusion & Future Work,"furthermore, future work may build on the ambigqa task with more open-ended approaches such as (1) applying the approach to qa over structured data (such as ambiguous questions that require returning tables), (2) handling questions with no answer or ill-formed questions that require inferring and satisfying more complex ambiguous information needs, and (3) more carefully evaluating usefulness to end users."
2020.emnlp-main.466.txt,2020,7 Conclusion & Future Work,"future research developing on ambigqa models may include explicitly modeling ambiguity over events and entities or in the retrieval step, as well as improving performance on the difficult problems of answer recall and question disambiguation."
2020.emnlp-main.466.txt,2020,7 Conclusion & Future Work,"our analysis shows the dataset contains diverse types of ambiguity, often not visible from the prompt question alone."
2020.emnlp-main.466.txt,2020,7 Conclusion & Future Work,"we also introduced a first baseline model for producing multiple answers to opendomain questions, with experiments showing its effectiveness in learning from our data while highlighting possible areas for improvement."
2020.emnlp-main.466.txt,2020,7 Conclusion & Future Work,"we constructed ambignq, a dataset with 14,042 annotations on nq-open questions."
2020.emnlp-main.466.txt,2020,7 Conclusion & Future Work,"we introduced ambigqa, a new task that involves providing multiple possible answers to a potentially ambiguous open-domain question, and providing a disambiguated question corresponding to each answer."
2020.emnlp-main.467.txt,2020,5 Conclusion,crqda treats the question data augmentation task as a constrained question rewriting problem.
2020.emnlp-main.467.txt,2020,5 Conclusion,"furthermore, crqda can be used to improve the model performance on question generation and question-answering language inference tasks, which achieves a new state-of-theart on the squad 1.1 question generation task."
2020.emnlp-main.467.txt,2020,5 Conclusion,"in this work, we present a novel question data augmentation method, called crqda, for contextrelevant answerable and unanswerable question generation."
2020.emnlp-main.467.txt,2020,5 Conclusion,the crqda augmented datasets can improve multiple reading comprehension models.
2020.emnlp-main.467.txt,2020,5 Conclusion,the experimental results demonstrate that crqda outperforms other strong baselines on squad 2.0.
2020.emnlp-main.467.txt,2020,5 Conclusion,"under the guidance of a pre-trained mrc model, the original question is revised in a continuous embedding space with gradient-based optimization and then decoded back to the discrete space as a new question data sample."
2020.emnlp-main.468.txt,2020,8 Conclusions and Future Work,"doing so required us to scale model size for our answer generators, question generators, and filtration models."
2020.emnlp-main.468.txt,2020,8 Conclusions and Future Work,extension of this work to unanswerable and boolean questions is also a future work direction.
2020.emnlp-main.468.txt,2020,8 Conclusions and Future Work,"finally, we generate synthetic text from a wikipedia-finetuned gpt-2 model, generate answer candidates and synthetic questions based on those answers, and then train a bert-large model and achieve similar question answering accuracy."
2020.emnlp-main.468.txt,2020,8 Conclusions and Future Work,finetuning the resulting model on real squad1.1 data further boosts the em score to 89.4.
2020.emnlp-main.468.txt,2020,8 Conclusions and Future Work,"for such a regime, one needs to analyze the effect of domain transfer and bootstrapping from a very small human labelled dataset."
2020.emnlp-main.468.txt,2020,8 Conclusions and Future Work,"more generally application of this work to multi dataset question generation with datasets such as multiqa (talmor and berant, 2019) is a promising avenue for future work."
2020.emnlp-main.468.txt,2020,8 Conclusions and Future Work,of particular interest for future work is handling low-resource question answering domains.
2020.emnlp-main.468.txt,2020,8 Conclusions and Future Work,this amounts to a 1.7 point improvement over our fully supervised baseline.
2020.emnlp-main.468.txt,2020,8 Conclusions and Future Work,we build upon existing work in large scale language modeling and question generation to push the quality of synthetic question generation.
2020.emnlp-main.468.txt,2020,8 Conclusions and Future Work,we hope that better synthetic questions will enable new breakthroughs in question answering systems and related natural language tasks.
2020.emnlp-main.468.txt,2020,8 Conclusions and Future Work,"with our best models, we generate large question answering datasets from unlabeled wikipedia documents and finetune a 345 million parameter bert-style model achieving 88.4 em score."
2020.emnlp-main.469.txt,2020,5 Conclusion,cqa refers to answering complex natural language questions on a kb.
2020.emnlp-main.469.txt,2020,5 Conclusion,"in the future, we plan to improve mrl-cqa by designing a retriever that could be optimized jointly with the programmer under the meta-learning paradigm, instead of manually pre-defining a static relevance function."
2020.emnlp-main.469.txt,2020,5 Conclusion,"in this paper, we propose a meta-learning method to npi in cqa, which quickly adapts the programmer to unseen questions to tackle the potential distributional bias in questions."
2020.emnlp-main.469.txt,2020,5 Conclusion,"other potential directions of research could be toward learning to cluster questions into fine-grained groups and assign each group a set of specific initial parameters, making the model finetune the parameters more precisely."
2020.emnlp-main.469.txt,2020,5 Conclusion,"to effectively create the support sets, we propose an unsupervised retriever to find the questions that are structurally and semantically similar to the new questions from the training dataset."
2020.emnlp-main.469.txt,2020,5 Conclusion,we take a meta-reinforcement learning approach to effectively adapt the meta-learned programmer to new questions based on the most similar questions retrieved.
2020.emnlp-main.469.txt,2020,5 Conclusion,"when evaluated on the large-scale complex question answering dataset, cqa, our proposed approach achieves state-of-the-art performance with overall macro and micro f1 score of 66.25% and 77.71%, respectively."
2020.emnlp-main.47.txt,2020,6 Conclusion,code to support this work can be found at https://github.com/mbwsims/ literary-information-propagation.
2020.emnlp-main.47.txt,2020,6 Conclusion,"considered from a narratological perspective, however, this is a benefit rather than a drawback, since our goal is not to understand the underlying reality of these imagined worlds but rather how authors opt to represent the informational dynamics from which their stories are constructed."
2020.emnlp-main.47.txt,2020,6 Conclusion,"in developing this pipeline to examine how authors depict the transmission of information within narrative texts, we hope to drive a variety of future research in this space, including not only such narratological questions as how “gossip impels plots” (spacks, 1985), but also questions pertaining to issues of bias in representation, the flow of information, and factuality."
2020.emnlp-main.47.txt,2020,6 Conclusion,"this study, of course, contains limitations: readers of fictional works are only afforded a partial perspective of the world that is represented— namely the interactions an author chooses to describe (and not, for example, the dialogue we might presume takes place “off-screen”)."
2020.emnlp-main.47.txt,2020,6 Conclusion,"this work offers a new perspective on the analysis of social networks in literary texts by considering the dynamics of how information flows through them—both as a result of the structural topology of the network (characters who successfully propagate are information bridges between communities), and as a result of the specific characteristics of each node (women are depicted more frequently as successful propagators than men)."
2020.emnlp-main.47.txt,2020,6 Conclusion,"we introduce the task of identifying information propagation in literary social networks, designing an nlp pipeline for extracting both implicit and explicit propagation."
2020.emnlp-main.470.txt,2020,6 Conclusion,"finally, we would also like to apply our models to languages with even less resources available to help coping with the problem of offensive language in social media."
2020.emnlp-main.470.txt,2020,6 Conclusion,"furthermore, the results we obtained for bengali show that it is possible to achieve high performance using transfer learning on off-domain (twitter vs. facebook) and off-task data when the labels do not have a direct correspondence in the projected dataset (two in english and three in bengali)."
2020.emnlp-main.470.txt,2020,6 Conclusion,"in future work, we would like to further evaluate our models using solid, a novel large english dataset with over 9 million tweets (rosenthal et al., 2020), along with datasets in four other languages (arabic, danish, greek, and turkish) that were made available for the second edition of offenseval (zampieri et al., 2020)."
2020.emnlp-main.470.txt,2020,6 Conclusion,"the results obtained by our models confirm that olid’s general hierarchical annotation model encompasses multiple types of offensive content such as aggression, included in the bengali dataset, and hate speech included in the hindi and spanish datasets, allowing us to model different subtasks jointly using the methods described in this paper."
2020.emnlp-main.470.txt,2020,6 Conclusion,these datasets were collected using the same methodology and were annotated according to olid’s guidelines.
2020.emnlp-main.470.txt,2020,6 Conclusion,"this opens exciting new avenues for future research considering the multitude of phenomena (e.g.hate speech, aggression, cyberbulling), annotation schemes and guidelines used in offensive language datasets."
2020.emnlp-main.470.txt,2020,6 Conclusion,"this paper is the first study to apply cross-lingual contextual word embeddings in offensive language identification projecting predictions from english to other languages using benchmarked datasets from shared tasks on bengali (kumar et al., 2020), hindi (mandl et al., 2019), and spanish (basile et al., 2019)."
2020.emnlp-main.470.txt,2020,6 Conclusion,we have showed that xlm-r with transfer learning outperforms all of the other methods we tested as well as the best results obtained by participants of the three competitions.
2020.emnlp-main.471.txt,2020,9 Conclusion and Future Work,"because they have much lower perplexities than widely-used n-gram models, they can distinguish between candidate plaintexts that resemble english at a distance, versus candidate plaintexts that are grammatical, sensible, and relevant to the historical context."
2020.emnlp-main.471.txt,2020,9 Conclusion and Future Work,"in this work, we show that it is possible to decipher a book-based cipher, using a known-plaintext attack and a neural english language model."
2020.emnlp-main.471.txt,2020,9 Conclusion and Future Work,"we apply our method to letters written to and from us general james wilkinson, and we recover 75.1% of the word tokens correctly."
2020.emnlp-main.471.txt,2020,9 Conclusion and Future Work,we believe word-based neural language models are a powerful tool for decrypting classical codes and ciphers.
2020.emnlp-main.472.txt,2020,11 Conclusion,"our models establish new sota on a wide range of tasks, thereby demonstrating their value."
2020.emnlp-main.472.txt,2020,11 Conclusion,"ultimately, we hope our work can open up new horizons for studying mds in various languages."
2020.emnlp-main.472.txt,2020,11 Conclusion,"we also introduced several novel mtl scenarios for modeling mds including at hierarchical levels, and with linguistically-motivated auxiliary tasks inspired by diaglossic and code-switching environments."
2020.emnlp-main.472.txt,2020,11 Conclusion,"we have also exploited our own data to train marbert, a very large and powerful masked language model covering all arabic varieties."
2020.emnlp-main.472.txt,2020,11 Conclusion,"we introduced the novel task of mdi and offered a large-scale, manually-labeled dataset covering 319 city-based arabic micro-varieties."
2020.emnlp-main.473.txt,2020,5 Conclusion,"in addition, we present a new dataset consisting of intent-parallel aave/sae tweet pairs, which can be used in future works studying sae and aave in nlp models."
2020.emnlp-main.473.txt,2020,5 Conclusion,"moreover, our bleu, rouge, and human evaluation results reveal a disparity in the quality of gpt-2’s text generation between aave and sae."
2020.emnlp-main.473.txt,2020,5 Conclusion,our sentiment analysis experiments indicate that gpt-2 produces more negative instances when prompted with aave text.
2020.emnlp-main.473.txt,2020,5 Conclusion,"through this work, we highlight the need for aave-inclusivity in nlg models, especially those perceived as state-of-the-art."
2020.emnlp-main.473.txt,2020,5 Conclusion,"to this end, we provide a new evaluation of nlg models by comparing gpt-2’s behavior on sae and aave."
2020.emnlp-main.473.txt,2020,5 Conclusion,we hope our findings can pave the way for further inclusion of diverse language in future nlg models.
2020.emnlp-main.474.txt,2020,5 Conclusion,"experiments on adapting translation models between specific domains and from general domain to specific domains demonstrate the effectiveness of our method, achieving significant improvements over strong back-translation baselines."
2020.emnlp-main.474.txt,2020,5 Conclusion,"in the future, we would like to extend our method to enhance the back-translation method in multidomain settings."
2020.emnlp-main.474.txt,2020,5 Conclusion,"in this paper, we argue that back-translation, the predominant unsupervised domain adaptation method in neural machine translation, suffers from the domain shift, restricting the performance of unsupervised domain adaptation."
2020.emnlp-main.474.txt,2020,5 Conclusion,then the iterative domain-repaired back-translation framework is designed to make full use of the advantage of the domain repair model.
2020.emnlp-main.474.txt,2020,5 Conclusion,we propose to remedy this mismatch by leveraging a domain repair model that corrects the errors in back-translation sentences.
2020.emnlp-main.475.txt,2020,7 Conclusion,extensive experiments are performed to evaluate the performance of our methods; analyses reveal the selected samples can represent the target domain well and our weighting strategies benefit noisy settings the most.
2020.emnlp-main.475.txt,2020,7 Conclusion,"in this paper, we provide a novel insight into a widely-used data selection method (moore and lewis, 2010) and generalize it to a curriculum strategy for iterative back-translation."
2020.emnlp-main.475.txt,2020,7 Conclusion,we also propose data weighting methods to down-weight examples of poor quality.
2020.emnlp-main.476.txt,2020,6 Conclusion,"additionally, we demonstrate that the m2 can also benefit low resource pairs in an unbalanced environment as a 1-1 model without being subject to cross-language effect."
2020.emnlp-main.476.txt,2020,6 Conclusion,"by extensively comparing the single models, 1-1 model, and m2 in varying conditions, we find that the m2 can benefit from multi-way training through data-diversification and regularization while suffering less from capacity bottlenecks."
2020.emnlp-main.476.txt,2020,6 Conclusion,"furthermore, we validate that the language invariance of the space enhances as the number of languages in the m2 increases."
2020.emnlp-main.476.txt,2020,6 Conclusion,"in this study, we re-evaluate the m2 model and suggest it as an appropriate choice for multilingual translation in industries."
2020.emnlp-main.476.txt,2020,6 Conclusion,"next, we suggest that the m2 model is easily maintainable because of its interlingual space."
2020.emnlp-main.476.txt,2020,6 Conclusion,"the interlingual space not only enables incremental training in a simple manner, but also accompanies competitive incremental zero-shot performance."
2020.emnlp-main.476.txt,2020,6 Conclusion,we hope that this study sheds light on the relatively disregarded m2 model and provide a benchmark for selecting a model among varying levels of shared components.
2020.emnlp-main.477.txt,2020,6 Conclusion,"among embedding-based models, our strongest baseline (“x-y”) actively removes language bias by augmenting training data to include machine-translated cross-lingual examples."
2020.emnlp-main.477.txt,2020,6 Conclusion,"however, to achieve strong alignment, this model sacrifices performance on both retrieval from a monolingual pool (table 4), as well as retrieval of same-language candidates (figure 4d diagonal)."
2020.emnlp-main.477.txt,2020,6 Conclusion,"it goes further than previous cross-lingual benchmarks in requiring “strong” cross-lingual alignment, which is a step closer to truly languageagnostic representations."
2020.emnlp-main.477.txt,2020,6 Conclusion,"it is an interesting question for future work whether strong alignment always comes at a cost, or if better training techniques will lead to models that can improve on all these measures simultaneously."
2020.emnlp-main.477.txt,2020,6 Conclusion,lareqa is a challenging new benchmark testing answer retrieval from a multilingual candidate pool.
2020.emnlp-main.477.txt,2020,6 Conclusion,our best initial baseline sidesteps the alignment problem by simply translating all test data to english.
2020.emnlp-main.477.txt,2020,6 Conclusion,we believe there is significant headroom for models to improve on lareqa.
2020.emnlp-main.478.txt,2020,8 Conclusion,"additionally, it would be valuable to examine whether our method can improve the ocr on highresource languages, which typically have much better recognition rates in the first pass transcription than the endangered languages in our dataset."
2020.emnlp-main.478.txt,2020,8 Conclusion,"as future work, we plan to investigate the effect of using other available data for the three languages (for example, word lists collected by documentary linguists or the additional griko folk tales collected by anastasopoulos et al.(2018))."
2020.emnlp-main.478.txt,2020,8 Conclusion,"further, we note our use of the google vision ocr system to obtain the first pass ocr for our experiments, primarily because it provides script-specific models as opposed to other generalpurpose ocr systems that rely on languagespecific models (as discussed in section 4)."
2020.emnlp-main.478.txt,2020,8 Conclusion,future work that focuses on overcoming the challenges of applying language-specific models to endangered language texts would be needed to confirm our method’s applicability to post-correcting the first pass transcriptions from different ocr systems.
2020.emnlp-main.478.txt,2020,8 Conclusion,"future work will focus on large-scale digitization of scanned documents, aiming to expand our ocr benchmark on as many endangered languages as possible, in the hope of both easing linguistic documentation and preservation efforts and collecting enough data for nlp system development in under-represented languages."
2020.emnlp-main.478.txt,2020,8 Conclusion,"lastly, given the annotation effort involved, this paper explores only a small fraction of the endangered language data available in linguistic and general-purpose archives."
2020.emnlp-main.478.txt,2020,8 Conclusion,this work presents a first step towards extracting textual data in endangered languages from scanned images of paper books.
2020.emnlp-main.478.txt,2020,8 Conclusion,"we create a benchmark dataset with transcribed images in three endangered languages: ainu, griko, and yakkha."
2020.emnlp-main.478.txt,2020,8 Conclusion,"we propose an ocr post-correction method that facilitates learning from small amounts of data, which results in a 34% average relative error reduction in both the character and word recognition rates."
2020.emnlp-main.479.txt,2020,8 Conclusion,future directions include other pre-training or fine-tuning methods to improve retrieval performance and methods that encourage the lm to predict entities of the right types.
2020.emnlp-main.479.txt,2020,8 Conclusion,"the results demonstrate the difficulty of this task, and that knowledge contained in lms varies across languages."
2020.emnlp-main.479.txt,2020,8 Conclusion,"we examine the intersection of multilinguality and the factual knowledge included in lms by creating a multilingual and multi-token benchmark xfactr, and performing experiments comparing and contrasting across languages and lms."
2020.emnlp-main.48.txt,2020,8 Conclusion,comprehensive modeling of social norms presents a promising challenge for nlp work in the future.
2020.emnlp-main.48.txt,2020,8 Conclusion,"our experiments demonstrate preliminary success in generative modeling of structured rots, and corroborate findings of moral leaning in an extrinsic task."
2020.emnlp-main.48.txt,2020,8 Conclusion,"we present social-chem-101, an attempt at providing a formalism and resource around the study of grounded social, moral, and ethical norms."
2020.emnlp-main.480.txt,2020,7 Conclusion & Future Works,additional work could also leverage aligned documents as supervision to learn better cross-lingual document representations.
2020.emnlp-main.480.txt,2020,7 Conclusion & Future Works,"finally, we perform a case-study showing that our url-aligned documents can be mined for high-quality parallel sentences which can be used to train machine translation models."
2020.emnlp-main.480.txt,2020,7 Conclusion & Future Works,"finally, while the aligned dataset is high-precision, leveraging this dataset for supervision in document alignment can potentially yield a larger, high-recall collection."
2020.emnlp-main.480.txt,2020,7 Conclusion & Future Works,"given the sheer size of this dataset, this has the potential to provide high-quality training data for many low-resource language pairs."
2020.emnlp-main.480.txt,2020,7 Conclusion & Future Works,"in this paper, we apply url-matching to curate a high-quality cross-lingual documents dataset from the commoncrawl corpus."
2020.emnlp-main.480.txt,2020,7 Conclusion & Future Works,one natural followup to this work is to develop techniques to better mine parallel sentences from these aligned documents – especially for low-resource language pairs.
2020.emnlp-main.480.txt,2020,7 Conclusion & Future Works,our dataset contains over 392 million document pairs from 8144 language pairs covering 138 distinct languages.
2020.emnlp-main.480.txt,2020,7 Conclusion & Future Works,"our results indicate there is further work to be done to improve document alignment, especially for low-resource languages and that intelligent alignment schemes can significantly improve alignment performance across many language directions."
2020.emnlp-main.480.txt,2020,7 Conclusion & Future Works,"to spur further work, we release the list of aligned urls as well as code to generate aligned documents given commoncrawl snapshots."
2020.emnlp-main.480.txt,2020,7 Conclusion & Future Works,we first directly evaluate the quality of the urlaligned pairs using human annotators.
2020.emnlp-main.480.txt,2020,7 Conclusion & Future Works,we then introduce and evaluate simple embedding-based baseline techniques for aligning documents based on content.
2020.emnlp-main.481.txt,2020,6 Conclusion,our generated datasets are automatically annotated using logical forms containing localized entities; we require no human annotations.
2020.emnlp-main.481.txt,2020,6 Conclusion,our methodology enables further investigation and creation of new benchmarks to trigger more research on this topic.
2020.emnlp-main.481.txt,2020,6 Conclusion,our model outperforms the previous state-of-the-art methodology by between 30% and 40% depending on the domain and the language.
2020.emnlp-main.481.txt,2020,6 Conclusion,our new datasets and resources are released open-source4 .
2020.emnlp-main.481.txt,2020,6 Conclusion,"spl can be used by any developer to extend their qa system’s current capabilities to a new language in less than 24 hours, leveraging professional services to translate the validation data and mature public nmt systems."
2020.emnlp-main.481.txt,2020,6 Conclusion,"spl was incorporated into the schema2qa toolkit (xu et al., 2020a) to give it a multilingual capability."
2020.emnlp-main.481.txt,2020,6 Conclusion,"this paper presents spl, a toolkit and methodology to extend and localize semantic parsers to a new language with higher accuracy, yet at a fraction of the cost compared to previous methods."
2020.emnlp-main.481.txt,2020,6 Conclusion,"we found our approach to be effective on a recently proposed qa semantic parsing dataset, which is significantly more challenging than other available multilingual datasets in terms of sentence complexity and ontology size."
2020.emnlp-main.482.txt,2020,6 Conclusion and Future Work,another line of future work is to investigate alternative user interfaces.
2020.emnlp-main.482.txt,2020,6 Conclusion and Future Work,bilingual users can quickly improve a model with the help of clime at a faster rate than an active learning baseline.
2020.emnlp-main.482.txt,2020,6 Conclusion and Future Work,"clime has a modular design with three components: keyword ranking, user interface, and embedding refinement."
2020.emnlp-main.482.txt,2020,6 Conclusion and Future Work,clime is an interactive system that enhances clwe for a task by asking a bilingual speaker for word-level similarity annotations.
2020.emnlp-main.482.txt,2020,6 Conclusion and Future Work,combining active learning with clime further improves the system.
2020.emnlp-main.482.txt,2020,6 Conclusion and Future Work,"for example, we could ask bilingual users to rank nearest neighbors (sakaguchi and van durme, 2018) or provide scalar grades (hill et al., 2015) instead of accepting/rejecting individual neighbors."
2020.emnlp-main.482.txt,2020,6 Conclusion and Future Work,"in the future, we plan to train a policy that dynamically combines the two interactions with reinforcement learning (fang et al., 2017)."
2020.emnlp-main.482.txt,2020,6 Conclusion and Future Work,simultaneously applying both methods is better than using either alone.
2020.emnlp-main.482.txt,2020,6 Conclusion and Future Work,"the keyword ranking and the embedding refinement modules build upon existing methods for interpreting neural networks (li et al., 2016) and fine-tuning word embeddings (mrkšic´ et al., 2017)."
2020.emnlp-main.482.txt,2020,6 Conclusion and Future Work,"therefore, future advances in these areas may also improve clime."
2020.emnlp-main.482.txt,2020,6 Conclusion and Future Work,we also explore a simple combination of active learning and clime.
2020.emnlp-main.482.txt,2020,6 Conclusion and Future Work,we test clime on cross-lingual information triage in international health emergencies for four low-resource languages.
2020.emnlp-main.483.txt,2020,5 Conclusion,"it also increases downstream mt performance in a low-resource setting over prior work, including a margin-based comparable corpora method (artetxe and schwenk, 2019a)."
2020.emnlp-main.483.txt,2020,5 Conclusion,our method outperforms all published results on the dataset released for the wmt16 shared task on document alignment.
2020.emnlp-main.483.txt,2020,5 Conclusion,"our method uses multilingual sentence embeddings and explicitly models the order of sentences in documents, in both candidate generation and candidate re-scoring."
2020.emnlp-main.483.txt,2020,5 Conclusion,we present a simple but effective method for document alignment.
2020.emnlp-main.483.txt,2020,5 Conclusion,"we use the same embeddings as the comparable corpora method, thus the improvement over that method demonstrates the importance of including sentence order in document alignment, even when document-level alignments are not required."
2020.emnlp-main.484.txt,2020,7 Conclusion,we present xglue as a new cross-lingual benchmark and conduct comprehensive evaluations with interesting findings observed.
2020.emnlp-main.484.txt,2020,7 Conclusion,"we thank stc-a nlp, bing answers, bing ads, bing relevance and microsoft news for providing the datasets."
2020.emnlp-main.485.txt,2020,4 Conclusion,ains can be parallelized over different positions in the sequence.
2020.emnlp-main.485.txt,2020,4 Conclusion,empirical results show that ains are significantly faster than traditional crf and do very well in tasks that require more local information.
2020.emnlp-main.485.txt,2020,4 Conclusion,"in this paper, we propose approximate inference networks (ain) that use mean-field variational inference (mfvi) instead of exact probabilistic inference algorithms such as the forward-backward and viterbi algorithms for training and prediction on the conditional random field for sequence labeling."
2020.emnlp-main.485.txt,2020,4 Conclusion,our approaches achieve competitive accuracy on 4 tasks with 15 datasets over three encoder types.
2020.emnlp-main.485.txt,2020,4 Conclusion,our code is publicly available at https://github.com/alibaba-nlp/ain.
2020.emnlp-main.485.txt,2020,4 Conclusion,the mfvi algorithm can be unfolded as a recurrent neural network and connected with the encoder to form an end-to-end neural network.
2020.emnlp-main.486.txt,2020,5 Conclusions,experiments on three public datasets show that our model achieves significant improvements over the state-of-the-art models.
2020.emnlp-main.486.txt,2020,5 Conclusions,"for future work, we will apply hit to other languages, and further explore potential cases of overlapping entities in nested ner task."
2020.emnlp-main.486.txt,2020,5 Conclusions,"furthermore, the token interaction tagger captures the internal token connection within the boundary."
2020.emnlp-main.486.txt,2020,5 Conclusions,"in this paper, we propose a novel neural model hit for recognizing nested named entity."
2020.emnlp-main.486.txt,2020,5 Conclusions,it leverages the head-tail pair and token interaction to express the entities with the nested structure.
2020.emnlp-main.486.txt,2020,5 Conclusions,"specifically, the head-tail detector can detect the head-tail pair of named entities."
2020.emnlp-main.487.txt,2020,4 Conclusion,"experimental results and the ablation study on the english ccgbank demonstrate the effectiveness of our approach to ccg supertagging, where state-of-the-art performance is obtained on both ccg supertagging and parsing."
2020.emnlp-main.487.txt,2020,4 Conclusion,"further analysis is performed to investigate using different types of edges, which reveals their quality and confirms the necessity of introducing attention to gcn for ccg supertagging."
2020.emnlp-main.487.txt,2020,4 Conclusion,"in this paper, we propose a-gcn for ccg supertagging, with its graph built from chunks extracted from a lexicon."
2020.emnlp-main.487.txt,2020,4 Conclusion,"we use two types of edges for the graph, namely, in-chunk and cross-chunk edges for word pairs within and across chunks, respectively, and propose an attention mechanism to distinguish the important word pairs according to their contribution to ccg supertagging."
2020.emnlp-main.488.txt,2020,6 Conclusion,"besides, experiments demonstrate that our method can also effectively utilize unlabeled data and knowledge base for semisupervised training."
2020.emnlp-main.488.txt,2020,6 Conclusion,"in this paper, we show that language models can be used to generate high quality synthetic data for sequence tagging tasks."
2020.emnlp-main.488.txt,2020,6 Conclusion,"our proposed method demonstrates promising performance improvements on various tagging tasks, especially in the low-resource settings."
2020.emnlp-main.488.txt,2020,6 Conclusion,"the generated data introduce more diversity to reduce overfitting, since they are generated from scratch instead of modifying the gold training data."
2020.emnlp-main.489.txt,2020,7 Discussion,"this is just a first step towards the goal of fully-automated interpretable evaluation, and applications to new attributes and tasks beyond ner are promising future directions."
2020.emnlp-main.489.txt,2020,7 Discussion,"this paper has provided a framework where we can covert our understanding of the ner task (i.e., which attributes matter for the current task?)into interpretable evaluation aspects, and define axes through which we can apply them to acquire insights and make model improvements."
2020.emnlp-main.49.txt,2020,6 Conclusion,"for future work, it would be interesting to try incorporating broader context (e.g., paragraph/document-level context (ji and grishman, 2008; huang and riloff, 2011; du and cardie, 2020) in our methods to improve the accuracy of the predictions."
2020.emnlp-main.49.txt,2020,6 Conclusion,"in this paper, we introduce a new paradigm for event extraction based on question answering."
2020.emnlp-main.49.txt,2020,6 Conclusion,"our framework outperforms prior works on the ace 2005 benchmark, and is capable of extracting event arguments of roles not seen at training time."
2020.emnlp-main.49.txt,2020,6 Conclusion,"we investigate how the question generation strategies affect the performance of our framework on both trigger detection and argument span extraction, and find that more natural questions lead to better performance."
2020.emnlp-main.490.txt,2020,4 Conclusion,experiments confirm the effectiveness of semantic decoupling.
2020.emnlp-main.490.txt,2020,4 Conclusion,"in this paper, we dive into the issues of openvocabulary slots in slot filling task and propose a novel model-agnostic adversarial semantic decoupling method which distinguishes local semantics inherent in open-vocabulary slot words from the global context."
2020.emnlp-main.490.txt,2020,4 Conclusion,we hope to provide new guidance for the future slot filling work.
2020.emnlp-main.491.txt,2020,5 Conclusion,"in this paper, we present emb2emb, a framework that reduces conditional text generation tasks to learning in the embedding space of a pretrained autoencoder."
2020.emnlp-main.491.txt,2020,5 Conclusion,"since our framework can be used with any pretrained autoencoder, it will benefit from large-scale pretraining in future research."
2020.emnlp-main.491.txt,2020,5 Conclusion,we propose an adversarial method and a neural architecture that are crucial for our method’s success by making learning stay on the manifold of the autoencoder.
2020.emnlp-main.492.txt,2020,5 Conclusion and Future Directions,"empirically, we find that sans-based negatives have comparable performance with sota approaches and even outperform previous sophisticated gan-based approaches."
2020.emnlp-main.492.txt,2020,5 Conclusion and Future Directions,"in this work, we introduced sans, a novel negative sampling strategy, which directly leverages information about k-hop neighborhoods to select negative examples."
2020.emnlp-main.492.txt,2020,5 Conclusion and Future Directions,"our work sheds light on the need and importance of incorporating graph structure when designing negative samplers for kgs, and for which sans can be seen as a cheap yet powerful baseline that requires no additional parameters or difficult optimization."
2020.emnlp-main.493.txt,2020,6 Conclusion,further qualitative analysis suggests that such good performance comes from its ability to adaptively mask meaningful words for the given task.
2020.emnlp-main.493.txt,2020,6 Conclusion,"to this end, we proposed the neural mask generator (nmg), which is trained with reinforcement learning to mask out words that are helpful for domain adaptation."
2020.emnlp-main.493.txt,2020,6 Conclusion,"we performed an empirical study of various rule-based masking strategies on multiple datasets for question answering and text classification tasks, which shows that the optimal masking strategy depends on both the language model and the domain."
2020.emnlp-main.493.txt,2020,6 Conclusion,"we proposed a novel framework which automatically generates an adaptive masking for masked language models based on the given context, for language model adaptation to low-resource domains."
2020.emnlp-main.493.txt,2020,6 Conclusion,"we then validated nmg against rule-based masking strategies, and the results show that it either outperforms, or obtains comparable performance to the best heuristic."
2020.emnlp-main.494.txt,2020,7 Conclusion,another possibility is to design imitation-based fine-tuning analogs to the seqinter method.
2020.emnlp-main.494.txt,2020,7 Conclusion,"finally, although our experiments in this paper focused on sequenceto-sequence settings, we are interested in exploring the use of imitkd for compressing large language models aimed at transfer learning."
2020.emnlp-main.494.txt,2020,7 Conclusion,"in this work, we developed a new knowledge distillation technique inspired by imitation learning for compressing large and cumbersome autoregressive models into smaller and faster counterparts."
2020.emnlp-main.494.txt,2020,7 Conclusion,"one branch of ideas involves incorporating more advanced il algorithms beyond dagger, such as lols (chang et al., 2015), to further improve the distillation process."
2020.emnlp-main.494.txt,2020,7 Conclusion,we are excited about several possible avenues for future work.
2020.emnlp-main.494.txt,2020,7 Conclusion,we demonstrated the empirical success of our method over popular baselines on several natural language generation tasks.
2020.emnlp-main.495.txt,2020,6 Conclusions,"in summary, we propose a general targeted attack framework for adversarial text generation."
2020.emnlp-main.495.txt,2020,6 Conclusions,our results confirmed that our attacks can achieve high attack success rate without fooling the human.
2020.emnlp-main.495.txt,2020,6 Conclusions,"these results shed light on an effective way to examine the robustness of a wide range of nlp models, thus paving the way for the development of a new generation of more reliable and effective nlp methods."
2020.emnlp-main.495.txt,2020,6 Conclusions,"to the best of our knowledge, this is the first method that successfully conducts arbitrary targeted attack on general nlp tasks."
2020.emnlp-main.496.txt,2020,8 Conclusion,"in this work, we present a generic structured pruning method based on adaptive low-rank factorization."
2020.emnlp-main.496.txt,2020,8 Conclusion,"this work contributes to reducing the growing overhead of large language models, and shines a light on the role of model capacity in language modeling."
2020.emnlp-main.496.txt,2020,8 Conclusion,"we show that our method can provide significant speedups and compression rates on large models while losing minimal performance compared to other methods, including unstructured magnitude pruning."
2020.emnlp-main.496.txt,2020,8 Conclusion,we systematically evaluate the performance of this method on large language models.
2020.emnlp-main.497.txt,2020,7 Conclusion,"comparing to other masking strategies, our proposed adversarial masking approach has achieve substantially better performance on uda problem of named entity span prediction for several domains."
2020.emnlp-main.497.txt,2020,7 Conclusion,the intuition behind the objective is that the adaptation effort should focus on a subset of tokens which are challenging to the mlm.
2020.emnlp-main.497.txt,2020,7 Conclusion,we establish a variational lower bound of the objective function and propose an effective sampling algorithm using dynamic programming and gumbel softmax trick.
2020.emnlp-main.497.txt,2020,7 Conclusion,we present an adversarial objective for further pretraining mlm in uda problem.
2020.emnlp-main.498.txt,2020,4 Conclusion,automatic and human evaluation on several datasets demonstrates the strength and effectiveness of our attack.
2020.emnlp-main.498.txt,2020,4 Conclusion,"in this paper, we have presented a new technique for generating adversarial examples (bae) through contextual perturbations based on the bert masked language model."
2020.emnlp-main.498.txt,2020,4 Conclusion,"we propose inserting and/or replacing tokens from a sentence, in their order of importance for the text classification task, using a bert-mlm."
2020.emnlp-main.499.txt,2020,5 Conclusion,experimental results on three text datasets demonstrate the effectiveness of as-dfd.
2020.emnlp-main.499.txt,2020,5 Conclusion,"however, it’s still challenging to ensure the diversity of generated embeddings under the weak supervision signal and we argue that the gap between synthetic and real sentences still exists."
2020.emnlp-main.499.txt,2020,5 Conclusion,"in the future, we would like to explore data-free distillation on more complex tasks."
2020.emnlp-main.499.txt,2020,5 Conclusion,"in this paper, we propose as-dfd, a novel datafree distillation method applied in text classification tasks."
2020.emnlp-main.499.txt,2020,5 Conclusion,"to dynamically adjust synthetic samples according to students’ situations, we involve an adversarial self-supervised module to quantify students’ abilities."
2020.emnlp-main.499.txt,2020,5 Conclusion,we use plug & play embedding guessing with alignment constraints to solve the problem that gradients cannot update on the discrete text.
2020.emnlp-main.5.txt,2020,8 Conclusions,"although a combined paraphrased reference shows slightly higher correlation for embedding based metrics, it is over twice as expensive to construct such a reference set."
2020.emnlp-main.5.txt,2020,8 Conclusions,"as a closing note, we would like to emphasize that it is more difficult for a human rater to rate a paraphrased translation than a translationese sentence, because the latter may share a similar structure and lexical choice to the source."
2020.emnlp-main.5.txt,2020,8 Conclusions,"combining two standard reference translations by selecting the best rated reference, on the other hand, did increase correlation for the standard reference translations."
2020.emnlp-main.5.txt,2020,8 Conclusions,"contrary to conventional wisdom, we find that multi-reference bleu does not exhibit better correlation with human judgments than single-reference bleu."
2020.emnlp-main.5.txt,2020,8 Conclusions,"future work, can investigate whether finer ratings could correct the bias in favor of lower effort ratings, and how this may interact with document-level evaluation."
2020.emnlp-main.5.txt,2020,8 Conclusions,"nevertheless, the combined paraphrasing references are of higher quality for all techniques when compared to the standard reference counter part."
2020.emnlp-main.5.txt,2020,8 Conclusions,releasing all reference translations gives the community a chance to revisit some of their decisions and measure quality differences for high quality systems and modelling techniques that produce more natural or fluent output.
2020.emnlp-main.5.txt,2020,8 Conclusions,"these findings are confirmed across a wide range of automated metrics, including bleu, chrf, meteor, bertscore and yisi."
2020.emnlp-main.5.txt,2020,8 Conclusions,this work presents a study on the impact of reference quality on the reliability of automated evaluation of machine translation.
2020.emnlp-main.5.txt,2020,8 Conclusions,"to drive this point home, our experiments suggest that standard reference translations may systematically bias against modelling techniques known to improve human-judged quality, raising the question of whether previous research has incorrectly discarded approaches that actually improved the quality of mt."
2020.emnlp-main.5.txt,2020,8 Conclusions,we consider collecting additional human translations as well as generating more diverse and natural references through paraphrasing.
2020.emnlp-main.5.txt,2020,8 Conclusions,"we explore two different approaches to multireference evaluation: (a) standard multi-reference bleu, and (b) selecting the best-rated references for each sentence."
2020.emnlp-main.5.txt,2020,8 Conclusions,"we further demonstrate that the paraphrased references correlate especially well for the top submissions of wmt, and additionally are able to correctly distinguish baselines from systems known to produce more natural output (those augmented with either bt or ape), whose quality tends to be underestimated by references with translationese artifacts."
2020.emnlp-main.5.txt,2020,8 Conclusions,"we observe that the paraphrased references result in more reliable automated evaluations, i.e.stronger correlation with human evaluation for the submissions of the wmt 2019 english→german evaluation campaign."
2020.emnlp-main.5.txt,2020,8 Conclusions,we suggest using a single paraphrased reference for more reliable automatic evaluation going forward.
2020.emnlp-main.5.txt,2020,8 Conclusions,we suspect that human evaluation is also less reliable for complex translations.
2020.emnlp-main.50.txt,2020,7 Conclusions and Future Work,"in the future, we aim to extend graph schemas to encode hierarchical and temporal relations, as well as rich ontologies in open domain."
2020.emnlp-main.50.txt,2020,7 Conclusions and Future Work,"we develope a path language model based method to construct graph schemas containing salient and semantically coherent eventevent paths, which also effectively enhances endto-end information extraction."
2020.emnlp-main.50.txt,2020,7 Conclusions and Future Work,we propose event graph schema induction as a new step towards semantic understanding of interevent connections.
2020.emnlp-main.50.txt,2020,7 Conclusions and Future Work,"we will also assemble our graph schemas to represent more complex scenarios involving multiple events, so they can be applied to more downstream applications including event graph completion and event prediction."
2020.emnlp-main.500.txt,2020,6 Conclusion,experiment results show that the proposed method achieves a high success rate while maintaining a minimum perturbation.
2020.emnlp-main.500.txt,2020,6 Conclusion,"in this work, we propose a high-quality and effective method bert-attack to generate adversarial samples using bert masked language model."
2020.emnlp-main.500.txt,2020,6 Conclusion,"nevertheless, candidates generated from the masked language model can sometimes be antonyms or irrelevant to the original words, causing a semantic loss."
2020.emnlp-main.500.txt,2020,6 Conclusion,"thus, enhancing language models to generate more semantically related perturbations can be one possible solution to perfect bert-attack in the future."
2020.emnlp-main.501.txt,2020,5 Conclusion,"our work prompts a deeper investigation into associated topics such as theoretical similarities to distillation, defenses against such multilingual extractions, and improving performance on out-ofdomain vocabulary."
2020.emnlp-main.501.txt,2020,5 Conclusion,this extracts the task-specific knowledge from the victim and transfers it to languages seen by the multilingual model during its own self-supervised pretraining.
2020.emnlp-main.501.txt,2020,5 Conclusion,we also show that post-hoc fine-tuning on real data is better than mixing real and gibberish data during extraction.
2020.emnlp-main.501.txt,2020,5 Conclusion,"we present results underscoring the importance of vocabulary on the extraction performance, and we provide preliminary evidence to support the hypothesis that the dynamics of model extraction are similar to that of model distillation."
2020.emnlp-main.501.txt,2020,5 Conclusion,we query the monolingual victim model with gibberish data.
2020.emnlp-main.501.txt,2020,5 Conclusion,"we show that high accuracy can be obtained on several languages using this approach, and that this performance improves when the extractor has access to a small fraction of real data."
2020.emnlp-main.501.txt,2020,5 Conclusion,we study the problem of extracting multilingual models by stealing from a monolingual model.
2020.emnlp-main.501.txt,2020,5 Conclusion,we then use the victim’s labels as groundtruth to fine-tune a separate multilingual.
2020.emnlp-main.502.txt,2020,7 Conclusion and Future Work,"as far as we know, this is the first work along this line."
2020.emnlp-main.502.txt,2020,7 Conclusion and Future Work,"by analyzing common corpora and datasets, we confirm that oop pairs are nonnegligible for the task."
2020.emnlp-main.502.txt,2020,7 Conclusion and Future Work,"in the future, we will extend the similar approach to multilingual (yu et al., 2020a) or crosslingual (upadhyay et al., 2018) lexical entailment tasks."
2020.emnlp-main.502.txt,2020,7 Conclusion and Future Work,"moreover, one interesting direction is to use hyperbolic embeddings (le et al., 2019; balazevic et al., 2019) for pattern-based models due to their inherent modeling ability of hierarchies."
2020.emnlp-main.502.txt,2020,7 Conclusion and Future Work,oracle performance analysis shows that our framework has high potentials on severa datasets.
2020.emnlp-main.502.txt,2020,7 Conclusion and Future Work,"supervised by the pattern-based model, the distributional model shows robust capability of scoring oop pairs and pushing the overall performance towards the oracle bounds."
2020.emnlp-main.502.txt,2020,7 Conclusion and Future Work,"to this end, we devise a complementary framework, where a pattern-based and distributional model handle ip and oop pairs separately, while collaborating seamlessly to give unified scores."
2020.emnlp-main.502.txt,2020,7 Conclusion and Future Work,"we formally depict two types of sparsity that extracted pairs face, and indicate that patternbased methods are invalid on the type-ii, i.e., outof-pattern pairs."
2020.emnlp-main.502.txt,2020,7 Conclusion and Future Work,we propose complementing pattern-based and distributional methods for hypernymy detection.
2020.emnlp-main.503.txt,2020,6 Conclusions,"current work explores the utility of alternative sources besides wikidata, in increasing the coverage of the annotations; and the role of the annotations in generating plausible categories for wikipedia articles."
2020.emnlp-main.503.txt,2020,6 Conclusions,they offer a better trade-off between precision vs. recall.
2020.emnlp-main.503.txt,2020,6 Conclusions,"this paper takes advantage of data from wikidata, to extract annotations for understanding the role played by various constituents in determining the meaning of wikipedia categories."
2020.emnlp-main.503.txt,2020,6 Conclusions,"unlike in previous work, the annotations are semanticallyanchored properties and values, rather than ambiguous strings."
2020.emnlp-main.504.txt,2020,7 Conclusion,"empirical experiments have proved the effectiveness of sref from both knowledgebased and supervised perspectives, obtaining major and minor improvements over previous state-of-the-art performance, respectively."
2020.emnlp-main.504.txt,2020,7 Conclusion,"for future work, we intend to scale srefkb to a multilingual version and explore the possibilities of using the multilingual wordnet so that abundant knowledge regarding english can be transferred to other languages."
2020.emnlp-main.504.txt,2020,7 Conclusion,it is also worth investigating regarding how to better incorporate sense embedding into other downstream tasks.
2020.emnlp-main.504.txt,2020,7 Conclusion,"we have introduced sref, a synset relationenhanced framework with a try-again mechanism that takes into account wordnet relations and augments wordnet glosses with mentions from the web under simple hypotheses and rules."
2020.emnlp-main.505.txt,2020,6 Conclusion,"extensive experiments on the real-world dataset show that our approach can generate highquality, keyphrase-relevant, and diverse news headlines, which outperforms many strong baselines."
2020.emnlp-main.505.txt,2020,6 Conclusion,"in this paper, we demonstrate how to enable news headline generation systems to be aware of keyphrases such that the model can generate diverse news headlines in a controlled manner."
2020.emnlp-main.505.txt,2020,6 Conclusion,"moreover, we propose a keyphraseaware news multi-headline generation model that contains a multi-source transformer decoder with three variants of attention-based fusing mechanisms."
2020.emnlp-main.505.txt,2020,6 Conclusion,"we also build a first large-scale keyphrase-aware news headline corpus, which is based on mining the keyphrases of users’ interests in news articles with user queries."
2020.emnlp-main.506.txt,2020,6 Conclusions,"however, low recall on the inconsistent summaries and false positive samples remain as challenges."
2020.emnlp-main.506.txt,2020,6 Conclusions,"in this paper, we proposed a novel approach to correct inconsistent content in summaries generated by abstractive summarization models."
2020.emnlp-main.506.txt,2020,6 Conclusions,our human evaluation indicates that our model is able to correct some factually inconsistent summaries generated by abstractive summarization model.
2020.emnlp-main.506.txt,2020,6 Conclusions,our model achieved promising performance on our artificial test set and outperformed previous models on the manually annotated test set by wide margins.
2020.emnlp-main.506.txt,2020,6 Conclusions,we train an end-to-end correction model with artificial examples created by corrupting reference summaries.
2020.emnlp-main.507.txt,2020,9 Conclusion,experiments across both in-domain and out-of-domain settings demonstrate our approach outperforms strong extractive baselines while creating well-formed summaries.
2020.emnlp-main.507.txt,2020,9 Conclusion,"in this work, we present a compressive summarization system that decomposes span-level compression into two learnable objectives, plausibility and salience, on top of a minimal set of rules derived from a constituency tree."
2020.emnlp-main.508.txt,2020,6 Conclusion,"all of these give insight into what conditions most heavily restrict the model’s generation: generating an observed bigram (copying), low syntactic distance, and attention which can easily identify decoder context in the source document."
2020.emnlp-main.508.txt,2020,6 Conclusion,"this work analyzes pre-trained summarization models via uncertainty, or the entropy of decoding decisions."
2020.emnlp-main.508.txt,2020,6 Conclusion,we believe this approach can power future analyses of pre-trained text generation systems.
2020.emnlp-main.508.txt,2020,6 Conclusion,"we pursue several lines of inquiry: uncertainty can help us understand copying document spans vs. generating novel text, the behavior of models in different syntactic environments, and coarse properties of the model’s attention distribution."
2020.emnlp-main.509.txt,2020,5 Conclusion,highlighting is important to help readers sift through a large amount of texts and quickly grasp the main points.
2020.emnlp-main.509.txt,2020,5 Conclusion,"the method can be extended to other text genres such as public policies to aid reader comprehension, which will be our future work to explore."
2020.emnlp-main.509.txt,2020,5 Conclusion,"we describe a novel methodology to generate a rich set of self-contained segments from the documents, then use determinantal point processes to identify summary highlights."
2020.emnlp-main.509.txt,2020,5 Conclusion,we make a first attempt to create sub-sentence summary highlights that are understandable and require minimum information from the surrounding context.
2020.emnlp-main.51.txt,2020,5 Conclusion,"for future work, we plan to extend the framework towards an end-to-end system with event extraction."
2020.emnlp-main.51.txt,2020,5 Conclusion,it also presents promising event complex extraction results on red that is external to training.
2020.emnlp-main.51.txt,2020,5 Conclusion,"on two benchmark datasets, the proposed method outperforms sota statistical learning methods and data-driven methods for each task, without using data that is jointly annotated with the two classes of relations."
2020.emnlp-main.51.txt,2020,5 Conclusion,"the proposed framework bridges temprel and subevent relation extraction tasks with a comprehensive set of logical constraints, which are enforced during learning by converting them into differentiable objective functions."
2020.emnlp-main.51.txt,2020,5 Conclusion,"thus, our work shows that the global consistency of the event complex significantly helps understanding both temporal order and event membership."
2020.emnlp-main.51.txt,2020,5 Conclusion,we also seek to extend the conjunctive constraints along with event argument relations.
2020.emnlp-main.51.txt,2020,5 Conclusion,we propose a joint constrained learning framework for extracting event complexes from documents.
2020.emnlp-main.510.txt,2020,5 Conclusions,"the promising empirical results motivate us to explore further the integration of more external knowledge and other rich forms of supervisions (e.g., constraints, interactions, auxiliary models, adversaries) (hu and xing, 2020; ziegler et al., 2019) in learning."
2020.emnlp-main.510.txt,2020,5 Conclusions,this paper studies the new problem of summarizing a document on arbitrary relevant aspects.
2020.emnlp-main.510.txt,2020,5 Conclusions,"to tackle the challenge of lacking supervised data, we have developed a new knowledge-informed weakly supervised method that leverages external knowledge bases."
2020.emnlp-main.510.txt,2020,5 Conclusions,"we are also interested in extending the aspect-based summarization in more application scenarios (e.g., summarizing a document corpus)."
2020.emnlp-main.511.txt,2020,4 Conclusion,"in particular, a novel relational pointer decoder is developed to incorporate the relative ordering information into the pointer network with a deep relational module, which leverages bert to fully exploit the pairwise relationships between sentences helping generate an ordered sequence."
2020.emnlp-main.511.txt,2020,4 Conclusion,"in this work, we develop a new bertenhanced relational sentence ordering network (berson) by integrating bert with the pointer network for a better coherence modeling."
2020.emnlp-main.511.txt,2020,4 Conclusion,"the experiments on six datasets demonstrate the superiority of berson to the baselines, which achieves the state-of-the-art performance across the datasets."
2020.emnlp-main.512.txt,2020,5 Conclusion,"extensive experiments have been conducted on the #ubuntu dataset, which show that our method achieves state-of-the-art performance on both link and conversation prediction tasks without using any handcrafted features."
2020.emnlp-main.512.txt,2020,5 Conclusion,"from the decoding side, a promising direction would be to make global inference in a more efficient way."
2020.emnlp-main.512.txt,2020,5 Conclusion,"from the encoding side, it would be ideal to encode an utterance within its context."
2020.emnlp-main.512.txt,2020,5 Conclusion,"in contrast to previous work, our method reduces the effort of complicated feature engineering by proposing an utterance encoder and a pointer module that models inter-utterance interactions."
2020.emnlp-main.512.txt,2020,5 Conclusion,"in this paper, we have proposed a novel online framework for disentangling multi-party conversations."
2020.emnlp-main.512.txt,2020,5 Conclusion,link prediction in our framework is modeled as a pointing function with a multinomial distribution over previous utterances.
2020.emnlp-main.512.txt,2020,5 Conclusion,"moreover, we propose a joint-training framework that enables the pointer network to learn more contextual information."
2020.emnlp-main.512.txt,2020,5 Conclusion,"one challenge for this problem is that conversations are tangled, so sequential encoding methods like the one of (sordoni et al., 2015) would not be appropriate."
2020.emnlp-main.512.txt,2020,5 Conclusion,there are some possible future directions from our work.
2020.emnlp-main.512.txt,2020,5 Conclusion,"this can be done in two ways, encoding, and decoding."
2020.emnlp-main.512.txt,2020,5 Conclusion,this reminds us that neither our and most of the existing methods took good advantage of graph information in disentangling conversations.
2020.emnlp-main.512.txt,2020,5 Conclusion,we also show that our framework supports online decoding.
2020.emnlp-main.512.txt,2020,5 Conclusion,we have shown in our experiments that self-link predictions have a significant impact on clustering results.
2020.emnlp-main.513.txt,2020,6 Conclusion,"empirical results on multiple corpora, including two new datasets released, show that our model is able to outperform previous work by a consistent margin, also successfully being able to leveraging contextualized word representations."
2020.emnlp-main.513.txt,2020,6 Conclusion,for future work we are interested in exploring how definition modeling could be adapted to a multilingual or cross-lingual setting.
2020.emnlp-main.513.txt,2020,6 Conclusion,in this paper we have introduced a generative model that directly combines distributional and lexical semantics via a continuous latent variable for the task of definition modeling.
2020.emnlp-main.514.txt,2020,7 Conclusion,further analysis verifies the necessity of utilizing ner knowledge for pre-training models.
2020.emnlp-main.514.txt,2020,7 Conclusion,"on three popular ner benchmarks, we found consistent improvements over both state-of-the-art supervised and weakly-supervised methods."
2020.emnlp-main.514.txt,2020,7 Conclusion,"though conceptually simple, our framework is highly effective and easy to implement."
2020.emnlp-main.514.txt,2020,7 Conclusion,"we investigated coarse-to-fine entity knowledge enhanced pre-training for named entity recognition, which integrates three kinds of entity knowledge with different granularity levels."
2020.emnlp-main.515.txt,2020,5 Conclusion and Future Work,attrgnn can integrate both attribute and relation triples with varying importance for better performance.
2020.emnlp-main.515.txt,2020,5 Conclusion and Future Work,"experimental results under the regular and hard settings present significant improvements of our proposed model, and the severe dataset bias can be effectively alleviated in our proposed hard setting."
2020.emnlp-main.515.txt,2020,5 Conclusion and Future Work,"in the future, we are interested in replacing bert with knowledge enhanced and number sensitive text representations models (cao et al., 2017; geva et al., 2020)."
2020.emnlp-main.515.txt,2020,5 Conclusion and Future Work,we propose a novel ea model (attrgnn) and contribute a hard experimental setting for practical evaluation.
2020.emnlp-main.516.txt,2020,7 Conclusion,"in the future, we want to extend our system to other few-shot sequence tagging problems such as part-of-speech tagging and slot filling."
2020.emnlp-main.516.txt,2020,7 Conclusion,our systems overcomes these challenges with nearest neighbor learning and structured decoding.
2020.emnlp-main.516.txt,2020,7 Conclusion,we further propose a standard evaluation setup for few-shot ner and show that structshot significantly outperforms prior sota systems on popular benchmarks across multiple domains.
2020.emnlp-main.516.txt,2020,7 Conclusion,we identify two weaknesses of previous systems related to their handling of o class and modeling label dependencies.
2020.emnlp-main.516.txt,2020,7 Conclusion,"we introduce structshot, a simple few-shot ner system that achieves sota performance without any few-shot specific training."
2020.emnlp-main.517.txt,2020,4 Concluding Remarks,experiments show the efficacy of our approach.
2020.emnlp-main.517.txt,2020,4 Concluding Remarks,"in particular, we focus on a challenging scenario, where entity names are given as textual strings without context."
2020.emnlp-main.517.txt,2020,4 Concluding Remarks,one immediate future work is to generate explanations for model predictions using structured vector.
2020.emnlp-main.517.txt,2020,4 Concluding Remarks,we proposed a framework for learning structured representation of entity names under low-resource settings.
2020.emnlp-main.518.txt,2020,5 Conclusion,"results on a news dataset and two long-text ner datasets show that it is highly effective to explicitly integrate the document-specific entities into bert pre-training with a char-entity-transformer structure, and our method outperforms the state-of-the-art methods for chinese ner."
2020.emnlp-main.518.txt,2020,5 Conclusion,we investigated an entity enhanced bert pretraining method for chinese ner.
2020.emnlp-main.519.txt,2020,7 Conclusion,"finally, we show that knowledge distillation can further improve biencoder model performance."
2020.emnlp-main.519.txt,2020,7 Conclusion,future work includes: • enriching entity representations by adding entity type and entity graph information; • modeling coherence by jointly resolving mentions in a document; • extending our work to other languages and other domains; • joint models for mention detection and entity linking.
2020.emnlp-main.519.txt,2020,7 Conclusion,"we present evaluations of the accuracy-speed trade-off inherent to large pre-trained models, and show that it is possible to achieve efficient linking with modest loss of accuracy."
2020.emnlp-main.519.txt,2020,7 Conclusion,"we proposed a conceptually simple, scalable, and highly effective two stage approach for entity linking."
2020.emnlp-main.519.txt,2020,7 Conclusion,"we show that our bert-based model outperforms ir methods for entity retrieval, and achieved new state-of-the-art results on recently introduced zero-shot entity linking dataset, wikilinksned unseen-mentions dataset, and the more established tackbp-2010 benchmark, without any task-specific heuristics or external entity knowledge."
2020.emnlp-main.52.txt,2020,6 Conclusion,experimental results demonstrate that our model outperforms previous state-of-the-art methods.
2020.emnlp-main.52.txt,2020,6 Conclusion,"in this paper, we introduce incremental learning into event detection and propose a knowledge consolidation network to preserve previously learned knowledge."
2020.emnlp-main.52.txt,2020,6 Conclusion,"moreover, to mitigate the adverse effect of class imbalance, we propose the hierarchical distillation to learn the previous knowledge from the original model."
2020.emnlp-main.52.txt,2020,6 Conclusion,"to alleviate semantic ambiguity, we devise the prototype enhanced retrospection to reserve the most representative examples."
2020.emnlp-main.520.txt,2020,7 Conclusion,an exciting direction is to leverage visuals of each step to deal with unmentioned entities and indirect effects.
2020.emnlp-main.520.txt,2020,7 Conclusion,"as future work, we will explore more sophisticated models that can address the highlighted shortcomings of the current model."
2020.emnlp-main.520.txt,2020,7 Conclusion,"to this end, we crowdsourced a large, high-quality dataset with examples for this task."
2020.emnlp-main.520.txt,2020,7 Conclusion,we also established a strong generation baseline highlighting the difficulty of this task.
2020.emnlp-main.520.txt,2020,7 Conclusion,we presented the first dataset to track entities in open domain procedural text.
2020.emnlp-main.521.txt,2020,7 Conclusion,"given that our experiments show a 25% increase in the candidate generation, one future research direction is to improve candidate ranking in lrl by incorporating coherence statistics and entity types."
2020.emnlp-main.521.txt,2020,7 Conclusion,"moreover, given the effectiveness of query logs, we believe it can be applied to other cross-lingual tasks like relation extraction and knowledge base completion."
2020.emnlp-main.521.txt,2020,7 Conclusion,the analysis identifies the inherent lack of sufficient inter-lingual supervision signals as a key shortcoming of the current approach.
2020.emnlp-main.521.txt,2020,7 Conclusion,"this leads to proposing a rather simple method that leverages query logs, that are highly effective in addressing these challenges."
2020.emnlp-main.521.txt,2020,7 Conclusion,"this work provides a thorough analysis of existing lrl xel techniques, focusing on the step of generating english candidates for foreign language mentions."
2020.emnlp-main.522.txt,2020,7 Conclusion,"despite being originally designed with questions in mind, we believe elq could also generalize to longer, well-formed documents."
2020.emnlp-main.522.txt,2020,7 Conclusion,"furthermore, when applied to a qa model, elq improves that model’s end qa accuracy."
2020.emnlp-main.522.txt,2020,7 Conclusion,we proposed an end-to-end model for entity linking on questions that jointly performs mention detection and disambiguation with one pass through bert.
2020.emnlp-main.522.txt,2020,7 Conclusion,"we showed that it is highly efficient, and that it outperforms previous state-of-the-art models on two benchmarks."
2020.emnlp-main.523.txt,2020,6 Conclusions,"future work involves applying luke to domain-specific tasks, such as those in biomedical and legal domains."
2020.emnlp-main.523.txt,2020,6 Conclusions,"in this paper, we propose luke, new pretrained contextualized representations of words and entities based on the transformer."
2020.emnlp-main.523.txt,2020,6 Conclusions,luke outputs the contextualized representations of words and entities using an improved transformer architecture with using a novel entity-aware self-attention mechanism.
2020.emnlp-main.523.txt,2020,6 Conclusions,the experimental results prove its effectiveness on various entity-related tasks.
2020.emnlp-main.524.txt,2020,8 Conclusion,"future directions include exploration of other knowledge bases to help the inference process and applying our simile generation approach to different creative nlg tasks such as pun (he et al., 2019), sarcasm (chakrabarty et al., 2020), and hyperbole (troiano et al., 2018)."
2020.emnlp-main.524.txt,2020,8 Conclusion,human and automatic evaluations show that our best model is successful at generating similes.
2020.emnlp-main.524.txt,2020,8 Conclusion,"our experimental results further show that to truly be able to generate similes based on actual metaphoric or conceptual mappings, it is important to incorporate some common sense knowledge about the topics and their properties."
2020.emnlp-main.524.txt,2020,8 Conclusion,we establish a new task for nlg: simile generation from literal sentences.
2020.emnlp-main.524.txt,2020,8 Conclusion,we propose a novel way of creating parallel corpora and a transfer-learning approach for generating similes.
2020.emnlp-main.525.txt,2020,7 Conclusion,"importantly, real storium authors evaluate model outputs by adding and removing text to create their own stories."
2020.emnlp-main.525.txt,2020,7 Conclusion,our dataset and evaluation platform will be made publicly available to spur progress into story generation.
2020.emnlp-main.525.txt,2020,7 Conclusion,storium contains 6k long stories annotated with structural metadata useful for conditioning language models.
2020.emnlp-main.525.txt,2020,7 Conclusion,"we devise a metric on top of their edits that correlates strongly with judgments of the relevance of the generated text, which user interviews suggest is the most important area for improvement moving forward."
2020.emnlp-main.525.txt,2020,7 Conclusion,"we introduce the storium dataset and evaluation platform for machine-in-the-loop story generation, built from an online collaborative storytelling community."
2020.emnlp-main.526.txt,2020,7 Conclusion,"although we focus on the recipe domain, our method naturally generalizes to other domains where procedural tasks can be substantively rewritten."
2020.emnlp-main.526.txt,2020,7 Conclusion,"as language generation becomes more grounded in signals outside of language, work in the area of substantive transfer becomes increasingly relevant."
2020.emnlp-main.526.txt,2020,7 Conclusion,"for example, one could rewrite technical documentation by constraining on the target operating system, rewrite lesson plans by constraining on the target grade level, or rewrite furniture assembly instructions by constraining on the tools used."
2020.emnlp-main.526.txt,2020,7 Conclusion,"further, we find that conditioning the model with step-level constraints allows the rewritten recipes to stay closer to the source recipe while successfully obeying the dietary restriction."
2020.emnlp-main.526.txt,2020,7 Conclusion,"more broadly, this approach makes it possible to customize existing content to better fit a user’s physical reality, whether that entails accommodating their dietary needs, updating their schedule based on the weather forecast, or providing information on a dashboard based on what’s in their field of view."
2020.emnlp-main.526.txt,2020,7 Conclusion,"we introduce the novel task of document-level targeted content transfer and address it in the recipe domain, where our documents are recipes and our targeted constraints are dietary restrictions."
2020.emnlp-main.526.txt,2020,7 Conclusion,we propose a novel model for rewriting a source recipe one step at time by making use of document-level context.
2020.emnlp-main.526.txt,2020,7 Conclusion,"we show that our proposed rewriter is able to outperform several existing techniques, as judged both by automatic metrics and human evaluators."
2020.emnlp-main.527.txt,2020,8 Conclusion and Future Work,"another interesting line of future work is to investigate the use of t2g2 for generating user utterances, which could be useful for dialogue data augmentation and user simulation."
2020.emnlp-main.527.txt,2020,8 Conclusion and Future Work,"coupled with pretrained language models, the template guided approach enables zero-shot generalization to new domains with little effort."
2020.emnlp-main.527.txt,2020,8 Conclusion and Future Work,"in this work, we proposed schema guided and template guided input representation schemes for task oriented response generation."
2020.emnlp-main.527.txt,2020,8 Conclusion and Future Work,"moreover, we show that it can lead to drastic reduction in annotation costs."
2020.emnlp-main.527.txt,2020,8 Conclusion and Future Work,"obtaining annotated data in nonenglish languages is an even bigger challenge, making the sample efficiency of our template rewriting approach especially suited to this setting."
2020.emnlp-main.527.txt,2020,8 Conclusion and Future Work,this requires adding the ability to generate utterances with stylistic variations to capture different user personalities while maintaining consistency in style and vocabulary over a single dialogue.
2020.emnlp-main.527.txt,2020,8 Conclusion and Future Work,we also hope to apply t2g2 to languages other than english.
2020.emnlp-main.527.txt,2020,8 Conclusion and Future Work,"we also present the first set of results on the multidomain sgd dataset, which we hope will pave the way for further research in few-shot, zero-shot and multi-domain language generation."
2020.emnlp-main.527.txt,2020,8 Conclusion and Future Work,"while in this paper we use standard pre-trained models, designing pre-training tasks tailored to sentence fusion is an interesting line of future work."
2020.emnlp-main.528.txt,2020,6 Conclusion,"error analysis reveals that there exist gaps in lerc’s ability to handle certain phenomena, such as correctly leveraging the passage."
2020.emnlp-main.528.txt,2020,6 Conclusion,future work involves collecting data to addresses weaknesses of lerc.
2020.emnlp-main.528.txt,2020,6 Conclusion,"this in turn will allow better learned metrics, which can be used to evaluate ever more complex models."
2020.emnlp-main.528.txt,2020,6 Conclusion,"using mocha, we train a learned metric, lerc, that outperforms all existing metrics and is much more robust when evaluated on a set of minimal pairs."
2020.emnlp-main.528.txt,2020,6 Conclusion,we also anticipate a continual cycle of generative rc model and dataset developments that will enable easier collection of more diverse and useful candidates.
2020.emnlp-main.528.txt,2020,6 Conclusion,"we present mocha, a dataset of human judgement scores for training and evaluating generative reading comprehension metrics."
2020.emnlp-main.528.txt,2020,6 Conclusion,"while we have demonstrated that lerc is a better metric for evaluating generative reading comprehension than any existing metric, considerable work remains."
2020.emnlp-main.529.txt,2020,6 Conclusion,a written paragraph itself contains various inductive coherence signals to be learned through selfsupervision.
2020.emnlp-main.529.txt,2020,6 Conclusion,combining the heterogeneous planning systems will be a crucial step towards developing a human-like language generation.
2020.emnlp-main.529.txt,2020,6 Conclusion,"for example, one can study the generation quality with respect to the position of the target sentences (beginning, middle, end), the comparison of plan keywords predicted by human and system, the effect of data augmentation by their positions (e.g., masking the only middle), the generation quality with respect to the ratio between masked and unmasked sentences, and more."
2020.emnlp-main.529.txt,2020,6 Conclusion,"for instance, one can write a simple structure like “(causality (elaborate (buy, coffee)) (pay, tip, 12 dollars))” then the system can generate a long, coherent text reflected by the structure."
2020.emnlp-main.529.txt,2020,6 Conclusion,"last, text planning is a cognitive function commonly used in human language generation."
2020.emnlp-main.529.txt,2020,6 Conclusion,"motivated by this, we propose a paragraph completion task for measuring textual coherence from a long document using different types of self-supervision signals."
2020.emnlp-main.529.txt,2020,6 Conclusion,"our results suggest several promising directions: although our ablation tests show the effect of each self-supervision module, types of plan keywords, and the amount of keywords with respect to generation quality, there are more spaces to explore in self-supervised text planning."
2020.emnlp-main.529.txt,2020,6 Conclusion,"our selfsupervised planning, in addition to other types of planning (e.g., discourse, goals, coreference, tenses) can be an important step toward modeling a long-term coherence in text generation."
2020.emnlp-main.529.txt,2020,6 Conclusion,predicting such structural plans from context and imposing them into the generator would be a potential direction for future work.
2020.emnlp-main.529.txt,2020,6 Conclusion,"second, we can extend the set of plan keywords to be more structured like a discourse tree."
2020.emnlp-main.529.txt,2020,6 Conclusion,"ssplanner consists of different kinds of self-supervision modules: sentence positions, a sequence of words or sentences, and the topical relationship between context and target."
2020.emnlp-main.529.txt,2020,6 Conclusion,"to generate more human-like utterances, different planning stages should be simultaneously combined together (kang, 2020), such as abstractive planning, strategic planning, coherence planning, and diversity planning."
2020.emnlp-main.529.txt,2020,6 Conclusion,"to solve the task, we propose a text planner ssplanner that explicitly predicts topical content keywords, and then guides the surface generator using the predicted plan keywords."
2020.emnlp-main.53.txt,2020,5 Conclusion and Future Work,experiments show that our approach achieves the state-of-the-art on supervised event extraction and discovers a set of high-quality unseen types.
2020.emnlp-main.53.txt,2020,5 Conclusion and Future Work,"in the future, we will extend this approach to argument role induction to discover complete event schemas."
2020.emnlp-main.53.txt,2020,5 Conclusion and Future Work,we have designed a semi-supervised vector quantized variational autoencoder approach which automatically learns a discrete representations for each seen and unseen type and predict a type for each candidate trigger.
2020.emnlp-main.530.txt,2020,7 Conclusion,"we present inquisitive, a large dataset of questions that reflect semantic and discourse processes during text comprehension."
2020.emnlp-main.530.txt,2020,7 Conclusion,we show that people use rich language and adopt a range of pragmatic strategies to generate such questions.
2020.emnlp-main.530.txt,2020,7 Conclusion,"we then present question generation models trained on this data, demonstrating several aspects that generating inquisitive questions is a feasible yet challenging task."
2020.emnlp-main.531.txt,2020,7 Conclusion,cobert is free from hyper-parameter tuning and universally applicable to the task of response selection in any domain.
2020.emnlp-main.531.txt,2020,7 Conclusion,"finally, we present the first empirical study on the impact of persona on empathetic responding."
2020.emnlp-main.531.txt,2020,7 Conclusion,the results reveal an empirical link between persona and empathy in human conversations and may suggest that persona has a greater impact on empathetic conversations than non-empathetic ones.
2020.emnlp-main.531.txt,2020,7 Conclusion,"we present a new task and a large-scale multidomain dataset, pec, towards persona-based empathetic conversations."
2020.emnlp-main.531.txt,2020,7 Conclusion,"we then propose cobert, an effective and efficient model that obtains substantially better performance than competitive baselines on pec, including the state-of-the-art poly-encoder and several bert-adapted models."
2020.emnlp-main.532.txt,2020,6 Conclusions and Future Work,further analysis on human-bot dialogue performance demonstrated the potential privacy risks with advanced personalized dialogue techniques.
2020.emnlp-main.532.txt,2020,6 Conclusions and Future Work,"in the future, we will explore fullfledged solutions to address the privacy concerns of both humans and dialogue systems."
2020.emnlp-main.532.txt,2020,6 Conclusions and Future Work,our experimental results demonstrate the effectiveness of our methods in comparison with the competitive baselines on that dataset.
2020.emnlp-main.532.txt,2020,6 Conclusions and Future Work,"this work is the first step towards fully preventing leakage of privacy in text, which still requires pas or users to select and hide sensitive information."
2020.emnlp-main.532.txt,2020,6 Conclusions and Future Work,"to tackle this task, we proposed two new alignment models and created a dataset persona-leakage for evaluation."
2020.emnlp-main.532.txt,2020,6 Conclusions and Future Work,we formulate protection of personal information in conversations as a weakly supervised alignment between personal information and dialogue utterances.
2020.emnlp-main.532.txt,2020,6 Conclusions and Future Work,we hope this work and the dataset will pave the way for the research on privacy leakage in conversations.
2020.emnlp-main.533.txt,2020,6 Conclusion,"based on our new formulation, we propose topic-bert for response selection in multi-party conversations, which consists of two steps: (1) a topic-based pretraining to embed topic information into bert with self-supervised learning, and (2) a multi-task learning on our pretrained model by jointly training response selection and dynamic topic prediction and disentanglement tasks."
2020.emnlp-main.533.txt,2020,6 Conclusion,empirically the proposed topic-bert achieved the state-of-the-art results on the dstc8 ubuntu irc datasets.
2020.emnlp-main.533.txt,2020,6 Conclusion,this paper presented a new formulation of response selection in multi-party conversations from a novel dynamic topic tracking perspective.
2020.emnlp-main.534.txt,2020,5 Conclusion,"as a result, the student is effectively regularized to reach a robust local minimum that represents better generalization performance."
2020.emnlp-main.534.txt,2020,5 Conclusion,"evaluation on four datasets demonstrates the effectiveness and the scalability of our approach, compared to the state-of-the-art baselines."
2020.emnlp-main.534.txt,2020,5 Conclusion,"for future work, we will incorporate pre-trained models into our framework (e.g., bert as a teacher and gpt as a student) to further unlock the performance improvement and explore how to balance diverse prior knowledge from multiple teachers."
2020.emnlp-main.534.txt,2020,5 Conclusion,"in this work, we introduce the future conversation with the corresponding dialogue history to learn the implicit conversation scenario, which entails latent context knowledge and specifies how people interact in the real world."
2020.emnlp-main.534.txt,2020,5 Conclusion,"moreover, detailed analyses illustrate how imitating implicit scenarios regularizes the student model."
2020.emnlp-main.534.txt,2020,5 Conclusion,the proposed framework enables the generation of responses that pertain more closely to the scenario indicated by the given dialogue history.
2020.emnlp-main.534.txt,2020,5 Conclusion,the scenario-based teacher model first learns to generate responses with access to both the future conversation and the dialogue history and then a conventional student model is trained to imitate the teacher by hierarchical supervisory signals.
2020.emnlp-main.534.txt,2020,5 Conclusion,"to incorporate such scenario knowledge without requiring future conversation in inference, we propose an imitation learning framework."
2020.emnlp-main.535.txt,2020,6 Conclusion,the model can be trained end-to-end with a simple unified language model architecture.
2020.emnlp-main.535.txt,2020,6 Conclusion,"we present moviechats: a movie-domain chatbot built upon a large-scale, high-quality conversational corpus with fine-grained annotations."
2020.emnlp-main.535.txt,2020,6 Conclusion,"we show that our model, powered by well-defined knowledge grounding, is able to approach human performance in some perspective, though still lagged behind when it comes to dealing with detailed knowledge or long-turn consistency."
2020.emnlp-main.536.txt,2020,7 Conclusions,"while space limitations preclude a reiteration of all the observations we have made, we believe the key conclusions are: (1) the relative performances of the resolvers are consistent across datasets; (2) for each resolver, higher mention detection performance always yields better coreference performance; (3) the newest resolvers perform better because of not only improved mention detection, but also improved mention span representations, and they improved the resolution of both easyand difficult-to-resolve anaphors; (4) all resolvers can be improved by improving mention detection, anaphoricity determination, and entity type detection; and (5) our perturbation results suggest that coreference performance is most sensitive to those words/phrases in the input that have the greatest impact on mention detection performance."
2020.emnlp-main.537.txt,2020,5 Conclusions,experimental results showed that introducing srl could significantly improve the rewriting performance without adding extra model parameters.
2020.emnlp-main.537.txt,2020,5 Conclusions,"for this purpose, we adapted traditional srl to the conversational scenario by annotating cross-turn annotations on 3,000 dialogues."
2020.emnlp-main.537.txt,2020,5 Conclusions,"in this paper, we introduce a novel srl-guided framework for enhancing dialogue rewriting."
2020.emnlp-main.538.txt,2020,6 Conclusion,"experiment results on two newly constructed online conversation datasets, weibo and reddit, show that our model outperforms the previous state-of-theart models."
2020.emnlp-main.538.txt,2020,6 Conclusion,further discussions provide more insights on quoting in online conversations.
2020.emnlp-main.538.txt,2020,6 Conclusion,"we present a novel quotation generation framework for online conversations via the modeling of topic, interaction, and query consistency."
2020.emnlp-main.539.txt,2020,7 Conclusion and Discussion,"although there has been a lot of dialogue generation models in this field, most of them still can’t understand the consistency relationship in the generation process."
2020.emnlp-main.539.txt,2020,7 Conclusion and Discussion,"because the kvpi dataset has paired key-value profiles and dialogues, it can also be a high-quality resource for personalized dialogue generation tasks."
2020.emnlp-main.539.txt,2020,7 Conclusion and Discussion,evaluation results show the effectiveness of the proposed approach.
2020.emnlp-main.539.txt,2020,7 Conclusion and Discussion,"furthermore, because we have fine-grained consistency labels, this dataset also provides an opportunity to leverage natural language understanding models to assist dialogue generation models."
2020.emnlp-main.539.txt,2020,7 Conclusion and Discussion,"in this work, we introduce a large-scale annotated dataset to facilitate the study of profile consistency identification in open-domain dialogues."
2020.emnlp-main.539.txt,2020,7 Conclusion and Discussion,one of the major bottlenecks is the lack of data.
2020.emnlp-main.539.txt,2020,7 Conclusion and Discussion,we believe kvpi will be a useful resource for the research of open-domain dialogue consistency.
2020.emnlp-main.539.txt,2020,7 Conclusion and Discussion,we further test the proposed method on two downstream tasks.
2020.emnlp-main.539.txt,2020,7 Conclusion and Discussion,we hope that the data will aid training dialogue agents to be more consistent.
2020.emnlp-main.539.txt,2020,7 Conclusion and Discussion,we leverage the structure information in profiles to enrich the bert representations and obtain significant performance improvements over strong baselines.
2020.emnlp-main.54.txt,2020,5 Conclusion,the proposed method leverages both the structural and semantic information of the external knowledge base by performing dynamic multi-hop reasoning on the relational paths.
2020.emnlp-main.54.txt,2020,5 Conclusion,we also demonstrate the interpretability of our method with inferred reasoning paths that provide rationale to the generated results.
2020.emnlp-main.54.txt,2020,5 Conclusion,we conduct extensive experiments and empirically show that our method outperforms existing approaches that integrate commonsense knowledge to pre-trained language models on three text generation tasks.
2020.emnlp-main.54.txt,2020,5 Conclusion,we present generation with multi-hop reasoning flow that reasons over structured commonsense knowledge during text generation.
2020.emnlp-main.540.txt,2020,5 Conclusion,it also fills the gap between experimental and practical applications on multi-label samples.
2020.emnlp-main.540.txt,2020,5 Conclusion,it uses self-attention to capture dependencies between law articles and makes a unique representation for each candidate label for prediction.
2020.emnlp-main.540.txt,2020,5 Conclusion,our proposed lemm model uses elements of the manually labeled law articles to generate multiple representations of a fact.
2020.emnlp-main.540.txt,2020,5 Conclusion,the experiments verify that the element-aware multirepresentation can better extract features of the factual information and the dependencies between law articles are beneficial to the law article prediction task.
2020.emnlp-main.540.txt,2020,5 Conclusion,the model achieves state-of-the-art performance in benchmark datasets.
2020.emnlp-main.540.txt,2020,5 Conclusion,we propose a model that predicts relevant law articles on multi-label samples by simulating the human judging process.
2020.emnlp-main.541.txt,2020,6 Conclusion,"interesting future work includes developing a fast and efficient version of re-net, and modeling lasting events and performing inference on the long-lasting graph structures."
2020.emnlp-main.541.txt,2020,6 Conclusion,"re-net defines the joint probability of all events, and thus is capable of inferring graphs in a sequential manner."
2020.emnlp-main.541.txt,2020,6 Conclusion,the experiment revealed that re-net outperforms all the static and temporal methods and our extensive analysis shows its strength.
2020.emnlp-main.541.txt,2020,6 Conclusion,"to tackle the extrapolation problem, we proposed recurrent event network (re-net) to model temporal, multi-relational, and concurrent interactions between entities."
2020.emnlp-main.542.txt,2020,6 Conclusion and Future Work,"emoji prediction has become a popular task in the nlp community, but the lack of publicly available large-scale datasets with high-quality annotations remains a bottleneck for this task."
2020.emnlp-main.542.txt,2020,6 Conclusion and Future Work,"first, the aspect-level annotation method can be applied to other nlp tasks."
2020.emnlp-main.542.txt,2020,6 Conclusion and Future Work,"in this paper, we annotated a publicly available twitter dataset for the emoji prediction task."
2020.emnlp-main.542.txt,2020,6 Conclusion and Future Work,"our dataset contains three types of annotations, namely the passage-level multi-class and multi-label classification labels, and the aspectlevel multi-class classification annotations."
2020.emnlp-main.542.txt,2020,6 Conclusion and Future Work,our labeled datasets are available upon request.
2020.emnlp-main.542.txt,2020,6 Conclusion and Future Work,"second, our annotations in the emoji prediction dataset can be enhanced by including an enriched label set."
2020.emnlp-main.542.txt,2020,6 Conclusion and Future Work,there are two main paths for extending this work.
2020.emnlp-main.542.txt,2020,6 Conclusion and Future Work,"this method showed great performance in labeling our dataset, and can potentially be used in other tasks as well."
2020.emnlp-main.542.txt,2020,6 Conclusion and Future Work,we also benchmarked our dataset using a pre-trained bert-largecased model.
2020.emnlp-main.542.txt,2020,6 Conclusion and Future Work,we designed an annotation method for aspect-level annotations using the self-attention mechanism in transformer networks.
2020.emnlp-main.542.txt,2020,6 Conclusion and Future Work,we validated our annotations both automatically and manually to ensure their quality.
2020.emnlp-main.543.txt,2020,7 Conclusion & Future Work,"specifically, we proposed a counterfactual signal for self-supervision, to augment task-level human annotation, into sample-level machine attention supervision, to increase both the accuracy and robustness of the model."
2020.emnlp-main.543.txt,2020,7 Conclusion & Future Work,"we hope future research to explore scenarios where human intuition is not working as well as text classification, such as graph attention (velickovic et al., 2017)."
2020.emnlp-main.543.txt,2020,7 Conclusion & Future Work,"we studied the problem of attention supervision, and showed that requiring sample-level human supervision is often less effective than task-level alternative with lower (and often zero-) overhead."
2020.emnlp-main.544.txt,2020,5 Conclusion,"in future work, we plan to validate its effectiveness for aspect-level sentiment classification."
2020.emnlp-main.544.txt,2020,5 Conclusion,"instead of the tradition of exploiting complicated operations by stacking cnns and rnns, or attaching overparameterized attention mechanisms, our work provides a lightweight method for improving the ability of neural models for sentence classification."
2020.emnlp-main.544.txt,2020,5 Conclusion,this study presents a novel parameter-efficient model called mode-lstm that can capture multiscale n-gram features in sentences.
2020.emnlp-main.544.txt,2020,5 Conclusion,"through disentangling the hidden states of the lstm and equipping the structure with multiple sliding windows of different scales, mode-lstm outperforms popular cnn/rnn-based methods and hybrid methods on various benchmark datasets."
2020.emnlp-main.545.txt,2020,4 Conclusion,"in future work, we will try other types of single networks (e.g., (lai et al., 2015; yang et al., 2016; shimura et al., 2019))."
2020.emnlp-main.545.txt,2020,4 Conclusion,"in this paper, we propose a hybrid solution with a hscnn model for dealing with extremely imbalanced multi-label text classification."
2020.emnlp-main.545.txt,2020,4 Conclusion,the proposed method can improve the performance of single networks with diverse loss objectives on the tail categories or entire categories.
2020.emnlp-main.546.txt,2020,5 Conclusion,another limitation of the method is the dependence on pre-training dataset.
2020.emnlp-main.546.txt,2020,5 Conclusion,"for example, the performance gain across prompts is inconsistent."
2020.emnlp-main.546.txt,2020,5 Conclusion,"in this paper, we presented a pre-training based approach to automated chinese essay scoring."
2020.emnlp-main.546.txt,2020,5 Conclusion,larger pre-training dataset with supervised labels or self-supervised learning strategies could be explored.
2020.emnlp-main.546.txt,2020,5 Conclusion,"moreover, we are interested in understanding what features or traits of essays are captured by the deep models for scoring."
2020.emnlp-main.546.txt,2020,5 Conclusion,"our method investigates multi-stage pre-training and incorporates multi-level supervision, including the weak supervision from large scale coarse ratings, the supervision from rated essays from other prompts and the target-prompt training data."
2020.emnlp-main.546.txt,2020,5 Conclusion,the experimental results show that the pretraining based approach is effective for aes in terms of both effectiveness and domain adaptation ability.
2020.emnlp-main.546.txt,2020,5 Conclusion,the pre-training dataset used in this paper is still small compared with the data used for pre-training language models.
2020.emnlp-main.546.txt,2020,5 Conclusion,"the proposed method has a limitation that it pays more attention to the score range that most essays are from, and may hurt the performance in other ranges."
2020.emnlp-main.546.txt,2020,5 Conclusion,we also observe some phenomena but do not have good explanations.
2020.emnlp-main.546.txt,2020,5 Conclusion,we carefully analyze the effects of each component and find that: multi-stage pre-training improves the base model in general; the domain adaptation ability can be consistently improved; target-prompt fine-tuning is still indispensable but the required amount of training data can be largely reduced; weakly supervised pre-training and supervised transfer fine-tuning are both helpful.
2020.emnlp-main.546.txt,2020,5 Conclusion,we plan to investigate these in future.
2020.emnlp-main.546.txt,2020,5 Conclusion,we suggest that the prompts’ properties should be investigated more for applying aes.
2020.emnlp-main.546.txt,2020,5 Conclusion,when the pre-trained scorer can work best should be further studied.
2020.emnlp-main.547.txt,2020,6 Conclusion,"experimental results show that the proposed method achieves state-ofthe-art performance on two benchmark non-factoid qa datasets, namely wikihow and pubmedqa."
2020.emnlp-main.547.txt,2020,6 Conclusion,we incorporate multihop reasoning to infer justification sentences for abstractive summarization.
2020.emnlp-main.547.txt,2020,6 Conclusion,"we propose a novel question-driven abstractive summarization method, multi-hop selective generator (msg), to summarize concise but informative answers for non-factoid qa."
2020.emnlp-main.548.txt,2020,6 Conclusion and Future Work,"experimental results demonstrate the effectiveness of each module, and analysis on intermediate outputs presents good interpretability for the inference process in contrasted with “black box” models."
2020.emnlp-main.548.txt,2020,6 Conclusion and Future Work,"in this paper, we aim to answer ropes questions in an interpretable way by leveraging five neural network modules."
2020.emnlp-main.548.txt,2020,6 Conclusion and Future Work,"meanwhile, extending these models to a larger scope of question types or more complex scenarios is still a challenge, and we will further investigate the trade-off between explainability and scalability."
2020.emnlp-main.548.txt,2020,6 Conclusion and Future Work,"moreover, we find that with explicitly designed compositional modeling of inference process, our approach with a few training examples achieves similar accuracy to strong baselines with full-size training data which indicates a better generalization capability."
2020.emnlp-main.548.txt,2020,6 Conclusion and Future Work,these modules are trained in an end-to-end manner and each module provides transparent intermediate outputs.
2020.emnlp-main.549.txt,2020,5 Conclusion,"in the future, we plan to extend our model to learn the heterogeneous graph automatically, which assures more flexibility for numerical reasoning."
2020.emnlp-main.549.txt,2020,5 Conclusion,"in this work, we propose a novel method named qdgat for numerical reasoning in the machine reading comprehension task."
2020.emnlp-main.549.txt,2020,5 Conclusion,"our method not only builds a more compact graph containing different types of numbers, entities, and relations, which can be a general method for other sophisticated reasoning tasks but also conditions the reasoning directly on the question language embedding, which modulates the attention over graph neighbors and change messages being passed iteratively to achieve reasoning."
2020.emnlp-main.549.txt,2020,5 Conclusion,the experimental results verify the effectiveness of our method.
2020.emnlp-main.549.txt,2020,5 Conclusion,"we would also explore to learn the types of numbers and entities together the reasoning modules using variational autoencoder techniques (kingma and welling, 2014), which may help the ner system better adapt to the numerical reasoning task."
2020.emnlp-main.55.txt,2020,8 Conclusion,"finally, we collect a new dataset containing 15m sentences from 11 diverse styles."
2020.emnlp-main.55.txt,2020,8 Conclusion,in this work we model style transfer as a controlled paraphrase generation task and present a simple unsupervised style transfer method using diverse paraphrasing.
2020.emnlp-main.55.txt,2020,8 Conclusion,"possible future work includes (1) exploring other applications of diverse paraphrasing, such as data augmentation; (2) performing style transfer at a paragraph level; (3) performing style transfer for styles unseen during training, using few exemplars provided during inference."
2020.emnlp-main.55.txt,2020,8 Conclusion,we critique current style transfer evaluation using a survey of 23 papers and propose fixes to common shortcomings.
2020.emnlp-main.550.txt,2020,8 Conclusion,"as a result of improved retrieval performance, we obtained new state-of-the-art results on multiple open-domain question answering benchmarks."
2020.emnlp-main.550.txt,2020,8 Conclusion,"in this work, we demonstrated that dense retrieval can outperform and potentially replace the traditional sparse retrieval component in open-domain question answering."
2020.emnlp-main.550.txt,2020,8 Conclusion,"moreover, our empirical analysis and ablation studies indicate that more complex model frameworks or similarity functions do not necessarily provide additional values."
2020.emnlp-main.550.txt,2020,8 Conclusion,"while a simple dual-encoder approach can be made to work surprisingly well, we showed that there are some critical ingredients to training a dense retriever successfully."
2020.emnlp-main.551.txt,2020,6 Discussion and Conclusion,"however, at the same time, the improvements for generalization were less substantial, indicating that some reasoning capacities are difficult to distill in this manner."
2020.emnlp-main.551.txt,2020,6 Discussion and Conclusion,"moreover, despite the improvements we observed, the performance of the nlp models is still substantially below the performance of the gnn teacher used for distillation (see appendices b & c), highlighting that significant work that remains to close the gap between the reasoning performance of text-based and gnn-based models."
2020.emnlp-main.551.txt,2020,6 Discussion and Conclusion,"most prominently, the structured distillation approach significantly improved the performance of the nlp models in settings where noisy facts were added to the clutrr reasoning problems."
2020.emnlp-main.551.txt,2020,6 Discussion and Conclusion,our structured distillation approach achieves promising results.
2020.emnlp-main.551.txt,2020,6 Discussion and Conclusion,"the gnn-based models are particularly strong in this setting (see appendix c), and this suggests that transferring knowledge about the relevancy of facts from structured to unstructured models may be a promising direction."
2020.emnlp-main.552.txt,2020,7 Conclusion,"in latent subclass learning, we have shown a general technique to uncover some of these features discretely, providing a starting point for descriptive analysis of our models’ latent ontologies."
2020.emnlp-main.552.txt,2020,7 Conclusion,"in these ontologies, we found clear connections to existing categories, such as personhood of named entities."
2020.emnlp-main.552.txt,2020,7 Conclusion,"the high specificity of our method opens doors to more insights from future work, which may include investigating how lsl results vary with probe architecture, developing intrinsic quality measures on latent ontologies, or applying the technique to discover new patterns in settings where gold annotations are not present."
2020.emnlp-main.552.txt,2020,7 Conclusion,"using this approach, we showed that encoders such as bert and elmo can be found to hold stable, consistent latent ontologies on a variety of linguistic tasks."
2020.emnlp-main.552.txt,2020,7 Conclusion,"we also found evidence of ontological distinctions beyond traditional gold categories, such as distinguishing large and small numbers, or preferring fine-grained semantic roles for core arguments."
2020.emnlp-main.552.txt,2020,7 Conclusion,"we introduced a new model analysis method based on latent subclass learning: by factoring a binary classifier through a forced choice of latent subclasses, latent ontologies can be coaxed out of input features."
2020.emnlp-main.553.txt,2020,7 Conclusion,"although finetuning from pretrained language models puts in phenomenal downstream performance, the reason is not fully uncovered."
2020.emnlp-main.553.txt,2020,7 Conclusion,our findings show that the learning speeds for reconstructing and predicting tokens differ across pos.
2020.emnlp-main.553.txt,2020,7 Conclusion,our results also reveal that the model’s world knowledge does not stay static even when pretraining loss converges.
2020.emnlp-main.553.txt,2020,7 Conclusion,this work aims to unveil the mystery of the pretrained language model by looking into how it evolves.
2020.emnlp-main.553.txt,2020,7 Conclusion,we find that the model acquires semantic and syntactic knowledge simultaneously at the early pretraining stage.
2020.emnlp-main.553.txt,2020,7 Conclusion,we hope our work can bring more insights into what makes a pretrained language model a pretrained language model.
2020.emnlp-main.553.txt,2020,7 Conclusion,we show that the model is already prepared for finetuning on downstream tasks at its early pretraining stage.
2020.emnlp-main.554.txt,2020,7 Discussion,"a first step in future work would be to test if the results of this paper hold on transformer architectures, or if instead transformers result in different patterns of structural encoding transfer."
2020.emnlp-main.554.txt,2020,7 Discussion,"by using tilts, we do not have to identify a structural feature of interest and investigate if it is being encoded, but instead asses if generalizable abstract structures are encoded in one language by examining if they can be used to model human language."
2020.emnlp-main.554.txt,2020,7 Discussion,"first, that vocabulary distribution has a very minor effect for modelling human language compared to structural similarity."
2020.emnlp-main.554.txt,2020,7 Discussion,"future work expanding on our results could focus on ablating specific structural features by creating hypothetical languages that differ in single grammatical features from the l2, in the style of galactic dependencies (wang and eisner, 2016), and testing the effect of structured data that’s completely unrelated to language, such as images."
2020.emnlp-main.554.txt,2020,7 Discussion,"in doing so, we treat the frozen lstm weights as the only structural faculty available to a human language model, and assess if the induced structure is general enough to be used to model human language."
2020.emnlp-main.554.txt,2020,7 Discussion,"in fact, previous research shows that lstms are able to successfully model stack-based hierarchical languages (suzgun et al., 2019b; yu et al., 2019; suzgun et al., 2019a)."
2020.emnlp-main.554.txt,2020,7 Discussion,"in this work we propose the test for inductive bias via language model transfer (tilt), a novel analytic method for neural language models which tests the ability of a model to generalize and use structural knowledge."
2020.emnlp-main.554.txt,2020,7 Discussion,"last, that encodings derived from hierarchically structured tokens are equally useful for modelling human language as those derived from texts made up of pairs of tokens that are linked but non-hierarchical."
2020.emnlp-main.554.txt,2020,7 Discussion,"our experiments are cross-lingual and crossmodal in nature, not searching for representations of high-level features in one language, but for representations that encode general ideas of structure."
2020.emnlp-main.554.txt,2020,7 Discussion,"our method could be used to test many other hypotheses regarding neural language models, by choosing a discerning set of pretraining languages."
2020.emnlp-main.554.txt,2020,7 Discussion,our non-linguistic and artificial language experiments suggest three facets of the structural encoding ability of lstm lms.
2020.emnlp-main.554.txt,2020,7 Discussion,"our results also contribute to the long-running nature-nurture debate in language acquisition: whether the success of neural models implies that unbiased learners can learn natural languages with enough data, or whether human abilities to acquire language given sparse stimulus implies a strong innate human learning bias (linzen and baroni, 2020)."
2020.emnlp-main.554.txt,2020,7 Discussion,our results on the parentheses corpora do not necessarily provide proof that the lstms trained on the nesting parentheses corpus aren’t encoding and utilizing hierarchical structure.
2020.emnlp-main.554.txt,2020,7 Discussion,"our work thus avoids known issues that have been pointed out with analytic methods like probing (voita and titov, 2020; pimentel et al., 2020; hewitt and liang, 2019)."
2020.emnlp-main.554.txt,2020,7 Discussion,"running experiments on a range of human languages, we conclude that the internal linguistic representation of lstm lms allows them to take advantage of structural similarities between languages even when unaided by lexical overlap."
2020.emnlp-main.554.txt,2020,7 Discussion,"second, that models can encode useful language modelling information from the latent structure inherent in non-linguistic structured data, even if the surface forms are vastly differing."
2020.emnlp-main.554.txt,2020,7 Discussion,"the results of our parentheses experiments suggest that simple structural head-dependent bias, which need not be hierarchical, goes a long way toward making language acquisition possible for neural networks, highlighting the possibility of a less central role for recursion in language learning for both humans and machines."
2020.emnlp-main.554.txt,2020,7 Discussion,"these results shine light on the importance of considering other types of structural awareness that may be used by neural natural language models, even if those same models also demonstrate the ability to model pure hierarchical structure."
2020.emnlp-main.554.txt,2020,7 Discussion,"we pretrain lstms on structured data, and then use the frozen lstm weights to model human language."
2020.emnlp-main.554.txt,2020,7 Discussion,"we run experiments on natural languages, artificial languages, and non-linguistic corpora."
2020.emnlp-main.554.txt,2020,7 Discussion,"what our results do indicate is that, in order for lstms to model human language, being able to model hierarchical structure is similar in utility to having access to a non-hierarchical ability to “look back” at one relevant dependency."
2020.emnlp-main.554.txt,2020,7 Discussion,"while the majority of past work analyzing the structural abilities of neural models looks at a model’s treatment of structural features that are realized in specific input sentences, our method compares the encoding and transfer of general grammatical features of different languages."
2020.emnlp-main.555.txt,2020,6 Conclusion,"as a result, it is believed that this study will benefit future work about choosing suitable positional encoding functions or designing other modeling methods for position information in the target nlp tasks based on their properties."
2020.emnlp-main.555.txt,2020,6 Conclusion,"on the other hand, the transformer decoders for autoregressive language modeling actually learn about absolute positions."
2020.emnlp-main.555.txt,2020,6 Conclusion,the empirical experiments on the pre-trained position embeddings validate our hypothesis.
2020.emnlp-main.555.txt,2020,6 Conclusion,this paper investigates the implicit meaning of pretrained transformer position embeddings.
2020.emnlp-main.555.txt,2020,6 Conclusion,transformer encoders learn the local position information that can only be effective in masked language modeling.
2020.emnlp-main.555.txt,2020,6 Conclusion,we also show that different nlp tasks with different model architectures and different training objectives may utilize the position information in different ways.
2020.emnlp-main.556.txt,2020,7 Ethical Considerations and Conclusion,"bertrand and mullainathan, 2004; moss-racusin et al., 2012)."
2020.emnlp-main.556.txt,2020,7 Ethical Considerations and Conclusion,"finally, there are many other important types of biases pertaining to given names that we do not focus on, including biases on the basis of perceived race or gender (e.g."
2020.emnlp-main.556.txt,2020,7 Ethical Considerations and Conclusion,"first, we evaluated only english lms, thus we cannot assume these results will extend to lms in different languages."
2020.emnlp-main.556.txt,2020,7 Ethical Considerations and Conclusion,"for our last name prediction experiment, we are forced to filter named entities whose given names don’t precede the surname, which is a cultural assumption that precludes naming conventions from many languages, like chinese and korean."
2020.emnlp-main.556.txt,2020,7 Ethical Considerations and Conclusion,"furthermore, as we observed with gpt2-xl’s freeform production of a white supremacist’s name conditioned only on a common given name (richard), further inquiry into the source of training data of these models is warranted."
2020.emnlp-main.556.txt,2020,7 Ethical Considerations and Conclusion,our methodology relies on a number of limitations that should be considered in understanding the scope of our conclusions.
2020.emnlp-main.556.txt,2020,7 Ethical Considerations and Conclusion,"pre-trained lms do not treat given names as interchangeable or anonymous; this has not only implications for the quality and accuracy of systems that employ these lms, but also for the fairness of those systems."
2020.emnlp-main.556.txt,2020,7 Ethical Considerations and Conclusion,"second, the lists of names we use to analyze these models are not broadly representative of english-speaking populations."
2020.emnlp-main.556.txt,2020,7 Ethical Considerations and Conclusion,the list of most common given names in the u.s. are over-representative of stereotypically white and western names.
2020.emnlp-main.556.txt,2020,7 Ethical Considerations and Conclusion,"the list of most frequently named people in the media as well as a&e’s (subjective) list of most influential people of the millennium both are male-skewed, owing to many sources of gender bias, both historical and contemporary."
2020.emnlp-main.556.txt,2020,7 Ethical Considerations and Conclusion,"we discuss two types of ethical considerations pertaining to this work: (1) the limitations of this work, and (2) the implications of our findings."
2020.emnlp-main.556.txt,2020,7 Ethical Considerations and Conclusion,we explored biases in pre-trained lms with respect to given names and the named entities that share them.
2020.emnlp-main.556.txt,2020,7 Ethical Considerations and Conclusion,"we hope future work may better address this limitation, as in the work of cao and daume iii ´ (2019)."
2020.emnlp-main.556.txt,2020,7 Ethical Considerations and Conclusion,"we used statistical resources that treat gender as a binary construct, which is a reductive view of gender."
2020.emnlp-main.556.txt,2020,7 Ethical Considerations and Conclusion,"what this work does do, however, is shed light on a particular behavior of pre-trained lms which has potential ethical implications."
2020.emnlp-main.556.txt,2020,7 Ethical Considerations and Conclusion,"while our experiments shed light on artifacts of certain common u.s. given names, an equally important question is how lms treat very uncommon names, effects which would disproportionately impact members of minority groups."
2020.emnlp-main.557.txt,2020,7 Conclusion,"powerful pre-trained models such as bert and roberta perform surprisingly poorly, even after fine-tuning with high-quality distant supervision."
2020.emnlp-main.557.txt,2020,7 Conclusion,"we collect a new diagnostic dataset carefully verified by human annotators, which covers 8 different topics."
2020.emnlp-main.557.txt,2020,7 Conclusion,we hope our findings and probing dataset will provide a basis for improving pre-trained masked language models’ numerical and other concrete types of commonsense knowledge.
2020.emnlp-main.557.txt,2020,7 Conclusion,"we present a probing task, numersense, to induce numerical commonsense knowledge from pretrained language models."
2020.emnlp-main.558.txt,2020,5 Conclusion and Future work,gazp improved parsing performance on three zero-shot parsing tasks.
2020.emnlp-main.558.txt,2020,5 Conclusion and Future work,"in future work, we will consider how to interpret environment specifications to facilitate grounded adaptation in these other areas."
2020.emnlp-main.558.txt,2020,5 Conclusion and Future work,"in principle, gazp applies to any problems that lack annotated data and differ between training and inference environments."
2020.emnlp-main.558.txt,2020,5 Conclusion and Future work,"one such area is robotics, where one trains in simulation because it is prohibitively expensive to collect annotated trajectories in the real world."
2020.emnlp-main.558.txt,2020,5 Conclusion and Future work,"our analyses showed that gazp outperforms data augmentation, performance improvement scales with the amount of gazp-synthesized data, and cycleconsistency is central to successful adaptation."
2020.emnlp-main.558.txt,2020,5 Conclusion and Future work,we proposed gazp to adapt an existing semantic parser to new environments by synthesizing cycle-consistent data.
2020.emnlp-main.559.txt,2020,7 Conclusion and Future Work,"as a pilot study, we experiment systems with simulated user interaction."
2020.emnlp-main.559.txt,2020,7 Conclusion and Future Work,"finally, we believe our algorithm can be applied to save annotation effort for other nlp tasks, especially the low-resource ones (mayhew et al., 2019)."
2020.emnlp-main.559.txt,2020,7 Conclusion and Future Work,"in experiments, we observe that neural semantic parsers tend to be overconfident and training them with more data does not mitigate this issue."
2020.emnlp-main.559.txt,2020,7 Conclusion and Future Work,"in the future, we will look into more accurate confidence measure via neural network calibration (guo et al., 2017) or using machine learning components (e.g., answer triggering (zhao et al., 2017) or a reinforced active selector (fang et al., 2017))."
2020.emnlp-main.559.txt,2020,7 Conclusion and Future Work,one important future work is thus to conduct large-scale user studies and train parsers from real user interaction.
2020.emnlp-main.559.txt,2020,7 Conclusion and Future Work,our work shows the possibility of continually learning semantic parsers from fine-grained end user interaction.
2020.emnlp-main.559.txt,2020,7 Conclusion and Future Work,this is not trivial and has to account for uncertainties such as noisy user feedback.
2020.emnlp-main.559.txt,2020,7 Conclusion and Future Work,"we also plan to derive a more realistic formulation of user/expert annotation costs by analyzing real user statistics (e.g., average time spent on each question)."
2020.emnlp-main.56.txt,2020,7 Conclusion and Future Work,"based on the ac-nlg method, in the future, we can explore the following directions: (1) improve the accuracy of judgment on a claim-level.(2) add external knowledge (e.g.a logic graph) to the predictor for the interpretability of the model."
2020.emnlp-main.56.txt,2020,7 Conclusion and Future Work,"in this paper, we propose a novel attentional and counterfactual based natural language generation (ac-nlg) method to solve the task of court’s view generation in civil cases and ensure the fairness of the judgment."
2020.emnlp-main.56.txt,2020,7 Conclusion and Future Work,the experimental results show the effectiveness of our method.
2020.emnlp-main.56.txt,2020,7 Conclusion and Future Work,"we design a claim-aware encoder to represent the fact description which emphasizes on the plaintiff’s claim, as well as a pair of backdoor-inspired counterfactual decoders to generate judgment-discriminative court’s views (both supportive and non-supportive views) and to eliminate the bias that arose from the data generation mechanism by connecting with a synergistic judgment predictive model."
2020.emnlp-main.560.txt,2020,7 Conclusion and Future work,empirical results demonstrate the efficacy of our model.
2020.emnlp-main.560.txt,2020,7 Conclusion and Future work,"for future work, we will explore methods attempting to solve hard and extra hard questions."
2020.emnlp-main.560.txt,2020,7 Conclusion and Future work,"in this paper, we focus on context-dependent crossdomain sql generation task."
2020.emnlp-main.560.txt,2020,7 Conclusion and Future work,thus we propose a model named igsql to model database schema items in a conversational scenario.
2020.emnlp-main.560.txt,2020,7 Conclusion and Future work,we also conduct ablation experiments to reveal the significance of our database schema interaction graph encoder.
2020.emnlp-main.560.txt,2020,7 Conclusion and Future work,"we find that previous state-of-the-art model only takes historical user inputs and previously predicted query into consideration, but ignores the historical information of database schema items."
2020.emnlp-main.561.txt,2020,8 Conclusion and Future Work,experimental results show this approach leads to significant performance boosts on two cross-domain datasets with five different base parsers.
2020.emnlp-main.561.txt,2020,8 Conclusion and Future Work,"in the future, we are interested in distilling and reusing the common knowledge from users’ selections."
2020.emnlp-main.561.txt,2020,8 Conclusion and Future Work,piia interacts with users via multi-choice questions and can be built on arbitrary parsers.
2020.emnlp-main.561.txt,2020,8 Conclusion and Future Work,"we propose a parser-independent interactive approach, piia, to enhance the text-to-sql process in nlidb systems."
2020.emnlp-main.562.txt,2020,7 Conclusion,"based on the analysis on questions from real-world applications, our dataset contains a considerable proportion of questions that require row/column calculations."
2020.emnlp-main.562.txt,2020,7 Conclusion,"for future work, we will continually improve the scale and quality of our dataset, to facilitate future research and to meet the need of database-oriented applications."
2020.emnlp-main.562.txt,2020,7 Conclusion,"we extend the state-of-the-art irnet model on spider to accommodate dusql, and obtain substantial performance boost."
2020.emnlp-main.562.txt,2020,7 Conclusion,we present the first large-scale and pragmatic chinese dataset for cross-domain text-to-sql parsing.
2020.emnlp-main.562.txt,2020,7 Conclusion,"yet, there is still a large room for improvement, especially on calculation questions which usually require incorporation of common-sense knowledege into the model."
2020.emnlp-main.563.txt,2020,5 Conclusion and Future Work,"equipped with automatic-generated labels and agg enhancement method, our model achieves state-of-the-art results on the wikisql benchmark."
2020.emnlp-main.563.txt,2020,5 Conclusion and Future Work,"since the current automatic-generated annotations are still noisy, it is useful to further improve the automatic annotation procedure."
2020.emnlp-main.563.txt,2020,5 Conclusion and Future Work,"thanks to the simple, unified model for mention and relation extraction and its capacity for capturing inter mention dependencies, the proposed method proves to be a promising approach to textto-sql task."
2020.emnlp-main.563.txt,2020,5 Conclusion and Future Work,we also plan to extend our approach to cope with multitable text-to-sql task spider.
2020.emnlp-main.564.txt,2020,6 Conclusion,"experimenting with our designed schema linking sql (slsql) model, we demonstrate that more accurate schema linking conclusively leads to better text-to-sql parsing performance."
2020.emnlp-main.564.txt,2020,6 Conclusion,"importantly, given oracular schema references, a simple bert model like slsql can achieve an impressive performance."
2020.emnlp-main.564.txt,2020,6 Conclusion,"our experiments show that schema linking, often overlooked as simple preprocessing, is actually a requisite for good sql parsing performance, providing an intriguing perspective for future improvements on this task."
2020.emnlp-main.564.txt,2020,6 Conclusion,"our study sheds light on the characteristics of text-tosql parsing for future efforts including advanced modeling, problem identification, dataset construction and model evaluation."
2020.emnlp-main.564.txt,2020,6 Conclusion,"to support modelindependent and thorough studies, we invest human resources to annotate schema references and contribute a high-quality, large-scale schema linking corpus."
2020.emnlp-main.564.txt,2020,6 Conclusion,we critically examine the role of schema linking for the text-to-sql task.
2020.emnlp-main.565.txt,2020,6 Conclusion,further research may be concerned with zero-shot learning on new categories.
2020.emnlp-main.565.txt,2020,6 Conclusion,"furthermore, the shared encoder and decoder layers weaken catastrophic forgetting in the incremental learning task."
2020.emnlp-main.565.txt,2020,6 Conclusion,"in this paper, in order to make multi-task learning feasible for incremental learning, we proposed cne-net with different attention mechanisms."
2020.emnlp-main.565.txt,2020,6 Conclusion,the category name features and the multi-task learning structure help the model achieve state-of-the-art on acsa and tacsa tasks.
2020.emnlp-main.565.txt,2020,6 Conclusion,we proposed a task for (t)acsa incremental learning and achieved the best performance with cne-net compared with other strong baselines.
2020.emnlp-main.566.txt,2020,4 Conclusion,"besides, there are still two important directions for future work: (1) how to apply task-guided pre-training to general domain data when the indomain data is limited.(2) how to design more effective strategies to capture domain-specific and task-specific patterns for selective masking."
2020.emnlp-main.566.txt,2020,4 Conclusion,experimental results show that our methods can achieve better performances with less computation cost.
2020.emnlp-main.566.txt,2020,4 Conclusion,"in this paper, we design task-guided pre-training with selective masking and present a three-stage training framework for plms."
2020.emnlp-main.566.txt,2020,4 Conclusion,"note that although we only conduct experiments on two sentiment classification tasks using bert as the base model, our method can easily generalize to other models using masked language modeling or its variants and other text classification tasks."
2020.emnlp-main.566.txt,2020,4 Conclusion,"with task-guided pre-training, models can effectively and efficiently learn domain-specific and task-specific patterns, which benefits downstream tasks."
2020.emnlp-main.567.txt,2020,5 Conclusion,"experiments show that sentilare outperforms state-of-the-art language representation models on various sentiment analysis tasks, and thus facilitates sentiment understanding."
2020.emnlp-main.567.txt,2020,5 Conclusion,"we present a novel pre-trained model called sentilare for sentiment analysis, which introduces linguistic knowledge from sentiwordnet via contextaware sentiment attention, and adopts label-aware masked language model to deeply integrate knowledge into bert-style models through pre-training tasks."
2020.emnlp-main.568.txt,2020,6 Conclusion,"another promising direction is to leverage taxonomy construction algorithms (huang et al., 2020) to capture more fine-grained aspects, such as “smell” and “taste” for “food”."
2020.emnlp-main.568.txt,2020,6 Conclusion,experiments show that our method learns high-quality joint topics and outperforms previous studies substantially.
2020.emnlp-main.568.txt,2020,6 Conclusion,"in the future, we plan to adapt our methods to more general applications that are not restricted to the field of sentiment analysis, such as doing multiple-dimension classification (e.g., topic, location) on general text corpus."
2020.emnlp-main.568.txt,2020,6 Conclusion,"in this paper we propose to enhance weaklysupervised aspect-based sentiment analysis by learning the representation of hsentiment, aspecti joint topic in the embedding space to capture more fine-grained information."
2020.emnlp-main.568.txt,2020,6 Conclusion,"the embedding-based predictions are then used for pretraining neural models, which are further refined via self-training on unlabeled corpus."
2020.emnlp-main.568.txt,2020,6 Conclusion,we introduce an embedding learning objective that leverages usergiven keywords for each aspect/sentiment and models their distribution over the joint topics.
2020.emnlp-main.569.txt,2020,7 Conclusions and Future Work,a new large-scale and challenging dataset rr is collected and fully annotated to facilitate the study of the proposed task.
2020.emnlp-main.569.txt,2020,7 Conclusions and Future Work,"in the future, we will explore the latent information between peer reviews and author responses to improve argument pair extraction."
2020.emnlp-main.569.txt,2020,7 Conclusions and Future Work,"in this paper, we introduce a new task of extracting argument pairs from review and rebuttal passages, which explores a new domain for the argument mining research field."
2020.emnlp-main.569.txt,2020,7 Conclusions and Future Work,we then propose a multi-task learning approach based on hierarchical lstm networks to work towards this problem.
2020.emnlp-main.569.txt,2020,7 Conclusions and Future Work,we will also explore related useful research tasks using extra collected information related to scientific work submissions in rr.
2020.emnlp-main.57.txt,2020,8 Conclusion,a bert-based planning model is first designed to assign and position keyphrases into different sentences.
2020.emnlp-main.57.txt,2020,8 Conclusion,both automatic evaluation and human judgments show that our model with planning and refinement enhances the relevance and coherence of the generated content.
2020.emnlp-main.57.txt,2020,8 Conclusion,we present a novel content-controlled generation framework that adds content planning to large pretrained transformers without modifying model architecture.
2020.emnlp-main.57.txt,2020,8 Conclusion,we then investigate an iterative refinement algorithm that works with the sequenceto-sequence models to improve generation quality with flexible editing.
2020.emnlp-main.570.txt,2020,5 Conclusion,detailed comparisons also show the necessity and effectiveness of our diversified regularizations.
2020.emnlp-main.570.txt,2020,5 Conclusion,"in order to guarantee the proper transfer from document-level supervision to aspect-level prediction, we further propose diversified textual regularization and diversified sentimental regularization."
2020.emnlp-main.570.txt,2020,5 Conclusion,"in the future, we plan to further improve d-miln with aspect-level annotations and find appropriate way to combine d-miln with pre-training methods (tian et al., 2020)."
2020.emnlp-main.570.txt,2020,5 Conclusion,"in this paper, we propose a diversified multiple instance learning network to achieve dmsc with only document-level supervision."
2020.emnlp-main.570.txt,2020,5 Conclusion,"through experiments on two benchmark datasets, we verify that our d-miln can properly capture the interaction between aspect-level and document-level, and achieve new sota on weakly supervised dmsc."
2020.emnlp-main.570.txt,2020,5 Conclusion,"we formulate this problem as multiple instance learning, so as to model the relation between aspect-level sentiment and document-level sentiment."
2020.emnlp-main.571.txt,2020,6 Conclusion,"our contributions lie in (1) creating a chinese corpus focused on exaggeration, (2) identifying different strategies used by humans for exaggeration and (3) showing that deep learners substantially outperform traditional learners on automatic hyperbole detection."
2020.emnlp-main.571.txt,2020,6 Conclusion,"the statistical and manual analyses of our corpus, which is absent from other computational studies on exaggeration, have shed light on various interesting questions about this rhetorical device."
2020.emnlp-main.571.txt,2020,6 Conclusion,"to stimulate research on this topic, we make hypo-cn publicly available.8 in future work, we plan to use hypo and hypo-cn to conduct a cross-lingual study on whether there are differences in the way exaggeration is expressed in english and chinese."
2020.emnlp-main.571.txt,2020,6 Conclusion,"we presented an empirical study of exaggeration, which is one of the most prevalent and yet one of the least studied rhetorical devices from a computational perspective."
2020.emnlp-main.572.txt,2020,6 Conclusion,extensive experiments on four benchmark datasets demonstrate the superiority of our unified domain adaptation (uda) approach over existing methods in both cross-domain end2end absa and cross-domain aspect extraction.
2020.emnlp-main.572.txt,2020,6 Conclusion,"in feature-based domain adaptation, we use domain-shared syntactic relations and pos tags to construct auxiliary tasks, which can help learn domain-invariant representations for domain adaptation."
2020.emnlp-main.572.txt,2020,6 Conclusion,"in instance-based domain adaptation, we employ a domain classifier to learn to assign appropriate weights for each word."
2020.emnlp-main.572.txt,2020,6 Conclusion,"in this paper, we explored the potential of bert to domain adaptation, and proposed a unified feature and instance-based adaptation approach for both tasks of cross-domain end2end absa and cross-domain aspect extraction."
2020.emnlp-main.573.txt,2020,8 Conclusion,"by and large, this paper provides additional evidence that models like bert lack linguistic abstraction abilities, often relying on superficial features such as entity name biases or word order to answer questions in the conversational question answering task."
2020.emnlp-main.573.txt,2020,8 Conclusion,even a small amount of training data for linguistic information such as negation can provide a very large boost to the model performance on the qa classes which rely on that information.
2020.emnlp-main.573.txt,2020,8 Conclusion,"finally, we show that all the bertlike models tested can be enhanced to a varying extent by feeding them linguistic knowledge through a multitask approach."
2020.emnlp-main.573.txt,2020,8 Conclusion,"furthermore, we find that while roberta improves over bert’s performance, a large portion of its gain comes from better lexical representations and it appears to fall short of solving the compositional semantic problems."
2020.emnlp-main.573.txt,2020,8 Conclusion,"more precisely, we show that bert, roberta and distilbert models mostly fail on questions that require inference over the compositional aspects of language, such as semantic roles and negation."
2020.emnlp-main.573.txt,2020,8 Conclusion,"what is more, we provide the first evaluation and analysis of distilbert on coqa, showing that distilbert, more so than bert, relies on lexical information most and lacks capacity to learn compositional representations."
2020.emnlp-main.574.txt,2020,7 Conclusions and future work,"furthermore, we expect to extend the scope of analysis from the attention to an entire transformer architecture to better understand the inner workings and linguistic capabilities of the current powerful systems in nlp."
2020.emnlp-main.574.txt,2020,7 Conclusions and future work,"in future work, we plan to apply our norm-based analysis to attention in other models, such as finetuned bert, roberta (liu et al., 2019), and albert (lan et al., 2020)."
2020.emnlp-main.574.txt,2020,7 Conclusions and future work,one possible direction is to design an attention mechanism that can collect almost no information from an input sequence as the current systems achieve it by exploiting the [sep] token.
2020.emnlp-main.574.txt,2020,7 Conclusions and future work,this paper showed that attention weights alone are only one of two factors that determine the output of attention.
2020.emnlp-main.574.txt,2020,7 Conclusions and future work,"using our norm-based method, we provided a more detailed interpretation of the inner workings of transformers, compared to the studies using the weight-based analysis."
2020.emnlp-main.574.txt,2020,7 Conclusions and future work,we believe that these findings can provide insights not only into the interpretation of the behaviors of blackbox nlp systems but also into developing a more sophisticated transformer-based system.
2020.emnlp-main.574.txt,2020,7 Conclusions and future work,we hope that this paper will inspire researchers to have a broader view of the possible methodological choices for analyzing the behavior of transformer-based models.
2020.emnlp-main.574.txt,2020,7 Conclusions and future work,"we proposed the incorporation of another factor, the transformed input vectors."
2020.emnlp-main.575.txt,2020,7 Conclusion,"as a remedy, we addressed both modeling and evaluation, proposing a hierarchical neural architecture, a regularization term, as well as two new evaluation scores."
2020.emnlp-main.575.txt,2020,7 Conclusion,"in this paper, we investigated explainable question answering, revealing that existing models lack an explicit coupling of answers and explanations and that evaluation scores used in related work fail to quantify that."
2020.emnlp-main.575.txt,2020,7 Conclusion,our user study showed that our models help the users assess their correctness and that our proposed evaluation scores are better correlated with user experience than standard measures like f1.
2020.emnlp-main.575.txt,2020,7 Conclusion,this highly impairs their applicability in real-life scenarios with human users.
2020.emnlp-main.576.txt,2020,8 Discussion,"although our results have some implications on them, we leave a detailed study on context-free languages for future work."
2020.emnlp-main.576.txt,2020,8 Discussion,another interesting direction would be to understand whether certain modifications or recently proposed variants of transformers improve their performance on formal languages.
2020.emnlp-main.576.txt,2020,8 Discussion,"at the same time, our results show clear limitations of transformers compared to lstms on a large class of regular languages."
2020.emnlp-main.576.txt,2020,8 Discussion,clarifying this hypothesis theoretically and empirically is an attractive challenge.
2020.emnlp-main.576.txt,2020,8 Discussion,"evidently, the performance and capabilities of transformers heavily depend on architectural constituents e.g., the positional encoding schemes and the number of layers."
2020.emnlp-main.576.txt,2020,8 Discussion,"our results are consistent with the hypothesis that transformers generalize well for star-free languages with dot-depth 1, but not for higher depths."
2020.emnlp-main.576.txt,2020,8 Discussion,our visualizations imply that transformers do so with a generalizable mechanism instead of overfitting on some statistical regularities.
2020.emnlp-main.576.txt,2020,8 Discussion,our work poses a number of open questions.
2020.emnlp-main.576.txt,2020,8 Discussion,"recently, papadimitriou and jurafsky (2020) showed that pretraining lstms on formal languages like shuffle-dyck transfers to lm performance on natural languages."
2020.emnlp-main.576.txt,2020,8 Discussion,"recurrent models have a more automata-like structure wellsuited for counter and regular languages, whereas self-attention networks’ structure is very different, which seems to limit their abilities for the considered tasks."
2020.emnlp-main.576.txt,2020,8 Discussion,regular and counter languages model some aspects of natural language while contextfree languages model other aspects such as hierarchical dependencies.
2020.emnlp-main.576.txt,2020,8 Discussion,"similar to natural languages, boolean expressions consist of recursively nested hierarchical constituents."
2020.emnlp-main.576.txt,2020,8 Discussion,we showed that transformers can easily generalize on certain counter languages such as shuffle-dyck and boolean expressions in a manner similar to our proposed construction.
2020.emnlp-main.576.txt,2020,8 Discussion,what does the disparity between the performance of transformers on natural and formal languages indicate about the complexity of natural languages and their relation to linguistic analysis?(see also hahn (2020)).
2020.emnlp-main.577.txt,2020,8 Conclusion,we explored the impact of different unsupervised objectives in an ablation study and found that our newly introduced unsupervised objective using rule-based translations is essential for the success of unsupervised learning.
2020.emnlp-main.577.txt,2020,8 Conclusion,we presented the first fully unsupervised approach to text generation from kgs and a novel approach to unsupervised semantic parsing that automatically adapts to a target kg.
2020.emnlp-main.577.txt,2020,8 Conclusion,we quantitatively and qualitatively analyzed our method on text↔graph conversion.
2020.emnlp-main.577.txt,2020,8 Conclusion,"we showed the effectiveness of our approach on two datasets, webnlg v2.1 and a new text↔graph benchmark in the visual domain, derived from visual genome."
2020.emnlp-main.578.txt,2020,4 Conclusion,"extensive experiments on two public datasets show that our model yields competitive performance compared to several strong baselines, despite of our simpler model architecture design."
2020.emnlp-main.578.txt,2020,4 Conclusion,"in this paper, we propose a novel and simple dualgenerator network architecture for text style transfer, which does not rely on any discriminators or parallel corpus for training."
2020.emnlp-main.579.txt,2020,5 Conclusion,"in addition, we proposed a tree-structured decoder with a state aggregation mechanism for generating math expressions."
2020.emnlp-main.579.txt,2020,5 Conclusion,"in this study, we proposed a novel knowledgeaware sequence-to-tree model that can automatically solve math word problems."
2020.emnlp-main.579.txt,2020,5 Conclusion,our experimental results confirmed that our ka-s2t model outperformed other state-of-the-art models.
2020.emnlp-main.579.txt,2020,5 Conclusion,we used an entity graph to incorporate common sense knowledge from external knowledge bases into the proposed model.
2020.emnlp-main.58.txt,2020,7 Conclusion,our method is general and can be easily adapted for other generative reasoning tasks.
2020.emnlp-main.58.txt,2020,7 Conclusion,"we demonstrated its effectiveness for abductive and counterfactual reasoning, on which it performed substantially better than unsupervised baselines."
2020.emnlp-main.58.txt,2020,7 Conclusion,"we presented delorean, an unsupervised lmbased approach to generate text conditioned on past context as well as future constraints, through forward and backward passes considering each condition."
2020.emnlp-main.580.txt,2020,9 Conclusion,"crucially, we develop qabriefer and release the accompanying qabriefdataset, to create qabriefs."
2020.emnlp-main.580.txt,2020,9 Conclusion,"we propose the concept of fact checking briefs, to be read before performing a fact check."
2020.emnlp-main.580.txt,2020,9 Conclusion,we show in extensive empirical studies with crowdworkers and volunteers that qabriefs can improve accuracy and efficiency of fact checking.
2020.emnlp-main.581.txt,2020,6 Conclusion and Future Work,"it is inspiring and promising to be generalized to more rewriting tasks, which will be studied as our future work."
2020.emnlp-main.581.txt,2020,6 Conclusion and Future Work,"our approach performs comparably to the state-of-the-art seq2seq model with a considerable reduction in inference time, and can be easily adapted to other languages and offer more flexibility to control correction behavior (e.g., trading precision for recall)."
2020.emnlp-main.581.txt,2020,6 Conclusion and Future Work,"through our experiments in gec, we verify the feasibility of span-specific decoding, which has been explored for text infilling (raffel et al., 2019) and text rewriting."
2020.emnlp-main.581.txt,2020,6 Conclusion and Future Work,we propose a novel language-independent approach to improve the efficiency of gec.
2020.emnlp-main.582.txt,2020,6 Conclusion and Future Work,experimental results on several downstream nlp tasks show that our corefbert significantly outperforms bert by considering the coreference information within the text and even improve the performance of the strong roberta model.
2020.emnlp-main.582.txt,2020,6 Conclusion and Future Work,"hence, it is worth developing a novel strategy such as selfsupervised learning to further consider the pronoun."
2020.emnlp-main.582.txt,2020,6 Conclusion and Future Work,"however, the automatic labeling mechanism inevitably accompanies with the wrong labeling problem and it is still an open problem to mitigate the noise.(2) the ds assumption does not consider pronouns in the text, while pronouns play an important role in coreferential reasoning."
2020.emnlp-main.582.txt,2020,6 Conclusion and Future Work,"in the future, there are several prospective research directions: (1) we introduce a distant supervision (ds) assumption in our mrp training task."
2020.emnlp-main.582.txt,2020,6 Conclusion and Future Work,"in this paper, we present a language representation model named corefbert, which is trained on a novel task, mention reference prediction (mrp), for strengthening the coreferential reasoning ability of bert."
2020.emnlp-main.583.txt,2020,4 Conclusions,experiments and visualized analysis demonstrate both graph-attention and graph structure can be replaced by self-attention or transformers.
2020.emnlp-main.583.txt,2020,4 Conclusions,"in addition, we point out the adjacency matrix and the graph structure can be regarded as some kind of task-related prior knowledge."
2020.emnlp-main.583.txt,2020,4 Conclusions,our results suggest that future works introducing graph structure into nlp tasks should explain their necessity and superiority.
2020.emnlp-main.583.txt,2020,4 Conclusions,this study set out to investigate whether graph structure is necessary for multi-hop qa and what role it plays.
2020.emnlp-main.583.txt,2020,4 Conclusions,"we established that with the proper use of pre-trained models, graph structure may not be necessary."
2020.emnlp-main.584.txt,2020,7 Conclusions,"as for future work, we plan to investigate using languages other than english for training (e.g., our larger french and german training sets) in our cross-lingual transfer experiments, since english may not always be the optimal source language (anastasopoulos and neubig, 2020)."
2020.emnlp-main.584.txt,2020,7 Conclusions,"finally, while in our comparative analysis we have focused on a quantitative evaluation for all languages, an additional error analysis per language would be beneficial in revealing the weaknesses and limitations of cross-lingual models."
2020.emnlp-main.584.txt,2020,7 Conclusions,"in this paper we have introduced xl-wic, a large benchmark for evaluating context-sensitive models."
2020.emnlp-main.584.txt,2020,7 Conclusions,"our evaluations show that, even though current language models are effective performers in the zero-shot cross-lingual setting (where no instances in the target language are provided), there is still room for improvement, especially for far languages such as japanese or korean."
2020.emnlp-main.584.txt,2020,7 Conclusions,"xl-wic comprises datasets for a heterogeneous set of 13 languages, including the original english data in wic (pilehvar and camacho-collados, 2019), providing an evaluation framework not only for contextualized models in those languages, but also for experimentation in a cross-lingual transfer setting."
2020.emnlp-main.585.txt,2020,9 Conclusion,"finally, human evaluation showed that generationary is often able to provide a definition that is on a par with or better than one written by a lexicographer."
2020.emnlp-main.585.txt,2020,9 Conclusion,"from two points of view, generationary represents a unified approach: first, it exploits multiple inventories simultaneously, hence going beyond the quirks of each one; second, it is able to tackle both generative (definition modeling) and discriminative tasks (word sense disambiguation and wordin-context), obtaining competitive to state-of-theart results, with particularly strong performances on zero-shot settings."
2020.emnlp-main.585.txt,2020,9 Conclusion,"we introduced generationary, an approach to automatic definition generation which, thanks to a flexible encoding scheme, can (i) encode targets of arbitrary length (including unseen multi-word expressions), and (ii) exploit the vast amount of knowledge encoded in the bart pre-trained encoder-decoder, through fine-tuning."
2020.emnlp-main.585.txt,2020,9 Conclusion,"we make the software and reproduction materials, along with a new evaluation dataset of definitions for adjective-noun phrases (hei++), available at http://generationary.org."
2020.emnlp-main.585.txt,2020,9 Conclusion,"with this work, we showed that generating a definition can be a viable, suitable alternative to the traditional use of sense inventories in computational lexical semantics, and one that better reflects the non-discrete nature of word meaning."
2020.emnlp-main.586.txt,2020,5 Further Discussion and Conclusion,"aspects of lm pretraining, such as the number of model parameters or the size of pretraining data, also impact lexical knowledge stored in the lm’s parameters."
2020.emnlp-main.586.txt,2020,5 Further Discussion and Conclusion,"given the current large gaps between monolingual and multilingual lms, we will also focus on lightweight methods to enrich lexical content in multilingual lms (wang et al., 2020; pfeiffer et al., 2020)."
2020.emnlp-main.586.txt,2020,5 Further Discussion and Conclusion,"in a similar vein, we have run additional experiments with two available italian (it) bert-base models with identical parameter setups, where one was trained on 13gb of it text, and the other on 81gb."
2020.emnlp-main.586.txt,2020,5 Further Discussion and Conclusion,"in en (bert-base)–it bli and clir evaluations we measure improvements from 0.548 to 0.572 (bli), and from 0.148 to 0.160 (clir) with the 81gb it model."
2020.emnlp-main.586.txt,2020,5 Further Discussion and Conclusion,"in future work, we plan to investigate how domains of external corpora affect aoc configurations, and how to sample representative contexts from the corpora."
2020.emnlp-main.586.txt,2020,5 Further Discussion and Conclusion,"in particular, some universal choices of configuration can be recommended: i) choosing monolingual lms; ii) encoding words with multiple contexts; iii) excluding special tokens; iv) averaging over lower layers."
2020.emnlp-main.586.txt,2020,5 Further Discussion and Conclusion,"in-depth analyses of these factors are out of the scope of this work, but they warrant further investigations.opening future research avenues."
2020.emnlp-main.586.txt,2020,5 Further Discussion and Conclusion,"moreover, we found that type-level wes extracted from pretrained lms can surpass static wes like fasttext (bojanowski et al., 2017)."
2020.emnlp-main.586.txt,2020,5 Further Discussion and Conclusion,"our preliminary experiments have verified that en bert-large yields slight gains over the en bertbase architecture used in our work (e.g., peak en lsim scores rise from 0.518 to 0.531)."
2020.emnlp-main.586.txt,2020,5 Further Discussion and Conclusion,"our study has empirically validated that (monolingually) pretrained lms store a wealth of type-level lexical knowledge, but effectively uncovering and extracting such knowledge from the lms’ parameters depends on several crucial components (see §2)."
2020.emnlp-main.586.txt,2020,5 Further Discussion and Conclusion,"the difference in performance across layers also calls for more sophisticated lexical representation extraction methods (e.g., through layer weighting or attention) similar to meta-embedding approaches (yin and schutze ¨ , 2016; bollegala and bao, 2018; kiela et al., 2018)."
2020.emnlp-main.586.txt,2020,5 Further Discussion and Conclusion,this study has only scratched the surface of this research avenue.
2020.emnlp-main.586.txt,2020,5 Further Discussion and Conclusion,"we will also extend the study to more languages, more lexical semantic probes, and other larger underlying lms."
2020.emnlp-main.586.txt,2020,5 Further Discussion and Conclusion,what about larger lms and corpora?
2020.emnlp-main.587.txt,2020,6 Conclusion,current cross-lingual slu models still suffer from imperfect cross-lingual alignments between the source and target languages.
2020.emnlp-main.587.txt,2020,6 Conclusion,"experiments on the cross-lingual slu task illustrate that our model achieves a remarkable performance boost compared to the strong baselines in both zero-shot and few-shot scenarios, and our model has a robust adaptation ability to unrelated target languages in the few-shot scenario."
2020.emnlp-main.587.txt,2020,6 Conclusion,"in addition, visualization for latent variables further proves that our approaches are effective at improving the alignment of crosslingual representations."
2020.emnlp-main.587.txt,2020,6 Conclusion,"in this paper, we propose label regularization (lr) and the adversarial latent variable model (alvm) to regularize and further align the word-level and sentence-level representations across languages without utilizing any additional bilingual resources."
2020.emnlp-main.588.txt,2020,8 Conclusion,"finally, we analyse the performance of two state-of-the-art nlu systems on asr data."
2020.emnlp-main.588.txt,2020,8 Conclusion,"first, we present a novel dataset, which is substantially bigger than other publicly available resources."
2020.emnlp-main.588.txt,2020,8 Conclusion,in a detailed error analysis we demonstrate that the distribution of this metric can be inspected by system developers to identify error types and system weaknesses.
2020.emnlp-main.588.txt,2020,8 Conclusion,"in future work, we hope that slurp will be a valuable resource for developing e2e-slu systems, as well as more traditional pipeline approaches to slu."
2020.emnlp-main.588.txt,2020,8 Conclusion,"in this paper, we present slurp, a new resource package for slu."
2020.emnlp-main.588.txt,2020,8 Conclusion,"our error analysis suggests that this is due to the former approach being able to better account for noise by priming entity tagging, which is a more challenging task than scenario or action recognition."
2020.emnlp-main.588.txt,2020,8 Conclusion,"second, we propose the new slu-f1 metric for evaluating entity prediction in slu tasks."
2020.emnlp-main.588.txt,2020,8 Conclusion,"the next step is to extend slurp with spontaneous speech, which would again increase its complexity, but also move it one step closer to real-life applications."
2020.emnlp-main.588.txt,2020,8 Conclusion,"we find that a sequential decoding approach for slu, which starts from the more abstract notion of scenario and action produces better results for entity tagging, than an approach which works bottom up, i.e.starting from the entities."
2020.emnlp-main.588.txt,2020,8 Conclusion,"we show that this dataset is also more challenging by first conducting a linguistic analysis, and then demonstrating the reduced performance of state-of-the-art asr and nlu systems."
2020.emnlp-main.589.txt,2020,6 Recommendations & Conclusion,for dataset creators: patterns may exist in a real-world task and artificially introducing perturbations may be an easy way to help reduce their effects.
2020.emnlp-main.589.txt,2020,6 Recommendations & Conclusion,for model creators: (1) model probing and experimenting with perturbed inputs can give deep insights about how a model is reasoning (2) experimenting with adversarial inputs early on in the design process can help build better models.
2020.emnlp-main.589.txt,2020,6 Recommendations & Conclusion,in this paper we demonstrate how a popular neural conversational qa dataset inadvertently encodes patterns.
2020.emnlp-main.589.txt,2020,6 Recommendations & Conclusion,this may result in ‘unnatural’ instances in the dataset but could help train better models.
2020.emnlp-main.589.txt,2020,6 Recommendations & Conclusion,we conclude the paper with a few recommendations for the community.
2020.emnlp-main.589.txt,2020,6 Recommendations & Conclusion,we release a modified version of this dataset and also improve evaluation criteria that better reflects model performance.
2020.emnlp-main.589.txt,2020,6 Recommendations & Conclusion,"we would like to emphasize that the patterns found, by their very nature, are likely to occur in real world tasks but the same patterns can also cause neural models to learn poorly."
2020.emnlp-main.59.txt,2020,5 Conclusion and Future Work,"in summary, we propose a new set of embodied localization tasks: localization from embodied dialog - led (localizing the observer from dialog history), embodied visual dialog - evd (modeling the observer), and cooperative localization - cl (modeling both agents)."
2020.emnlp-main.59.txt,2020,5 Conclusion and Future Work,on the led task we show that a lingunet-skip model improves over simple baselines and model ablations but without taking full advantage of the second half of the dialog.
2020.emnlp-main.59.txt,2020,5 Conclusion and Future Work,"since way encapsulates multiple embodied localization tasks, there remains much to be explored."
2020.emnlp-main.59.txt,2020,5 Conclusion and Future Work,to support these tasks we introduce where are you?a dataset containing ∼6k human dialogs from a cooperative localization scenario in a 3d environment.
2020.emnlp-main.59.txt,2020,5 Conclusion and Future Work,way is the first dataset to present extensive human dialog for an embodied localization task.
2020.emnlp-main.590.txt,2020,6 Conclusion,"importantly, our method eliminates part of the spurious correlations between input features and output labels."
2020.emnlp-main.590.txt,2020,6 Conclusion,"in this paper, we propose a weakly-supervised method from a causal perspective and provide the interpretability of our method with the structural causal model."
2020.emnlp-main.590.txt,2020,6 Conclusion,our causal experiments suggest the spurious correlations are more located in entity representation rather than context representation.
2020.emnlp-main.590.txt,2020,6 Conclusion,our method improves generalization ability under limited observational examples.
2020.emnlp-main.591.txt,2020,6 Conclusion,experiments on propara show that ien can better understand scientific procedural texts and outperforms state-of-the-art models.
2020.emnlp-main.591.txt,2020,6 Conclusion,"in this paper, we propose the interactive entity network, ien, for the multi-entity state tracking task, which learns to interpret complex processes by explicitly modeling the synergy among different entities involved in one event and leveraging the causal relationship between entity actions and their subsequent state changes."
2020.emnlp-main.592.txt,2020,6 Conclusion and Future work,"from the perspective of name regularity, mention coverage and context diversity, we conducted both randomization test and verification experiments to evaluate the generalization ability of models."
2020.emnlp-main.592.txt,2020,6 Conclusion and Future work,"our investigation leads to three valuable conclusions, which shows the necessity of decent name regularity to identify unseen mentions, the hazard of high mention coverage to model generalization, and the redundancy of enormous data to capture context patterns."
2020.emnlp-main.592.txt,2020,6 Conclusion and Future work,"the above findings shed light on the promising directions for open ner, including 1) exploiting name regularity more efficiently with easilyobtainable resources such as gazetteers; 2) preventing the overfit on popular in-dictionary mentions with constraints or regularizers; and 3) reducing the need of training data by decoupling the acquisition of context knowledge and name knowledge."
2020.emnlp-main.592.txt,2020,6 Conclusion and Future work,this paper investigates whether current state-ofthe-art models on regular ner can still work well on open ner.
2020.emnlp-main.593.txt,2020,6 Conclusion,"in this paper, we propose an embedding approach for temporal knowledge graphs on a product of riemannian manifolds with heterogeneous curvatures."
2020.emnlp-main.593.txt,2020,6 Conclusion,"to capture the temporal evolution of temporal kgs, we use velocity vectors defined in tangent spaces to learn time-dependent entity representations."
2020.emnlp-main.593.txt,2020,6 Conclusion,"we show that our model significantly outperforms its euclidean counterpart and other state-of-the-art approaches on three benchmark datasets of temporal kgs, which demonstrates the significance of geometrical spaces for the temporal knowledge graph completion task."
2020.emnlp-main.594.txt,2020,7 Conclusions,"also, given the recent success of models such as elmo and bert, it would be interesting to explore extensions of graphglove to the class of contextualized embeddings."
2020.emnlp-main.594.txt,2020,7 Conclusions,our analysis reveals that the structure of the learned graphs is hierarchical and similar to that of wordnet; the geometry is highly non-trivial and contains subgraphs with different local topology.
2020.emnlp-main.594.txt,2020,7 Conclusions,"possible directions for future work include using graphglove for unsupervised hypernymy detection, analyzing undesirable word associations, comparing learned graph topologies for different languages, and downstream applications such as sequence classification."
2020.emnlp-main.594.txt,2020,7 Conclusions,the graph is learned end-to-end in an unsupervised manner.
2020.emnlp-main.594.txt,2020,7 Conclusions,"we introduce graphglove — graph word embeddings, where each word is a node in a weighted graph and the distance between words is the shortest path distance between the corresponding nodes."
2020.emnlp-main.594.txt,2020,7 Conclusions,we show that graphglove substantially outperforms both euclidean and poincare glove ´ on word similarity and word analogy tasks.
2020.emnlp-main.595.txt,2020,7 Summary,"we demonstrated that the approach is also more effective in removing sensitive information from trained embeddings than previous methods, and through attribute vectors, give the user the flexibility to allow sensitive information to be used in predictions when desired."
2020.emnlp-main.595.txt,2020,7 Summary,"we have presented a novel method for debiasing knowledge graph embeddings, which is both significantly faster (allowing training on large knowledge graphs such as wikidata in realistic timeframes) and less disruptive to accuracy than previous approaches."
2020.emnlp-main.596.txt,2020,7 Conclusion,experimental results suggest that stare performs competitively on link prediction tasks over existing hyper-relational approaches and greatly outperforms triple-only baselines.
2020.emnlp-main.596.txt,2020,7 Conclusion,"in the future, we aim at applying stare for node and graph classification tasks as well as extend our approach to large-scale kgs."
2020.emnlp-main.596.txt,2020,7 Conclusion,"in the future, we plan to enrich wd50k entities with class labels and probe it against node classification tasks."
2020.emnlp-main.596.txt,2020,7 Conclusion,"we also identified significant flaws in existing link prediction datasets and proposed wd50k, a novel, wikidata-based hyper-relational dataset that is closer to real-world graphs and better captures the complexity of the link prediction task."
2020.emnlp-main.596.txt,2020,7 Conclusion,"we presented stare, an instance of the message passing framework for representation learning over hyper-relational kgs."
2020.emnlp-main.597.txt,2020,6 Conclusion,additional experimental studies demonstrated that the relational position encoding approach outperformed the other position encodings and showed that it is robust to changes in window size.
2020.emnlp-main.597.txt,2020,6 Conclusion,"in future studies, we plan to increase the number of dimensions of the relational position encodings, since a scalar value may not be able to express positional information adequately."
2020.emnlp-main.597.txt,2020,6 Conclusion,"on four erc datasets, our model improved recognition performance over those of the baselines and existing state-of-the-art methods."
2020.emnlp-main.597.txt,2020,6 Conclusion,we incorporated the relational position encodings in the rgat structure to capture both speaker dependency and the sequential order of utterances.
2020.emnlp-main.597.txt,2020,6 Conclusion,we proposed relational position encodings for rgat to recognize human emotions in textual conversation.
2020.emnlp-main.598.txt,2020,8 Conclusion,"although our method is simple and resource-light, solely relying on an intensity vector which can be derived from as few as a single example, it clearly outperforms previous work on the scalar adjective ranking and indirect question answering tasks."
2020.emnlp-main.598.txt,2020,8 Conclusion,"in future work, we plan to extend our methodology to new languages, and experiment with multilingual and language specific bert models."
2020.emnlp-main.598.txt,2020,8 Conclusion,our intention is also to address adjective ranking in full scales (instead of half-scales) and evaluate the capability of contextualised representations to detect polarity.
2020.emnlp-main.598.txt,2020,8 Conclusion,our performance analysis across bert layers highlights that the lexical semantic knowledge needed for these tasks is mostly located in the higher layers of the bert model.
2020.emnlp-main.598.txt,2020,8 Conclusion,"to create scalar adjective resources in new languages, we could either translate the english datasets or mine adjective scales from starred product reviews as in de marneffe et al.(2010)."
2020.emnlp-main.598.txt,2020,8 Conclusion,we have shown that bert representations encode rich information about the intensity of scalar adjectives which can be efficiently used for their ranking.
2020.emnlp-main.599.txt,2020,6 Conclusion,experiments on sentiment analysis in cross-language and crossdomain settings demonstrate the effectiveness of our method.
2020.emnlp-main.599.txt,2020,6 Conclusion,"in this paper, we study how to adapt the features from the pre-trained language models without tuning."
2020.emnlp-main.599.txt,2020,6 Conclusion,"to enhance the robustness of self-training, we present the method of class-aware feature self-distillation to learn discriminative features."
2020.emnlp-main.599.txt,2020,6 Conclusion,we build our adaptation method based on self-training.
2020.emnlp-main.599.txt,2020,6 Conclusion,"we specifically study unsupervised domain adaptation of prlms, where we transfer the models trained in labeled source domain to the unlabeled target domain based on prlm features."
2020.emnlp-main.6.txt,2020,5 Conclusion,"firstly, we provide a detailed analysis of how the presence of translationese phenomena can adversely affect machine translation results."
2020.emnlp-main.6.txt,2020,5 Conclusion,"in terms of the legitimacy of machine translation evaluation results, our analysis provides sufficient evidence that translationese is a problem for evaluation of systems, in particular in terms of comparison of system performance with automatic metrics such as bleu."
2020.emnlp-main.6.txt,2020,5 Conclusion,this guidance will help to avoid false conclusions due to the application of low powered statistical tests.
2020.emnlp-main.6.txt,2020,5 Conclusion,this results in our first recommendation in future mt evaluations to avoid the use of source side test data that was created via human translation from another language.
2020.emnlp-main.6.txt,2020,5 Conclusion,we explore issues relating to the reliability of machine translation evaluations.
2020.emnlp-main.6.txt,2020,5 Conclusion,"we provided guidance in relation to sample size and statistical power to help planning future human evaluations of mt, particularly relevant to document-level human-parity investigations."
2020.emnlp-main.60.txt,2020,6 Conclusion,"besides treating text annotations as “categorical” labels, in this paper, we show that we can make full use of those labels."
2020.emnlp-main.60.txt,2020,6 Conclusion,"concretely, denotation graphs (dgs) encode structural relations that can be automatically extracted from those texts with linguistic analysis tools."
2020.emnlp-main.60.txt,2020,6 Conclusion,image and text aligned data is rich in semantic correspondence.
2020.emnlp-main.60.txt,2020,6 Conclusion,we plan to investigate other automatic tools in curating more accurate denotation graphs with a complex composition of fine-grained concepts for future directions.
2020.emnlp-main.60.txt,2020,6 Conclusion,we proposed several ways to incorporate dgs into learning representation and validated the proposed approach on several tasks.
2020.emnlp-main.600.txt,2020,5 Conclusion,"in future work, we plan to extend our approach to further improve the reward and policy functions, and to reduce the human-labeling factor."
2020.emnlp-main.600.txt,2020,5 Conclusion,in this paper we proposed a framework for improving a classifier’s performance with synthetic data.
2020.emnlp-main.600.txt,2020,5 Conclusion,"in this work for instance, we did not need more than 20 examples to fine-tune gpt-2 for the sst-2 experiments, or 30 for the trec-6 experiments."
2020.emnlp-main.600.txt,2020,5 Conclusion,we believe this approach is likely to work for any domain or language so long as the language model is able to generate meaningful output.
2020.emnlp-main.600.txt,2020,5 Conclusion,we expect even better results when more examples are provided or with the application of an improved language model like gpt-3.
2020.emnlp-main.600.txt,2020,5 Conclusion,"we have shown in our experiments that even when starting with just a few examples, we are able to achieve noticeable improvements."
2020.emnlp-main.601.txt,2020,7 Conclusion,"let’s go for italian.’ indicates not only a ‘yes’ answer, but also a preference for italian food.)."
2020.emnlp-main.601.txt,2020,7 Conclusion,"moreover, we have explored the phenomena in english."
2020.emnlp-main.601.txt,2020,7 Conclusion,"our first approach towards automatic interpretation is promising, but there is a significant gap especially for examples outside training scenarios."
2020.emnlp-main.601.txt,2020,7 Conclusion,our model does not yet classify additional information in responses (‘dinner?
2020.emnlp-main.601.txt,2020,7 Conclusion,there are exciting avenues for multilingual work to account for language and cultural differences.
2020.emnlp-main.601.txt,2020,7 Conclusion,"we have presented a new dataset containing natural indirect yes/no answers, as well as other significant pragmatic moves in the form of conditionals and uncertain utterances."
2020.emnlp-main.602.txt,2020,7 Conclusion,"finally, as a case study, we show the feasibility for controllable debiasing at debiasing the portrayal of characters in movie scripts."
2020.emnlp-main.602.txt,2020,7 Conclusion,"our approach demonstrates promising results to revise sentences with targeted power and agency, and outperforms ablations and baselines on both automatic and human evaluations."
2020.emnlp-main.602.txt,2020,7 Conclusion,our findings highlight the potential of neural models as a tool for editing out social biases in text.
2020.emnlp-main.602.txt,2020,7 Conclusion,"to this end, we create powertransformer, a transformer-based encoderdecoder trained on a joint reconstruction and paraphrasing objective."
2020.emnlp-main.602.txt,2020,7 Conclusion,"we introduce a new text revision task of controllable debiasing, to help debias the portrayal of characters through the lens of connotation frames of power and agency."
2020.emnlp-main.603.txt,2020,5 Conclusions and Future Work,"as a case study for the effectiveness of the approach, we annotate and publish our mega-dt corpus as a high quality rststyle discourse treebank, which has been shown to outperform previously proposed discourse treebanks (namely yelp13-dt, rst-dt and instr-dt) on most tasks of inter-domain discourse parsing."
2020.emnlp-main.603.txt,2020,5 Conclusions and Future Work,"in conclusion, our new approach allows the nlp community to augment any existing sentimentannotated dataset with discourse trees, enabling the automated generation of large-scale domain/genrespecific discourse treebanks."
2020.emnlp-main.603.txt,2020,5 Conclusions and Future Work,"in this work, we present a novel distant supervision approach to predict the discourse-structure and - nuclearity for documents of arbitrary length solely using document-level sentiment information."
2020.emnlp-main.603.txt,2020,5 Conclusions and Future Work,"our results on the challenging inter-domain discourse-structure and -nuclearity prediction task strongly suggests that the heuristic approach taken (1) enhances the structure prediction task through more diversity in the early-stage tree selection, (2) allows us to effectively predict nuclearity and (3) helps to significantly reduce the complexity of the unrestricted cky approach to scale for arbitrary length documents."
2020.emnlp-main.603.txt,2020,5 Conclusions and Future Work,this suggests that parsers trained on our megadt corpus (or further domain-specific treebanks generated according to our approach) should be used to derive discourse trees in target domains where no gold-labeled data is available.
2020.emnlp-main.603.txt,2020,5 Conclusions and Future Work,"this work can be extended in several ways: (i) we plan to investigate into further functions for τ to enhance the exploration-exploitation tradeoff.(ii) additional strategies to assign nuclearity should be explored, considering the excessive n-nclassification shown in our evaluation.(iii) we plan to apply our approach to more sentiment datasets (e.g., diao et al.(2014)), creating even larger treebanks.(iv) our new and scalable solution can be extended to also predict discourse relations besides structure and nuclearity.(v) we also plan to use a neural discourse parser (e.g."
2020.emnlp-main.603.txt,2020,5 Conclusions and Future Work,"to deal with the increasing spatial complexity, we apply and compare heuristic beam-search strategies, including a stochastic variant inspired by rl techniques."
2020.emnlp-main.603.txt,2020,5 Conclusions and Future Work,"yu et al.(2018)) in combination with our large-scale treebank to fully leverage the potential of data-driven discourse parsing approaches.(vi) taking advantage of the new mega-dt corpus, we want to revisit the potential of discourse-guided sentiment analysis, to enhance current systems, especially for long documents.(vii) finally, more long term, we intend to explore other auxiliary tasks for distant supervision of discourse, like summarization, question answering and machine translation, for which plenty of annotated data exists (e.g., nallapati et al.(2016); cohan et al.(2018); rajpurkar et al.(2016, 2018))."
2020.emnlp-main.604.txt,2020,5 Conclusions,"interestingly, we find statistical differences of trees generated from texts of different quality."
2020.emnlp-main.604.txt,2020,5 Conclusions,"our model identifies the hierarchy of discourse segments without human annotations, and incorporates structural information into the model."
2020.emnlp-main.604.txt,2020,5 Conclusions,the intuition is that it describes coherence by tracking the changes of the focus between discourse segments.
2020.emnlp-main.604.txt,2020,5 Conclusions,"we demonstrate that the identified hierarchical discourse segments improve performance of the model on two tasks, automated essay scoring and assessing writing quality."
2020.emnlp-main.604.txt,2020,5 Conclusions,we propose a neural model of coherence inspired by centering theory.
2020.emnlp-main.605.txt,2020,7 Conclusion and Future Work,"in this paper, we present a generalized computational framework based on the notion of face to operationalize face dynamics in conversations."
2020.emnlp-main.605.txt,2020,7 Conclusion and Future Work,"moreover, we intend to instantiate our proposed framework to other domains such as teacher/student conversations and other types of discourse such as social media narratives."
2020.emnlp-main.605.txt,2020,7 Conclusion and Future Work,one important limitation of the current work is the assumption that all face acts have the same intensity/ranking.
2020.emnlp-main.605.txt,2020,7 Conclusion and Future Work,we also wish to expand the current face framework to a more comprehensive politeness framework that incorporates notions of power and social distance between the interlocutors.
2020.emnlp-main.605.txt,2020,7 Conclusion and Future Work,we believe that our work may be extended to language generation in chatbots for producing more polite language to mediate face threats.
2020.emnlp-main.605.txt,2020,7 Conclusion and Future Work,we develop computational models for predicting face acts as well as observe the impact of these predicted face acts on the donation outcome.
2020.emnlp-main.605.txt,2020,7 Conclusion and Future Work,we instantiate these face act exchanges in the context of persuasion and propose a dataset of 296 conversations annotated with face acts.
2020.emnlp-main.605.txt,2020,7 Conclusion and Future Work,we seek to rectify this by separating the content and style of these face acts.
2020.emnlp-main.606.txt,2020,6 Conclusion,"habertor understands the language of the hatespeech datasets better, is 4-5 times faster, uses less than 1/3 of the memory, and has a better performance in hatespeech classification."
2020.emnlp-main.606.txt,2020,6 Conclusion,"in this paper, we presented the habertor model for detecting hatespeech."
2020.emnlp-main.606.txt,2020,6 Conclusion,"overall, habertor outperforms 15 state-of-the-art hatespeech classifiers and generalizes well to unseen hatespeech datasets, verifying not only its efficiency but also its effectiveness."
2020.emnlp-main.607.txt,2020,6 Conclusions,"a condensed summary of our findings is that (1) tf-idf plt-based methods are definitely worth considering, but are not always competitive, while attention-xml, a neural plt-based method that captures word order, is robust across datasets; (2) transfer learning leads to state-of-the-art results in general, but bert-based models can fail spectacularly when documents are long and technical terms get over-fragmented; (3) the best way to use the label hierarchy in neural methods depends on the proximity of the label assignments in each dataset."
2020.emnlp-main.607.txt,2020,6 Conclusions,"an even shorter summary is that no single method is best across all domains and label groups (all, few, zero) as the language, the size of documents, and the label assignment strongly vary with direct implications in the performance of each method."
2020.emnlp-main.607.txt,2020,6 Conclusions,"finally, we would like to combine plts with bert, similarly to attention-xml, but the computational cost of fine-tuning multiple bert encoders, one for each plt node, would be massive, surpassing the training cost of very large transformerbased models, like t5-3b (raffel et al., 2019) and megatron-lm (shoeybi et al., 2019) with billions of parameters (30-100x the size of bert-base)."
2020.emnlp-main.607.txt,2020,6 Conclusions,"in future work, we would like to further investigate few and zero-shot learning in lmtc, especially in bert models that are currently unable to cope with zero-shot labels."
2020.emnlp-main.607.txt,2020,6 Conclusions,"it is also important to shed more light on the poor performance of bert models in mimic-iii and propose alternatives that can cope both with long documents (kitaev et al., 2020; beltagy et al., 2020) and domain-specific terminology, reducing word over-fragmentation."
2020.emnlp-main.607.txt,2020,6 Conclusions,pretraining bert from scratch on discharge summaries with a new bpe vocabulary is a possible solution.
2020.emnlp-main.607.txt,2020,6 Conclusions,"we presented an extensive study of lmtc methods in three domains, to answer three understudied questions on (1) the competitiveness of plt-based methods against neural models, (2) the use of the label hierarchy, (3) the benefits from transfer learning."
2020.emnlp-main.608.txt,2020,9 Conclusions,"as there is now a large selection of models to choose from, we discuss tradeoffs that emerge between models."
2020.emnlp-main.608.txt,2020,9 Conclusions,in this survey we categorize research in contextualized encoders and discuss some issues regarding its conclusions.
2020.emnlp-main.608.txt,2020,9 Conclusions,"we cover background on contextualized encoders, pretraining objectives, efficiency, data, approaches in model interpretability, and research in multilingual systems."
2020.emnlp-main.608.txt,2020,9 Conclusions,we hope this work provides some assistance to both those entering the nlp community and those already using contextualized encoders in looking beyond sota (and twitter) to make more educated choices.
2020.emnlp-main.609.txt,2020,8 Conclusion and future work,claim verification allows us to trace the sources and measure the veracity of scientific claims.
2020.emnlp-main.609.txt,2020,8 Conclusion and future work,"in this article, we formalize the task of scientific claim verification, and release a dataset (scifact) and models (verisci) to support work on this task."
2020.emnlp-main.609.txt,2020,8 Conclusion and future work,our results indicate that it is possible to train models for scientific fact-checking and deploy them with reasonable efficacy on real-world claims related to covid-19.
2020.emnlp-main.609.txt,2020,8 Conclusion and future work,"scientific claim verification presents a number of promising avenues for research on models capable of incorporating background information, reasoning about scientific processes, and assessing the strength and provenance of various evidence sources."
2020.emnlp-main.609.txt,2020,8 Conclusion and future work,"these abilities have emerged as particularly important in the context of the current pandemic, and the broader reproducibility crisis in science."
2020.emnlp-main.609.txt,2020,8 Conclusion and future work,"this last challenge will be especially crucial for future work that seeks to verify scientific claims against sources other than the research literature – for instance, social media and the news."
2020.emnlp-main.609.txt,2020,8 Conclusion and future work,"we hope that the resources presented in this paper encourage future research on these important challenges, and help facilitate progress toward the broader goal of scientific document understanding."
2020.emnlp-main.61.txt,2020,8 Conclusion,"in this paper, we explore a novel and challenging task to generate video descriptions with rich commonsense descriptions that complement the factual captions."
2020.emnlp-main.61.txt,2020,8 Conclusion,"our evaluation verifies the effectiveness of our method, while also indicating a scope for further study, enhancement, and extensions in the future."
2020.emnlp-main.61.txt,2020,8 Conclusion,"our experiments on using the v2c-transformer as a component for the v2cqa task show that the model has transfer learning capabilities that can be applied to other vision-andlanguage tasks such as question-answering, that require commonsense reasoning."
2020.emnlp-main.61.txt,2020,8 Conclusion,"we expand an existing video captioning dataset for the v2c task through automated retrieval from a textual commonsense corpus followed by human labeling, and present a novel v2ctransformer model to serve as a strong baseline method for the v2c task."
2020.emnlp-main.610.txt,2020,6 Conclusion,"accordingly, we propose to reduce the task of srl to syntactic dependency parsing through back-and-forth conversion to and from a joint label space."
2020.emnlp-main.610.txt,2020,6 Conclusion,experiments show that dependency parsers achieve competitive results on propbank-style srl with the state of the art.
2020.emnlp-main.610.txt,2020,6 Conclusion,linguistic theories assume a close relationship between the realization of semantic arguments and syntactic configurations.
2020.emnlp-main.610.txt,2020,6 Conclusion,this work provides a detailed analysis of the syntactic structures of propbank-style srl and reveals that three common syntactic patterns account for 98% of annotated srl relations for both english and chinese data.
2020.emnlp-main.610.txt,2020,6 Conclusion,this work shows promise of a syntactic treatment of srl and opens up possibilities of applying existing dependency parsing techniques to srl.
2020.emnlp-main.610.txt,2020,6 Conclusion,"we invite future research into further integration of syntactic methods into shallow semantic analysis in other languages and other formulations, such as frame-semantic parsing, and other semantically oriented tasks."
2020.emnlp-main.611.txt,2020,6 Conclusion and Future Work,"in addition, since parade provides entities like “machine code” for definitions, this new dataset could also be useful for other tasks like entity linking (shen et al., 2014), entity retrieval (petkova and croft, 2007) and entity or word sense disambiguation (navigli, 2009)."
2020.emnlp-main.611.txt,2020,6 Conclusion and Future Work,"in the future, we will continue to investigate effective ways to obtain domain knowledge and incorporate it into enhanced models for paraphrase identification."
2020.emnlp-main.611.txt,2020,6 Conclusion and Future Work,we conducted extensive experiments and analysis showing that both state-of-the-art neural models and non-expert human annotators perform poorly on parade.
2020.emnlp-main.611.txt,2020,6 Conclusion and Future Work,"we have presented parade, a new dataset for sentential paraphrase identification requiring domain knowledge."
2020.emnlp-main.612.txt,2020,6 Conclusions and Future Work,"a causal definition is in no way limited to this pairwise case, and future work may generalize it to the sequential case or to event representations that are compositional."
2020.emnlp-main.612.txt,2020,6 Conclusions and Future Work,"having a causal model shines a light on the assumptions made here, and indeed, future work may further refine or overhaul them, a process which may further shine a light on the nature of the knowledge we are after."
2020.emnlp-main.612.txt,2020,6 Conclusions and Future Work,"in the current work, we showed a method calculating the ‘goodness’ of a script in the simplest case: between pairwise events, which we showed still to be quite useful."
2020.emnlp-main.612.txt,2020,6 Conclusions and Future Work,in this work we argued for a causal basis in script learning.
2020.emnlp-main.612.txt,2020,6 Conclusions and Future Work,"we showed how this causal definition could be formalized and used in practice utilizing the tools of causal inference, and verified our method with human and automatic evaluations."
2020.emnlp-main.613.txt,2020,7 Conclusion,"based on our analysis, future work in the direction of automatic bias mitigation may include identifying potentially biased examples in an online fashion and discouraging models from exploiting them throughout the training."
2020.emnlp-main.613.txt,2020,7 Conclusion,the evaluation also suggests that our framework results in better overall robustness compared to the biasspecific counterparts.
2020.emnlp-main.613.txt,2020,7 Conclusion,we adopt the existing debiasing methods into our framework and enable them to obtain equally high improvements on several challenge test sets without targeting a specific bias.
2020.emnlp-main.613.txt,2020,7 Conclusion,we present a general self-debiasing framework to address the impact of unknown dataset biases by omitting the need for thorough dataset-specific analysis to discover the types of biases for each new dataset.
2020.emnlp-main.614.txt,2020,5 Discussion,"an alternative is to use as few labeled examples in the development set as possible, and compare to few-shot parsing trained on the used examples as a strong baseline."
2020.emnlp-main.614.txt,2020,5 Discussion,"if this is the case, then extrinsic evaluation of parsers in downstream tasks (shi et al., 2018), e.g., machine translation (denero and uszkoreit, 2011; neubig et al., 2012; gimpel and smith, 2014), may better show the potential of unsupervised methods."
2020.emnlp-main.614.txt,2020,5 Discussion,"in addition, we find that self-training is a useful post-processing step for unsupervised parsing."
2020.emnlp-main.614.txt,2020,5 Discussion,"our work does not necessarily imply that unsupervised parsers produce poor parses; they may be producing good parses that clash with the conventions of treebanks (klein, 2005)."
2020.emnlp-main.614.txt,2020,5 Discussion,"we suggest that one possibility for future work is to focus on fully unsupervised criteria, such as language model perplexity (shen et al., 2018a, 2019; kim et al., 2019b; peng et al., 2019; li et al., 2020) and model stability across different random seeds (shi et al., 2019), for model selection, as discussed in unsupervised learning work (smith and eisner, 2005, 2006; spitkovsky et al., 2010a,b, inter alia)."
2020.emnlp-main.614.txt,2020,5 Discussion,"while many state-of-the-art unsupervised parsing models are tuned on all labeled examples in a development set (drozdov et al., 2019; kim et al., 2019b; wang et al., 2019, inter alia), we have demonstrated that, given the same data, few-shot parsing with simple data augmentation and self-training can consistently outperform all of these models by a large margin."
2020.emnlp-main.615.txt,2020,7 Conclusions,"also, it yields consistent performance gains even with modest monolingual data (3m sentences) across all translation directions."
2020.emnlp-main.615.txt,2020,7 Conclusions,"also, we avoid the translation errors introduced by lm-fusion, because the tm is able to deviate from the prior when needed."
2020.emnlp-main.615.txt,2020,7 Conclusions,"also, we would like to explore how to overcome the obstacles that prevent us from fully exploiting large pretrained lms (e.g., gpt-2) in low-resource settings."
2020.emnlp-main.615.txt,2020,7 Conclusions,"in future work, we intend to experiment with the lm-prior under more challenging conditions, such as when there is domain discrepancy between the parallel and monolingual data."
2020.emnlp-main.615.txt,2020,7 Conclusions,"in this work, we present a simple approach for incorporating knowledge from monolingual data to nmt."
2020.emnlp-main.615.txt,2020,7 Conclusions,"specifically, we use a lm trained on targetside monolingual data, to regularize the output distributions of a tm."
2020.emnlp-main.615.txt,2020,7 Conclusions,"this makes it useful for low-resource languages, where not only parallel but also monolingual data are scarce."
2020.emnlp-main.615.txt,2020,7 Conclusions,"this method is more efficient than alternative approaches that used pretrained lms, because it is not required during inference."
2020.emnlp-main.615.txt,2020,7 Conclusions,"we empirically show that while this method works by simply changing the training objective, it achieves better results than alternative lm-fusion techniques."
2020.emnlp-main.616.txt,2020,6 Conclusion,"as a continuation to this work, we intend to evaluate whether multilingual translation models are more resilient to lexical disambiguation biases and, as a consequence, are less susceptible to adversarial attacks that exploit source-side homography."
2020.emnlp-main.616.txt,2020,6 Conclusion,"as such, the presented approach is expected to be transferable to other language pairs and translation directions, assuming that the employed translation models share this underlying weakness."
2020.emnlp-main.616.txt,2020,6 Conclusion,extending model-agnostic attack strategies to incorporate other types of dataset biases and to target natural language processing tasks other than machine translation is likewise a promising avenue for future research.
2020.emnlp-main.616.txt,2020,6 Conclusion,"given the model-agnostic nature of our findings, this is likely to be the case."
2020.emnlp-main.616.txt,2020,6 Conclusion,"lastly, the targeted development of models that are resistant to dataset artifacts is a promising direction that is likely to aid generalization across linguistically diverse domains."
2020.emnlp-main.616.txt,2020,6 Conclusion,"our results show that wsd is not yet a solved problem in nmt, and while the general performance of popular model architectures is high, we can identify or create sentences where models are more likely to fail due to data biases."
2020.emnlp-main.616.txt,2020,6 Conclusion,the effectiveness of our methods owes to neural models struggling to accurately distinguish between meaningful lexical correlations and superficial ones.
2020.emnlp-main.616.txt,2020,6 Conclusion,we conducted an initial investigation into leveraging data artifacts for the prediction of wsd errors in machine translation and proposed a simple adversarial attack strategy based on the presented insights.
2020.emnlp-main.617.txt,2020,8 Conclusion,"in future work, we will apply mad-x to other pre-trained models, and employ adapters that are particularly suited for languages with certain properties (e.g.with different scripts)."
2020.emnlp-main.617.txt,2020,8 Conclusion,it leverages a small number of additional parameters to mitigate the capacity issue which fundamentally hinders current multilingual models.
2020.emnlp-main.617.txt,2020,8 Conclusion,mad-x is model-agnostic and can be adapted to any current pre-trained multilingual model as foundation.
2020.emnlp-main.617.txt,2020,8 Conclusion,"we have proposed mad-x, a general modular framework for transfer across tasks and languages."
2020.emnlp-main.617.txt,2020,8 Conclusion,"we have shown that it is particularly useful for adapting to languages not covered by the multilingual model’s training model, while also achieving competitive performance on high-resource languages."
2020.emnlp-main.617.txt,2020,8 Conclusion,"we will also evaluate on additional tasks, and investigate leveraging pre-trained language adapters from related languages for improved transfer to truly low-resource languages with limited monolingual data."
2020.emnlp-main.618.txt,2020,7 Conclusions,"based on the gained insights, we have improved the state-of-theart in xnli for the translate-test and zero-shot approaches by a substantial margin."
2020.emnlp-main.618.txt,2020,7 Conclusions,"finally, we have shown that the phenomenon is not specific to nli but also affects qa, although it is less pronounced there thanks to the translation procedure used in the corresponding benchmarks."
2020.emnlp-main.618.txt,2020,7 Conclusions,"in this paper, we have shown that both human and machine translation can alter superficial patterns in data, which requires reconsidering previous findings in cross-lingual transfer learning."
2020.emnlp-main.618.txt,2020,7 Conclusions,"so as to facilitate similar studies in the future, we release our nli dataset,13 which, unlike previous benchmarks, was annotated in a non-english language and human translated into english."
2020.emnlp-main.619.txt,2020,7 Conclusion,"additionally, in the future, we would also want to quantify the impact of varying degrees of granularity of learning emotional features from tweets on statenet’s performance."
2020.emnlp-main.619.txt,2020,7 Conclusion,"building on psychological studies on analyzing a user’s temporal emotional spectrum, statenet models the time aware emotional context of users through historical tweets for more accurate suicide risk estimation on social media."
2020.emnlp-main.619.txt,2020,7 Conclusion,"motivated by the rising use of social media for exhibiting suicide ideation as opposed to standard clinical practice (mchugh et al., 2019), we present statenet."
2020.emnlp-main.619.txt,2020,7 Conclusion,"priority-based suicide risk assessment for ranking tweets for suicidal risk, rather than classifying them forms our future direction."
2020.emnlp-main.619.txt,2020,7 Conclusion,"through this work, we aim to form a component in a larger human-in-the-loop infrastructure for analyzing potentially concerning suicide-related social media posts."
2020.emnlp-main.619.txt,2020,7 Conclusion,we plan to explore the impact of varying amounts of historical context for a user in our future work.
2020.emnlp-main.619.txt,2020,7 Conclusion,we present a qualitative analysis for a deeper understanding of statenet.
2020.emnlp-main.619.txt,2020,7 Conclusion,we show statenet’s applicability as a preliminary tool in assessing suicidality in tweets.
2020.emnlp-main.62.txt,2020,7 Conclusion and Future Work,"but even in cases where an additive model matches or exceeds human performance on a fixed dataset, additive models may still be insufficient."
2020.emnlp-main.62.txt,2020,7 Conclusion and Future Work,"emap (and related algorithms) has practical implications beyond image+text classification: there are straightforward extensions to nonvisual/non-textual modalities, to classifiers using more than 2 modalities as input, and to singlemodal cases where one wants to check for feature interactions between two groups of features, e.g., premise/hypothesis in nli."
2020.emnlp-main.62.txt,2020,7 Conclusion and Future Work,future work would be well-suited to explore 1) methods for better understanding which datasets (and individual instances) can be rebalanced and which cannot; and 2) the non-trivial task of estimating additive human baselines to compare against.• hypothesis 2: modeling feature interactions can be data-hungry.
2020.emnlp-main.62.txt,2020,7 Conclusion and Future Work,jayakumar et al.(2020) show that feed-forward neural networks can require a very high number of training examples to learn feature interactions.
2020.emnlp-main.62.txt,2020,7 Conclusion and Future Work,"notably, the feature interactions learned even in balanced cases are often not interpretable (subramanian et al., 2019).• hypothesis 3: paradoxically, unimodal models may be too weak."
2020.emnlp-main.62.txt,2020,7 Conclusion and Future Work,"our hope is that future work on multimodal classification tasks report not only the predictive performance of their best model + baselines, but also the emap of that model."
2020.emnlp-main.62.txt,2020,7 Conclusion and Future Work,"so, improvements in unimodal modeling could feasibly improve feature interaction learning.concluding thoughts."
2020.emnlp-main.62.txt,2020,7 Conclusion and Future Work,"so, we may need models with different inductive biases and/or much more training data."
2020.emnlp-main.62.txt,2020,7 Conclusion and Future Work,"the last question on our faq list in §6 leaves us with the following conundrum: 1) additive models are incapable of most cross-modal reasoning; but 2) for most of the unbalanced tasks we consider, emap finds an additive approximation that makes nearly identical predictions to the full, interactive model."
2020.emnlp-main.62.txt,2020,7 Conclusion and Future Work,"the mere fact that unimodal and additive models can often be disarmed by adding valid (but carefully selected) instances post hoc (as in, e.g., kiela et al.(2020)) suggests that their inductive bias can simultaneously be sufficient for train/test generalization, but also fail to capture the spirit of the task."
2020.emnlp-main.62.txt,2020,7 Conclusion and Future Work,"this purported conclusion cannot account for gaps between human and machine performance: if an additive model underperforms relative to human judgment, the gap could plausibly be explained by cross-modal feature interactions."
2020.emnlp-main.62.txt,2020,7 Conclusion and Future Work,"we postulate the following potential explanations, pointing towards future work: • hypothesis 1: these unbalanced tasks don’t require complex cross-modal reasoning."
2020.emnlp-main.62.txt,2020,7 Conclusion and Future Work,"without expressive enough single-modal processing methods, opportunities for learning cross-modal interaction patterns may not be present during training."
2020.emnlp-main.620.txt,2020,6 Summary,our study serves as a starting point for additional work on hierarchical framing classification that can combine issue-specific or event-specific framing analysis with generalized framing dimensions that are comparable across different events and issues.
2020.emnlp-main.620.txt,2020,6 Summary,"to assist this effort, and improve reproducibility we provide additional details in appendix h."
2020.emnlp-main.620.txt,2020,6 Summary,we demonstrate that the subframes can account better for the way issues are framed in the news by both sides to influence their readers.
2020.emnlp-main.620.txt,2020,6 Summary,"we propose a novel embedding-based model extending our subframe lexicon to new text, allowing us to perform analysis more broadly."
2020.emnlp-main.620.txt,2020,6 Summary,"we study the news media coverage of 3 politically polarized topics - abortion, immigration, and gun control; by breaking the high level policy frames into more fine grained, and topic-specific, subframes."
2020.emnlp-main.621.txt,2020,8 Conclusions,"by searching for fc-articles and incorporating fact-checked information into social media posts, we can warn users about fake news and discourage them from spreading misinformation."
2020.emnlp-main.621.txt,2020,8 Conclusions,"complementary to fake news detection methods, our method proactively scales up verified content on social media."
2020.emnlp-main.621.txt,2020,8 Conclusions,"in this paper, we propose a novel method to alleviate the spread of fake news."
2020.emnlp-main.621.txt,2020,8 Conclusions,our framework can be used for other multimodal retrieval tasks (e.g.searching for verified sites as we suggested in the previous section).
2020.emnlp-main.621.txt,2020,8 Conclusions,"our framework uses text and images to search for fc-articles, achieving an average increase of 4.8% over best baselines with the maximum improvement of 11.2%."
2020.emnlp-main.622.txt,2020,4 Conclusion,"future work can explore how to enhance a sub-optimal model using the teacher-student setup for tasks that change across domains or over time, or in scenarios where the original model and data are restricted for privacy reasons."
2020.emnlp-main.622.txt,2020,4 Conclusion,"our framework, however, is not limited to toxicity detection."
2020.emnlp-main.622.txt,2020,4 Conclusion,"through a few labeled probing examples, we can accurately surface orders of magnitude more disguised toxic messages missed by a compromised classifier, using interpretable ml techniques that track the influence of training examples on the probing examples."
2020.emnlp-main.622.txt,2020,4 Conclusion,we propose a framework to robustify toxicity classifiers against veiled toxicity.
2020.emnlp-main.623.txt,2020,6 Conclusion and Future work,"furthermore, we hope to explore the quality of fact-checking explanations with respect to properties other than coherence, e.g., actionability and impartiality.lastly, we plan to explore congruity between veracity prediction and explanation generation tasks, i.e., generating explanations which are compatible with the predicted label and vice versa."
2020.emnlp-main.623.txt,2020,6 Conclusion and Future work,"in order to do this, we hope to explore other subjects, in addition to public health, for which factchecking requires a level of expertise in the subject area."
2020.emnlp-main.623.txt,2020,6 Conclusion and Future work,"in this paper, we explored fact-checking for claims for which specific expertise is required to produce a veracity prediction and explanations (i.e., judgments used for awarding the label/veracity prediction)."
2020.emnlp-main.623.txt,2020,6 Conclusion and Future work,our results show that training veracity prediction and explanation generation models on in-domain data improves the accuracy of veracity prediction and the quality of generated explanations compared to training on generic language models without explanation.
2020.emnlp-main.623.txt,2020,6 Conclusion and Future work,"to support this exploration we constructed pubhealth, a sizeable dataset for public health fact-checking and the first fact-checking dataset to include explanations as annotations."
2020.emnlp-main.623.txt,2020,6 Conclusion and Future work,we hope to explore the topics of explainable fact-checking and specialist fact-checking further.
2020.emnlp-main.624.txt,2020,5 Conclusion,our approaches achieved significant improvement over the previous state-of-the-art on both game scores and training data efficiency.
2020.emnlp-main.624.txt,2020,5 Conclusion,"our formulation also bridges broader nlu/rc techniques to address other critical challenges in if games for future work, e.g., common-sense reasoning, noveltydriven exploration, and multi-hop inference."
2020.emnlp-main.624.txt,2020,5 Conclusion,"we formulate the general if game playing as mprc tasks, enabling an mprc-style solution to efficiently address the key if game challenges on the huge combinatorial action space and the partial observability in a unified framework."
2020.emnlp-main.625.txt,2020,6 Conclusion,"we conducted experiments on two challenging language generation tasks: question generation and data-to-text generation, and our method achieved strong improvements based on human evaluation over previous approaches."
2020.emnlp-main.625.txt,2020,6 Conclusion,"we described two such mechanisms, namely single bandit and hierarchical bandit with multiple rewards."
2020.emnlp-main.625.txt,2020,6 Conclusion,we further presented interpretable analysis on our bandit methods.
2020.emnlp-main.625.txt,2020,6 Conclusion,we presented novel approaches for dynamically optimizing multiple reward metrics simultaneously via multi-armed bandit approach in the context of language generation.
2020.emnlp-main.626.txt,2020,9 Conclusion,"in particular, we have demonstrated that our utterance classification model, medfilter, benefits from discourse information, domain knowledge, speaker-specific context modeling, and a hierarchical loss to reach a new state-of-theart performance on a doctor-patient interactions dataset."
2020.emnlp-main.626.txt,2020,9 Conclusion,"in this paper, we have proposed a novel text classification approach that specifically leverages insights into the organization of task-oriented conversations in order to improve performance at topicbased utterance classification over sota baseline approaches."
2020.emnlp-main.626.txt,2020,9 Conclusion,we believe that the contributions made in this work would also generalize to other kinds of expert-lay dialogue like customer-service chats.
2020.emnlp-main.626.txt,2020,9 Conclusion,"we find that using topic-based utterance classification in general, and medfilter in particular, as a pre-processing step before medical extraction tasks, significantly improves the extraction scores."
2020.emnlp-main.627.txt,2020,6 Conclusion,hesm operates at evidence set level initially and combines information from all the evidence sets using hierarchical aggregation to verify the claim.
2020.emnlp-main.627.txt,2020,6 Conclusion,"in this paper, we have proposed hesm framework for automated fact extraction and verification."
2020.emnlp-main.627.txt,2020,6 Conclusion,our aggregation and ablation study show that our hierarchical aggregation works better than many baseline aggregation methods.
2020.emnlp-main.627.txt,2020,6 Conclusion,our analysis of contextual and non-contextual aggregations illustrates that the aggregations perform different roles and positively contribute to different aspects of fact-verification.
2020.emnlp-main.627.txt,2020,6 Conclusion,"our experiments confirm that our hierarchical evidence set modeling outperforms 7 state-of-the-art baselines, producing more accurate claim verification."
2020.emnlp-main.628.txt,2020,6 Conclusions,"in the future, we will investigate the properties of our proposed method on verifying statements with more complicated operations and explore the explainability of the model."
2020.emnlp-main.628.txt,2020,6 Conclusions,"in this paper, we propose a framework centered around programs and execution to provide symbolic manipulations for table fact verification."
2020.emnlp-main.628.txt,2020,6 Conclusions,"moreover, we design a new training strategy adapting margin loss for the program selection module to derive more accurate programs."
2020.emnlp-main.628.txt,2020,6 Conclusions,our studies also reveal the importance of accurate program acquisition for improving the performance of table fact verification.
2020.emnlp-main.628.txt,2020,6 Conclusions,the experiments show that the proposed model improves the stateof-the-art performance to a 74.4% accuracy on the benchmark dataset tabfact.
2020.emnlp-main.628.txt,2020,6 Conclusions,we propose a verbalization technique together with a graph-based verification network to aggregate and fuse evidences inherently embedded in programs and the original tables for fact verification.
2020.emnlp-main.629.txt,2020,5 Conclusion,"we identify a critical issue with existing claim verification systems, especially the recent models that utilize large pre-trained lms."
2020.emnlp-main.629.txt,2020,5 Conclusion,we propose to perform fact verification under a closed-world setting and present our results on the task of fever.
2020.emnlp-main.629.txt,2020,5 Conclusion,"while it is hard to evaluate the reliance on implicit pretrained knowledge, our initial results indicate that such reliance is helpful for fever."
2020.emnlp-main.63.txt,2020,6 Discussion and Conclusion,"concurrent work in the domain of image classification shows that carefully designed perturbations or manipulations of the input can benefit generalization and lead to performance improvements (chen et al., 2020b; hendrycks et al., 2019)."
2020.emnlp-main.63.txt,2020,6 Discussion and Conclusion,"coupled with pairwise consistency, these modules achieve a new state-of-the-art accuracy on the vqa-cp-v2 dataset and reduce the gap between model performance on vqa-v2 data."
2020.emnlp-main.63.txt,2020,6 Discussion and Conclusion,"in this paper, we present a method that uses input mutations to train vqa models with the goal of out-of-distribution generalization."
2020.emnlp-main.63.txt,2020,6 Discussion and Conclusion,instead we view input mutations as structured perturbations which lead to a semantic change in the input space and a deterministic change in the output space.
2020.emnlp-main.63.txt,2020,6 Discussion and Conclusion,our novel answer projection module trained for minimizing distance between answer and input projections complements the canonical vqa classification task.
2020.emnlp-main.63.txt,2020,6 Discussion and Conclusion,"our type exposure model allows our network to consider all valid answers per question type as equally probable answer candidates, thus moving away from the negative question-answer linguistic priors."
2020.emnlp-main.63.txt,2020,6 Discussion and Conclusion,the mutant paradigm is an effort towards “what if” decision making.code is available here.
2020.emnlp-main.63.txt,2020,6 Discussion and Conclusion,"we differentiate our work from methods using random adversarial perturbations for robust learning (madry et al., 2018)."
2020.emnlp-main.63.txt,2020,6 Discussion and Conclusion,we envision that the concept of input mutations can be extended to other vision and language tasks for robustness.
2020.emnlp-main.63.txt,2020,6 Discussion and Conclusion,"while perception is a cornerstone of understanding, the ability to imagine changes in the scene or language query, and predict outputs for that imagined input allows models to supplement “what” decision making (based on observed inputs) with “what if” decision making (based on imagined inputs)."
2020.emnlp-main.630.txt,2020,6 Conclusion,future work could investigate the use of non-expert human raters to improve the dataset quality further.
2020.emnlp-main.630.txt,2020,6 Conclusion,"in pursuit of improved entity representations, future work could explore the joint use of complementary multi-language descriptions per entity, methods to update representations in a light-weight fashion when descriptions change, and incorporate relational information stored in the kb."
2020.emnlp-main.630.txt,2020,6 Conclusion,"operationalized through wikipedia and wikidata, our experiments using enhanced dual encoder retrieval models and frequency-based evaluation provide compelling evidence that it is feasible to perform this task with a single model covering over a 100 languages."
2020.emnlp-main.630.txt,2020,6 Conclusion,our automatically extracted mewsli-9 dataset serves as a starting point for evaluating entity linking beyond the entrenched english benchmarks and under the expanded multilingual setting.
2020.emnlp-main.630.txt,2020,6 Conclusion,we have proposed a new formulation for multilingual entity linking that seeks to expand the scope of entity linking to better reflect the real-world challenges of rare entities and/or low resource languages.
2020.emnlp-main.631.txt,2020,5 Conclusions,"after demonstrating examples (1) and the effects of oov triggered information loss, we propose multiple methods for mitigating oov during downstream task fine-tuning."
2020.emnlp-main.631.txt,2020,5 Conclusions,"in particular, we show that vocabulary surrogates can provide performance boosts with no additional computation cost, especially when paired with fine-tuning."
2020.emnlp-main.631.txt,2020,5 Conclusions,"we also empirically show that tasks with lower oov suffer less when compared to languages that do not, as seen in table 1."
2020.emnlp-main.631.txt,2020,5 Conclusions,"we then demonstrate and compare with no mitigation, mitigation through network modification, and surrogates, which require no network modification, and show how each approach affects downstream tasks."
2020.emnlp-main.631.txt,2020,5 Conclusions,"while our experiments are limited to cjk languages on bert, we believe the methods proposed are generic and simple to implement and expect the performance gains to also apply to different languages and models."
2020.emnlp-main.632.txt,2020,8 Conclusion,"these preliminary results pave the way for further experiments with other language models, various architectures and new downstream tasks."
2020.emnlp-main.632.txt,2020,8 Conclusion,we investigated the importance of pre-training data volume when training compact transformer-based models.
2020.emnlp-main.632.txt,2020,8 Conclusion,"we made the observation that 100 mb of raw text are sufficient to reach similar performance as with larger datasets on a question answering task, and that corpus-specific self-supervised learning does not bring significant improvements on that particular problem."
2020.emnlp-main.633.txt,2020,6 Discussion,"also, it is possible to use a feed-forward neural network to map features between the hidden spaces of different sizes (jiao et al., 2019) to enable replacement between modules with different input and output sizes."
2020.emnlp-main.633.txt,2020,6 Discussion,"although our model has achieved good performance compressing bert, it would be interesting to explore its possible applications in other neural models."
2020.emnlp-main.633.txt,2020,6 Discussion,"as summarized in table 1, our model does not rely on any model-specific features to compress bert."
2020.emnlp-main.633.txt,2020,6 Discussion,"first, many developed in-place substitutes (e.g., shufflenet unit (zhang et al., 2018) for resblock (he et al., 2016), reformer layer (kitaev et al., 2020) for transformer layer (vaswani et al., 2017)) are natural successor modules that can be directly adopted in theseus compression."
2020.emnlp-main.633.txt,2020,6 Discussion,"for future work, we would like to explore the possibility of applying theseus compression on heterogeneous network modules."
2020.emnlp-main.633.txt,2020,6 Discussion,"in addition, we would like to conduct theseus compression on more types of neural networks including convolutional neural networks and graph neural networks."
2020.emnlp-main.633.txt,2020,6 Discussion,"in this paper, we propose theseus compression, a novel model compression approach."
2020.emnlp-main.633.txt,2020,6 Discussion,our work highlights a new genre of model compression and reveals a new path towards model compression.
2020.emnlp-main.633.txt,2020,6 Discussion,"therefore, it is potential to apply theseus compression to other large models (e.g., resnet (he et al., 2016) in computer vision)."
2020.emnlp-main.633.txt,2020,6 Discussion,we use this approach to compress bert to a compact model that outperforms other models compressed by knowledge distillation.
2020.emnlp-main.633.txt,2020,6 Discussion,"we will also investigate the combination of our compression-based approach with recently proposed dynamic acceleration method (zhou et al., 2020b) to further improve the efficiency of pretrained language models."
2020.emnlp-main.634.txt,2020,6 Conclusion,"experiments demonstrate the superiority of our method in transferring deep pretrained language models, and we provide the open-source recadam optimizer by integrating the proposed mechanisms into adam optimizer to facilitate better usage of deep pretrained language models."
2020.emnlp-main.634.txt,2020,6 Conclusion,"in this paper, we propose to tackle the catastrophic forgetting in transferring deep pretrained language models by bridging two transfer learning paradigms: sequential fine-tuning and multi-task learning."
2020.emnlp-main.634.txt,2020,6 Conclusion,then we introduce the objective shifting mechanism to better balance the learning of the pretraining and downstream tasks.
2020.emnlp-main.634.txt,2020,6 Conclusion,"to cope with the absence of pretraining data during the joint learning of the pretraining task, we introduce a pretraining simulation mechanism to learn the pretraining task without data."
2020.emnlp-main.635.txt,2020,5 Conclusion,"our analysis suggests that data size, the similarity between the source and target tasks and domains, and task complexity are crucial for effective transfer, particularly in data-constrained regimes."
2020.emnlp-main.635.txt,2020,5 Conclusion,these task embeddings allow us to predict source tasks that will likely improve target task performance.
2020.emnlp-main.635.txt,2020,5 Conclusion,we conduct a large-scale empirical study of the transferability between 33 nlp tasks across three broad classes of problems.
2020.emnlp-main.635.txt,2020,5 Conclusion,"we show that the benefits of transfer learning are more pronounced than previously thought, especially when target training data is limited, and we develop methods that learn vector representations of tasks that can be used to reason about the relationships between them."
2020.emnlp-main.636.txt,2020,5 Conclusion & Future Work,"as part of future work, we will explore cvt on other sequence-labeling tasks such as chunking, elementary discourse unit segmentation and argumentative discourse unit segmentation, thus moving beyond entity-level spans."
2020.emnlp-main.636.txt,2020,5 Conclusion & Future Work,"furthermore, the financial and environmental costs incurred are also significantly lower using cvt as compared to bert."
2020.emnlp-main.636.txt,2020,5 Conclusion & Future Work,"furthermore, we intend to implement cvt as a training strategy over transformers (bert) and compare it with adaptivelypretrained bert."
2020.emnlp-main.636.txt,2020,5 Conclusion & Future Work,"moreover, other supervised tasks such as classification could also be studied in this context."
2020.emnlp-main.636.txt,2020,5 Conclusion & Future Work,"we compare the task-specific semi-supervised method, cvt, with a task-agnostic semi-supervised approach, bert (with and without adaptive pretraining), on a variety of problems that can be modeled as sequence tagging tasks."
2020.emnlp-main.636.txt,2020,5 Conclusion & Future Work,we find that the cvt-based approach is more robust than bertbased models across tasks and types of unsupervised data available to them.
2020.emnlp-main.637.txt,2020,8 Conclusion,"al should level the playing field by directing limited annotations most effectively so that labels complement, rather than duplicate, unsupervised data."
2020.emnlp-main.637.txt,2020,8 Conclusion,future work may focus on finding representations that encode the most important information for al.
2020.emnlp-main.637.txt,2020,8 Conclusion,"like badge, we project data into an embedding space and then select the most representative points."
2020.emnlp-main.637.txt,2020,8 Conclusion,"luckily, transformers have generalized knowledge about language that can help acquire data for fine-tuning."
2020.emnlp-main.637.txt,2020,8 Conclusion,"nevertheless, like other deep models, their accuracy and stability require fine-tuning on large amounts of data."
2020.emnlp-main.637.txt,2020,8 Conclusion,our method is unique because it only relies on self-supervision to conduct sampling.
2020.emnlp-main.637.txt,2020,8 Conclusion,transformers are powerful models that have revolutionized nlp.
2020.emnlp-main.637.txt,2020,8 Conclusion,using the pre-trained loss guides the al process to sample diverse and uncertain examples in the cold-start setting.
2020.emnlp-main.638.txt,2020,6 Conclusions,a natural future direction is to conduct a similar empirical investigation of al over bert in the context of multi-class classification and regression tasks.
2020.emnlp-main.638.txt,2020,6 Conclusions,"a parallel line of research, dating back nearly three decades, is the notion of al, aiming to minimize labeling burden within the supervised learning paradigm."
2020.emnlp-main.638.txt,2020,6 Conclusions,"aiming to further bridge the gap between research and practice, in our imbalanced-practical mode we simulate a user within this challenging scenario, armed only with simple queries to define the labeled seed that will bootstrap the al process."
2020.emnlp-main.638.txt,2020,6 Conclusions,"finally, the present work focused on existing al strategies, which were mostly developed in the vision domain for cnns."
2020.emnlp-main.638.txt,2020,6 Conclusions,"for example, bert arguably attains excellent performance with relatively little labeled data, used to fine-tune this pre-trained model for a concrete task."
2020.emnlp-main.638.txt,2020,6 Conclusions,"however, while the random al baseline is limited in its ability to help bert emerge from this poor initial model, al strategies turn out to be very helpful."
2020.emnlp-main.638.txt,2020,6 Conclusions,"it is not obvious to begin with, to what extent al strategies can be used to outperform this already high bar."
2020.emnlp-main.638.txt,2020,6 Conclusions,"it would also be interesting to investigate the realm of larger annotation budgets, and more recent bert variants (liu et al., 2019; lan et al., 2019)."
2020.emnlp-main.638.txt,2020,6 Conclusions,"moreover, we further focus our attention on a scenario well known to many practitioners – and notoriously difficult from a learning perspective – that of building a classifier when the class of interest is scarce in the data at hand."
2020.emnlp-main.638.txt,2020,6 Conclusions,"notably, a training data seed resulting from a simple query is expected to capture only limited, and perhaps somewhat obvious, aspects of the class under consideration."
2020.emnlp-main.638.txt,2020,6 Conclusions,"our results demonstrate the potential of al on top of bert, especially in this latter scenario."
2020.emnlp-main.638.txt,2020,6 Conclusions,"our study shows that the initial bert model indeed suffers from poor prediction performance, mainly due to low recall values."
2020.emnlp-main.638.txt,2020,6 Conclusions,"the development of novel al methods, that are tailored for pre-trained models such as bert, seems like an important direction for future work."
2020.emnlp-main.638.txt,2020,6 Conclusions,the pairing of these two influential threads raises non-trivial questions.
2020.emnlp-main.638.txt,2020,6 Conclusions,"the promise embodied in these models is their ability to exploit massive unlabeled textual data to learn versatile, arguably universal language representations."
2020.emnlp-main.638.txt,2020,6 Conclusions,"the recent emergence of pre-trained models, with bert as a prominent example, is reshaping the nlp arena (qiu et al., 2020)."
2020.emnlp-main.638.txt,2020,6 Conclusions,"these representations, in turn, are proven to be effective for a multitude of downstream nlp tasks."
2020.emnlp-main.638.txt,2020,6 Conclusions,this work focused on various binary classification tasks.
2020.emnlp-main.638.txt,2020,6 Conclusions,"to the best of our knowledge, the present work provides the first systematic study in this context, while focusing on the prevalent problem of text classification."
2020.emnlp-main.638.txt,2020,6 Conclusions,"using the al pipeline, bert improves its recall by a large margin, generalizing beyond the narrow data it was initially exposed to."
2020.emnlp-main.638.txt,2020,6 Conclusions,"we hope that the experimental results and analyses reported here, as well as the release of the research framework we developed, would be instrumental for these and other future studies."
2020.emnlp-main.639.txt,2020,6 Conclusion,"additionally, we demonstrated that while techniques for mixing domain experts can lead to improved performance for both sentiment analysis and rumour detection, determining a beneficial mixing of such experts is challenging."
2020.emnlp-main.639.txt,2020,6 Conclusion,both domain adversarial training and mixture of experts techniques were explored.
2020.emnlp-main.639.txt,2020,6 Conclusion,"in this work, we investigated the problem of multisource domain adaptation with large pretrained transformer models."
2020.emnlp-main.639.txt,2020,6 Conclusion,"the best method we tested was a simple averaging of the domain experts, and we provided some evidence as to why this effect was observed."
2020.emnlp-main.639.txt,2020,6 Conclusion,"we find that lpx models may be better suited for datadriven techniques such as that of gururangan et al.(2020), which focus on inducing a better prior into the model through pretraining, as opposed to techniques which focus on learning a better posterior with architectural enhancements."
2020.emnlp-main.639.txt,2020,6 Conclusion,we hope that this work can help inform researchers of considerations to make when using lpx models in the presence of domain shift.
2020.emnlp-main.639.txt,2020,6 Conclusion,"while domain adversarial training could effectively induce more domain agnostic representations, it had a mixed effect on model performance."
2020.emnlp-main.64.txt,2020,5 Conclusion,"experiments on two human conversation datasets demonstrate that our model successfully mitigates gender bias in dialogue models and outperforms baselines by producing more engaging, diverse, and gender-specific responses."
2020.emnlp-main.64.txt,2020,5 Conclusion,"in the future, we will investigate debiasing retrieval-based dialogue models and more complicated pipeline-based dialogue systems."
2020.emnlp-main.64.txt,2020,5 Conclusion,"in this work, we focus on the problem of mitigating gender bias in neural dialogue models."
2020.emnlp-main.64.txt,2020,5 Conclusion,we propose an adversarial training framework debiasedchat to reduce the bias of a dialogue model during the training process.
2020.emnlp-main.64.txt,2020,5 Conclusion,"with the help of a disentanglement model, we design an adversarial learning framework that trains dialogue models to cleverly include unbiased gender features and exclude biased gender features in responses."
2020.emnlp-main.640.txt,2020,6 Conclusion and Future Work,"distilling models to their vvma counterparts would be an interesting experiment, and potentially an orthogonal enhancement to pre-existing frameworks (sanh et al., 2019)."
2020.emnlp-main.640.txt,2020,6 Conclusion and Future Work,"in future work, we plan to optimize the lowlevel code and to develop new hardware to deploy vvmas in real-world applications."
2020.emnlp-main.640.txt,2020,6 Conclusion and Future Work,"vvmas could also be an orthogonal contribution to other factorizations of nlp models, such as in (lan et al., 2020)."
2020.emnlp-main.640.txt,2020,6 Conclusion and Future Work,"we have proposed a novel vector-vector-matrix architecture for low-latency inference, and we have demonstrated theoretical and empirical speed-ups for seq2seq-lstm and transformer models, with application to neural machine translation, language modeling, and image classification."
2020.emnlp-main.640.txt,2020,6 Conclusion and Future Work,we hope that this work would bring the novel concept of ai co-design (between software and hardware) to the domain of nlp applications.
2020.emnlp-main.641.txt,2020,4 Conclusion,"besides, we propose and compare various token representation and pre-processing strategies in order to integrate fillers."
2020.emnlp-main.641.txt,2020,4 Conclusion,"we plan to extend these results by studying the mixing of such textual filler-oriented representations with acoustic representations, and further investigate the representation of fillers learnt during pre-training."
2020.emnlp-main.641.txt,2020,4 Conclusion,"when working with deep contextualised representations of transcribed spoken language, we showed that retaining fillers can improve results, both when modelling language and on a downstream task (foak and stance prediction)."
2020.emnlp-main.642.txt,2020,5 Conclusions,"finally, we show that a speech-only model can successfully predict pitch accent in cases where a text-only model cannot, and that combining text and speech provides only a tiny benefit."
2020.emnlp-main.642.txt,2020,5 Conclusions,"first, we show that a speech-only model benefits from having utterance-level context."
2020.emnlp-main.642.txt,2020,5 Conclusions,"in fact, our bilstm-based text model can hardly outperform a content-word baseline."
2020.emnlp-main.642.txt,2020,5 Conclusions,"second, we show that both the text and the speech-only model derive at least some of their performance from being able to distinguish function words from content words."
2020.emnlp-main.642.txt,2020,5 Conclusions,"these results indicate that the speech-only model uses information available in the prosodic features to surpass the contentword baseline, and that knowing the actual words doesn’t provide much further useful information."
2020.emnlp-main.642.txt,2020,5 Conclusions,this work demonstrates some important principles for predicting pitch accent from text and speech.
2020.emnlp-main.643.txt,2020,8 Conclusion and Future Work,a rich source of company information is earnings calls that provide high risk-reward opportunities given their uniqueness and critical information disclosure.
2020.emnlp-main.643.txt,2020,8 Conclusion and Future Work,"although evidence shows that enriching models with speech and inter-stock correlations can improve volatility forecasting, this area is underexplored."
2020.emnlp-main.643.txt,2020,8 Conclusion and Future Work,"analyzing the demographic, cultural, and gender bias in research pertaining to financial disclosures, particularly earnings calls, forms a future direction of research."
2020.emnlp-main.643.txt,2020,8 Conclusion and Future Work,"existing research (qin and yang, 2019; yang et al., 2020) and this work at the intersection of natural language processing and earnings calls are limited to a small set of companies and earnings calls."
2020.emnlp-main.643.txt,2020,8 Conclusion and Future Work,"experimenting with other sets of commonly used acoustic features such as mfcc coefficients, opensmile features and audeep features for representing audio utterances also form a future direction for audio feature extraction."
2020.emnlp-main.643.txt,2020,8 Conclusion and Future Work,"first, we want to improve upon the audio feature extraction."
2020.emnlp-main.643.txt,2020,8 Conclusion and Future Work,"second, we want to expand the analysis presented in this paper beyond the s&p 500 index and us-based companies."
2020.emnlp-main.643.txt,2020,8 Conclusion and Future Work,there are several promising directions of future work that we wish to explore.
2020.emnlp-main.643.txt,2020,8 Conclusion and Future Work,"through experiments on s&p 500 index data, we show the merit of crossmodal gated attention fusion, graph-based learning, and multi-task prediction for volatility forecasting."
2020.emnlp-main.643.txt,2020,8 Conclusion and Future Work,"to model the speech of ceos in earnings calls, using semitones rather than raw frequency for pitch-related features."
2020.emnlp-main.643.txt,2020,8 Conclusion and Future Work,"volatility, measured as a deviation in returns, is a reliable indicator of market risk linked with a stock."
2020.emnlp-main.643.txt,2020,8 Conclusion and Future Work,"we propose voltage, a neural architecture that jointly exploits coherence over speech, text, and inter-stock correlations for volatility forecasting following earnings calls."
2020.emnlp-main.643.txt,2020,8 Conclusion and Future Work,"we would also want to work on studying a wider set of earnings calls and companies spanning multiple languages, demographics, speakers and gender."
2020.emnlp-main.644.txt,2020,6 Conclusion,"in this paper, we study the impact of pretraining an ast decoder using an mt model and propose a method to make the pretraining step more effective."
2020.emnlp-main.644.txt,2020,6 Conclusion,our experiments demonstrate that we can improve the performance by around 1.5 bleu points on two language pairs compared to conventional pretraining methods.
2020.emnlp-main.644.txt,2020,6 Conclusion,we show that we can align the latent representations of different modalities by using adversarial loss and make the asr encoder more compatible with the mt decoder.
2020.emnlp-main.645.txt,2020,4 Conclusions and Future work,further analysis of the results (details in supplementary materials) suggest that our model is effective in removing duplicates and generating diverse keyphrases.
2020.emnlp-main.645.txt,2020,4 Conclusions and Future work,"future direction also include alternate architectures, reward schemes, and evaluation using human judges."
2020.emnlp-main.645.txt,2020,4 Conclusions and Future work,"however, we observed the recall of our model is capped by the catseq generator, it did not produce any new keyphrases."
2020.emnlp-main.645.txt,2020,4 Conclusions and Future work,"in this paper, we proposed the first gan architecture for keyphrase generation."
2020.emnlp-main.645.txt,2020,4 Conclusions and Future work,our results show that the proposed model obtains better performance in generating abstractive keyphrases but fails to outperform baseline model for extractive keyphrases.
2020.emnlp-main.645.txt,2020,4 Conclusions and Future work,"recent works (lu et al., 2018; d’autume et al., 2019) have proposed some solutions to address these challenges and we plan to explore them."
2020.emnlp-main.645.txt,2020,4 Conclusions and Future work,"the model consists of a generator that produces a sequence of keyphrases given a document, and a discriminator that distinguishes between human-curated and machine-generated keyphrases."
2020.emnlp-main.645.txt,2020,4 Conclusions and Future work,"the two components of the gan model are trained in an alternating fashion: the scores from the discriminator are used in the policy update of the generator, the keyphrases produced by the generator are used in the training of the discriminator."
2020.emnlp-main.645.txt,2020,4 Conclusions and Future work,we also observed that that our model suffers with some of the common challenges observed in gan training such as vanishing gradient and mode collapse.
2020.emnlp-main.646.txt,2020,7 Conclusion and future work,another important direction is to investigate how to integrate the ability to aggregate entities derived from training on tesa into an abstractive summarizer.
2020.emnlp-main.646.txt,2020,7 Conclusion and future work,"in future work, we would like to expand the domains covered by our dataset, which is biased towards topics found in the source corpus, such as politics."
2020.emnlp-main.646.txt,2020,7 Conclusion and future work,tesa directly measures the ability of summarizers to abstract at a semantic level.
2020.emnlp-main.646.txt,2020,7 Conclusion and future work,this would require models to tackle another challenging issue which we have not addressed: which set of entities should a model aggregate in the first place?
2020.emnlp-main.646.txt,2020,7 Conclusion and future work,"we have compared several baseline models and models adapted from existing abstractive summarizers on tesa, and find that a discriminative fine-tuning achieves the best performance, though this model inherently cannot generate aggregations."
2020.emnlp-main.646.txt,2020,7 Conclusion and future work,"we have proposed tesa, a novel task and an accompanying dataset of crowd-sourced entity aggregations in context."
2020.emnlp-main.647.txt,2020,7 Conclusion,"in future works we plan to add other languages including arabic and hindi, and to investigate the adaptation of neural metrics to multilingual summarization."
2020.emnlp-main.647.txt,2020,7 Conclusion,"we detailed its construction, and its complementary nature to the cnn/dm english summarization dataset."
2020.emnlp-main.647.txt,2020,7 Conclusion,"we presented mlsum, the first large-scale multilingual summarization dataset, comprising over 1.5m article/summary pairs in french, german, russian, spanish, and turkish."
2020.emnlp-main.647.txt,2020,7 Conclusion,"we reported extensive preliminary experiments, highlighting biases observed in existing models, and analyzed the relative performances across languages of state-of-the-art models."
2020.emnlp-main.648.txt,2020,6 Conclusion,experimental results show that our dataset is amenable to abstractive summarization models and is challenging for current models.
2020.emnlp-main.648.txt,2020,6 Conclusion,"multi-xscience is better suited to abstractive summarization than previous mds datasets, since it requires summarization models to exhibit high text understanding and abstraction capabilities."
2020.emnlp-main.648.txt,2020,6 Conclusion,the lack of large-scale dataset has slowed the progress of multi-document summarization (mds) research.
2020.emnlp-main.648.txt,2020,6 Conclusion,"we introduce multi-xscience, a largescale dataset for mds using scientific articles."
2020.emnlp-main.649.txt,2020,9 Conclusion,"in this work, we demonstrate that various aspects of summarization datasets can be intrinsically evaluated for."
2020.emnlp-main.649.txt,2020,9 Conclusion,our findings suggest that more intentional and deliberate decisions should be made in selecting summarization datasets for downstream modelling research and that further scrutiny should be placed upon summarization datasets released in the future.
2020.emnlp-main.649.txt,2020,9 Conclusion,we also find that some aspectlevel estimators may be surprisingly effective at detecting low quality dataset examples.
2020.emnlp-main.649.txt,2020,9 Conclusion,"we specifically show this for 5 properties across 10 popular datasets, uncovering that dataset use is sometimes incongruous with the attributes of the underlying data."
2020.emnlp-main.65.txt,2020,6 Conclusion,an important future direction will be generating the distractors and learning the rationality coefficients.
2020.emnlp-main.65.txt,2020,6 Conclusion,"furthermore, we demonstrated how our approach can be generalized to improve dialogue context-consistency."
2020.emnlp-main.65.txt,2020,6 Conclusion,"our self-conscious agents improved the base agents on the dialogue nli (welleck et al., 2019) and personachat (zhang et al., 2018) dataset, without consistency labels and nli models."
2020.emnlp-main.65.txt,2020,6 Conclusion,this work investigated how modeling public selfconsciousness can help dialogue agents improve persona-consistency.
2020.emnlp-main.65.txt,2020,6 Conclusion,"we also designed a learning method for distractor selection, named distractor memory and proposed a better update for the listener’s world prior."
2020.emnlp-main.65.txt,2020,6 Conclusion,"we showed existing dialogue agents are highly insensitive to contradiction, and introduced an orthogonally applicable method using the rsa framework (frank and goodman, 2012) to alleviate the issue."
2020.emnlp-main.650.txt,2020,6 Conclusion,"collecting data by combining the standard approach, outlier-based collection, and our taboo-based approach produces better training data that in turn leads to more robust models."
2020.emnlp-main.650.txt,2020,6 Conclusion,"finally, we show our approach is complementary to other efforts to increase data diversity, producing higher quality datasets."
2020.emnlp-main.650.txt,2020,6 Conclusion,"in experiments on a range of datasets, we show that prior data collection approaches fail to capture diverse examples, leading to brittle models."
2020.emnlp-main.650.txt,2020,6 Conclusion,this paper presents a novel way of guiding data collection away from over-represented areas in the sample space.
2020.emnlp-main.650.txt,2020,6 Conclusion,we show how the approach is a generalization of prior work in crowdsourcing and present a new form of it for dialog data.
2020.emnlp-main.651.txt,2020,7 Conclusion,a parent-pointer decoder is further proposed to speed up tree prediction.
2020.emnlp-main.651.txt,2020,7 Conclusion,"dialog states are represented as rooted relational graphs to encode compositionality, and encourage knowledge sharing across different domains, verbs, slot types and dialog participators."
2020.emnlp-main.651.txt,2020,7 Conclusion,experimental results show that our dst solution outperforms slot-filling-based trackers by a large margin.
2020.emnlp-main.651.txt,2020,7 Conclusion,this work reformulates dialog state tracking as a conversational semantic parsing task to overcome the limitations of slot filling.
2020.emnlp-main.651.txt,2020,7 Conclusion,we demonstrated how a dialog dataset with structured labels for both user and system utterances can be collected with the aid of a generative dialog simulator.
2020.emnlp-main.651.txt,2020,7 Conclusion,we then proposed a conversational semantic parser that performs dst with an encoder-decoder model and a stackbased memory.
2020.emnlp-main.652.txt,2020,5 Conclusion,"compared to previous work, our dialogues cover a greater variety of dialogue scenes that correspond to a much wider span of document content."
2020.emnlp-main.652.txt,2020,5 Conclusion,"for evaluation, we investigated three types of dialogue tasks and proposed baseline approaches."
2020.emnlp-main.652.txt,2020,5 Conclusion,"we have introduced doc2dial, a new dialogue dataset for goal-oriented tasks that are grounded in documents from multiple domains."
2020.emnlp-main.652.txt,2020,5 Conclusion,we hope this work will inspire and assist both dialogue and document modeling for tackling more real-life dialogue tasks.
2020.emnlp-main.653.txt,2020,7 Conclusion,"in this work, we perform the first large-scale analysis of discourse patterns in media dialog, using a new dataset of 23k annotated news interview transcripts: interview."
2020.emnlp-main.653.txt,2020,7 Conclusion,"our results mirror findings from linguistic studies of news interviews (weizman, 2008; heritage, 1985)."
2020.emnlp-main.653.txt,2020,7 Conclusion,we demonstrate that adding auxiliary tasks for discourse pattern and interrogative type prediction helps model such media dialog.
2020.emnlp-main.653.txt,2020,7 Conclusion,"we observe that responses depend heavily on external knowledge, and present a probabilistic framework for linking factual documents with a conversation."
2020.emnlp-main.653.txt,2020,7 Conclusion,"while we focus on discourse pattern analysis, interview also supports analysis of temporal patterns in interviewing, argumentation, and knowledge grounding in long conversations."
2020.emnlp-main.654.txt,2020,7 Conclusion and Future Work,"finally, another interesting exploration is to extend the model with a jointly trainable movie recommendation and movie information modules."
2020.emnlp-main.654.txt,2020,7 Conclusion and Future Work,"in this work, we have introduced inspired, a new recommendation dialog dataset collected in natural setting and annotated with sociable recommendation strategies."
2020.emnlp-main.654.txt,2020,7 Conclusion and Future Work,our findings show that sociable strategies do have a positive impact on the acceptance of recommendation and dialog quality.
2020.emnlp-main.654.txt,2020,7 Conclusion and Future Work,"then, we plan to investigate the strategy patterns for people with different personalities and movie preferences to make dialog system more personalized."
2020.emnlp-main.654.txt,2020,7 Conclusion and Future Work,"this work opens up several directions for future studies in building sociable and personalized recommendation dialog systems as follows: first, we will explore more ways of utilizing the strategies, including dynamic strategy selection after decoding."
2020.emnlp-main.654.txt,2020,7 Conclusion and Future Work,we analyze the connection between different strategies and the recommendation results.
2020.emnlp-main.655.txt,2020,7 Future Work and Conclusion,"by conditioning on dialog acts or sequences of dialog acts, textual outputs could be better-controlled (sankar and ravi, 2019; see et al., 2019) and combined with knowledge grounding (hedayatnia et al., 2020)."
2020.emnlp-main.655.txt,2020,7 Future Work and Conclusion,"consequently, automatic speech recognition (asr) introduces transcription errors which are especially prevalent in knowledge-oriented text like question answering (peskov et al., 2019)."
2020.emnlp-main.655.txt,2020,7 Future Work and Conclusion,gopalakrishnan et al.(2020) show this is also problematic in information-seeking dialog by comparing models on textual and asr versions of topical chat.
2020.emnlp-main.655.txt,2020,7 Future Work and Conclusion,"however, text is not the native modality of digital assistants."
2020.emnlp-main.655.txt,2020,7 Future Work and Conclusion,"in summary, this work introduces curiosity: a large-scale conversational information seeking dataset."
2020.emnlp-main.655.txt,2020,7 Future Work and Conclusion,"second, dialog act sequences could identify additional data-driven policies that could be used to define rewards or losses."
2020.emnlp-main.655.txt,2020,7 Future Work and Conclusion,the first is to augment our charm model with a text generation module to make a digital version of our human assistants.
2020.emnlp-main.655.txt,2020,7 Future Work and Conclusion,this involves contextualizing and paraphrasing facts which our dataset supports.
2020.emnlp-main.655.txt,2020,7 Future Work and Conclusion,"to close the loop in conversational informationseeking, models need to account for the speechbased environment of digital assistants."
2020.emnlp-main.655.txt,2020,7 Future Work and Conclusion,"we envision digital assistants participating in information-seeking, which means handling speech input."
2020.emnlp-main.655.txt,2020,7 Future Work and Conclusion,we hope that our dataset will encourage further interest in curiosity-driven dialog.
2020.emnlp-main.655.txt,2020,7 Future Work and Conclusion,we see two immediate directions for future work.
2020.emnlp-main.655.txt,2020,7 Future Work and Conclusion,"with curiosity’s unique set of annotations, we design charm which jointly learns to choose facts, predict a policy for the next message, classify dialog acts of messages, and predict if a message will be liked."
2020.emnlp-main.656.txt,2020,6 Conclusion,newly collected or constructed datasets should consider how to carefully craft the collection to mitigate bias issues from the very start.
2020.emnlp-main.656.txt,2020,6 Conclusion,"the methods described in this paper combine data augmentation, positive-bias data collection, and bias controlled training."
2020.emnlp-main.656.txt,2020,6 Conclusion,"they are especially effective when combined, producing less gendered, more balanced, safer utterances that maintain the engagingness of the dialogue."
2020.emnlp-main.656.txt,2020,6 Conclusion,we analyze gender bias in dialogue data and resulting model generations for models trained on dialogue data.
2020.emnlp-main.656.txt,2020,6 Conclusion,"we note that our results show that data collection techniques help mitigate issues, so when it is possible, bias should be considered at the earliest stages of a project."
2020.emnlp-main.656.txt,2020,6 Conclusion,we propose general purpose techniques for reducing gender bias in generated text.
2020.emnlp-main.656.txt,2020,6 Conclusion,"when this is not possible, however, such as in the case of using real-world data or a dataset that already exists, the techniques presented in this paper are shown to be effective at reducing gender bias."
2020.emnlp-main.657.txt,2020,8 Conclusions and Future Work,"future work may explore generating of diverse sets of hypotheses for a given premise and label, with the goal of performing data augmentation."
2020.emnlp-main.657.txt,2020,8 Conclusions and Future Work,other future work will be to measure the performance of gennli on adversarial and similarly challenging nli datasets.
2020.emnlp-main.657.txt,2020,8 Conclusions and Future Work,we also showed its ability to generate hypotheses given premises and particular labels.
2020.emnlp-main.657.txt,2020,8 Conclusions and Future Work,"we conducted extensive experiments with gennli, showing its robustness across challenging empirical conditions."
2020.emnlp-main.657.txt,2020,8 Conclusions and Future Work,"we found several discriminative fine-tuning objectives to outperform log loss, including infinilog, a simple but effective choice."
2020.emnlp-main.657.txt,2020,8 Conclusions and Future Work,"we proposed gennli, a discriminatively-finetuned generative classifier for nli tasks, and empirically characterized its performance by comparing it to discriminative models and pretrained models."
2020.emnlp-main.658.txt,2020,5 Conclusion,beyond this: work on incentive structures and task design could facilitate the creation of crowdsourced datasets that are both creative and consistently labeled.
2020.emnlp-main.658.txt,2020,5 Conclusion,bias mitigation in models and datasets remains a crucial direction for future work if systems based on datasets like the ones we study are to be widely deployed.
2020.emnlp-main.658.txt,2020,5 Conclusion,"finally, there remains room for further empirical work investigating the kinds of task definitions and data collection protocols most likely to yield training data that teaches models transferrable skills."
2020.emnlp-main.658.txt,2020,5 Conclusion,"however, we also observe promising signs that all four of our interventions help to reduce the prevalence of artifacts in the generated hypotheses that reveal the label."
2020.emnlp-main.658.txt,2020,5 Conclusion,machine learning methods work on transfer learning could help to better understand and exploit the effects that drive the successes we have seen with nli data so far.
2020.emnlp-main.658.txt,2020,5 Conclusion,"on another note, most available text corpora, including our wikipedia source text and comparable past nli datasets, contain evidence of social inequalities and stereotypes, which models can easily learn to reproduce (wagner et al., 2015; rudinger et al., 2017)."
2020.emnlp-main.658.txt,2020,5 Conclusion,our chief results on transfer learning are conclusively negative: all four interventions yield substantially worse transfer performance than our base mnli data collection protocol.
2020.emnlp-main.658.txt,2020,5 Conclusion,"our interventions are not meant to address this, and are likely orthogonal."
2020.emnlp-main.658.txt,2020,5 Conclusion,"the need and opportunity that motivated this work remains compelling: human-annotated data like mnli has already proven itself as a valuable tool in teaching machines general-purpose skills for language understanding, and discovering ways to more effectively build and use such data could further accelerate the field’s already fast progress toward robust, general-purpose language understanding technologies."
2020.emnlp-main.658.txt,2020,5 Conclusion,"while these interventions may be helpful for future evaluation data, it appears that the type of creativity induced by our relatively open-ended base prompt works well for pretraining, and the resulting artifacts are a tolerable side-effect of that creativity."
2020.emnlp-main.659.txt,2020,6 Conclusions,auxiliary analysis datasets are meant to be important resources for debugging and understanding models.
2020.emnlp-main.659.txt,2020,6 Conclusions,"finally, we give suggestions on future research directions and on better analysis variance reporting."
2020.emnlp-main.659.txt,2020,6 Conclusions,"however, large instability of current models on some of these analysis sets undermine such benefits and bring non-ignorable obstacles for future research."
2020.emnlp-main.659.txt,2020,6 Conclusions,"in this paper, we examine the issue of instability in detail, provide theoretical and empirical evidence discovering the high inter-example correlation that causes this issue."
2020.emnlp-main.659.txt,2020,6 Conclusions,we hope this paper will guide researchers on how to handle instability and inspire future work in this direction.
2020.emnlp-main.66.txt,2020,8 Conclusion,it also has a clear advantage in the few-shot experiments when only limited labeled data is available.
2020.emnlp-main.66.txt,2020,8 Conclusion,"tod-bert is easy-to-deploy and will be open-sourced, allowing the nlp research community to apply or fine-tune any task-oriented conversational problem."
2020.emnlp-main.66.txt,2020,8 Conclusion,"tod-bert outperforms bert on four dialogue downstream tasks, including intention classification, dialogue state tracking, dialogue act prediction, and response selection."
2020.emnlp-main.66.txt,2020,8 Conclusion,we propose task-oriented dialogue bert (todbert) trained on nine human-human and multiturn task-oriented datasets across over 60 domains.
2020.emnlp-main.660.txt,2020,5 Summary,"in this work, we studied how to build a textual entailment system that can work in open domains given only a couple of examples, and studied the common patterns in a variety of nlp tasks in which textual entailment can be used as a unified solver."
2020.emnlp-main.660.txt,2020,5 Summary,our goal is to push forward the research and practical use of textual entailment in a broader vision of natural language processing.
2020.emnlp-main.660.txt,2020,5 Summary,"our work demonstrates an example that exploring the uniform pattern behind various nlp problems, enabling us to understand the common reasoning process and create potential for machines to learn across tasks and make easy use of indirect supervision."
2020.emnlp-main.660.txt,2020,5 Summary,the final entailment system ufo-entail generalizes well to open domain entailment benchmarks and downstream nlp tasks including question answering and coreference resolution.
2020.emnlp-main.660.txt,2020,5 Summary,"to that end, we proposed utilizing mnli, the largest entailment dataset, and a few examples from the new domain or new task to build an entailment system via crosstask nearest neighbor."
2020.emnlp-main.661.txt,2020,7 Conclusion,"however, we also show limitations of our proposed methods, thereby encouraging future work on conjnli for better understanding of conjunctive semantics."
2020.emnlp-main.661.txt,2020,7 Conclusion,large-scale pre-trained lms like roberta are not able to optimally understand the conjunctive semantics in our dataset.
2020.emnlp-main.661.txt,2020,7 Conclusion,"we presented conjnli, a new stress-test dataset for nli in conjunctive sentences (“and”, “or”, “but”, “nor”) in the presence of negations and quantifiers and requiring diverse “boolean” and “nonboolean” inferences over conjuncts."
2020.emnlp-main.661.txt,2020,7 Conclusion,"we presented some initial solutions via adversarial training and a predicate-aware roberta model, and achieved some reasonable performance gains on conjnli."
2020.emnlp-main.662.txt,2020,5 Conclusion,"first, we used nli-tr to analyze the effects of in-language pretraining."
2020.emnlp-main.662.txt,2020,5 Conclusion,"in addition, mt will presumably get cheaper, faster, and better over time, thereby further strengthening our core claims."
2020.emnlp-main.662.txt,2020,5 Conclusion,"in our final case study, we returned to the general issue of translation quality, but now from the perspective of developing nli systems."
2020.emnlp-main.662.txt,2020,5 Conclusion,"on the basis of these findings, we argue that mt can be more widely adopted for advancing nlp studies on resource-constrained languages."
2020.emnlp-main.662.txt,2020,5 Conclusion,"second, we compared three morphological parsers for turkish with simpler tokenization schemes."
2020.emnlp-main.662.txt,2020,5 Conclusion,these results suggest that mt can help address the paucity of datasets for turkish nli.
2020.emnlp-main.662.txt,2020,5 Conclusion,"though english and turkish have very different grammars and thus stress-test automatic approaches, our team of experts judged the translations to be of very high quality and to preserve the original nli labels consistently."
2020.emnlp-main.662.txt,2020,5 Conclusion,"though language-dependent tasks like dependency parsing are challenging to translate, mt can efficiently transfer large and expensive-to-create labeled datasets from english to other languages in many nlp tasks, including text classification, question answering, and text summarization."
2020.emnlp-main.662.txt,2020,5 Conclusion,we also used nli-tr to investigate central issues in turkish nli.
2020.emnlp-main.662.txt,2020,5 Conclusion,"we created and released the first large turkish nli dataset, nli-tr, by machine translating snli and multinli."
2020.emnlp-main.662.txt,2020,5 Conclusion,"we found that a turkish-only pretraining regime can enhance turkish models significantly, and that morphological parsing is arguably worth its costs only when the training dataset is small."
2020.emnlp-main.662.txt,2020,5 Conclusion,"we release code, models, and data publicly for further research."
2020.emnlp-main.662.txt,2020,5 Conclusion,we showed that models trained on multinli-tr perform well on the expert-translated test set from xnli.
2020.emnlp-main.663.txt,2020,7 Conclusion,we demonstrate that our multitask model outperforms the single-task model on both in-domain and out-of-domain test sets on the czech language.
2020.emnlp-main.663.txt,2020,7 Conclusion,we enhance the target semantic model by incorporating syntax in a multitask learning framework.
2020.emnlp-main.663.txt,2020,7 Conclusion,we have described a semantic dependency parsing model based on annotation projection that do not use any annotated semantic data in the target language.
2020.emnlp-main.664.txt,2020,5 Conclusion,"at a minimum, greater emphasis should be given to task formalization decisions when they deviate from the prevailing standard."
2020.emnlp-main.664.txt,2020,5 Conclusion,"by only varying task formalization, we observe a wide range of results among reasonable task formalizations on wsc and winogrande evaluations."
2020.emnlp-main.664.txt,2020,5 Conclusion,"finally, we find that differences between reasonable formalizations can have big impacts on performance with our case study using wsc."
2020.emnlp-main.664.txt,2020,5 Conclusion,"for example, using a pretrained mlm task head as the basis for a downstream task classifier yields strong results with very little hyperparameter sensitivity."
2020.emnlp-main.664.txt,2020,5 Conclusion,"for mc formalizations, we follow liu et al.for wsc and use spacy to mine candidate nps."
2020.emnlp-main.664.txt,2020,5 Conclusion,having access to candidate nps during inference alone improves the performance on superglue wsc.
2020.emnlp-main.664.txt,2020,5 Conclusion,"however, models with mc evaluation are highly sensitive to hyperparameters and fail to perform better on winogrande."
2020.emnlp-main.664.txt,2020,5 Conclusion,this echoes the strong results seen with t5 and offers further motivation to explore these kinds of design decisions in other tasks.
2020.emnlp-main.664.txt,2020,5 Conclusion,this extrinsic preprocessing step yields dramatic gains without significantly changing the reasoning ability of the model.
2020.emnlp-main.664.txt,2020,5 Conclusion,we also encourage future reports of system performances to use the same task formalization whenever possible.
2020.emnlp-main.664.txt,2020,5 Conclusion,"we believe this will help disentangle gains due to models’ reasoning abilities, especially in situations where these decisions significantly impact performance, such as in wsc."
2020.emnlp-main.664.txt,2020,5 Conclusion,"we find training with paired inputs, using a softmax over candidates, and reusing a pretrained mlm head all help to learn commonsense reasoning and reduce this sensitivity."
2020.emnlp-main.664.txt,2020,5 Conclusion,"we view such gains as orthogonal to the intent of the task and urge benchmark creators to minimize the opportunity for these insubstantial improvements by imposing as much structure as is possible in the released data, for example, by providing candidate nps explicitly."
2020.emnlp-main.664.txt,2020,5 Conclusion,"while we find evidence that these formalization choices can largely influence wsc performance, we do not see obvious evidence of similar occurrences on other task comparisons with roberta."
2020.emnlp-main.665.txt,2020,5 Conclusions,"by preventing a linear classifier from learning the bias from the de-biased representations, we conclusively show that a model using such a classifier with these representations will not make decisions based on the bias."
2020.emnlp-main.665.txt,2020,5 Conclusions,"furthermore, the models trained with an ensemble of adversaries also performed better when tested on snli-hard compared to using only one adversarial classifier."
2020.emnlp-main.665.txt,2020,5 Conclusions,"however, after implementing the adversarial training, a non-linear classifier may still be able to detect the bias in the sentence representations where linear classifiers are not able to."
2020.emnlp-main.665.txt,2020,5 Conclusions,"our method produced sentence representations with significantly less bias, and these more robust models generalised better to 12 different nli datasets, improving over previous approaches in the literature (belinkov et al., 2019b; mahabadi et al., 2020)."
2020.emnlp-main.665.txt,2020,5 Conclusions,"the higher the dimensionality of the sentence representations, the harder it is to de-bias these representations and the higher the optimal number of adversarial classifiers appears to be."
2020.emnlp-main.665.txt,2020,5 Conclusions,this is the behaviour expected from de-biased models that no longer use the hypothesis-only bias to inform their predictions.
2020.emnlp-main.665.txt,2020,5 Conclusions,we set out to prevent nli models learning from the hypothesis-only bias by using an ensemble of adversarial classifiers.
2020.emnlp-main.665.txt,2020,5 Conclusions,"while we illustrate the conditions under which biases are removed from a linear classifier, preventing a nonlinear classifier from learning the biases is more difficult and merits further experimentation."
2020.emnlp-main.666.txt,2020,7 Conclusions,extensive experiments on se2 and several other benchmark datasets demonstrate the effectiveness of synsetexpan on both tasks.
2020.emnlp-main.666.txt,2020,7 Conclusions,"in the future, we plan to study how we can apply synsetexpan at the entity mention level for conducting contextualized synonym discovery and set expansion."
2020.emnlp-main.666.txt,2020,7 Conclusions,this paper shows entity set expansion and synonym discovery are two tightly coupled tasks and can mutually enhance each other.
2020.emnlp-main.666.txt,2020,7 Conclusions,"we present synsetexpan, a novel framework jointly conducting two tasks, and se2 dataset, the first large-scale synonym-enhanced set expansion dataset."
2020.emnlp-main.667.txt,2020,7 Conclusion,"as knowledge graphs are increasingly used as gold standard data sources in artificial intelligence systems, our work is a first step toward making kge predictions more trustworthy."
2020.emnlp-main.667.txt,2020,7 Conclusion,"we investigate calibration as a technique for improving the trustworthiness of link prediction with kge, and uniquely contribute both closed-world and open-world evaluations; the latter is rarely studied for kge, even though it is more faithful to how practitioners would use kge for completion."
2020.emnlp-main.667.txt,2020,7 Conclusion,"we show that there is significant room for improvement in calibrating kge under the owa, and motivate the importance of this direction with our case study of human-ai knowledge graph completion."
2020.emnlp-main.668.txt,2020,5 Conclusion,experimental results prove its effectiveness and efficiency compared to state-of-the-art methods.
2020.emnlp-main.668.txt,2020,5 Conclusion,"in this paper, we proposed a scalable heterogeneous graph model, tg-transformer, for text classification."
2020.emnlp-main.668.txt,2020,5 Conclusion,it also enables parallelization and pre-training in gnn models for further research.
2020.emnlp-main.669.txt,2020,7 Conclusion and outlook,"furthermore, text can be used for few-shot link prediction, an emerging research direction (xiong et al., 2017; shi and weninger, 2017)."
2020.emnlp-main.669.txt,2020,7 Conclusion and outlook,"overall, we hope that codex will provide a boost to research in kgc, which will in turn impact many other fields of artificial intelligence."
2020.emnlp-main.669.txt,2020,7 Conclusion and outlook,"some promising future directions on codex include: • better model understanding codex can be used to analyze the impact of hyperparameters, training strategies, and model architectures in kgc tasks.• revival of triple classification we encourage the use of triple classification on codex in addition to link prediction because it directly tests discriminative power.• fusing text and structure including text in both the link prediction and triple classification tasks should substantially improve performance (toutanova et al., 2015)."
2020.emnlp-main.669.txt,2020,7 Conclusion and outlook,"we present codex, a set of knowledge graph completion datasets extracted from wikidata and wikipedia, and show that codex is suitable for multiple kgc tasks."
2020.emnlp-main.669.txt,2020,7 Conclusion and outlook,"we release data, code, and pretrained models for use by the community at https://bit.ly/2epbrjs."
2020.emnlp-main.67.txt,2020,6 Conclusion,"in addition, the process of data creation and annotation is described in detail."
2020.emnlp-main.67.txt,2020,6 Conclusion,"in this paper, we have presented risawoz, to date the largest human-to-human multi-domain dataset annotated with rich semantic information for taskoriented dialogue modeling."
2020.emnlp-main.67.txt,2020,6 Conclusion,"risawoz is featured with large scale, wide domain coverage, rich semantic annotation and functional diversity, which can facilitate the research of task-oriented dialogue modeling from different aspects."
2020.emnlp-main.67.txt,2020,6 Conclusion,"we also report the benchmark models and results of five evaluation tasks on risawoz, indicating that the dataset is a challenging testbed for future work."
2020.emnlp-main.67.txt,2020,6 Conclusion,"we manually label each dialogue in risawoz not only with comprehensive dialogue annotations for various sub-tasks of task-oriented dialogue systems (e.g., nlu, dst, response generation), but also linguistic annotations over ellipsis and coreference in multi-turn dialogue."
2020.emnlp-main.670.txt,2020,6 Conclusion and Future Work,"also, we only consider positively label-indicative metadata combinations currently."
2020.emnlp-main.670.txt,2020,6 Conclusion and Future Work,"experimental results and case studies demonstrate that our model outperforms previous methods significantly, thereby signifying the advantages of leveraging metadata as weak supervision."
2020.emnlp-main.670.txt,2020,6 Conclusion and Future Work,"in the future, we are interested in effectively integrating different forms of supervision including annotated documents."
2020.emnlp-main.670.txt,2020,6 Conclusion and Future Work,"in this paper, we propose meta, a novel framework that leverages metadata information as an additional source of weak supervision and incorporates it into the classification framework."
2020.emnlp-main.670.txt,2020,6 Conclusion and Future Work,our method organizes the text data and metadata together into a text-rich network and employs motif patterns to capture appropriate metadata combinations.
2020.emnlp-main.670.txt,2020,6 Conclusion and Future Work,there should be negatively label-indicative combinations as well which can eliminate some classes from potential labels.
2020.emnlp-main.670.txt,2020,6 Conclusion and Future Work,this is another potential direction for the extension of our method.
2020.emnlp-main.670.txt,2020,6 Conclusion and Future Work,"using the initial user-provided seed words and motif patterns, our method generates pseudo labels, trains classifier, and ranks and filters highly label-indicative words, motifs in a unified manner and adds them to their respective seed set."
2020.emnlp-main.671.txt,2020,5 Conclusion,"extensive experiments on four real-world datasets demonstrate that msd obtains more accurate uncertainty scores, and superiorly improved classification performance when partial uncertain predictions are simulatively assigned to the experts."
2020.emnlp-main.671.txt,2020,5 Conclusion,"msd can be applied to various dnns (cnn, rnn and transformer) and each component in msd can be arbitrarily assembled."
2020.emnlp-main.671.txt,2020,5 Conclusion,we aims at generating more accurate uncertainty score to improve the performance of text classification with human involvement.
2020.emnlp-main.671.txt,2020,5 Conclusion,we propose msd with three independent components to improve the cws by mitigating the effect of overconfidence and handling the impact of three categories of uncertainty.
2020.emnlp-main.672.txt,2020,6 Conclusion and Future Work,"finally, it would be interesting to do a deeper dive into variations of author strategies in chapterization, focusing more intently on books with large numbers of short chapters as being more reflective of episode boundaries."
2020.emnlp-main.672.txt,2020,6 Conclusion and Future Work,"further, we use this dataset, remove structural cues, and address the task of predicting chapter boundaries."
2020.emnlp-main.672.txt,2020,6 Conclusion and Future Work,"our supervised approach achieves the best performance in exact break prediction, while our unsupervised approach provides information about potential subchapter break points."
2020.emnlp-main.672.txt,2020,6 Conclusion and Future Work,"our work opens up avenues for further research in text segmentation, with potential applications in summarization and discourse analysis."
2020.emnlp-main.672.txt,2020,6 Conclusion and Future Work,potential future work includes combining the neural and cut-based approaches into a stronger method.
2020.emnlp-main.672.txt,2020,6 Conclusion and Future Work,we achieve and f1 score of 0.77 on this task.
2020.emnlp-main.672.txt,2020,6 Conclusion and Future Work,"we build a chapter segmentation dataset resource consisting of 9,126 english fiction novels, using a hybrid approach combining neural inference and regular expression-based rule matching."
2020.emnlp-main.672.txt,2020,6 Conclusion and Future Work,we present two methods for chapter segmentation.
2020.emnlp-main.673.txt,2020,10 Conclusion,"notable findings from our empirical evaluation include: (1) not all neural text generation methods generate high-quality human-mimicking texts–in particular, gpt2, grover, and fair generated better-quality texts and (2) using specific linguistic features and simple neural architectures, we can solve three problems reasonably well, except gpt2 and fair in p2 and grover in p3."
2020.emnlp-main.673.txt,2020,10 Conclusion,"we have conducted comprehensive experiments on three versions of the authorship attribution (aa) problem: (1) the same method or not, (2) human vs. machine (turing test), and (3) who is the author."
2020.emnlp-main.674.txt,2020,7 Conclusion,for a 6 class classification this leap in accuracy is notable.
2020.emnlp-main.674.txt,2020,7 Conclusion,in this paper we proposed a novel multimodal deep learning based model nwqm for quality assessment of english wikipedia articles.
2020.emnlp-main.674.txt,2020,7 Conclusion,"our model combines signals from article text, meta pages and image rendering to construct an improved document representation."
2020.emnlp-main.674.txt,2020,7 Conclusion,"to the best of our knowledge this is the first work which combines several aspects of information available for wikipedia articles and, in particular, the talk page dynamics toward quality assessment."
2020.emnlp-main.674.txt,2020,7 Conclusion,we also perform extensive investigation of the different components of our model to understand their individual utility.
2020.emnlp-main.674.txt,2020,7 Conclusion,we also showcase the utility of fine tuned bidirectional transformers toward document classification especially when combined with niche platform specific signals.
2020.emnlp-main.674.txt,2020,7 Conclusion,we believe our work opens up the necessity of further investigation pertaining to careful information fusion techniques for downstream tasks.
2020.emnlp-main.674.txt,2020,7 Conclusion,we evaluate it against several existing approaches and obtain at most 8% improvement compared to the state-of-the-art method.
2020.emnlp-main.674.txt,2020,7 Conclusion,we perform in-depth qualitative analysis of the obtained predictions and contrast them with the closest baseline.
2020.emnlp-main.675.txt,2020,6 Conclusions,"as next steps, we plan to address further types of revisions and extend our experiments to document-level settings."
2020.emnlp-main.675.txt,2020,6 Conclusions,we demonstrated in an experimental comparison that it is easier to distinguish sentence versions computationally in wikihowtoimprove than in wikiatomicedits.
2020.emnlp-main.675.txt,2020,6 Conclusions,we further introduced a new task of predicting whether a sentence requires revision and showed promising first results on specific types of revisions.
2020.emnlp-main.676.txt,2020,8 Conclusion and Future Work,"another interesting direction of future research is to explore the cold start problem, where man-sf could be leveraged to predict stock movements for new stocks."
2020.emnlp-main.676.txt,2020,8 Conclusion and Future Work,extensive quantitative and qualitative experiments on real market data demonstrate man-sf’s applicability for neural stock forecasting.
2020.emnlp-main.676.txt,2020,8 Conclusion and Future Work,"lastly, we would also like to extend man-sf’s architecture to not be limited to model all stocks together (because of its gat component) to increase scalability to cross-market scenarios."
2020.emnlp-main.676.txt,2020,8 Conclusion and Future Work,"we plan to further use news articles, earnings calls, and other data sources to capture market dynamics better."
2020.emnlp-main.676.txt,2020,8 Conclusion and Future Work,"we propose man-sf, a neural model that jointly learns temporally relevant signals from chaotic multimodal data spanning historical prices, tweets, and inter stock correlations in a hierarchical fashion."
2020.emnlp-main.676.txt,2020,8 Conclusion and Future Work,"we study stock movement prediction by using natural language, graph-based and numeric features."
2020.emnlp-main.677.txt,2020,7 Conclusions and Future Work,"as a case study, we investigated the opportunity of nlp to enhancing project sustainability through improved community profiling by providing a cost effective way towards structuring qualitative data."
2020.emnlp-main.677.txt,2020,7 Conclusions and Future Work,"future work will investigate ways to improve performance (and especially precision scores) on our data, in particular on low-support labels."
2020.emnlp-main.677.txt,2020,7 Conclusions and Future Work,"in this context, we proposed the new challenging task of automatic user-perceived values classification: we provided the task definition, an annotated dataset (the stories2insights corpus) and a set of light (in terms of overall number of parameters) neural baselines for future reference."
2020.emnlp-main.677.txt,2020,7 Conclusions and Future Work,"in this study, we provided a first stepping stone towards future research at the intersection of nlp and sustainable development (sd)."
2020.emnlp-main.677.txt,2020,7 Conclusions and Future Work,"possible research direction could include more sophisticated thresholding selection techniques (fan and lin, 2007; read et al., 2011) to replace the simple threshold finetuning which is currently used for simplicity."
2020.emnlp-main.677.txt,2020,7 Conclusions and Future Work,"this research is in line with a general call for ai towards social good, where the potential positive impact of nlp is notably missing."
2020.emnlp-main.677.txt,2020,7 Conclusions and Future Work,"we hope our work will spark interest and open a constructive dialogue between the fields of nlp and sd, and result in new interesting applications."
2020.emnlp-main.677.txt,2020,7 Conclusions and Future Work,"while deeper and computationally heavier models as devlin et al.(2019) could possibly obtain notable gains in performance on our data, it is the responsibility of the nlp community – especially with regards to social good applications – to provide solutions which don’t penalise countries suffering from access biases (as contexts with low access to computational power), as it is the case of many developing countries."
2020.emnlp-main.678.txt,2020,7 Conclusion,"further, the model substantially outperforms baseline methods for the task of identifying relevant date-time entities for the task of scheduling a meeting."
2020.emnlp-main.678.txt,2020,7 Conclusion,identifying the negation constraints associated with date-time entities correctly is necessary for the task of scheduling.
2020.emnlp-main.678.txt,2020,7 Conclusion,"in this paper, we presented a novel model that leverages conventional high recall rule-based models and neural models for utilizing contextual information for identifying task relevant temporal entities."
2020.emnlp-main.678.txt,2020,7 Conclusion,"our proposed model, when used in conjunction with 3 different rule-based models, achieves substantial precision gains for all of them without suffering from a huge recall drop."
2020.emnlp-main.678.txt,2020,7 Conclusion,we also presented a novel approach for identifying the negation constraints of date-time entities.
2020.emnlp-main.678.txt,2020,7 Conclusion,"we showed that the existing neural approaches for detecting negation scopes do not transfer well, and that our proposed model based on heuristics defined over constituency and dependency parses achieves strong performance gains, especially for the case of explicit negations."
2020.emnlp-main.679.txt,2020,6 Conclusion,"the accuracies achieved by our best models, 73.3 for t1 and 79.2 for t2, show a good promise for these models to be deployed in real hr systems."
2020.emnlp-main.679.txt,2020,6 Conclusion,"this paper proposes two novel tasks, competencelevel classification (t1) and resume-description matching (t2), and provides a high-quality dataset as well as robust models using several transformerbased approaches."
2020.emnlp-main.679.txt,2020,6 Conclusion,"to the best of our knowledge, this is the first time that those two tasks are thoroughly studies, especially with the latest transformer architectures."
2020.emnlp-main.679.txt,2020,6 Conclusion,we will continuously explore to improve these models by integrating expert’s knowledge.
2020.emnlp-main.68.txt,2020,10 Conclusion,"furthermore, we provided empirical evidence that our method improves the performance of a response generation model by removing unacceptable utterance pairs from its training data."
2020.emnlp-main.68.txt,2020,10 Conclusion,"in light of the success of noisy corpus filtering in neural machine translation, we attempted to filter out unacceptable utterance pairs from large dialogue corpora in an unsupervised manner."
2020.emnlp-main.68.txt,2020,10 Conclusion,"the proposed scoring method estimates the quality of utterance pairs by focusing on the two crucial aspects of dialogue, namely, the connectivity and content relatedness of utterance pairs."
2020.emnlp-main.68.txt,2020,10 Conclusion,we demonstrated that our scoring method has a higher correlation with human judgment than recently proposed methods.
2020.emnlp-main.68.txt,2020,10 Conclusion,we hope that this study will facilitate discussions in the dialogue response generation community regarding the issue of filtering noisy corpora.
2020.emnlp-main.680.txt,2020,6 Conclusion,comparisons against existing benchmarks demonstrate that cweb differs in many respects: 1) in the distribution of sentences (higher vocabulary variation and named entity frequency); 2) in error density (lower); and 3) in the types of edits and their impact on language model perplexity and semantic change.
2020.emnlp-main.680.txt,2020,6 Conclusion,we argue that a factor behind this is the inability of systems to rely on a strong internal language model in low error density domains.
2020.emnlp-main.680.txt,2020,6 Conclusion,we hope that the dataset shall broaden the target domain of gec beyond learner and/or exam writing and facilitate the development of robust gec models in the open-domain setting.
2020.emnlp-main.680.txt,2020,6 Conclusion,"we release a new gec benchmark, cweb, consisting of website text generated by english speakers at varying levels of proficiency."
2020.emnlp-main.680.txt,2020,6 Conclusion,"we showed that existing state-of-the-art gec models achieve considerably lower performance when evaluated on this new domain, even after finetuning."
2020.emnlp-main.681.txt,2020,6 Conclusion,"indeed, we offer the following conjectures: deep embedding models would benefit by incorporating pmi statistics into their training objective; such models will also benefit from sub-linear scaling of frequent word pairs during training; and, lastly, such models would benefit by learning representations with a dual character, as all of the embedding algorithms we described do by learning vectors and covectors."
2020.emnlp-main.681.txt,2020,6 Conclusion,our low rank embedder framework has evoked the commonalities between many word embedding algorithms.
2020.emnlp-main.681.txt,2020,6 Conclusion,we believe a robust understanding of these algorithms is a prerequisite for theoretically motivated development of deeper models.
2020.emnlp-main.682.txt,2020,6 Conclusion and Future Work,"a limitation of our work is that it has been tested on a single dataset, where 65 words have undergone semantic change; testing our models in datasets of different duration and in different languages will provide clearer evidence of their effectiveness."
2020.emnlp-main.682.txt,2020,6 Conclusion and Future Work,future work can use anomaly detection approaches operating on our model’s predicted word vectors to detect anomalies in a word’s representation across time.
2020.emnlp-main.682.txt,2020,6 Conclusion and Future Work,"importantly, we show that their performance increases alongside the duration of the time period under study, confidently outperforming competitive models and common practices on semantic change."
2020.emnlp-main.682.txt,2020,6 Conclusion and Future Work,through extensive experiments on synthetic and real data we showcase the effectiveness of the proposed models under various settings and in comparison to state-of-the-art on the uk web archive dataset.
2020.emnlp-main.682.txt,2020,6 Conclusion and Future Work,"we also plan to investigate different architectures, such as variational autoencoders (kingma and welling, 2014), and incorporate contextual representations (devlin et al., 2019; hu et al., 2019) to detect new senses of words."
2020.emnlp-main.682.txt,2020,6 Conclusion and Future Work,we introduce three sequential models for semantic change detection that effectively exploit the full sequence of a word’s representations through time to determine its level of semantic change.
2020.emnlp-main.683.txt,2020,5 Conclusions,"in our experiments, we managed to obtain solid results for multiple fine-grained word sense disambiguation benchmarks with the help of our information theory-inspired algorithm."
2020.emnlp-main.683.txt,2020,5 Conclusions,in this paper we investigated how the application of sparse word representations obtained from contextualized word embeddings can provide a substantially increased ability for solving problems that require the distinction of fine-grained word senses.
2020.emnlp-main.683.txt,2020,5 Conclusions,our source code is made available at https://github.com/begab/sparsity_makes_sense.
2020.emnlp-main.683.txt,2020,5 Conclusions,we additionally carefully investigated the effects of increasing the amount of sense-annotated training data and the different design choices we made.
2020.emnlp-main.683.txt,2020,5 Conclusions,we also demonstrated the general applicability of our approach by evaluating it in pos tagging.
2020.emnlp-main.684.txt,2020,5 Conclusion,"after that, we propose a two-step model to investigate semantic capacity of terms, which consists of the offline construction and the online query processes."
2020.emnlp-main.684.txt,2020,5 Conclusion,extensive experiments with datasets from three fields demonstrate the effectiveness and rationality of our model compared with well-designed baselines and human-level evaluations.
2020.emnlp-main.684.txt,2020,5 Conclusion,"in addition, while semantic capacity studied in this paper is restricted to a specific domain, we believe the notion of semantic capacity can be extended to all terms in human language."
2020.emnlp-main.684.txt,2020,5 Conclusion,"in this paper, we explore semantic capacity of terms."
2020.emnlp-main.684.txt,2020,5 Conclusion,the extension of the scope will be the future work.
2020.emnlp-main.684.txt,2020,5 Conclusion,"the offline construction process places domain-specific terms in the hyperbolic space by our proposed lorentz model with npmi, and the online query process deals with user requirements, where semantic capacity is interpreted by norms of embeddings."
2020.emnlp-main.684.txt,2020,5 Conclusion,we first introduce the definition of semantic capacity and propose the semantic capacity association hypothesis.
2020.emnlp-main.685.txt,2020,7 Conclusion and Future Work,empirical results on litbank and ontonotes show that the model is competitive with an unbounded memory version and outperforms a strong rule-based baseline.
2020.emnlp-main.685.txt,2020,7 Conclusion and Future Work,"in future work we plan to apply our model to longer, book length documents, and plan to add more structure to the memory."
2020.emnlp-main.685.txt,2020,7 Conclusion and Future Work,"in particular, we report state of the art results on litbank."
2020.emnlp-main.685.txt,2020,7 Conclusion and Future Work,"the proposed model guarantees a linear runtime in document length, and in practice significantly reduces peak memory usage during training."
2020.emnlp-main.685.txt,2020,7 Conclusion and Future Work,"we propose a memory model which tracks a small, bounded number of entities."
2020.emnlp-main.686.txt,2020,5 Conclusion,our best model shows the new result of 80.2 on the conll 2012 dataset.
2020.emnlp-main.686.txt,2020,5 Conclusion,"we implement the end-to-end coreference resolution model and investigate four higher-order inference methods, including two of our own methods."
2020.emnlp-main.686.txt,2020,5 Conclusion,"we show that current hoi does not meet up with the original motivation, suggesting that a new perspective of hoi is needed for this task in the era of deep learning-based nlp."
2020.emnlp-main.686.txt,2020,5 Conclusion,we thoroughly analyze the empirical effectiveness of hoi and demonstrate why it fails to boost performance on the conll 2012 dataset compared to the improvement from encoders.
2020.emnlp-main.687.txt,2020,8 Conclusion,"our approach directly optimizes the mention representations used by the coreference model, allowing it to be fine-tuned on relatively little data, with improved accuracy."
2020.emnlp-main.687.txt,2020,8 Conclusion,our results demonstrate the potential of pre-training for coreference.
2020.emnlp-main.687.txt,2020,8 Conclusion,we believe there is much potential for additional selfsupervision tasks and leave those for future work.
2020.emnlp-main.687.txt,2020,8 Conclusion,we proposed two self-supervision tasks to improve span representations of coreference resolution models.
2020.emnlp-main.688.txt,2020,5 Conclusions,"experimentally, our approach improved the performance of the state-of-the-art walk-based models on two benchmark kgs."
2020.emnlp-main.688.txt,2020,5 Conclusions,"in future work, we would like to study how to introduce acyclic rules to the walk-based systems."
2020.emnlp-main.688.txt,2020,5 Conclusions,"in this paper, we proposed a collaborative framework utilizing both symbolic-based and walk-based models."
2020.emnlp-main.688.txt,2020,5 Conclusions,we separate the walk-based agent into an entity and relation agent to effectively leverage the symbolic rules and significantly reduce the action space.
2020.emnlp-main.689.txt,2020,6 Conclusion and Future Work,"as discussed in section 5, we also plan to develop a more flexible scoring function which can handle equivalent trees."
2020.emnlp-main.689.txt,2020,6 Conclusion and Future Work,"finally, we plan to evaluate bertft on other temporal relation datasets as part of a larger pipeline, which will include a mapping between tdts and other temporal relation annotation schemas such as the tempeval-3 dataset (uzzaman et al., 2013)."
2020.emnlp-main.689.txt,2020,6 Conclusion and Future Work,"for future research, we plan to explore other types of deep neural lms such as transformerxl (dai et al., 2019) and xlnet (yang et al., 2019)."
2020.emnlp-main.689.txt,2020,6 Conclusion and Future Work,we present an analysis of where and how bert helps with this challenging task.
2020.emnlp-main.689.txt,2020,6 Conclusion and Future Work,"we present two models that incorporate bert into temporal dependency parsers, and observe significant gains compared to previous approaches."
2020.emnlp-main.69.txt,2020,6 Conclusions,each latent factor is associated with an intuitively appealing geographical interpretation.
2020.emnlp-main.69.txt,2020,6 Conclusions,"future directions include the incorporation of phonological and morphosyntactic features, application to other languages, and most importantly, a model extension to infer temporal ordering."
2020.emnlp-main.69.txt,2020,6 Conclusions,"in the experiments, we used synthetic data and fijian lexical data."
2020.emnlp-main.69.txt,2020,6 Conclusions,"in this paper, we proposed a bayesian generative model to analyze dialectal variation."
2020.emnlp-main.69.txt,2020,6 Conclusions,"with this model, we successfully induced a large number of latent factors from a set of noisy surface features."
2020.emnlp-main.690.txt,2020,6 Conclusion,"in doing so, we improved the state of the art on the oie16 benchmark by 0.421 f1 and 0.420 auc-pr, respectively, (i.e., by more than 200% in both cases) with a novel model, consisting of an albert embedding block, a transformer encoding block, and an lstm prediction block, which was trained by means of a training scheme using a newly introduced loss formulation."
2020.emnlp-main.690.txt,2020,6 Conclusion,"in this work, we systematically compared a range of different nn architectures for oie as well as different schemes for training them."
2020.emnlp-main.690.txt,2020,6 Conclusion,"subsequent analysis revealed that choosing the right training scheme is as important as selecting the neural model architecture, as the standard nll loss attributes too much importance to non-essential aspects of the data, and consistently leads to inferior results."
2020.emnlp-main.691.txt,2020,6 Conclusion,"by performing sequence mixup in the latent space, seqmix improves data diversity during active learning, while being able to generate plausible augmented sequences."
2020.emnlp-main.691.txt,2020,6 Conclusion,"for future research, it is interesting to enhance seqmix with language models during the mixup process, and harness external knowledge for further improving diversity and plausibility."
2020.emnlp-main.691.txt,2020,6 Conclusion,our experiments demonstrate that seqmix can improve active learning baselines consistently for ner and event detection tasks; and its benefits are especially prominent in low-data regimes.
2020.emnlp-main.691.txt,2020,6 Conclusion,this method is generic to different active learning policies and various sequence labeling tasks.
2020.emnlp-main.691.txt,2020,6 Conclusion,we propose a simple data augmentation method seqmix to enhance active sequence labeling.
2020.emnlp-main.692.txt,2020,8 Conclusions,additionally we released a new collection of datasets for training and evaluating on the results extraction task.
2020.emnlp-main.692.txt,2020,8 Conclusions,"future work may want to build on our approach for more comprehensive extraction tasks, focussing on more types of result, as well as other information contained in papers such as architectural details and hyperparameters."
2020.emnlp-main.692.txt,2020,8 Conclusions,"our method performs well across various tasks and leaderboards within machine learning, with a taxonomy that can be easily extended without retraining."
2020.emnlp-main.692.txt,2020,8 Conclusions,these datasets enable the training of more fine-grained feature extractors and detailed error analysis.
2020.emnlp-main.692.txt,2020,8 Conclusions,we demonstrated that our approach achieves significant performance gains over the state-of-the-art.
2020.emnlp-main.692.txt,2020,8 Conclusions,we presented a pipeline for extracting results from machine learning papers.
2020.emnlp-main.693.txt,2020,6 Conclusion and discussion,a potential attempt might be to use kg to design the reward in the rl framework to provide weak supervision.
2020.emnlp-main.693.txt,2020,6 Conclusion and discussion,kg has huge potential to provide rich background information in many nlp applications.
2020.emnlp-main.693.txt,2020,6 Conclusion and discussion,our framework also contributes to areas of knowledge graph completion and automatic question-answering for attribute values.
2020.emnlp-main.693.txt,2020,6 Conclusion and discussion,our solution for attribute value extraction can be extended to other nlp tasks.
2020.emnlp-main.693.txt,2020,6 Conclusion and discussion,"specifically, during our experiments, we trained a three-layer deep neural network model, which has much fewer parameters compared to the information extraction system."
2020.emnlp-main.693.txt,2020,6 Conclusion and discussion,"the proposed rl method demonstrates promising performance, where the kg showed its ability to provide guidance in open attribute extraction task."
2020.emnlp-main.693.txt,2020,6 Conclusion and discussion,the remaining computation cost from rl framework is comparably small during both the training and the prediction process.
2020.emnlp-main.693.txt,2020,6 Conclusion and discussion,this paper presents a novel rl framework to perform open attribute value extraction.
2020.emnlp-main.693.txt,2020,6 Conclusion and discussion,"through a set of experiments, we observe that the most of the computation cost is incurred by training the information extraction system."
2020.emnlp-main.693.txt,2020,6 Conclusion and discussion,we leave this as our future work.
2020.emnlp-main.694.txt,2020,6 Conclusion,"although our model is designed to work in unsupervised settings, we investigated the impact of weak-supervision by creating a weakly-supervised dataset and showed that even a slight amount of weak-supervision improves significantly model performance."
2020.emnlp-main.694.txt,2020,6 Conclusion,"assembling paths together results in a graph showing the presence of inherent structure, while generated sentences exhibit coherent and relevant semantics."
2020.emnlp-main.694.txt,2020,6 Conclusion,"for evaluation, we proposed a novel commonsense kb completion task tailored to generative models."
2020.emnlp-main.694.txt,2020,6 Conclusion,"future work can focus on expanding the capabilities to generating whole paragraphs of text from graphs in kb, as well as converting large parts of text into coherent graph structures."
2020.emnlp-main.694.txt,2020,6 Conclusion,"in this approach, a generative model is trained to transfer a sentence to a path and back."
2020.emnlp-main.694.txt,2020,6 Conclusion,in this paper we proposed to use a dual learning bridge between text and commonsense kb.
2020.emnlp-main.694.txt,2020,6 Conclusion,the current work is one step towards the overarching goal of kb construction/completion and generation of human-readable text from kbs.
2020.emnlp-main.695.txt,2020,5 Conclusion,"in so doing, we greatly reduce the memory usage of the model during inference at virtually no cost to performance, thereby providing an option for researchers and practitioners interested in modern coreference resolution models for tasks constrained by memory, like the modeling of book-length texts."
2020.emnlp-main.695.txt,2020,5 Conclusion,we present an online algorithm for space efficient coreference resolution that incorporates contributions from recent neural end-to-end models.
2020.emnlp-main.695.txt,2020,5 Conclusion,we show it is possible to transform a model which performs document-level inference into an incremental algorithm.
2020.emnlp-main.696.txt,2020,5 Conclusion,initialising embeddings with vectors trained on indomain data can improve performance by providing better representations for rare words.
2020.emnlp-main.696.txt,2020,5 Conclusion,our work also suggests that standard model components like embedding tying should be retested as we continue to explore the space of language modeling.
2020.emnlp-main.696.txt,2020,5 Conclusion,this effect persists even as more in-domain data is used to train the language model.
2020.emnlp-main.697.txt,2020,5.8 Conclusion,"in this paper, we propose a pre-training recipe to exploit external unlabeled data for data-to-text generation tasks."
2020.emnlp-main.697.txt,2020,5.8 Conclusion,our proposed model has achieved significant performance under zero-shot and fewshot settings.
2020.emnlp-main.697.txt,2020,5.8 Conclusion,such a framework provides a plausible solution to greatly reduce human annotation costs in future nlg applications.
2020.emnlp-main.698.txt,2020,5 Conclusion,both automatic and human evaluation demonstrate the effectiveness of pointer.
2020.emnlp-main.698.txt,2020,5 Conclusion,"in future work, we hope to leverage sentence structure, such as the use of constituency parsing, to further enhance the design of the progressive hierarchy."
2020.emnlp-main.698.txt,2020,5 Conclusion,our model can be also extended to allow inflected/variant forms and arbitrary ordering of given lexical constraints.
2020.emnlp-main.698.txt,2020,5 Conclusion,the proposed method leverages a large-scale pre-trained model (such as bert initialization and our insertion-based pre-training on wikipedia) to generate text in a progressive manner using an insertion-based transformer.
2020.emnlp-main.698.txt,2020,5 Conclusion,"we have presented pointer, a simple yet powerful approach to generating text from a given set of lexical constraints in a non-autoregressive manner."
2020.emnlp-main.699.txt,2020,5 Conclusions,"the method is based on training an mlm for source and target domains, identifying the tokens to delete by finding the spans where the two models disagree in terms of likelihood, and infilling more appropriate text with the target mlm."
2020.emnlp-main.699.txt,2020,5 Conclusions,this approach yields a competitive performance in fully unsupervised settings and substantially improves over previous works in lowresource settings.
2020.emnlp-main.699.txt,2020,5 Conclusions,we have introduced a novel way of using masked language models for text-editing tasks where no parallel data is available.
2020.emnlp-main.7.txt,2020,7 Conclusion,"as paraphrasing continues to improve and cover more languages, we are optimistic smrt will provide larger improvements across the board—including for higher-resource mt and for additional target languages beyond english."
2020.emnlp-main.7.txt,2020,7 Conclusion,"neural paraphrasers are rapidly improving (wieting et al., 2017, 2019b; li et al., 2018; wieting and gimpel, 2018; hu et al., 2019a,b,c), and the concurrently released prism multi-lingual paraphraser thompson and post (2020a,b) has coverage of 39 languages and outperforms prior work in english paraphrasing."
2020.emnlp-main.7.txt,2020,7 Conclusion,"we present simulated multiple reference training (smrt), which significantly improves performance in low-resource settings—by 1.2 to 7.0 bleu—and is complementary to back-translation."
2020.emnlp-main.70.txt,2020,5 Conclusions,"although the stimuli employed were modeled after those used in experimental studies, to improve the robustness of the findings we felt it necessary to compute means over a variety of sentence frames (table 1), so that any idiosyncrasies of particular frames that are independent of the manipulation under scrutiny wouldn’t unduly (and undetectably) drive the results."
2020.emnlp-main.70.txt,2020,5 Conclusions,examples would include variants that employ longer and more realistic contexts.
2020.emnlp-main.70.txt,2020,5 Conclusions,"finally, we want to be clear that we do not claim that the two lms examined have in any sense ‘failed’ at this task—they were obviously not trained for this purpose."
2020.emnlp-main.70.txt,2020,5 Conclusions,"first, we have analyzed the behavior of only two systems."
2020.emnlp-main.70.txt,2020,5 Conclusions,future work will be required to assess the extent to which these effects do in fact reflect the acquisition of a latent form of discourse modeling ability.
2020.emnlp-main.70.txt,2020,5 Conclusions,in this initial investigation we focused on single-sentence contexts so as to hew as closely as possible to previous experimental work.
2020.emnlp-main.70.txt,2020,5 Conclusions,"on the one hand, we found no compelling evidence that the lms are sensitive to any of the three manipulations within the verbal complex in the context sentence."
2020.emnlp-main.70.txt,2020,5 Conclusions,"on the other hand, one could argue for preliminary support for the claim that one of the lms—gpt-2—is sensitive to the occurence of the two connectives examined here."
2020.emnlp-main.70.txt,2020,5 Conclusions,"our conclusions, of course, remain preliminary in a number of respects."
2020.emnlp-main.70.txt,2020,5 Conclusions,our goal instead was to pose the novel question of to what extent discourse knowledge of the sort examined here may exist latently in the models.
2020.emnlp-main.70.txt,2020,5 Conclusions,"second, we have focused here on broad contrasts between context types that have been studied in the psycholinguistic literature."
2020.emnlp-main.70.txt,2020,5 Conclusions,"since an experiment that collects data on this scale would require a substantial annotation effort, a more careful comparison of this sort must be left for future work."
2020.emnlp-main.70.txt,2020,5 Conclusions,"since each system can be said to stand proxy for a single experimental participant, these results could be argued to be less robust than human language studies, which typically utilize several dozen participants."
2020.emnlp-main.70.txt,2020,5 Conclusions,"that having been said, we consider the identification of alternative language model architectures that are capable of capturing the requisite discourse modeling capability for this task to be an interesting challenge problem for future work."
2020.emnlp-main.70.txt,2020,5 Conclusions,"the results were mostly, but not entirely, negative."
2020.emnlp-main.70.txt,2020,5 Conclusions,"third, there are many variations of the studies presented here that could be attempted."
2020.emnlp-main.70.txt,2020,5 Conclusions,"this improves the robustness of our results in terms of items—whereas participants in psycholinguistic studies typically see only one example sentence for each verb, the lms here saw 24—it also means that no lab data exists for the exact stimuli used here."
2020.emnlp-main.70.txt,2020,5 Conclusions,"we examined three context pairs with superficially similar linguistic properties that the experimental literature has shown to result in divergent next-mention biases, both with and without connectives."
2020.emnlp-main.70.txt,2020,5 Conclusions,we hope that this short paper will inspire further research that takes next steps in this and a variety of other directions.
2020.emnlp-main.70.txt,2020,5 Conclusions,we set out to evaluate the extent to which neural lms latently acquire the discourse modeling capability necessary to perform a particular type of incremental processing that human language users do: the ability to predict what entities are most likely to be mentioned next.
2020.emnlp-main.70.txt,2020,5 Conclusions,"whereas this limitation is shared with previous work that probes lms for inherently acquired syntactic knowledge, the robustness of the findings would be enhanced by examining a broader range of systems and/or system configurations so as to better capture the kinds of variation found among groups of human participants."
2020.emnlp-main.700.txt,2020,5 Conclusions,"in this work, we propose palm, a novel approach to pre-training an autoencoding and autoregressive language model on a large unlabeled corpus, designed to be fine-tuned on downstream generation conditioned on context."
2020.emnlp-main.700.txt,2020,5 Conclusions,"it has been shown in prior work (liu et al., 2019) that training for more steps over a larger corpus can potentially improve the performance of pre-training."
2020.emnlp-main.700.txt,2020,5 Conclusions,"it is built upon an extension of the transformer encoder-decoder, and jointly pre-trains the encoder and the decoder in an autoencoding denoising stage followed by an autoregressive generation stage."
2020.emnlp-main.700.txt,2020,5 Conclusions,our future work will explore the potential of training palm for longer on much more unlabeled text data.
2020.emnlp-main.700.txt,2020,5 Conclusions,"palm significantly advances the state-of-the-art results on a variety of context-conditioned generation applications, including generative qa (rank 1 on the marco leaderboard), abstractive summarization, question generation, and conversational response generation."
2020.emnlp-main.701.txt,2020,5 Conclusion & Future Works,future research works can be conducted to make the generation process more robust and interpretable.
2020.emnlp-main.701.txt,2020,5 Conclusion & Future Works,"in this paper, we propose a gradient guided method to conduct unsupervised lexical constraint generation."
2020.emnlp-main.701.txt,2020,5 Conclusion & Future Works,our method achieved state-of-the-art performance on both of these tasks.
2020.emnlp-main.701.txt,2020,5 Conclusion & Future Works,the lexical constraints include hard constraints (keywords) and soft constraints (paraphrasing).
2020.emnlp-main.701.txt,2020,5 Conclusion & Future Works,"then, we use the value of the gradient norm to decide which word in the sentence has the most urgent need to be edited, including being inserted in the front or the back, being deleted, and being changed."
2020.emnlp-main.701.txt,2020,5 Conclusion & Future Works,using post-editing methods for lexical constraint generation can be taken as an initial step of controlling the generation result.
2020.emnlp-main.701.txt,2020,5 Conclusion & Future Works,"we applied our method in two tasks, keyword-to-sentence generation, and unsupervised paraphrasing."
2020.emnlp-main.701.txt,2020,5 Conclusion & Future Works,we first defined a series of differentiable loss functions which represents the fluency of the generated sentence as well as whether the constraints are satisfied.
2020.emnlp-main.702.txt,2020,5 Conclusion,"further, we show that teaforn can match the prior state-of-the-art rouge-l scores on the summarization benchmarks without beam search, representing an 8x reduction in decoder cost at inference."
2020.emnlp-main.702.txt,2020,5 Conclusion,"in this work, we introduce a new technique for sequence generation models called teacher-forcing with n-grams (teaforn), which (a) addresses exposure bias, (b) allows the decoder to better take into account future decisions, and (c) requires no curriculum training."
2020.emnlp-main.702.txt,2020,5 Conclusion,"overall, teaforn is a promising approach for improving quality and/or reducing inference costs in sequence generation models."
2020.emnlp-main.702.txt,2020,5 Conclusion,we show empirical evidence of the efficacy of teaforn on several sequence generation tasks.
2020.emnlp-main.702.txt,2020,5 Conclusion,"with pegasus large (zhang et al., 2019), we improve upon the existing rouge-l scores the gigaword and cnn/dailymail benchmarks (rush et al., 2015)."
2020.emnlp-main.702.txt,2020,5 Conclusion,"with transformer big (vaswani et al., 2017), we boost the performance of transformerbig significantly on the en-fr benchmark."
2020.emnlp-main.703.txt,2020,7 Conclusions,"as with many who have analyzed the history of nlp, its trends (church, 2007), its maturation toward a science (steedman, 2008), and its major challenges (hirschberg and manning, 2015; mcclelland et al., 2019), we hope to provide momentum for a direction many are already heading."
2020.emnlp-main.703.txt,2020,7 Conclusions,computer vision and speech recognition are mature enough for investigation of broader linguistic contexts (ws3).
2020.emnlp-main.703.txt,2020,7 Conclusions,"our call to action is to encourage the community to lean in to trends prioritizing grounding and agency, and explicitly aim to broaden the corresponding world scopes available to our models."
2020.emnlp-main.703.txt,2020,7 Conclusions,our world scopes are steep steps.
2020.emnlp-main.703.txt,2020,7 Conclusions,simulators and videogames provide potential environments for social language learners (ws5).
2020.emnlp-main.703.txt,2020,7 Conclusions,the robotics industry is rapidly developing commodity hardware and sophisticated software that both facilitate new research and expect to incorporate language technologies (ws4).
2020.emnlp-main.703.txt,2020,7 Conclusions,this is difficult to test under current evaluation paradigms for generalization.
2020.emnlp-main.703.txt,2020,7 Conclusions,"we call for and embrace the incremental, but purposeful, contextualization of language in human experience."
2020.emnlp-main.703.txt,2020,7 Conclusions,what happens if a machine is allowed to participate consistently?
2020.emnlp-main.703.txt,2020,7 Conclusions,"with all that we have learned about what words can tell us and what they keep implicit, now is the time to ask: what tasks, representations, and inductive-biases will fill the gaps?"
2020.emnlp-main.703.txt,2020,7 Conclusions,ws5 implies a persistent agent experiencing time and a personalized set of experiences.confined to iid datasets that lack the structure in time from which humans draw correlations about long-range causal dependencies.
2020.emnlp-main.703.txt,2020,7 Conclusions,"yet, this is the structure of generalization in human development: drawing analogies to episodic memories and gathering new data through non-independent experiments."
2020.emnlp-main.704.txt,2020,6 Conclusion,by releasing our dataset and code we hope to provide a solid foundation to accelerate work in this direction.
2020.emnlp-main.704.txt,2020,6 Conclusion,"even with access to ground truth admissible actions, sparse rewards and partial observability pose daunting challenges for current agents."
2020.emnlp-main.704.txt,2020,6 Conclusion,"from the results in table 2, it is safe to conclude that text-based games are still far from being solved."
2020.emnlp-main.704.txt,2020,6 Conclusion,"in the future, we believe that strong linguistic priors will continue to be a key ingredient for building nextlevel learning agents in these games."
2020.emnlp-main.704.txt,2020,6 Conclusion,"in this paper, we proposed the contextual action language model (calm), a language model approach to generating action candidates for reinforcement learning agents in text-based games."
2020.emnlp-main.704.txt,2020,6 Conclusion,our key insight is to use language models to capture linguistic priors and game sense from humans gameplay on a diverse set of games.
2020.emnlp-main.704.txt,2020,6 Conclusion,"remarkably, on many of these games, our approach is competitive even with models that use ground truth admissible actions, implying that calm is able to generate high-quality actions across diverse games and contexts."
2020.emnlp-main.704.txt,2020,6 Conclusion,"we demonstrated that calm can generate high-quality, contextuallyrelevant actions even for games unseen in its training set, and when paired with a drrn agent, outperforms previous approaches on the jericho benchmark (hausknecht et al., 2019a) by as much as 69% in terms of average normalized score."
2020.emnlp-main.705.txt,2020,7 Conclusion,"our results indicate that measuring caption content by its ability to logically support the answers to typical qa pairs from a target audience is (1) not only feasible, but also (2) a good proxy for uncovering information need."
2020.emnlp-main.705.txt,2020,7 Conclusion,"we defined and studied the capwap task, where question-answer pairs provided by users are used as a source of supervision for learning their visual information needs."
2020.emnlp-main.705.txt,2020,7 Conclusion,we hope this work will motivate the image captioning field to learn to anticipate and provide for the information needs of specific user communities.
2020.emnlp-main.706.txt,2020,6 Conclusion,"to support this task, vlep dataset is collected."
2020.emnlp-main.706.txt,2020,6 Conclusion,"we introduce a new task, video-and-language event prediction (vlep) - given a video with aligned dialogue, and two future events, machines is required to predict which event is more likely to happen."
2020.emnlp-main.706.txt,2020,6 Conclusion,"we present a strong transformer-based baseline that incorporates information from video, dialogue, and commonsense knowledge, each of which is necessary for this challenging task."
2020.emnlp-main.707.txt,2020,8 Conclusion,"we develop a probing mechanism and find that lxmert, a powerful vision-and-language transformer model, is not able to generate meaningful images conditioned on text."
2020.emnlp-main.707.txt,2020,8 Conclusion,"we present x-lxmert, a unified model for image generation, captioning, qa and visual reasoning, and show that our extensions can easily be applied to other vision-and-language transformer models."
2020.emnlp-main.708.txt,2020,6 Conclusion,"along with some intriguing findings, we urge researchers to report sample variance in addition to the metric scores when comparing models’ performance."
2020.emnlp-main.708.txt,2020,6 Conclusion,"we also recommend that when collecting a new dataset, the test set should include more parallel references for fair evaluation, while for the training set, when the text generations are expected to be distinctive and complicated, more parallel references should be collected otherwise a larger variety of visual appearances is more favorable."
2020.emnlp-main.708.txt,2020,6 Conclusion,"we study the sample variance in visually-grounded language generation, in terms of reference sample variance within datasets, effects of training or testing sample variance on metric scores, and the trade-off between the visual instance number and the parallel reference number per visual."
2020.emnlp-main.709.txt,2020,6 Conclusion,"peeking through the lens of a joint embedding model, we probe into learning visual-textual grounding over a more diverse corpus of youtube videos vs. prior work."
2020.emnlp-main.709.txt,2020,6 Conclusion,"we find that learning visual-textual grounding is possible across many yet-to-be-explored categories of youtube videos, and that it’s possible to learn generalizable representations from a more diverse video set."
2020.emnlp-main.71.txt,2020,7 Conclusion,our probing task reveals that the upper layers of bert contextual embeddings best reflect human judgment of semantic similarity.
2020.emnlp-main.71.txt,2020,7 Conclusion,"this work demonstrates the utility of deep contextualized models in linguistic typology, especially for characterizing cross-linguistic semantic phenomena that are otherwise difficult to quantify."
2020.emnlp-main.71.txt,2020,7 Conclusion,"we also find that in english, noun-to-verb flexibility is associated with more semantic shift than verb-to-noun flexibility, but this is not the case for most languages."
2020.emnlp-main.71.txt,2020,7 Conclusion,"we find that the majority class often exhibits more semantic variation than the minority class, supporting the view that word class flexibility is a directional process."
2020.emnlp-main.71.txt,2020,7 Conclusion,we obtain similar results in different datasets and language models in english that support the robustness of our method.
2020.emnlp-main.71.txt,2020,7 Conclusion,we use contextual language models to examine shared tendencies in word class flexibility across languages.
2020.emnlp-main.710.txt,2020,5 Conclusion,"currently, in the fullwiki setting, an off-the-shelf paragraph retriever is adopted for selecting relevant context from large corpus of text."
2020.emnlp-main.710.txt,2020,5 Conclusion,"experiments with detailed analysis demonstrate the effectiveness of our proposed model, which achieves state-of-the-art performances on the hotpotqa benchmark."
2020.emnlp-main.710.txt,2020,5 Conclusion,future work includes investigating the interaction and joint training between hgn and paragraph retriever for performance improvement.
2020.emnlp-main.710.txt,2020,5 Conclusion,"in this paper, we propose a new approach, hierarchical graph network (hgn), for multi-hop question answering."
2020.emnlp-main.710.txt,2020,5 Conclusion,"to capture clues from different granularity levels, our hgn model weaves heterogeneous nodes into a single unified graph."
2020.emnlp-main.711.txt,2020,5 Conclusion,"by operating jointly over these sentences chosen from multiple paragraphs, we arrive at answers and supporting sentences on par with state-of-theart approaches."
2020.emnlp-main.711.txt,2020,5 Conclusion,"our work shows that on the hotpotqa tasks, a simple pipeline model can do as well as or better than more complex solutions, such as graph networks, cross-document attention, or ner."
2020.emnlp-main.711.txt,2020,5 Conclusion,"powerful pre-trained models allow us to score sentences one at a time, without looking at other paragraphs."
2020.emnlp-main.711.txt,2020,5 Conclusion,"this result shows that supporting sentence identification in hotpotqa is itself not a multi-hop problem, and suggests focusing on other multi-hop datasets to demonstrate the value of more complex retrieval techniques."
2020.emnlp-main.712.txt,2020,6 Conclusions,"additionally, for factual reading comprehension datasets where the correct answer can be arrived at without consulting all annotated facts in the input context, our probe will unfairly penalize a model that uses implicitly known facts, even if it correctly connects information across these facts."
2020.emnlp-main.712.txt,2020,6 Conclusions,"extending it to a sequence of facts (e.g., as in multirc (khashabi et al., 2018)) requires accounting for the potential of new artifacts by, for instance, carefully replacing rather than dropping facts."
2020.emnlp-main.712.txt,2020,6 Conclusions,"however, our transformation alleviates this issue: a model that connects information will have an edge in determining the sufficiency of the given context."
2020.emnlp-main.712.txt,2020,6 Conclusions,"it is difficult to create large-scale multihop qa datasets that do not have unintended artifacts, and it is also difficult to design models that do not exploit such shortcuts."
2020.emnlp-main.712.txt,2020,6 Conclusions,it showed that a large portion of current progress in multifact reasoning can be attributed to disconnected reasoning.
2020.emnlp-main.712.txt,2020,6 Conclusions,our probing and transformed dataset construction assumed that the context is an unordered set of facts.
2020.emnlp-main.712.txt,2020,6 Conclusions,our results suggest that carefully devising tests that probe for desirable aspects of multifact reasoning is an effective way forward.
2020.emnlp-main.712.txt,2020,6 Conclusions,progress in multi-hop qa under the reading comprehension setting relies on understanding and quantifying the types of undesirable reasoning current models may perform.
2020.emnlp-main.712.txt,2020,6 Conclusions,"this work introduced a formalization of disconnected reasoning, a form of bad reasoning prevalent in multi-hop models."
2020.emnlp-main.712.txt,2020,6 Conclusions,"using a notion of contrastive sufficiency, it showed how to automatically transform existing support-annotated multi-hop datasets to create a more difficult and less cheatable dataset that results in reduced disconnected reasoning."
2020.emnlp-main.712.txt,2020,6 Conclusions,we leave further exploration to future work.
2020.emnlp-main.713.txt,2020,7 Conclusion,"our approach relies only on the final answer as supervision but works as effectively as state-ofthe-art methods that rely on much stronger supervision, such as supporting fact labels or example decompositions."
2020.emnlp-main.713.txt,2020,7 Conclusion,"overall, this work opens up exciting avenues for leveraging methods in unsupervised learning and natural language generation to improve the interpretability and generalization of machine learning systems."
2020.emnlp-main.713.txt,2020,7 Conclusion,"we found that onus generates fluent sub-questions whose answers often match the gold-annotated, question-relevant text."
2020.emnlp-main.713.txt,2020,7 Conclusion,"we proposed a qa system that answers a question via decomposition, without supervised question decompositions, using three stages: (1) decompose a question into many sub-questions using one-ton unsupervised sequence transduction (onus), (2) answer sub-questions with an off-the-shelf qa system, and (3) recompose sub-answers into a final answer."
2020.emnlp-main.713.txt,2020,7 Conclusion,"when evaluated on three hotpotqa dev sets, our approach significantly improved qa over an equivalent model that did not use decompositions."
2020.emnlp-main.714.txt,2020,7 Conclusion,"in creating the edges and nodes of the graph, we exploit a semantic role labeling sub-graph for each sentence and connect the candidate supporting facts."
2020.emnlp-main.714.txt,2020,7 Conclusion,"moreover, we evaluate the model (excluding the paragraph selection module) on other reading comprehension benchmarks."
2020.emnlp-main.714.txt,2020,7 Conclusion,our approach achieves competitive performance on squad v1.1 and v2.0.
2020.emnlp-main.714.txt,2020,7 Conclusion,srlgrn exceeds most of the sota results on the hotpotqa benchmark.
2020.emnlp-main.714.txt,2020,7 Conclusion,the backbone graph of our proposed graph convolutional network (gcn) is created based on the semantic structure of the sentences.
2020.emnlp-main.714.txt,2020,7 Conclusion,the cross paragraph argument-predicate structure of the sentences expressed in the graph provides an explicit representation of the reasoning path and helps in both finding and explaining the multiple hops of reasoning that lead to the final answer.
2020.emnlp-main.714.txt,2020,7 Conclusion,we proposed a novel semantic role labeling graph reasoning network (srlgrn) to deal with multihop qa.
2020.emnlp-main.715.txt,2020,9 Conclusion and Future Work,a deeper understanding of emotion causes can potentially help make people feel better.
2020.emnlp-main.715.txt,2020,9 Conclusion and Future Work,"composed of 8, 500 sentences that convey at least one emotion, and 16, 500 sentences that convey no emotion at all, canceremo is a challenging benchmark for fine-grained emotion detection, as shown by our results."
2020.emnlp-main.715.txt,2020,9 Conclusion and Future Work,"finally, we will carry out a thorough investigation into emotion-cause pairs (xia and ding, 2019)."
2020.emnlp-main.715.txt,2020,9 Conclusion and Future Work,"in the future, we plan to study how contextual information (i.e., different aspects of people’s interactions captured through contiguous posts in a discussion thread) affects the perceived emotions."
2020.emnlp-main.715.txt,2020,9 Conclusion and Future Work,"our dataset, which is anonymized and follows ethical considerations, can be used as a benchmark for both multi-class and multi-label emotion detection."
2020.emnlp-main.715.txt,2020,9 Conclusion and Future Work,"specifically, in the health domain, the cause that leads to an emotion expressed in text can be just as important as the emotion itself."
2020.emnlp-main.715.txt,2020,9 Conclusion and Future Work,the value of our dataset arises also from: the expressions of emotions even in the absence of emotion words and the expressions of mixtures of (sometimes opposing) emotions in the same text.
2020.emnlp-main.715.txt,2020,9 Conclusion and Future Work,we also plan to perform a cross-corpus analysis to investigate if emotions are expressed differently in the health domain compared to other domains.
2020.emnlp-main.715.txt,2020,9 Conclusion and Future Work,we believe that canceremo is novel and has unique characteristics: 1) covers a large spectrum of emotions - being annotated with the plutchik-8 fine-grained emotions; 2) has a large dataset size for exploring deep learning models; and 3) provides an invaluable context - cancer - for dealing with emotions.
2020.emnlp-main.715.txt,2020,9 Conclusion and Future Work,"we believe that these characteristics add interestingness and challenges to our dataset and we hope that our work will spur future research in emotion detection from health data, especially in the context of life-threatening diseases such as cancer."
2020.emnlp-main.715.txt,2020,9 Conclusion and Future Work,"we introduced canceremo , a cancer-related health dataset for perceived emotion detection, which is an order of magnitude larger and more fine-grained compared with previous datasets for health-related emotion detection."
2020.emnlp-main.716.txt,2020,6 Conclusion,"in this work, we explore the role of argument structure in online debate persuasion and find that incorporating argument structure features along with the linguistic features achieves the best predictive performance models."
2020.emnlp-main.716.txt,2020,6 Conclusion,"moreover, we observe that argument structure features provide important cues about effective persuasion strategies in online debates."
2020.emnlp-main.717.txt,2020,6 Conclusion,"in addition, extensive analysis shows our model provides substantial improvement on a number of challenging phenomena (e.g., sarcasm) and is less reliant on sentiment cues that tend to mislead the models."
2020.emnlp-main.717.txt,2020,6 Conclusion,"in addition, we will study more explicitly how to decouple stance models from sentiment, and how to improve performance further on difficult phenomena."
2020.emnlp-main.717.txt,2020,6 Conclusion,"in future work we plan to investigate additional methods to represent and use generalized topic information, such as topic modeling."
2020.emnlp-main.717.txt,2020,6 Conclusion,"our models are evaluated on a new dataset, vast, that has a large number of topics with wide linguistic variation and that we create and make available."
2020.emnlp-main.717.txt,2020,6 Conclusion,"we find that our model tga net, which uses generalized topic representations to implicitly capture relationships between topics, performs significantly better than bert for stance detection on pro labels, and performs similarly on other labels."
2020.emnlp-main.718.txt,2020,6 Conclusion,"from various experimental observations, it is evident that sentiment-oriented node expansion can reduce under-specificity, noise in a tweet, and enhance the representation."
2020.emnlp-main.718.txt,2020,6 Conclusion,our proposed centrality aware random-walk method can generate walk sequences that capture better semantic relations than its unbiased and biased random walk based counterparts.
2020.emnlp-main.718.txt,2020,6 Conclusion,the proposed method outperforms its text-based counterpart in a majority of the cases.
2020.emnlp-main.718.txt,2020,6 Conclusion,this study investigates the efficacy of transforming tweets to heterogeneous multi-layer network for the sentiment classification task.
2020.emnlp-main.719.txt,2020,5 Conclusion,comprehensive analysis is done to demonstrate the effectiveness of the proposed model over four datasets.
2020.emnlp-main.719.txt,2020,5 Conclusion,"two types of syntactic information are introduced in this work, i.e., the syntax-based possibility scores for words (integrated with the on-lstm model) and the syntactic connections between the words (applied with the gcn model with novel adjacency matrices)."
2020.emnlp-main.719.txt,2020,5 Conclusion,"we also present a novel inductive bias to improve the model, leveraging the representation distinction between the words in towe."
2020.emnlp-main.719.txt,2020,5 Conclusion,we propose a novel deep learning model for towe that seeks to incorporate the syntactic structures of the sentences into the model computation.
2020.emnlp-main.72.txt,2020,8 Conclusions,"also, we have developed a shallow-to-deep training strategy and employ sparse connections across blocks to ease the optimization."
2020.emnlp-main.72.txt,2020,8 Conclusions,"furthermore, our sdt-rpr-24l (big) achieves a bleu score of 30.46 on wmt’16 english-german task, and speeds up the training by 1.5×."
2020.emnlp-main.72.txt,2020,8 Conclusions,higher layers share more global information over different positions and adjacent layers behave similarly.
2020.emnlp-main.72.txt,2020,8 Conclusions,we have investigated the behaviour of the welltrained deep transformer models and found that stacking more layers could improve the representation ability of nmt systems.
2020.emnlp-main.72.txt,2020,8 Conclusions,with the help of learning rate restart and appropriate initialization we successfully train a 48-layer rpr model by progressive stacking and achieve a 40% speedup on both wmt’16 english-german and wmt’14 englishfrench tasks.
2020.emnlp-main.720.txt,2020,5 Conclusion,an important avenue of future work will be to assess to what extent there may be cultural differences in these associations (see discussion in section 3.1).
2020.emnlp-main.720.txt,2020,5 Conclusion,"as emoji use is now ubiquitous on mobile devices and social media, we believe that ultimately any nlp task involving social media text may benefit from such emoji resources."
2020.emnlp-main.720.txt,2020,5 Conclusion,"finally, we rely on our annotated data to study how well we can automatically estimate emotional association ratings for a given emoji, considering a series of different baseline methods and resources."
2020.emnlp-main.720.txt,2020,5 Conclusion,"from each of 9 human raters, we solicit 1,200 ratings covering a set of 150 emojis with regard to 8 core emotions from plutchik (1980)’s wheel of emotions."
2020.emnlp-main.720.txt,2020,5 Conclusion,"hence, we are able to predict high-quality emotion scores for a larger set of emojis."
2020.emnlp-main.720.txt,2020,5 Conclusion,"however, this connection has not been studied in sufficient detail, at the level of individual emojis."
2020.emnlp-main.720.txt,2020,5 Conclusion,"in this work, we shed light on this connection by compiling the emotag1200 dataset, which quantifies people’s reported association between emojis and emotion."
2020.emnlp-main.720.txt,2020,5 Conclusion,our findings suggest that data-driven methods can fare quite well at this if combined with high-quality affective intensity information at the lexical level.
2020.emnlp-main.720.txt,2020,5 Conclusion,"similarly, variation with respect to age and other variables merits further study as well."
2020.emnlp-main.720.txt,2020,5 Conclusion,"temporal aspects could be considered in diachronic studies, to account for the fact that emoji use has been evolving."
2020.emnlp-main.720.txt,2020,5 Conclusion,the desire to express an emotion is one of the factors that has driven the tremendous proliferation of emojis in interpersonal communication.
2020.emnlp-main.720.txt,2020,5 Conclusion,"the most obvious use cases are sentiment analysis (dong and de melo, 2018), emotion analysis (raji and de melo, 2020), consumer behaviour analytics (dong et al., 2020), context-sensitive emoji recommendation (felbo et al., 2017), computational social science and public opinion mining (wang et al., 2018; du et al., 2020), and user modeling (guo et al., 2018), but it may also be useful in dialogue systems (delobelle and berendt, 2019), e.g.to detect sarcasm."
2020.emnlp-main.720.txt,2020,5 Conclusion,"this constitutes the first resource of this kind, which we thoroughly analyze and make freely available to enable further research."
2020.emnlp-main.720.txt,2020,5 Conclusion,this opens up further research avenues on possible downstream applications exploiting this knowledge.
2020.emnlp-main.721.txt,2020,6 Conclusion,"also, stochasticity was applied to the emotion mixture for varied response generation."
2020.emnlp-main.721.txt,2020,6 Conclusion,"however, there remains much room for improvement, particularly in terms of fluency where our model falters."
2020.emnlp-main.721.txt,2020,6 Conclusion,"moreover, emotions like ‘surprise’ and ‘anticipation’ might be explicitly dealt with due to their ambiguous polarity."
2020.emnlp-main.721.txt,2020,6 Conclusion,this paper introduced a novel empathetic generation strategy that relies on two key ideas: emotion grouping and emotion mimicry.
2020.emnlp-main.721.txt,2020,6 Conclusion,we have shown through several human evaluations and ablation studies that our model is better equipped for empathetic response generation than existing models.
2020.emnlp-main.722.txt,2020,6 Conclusion,"building on entity-level mlms, we propose an entity masking scheme under kg’s guidance."
2020.emnlp-main.722.txt,2020,6 Conclusion,experiments show finetuning our kg-guided pre-trained mlms yields improved performance on related downstream tasks.
2020.emnlp-main.722.txt,2020,6 Conclusion,"in the future, instead of pre-training on sentences, we will leverage raw text at passage or document level to alleviate the performance degeneration brought by short context."
2020.emnlp-main.722.txt,2020,6 Conclusion,"in this work, we aim at equipping pre-trained lms with structured knowledge via self-supervised tasks."
2020.emnlp-main.722.txt,2020,6 Conclusion,it masks informative mentions and facilitates learning structured knowledge in free-form text.
2020.emnlp-main.722.txt,2020,6 Conclusion,"moreover, we propose a distractor-suppressed ranking objective to utilize negative samples from kg as distractors for effective training."
2020.emnlp-main.722.txt,2020,6 Conclusion,"moreover, we will use a combination of commonsense and ontological kgs, and large-scale corpora (e.g., common crawl) to pre-train an mlm from scratch, which we expect to benefit a wide range of tasks."
2020.emnlp-main.723.txt,2020,6 Conclusion,"the proposed approach yields, to the best of our knowledge, first fully unsupervised ne recognition work on these two benchmark datasets without any annotation data or extra knowledge base."
2020.emnlp-main.723.txt,2020,6 Conclusion,this paper presents an ne recognition model with only pre-trained word embeddings and achieves remarkable results on conll 2003 english and conll 2002 spanish benchmark datasets.
2020.emnlp-main.724.txt,2020,6 Conclusions,"in this paper, we propose the lotclass model built upon pre-trained neural lms for text classification with label names as the only supervision in three steps: category understanding via label name replacement, word-level classification via masked category prediction, and self-training on unlabeled corpus for generalization."
2020.emnlp-main.724.txt,2020,6 Conclusions,the effectiveness of lotclass is validated on four benchmark datasets.
2020.emnlp-main.724.txt,2020,6 Conclusions,we also point out several directions for future work by generalizing our methods to other tasks or combining with other techniques.
2020.emnlp-main.724.txt,2020,6 Conclusions,we show that label names is an effective supervision type for text classification but has been largely overlooked by the mainstreams of literature.
2020.emnlp-main.725.txt,2020,5 Conclusion,"in the future, we plan to extend our model to cope with external word or document semantics."
2020.emnlp-main.725.txt,2020,5 Conclusion,it would also be interesting to explore alternative architectures other than cyclegan under our formulation of topic modeling.
2020.emnlp-main.725.txt,2020,5 Conclusion,the effectiveness of tomcat and stomcat is verified by experiments on topic modeling and text classification.
2020.emnlp-main.725.txt,2020,5 Conclusion,tomcat employs a generator to capture semantic patterns in topics and an encoder to encode documents into their corresponding topics.stomcat further incorporates document labels into topic modeling.
2020.emnlp-main.725.txt,2020,5 Conclusion,"we have presented tomcat, a neural topic model with adversarial and cycle-consistent objectives, and its supervised extension, stomcat."
2020.emnlp-main.726.txt,2020,8 Conclusion,"by leveraging an off-the-shelf language model (gpt-2), we successfully guide the generation towards a specified direction (i.e, target class), with the help of reinforcement learning."
2020.emnlp-main.726.txt,2020,8 Conclusion,"in the future, we plan to implement a more sophisticated guidance for the augmentation by adding syntactic and position features to the reward function, to enable augmentation of more diverse types of text data."
2020.emnlp-main.726.txt,2020,8 Conclusion,the code will be made available upon request.
2020.emnlp-main.726.txt,2020,8 Conclusion,"we find that data boost improves the performance of classification tasks, is classifier-agnostic, and that it surpasses several prior augmentation methods in three diverse classification tasks."
2020.emnlp-main.726.txt,2020,8 Conclusion,we have proposed a powerful and easy to deploy approach to augment text data through conditional generation.
2020.emnlp-main.727.txt,2020,5 Conclusion and Future Work,"in this paper, we first introduce the kleinberg algorithm to identify the propagation states for an event composed of a sequence of posts and segment the sequence into several state-independent sub-events."
2020.emnlp-main.727.txt,2020,5 Conclusion and Future Work,it would be even better if the state detection and segmentation step can be integrated with subsequent state-independent feature extraction and rumor detection in an end-to-end framework.
2020.emnlp-main.727.txt,2020,5 Conclusion and Future Work,"on this basis, we propose a state-independent and time-evolving network (stn) for rumor detection as well as early rumor detection."
2020.emnlp-main.727.txt,2020,5 Conclusion and Future Work,one disadvantage of this work is that the kleinberg algorithm is performed on the combination of all events in the dataset to maintain global states.
2020.emnlp-main.727.txt,2020,5 Conclusion and Future Work,"secondly, it is a retrospective algorithm which depends on the condition all posts along the timeline should be provided in advance."
2020.emnlp-main.727.txt,2020,5 Conclusion and Future Work,"the experimental results on two real-world microblog rumor datasets demonstrates the advantages of our stn approach in terms of both rumor detection accuracy and our proposed ts-acc metric, in comparison with some strong rumor detection systems."
2020.emnlp-main.727.txt,2020,5 Conclusion and Future Work,"therefore, one direction for future work is to explore an online state detection algorithm and perform it for each event, but at the same time ensure that the state of each event is globally defined."
2020.emnlp-main.727.txt,2020,5 Conclusion and Future Work,this way may fail to capture the individual state transition in single events.
2020.emnlp-main.727.txt,2020,5 Conclusion and Future Work,we also present a new metric called time-series smoothing accuracy (ts-acc) for measuring the efficiency of early rumor detection.
2020.emnlp-main.728.txt,2020,5 Conclusion,focusing on two modeling tasks – predicting python methods from docstrings and summarizing python source code methods into docstrings of various commonly occurring styles – we have compared this new approach to the auto-regressive gpt2 baselines trained on individual docstring or method generation tasks.
2020.emnlp-main.728.txt,2020,5 Conclusion,"further, we introduced control token prefixes for docstring generation to facilitate docstring generation of various styles."
2020.emnlp-main.728.txt,2020,5 Conclusion,"in this work, we presented a novel multi-mode python method text-to-text transfer transformer model pymt5as well as the largest parallel corpus of python source code and docstrings reported in the literature to date."
2020.emnlp-main.728.txt,2020,5 Conclusion,"looking forward, we plan to leverage pymt5 for various downstream automated software engineering tasks—including code documentation and method generation from natural language statements—and develop more model evaluation criteria to leverage the unique properties of source codes."
2020.emnlp-main.728.txt,2020,5 Conclusion,"on the codesearchnet test set pymt5 achieves a bleu score of 8.59 for method generation and 16.3 for docstring generation, and a rouge-l f-score of 24.8 for method generation and 36.7 for docstring generation."
2020.emnlp-main.728.txt,2020,5 Conclusion,"we have demonstrated the effectiveness of dynamic masked pre-training, reducing docstring generation training time by 25×."
2020.emnlp-main.728.txt,2020,5 Conclusion,"we have trained pymt5 to translate between all pairs of combinations of method signatures, docstrings, and method bodies which do not have the same feature in both the source and target."
2020.emnlp-main.729.txt,2020,7 Conclusion and Future Work,experimental results proves the effectiveness of our proposed model qualitatively and quantitatively.
2020.emnlp-main.729.txt,2020,7 Conclusion and Future Work,"first, we would like to explore more explainable reasoning method for question generation, such as symbolic-based models."
2020.emnlp-main.729.txt,2020,7 Conclusion and Future Work,"in the future, there can be two research directions."
2020.emnlp-main.729.txt,2020,7 Conclusion and Future Work,"in this paper, we propose to model facts in the input text as knowledge graph for question generation."
2020.emnlp-main.729.txt,2020,7 Conclusion and Future Work,"second, novel evaluation metrics for question generation taking consistency and informativeness into consideration would be of interest."
2020.emnlp-main.729.txt,2020,7 Conclusion and Future Work,we extend the squad dataset by automatic constructing kg for each input sentence and identifying corresponding query paths for ground truth questions.
2020.emnlp-main.729.txt,2020,7 Conclusion and Future Work,we present a novel task of generating a question based on a query path from the constructed kg.
2020.emnlp-main.729.txt,2020,7 Conclusion and Future Work,we propose to learn query representation for question generation in a joint model and a variational inference model is also proposed.
2020.emnlp-main.73.txt,2020,8 Conclusion,"as we avoid discrete search over a large vocabulary, our inference procedure is more efficient than previous inference procedures that refine in the token space."
2020.emnlp-main.73.txt,2020,8 Conclusion,"given a latent variable model for machine translation, we train an inference network to approximate the gradient of the marginal log probability with respect to the target sentence, using only the latent variable."
2020.emnlp-main.73.txt,2020,8 Conclusion,this allows us to use gradient based optimization to find a target sentence at inference time that approximately maximizes the marginal log probability.
2020.emnlp-main.73.txt,2020,8 Conclusion,"this will be particularly interesting, as recent study showed latent variable models with a flexible prior give high test loglikelihoods, but suffer from poor generation quality as inference is challenging (lee et al., 2020)."
2020.emnlp-main.73.txt,2020,8 Conclusion,"we compare our approach with a recently proposed delta inference procedure that optimizes jointly in discrete and continuous space on three machine translation datasets: wmt’14 en→de, wmt’16 ro→en and iwslt’16 de→en."
2020.emnlp-main.73.txt,2020,8 Conclusion,we propose an efficient inference procedure for non-autoregressive machine translation that refines translations purely in the continuous space.
2020.emnlp-main.73.txt,2020,8 Conclusion,"while we showed that iterative inference with a learned score function is effective for spherical gaussian priors, more work is required to investigate if such an approach will also be successful for more sophisticated priors, such as gaussian mixtures or normalizing flows."
2020.emnlp-main.73.txt,2020,8 Conclusion,"with the same underlying latent variable model, the proposed inference procedure using a learned score function has following advantages: (1) it is twice as fast as delta inference, and (2) it is able to find target sentences resulting in higher marginal probabilities and bleu scores."
2020.emnlp-main.730.txt,2020,7 Conclusions and Future Work,full time annotations of novels additionally include the challenging task of distinguishing between narrator and recall time in discussing past events.
2020.emnlp-main.730.txt,2020,7 Conclusions and Future Work,future work includes applying time inference models to question answering and other nlp systems.
2020.emnlp-main.730.txt,2020,7 Conclusions and Future Work,more annotations of complete novels would permit better models and evaluation.
2020.emnlp-main.730.txt,2020,7 Conclusions and Future Work,"our models are a good start, but we release the dataset to encourage others to improve on this task."
2020.emnlp-main.730.txt,2020,7 Conclusions and Future Work,we also seek to annotate information about dates and seasons.
2020.emnlp-main.730.txt,2020,7 Conclusions and Future Work,we have constructed a dataset of time phrases to build models that can predict the most relevant hour of the day for a given text window.
2020.emnlp-main.730.txt,2020,7 Conclusions and Future Work,we note that this dataset can be further cleaned by resolving ocr errors in the source text as well as improving upon the time extraction algorithm.
2020.emnlp-main.731.txt,2020,8 Conclusion,"for lexical generalization cases, the rnn-based model from gordon et al.(2020) that implements permutation equivariance may help, considering that it was able to solve all primitive generalizations in scan."
2020.emnlp-main.731.txt,2020,8 Conclusion,"for structural generalization cases, the results of bowman et al.(2015); evans et al.(2018) and mccoy et al.(2019) suggest that treestructured models may provide a better inductive bias."
2020.emnlp-main.731.txt,2020,8 Conclusion,"furthermore, the models found structural generalization much more challenging compared to lexical generalization."
2020.emnlp-main.731.txt,2020,8 Conclusion,"in particular, bowman et al.(2015) showed that tree-structured neural networks generalized to longer sequences."
2020.emnlp-main.731.txt,2020,8 Conclusion,"our results suggest that achieving high generalization accuracy on cogs is beyond the capacity of models that we tested, and cogs can therefore motivate the development of new computational models."
2020.emnlp-main.731.txt,2020,8 Conclusion,"we have proposed cogs, a challenge set for compositional generalization, which uses a synthetic sentence-to-logical-form mapping task that approximates meaning interpretation in english."
2020.emnlp-main.731.txt,2020,8 Conclusion,what architecture would be needed to solve cogs?
2020.emnlp-main.731.txt,2020,8 Conclusion,"when tested on cogs, both transformers and lstms performed poorly on the generalization set, with high variability across runs, while their performance on the in-domain test set was consistently near-perfect."
2020.emnlp-main.732.txt,2020,7 Conclusions,"additionally, one can ignore negation and still make the correct inference judgment with many text-hypothesis pairs in existing natural language inference benchmarks (rte, snli and mnli)."
2020.emnlp-main.732.txt,2020,7 Conclusions,"despite these facts, negation is underrepresented in some natural language inference benchmarks (rte and snli)."
2020.emnlp-main.732.txt,2020,7 Conclusions,"in addition, our experimental results show that transformers struggle even after fine-tuning with new pairs containing negation."
2020.emnlp-main.732.txt,2020,7 Conclusions,"in this paper, we have presented a new benchmark of text-hypothesis pairs containing negation (4,500 pairs)."
2020.emnlp-main.732.txt,2020,7 Conclusions,"negation is ubiquitous in english and critical to understand language and make inferences, as it denies or inverts meaning."
2020.emnlp-main.732.txt,2020,7 Conclusions,"state-of-the art transformers trained with the original training splits from rte, snli and mnli obtain much worse results results with the new benchmark than with the original pairs—including the few original texthypothesis pairs that do contain negation."
2020.emnlp-main.732.txt,2020,7 Conclusions,"we generate and annotate these pairs after systematically adding negation to the main verb of the texts and hypotheses—either one or both—from rte, snli and mnli thus they are as difficult to solve as the original pairs except for the presence of negation."
2020.emnlp-main.733.txt,2020,5 Conclusion and Future Work,"in the future, we are looking forward to diving in representation learning with flow-based generative models from a broader perspective."
2020.emnlp-main.733.txt,2020,5 Conclusion and Future Work,"in this paper, we investigate the deficiency of the bert sentence embeddings on semantic textual similarity, and propose a flow-based calibration which can effectively improve the performance."
2020.emnlp-main.734.txt,2020,6 Discussion & Conclusion,"in conclusion, we hope our data and analysis inspire future directions such as explicit modeling of collective human opinions; providing theoretical supports for the connection between human disagreement and the difficulty of acquiring language understanding in general; exploring potential usage of these human agreements; and studying the source of the human disagreements and its relations to different linguistic phenomena."
2020.emnlp-main.734.txt,2020,6 Discussion & Conclusion,it is also important to note that the level of human agreement is an intrinsic property of a data point.
2020.emnlp-main.734.txt,2020,6 Discussion & Conclusion,section 5.3 shows that such ability is missing from current models and potential room for improvement is huge.
2020.emnlp-main.734.txt,2020,6 Discussion & Conclusion,section 5.4 demonstrates that such a property can be an indicator of the difficulty of the modeling.
2020.emnlp-main.734.txt,2020,6 Discussion & Conclusion,"this hints at the connections between human agreements and uncertainty estimation or calibration (guo et al., 2017) where machine learning models are required to produce the confidence value of their predictions, leading to important benefits in real-world applications."
2020.emnlp-main.734.txt,2020,6 Discussion & Conclusion,"this not only complements prior evaluations by helping researchers understand whether model performance on a specific data point is reliable based on its human agreement, but also makes it possible to evaluate models’ ability to capture the whole picture of human opinions."
2020.emnlp-main.734.txt,2020,6 Discussion & Conclusion,"to address this concern, we suggest nlp models be evaluated against the collective human opinion distribution rather than one opinion aggregated from a set of opinions, especially on tasks which take a descriptivist approach10 to language and meaning, including nli and common sense reasoning."
2020.emnlp-main.734.txt,2020,6 Discussion & Conclusion,"while common practice in natural language evaluation compares the model prediction to the majority label, section 5.4 questions the value of continuing such evaluation on current benchmarks as most of the unsolved examples are of low human agreement."
2020.emnlp-main.735.txt,2020,5 Conclusions,"experiments on neural machine translation, text summarization, and text generation have demonstrated the effectiveness of our sfot algorithm, yielding improved performance over strong baselines on these tasks."
2020.emnlp-main.735.txt,2020,5 Conclusions,the proposed model captures positional and contextual information of word tokens in ot matching.
2020.emnlp-main.735.txt,2020,5 Conclusions,we have introduced sfot to mitigate exposure bias in text generation.
2020.emnlp-main.736.txt,2020,5 Conclusion,"as future work, we will explore the similar idea of designing unreferenced metrics for dialog generation."
2020.emnlp-main.736.txt,2020,5 Conclusion,"extensive experiments show that union outperforms stateof-the-art metrics in terms of correlation with human judgments on two story datasets, and is more robust to dataset drift and quality drift."
2020.emnlp-main.736.txt,2020,5 Conclusion,results also show the effectiveness of the proposed four negative sampling techniques.
2020.emnlp-main.736.txt,2020,5 Conclusion,union is trained to distinguish human-written stories from auto-constructed negative samples and to recover the perturbation in negative samples.
2020.emnlp-main.736.txt,2020,5 Conclusion,"we present union, an unreferenced metric for evaluating open-ended story generation."
2020.emnlp-main.737.txt,2020,5 Conclusion,f2-softmax encourages models to diversify text generation by readjusting class formation and motivating models to learn a more balanced token distribution.
2020.emnlp-main.737.txt,2020,5 Conclusion,"in this paper, we proposed f2-softmax, a simple but effective method for better learning the rich diversity in text."
2020.emnlp-main.737.txt,2020,5 Conclusion,quantitative and qualitative analyses validate the diversity-promoting performances of our approach.
2020.emnlp-main.737.txt,2020,5 Conclusion,"since it can be quickly adopted to replace the traditional likelihood objective, we believe in broader applicability of f2-softmax."
2020.emnlp-main.737.txt,2020,5 Conclusion,"thus, future work involves extending the method to other related tasks, such as machine translation and text summarization, and investigating the potential gains from transfer learning."
2020.emnlp-main.738.txt,2020,6 Conclusions,"in this work, we propose a new task, namely, partially-aligned data-to-text generation, in which we generate human-readable text based on automatically produced training data."
2020.emnlp-main.738.txt,2020,6 Conclusions,"moreover, we contribute a partially-aligned dataset wita produced by our novel automatically annotating framework which is suitable for this new task."
2020.emnlp-main.738.txt,2020,6 Conclusions,the experimental results show that our proposed model solves the overgeneration problem effectively and outperforms all baseline models.
2020.emnlp-main.738.txt,2020,6 Conclusions,this task is more practical and extensible to any domains.
2020.emnlp-main.738.txt,2020,6 Conclusions,we propose a distant supervision generation framework that tackling the task.
2020.emnlp-main.739.txt,2020,7 Conclusion,"as future work, we would like extend the prior network to sample more than one persona sentences by expanding the sample space of the discrete random variable to generate more interesting responses."
2020.emnlp-main.739.txt,2020,7 Conclusion,"in this work, we showed that expanding persona sentences with commonsense helps a dialog model to generate high-quality and diverse personagrounded responses."
2020.emnlp-main.739.txt,2020,7 Conclusion,"moreover, we found that finegrained persona grounding is crucial to effectively condition on a large pool of commonsense persona expansions, which further provided additional controllability in conditional generation."
2020.emnlp-main.739.txt,2020,7 Conclusion,"while our expansions are limited by the performance of comet or paraphrase systems, we envision future work to train the dialog model endto-end along with the expansion generation."
2020.emnlp-main.74.txt,2020,5 Conclusion and Future Work,"in our future work, i) we are interested in distilling from deep nmt models into extremely small students with ckd, in the hope of achieving the same results of large models with much smaller counterparts.ii) we also try to improve the combination module and find a better alternative than concatenation.iii) finally, we plan to evaluate ckd in other tasks such as language modeling."
2020.emnlp-main.74.txt,2020,5 Conclusion and Future Work,"in this paper, we proposed a novel model to distill from intermediate layers as well as final predictions."
2020.emnlp-main.74.txt,2020,5 Conclusion and Future Work,"moreover, we addressed the skip problem of pkd."
2020.emnlp-main.74.txt,2020,5 Conclusion and Future Work,we applied our technique in nmt and showed its potential in training high-quality and compact models.
2020.emnlp-main.740.txt,2020,7 Conclusion and Future Work,"first, the labes model is general and can be enhanced by, e.g.incorporating largescale pre-trained language models, allowing other options for the belief state decoder and the response decoder such as transformer based."
2020.emnlp-main.740.txt,2020,7 Conclusion and Future Work,"furthermore, we develop labes-s2s, which is a copy-augmented seq2seq model instantiation of labes."
2020.emnlp-main.740.txt,2020,7 Conclusion and Future Work,"in our experiments on multiwoz, we can save around 50%, i.e.around 30,000 belief state annotations without performance loss."
2020.emnlp-main.740.txt,2020,7 Conclusion and Future Work,in this paper we are interested in reducing belief state annotation cost for building e2e task-oriented dialog systems.
2020.emnlp-main.740.txt,2020,7 Conclusion and Future Work,"second, we can analogously introduce dialog acts a1:t as latent variables to define the joint distribution pθ(b1:t , a1:t , r1:t |u1:t ), which can be trained with semi-supervised learning and reinforcement learning as well."
2020.emnlp-main.740.txt,2020,7 Conclusion and Future Work,there are some interesting directions for future work.
2020.emnlp-main.740.txt,2020,7 Conclusion and Future Work,"we propose a conditional generative model of dialogs - labes, where belief states are modeled as latent variables, and unlabeled dialog data can be effectively leveraged to improve belief tracking through semi-supervised variational learning."
2020.emnlp-main.740.txt,2020,7 Conclusion and Future Work,we show the strong benchmark performance of labes-s2s and the effectiveness of our semi-supervised learning method on three benchmark datasets.
2020.emnlp-main.741.txt,2020,7 Conclusions,experimental results on three benchmark datasets and four state-of-the-art models demonstrated the effectiveness of the proposed training approach.
2020.emnlp-main.741.txt,2020,7 Conclusions,it automatically constructs different types of grayscale data and uses a multi-level ranking objective.
2020.emnlp-main.741.txt,2020,7 Conclusions,the proposed approach can teach a matching model to capture fine-grained quality differences better and reduce the train-test discrepancy in distractor strength.
2020.emnlp-main.741.txt,2020,7 Conclusions,we presented a novel approach for training response selection models for multi-turn conversations.
2020.emnlp-main.742.txt,2020,5 Conclusion and Discussion,a limitation of grade is the inconsistency between the training objective (relative ranking) and the expected behavior (absolute scoring).
2020.emnlp-main.742.txt,2020,5 Conclusion and Discussion,"besides, we also release a new large-scale human evaluation bench-mark to facilitate future research on automatic metrics."
2020.emnlp-main.742.txt,2020,5 Conclusion and Discussion,empirical results show that grade has stronger correlations with human judgements and can generalize to other unseen chit-chat datasets.
2020.emnlp-main.742.txt,2020,5 Conclusion and Discussion,"in this paper, we proposed grade (graphenhanced representations for automatic dialogue evaluation), a novel metric for dialogue coherence evaluation of open-domain dialogue systems."
2020.emnlp-main.742.txt,2020,5 Conclusion and Discussion,"overall, to develop a dialogue metric that can quantify in a more human-like manner, it is critical to reducing the gap between the training objective and the model behavior we truly care about."
2020.emnlp-main.742.txt,2020,5 Conclusion and Discussion,"specifically, the ranking loss we adopted only requires good responses to be ranked higher than bad responses, which is a relatively loose constraint compared with the absolute scoring that humans do."
2020.emnlp-main.742.txt,2020,5 Conclusion and Discussion,"therefore, grade may deviate from the human scoring criterion and fail to quantify the dialogue responses accurately, and that the human correlation results fluctuate over different runs."
2020.emnlp-main.743.txt,2020,6 Conclusions and Future Works,"for future work, we will annotate medical entities in our datasets."
2020.emnlp-main.743.txt,2020,6 Conclusions and Future Works,"on a covid19 dialogue generation task where the dataset is small, human evaluation and automatic evaluation show that models pretrained on meddialog-cn can effectively improve the quality of generated responses."
2020.emnlp-main.743.txt,2020,6 Conclusions and Future Works,such annotations can facilitate the development of goal-oriented medical dialog systems.
2020.emnlp-main.743.txt,2020,6 Conclusions and Future Works,the results show that the generated dialogues by these pretrained models are clinically meaningful and human-like.
2020.emnlp-main.743.txt,2020,6 Conclusions and Future Works,"to facilitate the research and development of medical dialogue systems that can potentially assist in telemedicine, we build large-scale medical dialogue datasets – meddialog – which contain 1) a chinese dataset with 3.4 million conversations between patients and doctors, 11.3 million utterances, 660.2 million tokens, covering 172 specialties of diseases, and 2) an english dataset with 0.26 million conversations, 0.51 million utterances, 44.53 million tokens, covering 96 specialties of diseases."
2020.emnlp-main.743.txt,2020,6 Conclusions and Future Works,"to our best knowledge, they the largest of their kind."
2020.emnlp-main.743.txt,2020,6 Conclusions and Future Works,"we pretrain transformer, gpt, and bert-gpt on meddialog-cn."
2020.emnlp-main.743.txt,2020,6 Conclusions and Future Works,we use transfer learning to apply these pretrained models for low-resource dialogue generation.
2020.emnlp-main.744.txt,2020,5 Conclusion,"we show both information-theoretically and empirically, that controlling the targets and representations are equivalent, as long as the control mechanism is randomized."
2020.emnlp-main.744.txt,2020,5 Conclusion,"when selecting probes that better approximate i(t; r), we recommend measuring with a control mechanism instead of relying on the traditional cross entropy on probing task."
2020.emnlp-main.745.txt,2020,7 Conclusion,"for comparisons based on small samples, there is little reason to think that such an evaluation could reliably provide evidence of a significant improvement, and good reason to believe that improvements found to be significant will exaggerate or reverse the true effect."
2020.emnlp-main.745.txt,2020,7 Conclusion,"going forward, a combination of larger test sets, simple power analyses, and wider sharing of code, data, and experimental details will help to build the foundation for a higher standard of experimental methodology in nlp."
2020.emnlp-main.745.txt,2020,7 Conclusion,"in this paper, we have presented evidence that underpowered experiments are widespread in nlp."
2020.emnlp-main.745.txt,2020,7 Conclusion,"recent progress in nlp has been extraordinarily rapid, sometimes at the cost of experimental rigor."
2020.emnlp-main.746.txt,2020,8 Conclusion,"data maps not only help diagnose and make better use of existing datasets, but also hold potential for guiding the construction of new datasets."
2020.emnlp-main.746.txt,2020,8 Conclusion,"moreover, data maps could facilitate comparison of different model architectures trained on a given dataset, resulting in alternative evaluation methodologies."
2020.emnlp-main.746.txt,2020,8 Conclusion,"our data maps for four different datasets reveal similar terrains in each dataset: groups of ambiguous instances useful for high performance, easy-to-learn instances which aid optimization, and hard-to-learn instances which often correspond to data errors."
2020.emnlp-main.746.txt,2020,8 Conclusion,our implementation is publicly available to facilitate such efforts.
2020.emnlp-main.746.txt,2020,8 Conclusion,our work shows the effectiveness of simple training dynamics measures based on mean and standard deviation; exploration of more sophisticated measures to build data maps is an exciting future direction.
2020.emnlp-main.746.txt,2020,8 Conclusion,we presented data maps: an automatic method to visualize and diagnose large datasets using training dynamics.
2020.emnlp-main.746.txt,2020,8 Conclusion,"while our maps are based on roberta-large, the methods to build them are model-agnostic (app.§c.1)."
2020.emnlp-main.747.txt,2020,7 Concluding Discussion,"as more human-rationale datasets are released, it will become increasingly possible to categorize them by rationale properties."
2020.emnlp-main.747.txt,2020,7 Concluding Discussion,"as we note in fig.2, modeling outcomes also have a heavy impact on explanation fidelity."
2020.emnlp-main.747.txt,2020,7 Concluding Discussion,"each addresses one shortcoming of the basic metric: normalization addresses the differences in class biases across models, adaptation the problem of domain inconsistency between full and rationale-only data, and ablation the inability of existing metrics to capture qualities like redundancy."
2020.emnlp-main.747.txt,2020,7 Concluding Discussion,"furthermore, there exists significant variance across datasets and classes."
2020.emnlp-main.747.txt,2020,7 Concluding Discussion,"however, with such a small sample size of datasets (n = 6), it is difficult to determine whether these differences are due solely to task type or to other factors such as annotation instructions or individual dataset semantics."
2020.emnlp-main.747.txt,2020,7 Concluding Discussion,human explanations contain a lot of promise.
2020.emnlp-main.747.txt,2020,7 Concluding Discussion,"in §5.2, we speculate that some of these differences (e.g., dependency) can be explained by the semantic differences between classification and document/query-style tasks."
2020.emnlp-main.747.txt,2020,7 Concluding Discussion,"in either case, analysis can expose inconsistencies between human and model understanding of the task."
2020.emnlp-main.747.txt,2020,7 Concluding Discussion,"in this work, we contribute to that effort by analyzing human rationales through the lens of automatic rationale evaluation methods, namely, sufficiency and comprehensiveness."
2020.emnlp-main.747.txt,2020,7 Concluding Discussion,our goal is to highlight the variance in these properties and call for more widespread empirical evaluations thereof.actionable implications.
2020.emnlp-main.747.txt,2020,7 Concluding Discussion,"overall, our results suggest that the idea of onesize-fits-all fidelity benchmarks might be problematic: human rationales may not be simply treated as gold standard."
2020.emnlp-main.747.txt,2020,7 Concluding Discussion,"the explainable ai community hopes to use them as a guide for evaluating model explanations and, possibly, for teaching models to make robust and wellreasoned decisions."
2020.emnlp-main.747.txt,2020,7 Concluding Discussion,we find that human rationales do not necessarily have high sufficiency or comprehensiveness.interpreting fidelity variance.
2020.emnlp-main.747.txt,2020,7 Concluding Discussion,"we need to design careful procedures to collect human rationales, understand properties of the resulting human rationales, and cautiously interpret the evaluation metrics."
2020.emnlp-main.747.txt,2020,7 Concluding Discussion,"we propose three ways to extend fidelity metrics: normalization, model adaptation, and random ablation."
2020.emnlp-main.747.txt,2020,7 Concluding Discussion,"when human rationales are found to be unfaithful, this can mean that either they fail to capture relevant signal, or that the model improperly utilizes that signal, perhaps as a result of learning spurious associations."
2020.emnlp-main.747.txt,2020,7 Concluding Discussion,"while e-snli comprises an even class balance, our model learns a strong bias in favor of the neutral class, which contributes to a class imbalance in fidelity for that dataset (fig.4)."
2020.emnlp-main.747.txt,2020,7 Concluding Discussion,"while not all of these issues are salient for every application involving rationale fidelity, we offer them as potential solutions where necessary."
2020.emnlp-main.747.txt,2020,7 Concluding Discussion,"wikiattack and e-snli, for example, display class asymmetry in their rationales, which likely contribute to their outlier status in fig.4 and 6 respectively."
2020.emnlp-main.748.txt,2020,5 Conclusion,"as with other problem domains, we have observed that abstractive summaries generated by transformers can generate imaginary content."
2020.emnlp-main.748.txt,2020,5 Conclusion,"however, the tlms we use here are of moderate size compared to what is now possible."
2020.emnlp-main.748.txt,2020,5 Conclusion,it would be very interesting to see what kind of performance larger models could achieve.
2020.emnlp-main.748.txt,2020,5 Conclusion,"our approach outperforms previous extractive and abstractive summarization methods on the arxiv, pubmed and bigpatent datasets and is less prone to copying entire phrases or sentences from the input text."
2020.emnlp-main.748.txt,2020,5 Conclusion,such studies could therefore require significant investments of resources.
2020.emnlp-main.748.txt,2020,5 Conclusion,we advise that such evaluations should probe multiple aspects of the summarization results including both factual correctness and coherency.
2020.emnlp-main.748.txt,2020,5 Conclusion,we also note that for evaluating the correctness of the summaries of scientific articles and patents one must have highly trained evaluators who are willing to invest significant amounts of time to read the underlying papers and patents.
2020.emnlp-main.748.txt,2020,5 Conclusion,we have demonstrated that transformer language models can generate high-quality summaries of long sequences of text via an extractive step followed by an abstractive step.
2020.emnlp-main.748.txt,2020,5 Conclusion,"we quantitatively measure the positive impact of the extractive step, by comparing it to a abstractive model variant that only sees the input text itself."
2020.emnlp-main.748.txt,2020,5 Conclusion,"while we believe that this work is a step forward towards generating more abstractive summaries, it remains an open challenge to develop abstactive models that respect the underlying facts of the content being summarized while matching the creative ability of humans to coherently and concisely synthesize summaries."
2020.emnlp-main.749.txt,2020,5 Conclusion,empirical results show that our models improve the factuality of summaries generated by state-of-the-art abstractive summarization systems without a huge drop on rouge scores.
2020.emnlp-main.749.txt,2020,5 Conclusion,"for future work, we plan to apply our method for other type of spans, such as noun phrases, verbs, and clauses."
2020.emnlp-main.749.txt,2020,5 Conclusion,spanfact can be used for fact correction on any abstractive summaries.
2020.emnlp-main.749.txt,2020,5 Conclusion,"we present spanfact, a suite of two factual correction models that use span selection mechanisms to replace one or multiple entity masks at a time."
2020.emnlp-main.75.txt,2020,6 Conclusion,"for future work, we are interested in investigating the proposed approach in a scaled setting with more languages and a larger amount of monolingual data."
2020.emnlp-main.75.txt,2020,6 Conclusion,"furthermore, we showed the effectiveness of multitask learning for cross-lingual downstream tasks outperforming sota larger models trained on single task."
2020.emnlp-main.75.txt,2020,6 Conclusion,"furthermore, we would also like to explore the most sample efficient strategy to add a new language to a trained mnmt system."
2020.emnlp-main.75.txt,2020,6 Conclusion,"in this work, we propose a multi-task learning framework that jointly trains the model with the translation task on bitext data, the masked language modeling task on the source-side monolingual data and the denoising auto-encoding task on the targetside monolingual data."
2020.emnlp-main.75.txt,2020,6 Conclusion,scheduling the different tasks and different types of data would be an interesting problem.
2020.emnlp-main.75.txt,2020,6 Conclusion,we explore data and noising scheduling approaches and demonstrate their efficacy for the proposed approach.
2020.emnlp-main.75.txt,2020,6 Conclusion,"we show that the proposed mtl approach can effectively improve the performance of mnmt on both high-resource and low-resource languages with large margin, and can also significantly improve the translation quality for zero-shot language pairs without bitext training data."
2020.emnlp-main.75.txt,2020,6 Conclusion,we showed that the proposed approach is more effective than pre-training followed by finetuning for nmt.
2020.emnlp-main.750.txt,2020,6 Conclusions,"experiments with human annotators showed that our proposed approach, including an explainable factual consistency checking model, can be a valuable tool for assisting humans in factual consistency checking."
2020.emnlp-main.750.txt,2020,6 Conclusions,"in our approach, models are trained to perform factual consistency checking on the document-sentence level, which allows them to handle a broader range of errors in comparison to previously proposed sentencesentence approaches."
2020.emnlp-main.750.txt,2020,6 Conclusions,"models are trained using artificially generated, weakly-supervised data created based on insights coming from the analysis of errors made by state-of-the-art summarization models."
2020.emnlp-main.750.txt,2020,6 Conclusions,"quantitative studies showed that on less abstractive domains, such as cnn/dailymail news articles, our proposed approach outperforms other models trained on existing textual entailment and fact-checking data, motivating our use of weak-supervision over transfer learning from related domains."
2020.emnlp-main.750.txt,2020,6 Conclusions,shortcomings of our approach explained in section 5.2 can serve as guidelines for future work.
2020.emnlp-main.750.txt,2020,6 Conclusions,we hope that this work will encourage continued research into factual consistency checking of abstractive summarization models.
2020.emnlp-main.750.txt,2020,6 Conclusions,we introduced a novel approach for factual consistency checking of summaries generated by abstractive neural models.
2020.emnlp-main.751.txt,2020,6 Implications and Future Directions,future works on meta-evaluation should investigate the effect of these settings on the performance of metrics.(2) metrics easily overfit on limited datasets.
2020.emnlp-main.751.txt,2020,6 Implications and Future Directions,"in closing, we highlight some potential future directions: (1) the choice of metrics depends not only on different tasks (e.g, summarization, translation) but also on different datasets (e.g., tac, cnndm) and application scenarios (e.g, system-level, summary-level)."
2020.emnlp-main.751.txt,2020,6 Implications and Future Directions,"multidataset meta-evaluation can help us better understand each metric’s peculiarity, therefore achieving a better choice of metrics under diverse scenarios.(3) our collected human judgments can be used as supervision to instantiate the most recentlyproposed pretrain-then-finetune framework (originally for machine translation) (sellam et al., 2020), learning a robust metric for text summarization."
2020.emnlp-main.751.txt,2020,6 Implications and Future Directions,"our work not only diagnoses the limitations of current metrics but also highlights the importance of upgrading the existing meta-evaluation testbed, keeping it up-to-date with the rapid development of systems and datasets."
2020.emnlp-main.752.txt,2020,7 Conclusion,"in near future, we aim to incorporate the video script information in the multimodal summarization process."
2020.emnlp-main.752.txt,2020,7 Conclusion,"in this paper, we propose the task of video-based multimodal summarization with multimodal output (vmsmo) which chooses a proper video cover and generates an appropriate textual summary for a video-attached article."
2020.emnlp-main.752.txt,2020,7 Conclusion,our model achieves state-of-the-art results in terms of autometrics and outperforms human evaluations by a large margin.
2020.emnlp-main.752.txt,2020,7 Conclusion,we propose a model named dual-interaction-based multimodal summarizer (dims) including a local conditional self-attention mechanism and a global-attention mechanism to jointly model and summarize multimodal input.
2020.emnlp-main.76.txt,2020,7 Conclusion,further analyses show that our method can also improve the lexical diversity.
2020.emnlp-main.76.txt,2020,7 Conclusion,"in this work, we focus on the token imbalance problem of nmt."
2020.emnlp-main.76.txt,2020,7 Conclusion,"next, we gave two simple but effective forms based on the criteria, which can assign appropriate training weights to target tokens."
2020.emnlp-main.76.txt,2020,7 Conclusion,"the final results show that our methods can achieve significant improvement in performance, especially on sentences that contain more low-frequency tokens."
2020.emnlp-main.76.txt,2020,7 Conclusion,"to alleviate this problem, we investigated existing adaptive objectives for other tasks and then proposed two heuristic criteria based on the observations."
2020.emnlp-main.76.txt,2020,7 Conclusion,we show that the output of vanilla nmt contains more high-frequency tokens and has lower lexical diversity.
2020.emnlp-main.77.txt,2020,7 Conclusion,experimental results show that our methods can significantly outperform the baseline methods and achieve comparable / better performance compared with existing strong nmt systems.
2020.emnlp-main.77.txt,2020,7 Conclusion,"in addition, we propose two novel techniques, namely bias module and sequential dependency to further improve the diversity and complementariness among units."
2020.emnlp-main.77.txt,2020,7 Conclusion,"in the meantime, our methods use much fewer parameters and only introduce mild inference speed degradation, which proves the efficiency of our models."
2020.emnlp-main.77.txt,2020,7 Conclusion,"in this paper, we propose multi-unit transformers for nmt to improve the expressiveness by introducing diverse and complementary units."
2020.emnlp-main.78.txt,2020,5 Conclusion,empirical results on a variety of language pairs and architectures demonstrate the effectiveness and universality of the presented method.
2020.emnlp-main.78.txt,2020,5 Conclusion,"future directions include continuing the exploration of this research topic for large sequenceto-sequence pre-training models (liu et al., 2020) and multi-domain translation models (wang et al., 2019b)."
2020.emnlp-main.78.txt,2020,5 Conclusion,"in this paper, we prove that existing nmt systems are over-parameterized and propose to improve the utilization efficiency of parameters in nmt models by introducing a rejuvenation approach."
2020.emnlp-main.78.txt,2020,5 Conclusion,"we also analyze the gains from perspectives of learning dynamics and linguistic probing, which give insightful research directions for future work."
2020.emnlp-main.78.txt,2020,5 Conclusion,"we will employ recent analysis methods to better understand the behaviors of rejuvenated models (he et al., 2019; yang et al., 2020)."
2020.emnlp-main.79.txt,2020,6 Conclusion,"in this work, we incorporate a novel local autoregressive translation mechanism (lat) into nonautoregressive translation, predicting multiple short sequences of tokens in parallel."
2020.emnlp-main.79.txt,2020,6 Conclusion,"moreover, analysis shows that lat can reduce repeated translations and perform better at longer sentences."
2020.emnlp-main.79.txt,2020,6 Conclusion,"we show that our method could achieve similar results to cmlm with less decoding iterations, which brings a 2.5x speedup."
2020.emnlp-main.79.txt,2020,6 Conclusion,"with a simple and efficient merging algorithm, we integrate lat into the conditional masked language model (cmlm ghazvininejad et al., 2019) and similarly adopt iterative decoding."
2020.emnlp-main.8.txt,2020,7 Conclusion and Future Work,"in future work, we would like to extend prism to paragraph- or document-level evaluation by training a paragraph- or document-level multilingual nmt system, as there is growing evidence that mt evaluation would be better conducted at the document level, rather than the sentence level (laubli et al., 2018)."
2020.emnlp-main.8.txt,2020,7 Conclusion and Future Work,nothing in our method is specific to sentencelevel mt.
2020.emnlp-main.8.txt,2020,7 Conclusion and Future Work,"our method achieves state-of-the-art performance on the most recent wmt shared metrics and qe tasks, without training on prior human judgements."
2020.emnlp-main.8.txt,2020,7 Conclusion and Future Work,"to the best of our knowledge, we are the first to release a large multilingual nmt system, and we hope others follow suit."
2020.emnlp-main.8.txt,2020,7 Conclusion and Future Work,we are optimistic our method will improve further as stronger multilingual nmt models become publicly available.
2020.emnlp-main.8.txt,2020,7 Conclusion and Future Work,we compare our method to several contrastive methods and present analysis showing that we have not simply reduced the task of evaluation to that of building a state-of-the-art mt system; the work done by the human translator to create references helps the evaluation model to judge systems that are stronger (at translation) than it is.
2020.emnlp-main.8.txt,2020,7 Conclusion and Future Work,we release a single model which supports 39 languages.
2020.emnlp-main.8.txt,2020,7 Conclusion and Future Work,"we show that a multilingual nmt system can be used as a lexically/syntactically unbiased, multilingual paraphraser, and that the resulting paraphraser can be used as an mt metric and qe metric."
2020.emnlp-main.80.txt,2020,5 Conclusion,"besides, as this idea is not limited to machine translation, it is also interesting to validate our model in other nlp tasks, such as low-resource nmt model training (lample et al., 2018; wan et al., 2020) and neural architecture search (guo et al., 2020)."
2020.emnlp-main.80.txt,2020,5 Conclusion,experimental results on three translation tasks verify the universal effectiveness of our approach.
2020.emnlp-main.80.txt,2020,5 Conclusion,"in this paper, we propose a novel self-paced learning model for nmt in which the learning schedule is determined by model itself rather than being intuitively predefined by humans."
2020.emnlp-main.80.txt,2020,5 Conclusion,"it is interesting to combine with other techniques (li et al., 2018; hao et al., 2019) to further improve nmt."
2020.emnlp-main.80.txt,2020,5 Conclusion,quantitative analyses confirm that exploiting self-paced strategy presents a more flexible way to facilitate the model convergence than its cl counterparts.
2020.emnlp-main.81.txt,2020,4 Discussions and Conclusions,"big or small, complex or simple, each has its distinct advantages."
2020.emnlp-main.81.txt,2020,4 Discussions and Conclusions,"in this work, we present a simple but effective variation with the long-short term masking strategy, and we performed comparative studies with the k-to-k translation model of the standard transformer."
2020.emnlp-main.81.txt,2020,4 Discussions and Conclusions,"just as the big, complex neural network architectures with great many parameters has its power, small but efficient modification like ours to the classical transformer has its unique appeals."
2020.emnlp-main.81.txt,2020,4 Discussions and Conclusions,"other examples of simple but impactful ideas are data augmentation and the round-trip back-translation (voita et al., 2019a), to name just a few."
2020.emnlp-main.81.txt,2020,4 Discussions and Conclusions,"we’re encouraged by our findings that in tandem with the great machinery that could bring powerful results, simplistic approaches could be just as efficacious."
2020.emnlp-main.82.txt,2020,8 Conclusion,"also, we’ll try to extend our methods in a wider range of nlp tasks."
2020.emnlp-main.82.txt,2020,8 Conclusion,"experiment results in chinese-english and english-german translation tasks suggest that by properly adjusting trained modules and prior parameters, we can generate translations which balance accuracy and diversity well."
2020.emnlp-main.82.txt,2020,8 Conclusion,"in future work, firstly, since our model is randomly sampled from model distribution to generate diverse translation, it is meaningful to explore better algorithms and training strategies to represent model distribution and search for the most distinguishable results in model distribution."
2020.emnlp-main.82.txt,2020,8 Conclusion,"in this paper, we propose to utilize variational inference in diverse machine translation tasks."
2020.emnlp-main.82.txt,2020,8 Conclusion,then we generate diverse translations with the models sampled from the trained model distribution.
2020.emnlp-main.82.txt,2020,8 Conclusion,we further analyze the correlations between module importance and trained dropout probabilities.
2020.emnlp-main.82.txt,2020,8 Conclusion,"we represent the transformer model distribution with dropout, and train the model distributions to minimize its distance to the posterior distribution under specific training dataset."
2020.emnlp-main.83.txt,2020,7 Conclusion,applying these latent alignment models for parallel translation of long documents can be an interesting research direction.
2020.emnlp-main.83.txt,2020,7 Conclusion,"ctc is a single step generation model, while imputer is an iterative generative model requiring only a constant number of generation steps."
2020.emnlp-main.83.txt,2020,7 Conclusion,"in this paper, we investigated two latent alignments models, ctc and imputer, for nonautoregressive machine translation."
2020.emnlp-main.83.txt,2020,7 Conclusion,our models rely on dynamic programming to marginalize out the latent alignments.
2020.emnlp-main.83.txt,2020,7 Conclusion,"unlike many prior works, our models do not need to perform target length prediction, or re-scoring of candidates and our models use a simplified neural architecture without the need of cross-attention mechanism found in many prior encoder-decoder architectures."
2020.emnlp-main.83.txt,2020,7 Conclusion,we demonstrate the ease and effectiveness of the application of these simple latent alignment models primarily used in speech recognition to the task of machine translation.
2020.emnlp-main.84.txt,2020,6 Conclusion,most qa studies frequently utilize start and end positions of answers as training targets without much considerations.
2020.emnlp-main.84.txt,2020,6 Conclusion,one limitation of our approach is that our method and analysis are based on a single paragraph setting which should be extended to a multiple paragraph setting to be more practically useful.
2020.emnlp-main.84.txt,2020,6 Conclusion,our findings also generalize to different positions and different datasets.
2020.emnlp-main.84.txt,2020,6 Conclusion,our findings show that position can work as a spurious bias and alert researchers when building qa models and datasets.
2020.emnlp-main.84.txt,2020,6 Conclusion,our study shows that most qa models fail to generalize over different positions when trained on datasets having answers in a specific position.
2020.emnlp-main.84.txt,2020,6 Conclusion,"we introduce several de-biasing methods to make models to ignore the spurious positional cues, and find out that the sentence-level answer prior is very useful."
2020.emnlp-main.85.txt,2020,7 Conclusion,"as shown in table 3, existing fine-tuned state-of-theart models have a way to go before modeling the distribution of this common sense data."
2020.emnlp-main.85.txt,2020,7 Conclusion,"in addition to the elements of this task which are appealing from a common sense modeling perspective, the inherent appeal of this task to humans opens a number of possibilities for future data collection and evaluation."
2020.emnlp-main.85.txt,2020,7 Conclusion,"millions of people have played phone-based games based upon this same premise8 , and prior works have obtained valuable annotations from trivia game participants (rodriguez et al., 2019)."
2020.emnlp-main.85.txt,2020,7 Conclusion,"the collection of a large set of raw answer strings and further clustering of these strings facilitates a generative evaluation method, enabling actual use of trained models to answer real common sense questions."
2020.emnlp-main.85.txt,2020,7 Conclusion,the inclusion of counts over clusters of answers provides a very rich dataset for training and evaluation.
2020.emnlp-main.85.txt,2020,7 Conclusion,this dataset lays the foundation for larger-scale data collection which leverages people’s natural interest to encourage high-quality answers to more common sense questions.
2020.emnlp-main.85.txt,2020,7 Conclusion,we have presented a new common sense dataset with many novel features.
2020.emnlp-main.86.txt,2020,7 Conclusion,iirc both provides a promising new avenue for studying complex reading and retrieval problems and demonstrates that much more research is needed in this area.
2020.emnlp-main.86.txt,2020,7 Conclusion,"our baseline model, built on top of state-ofthe-art models for the most closely related existing datasets, performs quite poorly in this setting, even when given oracle retrieval results, and especially when combined with other reading comprehension datasets."
2020.emnlp-main.86.txt,2020,7 Conclusion,"these questions require identifying what information is missing from a paragraph in order to answer a question, predicting where to find it, then synthesizing the retrieved information in complex ways."
2020.emnlp-main.86.txt,2020,7 Conclusion,"we introduced iirc, a new dataset of incompleteinformation reading comprehension questions."
2020.emnlp-main.87.txt,2020,13 Discussion and Future Work,how to most efficiently and effectively adapt transformer-based qa systems remains an important topic for future research.
2020.emnlp-main.87.txt,2020,13 Discussion and Future Work,"in general, the techniques work quite well, particularly in better-matched conditions."
2020.emnlp-main.87.txt,2020,13 Discussion and Future Work,"in this paper we have related self-training of question answering systems to the generative modeling of their associated context, and showed the former can be specified to optimize an approximate lower bound on the probability of the data."
2020.emnlp-main.87.txt,2020,13 Discussion and Future Work,"we then investigated iterative “pre-train then fine-tune” approaches to target domain adaptation, proposed question answer posterior (qap) as an alternative form of consistency filtering, and provided theoretical justification for roundtrip consistency filtering."
2020.emnlp-main.87.txt,2020,13 Discussion and Future Work,"while effective, iteratively re-training the qa generators and qa system is inefficient, even with strong data filters."
2020.emnlp-main.88.txt,2020,8 Conclusion,"results show that even a state-of-the-art language model, roberta-large, falls behind human performance by a large margin, necessitating more investigation on improving mrc on temporal relationships in the future."
2020.emnlp-main.88.txt,2020,8 Conclusion,"this paper presents torque, a new english machine reading comprehension (mrc) dataset of temporal ordering questions."
2020.emnlp-main.88.txt,2020,8 Conclusion,"torque has 3.2k news snippets, 9.5k hard-coded questions asking which events had happened, were ongoing, or were still in the future, and 21.2k human-generated questions querying more complex phenomena."
2020.emnlp-main.88.txt,2020,8 Conclusion,"understanding temporal ordering of events is critical in reading comprehension, but existing works have studied very little about it."
2020.emnlp-main.88.txt,2020,8 Conclusion,we argue that an mrc setting allows for more convenient representation of these temporal phenomena than conventional formalisms.
2020.emnlp-main.89.txt,2020,10 Conclusion,totto is available at https://github.com/ google-research-datasets/totto.
2020.emnlp-main.89.txt,2020,10 Conclusion,"we also provided several state-of-the-art baselines, and demonstrated totto could serve as a useful research benchmark for model and metric development."
2020.emnlp-main.89.txt,2020,10 Conclusion,"we presented totto, a table-to-text dataset that presents a controlled generation task and a data annotation process based on iterative sentence revision."
2020.emnlp-main.9.txt,2020,5 Discussion and Future Work,all logic fragments from formal logic fit this idea and may only differ in the nature of the graphs generated.
2020.emnlp-main.9.txt,2020,5 Discussion and Future Work,another modeling choice could explicitly condition the qa module on the node and edge modules so that the answer is predicted from the proof.
2020.emnlp-main.9.txt,2020,5 Discussion and Future Work,broader implications in formal logic: prover’s framework is not conceptually constrained to a particular logic fragment.
2020.emnlp-main.9.txt,2020,5 Discussion and Future Work,"e.g., if there is a disjunction rule “if x or y then z” instead of a conjunction rule “if x and y then z”, only the shape of the graph changes."
2020.emnlp-main.9.txt,2020,5 Discussion and Future Work,"for a fact “robin is a bird” and a rule with universal quantification “all birds can fly”, prover’s graph will have an edge from the fact to the rule to generate “robin can fly”."
2020.emnlp-main.9.txt,2020,5 Discussion and Future Work,"for example, in multi-hop qa tasks, the node module can choose all the relevant sentences in the context and the edge module can identify the flow of information between these to arrive at the answer (in the presence of task-specific constraints)."
2020.emnlp-main.9.txt,2020,5 Discussion and Future Work,"graph-based explanations: while we have presented prover as a model that can emulate formal reasoning, it has further potential use as an explanation generation system."
2020.emnlp-main.9.txt,2020,5 Discussion and Future Work,"graphical explanations, in contrast to natural language ones, are more structured and can allow explicit modeling of causality (and are easier to evaluate, as opposed to free-form natural language generation)."
2020.emnlp-main.9.txt,2020,5 Discussion and Future Work,"however, in scenarios where questions have open-ended answers, generating answer from a ‘proof’ in a consistent manner needs more exploration."
2020.emnlp-main.9.txt,2020,5 Discussion and Future Work,"however, other tasks may require imposing additional constraints to ensure valid explanations.prover’s inference mechanism can be extended to incorporate these."
2020.emnlp-main.9.txt,2020,5 Discussion and Future Work,"in such scenarios, prover’s unweighted proof graphs can be extended to weighted ones to represent this probabilistic nature."
2020.emnlp-main.9.txt,2020,5 Discussion and Future Work,"in the former, z is proved by either an edge from x or from y to the rule, while in the latter, both edges have to be necessarily present."
2020.emnlp-main.9.txt,2020,5 Discussion and Future Work,inferences over modals like “might” and disjunction rules like “if x then y or z” will mean that both the answer and the proof will be probabilistic.
2020.emnlp-main.9.txt,2020,5 Discussion and Future Work,"prover generates compositional explanations in the form of graphs and qa systems, in general, can potentially benefit from generating such graphical explanations."
2020.emnlp-main.9.txt,2020,5 Discussion and Future Work,prover uses the idea that applying a rule to fact(s) can produce new fact(s).
2020.emnlp-main.9.txt,2020,5 Discussion and Future Work,prover’s constraints like ensuring connectivity are necessary constraints for generating valid proofs for any graph-based explanation generation system.
2020.emnlp-main.9.txt,2020,5 Discussion and Future Work,"qa and proof consistency: currently, prover predicts the answer and generates the proof by jointly optimizing the qa, node and edge modules using a shared roberta model."
2020.emnlp-main.9.txt,2020,5 Discussion and Future Work,we empirically verify the consistency between the predicted answer and the generated proof by showing that the full accuracy matches the proof accuracy.
2020.emnlp-main.9.txt,2020,5 Discussion and Future Work,we experiment with datasets which already contain negations in facts.
2020.emnlp-main.9.txt,2020,5 Discussion and Future Work,we hope that prover will encourage further work towards developing interpretable nlp models with structured explanations.
2020.emnlp-main.9.txt,2020,5 Discussion and Future Work,"while these datasets currently do not contain disjunctions, our graphical representations of proofs allow an easy extension in such scenarios."
2020.emnlp-main.90.txt,2020,7 Conclusions and Future Work,extensive experiments and analysis show the effectiveness of our proposed mgcn model architecture with multiple aggregation methods.
2020.emnlp-main.90.txt,2020,7 Conclusions and Future Work,"in the future, we will explore more informative generation and consider applying mgcn to other nlp tasks for better information extraction and aggregation."
2020.emnlp-main.90.txt,2020,7 Conclusions and Future Work,"we present a practical task of generating sentences from relevant entities empowered by kg, and construct a large-scale and challenging dataset entdesc to facilitate the study of this task."
2020.emnlp-main.91.txt,2020,7 Conclusion and Future Work,"after showing the flaws of the current benchmarks in split and rephrase, we release two crowdsourced benchmarks, wiki benchmark and contract benchmark, created from wikipedia articles and legal documents respectively."
2020.emnlp-main.91.txt,2020,7 Conclusion and Future Work,"moreover, future work should inspect the effect of split and rephrase on downstream tasks such as machine translation or information retrieval, and examine if models’ performance on these tasks correlate with that on our benchmarks."
2020.emnlp-main.91.txt,2020,7 Conclusion and Future Work,our benchmarks contain significantly more diverse syntax and provide additional challenges to models.
2020.emnlp-main.91.txt,2020,7 Conclusion and Future Work,"using fine-grained crowdsourcing evaluation on 6 welldefined criteria, we show that they provide a greater challenge to models."
2020.emnlp-main.91.txt,2020,7 Conclusion and Future Work,"we hope our benchmark datasets and human judgements facilitate model development and metric design, respectively."
2020.emnlp-main.92.txt,2020,6 Conclusion,experiments on two benchmarks show advantage of our model over a state-of-the-art baseline.
2020.emnlp-main.92.txt,2020,6 Conclusion,the resulting model benefits from both richer loss and more structual features during decoding.
2020.emnlp-main.92.txt,2020,6 Conclusion,we investigated back-parsing for amr-to-text generation by integrating the prediction of projected amrs into sentence decoding.
2020.emnlp-main.93.txt,2020,6 Conclusions and Future Work,"in future, we plan to explore the following two directions: (1) interpolating the contexts between consecutive steps by introducing a new infilled image, and (2) addressing the underspecification problem by controlling the content in infilled image with explicit guidance."
2020.emnlp-main.93.txt,2020,6 Conclusions and Future Work,infilling is the strategy of enabling the model to learn surrounding contextual information by masking spans of input while the decoding attempts in generating the entire text.
2020.emnlp-main.93.txt,2020,6 Conclusions and Future Work,these infilling techniques are also immensely useful when dealing with data imputation with missing contexts and collaborative authoring in real world scenarios.
2020.emnlp-main.93.txt,2020,6 Conclusions and Future Work,"to experimentally support our hypothesis, we collect a new large scale vipt dataset of 46k procedures comprising 10 categories."
2020.emnlp-main.93.txt,2020,6 Conclusions and Future Work,we compare the performance of our model and conclusively show the higher significance of infilling based techniques in visual procedures compared to visual stories.
2020.emnlp-main.93.txt,2020,6 Conclusions and Future Work,we demonstrate that infilling is a simple yet effective technique and a step towards maximizing the utilization of surrounding contexts in visual narratives.
2020.emnlp-main.94.txt,2020,6 Conclusion,a separate rhyming model is responsible for generating rhymes.
2020.emnlp-main.94.txt,2020,6 Conclusion,"furthermore, our model’s poems are indeed topic-wise closely related to the acrostic word."
2020.emnlp-main.94.txt,2020,6 Conclusion,"our additional constraints only slightly decrease fluency and meaningfulness and, in fact, even increase the poeticness of the generated poems."
2020.emnlp-main.94.txt,2020,6 Conclusion,our neural poet is available at https://nala-cub.github.io/resources as a baseline for future research on the task.
2020.emnlp-main.94.txt,2020,6 Conclusion,"the task consists of creating poems with the following constraints: 1) the first letters of all lines should spell out a given word, 2) the poem’s content should also be related to that word, and 3) the poem should conform to a rhyming scheme."
2020.emnlp-main.94.txt,2020,6 Conclusion,"we further present a baseline for the task, based on a neural language model which has been pretrained on wikipedia and fine-tuned on a combination of poems with gold standard and automatically predicted topics."
2020.emnlp-main.94.txt,2020,6 Conclusion,we introduce a new task in the area of computational creativity: acrostic poem generation in english.
2020.emnlp-main.94.txt,2020,6 Conclusion,"we perform a manual evaluation of the generated poems and find that, while human poets still outperform automatic approaches, poems written by our neural poet obtain good ratings."
2020.emnlp-main.95.txt,2020,6 Conclusion,experiments have been conducted and proved our proposed methods’ effectiveness through comparing with several state-ofthe-art models on two ner benchmarks.
2020.emnlp-main.95.txt,2020,6 Conclusion,this paper introduced a local additivity based data augmentation (lada) methods for named entity recognition (ner) with two different interpolation strategies.
2020.emnlp-main.95.txt,2020,6 Conclusion,"to utilize unlabeled data, we introduced a novel consistent training objective combined with lada."
2020.emnlp-main.96.txt,2020,6 Conclusion,"in future work, it would be interesting to investigate to what extent pretrained language models benefit from groc on such zero-resource or lowresource adaptation settings."
2020.emnlp-main.96.txt,2020,6 Conclusion,"in principle, our results should be applicable to word-piece language models which are currently based on lookup tables to improve their sample efficiency and compactness."
2020.emnlp-main.96.txt,2020,6 Conclusion,"this work indicates several other future directions for language modeling in low-resource domains: extension to other languages, scaling training to even larger vocabularies, and applying groc in a large pretraining setting to expand its zero-shot generalization."
2020.emnlp-main.96.txt,2020,6 Conclusion,"we demonstrated that it reduces the number of parameters and increases sample efficiency, outperforming strong output embedding methods and adaptation baselines on both in-domain and open-vocabulary settings respectively."
2020.emnlp-main.96.txt,2020,6 Conclusion,we proposed an adaptive language model based on grounded compositional outputs.
2020.emnlp-main.97.txt,2020,8 Conclusion,"future work will explore applying ssmba to the target side manifold in structured prediction tasks, as well as other natural language tasks and settings where data augmentation is difficult."
2020.emnlp-main.97.txt,2020,8 Conclusion,"in contrast to other data augmentation methods, ssmba is applicable to any supervised task, requires no task-specific knowledge, and does not rely on dataset-specific fine-tuning."
2020.emnlp-main.97.txt,2020,8 Conclusion,"in this paper, we introduce ssmba, a method for generating synthetic data in settings where the underlying data manifold is difficult to characterize."
2020.emnlp-main.97.txt,2020,8 Conclusion,"our analysis shows that ssmba is robust to the initial dataset size, reconstruction model choice, and corruption amount, offering ood robustness improvements in most settings."
2020.emnlp-main.97.txt,2020,8 Conclusion,"we achieve gains of 0.8% accuracy on ood amazon reviews, 1.8% accuracy on ood mnli, and 1.4 bleu on indomain iwslt14 de→en."
2020.emnlp-main.97.txt,2020,8 Conclusion,"we demonstrate ssmba’s effectiveness on three nlp tasks spanning classification and sequence modeling: sentiment analysis, natural language inference, and machine translation."
2020.emnlp-main.98.txt,2020,5 Conclusion,"although the performance of setconv shows its advantage in classification, it may not be appropriate for high-dimensional sparse data."
2020.emnlp-main.98.txt,2020,5 Conclusion,combining sparse deep learning techniques with setconv is a potential solution to this issue.
2020.emnlp-main.98.txt,2020,5 Conclusion,experiment results demonstrates the superiority of our approach when compared to sota methods.
2020.emnlp-main.98.txt,2020,5 Conclusion,"in this paper, we propose a novel permutationinvariant setconv operation and a new training strategy named as episodic training for learning from imbalanced class distributions."
2020.emnlp-main.98.txt,2020,5 Conclusion,it is because the large amount of 0s in these data may lead to close-to-zero convolution kernels and limit the model’s capacity for classification.
2020.emnlp-main.98.txt,2020,5 Conclusion,"moreover, the proposed method can be easily migrated and applied to data of other types (e.g., images) with few modifications."
2020.emnlp-main.98.txt,2020,5 Conclusion,the combined utilization of them enables extracting the most discriminative features from data and automatically balancing the class distribution for the subsequent classifier.
2020.emnlp-main.98.txt,2020,5 Conclusion,we leave it for future work.
2020.emnlp-main.99.txt,2020,8 Conclusion,it explicitly performs multi-hop relational reasoning and is empirically shown to outperform existing methods with superior scalablility and interpretability.
2020.emnlp-main.99.txt,2020,8 Conclusion,the proposed mhgrn generalizes and combines the advantages of gnns and path-based reasoning models.
2020.emnlp-main.99.txt,2020,8 Conclusion,"we present a principled, scalable method, mhgrn, that can leverage general knowledge via multi-hop reasoning over interpretable structures (e.g.conceptnet)."
P16-1001,2016,7 Conclusions,"an unbounded t meant that the lols rollin was not ideal, but this could be modified to slow the loss of influence of the expert policy."
P16-1001,2016,7 Conclusions,imitation learning provides a total benefit of 4.5 points with our amr transition-based parser over exact imitation.
P16-1001,2016,7 Conclusions,"this is a more complex task than many previous applications of imitation learning, and we found that noise reduction was an essential pre-requisite."
P16-1001,2016,7 Conclusions,"using a simple 0/1 binary action cost using a heuristic expert provided a benefit of 1.8, with the remaining 2.7 points coming from rollouts with targeted exploration, focused costing and a non-decomposable loss function that was a better approximation to our objective."
P16-1001,2016,7 Conclusions,"we anticipate the approaches that we have found useful in the case of amr to reduce the impact of noise, efficiently support large action spaces with targeted exploration, and cope with unbounded trajectories in the transition system will be of relevance to other structured prediction tasks."
P16-1001,2016,7 Conclusions,we have considered imitation learning algorithms as a toolbox that can be tailored to fit the characteristics of the task.
P16-1002,2016,6 Discussion,"additionally, these paraphrasing-based transformations can be described in terms of grammar induction, so they can be incorporated into our framework."
P16-1002,2016,6 Discussion,"another piece of related work is luong et al.(2015b), who train a neural machine translation system to copy rare words, relying on an external system to generate alignments."
P16-1002,2016,6 Discussion,"dong and lapata (2016) concurrently developed an attention-based rnn model for semantic parsing, although they did not use data recombination."
P16-1002,2016,6 Discussion,"dropout training has been shown to be a form of adaptive regularization (hinton et al., 2012; wager et al., 2013)."
P16-1002,2016,6 Discussion,"generative oversampling (liu et al., 2007) learns a generative model in a multiclass classification setting, then uses it to generate additional examples from rare classes in order to combat label imbalance."
P16-1002,2016,6 Discussion,"grefenstette et al.(2014) proposed a non-recurrent neural model for semantic parsing, though they did not run experiments."
P16-1002,2016,6 Discussion,gu et al.(2016) apply a very similar copying mechanism to text summarization and singleturn dialogue generation.
P16-1002,2016,6 Discussion,"gulcehre et al.(2016) propose a model that decides at each step whether to write from a “shortlist” vocabulary or copy from the input, and report improvements on machine translation and text summarization."
P16-1002,2016,6 Discussion,guu et al.(2015) showed that encouraging a knowledge base completion model to handle longer path queries acts as a form of structural regularization.
P16-1002,2016,6 Discussion,"in data recombination, data generated by a highprecision generative model is used to train a second, domain-general model."
P16-1002,2016,6 Discussion,"in this paper, we have presented a novel framework we term data recombination, in which we generate new training examples from a highprecision generative model induced from the original training dataset."
P16-1002,2016,6 Discussion,language is a blend of crisp regularities and soft relationships.
P16-1002,2016,6 Discussion,mei et al.(2016) use an rnn model to perform a related task of instruction following.
P16-1002,2016,6 Discussion,our proposed attention-based copying mechanism bears a strong resemblance to two models that were developed independently by other groups.
P16-1002,2016,6 Discussion,"our work takes rnns, which excel at modeling soft phenomena, and uses a highly structured tool—synchronous context free grammars—to infuse them with an understanding of crisp structure."
P16-1002,2016,6 Discussion,prior work has explored using paraphrasing for data augmentation on nlp tasks.
P16-1002,2016,6 Discussion,"related work has also explored the idea of training on altered or out-of-domain data, often interpreting it as a form of regularization."
P16-1002,2016,6 Discussion,"some of our induced grammars generate examples that are not in the test distribution, but nonetheless aid in generalization."
P16-1002,2016,6 Discussion,there has been growing interest in applying neural networks to semantic parsing and related tasks.
P16-1002,2016,6 Discussion,"unlike our data recombination strategies, these techniques only change inputs x, while keeping the labels y fixed."
P16-1002,2016,6 Discussion,"uptraining (petrov et al., 2010) uses data labeled by an accurate but slow model to train a computationally cheaper second model."
P16-1002,2016,6 Discussion,"vinyals et al.(2015b) generate a large dataset of constituency parse trees by taking sentences that multiple existing systems parse in the same way, and train a neural model on this dataset."
P16-1002,2016,6 Discussion,"wang and yang (2015) use a similar strategy, but identify similar words and phrases based on cosine distance between vector space embeddings."
P16-1002,2016,6 Discussion,we believe this paradigm for simultaneously modeling the soft and hard aspects of language should have broader applicability beyond semantic parsing.
P16-1002,2016,6 Discussion,"we have demonstrated its effectiveness in improving the accuracy of a sequence-to-sequence rnn model on three semantic parsing datasets, using a synchronous context-free grammar as our generative model."
P16-1002,2016,6 Discussion,zhang et al.(2015) augment their data by swapping out words for synonyms from wordnet.
P16-1003,2016,7 Related Work and Discussion,a central challenge in learning from denotations is finding consistent logical forms (those that execute to a given denotation).
P16-1003,2016,7 Related Work and Discussion,"as kwiatkowski et al.(2013) and berant and liang (2014) both noted, a chief difficulty with executable semantic parsing is the “schema mismatch”—words in the utterance do not map cleanly onto the predicates in the logical form."
P16-1003,2016,7 Related Work and Discussion,"continuing the program analogy, generating fictitious worlds is similar in spirit to fuzz testing for generating new test cases (miller et al., 1990), but the goal there is coverage in a single program rather than identifying the correct (equivalence class of) programs."
P16-1003,2016,7 Related Work and Discussion,"finally, the effectiveness of dynamic programming on denotations relies on having a manageable set of denotations."
P16-1003,2016,7 Related Work and Discussion,"for more complex logical forms and larger knowledge graphs, there are many possible angles worth exploring: performing abstract interpretation to collapse denotations into equivalence classes (cousot and cousot, 1977), relaxing the notion of getting the correct denotation (steinhardt and liang, 2015), or working in a continuous space and relying on gradient descent (guu et al., 2015; neelakantan et al., 2016; yin et al., 2016; reed and de freitas, 2016)."
P16-1003,2016,7 Related Work and Discussion,"here, previous work has also leveraged the idea of dynamic programming on denotations (lau et al., 2003; liang et al., 2010; gulwani, 2011), though for more constrained spaces of programs."
P16-1003,2016,7 Related Work and Discussion,"in the second example of figure 6, “how long” is realized by a logical form that computes a difference between two dates."
P16-1003,2016,7 Related Work and Discussion,the ramification of this mismatch is that finding consistent logical forms cannot solely proceed from the language side.
P16-1003,2016,7 Related Work and Discussion,this connection can potentially improve the flow of ideas between the two fields.
P16-1003,2016,7 Related Work and Discussion,this mismatch is especially pronounced in the wikitablequestions of pasupat and liang (2015).
P16-1003,2016,7 Related Work and Discussion,this paper is about using annotated denotations to drive the search over logical forms.
P16-1003,2016,7 Related Work and Discussion,"this paper, by virtue of exact dynamic programming, sets the standard."
P16-1003,2016,7 Related Work and Discussion,"this takes us into the realm of program induction, where the goal is to infer a program (logical form) from input-output pairs (for us, world-denotation pairs)."
P16-1003,2016,7 Related Work and Discussion,"this work evolved from a long tradition of learning executable semantic parsers, initially from annotated logical forms (zelle and mooney, 1996; kate et al., 2005; zettlemoyer and collins, 2005; zettlemoyer and collins, 2007; kwiatkowski et al., 2010), but more recently from denotations (clarke et al., 2010; liang et al., 2011; berant et al., 2013; kwiatkowski et al., 2013; pasupat and liang, 2015)."
P16-1004,2016,5 Conclusions,"beyond semantic parsing, we would also like to apply our seq2tree model to related structured prediction tasks such as constituency parsing."
P16-1004,2016,5 Conclusions,experimental results show that enhancing the model with a hierarchical tree decoder and an attention mechanism improves performance across the board.
P16-1004,2016,5 Conclusions,"extensive comparisons with previous methods show that our approach performs competitively, without recourse to domain- or representation-specific features."
P16-1004,2016,5 Conclusions,"for example, it would be interesting to learn a model from question-answer pairs without access to target logical forms."
P16-1004,2016,5 Conclusions,in this paper we presented an encoder-decoder neural network model for mapping natural language descriptions to their meaning representations.
P16-1004,2016,5 Conclusions,we encode natural language utterances into vectors and generate their corresponding logical forms as sequences or trees using recurrent neural networks with long short-term memory units.
P16-1005,2016,8 Conclusions and Future Work,a promising solution is to rank all the entities in the sentence based on their importance relative to the identified trigger and the filler candidate.
P16-1005,2016,8 Conclusions and Future Work,a trigger identified by our approach is the most important node in the dependency tree relative to the given entity pair.
P16-1005,2016,8 Conclusions and Future Work,"besides considering the cross-sentence conflicts, we also want to investigate the within-sentence conflicts caused by the competition of triggers."
P16-1005,2016,8 Conclusions and Future Work,"first, a trigger can serve for multiple slot types."
P16-1005,2016,8 Conclusions and Future Work,"for example, a sibling trigger word “sister” can also represent a female member of a religious community."
P16-1005,2016,8 Conclusions and Future Work,"for example, slot children and its inverse slot parents share a subset of triggers."
P16-1005,2016,8 Conclusions and Future Work,"however, this trigger might be more important to another entity pair, which shares the same filler, in the same sentence."
P16-1005,2016,8 Conclusions and Future Work,in the future we aim to label slot types based on contextual information as well as sentence structures instead of trigger gazetteers only.
P16-1005,2016,8 Conclusions and Future Work,"in this paper, we demonstrate the importance of deep mining of dependency structures for slot filling."
P16-1005,2016,8 Conclusions and Future Work,"our approach outperforms state-of-the-art and can be rapidly portable to a new language or a new slot type, as long as there exists capabilities of name tagging, pos tagging, dependency parsing and trigger gazetteers."
P16-1005,2016,8 Conclusions and Future Work,"second, a trigger word can have multiple different meanings."
P16-1005,2016,8 Conclusions and Future Work,there are two primary reasons.
P16-1005,2016,8 Conclusions and Future Work,"we attempt to combine multi-prototype approaches (e.g., (reisinger and mooney, 2010)) to better disambiguate senses of trigger words."
P16-1006,2016,9 Conclusions and Future Work,"furthermore, we will exploit edge labels while walking through a knowledge base to retrieve more relevant entities."
P16-1006,2016,9 Conclusions and Future Work,"in the future we will apply visual pattern recognition and concept detection techniques to perform deep content analysis of the retrieved images, so we can do matching and inference on concept/entity level instead of shallow visual similarity."
P16-1006,2016,9 Conclusions and Future Work,our long-term goal is to extend this framework to other knowledge extraction and population tasks such as event extraction and slot filling to construct multimedia knowledge bases effectively from multiple languages with low cost.
P16-1006,2016,9 Conclusions and Future Work,we describe a novel multi-media approach to effectively transfer entity knowledge from highresource languages to low-resource languages.
P16-1006,2016,9 Conclusions and Future Work,we will also extend anchor image retrieval from documentlevel into phrase-level or sentence-level to obtain richer background information.
P16-1007,2016,9 Conclusion,"in light of the dramatic improvements in prediction quality that result from the techniques we have described, we look forward to investigating the effect on user experience for interactive translation systems that employ these methods."
P16-1007,2016,9 Conclusion,"instead, the learning objective, model, and inference procedure should all be tailored to the task."
P16-1007,2016,9 Conclusion,the combination of these changes can adapt a phrase-based translation system to perform prefix alignment and suffix prediction jointly with fewer search errors and greater accuracy for the critical first words of the suffix.
P16-1007,2016,9 Conclusion,the complementary strengths of both systems suggest future work in combining these techniques.
P16-1007,2016,9 Conclusion,"the phrase-based system is fast, produces diverse n-best lists, and provides reasonable prefix-bleu performance."
P16-1007,2016,9 Conclusion,"the recurrent neural system provides higher word prediction accuracy, but requires lengthy inference on a gpu."
P16-1007,2016,9 Conclusion,we have also shown decisively that simply performing constrained decoding for a phrase-based model is not an effective approach to the task of completing translations.
P16-1007,2016,9 Conclusion,we have shown that both phrase-based and neural translation approaches can be used to complete partial translations.
P16-1008,2016,7 Conclusion,"by encouraging nmt to pay less attention to translated words and more attention to untranslated words, our approach alleviates the serious over-translation and under-translation problems that traditional attention-based nmt suffers from."
P16-1008,2016,7 Conclusion,experimental results show that both variants achieve significant improvements in terms of translation quality and alignment quality over nmt without coverage.
P16-1008,2016,7 Conclusion,"we have presented an approach for enhancing nmt, which maintains and utilizes a coverage vector to indicate whether each source word is translated or not."
P16-1008,2016,7 Conclusion,we propose two variants of coverage models: linguistic coverage that leverages more linguistic information and nn-based coverage that resorts to the flexibility of neural network approximation .
P16-1009,2016,6 Conclusion,"because we do not change the neural network architecture to integrate monolingual training data, our approach can be easily applied to other nmt systems."
P16-1009,2016,6 Conclusion,future work will explore the effectiveness of our approach in more settings.
P16-1009,2016,6 Conclusion,"in our analysis, we identified domain adaptation effects, a reduction of overfitting, and improved fluency as reasons for the effectiveness of using monolingual data for training."
P16-1009,2016,6 Conclusion,"in this paper, we propose two simple methods to use monolingual training data during training of nmt systems, with no changes to the network architecture."
P16-1009,2016,6 Conclusion,"it is conceivable that larger synthetic data sets, or data sets obtained via data selection, will provide bigger performance benefits."
P16-1009,2016,6 Conclusion,"providing training examples with dummy source context was successful to some extent, but we achieve substantial gains in all tasks, and new sota results, via back-translation of monolingual target data into the source language, and treating this synthetic data as additional training data."
P16-1009,2016,6 Conclusion,"we also show that small amounts of indomain monolingual data, back-translated into the source language, can be effectively used for domain adaptation."
P16-1009,2016,6 Conclusion,"we expect that the effectiveness of our approach not only varies with the quality of the mt system used for back-translation, but also depends on the amount (and similarity to the test set) of available parallel and monolingual data, and the extent of overfitting of the baseline model."
P16-1009,2016,6 Conclusion,"while our experiments did make use of monolingual training data, we only used a small random sample of the available data, especially for the experiments with synthetic parallel data."
P16-1010,2016,7 Conclusion,based translation model by allowing discontinuous phrases.
P16-1010,2016,7 Conclusion,experiments on chinese–english and german–english show our model to be significantly better than the phrase-based model as well as other more sophisticated models.
P16-1010,2016,7 Conclusion,"in addition, we present a graph segmentation model to explicitly guide the selection of subgraphs."
P16-1010,2016,7 Conclusion,"in experiments, this model further improves our system."
P16-1010,2016,7 Conclusion,"in the future, we will extend this model to allow discontinuity on target sides and explore the possibility of directly encoding reordering information in translation rules."
P16-1010,2016,7 Conclusion,we are also interested in using graphs for neural machine translation to see how it can translate and benefit from graphs.
P16-1010,2016,7 Conclusion,we use graphs which combine bigram and dependency relations together as inputs and present a graph-based translation model.
P16-1011,2016,9 Conclusion,"given a language command, the induced hypothesis space, together with a learned hypothesis selector, can be applied by the agent to plan for lower-level actions."
P16-1011,2016,9 Conclusion,"more importantly, as our approach is based on incremental learning, it can be potentially integrated in a dialogue system to support life-long learning from humans."
P16-1011,2016,9 Conclusion,our empirical results have demonstrated a significant improvement in performance compared to a previous leading approach.
P16-1011,2016,9 Conclusion,our future work will extend the current approach with dialogue modeling to learn more reliable hypothesis spaces of resulting states for verb semantics.
P16-1011,2016,9 Conclusion,"specifically, we propose a hierarchical hypothesis space, where each node in the space describes a possible effect on the world from the verb."
P16-1011,2016,9 Conclusion,this paper presents an incremental learning approach that represents and acquires semantics of action verbs based on state changes of the environment.
P16-1012,2016,6 Conclusion,as future work we aim to make our approach completely knowledgefree by eliminating this dependency.
P16-1012,2016,6 Conclusion,"first experiments confirm that this already yields a much better coverage, i.e.upper bound on recall, while introducing more noise."
P16-1012,2016,6 Conclusion,"for obtaining substitution candidates, we still rely on lexical resources such as wordnet, which have to be available for each language."
P16-1012,2016,6 Conclusion,in all three datasets we were able to improve the current state of the art for the full lexical substitution task.
P16-1012,2016,6 Conclusion,"in our experiments, a single model trained on all data performed best on each language."
P16-1012,2016,6 Conclusion,"the remaining key challenge is to better characterize possible substitutes from bad substitutes in ranked lists of distributionally similar words, which frequently contain antonyms and cohyponyms."
P16-1012,2016,6 Conclusion,"the resulting model can be regarded as languageindependent; given an unannotated background corpus for computing language-specific resources and a source of substitution candidates, the system can be used almost out of the box."
P16-1012,2016,6 Conclusion,"we are the first to model lexical substitution as a language-independent task by considering not just a single-language dataset, but by merging data from distinct tasks in english, german and italian."
P16-1012,2016,6 Conclusion,we can consider substitution candidates based on their distributional similarity.
P16-1012,2016,6 Conclusion,"we extended an existing supervised learning-to-rank approach for lexical substitution (szarvas et al., 2013b) with stateof-the-art embedding features (melamud et al., 2015b)."
P16-1012,2016,6 Conclusion,we have further shown that incorporating more data helps training a more robust model and can consistently improve system performance by adding foreign language training data.
P16-1012,2016,6 Conclusion,"we have shown that a supervised, delexicalized approach can successfully learn a single model across languages – and thus perform transfer learning for lexical substitution."
P16-1012,2016,6 Conclusion,we observe that a listwise ranker model such as lambdamart facilitates this transfer learning.
P16-1012,2016,6 Conclusion,"we will explore unsupervised acquisition of relational similarity (mikolov et al., 2013b) for this task."
P16-1013,2016,7 Conclusion,our experiments confirmed that better curricula yield stronger models.
P16-1013,2016,7 Conclusion,"the proposed novel technique for finding an optimal curriculum is general, and can be used with other datasets and models."
P16-1013,2016,7 Conclusion,"we also conducted an extensive analysis, which sheds better light on understanding of text properties that are beneficial for model initialization."
P16-1013,2016,7 Conclusion,"we used bayesian optimization to optimize curricula for training dense distributed word representations, which, in turn, were used as the sole features in nlp tasks."
P16-1014,2016,6 Conclusion,"by doing a very simple modification over the nmt, our model is able to generalize to the unseen words and can deal with rarewords more efficiently."
P16-1014,2016,6 Conclusion,"for french to english machine translation on europarl corpora, we observe that using the pointer softmax can also improve the training convergence of the model."
P16-1014,2016,6 Conclusion,"for the summarization task on gigaword dataset, the pointer softmax was able to improve the results even when it is used together with the large-vocabulary trick."
P16-1014,2016,6 Conclusion,"in the case of neural machine translation, we observed that the training with the pointer softmax is also improved the convergence speed of the model as well."
P16-1014,2016,6 Conclusion,"in this paper, we propose a simple extension to the traditional soft attention-based shortlist softmax by using pointers over the input sequence."
P16-1014,2016,6 Conclusion,we observe noticeable improvements over the baselines on machine translation and summarization tasks by using pointer softmax.
P16-1014,2016,6 Conclusion,we show that the whole model can be trained jointly with single objective function.
P16-1015,2016,8 Conclusion,"finally, we would like to highlight two insights that the experiments provide."
P16-1015,2016,8 Conclusion,"first, a few more active tokens than two can boost the accuracy level of an arc-standard transition system towards the level of an easy-first transition system."
P16-1015,2016,8 Conclusion,"second, non-projective trees can be parsed by allowing a larger arc-distance which is a simple way to allow for non-projective edges."
P16-1015,2016,8 Conclusion,the results produced by this system are more comparable as they can be executed with the same classifier and feature extraction system.
P16-1015,2016,8 Conclusion,the transition system shows perfect alignment between the elementary operations on one hand and their preconditions and the oracle on the other hand.
P16-1015,2016,8 Conclusion,these parsing systems maintain very nicely the linear complexity of the arc-standard transition system while they provide a higher accuracy similar to those of easy-first.
P16-1015,2016,8 Conclusion,"these transition systems include systems such as arc-standard, arc-eager, easy-first."
P16-1015,2016,8 Conclusion,transitions can be freely composed of elementary operations.
P16-1015,2016,8 Conclusion,we adjust the transition system to work on a stack in a uniform way starting at a node on the stack and ending with the top node of the stack.
P16-1015,2016,8 Conclusion,we presented a generalized transition system that is capable of representing and executing a wide range of transition systems within one single implementation.
P16-1015,2016,8 Conclusion,we think that the transition systems with more active tokens or the combination with edges that span over more words provide very attractive transition systems for possible future parsers.
P16-1016,2016,6 Conclusion,experimental results show that mwe identification is greatly improved with respect to the mainstream joint approach.
P16-1016,2016,6 Conclusion,it is based on a new representation having two linguistic layers sharing lexical nodes.
P16-1016,2016,6 Conclusion,"this can be a useful starting point for several lines of research: implementing more advanced transitionbased techniques (beam search, dynamic oracles, deep learning); extending other classical transition systems like arc-eager and hybrid as well as handling non-projectivity."
P16-1016,2016,6 Conclusion,this paper proposes a transition-based system that extends a classical arc-standard parser to handle both lexical and syntactic analysis.
P16-1017,2016,6 Conclusion,experiments show that training a parser against this oracle leads to an improvement in accuracy over a static oracle.
P16-1017,2016,6 Conclusion,"together with morphological features, we obtain a greedy parser as accurate as state-of-the-art (non reranking) parsers for morphologically-rich languages."
P16-1017,2016,6 Conclusion,we have described a dynamic oracle for constituent parsing.
P16-1018,2016,7 Conclusion,"according to our results, the systematicity of metaphor can be exploited to learn linear transformations that represent the action of metaphorical mappings across many different adjectives in the same semantic domain."
P16-1018,2016,7 Conclusion,"beyond improvements to the applications we presented, the principles underlying our methods also show potential for other tasks."
P16-1018,2016,7 Conclusion,"finally, it would be interesting to investigate modeling metaphorical mappings as nonlinear mappings within the deep learning framework."
P16-1018,2016,7 Conclusion,"for instance, the lit and met adjective matrices and the cm mapping matrix learned with our methods could be applied to improve automated paraphrasing of an phrases."
P16-1018,2016,7 Conclusion,"in the cdsm framework we apply, verbs would be represented as third-order tensors."
P16-1018,2016,7 Conclusion,"it may also be possible to extend the coverage of our system by using automated word-sense disambiguation to bootstrap annotations and therefore construct lit and met matrices in a minimally supervised fashion (kartsaklis et al., 2013b)."
P16-1018,2016,7 Conclusion,our classification results suggest that the compositional distributional semantics of a phrase can inform classification of the phrase for metaphoricity.
P16-1018,2016,7 Conclusion,our work is also directly extendable to other syntactic constructions.
P16-1018,2016,7 Conclusion,"tractable and efficient methods for estimating these verb tensors are now available (fried et al., 2015)."
P16-1018,2016,7 Conclusion,we have shown that modeling metaphor explicitly within a cdsm can improve the resulting vector representations.
P16-1019,2016,6 Conclusions and Future Work,"in addition, we also plan to compare our work to the method of sporleder et al.(2010) as well apply our work on the idx corpus (sporleder et al., 2010) and to other languages."
P16-1019,2016,6 Conclusions and Future Work,in future work we plan to investigate the use of sent2vec to encode larger samples of text - not only the sentence containing idioms.
P16-1019,2016,6 Conclusions and Future Work,"in this paper we have investigated the use of distributed compositional semantics in literal and idiomatic language classification, more specifically using skip-thought vectors (sent2vec)."
P16-1019,2016,6 Conclusions and Future Work,the focus of these future experiments will be to test how our approach which is relatively less dependent on nlp resources compares with these other methods for idiom token classification.
P16-1019,2016,6 Conclusions and Future Work,we also investigated the capability of sent2vec clustering representations of sentences within the same class in feature space.
P16-1019,2016,6 Conclusions and Future Work,we also plan to further analyse the errors made by our “general” model and investigate the “general” approach on the skewed part of the vnc-tokens dataset.
P16-1019,2016,6 Conclusions and Future Work,we also plan to investigate an end-to-end approach based on deep learning-based representations to classify literal and idiomatic language use.
P16-1019,2016,6 Conclusions and Future Work,we followed the intuition presented by previous experiments with distributed representations that words with similar meaning are clustered together in feature space and experimented with a “general” classifier that is trained on a dataset of mixed expressions.
P16-1019,2016,6 Conclusions and Future Work,we followed the intuition that the distributed representations generated by sent2vec also include information regarding the context where the potential idiomatic expression is inserted and therefore is sufficient for distinguishing between idiomatic and literal language use.
P16-1019,2016,6 Conclusions and Future Work,we have also shown that our models generally present better precision and/or recall than the baselines.
P16-1019,2016,6 Conclusions and Future Work,we have shown that the “general” classifier is feasible but the traditional “per-expression” does achieve better results in some cases.
P16-1019,2016,6 Conclusions and Future Work,we have shown that using the sent2vec representations our classifiers achieve better results in 3 out of 4 expressions tested.
P16-1019,2016,6 Conclusions and Future Work,we tested this approach with different machine learning (ml) algorithms (k-nearest neighbours and support vector machines) and compared our work against a topic model representation that include the full paragraph or the surrounding paragraphs where the potential idiom is inserted.
P16-1020,2016,8 Conclusion and Future Work,"in future work, we will apply our method to other kinds of phrases and tasks."
P16-1020,2016,8 Conclusion and Future Work,"our method achieves the state of the art on a compositionality detection task of verb-object pairs, and also improves upon the previous state-of-the-art method on a transitive verb disambiguation task."
P16-1020,2016,8 Conclusion and Future Work,we have presented a method for adaptively learning compositional and non-compositional phrase embeddings by jointly detecting compositionality levels of phrases.
P16-1021,2016,6 Conclusion,"additionally, language expressing emotion and cognition relates to metaphor, but in ways specific to particular candidate words."
P16-1021,2016,6 Conclusion,both types of features showed significant improvement over the state-of-the-art.
P16-1021,2016,6 Conclusion,"for our breast cancer forum dataset, we find more words related to anxiety around metaphors."
P16-1021,2016,6 Conclusion,"in particular, our system made significant gains in solving the problem of overclassification in metaphor detection."
P16-1021,2016,6 Conclusion,our proposed features can be expanded to other domains.
P16-1021,2016,6 Conclusion,"though in other domains, the specific topic transition and emotion/cognition patterns would likely be different, these features would still be relevant to metaphor detection."
P16-1021,2016,6 Conclusion,"we also find that personal topics are markers of metaphor, as well as certain patterns in topic transition."
P16-1021,2016,6 Conclusion,"we propose a new, effective method for metaphor detection using (1) sentence level topic transitions between target sentences and surrounding contexts and (2) emotion and cognition words."
P16-1022,2016,5 Conclusion,"based on such combinations, we managed to compress an lstm language model (lm), where memory does not increase with the vocabulary size except a bias and a sparse code for each word."
P16-1022,2016,5 Conclusion,"in this paper, we proposed an approach to represent rare words by sparse linear combinations of common ones."
P16-1022,2016,5 Conclusion,our experimental results also show that the compressed lm has yielded a better performance than the uncompressed base lm.
P16-1023,2016,6 Conclusion and future work,"a single unified grammar may make it harder to interpret the results, but may give additional and more fine-grained insights as to how the performance of embedding models is influenced by different fundamental properties of natural language and their interactions."
P16-1023,2016,6 Conclusion and future work,"additionally, while most intrinsic evaluations consider word vectors as points, we used classifiers that identify different small subspaces of the full space."
P16-1023,2016,6 Conclusion and future work,"as an alternative to the common evaluation tasks, we proposed to identify generic criteria that are important for an embedding model to represent properties of words accurately and consistently."
P16-1023,2016,6 Conclusion and future work,"based on this paper, there are serveral lines of investigation we plan to conduct in the future.(i) we will attempt to support our results on artificially generated corpora by conducting experiments on real natural language data.(ii) we will study the coverage of our four criteria in evaluating word representations.(iii) we modeled the four criteria using separate pcfgs, but they could also be modeled by one single unified pcfg."
P16-1023,2016,6 Conclusion and future work,but the validity of the assumption that embedding spaces can be decomposed into “linear” subspaces should be investigated in the future.
P16-1023,2016,6 Conclusion and future work,"finally, we have made the simplifying assumption in this paper that the best conceptual framework for thinking about embeddings is that the embedding space can be decomposed into subspaces: either into completely orthogonal subspaces or – less radically – into partially “overlapping” subspaces."
P16-1023,2016,6 Conclusion and future work,"furthermore, we have made the assumption that the smoothness and robustness properties that are the main reasons why embeddings are used in nlp can be reduced to similarities in subspaces."
P16-1023,2016,6 Conclusion and future work,"however even if this was the case, then much of the general framework of what we have presented in this paper would still apply; e.g., the criterion that a particular facet be fully and correctly represented is as important as before."
P16-1023,2016,6 Conclusion and future work,one question that arises is then to what extent the four criteria are orthogonal and to what extent interdependent.
P16-1023,2016,6 Conclusion and future work,our evaluation method gave direct insight about the quality of embeddings.
P16-1023,2016,6 Conclusion and future work,see rothe et al.(2016) and rothe and schutze (2016) for work that makes ¨ similar assumptions.
P16-1023,2016,6 Conclusion and future work,the fundamental assumptions here are decomposability and linearity.
P16-1023,2016,6 Conclusion and future work,the smoothness properties could be much more complicated.
P16-1023,2016,6 Conclusion and future work,"this is an important desideratum when designing evaluation methods because of the multifacetedness of natural language words: they have a large number of properties, each of which only occupies a small proportion of the full-space capacity of the embedding."
P16-1023,2016,6 Conclusion and future work,we developed this evaluation methodology using pcfg-generated corpora and applied it on a case study to compare different models of learning distributional representations.
P16-1023,2016,6 Conclusion and future work,we have introduced a new way of evaluating distributional representation models.
P16-1023,2016,6 Conclusion and future work,we proposed an innovative way of doing intrinsic evaluation of embeddings.
P16-1023,2016,6 Conclusion and future work,we suggested four criteria based on fundamental characteristics of natural language and designed tests that evaluate models on the criteria.
P16-1023,2016,6 Conclusion and future work,"while we showed important differences of the embedding models, the goal was not to do a comprehensive comparison of them."
P16-1024,2016,5 Conclusions and Future Work,"encouraged by the excellent results, we also plan to test the portability of the approach to more language pairs, and other tasks and applications."
P16-1024,2016,5 Conclusions and Future Work,"in future work, we plan to investigate other methods for seed pairs selection, settings with scarce resources (agic et al., 2015; zhang et al., 2016), ′ other context types inspired by recent work in the monolingual settings (levy and goldberg, 2014a; melamud et al., 2016), as well as model adaptations that can work with multi-word expressions."
P16-1024,2016,5 Conclusions and Future Work,it learns the mapping between two monolingual embedding spaces using only highly reliable symmetric translation pairs from an inexpensive seed document-level embedding space.
P16-1024,2016,5 Conclusions and Future Work,"on the basis of the analysis, we proposed a simple yet effective hybrid bilingual word embedding model called hybwe."
P16-1024,2016,5 Conclusions and Future Work,"the results in the tasks of (1) bilingual lexicon learning and (2) suggesting word translations in context demonstrate that – due to its careful selection of reliable translation pairs for seed lexicons – hybwe outperforms benchmarking bwe induction models, all of which use more expensive bilingual signals for training."
P16-1024,2016,5 Conclusions and Future Work,we presented a detailed analysis of the importance and properties of seed bilingual lexicons in learning bilingual word embeddings (bwes) which are valuable for many cross-lingual/multilingual nlp tasks.
P16-1025,2016,5 Conclusions and Future Work,and it can produce high-quality event annotations simultaneously without using annotated training data.
P16-1025,2016,5 Conclusions and Future Work,"experiments on news and biomedical domain demonstrate that this framework can discover explicitly defined rich event schemas which cover not only most types in existing manually defined schemas, but also new event types and argument roles."
P16-1025,2016,5 Conclusions and Future Work,"in the future, we will extend this framework to other information extraction tasks."
P16-1025,2016,5 Conclusions and Future Work,the granularity of event types is also customized for specific input corpus.
P16-1025,2016,5 Conclusions and Future Work,we proposed a novel liberal event extraction framework which combines the merits of symbolic semantics and distributed semantics.
P16-1026,2016,5 Conclusions,experimental results show that the proposed approach outperforms the previously reported best result on dataset i by nearly 10% in f-measure.
P16-1026,2016,5 Conclusions,"in future work, we will investigate scalable and parallel model learning to explore the performance of our model for large-scale real-time event extraction and visualization."
P16-1026,2016,5 Conclusions,"in this paper, we have proposed an unsupervised bayesian model, called latent event extraction & visualization (leev) model, to extract the structured representations of events from social media and simultaneously visualize them in a twodimensional euclidean space."
P16-1026,2016,5 Conclusions,the proposed approach has been evaluated on two datasets.
P16-1026,2016,5 Conclusions,"these results show that by jointly learning event extraction and visualization, our proposed approach is able to give better results on both tasks."
P16-1026,2016,5 Conclusions,visualization results show that the proposed approach with manifold regularization can significantly improve the quality of event visualization.
P16-1027,2016,7 Conclusion,"this provides evidence that, for the task of held-out event prediction, encoder/decoder models mediated by automatically extracted events may not be learning appreciably more structure than systems trained on raw tokens alone."
P16-1027,2016,7 Conclusion,"we have found that models operating on raw text perform roughly comparably to identical models operating on predicate-argument event structures when predicting the latter, and that text models provide superior predictions of raw text."
P16-1027,2016,7 Conclusion,we have given what we believe to be the first systematic evaluation of sentence-level rnn language models on the task of predicting held-out document text.
P16-1028,2016,7 Conclusion,"in future work, we plan to apply semlms to other semantic related nlp tasks e.g.machine translation and question answering."
P16-1028,2016,7 Conclusion,the paper builds two types of discourse driven semantic language models with four different language model implementations that make use of neural embeddings for semantic frames.
P16-1028,2016,7 Conclusion,"we use perplexity and a narrative cloze test to prove that the proposed semlms have a good level of abstraction and are of high quality, and then apply them successfully to the two challenging tasks of co-reference resolution and shallow discourse parsing, exhibiting improvements over state-ofthe-art systems."
P16-1029,2016,6 Conclusion,"besides, we proposed a novel domain similarity measure based on sentiment graphs, and incorporated the domain similarities between target and different source domains into the domain adaptation process."
P16-1029,2016,6 Conclusion,"first, we extract both global and domain-specific sentiment knowledge from the data of multiple source domains."
P16-1029,2016,6 Conclusion,our approach consists of two steps.
P16-1029,2016,6 Conclusion,"second, we transfer these two kinds of sentiment knowledge to target domain with the help of the words’ sentiment graph."
P16-1029,2016,6 Conclusion,the experimental results on a benchmark dataset show that our approach can effectively improve the performance of cross-domain sentiment classification.
P16-1029,2016,6 Conclusion,this paper presents a sentiment domain adaptation approach which transfers the sentiment knowledge from multiple source domains to target domain.
P16-1029,2016,6 Conclusion,we proposed to build words’ sentiment graph for target domain by extracting their sentiment polarity relations from massive unlabeled data.
P16-1030,2016,6 Conclusion,all the learned connotation frames and annotations will be shared at http://homes.cs.washington.edu/?hrashkin/connframe.html.
P16-1030,2016,6 Conclusion,"in this paper, we presented a novel system of connotative frames that define a set of implied sentiment and presupposed facts for a predicate."
P16-1030,2016,6 Conclusion,"our work also empirically explores different methods of inducing and modelling these connotation frames, incorporating the interplay between relations within frames."
P16-1030,2016,6 Conclusion,"our work suggests new research avenues on learning connotation frames, and their applications to deeper understanding of social and political discourse."
P16-1031,2016,5 Conclusions and Future Work,"experimental results show that btdnns significantly outperforms the several baselines, and achieves an accuracy which is competitive with the state-of-the-art method for sentiment classification adaptation."
P16-1031,2016,5 Conclusions and Future Work,"first, since deep learning may obtain better generalization on large-scale data sets (bengio, 2009), a straightforward path of the future research is to apply the proposed btdnns for domain adaptation on a much larger industrial-strength data set of 22 domains (glorot et al., 2011)."
P16-1031,2016,5 Conclusions and Future Work,"in this paper, we propose a novel bi-transferring deep neural networks (btdnns) for crossdomain sentiment classification."
P16-1031,2016,5 Conclusions and Future Work,"second, we will try to investigate the use of the proposed approach for other kinds of data set, such as 20 newsgroups and reuters21578 (li et al., 2012; zhuang et al., 2013)."
P16-1031,2016,5 Conclusions and Future Work,"the linear transformation of btdnns ensures the feasibility of transferring between domains, and the distribution consistency between the transferred domain and the desirable domain is constrained with a linear data reconstruction manner."
P16-1031,2016,5 Conclusions and Future Work,"the proposed btdnns attempts to transfer the source domain examples to the target domain, and also transfer the target domain examples to the source domain."
P16-1031,2016,5 Conclusions and Future Work,there are some ways in which this research could be continued.
P16-1032,2016,8 Conclusion,"experiments demonstrated that the approach can infer implied sentiment and point toward potential directions for future work, including joint entity detection and incorporation of more varied types of factual relationships."
P16-1032,2016,8 Conclusion,"we presented an approach to interpreting sentiment among entities in news articles, with global constraints provided by social, faction and discourse context."
P16-1033,2016,7 Conclusions,"finally, one anonymous reviewer comments that we may use automatically projected trees (rasooli and collins, 2015; guo et al., 2015; ma and xia, 2014) as the initial seed labeled data, which is cheap and interesting."
P16-1033,2016,7 Conclusions,"finally, we conduct human annotation experiments on chinese to compare pa and fa on real annotation time and quality."
P16-1033,2016,7 Conclusions,"for future work, we would like to advance this study in the following directions."
P16-1033,2016,7 Conclusions,"however, it is more reasonable that human are good at resolving some ambiguities but bad at others."
P16-1033,2016,7 Conclusions,"intuitively, it would be more profitable to annotate instances that are both difficult for the current model and representative in capturing common language phenomena."
P16-1033,2016,7 Conclusions,it is shown that the crfbased parser can on the one hand provide tree probabilities and dependency marginal probabilities as principled uncertainty metrics and on the other hand elegantly learn from partially annotated data.
P16-1033,2016,7 Conclusions,"moreover, the results also indicate that annotators tend to perform better under pa than fa."
P16-1033,2016,7 Conclusions,"our plan is to study which syntactic structures are more suitable for human annotation, and balance informativeness of a candidate task and its suitability for human annotation."
P16-1033,2016,7 Conclusions,"second, we so far assume that the selected tasks are equally difficult and take the same amount of effort for human annotators."
P16-1033,2016,7 Conclusions,the first idea is to combine uncertainty and representativeness for measuring informativeness of annotation targets in concern.
P16-1033,2016,7 Conclusions,this paper for the first time applies a state-ofthe-art probabilistic model to al with pa for dependency parsing.
P16-1033,2016,7 Conclusions,this suggests that al with pa can reduce annotation time by 23.2% over with fa on chinese.
P16-1033,2016,7 Conclusions,we find that annotating a dependency in pa takes about 2 times long as in fa.
P16-1033,2016,7 Conclusions,"we have proposed and compared several uncertainty metrics through simulation experiments, and show that al with pa can greatly reduce the amount of annotated dependencies by 62.2% on chinese 74.2% on english."
P16-1034,2016,8 Conclusion,"in this paper we focused on arc-factor models, but our method could be extended to higher order models, following the dual decomposition method presented in (koo et al., 2010) in which the maximum-weight spanning arborescence component would be replaced by our constrained model."
P16-1034,2016,8 Conclusion,"moreover, while well-nested dependencies with 2-bounded block degree can represent ltag derivations, toggling the wellnestedness or setting the block degree bound allows to express the whole range of derivations in lexicalized lcfrs, whether well-nested or with a bounded fan-out."
P16-1034,2016,8 Conclusion,our algorithm can exactly represent these settings with a comparable complexity.
P16-1034,2016,8 Conclusion,"our method opens new perspectives for ltag parsing, in particular using decomposition techniques where dependencies and templates are predicted separately."
P16-1034,2016,8 Conclusion,the first one is a heuristic which relies on lagrangian relaxation and the chu-liu-edmonds efficient maximum spanning arborescence algorithm.
P16-1034,2016,8 Conclusion,the second one is an exact branch-and-bound procedure where bounds are provided by lagrangian relaxation.
P16-1034,2016,8 Conclusion,we presented a novel characterization of dependency trees complying with two structural rules: bounded block degree and well-nestedness from which we derived two methods for arc-factored dependency parsing.
P16-1034,2016,8 Conclusion,"we showed experimentally that these methods give results comparable with state-of-the-art arcfactored parsers, while enforcing constraints in all cases."
P16-1035,2016,9 Conclusion,"importantly, our results highlight the value of locally-training word embeddings in a query-specific manner."
P16-1035,2016,9 Conclusion,"instead of using a “sriracha sauce of deep learning,” as embedding techniques like word2vec have been called, we contend that the situation sometimes requires, say, that we make a b′echamel or a mole verde or a sambal—or otherwise learn to cook."
P16-1035,2016,9 Conclusion,the strength of these results suggests that other research adopting global embedding vectors should consider local embeddings as a potentially superior representation.
P16-1035,2016,9 Conclusion,we have demonstrated a simple and effective method for performing query expansion with word embeddings.
P16-1036,2016,9 Conclusions,"also, we would like to experiment with other deep neural architectures such as recurrent neural networks, long short term memory networks, etc.to form the sub-networks."
P16-1036,2016,9 Conclusions,"answers” dataset revealed that t-scqa outperforms current state-ofthe-art approaches based on translation models, topic models and deep neural network based models which use non-shared parameters."
P16-1036,2016,9 Conclusions,"as part of future work, we would like to enhance scqa with the meta-data information like categories, user votes, ratings, user reputation of the questions and answer pairs."
P16-1036,2016,9 Conclusions,"experiments on large scale real-life “yahoo! answers” dataset revealed that t-scqa outperforms current state-ofthe-art approaches based on translation models, topic models and deep neural network based models which use non-shared parameters."
P16-1036,2016,9 Conclusions,"in this paper, we proposed scqa for similar question retrieval which tries to bridge the lexicosyntactic gap between the question posed by the user and the archived questions."
P16-1036,2016,9 Conclusions,interpolating bm25 scores into the model t-scqa results in improved matching performance for both textual and semantic matching.
P16-1036,2016,9 Conclusions,scqa employs twin convolutional neural networks with shared parameters to learn the semantic similarity between the question and answer pairs.
P16-1037,2016,7 Discussions,"in this paper, we propose a novel problem of news citation recommendation, which aims to recommend news citations for references based on a citing context."
P16-1037,2016,7 Discussions,the experimental results show the efficacy of our approach.
P16-1037,2016,7 Discussions,we construct a real-world dataset.
P16-1037,2016,7 Discussions,we develop a re-ranking system leveraging implicit and explicit semantics for content similarity.
P16-1038,2016,10 Conclusion,"4 future directions for this work include further improving the number and quality of g2p models, as well as performing external evaluations of the models in speech- and text-processing tasks."
P16-1038,2016,10 Conclusion,our experiments show that adapting training data for low-resource languages outperforms adapting output.
P16-1038,2016,10 Conclusion,"tionary from wiktionary and rule tables from wikipedia, we build high-resource g2p models and show that adding g-p rules as training data can improve g2p performance."
P16-1038,2016,10 Conclusion,"to our knowledge, these are the most broadly multilingual g2p experiments to date."
P16-1038,2016,10 Conclusion,we plan to use the presented data and methods for other areas of multilingual natural language processing.
P16-1038,2016,10 Conclusion,we then leverage lang2lang distance metrics and phon2phon phoneme distances to adapt g2p resources for highresource languages for 229 related low-resource languages.
P16-1038,2016,10 Conclusion,"with this publication, we release a number of resources to the nlp community: a large multilingual wiktionary pronunciation dictionary, scraped wikipedia ipa help tables, compiled named entity resources (including a multilingual gazetteer), and our phon2phon and lang2lang distance tables."
P16-1039,2016,8 Conclusion,the proposed framework makes a latest attempt to formalize word segmentation as a direct structured learning procedure in terms of the recent distributed representation framework.
P16-1039,2016,8 Conclusion,"the task of chinese word segmentation, which contains three main components: (1) a factory to produce word representation when given its governed characters; (2) a sentence-level likelihood evaluation system for segmented sentence; (3) an efficient and effective algorithm to find the best segmentation."
P16-1039,2016,8 Conclusion,"though our system outputs results that are better than the latest neural network segmenters but comparable to all previous state-of-the-art systems, the framework remains a great of potential that can be further investigated and improved in the future."
P16-1040,2016,7 Conclusion,"finally, we conducted several comparisons to study the differences between our word-based model with character-based neural models, showing that they have different error characteristics."
P16-1040,2016,7 Conclusion,"the model achieved comparable performances compared with a discrete word-based baseline, and also the state-of-the-art characterbased neural models in the literature."
P16-1040,2016,7 Conclusion,"we further demonstrated that the model can utilize discrete features conveniently, resulting in a combined model that achieved top performances compared with previous work."
P16-1040,2016,7 Conclusion,"we proposed a word-based neural model for chinese segmentation, which exploits not only character embeddings as previous work does, but also word embeddings pre-trained from large scale corpus."
P16-1041,2016,7 Conclusion,exogenous attention (the trainable word weights) may be broadly helpful for nlp.
P16-1041,2016,7 Conclusion,good comprehension of language is supported by hierarchical levels of understanding (cf.hill et al.(2015)).
P16-1041,2016,7 Conclusion,"our model achieves state-of-the-art results, outperforming several feature-engineered and neural approaches."
P16-1041,2016,7 Conclusion,"reasoning over language is challenging, but easily simulated in some cases."
P16-1041,2016,7 Conclusion,"the training wheels approach, that is, initializing neural networks to perform sensible heuristics, appears helpful for small datasets."
P16-1041,2016,7 Conclusion,"we have presented the novel parallel-hierarchical model for machine comprehension, and evaluated it on the small but complex mctest."
P16-1041,2016,7 Conclusion,"working with our model has emphasized to us the following (not necessarily novel) concepts, which we record here to promote further empirical validation."
P16-1042,2016,7 Conclusion,"lastly, we show that relational entailment and meronymy can be elegantly incorporated into natural logic."
P16-1042,2016,7 Conclusion,"these features allow us to perform large-scale broad domain question answering, achieving strong results on the aristo science exams corpus."
P16-1042,2016,7 Conclusion,"we have improved naturalli to be more robust for question answering by running the inference over dependency trees, pre-computing deletions, and incorporating a soft evaluation function for predicting likely entailments when formal support could not be found."
P16-1043,2016,7 Conclusion,"curriculum learning is inspired by the way humans acquire knowledge and skills: by mastering simple concepts first, and progressing through information with increasing difficulty to grasp more complex topics."
P16-1043,2016,7 Conclusion,the approach is quite general and we hope that this paper will encourage more nlp researchers to explore curriculum learning in their own works.
P16-1043,2016,7 Conclusion,"we proposed a number of heuristics, an ensemble, and several improvements for selecting the curriculum that improves upon self-paced learning."
P16-1043,2016,7 Conclusion,we showed that our heuristics when coupled with diversity lead to significant improvements in a number of question answering tasks.
P16-1043,2016,7 Conclusion,"we stressed on another important aspect of human learning – diversity, that requires that the right curriculum should not only arrange the data samples in increasing order of difficulty but should also introduce the learner to a small number of samples that are sufficiently dissimilar to the samples that have already been introduced to the learning process."
P16-1043,2016,7 Conclusion,"we studied self-paced learning, an approach for curriculum learning that expresses the difficulty of a data sample in terms of the value of the objective function and builds the curriculum via a joint optimization framework."
P16-1044,2016,6 Conclusion,"all proposed models are departed from a basic architecture, built on bidirectional lstms."
P16-1044,2016,6 Conclusion,"first, we develop two hybrid models which combine the strength of both recurrent and convolutional neural networks."
P16-1044,2016,6 Conclusion,"in this paper, we address the following problem for the answer passage selection: how can we construct the embeddings for questions and candidate answers, in order to better distinguish the correct answers from other candidates?"
P16-1044,2016,6 Conclusion,"potential future work include: 1) evaluating the proposed approaches for different tasks, such as community qa and textual entailment; 2) including the sentential attention mechanism; 3) integrating the hybrid and the attentive mechanisms into a single framework."
P16-1044,2016,6 Conclusion,"second, we introduce a simple oneway attention mechanism, in order to generate answer embeddings influenced by the question context."
P16-1044,2016,6 Conclusion,such attention fixes the issue of independent generation of the question and answer embeddings in previous work.
P16-1044,2016,6 Conclusion,"we conduct experiments on insuranceqa and trec-qa datasets, and the experimental results demonstrate that the proposed models outperform a variety of strong baselines."
P16-1044,2016,6 Conclusion,we propose three independent models in two directions.
P16-1045,2016,7 Conclusions,a central research question of this paper was the trade-off between the degree of structure in a knowledge base and its ability to be harvested or reasoned with.
P16-1045,2016,7 Conclusions,on three benchmark evaluation sets our consistently and significantly better scores over an unstructured and a highly-structured baseline strongly suggest that tables can be considered a balanced compromise in this trade-off.
P16-1045,2016,7 Conclusions,"we also showed that our model is able to generalize from content to structure, thus reasoning about questions whose answer may not even be contained in the knowledge base."
P16-1045,2016,7 Conclusions,"we are releasing our dataset of more than 9000 mcqs and their alignment information, to the research community."
P16-1045,2016,7 Conclusions,"we believe it offers interesting challenges that go beyond the scope of this paper – such as question parsing, or textual entailment – and are exciting avenues for future research."
P16-1045,2016,7 Conclusions,"we explored a connected framework in which tables are first used to guide the creation of mcq data with alignment information to table elements, then jointly with this data are used in a feature-driven model to answer unseen mcqs."
P16-1045,2016,7 Conclusions,we have presented tables as knowledge bases for question answering.
P16-1046,2016,7 Conclusions,"a third direction would be to adopt an information theoretic perspective and devise a purely unsupervised approach that selects summary sentences and words so as to minimize information loss, a task possibly achievable with the dataset created in this work."
P16-1046,2016,7 Conclusions,in this work we presented a data-driven summarization framework based on an encoder-extractor architecture.
P16-1046,2016,7 Conclusions,it would also be interesting to apply the neural models presented here in a phrase-based setting similar to lebret et al.(2015).
P16-1046,2016,7 Conclusions,"one way to improve the word-based model would be to take structural information into account during generation, e.g., by combining it with a tree-based algorithm (cohn and lapata, 2009)."
P16-1046,2016,7 Conclusions,our models can be trained on large scale datasets and learn informativeness features based on continuous representations without recourse to linguistic annotations.
P16-1046,2016,7 Conclusions,"the later effectively enables us to sidestep the difficulties of generating under a large vocabulary, essentially covering the entire dataset, with many low-frequency words and named entities."
P16-1046,2016,7 Conclusions,two important ideas behind our work are the creation of hierarchical neural structures that reflect the nature of the summarization task and generation by extraction.
P16-1046,2016,7 Conclusions,we developed two classes of models based on sentence and word extraction.
P16-1047,2016,6 Conclusion and Future Work,can we transfer our model to other languages?
P16-1047,2016,6 Conclusion and Future Work,future work can be directed towards answering two main questions: can we improve the performance of our classifier?
P16-1047,2016,6 Conclusion and Future Work,"in doing so we offer a detailed analysis of its performance on data of different genre and containing different types of negation, also in comparison with previous classifiers, and found that non-english specific continuous representation can perform batter than or on par with more fine-grained structural features."
P16-1047,2016,6 Conclusion and Future Work,"in this work, we investigated and confirmed that neural networks sequence-to-sequence models are a valid alternative for the task of detecting the scope of negation."
P16-1047,2016,6 Conclusion and Future Work,"most importantly, we are going to test the model using word-embedding features extracted from a bilingual embedding space."
P16-1047,2016,6 Conclusion and Future Work,"to do this, we are going to explore whether adding language-independent structural information (e.g.universal dependency information) can help the performance on exact scope matching."
P16-1048,2016,5 Conclusion,"by inducing concept information, the proposed conceptual sentence embedding maintains and enhances the semantic information of sentence embedding."
P16-1048,2016,5 Conclusion,"furthermore, we extend the proposed models by introducing attention model, which allows it to consider contextual words within the window in a non-uniform way while maintaining the efficiency."
P16-1048,2016,5 Conclusion,"the experimental results demonstrate that the proposed method performs the best and shows improvement over the compared methods, especially for short-texts."
P16-1048,2016,5 Conclusion,"we compare them with different algorithms, including bag-of-word models, topic model-based model and other state-of-the-art sentence embedding models."
P16-1049,2016,8 Conclusion,this paper presents a response retrieval method for chatbot engines based on unstructured documents.
P16-1049,2016,8 Conclusion,"we evaluate our method on both question answering and chatbot scenarios, and obtain promising results."
P16-1049,2016,8 Conclusion,we leave better triggering component and multiple rounds of conversation handling to be addressed in our future work.
P16-1050,2016,7 Discussion,"linguistic alignment is a prominent type of communicative accommodation, but its sources are unclear."
P16-1050,2016,7 Discussion,the effect of discourse acts on alignment further suggests that alignment is not a completely automatic process but rather one of many discourse strategies that speakers use to achieve their conversational goals.
P16-1050,2016,7 Discussion,this combination of a primarily-lexical origin for linguistic alignment and its variation by word category and discourse act suggest that alignment is primarily a higher-level discourse strategy rather than a low-level priming-based mechanism.
P16-1050,2016,7 Discussion,"this set of results is consistent with both accommodation theory and the set of findings, reviewed above, that sociological factors affect the level of observed alignment."
P16-1050,2016,7 Discussion,"using this model, we find evidence that linguistic alignment is primarily lexical, and that it is strongly affected by at least some aspects of the discourse goal of a message."
P16-1050,2016,7 Discussion,"we presented wham, a length-robust extension of a probabilistic alignment model."
P16-1051,2016,6 Conclusion,"a model of this phenomenon may provide explanations from the perspectives of information exchange, common ground building, and the convergence of linguistic behaviors in general."
P16-1051,2016,6 Conclusion,"as much dialogue happens for the purpose of information exchange, loosely defined, it makes sense to apply information-theoretic models to the semantics as well as the form of speaker’s messages."
P16-1051,2016,6 Conclusion,"besides the results that are consistent with previous findings on written text, we find new entropy change patterns unique to dialogue."
P16-1051,2016,6 Conclusion,"furthermore, our approach actually provides a unified perspective of dialogue that combines grounding theory (clark and brennan, 1991) and interactive alignment (pickering and garrod, 2004)."
P16-1051,2016,6 Conclusion,"in this study, we validate the principle of entropy rate constancy in spoken dialogue, using two common corpora."
P16-1051,2016,6 Conclusion,speakers that actively initiate a new topic tend to use language with higher entropy compared to the language of those who passively respond to the topic shift.
P16-1051,2016,6 Conclusion,the entropy measure of information content quantifies interlocutors’ contributions to common ground and also allows us to show convergence patterns.
P16-1051,2016,6 Conclusion,"the quantitative approach taken here augments rather than supplants speech acts (searle, 1976), identifying who leads the dialogic process by introducing topics and shifting them."
P16-1051,2016,6 Conclusion,"there is, of course, no reason to think that multi-party dialogue should work differently; we leave the empirical examination as an open task."
P16-1051,2016,6 Conclusion,"these two models are often described as opposite; by applying each theory to the dialogic structure between and within topic episodes, we find both of them can explain our findings."
P16-1051,2016,6 Conclusion,these two speaker’s respective entropy levels converge as the topic develops.
P16-1051,2016,6 Conclusion,this unified information-theoretic perspective may eventually allow us to identify further systematic patterns of information exchange between dialogue participants.
P16-1051,2016,6 Conclusion,"with this, we put forward what we think is a new perspective to analyzing dialogue."
P16-1052,2016,5 Conclusion,"despite of the overall presence of dialogues with such mixed motives in everyday life, little attention has been given to this topic in text planning in contrast to scrutinized dialogue systems that support dialogues fully aligned with anticipated user interests."
P16-1052,2016,5 Conclusion,"due to the fact that fairness is subjective per se, we presented results from an empirical study (n=107) in which human subjects interacted with the qa system in various mixed motive settings."
P16-1052,2016,5 Conclusion,"focusing on question-answering (qa) settings, we introduced a model that formalizes answer planning as psychological game embedded in text planning approaches for supporting fair dialogues under mixed motives."
P16-1052,2016,5 Conclusion,results indicate a positive evaluation of the systems performance in planning answers that support fair dialogues despite of mixed motives of interlocutors.
P16-1052,2016,5 Conclusion,the model was exemplified within a qa sales assistant with domain-specific world knowledge for conducting sales dialogues.
P16-1052,2016,5 Conclusion,"we considered dialogues with congruent as well as incongruent interlocutor motives, where dialogue partners are facing the constant challenge of finding a balance between cooperation and competition."
P16-1053,2016,5 Conclusion and Future Work,"experiments show that the proposed interaction mechanism is effective, and we obtain significant improvements on answer selection and dialogue act analysis without any handcrafted features."
P16-1053,2016,5 Conclusion and Future Work,"in this work, we propose sentence interaction network (sin) which utilizes a new mechanism for modeling interactions between two sentences."
P16-1053,2016,5 Conclusion and Future Work,"moreover, applying the models to other tasks, such as semantic relatedness measurement and paraphrase identification, would also be interesting attempts."
P16-1053,2016,5 Conclusion and Future Work,previous works have showed that it is important to utilize the syntactic structures for modeling sentences.
P16-1053,2016,5 Conclusion and Future Work,sin is powerful and flexible to model sentence interactions for different tasks.
P16-1053,2016,5 Conclusion and Future Work,"so, we are going to extend sin to tree-based sin for sentence modeling as future work."
P16-1053,2016,5 Conclusion and Future Work,we also find out that lstm is sometimes unable to model complex phrases.
P16-1053,2016,5 Conclusion and Future Work,we also introduce a convolution layer into sin (sinconv) to improve its phrase modeling ability so that phrase interactions can be handled.
P16-1056,2016,6 Conclusion,"finally, we use our best performing neural network model to generate a corpus of 30m question and answer pairs, which we hope will enable future researchers to improve their question answering systems."
P16-1056,2016,6 Conclusion,"the neural networks combine ideas from recent neural network architectures for statistical machine translation, as well as multi-relational knowledge base embeddings for overcoming sparsity issues and placeholder techniques for handling rare words."
P16-1056,2016,6 Conclusion,"the produced question and answer pairs are evaluated using automatic evaluation metrics, including bleu, meteor and sentence similarity, and are found to outperform a template-based baseline model."
P16-1056,2016,6 Conclusion,we propose new neural network models for mapping knowledge base facts into corresponding natural language questions.
P16-1056,2016,6 Conclusion,"when evaluated by untrained human subjects, the question and answer pairs produced by our best performing neural network appears to be comparable in quality to real human-generated questions."
P16-1057,2016,9 Conclusion,"along with other extensions, namely structured attention and code compression, our model is applied on on both existing datasets and also on a newly created one with implementations of tcg game cards."
P16-1057,2016,9 Conclusion,"our experiments show that our model out-performs multiple benchmarks, which demonstrate the importance of combining different types of predictors."
P16-1057,2016,9 Conclusion,"under this architecture, we propose a generative model for code generation that combines a character level softmax to generate language-specific tokens and multiple pointer networks to copy keywords from the input."
P16-1057,2016,9 Conclusion,"we introduced a neural network architecture named latent prediction network, which allows efficient marginalization over multiple predictors."
P16-1058,2016,5 Discussion and Conclusion,"an obvious direction for future work is to automatically induce such a strategy, based on confidence measures that automatically predict the trust-worthiness of a word for an object."
P16-1058,2016,5 Discussion and Conclusion,"another extension that we have planned for future work is to implement relational expressions, similar to (kennington and schlangen, 2015)."
P16-1058,2016,5 Discussion and Conclusion,"based on relational expressions, we will be able to generate reformulations and installments tailored to the interaction with the user."
P16-1058,2016,5 Discussion and Conclusion,"for instance, a very natural option for installments is to relate the wrong target object clicked on by the user to the intended target, e.g.something like to the left of that one, the bigger object."
P16-1058,2016,5 Discussion and Conclusion,"in order to achieves this, we have augmented our approach with some manually designed installment strategies."
P16-1058,2016,5 Discussion and Conclusion,this knowledge-lean approach allows us to use automatically learned convnet features and obtain a promising baseline that predicts semantically appropriate words based on visual object representations.
P16-1058,2016,5 Discussion and Conclusion,"we have argued and demonstrated that reg in more realistic settings greatly benefits from a taskoriented, interactive account and should explore principled strategies for repairing and avoiding misunderstandings due to semantically inaccurate res."
P16-1058,2016,5 Discussion and Conclusion,"we have presented an reg system that approaches the task as a word selection problem and circumvents manual specification of attributes in symbolic scene representations as required in traditional reg (krahmer and van deemter, 2012), or manual specification of attribute-specific functions that map particular low-level visual features to attributes or words as in (roy, 2002; kazemzadeh et al., 2014)."
P16-1059,2016,7 Conclusion,"deep learning has recently been used in mutliple nlp applications, including parsing (chen and manning, 2014) and translation (bahdanau et al., 2014)."
P16-1059,2016,7 Conclusion,experiments in varying the size of the attention beam k in the star-shaped model suggest that multi-focus attention is beneficial.
P16-1059,2016,7 Conclusion,"finally, one may consider a more elaborate model in which attention depends on the current state of the system; for example, the state can summarize the mention context."
P16-1059,2016,7 Conclusion,"however, the simplicity of the star-shaped model, its empirical effectiveness, and ease of learning parameters make it an attractive approach for easily incorporating attention into existing resolution models."
P16-1059,2016,7 Conclusion,"in conclusion, we have shown that attention is an effective mechanism for improving entity resolution models, and that it can be implemented via a simple inference mechanism, where model parameters can be easily learned."
P16-1059,2016,7 Conclusion,"it is of course possible to extend the global single-link model to the multi-focus case, by modifying the model factors and resulting messages."
P16-1059,2016,7 Conclusion,learning the local and pairwise scores in our model using a deep architecture rather than a linear model would likely lead to performance improvements.
P16-1059,2016,7 Conclusion,our empirical results show that the methods results in significant performance gains across several benchmarks.
P16-1059,2016,7 Conclusion,"the dynamics of the underlying state can be modeled by recurrent neural networks or lstms (bahdanau et al., 2014)."
P16-1059,2016,7 Conclusion,"the model can also readily be applied to other structured prediction problems in language processing, such as selecting antecedents in coreference resolution."
P16-1059,2016,7 Conclusion,"the star-shaped model is particularly amenable to this architecture, as it can be implemented via a feed-forward sequence of operations (including sorting, which can be implemented with soft-max gates)."
P16-1059,2016,7 Conclusion,"we explored two approaches to attention: a multi-focus attention model with tractable inference decomposed over mentions, and a single-focus model with global inference implemented using belief propagation."
P16-1059,2016,7 Conclusion,"we have described an attention-based approach to collective entity resolution, motivated by the observation that a non-salient entity in a long document may only have relations to a small subset of other entities."
P16-1060,2016,8 Conclusions,averaging unreliable scores does not result in a reliable one.
P16-1060,2016,8 Conclusions,current coreference resolution evaluation metrics have flaws which make them unreliable for comparing coreference resolvers.
P16-1060,2016,8 Conclusions,"indeed, recall and precision comparisons of coreference resolvers are not possible based on an average score."
P16-1060,2016,8 Conclusions,it can be easily adapted for entity evaluation in different domains or applications in which entities with various attributes are of different importance.
P16-1060,2016,8 Conclusions,lea is a simple intuitive metric that overcomes the drawbacks of the current metrics.
P16-1060,2016,8 Conclusions,the current solution is to use an average value of different metrics for comparisons.
P16-1060,2016,8 Conclusions,"the only metric that is resistant to the mention identification effect is the least discriminative one, i.e.muc."
P16-1060,2016,8 Conclusions,there is also a low agreement between the rankings of different metrics.
P16-1060,2016,8 Conclusions,"we first report the mention identification effect on b 3 , ceaf and blanc which causes these metrics to report misleading values."
P16-1060,2016,8 Conclusions,"we introduce lea, the link-based entity-aware metric, as a new evaluation metric for coreference resolution."
P16-1061,2016,8 Conclusion,the model is trained with a learning-to-search algorithm that allows it to learn how local decisions will affect the final coreference score.
P16-1061,2016,8 Conclusion,"these learned, dense, high-dimensional feature vectors provide our cluster-ranking coreference model with a strong ability to distinguish beneficial cluster merges from harmful ones."
P16-1061,2016,8 Conclusion,we evaluate our system on the english and chinese portions of the conll 2012 shared task and report a substantial improvement over the current state-of-the-art.
P16-1061,2016,8 Conclusion,we have presented a coreference system that captures entity-level information with distributed representations of coreference cluster pairs.
P16-1062,2016,7 Conclusions and Future Work,"a pilot effort to use word embeddings to alter the variety of vocabulary in a dataset has so far not succeeded, but future experiments altering vocabulary width or modularity of a dataset and finding that the modified dataset behaved like natural datasets with the same properties could increase confidence in causality."
P16-1062,2016,7 Conclusions and Future Work,"even for a creative dataset, if the underlying data is tightly clustered, the use of semantics-based similarity measures can actually hurt performance."
P16-1062,2016,7 Conclusions and Future Work,"future work can also explore finer clusters within these datasets, such as clustering clue by word sense of the answers and toon by joke sense."
P16-1062,2016,7 Conclusions and Future Work,future work can manipulate datasets’ text properties to confirm that a specific property is the cause of observed differences in clustering.
P16-1062,2016,7 Conclusions and Future Work,"future work will explore further how the goals of short text authors translate into measurable properties of the texts they write, and how measuring those properties can help predict which similarity metrics and clustering methods will combine to provide the best performance."
P16-1062,2016,7 Conclusions and Future Work,"since traditional similarity metrics are often faster to calculate, use of slower semantics-based methods should be limited to creative datasets."
P16-1062,2016,7 Conclusions and Future Work,such work should alter the datasets ttr and nttr while holding mean length of texts constant.
P16-1062,2016,7 Conclusions and Future Work,these results are a first step towards determining the best way to cluster a new dataset based on properties of the text.
P16-1062,2016,7 Conclusions and Future Work,this work has shown that creativity can influence the best way to cluster text.
P16-1062,2016,7 Conclusions and Future Work,"traditional metrics applied to such tightly clustered data generate more modular output that enables the use of sophisticated, graph-based clustering methods such as mcl and louvain."
P16-1062,2016,7 Conclusions and Future Work,"unlike most work on clustering short texts, we examined how the similarity metric interacts with the clustering method."
P16-1062,2016,7 Conclusions and Future Work,we also showed that semanticsbased methods do not provide a notable advantage when applying k-means to less creative datasets.
P16-1062,2016,7 Conclusions and Future Work,"when either the underlying data or the similarity metrics applied to it produce loose clusters with low modularity, the sophisticated graph clustering algorithms fail, and we must fall back on simpler methods."
P16-1062,2016,7 Conclusions and Future Work,"when using k-means to cluster a dataset where authors tried to be creative, similarity metrics utilizing distrbutional semantics outperformed those that relied on surface forms."
P16-1063,2016,8 Conclusions and Future Work,"efficient algorithms for this task have been proposed (kusner et al., 2015)."
P16-1063,2016,8 Conclusions and Future Work,"experiments show that topicvec can learn high-quality document representations, even given only one document."
P16-1063,2016,8 Conclusions and Future Work,"however, jointly representing a document by topic proportions and topic embeddings would be more accurate."
P16-1063,2016,8 Conclusions and Future Work,in our classification tasks we only explored the use of topic proportions of a document as its representation.
P16-1063,2016,8 Conclusions and Future Work,"in this paper, we proposed topicvec, a generative model combining word embedding and lda, with the aim of exploiting the word collocation patterns both at the level of the local context and the global document."
P16-1063,2016,8 Conclusions and Future Work,"our method has potential applications in various scenarios, such as document retrieval, classification, clustering and summarization."
P16-1064,2016,6 Conclusions,"to tackle the task, we develop a new model called mcta which can cope with the language gap and extract coherent culturalcommon topics from multilingual news reader comments."
P16-1064,2016,6 Conclusions,we also develop a partially collapsed gibbs sampler which incorporates the term translation relationship into the detection of culturalcommon topics effectively for model parameter learning.
P16-1064,2016,6 Conclusions,we investigate the task of cultural-common discussion topic detection from multilingual news reader comments.
P16-1065,2016,7 Conclusions and Future Work,"a separate dirichlet prior for each block captures its topic preferences, serving as an informed prior when inferring documents’ topic distributions."
P16-1065,2016,7 Conclusions and Future Work,"as next steps, we plan to explore model variations to support a wider range of use cases."
P16-1065,2016,7 Conclusions and Future Work,"for example, although we have presented a version of the model defined using undirected binary weight edges in the experiment, it would be straightforward to adapt to model both directed/undirected and binary/nonnegative real weight edges."
P16-1065,2016,7 Conclusions and Future Work,"in the spirit of treating links probabilistically, we plan to explore application of the model in suggesting links that do not exist but should, for example in discovering missed citations, marking social dynamics (nguyen et al., 2014), and identifying topically related content in multilingual networks of documents (hu et al., 2014)."
P16-1065,2016,7 Conclusions and Future Work,"lbh-rtm yields topics with better coherence, though not all techniques contribute to the improvement."
P16-1065,2016,7 Conclusions and Future Work,maxmargin learning learns to predict links from documents’ topic and word distributions and block assignments.
P16-1065,2016,7 Conclusions and Future Work,"our model better captures the connections and content of paper abstracts, as measured by predictive link rank and topic quality."
P16-1065,2016,7 Conclusions and Future Work,"we are also interested in modeling changing topics and vocabularies (blei and lafferty, 2006; zhai and boyd-graber, 2013)."
P16-1065,2016,7 Conclusions and Future Work,"we introduce lbh-rtm, a discriminative topic model that jointly models topics and document links, detecting blocks in the document network probabilistically by embedding the weighted stochastic block model, rather via connectedcomponents as in previous models."
P16-1065,2016,7 Conclusions and Future Work,"we support our quantitative results with qualitative analysis looking at a pair of example documents and at a pair of blocks, highlighting the robustness of embedded wsbm over blocks defined as scc."
P16-1066,2016,7 Conclusion,"however, there is much room for improvement given the simple method used in evaluation."
P16-1066,2016,7 Conclusion,"in this paper, two large-scale arabic sentiment lexicons were generated from a large dataset of arabic tweets."
P16-1066,2016,7 Conclusion,"moreover, their high coverage suggests the possibility of using them in different genres such as product reviews."
P16-1066,2016,7 Conclusion,the performance of the lexicons on external datasets also suggests their ability to be used in classifying new datasets.
P16-1066,2016,7 Conclusion,the significance of these lexicons lies in their ability to capture the idiosyncratic nature of social media text.
P16-1066,2016,7 Conclusion,this is a possible future research direction.
P16-1066,2016,7 Conclusion,this simple lexicon-based method could be further enhanced by incorporating arabic valence shifters and certain linguistic rules to handle them.
P16-1066,2016,7 Conclusion,"we also plan to make the classification multi-way: positive, negative, neutral and mixed."
P16-1067,2016,5 Conclusions,an application of the proposed approach on authorship attribution has also achieved perfect results of 100% accuracies together with confidence measurement for the first time.
P16-1067,2016,5 Conclusions,an unsupervised learning method has also been developed to estimate the initial parameter values of hmm.
P16-1067,2016,5 Conclusions,comparative experiments conducted on benchmark datasets have demonstrated the effectiveness of our ideas with superior performance achieved on both artificial and authentic documents.
P16-1067,2016,5 Conclusions,"different from the stateof-the-art approaches, we have innovatively made use of the sequential information hidden among document elements."
P16-1067,2016,5 Conclusions,"for this purpose, we have used hmm and constructed a sequential probabilistic model, which is used to find the best sequence of authors that represents the sentences of the document."
P16-1067,2016,5 Conclusions,we have developed an unsupervised approach for decomposing a multi-author document based on authorship.
P16-1068,2016,7 Conclusion,"in this paper, we introduced a deep neural network model capable of representing both local contextual and usage information as encapsulated by essay scoring."
P16-1068,2016,7 Conclusion,this model yields score-specific word embeddings used later by a recurrent neural network in order to form essay representations.
P16-1068,2016,7 Conclusion,"we also introduced a novel way of exploring the basis of the network’s internal scoring criteria, and showed that such models are interpretable and can be further exploited to provide useful feedback to the author."
P16-1068,2016,7 Conclusion,"we have shown that this kind of architecture is able to surpass similar state-of-the-art systems, as well as systems based on manual feature engineering which have achieved results close to the upper bound in past work."
P16-1069,2016,7 Conclusion,"in this paper, we address a semantic parsing task, namely translating sentences to if-then statements."
P16-1069,2016,7 Conclusion,we achieve a new state-ofthe-art with 8% absolute accuracy improvement.
P16-1069,2016,7 Conclusion,"we also discussed various ways to improve generalization and reduce overfitting, including adding synthetic training data by paraphrasing sentences, using multiple grammars, applying feature selection and ensembling multiple systems."
P16-1069,2016,7 Conclusion,"we model the task as structure prediction, and show improved models using both neural networks and logistic regression."
P16-1070,2016,9 Conclusion,"finally, we benchmark automatic tagging and parsing on our corpus, and measure the effect of grammatical errors on tagging and parsing quality."
P16-1070,2016,9 Conclusion,the annotation is accompanied by a linguistically motivated framework for handling syntactic structures associated with grammatical errors.
P16-1070,2016,9 Conclusion,"the treebank will support empirical study of learner syntax in nlp, corpus linguistics and second language acquisition."
P16-1070,2016,9 Conclusion,"we present the first large scale treebank of learner language, manually annotated and doublereviewed for pos tags and universal dependencies."
P16-1071,2016,8 Conclusion,cognitive psychologists have debated whether grammatical differences lead to different brain activation patterns.
P16-1071,2016,8 Conclusion,"somewhat surprisingly, we find that the fmri data contains a strong signal, enabling a 4% error reduction over a state-of-the-art unsupervised pos tagger."
P16-1071,2016,8 Conclusion,this paper presents the first experiments inducing part of speech from fmri reading data.
P16-1071,2016,8 Conclusion,"while our approach may not be readily applicable for developing nlp models today, we believe that the presented results may inspire nlp researchers to consider learning models from combinations of linguistic resources and auxiliary, behavioral data that reflects human cognition."
P16-1072,2016,5 Conclusion,"a significant improvement is observed when brcnn is used, outperforming state-of-the-art methods."
P16-1072,2016,5 Conclusion,"in this paper, we proposed a novel bidirectional neural network brcnn, to improve the performance of relation classification."
P16-1072,2016,5 Conclusion,information of words and dependency relations are used utilizing a two-channel recurrent neural network with lstm units.
P16-1072,2016,5 Conclusion,"rcnn achieves a better performance at learning features along the shortest dependency path, compared with some common neural networks."
P16-1072,2016,5 Conclusion,"the brcnn model, consisting of two rcnns, learns features along sdp and inversely at the same time."
P16-1072,2016,5 Conclusion,the features of dependency units in sdp are extracted by a convolution layer.
P16-1072,2016,5 Conclusion,we demonstrate the effectiveness of our model by evaluating the model on semeval-2010 relation classification task.
P16-1073,2016,6 Conclusions,"currently, our approach only leverages simple sentence rewriting methods."
P16-1073,2016,6 Conclusions,"furthermore, we also want to employ sentence rewriting techniques for other challenges in semantic parsing, such as the spontaneous, unedited natural language input, etc."
P16-1073,2016,6 Conclusions,"in future work, we will explore more advanced sentence rewriting methods."
P16-1073,2016,6 Conclusions,"in this paper, we present a novel semantic parsing method, which can effectively deal with the mismatch between natural language and target ontology using sentence rewriting."
P16-1073,2016,6 Conclusions,the resulting system significantly outperforms the base system on the webquestions dataset.
P16-1073,2016,6 Conclusions,"then we present two sentence rewriting methods, dictionary-based method for 1-n mismatch and template-based method for n-1 mismatch."
P16-1073,2016,6 Conclusions,"we resolve two common types of mismatch (i) one word in natural language sentence vs one compound formula in target ontology (1-n), (ii) one complicated expression in natural language sentence vs one formula in target ontology (n-1)."
P16-1074,2016,5 Conclusions,"to our knowledge, this is the first neural networkbased approach to zero pronoun resolution."
P16-1074,2016,5 Conclusions,we proposed an embedding matching approach to zero pronoun resolution based on deep networks.
P16-1074,2016,5 Conclusions,"when evaluated on the chinese portion of the ontonotes corpus, our approach achieved state-of-the-art results."
P16-1075,2016,9 Discussion and Conclusion,"another avenue of potential research is to use multi-task learning to predict scores for different aspects of text quality (e.g.coherence, grammaticality, topicality)."
P16-1075,2016,9 Discussion and Conclusion,"as shown in section 7.2, our approach is robust to increases in the number of tasks, meaning that one can freely add extra data when available and expect the approach to use this data appropriately."
P16-1075,2016,9 Discussion and Conclusion,"furthermore, it was concluded in previous work that at least some target-task training data is necessary to build high performing aes systems."
P16-1075,2016,9 Discussion and Conclusion,"future work will aim to study different dimensions of the prompt (e.g.genre, topic) using multitask learning at a finer level."
P16-1075,2016,9 Discussion and Conclusion,"however, as seen in table 3, high performance rankers (ρ) can be built without any target-task data."
P16-1075,2016,9 Discussion and Conclusion,"nevertheless, it is worth noting that without any target-data, accurately predicting the actual score (high κ) is extremely difficult."
P16-1075,2016,9 Discussion and Conclusion,"the main approach adopted in this paper is quite similar to using svmrank (joachims, 2002) while encoding the prompt id as the qid."
P16-1075,2016,9 Discussion and Conclusion,"therefore, although some extra information (i.e.the expected distribution of gold scores) would need to be used to produce accurate scores with a high quality ranker, the ranking is still useful for assessment in a number of scenarios (e.g.grading on a curve where the distribution of student scores is predefined)."
P16-1075,2016,9 Discussion and Conclusion,"this constrained multi-task preferenceranking approach is likely to be useful for many applications of multi-task learning, when the goldscores across tasks are not directly comparable."
P16-1075,2016,9 Discussion and Conclusion,this is because our approach uses information from multiple tasks without directly relying on the comparability of gold scores across tasks.
P16-1075,2016,9 Discussion and Conclusion,ultimately these complementary techniques (multi-task learning and constrained pairwise preference-ranking) allow essay scoring data from any source to be included during training.
P16-1075,2016,9 Discussion and Conclusion,"unlike previous work (phandi et al., 2015) we have shown, for the first time, that mtl outperforms an approach of simply using source task data as extra training data."
P16-1075,2016,9 Discussion and Conclusion,we also aim to further study the characteristics of the multi-task model in order to determine which features transfer well across tasks.
P16-1075,2016,9 Discussion and Conclusion,"when combined with a multi-task learning technique this allows the preference-ranking algorithm to learn both task-specific and shared-representations in a theoretically sound manner (i.e.without making any speculative assumptions about the relative orderings of essays that were graded on different scales using different marking criteria), and is general enough to be used in many situations."
P16-1076,2016,7 Conclusion,"compared with multiple baselines across three aspects, our method achieves the state-of-the-art accuracy on a 108k question dataset, the largest publicly available one."
P16-1076,2016,7 Conclusion,future work could be extending the proposed method to handle more complex questions.
P16-1076,2016,7 Conclusion,"in this paper, we propose cfo, a novel approach to single-fact question answering."
P16-1076,2016,7 Conclusion,"our focused pruning largely reduces the candidate space without loss of recall rate, leading to significant improvement of overall accuracy."
P16-1076,2016,7 Conclusion,"to resolve the representation for millions of entities, we proposed type-vector scheme which requires no training."
P16-1076,2016,7 Conclusion,we employ a conditional factoid factorization by inferring the target relation first and then the target subject associated with the candidate relations.
P16-1077,2016,6 Conclusion,"our thorough evaluation and analysis yields detailed insights into the semantic characteristics of the inferred classes, and we hope that this allows an informed use of the resulting resource in various semantic nlp tasks."
P16-1077,2016,6 Conclusion,we inferred semantic classes from a large syntactic classification of german ao-selecting verbs based on findings from formal semantics about correspondences between verb syntax and meaning.
P16-1078,2016,7 Conclusion,experimental results on the wat’15 english-to-japanese translation dataset demonstrate that our proposed model achieves the best ribes score and outperforms the sequential attentional nmt model.
P16-1078,2016,7 Conclusion,"in this paper, we propose a novel syntactic approach that extends attentional nmt models."
P16-1078,2016,7 Conclusion,"moreover, the attention mechanism allows the tree-based encoder to align not only the input words but also input phrases with the output words."
P16-1078,2016,7 Conclusion,"our proposed tree-based encoder is a natural extension of the sequential encoder model, where the leaf units of the tree-lstm in the encoder can work together with the original sequential lstm encoder."
P16-1078,2016,7 Conclusion,we focus on the phrase structure of the input sentence and build a tree-based encoder following the parsed tree.
P16-1079,2016,8 Conclusions,"coordination is a frequent and important syntactic phenomena, that pose a great challenge to automatic syntactic annotation."
P16-1079,2016,8 Conclusions,"the new annotation adds structure to the previously flat nps, unifies inconsistencies, fix errors, and marks the role of different participants in the coordination structure with respect to the coordination."
P16-1079,2016,8 Conclusions,this resource is a necessary first step towards better disambiguation of coordination structures in syntactic parsers.
P16-1079,2016,8 Conclusions,"unfortunately, the current state of coordination annotation in the ptb is lacking."
P16-1079,2016,8 Conclusions,we make our annotation available to the nlp community.
P16-1079,2016,8 Conclusions,we present a version of the ptb with improved annotation for coordination structure.
P16-1080,2016,8 Conclusions,annotators generally exaggerated the diagnostic utility of behaviors that they correctly associated with one group or another.
P16-1080,2016,8 Conclusions,"another avenue of future research can look at the annotators’ own traits and how these relate to perception (flekova et al., 2015)."
P16-1080,2016,8 Conclusions,correlation analysis showed that aspects of stereotypes associated with errors tended not to be completely wrong but rather poorly applied.
P16-1080,2016,8 Conclusions,"follow-up studies can analyze the perception of other user traits such as education level, race or political orientation."
P16-1080,2016,8 Conclusions,"further, we used the same methodology to analyze self-reported confidence."
P16-1080,2016,8 Conclusions,"however, we have demonstrated that humans use stereotypes which lead to systematic biases by comparing their guesses to predictions from statistical models using the bag-ofwords assumption."
P16-1080,2016,8 Conclusions,our experimental design allowed us to directly test which textual cues lead to inaccurate assessments.
P16-1080,2016,8 Conclusions,"overall, participants were generally accurate in guessing a person’s traits supporting earlier research that stereotypical associations are frequently accurate (mccauley, 1995)."
P16-1080,2016,8 Conclusions,this is particularly useful in offering task requesters a prior over which annotators are expected to perform tasks better.
P16-1080,2016,8 Conclusions,"this is the first study to systematically analyze differences between real user traits and traits as perceived from text, here twitter posts."
P16-1080,2016,8 Conclusions,this would allow to uncover demographic or psychological traits that influence the ability to make more accurate judgements.
P16-1080,2016,8 Conclusions,"while qualitatively different, these predictions were shown to offer complimentary information in case of gender, boosting overall accuracy when used jointly."
P16-1081,2016,5 Conclusions,extensive experiments on two large review collections from amazon and yelp confirmed the effectiveness of our proposed solution.
P16-1081,2016,5 Conclusions,"in addition, this method can also be applied to domain adaptation, where a domain taxonomy enables a hierarchically shared model adaptation."
P16-1081,2016,5 Conclusions,"in the proposed mt-linadapt algorithm, global model sharing alleviates data sparsity issue, and individualized model adaptation captures the heterogeneity in humans’ sentiments and enables efficient online model learning."
P16-1081,2016,5 Conclusions,"in this work, we proposed to perform personalized sentiment classification based on the notion of shared model adaptation, which is motivated by the social theories that humans’ opinions are diverse but shaped by the ever-changing social norms."
P16-1081,2016,5 Conclusions,"the adaptation can be performed at the user group level, i.e., three-level model adaptation."
P16-1081,2016,5 Conclusions,the idea of shared model adaptation is general and can be further extended.
P16-1081,2016,5 Conclusions,the user groups can be automatically identified to maximize the effectiveness of shared model adaptation.
P16-1081,2016,5 Conclusions,we currently used a two-level model adaptation scheme.
P16-1082,2016,7 Conclusions,problems such as reading list generation require a representation of the structure of the content of a scientific corpus.
P16-1082,2016,7 Conclusions,"the most important link in the graph is the concept dependency relation, which indicates that one concept helps a learner to understand another, e.g., markov logic networks depends on probability."
P16-1082,2016,7 Conclusions,"we also present baselines that compute the similarity of the word distributions associated with each concept, the likelihood of a citation connecting the concepts, and a hierarchical clustering approach."
P16-1082,2016,7 Conclusions,we are releasing human annotations of concept nodes and possible dependency edges learned from the acl anthology as well as implementations of the methods described in this paper to enable future research on modeling scientific corpora.
P16-1082,2016,7 Conclusions,we have presented four approaches to predicting these relations.
P16-1082,2016,7 Conclusions,"we have proposed the concept graph framework, which gives weighted links from documents to the concepts they discuss and links concepts to one another."
P16-1082,2016,7 Conclusions,we propose information-theoretic measures based on cross entropy and on information flow.
P16-1082,2016,7 Conclusions,"while word similarity proves a strong baseline, the strongest edges predicted by the crossentropy approach are more precise."
P16-1083,2016,7 Conclusion,"contrary to hsu (2007), we proved that these models can be exactly collapsed into a single backoff language model."
P16-1083,2016,7 Conclusion,"empirically, compiling the log-linear model is faster than srilm can collapse its approximate offline linear model."
P16-1083,2016,7 Conclusion,"in future work, we plan to improve performace of feature weight tuning and investigate more general features."
P16-1083,2016,7 Conclusion,normalized log-linear interpolation is now a tractable alternative to linear interpolation for backoff language models.
P16-1083,2016,7 Conclusion,this solves the query speed problem.
P16-1084,2016,5 Conclusion,interesting and surprising observations are made from the experimental results.
P16-1084,2016,5 Conclusion,"the new dataset is almost one order of magnitude larger than most of previous ones, and has a much higher level of diversity in term of problem types."
P16-1084,2016,5 Conclusion,we have also conducted experiments on our dataset to evaluate state-of-the-art systems.
P16-1084,2016,5 Conclusion,"we have constructed dolphin18k, a large dataset for training and evaluating automatic math word problem solving systems."
P16-1084,2016,5 Conclusion,we reduce human annotation cost by automatically extracting gold answers and equation systems from the unstructured answer text of cqa posts.
P16-1085,2016,7 Conclusions,"as future work, we plan to investigate the possibility of designing word representations that best suit the wsd framework."
P16-1085,2016,7 Conclusions,"first, word embeddings can be used as new features to improve a state-of-the-art supervised wsd that only uses standard features."
P16-1085,2016,7 Conclusions,"however, the best performance is obtained when standard wsd features are augmented with the additional knowledge from word2vec vectors on the basis of a decay function strategy."
P16-1085,2016,7 Conclusions,in this paper we studied different ways of integrating the semantic knowledge of word embeddings in the framework of wsd.
P16-1085,2016,7 Conclusions,our hope is that this work will serve as the first step for further studies on re-designing standard wsd features.
P16-1085,2016,7 Conclusions,"second, integrating embeddings on the basis of an exponential decay strategy proves to be more consistent in producing high performance than the other conventional strategies, such as vector concatenation and centroid."
P16-1085,2016,7 Conclusions,"third, the retrofitted embeddings that take advantage of the knowledge derived from semi-structured resources, when used as the only feature for wsd can outperform stateof-the-art supervised models which use standard wsd features."
P16-1085,2016,7 Conclusions,we carried out a deep analysis of different parameters and strategies across several wsd tasks.
P16-1085,2016,7 Conclusions,we release at https:// github.com/iiacobac/ims_wsd_emb all the codes and resources used in our experiments in order to provide a framework for research on the evaluation of new vsm models in the wsd framework.
P16-1086,2016,7 Conclusion,"an analysis by (chen et al., 2016) suggests that on cnn and daily mail datasets a significant proportion of questions is ambiguous or too difficult to answer even for humans (partly due to entity anonymization) so the ensemble of our models may be very near to the maximal accuracy achievable on these datasets."
P16-1086,2016,7 Conclusion,in this article we presented a new neural network architecture for natural language text comprehension.
P16-1086,2016,7 Conclusion,"while our model is simpler than previously published models, it gives a new state-of-the-art accuracy on all the evaluated datasets."
P16-1087,2016,8 Conclusion,"experimentally, we found that adding sentence-level and relation-level dependencies on the output layer improves the performance on opinion entity extraction, obtaining results within 1-3% of the ilp-based joint model on opinion entities, within 3% for is-from relation and comparable for is-about relation."
P16-1087,2016,8 Conclusion,"in future work, we plan to explore the effects of pre-training (bengio et al., 2009) and scheduled sampling (bengio et al., 2015) for training our lstm network."
P16-1087,2016,8 Conclusion,"in this paper, we explored lstm-based models for the joint extraction of opinion entities and relations."
P16-1087,2016,8 Conclusion,we would also like to explore re-ranking methods for our problem.
P16-1087,2016,8 Conclusion,"with respect to the fine-grained opinion mining task, a potential future direction to be able to model overlapping and embedded entities and relations and also to extend this model to handle cross-sentential relations."
P16-1088,2016,7 Conclusion,"in future work, we will develop lexical features which are captured by nonlocal dependencies."
P16-1088,2016,7 Conclusion,"in the experiment reported in this paper, we used simple features which are captured by nonlocal dependencies."
P16-1088,2016,7 Conclusion,our parser achieves a good balance between constituent parsing and nonlocal dependency identification.
P16-1088,2016,7 Conclusion,this paper proposed a transition-based parser which identifies nonlocal dependencies.
P16-1089,2016,6 Conclusion,"as further analysis on various choices of parameters show that the method is stable across settings, we conclude that siamese cbow provides a robust way of generating high-quality sentence representations."
P16-1089,2016,6 Conclusion,"however, it would be interesting to see how siamese cbow embeddings would affect results in supervised tasks."
P16-1089,2016,6 Conclusion,it is beyond the scope of this paper to provide a comprehensive analysis of all supervised methods using word or sentence embeddings and the effect siamese cbow would have on them.
P16-1089,2016,6 Conclusion,"it predicts, from an input sentence representation, the preceding and following sentence."
P16-1089,2016,6 Conclusion,"it would be interesting to see how embeddings for larger pieces of texts, such as documents, would perform in document clustering or filtering tasks."
P16-1089,2016,6 Conclusion,"lastly, although we evaluated siamese cbow on sentence pairs, there is no theoretical limitation restricting it to sentences."
P16-1089,2016,6 Conclusion,the model is trained using only unlabeled text data.
P16-1089,2016,6 Conclusion,"we evaluated the model on 20 test sets and show that in a majority of cases, 14 out of 20, siamese cbow outperforms a word2vec baseline and a baseline based on the recently proposed skip-thought architecture."
P16-1089,2016,6 Conclusion,"we have presented siamese cbow, a neural network architecture that efficiently learns word embeddings optimized for producing sentence representations."
P16-1089,2016,6 Conclusion,word and sentence embeddings are ubiquitous and many different ways of using them in supervised tasks have been proposed.
P16-1090,2016,6 Discussion and related work,"a natural next step is to explore our framework with additional modeling improvements—especially in dealing with context, structure, and noise."
P16-1090,2016,6 Discussion and related work,"another avenue for providing user confidence is probabilistic calibration (platt, 1999), which has been explored more recently for structured prediction (kuleshov and liang, 2015)."
P16-1090,2016,6 Discussion and related work,another difference is that we operate in a more adversarial setting by leaning on the unanimity principle.
P16-1090,2016,6 Discussion and related work,"for the task of learning semantic mappings, we leveraged the linear algebraic structure in our problem to make unanimous prediction efficient."
P16-1090,2016,6 Discussion and related work,"however, only popescu et al.(2003) focuses on precision."
P16-1090,2016,6 Discussion and related work,"however, these methods do not guarantee precision for any training set and test input."
P16-1090,2016,6 Discussion and related work,"in summary, we have presented the unanimity principle for guaranteeing 100% precision."
P16-1090,2016,6 Discussion and related work,"our “safe set” of inputs appears in the literature as the complement of the disagreement region (hanneke, 2007)."
P16-1090,2016,6 Discussion and related work,"our version space is used in the context of the unanimity principle, and we explore a novel linear algebraic structure."
P16-1090,2016,6 Discussion and related work,our work is motivated by the semantic parsing task (though it can be applied to any set-to-set prediction task).
P16-1090,2016,6 Discussion and related work,"over the last decade, there has been much work on semantic parsing, mostly focusing on learning from weaker supervision (liang et al., 2011; goldwasser et al., 2011; artzi and zettlemoyer, 2011; artzi and zettlemoyer, 2013), scaling up beyond small databases (cai and yates, 2013; berant et al., 2013; pasupat and liang, 2015), and applying semantic parsing to other tasks (matuszek et al., 2012; kushman and barzilay, 2013; artzi and zettlemoyer, 2013)."
P16-1090,2016,6 Discussion and related work,"the idea of computing consistent hypotheses appears in the classic theory of version spaces for binary classification (mitchell, 1977) and has been extended to more structured settings (vanlehn and ball, 1987; lau et al., 2000)."
P16-1090,2016,6 Discussion and related work,"there is classic work on learning classifiers that can abstain (chow, 1970; tortorella, 2000; balsubramani, 2016)."
P16-1090,2016,6 Discussion and related work,"they also obtain 100% precision, but with a hand-crafted system, whereas we learn a semantic mapping."
P16-1090,2016,6 Discussion and related work,"they use this notion for active learning, whereas we use it to support unanimous prediction."
P16-1090,2016,6 Discussion and related work,"this work, however, focuses on the classification setting, whereas we considered more structured output settings (e.g., for semantic parsing)."
P16-1090,2016,6 Discussion and related work,we view our work as a first step in learning reliable semantic parsers.
P16-1091,2016,5 Conclusions,"although the previous multi-topic state tracking task has assumed that the topics should be given as inputs to state trackers, we expect that a joint approach can contribute to both problems by dealing with the bi-directional relationships between them."
P16-1091,2016,5 Conclusions,"convolutional neural networks were proposed to capture the semantic aspects of utterances given at each moment, while recurrent neural networks were intended to incorporate temporal aspects in dialogue histories into tracking models."
P16-1091,2016,5 Conclusions,experimental results showed that the proposed approaches helped to improve the topic tracking performance with respect to the linear baseline models.
P16-1091,2016,5 Conclusions,"firstly, the architectures based on a single convolutional layer and a single bi-directional recurrent layer in the proposed models can be extended by adding more layers as well as utilizing more advanced components including hierarchical cnns (kalchbrenner et al., 2014b) to deal with utterance compositionalities or attention mechanisms (denil et al., 2012) to focus on more important segments in dialogue sequences."
P16-1091,2016,5 Conclusions,"furthering this work, there would be still much room for improvement in future."
P16-1091,2016,5 Conclusions,"however, this work only takes internal dialogue information into account for making decisions."
P16-1091,2016,5 Conclusions,"if we develop a good way of leveraging other useful resources into the neural network architectures, better performance can be expected especially for guide-driven and inter-categorical topic transitions that are considered to be more dependent on background knowledge of the speakers."
P16-1091,2016,5 Conclusions,"secondly, the use of external knowledge could be a key to success in dialogue topic tracking, as proved in the previous studies."
P16-1091,2016,5 Conclusions,the other direction of our future work is to investigate joint models for tracking dialogue topics and states simultaneously.
P16-1091,2016,5 Conclusions,this paper presented various neural network architectures for dialogue topic tracking.
P16-1092,2016,8 Conclusion,our results demonstrate that l1 models improve the coverage of the error detection system on a range of other l1s.
P16-1092,2016,8 Conclusion,"we also investigated whether the semantic models learned from particular l1s are portable to other languages, and in particular to languages that are typologically close to the investigated l1s."
P16-1092,2016,8 Conclusion,"we focused on two typologically different l1s – russian and spanish, and experimentally confirmed the hypothesis that statistical semantic models learned from these l1s significantly improve automatic error detection in l2 data produced by the speakers of the respective l1s."
P16-1092,2016,8 Conclusion,"we have investigated whether lexico-semantic models from the native language are transferred to the second language, and what effect this transfer has on lexical choice in l2."
P16-1093,2016,6 Conclusions,"finally, in a study on the short- and long-term impact on the learners, we showed that difficult items had lower retention rate."
P16-1093,2016,6 Conclusions,"further, we evaluated the extent to which mismatched native language (l1) affects distractor plausibility."
P16-1093,2016,6 Conclusions,"in future work, we plan to conduct larger-scale evaluations to further validate these results, and to apply these methods on other common learner errors."
P16-1093,2016,6 Conclusions,"the items produced jointly by these automatic methods, in both expert and learner evaluations, rivalled the quality of human-authored items."
P16-1093,2016,6 Conclusions,"the method based on learner error statistics yielded the most plausible distractors, followed by the one based on learner revision statistics."
P16-1093,2016,6 Conclusions,"we compared the performance of three methods for distractor generation, including a novel method that exploits learner revision statistics."
P16-1093,2016,6 Conclusions,we found that the preposition usage tested in automatically selected carrier sentences were only slightly less challenging than those crafted by humans.
P16-1093,2016,6 Conclusions,we have presented a computer-assisted language learning (call) system that automatically creates fill-in-the-blank items for prepositions.
P16-1094,2016,7 Conclusions,"although the gains presented by our new models are not spectacular, the systems outperform our baseline seq2seq systems in terms of bleu, perplexity, and human judgments of speaker consistency."
P16-1094,2016,7 Conclusions,"given a sufficiently large training corpus in which a sufficiently rich variety of speakers is represented, this objective does not seem too far-fetched."
P16-1094,2016,7 Conclusions,"in the speakeraddressee model, moreover, the evidence suggests that there is benefit in capturing dyadic interactions."
P16-1094,2016,7 Conclusions,"our ultimate goal is to be able to take the profile of an arbitrary individual whose identity is not known in advance, and generate conversations that accurately emulate that individual’s persona in terms of linguistic response behavior and other salient characteristics."
P16-1094,2016,7 Conclusions,"such a capability will dramatically change the ways in which we interact with dialog agents of all kinds, opening up rich new possibilities for user interfaces."
P16-1094,2016,7 Conclusions,"there are many other dimensions of speaker behavior, such as mood and emotion, that are beyond the scope of the current paper and must be left to future work."
P16-1094,2016,7 Conclusions,"we have demonstrated that by encoding personas in distributed representations, we are able to capture personal characteristics such as speaking style and background information."
P16-1094,2016,7 Conclusions,we have presented two persona-based response generation models for open-domain conversation generation.
P16-1095,2016,7 Conclusion,"although ddrw has already combined supervised and unsupervised learning together, better performance can be expected after introducing well-developed methods."
P16-1095,2016,7 Conclusion,"by simultaneously optimizing embedding and classification objectives, ddrw gains significantly better performances in network classification tasks than baseline methods."
P16-1095,2016,7 Conclusion,ddrw is also naturally an online algorithm and thus easy to parallel.
P16-1095,2016,7 Conclusion,experiments on different real-world datasets represent adequate stability of ddrw.
P16-1095,2016,7 Conclusion,"furthermore, the representations produced by ddrw is both an intermediate variable and a by-product."
P16-1095,2016,7 Conclusion,it would be great if a better form of random walk is found.
P16-1095,2016,7 Conclusion,"literature has represented the good combination of random walk and language models, but this combination may be unsatisfactory for classification."
P16-1095,2016,7 Conclusion,"same as other embedding methods like deepwalk, ddrw can provide wellformed inputs for statistical analyses other than classification tasks."
P16-1095,2016,7 Conclusion,the future work has two main directions.one is semi-supervised learning.
P16-1095,2016,7 Conclusion,the low proportion of labeled vertices is a good platform for semisupervised learning.
P16-1095,2016,7 Conclusion,the other direction is to promote the random walk step.
P16-1095,2016,7 Conclusion,"this paper presents discriminative deep random walk (ddrw), a novel approach for relational multi-class classification on social networks."
P16-1096,2016,7 Conclusions,"from the analysis of the results, we found that while some existing approaches can capture synonyms of words, they could not leverage the semantic meaning of the social media message."
P16-1096,2016,7 Conclusions,"in particular, as social media messages are typically ambiguous, we argue that effective concept normalisation should deal with them at the semantic level."
P16-1096,2016,7 Conclusions,our approaches overcomes this by learning the semantic representation of the social media message before passing it to a classifier to match an appropriate concept.
P16-1096,2016,7 Conclusions,"our experimental results evaluated on three different social media datasets showed that both of our approaches markedly and significantly outperformed several strong baselines, including an existing approach that achieved state-of-the-art performance on several medical concept normalisation tasks."
P16-1096,2016,7 Conclusions,"to do so, we introduced two neural network-based approaches for medical concept normalisation, which are based on convolutional and recurrent neural network architectures."
P16-1096,2016,7 Conclusions,we have motivated the importance of semantics when normalising medical concepts in social media messages.
P16-1097,2016,5 Conclusion,"by modeling the agreement on both phrase alignment and word alignment, our approach achieves significant improvements in both alignment and translation evaluations."
P16-1097,2016,5 Conclusion,"in the future, we plan to apply our approach to real-world non-parallel corpora to further verify its effectiveness."
P16-1097,2016,5 Conclusion,"it is also interesting to extend the phrase translation model to more sophisticated models such as ibm models 2-5 (brown et al., 1993) and hmm (vogel and ney, 1996)."
P16-1097,2016,5 Conclusion,we have presented agreement-based training for learning parallel lexicons and phrases from nonparallel corpora.
P16-1098,2016,8 Conclusion and Future Work,"besides, our visualization analysis revealed that multiple interpretable neurons in our model can capture the contextual interactions of the words or phrases."
P16-1098,2016,8 Conclusion and Future Work,experiments on two large scale text matching tasks demonstrate the efficacy of our proposed model and its superiority to competitor models.
P16-1098,2016,8 Conclusion and Future Work,"in future work, we would like to investigate our model on more text matching tasks."
P16-1098,2016,8 Conclusion and Future Work,"in this paper, we propose a model of deep fusion lstms to capture the strong interaction for text semantic matching."
P16-1099,2016,6 Conclusion,"additionally we recognize that the hashtag inventory used to discover business accounts from job-related topics might need to change over time, to achieve robust performance in the future."
P16-1099,2016,6 Conclusion,"as another point, due to twitter demographics, we are less likely to observe working seniors."
P16-1099,2016,6 Conclusion,"besides providing insights for discourse and its links to social science, our study could lead to practical applications, such as: aiding policy-makers with macro-level insights on job markets, connecting job-support resources to those in need, and facilitating the development of job recommendation systems.this work has limitations."
P16-1099,2016,6 Conclusion,"examining affective changes reveals that pa and na change independently; low na appears to indicate the absence of negative feelings, not the presence of positive ones."
P16-1099,2016,6 Conclusion,"our work is of social importance to workingage adults, especially for those who may struggle with job-related issues."
P16-1099,2016,6 Conclusion,this is left for future work.
P16-1099,2016,6 Conclusion,"we also analyzed identified tweets integrating temporal, affective, geospatial, and statistical information."
P16-1099,2016,6 Conclusion,we did not study whether providing contextual information in our humans-in-the-loop framework would influence the model performance.
P16-1099,2016,6 Conclusion,we separated business accounts from individual in job-related discourse.
P16-1099,2016,6 Conclusion,we used crowdsourcing and local expertise to power a humans-in-the-loop classification framework that iteratively improves identification of public job-related tweets.
P16-1099,2016,6 Conclusion,"while jobs take up enormous amounts of most adults’ time, job-related tweets are still rather infrequent."
P16-1100,2016,7 Conclusion,"additionally, we have demonstrated the potential of purely character-based models in producing good translations; they have outperformed past word-level nmt models."
P16-1100,2016,7 Conclusion,"for future work, we hope to be able to improve the memory usage and speed of purely character-based models."
P16-1100,2016,7 Conclusion,"moreover, we have succeeded in replacing the standard unk replacement technique in nmt with our characterlevel components, yielding an improvement of +2.1?11.4 bleu points."
P16-1100,2016,7 Conclusion,"our analysis has shown that our model has the ability to not only generate well-formed words for czech, a highly inflected language with an enormous and complex vocabulary, but also build accurate representations for english source words."
P16-1100,2016,7 Conclusion,our best hybrid model has surpassed the performance of both the best word-based nmt system and the best non-neural model to establish a new state-of-the-art result for english-czech translation in wmt’15 with 20.7 bleu.
P16-1100,2016,7 Conclusion,we have demonstrated these two aspects through our experimental results and translation examples.
P16-1100,2016,7 Conclusion,we have proposed a novel hybrid architecture that combines the strength of both word- and character-based models.
P16-1100,2016,7 Conclusion,"word-level models are fast to train and offer high-quality translation; whereas, character-level models help achieve the goal of open vocabulary nmt."
P16-1101,2016,6 Conclusion,another interesting direction is to apply our model to data from other domains such as social media (twitter and weibo).
P16-1101,2016,6 Conclusion,"first, our model can be further improved by exploring multi-task learning approaches to combine more useful and correlated information."
P16-1101,2016,6 Conclusion,"for example, we can jointly train a neural network model with both the pos and ner tags to improve the intermediate representations learned in our network."
P16-1101,2016,6 Conclusion,"in this paper, we proposed a neural network architecture for sequence labeling."
P16-1101,2016,6 Conclusion,"it is a truly end-to-end model relying on no task-specific resources, feature engineering or data pre-processing."
P16-1101,2016,6 Conclusion,"since our model does not require any domain- or taskspecific knowledge, it might be effortless to apply it to these domains."
P16-1101,2016,6 Conclusion,there are several potential directions for future work.
P16-1101,2016,6 Conclusion,"we achieved state-of-the-art performance on two linguistic sequence labeling tasks, comparing with previously state-of-the-art systems."
P16-1102,2016,6 Conclusion and Future Work,"a limitation of both the standard and proposed approach is that if a new question is created by the test-makers, then it will be necessary to collect example responses before it can be widely deployed."
P16-1102,2016,6 Conclusion and Future Work,further exploration of different topic vector representations and their combinations is necessary in future work.
P16-1102,2016,6 Conclusion and Future Work,"however, since the system can be trained on asr transcriptions, the example responses do not need to be hand-transcribed."
P16-1102,2016,6 Conclusion and Future Work,in this work a novel off-topic content detection framework based on topic-adapted rnnlms was developed.
P16-1102,2016,6 Conclusion and Future Work,the proposed approach achieves better topic classification and off-topic detection performance than the standard approaches.
P16-1102,2016,6 Conclusion and Future Work,the system was evaluated on the task of detecting off-topic spoken responses on the bulats test.
P16-1102,2016,6 Conclusion and Future Work,"this is an attractive deployment scenario, as only a smaller hand-transcribed data set is needed to train an asr system with which to cost-effectively transcribe a large number of candidate recordings."
P16-1103,2016,7 Conclusion,"by dynamically adding compound words to our translation grammars in this way we allow the decoder, which is in turn informed by the language model, to determine which, if any, of our hypothesized compounds look good in context."
P16-1103,2016,7 Conclusion,first an rnn classifier detects compoundable spans in the source sentence.
P16-1103,2016,7 Conclusion,"however, this technique relies heavily on a strong target language model."
P16-1103,2016,7 Conclusion,in addition to our generation technique we have presented a new human-quality data set that specifically targets compounding and use it to demonstrate tremendous improvements in our translation system’s ability to correctly generalize from compound words found in parallel text to match human translations of unseen compoundable phrases.
P16-1103,2016,7 Conclusion,in this paper we have presented a technique for generating compound words for target languages with open vocabularies by dynamically introducing synthetic translation options that allow spans of source text to translate as a single compound word.
P16-1103,2016,7 Conclusion,"our approach does away with the need for post processing, and avoids complications caused by reordering of morphemes in previous approaches."
P16-1103,2016,7 Conclusion,our method for generating such synthetic rules decomposes into two steps.
P16-1103,2016,7 Conclusion,"second, a word-to-character machine translation system translates the span of text into a compound word."
P16-1103,2016,7 Conclusion,"therefore, one important extension of our work is to further study the interaction between our model and the underlying language model."
P16-1104,2016,7 Conclusion,"in the current work, we created a novel framework to detect sarcasm, that derives insights from human cognition, that manifests over eye movement patterns."
P16-1104,2016,7 Conclusion,"our general approach may be useful in other nlp sub-areas like sentiment and emotion analysis, text summarization and question answering, where considering textual clues alone does not prove to be sufficient."
P16-1104,2016,7 Conclusion,"this extended feature-set improved the success rate of the sarcasm detector by 3.7%, over the best available system."
P16-1104,2016,7 Conclusion,using cognitive features in an nlp processing system like ours is the first proposal of its kind.
P16-1104,2016,7 Conclusion,"we also propose to develop models for the purpose of learning complex gaze feature representation, that accounts for the power of individual eye movement patterns along with the aggregated patterns of eye movements."
P16-1104,2016,7 Conclusion,we augmented traditional linguistic features with cognitive features obtained from readers’ eye-movement data in the form of simple gaze-based features and complex features derived from a graph structure.
P16-1104,2016,7 Conclusion,"we hypothesized that distinctive eye-movement patterns, associated with reading sarcastic text, enables improved detection of sarcasm."
P16-1104,2016,7 Conclusion,we propose to augment this work in future by exploring deeper graph and gaze features.
P16-1105,2016,5 Conclusion,"finally, the shortest path, which has been widely used in relation classification, is also appropriate for representing tree structures in neural lstm models."
P16-1105,2016,5 Conclusion,"first, the use of both word sequence and dependency tree structures is effective."
P16-1105,2016,5 Conclusion,our evaluation and ablation led to three key findings.
P16-1105,2016,5 Conclusion,"second, training with the shared parameters improves relation extraction accuracy, especially when employed with entity pretraining, scheduled sampling, and label embeddings."
P16-1105,2016,5 Conclusion,"this allowed us to represent both entities and relations in a single model, achieving gains over the state-of-the-art, feature-based system on end-to-end relation extraction (ace04 and ace05), and showing favorably comparable performance to recent state-of-the-art cnnbased models on nominal relation classification (semeval-2010 task 8)."
P16-1105,2016,5 Conclusion,we presented a novel end-to-end relation extraction model that represents both word sequence and dependency tree structures by using bidirectional sequential and bidirectional tree-structured lstm-rnns.
P16-1106,2016,7 Conclusions and outlook,"at some stage the shifted path must intersect with the path of z from py to ?px, before the entanglement of the two paths is broken."
P16-1106,2016,7 Conclusions and outlook,"at this time, no proof is within reach that generalizes the result to o3, i.e.the language of strings over an alphabet of six symbols, in which the number of a’s equals the number of a’s, the number of b’s equals the number of b’s, and the number of c’s equals the number of c’s; this language is rationally equivalent to mix-4, which is defined analogously to mix, but with four symbols."
P16-1106,2016,7 Conclusions and outlook,"both proofs use essentially the same mcfg, both are geometric in nature, and both involve a continuous view of paths next to a discrete view."
P16-1106,2016,7 Conclusions and outlook,"for a geometric interpretation, consider the paths of x, y and z, leading from point p0 = (0, 0, 0) to points px, py and pz, respectively."
P16-1106,2016,7 Conclusions and outlook,"for a proof by contradiction, therefore assume that no pair of prefixes of x and y and a suffix of z together form a non-empty string in o3 shorter than xyz , etc., in the light of the first three rules above, and assume that no ’short enough’ prefix of x, a prefix of y and a ’short enough’ suffix of x together form a non-empty string in o3, etc., in the light of the next six rules above."
P16-1106,2016,7 Conclusions and outlook,"if we can use any of the above rules to divide these into six strings out of which we can select three, which concatenated together are a non-empty string in o3 shorter than xyz , then we can use the inductive hypothesis, much as in section 2."
P16-1106,2016,7 Conclusions and outlook,"in addition, no pair of strings from x, y and z should end on complementing symbols, i.e.a and a, b and b, or c and c. this means that the three paths leading towards p0 must all end in p0 strictly ‘above’ or all strictly ‘below’ the surface."
P16-1106,2016,7 Conclusions and outlook,"in our case, the key concepts are excursions and truncation thereof, and the identification of top and bottom regions."
P16-1106,2016,7 Conclusions and outlook,"in the case of salvati (2015), the key concept is that of the complex exponential function, which seems to restrict the proof technique to two-dimensional geometry."
P16-1106,2016,7 Conclusions and outlook,"it appears this can be achieved by adding three more rules, namely: the physical interpretation of, say, the last rule seems to be that the path of z from ?pz to p0 can be iteratively shifted such that points other than its ending point coincide with p0."
P16-1106,2016,7 Conclusions and outlook,it has at least superficial elements in common with the proof by salvati (2015).
P16-1106,2016,7 Conclusions and outlook,"omitting the start rule and the axioms, an obvious candidate mcfg to generate o3 would among others have the three rules: consider three strings x, y and z such that xyz ∈ o3."
P16-1106,2016,7 Conclusions and outlook,one may expect however that a proof would use threedimensional geometry and generalize some of the arguments from this paper.
P16-1106,2016,7 Conclusions and outlook,"our aim here is to make this plausible, while emphasizing that an actual proof will require a novel framework at least as involved as that presented in the previous sections."
P16-1106,2016,7 Conclusions and outlook,"our assumptions imply that the final parts of the paths of x, y and z from ?px, ?py and ?pz, respectively, to p0 should not intersect with this surface."
P16-1106,2016,7 Conclusions and outlook,"the concatenations of prefixes of x and y, and similarly those of x and z and those of y and z form three connecting surfaces, together forming one surface dividing the space around p0 into an ‘above’ and a ‘below’; cf.figure 14."
P16-1106,2016,7 Conclusions and outlook,"the considerable challenges ahead involve finding a suitable definition of ‘excursions’ in three dimensions, and proving that these can be systematically truncated without violating appropriate constraints that preclude application of the above 12 rules."
P16-1106,2016,7 Conclusions and outlook,the major difference lies in the approach to tackling the myriad ways in which the paths can wind around each other and themselves.
P16-1106,2016,7 Conclusions and outlook,"this is illustrated in figure 15, where the path of z is ‘entangled’ with a copy of itself."
P16-1106,2016,7 Conclusions and outlook,"this might lead to a contradiction, similar to that in section 6, but only if one can ensure that none of the three paths to p0 ‘sneak around’ the surface."
P16-1106,2016,7 Conclusions and outlook,we have presented a new proof that o2 is generated by a mcfg.
P16-1107,2016,7 Conclusions and Future Work,"also, we will apply our contextaware argumentative relation mining to different argument mining corpora to further evaluate its generality."
P16-1107,2016,7 Conclusions and Future Work,experimental results show that topic-context and window-context features are both effective but impact predictive performance measures differently.
P16-1107,2016,7 Conclusions and Future Work,"in addition, predicting an argumentative relation will benefit most from combining these two set of features as they capture complementary aspects of context to better characterize the argumentation in justification."
P16-1107,2016,7 Conclusions and Future Work,"in this paper, we have presented context-aware argumentative relation mining that makes use of contextual features by exploiting information from topic context and context sentences."
P16-1107,2016,7 Conclusions and Future Work,our next step will investigate uses of topic segmentation to identify context sentences and compare this linguistically-motivated approach to our current window-size heuristic.
P16-1107,2016,7 Conclusions and Future Work,the results obtained in this preliminary study are promising and encourage us to explore more directions to enable contextual features.
P16-1107,2016,7 Conclusions and Future Work,"we have explored different ways to incorporate our proposed features with baseline features used in a prior study, and obtained insightful results about feature effectiveness."
P16-1107,2016,7 Conclusions and Future Work,we plan to follow prior research on graph optimization to refine the argumentation structure and improve argumentative relation prediction.
P16-1108,2016,7 Conclusion,"in the future, we plan to expand our method to predict morphological analyses, as well as to incorporate other information such as parts-of-speech."
P16-1108,2016,7 Conclusion,we have presented novel methods that leverage readily available inflection tables to produce highquality stems and lemmas.
P16-1109,2016,5 Conclusion,"in this paper we have presented s-struct, which enhances the struct system to enable better scaling to real generation tasks."
P16-1109,2016,5 Conclusion,s-struct is available through github upon request.
P16-1109,2016,5 Conclusion,"to our knowledge, these results and the scale of these nlg experiments (in terms of grammar size, world size, and lookahead complexity) represents the state-of-the-art for planning-based nlg systems."
P16-1109,2016,5 Conclusion,we conjecture that the parallelization of sstruct could achieve the response times necessary for real-time applications such as dialog.
P16-1109,2016,5 Conclusion,we show via experiments that this system can scale to large worlds and generate complete sentences in realworld datasets with a median time of 8.5s.
P16-1110,2016,8 Conclusion and Future Work,"finally, with slight changes to what the system considers a document, we believe alto can be extended to nlp applications other than classification, such as named entity recognition or semantic role labeling, to reduce the annotation effort."
P16-1110,2016,8 Conclusion and Future Work,"moreover, the topics are static and do not adapt to better reflect users’ labels."
P16-1110,2016,8 Conclusion and Future Work,our current system limits users to view only 20k documents at a time and allows for one label assignment per document.
P16-1110,2016,8 Conclusion and Future Work,users should have better support for browsing documents and assigning multiple labels.
P16-1110,2016,8 Conclusion and Future Work,we can further improve alto to help users gain better and faster understanding of text corpora.
P16-1110,2016,8 Conclusion and Future Work,"we introduce alto, an interactive framework that combines both active learning selections with topic model overviews to both help users induce a label set and assign labels to documents."
P16-1110,2016,8 Conclusion and Future Work,"we show that users can more effectively and efficiently induce a label set and create training data using alto in comparison with other conditions, which lack either topic overview or active selection."
P16-1111,2016,8 Conclusion,access to longer time-spans—along with varying data sources such as grants and patents—would also allow us to more completely model the trajectory of a topic as it moves from being active area of research to potentially impacting commercial industries and economic development.
P16-1111,2016,8 Conclusion,"examining only 20 years of scientific progress prevents us from analyzing drastic scientific changes, e.g.paradigm shifts, that are only obvious over longer time-scales (kuhn, 2012)."
P16-1111,2016,8 Conclusion,"in some ways these results are counter-intuitive; for example, one might expect topics that are being discussed as results to be the focus of current cutting edge research, and that methodological topics might be more mundane and lacking in novelty."
P16-1111,2016,8 Conclusion,"instead our results suggest that results-oriented topics are at the peak of their life-cycle, while methodological topics still have room to grow."
P16-1111,2016,8 Conclusion,"nonetheless, we hope this work offers another step towards using computational tools to better understand the ‘rhetorical structure of science’ (latour, 1987)."
P16-1111,2016,8 Conclusion,our analysis does suffer from some limitations.
P16-1111,2016,8 Conclusion,our analysis reveals important regularities with respect to how a topic’s usage evolves over its lifecycle.
P16-1111,2016,8 Conclusion,"this result has important implications for research funding and public policy: the most promising topics—in terms of potential for future growth—are not those that are currently generating the most results, but are instead those that are active areas of methodological inquiry."
P16-1111,2016,8 Conclusion,"we find that topics that are currently discussed as results tend to be in decline, whereas topics that are playing a methodological role tend to be in the early phases of growth."
P16-1111,2016,8 Conclusion,"we introduce a novel framework for assigning rhetorical functions to associations between scientific topics and papers, and we show how these rhetorical functions are predictive of a topic’s growth vs. decline."
P16-1112,2016,9 Conclusion,"as part of future work, it would be beneficial to investigate the effect of automatically generated training data for error detection (e.g., rozovskaya and roth (2010))."
P16-1112,2016,9 Conclusion,"based on the findings, we propose a novel error detection framework using token-level embeddings, bidirectional lstms for context representation, and a multi-layer architecture for learning more complex features."
P16-1112,2016,9 Conclusion,"even without any additional data, the combination further improved performance which is already close to the results from human annotators."
P16-1112,2016,9 Conclusion,"finally, we performed an extrinsic evaluation by incorporating probabilities from the error detection system as features in an essay scoring model."
P16-1112,2016,9 Conclusion,"in addition, the neural sequence tagging model, specialised for error detection, was able to outperform all other participating systems."
P16-1112,2016,9 Conclusion,"in addition, when the error detection model was trained on a larger training set, the essay scorer was able to exceed human-level performance."
P16-1112,2016,9 Conclusion,"in contrast, including even more data from higher-proficiency learners gave marginal further improvements."
P16-1112,2016,9 Conclusion,"in this paper, we presented the first experiments using neural network models for the task of error detection in learner writing."
P16-1112,2016,9 Conclusion,six alternative compositional network architectures for modeling context were evaluated.
P16-1112,2016,9 Conclusion,substantial performance improvements were achieved by training the best model on additional datasets.
P16-1112,2016,9 Conclusion,"the experiments showed that success on error correction does not necessarily mean success on error detection, as the current best correction system (p1+p2+s1+s2) is not the same as the best shared task detection system (camb)."
P16-1112,2016,9 Conclusion,"the self-modulation architecture of lstms was also shown to be beneficial, as it allows the network to learn more advanced composition rules and remember dependencies over longer distances."
P16-1112,2016,9 Conclusion,"this structure allows the model to classify each token as being correct or incorrect, using the full sentence as context."
P16-1112,2016,9 Conclusion,we evaluated the performance of existing error correction systems from conll-14 on the task of error detection.
P16-1112,2016,9 Conclusion,we found that the largest benefit was obtained from training on 8 million tokens of text from learners with varying levels of language proficiency.
P16-1113,2016,7 Conclusions,"beyond srl, we expect dependency path embeddings to be useful in related tasks and downstream applications."
P16-1113,2016,7 Conclusions,"for instance, our representations may be of direct benefit for semantic and discourse parsing tasks."
P16-1113,2016,7 Conclusions,"in a qualitive analysis, we found that our model is able to cover instances of various linguistic phenomena that are missed by other methods."
P16-1113,2016,7 Conclusions,"our experimental results indicate that our model substantially increases classification performance, leading to new state-of-the-art results."
P16-1113,2016,7 Conclusions,"the jointly learned feature space also makes our model a good starting point for cross-lingual transfer methods that rely on feature representation projection to induce new models (kozhevnikov and titov, 2014)."
P16-1113,2016,7 Conclusions,we introduced a neural network architecture for semantic role labeling that jointly learns embeddings for dependency paths and feature combinations.
P16-1114,2016,7 Conclusion,"also, we examined how the length of the observation period affects prediction performance, and investigated the trade-off between prediction accuracy and instancy."
P16-1114,2016,7 Conclusion,the experiments successfully demonstrated that reasonable performance can be archived in both tasks.
P16-1114,2016,7 Conclusion,the future work includes using those prediction models in a real service to take targeted actions to users who are likely to stop using intelligent assistants.
P16-1114,2016,7 Conclusion,this paper explored two tasks of predicting prospective user engagement with intelligent assistants: dropout prediction and engagement level prediction.
P16-1115,2016,7 Conclusions,all this is future work.
P16-1115,2016,7 Conclusions,"but there is a clear path for bringing it back in, by defining other composition types for other construction types and different word models for other word types."
P16-1115,2016,7 Conclusions,"here, we have disregarded much of the internal structure of the expressions."
P16-1115,2016,7 Conclusions,"in its current state— besides, we believe, strongly motivating this future work—, we hope that the model can also serve as a strong baseline to other future approaches to reference resolution, as it is conceptually simple and easy to implement."
P16-1115,2016,7 Conclusions,it achieves results that are comparable to those of more complex models.
P16-1115,2016,7 Conclusions,"its basis, the word/object classifiers, ties in more directly with more standard approaches to semantic analysis and composition."
P16-1115,2016,7 Conclusions,"kennington and schlangen (2015) do this for spatial relations in their simpler domain; for our domain, new and more richly annotated data such as visualgenome looks promising for learning a wide variety of relations.9 the use of denotations / extensions might make possible transfer of methods from extensional semantics, e.g.for the addition of operators such as negation or generalised quantifiers."
P16-1115,2016,7 Conclusions,"lastly, the word/object classifiers also show promise in the reverse task, generation of referring expressions (zarrie?and schlangen, 2016)."
P16-1115,2016,7 Conclusions,"the design of the model, as mentioned in the introduction, makes it amenable for use in interactive systems that learn; we are currently exploring this avenue."
P16-1115,2016,7 Conclusions,"we have shown that the “words-as-classifiers” model scales up to a larger set of object types with a much larger variety in appearance (saiapr and mscoco); to a larger vocabulary and much less restricted expressions (referit, refcoco, grexp); and to use of automatically learned feature types (from a cnn)."
P16-1115,2016,7 Conclusions,we see as advantage that the model we use is “transparent” and modular.
P16-1116,2016,7 Conclusion,"also, the argument classification seems not to be affected too much by the regularization."
P16-1116,2016,7 Conclusion,"for a trigger, if no pattern can be matched, the event type cannot be assigned and the arguments cannot be correctly identified and classified."
P16-1116,2016,7 Conclusion,"future work may be done to integrate our method into a joint approach, use some global feature, which may improve our performance."
P16-1116,2016,7 Conclusion,"however, patterns cannot cover all events and the relationship between candidate arguments may help when identifying arguments."
P16-1116,2016,7 Conclusion,"however, we only use sentence-level features and our method is a pipelined approach."
P16-1116,2016,7 Conclusion,"in summary, by using the event type classifier and the regularization method, we have achieved a good performance in which the trigger classification is comparable to state-of-theart methods, and the argument identification & classification performance is significantly better than state-of-the-art methods."
P16-1116,2016,7 Conclusion,"in this paper, we propose two improvements based on the event extraction baseline jet."
P16-1116,2016,7 Conclusion,"on the other hand, we train a maximum entropy classifier to predict the relationship between candidate arguments."
P16-1116,2016,7 Conclusion,our experiment results show that the regularization method is a significant improvement in argument identification over previous works.
P16-1116,2016,7 Conclusion,the code is available at https://github.com/shalei120/ rbpb/tree/master/rbet_release
P16-1116,2016,7 Conclusion,then we propose a regularization method to make full use of the argument relationship.
P16-1116,2016,7 Conclusion,"therefore, we develop an event type classifier to assign the event type, using both pattern matching information and other features, which gives our system the capability to deal with failed match cases when using patterns alone."
P16-1116,2016,7 Conclusion,we find that jet depends too much on event patterns for event type priori and jet considers each candidate argument separately.
P16-1117,2016,6 Conclusion,"in our experiments, we demonstrated that the proposed pas analysis model significantly outperformed the base sota model."
P16-1117,2016,6 Conclusion,"in the future, we plan to extend our model to incorporate coreference resolution and intersentential zero anaphora resolution."
P16-1117,2016,6 Conclusion,in this paper we presented a novel model for japanese pas analysis based on neural network framework.
P16-1117,2016,6 Conclusion,"we learned selectional preferences from a large raw corpus, and incorporated them into a pas analysis model, which considers the consistency of all pass in a given sentence."
P16-1118,2016,8 Conclusion,"in the process, we first created an entailment dataset for the clinical domain, and a highly competitive supervised entailment system, called ent, which is effective (out of the box) on two domains."
P16-1118,2016,8 Conclusion,"on the other hand, our active learning experiments demonstrated that we could match (and even beat) the baseline ent system with only 6.6% of the training data in the clinical domain, and only 5.8% of the training data in the newswire domain."
P16-1118,2016,8 Conclusion,"our self-training system substantially improved over ent, achieving an f-score gain of 15% on newswire and 13% on clinical, using only additional unlabeled data."
P16-1118,2016,8 Conclusion,we explored the problem of textual entailment search in two domains – newswire and clinical – and focused a spotlight on the cost of obtaining labeled data in certain domains.
P16-1118,2016,8 Conclusion,"we then explored two strategies – self-training and active learning – to address the lack of labeled data, and observed some interesting results."
P16-1119,2016,7 Conclusions and Future Work,"furthermore, we presented a linguistically motivated classifier, surpassing the previous baseline by 8% gain in f1."
P16-1119,2016,7 Conclusions and Future Work,"future work can use our annotated corpus to develop classifiers that deal better with prepositional and adjectival modifiers, which require deeper semantic analysis."
P16-1119,2016,7 Conclusions and Future Work,"we presented an end-to-end framework for restrictiveness annotation, including a novel qa-srl based crowdsourcing methodology and a first consistent human-annotated corpus."
P16-1120,2016,8 Conclusions,"as future work, we would like to verify the effectiveness of the proposed models for other datasets or other cross-lingual tasks, such as cross-lingual document classification (ni et al., 2009; platt et al., 2010; ni et al., 2011; smet et al., 2011) and cross-lingual information retrieval (vulic et al., ′ 2013)."
P16-1120,2016,8 Conclusions,bistm assigns the same topic distribution to both aligned documents and aligned segments.
P16-1120,2016,8 Conclusions,"in this paper, we proposed bistm, which models a document hierarchically and deals with segmentlevel alignments."
P16-1120,2016,8 Conclusions,"our experimental results show that capturing segmentlevel alignments improves perplexity and translation extraction performance, and that bistm+ts yields a significant benefit even if the boundaries of segments are not given."
P16-1120,2016,8 Conclusions,"this paper presented an extension to bilda, but hierarchical structures can also be incorporated into other bilingual topic models (section 7)."
P16-1120,2016,8 Conclusions,"we also presented an extended model, bistm+ts, that infers segmentation boundaries in addition to latent topics by incorporating unsupervised topic segmentation (du et al., 2013)."
P16-1122,2016,7 Conclusion and Future Work,in the future we plan to apply our inner-attention intuition to other neural networks such as cnn or multi-layer perceptron.
P16-1122,2016,7 Conclusion and Future Work,in this work we present some variants of traditional attention-based rnn models with gru.
P16-1122,2016,7 Conclusion and Future Work,occam’s razor is further implemented to this attention for better representation.
P16-1122,2016,7 Conclusion and Future Work,our models can be further extended to other nlp tasks such as recognizing textual entailments where attention mechanism is important for sentence representation.
P16-1122,2016,7 Conclusion and Future Work,our results on answer selection demonstrate that the inner attention outperforms the outer attention in rnn.
P16-1122,2016,7 Conclusion and Future Work,the key idea is attention before representation.
P16-1122,2016,7 Conclusion and Future Work,we analyze the deficiency of traditional outer attention-based rnn models qualitatively and quantitatively.
P16-1122,2016,7 Conclusion and Future Work,we propose three models where attention is embedded into representation process.
P16-1123,2016,5 Conclusion,our results show that this simple but effective model is able to outperform previous work relying on substantially richer prior knowledge in the form of structured models and nlp resources.
P16-1123,2016,5 Conclusion,"we expect this sort of architecture to be of interest also beyond the specific task of relation classification, which we intend to explore in future work."
P16-1123,2016,5 Conclusion,we have presented a cnn architecture with a novel objective and a new form of attention mechanism that is applied at two different levels.
P16-1124,2016,6 Conclusion,"another example is that the current method is a two-step approach, performing relation clustering first and then relation coupling."
P16-1124,2016,6 Conclusion,"by coupling different relations, cpra takes into account relation associations and enables implicit data sharing among them."
P16-1124,2016,6 Conclusion,"by further coupling such relations, cpra significantly outperforms pra, in terms of both predictive this is the first work that investigates the possibility of multi-task learning with pra, and we just provide a very simple solution."
P16-1124,2016,6 Conclusion,experimental results show that cpra can effectively identify coherent clusters in which relations are highly correlated.
P16-1124,2016,6 Conclusion,"for instance, the agglomerative clustering strategy can only identify highly correlated relations, i.e., those sharing a lot of common paths."
P16-1124,2016,6 Conclusion,in this paper we have studied the path ranking algorithm (pra) from the viewpoint of multi-task learning.
P16-1124,2016,6 Conclusion,it will be interesting to study whether one can merge the clustering step and the coupling step so as to have a richer inter-task dependent structure.
P16-1124,2016,6 Conclusion,"relations that are only loosely correlated, e.g., those sharing no common paths but a lot of sub-paths, will not be identified."
P16-1124,2016,6 Conclusion,"the key idea of cpra is to (i) automatically discover relations highly correlated to each other through agglomerative clustering, and (ii) effectively couple the prediction of such relations through multi-task learning."
P16-1124,2016,6 Conclusion,there are still many interesting topics to study.
P16-1124,2016,6 Conclusion,"we have designed a novel multi-task learning framework for pra, called coupled pra (cpra)."
P16-1124,2016,6 Conclusion,we have tested cpra on benchmark data created from freebase.
P16-1124,2016,6 Conclusion,we will investigate such topics in our future work.
P16-1124,2016,6 Conclusion,"we would like to design new mechanisms to discover loosely correlated relations, and investigate whether coupling such relations still provides benefits."
P16-1125,2016,7 Conclusion,"first, the four datasets we used in this paper are relatively small in the context of language modelling, therefore the proposed largercontext language model should be evaluated on larger corpora."
P16-1125,2016,7 Conclusion,"from our experiments, we found that the sequence of bag-of-words with attention is better than bag-of-words for representing the context sentences (see sec.3.1), and the late fusion is better than the early fusion for feeding the context vector into the main recurrent language model (see sec.3.2)."
P16-1125,2016,7 Conclusion,"in this paper, we proposed a method to improve language model on corpus-level by incorporating larger context."
P16-1125,2016,7 Conclusion,"lastly, it is important to evaluate the impact of the proposed largercontext models in downstream tasks such as machine translation and speech recognition."
P16-1125,2016,7 Conclusion,"our part-of-speech analysis revealed that content words, including nouns, adjectives and verbs, benefit most from an increasing number of context sentences."
P16-1125,2016,7 Conclusion,"second, more analysis, beyond the one based on part-of-speech tags, should be conducted in order to better understand the advantage of such larger-context models."
P16-1125,2016,7 Conclusion,this analysis suggests that larger-context language model improves perplexity because it captures the theme of a document better and more easily.
P16-1125,2016,7 Conclusion,"to explore the potential of such a model, there are several aspects in which more research needs to be done."
P16-1125,2016,7 Conclusion,"using this model results in the improvement in perplexity on the imdb, bbc, penn treebank and fil9 corpora, validating the advantage of providing larger context to a recurrent language model."
P16-1126,2016,7 Conclusion,"the annotations reveal the structure and complexity of these documents, which internet users are expected to understand and accept."
P16-1126,2016,7 Conclusion,this corpus should serve as a resource for language technologies research to help internet users understand the privacy practices of businesses and other entities that they interact with online.
P16-1126,2016,7 Conclusion,"we have described the motivation, creation, and analysis of a unique corpus of 115 privacy policies and 23k fine-grained data practice annotations, and we have demonstrated the feasibility of partly automating the annotation process."
P16-1127,2016,6 Conclusion and Future Work,"but there are other possible choices that might make the encoding even more easily learnable by the lstm, and we would like to explore those in future work."
P16-1127,2016,6 Conclusion and Future Work,"in all cases, we obtain competitive results with previously reported experiments."
P16-1127,2016,6 Conclusion and Future Work,"in order to encode the underlying derivation tree, we chose to use a leftmost derivation sequence."
P16-1127,2016,6 Conclusion and Future Work,"in order to improve performance, other promising directions would involve adding re-reranking techniques and extending our neural networks with attention models in the spirit of (bahdanau et al., 2015)."
P16-1127,2016,6 Conclusion and Future Work,the most effective model is one using derivation sequences and taking into account the grammatical constraints.
P16-1127,2016,6 Conclusion and Future Work,"we encode the target logical form, a structured object, through three types of sequences: direct linearization of the logical form, canonical form, derivation sequence in an underlying grammar."
P16-1127,2016,6 Conclusion and Future Work,we propose a sequence-based approach for the task of semantic parsing.
P16-1128,2016,7 Conclusion,"experiments on word similarity and analogy and pos tagging show the high quality of the metaembeddings; e.g., they outperform glove and word2vec on analogy."
P16-1128,2016,7 Conclusion,the ensemble methods have the added advantage of increasing vocabulary coverage.
P16-1128,2016,7 Conclusion,"this work presented four ensemble methods for learning metaembeddings from multiple embedding sets: conc, svd, 1ton and 1ton +."
P16-1129,2016,8 Conclusion and Future Work,"although more data can be easily collected in this case, the noisiness of audio transcripts may bring some additional challenges, therefore worthwhile for further study."
P16-1129,2016,8 Conclusion and Future Work,another important direction is to focus on the construction of datasets in larger scale.
P16-1129,2016,8 Conclusion and Future Work,"as a preliminary work, we only perform sentence extraction in this work."
P16-1129,2016,8 Conclusion and Future Work,experimental results demostrate that this task is feasible and our proposed methods are appropriate.
P16-1129,2016,8 Conclusion and Future Work,in this paper we study a challenging task to automatically construct sports news from live text commentary.
P16-1129,2016,8 Conclusion and Future Work,one feasible approach is to use a speech recognition system on live videos or broadcasts of sports games to collect huge amount of transcripts as our raw data source.
P16-1129,2016,8 Conclusion and Future Work,"since sports news and live commentary are in different genres, some post-editing rewritings will make the system generating more natural descriptions for sports news."
P16-1129,2016,8 Conclusion and Future Work,"to generate the final news summary and tackle the local redundancy problem, we also propose a probabilistic sentence selection method."
P16-1129,2016,8 Conclusion and Future Work,"using football live texts as an instance, we collect training data jointly from live text commentary services and sports news portals."
P16-1129,2016,8 Conclusion and Future Work,"we develop a system based on learning to rank models, with several novel task-specific features."
P16-1129,2016,8 Conclusion and Future Work,we would like to extend our system to produce sports news beyond pure sentence extraction.
P16-1130,2016,6 Conclusion,"compared with the previous mers model that used discrete representations of words as features, the csrs model uses real-valued vector representations of words and can exploit similarity information between words for better generalization."
P16-1130,2016,6 Conclusion,"for future work, we will explore more sophisticated features for the csrs model, such as syntactic dependency relationships and head words, since only simple lexical features are used in the current incarnation."
P16-1130,2016,6 Conclusion,"in addition, we propose to use only minimal rules for rule selection to further relieve the data sparsity problem, since minimal rules are more frequent and have richer training data."
P16-1130,2016,6 Conclusion,"in our experiments, the csrs model outperformed the previous mers model and the usage of minimal rules benefitted both csrs and mers models on different translation tasks."
P16-1130,2016,6 Conclusion,"in this paper, we propose a csrs model for syntax-based smt, which is learned by a feedforward neural network on a continuous space."
P16-1131,2016,6 Conclusions,"this work presents neural probabilistic graphbased models for dependency parsing, together with a convolutional part which could capture the sentence-level information."
P16-1131,2016,6 Conclusions,"with distributed vectors for representations and complex non-linear neural network for calculations, the model can effectively capture more complex features when deciding the scores for sub-tree factors and experiments on standard treebanks show that the proposed techniques improve parsing accuracies."
P16-1132,2016,7 Conclusion,"in this paper, we proposed a search-based dynamic reranking model using a hierarchical neural base parser and a recursive convolutional neural score model."
P16-1132,2016,7 Conclusion,it achieves significant accuracy improvement (+1.78%) upon the baseline deterministic parser.
P16-1132,2016,7 Conclusion,the dynamic model is the first reranker integrating search and learning for dependency parsing.
P16-1132,2016,7 Conclusion,"with the dynamic search process, our reranker obtains a 0.44% accuracy improvement upon the static reranker."
P16-1133,2016,5 Conclusion and Future Work,both sentiment and semantic correlations are exploited in our algorithm while previous works only use the semantic relatedness between parallel documents.
P16-1133,2016,5 Conclusion and Future Work,"different from previous studies which only get bilingual word embeddings, we directly learn the vector representation for documents in different languages."
P16-1133,2016,5 Conclusion and Future Work,"in addition, we will also explore the possibility of using more complex neural network models such as convolutional neural network and recurrent neural network to build bilingual document representation system."
P16-1133,2016,5 Conclusion and Future Work,"in this study, we propose a bilingual document representation learning method for cross-lingual sentiment classification."
P16-1133,2016,5 Conclusion and Future Work,our algorithm outperforms all the baseline methods on all the nine tasks in the experiment.
P16-1133,2016,5 Conclusion and Future Work,our future work will focus on extending the bilingual document representation model into the multilingual scenario.
P16-1133,2016,5 Conclusion and Future Work,our model is evaluated on a benchmarking dataset which contains three different target languages and three different domains.
P16-1133,2016,5 Conclusion and Future Work,several stateof-the-art methods including several bilingual representation learning models are used for comparison.
P16-1133,2016,5 Conclusion and Future Work,we propose three strategies to achieve a consistent embedding space for the source and target languages.
P16-1133,2016,5 Conclusion and Future Work,we will try to learn a single embedding space for a source language and multiple target languages simultaneously.
P16-1134,2016,6 Conclusions,experimental evaluations demonstrate the effectiveness of grsemi-crfs on both text chunking and ner tasks.
P16-1134,2016,6 Conclusions,"in future work, we are interested in exploring better ways of utilizing vast unlabelled data to improve grsemi-crfs, e.g., to learn phrase embeddings from unlabelled data or designing a semisupervised version of grsemi-crfs."
P16-1134,2016,6 Conclusions,"in this paper, we propose gated recursive semimarkov conditional random fields (grsemicrfs) for segment-level sequence tagging tasks."
P16-1134,2016,6 Conclusions,"unlike word-level models such as crfs, grsemicrfs model segments directly without the need of using extra tagging schemes and also readily utilize segment-level features, both hand-crafted and automatically extracted by a grconv."
P16-1135,2016,7 Conclusion,"although we have focused exclusively on wikipedia, these methods could be adapted to other domains and languages."
P16-1135,2016,7 Conclusion,"causality is not easily expressed in english using a fixed set of phrases, so we would expect these methods to apply to formal and informal text ranging from news and journals to social media."
P16-1135,2016,7 Conclusion,"in the future, we may improve this step using a machine learning approach."
P16-1135,2016,7 Conclusion,"linguistic expressions of causality in other languages is another avenue for future research, and it would be interesting to note if other languages have the same variety of expression."
P16-1135,2016,7 Conclusion,our method for automatically building a training set for causality is a new contribution.
P16-1135,2016,7 Conclusion,our use of distant supervision demonstrates that we can use a large amount of possibly noisy data to develop an accurate classifer.
P16-1135,2016,7 Conclusion,the text in the altlex alone is not sufficient to accurately identify causality.
P16-1135,2016,7 Conclusion,"thus, we did not evaluate some intermediate steps, such as the quality of the automatically annotated corpus."
P16-1135,2016,7 Conclusion,to evaluate on the intermediate step would have required an additional annotation process.
P16-1135,2016,7 Conclusion,"ultimately, the focus of this work is to improve detection of causal relations."
P16-1135,2016,7 Conclusion,we have shown a method for identifying and classifying phrases that indicate causality.
P16-1135,2016,7 Conclusion,we have shown statistically significant improvement over the naive baseline using semantic and parallel corpus features.
P16-1135,2016,7 Conclusion,we show that our features are informative by themselves and perform well even on rarely occurring examples.
P16-1136,2016,5 Conclusions,experimental results on two datasets show that it outperforms prior approaches by modeling intermediate path nodes.
P16-1136,2016,5 Conclusions,"in the future, we would like to study the impact of relation paths for additional basic kb embedding models and knowledge domains."
P16-1136,2016,5 Conclusions,"in this work, we propose the first approach to efficiently incorporate all relation paths of bounded length in a knowledge base, while modeling both relations and intermediate nodes in the compositional path representations."
P16-1137,2016,8 Conclusion,"by scoring novel tuples, we showed how we can increase the applicability of the knowledge contained in conceptnet."
P16-1137,2016,8 Conclusion,"in future work, we will explore how to use our model to improve downstream nlp tasks, and consider applying our methods to other knowledge bases."
P16-1137,2016,8 Conclusion,"we have released all of our resources—code, data, and trained models—to the research community."
P16-1137,2016,8 Conclusion,we proposed methods to augment curated commonsense resources using techniques from knowledge base completion.
P16-1139,2016,5 Conclusions and future work,"because this paper aims to introduce a general purpose model for sentence encoding, we do not pursue the use of soft attention (bahdanau et al., 2015; rockt?schel et al., 2016), despite its demonstrated effectiveness on the snli task.4 however, we expect that it should be possible to productively combine our model with soft attention to reach state-of-the-art performance."
P16-1139,2016,5 Conclusions and future work,"finally, we show that it is possible to exploit the strengths of this model without the need for an external parser by integrating a fast parser into the model (as in the full spinn), and that the lack of external parse information yields little loss in accuracy."
P16-1139,2016,5 Conclusions and future work,"for a more ambitious goal, we expect that it should be possible to implement a variant of spinn on top of a modified stack data structure with differentiable push and pop operations (as in grefenstette et al., 2015; joulin and mikolov, 2015)."
P16-1139,2016,5 Conclusions and future work,"it is plausible that giving the tracking lstm access to more information from the buffer and stack at each step would allow it to better represent the context at each tree node, yielding both better parsing and better sentence encoding."
P16-1139,2016,5 Conclusions and future work,one promising way to pursue this goal would be to encode the full contents of the stack and buffer at each time step following the method used by dyer et al.(2015).
P16-1139,2016,5 Conclusions and future work,"our tracking lstm uses only simple, quick-tocompute features drawn from the head of the buffer and the head of the stack."
P16-1139,2016,5 Conclusions and future work,"this change would allow the model to learn to produce parses that are, in aggregate, better suited to supporting semantic interpretation than those supplied in the training data."
P16-1139,2016,5 Conclusions and future work,"this would make it possible for the model to learn to parse using guidance from the semantic representation objective, which currently is blocked from influencing the key parsing parameters by our use of hard shift/reduce decisions."
P16-1139,2016,5 Conclusions and future work,"we expand that architecture into a tree-sequence hybrid model (spinnpi), and show that this yields significant gains on the snli entailment task."
P16-1139,2016,5 Conclusions and future work,"we introduce a model architecture (spinn-pi-nt) that is equivalent to a treelstm, but an order of magnitude faster at test time."
P16-1140,2016,7 Conclusion,crosslanguage perspective and novel analysis of neuron behaviours provide us with new evidence about the typological universal and specific revealed in word embedding.
P16-1140,2016,7 Conclusion,"in this paper, we quantify the utility of word form and the effect of language typological diversity in learning word representations."
P16-1140,2016,7 Conclusion,"it is plausible (and sometimes even better) to decode grammatical function just from the word form, for certain inflectional languages.?"
P16-1140,2016,7 Conclusion,it would also be a promising direction to incorporate the factor of language typological diversity when designing advanced word representation model for languages other than english.
P16-1140,2016,7 Conclusion,"language typological diversity, especially the specific word order type and morphological complexity, does influence how linguistic information is encoded in word embedding.?"
P16-1140,2016,7 Conclusion,quantification of neuron activation pattern reveals different characteristics of the context-based model and the character-based counterpart.
P16-1140,2016,7 Conclusion,"therefore, we think that it is necessary to maximize both the utility of word form and the advantage of the context for a better word representation."
P16-1140,2016,7 Conclusion,we summarize from our experiments on a massive set of languages that: ?
P16-1141,2016,5 Discussion,"future studies of semantic change must account for frequency’s conforming effect: when examining the interaction between some linguistic process and semantic change, the law of conformity should serve as a null model in which the interaction is driven primarily by underlying frequency effects."
P16-1141,2016,5 Discussion,"highly polysemous words tend to have more rare senses (kilgarriff, 2004), and rare senses may be unstable by the law of conformity."
P16-1141,2016,5 Discussion,"however, our results show that polysemous words change faster, which suggests that polysemy may actually lead to semantic change."
P16-1141,2016,5 Discussion,"moreover, such mechanisms may also be partially responsible for the law of innovation."
P16-1141,2016,5 Discussion,"or perhaps a sociocultural conformity bias makes people less likely to accept novel innovations of common words, a mechanism analogous to the biological process of purifying selection (boyd and richerson, 1988; pagel et al., 2007)."
P16-1141,2016,5 Discussion,"our work builds upon a wealth of previous research on quantitative approaches to semantic change, including prior work with distributional methods (sagi et al., 2011; wijaya and yeniterzi, 2011; gulordava and baroni, 2011; jatowt and duh, 2014; kulkarni et al., 2014; xu and kemp, 2015), as well as recent work on detecting the emergence of novel word senses (lau et al., 2012; mitra et al., 2014; cook et al., 2014; mitra et al., 2015; frermann and lapata, 2016)."
P16-1141,2016,5 Discussion,"overall, these two factors—frequency and polysemy—explain between 48% and 88% of the variance10 in rates of semantic change (across conditions)."
P16-1141,2016,5 Discussion,"previous works argued that seman- ′ tic change leads to polysemy (wilkins, 1993; hopper and traugott, 2003)."
P16-1141,2016,5 Discussion,"the law of conformity might be a consequence of learning: perhaps people are more likely to use rare words mistakenly in novel ways, a mechanism formalizable by bayesian models of word learning and corresponding to the biological notion of genetic drift (reali and griffiths, 2010)."
P16-1141,2016,5 Discussion,the law of conformity—frequent words change more slowly—clarifies frequency’s role in semantic change.
P16-1141,2016,5 Discussion,"the law of innovation—polysemous words change more quickly—quantifies the central role polysemy plays in semantic change, an issue that has concerned linguists for more than 100 years (breal, 1897)."
P16-1141,2016,5 Discussion,the two statistical laws we propose have strong implications for future work in historical semantics.
P16-1141,2016,5 Discussion,these empirical statistical laws also lend themselves to various causal mechanisms.
P16-1141,2016,5 Discussion,this remarkable degree of explanatory power indicates that frequency and polysemy are perhaps the two most crucial linguistic factors that explain rates of semantic change over time.
P16-1141,2016,5 Discussion,we extend these lines of work by rigorously comparing different approaches to quantifying semantic change and by using these methods to propose new statistical laws of semantic change.
P16-1141,2016,5 Discussion,we show how distributional methods can reveal statistical laws of semantic change and offer a robust methodology for future work in this area.
P16-1141,2016,5 Discussion,"while our results cannot confirm such causal links, they nonetheless highlight a new role for frequency and polysemy in language change and the importance of distributional models in historical research."
P16-1142,2016,7 Conclusions,crowdsourced annotations show that this kind of inferences are intuitive to humans.
P16-1142,2016,7 Conclusions,"experimental results with gold-standard semantic roles and named entities show that inference can be done with standard supervised machine learning (overall f-measure: 0.65, yes: 0.77, no: 0.64)."
P16-1142,2016,7 Conclusions,"for example, one could infer who is in possession of something over time by manipulating the events in which the object in question participates in."
P16-1142,2016,7 Conclusions,"moreover, most potential additional spatial knowledge generated following a few simple deterministic rules was validated by annotators (yes and no; before: 67.7%, during: 77.4%, after: 63.1%)."
P16-1142,2016,7 Conclusions,"this is mostly due to the fact that predicted semantic roles and named entities are often wrong or missing, and this fact unequivocally makes the inference process more challenging."
P16-1142,2016,7 Conclusions,"this paper demonstrates that semantic roles are a reliable semantic layer from which one can infer whether entities are located or not located somewhere, and for how long (seconds, minutes, days, years, etc.)."
P16-1142,2016,7 Conclusions,"using predicted linguistic information, results decrease substantially (yes: 0.64, no: 0.51)."
P16-1142,2016,7 Conclusions,we believe that combining semantic roles and other semantic representation in a similar fashion to the one used in this paper could be useful to infer knowledge beyond spatial inferences.
P16-1143,2016,8 Discussion and Future Work,another obvious extension would be to further explore the alignment component of hca-wsi.
P16-1143,2016,8 Discussion and Future Work,"finally, we demonstrated that hca topic modelling is more efficient than hdp, providing guidance for others who wish to do large-scale unsupervised sense distribution learning."
P16-1143,2016,8 Discussion and Future Work,"given that lexsemtm also contains wsi output for nonpolysemous wordnet lemmas (37,566 in total), this could be lead to the discovery of many new polysemous lemmas."
P16-1143,2016,8 Discussion and Future Work,"in addition, previous work by lau et al.(2012) and lau et al.(2014) also provided methods for detecting novel and unattested senses, using the topic modelling output from the wsi step of hdp-wsi."
P16-1143,2016,8 Discussion and Future Work,"in addition, we have produced goldstandard distributions for a subset of the lemmas in lexsemtm, which we have used to demonstrate that lexsemtm sense distributions are at least on-par with those based on semcor for lemmas with a reasonable frequency in wikipedia, and strongly superior for lemmas missing from semcor."
P16-1143,2016,8 Discussion and Future Work,"in conclusion, we have created extensive resources for future work in nlp and related disciplines."
P16-1143,2016,8 Discussion and Future Work,"in particular, we intend to expand lexsemtm by applying hca-wsi across the vocabularies of languages other than english, and also to multiword lemmas."
P16-1143,2016,8 Discussion and Future Work,"it also contains lemma topic models, for both polysemous and nonpolysemous lemmas, which provide rich semantic information about lemma usage, and can be re-aligned to sense inventories to produce new sense distributions at trivial cost."
P16-1143,2016,8 Discussion and Future Work,the most immediate extension of our work would be to apply our sense learning method to a broader range of data.
P16-1143,2016,8 Discussion and Future Work,these could be applied with lexsemtm— which contains this wsi output as well as sense frequencies — to search for novel and unattested senses throughout the entire vocabulary of english.
P16-1143,2016,8 Discussion and Future Work,"this could be used to expand existing sense inventories with new senses, for example using the methodology of cook et al.(2013)."
P16-1143,2016,8 Discussion and Future Work,this dataset contains sense distributions for the majority of polysemous lemmas in wordnet 3.0.
P16-1143,2016,8 Discussion and Future Work,"we currently use a simple approach, and we believe this process could be improved, e.g.by using word embeddings."
P16-1143,2016,8 Discussion and Future Work,"we have produced lexsemtm, which was trained on english wikipedia and spans approximately 88% of polysemous english lemmas."
P16-1144,2016,5 Conclusion,"a number of linguistic phenomena make the target words in lambada easy to guess by human subjects when they can look at the whole passages they come from, but nearly impossible if only the last sentence is considered."
P16-1144,2016,5 Conclusion,"on a more general note, we believe that leveraging human performance on word prediction is a very promising strategy to construct benchmarks for computational models that are supposed to capture various aspects of human text understanding."
P16-1144,2016,5 Conclusion,our preliminary experiments suggest that even some cutting-edge neural network approaches that are in principle able to track long-distance effects are far from passing the lambada challenge.
P16-1144,2016,5 Conclusion,the influence of broad context as explored by lambada is only one example of this idea.
P16-1144,2016,5 Conclusion,"the test set will be made available at the time of the competition.stored in memory, in order to retrieve the right information from it."
P16-1144,2016,5 Conclusion,"this paper introduced the new lambada dataset, aimed at testing language models on their ability to take a broad discourse context into account when predicting a word."
P16-1144,2016,5 Conclusion,"to promote research in this direction, we plan to announce a public competition based on the lambada data.5 our own hunch is that, despite the initially disappointing results of the “vanilla” memory network we tested, the ability to store information in a longer-term memory will be a crucial component of successful models, coupled with the ability to perform some kind of reasoning about what’s 5the development set of lambada, along with the training corpus, can be downloaded at http://clic.cimec.unitn.it/lambada/."
P16-1144,2016,5 Conclusion,we hope the computational community will be stimulated to develop novel language models that are genuinely capturing the non-local phenomena that lambada reflects.
P16-1145,2016,6 Conclusion,"after comparing a diverse array of models spanning classification and extraction, we conclude that end-to-end sequence to sequence models are the most promising."
P16-1145,2016,6 Conclusion,"document length poses a problem for rnn-based models, which might be addressed with convolutional neural networks that are easier to parallelize."
P16-1145,2016,6 Conclusion,"finally, we note that these models are not intrinsically limited to english, as they rely on little or no pre-processing with traditional nlp systems."
P16-1145,2016,6 Conclusion,"in light of this finding, we suggest some focus areas for future research."
P16-1145,2016,6 Conclusion,"our character-level model improved substantially after language model pretraining, suggesting that further training optimizations may yield continued gains."
P16-1145,2016,6 Conclusion,these models simultaneously learned to classify documents and copy arbitrary strings from them.
P16-1145,2016,6 Conclusion,"this means that they should generalize effectively to other languages, which could be demonstrated by a multilingual version of wikireading."
P16-1145,2016,6 Conclusion,we have demonstrated the complexity of the wikireading task and its suitability as a benchmark to guide future development of dnn models for natural language understanding.
P16-1146,2016,5 Conclusion,our results are comparable to other stateof-the-art results for these languages.
P16-1146,2016,5 Conclusion,"our results demonstrate that when comparing the expectationmaximization with coarse-to-fine techniques to our spectral algorithm with latent state optimization, spectral learning performs better on six of the datasets."
P16-1146,2016,5 Conclusion,using a diverse set of models to parse these datasets further improves the results.
P16-1146,2016,5 Conclusion,we demonstrated that a careful selection of the number of latent states in a latent-variable pcfg with spectral estimation has a significant effect on the parsing accuracy of the l-pcfg.
P16-1146,2016,5 Conclusion,"we described a search procedure to do this kind of optimization, and described parsing results for eight languages (with nine datasets)."
P16-1147,2016,6 Conclusions,"in future work, we will extend this idea beyond sequence modeling to improve models in nlp that utilize parse trees as features."
P16-1147,2016,6 Conclusions,our model outperforms all state-of-the-art parsers when evaluated on 19 languages of the universal dependencies treebank and outperforms other greedy models on the wall street journal.
P16-1147,2016,6 Conclusions,the basic tenet of stack-propagation is that the hidden layers of neural models used to generate annotations can be used instead of the annotations themselves.
P16-1147,2016,6 Conclusions,this suggests a new methodology to building deep neural models for nlp: we can design them from the ground up to incorporate multiple sources of annotation and learn far more effective intermediate representations.
P16-1147,2016,6 Conclusions,"through a simple learning method we call “stack-propagation,” our model learns effective intermediate representations for parsing by using pos tags as regularization of implicit representations."
P16-1147,2016,6 Conclusions,we observe that the ideas presented in this work can also be as a principled way to optimize upstream nlp components for down-stream applications.
P16-1147,2016,6 Conclusions,we present a stacking neural network model for dependency parsing and tagging.
P16-1148,2016,7 Summary,"further, we showed that one can accurately predict a wide range of perceived demographics of a user based solely on the emotions expressed by that user and user’s social environment."
P16-1148,2016,7 Summary,"moreover, our models for predicting user demographics can be effectively used for a variety of downstream nlp tasks e.g., text classification (hovy, 2015), sentiment analysis (volkova et al., 2013), paraphrasing (preotiucpietro et al., 2016), part-of-speech tagging (hovy and s?gaard, 2015; johannsen et al., 2015) and visual analytics (dou et al., 2015)."
P16-1148,2016,7 Summary,"our findings may advance the current understanding of social media population, their online behavior and well-being (nguyen et al., 2015)."
P16-1148,2016,7 Summary,our observations can effectively improve personalized intelligent user interfaces in a way that reflects and adapts to user-specific characteristics and emotions.
P16-1148,2016,7 Summary,our results indicated that many sociodemographic traits correlate with user-environment emotional contrast.
P16-1148,2016,7 Summary,we examined a large-scale twitter dataset to analyze the relation between perceived user demographics and the emotional contrast between users and their neighbors.
P16-1149,2016,7 Conclusion,"in this paper we present lobbyback, a system to reconstruct the “dark corpora” that is comprised of model bills which are copied (and modified) by resource constrained state legislatures."
P16-1149,2016,7 Conclusion,lobbyback first identifies clusters of text reuse in a large corpora of state legislation and then generates prototype sentences that summarizes the similarity and variation of the copied text in a bill cluster.
P16-1149,2016,7 Conclusion,"we believe that by open-sourcing lobbyback and releasing our data of prototype bills to the public, journalists and legal scholars can use our findings to better understand the origination of u.s state laws."
P16-1150,2016,5 Conclusion and future work,"in the current article, we have only slightly touched the annotated natural text reasons."
P16-1150,2016,5 Conclusion and future work,"we believe that the presence of 44k reasons (550k tokens) is another important asset of the newly created corpus, which deserves future investigation."
P16-1150,2016,5 Conclusion and future work,we crowdsourced a large corpus of 16k argument pairs over 32 topics and used global constraints based on transitivity properties of convincingness relation for cleaning the data.
P16-1150,2016,5 Conclusion and future work,we experimented with feature-rich svm and bidirectional lstm and obtain 0.76-0.78 accuracy and 0.35-0.40 spearman’s correlation in a crosstopic scenario.
P16-1150,2016,5 Conclusion and future work,we propose a novel task of predicting web argument convincingness.
P16-1150,2016,5 Conclusion and future work,"we release the newly created corpus ukpconvarg1 and the experimental software under free licenses.11 to the best of our knowledge, we are the first who deal with argument convincingness in web data on such a large scale."
P16-1151,2016,4 Conclusion,"in an application to candidate biographies, we discover a penalty for political and legal experience and a bonus for military service and non-legal advanced degrees."
P16-1151,2016,4 Conclusion,our methodology has a wide variety of applications.
P16-1151,2016,4 Conclusion,"the methodology is also useful for observational data—for studying the effects of complicated treatments, such as how a legislator’s roll call voting record affects their electoral support."
P16-1151,2016,4 Conclusion,the use of a training and test set ensures that our method provides accurate confidence intervals and avoids the problems of overfitting or “p-hacking” in experiments.
P16-1151,2016,4 Conclusion,"this includes numerous alternative experimental designs, providing a methodology that computational social scientists could use widely to discover and then confirm the effects of messages in numerous domains—including images and other high dimensional data."
P16-1151,2016,4 Conclusion,we have presented a methodology for discovering treatments in text and then inferring the effect of those treatments on respondents’ decisions.
P16-1151,2016,4 Conclusion,we prove that randomizing texts is sufficient to identify the underlying treatments and introduce the supervised indian buffet process for discovering the effects.
P16-1152,2016,5 Conclusion,"given the advantages of faster convergence and the fact that only relative feedback in terms of comparative evaluations is required, bandit pairwise preference learning is a promising framework for future real-world interactive learning."
P16-1152,2016,5 Conclusion,"however, the result can be explained by considering important empirical factors such as the variance of stochastic updates."
P16-1152,2016,5 Conclusion,"in light of the standard stochastic approximation analysis, which predicts a convergence advantage for strongly convex objectives over convex or non-convex objectives, this result is surprising."
P16-1152,2016,5 Conclusion,our experimental results on different nlp tasks showed a consistent advantage of convergence speed under this criterion for bandit pairwise preference learning.
P16-1152,2016,5 Conclusion,"our experimental results support the numerical results of smallest stochastic variance and fastest convergence in gradient norm (sokolov et al., 2016) by consistent fastest empirical convergence for bandit pairwise preference learning under the criterion of early stopping on heldout data performance."
P16-1152,2016,5 Conclusion,"we investigated the performance of all algorithms by test set performance on different tasks, however, the main interest of this paper was a comparison of convergence speed across different objectives by early stopping on a convergence criterion based on heldout data performance."
P16-1152,2016,5 Conclusion,"we presented objectives and algorithms for structured prediction from bandit feedback, with a focus on improving convergence speed and ease of elicitability of feedback."
P16-1153,2016,5 Conclusion,"future work includes: (i) adding an attention model to robustly analyze which part of state/actions text correspond to strategic planning, and (ii) applying the proposed methods to more complex text games or other tasks with actions defined through natural language."
P16-1153,2016,5 Conclusion,"in this paper we develop a deep reinforcement relevance network, a novel dnn architecture for handling actions described by natural language in decision-making tasks such as text games."
P16-1153,2016,5 Conclusion,we show that the drrn converges faster and to a better solution for q-learning than alternative architectures that do not use separate embeddings for the state and action spaces.
P16-1154,2016,7 Conclusion and Future Work,"for future work, we will extend this idea to the task where the source and target are in heterogeneous types, for example, machine translation."
P16-1154,2016,7 Conclusion and Future Work,we proposed copynet to incorporate copying into the sequence-to-sequence learning framework.
P16-1155,2016,6 Conclusions,"finally, a classifier was trained using this auxiliary training set, following a distribution normalizing instance weighing technique, to perform the classification task in the target domain."
P16-1155,2016,6 Conclusions,it proposed to extract partial transferable knowledge from across multiple source domains which was beneficial for inducing class-separability in the target domain.
P16-1155,2016,6 Conclusions,the efficacy of the proposed algorithm for cross-domain classification across disparate label sets will expand the horizon for ml-based algorithms to be more widely applicable in more general and practically observed scenarios.
P16-1155,2016,6 Conclusions,the transferable knowledge was assimilated in terms of selective labeled instances from different source domain to form a k-class auxiliary training set.
P16-1155,2016,6 Conclusions,this paper presented the first study on crossdomain text classification in presence of multiple domains with disparate label sets and proposed a novel algorithm for the same.
P16-1156,2016,9 Conclusion and Future Work,"for morphologically rich languages, we generally will not observe, even in a large corpus, a high proportion of the word forms that exist in lexical resources."
P16-1156,2016,9 Conclusion and Future Work,future work will consider the role of derivational morphology in embeddings as well as noncompositional cases of inflectional morphology.
P16-1156,2016,9 Conclusion and Future Work,we have presented a gaussian graphical model that exploits lexical relations documented in existing morphological resources to smooth vectors for observed words and extrapolate vectors for new words.
P16-1156,2016,9 Conclusion and Future Work,we show that our method achieves large improvements over strong baselines for the tasks of morpho-syntactic analogies and predicting words in context.
P16-1157,2016,7 Conclusion,"in contrast, for the syntactic task of dependency parsing, models that are supervised at a word alignment level perform slightly better."
P16-1157,2016,7 Conclusion,"on the other hand, for cross-lingual semantic tasks, like cross-lingual document classification and dictionary induction, the model with the most informative supervision performs best overall."
P16-1157,2016,7 Conclusion,"our choice of methods spans a diverse range of approaches, in that each requires a different form of supervision."
P16-1157,2016,7 Conclusion,our experiments reveal interesting trends.
P16-1157,2016,7 Conclusion,"overall this suggests that semantic tasks can benefit more from richer cross-lingual supervision, as compared to syntactic tasks."
P16-1157,2016,7 Conclusion,"we presented the first systematic comparative evaluation of cross-lingual embedding methods on several downstream nlp tasks, both intrinsic and extrinsic."
P16-1157,2016,7 Conclusion,"we provided a unified representation for all approaches, showing them as instances of a general algorithm."
P16-1157,2016,7 Conclusion,"when evaluating on intrinsic tasks such as monolingual word similarity, models relying on cheaper forms of supervision (such as bivcd) perform almost on par with models requiring expensive supervision."
P16-1158,2016,6 Conclusions,"classification performs less well over open data, although with the introduction of automatically-generated negative samples, the results improve substantially."
P16-1158,2016,6 Conclusions,"in contrast, classification over the diffvecs works extremely well in a closed-world setting, showing that dimensions of diffvecs encode lexical relations."
P16-1158,2016,6 Conclusions,negative sampling also improves classification when the training and test vocabulary are split to minimise lexical memorisation.
P16-1158,2016,6 Conclusions,"overall, we conclude that the diffvec approach has impressive utility over a broad range of lexical relations, especially under supervised classification."
P16-1158,2016,6 Conclusions,this paper is the first to test the generalisability of the vector difference approach across a broad range of lexical relations (in raw number and also variety).
P16-1158,2016,6 Conclusions,"using clustering we showed that many types of morphosyntactic and morphosemantic differences are captured by diffvecs, but that lexical semantic relations are captured less well, a finding which is consistent with previous work (koper ¨ et al., 2015)."
P16-1159,2016,6 Conclusion,"as our approach is transparent to loss functions and architectures, we believe that it will also benefit more end-to-end neural architectures for other nlp tasks."
P16-1159,2016,6 Conclusion,"experiments show that mrt leads to significant improvements over maximum likelihood estimation for neural machine translation, especially for distantly-related languages such as chinese and english."
P16-1159,2016,6 Conclusion,"in the future, we plan to test our approach on more language pairs and more end-to-end neural mt systems."
P16-1159,2016,6 Conclusion,"in this paper, we have presented a framework for minimum risk training in end-to-end neural machine translation."
P16-1159,2016,6 Conclusion,it is also interesting to extend minimum risk training to minimum risk annealing following smith and eisner (2006).
P16-1159,2016,6 Conclusion,the basic idea is to minimize the expected loss in terms of evaluation metrics on the training data.
P16-1159,2016,6 Conclusion,we sample the full search space to approximate the posterior distribution to improve efficiency.
P16-1160,2016,8 Conclusion,"however, this has allowed us a more fine-grained analysis, but in the future, a setting where the source side is also represented as a character sequence must be investigated."
P16-1160,2016,8 Conclusion,"in this paper, we addressed a fundamental question on whether a recently proposed neural machine translation system can directly handle translation at the level of characters without any word segmentation."
P16-1160,2016,8 Conclusion,"our extensive experiments, on four language pairs–en-cs, ende, en-ru and en-fi– strongly suggest that it is indeed possible for neural machine translation to translate at the level of characters, and that it actually benefits from doing so."
P16-1160,2016,8 Conclusion,our result has one limitation that we used subword symbols in the source side.
P16-1160,2016,8 Conclusion,"we focused on the target side, in which a decoder was asked to generate one character at a time, while soft-aligning between a target character and a source subword."
P16-1161,2016,7 Conclusions,our work is available in the main branch of moses for use by other researchers.
P16-1161,2016,7 Conclusions,we have provided an analysis showing concrete examples of improved lexical selection and morphological coherence.
P16-1161,2016,7 Conclusions,we have shown that such a model can be used directly during decoding in a relatively efficient way.
P16-1161,2016,7 Conclusions,we have shown that this model consistently significantly improves the quality of english-czech translation over a strong baseline with large training data.
P16-1161,2016,7 Conclusions,we have validated the effectiveness of our model on several additional language pairs.
P16-1161,2016,7 Conclusions,we presented a discriminative model for mt which uses both source and target context information.
P16-1162,2016,6 Conclusion,"in this work, our choice of vocabulary size is somewhat arbitrary, and mainly motivated by comparison to prior work."
P16-1162,2016,6 Conclusion,"one avenue of future research is to learn the optimal vocabulary size for a translation task, which we expect to depend on the language pair and amount of training data, automatically."
P16-1162,2016,6 Conclusion,"our analysis shows that not only out-ofvocabulary words, but also rare in-vocabulary words are translated poorly by our baseline nmt system, and that reducing the vocabulary size of subword models can actually improve performance."
P16-1162,2016,6 Conclusion,the main contribution of this paper is that we show that neural machine translation systems are capable of open-vocabulary translation by representing rare and unseen words as a sequence of subword units.14 this is both simpler and more effective than using a back-off translation model.
P16-1162,2016,6 Conclusion,"we also believe there is further potential in bilingually informed segmentation algorithms to create more alignable subword units, although the segmentation algorithm cannot rely on the target text at runtime."
P16-1162,2016,6 Conclusion,"we introduce a variant of byte pair encoding for word segmentation, which is capable of encoding open vocabularies with a compact symbol vocabulary of variable-length subword units."
P16-1162,2016,6 Conclusion,"we show performance gains over the baseline with both bpe segmentation, and a simple character bigram segmentation."
P16-1162,2016,6 Conclusion,"while the relative effectiveness will depend on language-specific factors such as vocabulary size, we believe that subword segmentations are suitable for most language pairs, eliminating the need for large nmt vocabularies or back-off models."
P16-1163,2016,5 Conclusion,experiment results on pdtb show the proposed model outperforms the existing methods using traditional features on all of the relations.
P16-1163,2016,5 Conclusion,"in order to preserve contextual information, we encode a sentence to its positional representation via a recurrent neural network, specifically, a lstm."
P16-1163,2016,5 Conclusion,"in this work, we propose to use word embeddings to fight against the data sparsity problem of word pairs."
P16-1163,2016,5 Conclusion,"to solve the semantic gap between the word pairs, we propose to use a gated relevance network which incorporates both the linear and nonlinear interactions between pairs."
P16-1164,2016,8 Conclusion,"a simple, greedy algorithm using non-sequential models of quotation boundaries rivals the crf’s performance."
P16-1164,2016,8 Conclusion,"for further improvements, we introduce a semi-markov model capable of taking into account global information about the complete span not available to a linear-chain crf, such as the presence of cues on both sides of the quotation candidate."
P16-1164,2016,8 Conclusion,"indeed, our analyses find that the features most important to recognize a quotation consider its direct context of orthographic evidence (such as quotation marks) and lexical evidence (such as cue words)."
P16-1164,2016,8 Conclusion,more expressive models however come with harder inference problems which are compounded when applied to longsequence tasks.
P16-1164,2016,8 Conclusion,"on a more general level, we believe that quotation detection is interesting as a representative of tasks involving long sequences, where markov assumptions become inappropriate."
P16-1164,2016,8 Conclusion,"other examples of such tasks include the identification of chemical compound names (krallinger et al., 2015) and the detection of annotator rationales (zaidan and eisner, 2008)."
P16-1164,2016,8 Conclusion,the informed sampling algorithm we have described performs such efficient inference for our semi-markov quotation detection model.
P16-1164,2016,8 Conclusion,this leads to a significant improvement of 3 points f1 over the state of the art.
P16-1164,2016,8 Conclusion,"we have considered the task of quotation detection, starting from the hypothesis that linear-chain crfs cannot take advantage of all available sequence information due to its markov assumptions."
P16-1164,2016,8 Conclusion,we have shown that a more expressive semi-markov model which avoids these assumptions can improve performance.
P16-1165,2016,6 Conclusions and Future Work,a lstm rnn first composes sentences into vector representations by considering the word order.
P16-1165,2016,6 Conclusions and Future Work,"in the future, we would like to combine crfs with lstms for doing the two steps jointly, so that the lstms can learn the embeddings using the global thread-level feedback."
P16-1165,2016,6 Conclusions and Future Work,"our results show that lstm rnns provide better representations but requires more data, and global joint models improve over local models given that it considers the right graph structure."
P16-1165,2016,6 Conclusions and Future Work,then a pairwise crf jointly models the intersentence dependencies in the conversation.
P16-1165,2016,6 Conclusions and Future Work,this would require the backpropagation algorithm to take error signals from the loopy bp inference.
P16-1165,2016,6 Conclusions and Future Work,"we experimented with different lstm variants (uni- vs. bi-directional, random vs. pretrained initialization), and different crf variants depending on the underlying graph structure."
P16-1165,2016,6 Conclusions and Future Work,we have presented a two-step framework for speech act recognition in asynchronous conversation.
P16-1165,2016,6 Conclusions and Future Work,"we trained our models on many different settings using synchronous and asynchronous corpora and evaluated on two forum datasets, one of which is presented in this work."
P16-1165,2016,6 Conclusions and Future Work,"we would also like to apply our models to conversations, where the graph structure is extractable using the meta data or other clues, e.g., the fragment quotation graphs for email threads (carenini et al., 2008)."
P16-1166,2016,8 Conclusion,"a next major step in our research agenda is to integrate se type information into various applications, including argument mining, temporal reasoning, and summarization."
P16-1166,2016,8 Conclusion,"among others, we plan to create subtypes of the state label, which currently subsumes clauses stativized by negation, modals, lexical information or other aspectual operators."
P16-1166,2016,8 Conclusion,"as we have reported before (friedrich et al., 2015), the most difficult case is the identification of generic sentences, which are defined as making a statement about a kind or class."
P16-1166,2016,8 Conclusion,distinguishing these is relevant for temporal relation processing or veridicality recognition.
P16-1166,2016,8 Conclusion,"examples include temporal processing of text (smith, 2008), summarization, or machine translation (for work on genres see van der wees et al., 2015)."
P16-1166,2016,8 Conclusion,"here we focus on the automatic identification of se types, leaving the identification of discourse modes to future work."
P16-1166,2016,8 Conclusion,"in addition, the distributional and targeted syntactic-semantic features we introduce enable se type prediction for large and diverse data sets."
P16-1166,2016,8 Conclusion,"our annotation scheme and guidelines for se types (friedrich and palmer, 2014b) follow established traditions in linguistics and semantic theory."
P16-1166,2016,8 Conclusion,"our publicly available system can readily be applied to any written english text, making it easy to explore the utility of se types for other nlp tasks.discussion."
P16-1166,2016,8 Conclusion,"the present work, using the se type inventory introduced by smith (2003), is also the basis for research on more fine-grained aspectual type inventories."
P16-1166,2016,8 Conclusion,the system benefits from capturing contexual effects by using a linear chain crf with label bigram features.
P16-1166,2016,8 Conclusion,"therefore the automatic labeling of clauses with their se type is a prerequisite for automatically identifying a passage’s discourse mode, which in turn has promising applications in many areas of nlp, as the mode of a text passage has implications for the linguistic phenomena to be found in the passage."
P16-1166,2016,8 Conclusion,"together with the mode of progression through the text, e.g., temporal or spatial, se type distribution is a key factor for a reader or listener’s intuitive recognition of the discourse mode of a text passage."
P16-1166,2016,8 Conclusion,"we find that making this task becomes particularly difficult for argumentative essays (becker et al., to appear)."
P16-1166,2016,8 Conclusion,"we have presented a system for automatically labeling clauses with their se type which is mostly robust to changes in genre and which reaches accuracies of up to 76%, comparing favorably to the human upper bound of 80%."
P16-1166,2016,8 Conclusion,"when applying these to a large number of natural texts, though, we came across a number of borderline cases where it is not easy to select just one se type label."
P16-1167,2016,8 Conclusion,"finally, we provide a dataset depicting 12 scenarios with ～1.5 m images for future research."
P16-1167,2016,8 Conclusion,future directions could include exploring nuances in the type of temporal knowledge that can be learned across different scenarios.
P16-1167,2016,8 Conclusion,"our event induction method incorporates learned knowledge about events, partitions photo albums into segments, and assigns events to those segments."
P16-1167,2016,8 Conclusion,we introduce a novel exploration to learn scriptlike knowledge from photo albums.
P16-1167,2016,8 Conclusion,we model stochastic event structure to learn both the event representations (textual and visual) and the temporal relations among those events.
P16-1167,2016,8 Conclusion,"we show the significance of our model in learning and using learned knowledge for photo ordering, album segmentation, and summarization."
P16-1168,2016,7 Conclusion,"pretraining the model using the english captions of 119,287 images was roughly equivalent to training the model using the captions of 10,000 additional images in japanese."
P16-1168,2016,7 Conclusion,"since this performance gain is obtained without modifying the original monolingual image caption generator, the proposed model can serve as a strong baseline for future research in this area."
P16-1168,2016,7 Conclusion,"this, in our case, nearly halves the cost of building a corpus."
P16-1168,2016,7 Conclusion,"we have created an image caption dataset for the japanese language by collecting 131,740 captions for 26,500 images using the yahoo!crowdsourcing service in japan."
P16-1168,2016,7 Conclusion,we hope that our dataset and proposed method kick start studies on cross-lingual image caption generation and that many others follow our lead.
P16-1168,2016,7 Conclusion,we showed that pretraining a neural image caption model with the english portion of the corpus improves the performance of a japanese caption generation model subsequently trained using japanese data.
P16-1169,2016,6 Conclusion,"in this paper, we study the problem of automatically inducing semantically meaningful concept taxonomies from multi-modal data."
P16-1169,2016,6 Conclusion,"we compare our model and features to previous ones on two different tasks using the imagenet hierarchies, and demonstrate superior performance of our model, and the effectiveness of exploiting visual contents for taxonomy induction."
P16-1169,2016,6 Conclusion,we further conduct qualitative studies and distinguish the relative importance of visual and textual features in constructing various parts of a taxonomy.
P16-1169,2016,6 Conclusion,we propose a probabilistic bayesian model which leverages distributed representations for images and words.
P16-1171,2016,7 Conclusion,"as object recognition and tracking are undergoing significant advancements in the computer vision field, such a resource together with causality detectors can be immediately applied for any applications that require grounded language understanding."
P16-1171,2016,7 Conclusion,"in the future, we plan to build a resource for modeling physical causality for action verbs."
P16-1171,2016,7 Conclusion,"on the other hand, the knowledge-based approach also achieves competitive performance (even better than previous learned models), without any training."
P16-1171,2016,7 Conclusion,our empirical evaluations have shown encouraging results for both approaches.
P16-1171,2016,7 Conclusion,"the most exciting aspect about the knowledge-based approach is that causality knowledge for verbs can be acquired from humans (e.g., through crowd-sourcing) and generalized to novel verbs about which we have not yet acquired causality knowledge."
P16-1171,2016,7 Conclusion,"this paper presents, to the best of our knowledge, the first attempt that explicitly models the physical causality of action verbs."
P16-1171,2016,7 Conclusion,we have applied causality modeling to the task of grounding semantic roles to the environment using two approaches: a knowledge-based approach and a learning-based approach.
P16-1171,2016,7 Conclusion,"when annotated data is available (in which semantic roles of verbs are grounded to physical objects), the learning-based approach, which learns the associations between verbs and causality detectors, achieves the best overall performance."
P16-1172,2016,Future Work,"essentially, our framework performs a modularization of the task of mds, where all characteristics of the data and feature representations are pushed into a separate machine learning module – they should not affect the subsequent optimization step which remains fixed."
P16-1172,2016,Future Work,"for example, we plan to incorporate lexicalsemantic information in the feature representation and leverage large-scale unsupervised pretraining."
P16-1172,2016,Future Work,"in this work, we developed a principled subset selection framework and empirically justified it."
P16-1172,2016,Future Work,the promising results we obtained for summarization with a basic learner (see section 4.3) encourage future work on plugging in more sophisticated supervised learners in our framework.
P16-1172,2016,Future Work,this direction is particularly promising because we have shown that we can expect significant performance gains for end-to-end mds as the sentence scoring component improves.
P16-1172,2016,Future Work,we focused on solving the second step of the framework while keeping the machine learning component as simple as possible.
P16-1173,2016,6 Conclusion,we defined a principled discrete optimization problem for sentence selection which relies on an approximation of rouge.
P16-1173,2016,6 Conclusion,we empirically checked the validity of the approach on standard datasets and observed that even with a basic learner the framework produces promising results.
P16-1173,2016,6 Conclusion,"we proposed a problem-reduction approach to extractive mds, which performs a reduction to the problem of scoring individual sentences with their rouge scores based on supervised learning."
P16-1174,2016,6 Conclusion,"finally, while we conducted a cursory analysis of model weights in §4.2, an interesting next step would be to study such weights for even deeper insight.(note that using lexeme tag component features, as suggested above, should make this anaysis more robust since features would be less sparse.)"
P16-1174,2016,6 Conclusion,"for example, one could see whether the ranking of vocabulary and/or grammar components by feature weight is correlated with external standards such as the cefr (council of europe, 2001)."
P16-1174,2016,6 Conclusion,"hlr combines a psycholinguistic model of human memory with modern machine learning techniques, and generalizes two popular algorithms used in language learning technology: leitner and pimsleur."
P16-1174,2016,6 Conclusion,"instead of the sparse indicator variables used here, it may be better to decompose lexeme tags into denser and more generic features of tag components9 (e.g., part of speech, tense, gender, case), and also use corpus frequency, word length, etc."
P16-1174,2016,6 Conclusion,"one result we found surprising was that lexeme tag features failed to improve predictions much, and in fact seemed to frustrate the student learning experience due to over-fitting."
P16-1174,2016,6 Conclusion,this and other uses of hlr hold the potential to transform data-driven curriculum design.
P16-1174,2016,6 Conclusion,"this approach is significantly more accurate at predicting student recall rates than either of the previous methods, and is also better than a conventional machine learning approach like logistic regression."
P16-1174,2016,6 Conclusion,this representation might be able to capture useful and interesting regularities without negative side-effects.
P16-1174,2016,6 Conclusion,we can do this by incorporating arbitrarily rich features and fitting their weights to data.
P16-1174,2016,6 Conclusion,"we have introduced half-life regression (hlr), a novel spaced repetition algorithm with applications to second language acquisition."
P16-1175,2016,7 Conclusion,both are novel contributions to the study of l2 acquisition.
P16-1175,2016,7 Conclusion,"our current model is arguably crude, with only 6 features, yet it can already often do a reasonable job of predicting what a user might guess and whether the user’s guess will be roughly correct."
P16-1175,2016,7 Conclusion,this opens the door to a number of future directions with applications to language acquisition using personalized content and learners’ knowledge.
P16-1175,2016,7 Conclusion,"we also leave as future work the integration of this model into an adaptive system that tracks learner understanding and creates scaffolded content that falls in their zone of proximal development, keeping them engaged while stretching their understanding."
P16-1175,2016,7 Conclusion,we have presented a methodology for collecting data and training a model to estimate a foreign language learner’s understanding of l2 vocabulary in partially understood contexts.
P16-1175,2016,7 Conclusion,we plan a deeper investigation into how learners detect and combine cues for incidental comprehension.
P16-1176,2016,7 Conclusion,"more dramatically, the language modeling experiments reveal salient ties between the native language of non-native speakers and the source language of translationese, highlighting the unified l1-related traces of l1 in both scenarios."
P16-1176,2016,7 Conclusion,"our findings are intriguing: native speakers and translators, in contrast to non-native speakers, use their native language, yet translation seems to gravitate towards non-native language use."
P16-1176,2016,7 Conclusion,"the main contribution of this work is empirical, establishing the connection between these types of language production."
P16-1176,2016,7 Conclusion,we also plan to investigate how the results vary when limited to specific l1s.
P16-1176,2016,7 Conclusion,"we are interested in expanding the preliminary results of this work: we intend to replicate the experiments with more languages and more domains, investigate additional varieties of constrained language and employ more complex lexical, syntactic and discourse features."
P16-1176,2016,7 Conclusion,"we demonstrated that while translations and nonnative productions are two distinct language varieties, they share similarities that stem from lower lexical richness, more careful choice of idiomatic expressions and pronouns, and (presumably) subconscious excessive usage of explicitation cohesive devices."
P16-1176,2016,7 Conclusion,"we presented a unified computational approach for studying constrained language, where many of the features were theoretically motivated."
P16-1176,2016,7 Conclusion,"while we believe that these common tendencies are not incidental, more research is needed in order to establish a theoretical explanation for the empirical findings, presumably (at least partially) on the basis of the cognitive load resulting from the simultaneous presence of two linguistic systems."
P16-1178,2016,7 Conclusions,"another finding was that fluency, completeness and richness are the aspects that best correlate with quality, while technicality, subjectivity and polarity aspects show a poor correlation with quality."
P16-1178,2016,7 Conclusions,"as future work, we plan to investigate other linguistic representations that can improve the automated extraction of the proposed aspects to better predict the article’s perceived quality."
P16-1178,2016,7 Conclusions,"in this paper, we proposed an annotated corpus for controlling the editorial quality of online news through 14 aspects related to editors perceived quality of news articles."
P16-1178,2016,7 Conclusions,"later, we showed that using the entire set of 14 aspects we could predict the text quality with an rmse of only 0.400 in a 5-point likertscale."
P16-1178,2016,7 Conclusions,"one important finding was that high quality articles share a significant amount of variability with several of the proposed aspects, which supports the claim that the proposed aspects may characterise news article quality in an automatic and scalable way."
P16-1178,2016,7 Conclusions,the judges assessed a total of 561 news articles with respect to 14 aspects.
P16-1178,2016,7 Conclusions,the study produced valuable insights.
P16-1178,2016,7 Conclusions,this renders a very effective decomposition of news article quality into the 14 aspects.
P16-1178,2016,7 Conclusions,this shows that the text comprehension and writing style are aspects that are more relevant than sentiment.
P16-1178,2016,7 Conclusions,"to this end, we performed an editorial study with expert judges either in computational linguistics, journalism, or media monitoring experts."
P16-1179,2016,9 Conclusions and Future Work,13 the analysis of the acquired information and the error analysis show several avenues for future work.
P16-1179,2016,9 Conclusions and Future Work,both resources are freely available for reproducibility.
P16-1179,2016,9 Conclusions and Future Work,"first larger corpora should allow to increase the applicability of the similarity resource, and specially, that of the dependency templates, and also provide better quality resources."
P16-1179,2016,9 Conclusions and Future Work,"in fact, when integrating all knowledge resources we yield the state-of-the-art in the tac kbp del 2014 dataset and get the third best results in the conll 2003 dataset."
P16-1179,2016,9 Conclusions and Future Work,"in this article we introduced two novel kinds of background information induced from corpora to the usual context of occurrence in ned: (1) given a mention we used distributionally similar entities as additional context; (2) given a mention and the syntactic dependencies in the context sentence, we used the selectional preferences of those syntactic dependencies as additional context."
P16-1179,2016,9 Conclusions and Future Work,we integrated them in a bayesian generative ned model which provides very strong results.
P16-1179,2016,9 Conclusions and Future Work,"we showed that similar entities are specially useful when no textual context is present, and that selectional preferences are useful when limited context is present."
P16-1180,2016,9 Conclusion and Future Work,another task for future work is semantic alignment.
P16-1180,2016,9 Conclusion and Future Work,"first, we do not rely on a parallel or comparable corpus, which are not as easily obtained; second, we produce typed templates that utilize a rich, fine-grained type system, which can make them more suitable for generation; and third, by using such a type system we are able to find paraphrases from sentence pairs that are not, before templatization, really paraphrases."
P16-1180,2016,9 Conclusion and Future Work,"in this paper we focused on a traditional distributional approach that has the advantage of being explainable, but it would be interesting and useful to explore other options such as word embeddings, matrix factorization and semantic similarity metrics."
P16-1180,2016,9 Conclusion and Future Work,"many, if not most, of the worst misidentifications seem to be the result of errors in the second stage of the approach - disambiguating the sense and specificity of the slot types."
P16-1180,2016,9 Conclusion and Future Work,"our approach discovers paraphrasal templates without aligning them to a semantic meaning representation; while these are perfectly usable by summarization, question answering, and other text-to-text generation applications, it would be useful for concept-to-text generation and other applications to have each cluster of templates aligned to a semantic representation of the meaning expressed."
P16-1180,2016,9 Conclusion and Future Work,"since we already discover all the entity types involved, all that is missing is the proposition (or frame, or set of propositions); this seems to be a straightforward, though not necessarily easy, task to tackle in the near future."
P16-1180,2016,9 Conclusion and Future Work,"we conducted a crowd-sourced human evaluation and showed that our method performs similarly to or better than prior work on mining paraphrases, with three major improvements."
P16-1180,2016,9 Conclusion and Future Work,we leave these to future work.
P16-1180,2016,9 Conclusion and Future Work,we presented a method for extracting paraphrasal templates from a plain text corpus in three steps: templatizing the sentences of the corpus; finding the most appropriate type for each slot; and clustering groups of templates that share the same set of types into paraphrasal sub-groups.
P16-1181,2016,8 Conclusion,"this effect only occurs when the training instances are very long, e.g., on whole documents, but not when training on single sentences."
P16-1181,2016,8 Conclusion,we also showed that the lower performance of early update and max-violation cannot be compensated for by increasing beam size or number of iterations.
P16-1181,2016,8 Conclusion,we compared our system for joint sentence boundary detection and dependency parsing to competitive pipeline systems showing that syntax can provide valuable information to sentence boundary prediction when punctuation and capitalization is not available.
P16-1181,2016,8 Conclusion,we have demonstrated that training a structured perceptron for inexact search on very long input sequences ignores significant portions of the training data when using early update or max-violation.
P16-1181,2016,8 Conclusion,"we then showed how this effect can be avoided by applying a different update strategy, dlaso, which leads to considerably better models in significantly less time."
P16-1182,2016,8 Conclusion,"finally, we demonstrate the process of how this analysis can guide the development and strengthening of newly created metrics that are developed."
P16-1182,2016,8 Conclusion,"not only is this evaluation procedure able to highlight particular metric weaknesses, it also demonstrates results which are far more consistent than correlation with human judgment; a good metric will be able to score well regardless of how noisy the human-derived scores are."
P16-1182,2016,8 Conclusion,the main contribution of this work is a novel approach for analyzing evaluation metrics for language generation tasks using metric unit tests.
P16-1183,2016,6 Conclusion,an alternative method of integration would be to move the decoder itself to gpu.
P16-1183,2016,6 Conclusion,"because it uses batch processing, our on-chip language model could be integrated into a machine translation decoder using strategies similar to those used to integrate an on-network language model nearly a decade ago (brants et al., 2007)."
P16-1183,2016,6 Conclusion,"for phrase-based translation, this would require a translation model and dynamic programming search algorithm on gpu."
P16-1183,2016,6 Conclusion,"our language model is implemented on a gpu, but its general design (and much of the actual code) is likely to be useful to other hardware that supports simd parallelism, such as the xeon phi."
P16-1183,2016,6 Conclusion,"translation models have been implemented on gpu by he et al.(2015), while related search algorithms for (chong et al., 2009; chong et al., 2008) and parsing (canny et al., 2013; hall et al., 2014) have been developed for gpu."
P16-1183,2016,6 Conclusion,we intend to explore these possibilities in future work.
P16-1184,2016,7 Conclusion,in this paper we proposed a method that can successfully induce morphological taggers for resource-scarce languages using tags projected across bitext.
P16-1184,2016,7 Conclusion,it relies on access to a morphological tagger for a source-language and a moderate amount of bitext.
P16-1184,2016,7 Conclusion,our results provide a strong baseline for future work in weakly supervised morphological tagging.
P16-1184,2016,7 Conclusion,the method obtains strong performance on a range of language pairs.
P16-1184,2016,7 Conclusion,we showed that downstream tasks such as dependency parsing can be improved by using the predictions from the tagger as features.
P16-1185,2016,5 Conclusion,"another interesting direction is to enhance the connection between source-to-target and target-tosource models (e.g., letting the two models share the same word embeddings) to help them benefit more from interacting with each other."
P16-1185,2016,5 Conclusion,"as our method is sensitive to the oovs present in monolingual corpora, we plan to integrate jean et al.(2015)’s technique on using very large vocabulary into our approach."
P16-1185,2016,5 Conclusion,experiments on chineseenglish nist datasets show that our approach leads to significant improvements.
P16-1185,2016,5 Conclusion,it is also necessary to further validate the effectiveness of our approach on more language pairs and nmt architectures.
P16-1185,2016,5 Conclusion,the central idea is to introduce autoencoders on the monolingual corpora with source-totarget and target-to-source translation models as encoders and decoders.
P16-1185,2016,5 Conclusion,we have presented a semi-supervised approach to training bidirectional neural machine translation models.
P16-1186,2016,6 Conclusions,a combination of the two is therefore very effective.
P16-1186,2016,6 Conclusions,"an interesting future direction is to combine complementary approaches, either through combined parameterization (e.g.hierarchical softmax with differentiated capacity per word) or through a curriculum (e.g.transitioning from target sampling to regular softmax as training progresses)."
P16-1186,2016,6 Conclusions,"compared to classical kneser-ney models, neural models are better at modeling frequent words, but are less effective for rare words."
P16-1186,2016,6 Conclusions,further promising areas are parallel training as well as better rare word modeling.
P16-1186,2016,6 Conclusions,"furthermore, we extend infrequent normalization to be a proper estimator of likelihood and we introduce differentiated softmax, a novel variant of softmax assigning less capacity to rare words to reduce computation."
P16-1186,2016,6 Conclusions,"in our setting, target sampling and noise contrastive estimation failed to outperform the softmax baseline."
P16-1186,2016,6 Conclusions,our results show that methods which are effective on small vocabularies are not necessarily equally so on large vocabularies.
P16-1186,2016,6 Conclusions,"overall, differentiated softmax and hierarchical softmax are the best strategies for large vocabularies."
P16-1186,2016,6 Conclusions,this paper presents a comprehensive analysis of strategies to train neural language models with large vocabularies.
P16-1186,2016,6 Conclusions,this setting is very challenging for neural networks as they need to compute the partition function over the entire vocabulary at each evaluation.
P16-1186,2016,6 Conclusions,"we compared classical softmax to hierarchical softmax, target sampling, noise contrastive estimation and infrequent normalization, commonly referred to as self-normalization."
P16-1186,2016,6 Conclusions,we conclude that there is a lot to explore in training from a combination of normalized and unnormalized objectives.
P16-1187,2016,5 Conclusions,"as future work, we plan to examine the use of a voting scheme for combining the output of complementary dsms."
P16-1187,2016,5 Conclusions,"evaluation on 3 english datasets and a french one revealed that a large dimension is consistently better, and corpus preprocessing is usually beneficial."
P16-1187,2016,5 Conclusions,"in this paper we presented a multilingual, largescale evaluation of dsms for compound compositionality prediction."
P16-1187,2016,5 Conclusions,"moreover, the results obtained were comparable and even outperformed the state-of-the-art."
P16-1187,2016,5 Conclusions,"moreover, we also plan to combine additional sources of information for building the models, such as multilingual resources or translation data, to improve even further the compositionality prediction."
P16-1187,2016,5 Conclusions,"the choice of window size varies according to language and dataset, but a small window can often provide a good performance."
P16-1187,2016,5 Conclusions,the dsms w2v and ppmi alternated in providing the best results.
P16-1187,2016,5 Conclusions,"we have built 816 dsms and performed 2,856 evaluations, examining the impact of corpus and context parameters, namely the level of corpus preprocessing, the context window size and the number of dimensions."
P16-1187,2016,5 Conclusions,we would also like to propose and evaluate more sophisticated compositionality functions that take into account the unbalanced contribution of individual words to the global meaning of a compound.
P16-1188,2016,5 Conclusion,our system improves substantially over baseline systems on rouge while still maintaining good linguistic quality.
P16-1188,2016,5 Conclusion,we integrate a compression model that enforces grammaticality as well as pronoun anaphoricity constraints that enforce coherence.
P16-1188,2016,5 Conclusion,we presented a single-document summarization system trained end-to-end on a large corpus.
P16-1189,2016,6 Conclusions,"building on fairly standard components for the computation of similarity, this method is shown to perform better than current alternatives."
P16-1189,2016,6 Conclusions,"it thus allows for the fast deployment of a crucial component in comparable corpora alignment, which opens the path for an increase in the amount of such corpora that can be exploited in the future."
P16-1189,2016,6 Conclusions,"stacc also performed better than competing approaches on noisier corpora, showing promises for the exploitation of the typically noisy data found when mining comparable corpora."
P16-1189,2016,6 Conclusions,stacc is a highly portable method which requires no adaptation for its application to new domains and language pairs.
P16-1189,2016,6 Conclusions,"the approach was evaluated on a large range of datasets from various domains for ten language pairs, giving the best results overall when compared to sophisticated state-of-the-art methods."
P16-1189,2016,6 Conclusions,"we described a simple approach to comparable sentence alignment, termed stacc, which is based on automatically extracted seed lexical translations, the jaccard similarity coefficient, and simple set expansion operations that target named entities, numbers, and morphological variation using longest common prefixes."
P16-1190,2016,6 Conclusions,"on the ted corpus, we obtained the highest reported scores for 10 out of 11 languages, using an auxiliary task with pre-trained english embeddings."
P16-1190,2016,6 Conclusions,"our method achieved state-of-the-art accuracy on the reuters rcv1/rcv2 cross-lingual classification task in the english to german direction, while being extremely simple and computationally efficient."
P16-1190,2016,6 Conclusions,"our results in the reuters rcv1/rcv2 task, obtained using europarl v7 as parallel data, show that our method has no trouble handling different levels of representations simutaneously (document, sentence and word)."
P16-1190,2016,6 Conclusions,this allows learning task-specific multilingual embeddings together with a classifier for the task.
P16-1190,2016,6 Conclusions,we proposed a new formulation which jointly minimizes a combination of a supervised loss function with a multilingual co-regularization term using unlabeled parallel data.
P16-1191,2016,8 Conclusions and Future Work,"in follow-up work, we aim to apply our embedding method on smaller, yet gold-standard corpora such as semcor (miller et al., 1994) and streusle (schneider and smith, 2015) to examine the impact of the corpus choice in detail and extend the training data beyond wordnet vocabulary."
P16-1191,2016,8 Conclusions and Future Work,"moreover, the coarse semantic categorization contained in supersenses was shown to be preserved in translation (schneider et al., 2013), making them a perfect candidate for a multilingual adaptation of the vector space, e.g.extending faruqui and dyer (2014)."
P16-1191,2016,8 Conclusions and Future Work,the outcomes of this work are available to the research community.11.
P16-1191,2016,8 Conclusions and Future Work,"we demonstrated the utility of these embeddings for predicting supersenses and manifested that the supersense enrichment can lead to a significant improvement in a range of downstream classification tasks, using our embeddings in a neural network model."
P16-1191,2016,8 Conclusions and Future Work,"we have presented a novel joint embedding set of words and supersenses, which provides a new insight into the word and supersense positions in the vector space."
P16-1192,2016,8 Conclusion,"a java implementation of our algorithms is available as part of the alto parser, http:// bitbucket.org/tclup/alto."
P16-1192,2016,8 Conclusion,"an evaluation on practical data from three different grammar formalisms shows consistent speed improvements of several orders of magnitude, and our graph parser has the fastest published runtimes."
P16-1192,2016,8 Conclusion,"in the presence of a probability model (e.g.for irtg encodings of pcfgs), our algorithms could be made faster through the use of appropriate pruning techniques."
P16-1192,2016,8 Conclusion,it would also be interesting to combine the strengths of the condensed and sibling-finder algorithms for further efficiency gains.
P16-1192,2016,8 Conclusion,"these can be used to implement a generic algorithm for irtg parsing, and apply directly to any grammar formalism that can be represented as an irtg."
P16-1192,2016,8 Conclusion,"we focused here purely on symbolic parsing, and on computing complete parse charts."
P16-1192,2016,8 Conclusion,we have presented novel algorithms for computing the intersection and the inverse homomorphic image of finite tree automata.
P16-1193,2016,6 Conclusion,"a crucial distinction between the semisupervised models here and much previous work is that they learn a mapping into a vector space which represents entailment, rather than learning a parametrised entailment classifier."
P16-1193,2016,6 Conclusion,and we used this framework to derive vector operators for entailment and vector inference equations for entailment graphs.
P16-1193,2016,6 Conclusion,"further work is needed to explore the full power of these abilities to extract information about entailment from both unlabelled text and labelled entailment data, encode it all in a single vector space, and efficiently perform complex inferences about vectors and entailments."
P16-1193,2016,6 Conclusion,"in this work, we propose a vector-space model which provides a formal foundation for a distributional semantics of entailment."
P16-1193,2016,6 Conclusion,semi-supervised evaluations confirm these results.
P16-1193,2016,6 Conclusion,"this framework allows us to reinterpret word2vec as approximating an entailment-based distributional semantic model of words in context, and show that more accurate interpretations result in more accurate unsupervised models of lexical entailment, achieving better accuracies than previous models."
P16-1193,2016,6 Conclusion,this future work on compositional distributional semantics should further demonstrate the full power of the proposed framework for modelling entailment in a vector space.
P16-1193,2016,6 Conclusion,we developed a mean-field approximation to probabilistic entailment between vectors which represent known versus unknown features.
P16-1193,2016,6 Conclusion,"within this new vector space, the entailment operators and inference equations apply, thereby generalising naturally from these lexical representations to the compositional semantics of multi-word expressions and sentences."
P16-1194,2016,5 Conclusions,"a dialogue may actually have a hierarchy structure of latent states, therefore the proposed model can be extended to a deep model to capture more complex structures."
P16-1194,2016,5 Conclusions,"according to recent study (srivastava et al., 2013), a deep network model exhibits much benefits for latent variable learning."
P16-1194,2016,5 Conclusions,another possible way to extend the model is to consider modeling long distance dependency between latent states.
P16-1194,2016,5 Conclusions,"compared with traditional models, the proposed hssm has more powerful ability of discovering structures of latent states and modeling different word sources, including latent states, dialogue specific topics and global general topic."
P16-1194,2016,5 Conclusions,qualitative evaluations and quantitative experimental results demonstrate that the proposed model achieves better performance than state-of-the-art approaches.
P16-1194,2016,5 Conclusions,this may further improve the model’s performance.
P16-1194,2016,5 Conclusions,"we develope an undirected generative model, hssm, for dialogue structure analysis, and examine the effectiveness of our model on two different datasets, twitter posts occurred in open-domain and task-oriented dialogues from airline ticket booking domain."
P16-1195,2016,8 Conclusion,"in future work, we plan to develop better models for capturing the structure of the input, as well as extend the use of our system to other applications such as automatic documentation of source code."
P16-1195,2016,8 Conclusion,"in this paper, we presented code-nn , an endto-end neural attention model using lstms to generate summaries of c# and sql code by learning from noisy online programming websites."
P16-1195,2016,8 Conclusion,"our model outperforms competitive baselines and achieves state of the art performance on automatic metrics, namely meteor and bleu, as well as on a human evaluation study."
P16-1195,2016,8 Conclusion,"we also used code-nn to answer programming questions by retrieving the most appropriate code snippets from a corpus, and beat previous baselines for this task in terms of mrr."
P16-1195,2016,8 Conclusion,"we have published our c# and sql datasets, the accompanying human annotated test sets, and our code for the tasks described in this paper."
P16-1196,2016,4 Conclusion and Future Work,"in addition we plan to further investigate how to fine-tune some of the hyper parameters of the cpm such as spline scaling, single global scaling factor, convergence tolerance, and initialization of the latent trace with a centroid."
P16-1196,2016,4 Conclusion and Future Work,"in addition, to aid cpm convergence to a good local optimum, in future work we will investigate dimensionality reduction approaches that are reversible such as principal component analysis (pearson, 1901) and other pre-processing approaches similar to (listgarten, 2007), where the training data set is coarsely pre-aligned and pre-scaled based on the center of mass of the time series."
P16-1196,2016,4 Conclusion and Future Work,"in subsequent work, we would like to explore alternatives for enhancing cpms by incorporating contextual features in the training data set such as timing of hand movements, and preceding, succeeding, and co-occurring facial expressions."
P16-1196,2016,4 Conclusion and Future Work,"specifically, this paper has focused on the synthesis of syntactic asl facial expressions, which are essential to sentence meaning, using a data-driven methodology in which recordings of human asl signers are used as a basis for generating face and head movements for animation."
P16-1196,2016,4 Conclusion and Future Work,"through both a metric evaluation and an experimental user study, we found that the facial expressions driven by our cpm models produce high-quality facial expressions that are more similar to human performance of novel sentences."
P16-1196,2016,4 Conclusion and Future Work,"to avoid idiosyncratic aspects of a single performance, we have modeled a facial expression based on the underlying trace of the movement trained on multiple recordings of different sentences where this type of facial expression occurs."
P16-1196,2016,4 Conclusion and Future Work,"to facilitate the creation of asl content that can easily be updated or maintained, we have investigated technologies for automating the synthesis of asl animations from a sparse representation of the message."
P16-1196,2016,4 Conclusion and Future Work,"we assessed our modeling approach through comparison to an alternative centroid approach, where a single performance was selected as a representative."
P16-1196,2016,4 Conclusion and Future Work,"we obtain the latent trace with continuous profile model (cpm), a probabilistic generative model that relies on hidden markov models."
P16-1196,2016,4 Conclusion and Future Work,"while this work used the latent trace as the basis for animation, in future work, we also plan to explore methods for sampling from the model to produce variations in face and head movement."
P16-1197,2016,5 Conclusion,"although high annual returns are achieved simply by utilizing sentiment labels while trading, they can be increased by incorporating the output of the svm’s decision function."
P16-1197,2016,5 Conclusion,but classification accuracy alone is not an accurate predictor of task-based performance.
P16-1197,2016,5 Conclusion,"deriving sentiment labels for supervised training is an important topic for future study, as is inferring the sentiment of published news from stock price fluctuations instead of the reverse."
P16-1197,2016,5 Conclusion,"in four task-based experiments, we evaluated the usefulness of sentiment analysis to simple trading strategies."
P16-1197,2016,5 Conclusion,"in this paper, we examined sentiment analysis applied to stock trading strategies."
P16-1197,2016,5 Conclusion,"our price data only included adjusted opening and closing prices and most of our news data contain only the date of the article, with no specific time."
P16-1197,2016,5 Conclusion,"so sentiment analysis is an important component, but it must be tuned against task data."
P16-1197,2016,5 Conclusion,"this calls into question the suitability of intrinsic sentiment classification accuracy, particularly (as here) when the relative cost of a task-based evaluation may be comparably low."
P16-1197,2016,5 Conclusion,this limits our ability to test much shorterterm trading strategies.
P16-1197,2016,5 Conclusion,"this study has used a rather general definition of news sentiment, and a more precise definition may improve trading performance."
P16-1197,2016,5 Conclusion,we built a binary sentiment classifier that achieves high accuracy when tested on movie data and financial news data from reuters.
P16-1197,2016,5 Conclusion,we have also determined that training on human-annotated sentiment does in fact perform better than training on market returns themselves.
P16-1197,2016,5 Conclusion,we should also study how “sentiment” is defined in the financial world.
P16-1198,2016,6 Discussion,"beyond its practical implications, our work provides a novel intellectual contribution in demonstrating the power of undirected graph based methods in solving an nlp problem that is directed in nature."
P16-1198,2016,6 Discussion,"finally, the complementary nature of the directed and undirected parsers motivates the development of methods for their combination, such as dual decomposition (e.g."
P16-1198,2016,6 Discussion,"for example, mcdonald et al.(2005b) observed that k calls to the clu algorithm might prove to be too inefficient; our more efficient algorithm may provide the remedy.?"
P16-1198,2016,6 Discussion,"in addition, we are currently exploring potential extensions of the techniques presented in this paper to higher order, projective and non-projective, dependency parsing."
P16-1198,2016,6 Discussion,in extensive multilingual experiments our model performs very similarly to a standard directed firstorder parser.
P16-1198,2016,6 Discussion,it may also be utilized as an inference/initialization subroutine as a part of more complex approximation frameworks such as belief propagation (e.g.
P16-1198,2016,6 Discussion,"moreover, our results demonstrate the complementary nature of the models, with our model outperforming its directed counterpart on an average of 22.2% of the test sentences."
P16-1198,2016,6 Discussion,our algorithm may be used for efficient mst computation in k-best trees methods which are instrumental in margin-based training algorithms.
P16-1198,2016,6 Discussion,"particularly, we have shown that our undirected inference algorithm converges to a different solution than the standard directed solution while still maintaining high quality (table 2)."
P16-1198,2016,6 Discussion,"rush et al.(2010), koo et al.(2010a))."
P16-1198,2016,6 Discussion,"smith and eisner (2008), gormley et al.(2015)).?"
P16-1198,2016,6 Discussion,such techniques can exploit this diversity to produce a higher quality unified solution.
P16-1198,2016,6 Discussion,the potential embodied in this work extends to a number of promising research directions: ?
P16-1198,2016,6 Discussion,we believe this contribution has the potential to affect future research on additional nlp problems.
P16-1198,2016,6 Discussion,we intend to investigate all of these directions in future work.
P16-1198,2016,6 Discussion,we present a first-order graph-based dependency parsing model which runs in edge linear time at expectation and with very high probability.
P16-1199,2016,6 Conclusion and Future Works,"another extension is to extract topic hierarchies by integrating the conversation structures into hierarchical topic models like hlda (blei et al., 2003a) to extract fine-grained topics from microblog posts."
P16-1199,2016,6 Conclusion and Future Works,"by rigorously comparing our proposed model with a number of competitive baselines on real-world microblog datasets, we have demonstrated the effectiveness of using conversation structures to help model topics embedded in short and colloquial microblog messages."
P16-1199,2016,6 Conclusion and Future Works,"in the next step, we plan to exploit fine-grained discourse structures, e.g., dialogue acts (ritter et al., 2010), and propose a unified model that jointly inferring discourse roles and topics of posts in context of conversation tree structures."
P16-1199,2016,6 Conclusion and Future Works,this paper has proposed a novel topic model by considering the conversation tree structures of microblog posts.
P16-1199,2016,6 Conclusion and Future Works,"this work has proven that detecting leaders and followers, which are coarse-grained discourse derived from conversation structures, is useful to model microblogging topics."
P16-1200,2016,5 Conclusion and Future Works,cnn is one of the effective neural networks for neural relation extraction.
P16-1200,2016,5 Conclusion and Future Works,"in experiments, we evaluate our model on relation extraction task."
P16-1200,2016,5 Conclusion and Future Works,"in the future, we will explore the following directions: ?"
P16-1200,2016,5 Conclusion and Future Works,"in the future, we will incorporate our instance-level selective attention technique with those models for relation extraction."
P16-1200,2016,5 Conclusion and Future Works,"in this paper, we develop cnn with sentencelevel selective attention."
P16-1200,2016,5 Conclusion and Future Works,it can be used in not only distant supervised relation extraction but also other multi-instance learning tasks.
P16-1200,2016,5 Conclusion and Future Works,our model can make full use of all informative sentences and alleviate the wrong labelling problem for distant supervised relation extraction.
P16-1200,2016,5 Conclusion and Future Works,our model incorporates multi-instance learning with neural network via instance-level selective attention.
P16-1200,2016,5 Conclusion and Future Works,researchers also propose many other neural network models for relation extraction.
P16-1200,2016,5 Conclusion and Future Works,the experimental results show that our model significantly and consistently outperforms state-of-the-art featurebased methods and neural network methods.
P16-1200,2016,5 Conclusion and Future Works,we will explore our model in other area such as text categorization.?
P16-1201,2016,6 Conclusions and Future Work,"attack may be divided into terrorism, invading, etc.)."
P16-1201,2016,6 Conclusions and Future Work,"consequently, we obtain a remarkable improvement and achieve a new state-of-the-art result for the ed task."
P16-1201,2016,6 Conclusions and Future Work,"event detection is only a component of the overall task of event extraction, which also includes event role detection."
P16-1201,2016,6 Conclusions and Future Work,"finally, we alleviate the data sparseness problem of ed by using events detected from fn as extra training data."
P16-1201,2016,6 Conclusions and Future Work,"for evaluation, we first conduct manual evaluations on events detected from fn."
P16-1201,2016,6 Conclusions and Future Work,"for example, all kinds of violent acts, such as street fights and wars, are treated as a single event type attack."
P16-1201,2016,6 Conclusions and Future Work,"furthermore, based on the detected results, we analyze the mappings from frames/lus to event types."
P16-1201,2016,6 Conclusions and Future Work,"furthermore, event schemas in ace are quite coarse."
P16-1201,2016,6 Conclusions and Future Work,"in addition, we also perform automatic evaluations."
P16-1201,2016,6 Conclusions and Future Work,"in the future, we will extend this work to the complete event extraction task."
P16-1201,2016,6 Conclusions and Future Work,"motivated by the high similarity between frames and events, we conduct this work to study their relations."
P16-1201,2016,6 Conclusions and Future Work,the key of this research is to detect events in fn.
P16-1201,2016,6 Conclusions and Future Work,the results further demonstrate the effectiveness of our proposed approach for detecting events in fn.
P16-1201,2016,6 Conclusions and Future Work,the results reveal that our hypotheses are very useful and it is an effective way to jointly utilize them as soft rules through psl.
P16-1201,2016,6 Conclusions and Future Work,"to solve this problem, we proposed a psl-based global inference approach based on three hypotheses between frames and events."
P16-1201,2016,6 Conclusions and Future Work,we plan to refine the event schemas by the finer-grained frames defined in fn (i.e.
P16-1202,2016,6 Conclusion,"in this research, we use well-known math formulas to model the word problem and develop an algorithm that learns to map the assertions in the story to the correct formula."
P16-1202,2016,6 Conclusion,our future plan is to apply this model to general arithmetic problems which require multiple applications of formulas.
P16-1202,2016,6 Conclusion,solving math word problems often requires explicit modeling of the word.
P16-1203,2016,6 Conclusions and Future Work,a further contribution of our work is that we ran an annotation campaign and created an annotated corpus of fictional movie characters and their corresponding polarity.
P16-1203,2016,6 Conclusions and Future Work,"character polarity annotations in written literature could be created by, for example, applying sentiment analysis to the full text of the work."
P16-1203,2016,6 Conclusions and Future Work,"furthermore, we have discovered that the most discriminative features are of phonological nature, rather than features that hint at pragmatic information such as the gender or origin of the character."
P16-1203,2016,6 Conclusions and Future Work,"in addition, using written literature will allow us to compare texts from different periods, pushing earlier than the relatively young age of motion pictures."
P16-1203,2016,6 Conclusions and Future Work,"in this paper we test the hypothesis that the sound and the form of fictional characters’ names correlates with meaning, in our particular case with the respective characters’ role in the work of fiction."
P16-1203,2016,6 Conclusions and Future Work,"it would, for example, be interesting to seek differences between spoken names (as in films) and names that are only meant to be read (as in literature)."
P16-1203,2016,6 Conclusions and Future Work,"our experiments have verified that features intrinsic to the names and without any reference to the plot or, in general, any other context are discriminative."
P16-1203,2016,6 Conclusions and Future Work,our future research will test the correlation between the polarity and the name of a fictional character beyond the movie domain.
P16-1203,2016,6 Conclusions and Future Work,"this corpus is offered publicly, and can serve experimentation in the digital humanities beyond the scope of the experiments presented here."
P16-1203,2016,6 Conclusions and Future Work,"we restricted our study to fictional characters since they are not tied to cultural conventions of naming, such as names that run in a family, so that we are able to look for patterns that are perceived as positive or negative by the audience and used as such (consciously or not) by the creator."
P16-1204,2016,9 Conclusion,"an composition is capable of producing a range of natural logic entailment relationship, at odds with commonly-used heuristics which treat all adjectives a restrictive."
P16-1204,2016,9 Conclusion,"the add-one entailment task we have introduced will allow ongoing rte research to better diagnose systems’ abilities to capture these subtleties of ans, which that have practical effects on natural language inference."
P16-1204,2016,9 Conclusion,"this is an important distinction for carrying out human-like reasoning, and our results reveal important weaknesses in the representations and algorithms employed by current nlu systems."
P16-1204,2016,9 Conclusion,"we have investigated the problem of adjective-noun composition, specifically in relation to the task of rte."
P16-1204,2016,9 Conclusion,"we have shown that predicting these entailment relations is dependent on context and on world knowledge, making it a difficult problem for current nlu technologies."
P16-1204,2016,9 Conclusion,"when tested, state-of-the-art rte systems fail to learn to differentiate entailmentpreserving insertions of adjectives from non-entailing ones."
P16-1205,2016,6 Conclusion,"compared to the best baselines, our approach yielded relative error reductions of 11.3% and 5.3%, in micro and macro fscore, respectively."
P16-1205,2016,6 Conclusion,"in an evaluation on 826 argumentative essays, our learning-based approach, which combines our novel features with n-gram features and faulkner’s features, significantly outperformed four baselines, including our re-implementation of faulkner’s system."
P16-1205,2016,6 Conclusion,"nevertheless, accurately predicting the somewhat, neutral, and never addressed stances remains a challenging task."
P16-1205,2016,6 Conclusion,"to stimulate further research on this task, we make all of our stance annotations publicly available."
P16-1205,2016,6 Conclusion,"we addressed this task by proposing two novel types of features, stancetaking path-based features and knowledge-based features."
P16-1205,2016,6 Conclusion,"we examined the new task of fine-grained essay stance classification, in which we determine stance for each prompt part and allow stance to take one of six values."
P16-1206,2016,8 Conclusion,"additionally, the proposed evaluation metric can be easily extended to word segmentation task for other languages (e.g."
P16-1206,2016,8 Conclusion,experiment results reveal that our weighted evaluation metrics gives more reasonable and distinguishable scores and correlates well with human judgement.
P16-1206,2016,8 Conclusion,"in this paper, we put forward a new psychometric-inspired method for chinese word segmentation evaluation by weighting all the words in test dataset based on the methodology applied to psychological tests and standardized exams."
P16-1206,2016,8 Conclusion,"japanese) and other sequence labelling-based nlp tasks, with just tiny changes."
P16-1206,2016,8 Conclusion,our metric also points out a promising direction for the researchers to take into the account of the biased distribution of test case difficulty and focus on tackling the hard bones of natural language processing.
P16-1206,2016,8 Conclusion,we empirically analyze the validity and reliability of the new metric on a real evaluation dataset.
P16-1206,2016,8 Conclusion,we will release the weighted datasets to the academic community.
P16-1207,2016,7 Conclusion,"a drawback of the proposed scheme is the lack of temporal ordering of events beyond the smallest unit of granularity, which was in our case one day."
P16-1207,2016,7 Conclusion,"another option is to increase the granularity, but this requires that the information in the documents also allow this more precise anchoring."
P16-1207,2016,7 Conclusion,"as a consequence, inferring 2https://www.ukp.tu-darmstadt.de/data /timeline-generation/temporal-anchoring -of-events/ from tlinks when an event happened is less precise as temporal information that is more than one sentence away can often not be taken into account."
P16-1207,2016,7 Conclusion,"as figure 2 depicts, in more than 58.72% of the cases the most informative temporal expression is more than one sentence apart from the event mention."
P16-1207,2016,7 Conclusion,"even with this restriction, the annotation effort is quite significant, as on average 6.3 links per mention must be annotated."
P16-1207,2016,7 Conclusion,"for 187 single day events, no event time at all could be inferred, as no temporal expression was within the one sentence window of that event."
P16-1207,2016,7 Conclusion,"for the 872 single day events, the correct event time could be inferred from the tlinks only in 487 cases."
P16-1207,2016,7 Conclusion,"in case the temporal ordering is important for the application scenario, the annotation scheme could be extended and tlinks could be annotated for events that fall on the same date."
P16-1207,2016,7 Conclusion,"the annotation guidelines as well as the annotated corpus are publicly available.2 in the performed annotation study, the krippendorff’s α inter-annotator agreement was considerably high at α = 0.617."
P16-1207,2016,7 Conclusion,the effort for annotating tlinks on the other hand scales quadratic with the number of events and temporal expressions.
P16-1207,2016,7 Conclusion,the largest disagreement resulted from events in which it was not explicitly mentioned when the event happened.
P16-1207,2016,7 Conclusion,"the scheme is suitable to note that several events occurred at the same date, but their order on that date cannot be encoded."
P16-1207,2016,7 Conclusion,this imposes the often used restriction that only temporal links between events and temporal expressions in the same or in succeeding sentences are annotated.
P16-1207,2016,7 Conclusion,"using a more relaxed measure for krippendorff’s α which only assigns a distance to mutual exclusive annotations, the agreement changed to αme = 0.912."
P16-1207,2016,7 Conclusion,"we can conclude that after little training, annotators are able to perform the annotation with a high agreement."
P16-1207,2016,7 Conclusion,"we presented a new annotation scheme for anchoring events in time and annotated a subset of the timebank corpus (pustejovsky et al., 2003) using this annotation scheme."
P16-1208,2016,6 Discussion and Conclusions,a recent surge in gec research has produced two leading state-of-the-art approaches – machine learning classification and machine translation.
P16-1208,2016,6 Discussion and Conclusions,"as a result, we built robust systems and showed substantial improvement over existing state-of-the-art."
P16-1208,2016,6 Discussion and Conclusions,"based on the analysis of the methods and an error analysis on the outputs of state-of-the-art systems that adopt these approaches, we explained the differences and the key advantages of each."
P16-1208,2016,6 Discussion and Conclusions,"for future work, we intend to study the problem in the context of other languages."
P16-1208,2016,6 Discussion and Conclusions,"however, it is important to realize that the problem is far from being solved even in english, and the current work makes very significant progress on it."
P16-1208,2016,6 Discussion and Conclusions,"the best classifier system and the pipelines outperform reported best results on the task, often by a large margin."
P16-1208,2016,6 Discussion and Conclusions,the purpose of this work is to gain a better understanding of the advantages offered by each learning method in order to make further progress on the gec task.
P16-1208,2016,6 Discussion and Conclusions,we built several systems that draw on the strengths of each approach individually and in a pipeline.
P16-1208,2016,6 Discussion and Conclusions,we further showed that the key strengths of the classification framework are its flexibility and the ability to train without supervision.
P16-1208,2016,6 Discussion and Conclusions,"we showed that the values provided by each method can be exploited within each approach and in combination, depending on the resources available, such as annotated learner data (mt), and additional linguistic resources (classifiers)."
P16-1208,2016,6 Discussion and Conclusions,"with respect to error phenomena, we showed that while mt is better at handling complex mistakes, classifiers are better at correcting mistakes that require abstracting beyond lexical context."
P16-1209,2016,7 Conclusions,all our results were obtained without putting any effort on feature engineering or requiring domain-specific knowledge.
P16-1209,2016,7 Conclusions,bidirectional rnn models are used to capture both forward and backward long term dependencies among words within a sentence.
P16-1209,2016,7 Conclusions,further our results have shown a significantly improved results on the relatively harder task of disease classification which has not been studied much.
P16-1209,2016,7 Conclusions,"in this work, we used three different variants of bidirectional rnn models with word embedding features for the first time for disease name and class recognition tasks."
P16-1209,2016,7 Conclusions,our results also indicate that rnn based models perform better than window based neural network model for the two tasks.
P16-1209,2016,7 Conclusions,this could be due to the implicit ability of rnn models to capture variable range dependencies of words compared to explicit dependency on context window size of window based neural network models.
P16-1209,2016,7 Conclusions,we have shown that these models are able to obtain quite competitive results compared to the benchmark result on the disease name recognition task.
P16-1210,2016,7 Discussion,"finally, we introduced a new approach to combining the original features with the learned correspondence features, and showed that replacing (rather than concatenating) the non-pivot features with the correspondence features generally yields better performance."
P16-1210,2016,7 Discussion,"finally, we would like to explore further our finding that the performance of the overall model seems to be predicted by the difference in performance between the non-pivot features and the new correspondence features, especially to see if this can be predicted at training time rather than as a post-hoc analysis."
P16-1210,2016,7 Discussion,"first, though our median-based approach was successful in converting pivot feature values to binary classification problems, learning a regression model might be an even better approach for count-based features."
P16-1210,2016,7 Discussion,"in the future, we would like to extend this work in several ways."
P16-1210,2016,7 Discussion,"second, since the svd basis-shift seems to be the source of much of the gains, we would like to explore replacing the svd with other algorithms, such as independent component analysis."
P16-1210,2016,7 Discussion,"the scl algorithm requires the manual identification of domain independent pivot features for each task, so we proposed two feature formulations using character n-grams as the pivot features, and showed that both yielded state-of-the-art performance."
P16-1210,2016,7 Discussion,"to the best of our knowledge, we are the first to introduce a domain adaption model for authorship attribution that combines labeled data in a source domain with unlabeled data from a target domain to improve performance on the target domain."
P16-1210,2016,7 Discussion,"we also showed that for the binary classification task that is used by scl to learn the feature correspondences, replacing the traditional greater-than-zero classification task with a median-based classification task allowed the model to better handle our count-based features."
P16-1210,2016,7 Discussion,we explored the dimensionality reduction step of scl and showed that singular value decomposition (svd) over the feature correspondence matrix is critical to achieving high performance.
P16-1210,2016,7 Discussion,we proposed several extensions to the popular structural correspondence learning (scl) algorithm for domain adaptation to make it more amenable to tasks like authorship attribution.
P16-1211,2016,6 Conclusion,"our analysis suggests 1) the canonical word order of such constructions varies from verb to verb, 2) there is only a weak relation between the canonical word order and the verb type: show-type and pass-type, 3) an argument whose grammatical case is infrequently omitted with a given verb tends to be placed near the verb, 4) the canonical word order varies depending on the semantic role of the dative argument, and 5) an argument that frequently co-occurs with the verb tends to be placed near the verb."
P16-1211,2016,6 Conclusion,this paper presented a corpus-based analysis of canonical word order of japanese double object constructions.
P16-1212,2016,5 Conclusion and Future Work,"experiments are conducted on a electronic business and movie data sets, and the results show that our proposed method can achieve significant improvement, compared with conventional phrase smt system and the state-ofthe-art encoder-decoder system."
P16-1212,2016,5 Conclusion and Future Work,"in the future, we will conduct experiments on large corpus in different domains."
P16-1212,2016,5 Conclusion and Future Work,"in this paper, we propose a knowledge based semantic embedding method for machine translation, in which source grounding maps the source sentence into a semantic space, based on which target generation is used to generate the translation."
P16-1212,2016,5 Conclusion and Future Work,"semantic vector generated by kbse can extract and ground the key information, with the help of knowledge base, which is preserved in the translation sentence."
P16-1212,2016,5 Conclusion and Future Work,"unlike the encoder-decoder neural network, in which the semantic space is implicit, the semantic space of kbse is defined by a given knowledge base."
P16-1212,2016,5 Conclusion and Future Work,we also want to introduce the attention method to leverage all the hidden states of the source sentence generated by recurrent neural network of source grounding.
P16-1213,2016,5 Conclusion,"in this paper we discussed a new strategy for multilingual entity linking that, once trained on one language source with accompanying knowledge base, performs without adaptation in multiple target languages."
P16-1213,2016,5 Conclusion,"one of the main characteristics of the system is that it makes effective use of features that are built exclusively around computing similarity between the text/context of the mention and the document text of the candidate entity, allowing it to transcend language and perform inference on a completely new language or domain, without change or adaptation."
P16-1213,2016,5 Conclusion,"our proposed system, liel is trained on the english wikipedia corpus, after building its own knowledge-base by exploiting the rich information present in wikipedia."
P16-1213,2016,5 Conclusion,"the system displays a robust and strong empirical evidence by not only outperforming all stateof-the-art english el systems, but also achieving very good performance on multiple spanish and chinese entity linking benchmark datasets, and it does so without the need to switch, retrain, or even translate, a major differentiating factor from the existing multi-lingual el systems out there."
P16-1214,2016,4 Conclusion,our future research includes improving the search performance for embeddings with heavy-tailed distributions and creating embeddings that can keep both task quality and search performance high.
P16-1214,2016,4 Conclusion,"since we need to implement additional glue codes for running flann and sash, our code would be useful for researchers who want to compare their results with ours."
P16-1214,2016,4 Conclusion,the results of experiments we conducted from various aspects indicated that ngt generally performed the best and that the distribution shape of embeddings is a key factor relating to the search performance.
P16-1214,2016,4 Conclusion,"we compared three methods: a graph-based method (ngt), a tree-based method (flann), the sash method, which was reported to have the best performance in a previous study (gorman and curran, 2006)."
P16-1214,2016,4 Conclusion,we investigated approximate similarity search for word embeddings.
P16-1214,2016,4 Conclusion,"we will release the source code used for our comparative experiments from the ngt page (iwasaki, 2015)."
P16-1215,2016,6 Conclusion,"additionally, we explored different encoders for composing distributed representations of relational patterns."
P16-1215,2016,6 Conclusion,"analyzing the internal mechanism of lstm, gru, and gac, we plan to explore an alternative architecture of neural networks that is optimal for relational patterns."
P16-1215,2016,6 Conclusion,"furthermore, we demonstrated that the presented dataset is useful to predict successes of the distributed representations in the relation classification task."
P16-1215,2016,6 Conclusion,"in this paper, we addressed the semantic modeling of relational patterns."
P16-1215,2016,6 Conclusion,"the experimental results shows that gated additive composition (gac), which is a combination of additive composition and the gating mechanism, is effective to compose distributed representations of relational patterns."
P16-1215,2016,6 Conclusion,"we expect that several further studies will use the new dataset not only for distributed representations of relational patterns but also for other nlp tasks (e.g., paraphrasing)."
P16-1215,2016,6 Conclusion,"we introduced the new dataset in which humans rated multiple similarity scores for every pair of relational patterns on the dataset of semantic inference (zeichner et al., 2012)."
P16-1216,2016,8 Conclusion,"also, negation of group membership is a complex linguistic phenomenon that was handled in a crude manner in our system."
P16-1216,2016,8 Conclusion,"an important component of the m-anaphor resolution problem that falls outside the scope of this study, but is important for practical application, is the detection of m-anaphoric mentions."
P16-1216,2016,8 Conclusion,"finally, we paired this system with a coreference resolver to solve the general coreference resolution task, showing that m-anaphor prediction can help boost performance."
P16-1216,2016,8 Conclusion,"furthermore, we developed a system combining a mention-pair model, an existing coreference resolver, and corpus knowledge to resolve m-anaphors that scores higher than a number of baseline methods."
P16-1216,2016,8 Conclusion,"in future work, we look to incorporate methods that incur less cost, possibly tolerating some error in the formation of sentences without significantly degrading performance."
P16-1216,2016,8 Conclusion,"moreover, for simplicity, this study focused solely on m-anaphoric they and them mentions, but as explained earlier, m-anaphoric mentions can take many forms, each introducing their own particular complexities that warrant special attention."
P16-1216,2016,8 Conclusion,"regarding the system developed for m-anaphor resolution, resorting to an external corpus to obtain well-formed sentences proved to be very computationally expensive."
P16-1216,2016,8 Conclusion,section 4 gives some insight into the problem but a much deeper investigation is necessary to devise a detection method.
P16-1216,2016,8 Conclusion,"we introduced a new class of anaphors to the anaphor resolution problem, m-anaphors, and extended the problem formulation to incorporate them."
P16-1216,2016,8 Conclusion,we look to devote future work to handling such cases.
P16-1216,2016,8 Conclusion,"we offered insights into the linguistic behaviour of m-anaphors, finding that surface-level syntactic and semantic features do not carry enough discriminative power in distinguishing them from 1-anaphors."
P16-1217,2016,6 Conclusions and Future Work,"evaluation results demonstrate that the summaries produced by our proposed algorithm have high relevance, coverage and discrimination, and the use of summaries as labels has obvious advantages over the use of words and phrases."
P16-1217,2016,6 Conclusions and Future Work,"in future work, we will explore to make use of all the three kinds of labels together to improve the users’ experience when they want to browse, understand and leverage the topics."
P16-1217,2016,6 Conclusions and Future Work,"in future work, we will try to make the summary label more coherent by considering the discourse structure of the summary and leveraging sentence ordering techniques."
P16-1217,2016,6 Conclusions and Future Work,"in this study, we addressed the problem of topic labeling by using text summaries."
P16-1217,2016,6 Conclusions and Future Work,"in this study, we do not consider the coherence of the topic summaries because it is really very challenging to get a coherent summary by extracting different sentences from a large set of different documents."
P16-1217,2016,6 Conclusions and Future Work,we propose a summarization algorithm based on submodular optimization to extract representative summaries for all the topics.
P16-1218,2016,7 Conclusion,experiments on ptb and ctb show that our model could be competitive with conventional high-order models with a faster speed.
P16-1218,2016,7 Conclusion,"in this paper, we propose an lstm-based neural network model for graph-based dependency parsing."
P16-1218,2016,7 Conclusion,"utilizing bidirectional lstm and segment embeddings learned by lstm-minus allows our model access to sentence-level information, making our model more accurate in recovering longdistance dependencies with only first-order factorization."
P16-1219,2016,5 Conclusion,extensive experiments show our method achieves substantial improvements against the state-of-theart baselines.
P16-1219,2016,5 Conclusion,"in this paper, we propose a generative bayesian non-parametric infinite mixture embedding model, transg, to address a new issue, multiple relation semantics, which can be commonly seen in knowledge graph."
P16-1219,2016,5 Conclusion,transg can discover the latent semantics of a relation automatically and leverage a mixture of relation components for embedding.
P16-1220,2016,7 Conclusion and Future Work,a potential application of our method is to improve kb-question answering using the documents retrieved by a search engine.
P16-1220,2016,7 Conclusion and Future Work,our experiments reveal that unstructured inference helps in mitigating representational issues in structured inference.
P16-1220,2016,7 Conclusion and Future Work,our future work involves exploring other alternatives such as treating structured and unstructured data as two independent resources in order to overcome the knowledge gaps in either of the two resources.
P16-1220,2016,7 Conclusion and Future Work,our main model which uses joint entity linking and relation extraction along with unstructured inference achieves the state-of-the-art results on webquestions dataset.
P16-1220,2016,7 Conclusion and Future Work,"since we pipeline structured inference first and then unstructured inference, our method is limited by the coverage of freebase."
P16-1220,2016,7 Conclusion and Future Work,we have also introduced a relation extraction method using mccnn which is capable of exploiting syntax in addition to sentential features.
P16-1220,2016,7 Conclusion and Future Work,we have presented a method that could infer both on structured and unstructured data to answer natural language questions.
P16-1222,2016,6 Conclusions,"besides, both attention mechanism and polishing schema contribute to the better performance of the proposed approach."
P16-1222,2016,6 Conclusions,"given an antecedent clause, we generate a subsequent clause to create a couplet pair using a sequential generation process."
P16-1222,2016,6 Conclusions,the chinese couplet generation is a difficult task in the field of natural language generation.
P16-1222,2016,6 Conclusions,"the two innovative insights are that 1) we adapt the attention mechanism for the couplet coupling constraint, and 2) we propose a novel polishing schema to refine the generated couplets using additional information."
P16-1222,2016,6 Conclusions,we apply perplexity and bleu to evaluate the performance of couplet generation as well as human judgments.
P16-1222,2016,6 Conclusions,we compare our approach with several baselines.
P16-1222,2016,6 Conclusions,we demonstrate that the neural couplet machine can generate rather good couplets and outperform baselines.
P16-1222,2016,6 Conclusions,we propose a novel neural couplet machine to tackle this problem based on neural network structures.
P16-1223,2016,7 Conclusion,"as future work, we need to consider how we can utilize these datasets (and the models trained upon them) to help solve more complex rc reasoning tasks (with less annotated data)."
P16-1223,2016,7 Conclusion,"in this paper, we carefully examined the recent cnn/daily mail reading comprehension task."
P16-1223,2016,7 Conclusion,"nevertheless, we argue that: (i) this dataset is still quite noisy due to its method of data creation and coreference errors; (ii) current neural networks have almost reached a performance ceiling on this dataset; and (iii) the required reasoning and inference level of this dataset is still quite simple."
P16-1223,2016,7 Conclusion,"our systems demonstrated state-of-the-art results, but more importantly, we performed a careful analysis of the dataset by hand."
P16-1223,2016,7 Conclusion,"overall, we think the cnn/daily mail datasets are valuable datasets, which provide a promising avenue for training effective statistical models for reading comprehension tasks."
P16-1224,2016,6 Related Work and Discussion,"examples include playing games (branavan et al., 2009, 2010; reckman et al., 2010) interacting with robotics (tellex et al., 2011, 2014), and following instructions (vogel and jurafsky, 2010; chen and mooney, 2011; artzi and zettlemoyer, 2013) semantic parsing utterances to logical forms, which we leverage, plays an important role in these settings (kollar et al., 2010; matuszek et al., 2012; artzi and zettlemoyer, 2013)."
P16-1224,2016,6 Related Work and Discussion,"finaly, we treat both the utterance and the logical forms as featurized compositional objects."
P16-1224,2016,6 Related Work and Discussion,"first, we model pragmatics in the online learning setting where we use an online update for the pragmatics model."
P16-1224,2016,6 Related Work and Discussion,"if these systems could quickly adapt to user feedback in real-time as in this work, then we might be able to more readily create systems for resource-poor languages and new domains, that are customizable and improve through use."
P16-1224,2016,6 Related Work and Discussion,"in contrast, we use pragmatics to speed up the learning process by capturing phenomena like mutual exclusivity (markman and wachtel, 1988)."
P16-1224,2016,6 Related Work and Discussion,"looking forward, we believe that the illg setting is worth studying and has important implications for natural language interfaces."
P16-1224,2016,6 Related Work and Discussion,monroe and potts (2015) uses learning to improve the pragmatics model.
P16-1224,2016,6 Related Work and Discussion,"our work connects with a broad body of work on grounded language, in which language is used in some environment as a means towards some goal."
P16-1224,2016,6 Related Work and Discussion,"second, unlikely the reference games where pragmatic effects plays an important role by design, shrdlurn is not specifically designed to require pragmatics."
P16-1224,2016,6 Related Work and Discussion,"smith et al.(2013) treats utterances (i.e.words) and logical forms (i.e.objects) as categories; monroe and potts (2015) used features, but also over flat categories."
P16-1224,2016,6 Related Work and Discussion,the improvement we get is mainly due to players trying to be consistent in their language use.
P16-1224,2016,6 Related Work and Discussion,"the main difference is these previous works use pragmatics with a trained base model, whereas we learn the model online."
P16-1224,2016,6 Related Work and Discussion,"to speed up learning, we leverage computational models of pragmatics (j?ger, 2008; golland et al., 2010; frank and goodman, 2012; smith et al., 2013; vogel et al., 2013)."
P16-1224,2016,6 Related Work and Discussion,"today, these systems are trained once and deployed."
P16-1224,2016,6 Related Work and Discussion,we also differ from prior work in several details.
P16-1224,2016,6 Related Work and Discussion,"what makes this work unique is our new interactive learning of language games (illg) setting, in which a model has to learn a language from scratch through interaction."
P16-1224,2016,6 Related Work and Discussion,"while online gradient descent is frequently used, for example in semantic parsing (zettlemoyer and collins, 2007; chen, 2012), we using it in a truly online setting, taking one pass over the data and measuring online accuracy (cesa-bianchi and lugosi, 2006)."
P16-1225,2016,6 Conclusion,"finally, we would like to test our model using other representations of word-form and wordmeaning."
P16-1225,2016,6 Conclusion,future work could also test whether a more interpretable meaningspace representation such as that provided by binary wordnet feature vectors reveals patterns of systematicity not found using a distributional semantic space.
P16-1225,2016,6 Conclusion,future work may investigate to what extent the smlkr model can predict human intuitions about form-meaning systematicity in language.
P16-1225,2016,6 Conclusion,"however, it would be interesting to verify our results in a phonological setting, perhaps using a monodialectal corpus."
P16-1225,2016,6 Conclusion,"in this paper, we proposed smlkr, a novel algorithm that can learn weighted string edit distances that minimize kernel regression error."
P16-1225,2016,6 Conclusion,"moreover, previous locallevel analyses suggest that systematicity seems to be concentrated in word-beginnings and word endings."
P16-1225,2016,6 Conclusion,"moreover, the measure of systematicity that we compute using smlkr accords significantly with human raters’ judgments about formmeaning correspondences in english."
P16-1225,2016,6 Conclusion,our algorithm offers improved global predictions of word-meaning given word-form at the lexicon-wide level.
P16-1225,2016,6 Conclusion,our model makes precise quantitative predictions that should allow us to address these questions.
P16-1225,2016,6 Conclusion,"this is a question that has received attention in the market research literature, where new brand names are tested for the emotions they elicit (klink, 2000)."
P16-1225,2016,6 Conclusion,"thus, it may be worthwhile to augment the representation of edit distance in our model by making it context-sensitive."
P16-1225,2016,6 Conclusion,"unlike previous lexicon-wide analyses, we find that form-meaning systematicity is not randomly distributed throughout the english lexicon."
P16-1225,2016,6 Conclusion,we chose to use orthographic rather than phonetic representations of words because of the variance in pronunciation present in the dialects of english that are manifested in our corpus.
P16-1225,2016,6 Conclusion,"we do not know, for instance, if our model can predict human semantic judgments of novel words that have never been encountered."
P16-1225,2016,6 Conclusion,"we removed such words in the final analysis since they are polymorphemic, but this observation suggests that our algorithm may have applications in unsupervised morpheme discovery."
P16-1225,2016,6 Conclusion,we show that this improvement seems related to localized pockets of form-meaning systematicity such as those previously uncovered in behavioral and corpus analyses.
P16-1225,2016,6 Conclusion,we succeed in applying this algorithm to the problem of finding form-meaning systematicity in the monomorphemic english lexicon.
P16-1225,2016,6 Conclusion,we would also like to investigate the degree to which our statistical model predicts the behavioral effects of phonosemantic systematicity during human semantic processing that have been reported in the psycholinguistics literature.
P16-1225,2016,6 Conclusion,"while developing our model on preliminary versions of the monomorphemic lexicon, we noticed that the model detected high degrees of systematicity in words with suffixes such as -ate and -tet (e.g., quintet, quartet)."
P16-1226,2016,8 Conclusion,"finally, our architecture seems straightforwardly applicable for multi-class classification, which, in future work, could be used to classify term-pairs to multiple semantic relations."
P16-1226,2016,8 Conclusion,"first, we focused on improving path representation using lstm, resulting in a path-based model that performs significantly better than prior path-based methods, and matches the performance of the previously superior distributional methods."
P16-1226,2016,8 Conclusion,"in particular, we demonstrated that the increase in recall is a result of generalizing semantically-similar paths, in contrast to prior methods, which either make no generalizations or over-generalize paths."
P16-1226,2016,8 Conclusion,we presented hypenet: a neural-networks-based method for hypernymy detection.
P16-1226,2016,8 Conclusion,"we then extended our network by integrating distributional signals, yielding an improvement of additional 14 f1 points, and demonstrating that the path-based and the distributional approaches are indeed complementary."
P16-1227,2016,5 Conclusions and Further Work,a further avenue of future research is improving models such as that presented in elliott et al.(2015) by crucial components of neural mt such as “attention mechanisms”.
P16-1227,2016,5 Conclusions and Further Work,a similar mechanism is used in xu et al.(2015) to decide which part of the image should influence which part of the generated caption.
P16-1227,2016,5 Conclusions and Further Work,"as with all retrieval-based methods, generalized statements about the relative performance on corpora of various domains, sizes and qualities are difficult to substantiate."
P16-1227,2016,5 Conclusions and Further Work,combining these two types of attention mechanisms in a neural caption translation model is a natural next step in caption translation.
P16-1227,2016,5 Conclusions and Further Work,"crucially, this is possible without using large amounts of indomain parallel text data, but instead using large amounts of monolingual image captions that are more readily available."
P16-1227,2016,5 Conclusions and Further Work,"despite the fact that our simple distance metric performed comparably to human object annotations, using such high-level semantic distance metrics for caption translation by multimodal pivots is a promising avenue for further research."
P16-1227,2016,5 Conclusions and Further Work,"for example, the attention mechanism of bahdanau et al.(2015) serves as a soft alignment that helps to guide the translation process by influencing the sequence in which source tokens are translated."
P16-1227,2016,5 Conclusions and Further Work,"in future work, we plan to evaluate our approach in more naturalistic settings, such machine translation for captions in online multimedia repositories such as wikimedia commons16 and digitized art catalogues, as well as e-commerce localization."
P16-1227,2016,5 Conclusions and Further Work,"learning semantically informative distance metrics using deep learning techniques is an area under active investigation (wu et al., 2013; wang et al., 2014; wang et al., 2015)."
P16-1227,2016,5 Conclusions and Further Work,"the gain in performance was comparable between a distance metric based on a deep convolutional network and one based on human object category annotations, demonstrating the effectiveness of the cnn-derived distance measure."
P16-1227,2016,5 Conclusions and Further Work,the results were achieved on one language pair (german-english) and one corpus (ms coco) only.
P16-1227,2016,5 Conclusions and Further Work,"this problem is aggravated in the multimodal case, since the relevance of captions with respect to images varies greatly between different corpora (hodosh et al., 2013)."
P16-1227,2016,5 Conclusions and Further Work,"using our approach, smt can, in certain cases, profit from multimodal context information."
P16-1227,2016,5 Conclusions and Further Work,"we demonstrated that the incorporation of multimodal pivots into a target-side retrieval model improved smt performance compared to a strong in-domain baseline in terms of bleu, meteor and ter on our parallel dataset derived from ms coco."
P16-1227,2016,5 Conclusions and Further Work,"while this is beyond the scope of this work, our models should provide an informative baseline against which to evaluate such methods."
P16-1228,2016,6 Discussion and Future Work,"finally, we also would like to generalize our framework to automatically learn the confidence of different rules, and derive new rules from data."
P16-1228,2016,6 Discussion and Future Work,"in particular, we proposed an iterative distillation procedure that transfers the structured information of logic rules into the weights of neural networks."
P16-1228,2016,6 Discussion and Future Work,our framework is general and applicable to various types of neural architectures.
P16-1228,2016,6 Discussion and Future Work,"the encouraging empirical results indicate a strong potential of our approach for improving other application domains such as vision tasks, which we plan to explore in the future."
P16-1228,2016,6 Discussion and Future Work,"the proposed iterative distillation procedure also reveals connections to recent neural autoencoders (kingma and welling, 2014; rezende et al., 2014) where generative models encode probabilistic structures and neural recognition models distill the information through iterative optimization (rezende et al., 2016; johnson et al., 2016; karaletsos et al., 2016)."
P16-1228,2016,6 Discussion and Future Work,the transferring is done via a teacher network constructed using the posterior regularization principle.
P16-1228,2016,6 Discussion and Future Work,"though we have focused on first-order logic rules, we leveraged soft logic formulation which can be easily extended to general probabilistic models for expressing structured distributions and performing inference and reasoning (lake et al., 2015)."
P16-1228,2016,6 Discussion and Future Work,we have developed a framework which combines deep neural networks with first-order logic rules to allow integrating human knowledge and intentions into the neural models.
P16-1228,2016,6 Discussion and Future Work,we plan to explore these diverse knowledge representations to guide the dnn learning.
P16-1228,2016,6 Discussion and Future Work,"with a few intuitive rules, our framework significantly improves base networks on sentiment analysis and named entity recognition, demonstrating the practical significance of our approach."
P16-1230,2016,5 Conclusion,a key advantage of this bayesian model is that its uncertainty estimate allows active learning and noise handling in a natural way.
P16-1230,2016,5 Conclusion,"consistent with all of our previous work, the reward function studied here is focused primarily on task success."
P16-1230,2016,5 Conclusion,in this paper we have proposed an active reward learning model using gaussian process classification and an unsupervised neural network-based dialogue embedding to enable truly on-line policy learning in spoken dialogue systems.
P16-1230,2016,5 Conclusion,"overall, the techniques developed in this paper enable for the first time a viable approach to on-line learning in deployed real-world dialogue systems which does not need a large corpus of manually annotated data or the construction of a user simulator."
P16-1230,2016,5 Conclusion,the system enables stable policy optimisation by robustly modelling the inherent noise in real user feedback and uses active learning to minimise the number of feedback requests to the user.
P16-1230,2016,5 Conclusion,the unsupervised dialogue embedding function required no labelled data to train whilst providing a compact and useful input to the reward predictor.
P16-1230,2016,5 Conclusion,this may be too simplistic for many commercial applications and further work will be needed in conjunction with human interaction experts to identify and incorporate the extra dimensions of dialogue quality that will be needed to achieve the highest levels of user satisfaction.
P16-1230,2016,5 Conclusion,we found that the proposed model achieved efficient policy learning and better performance compared to other stateof-the-art methods in the cambridge restaurant domain.
P16-1231,2016,6 Conclusions,our model combines the flexibility of transition-based algorithms and the modeling power of neural networks.
P16-1231,2016,6 Conclusions,our results demonstrate that feed-forward network without recurrence can outperform recurrent models such as lstms when they are trained with global normalization.
P16-1231,2016,6 Conclusions,we further support our empirical findings with a proof showing that global normalization helps the model overcome the label bias problem from which locally normalized models suffer.
P16-1231,2016,6 Conclusions,"we presented a simple and yet powerful model architecture that produces state-of-the-art results for pos tagging, dependency parsing and sentence compression."
p17-1001,2017,7 Conclusion,"in this paper, we have proposed an adversarial multi-task learning framework, in which the taskspecific and task-invariant features are learned non-redundantly, therefore capturing the sharedprivate separation of different tasks."
p17-1001,2017,7 Conclusion,"we also perform extensive qualitative analysis, deriving insights and indirectly explaining the quantitative improvements in the overall performance."
p17-1001,2017,7 Conclusion,we have demonstrated the effectiveness of our approach by applying our model to 16 different text classification tasks.
p17-1002,2017,6 Conclusion,"another possible framing, not considered here, is to frame am as an encoder-decoder problem (bahdanau et al., 2015; vinyals et al., 2015)."
p17-1002,2017,6 Conclusion,"its suitability for the end-to-end learning task is scope for future work, but its adequacy for component classification and relation identification has been investigated in potash et al.(2016)."
p17-1002,2017,6 Conclusion,our work yields new state-of-the-art results in end-to-end am on the pe dataset from stab and gurevych (2017).
p17-1002,2017,6 Conclusion,these are also our policy recommendations.
p17-1002,2017,6 Conclusion,this is an even more general modeling than lstm-er.
p17-1002,2017,6 Conclusion,"we experimented with different framings, such as encoding am as a dependency parsing problem, as a sequence tagging problem with particular label set, as a multi-task sequence tagging problem, and as a problem with both sequential and tree structure information."
p17-1002,2017,6 Conclusion,we present the first study on neural end-to-end am.
p17-1002,2017,6 Conclusion,"we show that (1) neural computational am is as good or (substantially) better than a competing feature-based ilp formulation, while eliminating the need for manual feature engineering and costly ilp constraint designing.(2) bilstm taggers perform very well for component identification, as demonstrated for our stagt frameworks, for t = blcc and t = bl, as well as for lstm-er (blc tagger).(3) (naively) coupling component and relation identification is not optimal, but both tasks should be treated separately, but modeled jointly.(4) relation identification is more difficult: when there are few entities in a text (鈥渟hort documents鈥), a more general framework such as that provided in lstm-er performs reasonably well."
p17-1002,2017,6 Conclusion,"when there are many entities (鈥渓ong documents鈥), a more restrained modeling is preferable."
p17-1003,2017,5 Conclusion,"because the interpreter is non-differentiable and to directly optimize the task reward, we apply reinforce and use pseudo-gold programs found by an iterative ml training process to bootstrap training."
p17-1003,2017,5 Conclusion,"it integrates neural networks with a symbolic nondifferentiable computer to support abstract, scalable and precise operations through a friendly neural computer interface."
p17-1003,2017,5 Conclusion,"it is trained endto-end, and does not require any feature engineering or domain-specific knowledge."
p17-1003,2017,5 Conclusion,"nsm achieves new state-of-the-art results on a challenging semantic parsing dataset with weak supervision, and significantly closes the gap between weak and full supervision."
p17-1003,2017,5 Conclusion,we propose the manager-programmer-computer framework for neural program induction.
p17-1003,2017,5 Conclusion,"within this framework, we introduce the neural symbolic machine, which integrates a neural sequence-to-sequence 鈥減rogrammer鈥 with key-variable memory, and a symbolic lisp interpreter with code assistance."
p17-1004,2017,5 Conclusion,"hence, the word-level multi-lingual attention, which may discover implicit alignments between words in multiple languages, will further improve multi-lingual relation extraction."
p17-1004,2017,5 Conclusion,"in fact, we find that the word alignment information may be also helpful for capturing relation patterns."
p17-1004,2017,5 Conclusion,"in future, we will extend mnre to more languages and explore its significance."
p17-1004,2017,5 Conclusion,"in this paper, we introduce a neural relation extraction framework with multi-lingual attention to take pattern consistency and complementarity among multiple languages into consideration."
p17-1004,2017,5 Conclusion,"we evaluate our framework on multi-lingual relation extraction task, and the results show that our framework can effectively model relation patterns among languages and achieve state-of-the-art results."
p17-1004,2017,5 Conclusion,"we will explore the effectiveness of word-level multilingual attention for relation extraction as our future work.(2) mnre can be flexibly implemented in the scenario of multiple languages, and this paper focuses on two languages of english and chinese."
p17-1004,2017,5 Conclusion,"we will explore the following directions as future work: (1) in this paper, we only consider sentence-level multi-lingual attention for relation extraction."
p17-1005,2017,5 Discussion,an advantage of this assumption is that tokens in the ungrounded and grounded representations are strictly aligned.
p17-1005,2017,5 Discussion,an assumption our model imposes is that ungrounded and grounded representations are structurally isomorphic.
p17-1005,2017,5 Discussion,"aside from relaxing strict isomorphism, we would also like to perform crossdomain semantic parsing where the first stage of the semantic parser is shared across domains."
p17-1005,2017,5 Discussion,"compared to previous neural semantic parsers, our model is more interpretable as the intermediate structures are useful for inspecting what the model has learned and whether it matches linguistic intuition."
p17-1005,2017,5 Discussion,"for instance, freebase does not contain a relation representing daughter, using instead two relations representing female and child."
p17-1005,2017,5 Discussion,"on the negative side, the structural isomorphism assumption restricts the expressiveness of the model, especially since one of the main benefits of adopting a two-stage parser is the potential of capturing domain-independent semantic information via the intermediate representation."
p17-1005,2017,5 Discussion,our model essentially jointly learns how to parse natural language semantics and the lexicons that help grounding.
p17-1005,2017,5 Discussion,"previous work (kwiatkowski et al., 2013) models such cases by introducing collapsing (for many-to-one mapping) and expansion (for one-to-many mapping) operators."
p17-1005,2017,5 Discussion,"this allows the neural network to focus on parsing and lexical mapping, sidestepping the challenging structure mapping problem which would result in a larger search space and higher variance."
p17-1005,2017,5 Discussion,we presented a neural semantic parser which converts natural language utterances to grounded meaning representations via intermediate predicate-argument structures.
p17-1005,2017,5 Discussion,"while it would be challenging to handle drastically non-isomorphic structures in the current model, it is possible to perform local structure matching, i.e., when the mapping between natural language and domainspecific predicates is many-to-one or one-to-many."
p17-1005,2017,5 Discussion,"within our current framework, these two types of structural mismatches can be handled with semi-markov assumptions (sarawagi and cohen, 2005; kong et al., 2016) in the parsing (i.e., predicate selection) and the grounding steps, respectively."
p17-1006,2017,7 Conclusion and Future Work,"finally, we have shown that the use of morph-fitted vectors boosts the performance of downstream language understanding models which rely on word representations as features, especially for morphologically rich languages such as german."
p17-1006,2017,7 Conclusion and Future Work,"future work will focus on other potential sources of morphological knowledge, porting the framework to other morphologically rich languages and downstream tasks, and on further refinements of the post-processing specialisation algorithm and the constraint selection."
p17-1006,2017,7 Conclusion and Future Work,the method makes use of implicit semantic signals encoded in inflectional and derivational rules which describe the morphological processes in a language.
p17-1006,2017,7 Conclusion and Future Work,the results in intrinsic word similarity tasks show that morph-fitting improves vector spaces induced by distributional models across four languages.
p17-1006,2017,7 Conclusion and Future Work,we have presented a novel morph-fitting method which injects morphological knowledge in the form of linguistic constraints into word vector spaces.
p17-1008,2017,7 Conclusion,"in practice, anchoring facilitates parsing, while unanchored representations are more flexible to use where words and semantic components are not in a one-to-one correspondence."
p17-1008,2017,7 Conclusion,"our survey concludes that the main distinguishing factors between schemes are their relation to syntax, their degree of universality, and the expertise and training they require from annotators, an important factor in addressing the annotation bottleneck."
p17-1008,2017,7 Conclusion,"recent years have seen increasing interest in an alternative approach that defines semantic structures independently from any syntactic or distributional criteria, much due to the availability of semantic treebanks that implement this approach."
p17-1008,2017,7 Conclusion,semantic representation in nlp is undergoing rapid changes.
p17-1008,2017,7 Conclusion,"semantic schemes diverge in whether they are anchored in the words and phrases of the text (e.g., all types of semantic dependencies and ucca) or not (e.g., amr and logic-based representations)."
p17-1008,2017,7 Conclusion,"traditional semantic work has either used shallow methods that focus on specific semantic phenomena, or adopted formal semantic theories which are coupled with a syntactic scheme through a theory of the syntax-semantics interface."
p17-1008,2017,7 Conclusion,"we do not view this as a major difference, because most unanchored representations (including amr) retain their close affinity with the words of the sentence, possibly because of the absence of a workable scheme for lexical decomposition, while dependency structures can be converted into logic-based representations (reddy et al., 2016)."
p17-1008,2017,7 Conclusion,"we hope this survey of the state of the art in semantic representation will promote discussion, expose more researchers to the most pressing questions in semantic representation, and lead to the wide adoption of the best components from each scheme."
p17-1009,2017,6 Conclusion,the model is novel in its choice of tasks and the cross-task interaction features.
p17-1009,2017,6 Conclusion,"we proposed a joint model of event coreference resolution, trigger detection, and event anaphoricity determination."
p17-1009,2017,6 Conclusion,"when evaluated on the kbp 2016 english and chinese corpora, our model not only outperforms the independent models but also achieves the best results to date on these corpora."
p17-1010,2017,6 Conclusion,"for training purpose, two-step training approach is employed, i.e.a pre-training and adaptation step, and this can be also easily applied to other tasks as well."
p17-1010,2017,6 Conclusion,"in this study, we propose an effective way to generate and exploit large-scale pseudo training data for zero pronoun resolution task."
p17-1010,2017,6 Conclusion,"second, we will develop other neural network architecture to make it more appropriate for zero pronoun resolution task."
p17-1010,2017,6 Conclusion,"the experimental results on ontonotes 5.0 corpus are encouraging, showing that the proposed model and accompanying approaches significantly outperforms the stateof-the-art systems."
p17-1010,2017,6 Conclusion,"the future work will be carried out on two main aspects: first, as experimental results show that the unknown words processing is a critical part in comprehending context, we will explore more effective way to handle the unk issue."
p17-1010,2017,6 Conclusion,the main idea behind our approach is to automatically generate large-scale pseudo training data and then utilize an attention-based neural network model to resolve zero pronouns.
p17-1011,2017,6 Conclusion,"a corpus of narrative student essays was manually annotated with discourse modes at sentence level, with acceptable inter-annotator agreement."
p17-1011,2017,6 Conclusion,"considering these characteristics, we proposed a neural sequence labeling approach for identifying discourse modes."
p17-1011,2017,6 Conclusion,"discourse mode features can make positive contributions, especially in challenging situations when simple surface features don鈥檛 work well."
p17-1011,2017,6 Conclusion,"in future, we plan to exploit discourse mode identification for providing novel features for more downstream nlp applications."
p17-1011,2017,6 Conclusion,"the corpus analysis revealed several aspects of characteristics of discourse modes including the distribution, co-occurrence and transition patterns."
p17-1011,2017,6 Conclusion,the experimental results demonstrate that automatic discourse mode identification is feasible.
p17-1011,2017,6 Conclusion,the ratio of description and emotion expressing is shown to be positively correlated to essay scores.
p17-1011,2017,6 Conclusion,"this paper has introduced a fundamental but less studied task in nlp鈥攄iscourse mode identification, which is designed in this work to automatically identify five discourse modes in essays."
p17-1011,2017,6 Conclusion,we evaluated discourse mode features for automatic essay scoring and draw preliminary observations.
p17-1012,2017,6 Conclusion,"also, we plan to investigate the effectiveness of our architecture on other sequence-tosequence tasks, e.g.summarization, constituency parsing, dialog modeling."
p17-1012,2017,6 Conclusion,future work includes better training to enable faster convergence with the convolutional encoder to better leverage the higher processing speed.
p17-1012,2017,6 Conclusion,"in comparison to other recent work, our deep convolutional encoder is competitive to the best published results to date (wmt鈥16 english-romanian) which are obtained with significantly more complex models (wmt鈥14 english-french) or stem from improvements that are orthogonal to our work (wmt鈥15 english-german)."
p17-1012,2017,6 Conclusion,our architecture also leads to large generation speed improvements: translation models with our convolutional encoder can translate twice as fast as strong baselines with bi-directional recurrent encoders.
p17-1012,2017,6 Conclusion,our experiments show that convolutional encoders perform on par or better than baselines based on bi-directional lstm encoders.
p17-1012,2017,6 Conclusion,our fast architecture is interesting for character level encoders where the input is significantly longer than for words.
p17-1012,2017,6 Conclusion,this approach is more parallelizable than recurrent networks and provides a shorter path to capture long-range dependencies in the source.
p17-1012,2017,6 Conclusion,we find it essential to use source position embeddings as well as different cnns for attention score computation and conditional input aggregation.
p17-1012,2017,6 Conclusion,we introduced a simple encoder model for neural machine translation based on convolutional networks.
p17-1013,2017,5 Conclusion,"on this way, gradients decay much slower compared to the standard deep networks which enable us to build a deep neural network for machine translation."
p17-1013,2017,5 Conclusion,our empirical study shows that it can significantly improve the performance of nmt.
p17-1013,2017,5 Conclusion,we propose a linear associative unit (lau) which makes a fusion of both linear and nonlinear transformation inside the recurrent unit.
p17-1014,2017,9 Conclusions,"crucially, we avoid relying on resources such as knowledge bases and externally trained parsers."
p17-1014,2017,9 Conclusions,"for future work, we would like to extend our work to different meaning representations such as the minimal recursion semantics (mrs; copestake et al.(2005))."
p17-1014,2017,9 Conclusions,"taking a step further, we would like to apply our models on semantics-based machine translation using mrs as an intermediate representation between pairs of languages, and investigate the added benefit compared to directly translating the surface strings, especially in the case of distant language pairs such as english and japanese (siegel, 2000)."
p17-1014,2017,9 Conclusions,"this formalism tackles certain linguistic phenomena differently from amr (e.g., negation, and co-reference), contains explicit annotation on concepts for number, tense and case, and finally handles multiple languages10 (bender, 2014)."
p17-1014,2017,9 Conclusions,we achieve competitive results for the parsing task (smatch 62.1) and state-of-theart performance for generation (bleu 33.8).
p17-1014,2017,9 Conclusions,"we applied sequence-to-sequence models to the tasks of amr parsing and amr generation, by carefully preprocessing the graph representation and scaling our models via pretraining on millions of unlabeled sentences sourced from gigaword corpus."
p17-1015,2017,8 Conclusion,"experiments show that our method outperforms existing neural models, in both the fluency of the rationales that are generated and the ability to solve the problem."
p17-1015,2017,8 Conclusion,"in this work, we addressed the problem of generating rationales for math problems, where the task is to not only obtain the correct answer of the problem, but also generate a description of the method used to solve the problem."
p17-1015,2017,8 Conclusion,"to this end, we collect 100,000 question and rationale pairs, and propose a model that can generate natural language and perform arithmetic operations in the same decoding process."
p17-1016,2017,6 Conclusions,"although example output looked promising, this model was limited by its inability to generalise to novel forms of verse."
p17-1016,2017,6 Conclusions,"an indistinguishability test, where participants were asked to classify a randomly selected set of human 鈥渘onsense verse鈥 and machine-generated poetry, showed generated poetry to be indistinguishable from that written by humans."
p17-1016,2017,6 Conclusions,"first, we developed a neural language model trained on a phonetic transliteration of poetic form and content."
p17-1016,2017,6 Conclusions,"in addition, the poems that were deemed most 鈥榟umanlike鈥 and most aesthetic were both machinegenerated."
p17-1016,2017,6 Conclusions,"in future work, it would be useful to investigate models based on morphemes, rather than characters, which offers potentially superior performance for complex and rare words (luong et al., 2013), which are common in poetry."
p17-1016,2017,6 Conclusions,our contributions are twofold.
p17-1016,2017,6 Conclusions,"this approach offers greater control over the style of the generated poetry than the earlier method, and facilitates themes and poetic devices."
p17-1016,2017,6 Conclusions,"we then proposed a more robust model trained on unformed poetic text, whose output form is constrained at sample time."
p17-1017,2017,5 Conclusion,"another important feature of our framework is that it permits creating semantically and linguistically diverse datasets which should support the learning of lexically and syntactically, wide coverage micro-planners."
p17-1017,2017,5 Conclusion,"conversely, the ratio of text per input is twice higher thereby providing better support for learning paraphrases."
p17-1017,2017,5 Conclusion,"despite the disparity in size, the number of attributes is comparable in the two datasets."
p17-1017,2017,5 Conclusion,"following the development of the semantic web, many large scale datasets are encoded in the rdf language (e.g., musicbrainz, foaf, linkedgeodata) and official institutions13 increasingly publish their data in this format."
p17-1017,2017,5 Conclusion,"in this context, our framework is useful both for creating training data from rdf kb verbalisers and to increase the number of datasets available for training and testing nlg."
p17-1017,2017,5 Conclusion,"more generally, they indicate that the data-to-text corpora built by our framework are challenging for such models."
p17-1017,2017,5 Conclusion,"one feature of our framework is that datasets created using this framework can be used for training and testing kb verbalisers an in particular, verbalisers for rdf knowledge bases."
p17-1017,2017,5 Conclusion,our experiments suggest that these are not optimal when it comes to generate linguistically complex texts from rich data.
p17-1017,2017,5 Conclusion,"recently, several sequence-to-sequence models have been proposed for generation."
p17-1017,2017,5 Conclusion,"the input describes entities belonging to 9 distinct dbpedia categories namely, astronaut, university, monument, building, comicscharacter, food, airport, sportsteam and writtenwork."
p17-1017,2017,5 Conclusion,the ratio between input and input patterns is five times lower in our dataset thereby making learning harder but also diminishing the risk of overfitting and providing for wider linguistic coverage.
p17-1017,2017,5 Conclusion,the webnlg data is licensed under the following license: cc attribution-noncommercialshare alike 4.0 international and can be downloaded at http://talc1.loria.fr/ webnlg/stories/challenge.html.
p17-1017,2017,5 Conclusion,"this new dataset consists of 21,855 data/text pairs with a total of 8,372 distinct data input."
p17-1017,2017,5 Conclusion,"we applied our framework to dbpedia data and showed that although twice smaller than the largest corpora currently available for training data-to-text microplanners, the resulting dataset is more semantically and linguistically diverse."
p17-1017,2017,5 Conclusion,we have recently released a first version of the webnlg dataset in the context of a shared task on micro-planning14.
p17-1017,2017,5 Conclusion,we hope that the webnlg dataset which we have made available for the webnlg shared task will drive the deep learning community to take up this new challenge and work on the development of neural generators that can handle the generation of kb verbalisers and of linguistically rich texts.
p17-1017,2017,5 Conclusion,we presented a framework for building nlg datato-text training corpora from existing knowledge bases.
p17-1018,2017,7 Conclusion,"as for future work, we are applying the gated self-matching networks to other reading comprehension and question answering datasets, such as the ms marco dataset (nguyen et al., 2016)."
p17-1018,2017,7 Conclusion,"in this paper, we present gated self-matching networks for reading comprehension and question answering."
p17-1018,2017,7 Conclusion,"our model achieves state-of-the-art results on the squad dataset, outperforming several strong competing systems."
p17-1018,2017,7 Conclusion,"we introduce the gated attentionbased recurrent networks and self-matching attention mechanism to obtain representation for the question and passage, and then use the pointernetworks to locate answer boundaries."
p17-1019,2017,6 Conclusion and Future Work,"and the future work includes: a) lots of questions cannot be answered directly by facts in a kb (e.g.鈥淲ho is jet li鈥檚 father-in-law?鈥), we plan to learn qa system with latent knowledge (e.g."
p17-1019,2017,6 Conclusion and Future Work,"in this paper, we propose an end-to-end system to generate natural answers through incorporating copying and retrieving mechanisms in sequenceto-sequence learning."
p17-1019,2017,6 Conclusion and Future Work,"kb embedding (bordes et al., 2013)); b) we plan to adopt memory networks (sukhbaatar et al., 2015) to encode the temporary kb for each question."
p17-1019,2017,6 Conclusion and Future Work,"specifically, the sequences of sus in the generated answer may be predicted from the vocabulary, copied from the given question and retrieved from the corresponding kb."
p17-1020,2017,8 Conclusion,"in future work we would like to deepen the use of structural clues and answer questions over multiple documents, using paragraph structure, titles, sections and more."
p17-1020,2017,8 Conclusion,incorporating coreference resolution would be another important direction for future work.
p17-1020,2017,8 Conclusion,we argue that this is necessary for developing systems that can efficiently answer the information needs of users over large quantities of text.
p17-1020,2017,8 Conclusion,we presented a coarse-to-fine framework for qa over long documents that quickly focuses on the relevant portions of a document.
p17-1021,2017,6 Conclusion,"firstly, we consider the impacts of the different answer aspects when representing the question, and propose a novel cross-attention model for kb-qa."
p17-1021,2017,6 Conclusion,"in this paper, we focus on kb-qa task."
p17-1021,2017,6 Conclusion,"secondly, we leverage the global kb information, which could take full advantage of the complete kb, and also alleviate the oov problem for the attention model."
p17-1021,2017,6 Conclusion,"specifically, we employ the focus of the answer aspects to each question word and the attention weights of the question towards the answer aspects."
p17-1021,2017,6 Conclusion,the extensive experiments demonstrate that the proposed approach could achieve better performance compared with state-of-the-art end-to-end methods.
p17-1021,2017,6 Conclusion,this kind of dynamic representation is more precise and flexible.
p17-1022,2017,9 Conclusion,"after introducing a translation criterion based on matching listener beliefs about speaker states, we presented both theoretical and empirical evidence that this criterion outperforms a conventional machine translation approach at recovering the content of message vectors and facilitating collaboration between humans and learned agents."
p17-1022,2017,9 Conclusion,"any encoder鈥 decoder model (sutskever et al., 2014) can be thought of as a kind of communication game played between the encoder and the decoder, so we can analogously imagine computing and translating 鈥渂eliefs鈥 induced by the encoding to explain what features of the input are being transmitted."
p17-1022,2017,9 Conclusion,"more broadly, the work here shows that the denotational perspective from formal semantics provides a framework for precisely framing the demands of interpretable machine learning (wilson et al., 2016), and particularly for ensuring that human users without prior exposure to a learned model are able to interoperate with it, predict its behavior, and diagnose its errors."
p17-1022,2017,9 Conclusion,"the current work has focused on learning a purely categorical model of the translation process, supported by an unstructured inventory of translation candidates, and future work could explore the compositional structure of messages, and attempt to synthesize novel natural language or neuralese messages from scratch."
p17-1022,2017,9 Conclusion,we have investigated the problem of interpreting message vectors from deep networks by translating them.
p17-1022,2017,9 Conclusion,"while our evaluation has focused on understanding the behavior of deep communicating policies, the framework proposed in this paper could be much more generally applied."
p17-1023,2017,7 Discussion and Conclusion,"also from an reg perspective, various extensions of this approach are possible, such as the inclusion of contextual information during object naming and its combination with attribute selection."
p17-1023,2017,7 Discussion and Conclusion,"as we have tested these approaches on a rather small vocabulary, which may limit generality of conclusions, future work will be devoted to scaling up these findings to larger test sets, as e.g.recently collected through conversational agents (das et al., 2016) that circumvent the need for human-human interaction data."
p17-1023,2017,7 Discussion and Conclusion,"in this paper, we have investigated models of referential word meaning, using different ways of combining visual information about a word鈥檚 referent and distributional knowledge about its lexical similarities."
p17-1023,2017,7 Discussion and Conclusion,previous cross-modal mapping models essentially force semantically similar objects to be mapped into the same area in the semantic space regardless of their actual visual similarity.
p17-1023,2017,7 Discussion and Conclusion,"we found that cross-modal mapping produces semantically appropriate and mutually highly similar object names in its top-n list, but does not preserve differences in referential word use (e.g.appropriatness of person vs. woman) especially within the same semantic field."
p17-1023,2017,7 Discussion and Conclusion,"we have shown that it is beneficial for performance in standard and zeroshot object naming to treat words as individual predictors that capture referential appropriateness and are only indirectly linked to a distributional space, either through lexical mapping during application or through cross-modal similarity mapping during training."
p17-1024,2017,6 Conclusion,"by associating the dataset with a series of tasks, we allow for diagnosing various failures of current lavi systems, from their coarse understanding of the correspondence between text and vision to their grasp of language and image structure."
p17-1024,2017,6 Conclusion,"lavi models are a great success of recent research, and we are impressed by the amount of ideas, data and models produced in this stimulating area."
p17-1024,2017,6 Conclusion,"our hypothesis is that systems which, like humans, deeply integrate the language and vision modalities, should spot foil captions quite easily."
p17-1024,2017,6 Conclusion,"the error production is automatically generated, but carefully thought out, making the task of spotting foils particularly challenging."
p17-1024,2017,6 Conclusion,"the soa lavi models we have tested fall through that test, implying that they fail to integrate the two modalities."
p17-1024,2017,6 Conclusion,this extra step would allow us to fully diagnose the failure of the tested systems and confirm what is implicit in our results from task 3: that the algorithms are unable to map particular elements of the text to their visual counterparts.
p17-1024,2017,6 Conclusion,"to complete the analysis of these results, we plan to carry out a further task, namely ask the system to detect in the image the area that produces the mismatch with the foil word (the red box around the bird in figure 1.)"
p17-1024,2017,6 Conclusion,"we have introduced foil-coco, a large dataset of images associated with both correct and foil captions."
p17-1024,2017,6 Conclusion,"we note that the addition of this extra step will move this work closer to the textual/visual explanation research (e.g., (park et al., 2016; selvaraju et al., 2016))."
p17-1024,2017,6 Conclusion,"we will then have a pipeline able to not only test whether a mistake can be detected, but also whether the system can explain its decision: 鈥榯he wrong word is dog because the cyclists are in fact approaching a bird, there, in the image鈥."
p17-1024,2017,6 Conclusion,"with our work, we would like to push the community to think of ways that models can better merge language and vision modalites, instead of merely using one to supplement the other"
p17-1025,2017,8 Conclusion,"empirical results confirm that by modeling changes in physical attributes entailed by verbs together with objects that exhibit these properties, we are able to better infer new knowledge in both domains."
p17-1025,2017,8 Conclusion,we presented a novel take on verb-centric frame semantics to learn implied physical knowledge latent in verbs.
p17-1026,2017,8 Conclusion,"by explicitly modeling the dependency structure, we do not require any deterministic heuristics to resolve attachment ambiguities, and keep the model locally factored so that all the probabilities can be precomputed before running the search."
p17-1026,2017,8 Conclusion,our parser efficiently finds the optimal parse and achieves the state-of-the-art performance in both english and japanese parsing.
p17-1026,2017,8 Conclusion,"we have presented a new a* ccg parsing method, in which the probability of a ccg tree is decomposed into local factors of the ccg categories and its dependency structure."
p17-1027,2017,7 Conclusion,"due to the unpredictability of a non-monotonic scenario, the real loss of each configuration cannot be computed."
p17-1027,2017,7 Conclusion,"in particular, one of the loss expressions developed proved very promising by providing the best average accuracy, in spite of being the farthest approximation from the actual loss."
p17-1027,2017,7 Conclusion,"on average, our non-monotonic algorithm obtains better performance than the monotonic version, regardless of which of the variants of the loss calculation is used."
p17-1027,2017,7 Conclusion,"on the other hand, the proposed lower bound makes the non-monotonic oracle the fastest one among all dynamic oracles developed for the non-projective covington algorithm."
p17-1027,2017,7 Conclusion,"to our knowledge, this is the first implementation of non-monotonicity for a nonprojective parsing algorithm, and the first approximate dynamic oracle that uses close, efficientlycomputable approximations of the loss, showing this to be a feasible alternative when it is not practical to compute the actual loss."
p17-1027,2017,7 Conclusion,"to overcome this, we proposed three different loss expressions that closely bound the loss and enable us to implement a practical non-monotonic dynamic oracle."
p17-1027,2017,7 Conclusion,"we believe that gains from both techniques should be complementary, as they apply to orthogonal components of the parsing system (the scoring model vs. the transition system), although we might see a 鈥漝iminishing returns鈥漞ffect."
p17-1027,2017,7 Conclusion,"we presented a novel, fully non-monotonic variant of the well-known non-projective covington parser, trained with a dynamic oracle."
p17-1027,2017,7 Conclusion,"while we used a perceptron classifier for our experiments, our oracle could also be used in neuralnetwork implementations of greedy transitionbased parsing (chen and manning, 2014; dyer et al., 2015), providing an interesting avenue for future work."
p17-1028,2017,6 Conclusions and Future Work,"given a dataset of crowdsourced sequence labels, we presented novel methods to: (1) aggregate sequential crowd labels to infer a best single set of consensus annotations; and (2) use crowd annotations as training data for a model that can predict sequences in unannotated text."
p17-1028,2017,6 Conclusions and Future Work,"our methods also provide an estimate of each worker鈥檚 label quality, which can be transfered between tasks and is useful for error analysis and intelligent task routing (bragg et al., 2014)."
p17-1028,2017,6 Conclusions and Future Work,results showed that our methods show improvement over strong baselines.
p17-1028,2017,6 Conclusions and Future Work,"we also plan to investigate extension of the crowd component in our hmm method with hierarchical models, as well as a fully-bayesian approach."
p17-1028,2017,6 Conclusions and Future Work,we evaluated our approaches on two datasets representing different domains and tasks: general ner and biomedical ie.
p17-1028,2017,6 Conclusions and Future Work,"we expect our methods to be applicable to and similarly benefit other sequence labeling tasks, such as pos tagging and chunking (hovy et al., 2014)."
p17-1029,2017,7 Conclusion and Future Work,"future work will adapt this framework to other sequence transduction scenarios such as machine translation, dialogue generation, question answering, where continuous and discrete latent variables can be abstracted to guide sequence generation."
p17-1029,2017,7 Conclusion and Future Work,"in this work, we propose a multi-space variational encoder-decoder framework for labeled sequence transduction problem."
p17-1029,2017,7 Conclusion and Future Work,"the msved performs well in the task of morphological reinflection, outperforming the state of the art, and further improving with the addition of external unlabeled data."
p17-1030,2017,6 Conclusion,"our algorithm outperforms stochastic optimization algorithms, indicating the importance of learning weight uncertainty in recurrent neural networks."
p17-1030,2017,6 Conclusion,"our algorithm requires little additional computational overhead in training, and multiple times of forward-passing for model averaging in testing."
p17-1030,2017,6 Conclusion,"the learning framework is tested on several tasks, including language models, image caption generation and sentence classification."
p17-1030,2017,6 Conclusion,"we propose a scalable bayesian learning framework using sg-mcmc, to model weight uncertainty in recurrent neural networks."
p17-1031,2017,7 Conclusion and Future Work,"currently, we only consider word forms in isolation, which is problematic for ambiguous cases (such as jn, which can normalize to in 鈥榠n鈥 or ihn 鈥榟im鈥) and conceivably makes the task harder for others."
p17-1031,2017,7 Conclusion and Future Work,"encouragingly, our work proves to be fully competitive with the sequence-labeling approach by bollmann and s酶gaard (2016), without requiring a prior character alignment."
p17-1031,2017,7 Conclusion and Future Work,"finally, many improvements to the presented approach are conceivable, most notably introducing some form of token context to the model."
p17-1031,2017,7 Conclusion and Future Work,"ljube拧ic et al.麓 (2016), for example, experiment with segment-based normalization, using a character-based smt model with character input derived from segments (essentially, token ngrams) instead of single tokens, which also introduces context."
p17-1031,2017,7 Conclusion and Future Work,reranking the predictions with a language model could be one possible way to improve on this.
p17-1031,2017,7 Conclusion and Future Work,"specifically, we demonstrated the aptitude of multi-task learning to mitigate the shortage of training data for the named task."
p17-1031,2017,7 Conclusion and Future Work,"such an approach could also deal with the issue of tokenization differences between the historical and the modern text, which is another challenge often found in datasets of historical text."
p17-1031,2017,7 Conclusion and Future Work,"we believe that this analysis is a valuable contribution to the understanding of mtl approaches also beyond spelling normalization, and we are confident that our observations will stimulate further research into the relationship between mtl and attention."
p17-1031,2017,7 Conclusion and Future Work,we included a multifaceted analysis of the effects that mtl introduces to our models and the resemblance that it bears to attention mechanisms.
p17-1031,2017,7 Conclusion and Future Work,"we presented an approach to historical spelling normalization using neural networks with an encoder-decoder architecture, and showed that it consistently outperforms several existing baselines."
p17-1032,2017,5 Discussion and Conclusions,"at the best our knowledge, this work is one of the few attempts to systematically integrate linguistic kernels within a deep neural network architecture."
p17-1032,2017,5 Discussion and Conclusions,cho and saul (2009) introduced a family of kernel functions that mimic the computation of large multilayer neural networks.
p17-1032,2017,5 Discussion and Conclusions,"finally, the optimization of the kda methodology through the suitable parallelization on multicore architectures, as well as the exploration of mechanisms for the dynamic reconstruction of kernel spaces (e.g., operating over hn y) also constitute interesting future research directions on this topic."
p17-1032,2017,5 Discussion and Conclusions,"first, a general framework is promoted: it is largely applicable to any complex kernel, e.g., structural kernels or combinations of them."
p17-1032,2017,5 Discussion and Conclusions,"future work will address experimentations with larger scale datasets; moreover, it is interesting to experiment with more landmarks in order to better understand the trade-off between the representation capacity of the nystrom approximation 篓 of the kernel functions and the over-fitting that can be introduced in a neural network architecture."
p17-1032,2017,5 Discussion and Conclusions,"given the nystrom approximation, the learning 篓 setting corresponds to a general well-known neural network architecture, i.e., a multilayer perceptron, and does not require any manual feature engineering or the design of ad-hoc network architectures."
p17-1032,2017,5 Discussion and Conclusions,"however, such kernels can be applied only on vector inputs."
p17-1032,2017,5 Discussion and Conclusions,"however, these assume that (i) instances have vectorial form and (ii) shift-invariant kernels are adopted."
p17-1032,2017,5 Discussion and Conclusions,"in baldi et al.(2011) the authors propose a hybrid classifier, for bridging kernel methods and neural networks."
p17-1032,2017,5 Discussion and Conclusions,"in mairal et al.(2014) the invariance properties of convolutional neural networks (lecun et al., 1998) are modeled through kernel functions, resulting in a convolutional kernel network."
p17-1032,2017,5 Discussion and Conclusions,"in particular, they use the output of a kernelized k-nearest neighbors algorithm as input to a neural network."
p17-1032,2017,5 Discussion and Conclusions,"in this work, we promoted a methodology to embed structured linguistic information within nns, according to mathematically rich semantic similarity models, based on kernel functions."
p17-1032,2017,5 Discussion and Conclusions,"in yu et al.(2009), deep neural networks for rapid visual recognition are trained with a novel regularization method taking advantage of kernels as an oracle representing prior knowledge."
p17-1032,2017,5 Discussion and Conclusions,"in zhuang et al.(2011) a different approach has been promoted: a multiple (two) layer architecture of kernel functions, inspired by neural networks, is studied to find the best kernel combination in a multiple kernel learning setting."
p17-1032,2017,5 Discussion and Conclusions,"last, the suggested kda framework is fully scalable, as (i) the network can be parallelized on multiple machines, and (ii) the computation of the nystrom reconstruction vector 篓 c can be easily parallelized on multiple processing units, ideally l, as each unit can compute one ci value."
p17-1032,2017,5 Discussion and Conclusions,"notice that other low-dimensional approximations of kernel functions have been studied, as for example the randomized feature mappings proposed in rahimi and recht (2008)."
p17-1032,2017,5 Discussion and Conclusions,"other effort for combining nns and kernel methods is described in tymoshenko et al.(2016), where a svm adopts a tree kernels combinations with embeddings learned through a cnn."
p17-1032,2017,5 Discussion and Conclusions,"second, we propose a novel learning strategy, as the capability of kernel methods to represent complex search spaces is combined with the ability of neural networks to find non-linear so lutions to complex tasks."
p17-1032,2017,5 Discussion and Conclusions,"structured data, such as trees, are transformed into dense vectors according to the nystrom method- 篓 ology, and the nn is effective in capturing nonlinearities in these representations, but still improving generalization at a reasonable complexity."
p17-1032,2017,5 Discussion and Conclusions,the approach here discussed departs from previous approaches in different aspects.
p17-1032,2017,5 Discussion and Conclusions,the authors transform the kernel regularizer into a loss function and carry out the neural network training by gradient descent.
p17-1032,2017,5 Discussion and Conclusions,"the efficiency of the nystrom methodology encourages its adoption, 篓 especially when complex kernel computations are required."
p17-1032,2017,5 Discussion and Conclusions,"the nystrom method adopted here does not suffer 篓 of such limitations: as our target is the application to structured (linguistic) data, more general kernels, i.e., non-shift-invariant convolution kernels are needed."
p17-1032,2017,5 Discussion and Conclusions,"the problem of combining such methodologies has been studied in specific works, such as (baldi et al., 2011; cho and saul, 2009; yu et al., 2009)."
p17-1032,2017,5 Discussion and Conclusions,the success in three different tasks confirms its large applicability without major changes or adaptations.
p17-1033,2017,8 Conclusion,"we additionally propose simple extensions of tdlm to incorporate information such as document labels and metadata, and achieved encouraging results."
p17-1033,2017,8 Conclusion,"we demonstrate that tdlm outperforms a state-of-the-art language model that incorporates larger context, and that its topics are potentially more coherent than lda topics."
p17-1033,2017,8 Conclusion,"we propose tdlm, a topically driven neural language model.tdlm has two components: a language model and a topic model, which are jointly trained using a neural network."
p17-1034,2017,6 Conclusion and Future Work,experimental results prove the proposed model achieves an effective performance and possesses preferable domain-adaptability.
p17-1034,2017,6 Conclusion and Future Work,it can learn to represent the new review of the new reviewer with the similar textual information and the correlated behavioral information in an unsupervised way.
p17-1034,2017,6 Conclusion and Future Work,it is also applicable to a large-scale dataset in an unsupervised way.
p17-1034,2017,6 Conclusion and Future Work,"then, a classifier is applied to detect the review spam."
p17-1034,2017,6 Conclusion and Future Work,this paper analyzes the importance and difficulty of the cold-start challenge in review spam combat.
p17-1034,2017,6 Conclusion and Future Work,"to our best knowledge, this is the first work to handle the cold-start problem in review spam detection."
p17-1034,2017,6 Conclusion and Future Work,we are going to explore more effective models in future.
p17-1034,2017,6 Conclusion and Future Work,we propose a neural network model that jointly embeds the existing textual and behavioral information for detecting review spam in the coldstart task.
p17-1035,2017,9 Conclusion and Future Directions,an analysis of the learned features confirms that the combination of automatically learned features is indeed capable of representing deep linguistic subtleties in text that pose challenges to sentiment and sarcasm classifiers.
p17-1035,2017,9 Conclusion and Future Directions,"in this work, we proposed a multimodal ensemble of features, automatically learned using variants of cnns from text and readers鈥 eye-movement data, for the tasks of sentiment and sarcasm classification."
p17-1035,2017,9 Conclusion and Future Directions,"on multiple published datasets for which gaze information is available, our systems could often achieve significant performance improvements over (a) systems that rely on handcrafted gaze and textual features and (b) cnn based systems that rely on text input alone."
p17-1035,2017,9 Conclusion and Future Directions,"our future agenda includes: (a) optimizing the cnn framework hyper-parameters (e.g., filter width, dropout, embedding dimensions, etc.)to obtain better results, (b) exploring the applicability of our technique for documentlevel sentiment analysis and (c) applying our framework to related problems, such as emotion analysis, text summarization, and questionanswering, where considering textual clues alone may not prove to be sufficient."
p17-1036,2017,6 Conclusion,"abae is intuitive and structurally simple, and also scales up well."
p17-1036,2017,6 Conclusion,all these benefits make it a promising alternative to lda-based methods in practice.
p17-1036,2017,6 Conclusion,"in contrast to lda models, abae explicitly captures word co-occurrence patterns and overcomes the problem of data sparsity present in review corpora."
p17-1036,2017,6 Conclusion,"our experimental results demonstrated that abae not only learns substantially higher quality aspects, but also more effectively captures the aspects of reviews than previous methods."
p17-1036,2017,6 Conclusion,"to the best of our knowledge, we are the first to propose an unsupervised neural approach for aspect extraction."
p17-1036,2017,6 Conclusion,"we have presented abae, a simple yet effective neural attention model for aspect extraction."
p17-1037,2017,6 Conclusion,"designing linguistic patterns for identifying support and opposition statements, we extracted users鈥 preferences regarding various topics from a large collection of tweets."
p17-1037,2017,6 Conclusion,"for our immediate future work, we plan to embed the topic and user vectors to create a crosstopic stance detector."
p17-1037,2017,6 Conclusion,"in this paper, we presented a novel approach for modeling inter-topic preferences of users on twitter."
p17-1037,2017,6 Conclusion,"it is possible to generalize our work to model heterogeneous signals, such as interests and behaviors of people, for example, 鈥渢hose who are interested in a also support b,鈥 and 鈥渢hose who favor a also vote for b鈥."
p17-1037,2017,6 Conclusion,"therefore, we believe that our work will bring about new applications in the field of nlp and other disciplines."
p17-1037,2017,6 Conclusion,"through our experimental results, we demonstrated that our approach was able to accurately predict missing topic preferences of users (80鈥94%) and that our latent vector representations of topics properly encoded inter-topic preferences."
p17-1037,2017,6 Conclusion,we formalized the task of modeling inter-topic preferences as a matrix factorization that maps both users and topics onto a latent feature space that abstracts users鈥 preferences.
p17-1038,2017,7 Conclusion and Future Work,"also, we provide a dmcnn-mil model for this data as a baseline for further research."
p17-1038,2017,7 Conclusion and Future Work,"in the future, we will use the proposed automatically data labeling method to more event types and explore more models to extract events by using automatically labeled data."
p17-1038,2017,7 Conclusion and Future Work,"in this paper, we present an approach to automatically label training data for ee."
p17-1038,2017,7 Conclusion and Future Work,the experimental results show the quality of our large scale automatically labeled data is competitive with elaborately human-annotated data.
p17-1039,2017,6 Conclusion and future work,"because our heuristic rules are quite simple, syntime is light-weight and runs in real time."
p17-1039,2017,6 Conclusion and future work,"experiments on three datasets show that syntime outperforms the stateof-the-art baselines, including rule-based time taggers and machine learning based time tagger."
p17-1039,2017,6 Conclusion and future work,in the future we will try our analytical method on other parts of language.
p17-1039,2017,6 Conclusion and future work,"in this paper, we test syntime on specific domains and specific text types in english."
p17-1039,2017,6 Conclusion and future work,"inspired by part-of-speech, based on the findings, we define a syntactic type system for the time expression, and propose a type-based time expression tagger, named by syntime."
p17-1039,2017,6 Conclusion and future work,"our findings provide evidence in terms of time expression for the principle of least effort (zipf, 1949)."
p17-1039,2017,6 Conclusion and future work,"our token types and heuristic rules are independent of specific tokens, syntime therefore is independent of specific domains, specific text types, and even specific languages that consist of specific tokens."
p17-1039,2017,6 Conclusion and future work,"since language usage relates to human habits (zipf, 1949; chomsky, 1986; pinker, 1995), we might expect that humans would share some common habits, and therefore expect that other parts of language would more or less follow the same principle."
p17-1039,2017,6 Conclusion and future work,syntime defines syntactic token types for tokens and on the token types it designs general heuristic rules based on the idea of boundary expansion.
p17-1039,2017,6 Conclusion and future work,the test for other languages needs only to construct a collection of token regular expressions in the target language under our defined token types.
p17-1039,2017,6 Conclusion and future work,time expression is part of language and follows the principle of least effort.
p17-1039,2017,6 Conclusion and future work,"we conduct an analysis on time expressions from four datasets, and find that time expressions in general are very short and expressed by a small vocabulary, and words in time expressions demonstrate similar syntactic behavior."
p17-1040,2017,8 Conclusions,"in this paper, we investigate the noise problem inherent in the ds-style training data."
p17-1040,2017,8 Conclusions,"one of our key innovations is to exploit a curriculum learning based training method to gradually learn to model the underlying noise pattern without direct guidance, and to provide the flexibility to exploit any prior knowledge of the data quality to further improve the effectiveness of the transition matrix."
p17-1040,2017,8 Conclusions,the experimental results show that the proposed method can better characterize the underlying noise and consistently outperform start-of-the-art extraction models under various scenarios.
p17-1040,2017,8 Conclusions,we argue that the data speak for themselves by providing useful clues to reveal their noise patterns.
p17-1040,2017,8 Conclusions,we evaluate our approach in two learning settings of the distantly supervised relation extraction.
p17-1040,2017,8 Conclusions,we thus propose a novel transition matrix based method to dynamically characterize the noise underlying such training data in a unified framework along the original prediction objective.
p17-1041,2017,7 Conclusion,experiments on both code generation and semantic parsing tasks demonstrate the effectiveness of our proposed approach.
p17-1041,2017,7 Conclusion,this paper proposes a syntax-driven neural code generation approach that generates an abstract syntax tree by sequentially applying actions from a grammar model.
p17-1042,2017,7 Conclusions and future work,"finally, we would like to apply our model in the decipherment scenario (dou et al., 2015)."
p17-1042,2017,7 Conclusions and future work,"in addition to that, we would like to explore non-linear transformations (lu et al., 2015) and alternative dictionary induction methods (dinu et al., 2015; smith et al., 2017)."
p17-1042,2017,7 Conclusions and future work,"in spite of its simplicity, a more detailed analysis shows that our method is implicitly optimizing a meaningful objective function that is independent from any bilingual data which, with a better optimization method, might allow to learn bilingual word embeddings in a completely unsupervised manner."
p17-1042,2017,7 Conclusions and future work,"in the future, we would like to delve deeper into this direction and fine-tune our method so it can reliably learn high quality bilingual word embeddings without any bilingual evidence at all."
p17-1042,2017,7 Conclusions and future work,"in this work, we propose a simple self-learning framework to learn bilingual word embedding mappings in combination with any embedding mapping and dictionary induction technique."
p17-1042,2017,7 Conclusions and future work,"our experiments on bilingual lexicon induction and crosslingual word similarity show that our method is able to learn high quality bilingual embeddings from as little bilingual evidence as a 25 word dictionary or an automatically generated list of numerals, obtaining results that are competitive with state-of-the-art systems using much richer bilingual resources like larger dictionaries or parallel corpora."
p17-1043,2017,7 Conclusion and Future Work,"a better approach to disambiguation is to consider predicates separately, solving for a set of coefficients for each verb found in the training set."
p17-1043,2017,7 Conclusion and Future Work,a general set of model parameters could then be used to handle unseen examples.
p17-1043,2017,7 Conclusion and Future Work,but it would also be fairly easy to add gazetteer information to the network features in order to remove the need for ner preprocessing.
p17-1043,2017,7 Conclusion and Future Work,"eliminating the need for syntactic pre-parsing is valuable since a syntactic parser takes up significant time and computational resources, and errors in the generated syntax will propagate into an amr parser."
p17-1043,2017,7 Conclusion and Future Work,improvements in the quality of the alignment in training data would improve parsing results.
p17-1043,2017,7 Conclusion and Future Work,"incorporating wikification gazetteers as b-lstm features might allow a performant, fully self contained parser to be created."
p17-1043,2017,7 Conclusion and Future Work,"likewise, high level args like arg2 and arg3 don鈥檛 generalize very well among different predicates, and arg inference accuracy could be improved with predicatespecific network parameters for the most common cases."
p17-1043,2017,7 Conclusion and Future Work,"our amr parser effectively exploits the ability of b-lstm networks to learn to selectively extract information from words separated by long distances in a sentence, and to build up higher level representations by rejecting or remembering important information during sequence processing."
p17-1043,2017,7 Conclusion and Future Work,"our approach avoids both of these problems, while generating high quality results."
p17-1043,2017,7 Conclusion and Future Work,"sense disambiguation is not a very generalizable task, senses other than 01 and 02 for different predicates may differ from each other in ways which are very difficult to discern."
p17-1043,2017,7 Conclusion and Future Work,"since our preferred wikifier application generates ner information, we used the generated ner tags as input to the sg network."
p17-1043,2017,7 Conclusion and Future Work,"the alignment between concepts and words is not a reliable, direct mapping: some concepts cannot be grounded to words, some are ambiguous, and automatic aligners tend to have high error rates relative to human aligning judgements."
p17-1043,2017,7 Conclusion and Future Work,there are changes which could be made to eliminate all pre-processing and to further improve parser performance.
p17-1043,2017,7 Conclusion and Future Work,"therefore, the wikification subtask is the only portion of the parser which requires any pre-processing at all."
p17-1043,2017,7 Conclusion and Future Work,we have shown that b-lstm neural networks can be used as the basis for a graph based semantic parser.
p17-1043,2017,7 Conclusion and Future Work,"wikification tasks are generally independent from parsing, but wiki links are a requirement for the latest amr specification."
p17-1044,2017,6 Conclusion and Future Work,"despite recent success without syntactic input, we found that our best neural model can still benefit from accurate syntactic parser output via straightforward constrained decoding."
p17-1044,2017,6 Conclusion and Future Work,"extensive error analysis sheds light on the strengths and limitations of our deep srl model, with detailed comparison against shallower models and two strong non-neural systems."
p17-1044,2017,6 Conclusion and Future Work,"finally, we posed the question of whether deep srl still needs syntactic supervision."
p17-1044,2017,6 Conclusion and Future Work,"in our oracle experiment, we observed a 3 f1 improvement by leveraging gold syntax, showing the potential for high quality parsers to further improve deep srl models."
p17-1044,2017,6 Conclusion and Future Work,"our ensemble of 8 layer bilstms incorporated some of the recent best practices such as orthonormal initialization, rnn-dropout, and highway connections, and we have shown that they are crucial for getting good results with deep models."
p17-1044,2017,6 Conclusion and Future Work,we presented a new deep learning model for spanbased semantic role labeling with a 10% relative error reduction over the previous state of the art.
p17-1044,2017,6 Conclusion and Future Work,"while our deep model is better at recovering longdistance predicate-argument relations, we still observe structural inconsistencies, which can be alleviated by constrained decoding."
p17-1045,2017,7 Conclusions and Discussion,"as the user interacts with this agent, the collected data can be used to train the e2e agent, which has a strong learning capability."
p17-1045,2017,7 Conclusions and Discussion,"effective implementation of this, however, requires the e2e agent to learn quickly and this is the research direction we plan to focus on in the future."
p17-1045,2017,7 Conclusions and Discussion,"given these results, we propose the following deployment strategy that allows a dialogue system to be tailored to specific users via learning from agent-user interactions."
p17-1045,2017,7 Conclusions and Discussion,"gradually, as more experience is collected, the system can switch from rl-soft to the personalized e2e agent."
p17-1045,2017,7 Conclusions and Discussion,the system could start off with an rl-soft agent (which gives good performance out-of-the-box).
p17-1045,2017,7 Conclusions and Discussion,this work is aimed at facilitating the move towards end-to-end trainable dialogue agents for information access.
p17-1045,2017,7 Conclusions and Discussion,"we also present an e2e agent for the task, which demonstrates a strong learning capacity in simulations but suffers from overfitting when tested on real users."
p17-1045,2017,7 Conclusions and Discussion,we propose a differentiable probabilistic framework for querying a database given the agent鈥檚 beliefs over its fields (or slots).
p17-1045,2017,7 Conclusions and Discussion,we show that such a framework allows the downstream reinforcement learner to discover better dialogue policies by providing it more information.
p17-1046,2017,6 Conclusion and Future Work,"besides, we publish the first human-labeled multi-turn response selection data set to research communities."
p17-1046,2017,6 Conclusion and Future Work,experiment results on open data sets show that the model can significantly outperform the stateof-the-art methods.
p17-1046,2017,6 Conclusion and Future Work,"in the future, we shall study how to model logical consistency of responses and improve candidate retrieval."
p17-1046,2017,6 Conclusion and Future Work,we present a new context based model for multiturn response selection in retrieval-based chatbots.
p17-1047,2017,6 Conclusions and Future Work,"additionally, by collecting a second dataset of captions for our images in a different language, such as spanish, our model could be extended to learn the acoustic correspondences for a given object category in both languages."
p17-1047,2017,6 Conclusions and Future Work,"an analogous procedure can be applied to visual images to discover visual patterns, and then the two modalities can be linked, allowing the network to learn, for example, that spoken instances of the word 鈥渢rain鈥 are associated with image regions containing trains."
p17-1047,2017,6 Conclusions and Future Work,"because our method creates a segmentation as well as an alignment between images and their spoken captions, a generative model could be trained using these alignments."
p17-1047,2017,6 Conclusions and Future Work,"further, this is done in o(n) time with respect to the number of image/caption pairs, whereas previous stateof-the-art acoustic pattern discovery algorithms which leveraged acoustic data alone run in o(n 2 ) time."
p17-1047,2017,6 Conclusions and Future Work,"in this paper, we have demonstrated that a neural network trained to associate images with the waveforms representing their spoken audio captions can successfully be applied to discover and cluster acoustic patterns representing words or short phrases in untranscribed audio data."
p17-1047,2017,6 Conclusions and Future Work,"modeling improvements are also possible, aimed at the goal of incorporating both visual and acoustic localization into the neural network itself."
p17-1047,2017,6 Conclusions and Future Work,the future directions in which this research could be taken are incredibly fertile.
p17-1047,2017,6 Conclusions and Future Work,"the model could provide a spoken caption for an arbitrary image, or even synthesize an image given a spoken description."
p17-1047,2017,6 Conclusions and Future Work,"the same framework we use here could be extended to video, enabling the learning of actions, verbs, environmental sounds, and the like."
p17-1047,2017,6 Conclusions and Future Work,"this is done without the use of a conventional automatic speech recognition system and zero text transcriptions, and therefore is completely agnostic to the language in which the captions are spoken."
p17-1047,2017,6 Conclusions and Future Work,"this paves the way for creating a speech-to-speech translation model not only with absolutely zero need for any sort of text transcriptions, but also with zero need for directly parallel linguistic data or manual human translations."
p17-1047,2017,6 Conclusions and Future Work,"we demonstrate the success of our methodology on a large-scale dataset of over 214,000 image/caption pairs comprising over 522 hours of spoken audio data, which is to our knowledge the largest scale acoustic pattern discovery experiment ever performed."
p17-1047,2017,6 Conclusions and Future Work,"we have shown that the shared multimodal embedding space learned by our model is discriminative not only across visual object categories, but also acoustically and semantically across spoken words."
p17-1048,2017,5 Summary and discussion,"actually, neural machine translation handles this issue by using a sub word unit (concatenating several letters to form a new sub word unit) (wu et al., 2016), which would be a promising direction for end-toend asr."
p17-1048,2017,5 Summary and discussion,"future work will apply this technique to the other languages including english, where we have to solve an issue of long sequence lengths, which requires heavy computation cost and makes it difficult to train a decoder network."
p17-1048,2017,5 Summary and discussion,"in addition, the proposed method does not require gmm/hmm construction for initial alignments, dnn pre-training, lattice generation for sequence discriminative training, complex search in decoding (e.g., fst decoder or lexical tree search based decoder)."
p17-1048,2017,5 Summary and discussion,"nevertheless, the method achieved comparable/superior performance to the state-of-theart conventional systems for the csj and mts tasks."
p17-1048,2017,5 Summary and discussion,"the joint decoding methods actually reduced most of the irregular alignments, which can be confirmed from the examples of recognition errors and alignment plots shown in appendix c. the proposed end-to-end asr does not require linguistic resources, such as morphological analyzer, pronunciation dictionary, and language model, which are essential components of conventional japanese and mandarin chinese asr systems."
p17-1048,2017,5 Summary and discussion,"this paper proposes end-to-end asr by using joint ctc/attention decoding, which outperformed ordinary attention-based end-to-end asr by solving the misalignment issues."
p17-1048,2017,5 Summary and discussion,"thus, the method greatly simplifies the asr building process, reducing code size and complexity."
p17-1049,2017,7 Conclusion,"given the fingerprints left on target language texts, translations very likely play a role in language change."
p17-1049,2017,7 Conclusion,"it depicts a very clear picture, reflecting language typology to the extent that disregarding the sources altogether, a phylogenetic tree can be reconstructed from a monolingual corpus consisting of multiple translations."
p17-1049,2017,7 Conclusion,it even holds after two phases of translations.
p17-1049,2017,7 Conclusion,"it has been attested that for certain languages, up to 30% of published materials are mediated through translation (pym and chrupa艂a, 2005)."
p17-1049,2017,7 Conclusion,"it may be possible to define classifiers implementing other universal facets of translation, e.g., simplification, which will yield good separation between o and t. however, explicitation fails in the reproduction of language typology, whereas interference-based features produce trees of considerable quality."
p17-1049,2017,7 Conclusion,"our trees reflect some of the historical connections among the languages, but of course they are related in other ways, too (whether incidental, areal, etc.)."
p17-1049,2017,7 Conclusion,"postulated universals in linguistics (greenberg, 1963) were confronted with much contradicting evidence in recent years (evans and levinson, 2009), and the long quest for translation universals (mauranen and kujamaki 篓 , 2004) should now be viewed in light of our finding: more than anything else, translations are typified by interference."
p17-1049,2017,7 Conclusion,"remarkably, translations to contemporary english and french capture part of the millenniumold history of the source languages from which the translations were made."
p17-1049,2017,7 Conclusion,"the major trends relate to loan translations (jahr, 1999), or the impact of canonical texts, such as luther鈥檚 translation of the bible to german (russ, 1994) or the case of the king james translation to english (crystal, 2010)."
p17-1049,2017,7 Conclusion,"this does not undermine the force of translation universals: we demonstrated how explicitation, in the form of cohesive markers, can help identify translations."
p17-1049,2017,7 Conclusion,"this holds for the product of highly professional translators, who conform to a common standard, and whose products are edited by native speakers, like themselves."
p17-1049,2017,7 Conclusion,this may explain the case of romanian in our reconstructed trees: it has been isolated for many years from other romance languages and was under heavy influence from balto-slavic languages.
p17-1049,2017,7 Conclusion,"translations may be considered distortions of the original text, but this distortion is far from random."
p17-1049,2017,7 Conclusion,very little research has been done in historical linguistics on how translations impact the evolvement of languages.
p17-1049,2017,7 Conclusion,we are presently trying to extend these results to translations in a different domain (literary texts) into a very different language (hebrew).
p17-1049,2017,7 Conclusion,we leave this as a direction for future research.
p17-1050,2017,6 Conclusion and Outlook,"furthermore, it holds promise for formulating language learning theory that is supported by empirical findings in naturalistic setups across language processing domains."
p17-1050,2017,6 Conclusion and Outlook,"in future work, we also plan to explore the role of native and second language writing system characteristics in second language reading."
p17-1050,2017,6 Conclusion and Outlook,"more broadly, our methodology introduces parallels with production studies in nlp, creating new opportunities for integration of data, methodologies and tasks between production and comprehension."
p17-1050,2017,6 Conclusion and Outlook,"specifically, we show that linguistic similarities between native languages are reflected in similarities in esl reading."
p17-1050,2017,6 Conclusion and Outlook,the effectiveness of linguistically motivated criteria for fixation clustering and our subsequent analysis suggest that the esl reading process is affected by linguistic factors.
p17-1050,2017,6 Conclusion and Outlook,the presented results demonstrate that eyetracking data can be instrumental for developing predictive and explanatory models of second language reading.
p17-1050,2017,6 Conclusion and Outlook,"we also identify several key features that characterize reading in different native languages, and discuss their potential connection to structural and lexical properties of the native langauge."
p17-1050,2017,6 Conclusion and Outlook,we demonstrate for the first time that this signal can be used to determine a reader鈥檚 native language.
p17-1050,2017,6 Conclusion and Outlook,we present a novel framework for studying crosslinguistic influence in multilingualism by measuring gaze fixations during reading of free-form en glish text.
p17-1050,2017,6 Conclusion and Outlook,"while this work is focused on nlir from fixations, our general framework can be used to address additional aspects of reading, such as analysis of saccades and gaze trajectories."
p17-1051,2017,7 Conclusions and Future Work,"for future work, we plan to address the limitations of morse: minimal supervision, greedy inference, and concatenative orthographic model."
p17-1051,2017,7 Conclusions and Future Work,"in this paper, we have presented morse, a first morpheme segmenter to consider semantic structure at this scale (local and vocabulary-wide)."
p17-1051,2017,7 Conclusions and Future Work,"moreover, we plan to computationally optimize the training stage for the sake of wider adoption by the community."
p17-1051,2017,7 Conclusions and Future Work,"we also pinpointed the weaknesses in current benchmarking datasets, and presented a new dataset free of these weaknesses."
p17-1051,2017,7 Conclusions and Future Work,we show its superiority over state-of-the-art algorithms using intrinsic evaluation on a variety of languages.
p17-1051,2017,7 Conclusions and Future Work,"with a relative increase in performance reaching 24% absolute increase over morfessor, this work proves the significance of semantic cues as well as validates a new state-of-the-art morpheme segmenter."
p17-1052,2017,4 Conclusion,it was shown to outperform the previous best models on six benchmark datasets.
p17-1052,2017,4 Conclusion,this paper tackled the problem of designing highperformance deep word-level cnns for text categorization in the large training data setting.
p17-1052,2017,4 Conclusion,"we proposed a deep pyramid cnn model which has low computational complexity, and can efficiently represent long-range associations in text and so more global information."
p17-1053,2017,7 Conclusion,"for example, our model could be integrated into the decoder in (liang et al., 2016), to provide better sequence prediction."
p17-1053,2017,7 Conclusion,"for future work, we will investigate the integration of our hr-bilstm into end-to-end systems."
p17-1053,2017,7 Conclusion,kb relation detection is a key step in kbqa and is significantly different from general relation extraction tasks.
p17-1053,2017,7 Conclusion,our model outperforms the previous methods on kb relation detection tasks and allows our kbqa system to achieve state-of-the-arts.
p17-1053,2017,7 Conclusion,"we propose a novel kb relation detection model, hr-bilstm, that performs hierarchical matching between questions and kb relations."
p17-1053,2017,7 Conclusion,"we will also investigate new emerging datasets like graphquestions (su et al., 2016) and complexquestions (bao et al., 2016) to handle more characteristics of general qa."
p17-1054,2017,7 Conclusions and Future Work,comprehensive empirical studies demonstrate the effectiveness of our proposed model for generating both present and absent keyphrases for different types of text.
p17-1054,2017,7 Conclusions and Future Work,"in the future, we are interested in comparing the model to human annotators and using human judges to evaluate the quality of predicted phrases.鈥 our current model does not fully consider correlation among target keyphrases."
p17-1054,2017,7 Conclusions and Future Work,"in this paper, we proposed an rnn-based generative model for predicting keyphrases in scientific text."
p17-1054,2017,7 Conclusions and Future Work,it would also be interesting to explore the multiple-output optimization aspects of our model.
p17-1054,2017,7 Conclusions and Future Work,"our future work may include the following two directions.鈥 in this work, we only evaluated the performance of the proposed model by conducting off-line experiments."
p17-1054,2017,7 Conclusions and Future Work,"our model summarizes phrases based the deep semantic meaning  of the text, and is able to handle rarely-occurred phrases by incorporating a copying mechanism."
p17-1054,2017,7 Conclusions and Future Work,"to the best of our knowledge, this is the first application of the encoder-decoder model to a keyphrase prediction task."
p17-1055,2017,8 Conclusion,"also, we are interested to see that if the machine really 鈥渃omprehend鈥 our language by utilizing neural networks approaches, but not only serve as a 鈥渄ocument-level鈥 language model."
p17-1055,2017,8 Conclusion,"among several public datasets, our model could give consistent and significant improvements over various state-of-theart systems by a large margin."
p17-1055,2017,8 Conclusion,"in this context, we are planning to investigate the problems that need comprehensive reasoning over several sentences."
p17-1055,2017,8 Conclusion,the future work will be carried out in the following aspects.
p17-1055,2017,8 Conclusion,"the proposed aoa reader aims to compute the attentions not only for the document but also the query side, which will benefit from the mutual information."
p17-1055,2017,8 Conclusion,then a weighted sum of attention is carried out to get an attended attention over the document for the final predictions.
p17-1055,2017,8 Conclusion,"we believe that our model is general and may apply to other tasks as well, so firstly we are going to fully investigate the usage of this architecture in other tasks."
p17-1055,2017,8 Conclusion,"we present a novel neural architecture, called attention-over-attention reader, to tackle the clozestyle reading comprehension task."
p17-1056,2017,7 Conclusions,"finally, we saw ways in which the application of alignment to cultural fit might help to refine ideas about alignment itself: preliminary analysis suggested that referential, rather than lexical, alignment was more predictive of employment outcomes."
p17-1056,2017,7 Conclusions,"more broadly, these results suggest ways that quantitative methods can be used to make precise application of concepts like 鈥渃ultural fit鈥 at scale."
p17-1056,2017,7 Conclusions,"our results showed substantial changes in the use of pronouns, with pronoun patterns varying by employees鈥 outcomes within the company.the use of the firstperson plural 鈥渨e鈥 during an employee鈥檚 first six months is particularly instructive."
p17-1056,2017,7 Conclusions,"quantitatively, rates of usage and alignment in the first six months of employment carried information about whether employees left involuntarily, pointing towards fit within the company culture early on as an indicator of eventual employment outcomes."
p17-1056,2017,7 Conclusions,this paper described an effort to use directed linguistic alignment as a measure of cultural fit within an organization.
p17-1056,2017,7 Conclusions,"we adapted a hierarchical alignment model from previous work to estimate fit within corporate email communications, focusing on changes in language during employees鈥 entry to and exit from the company."
p17-1056,2017,7 Conclusions,"whereas stayers exhibited increased baseline use, indicating internalization, those eventually departing involuntarily were on the one hand decreasingly likely to introduce 鈥渨e鈥 into conversation, but increasingly responsive to interlocutors鈥 use of the pronoun."
p17-1056,2017,7 Conclusions,"while not internalizing a shared identity with their peers, involuntarily departed employees were overly self-regulating in response to its invocation by others."
p17-1057,2017,5 Conclusion,going forward we would like to compare the representations learned by our model to the brain activity of people listening to speech in order to determine to what extent the patterns we found correspond to localized processing in the human cortex.
p17-1057,2017,5 Conclusion,this will hopefully lead to a better understanding of language learning and processing by both artificial and neural networks.
p17-1057,2017,5 Conclusion,"through detailed analysis we uncover how information in the input signal is transformed as it flows through the network: formal aspects of language such as word identities that not directly present in the input are discovered and encoded low in the layer hierarchy, while semantic information is most strongly expressed in the topmost layers."
p17-1057,2017,5 Conclusion,we present a multi-layer recurrent highway network model of language acquisition from visually grounded speech signal.
p17-1058,2017,8 Conclusions,"although the way we model information in language is simply the entropy at lexical level, we believe the findings still reveal the nature of information production and processing in dialogue."
p17-1058,2017,8 Conclusions,"our work adds a brick to the series of endeavors on studying the linguistic and behavioral factors of successful dialogue, and for the first time (as far as we know) demonstrates quantitatively that the dynamics of not just 鈥渨hat鈥 and 鈥渉ow鈥 we say, but also 鈥渉ow much鈥 we say and the 鈥渢iming鈥 of distributing what we say in dialogue, are relevant to the quality of communication."
p17-1058,2017,8 Conclusions,"the empirical results of the present study suggest that examining how the information contribution from interlocutors co-develops can provide a way to understand dialogue from a higher-level perspective, which has been missing in existing work."
p17-1058,2017,8 Conclusions,"we hope that by comparing and combining our methodology with other approaches of studying dialogue, we can reach a more comprehensive and holistic understanding of this common yet mysterious human practice."
p17-1059,2017,6 Conclusions and Future Work,"for future work, we wish to extend this model by investigating language generation conditioned on other modalities such as facial images and speech, and to applications such as dialogue generation for virtual agents."
p17-1059,2017,6 Conclusions and Future Work,"in this paper, we have introduced a novel language model affect-lm for generating affective conversational text conditioned on context words, an affective category and an affective strength parameter."
p17-1059,2017,6 Conclusions and Future Work,mturk perception studies show that the model can generate expressive text at varying degrees of emotional strength without affecting grammatical correctness.
p17-1059,2017,6 Conclusions and Future Work,we also evaluate affect-lm as a language model and show that it achieves lower perplexity than a baseline lstm model when the affect category is obtained from the words in the context.
p17-1060,2017,6 Conclusion,future work can include an extension of domain experts to take into account dialog history aiming for a holistic framework that can handle contextual interpretation as well.
p17-1060,2017,6 Conclusion,"in both intent classification and slot tagging tasks, the model significantly outperformed baselines that do not use domain adaptation and also performed better than the full re-training approach."
p17-1060,2017,6 Conclusion,"in this paper, we proposed a solution for scaling domains and experiences potentially to a large number of use cases by reusing existing data labeled for different domains and applications."
p17-1060,2017,6 Conclusion,our solution is based on attending an ensemble of domain experts.
p17-1060,2017,6 Conclusion,this approach enables creation of new virtual domains through a weighted combination of domain experts鈥 feedback reducing the need to collect and annotate the similar intent and slot types multiple times for different domains.
p17-1060,2017,6 Conclusion,"when given a new domain, our model uses a weighted combination of domain experts鈥 feedback along with its own opinion to make prediction on the new domain."
p17-1061,2017,6 Conclusion and Future Work,all of the above suggest a promising research direction.
p17-1061,2017,6 Conclusion and Future Work,"in addition to dialog acts, we plan to apply our kgcvae model to capture other different linguistic phenomena including sentiment, named entities,etc."
p17-1061,2017,6 Conclusion and Future Work,"in conclusion, we identified the one-to-many nature of open-domain conversation and proposed two novel models that show superior performance in generating diverse and appropriate responses at the discourse level."
p17-1061,2017,6 Conclusion and Future Work,"in turn, the output of this novel neural dialog model will be easier to explain and control by humans."
p17-1061,2017,6 Conclusion and Future Work,"last but not least, the recognition network in our model will serve as the foundation for designing a datadriven dialog manager, which automatically discovers useful high-level intents."
p17-1061,2017,6 Conclusion and Future Work,"while the current paper addresses diversifying responses in respect to dialogue acts, this work is part of a larger research direction that targets leveraging both past linguistic findings and the learning power of deep neural networks to learn better representation of the latent factors in dialog."
p17-1062,2017,7 Conclusion,"compared to existing end-to-end approaches, hcns afford more developer control and require less training data, at the expense of a small amount of developer effort."
p17-1062,2017,7 Conclusion,"finally, in a name-dialing domain, results from dialog simulation show that hcns can also be optimized with a mixture of reinforcement and supervised learning."
p17-1062,2017,7 Conclusion,"hcns support a separation of concerns where procedural knowledge and constraints can be expressed in software, and the control flow is learned."
p17-1062,2017,7 Conclusion,"in future work, we plan to extend hcns by incorporating lines of existing work, such as integrating the entity extraction step into the neural network (dhingra et al., 2017), adding richer utterance embeddings (socher et al., 2013), and supporting text generation (sordoni et al., 2015)."
p17-1062,2017,7 Conclusion,"more broadly, hcns are a general model for stateful control, and we would be interested to explore applications beyond dialog systems 鈥 for example, in nlp medical settings or humanrobot nl interaction tasks, providing domain constraints are important for safety; and in resourcepoor settings, providing domain knowledge can amplify limited data."
p17-1062,2017,7 Conclusion,"of course, we also plan to deploy the model in a live dialog system."
p17-1062,2017,7 Conclusion,"on a public benchmark in the restaurants domain, hcns exceeded performance of purely learned models."
p17-1062,2017,7 Conclusion,results in this paper have explored three different dialog domains.
p17-1062,2017,7 Conclusion,results in two troubleshooting domains exceeded performance of a commercially deployed rule-based system.
p17-1062,2017,7 Conclusion,this paper has introduced hybrid code networks for end-to-end learning of task-oriented dialog systems.
p17-1062,2017,7 Conclusion,"we will also explore using hcns with automatic speech recognition (asr) input, for example by forming features from n-grams of the asr n-best results (henderson et al., 2014b)."
p17-1063,2017,8 Conclusion,"finally, it would be interesting to combine our algorithm with a speech synthesis system."
p17-1063,2017,8 Conclusion,"furthermore, we could use this data to refine the costs c(d), c(ia) etc.for the edit operations, possibly assigning different costs to different edit operations."
p17-1063,2017,8 Conclusion,"in future work, we will complement the overhearer experiment presented here with an end-toend evaluation in an interactive nlg setting."
p17-1063,2017,8 Conclusion,"in this paper, we have presented an algorithm for generating contrastive feedback for a hearer who has misunderstood a referring expression."
p17-1063,2017,8 Conclusion,"in this way, we will be able to express focus with actual pitch accents, in contrast to the typographic approximation we made here."
p17-1063,2017,8 Conclusion,it will also give us the opportunity to investigate empirically the limits of the corruption model.
p17-1063,2017,8 Conclusion,our experiments show that this technique accurately predicts which words to mark as focused in a contrastive re.
p17-1063,2017,8 Conclusion,our technique is based on modeling likely user misunderstandings and then attempting to give feedback that contrasts with the most probable incorrect understanding.
p17-1063,2017,8 Conclusion,this will allow us to further investigate the quality of the correction strategies and refine the shortening strategy.
p17-1064,2017,7 Conclusion,experimentation on chinese-to-english translation shows that all proposed models yield improvements over a state-ofthe-art baseline nmt system.
p17-1064,2017,7 Conclusion,"in our future work, we expect several developments that will shed more light on utilizing source syntax, e.g., designing novel syntactic features (e.g., features showing the syntactic role that a word is playing) for nmt, and employing the source syntax to constrain and guild the attention models."
p17-1064,2017,7 Conclusion,"in this paper, we have also analyzed the translation behavior of our improved system against the state-of-the-art nmt baseline system from several perspectives."
p17-1064,2017,7 Conclusion,"in this paper, we have investigated whether and how source syntax can explicitly help nmt to improve its translation accuracy."
p17-1064,2017,7 Conclusion,"it is also interesting to note that the simplest model (i.e., mixed rnn) achieves the best performance, resulting in obtaining significant improvements of 1.4 bleu points on nist mt 02 to 05."
p17-1064,2017,7 Conclusion,our analysis shows that there is still much room for nmt translation to be consistent with source syntax.
p17-1064,2017,7 Conclusion,"specifically, we have described three different models to capture the syntax knowledge, i.e., parallel rnn, hierarchical rnn, and mixed rnn."
p17-1064,2017,7 Conclusion,"to obtain syntactic knowledge, we linearize a parse tree into a structural label sequence and let the model automatically learn useful information through it."
p17-1065,2017,6 Conclusion and Future Work,experimental results show that our method can boost the two procedures and achieve significant improvements on the translation quality of nmt systems.
p17-1065,2017,6 Conclusion and Future Work,"in addition, we will apply our method to other sequenceto-sequence tasks, such as text summarization, to verify the effectiveness."
p17-1065,2017,6 Conclusion and Future Work,"in future work, along this research direction, we will try to integrate other prior knowledge, such as semantic information, into nmt systems."
p17-1065,2017,6 Conclusion and Future Work,"in this paper, we propose a novel string-todependency translation model over nmt."
p17-1065,2017,6 Conclusion and Future Work,our model jointly performs target word generation and arc-standard dependency parsing.
p17-1066,2017,6 Conclusion and Future Work,"a propagation tree encodes the spread of a hypothesis (i.e., a source tweet) with complex structured patterns and flat information regarding content, user and time associated with the tree nodes."
p17-1066,2017,6 Conclusion and Future Work,"enlightened by tree kernel techniques, our kernel method learns discriminant clues for identifying rumors of finer-grained levels by directly measuring the similarity among propagation trees via kernel functions."
p17-1066,2017,6 Conclusion and Future Work,experiments on two twitter datasets show that our approach outperforms stateof-the-art baselines with large margin for both general and early rumor detection tasks.
p17-1066,2017,6 Conclusion and Future Work,"in the future, we will focus on improving the rumor detection task by exploring network representation learning framework."
p17-1066,2017,6 Conclusion and Future Work,"moreover, we plan to investigate unsupervised models considering massive unlabeled rumorous data from social media."
p17-1066,2017,6 Conclusion and Future Work,"since kernel-based approach covers more structural information than feature-based methods, it allows kernel to further incorporate information from a high dimensional space for possibly better discrimination."
p17-1066,2017,6 Conclusion and Future Work,we propose a novel approach for detecting rumors in microblog posts based on kernel learning method using propagation trees.
p17-1067,2017,8 Conclusion,"in this paper, we built a large, automatically curated dataset for emotion detection using distant supervision and then used grnns to model finegrained emotion, achieving a new state-of-the-art performance."
p17-1067,2017,8 Conclusion,we also extended the classification to 8 primary emotion dimensions situated in psychological theory of emotion.
p17-1068,2017,7 Conclusions,"another direction of future study will look at political ideology prediction in other countries and cultures, where ideology has different or multiple dimensions."
p17-1068,2017,7 Conclusions,"in addition, our work on user-level modeling can be integrated with work on message-level political bias to study how this is revealed across users with various levels of engagement."
p17-1068,2017,7 Conclusions,"in contrast to previous work, we made use of a novel data set where finegrained user political ideology labels are obtained through surveys as opposed to binary self-reports."
p17-1068,2017,7 Conclusions,"in fact, only self-reported extremists appear to devote much of their twitter activity to politics at all."
p17-1068,2017,7 Conclusions,"our results suggest a different direction: self-reported political extremity is more an indication of political engagement than of ideological self-placement (abramowitz, 2010)."
p17-1068,2017,7 Conclusions,"our work has implications for pollsters or marketers, who are most interested to identify and persuade moderate users."
p17-1068,2017,7 Conclusions,this study analyzed user-level political ideology through twitter posts.
p17-1068,2017,7 Conclusions,we analyzed language differences between the ideological groups and uncovered a dimension of political engagement separate from political leaning.
p17-1068,2017,7 Conclusions,we showed that users in our data set are far less likely to post about politics and real-world finegrained political ideology prediction is harder and more nuanced than previously reported.
p17-1068,2017,7 Conclusions,"while our study focused solely on text posted by the user, follow-up work can use other modalities such as images or social network analysis to improve prediction performance."
p17-1068,2017,7 Conclusions,"with respect to political conclusions, researchers commonly conceptualize ideology as a single, left-right dimension similar to what we observe in the u.s. congress (ansolabehere et al., 2008; bafumi and herron, 2010)."
p17-1069,2017,7 Conclusion,"finally, our global psl models can be applied to other domains, such as politics in other countries, simply by changing the initial unigram keywords to reflect the politics of those countries."
p17-1069,2017,7 Conclusion,in this paper we present the task of collective classification of twitter data for framing prediction.
p17-1069,2017,7 Conclusion,"we provide an analysis of our approach in both supervised and unsupervised settings, as well as a real world analysis of framing patterns over time."
p17-1069,2017,7 Conclusion,"we show that by incorporating twitter behaviors such as similar activity times and similar networks, we can increase f1 score prediction."
p17-1070,2017,6 Conclusions,"our nested attention hybrid model deeply combines the strengths of word and character level information in all components of an end-to-end neural model: the encoder, the attention layers, and the decoder."
p17-1070,2017,6 Conclusions,the model addresses the unique challenges of the grammatical error correction task and achieves the best reported results on the conll-14 benchmark among fully neural systems.
p17-1070,2017,6 Conclusions,the new architecture contributes substantial improvement in correction of confusions among rare or orthographically similar words compared to word-level sequence-to-sequence and non-nested hybrid models.
p17-1070,2017,6 Conclusions,this enables it to correct both global wordlevel and local character-level errors in a unified way.
p17-1070,2017,6 Conclusions,we have introduced a novel hybrid neural model with two nested levels of attention: word-level and character-level.
p17-1071,2017,5 Conclusion,"among the potential extensions of this work are the inclusion of different kinds of weights such as tf-idf, embedding relatedness and semantic relatedness."
p17-1071,2017,5 Conclusion,"an evaluation on eight datasets show promising results for textual entailment recognition, paraphrase detection and ranking."
p17-1071,2017,5 Conclusion,"we also intend to test other variants around the same concept, including considering the matched words and sequences to have a negative weight to balance further the weight of missing words."
p17-1071,2017,5 Conclusion,we presented a novel standalone similarity measure that takes into account continuous word sequences.
p17-1072,2017,6 Concluding Discussion,"for instance, we assume that both ideas and relations are statically grounded in keywords or topics."
p17-1072,2017,6 Concluding Discussion,"for instance, we cannot expect relations found in news articles to reflect shifts in public opinion if news articles do not effectively track public opinion."
p17-1072,2017,6 Concluding Discussion,"for the first time, we observe that the distribution of pairwise cooccurrence is unimodal, while the distribution of pairwise prevalence correlation is not always unimodal, and show that they are positively correlated."
p17-1072,2017,6 Concluding Discussion,"in reality, ideas and relations both evolve over time: a tryst relation might appear as friendship if we focus on a narrower time period."
p17-1072,2017,6 Concluding Discussion,"in scientific research, for example, it could simply be the progress of science, i.e., newer ideas overtake older ones deemed less valuable at a given time; on the other hand, history suggests that it is not always the correct ideas that are most expressed, and many other factors may be important."
p17-1072,2017,6 Concluding Discussion,it is important to note that the relations found using our approach depend on the nature of the representation of ideas and the source of texts.
p17-1072,2017,6 Concluding Discussion,it remains as a further stage of analysis to understand the underlying reasons that lead to these relations between ideas.
p17-1072,2017,6 Concluding Discussion,our method is entirely observational.
p17-1072,2017,6 Concluding Discussion,"similarly, in news coverage, underlying sociological and political situations have significant impact on which ideas are presented, and how."
p17-1072,2017,6 Concluding Discussion,"similarly, new ideas show up and even the same idea may change over time and be represented by different words."
p17-1072,2017,6 Concluding Discussion,there are many potential directions to improve our method to account for complex relations between ideas.
p17-1072,2017,6 Concluding Discussion,"this combination suggests four types of relations between ideas, and these four types are all found to varying extents in our experiments."
p17-1072,2017,6 Concluding Discussion,we illustrate our computational method by exploratory studies on news corpora and scientific research papers.
p17-1072,2017,6 Concluding Discussion,we not only confirm existing knowledge but also suggest hypotheses around the usage of arab and islam in terrorism and latino and asian in immigration.
p17-1072,2017,6 Concluding Discussion,we proposed a method to characterize relations between ideas in texts through the lens of cooccurrence within documents and prevalence correlation over time.
p17-1073,2017,5 Conclusions,"as we aim at building an evaluation dataset which is comparable to the sick corpus, the general assumptions of our procedure correspond to the design principles of the sick corpus."
p17-1073,2017,5 Conclusions,czech with an excellent dependency parser).
p17-1073,2017,5 Conclusions,"first, the polish seed-sentences have to be written based on the images which are selected from 8k imageflickr dataset and split into thematic groups, since usable datasets are not publicly available."
p17-1073,2017,5 Conclusions,"furthermore, we introduce an element of human verification of correctness of automatically transformed sentences and some additional post-corrections."
p17-1073,2017,5 Conclusions,"however, it is very likely that the annotation framework will work for other slavic languages (e.g."
p17-1073,2017,5 Conclusions,"however, the procedure of building the sick corpus cannot be adapted without modifications."
p17-1073,2017,5 Conclusions,"second, since the process of transforming sentences seems to be language-specific, the linguistic transformation rules appropriate for polish have to be defined from scratch."
p17-1073,2017,5 Conclusions,"since an entailment relation between two sentences must not be symmetric, each sentence pair is annotated for entailment in both directions."
p17-1073,2017,5 Conclusions,the discrepancies relative to the sick procedure also concern the annotation process itself.
p17-1073,2017,5 Conclusions,the goal of this paper is to present the procedure of building a polish evaluation dataset for the validation of compositional distributional semantics models.
p17-1073,2017,5 Conclusions,the presented procedure of building a dataset was tested on polish.
p17-1073,2017,5 Conclusions,"the presented procedure results in building the polish test corpus of relatively high quality, confirmed by the inter-annotator agreement coefficients of 魏 = 0.734 (measured with fleiss鈥 kappa) for entailment labels and of 伪 = 0.780 (measured with krippendorff鈥檚 ordinal alpha) for relatedness scores."
p17-1073,2017,5 Conclusions,"third, the process of arranging polish sentences into pairs is defined anew taking into account the data characteristic and bidirectional entailment annotations."
p17-1074,2017,6 Conclusion,a small-scale evaluation of our classifier found that each rater considered >95% of the predicted error types as either 鈥淕ood鈥 (85%) or 鈥淎cceptable鈥 (10%).
p17-1074,2017,6 Conclusion,"errant can be used to not only facilitate a detailed error type evaluation in gec, but also to standardise existing error correction corpora and reduce annotator workload."
p17-1074,2017,6 Conclusion,"in this paper, we described errant, a grammatical error annotation toolkit designed to automatically annotate parallel error correction data with explicit edit spans and error type information."
p17-1074,2017,6 Conclusion,our approach makes use of previous work to align sentences based on linguistic intuition and then introduces a new rule-based framework to classify edits.
p17-1074,2017,6 Conclusion,"this framework is entirely dataset independent, and relies only on automatically obtained information such as pos tags and lemmas."
p17-1074,2017,6 Conclusion,we demonstrated the value of errant by carrying out a detailed evaluation of system error type performance for all teams in the conll2014 shared task on grammatical error correction.
p17-1074,2017,6 Conclusion,we found that different systems had different strengths and weaknesses which we hope researchers can exploit to further improve general performance.
p17-1074,2017,6 Conclusion,we release errant with this paper.
p17-1075,2017,7 Conclusion,"in future work, we plan to use the analysis from the present study in constructing a system that can be applied to multiple datasets."
p17-1075,2017,7 Conclusion,"in this study, we adopted evaluation metrics that comprise two classes, namely refined prerequisite skills and readability, for analyzing the quality of rc datasets."
p17-1075,2017,7 Conclusion,our dataset analysis suggests that the readability of rc datasets does not directly affect the difficulty of the questions and that it is possible to create an rc dataset that is easy to read but difficult to answer.
p17-1075,2017,7 Conclusion,we applied these classes to six existing datasets and highlighted their characteristics according to each metric.
p17-1076,2017,8 Conclusion,"both approaches achieve state-of-the-art performance on the penn treebank, and our best model achieves competitive performance on the french treebank."
p17-1076,2017,8 Conclusion,"our experiments show that many of the key insights from recent neural transition-based approaches to parsing can be easily ported to the chart parsing setting, resulting in a pair of extremely simple models that nonetheless achieve excellent performance."
p17-1076,2017,8 Conclusion,our model supports both exact chart-based decoding and a novel top-down inference procedure.
p17-1076,2017,8 Conclusion,we have presented a minimal span-oriented parser that uses a recurrent input representation to score  trees with a sum of independent potentials on their constituent spans and labels.
p17-1077,2017,6 Conclusion,experiments demonstrate the effectiveness of the book embedding framework across a wide range of conditions.
p17-1077,2017,6 Conclusion,our graph-based parser obtains state-of-the-art accuracy.
p17-1077,2017,6 Conclusion,our work includes two contributions: 1. new algorithms for maximum noncrossing dependency parsing.2. a lagrangian relaxation based algorithm to combine noncrossing dependency subgraphs.
p17-1077,2017,6 Conclusion,"we propose a new data-driven parsing framework, namely book embedding, for semantic dependency analysis, viz.mapping from natural language sentences to bilexical semantic dependency graphs."
p17-1078,2017,6 Conclusion,"results show that rich pretraining leads to 15.4% relative error reduction, and our model gives results highly competitive to the best systems on six different benchmarks."
p17-1078,2017,6 Conclusion,"taking each type of external resource as an auxiliary classification task, we use neural multi-task learning to pre-train a set of shared parameters for character contexts."
p17-1078,2017,6 Conclusion,"we investigated rich external resources for enhancing neural word segmentation, by building a globally optimised beam-search model that leverages both character and word contexts."
p17-1079,2017,5 Conclusion,"experiments show that the proposed model can achieve comparative translation qualities to standard softmax prediction, while significantly suppressing the amount of parameters in the output layer, and improving calculation speeds while training and especially testing."
p17-1079,2017,5 Conclusion,"in algorithms 2 and 3 we use convolutions that were determined heuristically, and it is likely that learning these along with the model could result in improved accuracy or better compression capability."
p17-1079,2017,5 Conclusion,"in this study, we proposed neural machine translation models which indirectly predict output words via binary codes, and two model improvements: a hybrid prediction model using both softmax and binary codes, and introducing error-correcting codes to introduce robustness of binary code prediction."
p17-1079,2017,5 Conclusion,one interesting avenue of future work is to automatically learn encodings and error correcting codes that are well-suited for the type of binary code prediction we are performing here.
p17-1080,2017,7 Conclusion,"another area for future work is to extend the analysis to other word representations (e.g.byte-pair encoding), deeper networks, and more semantically-oriented tasks such as semantic rolelabeling or semantic parsing."
p17-1080,2017,7 Conclusion,"for instance, jointly learning translation and morphology can possibly lead to better representations and improved translation."
p17-1080,2017,7 Conclusion,"in this work, we investigated how neural mt models learn word structure."
p17-1080,2017,7 Conclusion,neural networks have become ubiquitous in machine translation due to their elegant architecture and good performance.
p17-1080,2017,7 Conclusion,our analysis indicates that this kind of approach should take into account factors such as the encoding layer and the type of word representation.
p17-1080,2017,7 Conclusion,"our results lead to the following conclusions: 鈥 character-based representations are better than word-based ones for learning morphology, especially in rare and unseen words.鈥 lower layers of the neural network are better at capturing morphology, while deeper networks improve translation performance."
p17-1080,2017,7 Conclusion,the representations they use for linguistic units are crucial for obtaining high-quality translation.
p17-1080,2017,7 Conclusion,these insights can guide further development of neural mt systems.
p17-1080,2017,7 Conclusion,"this is partly, but not completely, correlated with bleu scores.鈥 the attentional decoder learns impoverished representations that do not carry much information about morphology."
p17-1080,2017,7 Conclusion,we evaluated their representation quality on pos and morphological tagging in a number of languages.
p17-1080,2017,7 Conclusion,"we hypothesize that lower layers are more focused on word structure, while higher ones are focused on word meaning.鈥 translating into morphologically-poorer languages leads to better source-side representations."
p17-1081,2017,5 Conclusion,"as future work, we plan to develop a lstmbased attention model to determine the importance of each utterance and its specific contribution to each modality for sentiment classification."
p17-1081,2017,5 Conclusion,"in this paper, we developed a lstm-based network to extract contextual features from the utterances of a video for multimodal sentiment analysis."
p17-1081,2017,5 Conclusion,the contextual relationship among utterances in a video is mostly ignored in the literature.
p17-1081,2017,5 Conclusion,the proposed method has outperformed the state of the art and showed significant performance improvement over the baseline.
p17-1082,2017,8 Conclusion,our hope is that these stance dimensions will be valuable as a convenient building block for future research on interactional meaning.
p17-1082,2017,8 Conclusion,stancetaking provides a general perspective on the various linguistic phenomena that structure social interactions.
p17-1082,2017,8 Conclusion,"we have identified a set of several hundred stance markers, building on previouslyidentified lexicons by using word embeddings to perform lexicon expansion."
p17-1082,2017,8 Conclusion,"we then used multidimensional analysis to group these markers into stance dimensions, which we show to be internally coherent and extrinsically useful."
p17-1083,2017,5 Conclusion,"for interactive topic modeling, using anchor facets in place of single word anchors produces higher quality topic models and are more intuitive to use."
p17-1083,2017,5 Conclusion,"furthermore, our approach scales much better than existing interactive topic modeling techniques, allowing interactivity on large datasets for which interactivity was previous impossible."
p17-1083,2017,5 Conclusion,tandem anchors extend the anchor words algorithm to allow multiple words to be combined into anchor facets.
p17-1084,2017,8 Conclusion,"as a future work, we would like to explore the feasibility of marrying our semantic and neural models to exploit the benefits that each of them has to offer."
p17-1084,2017,8 Conclusion,"for benchmarking the progress, we filter a collection of these paragraphs to create a test set, on which humans perform with an accuracy of 94.2%."
p17-1084,2017,8 Conclusion,"for continuing our data collection, we would like to have a targeted entity pair selection where we particularly collect the missing relations in our partial ordering lattice."
p17-1084,2017,8 Conclusion,for the most recent statistics of the dataset and the best performing systems please check this website.
p17-1084,2017,8 Conclusion,"furthermore, guessing the two entities requires a system to go beyond only understanding one given passage and requires reasoning across paragraphs, which is one of the most under-explored, yet crucial, capabilities of an intelligent agent."
p17-1084,2017,8 Conclusion,our experiments show that the semantic approach outperforms the neural models by a large margin.
p17-1084,2017,8 Conclusion,"so far, we have crowdsourced a dataset of more than 14k comparison paragraphs comparing entities from nine major categories."
p17-1084,2017,8 Conclusion,the comparison paragraphs often have complex semantic structures which make this comprehension task demanding.
p17-1084,2017,8 Conclusion,the poor performance of the neural models we experimented with can motivate designing new architectures which are capable of performing basic reasoning across paragraphs.
p17-1084,2017,8 Conclusion,the results strongly suggest that bridging the gap between system and human performance on this task requires models with richer language representation and reasoning capabilities.
p17-1084,2017,8 Conclusion,we believe that this process can help developing more effective systems.
p17-1084,2017,8 Conclusion,"we introduced the novel task of guesstwo, in which given a short paragraph comparing two common entities, a system should guess what the two entities are."
p17-1084,2017,8 Conclusion,we presented a host of neural models and a novel semantic-driven approach for tackling the task of guesstwo.
p17-1085,2017,8 Conclusion,"experimentally, we found that our model significantly outperforms feature-rich structured perceptron joint model by li and ji (2014)."
p17-1085,2017,8 Conclusion,"however, this presents the challenge of combining predictions from the two directions."
p17-1085,2017,8 Conclusion,"in future, we plan to explore pretraining methods for our model which were shown to improve recall on entity and relation performance by miwa and bansal (2016)."
p17-1085,2017,8 Conclusion,"in this paper, we propose a novel attention-based lstm model for joint extraction of entity mentions and relations."
p17-1085,2017,8 Conclusion,"it would also be interesting to see the effect of reranking (collins and koo, 2005) on our joint model."
p17-1085,2017,8 Conclusion,we also compare our model to an endto-end lstm model by miwa and bansal (2016) which comprises of a sequence layer for entity extraction and a tree-based dependency layer for relation classification.
p17-1085,2017,8 Conclusion,"we also find that our model performs significantly better than their treebased model on the art relation, while their treebased model performs better on phys and partwhole relations; the two models perform comparably on all other relation types."
p17-1085,2017,8 Conclusion,we also plan to extend the identification of entities to full entity mention span instead of only the head phrase as in lu and roth (2015).
p17-1085,2017,8 Conclusion,"we also plan to use sparsemax (martins and astudillo, 2016) instead of softmax for multiple relations, as the former is more suitable for multi-label classification for sparse labels."
p17-1085,2017,8 Conclusion,"we find that our model, without access to dependency trees, pos tags, etc performs within 1% on entities and 2% on relations on ace05 dataset."
p17-1085,2017,8 Conclusion,we introduce bi-directional output encoding as well as an objective to learn multiple relations in this paper.
p17-1085,2017,8 Conclusion,we think that using probabilistic methods to combine model predictions from both directions may further improve the performance.
p17-1085,2017,8 Conclusion,we use heuristics in this paper to combine the predictions.
p17-1086,2017,7 Related work and discussion,allowing ambiguity and a flexible syntax is a key reason why natural language is easier to produce鈥攖his cannot be achieved by pls such as inform and cobol which look like natural language.
p17-1086,2017,7 Related work and discussion,"azaria et al.(2016) presents learning by instruction agent (lia), which also advocates learning from users."
p17-1086,2017,7 Related work and discussion,"compared to previous work, this work studied interactive learning in a shared community setting and hierarchical definitions resulting in more complex concepts."
p17-1086,2017,7 Related work and discussion,"in contrast, the action space is considered constant in the naturalizing pl approach, and the language adapts to be more natural while accommodating the action space."
p17-1086,2017,7 Related work and discussion,"in semantic parsing, the semantic representation and action space is usually designed to accommodate the natural language that is considered constant."
p17-1086,2017,7 Related work and discussion,"in the future, we wish to test these ideas in more domains, naturalize a real pl, and handle paraphrasing and implicit arguments."
p17-1086,2017,7 Related work and discussion,"in the process of naturalization, both data and the semantic grammar have important roles in the evolution of a language that is easier for humans to produce while still parsable by computers."
p17-1086,2017,7 Related work and discussion,"in this work, we use semantic parsing techniques that can handle ambiguity (zettlemoyer and collins, 2005, 2007; kwiatkowski et al., 2010; liang et al., 2011; pasupat and liang, 2015)."
p17-1086,2017,7 Related work and discussion,"instead of having a private language for each user, the user community in this work shares one language."
p17-1086,2017,7 Related work and discussion,"like jia et al.(2017), azaria et al.(2016) starts with an ad-hoc set of initial slot-filling commands in natural language as the basis of further instructions鈥攐ur approach starts with a more expressive core pl designed to interpolate with natural language."
p17-1086,2017,7 Related work and discussion,our work demonstrates that interactive definitions is a strong and usable form of supervision.
p17-1086,2017,7 Related work and discussion,"they argue that developers cannot anticipate all the actions that users want, and that the system cannot understand the corresponding natural language even if the desired action is built-in."
p17-1086,2017,7 Related work and discussion,"this work is an evolution of wang et al.(2016), but differs crucially in several ways: while wang et al.(2016) starts from scratch and relies on selecting candidates, this work starts with a programming language (pl) and additionally relies on definitions, allowing us to scale."
p17-1087,2017,9 Conclusion,another possibility is to use thesauri and word vector representations together with word sense disambiguation to generate semantically similar clusters for multiple senses of words.
p17-1087,2017,9 Conclusion,"by accounting for antonym relationships, our algorithm greatly outperforms simple normalized cuts."
p17-1087,2017,9 Conclusion,"finally, we examined our clustering method on the sentiment analysis task from socher et al.(2013) sentiment treebank dataset and showed that it improved performance versus comparable models."
p17-1087,2017,9 Conclusion,"finally, we plan to extend the hard signed clustering presented here to probabilistic soft clustering."
p17-1087,2017,9 Conclusion,"furthermore, signed spectral clustering has broader applications such as cellular biology, social networking, and electricity networks."
p17-1087,2017,9 Conclusion,"it could also be used to explore multi-view relationships, such as aligning synonym clusters across multiple languages."
p17-1087,2017,9 Conclusion,our automatically generated clusters give better coverage than manually constructed thesauri.
p17-1087,2017,9 Conclusion,our signed spectral clustering method allows us to incorporate the knowledge contained in these thesauri without modifying the word embeddings themselves.
p17-1087,2017,9 Conclusion,"our signed spectral clustering method could be applied to a broad range of nlp tasks, such as prediction of social group clustering, identification of personal versus non-personal verbs, and analyses of clusters which capture positive, negative, and objective emotional content."
p17-1087,2017,9 Conclusion,sc(w2v) are the signed clusters using word2vec word representations.perior semantically similar clusters which do not require new word embeddings but simply overlay thesaurus information on preexisting ones.
p17-1087,2017,9 Conclusion,the clusters are general and can be used with many out-of-the-box word embeddings.
p17-1087,2017,9 Conclusion,we developed a novel theory for signed normalized cuts and an algorithm for finding their discrete solution.
p17-1087,2017,9 Conclusion,we further showed that use of the thesauri can be tuned to the task at hand.
p17-1087,2017,9 Conclusion,"we showed that we can find sumodel accuracy nb (socher et al., 2013) 0.818 vecavg (w2v) 0.812 (faruqui et al., 2015) rvecavg (w2v) 0.821 (faruqui et al., 2015) rnn(socher et al., 2013) 0.824 nc(w2v) 0.79 sc(thes) 0.752 sc(w2v) 0.836 table 5: sentiment analysis accuracy for binary predictions of signed clustering algorithm (sc) versus other models."
p17-1088,2017,7 Conclusion and Future Work,"empirically, we show our model can improve the performance on two benchmark datasets without external resources, over all previous models of the same kind."
p17-1088,2017,7 Conclusion and Future Work,"in addition, our framework can also be applied to multi-task learning, promoting a finer sharing among different tasks."
p17-1088,2017,7 Conclusion and Future Work,"in summary, we propose a knowledge embedding model which can discover shared hidden concepts, and design a learning algorithm to induce the interpretable sparse representation."
p17-1088,2017,7 Conclusion and Future Work,"in the future, we plan to enable itransf to perform multi-step inference, and extend the sharing mechanism to entity and relation embeddings, further enhancing the statistical binding across parameters."
p17-1089,2017,7 Conclusion,"a key advantage of our approach is that it is not language-specific, and can easily be ported to other commonly used query languages, such as sparql or elasticsearch."
p17-1089,2017,7 Conclusion,"finally, we also release a new dataset of utterances and sql queries for an academic domain."
p17-1089,2017,7 Conclusion,"our approach uses an attentionbased neural sequence-to-sequence model, with data augmentation from the target database and paraphrasing, to parse utterances to sql."
p17-1089,2017,7 Conclusion,"this model is deployed in an online system, where user feedback on its predictions is used to select utterances to send for crowd worker annotation."
p17-1089,2017,7 Conclusion,we describe an approach to rapidly train a semantic parser as a nlidb that iteratively improves parser accuracy over time while requiring minimal intervention.
p17-1089,2017,7 Conclusion,"we find that the semantic parsing model is comparable in performance to previous systems that either map from utterances to logical forms, or generate sql, on two benchmark datasets, geo880 and atis."
p17-1089,2017,7 Conclusion,we further demonstrate the effectiveness of our online system by learning a semantic parser from scratch for an academic domain.
p17-1090,2017,7 Conclusion,experimental results on ami and icsi meeting corpora showed that our model can outperform state-of-the-art methods for both tasks.
p17-1090,2017,7 Conclusion,further evaluation on the task of predicting consistency-of-understanding in meetings demonstrated that classifiers trained with features constructed from our model output produced superior performance compared to the state-of-the-art model.
p17-1090,2017,7 Conclusion,this provides an evidence of our model being successfully applied in other prediction tasks in spoken meetings.
p17-1090,2017,7 Conclusion,we presented a joint model for performing phraselevel content selection and discourse relation prediction in spoken meetings.
p17-1091,2017,6 Conclusions and future work,our model also achieves state-of-the-art link prediction performance on the ukp essays dataset.
p17-1091,2017,6 Conclusions and future work,we demonstrate our model on a new argumentation mining dataset with more permissive argument structure annotation.
p17-1091,2017,6 Conclusions and future work,"we introduce an argumentation parsing model based on ad3 relaxed inference in expressive factor graphs, experimenting with both linear structured svms and structured rnns, parametrized with higher-order factors and link structure constraints."
p17-1092,2017,8 Conclusion,"these findings motivate further improvements to discourse parsing, especially for new genres."
p17-1092,2017,8 Conclusion,"we conclude that automatically-derived discourse structure can be helpful to text categorization, and the benefit increases with the accuracy of discourse parsing."
p17-1092,2017,8 Conclusion,"we did not see a benefit for categorizing legislative bills, a text genre whose discourse structure diverges from that of news."
p17-1093,2017,5 Discussions,"besides implicit connective examples, our model can naturally exploit enormous explicit connective data to further improve discourse parsing."
p17-1093,2017,5 Discussions,"besides, our adversarial mechanism provides an adaptive metric to measure and drive the imitation procedure."
p17-1093,2017,5 Discussions,our approach encourages imitation on the feature level instead of the final prediction level.
p17-1093,2017,5 Discussions,"our framework shares a similar spirit of the iterative knowledge distillation method (hu et al., 2016a,b) which train a 鈥渟tudent鈥 network to mimic the classification behavior of a knowledgeinformed 鈥渢eacher鈥 network."
p17-1093,2017,5 Discussions,our method achieved state-of-the-art performance for implicit discourse relation classification.
p17-1093,2017,5 Discussions,the proposed adversarial feature imitation scheme is also generally applicable to other context to incorporate indicative side information available at training time for enhanced inference.
p17-1093,2017,5 Discussions,"this allows our approach to apply to regression tasks, and more interestingly, the context in which the student and teacher networks have different prediction outputs, e.g., performing different tasks, while transferring knowledge between each other can be beneficial."
p17-1093,2017,5 Discussions,we have developed an adversarial neural framework that facilitates an implicit relation network to extract highly discriminative features by mimicking a connective-augmented network.
p17-1094,2017,7 Conclusions,"given the scale of our investigation, we limited our study to lsp, which is anyway considered state of the art."
p17-1094,2017,7 Conclusions,"however, as soon as separability becomes more complex simple loss functions lose optimality and rl becomes more accurate and faster.(v) our mela approximation provides a loss that is data invariant which, once learned, can be optimized in lsp on different datasets and in different languages."
p17-1094,2017,7 Conclusions,"in this paper, we studied the use of complex loss functions in structured prediction for cr."
p17-1094,2017,7 Conclusions,"our study opens several future directions, ranging from defining algorithms based on automatically learned loss functions to learning more effective measures from expert examples."
p17-1094,2017,7 Conclusions,"we derived several findings: (i) for the first time, up to our knowledge, we showed that a complex measure, such as mela, can be learned by a linear regressor (rl) with high accuracy and effective generalization.(ii) the latter was essential for designing a new lsp based on inexact search and rl.(iii) we showed that an automatically learned loss can be optimized and provides stateof-the-art performance in a real setting, including thousands of documents and millions of features, such as conll鈥2012 shared task.(iv) we defined a property of optimal loss functions for cr, which shows that in separable cases, such losses are enough to get the state of the art."
p17-1095,2017,6 Conclusion,experiments in semisupervised and low-resource settings have demonstrated its applicability to part-of-speech induction and low-resource named-entity recognition.
p17-1095,2017,6 Conclusion,"in the space of neural methods, differentiable memory (santoro et al., 2016) may be more flexible than the pyp prior, while retaining the ability of the model to cache strings observed in the gazetteer."
p17-1095,2017,6 Conclusion,"it would also be interesting to explore more expressive parameterizations, such recurrent neural networks for hy."
p17-1095,2017,6 Conclusion,our model may be useful in the context of active learning where efficient re-estimation and performance in low-data conditions are important.
p17-1095,2017,6 Conclusion,there are many potential avenues for future work.
p17-1095,2017,6 Conclusion,this paper has described a generative model for low-resource sequence labeling and segmentation tasks using lexical resources.
p17-1096,2017,6 Conclusions,"empirically, we show that our approach leads to substantial improvements over supervised learning models and outperforms several strong baselines including gans and dual learning."
p17-1096,2017,6 Conclusions,"in the future, we plan to apply our approach to more question answering datasets in different domains."
p17-1096,2017,6 Conclusions,it will also be intriguing to generalize gdans to other applications.
p17-1096,2017,6 Conclusions,"we propose a novel neural framework called generative domain-adaptive nets, which incorporate domain adaptation techniques in combination with generative models for semi-supervised learning."
p17-1096,2017,6 Conclusions,"we study a critical and challenging problem, semi-supervised question answering."
p17-1098,2017,8 Conclusion,in this work we proposed a query-based summarization method.
p17-1098,2017,8 Conclusion,the diversification model proposed is general enough to apply to other nlg tasks with suitable modifications and we are currently working on extending this to dialog systems and general summarization.
p17-1098,2017,8 Conclusion,the unique feature of the model is a novel diversification mechanism based on successive orthogonalization.
p17-1098,2017,8 Conclusion,this gives us the flexibility to: (i) provide diverse context vectors at successive time steps and (ii) pay attention to words repeatedly if need be later in the summary (as opposed to existing models which aggressively delete the history).
p17-1098,2017,8 Conclusion,we also compare with a state of the art diversity model and outperform it by a good margin (gain of 7% (absolute) in rouge-l score).
p17-1098,2017,8 Conclusion,we also introduced a new data set and empirically verified we perform significantly better (gain of 28% (absolute) in rouge-l score) than applying a plain encode-attend-decode mechanism to this problem.
p17-1098,2017,8 Conclusion,we observe that adding an attention mechanism on the query string gives significant improvements.
p17-1099,2017,8 Conclusion,"in this work we presented a hybrid pointergenerator architecture with coverage, and showed that it reduces inaccuracies and repetition."
p17-1099,2017,8 Conclusion,"our model exhibits many abstractive abilities, but attaining higher levels of abstraction remains an open research question."
p17-1099,2017,8 Conclusion,"we applied our model to a new and challenging longtext dataset, and significantly outperformed the abstractive state-of-the-art result."
p17-1100,2017,7 Conclusion,"our end-to-end evaluation showed that of our framework significantly outperforms strong baselines on the ap metric, but also revealed a large room for improvement in comparison to the upper-bound, which motivates future work on developing systems with better performance on the semantically motivated ap metric."
p17-1100,2017,7 Conclusion,we computed an upper-bound for ap and developed a supervised framework which learns an approximation of ap based on automatically generated training instances.
p17-1100,2017,7 Conclusion,we could access a large number of high-quality training data by using the population of a genetic algorithm.
p17-1100,2017,7 Conclusion,we presented the first work on ap in optimizationbased extractive summarization.
p17-1101,2017,7 Conclusion,"experimental results show that the selective encoding model greatly improves the performance with respect to the state-of-theart methods on english gigaword, duc 2004 and msr-atc test sets."
p17-1101,2017,7 Conclusion,"the selective mechanism mimics one of the human summarizers鈥 behaviors, selecting important information before writing down the summary."
p17-1101,2017,7 Conclusion,this paper proposes a selective encoding model which extends the sequence-to-sequence model for abstractive sentence summarization task.
p17-1101,2017,7 Conclusion,"with the proposed selective mechanism, we build an end-to-end neural network summarization model which consists of three phases: encoding, selection, and decoding."
p17-1102,2017,5 Conclusion and Future Work,"in the future, it would be interesting to explore the performance of positionrank on other types of documents, e.g., web pages and emails."
p17-1102,2017,5 Conclusion and Future Work,"our experiments on three datasets of research papers show that our proposed model achieves better results than strong baselines, with relative improvements in performance as high as 29.09%."
p17-1102,2017,5 Conclusion and Future Work,"specifically, unlike supervised approaches that use only the first position information, we showed that modeling the entire distribution of positions for a word outperforms models that use only the first position."
p17-1102,2017,5 Conclusion and Future Work,"to our knowledge, we are the first to integrate the position information in novel ways in unsupervised keyphrase extraction."
p17-1102,2017,5 Conclusion and Future Work,"we proposed a novel unsupervised graph-based algorithm, called positionrank, which incorporates both the position of words and their frequency in a document into a biased pagerank."
p17-1103,2017,7 Discussion,"alternatively, one can combine this with an adversarial evaluation model (kannan and vinyals, 2017; li et al., 2017) that assigns a score based on how easy it is to distinguish the dialogue model responses from human responses."
p17-1103,2017,7 Discussion,an important direction for future work is modifying adem such that it is not subject to this bias.
p17-1103,2017,7 Discussion,an important direction of future research is building models that can evaluate the capability of a dialogue system to have an engaging and meaningful interaction with a human.
p17-1103,2017,7 Discussion,"compared to evaluating a single response, this evaluation is arguably closer to the end-goal of chatbots."
p17-1103,2017,7 Discussion,"for example, one issue with building models to approximate human judgements of response quality is the problem of generic responses."
p17-1103,2017,7 Discussion,"however, our model could easily be extended to other general-purpose datasets, such as reddit, once similar pre-trained models become publicly available."
p17-1103,2017,7 Discussion,"however, such an evaluation is extremely challenging to do in a completely automatic way."
p17-1103,2017,7 Discussion,"in this case, a model that generates generic responses will easily be distinguishable and obtain a low score."
p17-1103,2017,7 Discussion,it is likely that this property does not fully capture the desired end-goal of chatbot systems.
p17-1103,2017,7 Discussion,"since humans often provide high scores to generic responses due to their appropriateness for many given contexts (shang et al., 2016), a model trained to predict these scores will exhibit the same behaviour."
p17-1103,2017,7 Discussion,"such models are necessary even for creating a test set in a new domain, which will help us determine if adem generalizes to related dialogue domains."
p17-1103,2017,7 Discussion,the evaluation model proposed in this paper favours dialogue models that generate responses that are rated as highly appropriate by humans.
p17-1103,2017,7 Discussion,"this could be done, for example, by censoring adem鈥檚 representations (edwards and storkey, 2016) such that they do not contain any information about length."
p17-1103,2017,7 Discussion,we leave investigating the domain transfer ability of adem for future work.
p17-1103,2017,7 Discussion,we use the twitter corpus to train our models as it contains a broad range of non-task-oriented conversations and it has been used to train many state-ofthe-art models.
p17-1103,2017,7 Discussion,"we view the evaluation procedure presented in this paper as an important step towards this goal; current dialogue systems are incapable of generating responses that are rated as highly appropriate by humans, and we believe our evaluation model will be useful for measuring and facilitating progress in this direction."
p17-1104,2017,7 Conclusion,"a parser for ucca will enable using the framework for new tasks, in addition to existing applications such as machine translation evaluation (birch et al., 2016)."
p17-1104,2017,7 Conclusion,"despite the recent diversity of semantic parsing work, the effectiveness of different approaches for structurally and semantically different schemes is not well-understood (kuhlmann and oepen, 2016)."
p17-1104,2017,7 Conclusion,"evaluated in in-domain and out-of-domain settings, we show that coupled with a nn classifier and bilstm feature extractor, it accurately predicts ucca graphs from text, outperforming a variety of strong baselines by a margin."
p17-1104,2017,7 Conclusion,"future work will evaluate tupa in a multilingual setting, assessing ucca鈥檚 cross-linguistic applicability."
p17-1104,2017,7 Conclusion,"in addition, we will explore different conversion procedures (kong et al., 2015) to compare different representations, suggesting ways for a data-driven design of semantic annotation."
p17-1104,2017,7 Conclusion,"our contribution to this literature is a general parser that supports multiple parents, discontinuous units and non-terminal nodes."
p17-1104,2017,7 Conclusion,"we believe ucca鈥檚 merits in providing a cross-linguistically applicable, broadcoverage annotation will support ongoing efforts to incorporate deeper semantic structures into various applications, such as sentence simplification (narayan and gardent, 2014) and summarization (liu et al., 2015)."
p17-1104,2017,7 Conclusion,"we present tupa, the first parser for ucca."
p17-1104,2017,7 Conclusion,"we will also apply the tupa transition scheme to different target representations, including amr and sdp, exploring the limits of its generality."
p17-1105,2017,5 Conclusion,asns provide a modular encoder-decoder architecture that can readily accommodate a variety of tasks with structured output spaces.
p17-1105,2017,5 Conclusion,"our results demonstrate their promise for tree prediction tasks, and we believe their application to more general output structures is an interesting avenue for future work."
p17-1105,2017,5 Conclusion,"they are particularly applicable in the presence of recursive decompositions, where they can provide a simple decoding process that closely parallels the inherent structure of the outputs."
p17-1106,2017,6 Conclusion,analyses of the state-of-art attention-based encoder-decoder framework on chinese-english translation show that our approach is able to offer more insights than the attention mechanism for interpreting neural machine translation.
p17-1106,2017,6 Conclusion,"in the future, we plan to apply our approach to more nmt approaches (sutskever et al., 2014; shen et al., 2016; tu et al., 2016; wu et al., 2016) on more language pairs to further verify its effectiveness."
p17-1106,2017,6 Conclusion,"in this work, we propose to use layer-wise relevance propagation to visualize and interpret neural machine translation."
p17-1106,2017,6 Conclusion,it is also interesting to develop relevancebased neural translation models to explicitly control relevance to produce better translations.
p17-1106,2017,6 Conclusion,our approach is capable of calculating the relevance between arbitrary hidden states and contextual words by back-propagating relevance along the network recursively.
p17-1107,2017,6 Conclusions,"in the paper, we addressed a severely understudied problem, namely the detection of errors in automatically annotated language resources."
p17-1107,2017,6 Conclusions,"our approach is language-agnostic and can be used without retraining the classifiers, which saves time and is of great practical use in an al setting."
p17-1107,2017,6 Conclusions,"our system architecture is generic and can be applied to any classification task, and we expect it to be of use in many annotation projects, especially when dealing with non-standard data or in out-of-domain settings."
p17-1107,2017,6 Conclusions,"using pos tagging and ner as test cases, we showed that our model can detect errors with high precision and recall, and works especially well in an out-of-domain setting."
p17-1107,2017,6 Conclusions,we also showed that combining an unsupervised generative model with human supervision is superior to using a query-by-committee strategy for al.
p17-1107,2017,6 Conclusions,we present an approach that combines an unsupervised generative model with human supervision in an al framework.
p17-1108,2017,5 Conclusion and Future Work,"an appealing direction is to investigate the neural abstractive method on the multi-document summarization task, which is more challenging and lacks training data."
p17-1108,2017,5 Conclusion and Future Work,experimental results on two large-scale datasets demonstrate our method achieves state-of-the-art abstractive document summarization performance.
p17-1108,2017,5 Conclusion and Future Work,extensive experiments verify the effectiveness of the proposed method.
p17-1108,2017,5 Conclusion and Future Work,further endeavor may be needed.
p17-1108,2017,5 Conclusion and Future Work,"in this paper we tackle the challenging task of abstractive document summarization, which is still less investigated to date."
p17-1108,2017,5 Conclusion and Future Work,it is also able to achieve competitive results with state-of-the-art neural extractive summarization models.
p17-1108,2017,5 Conclusion and Future Work,there is lots of future work we can do.
p17-1108,2017,5 Conclusion and Future Work,"we propose a novel graph-based attention mechanism in a hierarchical encoderdecoder framework, and propose a hierarchical beam search algorithm to generate multi-sentence summary."
p17-1108,2017,5 Conclusion and Future Work,"we study the difficulty of the abstractive document summarization task, and address the need of finding salient content from the original document, which is overlooked by previous studies."
p17-1109,2017,9 Conclusions,"additionally, we have introduced several novel evaluation metrics for research in vowelsystem typology, which we hope will spark further interest in the area."
p17-1109,2017,9 Conclusions,all models were additionally given a deep parameterization to learn representations similar to perceptual space in cognitive science.
p17-1109,2017,9 Conclusions,"also, we motivated our preference for probabilistic modeling in linguistic typology over previously proposed computational approaches and argued it is a more natural research paradigm."
p17-1109,2017,9 Conclusions,"their performance was empirically validated on the becker-kristal corpus, which includes data from over 200 languages."
p17-1109,2017,9 Conclusions,we have presented a series of point process models for the modeling of vowel system inventory typology with the goal of a mathematical grounding for research in phonological typology.
p17-1110,2017,9 Conclusions & Future Works,"experiments show that our proposed three shared-private models are effective to extract the shared information, and achieve significant improvements over the single-criterion methods."
p17-1110,2017,9 Conclusions & Future Works,"in this paper, we propose adversarial multi-criteria learning for cws by fully exploiting the underlying shared knowledge across multiple heterogeneous criteria."
p17-1111,2017,5 Conclusion,both of them use the character string embeddings.
p17-1111,2017,5 Conclusion,"our segtagdep joint model achieves better scores of chinese word segmentation and pos tagging than previous joint models, and our segtag and dep pipeline model achieves state-of-the-art score of dependency parsing."
p17-1111,2017,5 Conclusion,the bi-lstm models reduce the cost of feature engineering.
p17-1111,2017,5 Conclusion,the character string embeddings help to capture the similarities of incomplete tokens.
p17-1111,2017,5 Conclusion,we also explore the neural network with few features using n-gram bi-lstms.
p17-1111,2017,5 Conclusion,we propose the joint parsing models by the feedforward and bi-lstm neural networks.
p17-1112,2017,7 Conclusion,in this paper we advance the state of parsing by employing deep learning techniques to parse sentence to linguistically expressive semantic representations that have not previously been parsed in an end-to-end fashion.
p17-1112,2017,7 Conclusion,"we believe that there are many future avenues to explore to further increase the accuracy of such parsers, including different training objectives, more structured architectures and semisupervised learning."
p17-1112,2017,7 Conclusion,"we presented a robust, wide-coverage parser for mrs that is faster than existing parsers and amenable to batch processing."
p17-1113,2017,6 Conclusion,"although, our model can enhance the effect of entity tags, the association between two corresponding entities still requires refinement in next works."
p17-1113,2017,6 Conclusion,but it still has shortcoming on the identification of the overlapping relations.
p17-1113,2017,6 Conclusion,"in the future work, we will replace the softmax function in the output layer with multiple classifier, so that a word can has multiple tags."
p17-1113,2017,6 Conclusion,"in this paper, we propose a novel tagging scheme and investigate the end-to-end models to jointly extract entities and relations."
p17-1113,2017,6 Conclusion,"in this way, a word can appear in multiple triplet results, which can solve the problem of overlapping relations."
p17-1113,2017,6 Conclusion,the experimental results show the effectiveness of our proposed method.
p17-1114,2017,8 Conclusion,"in this paper, we propose a novel solution to ner and md by applying ffnn on top of fofe features."
p17-1114,2017,8 Conclusion,"this simple local-detection based approach has achieved almost state-of-the-art performance on various ner and md tasks, without using any external knowledge or feature engineering."
p17-1115,2017,7 Conclusions and Future Work,future work may involve testing prewin on an ner task to see if and how it can generalise to a different classification task and how the results compare to the sota and similar methods such as that of collobert et al.(2011) using the conll 2003 ner datasets.
p17-1115,2017,7 Conclusions and Future Work,"if it does perform better, this will be of considerable interest to classification research (and beyond) in nlp."
p17-1115,2017,7 Conclusions and Future Work,the pressing new question is: 鈥淗ow much better the performance could have been if our method availed itself of the extra training data and resources used by previous works?鈥 indeed this may be the next research chapter for prewin.
p17-1115,2017,7 Conclusions and Future Work,"we also introduced relocar, a new corpus for (location) metonymy resolution and encourage researchers to make effective use of it (including the additional conll 2003 subset we annotated for metonymy)."
p17-1115,2017,7 Conclusions and Future Work,we discussed how tasks such as geographical parsing can benefit from 鈥渕etonymy-enhanced鈥 ner tagging.
p17-1115,2017,7 Conclusions and Future Work,we fully agreed with the rest of the previous annotation guidelines.
p17-1115,2017,7 Conclusions and Future Work,"we have also presented a case for better annotation guidelines for mr (after consulting with a number of linguists), which now means that a government is not a literal class, rather it is a metonymic one."
p17-1115,2017,7 Conclusions and Future Work,"we showed how a minimalist neural approach can replace substantial external resources, handcrafted features and how the prewin method can even ignore most of the paragraph where the entity is positioned and still achieve competitive performance in metonymy resolution."
p17-1115,2017,7 Conclusions and Future Work,"word sense disambiguation (yarowsky, 2010; pilehvar and navigli, 2014) with neural networks (melamud et al., 2016) is another related classification task suitable for testing prewin."
p17-1116,2017,7 Conclusion,"additionally, we plan to apply the proposed model to other social media analyses such as gender analysis and age analysis."
p17-1116,2017,7 Conclusion,"as described in this paper, we proposed a complex neural network model for geolocation prediction."
p17-1116,2017,7 Conclusion,"as future works of this study, we are planning to expand the proposed model to handle multiple locations and a temporal state to capture location changes and states like traveling."
p17-1116,2017,7 Conclusion,"however, a user network is known to include various user attributes information including gender and age (mcpherson et al., 2001) which suggests the unification of text and user network information to result in a success as in geolocation prediction."
p17-1116,2017,7 Conclusion,"in these analyses, metadata like location fields and timezones may not be effective like in geolocation prediction."
p17-1116,2017,7 Conclusion,the model achieved the maximum of a 3.8% increase in accuracy and a maximum of 6.6% increase in accuracy@161 against several previous state-of-the-art models.
p17-1116,2017,7 Conclusion,"the model unifies text, metadata, and user network information."
p17-1116,2017,7 Conclusion,"we further analyzed the states of several attention layers, which revealed that the probabilities assigned to timeline representations and user network representations match to some statistical characteristics of datasets."
p17-1117,2017,6 Conclusion,"in future work, we are applying our entailment-based multi-task paradigm to other directed language generation tasks such as image captioning and document summarization."
p17-1117,2017,6 Conclusion,"we achieve the best reported results (and rank) on three datasets, based on multiple automatic and human evaluations."
p17-1117,2017,6 Conclusion,we also show mutual multi-task improvements on the new entailment generation task.
p17-1117,2017,6 Conclusion,"we presented a multimodal, multi-task learning approach to improve video captioning by incorporating temporally and logically directed knowledge via video prediction and entailment generation tasks."
p17-1118,2017,6 Conclusions and Future Work,"as this work is ongoing, we will keep collecting new transcriptions of the abcd retelling subtest to increase the corpus size and obtain more reliable results in our studies."
p17-1118,2017,6 Conclusions and Future Work,"furthermore, we found that combining machine and multi-view learning can improve accuracy."
p17-1118,2017,6 Conclusions and Future Work,"in future work, we intend to explore other methods to enrich cn, such as the recurrent language model, and use other metrics to characterize an adjacency network."
p17-1118,2017,6 Conclusions and Future Work,"in this study, we employed metrics of topological properties of cn in a machine learning classification approach to distinguish between healthy patients and patients with mci."
p17-1118,2017,6 Conclusions and Future Work,"linguistic features depend on grammatical texts to present good results, as can be seen in the results of the manually processed cinderella dataset (table 7)."
p17-1118,2017,6 Conclusions and Future Work,"our final goal is to apply neuropsychological assessment batteries, such as the abcd retelling subtest, to mobile devices, specifically tablets."
p17-1118,2017,6 Conclusions and Future Work,"the accuracies found here are comparable to the values reported by other authors, ranging from 60% to 85% (prud鈥檋ommeaux and roark, 2011; lehr et al., 2012; toth et al.麓 , 2015; vincze et al., 2016), which means that it is not easy to distinguish between healthy subjects and those with cognitive impairments."
p17-1118,2017,6 Conclusions and Future Work,"the comparison with our results is not straightforward, though, because the databases used in the studies are different."
p17-1118,2017,6 Conclusions and Future Work,"the pursuit of these strategies is relevant because language is one of the most efficient information sources to evaluate cognitive functions, commonly used in neuropsychological assessments."
p17-1118,2017,6 Conclusions and Future Work,the topological properties of cn outperform traditional linguistic metrics in individual classifiers鈥 results.
p17-1118,2017,6 Conclusions and Future Work,"there is a clear need for publicly available datasets to compare different methods, which would optimize the detection of mci in elderly people."
p17-1118,2017,6 Conclusions and Future Work,"this adaptation will enable large-scale applications in hospitals and facilitate the maintenance of application history in longitudinal studies, by storing the results in databases immediately after the test application."
p17-1118,2017,6 Conclusions and Future Work,"to the best of our knowledge, these metrics have never been used to detect mci in speech transcripts; cn were enriched with word embeddings to better represent short texts produced in neuropsychological assessments."
p17-1119,2017,5 Conclusion,"in both supervised and unsupervised adaptation scenarios, our approach yields clear improvement over strong baselines."
p17-1119,2017,5 Conclusion,"in this paper, we have addressed two types of data shift common in slu applications: 1. transferring from synthetic data to live user data (a deployment shift), and 2. transferring from stale data to current data (a temporal shift)."
p17-1119,2017,5 Conclusion,"our method is based on domain adaptation, treating the flawed training dataset as a source domain and the evaluation dataset as a target domain."
p17-1119,2017,5 Conclusion,"we use and build on several recent advances in neural domain adaptation such as adversarial training and domain separation network, proposing a new effective adversarial training scheme based on randomized predictions."
p17-1120,2017,6 Future Work,"although the studies on non-task-oriented sds have made substantial progress in the past few years, it unfortunately remains difficult for the systems to fluently chat with users (higashinaka et al., 2015)."
p17-1120,2017,6 Future Work,"although we used only text data to perform chat detection, we can also utilize contextual information such as the previous utterances (xu and sarikaya, 2014), the acoustic information (jiang et al., 2015), and the user profile (sano et al., 2016)."
p17-1120,2017,6 Future Work,"an automatic speech recognition (asr) error is a popular problem in sds, and previous studies have proposed sophisticated techniques, including re-ranking (morbini et al., 2012) and pomdp (williams and young, 2007), for addressing the asr errors."
p17-1120,2017,6 Future Work,"an important future work is to develop a sophisticated dialogue manager to handle such utterances, for example, by making clarification questions (schloder and fernandez, 2015).篓 we manually investigated the dialogue acts in the chat detection dataset (c.f., section 3.2)."
p17-1120,2017,6 Future Work,"as discussed in section 3.2, some user utterances such as 鈥淚 am hungry鈥 are ambiguous in nature and thus are difficult to handle in the current framework."
p17-1120,2017,6 Future Work,further efforts on improving nontask-oriented dialogue systems is an important future work.
p17-1120,2017,7 Conclusion,"in addition, we investigated using the external resources, tweets and web search queries, to handle open-domain user utterances, which characterize the task of chat detection."
p17-1120,2017,6 Future Work,incorporating these techniques into our methods is also an important future work.
p17-1120,2017,6 Future Work,it is an interesting research topic to use such contextual information beyond text.
p17-1120,2017,6 Future Work,it is considered promising to make use of a neural network for integrating such heterogeneous information.
p17-1120,2017,6 Future Work,it is interesting to automatically determine the dialogue acts to help producing appropriate system responses.
p17-1120,2017,6 Future Work,"some related studies exist in such a research direction (meguro et al., 2010)."
p17-1120,2017,7 Conclusion,"the empirical experiment demonstrated that the off-the-shelf supervised methods augmented with the external resources perform accurately, outperforming the baseline approaches."
p17-1120,2017,7 Conclusion,this paper investigated chat detection for combining domain-specific task-oriented sds and opendomain non-task-oriented sds.
p17-1120,2017,7 Conclusion,"to address the scarcity of benchmark datasets for this task, we constructed a new benchmark dataset from the real log data of a commercial intelligent assistant."
p17-1120,2017,7 Conclusion,"to facilitate future research, we are going to release the dataset together with the feature values derived from the tweets and web search queries."
p17-1120,2017,7 Conclusion,we hope that this study contributes to remove the long-standing boundary between task-oriented and non-task-oriented sds.
p17-1121,2017,7 Conclusion and Future Work,"in future, we would like to include other sources of information in our model."
p17-1121,2017,7 Conclusion and Future Work,"our architecture can model sufficiently long entity transitions, and can incorporate entity-specific features without loosing generalization power."
p17-1121,2017,7 Conclusion and Future Work,"our evaluation on discrimination, insertion and summary coherence rating tasks demonstrates the effectiveness of our approach yielding the best results reported so far on these tasks."
p17-1121,2017,7 Conclusion and Future Work,"our initial plan is to include rhetorical relations, which has been shown to benefit existing grid models (feng et al., 2014)."
p17-1121,2017,7 Conclusion and Future Work,we described a pairwise ranking approach to train the model on a target task and learn task-specific features.
p17-1121,2017,7 Conclusion and Future Work,we presented a local coherence model based on a convolutional neural network that operates over the distributed representation of entity transitions in the grid representation of a text.
p17-1121,2017,7 Conclusion and Future Work,"we would also like to extend our model to other forms of discourse, especially, asynchronous conversations, where participants communicate with each other at different times (e.g., forum, email)."
p17-1122,2017,8 Conclusion,"finally, we believe that future work should be evaluated in situ, to examine if, and to what extent, the generated responses participate in and affect the discourse (feed) in social media."
p17-1122,2017,8 Conclusion,"our contribution is threefold: (i) we designed three types of broad-coverage grammars appropriate for the task, (ii) we developed a new enriched data-set for inducing the grammars, and (iii) we empirically demonstrated the strengths of the lex and rr grammars for generation, as well as the overall usefulness of sentiment and topic models incorporated into the syntactic derivation."
p17-1122,2017,8 Conclusion,our results show that the proposed grammar-based architecture indeed avoids the repetitiveness and learning effects observed in the template-based onlg.
p17-1122,2017,8 Conclusion,"some future avenues for investigation include improving the relevance and human-likeness results by improving the automatic parses quality, acquiring more complex templates via abstract grammars, and experimenting with more sophisticated scoring functions for reranking."
p17-1122,2017,8 Conclusion,"to the best of our knowledge, this is the first data-driven agenda-driven baseline for onlg, and we believe it can be further improved."
p17-1122,2017,8 Conclusion,"we approached onlg from a data-driven perspective, aiming to overcome the shortcomings of previous template-based approaches."
p17-1122,2017,8 Conclusion,"with the emergence of deep learning, we further embrace the opportunity to combine the sequence-to-sequence modeling view explored so far with conditioning generation on speakers agendas and user profiles, pushing the envelope of opinionated generation further."
p17-1123,2017,7 Conclusion and Future Work,"besides this, it would also be interesting to consider to incorporate mechanisms for other language generation tasks (e.g., copy mechanism for dialogue generation) in our model to further improve the quality of generated questions."
p17-1123,2017,7 Conclusion and Future Work,"currently, our paragraph-level model does not achieve best performance across all categories of questions."
p17-1123,2017,7 Conclusion and Future Work,here we point out several interesting future research directions.
p17-1123,2017,7 Conclusion and Future Work,our best model achieves state-of-the-art performance in both automatic evaluations and human evaluations.
p17-1123,2017,7 Conclusion and Future Work,we have presented a fully data-driven neural networks approach to automatic question generation for reading comprehension.
p17-1123,2017,7 Conclusion and Future Work,we use an attentionbased neural networks approach for the task and investigate the effect of encoding sentence- vs. paragraph-level information.
p17-1123,2017,7 Conclusion and Future Work,we would like to explore how to better use the paragraph-level information to improve the performance of qg system regarding questions of all categories.
p17-1124,2017,6 Conclusion and Future Work,"as future work, we plan to investigate more sophisticated sampling strategies based on active learning and concept graphs to incorporate lexicalsemantic information for concept selection."
p17-1124,2017,6 Conclusion and Future Work,"in this paper, we investigate pool-based active learning and joint optimization techniques to collect user feedback for identifying important concepts for a summary."
p17-1124,2017,6 Conclusion and Future Work,our models show that interactively collecting feedback consistently steers a general summary towards a user-desired personalized summary.
p17-1124,2017,6 Conclusion and Future Work,"this is a promising direction as we have shown that interactive methods help to create user-desired personalized summaries, and with minimum amount of feedbacks, it has propitious use in scenarios where user-adapted content is a requirement."
p17-1124,2017,6 Conclusion and Future Work,"we also plan to look into ways to propagate feedback to similar and related concepts with partial feedback, to reduce the total amount of feedback."
p17-1124,2017,6 Conclusion and Future Work,we empirically checked the validity of our approach on standard datasets using simulated user feedback and observed that our framework shows promising results in terms of producing personalized multidocument summaries.
p17-1124,2017,6 Conclusion and Future Work,we propose a novel ilp-based approach using interactive user feedback to create multi-document user-desired summaries.
p17-1125,2017,6 Conclusions,"both strategies work well, although the former generated poetry that was preferred by experts in our experiments."
p17-1125,2017,6 Conclusions,"experimental results demonstrated that memory can boost innovation from two opposite di rections: either by encouraging creative generation for regularly-trained models, or by encouraging rule-compliance for overfitted models."
p17-1125,2017,6 Conclusions,"furthermore, we found that the memory can be used to modify the style of the generated poems in a flexible way."
p17-1125,2017,6 Conclusions,future work involves investigating a better memory selection scheme.
p17-1125,2017,6 Conclusions,"in this paper, we proposed a memory mechanism to support innovative chinese poem generation by neural models augmented with a memory."
p17-1125,2017,6 Conclusions,"other regularization methods (e.g., norm or drop out) are also interesting and may alleviate the over-fitting problem"
p17-1125,2017,6 Conclusions,the experts we collaborated with feel that the present generation is comparable to today鈥檚 experienced amateur poets.
p17-1126,2017,5 Conclusion and Future Work,descriptions of numerical time-series data written by humans such as market comments have several writing style characteristics.
p17-1126,2017,5 Conclusion and Future Work,"for example, (1) content to be mentioned in the market comments varies depending on short- or long-term changes of the time-series data, (2) expressions depending on delivery time at which text is written are used, and (3) numerical values obtained through arithmetic operations applied to the input data are often described."
p17-1126,2017,5 Conclusion and Future Work,"in future work, we plan to apply our model to descriptions of time-series data in various domains such as weather forecasts and sports, which share the above writing-style characteristics."
p17-1126,2017,5 Conclusion and Future Work,"in this study, we presented a novel encoder-decoder model to automatically generate market comments from numerical time-series data of stock prices, using the nikkei stock average as an example."
p17-1126,2017,5 Conclusion and Future Work,we also plan to use multiple time-series as input such as multiple brands of stock.
p17-1126,2017,5 Conclusion and Future Work,we developed approaches for generating comments that have these characteristics and showed the effectiveness of the proposed model.
p17-1127,2017,5 Conclusions,"in comparison, the original lstm model does not work well in the cross-domain setting, and a traditional ilp method does not work well in the in-domain setting."
p17-1127,2017,5 Conclusions,"in this paper, we studied how to modify an lstm model for deletion-based sentence compression so that the model works well in a cross-domain setting."
p17-1127,2017,5 Conclusions,the experiments showed that our proposed bi-lstm model with syntactic features and an ilp layer works well in both in-domain and cross-domain settings.
p17-1127,2017,5 Conclusions,"therefore, our proposed method is relatively more robust than these baselines."
p17-1127,2017,5 Conclusions,we also manually evaluated the compressed sentences generated by our method and found that the method works better than the baselines in terms of both readability and informativeness.
p17-1127,2017,5 Conclusions,we hypothesized that incorporation of syntactic information into the training of the lstm model would help.
p17-1127,2017,5 Conclusions,"we thus proposed two ways to incorporate syntactic information, one through directly adding pos tag embeddings and dependency type embeddings, and the other through the objective function and constraints of an integer linear programming (ilp) model."
p17-1128,2017,5 Conclusion,"by modeling linear projection models, linguistic rules and non-linear mappings, our method is able to identify chinese hypernyms with high accuracy."
p17-1128,2017,5 Conclusion,experiments show that the performance of our method outperforms previous approaches.
p17-1128,2017,5 Conclusion,"in our work, the candidate chinese hyponyms and hypernyms are extracted from user generated categories."
p17-1128,2017,5 Conclusion,"in summary, this paper introduces a transuctive learning approach for chinese hypernym prediction."
p17-1128,2017,5 Conclusion,"in the future, we will study how to construct a taxonomy from texts in chinese."
p17-1128,2017,5 Conclusion,we also discuss the potential applications of our method besides chinese hypernym prediction.
p17-1129,2017,5 Conclusions and Future Work,another direction to explore is joint learning of syntactic parser and chain-of-trees lstm.
p17-1129,2017,5 Conclusions and Future Work,"currently, the two are separated, which may lead to suboptimal performance."
p17-1129,2017,5 Conclusions and Future Work,evaluations on the squad dataset demonstrate the effectiveness of the constituent-centric neural architecture.
p17-1129,2017,5 Conclusions and Future Work,"for future work, we will investigate the wider applicability of chain-of-trees lstm as a general text encoder that can simultaneously capture local syntactic structure and long-range semantic dependency."
p17-1129,2017,5 Conclusions and Future Work,"it can be applied to named entity recognition, sentiment analysis, dialogue generation, to name a few."
p17-1129,2017,5 Conclusions and Future Work,"to represent these candidate answers, we propose a chain-of-trees lstm to encode constituents and a tree-guided attention mechanism to learn question-aware representations."
p17-1129,2017,5 Conclusions and Future Work,"to solve the squad question answering problem, we design a constituent centric neural network (ccnn), where the generation and representation learning of candidate answers are both based on constituents."
p17-1129,2017,5 Conclusions and Future Work,"we use a constituent expansion mechanism to produce candidate answers, which can greatly reduce the search space without losing the recall of hitting the correct answer."
p17-1129,2017,5 Conclusions and Future Work,"we will also apply the tree-guided attention mechanism to nlp tasks that need syntaxaware attention, such as machine translation, sentence summarization, textual entailment, etc."
p17-1130,2017,6 Conclusion,"the excellent performance of our approach is evident in our evaluation on two cltc benchmark datasets, compared to that of other state-of-the-art methods."
p17-1130,2017,6 Conclusion,"this work introduces a novel framework for distillation of discriminative knowledge across languages, providing effective and efficient algorithmic solutions for addressing domain/distribution mismatch issues in cltc."
p17-1131,2017,8 Conclusions,"conversation content: empathic counselors use reflective language and talk about behavior change, while less empathic counselors persuade more and focus on client resistance toward change."
p17-1131,2017,8 Conclusions,"coordination: empathic counselors match the linguistic style of their clients across the session, but maintain control of the conversation by coordinating less at immediate conversation turn-level."
p17-1131,2017,8 Conclusions,"in the future, we plan to build upon the acquired knowledge and the developed classifiers to create automatic tools that provide accurate evaluative feedback of counseling practice."
p17-1131,2017,8 Conclusions,"in this paper, we presented an extensive analysis of counselors and clients behaviors during mi encounters, and found significant differences in the way counselors and clients behave during high and low empathy encounters."
p17-1131,2017,8 Conclusions,"our main findings include: engagement: empathic counselors show more engagement during the conversation by a) showing levels of verbal interaction consistent with their client, and b) reducing their relative talking time with clients."
p17-1131,2017,8 Conclusions,"the results of these analyses were used to build accurate counselor empathy classifiers that rely on linguistic and acoustic cues, with accuracies of up to 80%."
p17-1131,2017,8 Conclusions,"we specifically explored the engagement, coordination, and discourse of counselors during mi interventions."
p17-1132,2017,5 Conclusion,experimental results show that our model achieves state-of-the-art performance on standard benchmarks for both entity extraction and event extraction.
p17-1132,2017,5 Conclusion,"in this paper, we introduce the kblstm network architecture as one approach to incorporating background kbs for improving recurrent neural networks for machine reading."
p17-1132,2017,5 Conclusion,our model can also be extended to integrate knowledge from a richer set of kbs in order to capture the diverse variety and depth of background knowledge required for accurate and deep language understanding.
p17-1132,2017,5 Conclusion,"this architecture employs an adaptive attention mechanism with a sentinel that allows for learning an appropriate tradeoff for blending knowledge from the kbs with information from the currently processed text, as well as selecting among relevant kb concepts for each word (e.g., to select the correct semantic categories for 鈥渃linton鈥 as a town or person in figure 2a)."
p17-1132,2017,5 Conclusion,"though our model is evaluated on entity extraction and event extraction, it can be useful for other machine reading tasks."
p17-1132,2017,5 Conclusion,we see many additional opportunities to integrate background knowledge with training of neural network models for language processing.
p17-1133,2017,6 Conclusions and Future Work,"moreover, we apply an embeddingbased method that jointly learns the semantic representations of wikipedia concepts and mooc concepts to help implement the features."
p17-1133,2017,6 Conclusions and Future Work,"promising future directions would be to investigate how to utilize user interaction in moocs for better prerequisite learning, as well as how deep learning models can be used to automatically learn useful features to help infer prerequisite relations."
p17-1133,2017,6 Conclusions and Future Work,we conducted a new investigation on automatically inferring prerequisite relations among concepts in moocs.
p17-1133,2017,6 Conclusions and Future Work,"we precisely define the problem and propose several useful features from different aspects, i.e., contextual, structural and semantic features."
p17-1134,2017,6 Conclusion and Future Work,a more sophisticated approach would be to incorporate features into the unsupervised model.
p17-1134,2017,6 Conclusion and Future Work,"additionally, the granularity of segmentation may need to be finer than sentence-level, as suggested by the examples in 搂1; this level of granularity hasn鈥檛 previously been tackled in unsupervised segmentation."
p17-1134,2017,6 Conclusion and Future Work,"applying the approach to our two illustrative applications of 搂1, patchwriting and literary analysis, would require development of relevant corpora."
p17-1134,2017,6 Conclusion and Future Work,"as far as this initial work is concerned we have shown that, framed as a segmentation task, it is possible to identify units of text that differ stylistically in their l1 influence."
p17-1134,2017,6 Conclusion and Future Work,"for the literary analysis, as well, to bridge the gap between work like morzinski (1994) and a computational application, it remains to be seen how precise an annotation is possible for this task."
p17-1134,2017,6 Conclusion and Future Work,"in both cases the distinction would be between native writing and writing that shows characteristics of a non-native speaker, rather than between two nonnative l1s."
p17-1134,2017,6 Conclusion and Future Work,"in terms of possible developments for the models presented for the task here, previous nli work has shown that other, syntactic features can be useful for capturing l1-based differences."
p17-1134,2017,6 Conclusion and Future Work,one such example is the work of berg-kirkpatrick et al.(2010) which demonstrates that each component multinomial of a generative model can be turned into a miniature logistic regression model with the use of a modified em algorithm.
p17-1134,2017,6 Conclusion and Future Work,"our best results come from a model that uses alternating asymmetric priors for each l1, with the priors selected using a grid search and then evaluated on a held-out test set."
p17-1134,2017,6 Conclusion and Future Work,the incorporation of these features for this segmentation task could be a potentially fruitful avenue for future work.
p17-1134,2017,6 Conclusion and Future Work,their results showed that the feature-enhanced unsupervised models which incorporate linguisticallymotivated features achieve substantial improvements for tasks such as pos induction and word segmentation.
p17-1134,2017,6 Conclusion and Future Work,"there isn鈥檛 yet a topic-balanced corpus like toefl11 which includes native speaker writing for evaluation, although we expect (given recent results on distinguishing native from nonnative text in malmasi and dras (2015)) that the techniques should carry over."
p17-1134,2017,6 Conclusion and Future Work,"we demonstrated that it is possible to define a generative story and associated bayesian models for stylistic segmentation, and further that segmentation results improve substantially by compacting the n-gram distributions, achieved by incorporating knowledge about discriminative features extracted from nli models."
p17-1134,2017,6 Conclusion and Future Work,we have taken a fairly straightforward approach which modifies the generative story.
p17-1134,2017,6 Conclusion and Future Work,we note also that the models are potentially applicable to other stylistic segmentation tasks beyond l1 influence.
p17-1135,2017,8 Conclusion,"experimental results show that the combined systems outperform three state-ofthe-art cross-lingual ner approaches, providing a strong baseline for building cross-lingual ner systems with no human annotation in target languages."
p17-1135,2017,8 Conclusion,"in this paper, we developed two weakly supervised approaches for cross-lingual ner based on effective annotation and representation projection."
p17-1135,2017,8 Conclusion,we also designed two co-decoding schemes that combine the two projection-based systems in an intelligent way.
p17-1136,2017,4 Conclusion,"additionally, it does not rely on human defined features and various morphological tags except the gold lemma annotated continuous text."
p17-1136,2017,4 Conclusion,"apart from it, we intend to propose a neural architecture that accomplishes the joint learning of lemmas with other morphological attributes."
p17-1136,2017,4 Conclusion,"except bengali, the proposed method outperforms the stateof-the-art models (lemming and morfette) on all the other languages."
p17-1136,2017,4 Conclusion,"for bengali, it produces the second best performance (91.14% using blstmblstm)."
p17-1136,2017,4 Conclusion,"for evaluation, nine languages are taken as the references."
p17-1136,2017,4 Conclusion,"for hindi, the change in accuracy is insignificant but for latin and spanish, accuracy drops by 3.50% and 6% respectively."
p17-1136,2017,4 Conclusion,"from the present study, one important finding comes out that for the unseen words, the lemmatization accuracy drops by a large margin in bengali and spanish, which may be the area of further research work."
p17-1136,2017,4 Conclusion,"in our work, we use the 鈥榢eras鈥 software keeping 鈥榯heano鈥 as backend."
p17-1136,2017,4 Conclusion,"once trained, the model takes negligible time to predict the appropriate edit trees for test words (e.g.844 and 930 words/second for bengali and hindi respectively)."
p17-1136,2017,4 Conclusion,"the codes were run on a single gpu (nvidia geforce gtx 960, 2gb memory)."
p17-1136,2017,4 Conclusion,"the proposed model learns the transformation patterns between word-lemma pairs and hence, can handle the unknown word forms too."
p17-1136,2017,4 Conclusion,the time requirement of the proposed method is also analyzed.
p17-1136,2017,4 Conclusion,this article presents a neural network based context sensitive lemmatization method which is language independent and supervised in nature.
p17-1136,2017,4 Conclusion,"training time depends on several parameters such as size of the data, number of epochs required for convergence, configuration of the system used etc."
p17-1136,2017,4 Conclusion,we develop a bengali lemmatization dataset which is definitely a notable contribution to the language resources.
p17-1136,2017,4 Conclusion,we explore different variations of the model architecture by changing the type of recurrent units.
p17-1136,2017,4 Conclusion,"we measure the accuracy on the partial data (keeping the data size comparable to the bengali dataset) for hindi, latin and spanish to check the effect of the data amount on the performance."
p17-1137,2017,8 Conclusion,and we empirically show that our model efficiently model the word sequences and achieved better perplexity in every standard dataset.
p17-1137,2017,8 Conclusion,"in this paper, we proposed a character-level language model with an adaptive cache which selectively assign word probability from past history or character-level decoding."
p17-1137,2017,8 Conclusion,the model proposed in this paper assumes the observation of word segmentation.
p17-1137,2017,8 Conclusion,"thus, the model is not directly applicable to languages, such as chinese and japanese, where word segments are not explicitly observable."
p17-1137,2017,8 Conclusion,"to further validate the performance of our model on different languages, we collected multilingual wikipedia corpus for 7 typologically diverse languages."
p17-1137,2017,8 Conclusion,we also show that our model performs better than character-level models by modeling burstiness of words in local context.
p17-1137,2017,8 Conclusion,we will investigate a model which can marginalise word segmentation as latent variables in the future work.
p17-1138,2017,6 Conclusion,"furthermore, we argued that pairwise ranking under bandit feedback can be interpreted as a use of antithetic variates, and we showed how to include average reward and score function baselines as control variates for improved training speed and generalization."
p17-1138,2017,6 Conclusion,"in future work, we would like to apply the presented non-linear bandit learners to other structured prediction tasks."
p17-1138,2017,6 Conclusion,"in our experimental evaluation on the task of neural machine translation domain adaptation, we found relative improvements of up to 5.89 bleu points over out-of-domain seed models, outperforming also linear bandit models."
p17-1138,2017,6 Conclusion,"in this paper, we showed how to lift structured prediction under bandit feedback from linear models to non-linear sequence-to-sequence learning using recurrent neural networks with attention."
p17-1138,2017,6 Conclusion,we introduced algorithms to train these models under numerical feedback to single output structures or under preference rankings over pairs of structures.
p17-1139,2017,6 Conclusion,experiments show that incorporating prior knowledge leads to significant improvements over both standard nmt and posterior regularization using constrained posterior sets.
p17-1139,2017,6 Conclusion,the basic idea is to guide nmt models towards desired behavior using a log-linear model that encodes prior knowledge.
p17-1139,2017,6 Conclusion,"we have presented a general framework for incorporating prior knowledge into end-to-end neural machine translation based on posterior regularization (ganchev et al., 2010)."
p17-1140,2017,6 Conclusions,"compared with previous work on identical corpora, our model achieves the state-of-the-art performance on average."
p17-1140,2017,6 Conclusions,experiments show that our models can evidently improve the word alignment quality and translation performance.
p17-1140,2017,6 Conclusions,"in the future, we plan to validate the effectiveness of our model on more language pairs."
p17-1140,2017,6 Conclusions,our model is convenient to be applied in the attention-based nmt and can be trained in the end-to-end style.
p17-1140,2017,6 Conclusions,the basic idea of proposed distortion models is to enable the attention mechanism to attend to the source words regarding both semantic requirement and the word reordering penalty.
p17-1140,2017,6 Conclusions,we also investigated the effect of hyper-parameters and pre-training strategy and further proved the stable effectiveness of our model.
p17-1140,2017,6 Conclusions,we have presented three distortion models to enhance attention-based nmt through incorporating the word reordering knowledge.
p17-1141,2017,6 Conclusion,"a wide spectrum of popular text generation models have this characteristic, and gbs should be straightforward to use with any model that already uses beam search."
p17-1141,2017,6 Conclusion,"by simulating this scenario, we have shown that such a workflow can provide a large improvement in translation quality at each iteration."
p17-1141,2017,6 Conclusion,"by using a domain-specific terminology to generate target-side constraints, we have shown that a general domain model can be adapted to a new domain without any retraining."
p17-1141,2017,6 Conclusion,"in future work, we hope to evaluate gbs with models outside of mt, such as automatic summarization, image captioning or dialog generation."
p17-1141,2017,6 Conclusion,"in translation interfaces where translators can provide corrections to an existing hypothesis, these user inputs can be used as constraints, generating a new output each time a user fixes an error."
p17-1141,2017,6 Conclusion,lexically constrained decoding is a flexible way to incorporate arbitrary subsequences into the output of any model that generates output sequences token-by-token.
p17-1141,2017,6 Conclusion,"surprisingly, this simple method can lead to significant performance gains, even when the terminology is created automatically."
p17-1141,2017,6 Conclusion,"we also hope to introduce new constraintaware models, for example via secondary attention mechanisms over lexical constraints."
p17-1142,2017,7 Conclusion and Future Work,"another direction involves working to understand text in images, which can provide more information about the subjects of the images."
p17-1142,2017,7 Conclusion and Future Work,"as new obfuscated words are introduced in escort advertisements, our hope is that character models will stay invariant to these obfuscations.understanding images."
p17-1142,2017,7 Conclusion and Future Work,exploring language through character modeling.
p17-1142,2017,7 Conclusion and Future Work,future direction involves using graphical modeling to understand interactions in the scene.
p17-1142,2017,7 Conclusion and Future Work,"given that the current state of the art in this area generally does not use deep models, this may be a major opportunity for improvement."
p17-1142,2017,7 Conclusion and Future Work,"in order to eliminate the need for retraining the word vectors as the language of the domain evolves, we plan to use character models to learn a better language model for trafficking."
p17-1142,2017,7 Conclusion and Future Work,"in this paper, we took a major step in multimodal modeling of suspected online trafficking advertisements."
p17-1142,2017,7 Conclusion and Future Work,the dataset contains two modalities of information per advertisement: text and images.
p17-1142,2017,7 Conclusion and Future Work,"the htdn outperformed all of these, indicating that using information from both sources may be more helpful than using just one."
p17-1142,2017,7 Conclusion and Future Work,"to this end, we encourage the research community to reach out to cara jones, an author of this paper, to obtain a copy of trafficking-10k and other training data."
p17-1142,2017,7 Conclusion and Future Work,we compared the performance of the htdn to various models that use language and vision alone.
p17-1142,2017,7 Conclusion and Future Work,we designed a deep multimodal model called the human trafficking deep network (htdn).
p17-1142,2017,7 Conclusion and Future Work,"we presented a novel dataset, trafficking10k, with more than 10,000 advertisements annotated for this task."
p17-1142,2017,7 Conclusion and Future Work,"while cnns have proven to be useful for many different computer vision tasks, we seek to improve the learning capability of the visual network."
p17-1143,2017,6 Conclusion,"finally, we discuss several factors that make these tasks extremely challenging given currently available models."
p17-1143,2017,6 Conclusion,"in this paper, we presented a framework for annotating malware reports."
p17-1143,2017,6 Conclusion,more details about this database can be found at http://statnlp.org/research/re/.
p17-1143,2017,6 Conclusion,we also introduced a database with 39 annotated apt reports and proposed several new tasks and built models for extracting information from the reports.
p17-1143,2017,6 Conclusion,we hope that this paper and the accompanying database serve as a first step towards nlp being applied in cybersecurity and that other researchers will be inspired to contribute to the database and to construct their own datasets and implementations.
p17-1144,2017,5 Conclusion and Future Work,currently the corpus focuses on essay revisions made by both native and l2 college students.
p17-1144,2017,5 Conclusion and Future Work,"in addition to three drafts of essays, we have analyzed the drafts to align semantically similar sentences and to assign revision purposes for each revised aligned sentence pair."
p17-1144,2017,5 Conclusion and Future Work,"some potential augmentations include more fine-grained revision categories, revision properties such as statement strength (tan and lee, 2014) and quality evaluations, and sub-sentential revision scopes."
p17-1144,2017,5 Conclusion and Future Work,we also plan to augment the corpus to support additional types of research on revision analysis.
p17-1144,2017,5 Conclusion and Future Work,we have also conducted two studies to demonstrate the use of the corpus for revision behavior analysis and for automatic revision purpose classification.
p17-1144,2017,5 Conclusion and Future Work,we have presented a new corpus for writing comparison research.
p17-1144,2017,5 Conclusion and Future Work,"while in this paper we explored language as one factor influencing rewriting behavior, our corpus also contains information about other potential factors such as gender and education level which we plan to investigate in the future."
p17-1145,2017,7 Conclusion,the disambiguated graph facilitates clustering as it contains fewer hubs connecting unrelated nodes from different communities.
p17-1145,2017,7 Conclusion,"the transformed 鈥渄isambiguated鈥 graph is then clustered using an efficient hard graph clustering algorithm, obtaining a fuzzy clustering as the result."
p17-1145,2017,7 Conclusion,"using ego network clustering, the nodes belonging to several local communities are split into several nodes each belonging to one community."
p17-1145,2017,7 Conclusion,"we apply this meta clustering algorithm to the task of synset induction on two languages, obtaining the best results on three datasets and competitive results on one dataset in terms of f-score as compared to five state-of-the-art graph clustering methods."
p17-1145,2017,7 Conclusion,we presented a new robust approach to fuzzy graph clustering that relies on hard graph clustering.
p17-1146,2017,7 Conclusion,"because our neural models are applicable to srl, applying our models for multilingual srl tasks presents an interesting future research direction."
p17-1146,2017,7 Conclusion,"in addition, in this work, the model parameters were learned without any external resources."
p17-1146,2017,7 Conclusion,"in future work, we plan to explore effective methods for exploiting large-scale unlabeled data to learn the neural models."
p17-1146,2017,7 Conclusion,"in particular, our multi-sequence model improved the performance of zero argument identification, one of the problematic issues facing japanese pas analysis, by considering the multi-predicate interactions with grid-rnns."
p17-1146,2017,7 Conclusion,"in this work, we introduced neural sequence models that automatically induce effective feature representations from the word sequence information of a sentence for japanese pas analysis."
p17-1146,2017,7 Conclusion,the experiments on the naist text corpus demonstrated that the models realize high performance without the need for syntactic information.
p17-1147,2017,8 Conclusion and Future Work,results from current state-of-the-art baselines indicate that triviaqa is a challenging testbed that deserves significant future study.
p17-1147,2017,8 Conclusion and Future Work,the evidence documents come from two domains 鈥 web search results and wikipedia pages 鈥 with highly differing levels of information redundancy.
p17-1147,2017,8 Conclusion and Future Work,"to our knowledge, triviaqa is the first dataset where questions are authored by trivia enthusiasts, independently of the evidence documents."
p17-1147,2017,8 Conclusion and Future Work,"we present triviaqa, a new dataset of 650k question-document-evidence triples."
p17-1147,2017,8 Conclusion and Future Work,"while not the focus of this paper, triviaqa also provides a provides a benchmark for a variety of other tasks such as ir-style question answering, qa over structured kbs and joint modeling of kbs and text, with much more data than previously available."
p17-1148,2017,7 Future Work,"first, for benchmarking semantic parsing models on the task of semantic translation."
p17-1148,2017,7 Future Work,"other document-level features, such as example input-output pairs, unit tests, might be useful in this endeavor."
p17-1148,2017,7 Future Work,we also see these resources as useful for investigations into natural language programming.
p17-1148,2017,7 Future Work,"we believe that good performance on our datasets should lead to better performance on more conventional semantic parsing tasks, and raise new challenges involving sparsity and multilingual learning."
p17-1148,2017,7 Future Work,we see two possible use cases for this data.
p17-1148,2017,7 Future Work,"while our experiments look at learning rudimentary translational correspondences between text and code, a next step might be learning to synthesize executable programs via these translations, along the lines of (desai et al., 2016; raza et al., 2015)."
p17-1148,2017,7 Future Work,"while there has been a trend towards learning executable semantic parsers (berant et al., 2013; liang, 2016), there has also been renewed interest in supervised learning of formal representations in the context of neural semantic parsing models (dong and lapata, 2016; jia and liang, 2016)."
p17-1149,2017,7 Conclusions and Future Work,"in the future, we will improve the scalability of our model and learn multi-prototype embeddings for the mentions without reference entities in a knowledge base, and introduce compositional approaches to model the internal structures of multiword mentions."
p17-1149,2017,7 Conclusions and Future Work,"in this paper, we propose a novel multi-prototype mention embedding model that jointly learns word, entity and mention sense embeddings."
p17-1149,2017,7 Conclusions and Future Work,"these mention senses capture both textual context information and knowledge from reference entities, and provide an efficient approach to disambiguate mention sense in text."
p17-1149,2017,7 Conclusions and Future Work,"using entity linking as a study case, we apply our disambiguation method as well as the multi-prototype mention embeddings on the benchmark dataset, and achieve the state-of-the-art."
p17-1149,2017,7 Conclusions and Future Work,we conduct a series of experiments to demonstrate that multiprototype mention embedding improves the quality of both word and entity representations.
p17-1150,2017,6 Conclusion,another limitation is that the current utility function is learned based on a set of pre-identified features.
p17-1150,2017,6 Conclusion,"as cognitive robots start to enter our daily lives, data-driven approaches to learning may not be possible in new situations."
p17-1150,2017,6 Conclusion,"as in previous works, we assume the world can be described by a closed set of predicates."
p17-1150,2017,6 Conclusion,"as shown in previous work (mourao et al.藴 , 2012), learning action models from the noisy and incomplete observation of the world is extremely challenging."
p17-1150,2017,6 Conclusion,"due to the limitations in their external sensors, their representations of the shared environment can be error prone and full of uncertainties."
p17-1150,2017,6 Conclusion,future work can explore deep neural network to alleviate feature engineering.
p17-1150,2017,6 Conclusion,human partners who work side-by-side with these cognitive robots are great resources that the robots can directly learn from.
p17-1150,2017,6 Conclusion,one of the important questions to address in the future is how to learn new predicates through interaction with humans.
p17-1150,2017,6 Conclusion,our empirical results have shown a significant improvement in model acquisition and action prediction.
p17-1150,2017,6 Conclusion,our future work will incorporate interactive learning of verb semantics with task learning to enable autonomy that can learn by communicating with humans.
p17-1150,2017,6 Conclusion,"recent years have seen an increasing amount of work on task learning from human partners (saunders et al., 2006; chernova and veloso, 2008; cantrell et al., 2012; mohan et al., 2013; asada et al., 2009; mohseni-kabir et al., 2015; nejati et al., 2006; liu et al., 2016)."
p17-1150,2017,6 Conclusion,robots live in a noisy environment.
p17-1150,2017,6 Conclusion,the current investigation also has several limitations.
p17-1150,2017,6 Conclusion,the interaction strategies are learned through reinforcement learning.
p17-1150,2017,6 Conclusion,the same problem applies to the acquisition of verb semantics that are grounded to the perceived world.
p17-1150,2017,6 Conclusion,this causes significant simplification for the physical world.
p17-1150,2017,6 Conclusion,"to address this problem, this paper presents an interactive learning approach which aims to handle uncertainties of the environment as well as incompleteness and conflicts in state representation by asking human partners intelligent questions."
p17-1150,2017,6 Conclusion,"when applying the learned models in new situations, the models acquired through interactive learning leads to over 140% performance gain in noisy environment."
p17-1151,2017,5 Discussion,"alternatively, we could imagine a dependent mixture model where the distributions over words are evolving with time and other covariates."
p17-1151,2017,5 Discussion,"as part of this effort, we can explore different metrics between distributions, such as kl divergences, which would be a natural choice for order embeddings that model entailment properties."
p17-1151,2017,5 Discussion,"elsewhere, latent probabilistic representations are proving to be exceptionally valuable, able to capture nuances such as face angles with variational autoencoders (kingma and welling, 2013) or subtleties in painting strokes with the infogan (chen et al., 2016)."
p17-1151,2017,5 Discussion,"in the future, multimodal word distributions could open the doors to a new suite of applications in language modelling, where whole word distributions are used as inputs to new probabilistic lstms, or in decision functions where uncertainty matters."
p17-1151,2017,5 Discussion,it would also be informative to explore inference over the number of components in mixture models for word distributions.
p17-1151,2017,5 Discussion,"moreover, classically deterministic deep learning architectures are now being generalized to probabilistic deep models, for full predictive distributions instead of point estimates, and significantly more expressive representations (wilson et al., 2016b,a; al-shedivat et al., 2016; gan et al., 2016; fortunato et al., 2017)."
p17-1151,2017,5 Discussion,"multimodal word distributions naturally represent our belief that words do not have single precise meanings: indeed, the shape of a word distribution can express much more semantic information than any point representation."
p17-1151,2017,5 Discussion,"one could also build new types of supervised language models, constructed to more fully leverage the rich information provided by word distributions."
p17-1151,2017,5 Discussion,"similarly, probabilistic word embeddings can capture a range of subtle meanings, and advance the state of the art in predictive tasks."
p17-1151,2017,5 Discussion,"such an approach could potentially discover an unbounded number of distinct meanings for words, but also distribute the support of each word distribution to express highly nuanced meanings."
p17-1151,2017,5 Discussion,"the resulting embeddings capture different semantics of polysemous words, uncertainty, and entailment, and also perform favorably on word similarity benchmarks."
p17-1151,2017,5 Discussion,"to learn the properties of each mixture, we proposed an analytic energy function for combination with a maximum margin objective."
p17-1151,2017,5 Discussion,we introduced a model that represents words with expressive multimodal distributions formed from gaussian mixtures.
p17-1152,2017,6 Conclusions and Future Work,"based on this, we further show that by explicitly considering recursive architectures in both local inference modeling and inference composition, we achieve additional improvement."
p17-1152,2017,6 Conclusions and Future Work,"future work interesting to us includes exploring the usefulness of external resources such as wordnet and contrasting-meaning embedding (chen et al., 2015) to help increase the coverage of wordlevel inference relations."
p17-1152,2017,6 Conclusions and Future Work,"modeling negation more closely within neural network frameworks (socher et al., 2013; zhu et al., 2014) may help contradiction detection."
p17-1152,2017,6 Conclusions and Future Work,"particularly, incorporating syntactic parsing information contributes to our best result: it further improves the performance even when added to the already very strong model."
p17-1152,2017,6 Conclusions and Future Work,"the results are first achieved through our enhanced sequential inference model, which outperformed the previous models, including those employing more complicated network architectures, suggesting that the potential of sequential inference models have not been fully exploited yet."
p17-1152,2017,6 Conclusions and Future Work,"we propose neural network models for natural language inference, which achieve the best results reported on the snli benchmark."
p17-1153,2017,6 Conclusion,"across several races, female actors are found to be younger than male actors on average."
p17-1153,2017,6 Conclusion,"as characters aged, their word sophistication seems to increase along with usage of words related to achievement and religion; there was also a significant reduction in word activation, usage of sexual and swear words as character age increases."
p17-1153,2017,6 Conclusion,eastasian characters seem to use significantly fewer religious words.
p17-1153,2017,6 Conclusion,female characters also appear to be more prominent in horror movies compared to male characters.
p17-1153,2017,6 Conclusion,female characters appear to be more positive in language use with fewer references to death and fewer swear words compared to male characters.
p17-1153,2017,6 Conclusion,future work includes expanding the analyses to non-english movies and combining the linguistic metrics with character networks.
p17-1153,2017,6 Conclusion,"in particular, movies with female writers and directors in the production team are observed to have balanced gender ratios in characters compared to male writers/directors."
p17-1153,2017,6 Conclusion,latino and mixed race characters appear to have higher usage of sexual words.
p17-1153,2017,6 Conclusion,several interesting patterns are revealed in the analysis.
p17-1153,2017,6 Conclusion,"specifically, character network edges can be weighted using the psycholinguistic metrics to analyze the emotional patterns in inter-character interactions."
p17-1153,2017,6 Conclusion,"we present a scalable automated analyses of differences in character portrayal along multiple factors such as gender, race and age using word usage, psycholinguistic and graph theoretic measures."
p17-1154,2017,6 Conclusion and Future Work,"as future work, we plan to apply the linguistic regularizers to tree-lstm to address the scope issue since the parsing tree is easier to indicate the modification scope explicitly."
p17-1154,2017,6 Conclusion and Future Work,"furthermore, our models are sequence lstms which do not depend on a parsing tree-structure and do not require expensive phrase-level annotation."
p17-1154,2017,6 Conclusion and Future Work,"results show that our models are able to address the linguistic role of sentiment, negation, and intensity words."
p17-1154,2017,6 Conclusion and Future Work,"the proposed models address the sentient shifting effect of sentiment, negation, and intensity words."
p17-1154,2017,6 Conclusion and Future Work,"to preserve the simplicity of the proposed models, we do not consider the modification scope of negation and intensity words, though we partially address this issue by applying a minimization operartor (see eq.11, eq.14) and bi-directional lstm."
p17-1154,2017,6 Conclusion and Future Work,we present linguistically regularized lstms for sentence-level sentiment classification.
p17-1155,2017,8 Discussion and Future Work,"another set of examples consists of tweets that lack an explicit sentiment word, for example, the tweet 鈥滳lear example they made of sharapova then, ey?#sarcasm鈥."
p17-1155,2017,8 Discussion and Future Work,automatic vs. human measures the performance gap between moses and sign may stem from the difference in their optimization criteria.
p17-1155,2017,8 Discussion and Future Work,"consequently, we do not observe substantial differences between the algorithms in the automatic measures that are mostly based on ngram differences between the source and the interpretation."
p17-1155,2017,8 Discussion and Future Work,consider the tweet 鈥滳an you imagine if lebron had help?#sarcasm鈥.
p17-1155,2017,8 Discussion and Future Work,"consider, for example, the following two cases: (a) the sarcastic tweet 鈥滳an鈥檛 wait until tomorrow #sarcasm鈥, where the positive sentiment is expressed in the phrase can鈥檛 wait; and (b) the sarcastic tweet 鈥漚nother shooting?yeah we totally need to make guns easier for people to get #sarcasm鈥, where the word totally receives a strong sentiment despite its normal use in language."
p17-1155,2017,8 Discussion and Future Work,designing automatic measures is hence left for future research.
p17-1155,2017,8 Discussion and Future Work,"even when there are no words of strong sentiment, the mt component of sign still performs well, interpreting tweets such as 鈥漷he cavs aren鈥檛 getting any calls, this is new #sarcasm鈥 into 鈥漷he cavs aren鈥檛 getting any calls, as usuall鈥."
p17-1155,2017,8 Discussion and Future Work,"finally, tweets that present sentiment in phrases or slang words are particularly challenging for our approach which relies on the identification and clustering of sentiment words."
p17-1155,2017,8 Discussion and Future Work,"for example, for the sarcastic tweet 鈥滳onstantly being irritated, anxious and depressed is a great feeling!#sarcasm鈥, sign-context produces the adequate interpretation: 鈥滳onstantly being irritated, anxious and depressed is a terrible feeling鈥."
p17-1155,2017,8 Discussion and Future Work,future research directions rise from cases in which the sign models left the tweet unchanged.
p17-1155,2017,8 Discussion and Future Work,in practice the sign models leave this tweet untouched.
p17-1155,2017,8 Discussion and Future Work,"likewise, the human fluency measure that accounts for the readability of the interpretation is not seriously affected by the translation process."
p17-1155,2017,8 Discussion and Future Work,"moreover, for fluency the correlation values are insignificant (using a correlation significance t-test with p = 0.05)."
p17-1155,2017,8 Discussion and Future Work,"moses aims to optimize the bleu score and given the overall lexical similarity between the original tweets and their interpretations, it therefore tends to keep them identical."
p17-1155,2017,8 Discussion and Future Work,one prominent set of examples consists of tweets that require world knowledge for correct interpretation.
p17-1155,2017,8 Discussion and Future Work,"our major contributions are: 鈥 construction of a dataset, first of its kind, that consists of 3000 tweets each augmented with five non-sarcastic interpretations generated by human experts.鈥 discussion of the proper evaluation in our task."
p17-1155,2017,8 Discussion and Future Work,"sarcasm interpretation as sentiment based monolingual mt: strengths and weaknesses the sign models鈥 strength is revealed when interpreting sarcastic tweets with strong sentiment words, transforming expressions such as 鈥滱udits are a blast to do #sarcasm鈥 and 鈥滲eing stuck in an airport is fun #sarcasm鈥 into 鈥滱udits are a bummer to do鈥 and 鈥滲eing stuck in an airport is boring鈥, respectively."
p17-1155,2017,8 Discussion and Future Work,several challenges are still to be addressed in future research so that sarcasm interpretation can be performed in a fully automatic manner.
p17-1155,2017,8 Discussion and Future Work,"sign, in contrast, targets sentiment words and changes them frequently."
p17-1155,2017,8 Discussion and Future Work,summary we presented a first attempt to approach the problem of sarcasm interpretation.
p17-1155,2017,8 Discussion and Future Work,the model requires knowledge of who lebron is and what kind of help he needs in order to fully understand and interpret the sarcasm.
p17-1155,2017,8 Discussion and Future Work,the sign models perform well even in cases where there are several sentiment words but not all of them require change.
p17-1155,2017,8 Discussion and Future Work,these include the design of appropriate automatic evaluation measures as well as improving the algorithmic approach so that it can take world knowledge into account and deal with cases where the sentiment of the input tweet is not expressed with a clear sentiment words.
p17-1155,2017,8 Discussion and Future Work,"to further understand the relationship between the automatic and the human based measures we computed the pearson correlations for each pair of (automatic, human) measures."
p17-1155,2017,8 Discussion and Future Work,"we are releasing our dataset with its sarcasm interpretation guidelines, the code of the sign algorithms, and the output of the various algorithms considered in this paper (https://github.com/lotemp/sarcasmsign)."
p17-1155,2017,8 Discussion and Future Work,we believe this indicates that these automatic measures do not provide appropriate evaluation for our task.
p17-1155,2017,8 Discussion and Future Work,we demonstrated the strength of our approach and pointed on cases that are currently beyond its reach.
p17-1155,2017,8 Discussion and Future Work,we hope this new resource will help researchers make further progress on this new task.
p17-1155,2017,8 Discussion and Future Work,"we observe that all correlation values are low (up to 0.12 for fluency, 0.13-0.18 for sentiment and 0.19-0.24 for adequacy)."
p17-1155,2017,8 Discussion and Future Work,we proposed a battery of human measures and compared their performance to the accepted measures in related fields such as machine translation.鈥 an algorithmic approach: sentiment based monolingual machine translation.
p17-1155,2017,8 Discussion and Future Work,"when it comes to the human adequacy and sentiment measures, which account for the understanding of the tweet鈥檚 meaning, sign reveals its power and demonstrates much better performance compared to moses."
p17-1155,2017,8 Discussion and Future Work,"while for a human reader it is apparent that the author means a clear example was not made of sharapova, the lack of strong sentiment words results in all sign models leaving this tweet uninterpreted."
p17-1155,2017,8 Discussion and Future Work,"while we believe that identifying the role of can鈥檛 wait and of totally in the sentiment of the above tweets can be a key to properly interpreting them, our approach that relies on a sentiment word lexicon is challenged by such cases."
p17-1156,2017,5 Conclusion,both classification uncertainty and density are considered when selecting informative samples to label.
p17-1156,2017,5 Conclusion,experimental results on benchmark datasets show that our approach can train accurate sentiment classifier and at same time reduce the manual annotation effort.
p17-1156,2017,5 Conclusion,"in addition, we extract domain-specific sentiment similarities among words from unlabeled samples of target domain based on both syntactic rules and cooccurrence patterns, and incorporate them into the domain adaptation process to propagate the general sentiment information to many domain-specific sentiment words in target domain."
p17-1156,2017,5 Conclusion,"in our approach, the general sentiment information in sentiment lexicons is adapted to target domain with the help of a small number of labeled samples which are selected and annotated in an active learning mode."
p17-1156,2017,5 Conclusion,in this paper we present an active sentiment domain adaptation approach to train accurate sentiment classifier for target domain with less labeled samples.
p17-1156,2017,5 Conclusion,we also propose a unified model to incorporate different types of sentiment information to train sentiment classifier for target domain.
p17-1157,2017,7 Conclusion,"additionally, we explored fusion methods to combine the text features with factual market features, achieved from historical prices i.e."
p17-1157,2017,7 Conclusion,"however, despite expectations, training different models on different sectors does not improve performance compared to the general model."
p17-1157,2017,7 Conclusion,"in addition, we studied the characteristics of each individual sector with regard to risk-sensitive terms."
p17-1157,2017,7 Conclusion,"in both cases, our approach outperforms state-ofthe-art volatility prediction methods with 10-k reports and demonstrates the effectiveness of sentiment analysis in long-term volatility forecasting."
p17-1157,2017,7 Conclusion,"in this work, we studied the sentiment of recent 10-k annual disclosures of companies in stock markets for forecasting volatility."
p17-1157,2017,7 Conclusion,our analysis shows that reports in same sectors considerably share particular risk and instability factors.
p17-1157,2017,7 Conclusion,our bag-ofwords sentiment analysis approach benefits from state-of-the-art models in information retrieval which use word embeddings to extend the weight of the terms to the similar terms in the document.
p17-1157,2017,7 Conclusion,"we traced this to the size of the available data in each sector, and show that there are still benefits in considering sectors, which could be further explored in the future as more data becomes available."
p17-1158,2017,6 Conclusion and Future Work,"besides, the learnt context-aware embeddings can compose high-quality context-free embeddings."
p17-1158,2017,6 Conclusion and Future Work,experimental results on link prediction demonstrate that cane is effective for modeling the relationship between vertices.
p17-1158,2017,6 Conclusion and Future Work,"furthermore, there usually exist explicit relations in social networks (e.g., families, friends and colleagues relations between social network users), which are expected to be critical to ne."
p17-1158,2017,6 Conclusion and Future Work,"in future, we will strive to implement cane on a wider variety of information networks with multi-modal data, such as labels, images and so on.(2) cane encodes latent relations between vertices into their context-aware embeddings."
p17-1158,2017,6 Conclusion and Future Work,"in this paper, we propose the concept of contextaware network embedding (cane) for the first time, which aims to learn various context-aware embeddings for a vertex according to the neighbors it interacts with."
p17-1158,2017,6 Conclusion and Future Work,"specifically, we implement cane on text-based information networks with proposed mutual attention mechanism, and conduct experiments on several real-world information networks."
p17-1158,2017,6 Conclusion and Future Work,"thus, we want to explore how to incorporate and predict these explicit relations between vertices in ne."
p17-1158,2017,6 Conclusion and Future Work,we will explore the following directions in future: (1) we have investigated the effectiveness of cane on text-based information networks.
p17-1159,2017,7 Conclusion,possible future work include expanding the investigation to other regional languages such as malay and indonesian.
p17-1159,2017,7 Conclusion,"we demonstrate the effectiveness of using neural stacking for feature transfer by boosting the singlish dependency parsing performance to from uas 79.29% to uas 84.47%, with a 25.01% relative error reduction over the parser with all available singlish resources."
p17-1159,2017,7 Conclusion,"we have investigated dependency parsing for singlish, an important english-based creole language, through annotations of a singlish dependency treebank with 10,986 words and building an enhanced parser by leveraging on knowledge transferred from a 20-times-bigger english treebank of universal dependencies."
p17-1159,2017,7 Conclusion,"we release the annotated singlish dependency treebank, the trained model and the source code for the parser with free public access."
p17-1160,2017,Future work,an interesting avenue for future work is to explore higher order factorizations for noncrossing digraphs and the related inference.
p17-1160,2017,Future work,"properties such as weakly projective, out, and strongly unambiguous prompt further study."
p17-1160,2017,Future work,"we are planning to extend the coverage of the approach by exploring 1-endpointcrossing and mhk trees (pitler et al., 2013; gomez-rodr 麓 麓谋guez, 2016), and related digraphs 鈥 see (yli-jyra篓, 2004; gomez-rodr 麓 麓谋guez et al., 2011)."
p17-1160,2017,Future work,we would also like to have more insight on the transformation of mso definable properties to the current framework and to logspace algorithms.
p17-1161,2017,5 Conclusion,"in this paper, we proposed a simple and general semi-supervised method using pre-trained neural language models to augment token representations in sequence tagging models."
p17-1161,2017,5 Conclusion,our analysis shows that adding a backward lm in addition to traditional forward lms consistently improves performance.
p17-1161,2017,5 Conclusion,our method significantly outperforms current state of the art models in two popular datasets for ner and chunking.
p17-1161,2017,5 Conclusion,"the proposed method is robust even when the lm is trained on unlabeled data from a different domain, or when the baseline model is trained on a large number of labeled examples."
p17-1163,2017,7 Conclusion,"finally, we have shown that the performance of nbt models improves with the semantic quality of the underlying word vectors."
p17-1163,2017,7 Conclusion,"in future work, we intend to explore applications of the nbt for multi-domain dialogue systems, as well as in languages other than english that require handling of complex morphological variation."
p17-1163,2017,7 Conclusion,"in this paper, we have proposed a novel neural belief tracking (nbt) framework designed to overcome current obstacles to deploying dialogue systems in real-world dialogue domains."
p17-1163,2017,7 Conclusion,our evaluation demonstrated these benefits: the nbt models match the performance of models which make use of such lexicons and vastly outperform them when these are not available.
p17-1163,2017,7 Conclusion,"the nbt models offer the known advantages of coupling spoken language understanding and dialogue state tracking, without relying on hand-crafted semantic lexicons to achieve state-of-the-art performance."
p17-1163,2017,7 Conclusion,"to the best of our knowledge, we are the first to move past intrinsic evaluation and show that semantic specialisation boosts performance in downstream tasks."
p17-1164,2017,6 Conclusions,"besides, we also investigate two strategies to construct gold attentions using the annotated arguments."
p17-1164,2017,6 Conclusions,"experimental results show that our approach outperforms state-of-the-art methods, which demonstrates that the proposed approach is effective for event detection."
p17-1164,2017,6 Conclusions,"in this work, we propose a novel approach to model argument information explicitly for ed via supervised attention mechanisms."
p17-1164,2017,6 Conclusions,"moreover, we also use events from fn to augment the performance of the proposed approach."
p17-1164,2017,6 Conclusions,"to demonstrate the effectiveness of the proposed method, we systematically conduc t a series of experiments on the widely used benchmark dataset ace 2005."
p17-1165,2017,5 Discussion,"as regards complexity, it is true that more complex models, as the one we are considering, are more prone to underfitting (when data is scarce) and overfitting than simpler models."
p17-1165,2017,5 Discussion,"by contrast, our unsupervised approach learns both segmentations and topics jointly in a domain and language independent manner."
p17-1165,2017,5 Discussion,"furthermore, existing supervised segmentation models are largely designed for a very different purpose with strong linguistic motivations, which may not align well with our main goal in this paper which is improving topic coherence in topic modeling."
p17-1165,2017,5 Discussion,"furthermore, the parameters p and f are simple parameters to choose between these two distributions."
p17-1165,2017,5 Discussion,"in addition, this model relies on both document and segment specific topic distributions so as to capture fine grained differences in topic assignments."
p17-1165,2017,5 Discussion,"in contrast, our approach aims at identifying fine-grained topics associated with coherent segments that do not overlap sentence boundaries."
p17-1165,2017,5 Discussion,"in the future, we plan on relying on other inference approaches, based for example on variational bayes known to yield better estimates for perplexity (asuncion et al., 2009); it is however not certain that the gain in perplexity one can expect from the use of variational bayes approaches will necessarily result in a gain in, say, topic coherence."
p17-1165,2017,5 Discussion,"in this paper, we have introduced an lda-based model that generates topically coherent segments within documents by jointly segmenting documents and assigning topics to their words."
p17-1165,2017,5 Discussion,"indeed, the impact of the inference approach on the different usages of latent topic models for text collections remains to be better understood."
p17-1165,2017,5 Discussion,our results confirm the importance of a flexible segmentation as well as a binding mechanism to produce topically coherent segments.
p17-1165,2017,5 Discussion,"similarly, unsupervised approaches, used for example in the tdt (topic detection and tracking) campaigns or more recently in du et al.(2013), usually consider coarse-grained topics, that can encompass several sentences."
p17-1165,2017,5 Discussion,"the approach requires external annotated data to train the segmentation models, where certain domain and language specific information need to be captured."
p17-1165,2017,5 Discussion,"the coherence between topics is ensured through frank鈥檚 copula, that binds the topics associated to the words of a segment."
p17-1165,2017,5 Discussion,the comparison with other segmentation methods is also an important point.
p17-1165,2017,5 Discussion,"these considerations, explain the choice of the baselines retained: they are based on segments of different granularities (words, nps, sentences) that do not overlap sentence boundaries."
p17-1165,2017,5 Discussion,"this said, the experimental results on perplexity (in which the word-topic distributions are fixed) and on classification (based on the topical induced representations) suggest that our model neither underfits nor overfits compared to simpler models."
p17-1165,2017,5 Discussion,we believe that this is due to the fact that the main additional parameters in our model (the segment specific topic distribution) do not really add complexity as they are drawn from the same distribution as the standard document specific topics.
p17-1165,2017,5 Discussion,"we have shown that this model naturally encompasses other state-of-the-art lda-based models proposed to accomplish the same task, and that it outperforms these models over six publicly available collections in terms of perplexity, normalized pointwise mutual information (npmi), a measure used to assess the coherence of topics with documents, and the micro f1-measure in a text classification context."
p17-1165,2017,5 Discussion,"while state-of-theart supervised segmentation models can be used before applying the lda model, we note such a pipeline approach comes with several limitations."
p17-1166,2017,5 Conclusion and Future Works,an effective method is proposed to relieve the impact of nr for model training.
p17-1166,2017,5 Conclusion and Future Works,experimental results on a widely used dataset show that leveraging class ties will enhance relation extraction and our model is effective to learn class ties.
p17-1166,2017,5 Conclusion and Future Works,"in the future, we will focus on two aspects: (1) our method in this paper considers pairwise intersections between labels, so to better exploit class ties, we will extend our method to exploit all other labels鈥 influences on each relation for relation extraction, transferring second-order to high-order (zhang and zhou, 2014); (2) we will focus on other problems by leveraging class ties between labels, specially on multi-label learning problems (zhou et al., 2012) such as multi-category text categorization (rousu et al., 2005) and multi-label image categorization (zha et al., 2008)."
p17-1166,2017,5 Conclusion and Future Works,"in this paper, we leverage class ties to enhance relation extraction by joint extraction using pairwise ranking combined with cnn."
p17-1166,2017,5 Conclusion and Future Works,our method significantly outperforms the baselines.
p17-1167,2017,6 Conclusion & Future Work,"because of the dynamic nature of our framework, it is not trivial to leverage the computational capabilities of gpus using minibatched training; we plan to investigate ways to take full advantage of modern computing machinery in the near future."
p17-1167,2017,6 Conclusion & Future Work,"by formulating semantic parsing as a state鈥揳ction search problem, our method learns modular neural network models through reward-guided search."
p17-1167,2017,6 Conclusion & Future Work,"dynsp outperforms existing state-of-the-art systems designed for answering complex questions when applied to sqa, and increases the gain after incorporating the subsequent actions."
p17-1167,2017,6 Conclusion & Future Work,"finally, better resolution of semantic matching errors is a top priority, and unsupervised learning from large external corpora is one way to make progress in this direction."
p17-1167,2017,6 Conclusion & Future Work,"for instance, although our current formal language design covers most question types in sqa, it is nevertheless important to extend it further to make the semantic parser more robust (e.g., by including union or allowing comparison of multiple previous answers)."
p17-1167,2017,6 Conclusion & Future Work,"in the future, we plan to investigate several interesting research questions triggered by this work."
p17-1167,2017,6 Conclusion & Future Work,"in this work we move towards a conversational, multi-turn qa scenario in which systems must rely on prior context to answer the user鈥檚 current question."
p17-1167,2017,6 Conclusion & Future Work,"practically, allowing a more complicated semantic parse structure鈥攅ither by increasing the number of primitive statements or the length of the parse鈥攑oses serious computational challenges in both model learning and inference."
p17-1167,2017,6 Conclusion & Future Work,"to the best of our knowledge, sqa is the first semantic parsing dataset that addresses sequential question answering."
p17-1167,2017,6 Conclusion & Future Work,"to this end, we introduce sqa, a dataset that consists of 6,066 unique sequences of inter-related questions about wikipedia tables, with 17,553 questions-answer pairs in total."
p17-1167,2017,6 Conclusion & Future Work,"we propose dynsp, a dynamic neural semantic parsing framework, for solving sqa."
p17-1168,2017,5 Conclusion,analysis of document and query attentions in intermediate layers of the reader further reveals that the model iteratively attends to different aspects of the query to arrive at the final answer.
p17-1168,2017,5 Conclusion,"in this paper we have focused on text comprehension, but we believe that the gated-attention mechanism may benefit other tasks as well where multiple sources of information interact"
p17-1168,2017,5 Conclusion,our model achieves the state-of-theart performance on several large-scale benchmark datasets with more than 4% improvements over competitive baselines.
p17-1168,2017,5 Conclusion,our model design is backed up by an ablation study showing statistically significant improvements of using gated attention as information filters.
p17-1168,2017,5 Conclusion,"the ga reader features a novel multiplicative gating mechanism, combined with a multi-hop architecture."
p17-1168,2017,5 Conclusion,"we also showed empirically that multiplicative gating is superior to addition and concatenation operations for implementing gated-attentions, though a theoretical justification remains part of future research goals."
p17-1168,2017,5 Conclusion,we presented the gated-attention reader for answering cloze-style questions over documents.
p17-1169,2017,6 Conclusions and Future Work,"a more appealing direction is to develop problem-driven methods to represent a document as a distributional entity, taking into consideration of phrases, sentence structures, and syntactical characteristics."
p17-1169,2017,6 Conclusions and Future Work,"finally, the gains acquired from word embeddings are quantitatively measured from a nonparametric unsupervised perspective."
p17-1169,2017,6 Conclusions and Future Work,it would also be interesting to investigate several possible extensions to the current clustering work.
p17-1169,2017,6 Conclusions and Future Work,"its computational tractability, robustness and supreme performance, as a fundamental tool, are empirically validated."
p17-1169,2017,6 Conclusions and Future Work,its ease of use enables data scientists to apply it for the pre-screening purpose of examining word embeddings in a specific task.
p17-1169,2017,6 Conclusions and Future Work,one direction is to learn a proper ground distance for word embeddings such that the final document clustering performance can be improved with labeled data.
p17-1169,2017,6 Conclusions and Future Work,"the work by (huang et al., 2016; cuturi and avis, 2014) have partly touched this goal with an emphasis on document proximities."
p17-1169,2017,6 Conclusions and Future Work,this paper introduces a nonparametric clustering framework for document analysis.
p17-1169,2017,6 Conclusions and Future Work,we believe the framework of wasserstein distance and d2-clustering creates room for further investigation on complex structures and knowledge carried by documents.
p17-1170,2017,7 Conclusion and Future Work,"also, given the promising results observed for supersenses, we plan to investigate taskspecific coarsening of sense inventories, particularly wikipedia, or the use of sentiwordnet (baccianella et al., 2010), which could be more suitable for polarity detection."
p17-1170,2017,7 Conclusion and Future Work,"as future work, we plan to investigate the extension of the approach to other languages and applications."
p17-1170,2017,7 Conclusion and Future Work,our pipeline is modular and can be used as an in vivo evaluation framework for wsd and sense representation techniques.
p17-1170,2017,7 Conclusion and Future Work,we hope that our work will foster future research on the integration of senselevel knowledge into downstream applications.
p17-1170,2017,7 Conclusion and Future Work,we proposed a pipeline for the integration of sense level knowledge into a state-of-the-art text classifier.
p17-1170,2017,7 Conclusion and Future Work,we release our code and data (including pre-trained sense and supersense embeddings) at https://pilehvar.github.io/ sensecnn/ to allow further checking of the choice of hyperparameters and to allow further analysis and comparison.
p17-1170,2017,7 Conclusion and Future Work,"we showed that a simple disambiguation of the input can lead to consistent performance gain, particularly for longer documents and when the granularity of the underlying sense inventory is reduced."
p17-1171,2017,6 Conclusion,evaluating the individual components as well as the full system across multiple benchmarks showed the efficacy of our approach.
p17-1171,2017,6 Conclusion,future work should aim to improve over our drqa system.
p17-1171,2017,6 Conclusion,machine comprehension systems alone cannot solve the overall task.
p17-1171,2017,6 Conclusion,"our method integrates search, distant supervision, and multitask learning to provide an effective complete system."
p17-1171,2017,6 Conclusion,our results indicate that mrs is a key challenging task for researchers to focus on.
p17-1171,2017,6 Conclusion,"two obvious angles of attack are: (i) incorporate the fact that document reader aggregates over multiple paragraphs and documents directly in the training, as it currently trains on paragraphs independently; and (ii) perform endto-end training across the document retriever and document reader pipeline, rather than independent systems."
p17-1171,2017,6 Conclusion,"we studied the task of machine reading at scale, by using wikipedia as the unique knowledge source for open-domain qa."
p17-1172,2017,5 Conclusions,"in four different tasks with six datasets (one synthetic and five real), we test the efficiency of the proposed method on various levels of text jumping, from character to word and then to sentence."
p17-1172,2017,5 Conclusions,"in particular, we propose a 鈥渏umping鈥 model that after reading every few tokens, it decides how many tokens should be skipped by sampling from a softmax."
p17-1172,2017,5 Conclusions,"in this paper, we focus on learning how to skim text for fast reading."
p17-1172,2017,5 Conclusions,"such jumping behavior is modeled as a discrete decision making process, which can be trained by reinforcement learning algorithm such as reinforce."
p17-1172,2017,5 Conclusions,"the results indicate our model is several times faster than, while the accuracy is on par with the baseline lstm model."
p17-1173,2017,8 Conclusion,"in this paper, we studied the process of feature extraction using an algebraic lens."
p17-1173,2017,8 Conclusion,we demonstrated the practical value of the refactoring algorithm by speeding up feature extraction for text chunking and relation extraction tasks.
p17-1173,2017,8 Conclusion,we exploited this characterization to develop a factorization algorithm that simplifies feature extractors to be more computationally efficient.
p17-1173,2017,8 Conclusion,we showed that the set of feature extractors form a commutative semiring over addition and conjunction.
p17-1174,2017,6 Conclusion,"as the attention mechanism in nmt plays a similar role to the translation model in phrase-based smt, our chunk-based decoders are intended to capture the notion of chunks in chunkbased (or phrase-based) smt."
p17-1174,2017,6 Conclusion,"in addition, we plan to combine our decoder with other encoders that capture language structure, such as a hierarchical rnn (luong and manning, 2016), a tree-lstm (eriguchi et al., 2016b), or an order-free encoder, such as a cnn (kalchbrenner and blunsom, 2013)."
p17-1174,2017,6 Conclusion,"in future work, we will explore the optimal structures of chunk-based decoder for other free word-order languages such as czech, german, and turkish."
p17-1174,2017,6 Conclusion,"in this paper, we propose chunk-based decoders for nmt."
p17-1174,2017,6 Conclusion,"we designed three models that have hierarchical rnnlike architectures, each of which consists of a word-level decoder and a chunk-level decoder."
p17-1174,2017,6 Conclusion,we performed experiments on the wat 鈥16 englishto-japanese translation task and found that our best model outperforms the strongest baselines by +0.93 bleu score and by +0.57 ribes score.
p17-1174,2017,6 Conclusion,we utilize the chunk structure to efficiently capture long-distance dependencies and cope with the problem of free word-order languages such as japanese.
p17-1175,2017,7 Conclusions and Future Work,"in the future, we will incorporate coverage into our model and study how to apply it to other natural language processing tasks."
p17-1175,2017,7 Conclusions and Future Work,our model also compares favourably to both nmt and pbsmt baselines evaluated on the same training data.
p17-1175,2017,7 Conclusions and Future Work,"we have also showed that our model can be efficiently pre-trained on both mediumsized back-translated in-domain multi-modal data as well as also large general-domain text-only mt corpora, finding that it is able to exploit the additional data regardless of the domain."
p17-1175,2017,7 Conclusions and Future Work,"we have introduced a novel attention-based, multi-modal nmt model to incorporate spatial visual information into nmt."
p17-1175,2017,7 Conclusions and Future Work,"we have reported state-of-the-art results on the m30kt test set, improving on previous multi-modal attention-based models."
p17-1176,2017,6 Conclusion,experiments on the europarl and wmt corpora across languages show that our proposed wordlevel sampling method can significantly outperforms the state-of-the-art pivot-based methods and multilingual methods in terms of translation quality and decoding efficiency.
p17-1176,2017,6 Conclusion,"in the future, we plan to test our approach on more diverse language pairs, e.g., zero-resource uyghur-english translation using chinese as a pivot."
p17-1176,2017,6 Conclusion,"in this paper, we propose a novel framework to train the student model without parallel corpora under the guidance of the pre-trained teacher model on a source-pivot parallel corpus."
p17-1176,2017,6 Conclusion,it is also interesting to extend the teacherstudent framework to other cross-lingual nlp applications as our method is transparent to architectures.
p17-1176,2017,6 Conclusion,the experiments on the europarl corpus show that our approach obtains an significant improvement over the pivot-based baseline.
p17-1176,2017,6 Conclusion,"we also analyze zero-resource translation with small source-pivot data, and combine our wordlevel sampling method with initialization and parameter freezing suggested by (zoph et al., 2016)."
p17-1176,2017,6 Conclusion,we introduce sentence-level and word-level teaching to guide the learning process of the student model.
p17-1177,2017,6 Conclusion,"for future work, it will be interesting to make use of node labels from the tree, or to use syntactic information on the target side, as well."
p17-1177,2017,6 Conclusion,our analysis suggests that the benefit of source-side syntax is especially strong for long sentences.
p17-1177,2017,6 Conclusion,"our current work only uses the structure part of the syntactic tree, without the labels."
p17-1177,2017,6 Conclusion,"our experiments have demonstrated that a top-down encoder is a useful enhancement for the original bottom-up tree encoder (eriguchi et al., 2016); and incorporating syntactic structure information into the decoder can better control the translation."
p17-1177,2017,6 Conclusion,we have investigated the potential of using explicit source-side syntactic trees in nmt by proposing a novel syntax-aware encoder-decoder model.
p17-1178,2017,6 Conclusions and Future Work,"in the future, we will explore the topological structure of related languages and exploit cross-lingual knowledge transfer to enhance the quality of extraction and linking."
p17-1178,2017,6 Conclusions and Future Work,"in this work, we treat all languages independently when training their corresponding name taggers."
p17-1178,2017,6 Conclusions and Future Work,the general idea of deriving noisy annotations from kb properties can also be extended to other ie tasks such as relation extraction.
p17-1178,2017,6 Conclusions and Future Work,"this framework follows a fully automatic training and testing pipeline, without the needs of any manual annotations or knowledge from native speakers."
p17-1178,2017,6 Conclusions and Future Work,"to the best of our knowledge, our multilingual name tagging and linking framework is applied to the largest number of languages."
p17-1178,2017,6 Conclusions and Future Work,we developed a simple yet effective framework that can extract names from 282 languages and link them to an english kb.
p17-1178,2017,6 Conclusions and Future Work,we evaluated our framework on both wikipedia articles and external formal and informal texts and obtained promising results.
p17-1178,2017,6 Conclusions and Future Work,"we release the following resources for each of these 282 languages: 鈥渟ilver-standard鈥 name tagging and linking annotations with multiple levels of granularity, morphology analyzer if it鈥檚 a morphologically-rich language, and an endto-end name tagging and linking system."
p17-1179,2017,6 Conclusion,"future work also includes investigating other divergences that adversarial training can minimize (nowozin et al., 2016), and broader mathematical tools that match distributions (mohamed and lakshminarayanan, 2016)."
p17-1179,2017,6 Conclusion,"in this work, we demonstrate the feasibility of connecting word embeddings of different languages without any cross-lingual signal."
p17-1179,2017,6 Conclusion,our work also opens up opportunities for the processing of extremely low-resource languages and domains that lack parallel data completely.
p17-1179,2017,6 Conclusion,our work is likely to benefit from advances in techniques that further stabilize adversarial training.
p17-1179,2017,6 Conclusion,the success of our approach signifies the existence of universal lexical semantic structure across languages.
p17-1179,2017,6 Conclusion,this is achieved by matching the distributions of the transformed source language embeddings and target ones via adversarial training.
p17-1180,2017,8 Conclusion and Future Work,"all the trends indicate a global dominance of english, which might be because twitter is primarily a medium for broadcast, and english tweets have a wider audience."
p17-1180,2017,8 Conclusion and Future Work,"as future directions, we plan to extend gwld to several other languages and conduct similar sociolinguistic studies on cs patterns including not only more languages and geographies, but also other aspects like topic and sentiment."
p17-1180,2017,8 Conclusion and Future Work,"bergsma et al.(2012) show that 鈥淸on twitter] bilinguals bridge between monolinguals with english as a hub, while monolinguals tend not to directly follow each other.鈥 androutsopoulos (2006) argues that due to linguistic non-homogenity of online public spaces, languages like en, fr and de are typically preferred for communication, even though in private spaces, 鈥漛ilingual talk鈥 differs considerably in terms of distribution and cs patterns."
p17-1180,2017,8 Conclusion and Future Work,"on the other hand, while a third of the population of houston speaks spanish and almost everybody english, only 1% of the tweets from the city are code-switched."
p17-1180,2017,8 Conclusion and Future Work,"one of the primary observations of this study is that while code-switching on twitter is common worldwide (3.5%), it is much more common in non-english speaking cities like istanbul (12%) where 90% of the population speak turkish."
p17-1180,2017,8 Conclusion and Future Work,"our results on monolingual and code-switched tweets in seven latin script languages show a high 0.963 accuracy, significantly out-performing existing systems."
p17-1180,2017,8 Conclusion and Future Work,"using gwld, we conducted a large-scale study of cs trends among these languages, both globally and in specific cities."
p17-1180,2017,8 Conclusion and Future Work,"we present gwld, a system for word-level language detection for an arbitrarily large set of languages that is completely unsupervised."
p17-1181,2017,5 Conclusions,cognates detection is an interesting and challenging task.
p17-1181,2017,5 Conclusions,"experimental results reveal that the new methods significantly improve state of the art performance in multiple cognates detection experiments conducted on standard freely and publicly available datasets with different language pairs and various conditions such as different levels of baseline performance and different data size conditions, including with more realistic large data size conditions than have been evaluated with in the past."
p17-1181,2017,5 Conclusions,"however, when assigning a score to a word pair, the current state of the art methods do not take into account scores assigned to other word pairs."
p17-1181,2017,5 Conclusions,"previous work has yielded state of the art approaches that create a matrix of scores for all word pairs based on optimized weighted combinations of component scores computed on the basis of various helpful sources of information such as phonetic information, word context information, temporal context information, word frequency information, and word burstiness information."
p17-1181,2017,5 Conclusions,"the methods presented in this paper are complementary to existing state of the art methods, easy to implement, computationally efficient, and practically effective in improving performance by large amounts."
p17-1181,2017,5 Conclusions,we proposed a method for rescoring the matrix that current state of the art methods produce by taking into account the scores assigned to other word pairs.
p17-1182,2017,8 Future Work,"in the future, we want to develop methods to make better use of languages with different alphabets or morphosyntactic features, in order to increase the applicability of our knowledge transfer method."
p17-1182,2017,7 Conclusion,our analysis indicated that the degree to which the source language data helps for a certain target language depends on their relatedness.
p17-1182,2017,7 Conclusion,our experiments showed that information from a high-resource language can be leveraged for paradigm completion in a related low-resource language.
p17-1182,2017,7 Conclusion,"our method led to significant improvements in settings with limited training data 鈥 up to 58% absolute improvement in accuracy 鈥 and, thus, enables the use of state-of-the-art models for paradigm completion in low-resource languages."
p17-1182,2017,7 Conclusion,"we presented a cross-lingual transfer learning method for paradigm completion, based on an rnn encoder-decoder model."
p17-1183,2017,7 Conclusion,"future work may include applying our model to other nearly-monotonic alignand-transduce tasks like abstractive summarization, transliteration or machine translation."
p17-1183,2017,7 Conclusion,"it is also computationally appealing as it enables linear time decoding while staying resolution preserving, i.e.not requiring to compress the input sequence to a single fixedsized vector."
p17-1183,2017,7 Conclusion,"our model performs better than previous neural and non-neural approaches on various morphological inflection generation datasets, while staying competitive with dedicated models even with very few training examples."
p17-1183,2017,7 Conclusion,the model employs an explicit alignment which is used to train a neural network to perform transduction by decoding with a hard attention mechanism.
p17-1183,2017,7 Conclusion,we presented a hard attention model for morphological inflection generation.
p17-1184,2017,6 Conclusion,"across languages with different typologies, our experiments show that the subword unit models are most effective on agglutinative languages."
p17-1184,2017,6 Conclusion,"although morphological analyses are available in limited quantities, our results suggest that there might be utility in semi-supervised learning from partially annotated data."
p17-1184,2017,6 Conclusion,"however, these results do not generalize to all languages, since factors such as morphology and orthography affect the utility of these representations."
p17-1184,2017,6 Conclusion,"moreover, our qualitative analysis suggests that they learn orthographic similarity of affixes, and lose the meaning of root morphemes."
p17-1184,2017,6 Conclusion,"our results confirm previous findings that character-level models are effective for many languages, but these models do not match the predictive accuracy of model with explicit knowledge of morphology, even after we increase the training data size by ten times."
p17-1184,2017,6 Conclusion,we plan to explore these effects in future work.
p17-1184,2017,6 Conclusion,"we presented a systematic comparison of word representation models with different levels of morphological awareness, across languages with different morphological typologies."
p17-1185,2017,7 Conclusions,"in our paper, we proposed the general two-step scheme of training sgns word embedding model and introduced the algorithm that performs the search of a solution in the low-rank form via riemannian optimization framework."
p17-1185,2017,7 Conclusions,possible direction of future work is to apply more advanced optimization techniques to the step 1 of the scheme proposed in section 1 and to explore the step 2 鈥 obtaining embeddings with a given low-rank matrix.
p17-1185,2017,7 Conclusions,we also demonstrated the superiority of our method by providing experimental comparison to existing state-of-the-art approaches.
p17-1186,2017,6 Conclusion,"because our techniques apply to labeled directed graphs in general, they can easily be extended to incorporate more formalisms, semantic or otherwise."
p17-1186,2017,6 Conclusion,in future work we hope to explore cross-task scoring and inference for tasks where parallel annotations are not available.
p17-1186,2017,6 Conclusion,our code is opensource and available at https://github.com/noahs-ark/neurboparser.
p17-1186,2017,6 Conclusion,"the first shares parameters when encoding tokens in the input with recurrent neural networks, and the second introduces interactions between output structures across formalisms."
p17-1186,2017,6 Conclusion,we showed two orthogonal ways to apply deep multitask learning to graph-based parsing.
p17-1186,2017,6 Conclusion,"without using syntactic parsing, these approaches outperform even state-of-the-art semantic dependency parsing systems that use syntax."
p17-1187,2017,5 Conclusion and Future Work,"in this paper, we propose a novel method to model sememe information for learning better word representations."
p17-1187,2017,5 Conclusion and Future Work,"specifically, we utilize sememe information to represent various senses of each word and propose sememe attention to select appropriate senses in contexts automatically."
p17-1187,2017,5 Conclusion and Future Work,"we also analyze several cases in wsd and wrl, which confirms our models are capable of selecting appropriate word senses with the favor of sememe attention."
p17-1187,2017,5 Conclusion and Future Work,"we evaluate our models on word similarity and word analogy, and results show the advantages of our sememeencoded wrl models."
p17-1187,2017,5 Conclusion and Future Work,we will explore the effectiveness of sememe information for wrl in other languages.
p17-1187,2017,5 Conclusion and Future Work,"we will explore the following research directions in future: (1) the sememe information in hownet is annotated with hierarchical structure and relations, which have not been considered in our framework."
p17-1187,2017,5 Conclusion and Future Work,we will explore to utilize these annotations for better wrl.(2) we believe the idea of sememes is universal and could be wellfunctioned beyond languages.
p17-1188,2017,7 Conclusion and Future Work,"additionally, by examining the character embeddings visually, we found that our visual model is able to learn visually related embeddings."
p17-1188,2017,7 Conclusion and Future Work,"furthermore, we showed that our visual model outperforms the lookup baseline model in low frequency instances."
p17-1188,2017,7 Conclusion and Future Work,"in summary, we tackled the problem of rare characters by using embeddings learned from images."
p17-1188,2017,7 Conclusion and Future Work,"in the future, we hope to further generalize this method to other tasks such as pronunciation estimation, which can take advantage of the fact that pronunciation information is encoded in parts of the characters as demonstrated in fig.1, or machine translation, which could benefit from a wholistic view that considers both semantics and pronunciation."
p17-1188,2017,7 Conclusion and Future Work,"in this paper, we proposed a new framework that utilizes appearance of characters, convolutional neural networks, recurrent neural networks to learn embeddings that are compositional in the component parts of the characters."
p17-1188,2017,7 Conclusion and Future Work,"more specifically, we collected a wikipedia dataset, which consists of short titles of three different languages and satisfies the compositionality in the characters of the language."
p17-1188,2017,7 Conclusion and Future Work,"next, we proposed an end-to-end model that learns visual embeddings for characters using cnn and showed that the features extracted from the cnn include both visual and semantic information."
p17-1188,2017,7 Conclusion and Future Work,"we also hope to apply the model to other languages with complicated compositional writing systems, potentially including historical texts such as hieroglyphics or cuneiform."
p17-1189,2017,7 Conclusion and Future Work,experiments have shown that our model yields better performance on cpb than all baseline models.
p17-1189,2017,7 Conclusion and Future Work,"in this paper, we proposed a progressive neural network model with gated recurrent adapters to leverage heterogeneous corpus for chinese srl."
p17-1189,2017,7 Conclusion and Future Work,"moreover, we proposed novel gated recurrent adapter to handle transfer on long sentences, the experiment has proved the effectiveness of the new adapter structure."
p17-1189,2017,7 Conclusion and Future Work,"so in the future, we might try to combine more heterogeneous semantic data for other tasks like event extraction and relation classification, etc."
p17-1189,2017,7 Conclusion and Future Work,"unlike previous methods like finetuning, ours leverage prior knowledge via lateral connections."
p17-1189,2017,7 Conclusion and Future Work,we also release the new corpus chinese sembank for chinese srl.
p17-1189,2017,7 Conclusion and Future Work,we believe that progressive learning with heterogeneous data is a promising avenue to pursue.
p17-1189,2017,7 Conclusion and Future Work,we hope that it will be helpful in providing common benchmarks for future work on chinese srl tasks.
p17-1190,2017,6 Conclusion,"furthermore, we analyzed the different errors produced by avg and the recurrent methods and found that the recurrent methods were learning composition that wasn鈥檛 being captured by avg."
p17-1190,2017,6 Conclusion,"future work will explore additional data sources, including from aligning different translations of novels (barzilay and mckeown, 2001), aligning new articles of the same topic (dolan et al., 2004), or even possibly using machine translation systems to translate bilingual text into paraphrastic sentence pairs."
p17-1190,2017,6 Conclusion,"our new techniques, combined with the promise of new data sources, offer a great deal of potential for improved universal paraphrastic sentence embeddings."
p17-1190,2017,6 Conclusion,"we also introduced a new recurrent network, the gated recurrent averaging network, that improves upon both avg and lstms for these tasks, and we release our code and trained models."
p17-1190,2017,6 Conclusion,we also investigated the gran in order to better understand the compositional phenomena it was learning by analyzing the l1 norm of its gate over various inputs.
p17-1190,2017,6 Conclusion,we showed how to modify and regularize lstms to improve their performance for learning paraphrastic sentence embeddings in both transfer and supervised settings.
p17-1191,2017,6 Conclusion,"as we have shown in 搂4.2, our model can deal with missing wordnet information by augmenting it with distributional information."
p17-1191,2017,6 Conclusion,"in this paper, we proposed a grounding of lexical items which acknowledges the semantic ambiguity of word types using wordnet and a method to learn a context-sensitive distribution over their representations."
p17-1191,2017,6 Conclusion,"moreover, the methods described in this paper can be extended to other kinds of structured knowledge sources like freebase which may be more suitable for tasks like question answering."
p17-1191,2017,6 Conclusion,"the models are implemented using keras (chollet, 2015), and the functionality is available at https://github.com/pdasigi/ onto-lstm in the form of keras layers to make it easier to use the proposed embedding model in other nlp problems."
p17-1191,2017,6 Conclusion,this approach may be extended to other nlp tasks that can benefit from using encoders that can access wordnet information.
p17-1191,2017,6 Conclusion,"we also showed how to integrate the proposed representation with recurrent neural networks for disambiguating prepositional phrase attachments, showing that the proposed wordnetgrounded context-sensitive token embeddings outperforms standard type-level embeddings for predicting pp attachments."
p17-1191,2017,6 Conclusion,we provided a detailed qualitative and quantitative analysis of the proposed model.implementation and code availability.
p17-1191,2017,6 Conclusion,"wordnet also has some drawbacks, and may not always have sufficient coverage given the task at hand."
p17-1192,2017,7 Conclusion,"as a result, current methods are not able to handle the infinite number of classes describable in natural language, most of which never appear in text."
p17-1192,2017,7 Conclusion,existing approaches often treat class labels as atomic units that must be observed in full in order to be populated with instances.
p17-1192,2017,7 Conclusion,"our method reasons about each modifier in the label individually, in terms of the properties that it implies about the instances."
p17-1192,2017,7 Conclusion,"this approach allows us to harness information that is spread across multiple sentences, significantly increasing the number of fine-grained classes that we are able to populate."
p17-1192,2017,7 Conclusion,we have presented an approach to isa extraction that takes advantage of the compositionality of natural language.
p17-1193,2017,6 Conclusion and Future Work,"in particular, we enhance the maximum subgraph model with new parsing algorithms for 1ec/p2 graphs."
p17-1193,2017,6 Conclusion and Future Work,"in this paper, we explore the strength of the graphbased approach."
p17-1193,2017,6 Conclusion and Future Work,our work indicates the importance of finding appropriate graph classes that on the one hand are linguistically expressive and on the other hand allow efficient search.
p17-1193,2017,6 Conclusion and Future Work,the arcfactored model proposed in this paper may be enhanced with higher-order features too.
p17-1193,2017,6 Conclusion and Future Work,we leave this for future investigation.
p17-1193,2017,6 Conclusion and Future Work,"within tree-structured dependency parsing, higher-order factorization that conditions on wider syntactic contexts than arc-factored relationships have been proved very useful."
p17-1194,2017,9 Conclusion,"at the same time, both of these are also combined, in order to predict the most probable label for each word."
p17-1194,2017,9 Conclusion,future work could investigate the extension of this architecture to additional unannotated resources.
p17-1194,2017,9 Conclusion,learning generalisable language features from large amounts of unlabeled in-domain text could provide sequence labeling models with additional benefit.
p17-1194,2017,9 Conclusion,"one half of a bidirectional lstm is trained as a forward-moving language model, whereas the other half is trained as a backward-moving language model."
p17-1194,2017,9 Conclusion,"the added language modeling objective allowed the system to take better advantage of the available training data, leading to 3.9% absolute improvement over the previous best architecture."
p17-1194,2017,9 Conclusion,"the architecture was evaluated on a range of datasets, covering the tasks of error detection in learner texts, named entity recognition, chunking and pos-tagging."
p17-1194,2017,9 Conclusion,"the label distribution in the original dataset is very sparse and unbalanced, making it a difficult task for the model to learn."
p17-1194,2017,9 Conclusion,"the language modeling objective also provided consistent improvements on other sequence labeling tasks, such as named entity recognition, chunking and pos-tagging."
p17-1194,2017,9 Conclusion,the largest benefit from the new architecture was observed on the task of error detection in learner writing.
p17-1194,2017,9 Conclusion,the model is incentivised to discover useful features in order to learn the language distribution and composition patterns in the training data.
p17-1194,2017,9 Conclusion,the objective of learning to predict surrounding words provides an additional source of information during training.
p17-1194,2017,9 Conclusion,this modification can be applied to several common sequence labeling architectures and requires no additional annotated or unannotated data.
p17-1194,2017,9 Conclusion,we found that the additional language modeling objective provided consistent performance improvements on every benchmark.
p17-1194,2017,9 Conclusion,we proposed a novel sequence labeling framework with a secondary objective 鈥 learning to predict surrounding words for each word in the dataset.
p17-1194,2017,9 Conclusion,"while it is common to pretrain word embeddings on large-scale unannotated corpora, only limited research has been done towards useful methods for pre-training or cotraining more advanced compositional modules."
p17-1194,2017,9 Conclusion,"while language modeling is not the main goal of the system, this additional training objective leads to more accurate sequence labeling models on several different tasks."
p17-1195,2017,10 Conclusion,experimental results demonstrated the effectiveness of the proposed techniques and showed the accuracy of the sentence-level logical form was 88% precision and 56% recall.
p17-1195,2017,10 Conclusion,our future work includes the expansion of the lexicon with the aid of the semantic parser and the development of a disambiguation model for the binding and scoping structures.
p17-1195,2017,10 Conclusion,the statistics of the benchmark data revealed that it includes far more complex semantic structures than the other benchmarks.
p17-1195,2017,10 Conclusion,we also presented an overview of an endto-end problem solving system and described two parsing techniques motivated by the scarcity of the annotated data and the need for the type coherency of the analysis.
p17-1195,2017,10 Conclusion,we have explained why the task of end-to-end math problem solving matters for a practical theory of natural language semantics and introduced the semantic parsing of pre-university math problems as a novel benchmark.
p18-1001,2018,6 Conclusion and Future Work,"currently, existing work on multi-lingual embeddings align the word semantics on pre-trained vectors (smith et al., 2017), which can be suboptimal due to polysemies."
p18-1001,2018,6 Conclusion and Future Work,"future work includes an investigation into the trade-off between learning full covariance matrices for each word distribution, computational complexity, and performance."
p18-1001,2018,6 Conclusion and Future Work,"moreover, our multimodal density models can provide interpretable and disentangled representations, and are the first multi-prototype embeddings that can handle rare words."
p18-1001,2018,6 Conclusion and Future Work,other future work involves co-training pft on many languages.
p18-1001,2018,6 Conclusion and Future Work,"our models offer better semantic quality, outperforming competing models on word similarity benchmarks."
p18-1001,2018,6 Conclusion and Future Work,the proposed probabilistic formulation incorporates uncertainty information and naturally allows one to uncover multiple meanings with multimodal density representations.
p18-1001,2018,6 Conclusion and Future Work,"this direction can potentially have a great impact on tasks where the variance information is crucial, such as for hierarchical modeling with probability distributions (athiwaratkun and wilson, 2018)."
p18-1001,2018,6 Conclusion and Future Work,we envision that the multi-prototype nature can help disambiguate words with multiple meanings and facilitate semantic alignment.
p18-1001,2018,6 Conclusion and Future Work,"we have proposed models for probabilistic word representations equipped with flexible sub-word structures, suitable for rare and out-of-vocabulary words."
p18-1002,2018,6 Conclusion,"a natural and principled integration of recent ideas for composing word vectors, the approach achieves strong performance on several tasks and promises to be useful in many linguistic settings and to yield many further research directions."
p18-1002,2018,6 Conclusion,"both in this area and for n-grams there is great scope for combining our approach with compositional approaches (bojanowski et al., 2016; poliak et al., 2017) that can handle settings such as zero-shot learning."
p18-1002,2018,6 Conclusion,"extensions of the mathematical formulation, such as the use of word weighting when building context vectors as in arora et al.(2018b) or of spectral information along the lines of mu and viswanath (2018), are also worthy of further study."
p18-1002,2018,6 Conclusion,"finally, there remain many language features, such as named entities and morphological forms, whose representation by our method remains unexplored."
p18-1002,2018,6 Conclusion,"more practically, the contextual rare words (crw) dataset we provide will support research on few-shot learning of word embeddings."
p18-1002,2018,6 Conclusion,"more work is needed to understand the usefulness of our method for representing (potentially cross-lingual) entities such as synsets, whose embeddings have found use in enhancing wordnet and related knowledge bases (camachocollados et al., 2016; khodak et al., 2017)."
p18-1002,2018,6 Conclusion,"of particular interest is the replacement of simple window contexts by other structures, such as dependency parses, that could yield results in domains such as question answering or semantic role labeling."
p18-1002,2018,6 Conclusion,"we have introduced a la carte ` embedding, a simple method for representing semantic features using unsupervised context information."
p18-1003,2018,6 Conclusions,"compared to approaches that rely on averaging word vectors, our method is able to learn more faithful representations by focusing on the words that are most strongly related to the considered relationship."
p18-1003,2018,6 Conclusions,"in contrast to neural network models for relation extraction, our model learns relation vectors in an unsupervised way, which means that it can be used for measuring relational similarities and related tasks."
p18-1003,2018,6 Conclusions,"moreover, even in (distantly) supervised tasks (where we need to learn a classifier on top of the unsupervised relation vectors), our model has proven competitive with state-of-the-art neural network models."
p18-1003,2018,6 Conclusions,we have proposed an unsupervised method which uses co-occurrences statistics to represent the relationship between a given pair of words as a vector.
p18-1004,2018,6 Conclusion,"in future work, we will investigate explicit retrofitting methods for asymmetric relations like hypernymy and meronymy."
p18-1004,2018,6 Conclusion,our global specialization approach resolves the well-known inability of retrofitting models to specialize vectors of words unseen in the constraints.
p18-1004,2018,6 Conclusion,"unlike existing retrofitting models, which directly update vectors of words from external constraints, we use the constraints as training examples to learn an explicit specialization function, implemented as a deep feedforward neural network."
p18-1004,2018,6 Conclusion,we also intend to apply the method to other downstream tasks and to investigate the zero-shot language transfer of the specialization function for more language pairs.
p18-1004,2018,6 Conclusion,we also showed that it is possible to transfer the specialization to languages without linguistic constraints.
p18-1004,2018,6 Conclusion,"we demonstrated the effectiveness of the proposed model on word similarity benchmarks, and in two downstream tasks: lexical simplification and dialog state tracking."
p18-1004,2018,6 Conclusion,we presented a novel method for specializing word embeddings to better discern similarity from other types of semantic relatedness.
p18-1005,2018,5 Conclusion and Future work,"additionally, the directional self-attention is introduced to model the temporal order information for our system."
p18-1005,2018,5 Conclusion and Future work,"besides, we decide to make more efforts to explore how to reinforce the temporal order information for the proposed model."
p18-1005,2018,5 Conclusion and Future work,"however, there is still a large room for improvement compared to the supervised nmt."
p18-1005,2018,5 Conclusion and Future work,"in the future, we would like to investigate how to utilize the monolingual data more effectively, such as incorporating the language model and syntactic information into unsupervised nmt."
p18-1005,2018,5 Conclusion and Future work,"in this paper, we propose the weight-sharing constraint in unsupervised nmt to address this issue."
p18-1005,2018,5 Conclusion and Future work,the ablation study shows that each component of our system achieves some improvement for the final translation performance.
p18-1005,2018,5 Conclusion and Future work,the experimental results reveal that our approach achieves significant improvement and verify our conjecture that the shared encoder is really a bottleneck for improving the unsupervised nmt.
p18-1005,2018,5 Conclusion and Future work,the models proposed recently for unsupervised nmt use a single encoder to map sentences from different languages to a shared-latent space.
p18-1005,2018,5 Conclusion and Future work,"to enhance the cross-language translation performance, we also propose the embedding-reinforced encoders, local gan and global gan into the proposed system."
p18-1005,2018,5 Conclusion and Future work,unsupervised nmt opens exciting opportunities for the future research.
p18-1005,2018,5 Conclusion and Future work,we conjecture that the shared encoder is problematic for keeping the unique and inherent characteristic of each language.
p18-1005,2018,5 Conclusion and Future work,"we test the proposed model on englishgerman, english-french and chinese-to-english translation tasks."
p18-1006,2018,5 Conclusion,"by introducing another rich language, our method can better exploit the additional language pairs to enrich the original low-resource pair."
p18-1006,2018,5 Conclusion,"compared with the rnnsearch (bahdanau et al., 2014), a teacherstudent alike method (chen et al., 2017) and the back-translation (sennrich et al., 2015) on the same data level, our method achieves significant improvement on the mutiun and iwslt2012 datasets."
p18-1006,2018,5 Conclusion,"in the future, we may extend our architecture to other scenarios, such as totally unsupervised training with no bilingual data for the rare language."
p18-1006,2018,5 Conclusion,"in this paper, we propose a triangular architecture (ta-nmt) to effectively tackle the problem of low-resource pairs translation with a unified bidirectional em framework."
p18-1006,2018,5 Conclusion,note that our method can be combined with methods exploiting monolingual data for nmt low-resource problem such as backtranslation and make further improvements.
p18-1007,2018,6 Conclusion,"additionally, we would like to explore the application of subword regularization for machine learning, including denoising auto encoder (vincent et al., 2008) and adversarial training (goodfellow et al., 2015)"
p18-1007,2018,6 Conclusion,"compared to machine translation, these tasks do not have enough training data, and thus there could be a large room for improvement with subword regularization."
p18-1007,2018,6 Conclusion,experiments on multiple corpora with different sizes and languages show that subword regularization leads to significant improvements especially on low resource and open-domain settings.
p18-1007,2018,6 Conclusion,"in addition, for better subword sampling, we propose a new subword segmentation algorithm based on the unigram language model."
p18-1007,2018,6 Conclusion,"in this paper, we presented a simple regularization method, subword regularization13, for nmt, with no change to the network architecture."
p18-1007,2018,6 Conclusion,"promising avenues for future work are to apply subword regularization to other nlp tasks based on encoder-decoder architectures, e.g., dialog generation (vinyals and le, 2015) and automatic summarization (rush et al., 2015)."
p18-1007,2018,6 Conclusion,"the central idea is to virtually augment training data with on-the-fly subword sampling, which helps to improve the accuracy as well as robustness of nmt models."
p18-1008,2018,7 Conclusion,"and what are the characteristic errors that each architecture makes, e.g., linguistic plausibility?"
p18-1008,2018,7 Conclusion,"applying these new techniques to rnmt models yields rnmt+, an enhanced rnmt model that significantly outperforms the three fundamental architectures on wmt鈥14 en鈫扚r and en鈫扗e tasks."
p18-1008,2018,7 Conclusion,how transferable are the representations learned by the different architectures to other tasks?
p18-1008,2018,7 Conclusion,in this work we explored the efficacy of several architectural and training techniques proposed in recent studies on seq2seq models for nmt.
p18-1008,2018,7 Conclusion,"our focus on a standard single-language-pair translation task leaves important open questions to be answered: how do our new architectures compare in multilingual settings, i.e., modeling an interlingua?"
p18-1008,2018,7 Conclusion,we demonstrated that many of these techniques are broadly applicable to multiple model architectures.
p18-1008,2018,7 Conclusion,"we further presented several hybrid models developed by combining encoders and decoders from the transformer and rnmt+ models, and empirically demonstrated the superiority of the transformer encoder and the rnmt+ decoder in comparison with their counterparts."
p18-1008,2018,7 Conclusion,"we hope that our work will motivate nmt researchers to further investigate generally applicable training and optimization techniques, and that our exploration of hybrid architectures will open paths for new architecture search efforts for nmt."
p18-1008,2018,7 Conclusion,"we then enhanced the encoder architecture by horizontally and vertically mixing components borrowed from these architectures, leading to hybrid architectures that obtain further improvements over rnmt+."
p18-1008,2018,7 Conclusion,"which architecture is more efficient and powerful in processing finer grained inputs and outputs, e.g., characters or bytes?"
p18-1009,2018,8 Conclusion,these new forms of distant supervision boost performance on our new dataset as well as on an existing fine-grained entity typing benchmark.
p18-1009,2018,8 Conclusion,"these results set the first performance levels for our evaluation dataset, and suggest that the data will support significant future work."
p18-1009,2018,8 Conclusion,using virtually unrestricted types allows us to expand the standard kb-based training methodology with typing information from wikipedia definitions and naturally-occurring head-word supervision.
p18-1010,2018,8 Conclusion,"additionally, we introduce two new humanannotated datasets: medmentions, a corpus of 246k mentions from pubmed abstracts linked to the umls knowledge base, and typenet, a new hierarchical fine-grained entity typeset an order of magnitude larger and deeper than previous datasets."
p18-1010,2018,8 Conclusion,"most of all, we are excited to see new techniques from the nlp community using the resources we have presented."
p18-1010,2018,8 Conclusion,we demonstrate that explicitly incorporating and modeling hierarchical information leads to increased performance in experiments on entity typing and linking across three challenging datasets.
p18-1010,2018,8 Conclusion,"while this work already demonstrates considerable improvement over non-hierarchical modeling, future work will explore techniques such as box embeddings (vilnis et al., 2018) and poincare麓 embeddings (nickel and kiela, 2017) to represent the hierarchical embedding space, as well as methods to improve recall in the candidate generation process for entity linking."
p18-1011,2018,5 Conclusion,"experimental results on benchmark kgs demonstrate that our method is simple yet surprisingly effective, showing significant and consistent improvements over strong baselines."
p18-1011,2018,5 Conclusion,"such constraints impose prior beliefs upon the structure of the embedding space, and will not significantly increase the space or time complexity."
p18-1011,2018,5 Conclusion,"the constraints indeed improve model interpretability, yielding a substantially increased structuring of the embedding space."
p18-1011,2018,5 Conclusion,this paper investigates the potential of using very simple constraints to improve kg embedding.
p18-1011,2018,5 Conclusion,"two types of constraints have been studied: (i) the non-negativity constraints to learn compact, interpretable entity representations, and (ii) the approximate entailment constraints to further encode logical regularities into relation representations."
p18-1012,2018,7 Conclusion,"in this paper, we have initiated a systematic study into the important but unexplored problem of analyzing geometry of various knowledge graph (kg) embedding methods."
p18-1012,2018,7 Conclusion,"through extensive experiments on multiple realworld datasets, we are able to identify several insights into the geometry of kg embeddings."
p18-1012,2018,7 Conclusion,"to the best of our knowledge, this is the first study of its kind."
p18-1012,2018,7 Conclusion,we have also explored the relationship between kg embedding geometry and its task performance.
p18-1012,2018,7 Conclusion,we have shared all our source code to foster further research in this area.
p18-1013,2018,6 Conclusion,"by end-to-end training of our model, we achieve the best rouge-recall and rouge while being the most informative and readable summarization on the cnn/daily mail dataset in a solid human evaluation."
p18-1013,2018,6 Conclusion,"most importantly, a novel inconsistency loss function is introduced to penalize the inconsistency between two levels of attentions."
p18-1013,2018,6 Conclusion,the inconsistency loss enables extractive and abstractive summarization to be mutually beneficial.
p18-1013,2018,6 Conclusion,we propose a unified model combining the strength of extractive and abstractive summarization.
p18-1014,2018,7 Conclusion,our experiments also suggest that modeling sentence-keyword interaction has the desirable property of less semantic redundancy in summaries generated by swap-net.
p18-1014,2018,7 Conclusion,swap-net models this interaction using a new two-level pointer network based architecture with a switching mechanism.
p18-1014,2018,7 Conclusion,"the architecture of swapnet is simpler than that of nn but due to its effective modeling of interaction between salient sentences and key words in a document, swapnet achieves superior performance."
p18-1014,2018,7 Conclusion,"we present swap-net, a neural sequence-tosequence model for extractive summarization that outperforms state-of-the-art extractive summarizers summarunner (nallapati et al., 2017) and nn (cheng and lapata, 2016) on large scale benchmark datasets."
p18-1015,2018,5 Conclusion and Future Work,"experiments show that our model can generate informative, readable and stable summaries."
p18-1015,2018,5 Conclusion and Future Work,"in addition, our model demonstrates promising prospect in generation diversity."
p18-1015,2018,5 Conclusion and Future Work,"on the one hand, since the candidate templates are far inferior to the optimal ones, we intend to improve the retrieve module, e.g., by indexing both the sentence and summary fields."
p18-1015,2018,5 Conclusion and Future Work,"on the other hand, we plan to test our system on the other tasks such as document-level summarization and short text conversation."
p18-1015,2018,5 Conclusion and Future Work,then we extend the seq2seq framework to jointly conduct template reranking and template-aware summary generation.
p18-1015,2018,5 Conclusion and Future Work,this paper proposes to introduce soft templates as additional input to guide the seq2seq summarization.
p18-1015,2018,5 Conclusion and Future Work,we believe our work can be extended in various aspects.
p18-1015,2018,5 Conclusion and Future Work,we use the popular ir platform lucene to retrieve proper existing summaries as candidate soft templates.
p18-1016,2018,8 Conclusion,future work will leverage ucca鈥檚 cross-linguistic applicability to support multi-lingual ts and ts pre-processing for mt.
p18-1016,2018,8 Conclusion,"the consideration of sentence splitting as a decomposition of a sentence into its scenes is further supported by recent work on structural ts evaluation (sulem et al., 2018), which proposes the samsa metric."
p18-1016,2018,8 Conclusion,"the proposed approach addresses the over-conservatism of mtbased systems for ts, which often fail to modify the source in any way."
p18-1016,2018,8 Conclusion,"the semantic component performs sentence splitting without relying on a specialized corpus, but only an off-theshelf semantic parser."
p18-1016,2018,8 Conclusion,"the two works, which apply this assumption to different ends (ts system construction, and ts evaluation), confirm its validity."
p18-1016,2018,8 Conclusion,"we presented the first simplification system combining semantic structures and neural machine translation, showing that it outperforms existing lexical and structural systems."
p18-1017,2018,8 Conclusions,"the nrc valence, arousal, and dominance lexicon is made available.17 it can be used in combination with other manually created affect lexicons such as the nrc word鈥揈motion association lexicon (mohammad and turney, 2013) and the nrc affect intensity lexicon (mohammad, 2018)."
p18-1017,2018,8 Conclusions,"these results show that certain demographic attributes impact how we view the world around us in terms of the relative valence, arousal, and dominance of the concepts in it."
p18-1017,2018,8 Conclusions,these scores are markedly higher than that of existing lexicons.
p18-1017,2018,8 Conclusions,"we analyzed demographic information to show that even though the annotations overall lead to consistent scores in repeated annotations, there exist statistically significant differences in agreements across demographic groups such as males and females, those above the age of 35 and those that are 35 or under, and across personality dimensions (extroverts and introverts, neurotic and secure, etc.)."
p18-1017,2018,8 Conclusions,"we obtained reliable human ratings of valence, arousal, and dominance for more than 20,000 english words.(it has about 40% more words than the largest existing manually created vad lexicon)."
p18-1017,2018,8 Conclusions,"we showed that the lexicon has split-half reliability scores of 0.95 for valence, 0.90 for arousal, and 0.90 for dominance."
p18-1017,2018,8 Conclusions,we used best鈥搘orst scaling to obtain finegrained scores (and word rankings) and addressed issues of annotation consistency that plague traditional rating scale methods of annotation.
p18-1018,2018,7 Conclusion,"our guidelines, corpus, and software are available at https://github.com/nert-gu/streusle/ blob/master/acl2018.md."
p18-1018,2018,7 Conclusion,"this paper introduced a new approach to comprehensive analysis of the semantics of prepositions and possessives in english, backed by a thoroughly documented hierarchy and annotated corpus."
p18-1018,2018,7 Conclusion,"we expect that future work will develop methods to scale the annotation process beyond requiring highly trained experts; bring this scheme to bear on other languages; and investigate the relationship of our scheme to more structured semantic representations, which could lead to more robust models."
p18-1018,2018,7 Conclusion,we found good interannotator agreement and provided initial supervised disambiguation results.
p18-1019,2018,6 Conclusions,"the ebm-nlp corpus, accompanying documentation, code for working with the data, and baseline models presented in this work are all publicly available at: http://www.ccs.neu.edu/home/bennye/ebm-nlp."
p18-1019,2018,6 Conclusions,the need for such technologies will only become more pressing as the literature continues its torrential growth.
p18-1019,2018,6 Conclusions,"this dataset fills a need for larger scale corpora to facilitate research on nlp methods for processing the biomedical literature, which have the potential to aid the conduct of ebm."
p18-1019,2018,6 Conclusions,"we have presented ebm-nlp: a new, publicly available corpus comprising 5,000 richly annotated abstracts of articles describing clinical randomized controlled trials."
p18-1020,2018,6 Conclusions,the model combines two approaches for scalar annotation: direct assessment and online pairwise ranking aggregation.
p18-1020,2018,6 Conclusions,the significant gains demonstrated suggests easl as a promising approach for future dataset curation and system evaluation in the community.
p18-1020,2018,6 Conclusions,"we conducted three illustrative experiments on lexical frequency inference, political spectrum inference, and ranking machine translation systems."
p18-1020,2018,6 Conclusions,"we have presented an efficient, online model to elicit scalar annotations for computational linguistic datasets and system evaluations."
p18-1020,2018,6 Conclusions,"we have shown that our approach, easl (efficient annotation of scalar labels), outperforms direct assessment in terms of annotation efficiency and outperforms online ranking aggregation in terms of accurately capturing the latent distributions of scalar values."
p18-1021,2018,10 Conclusion,both automatic evaluation against human arguments and human assessment showed that our model produced more informative arguments than popular sequence-to-sequence-based generation models.
p18-1021,2018,10 Conclusion,"separate decoders were designed to first produce a set of keyphrases as talking points, and then generate the final argument."
p18-1021,2018,10 Conclusion,we presented a neural argument generation framework enhanced with evidence retrieved from wikipedia.
p18-1021,2018,10 Conclusion,we studied the novel problem of generating arguments of a different stance for a given statement.
p18-1022,2018,6 Conclusion,"all of these results offer new, tangible, short-term avenues of development, lest large-scale fact-checking is still far out of reach."
p18-1022,2018,6 Conclusion,"employed as pre-filtering technologies to separate hyperpartisan news from mainstream news, our approach allows for directing the attention of human fact checkers to the most likely sources of fake news."
p18-1022,2018,6 Conclusion,"fact-checking for fake news detection poses an interdisciplinary challenge: technology is required to extract factual statements from text, to match facts with a knowledge base, to dynamically retrieve and maintain knowledge bases from the web, to reliably assess the overall veracity of an entire article rather than individual statements, to do so in real time as news events unfold, to monitor the spread of fake news within and across social media, to measure the reputation of information sources, and to raise awareness in readers."
p18-1022,2018,6 Conclusion,"moreover, for the first time, we found quantifiable evidence that the writing styles of news of the two opposing orientations are in fact very similar: there appears to be a common writing style of left and right extremism."
p18-1022,2018,6 Conclusion,"notwithstanding the many attacks on fake news by developing one way or another of fact-checking, we believe it worthwhile to mount our attack from another angle: writing style."
p18-1022,2018,6 Conclusion,"these are only the most salient things that need be done to tackle the problem, and as our cross-section of related work shows, a large body of work must be covered."
p18-1022,2018,6 Conclusion,"we further show that satire can be distinguished well from other news, ensuring that humor will not be outcast by fake news detection technology."
p18-1022,2018,6 Conclusion,we show that news articles conveying a hyperpartisan world view can be distinguished from more balanced news by writing style alone.
p18-1023,2018,7 Conclusion,"an alternative would be to generate counterarguments, but we believe that humans are better than machines in writing them 鈥 currently."
p18-1023,2018,7 Conclusion,"apart from some hyperparameters, however, our model is unsupervised and it does not make any assumption about an argument鈥檚 topic."
p18-1023,2018,7 Conclusion,"despite its simplicity, we鈫 found the best counterargument among 2801 candidates in almost a third of all cases, and ranked it into the top 15 on average."
p18-1023,2018,7 Conclusion,"for the restricted domain of debate portal arguments, our main result is quite intriguing: the best model (we鈫) rewards a high overall similarity to the argument鈥檚 conclusion and premises while punishing a too high similarity to either of them."
p18-1023,2018,7 Conclusion,"hence, it applies to any argument, given a pool of candidate counterarguments."
p18-1023,2018,7 Conclusion,"of course, our hypothesis is simplifying, i.e., there are counterarguments that will not be found based on aspect and stance similarity only."
p18-1023,2018,7 Conclusion,"still, returning one that addresses similar aspects with opposite stance makes sense then."
p18-1023,2018,7 Conclusion,"the intended practical application of our model is to retrieve counterarguments in automatic debating technologies (rinott et al., 2015) and argument search (wachsmuth et al., 2017b)."
p18-1023,2018,7 Conclusion,this paper has asked how to find the best counterargument to any argument without prior knowledge of the argument鈥檚 topic.
p18-1023,2018,7 Conclusion,this speaks for our hypothesis that the best counterargument often just addresses the same topical aspects with opposite stance.
p18-1023,2018,7 Conclusion,"to obtain further insights into the nature of counterarguments, deeper linguistic analysis along with supervised learning may be needed, though."
p18-1023,2018,7 Conclusion,we are confident that the modeled intuition generalizes beyond idebate.org.
p18-1023,2018,7 Conclusion,"we did not aim to engineer the best approach to this retrieval task, but to study whether we can model the simultaneous similarity and dissimilarity of a counterargument to an argument computationally."
p18-1023,2018,7 Conclusion,"we provide a corpus to train respective approaches, but leave the according research to future work."
p18-1023,2018,7 Conclusion,"while debate portal arguments are often suitable in this regard, in general not always a real counterargument exists to an argument."
p18-1023,2018,7 Conclusion,"while the model can be considered open-topic, a next step will be to study counterargument retrieval open-source."
p18-1024,2018,7 Concluding Remarks and Future Work,"a straightforward approach is to create a unified dataset out of more than two graphs by combining set of triplets as described in section 2, and apply learning and inference on the unified graph without any major change in the methodology."
p18-1024,2018,7 Concluding Remarks and Future Work,"alternatively, one can develop sophisticated approaches with iterative merging and learning over pairs of graphs until exhausting all graphs in an input collection."
p18-1024,2018,7 Concluding Remarks and Future Work,"for future work, we would like to extend the current evaluation of our work from a two-graph setting to multiple graphs."
p18-1024,2018,7 Concluding Remarks and Future Work,"however, we point out that our method is not restricted to such use cases鈥攐ne can readily apply our method to directly make inference over multiple graphs to support applications like question answering and conversations."
p18-1024,2018,7 Concluding Remarks and Future Work,"in real-world setting, we envision our method to be integrated in a large scale system that would include various other components for tasks like conflict resolution, active learning and human-in-loop learning to ensure quality of constructed super-graph."
p18-1024,2018,7 Concluding Remarks and Future Work,many data driven organizations such as google and microsoft take the approach of constructing a unified super-graph by integrating data from multiple sources.
p18-1024,2018,7 Concluding Remarks and Future Work,our inductive framework learns functions to encode contextual information and hence is graph independent.
p18-1024,2018,7 Concluding Remarks and Future Work,"still, the problem remains challenging for large scale knowledge graphs and this paper proposes a deep learning solution that can play a vital role in this construction process."
p18-1024,2018,7 Concluding Remarks and Future Work,"such unification has shown to significantly help in various applications, such as search, question answering, and personal assistance."
p18-1024,2018,7 Concluding Remarks and Future Work,the proposed representation learning framework leverage an efficient learning and inference procedure which takes into account the duplicate entities representing the same real-world entity in a multi-graph setting.
p18-1024,2018,7 Concluding Remarks and Future Work,"to this end, there exists a rich body of work on linking entities and relations, and conflict resolution (e.g., knowledge fusion (dong et al., 2014)."
p18-1024,2018,7 Concluding Remarks and Future Work,we believe that this work opens a new research direction in joint representation learning over multiple knowledge graphs.
p18-1024,2018,7 Concluding Remarks and Future Work,we demonstrate superior accuracies on link prediction and entity linkage tasks compared to the existing approaches that are trained only on individual graphs.
p18-1024,2018,7 Concluding Remarks and Future Work,we present a novel relational learning framework that learns entity and relationship embeddings across multiple graphs.
p18-1025,2018,6 Conclusion and Future Work,an exciting direction is the incorporation of multi-relational data for general knowledge representation and inference.
p18-1025,2018,6 Conclusion and Future Work,"improved inference of the latent boxes, either through better optimization or through bayesian approaches is another natural extension."
p18-1025,2018,6 Conclusion and Future Work,our greatest interest is in the application of this powerful new tool to the many areas where other structured embeddings have shown promise.
p18-1025,2018,6 Conclusion and Future Work,"secondly, more complex representations, such as 2n-dimensional products of 2-dimensional convex polyhedra, would offer greater flexibility in tiling event space."
p18-1025,2018,6 Conclusion and Future Work,we have only scratched the surface of possible applications.
p18-1026,2018,7 Discussion and Conclusion,"a better approach would be to allow the encoder to have a dynamic number of layers, possibly based on the diameter (longest path) in the input graph."
p18-1026,2018,7 Discussion and Conclusion,"an interesting alternative is weave module networks (kearnes et al., 2016), which explicitly decouples node and edge representations without incurring in parameter explosion."
p18-1026,2018,7 Discussion and Conclusion,"in particular, we showed how graph transformations can solve issues with graph-based networks without changing the underlying architecture."
p18-1026,2018,7 Discussion and Conclusion,incorporating both ideas to our architecture is an research direction we plan for future work.
p18-1026,2018,7 Discussion and Conclusion,"our approach addresses shortcomings from previous work, including loss of information from linearisation and parameter explosion."
p18-1026,2018,7 Discussion and Conclusion,our architecture nevertheless has two major limitations.
p18-1026,2018,7 Discussion and Conclusion,"overall, because our architecture can work with general graphs, it is straightforward to add linguistic biases in the form of extra node and/or edge information."
p18-1026,2018,7 Discussion and Conclusion,"the first one is that ggnns have a fixed number of layers, even though graphs can vary in size in terms of number of nodes and edges."
p18-1026,2018,7 Discussion and Conclusion,"the second limitation comes from the levi graph transformation: because edge labels are represented as nodes they end up sharing the vocabulary and therefore, the same semantic space."
p18-1026,2018,7 Discussion and Conclusion,"this is not ideal, as nodes and edges are different entities."
p18-1026,2018,7 Discussion and Conclusion,"this is the case of the proposed levi graph transformation, which ensures the decoder can attend to edges as well as nodes, but also to the sequential connections added to the dependency trees in the case of nmt."
p18-1026,2018,7 Discussion and Conclusion,we believe this is an interesting research direction in terms of applications.
p18-1026,2018,7 Discussion and Conclusion,"we proposed a novel encoder-decoder architecture for graph-to-sequence learning, outperforming baselines in two nlp tasks: generation from amr graphs and syntax-based nmt."
p18-1027,2018,8 Conclusion,"in addition, the model is able to regenerate words from nearby context, but heavily relies on caches to copy words from far away."
p18-1027,2018,8 Conclusion,"in this analytic study, we have empirically shown that a standard lstm language model can effectively use about 200 tokens of context on two benchmark datasets, regardless of hyperparameter settings such as model size."
p18-1027,2018,8 Conclusion,"it is sensitive to word order in the nearby context, but less so in the long-range context."
p18-1027,2018,8 Conclusion,"these findings not only help us better understand these models but also suggest ways for improving them, as discussed in section 7."
p18-1027,2018,8 Conclusion,"while observations in this paper are reported at the token level, deeper understanding of sentence-level interactions warrants further investigation, which we leave to future work."
p18-1028,2018,9 Conclusion,"as a simple version of an rnn, which is more expressive than one-layer cnns, we hope that sopa will encourage future research on the bridge between these two mechanisms."
p18-1028,2018,9 Conclusion,"it naturally models flexible-length spans with insertion and deletion, and it can be easily customized by swapping in different semirings."
p18-1028,2018,9 Conclusion,"on smaller training sets, sopa outperforms all four baselines."
p18-1028,2018,9 Conclusion,"sopa performs on par with or strictly better than four baselines on three text classification tasks, while requiring fewer parameters than the stronger baselines."
p18-1028,2018,9 Conclusion,"to facilitate such research, we release our implementation at https://github.com/ noahs-ark/soft_patterns."
p18-1028,2018,9 Conclusion,"we introduced sopa, a novel model that combines neural representation learning with wfsas."
p18-1028,2018,9 Conclusion,we showed that sopa is an extension of a one-layer cnn.
p18-1029,2018,6 Discussion and Future Work,"future work can model how these parameters can be adapted in a task specific way (e.g., cases such as cancer prediction where base rates are small), and provide better models of quantifier semantics.e.g., as distributions, rather than point values."
p18-1029,2018,6 Discussion and Future Work,"however, it does not address linguistic issues such as modifiers (e.g., very likely), nested quantification, etc."
p18-1029,2018,6 Discussion and Future Work,"on the other hand, we found no instances of nested quantification in the data, suggesting that people might be primed to use simpler language when teaching."
p18-1029,2018,6 Discussion and Future Work,our approach is a step towards the idea of using language to guide learning of statistical models.
p18-1029,2018,6 Discussion and Future Work,our approach is surprisingly effective in learning from free-form language.
p18-1029,2018,6 Discussion and Future Work,"this is an exciting direction, which contrasts with the predominant theme of using statistical learning methods to advance the field of nlp."
p18-1029,2018,6 Discussion and Future Work,"we believe that language may have as much to help learning, as statistical learning has helped nlp."
p18-1029,2018,6 Discussion and Future Work,"while we approximate quantifier semantics as absolute probability values, they may vary significantly based on the context, as shown by cognitive studies such as newstead and collis (1987)."
p18-1030,2018,5 Conclusion,"next directions also include the investigation of s-lstm to more nlp tasks, such as machine translation."
p18-1030,2018,5 Conclusion,"results on a range of classification and sequence labelling tasks show that s-lstm outperforms bilstms using the same number of parameters, demonstrating that s-lstm can be a useful addition to the neural toolbox for encoding sentences."
p18-1030,2018,5 Conclusion,"the structural nature in s-lstm states allows straightforward extension to tree structures, resulting in highly parallelisable tree lstms."
p18-1030,2018,5 Conclusion,"we have investigated s-lstm, a recurrent neural network for encoding sentences, which offers richer contextual information exchange with more parallelism compared to bilstms."
p18-1030,2018,5 Conclusion,we leave such investigation to future work.
p18-1031,2018,6 Discussion and future directions,another direction is to apply the method to novel tasks and models.
p18-1031,2018,6 Discussion and future directions,"finally, while we have provided a series of analyses and ablations, more studies are required to better understand what knowledge a pretrained language model captures, how this changes during fine-tuning, and what information different tasks require."
p18-1031,2018,6 Discussion and future directions,"given that transfer learning and particularly fine-tuning for nlp is under-explored, many future directions are possible."
p18-1031,2018,6 Discussion and future directions,"language modeling can also be augmented with additional tasks in a multi-task learning fashion (caruana, 1993) or enriched with additional supervision, e.g.syntax-sensitive dependencies (linzen et al., 2016) to create a model that is more general or better suited for certain downstream tasks, ideally in a weakly-supervised manner to retain its universal properties."
p18-1031,2018,6 Discussion and future directions,"one possible direction is to improve language model pretraining and fine-tuning and make them more scalable: for imagenet, predicting far fewer classes only incurs a small performance drop (huh et al., 2016), while recent work shows that an alignment between source and target task label sets is important (mahajan et al., 2018)鈥攆ocusing on predicting a subset of words such as the most frequent ones might retain most of the performance while speeding up training."
p18-1031,2018,7 Conclusion,our method significantly outperformed existing transfer learning techniques and the stateof-the-art on six representative text classification tasks.
p18-1031,2018,7 Conclusion,we have also proposed several novel fine-tuning techniques that in conjunction prevent catastrophic forgetting and enable robust learning across a diverse range of tasks.
p18-1031,2018,7 Conclusion,"we have proposed ulmfit, an effective and extremely sample-efficient transfer learning method that can be applied to any nlp task."
p18-1031,2018,7 Conclusion,we hope that our results will catalyze new developments in transfer learning for nlp.
p18-1031,2018,6 Discussion and future directions,"while an extension to sequence labeling is straightforward, other tasks with more complex interactions such as entailment or question answering may require novel ways to pretrain and fine-tune."
p18-1031,2018,6 Discussion and future directions,"while we have shown that ulmfit can achieve state-of-the-art performance on widely used text classification tasks, we believe that language model fine-tuning will be particularly useful in the following settings compared to existing transfer learning approaches (conneau et al., 2017; mccann et al., 2017; peters et al., 2018): a) nlp for non-english languages, where training data for supervised pretraining tasks is scarce; b) new nlp tasks where no state-of-the-art architecture exists; and c) tasks with limited amounts of labeled data (and some amounts of unlabeled data)."
p18-1032,2018,7 Summary,"based on our experimental results, we recommend lrp, deeplift and limsse for small context tasks and lrp and deeplift for large context tasks, on all five dnn architectures that we tested."
p18-1032,2018,7 Summary,neither paradigm requires manual annotations.
p18-1032,2018,7 Summary,"on cnns and possibly grus, the (integrated) gradient embedding dot product is a good alternative to deeplift and lrp."
p18-1032,2018,7 Summary,"to conduct this study, we introduced evaluation paradigms for explanation methods for two classes of nlp tasks, small context tasks (e.g., topic classification) and large context tasks (e.g., morphological prediction)."
p18-1032,2018,7 Summary,"we also introduced limsse, a substring-based explanation method inspired by lime and designed for nlp."
p18-1032,2018,7 Summary,"we conducted the first comprehensive evaluation of explanation methods for nlp, an important undertaking because there is a need for understanding the behavior of dnns."
p18-1033,2018,6 Conclusion,"and new systems should be evaluated on both question- and querybased splits, guiding the development of truly general systems for mapping natural language to structured database queries."
p18-1033,2018,6 Conclusion,developers of future large-scale datasets should incorporate joins and nesting to create more human-like data.
p18-1033,2018,6 Conclusion,evaluating on multiple datasets is necessary to ensure coverage of the types of questions humans generate.
p18-1033,2018,6 Conclusion,"first, by analyzing question and query complexity we find that human-written datasets require properties that have not yet been included in large-scale automatically generated query sets."
p18-1033,2018,6 Conclusion,in the process we also identify and fix hundreds of mistakes across multiple datasets and homogenize the sql query structures to enable effective multi-domain experiments.
p18-1033,2018,6 Conclusion,"in this work, we identify two issues in current datasets for mapping questions to sql queries."
p18-1033,2018,6 Conclusion,our analysis has clear implications for future work.
p18-1033,2018,6 Conclusion,"second, we show that the generalizability of systems is overstated by the traditional data splits."
p18-1034,2018,6 Conclusion and Future Work,experiments are conducted on the wikisql dataset.
p18-1034,2018,6 Conclusion and Future Work,"in the future, we plan to improve the accuracy of the column prediction component."
p18-1034,2018,6 Conclusion and Future Work,"in this work, the stamp model is not designed for the first and second cases, but it could be easily adapted to the third case through incorporating additional sql keywords and of course the learning of which requires dataset of the same type."
p18-1034,2018,6 Conclusion and Future Work,"in this work, we develop stamp, a syntax- and table- aware semantic parser that automatically maps natural language questions to sql queries, which could be executed on web table or relational dataset to get the answer."
p18-1034,2018,6 Conclusion and Future Work,results show that stamp achieves the new state-of-the-art performance on wikisql.
p18-1034,2018,6 Conclusion and Future Work,"sql language has more complicated queries than the cases included in the wikisql dataset, including (1) querying over multiple relational databases, (2) nested sql query as condition value, (3) more operations such as 鈥済roup by鈥 and 鈥渙rder by鈥, etc."
p18-1034,2018,6 Conclusion and Future Work,stamp considers cell information and the relation between cell and column name in the generation process.
p18-1034,2018,6 Conclusion and Future Work,"stamp has three channels, and it learns to switch to which channel at each time step."
p18-1034,2018,6 Conclusion and Future Work,we also plan to build a large-scale dataset that considers more sophisticated sql queries.
p18-1034,2018,6 Conclusion and Future Work,"we also plan to extend the approach to low-resource scenarios (feng et al., 2018)."
p18-1034,2018,6 Conclusion and Future Work,"we conduct extensive experiment analysis to show advantages and limitations of our approach, and where is the room for others to make further improvements."
p18-1035,2018,10 Conclusion,"future work will investigate whether a single algorithm and architecture can be competitive on all of these parsing tasks, an important step towards a joint many-task model for semantic parsing."
p18-1035,2018,10 Conclusion,"our experiments show that mtl improves ucca parsing, using amr, dm and ud parsing as auxiliaries."
p18-1035,2018,10 Conclusion,"we demonstrate that semantic parsers can leverage a range of semantically and syntactically annotated data, to improve their performance."
p18-1035,2018,10 Conclusion,"we propose a unified dag representation, construct protocols for converting these schemes into the unified format, and generalize a transitionbased dag parser to support all these tasks, allowing it to be jointly trained on them."
p18-1035,2018,10 Conclusion,"while we focus on ucca in this work, our parser is capable of parsing any scheme that can be represented in the unified dag format, and preliminary results on amr, dm and ud are promising (see 搂9)."
p18-1036,2018,7 Conclusion,character-level neural models are becoming the defacto standard for nlp problems due to their accessibility and ability to handle unseen data.
p18-1036,2018,7 Conclusion,"for agglutinative languages, their performance is limited on data with rich derivational morphology and high contextual ambiguity (morphological disambiguation); and for fusional languages, they struggle on tokens with high number of morphological tags, 鈥 similarity between character and morphology-level models is higher than the similarity within character-level (char and char-trigram) models on languages with high oov%; and vice versa, 鈥 their ability to handle long-range dependencies is very similar to morphology-level models, 鈥 they rely relatively more on training data size."
p18-1036,2018,7 Conclusion,"however, relatively less improvement is expected when model complexity is increased, 鈥 they generally perform better than models that only have access to predicted/silver morphological tags."
p18-1036,2018,7 Conclusion,"however, they still provide considerable advantages over whole-word models, 鈥 their shortcomings depend on the morphology type."
p18-1036,2018,7 Conclusion,"in this work, we investigated how they compare to models with access to gold morphological analysis, on a sentence-level semantic task."
p18-1036,2018,7 Conclusion,"our results lead to the following conclusions: 鈥 for in-domain data, character-level models cannot yet match the performance of morphology-level models."
p18-1036,2018,7 Conclusion,"therefore, given more training data their performance will improve faster than morphology-level models, 鈥 they perform substantially well on out of domain data, surpassing all morphology-level models."
p18-1036,2018,7 Conclusion,we evaluated their quality on semantic role labeling in a number of agglutinative and fusional languages.
p18-1037,2018,6 Conclusions,"a better alternative would be to use graph convolutional networks (marcheggiani and titov, 2017; kipf and welling, 2017): neighborhoods in the graph are likely to be more informative for predicting alignments than the neighborhoods in the graph traversal."
p18-1037,2018,6 Conclusions,another promising direction is integrating character seq2seq to substitute the copy function.
p18-1037,2018,6 Conclusions,the parser achieves state-of-the-art results and ablation tests show that joint modeling is indeed beneficial.
p18-1037,2018,6 Conclusions,this should also improve the handling of negation and rare words.
p18-1037,2018,6 Conclusions,"though our parsing model does not use any linearization of the graph, we relied on lstms and somewhat arbitrary linearization (depth-first traversal) to encode the amr graph in our alignment model."
p18-1037,2018,6 Conclusions,"we believe that the proposed approach may be extended to other parsing tasks where alignments are latent (e.g., parsing to logical form (liang, 2016))."
p18-1037,2018,6 Conclusions,"we introduced a neural amr parser trained by jointly modeling alignments, concepts and relations."
p18-1037,2018,6 Conclusions,we make such joint modeling computationally feasible by using the variational autoencoding framework and continuous relaxations.
p18-1038,2018,8 Conclusion,"especially, we emphasize the importance of modeling syntax-semantic interface and the compositional property of semantic annotations."
p18-1038,2018,8 Conclusion,"here, we have shown ways to improve shrg-based string-to-semanticgraph parsing."
p18-1038,2018,8 Conclusion,"just like recent explorations on many other nlp tasks, we also show that neural network models are very powerful to advance deep language understanding."
p18-1038,2018,8 Conclusion,the advantages of using graph grammars to resolve semantic parsing is clear in concept but underexploited in practice.
p18-1039,2018,8 Conclusion,"as shown in the error analysis, same types of problems can have different natural language expressions."
p18-1039,2018,8 Conclusion,experimental result shows that using intermediate forms is more effective than directly using equations.
p18-1039,2018,8 Conclusion,"furthermore, our iterative labeling effectively resolves ambiguities and leads to better performances."
p18-1039,2018,8 Conclusion,"in addition, we plan to expand the coverage of our meaning representation to support more mathematic concepts."
p18-1039,2018,8 Conclusion,"in the future, we will focus on tackling this challenge."
p18-1039,2018,8 Conclusion,this paper presents an intermediate meaning representation scheme for math problem solving that bridges the semantic gap between natural language and equation systems.
p18-1039,2018,8 Conclusion,"to generate intermediate forms, we propose a seq2seq model with novel attention regularization."
p18-1039,2018,8 Conclusion,"without explicit annotations of latent forms, we design an iterative labeling framework for training."
p18-1040,2018,8 Conclusions,"experimental results on the gmb show that our decoder is able to recover discourse representation structures to a good degree (77.54 f1), albeit with some simplifications."
p18-1040,2018,8 Conclusions,"in the future, we plan to model document-level representations which are more in line with drt and the gmb annotations."
p18-1040,2018,8 Conclusions,we introduced a new end-to-end model for opendomain semantic parsing.
p18-1041,2018,6 Conclusions,"however, a simple hierarchical pooling layer proposed here achieves comparable results to lstm/cnn on sentiment analysis tasks.鈥 to match natural language sentences, e.g., textual entailment, answer sentence selection, etc., simple pooling operations already exhibit similar or even superior results, compared to cnn and lstm.鈥 in swem with max-pooling operation, each individual dimension of the word embeddings contains interpretable semantic patterns, and groups together words with a common theme or topic."
p18-1041,2018,6 Conclusions,"our findings regarding when (and why) simple pooling operations are enough for text sequence representations are summarized as follows: 鈥 simple pooling operations are surprisingly effective at representing longer documents (with hundreds of words), while recurrent/convolutional compositional functions are most effective when constructing representations for short sentences.鈥 sentiment analysis tasks are more sensitive to word-order features than topic categorization tasks."
p18-1041,2018,6 Conclusions,"we further validated our experimental findings through additional exploration, and revealed some general rules for rationally selecting compositional functions for distinct problems."
p18-1041,2018,6 Conclusions,"we have performed a comparative study between swem (with parameter-free pooling operations) and cnn or lstm-based models, to represent text sequences on 17 nlp datasets."
p18-1042,2018,8 Conclusion,our procedure is immediately applicable to the wide range of languages for which we have parallel text.
p18-1042,2018,8 Conclusion,the key advantage of our approach is that it only requires parallel text.
p18-1042,2018,8 Conclusion,"there are hundreds of millions of parallel sentence pairs, and more are being generated continually."
p18-1042,2018,8 Conclusion,"we are actively investigating ways to apply these two new resources to downstream applications, including machine translation, question answering, and additional paraphrase generation tasks."
p18-1042,2018,8 Conclusion,"we described the creation of paranmt-50m, a dataset of more than 50m english sentential paraphrase pairs."
p18-1042,2018,8 Conclusion,"we hope that paranmt-50m, along with our embeddings, can impart a notion of meaning equivalence to improve nlp systems for a variety of tasks."
p18-1042,2018,8 Conclusion,"we release paranmt-50m, our code, and pretrained sentence embeddings, which also exhibit strong performance as general-purpose representations for a multitude of tasks."
p18-1042,2018,8 Conclusion,"we showed how to use paranmt50m to train paraphrastic sentence embeddings that outperform supervised systems on sts tasks, as well as how it can be used for generating paraphrases for purposes of data augmentation, robustness, and even grammar correction."
p18-1043,2018,7 Conclusion,our corpus supports learning representations over a diverse range of events and reasoning about the likely intents and reactions of previously unseen events.
p18-1043,2018,7 Conclusion,we also demonstrate that such inference can help reveal implicit gender bias in movie scripts.
p18-1043,2018,7 Conclusion,"we introduced a new corpus, task, and model for performing commonsense inference on textuallydescribed everyday events, focusing on stereotypical intents and reactions of people involved in the events."
p18-1044,2018,6 Conclusion,"in the future, we will apply this semi-supervised training method to other nlp tasks."
p18-1044,2018,6 Conclusion,"the generator neural network learns japanese pas and selectional preferences, while the validator is trained against the generator errors."
p18-1044,2018,6 Conclusion,this validator enables the generator to be trained from raw corpora and enhance it with external knowledge.
p18-1044,2018,6 Conclusion,we proposed a novel japanese pas analysis model that exploits a semi-supervised adversarial training.
p18-1045,2018,6 Conclusions and the Future Work,"in the future, we will explore the use of additional discourse structures that correlate highly with event coreference chains."
p18-1045,2018,6 Conclusions and the Future Work,"moreover, we will extend this work to other domains such as biomedical domains."
p18-1045,2018,6 Conclusions and the Future Work,our model outperformed the previous state-of-the-art model across all coreference scoring metrics.
p18-1045,2018,6 Conclusions and the Future Work,"we have presented an ilp based joint inference system for event coreference resolution that utilizes scores predicted by a pairwise event coreference resolution classifier, and models several aspects of correlations between event coreference chains and document level topic structures, including the correlation between the main event chains and topic transition sentences, interdependencies among event coreference chains, genre-specific coreferent mention distributions and subevents."
p18-1045,2018,6 Conclusions and the Future Work,we have shown that these structures are generalizable by conducting experiments on both the kbp 2016 and kbp 2017 datasets.
p18-1046,2018,5 Conclusion,distant supervision has become a standard method in relation extraction.
p18-1046,2018,5 Conclusion,"empirically, we show that our method can significantly improve the performances of many competitive baselines on the widely used new york time dataset."
p18-1046,2018,5 Conclusion,"however, while it brings the convenience, it also introduces noise in distantly labeled sentences."
p18-1046,2018,5 Conclusion,"in this work, we propose the first generative adversarial training method for robust distant supervision relation extraction."
p18-1046,2018,5 Conclusion,"more specifically, our framework has two components: a generator that generates true positives, and a discriminator that tries to classify positive and negative data samples."
p18-1046,2018,5 Conclusion,"our approach is model-agnostic, and thus can be applied to any distant supervision model."
p18-1046,2018,5 Conclusion,"with adversarial training, our goal is to gradually decrease the performance of the discriminator, while the generator improves the performance for predicting true positives when reaching equilibrium."
p18-1047,2018,Conclusions and Future Work,another future work is test our model in other nlp tasks like event extraction.
p18-1047,2018,Conclusions and Future Work,"in this paper, we proposed an end2end neural model based on seq2seq learning framework with copy mechanism for relational facts extraction."
p18-1047,2018,Conclusions and Future Work,"moreover, we analyze the different overlap types and adopt two strategies for this issue, including one unified decoder and multiple separated decoders."
p18-1047,2018,Conclusions and Future Work,our future work will concentrate on how to improve the performance further.
p18-1047,2018,Conclusions and Future Work,"our model can jointly extract relation and entity from sentences, especially when triplets in the sentences are overlapped."
p18-1047,2018,Conclusions and Future Work,the experiment results show that our models outperform the baseline method significantly and our models can extract relational facts from all three classes.
p18-1047,2018,Conclusions and Future Work,this challenging task is far from being solved.
p18-1047,2018,Conclusions and Future Work,we conduct experiments on two public datasets to evaluate the effectiveness of our models.
p18-1048,2018,7 Conclusion,"besides, the global features extracted in li et al (2013)鈥檚 work are potentially useful for detecting the event instances referred by pronouns, although involve noises."
p18-1048,2018,7 Conclusion,"considering this possibility, we suggest to drive the two discriminators in our self-regulation framework to cooperate with each other."
p18-1048,2018,7 Conclusion,"in the learning process, the adversarial and cooperative models are utilized in decontaminating the latent feature space."
p18-1048,2018,7 Conclusion,"in this study, the performance of the discriminator in the adversarial network is left to be evaluated."
p18-1048,2018,7 Conclusion,"most probably, the discriminator also performs well because it is gradually enhanced by fierce competition."
p18-1048,2018,7 Conclusion,"therefore, in the future, we will encode the global information by neural networks and use the self-regulation strategy to reduce the negative influence of noises."
p18-1048,2018,7 Conclusion,we use a self-regulated learning approach to improve event detection.
p18-1049,2018,6 Conclusion,experimental results show that the proposed model beats previous results without resorting to ad-hoc resolution of timegraph conflicts in postprocessing.
p18-1049,2018,6 Conclusion,"our model introduces a global context layer which is able to save and retrieve processed temporal relations, and then use this global context to infer new relations from new input."
p18-1049,2018,6 Conclusion,"the memory can be updated, allowing self-correction."
p18-1049,2018,6 Conclusion,we have proposed the first context-aware neural model for temporal information extraction using an external memory to represent global context.
p18-1050,2018,7 Conclusions,"for the future work, we plan to expand event temporal knowledge acquisition by dealing with event sense disambiguation and event synonym identification (e.g., drag, pull and haul)."
p18-1050,2018,7 Conclusions,the temporal event knowledge distilled from narrative texts were shown useful to improve temporal relation classification and outperform several neural language models on the narrative cloze task.
p18-1050,2018,7 Conclusions,this paper presents a novel approach for leveraging the double temporality characteristic of narrative texts and acquiring temporal event knowledge across sentences in narrative paragraphs.
p18-1050,2018,7 Conclusions,we developed a weakly supervised system that explores narratology principles and identifies narrative texts from three text corpora of distinct genres.
p18-1051,2018,5 Conclusion,"by crossing statistical approaches with neural networks, we propose a new strategy for automatically detecting complex linguistic observables, which up to now hardly detectable by frequency-based methods."
p18-1051,2018,5 Conclusion,improving the model and understanding all the mathematical and linguistic outcomes remains an import goal.
p18-1051,2018,5 Conclusion,"in a nutshell, text deconvolution saliency is efficient on a wide range of corpora."
p18-1051,2018,5 Conclusion,"in future work, we intend to thoroughly study the impact of tds given morphosyntactic information."
p18-1051,2018,5 Conclusion,recall that the linguistic matter and the topology recovered by our tds cannot return to chance: the zones of activation make it possible to obtain recognition rates of more than 91% on the french political speech and 93% on the latin corpus; both rates equivalent to or higher than the rates obtained by the statistical calculation of the key passages.
p18-1052,2018,7 Conclusion,our future goal is to use the coherence model to generate new conversations.
p18-1052,2018,7 Conclusion,our lexicalized grid model yields state of the art results on standard coherence assessment tasks in monologue and conversations.
p18-1052,2018,7 Conclusion,we also show a novel application of our model in forum thread reconstruction.
p18-1052,2018,7 Conclusion,"we designed a 3d grid representation for capturing spatio-temporal entity transitions in a conversation tree, and employed a 2d convolution to compose high-level features from this representation."
p18-1052,2018,7 Conclusion,we first extended the existing neural grid model by lexicalizing its entity transitions.
p18-1052,2018,7 Conclusion,we presented a coherence model for asynchronous conversations.
p18-1052,2018,7 Conclusion,we then adapt the model to conversational discourse by incorporating the thread structure in its grid representation and feature computation.
p18-1053,2018,5 Conclusion,"besides, to deal with the problematic scenario when ground truth parse tree and anaphoric zero pronoun are unavailable, we are interested in exploring the neural network model that integrates the anaphoric zero pronoun determination and anaphoric zero pronoun resolution jointly in a hierarchical architecture without using parser or anaphoric zero pronoun detector."
p18-1053,2018,5 Conclusion,"experiments on the benchmark dataset show that our reinforcement learning model achieves an f-score of 67.2% on the test dataset, surpassing all the existed models by a considerable margin."
p18-1053,2018,5 Conclusion,"in the future, we plan to explore neural network models for efficaciously resolving anaphoric zero pronoun documents and research on some specific components which might influence the performance of the model, such as the embedding."
p18-1053,2018,5 Conclusion,"meanwhile, we plan to research on the possibility of applying adversarial learning (goodfellow et al., 2014) to generate better rewards than the human-defined reward functions."
p18-1053,2018,5 Conclusion,our code is available at https://github.com/qyyin/reinforce4zp.git.
p18-1053,2018,5 Conclusion,"our model learns the policy on selecting antecedents in a sequential manner, leveraging effective information provided by the earlier predicted antecedents."
p18-1053,2018,5 Conclusion,"this strategy contributes to the predicting for later antecedents, bringing a natural view for the task."
p18-1053,2018,5 Conclusion,we introduce a deep reinforcement learning framework for chinese zero pronoun resolution.
p18-1054,2018,8 Conclusion,both of these analyses took the entity embedding into consideration to access the global information of entities.
p18-1054,2018,8 Conclusion,"each entity has its embedding, and the embeddings are updated according to the result of both of these analyses dynamically."
p18-1054,2018,8 Conclusion,"the experimental results demonstrated that the proposed method could improve the performance of the inter-sentential zero anaphora resolution drastically, which has been regarded as a notoriously difficult task."
p18-1054,2018,8 Conclusion,this paper has presented an entity-centric neural network-based joint model of coreference resolution and predicate argument structure analysis.
p18-1054,2018,8 Conclusion,we believe that our proposed method is also effective for other pro-drop languages such as chinese and korean.
p18-1055,2018,5 Conclusion,both techniques have proven useful for the generation of mgbank.
p18-1055,2018,5 Conclusion,the first extends the formalism with mechanisms for enforcing morphosyntactic agreements and selectional restrictions.
p18-1055,2018,5 Conclusion,"the second anchors computationally costly null heads to overt heads inside complex overt categories, rendering the formalism fully compatible with markovian supertagging techniques."
p18-1055,2018,5 Conclusion,we are now working on an a* mg parser which can consider the full distribution of supertags for each word and exploit the potential of these rich lexical categories.
p18-1055,2018,5 Conclusion,we presented two methods for constraining the parser鈥檚 search space and improving efficiency during wide-coverage mg parsing.
p18-1056,2018,5 Conclusion,"however, we want to stress the difference between the priming-induced alignment at lower linguistic levels and the intentional accommodation that is caused by higher-level perception of social power."
p18-1056,2018,5 Conclusion,"in particular, our findings suggest that the probability change of liwc categories is more likely to be a case of automatic alignment, rather than an intentional accommodation, because it is better explained by lowerlevel linguistic features (utterance length)."
p18-1056,2018,5 Conclusion,"instead, we consistently align towards language that shares certain low-level features."
p18-1056,2018,5 Conclusion,"perhaps in most scenarios, alignment is primarily influenced by linguistic features themselves, rather than social power."
p18-1056,2018,5 Conclusion,the latter should be a relatively stable effect that is independent on the lowlevel linguistic features.
p18-1056,2018,5 Conclusion,"therefore, we suggest that future work on social power and language use should consider other (maybe higher-level) linguistic elements."
p18-1056,2018,5 Conclusion,"to sum up, our findings suggest that the previously reported effect of power on linguistic alignment is not reliable."
p18-1056,2018,5 Conclusion,we are not denying the existence of accommodation caused by the social distance between interlocutors.
p18-1056,2018,5 Conclusion,"we call for the inclusion of a wider range of factors in future studies of social influences on language use, especially low-level but interpretable cognitive factors."
p18-1057,2018,7 Conclusion and Future Work,"additionally, the corpus promotes research on tasks not limited to pedagogical function classification, topic modeling and prerequisite relation labelling."
p18-1057,2018,7 Conclusion and Future Work,"finally, we formulate the problem of recommending resources for a given title and abstract pair as a new way to approach reading list generation and propose two baseline models."
p18-1057,2018,7 Conclusion and Future Work,for future work we plan to continue the collection and annotation of resources and to separately explore each of the above research tasks.
p18-1057,2018,7 Conclusion and Future Work,"in this paper we introduce the tutorialbank corpus, a collection of over 6,300 hand-collected resources on nlp and related fields."
p18-1057,2018,7 Conclusion and Future Work,our corpus is notably larger than similar datasets which deal with pedagogical resources and topic dependencies and unique in use as an educational tool.
p18-1057,2018,7 Conclusion and Future Work,"to this point, we believe that this dataset, with its multiple layers of annotation and usable interface, will be an invaluable tool to the students, educators and researchers of nlp."
p18-1058,2018,5 Conclusion,we believe that this corpus will push the frontiers of research in content-based essay grading by triggering the development of novel computational models concerning argument persuasiveness that could provide useful feedback to students on why their arguments are (un)persuasive in addition to how persuasive they are.
p18-1058,2018,5 Conclusion,"we presented the first corpus of 102 persuasive student essays that are simultaneously annotated with argument trees, persuasiveness scores, and attributes of argument components that impact these scores."
p18-1059,2018,5 Conclusion,"in a recent project (choshen and abend, 2018b), we proposed a complementary measure that measures the semantic faithfulness of the output to the source, in order to form a combined semantic measure that bypasses the pitfalls of low coverage."
p18-1059,2018,5 Conclusion,"indeed, if outputs all similarly under-correct, correlation studies will not be affected by whether an rbm is sensitive to undercorrection."
p18-1059,2018,5 Conclusion,"napoles et al.(2016) have made progress towards this goal in proposing a reference-less grammaticality measure, using grammatical error detection tools, as did asano et al.(2017), who added a fluency measure to the grammaticality."
p18-1059,2018,5 Conclusion,our findings demonstrate how these tools can help characterize the biases of existing systems and evaluation measures.
p18-1059,2018,5 Conclusion,"our results underscore the importance of developing alternative evaluation measures that transcend n-gram overlap, and use deeper analysis tools, e.g., by comparing the semantics of the reference and the source to the output (cf.lo and wu, 2011)."
p18-1059,2018,5 Conclusion,"the paper makes two methodological contributions to the monolingual translation evaluation literature: (1) a methodology for evaluating evaluation measures by the scores they assign a perfect system, using a bootstrapping procedure; (2) a methodology for assessing the distribution of valid monolingual translations."
p18-1059,2018,5 Conclusion,"therefore, the tendency of rbms to reward under-correction cannot be detected by such correlation experiments (cf.choshen and abend, 2018a)."
p18-1059,2018,5 Conclusion,"we argue that using low-coverage reference sets has adverse effects on the reliability of referencebased evaluation, with gec and ts as a test case, and consequently on the incentives offered to systems."
p18-1059,2018,5 Conclusion,we believe our findings and methodologies can be useful for similar tasks such as style conversion and automatic post-editing of raw mt outputs.
p18-1059,2018,5 Conclusion,we further argue that these effects cannot be overcome by re-scaling or increasing the number of references in a feasible way.
p18-1059,2018,5 Conclusion,"we note that the lcb further jeopardizes the reliability of common validation experiments for rbms, that assess the correlation between human and measure rankings of system outputs (grundkiewicz et al., 2015)."
p18-1060,2018,7 Discussion,"alternatively, we could give up on measuring absolute scores, and seek instead to find techniques stably rank methods and thus improve them."
p18-1060,2018,7 Discussion,"as a result, the evaluation is systematically biased against genuine system improvements that would lead to higher human evaluation scores but not improve automatic metrics."
p18-1060,2018,7 Discussion,"as an example of how evaluation prompts could be improved, we found that using post-edits of summarizes decreased normalized annotator variance by a factor of three relative to using a likert scale survey."
p18-1060,2018,7 Discussion,"as the nlp community tackles increasingly difficult tasks, human evaluation will only become more important."
p18-1060,2018,7 Discussion,"certainly, we can try to improve the automatic metric (which is potentially as difficult as solving the task) and brainstorming alternative ways of soliciting evaluation (which has been less explored)."
p18-1060,2018,7 Discussion,"in practice, we find that with current automatic metrics and evaluation prompts data efficiencies are only 1.08鈥1.15 (7鈥13% cost reduction)."
p18-1060,2018,7 Discussion,"in this paper, we have explored using an automatic metric to decrease the cost of human evaluation without introducing bias."
p18-1060,2018,7 Discussion,it should be noted that changing the evaluation prompt also changes the underlying ground truth f(z): it is up to us to find a prompt that still captures the essence of what we want to measure.
p18-1060,2018,7 Discussion,our theory shows that further improvements are only possible by improving the correlation of the automatic metric and reducing the annotator variance of the evaluation prompt.
p18-1060,2018,7 Discussion,prior work has shown that existing automatic metrics have poor instance-level correlation with mean human judgment and that they score many good quality responses poorly.
p18-1060,2018,7 Discussion,we hope our work provides some clarity on to how to make it more cost effective.
p18-1060,2018,7 Discussion,where do we go from here?
p18-1060,2018,7 Discussion,"without making stronger assumptions, the control variates estimator we proposed outlines the limitations of unbiased estimation."
p18-1061,2018,7 Conclusion,conventional approaches to extractive document summarization contain two separated steps: sentence scoring and sentence selection.
p18-1061,2018,7 Conclusion,"every time it selects a sentence, it scores the sentences according to the partial output summary and current extraction state."
p18-1061,2018,7 Conclusion,"in this paper, we present a novel neural network framework for extractive document summarization by jointly learning to score and select sentences to address this issue."
p18-1061,2018,7 Conclusion,rouge evaluation results show that the proposed joint sentence scoring and selection approach significantly outperforms previous separated methods.
p18-1061,2018,7 Conclusion,the most distinguishing feature of our approach from previous methods is that it combines sentence scoring and selection into one phase.
p18-1062,2018,7 Conclusion and Next Steps,"as a result, our framework is advantaged on asr input."
p18-1062,2018,7 Conclusion and Next Steps,"finally, thanks to its fully unsupervised nature, our method is applicable to other languages than english in an almost out-of-thebox manner."
p18-1062,2018,7 Conclusion and Next Steps,"finally, we use a language model to favor fluent paths, which is crucial when working with (meeting) speech but not that important when dealing with well-formed input."
p18-1062,2018,7 Conclusion and Next Steps,"future efforts should be dedicated to improving the community detection phase and generating more abstractive sentences, probably by harnessing deep learning."
p18-1062,2018,7 Conclusion and Next Steps,"however, the lack of large training sets for the meeting domain is an obstacle to the use of neural approaches."
p18-1062,2018,7 Conclusion and Next Steps,"in addition, the mscg is by design robust to noise, and our custom path re-ranking strategy, based on graph degeneracy, makes it even more robust to noise."
p18-1062,2018,7 Conclusion and Next Steps,"indeed, our generative component, the multi-sentence compression graph (mscg), needs redundancy to perform well."
p18-1062,2018,7 Conclusion and Next Steps,"our framework combines the strengths of 6 approaches that had previously been applied to 3 different tasks (keyword extraction, multi-sentence compression, and summarization) into a unified, fully unsupervised end-to-end summarization framework, and introduces some novel components."
p18-1062,2018,7 Conclusion and Next Steps,our framework was developed for the meeting domain.
p18-1062,2018,7 Conclusion and Next Steps,"rigorous evaluation on the ami and icsi corpora shows that we reach state-of-the-art performance, and generate reasonably grammatical abstractive summaries despite taking noisy utterances as input and not relying on any annotations or training data."
p18-1062,2018,7 Conclusion and Next Steps,such redundancy is typically present in meeting speech but not in traditional documents.
p18-1063,2018,8 Conclusion,"our model achieves the new state-of-the-art on both cnn/dm versions as well a better generalization on test-only duc-2002, along with a significant speed-up in training and decoding."
p18-1063,2018,8 Conclusion,"we propose a novel sentence-level rl model for abstractive summarization, which makes the model aware of the word-sentence hierarchy."
p18-1064,2018,8 Conclusion,"we presented a multi-task learning approach to improve abstractive summarization by incorporating the ability to detect salient information and to be logically entailed by the document, via question generation and entailment generation auxiliary tasks."
p18-1064,2018,8 Conclusion,"we propose effective soft and highlevel (semantic) layer-specific parameter sharing and achieve significant improvements over the state-of-the-art on two popular datasets, as well as a generalizability/transfer duc-2002 setup."
p18-1065,2018,5 Conclusions and Recommendations,"a substantial amount of work must still be done to find, approximate, and implement moderating factors in helpfulness prediction systems, as well as build models that can adequately reflect the effects of these factors."
p18-1065,2018,5 Conclusions and Recommendations,"although ideally, these tests would be carried out on our proposed gold-standard dataset, we believe that the ciao dataset introduced in tang et al.(2013) and ard (mcauley et al., 2015) can prove useful to define a baseline in the short term."
p18-1065,2018,5 Conclusions and Recommendations,"although significant advances have been made on finding hand-crafted features for helpfulness prediction, effective comparisons between proposed approaches have been hindered by the lack of standard evaluation datasets, well-defined baselines, and feature ablation studies."
p18-1065,2018,5 Conclusions and Recommendations,"as we have seen that product nature influences the voting process, these tests should be conducted over different products and product categories."
p18-1065,2018,5 Conclusions and Recommendations,"baseline systems to design a strong baseline system, first, researchers should consider all proposed features so far, including content features, context features, and features used to approach moderating factors."
p18-1065,2018,5 Conclusions and Recommendations,"consequently, we encourage researchers to evaluate the usefulness of these features and study these moderating factors in different domains, platforms, and languages, possibly identifying new features and moderating factors."
p18-1065,2018,5 Conclusions and Recommendations,"data given that we recommend user-specific helpfulness prediction, we propose the development of a gold standard that contains information that can facilitate the design of user-specific models (e.g., records of who voted and how, data relevant to user-profiling recommendations such as age, location, social networks, purchase and browsing history and patterns, product reviews written, and review and product rating histories)."
p18-1065,2018,5 Conclusions and Recommendations,"features and knowledge sources while we encourage the development of user-specific helpfulness prediction, we by no means imply that a model should be trained for each user."
p18-1065,2018,5 Conclusions and Recommendations,"further, 鈥渟imilar鈥 reviews (i.e., reviews on which users vote similarly) could be exploited (sarwar et al., 2001; linden et al., 2003)."
p18-1065,2018,5 Conclusions and Recommendations,"furthermore, a variety of insightful observations have been made on moderating factors."
p18-1065,2018,5 Conclusions and Recommendations,"furthermore, as users frequently vote on reviews in a different context (scores and neighboring reviews can vary over time), this dataset should include temporal information, which would allow researchers to reconstruct the context under which votes are cast."
p18-1065,2018,5 Conclusions and Recommendations,"however, there have been exciting developments in helpfulness prediction: systems that have attempted to exploit user and reviewer information, along with those based on sophisticated models (e.g., probabilistic matrix factorization, hmm-lda) and neural network architectures, are promising prospects for future work."
p18-1065,2018,5 Conclusions and Recommendations,"in fact, this may not be feasible if a user has cast only a small number of votes."
p18-1065,2018,5 Conclusions and Recommendations,"in particular, product opinions, user idiosyncrasy, product and review nature, along with review voting context have been shown to influence the way users vote."
p18-1065,2018,5 Conclusions and Recommendations,note that pursuing user-specific helpfulness prediction is not enough.
p18-1065,2018,5 Conclusions and Recommendations,"once product and user/reviewer factors are incorporated into a model, it may become feasible to use past instances to predict helpfulness votes (how similar is a test instance to past situations where a user has voted 鈥渉elpful鈥?)."
p18-1065,2018,5 Conclusions and Recommendations,one is to train a user-specific model for each cluster of 鈥渟imilar鈥 users.
p18-1065,2018,5 Conclusions and Recommendations,online product review helpfulness modeling and prediction is a multi-faceted task that involves using content and context information to understand and predict helpfulness scores.
p18-1065,2018,5 Conclusions and Recommendations,"other platforms, review domains and languages while we focused on amazon product reviews written in english, the majority of the features discussed in section 3 are platform-, domainand language-independent, and the existence and importance of moderating factors described in section 4 is by no means limited to product reviews."
p18-1065,2018,5 Conclusions and Recommendations,"researchers now have at their disposal at least three public, pre-collected product review datasets 鈥 mdsd, ard, and ciao 鈥 to build and test systems."
p18-1065,2018,5 Conclusions and Recommendations,"second, combinations of these features should be systematically tested on the different models proposed by researchers."
p18-1065,2018,5 Conclusions and Recommendations,"taking inspirations from collaborative filtering, we could define or learn user similarity based on their purchasing/browsing/review and product rating histories (liu et al., 2014) as well as profiling information (krulwich, 1997), which should be available in the aforementioned dataset."
p18-1065,2018,5 Conclusions and Recommendations,"task if one acknowledges the role that users play in determining whether a review is helpful or not, it seems contradictory to insist on predicting helpfulness scores, which represent the average perception of a subset of users that (1) may not be representative of the entire population and (2) may not serve users well if their perceptions do not align with the subset of users that voted (even if the subset consisted of the entire population)."
p18-1065,2018,5 Conclusions and Recommendations,there are multiple ways to approach this task.
p18-1065,2018,5 Conclusions and Recommendations,"this is why we consider that user-specific helpfulness prediction, first presented in moghaddam et al.(2012) and tang et al.(2013), should be the goal of future work, as it allows systems to tailor their predictions to users鈥 preferences and needs (much like a recommender system)."
p18-1065,2018,5 Conclusions and Recommendations,"this provides suggestive evidence that researchers should adopt a holistic view of the helpfulness voting process, which may require information not present in current datasets."
p18-1065,2018,5 Conclusions and Recommendations,"to build this dataset, we recommend that researchers work with companies such as amazon, which may have such information."
p18-1065,2018,5 Conclusions and Recommendations,"towards this purpose, the systems proposed in tang et al.(2013), mukherjee et al.(2017), malik and hussain (2017), and chen et al.(2018) could serve as baselines after being enriched with extra features."
p18-1065,2018,5 Conclusions and Recommendations,we conclude our survey with several recommendations for future work on computational modeling and prediction of review helpfulness.
p18-1065,2018,5 Conclusions and Recommendations,"we recommend identifying specific experience and search products, since the effects of product nature have already been proven for them."
p18-1066,2018,7 Conclusion,"future directions include: 1) mining cross-cultural differences in general concepts other than names and slang, 2) merging the mined knowledge into existing knowledge bases, and 3) applying the socvec in downstream tasks like machine translation."
p18-1066,2018,7 Conclusion,"through extensive experiments, we demonstrate that the proposed lightweight yet effective method outperforms a number of baselines, and can be useful in translation applications and cross-cultural studies in computational social science."
p18-1066,2018,7 Conclusion,"we present the socvec method to compute crosscultural differences and similarities, and evaluate it on two novel tasks about mining cross-cultural differences in named entities and computing crosscultural similarities in slang terms."
p18-1067,2018,7 Conclusion,"in future works, we will exploit the interesting connections between moral foundations and frames for the analysis of more detailed ideological leanings and stance prediction."
p18-1067,2018,7 Conclusion,"in this paper we present psl models for the classification of moral foundations expressed in political discourse on the microblog, twitter."
p18-1067,2018,7 Conclusion,moral foundations and policy frames are employed as political strategies by politicians to garner support from the public.
p18-1067,2018,7 Conclusion,"politicians carefully word their statements to express their moral and social positions on issues, while maximizing their base鈥檚 response to their message."
p18-1067,2018,7 Conclusion,we also provide an initial approach to the joint modeling of frames and moral foundations.
p18-1067,2018,7 Conclusion,we show the benefits and drawbacks of traditionally used mfd unigrams and domain-specific unigrams for initialization of the models.
p18-1068,2018,6 Conclusions,experimental results show that coarseto-fine decoding improves performance across tasks.
p18-1068,2018,6 Conclusions,"in the future, we would like to apply the framework in a weakly supervised setting, i.e., to learn semantic parsers from question-answer pairs and to explore alternative ways of defining meaning sketches."
p18-1068,2018,6 Conclusions,in this paper we presented a coarse-to-fine decoding framework for neural semantic parsing.
p18-1068,2018,6 Conclusions,the proposed framework can be easily adapted to different domains and meaning representations.
p18-1068,2018,6 Conclusions,we first generate meaning sketches which abstract away from low-level information such as arguments and variable names and then predict missing details in order to obtain full meaning representations.
p18-1069,2018,7 Conclusions,directions for future work are many and varied.
p18-1069,2018,7 Conclusions,experimental results show that our method achieves better performance than competitive baselines on two datasets.
p18-1069,2018,7 Conclusions,in this paper we presented a confidence estimation model and an uncertainty interpretation method for neural semantic parsing.
p18-1069,2018,7 Conclusions,"the proposed framework could be applied to a variety of tasks (bahdanau et al., 2015; schmaltz et al., 2017) employing sequence-to-sequence architectures."
p18-1069,2018,7 Conclusions,we could also utilize the confidence estimation model within an active learning framework for neural semantic parsing.
p18-1070,2018,6 Conclusion,"we apply structvae to semantic parsing and code generation tasks, and show it outperforms a strong supervised parser using extra unlabeled data."
p18-1070,2018,6 Conclusion,"we propose structvae, a deep generative model with tree-structured latent variables for semi-supervised semantic parsing."
p18-1071,2018,6 Conclusions,"by leveraging the advantages of semantic graph representation and exploiting the representation learning and prediction ability of seq2seq models, our method achieved significant performance improvements on three datasets."
p18-1071,2018,6 Conclusions,"for future work, to solve the problem of the lack of training data, we want to design weakly supervised learning algorithm using denotations (qa pairs) as supervision."
p18-1071,2018,6 Conclusions,"furthermore, structure and semantic constraints can be easily incorporated in decoding to enhance semantic parsing."
p18-1071,2018,6 Conclusions,"furthermore, we want to collect labeled data by designing an interactive ui for annotation assist like (yih et al., 2016), which uses semantic graphs to annotate the meaning of sentences, since semantic graph is more natural and can be easily annotated without the need of expert knowledge."
p18-1071,2018,6 Conclusions,"this paper proposes sequence-to-action, a method which models semantic parsing as an end-to-end semantic graph generation process."
p18-1072,2018,6 Conclusion,"further, we found eigenvector similarity of sampled nearest neighbor subgraphs to be predictive of unsupervised bdi performance."
p18-1072,2018,6 Conclusion,we hope that this work will guide further developments in this new and exciting field.
p18-1072,2018,6 Conclusion,"we investigated when unsupervised bdi (conneau et al., 2018) is possible and found that differences in morphology, domains or word embedding algorithms may challenge this approach."
p18-1073,2018,6 Conclusions,"in contrast to adversarial methods, we propose to use an initial weak mapping that exploits the structure of the embedding spaces in combination with a robust self-learning approach."
p18-1073,2018,6 Conclusions,"in order to make selflearning robust, we also added stochasticity to dictionary induction, used csls instead of nearest neighbor, and produced bidirectional dictionaries."
p18-1073,2018,6 Conclusions,"in the future, we would like to extend the method from the bilingual to the multilingual scenario, and go beyond the word level by incorporating embeddings of longer phrases."
p18-1073,2018,6 Conclusions,"in this paper, we show that previous unsupervised mapping methods (zhang et al., 2017a; conneau et al., 2018) often fail on realistic scenarios involving non-comparable corpora and/or distant languages."
p18-1073,2018,6 Conclusions,our implementation is available as an open source project at https://github.com/artetxem/vecmap.
p18-1073,2018,6 Conclusions,results also improved using smaller intermediate vocabularies and re-weighting the final solution.
p18-1073,2018,6 Conclusions,the ablation analysis shows that our initial solution is instrumental for making self-learning work without supervision.
p18-1073,2018,6 Conclusions,"the results show that our method succeeds in all cases, providing the best results with respect to all previous work on unsupervised and supervised mappings."
p18-1074,2018,5 Conclusions and Future Work,experiments show that our model can effectively transfer different types of knowledge to improve the main model.
p18-1074,2018,5 Conclusions and Future Work,"it substantially outperforms the mono-lingual single-task baseline model, cross-lingual transfer model, and crosstask transfer model."
p18-1074,2018,5 Conclusions and Future Work,"the next step of this research is to apply this architecture to other types of tasks, such as event extract and semantic role labeling that involve structure prediction."
p18-1074,2018,5 Conclusions and Future Work,we also plan to explore the possibility of integrating incremental learning into this architecture to adapt a trained model for new tasks rapidly.
p18-1074,2018,5 Conclusions and Future Work,we design a multi-lingual multi-task architecture for low-resource settings.
p18-1074,2018,5 Conclusions and Future Work,we evaluate the model on sequence labeling tasks with three language pairs.
p18-1075,2018,7 Conclusion,bilingual word embeddings trained on general domain data yield poor results in out-of-domain tasks.
p18-1075,2018,7 Conclusion,"furthermore, by adapting the broadly applicable semi-supervised approach of hausser et al.篓 (2017) (which until now has only been applied in computer vision) we were able to effectively exploit unlabeled data to further improve performance."
p18-1075,2018,7 Conclusion,"in addition, clsc results are competitive with a system that uses targetlanguage labeled data, even when we use no such target-language labeled data."
p18-1075,2018,7 Conclusion,our delightfully simple task independent method to adapt bwes to a specific domain uses unlabeled monolingual data only.
p18-1075,2018,7 Conclusion,we presented experiments on two different low-resource task/domain combinations.
p18-1075,2018,7 Conclusion,we showed that with the support of adapted bwes the performance of offthe-shelf methods can be increased for both crosslingual twitter sentiment classification and medical bilingual lexicon induction.
p18-1075,2018,7 Conclusion,"we showed that, when also using high-quality adapted bwes, the performance of the semi-supervised systems can be significantly increased by using unlabeled data at classifier training time."
p18-1076,2018,7 Conclusion and Future Work,"in future work, we will investigate even tighter integration of the attended knowledge and stronger reasoning methods."
p18-1076,2018,7 Conclusion and Future Work,"incorporating external knowledge improves its results with a relative error rate reduction of 9% on common nouns, thus the model is able to compete with more complex rc models."
p18-1076,2018,7 Conclusion and Future Work,"since our model directly integrates background knowledge with the document and questioncontext representations, it can be adapted to very different task settings where we have a pair of two arguments (i.e.entailment, question answering, etc.)"
p18-1076,2018,7 Conclusion and Future Work,"the attractiveness of our model lies in its transparency and flexibility: due to the attention mechanism, we can trace and analyze the facts considered in answering specific questions."
p18-1076,2018,7 Conclusion and Future Work,"this opens up for deeper investigation and future improvement of rc models in a targeted way, allowing us to investigate what knowledge sources are required for different data sets and domains."
p18-1076,2018,7 Conclusion and Future Work,"we propose a neural cloze-style reading comprehension model that incorporates external commonsense knowledge, building on a single-turn neural model."
p18-1076,2018,7 Conclusion and Future Work,"we provide quantitative and qualitative evidence of the effectiveness of our model, that learns how to select relevant knowledge to improve rc."
p18-1076,2018,7 Conclusion and Future Work,we show that the types of knowledge contained in conceptnet are useful.
p18-1077,2018,6 Conclusion,another contribution of our work is a thorough set of experiments and analysis of different types of endto-end architectures for qa at their ability to answer multi-relational questions of varying degrees of compositionality.
p18-1077,2018,6 Conclusion,"in this work, we have taken the first steps towards the task of multi-relational question answering expressed through personal narrative."
p18-1077,2018,6 Conclusion,one of our main contributions is a collection of diverse datasets that feature rich compositional questions over a dynamic knowledge graph expressed through simulated narrative.
p18-1077,2018,6 Conclusion,our hypothesis is that this task will become increasingly important as users begin to teach personal knowledge about their world to the personal assistants embedded in their devices.
p18-1077,2018,6 Conclusion,our long-term goal is that both the data and the simulation code we release will inspire and motivate the community to look towards the vision of letting end-users teach our personal assistants about the world around us.
p18-1077,2018,6 Conclusion,this task naturally synthesizes two main branches of question answering research: qa over kbs and qa over free text.
p18-1078,2018,7 Conclusion,"as shown by our demo, this work can be directly applied to building deep-learningpowered open question answering systems."
p18-1078,2018,7 Conclusion,"combining this with our suggestions for paragraph selection, using the summed training objective, and our model design allows us to advance the state of the art on triviaqa."
p18-1078,2018,7 Conclusion,"we have shown that, when using a paragraph-level qa model across multiple paragraphs, our training method of sampling non-answer-containing paragraphs while using a shared-norm objective function can be very beneficial."
p18-1079,2018,7 Limitations and Future Work,a building mountains verb what are the planes parked on?
p18-1079,2018,7 Limitations and Future Work,better bug fixing: our data augmentation has the human users accept/reject rules based on whether or not they preserve semantics.
p18-1079,2018,7 Limitations and Future Work,"concrete landing strip table 7: sears for vqa that are rejected by users other paraphrase limitations: paraphrase models based on neural machine translation are biased towards maintaining the sentence structure, and thus do not produce certain adversaries (e.g."
p18-1079,2018,7 Limitations and Future Work,"developing more effective ways of leveraging the expert鈥檚 time to close the loop, and facilitating more interactive collaboration between humans and sears are exciting areas for future work."
p18-1079,2018,7 Limitations and Future Work,"having demonstrated the usefulness of seas and sears in a variety of domains, we now describe their limitations and opportunities for future work."
p18-1079,2018,7 Limitations and Future Work,"more critically, existing models are inaccurate for long texts, restricting seas and sears to sentences."
p18-1079,2018,7 Limitations and Future Work,"semantic scoring errors: paraphrasing is still an active area of research, and thus our semantic scorer is sometimes incorrect in evaluating rules for semantic equivalence."
p18-1079,2018,7 Limitations and Future Work,"table 5b), which recent work on paraphrasing (iyyer et al., 2018) or generation using gans (zhao et al., 2018) may address."
p18-1079,2018,7 Limitations and Future Work,the presence of such errors is why we still need humans in the loop to accept or reject sears.
p18-1079,2018,8 Conclusion,we also close the loop by proposing a simple data augmentation solution that greatly reduced oversensitivity while maintaining accuracy.
p18-1079,2018,8 Conclusion,"we demonstrated that seas and sears can be an invaluable tool for debugging nlp models, while indicating their current limitations and avenues for future work."
p18-1079,2018,8 Conclusion,"we introduced seas and sears 鈥 adversarial examples and rules that preserve semantics, while causing models to make mistakes."
p18-1079,2018,8 Conclusion,"we presented examples of such bugs discovered in state-of-theart models for various tasks, and demonstrated via user studies that non-experts and experts alike are much better at detecting local and global bugs in nlp models by using our methods."
p18-1079,2018,7 Limitations and Future Work,"we show examples of sears that are rejected by users in table 7 鈥 the semantic scorer does not sufficiently penalize preposition changes, and is biased towards common terms."
p18-1079,2018,7 Limitations and Future Work,what is on the background?
p18-1080,2018,7 Conclusion,in future work we intend to apply this technique to debiasing sentences and anonymization of author traits such as gender and age.
p18-1080,2018,7 Conclusion,"in particular, it would be interesting to back-translate through multiple target languages with a single source language (johnson et al., 2016)."
p18-1080,2018,7 Conclusion,"in the future work, we will also explore whether an enhanced back-translation by pivoting through several languages will learn better grounded latent meaning representations."
p18-1080,2018,7 Conclusion,in transfer of political slant and sentiment we outperform an off-the-shelf state-of-the-art baseline using a cross-aligned autoencoder.
p18-1080,2018,7 Conclusion,it depends on the task and the context of the utterance within its discourse.
p18-1080,2018,7 Conclusion,"measuring the separation of style from content is hard, even for humans."
p18-1080,2018,7 Conclusion,"our model also outperforms the baseline in all the experiments of fluency, and in the experiments for meaning preservation in generated sentences of gender and political slant."
p18-1080,2018,7 Conclusion,the political slant task is a novel task that we introduce.
p18-1080,2018,7 Conclusion,"this technique is suitable not just for style transfer, but for enforcing style, and removing style too."
p18-1080,2018,7 Conclusion,ultimately we must evaluate our style transfer within some down-stream task where our style transfer has its intended use but we achieve the same task completion criteria.
p18-1080,2018,7 Conclusion,we apply this technique to three different style transfer tasks.
p18-1080,2018,7 Conclusion,"we propose a novel approach to the task of style transfer with non-parallel text.8 we learn a latent content representation using machine translation techniques; this aids grounding the meaning of the sentences, as well as weakening the style attributes."
p18-1080,2018,7 Conclusion,"yet, we acknowledge that the generated sentences do not always adequately preserve meaning."
p18-1081,2018,5 Conclusion,"generating them from facts in a knowledge graph requires not only mapping the structured fact information to natural language, but also identifying the type of entity and then discerning the most crucial pieces of information for that particular type from the long list of input facts and compressing them down to a highly succinct form."
p18-1081,2018,5 Conclusion,"in future work, we hope to explore the potential of this architecture on further kinds of data, including multimodal data (long et al., 2018), from which one can extract structured signals."
p18-1081,2018,5 Conclusion,our code and data is freely available.
p18-1081,2018,5 Conclusion,short textual descriptions of entities facilitate instantaneous grasping of key information about entities and their types.
p18-1081,2018,5 Conclusion,this is very challenging in light of the very heterogeneous kinds of entities in our data.
p18-1081,2018,5 Conclusion,"to this end, we have introduced a novel dynamic memory-based neural architecture that updates its memory at each step to continually reassess the relevance of potential input signals."
p18-1081,2018,5 Conclusion,we have shown that our approach outperforms several competitive baselines.
p18-1082,2018,8 Conclusion,"building on this dataset, we show through automatic and human evaluation that novel hierarchical models, self-attention mechanisms and model fusion significantly improves the fluency, topicality, and overall quality of the generated stories."
p18-1082,2018,8 Conclusion,this new dataset pushes the boundaries of text generation by requiring longer range dependencies and conditioning on an abstract premise.
p18-1082,2018,8 Conclusion,we have collected the first dataset for creative text generation based on short writing prompts.
p18-1083,2018,5 Conclusion,"in this paper, we not only introduce a novel adversarial reward learning algorithm to generate more human-like stories given image sequences, but also empirically analyze the limitations of the automatic metrics for story evaluation."
p18-1083,2018,5 Conclusion,"we believe there are still lots of improvement space in the narrative paragraph generation tasks, like how to better simulate human imagination to create more vivid and diversified stories."
p18-1084,2018,8 Conclusion and Future Work,"crucially, our proposed solution does not require access to images at inference/test time, in line with the realistic scenario where images that describe sentential queries are not readily available."
p18-1084,2018,8 Conclusion and Future Work,"first, we proposed to use the partial cca (pcca) method."
p18-1084,2018,8 Conclusion and Future Work,"in addition, we proposed a stochastic optimization algorithm for the deep version of pcca that overcomes the challenges posed by the covariance estimation required by the method."
p18-1084,2018,8 Conclusion and Future Work,in future work we plan to improve our methods by exploiting the internal structure of images and sentences as well as by effectively integrating signals from more than two languages.
p18-1084,2018,8 Conclusion and Future Work,our experiments reveal the effectiveness of these methods for both sentence-level and wordlevel tasks.
p18-1084,2018,8 Conclusion and Future Work,our main contribution is two-fold.
p18-1084,2018,8 Conclusion and Future Work,we addressed the problem of utilizing images as a bridge between languages to learn improved bilingual text representations.
p18-1085,2018,5 Conclusion,"in future work, we would like to explore other aspects of search engines for language grounding as well as the effect these embeddings may have on learning generic sentence representations (kiros et al., 2015b; hill et al., 2016; conneau et al., 2017a; logeswaran and lee, 2018)."
p18-1085,2018,5 Conclusion,in this work we demonstrated that picturebook complements traditional embeddings on a wide variety of tasks.
p18-1085,2018,5 Conclusion,picturebook embeddings offer an alternative approach to constructing word representations grounded in image search engines.
p18-1085,2018,5 Conclusion,"recently, contextualized word representations have shown promising improvements when combined with existing embeddings (melamud et al., 2016; peters et al., 2017; mccann et al., 2017; peters et al., 2018)."
p18-1085,2018,5 Conclusion,"through the use of multimodal gating, our models lead to interpretable weightings of abstract vs concrete words."
p18-1085,2018,5 Conclusion,"traditionally, word representations have been built on co-occurrences of neighbouring words; and such representations only make use of the statistics of the text distribution."
p18-1085,2018,5 Conclusion,we expect that integrating picturebook with these embeddings to lead to further performance improvements as well.
p18-1086,2018,6 Discussion and Conclusion,all of these point to the importance of the ability to understand causal relations between actions and the state of the world.
p18-1086,2018,6 Discussion and Conclusion,"nevertheless, we hope this work can motivate more research in this area, enabling physical action-effect reasoning, towards agents which can perceive, act, and communicate with humans in the physical world."
p18-1086,2018,6 Discussion and Conclusion,our current model is very simple and performance is yet to be improved.
p18-1086,2018,6 Discussion and Conclusion,"particularly, we focus on modeling the connection between an action (a verb-noun pair) and its effect as illustrated in an image and treat natural language effect descriptions as side knowledge to help acquiring web image data and bootstrap training."
p18-1086,2018,6 Discussion and Conclusion,"there are many challenges and unknowns, from problem formulation to knowledge representation; from learning and inference algorithms to methods and metrics for evaluations."
p18-1086,2018,6 Discussion and Conclusion,"they need to understand the current state, to map their goals to the world state, and to plan for actions that can lead to the goals."
p18-1086,2018,6 Discussion and Conclusion,this paper presents an initial investigation on action-effect prediction.
p18-1086,2018,6 Discussion and Conclusion,"to address this issue, this paper introduces a new task on action-effect prediction."
p18-1086,2018,6 Discussion and Conclusion,"we also plan to incorporate action-effect prediction to humanrobot collaboration, for example, to bridge the gap of commonsense knowledge about the physical world between humans and robots."
p18-1086,2018,6 Discussion and Conclusion,"we plan to apply more advanced approaches in the future, for example, attention models that jointly capture actions, image states, and effect descriptions."
p18-1086,2018,6 Discussion and Conclusion,"when robots operate in the physical world, they not only need to perceive the world, but also need to act to the world."
p18-1087,2018,5 Conclusions,"moreover, we employ cnn as the feature extractor for this classification problem, and rely on the contextpreserving and position relevance mechanisms to maintain the advantages of previous lstm-based models."
p18-1087,2018,5 Conclusions,our tnet model is carefully designed to solve these issues.
p18-1087,2018,5 Conclusions,"specifically, we propose target specific transformation component to better integrate target information into the word representation."
p18-1087,2018,5 Conclusions,"the ablation studies show the efficacy of its different modules, and thus verify the rationality of tnet鈥檚 architecture."
p18-1087,2018,5 Conclusions,the performance of tnet consistently dominates previous state-of-the-art methods on different types of data.
p18-1087,2018,5 Conclusions,"we re-examine the drawbacks of attention mechanism for target sentiment classification, and also investigate the obstacles that hinder cnn-based models to perform well for this task."
p18-1088,2018,7 Conclusion and Future Work,"after that, we discussed the basic memory network for asc and analyzed the reason why it is incapable of capturing such sentiment from a theoretical perspective."
p18-1088,2018,7 Conclusion and Future Work,"finally, we reported the experimental results quantitatively and qualitatively to show their effectiveness."
p18-1088,2018,7 Conclusion and Future Work,"in this paper, we first introduced the targetsensitive sentiment problem in asc."
p18-1088,2018,7 Conclusion and Future Work,"since asc is a fine-grained and complex task, there are many other directions that can be further explored, like handling sentiment negation, better embedding for multi-word phrase, analyzing sentiment composition, and learning better attention."
p18-1088,2018,7 Conclusion and Future Work,"the work presented in this paper lies in the direction of addressing target-sensitive sentiment, and we have demonstrated the usefulness of capturing this signal."
p18-1088,2018,7 Conclusion and Future Work,we believe all these can help improve the asc task.
p18-1088,2018,7 Conclusion and Future Work,we believe that there will be more effective solutions coming in the near future.
p18-1088,2018,7 Conclusion and Future Work,we then presented six techniques to construct target-sensitive memory networks.
p18-1089,2018,7 Conclusion,"essentially, a set of less erroneous transferable features leads to a more accurate classification system in the unlabeled target domain."
p18-1089,2018,7 Conclusion,"furthermore, we show that an ensemble of the classifiers trained on the scp features and target specific features overcomes the errors of the individual classifiers."
p18-1089,2018,7 Conclusion,"in this paper, we proposed that the significant consistent polarity (scp) words represent the transferable information from the labeled source domain to the unlabeled target domain for crossdomain sentiment classification."
p18-1089,2018,7 Conclusion,results show that the scp words given by our approach represent more accurate transferable information in comparison to the structured correspondence learning (scl) algorithm and common-unigrams.
p18-1089,2018,7 Conclusion,we have presented a technique based test and cosine-similarity between context vector of words to identify scp words.
p18-1089,2018,7 Conclusion,we showed a strong positive correlation of 0.78 between the scp words identified by our approach and the sentiment classification accuracy achieved in the unlabeled target domain.
p18-1090,2018,5 Conclusions and Future Work,"experimental results show that our method substantially outperforms the state-of-the-art systems, especially in terms of semantic preservation."
p18-1090,2018,5 Conclusions and Future Work,"for future work, we would like to explore a fine-grained version of sentiment-to-sentiment translation that not only reverses sentiment, but also changes the strength of sentiment."
p18-1090,2018,5 Conclusions and Future Work,"in this paper, we focus on unpaired sentimentto-sentiment translation and propose a cycled reinforcement learning approach that enables training in the absence of parallel training data."
p18-1090,2018,5 Conclusions and Future Work,we conduct experiments on two review datasets.
p18-1091,2018,7 Conclusion,future works involve the choice of discourse markers and some other transfer learning sources.
p18-1091,2018,7 Conclusion,"in this paper, we propose discourse marker augmented network for the task of the natural language inference."
p18-1091,2018,7 Conclusion,"moreover, we take the various views of the annotators into consideration and employ reinforcement learning to help optimize the model."
p18-1091,2018,7 Conclusion,the experimental evaluation shows that our model achieves the state-of-the-art results on snli and multinli datasets.
p18-1091,2018,7 Conclusion,we transfer the knowledge learned from the discourse marker prediction task to the nli task to augment the semantic representation of the model.
p18-1092,2018,5 Conclusion,"also, we showed that our model can be easily adapted for visual question answering."
p18-1092,2018,5 Conclusion,"although we have used rn as the reasoning module in this work, other options can be tested."
p18-1092,2018,5 Conclusion,evidence from cognitive sciences seems to show that all these abilities are needed in order to achieve human-level complex reasoning.
p18-1092,2018,5 Conclusion,it might be interesting to analyze how other reasoning modules can improve different weaknesses of the model.
p18-1092,2018,5 Conclusion,"our architecture combines perceptual input processing, short-term memory storage, an attention mechanism, and a reasoning module."
p18-1092,2018,5 Conclusion,"that opens the opportunity for using rns in larger problems, something that may be very useful, given the many tasks requiring a significant amount of memories."
p18-1092,2018,5 Conclusion,"we demonstrated that by augmenting the memnn architecture with a relation network, the computational complexity of the rn can be reduced, without loss of performance."
p18-1092,2018,5 Conclusion,we have proposed a novel working memory network architecture that introduces improved reasoning abilities to the original memnn model.
p18-1092,2018,5 Conclusion,"we presented results on the jointly trained babi10k dataset, where we achieve a new state-of-theart, with an average error of less than 0.5%."
p18-1092,2018,5 Conclusion,"while other models have focused on different parts of these components, we think that is important to find ways to combine these different mechanisms if we want to build models capable of complex reasoning."
p18-1093,2018,5 Conclusion,"analysis of the intra-attention scores shows that our model learns highly interpretable attention weights, paving the way for more explainable neural sarcasm detection methods."
p18-1093,2018,5 Conclusion,"based on the intuition of intra-sentence similarity (i.e., looking in-between), we proposed a new neural network architecture for sarcasm detection."
p18-1093,2018,5 Conclusion,extensive experiments over six public benchmarks confirm the empirical effectiveness of our proposed model.
p18-1093,2018,5 Conclusion,"our network incorporates a multi-dimensional intra-attention component that learns an intraattentive representation of the sentence, enabling it to detect contrastive sentiment, situations and incongruity."
p18-1093,2018,5 Conclusion,our proposed miarn model outperforms strong state-of-the-art baselines such as grnn and cnn-lstm-dnn.
p18-1094,2018,7 Conclusion,"finally, we find that controlling the entropy of the generator through a regularization term and properly handling false negatives is crucial for successful training."
p18-1094,2018,7 Conclusion,"in this paper, we propose adversarial contrastive estimation as a general technique for improving supervised learning problems that learn by contrasting observed and fictitious samples."
p18-1094,2018,7 Conclusion,"specifically, we use a generator network in a conditional gan like setting to propose hard negative examples for our discriminator model."
p18-1094,2018,7 Conclusion,we find that a mixture distribution of randomly sampling negative examples along with an adaptive negative sampler leads to improved performances on a variety of embedding tasks.
p18-1094,2018,7 Conclusion,we validate our hypothesis that hard negative examples are critical to optimal learning and can be proposed via our ace framework.
p18-1095,2018,6 Conclusions,"based on the marginal utility theory framework, our method leads to more effective, stable and transferable optimization of neural networks without introducing additional hyper-parameters."
p18-1095,2018,6 Conclusions,experiments on event detection verified the effectiveness and stability of our adaptive scaling algorithm.
p18-1095,2018,6 Conclusions,"in the future we want to apply our marginal utility based framework to other metrics, such as mean average precision (map)."
p18-1095,2018,6 Conclusions,the divergence between loss functions and evaluation metrics is common in nlp and machine learning.
p18-1095,2018,6 Conclusions,"this paper proposes adaptive scaling algorithm for detection tasks, which can deal with its positive sparsity problem and directly optimize f-measure by adaptively scaling the influence of negative instances in loss function."
p18-1096,2018,5 Conclusions,"for pos tagging, classic tri-training is superior, performing especially well on oovs and low frequency tokens, which suggests it is less affected by error propagation."
p18-1096,2018,5 Conclusions,for the two examined nlp tasks classic tri-training works the best and even outperforms a recent state-of-the-art method.
p18-1096,2018,5 Conclusions,overall we emphasize the importance of comparing neural approaches to strong baselines and reporting results across several runs.
p18-1096,2018,5 Conclusions,the drawback of tri-training it its time and space complexity.
p18-1096,2018,5 Conclusions,we re-evaluate a range of traditional generalpurpose bootstrapping algorithms in the context of neural network approaches to semi-supervised learning under domain shift.
p18-1096,2018,5 Conclusions,"we therefore propose a more efficient multi-task tri-training model, which outperforms both traditional tri-training and recent alternatives in the case of sentiment analysis."
p18-1097,2018,7 Conclusion,"our proposed fluency boost learning fully exploits both errorcorrected data and native data, largely improving the performance over normal seq2seq learning, while fluency boost inference utilizes the characteristic of gec to incrementally improve a sentence鈥檚 fluency through multi-round inference."
p18-1097,2018,7 Conclusion,the powerful learning and inference mechanism enables the seq2seq models to achieve state-ofthe-art results on both conll-2014 and jfleg benchmark datasets.
p18-1097,2018,7 Conclusion,we propose a novel fluency boost learning and inference mechanism to overcome the limitations of previous neural gec models.
p18-1098,2018,6 Conclusions and Discussions,a full study is ongoing.
p18-1098,2018,6 Conclusions and Discussions,adversarial learning improves the matching accuracy by alleviating the discrepancy among the writing styles of dds and cds.
p18-1098,2018,6 Conclusions and Discussions,"at present, the major limitations of this work include: (1) it does not perform well on infrequent codes; (2) it is less capable of dealing with abbreviations."
p18-1098,2018,6 Conclusions and Discussions,evaluations on the mimic-iii dataset demonstrate the following.
p18-1098,2018,6 Conclusions and Discussions,"first, the tree-of-sequences lstm network effectively discourages the co-selection of sibling codes and promotes the co-assignment of clinicallyrelevant codes."
p18-1098,2018,6 Conclusions and Discussions,"for example, in tweets, we can use the method to map an informally written mention 鈥榥bcbightlynews鈥 to a canonical entity 鈥楴bc nightly news鈥 in the knowledge base."
p18-1098,2018,6 Conclusions and Discussions,"fourth, the attentional matching mechanism is able to perform many-to-one and one-tomany mappings."
p18-1098,2018,6 Conclusions and Discussions,"in the coding practice of human coders, in addition to the diagnosis descriptions, other information contained in nursing notes, lab values, and medical procedures are also leveraged for code assignment."
p18-1098,2018,6 Conclusions and Discussions,"in this paper, we build a neural network model for automated icd coding."
p18-1098,2018,6 Conclusions and Discussions,it takes the textual descriptions of concepts in the ontology and their hierarchical structure as inputs and produces a latent representation for each concept.
p18-1098,2018,6 Conclusions and Discussions,the proposed adversarial reconciliation of writing styles and attentional matching can be applied for knowledge mapping or entity linking.
p18-1098,2018,6 Conclusions and Discussions,the proposed methods can be applied to other tasks in nlp.
p18-1098,2018,6 Conclusions and Discussions,the representations can simultaneously capture the semantics of codes and their relationships.
p18-1098,2018,6 Conclusions and Discussions,the sensitivity is improved from 0.29 to 0.32 and the specificity is improved from 0.33 to 0.35.
p18-1098,2018,6 Conclusions and Discussions,the tree-of-sequences model can be applied for ontology annotation.
p18-1098,2018,6 Conclusions and Discussions,"third, isotonic constraints promote the correct ranking of codes."
p18-1098,2018,6 Conclusions and Discussions,we have initiated preliminary investigation along this line and added two new input sources: (1) the rest of discharge summary and (2) lab values.
p18-1098,2018,6 Conclusions and Discussions,"we will address these two issues in future by investigating diversity-promoting regularization (xie et al., 2017) and leveraging an external knowledge base that maps medical abbreviations into their full names."
p18-1099,2018,6 Conclusions,"for domain adaptation, we considered a scenario, where we have only unlabeled data in the target event."
p18-1099,2018,6 Conclusions,"in this paper, we presented a deep learning framework that performs domain adaptation with adversarial training and graph-based semi-supervised learning to leverage labeled and unlabeled data from related events."
p18-1099,2018,6 Conclusions,"our evaluation on two crisis-related tweet datasets demonstrates that by combining domain adversarial training with semi-supervised learning, our model gives significant improvements over their respective baselines."
p18-1099,2018,6 Conclusions,we have also presented results of batch-wise incremental training of the graph-based semi-supervised approach and show approximation regarding the number of labeled examples required to get an acceptable performance at the onset of an event.
p18-1099,2018,6 Conclusions,"we use a convolutional neural network to compose high-level representation from the input, which is then passed to three components that perform supervised training, semisupervised learning and domain adversarial training."
p18-1100,2018,6 Conclusions & Future Work,"as demonstrated in the experiments, two kinds of established prompt-dependent aes models, namely, ranksvm for aes (yannakoudakis et al., 2011; chen et al., 2014) and the deep models for aes (alikaniotis et al., 2016; taghipour and ng, 2016; dong et al., 2017), fail to provide satisfactory performances, justifying our arguments in section 1 that the application of established prompt-dependent aes models on promptindependent aes is not straightforward."
p18-1100,2018,6 Conclusions & Future Work,further study of the uses of transfer learning algorithms on promptindependent aes needs to be undertaken.
p18-1100,2018,6 Conclusions & Future Work,"given that our approach in this paper is similar to the methods for transductive transfer learning (pan and yang, 2010), we argue that the proposed tdnn could be further improved by migrating the non-target training data to the target prompt (busto and gall, 2017)."
p18-1100,2018,6 Conclusions & Future Work,"therefore, a two-stage tdnn learning framework was proposed to utilize the prompt-independent features to generate pseudo training data for the target prompt, on which a hybrid deep neural network model is proposed to learn a rating model consuming semantic, part-of-speech, and syntactic signals."
p18-1100,2018,6 Conclusions & Future Work,"this study aims at addressing the promptindependent automated essay scoring (aes), where no rated essay for the target prompt is available."
p18-1100,2018,6 Conclusions & Future Work,"through the experiments on the asap dataset, the proposed tdnn model outperforms the baselines, and leads to promising improvement in the human-machine agreement."
p18-1101,2018,5 Conclusion and Future Work,experiments show the proposed methods outperform strong baselines in learning discrete latent variables and showcase the effectiveness of interpretable dialog response generation.
p18-1101,2018,5 Conclusion and Future Work,"our findings also suggest promising future research directions, including learning better context-based latent actions and using reinforcement learning to adapt policy networks."
p18-1101,2018,5 Conclusion and Future Work,"our main contributions reside in the two sentence representation models di-vae and divst, and their integration with the encoder decoder models."
p18-1101,2018,5 Conclusion and Future Work,this paper presents a novel unsupervised framework that enables the discovery of discrete latent actions and interpretable dialog response generation.
p18-1101,2018,5 Conclusion and Future Work,we believe that this work is an important step forward towards creating generative dialog models that can not only generalize to large unlabelled datasets in complex domains but also be explainable to human users.
p18-1102,2018,5 Conclusion,"empirical results showed that our model can generate either general or specific responses, and significantly outperform state-of-the-art generation methods."
p18-1102,2018,5 Conclusion,"we introduce an explicit specificity control variable into the seq2seq model, which interacts with the usage representation of words to generate responses at different specificity levels."
p18-1102,2018,5 Conclusion,we propose a novel controlled response generation mechanism to handle different utteranceresponse relationships in terms of specificity.
p18-1103,2018,6 Conclusion,empirical results on two large-scale datasets demonstrate the effectiveness of self-attention and cross-attention in multi-turn response selection.
p18-1103,2018,6 Conclusion,"in this paper, we investigate matching a response with its multi-turn context using dependency information based entirely on attention."
p18-1103,2018,6 Conclusion,our solution extends the attention mechanism of transformer in two ways: (1) using stacked selfattention to harvest multi-grained semantic representations.(2) utilizing cross-attention to match with dependency information.
p18-1103,2018,6 Conclusion,"we believe that both self-attention and cross-attention could benefit other research area, including spoken language understanding, dialogue state tracking or seq2seq dialogue generation."
p18-1103,2018,6 Conclusion,we would like to explore in depth how attention can help improve neural dialogue modeling for both chatbots and taskoriented dialogue systems in our future work.
p18-1104,2018,6 Conclusion and Future Work,"due to the nature of social media text, some emotions, such as fear and disgust, are underrepresented in the dataset, and the distribution of emojis is unbalanced to some extent."
p18-1104,2018,6 Conclusion and Future Work,"experimentally, it is shown that our model is capable of generating high-quality emotional responses, without the need of laborious human annotations."
p18-1104,2018,6 Conclusion and Future Work,"in this paper, we investigate the possibility of using naturally annotated emoji-rich twitter data for emotional response generation."
p18-1104,2018,6 Conclusion and Future Work,"more specifically, we collected more than half a million twitter conversations with emoji in the response and assumed that the fine-grained emoji label chosen by the user expresses the emotion of the tweet."
p18-1104,2018,6 Conclusion and Future Work,our work is a crucial step towards building intelligent dialog agents.
p18-1104,2018,6 Conclusion and Future Work,we applied several state-of-the-art neural models to learn a generation system that is capable of giving a response with an arbitrarily designated emotion.
p18-1104,2018,6 Conclusion and Future Work,we are also looking forward to transferring the idea of naturally-labeled emojis to task-oriented dialog and multi-turn dialog generation problems.
p18-1104,2018,6 Conclusion and Future Work,"we performed an amazon mechanical turk experiment, by which we compared our models with a baseline sequence-to-sequence model on metrics of relevance and emotion."
p18-1104,2018,6 Conclusion and Future Work,we performed automatic and human evaluations to understand the quality of generated responses.
p18-1104,2018,6 Conclusion and Future Work,we trained a large scale emoji classifier and ran the classifier on the generated responses to evaluate the emotion accuracy of the generated response.
p18-1104,2018,6 Conclusion and Future Work,"we will keep accumulating data and increase the ratio of underrepresented emojis, and advance toward more sophisticated abstractive generation methods."
p18-1105,2018,6 Conclusion,another direction will be to apply fluctuation analysis in formulating a statistical test to evaluate the structural complexity underlying a sequence.
p18-1105,2018,6 Conclusion,"in our method, a sequence of words is divided into given segments, and the mean and standard deviation of the frequency of every kind of word are measured."
p18-1105,2018,6 Conclusion,none of the real data exhibited an exponent equal to 0.5.
p18-1105,2018,6 Conclusion,"our future work will include an analysis using other kinds of data, such as twitter data and adult utterances, and a study of how taylor鈥檚 law relates to grammatical complexity for different sequences."
p18-1105,2018,6 Conclusion,"our transformation preserves all of the alignments generated by scfg, and retains properties such as o(n3k) parsing complexity for grammars of rank k."
p18-1105,2018,6 Conclusion,taylor鈥檚 law and its exponent can also be applied to evaluate machine-generated text.
p18-1105,2018,6 Conclusion,"the law is considered to hold when the standard deviation varies with the mean according to a power law, thus giving the taylor exponent."
p18-1105,2018,6 Conclusion,"theoretically, an i.i.d.process has a taylor exponent of 0.5, whereas larger exponents indicate sequences in which words co-occur systematically."
p18-1105,2018,6 Conclusion,these taylor exponents imply that a written text is more complex than programming source code or music with regard to fluctuation of its components.
p18-1105,2018,6 Conclusion,this indicates one limitation of that model.
p18-1105,2018,6 Conclusion,"this value differed greatly from the exponents for other data sources: enwiki8 (tagged wikipedia, 0.63), child-directed speech (childes, around 0.68), and programming language and music data (around 0.79)."
p18-1105,2018,6 Conclusion,"using over 1100 texts across 14 languages, we showed that written natural language texts follow taylor鈥檚 law, with the exponent distributed around 0.58."
p18-1105,2018,6 Conclusion,we conducted more detailed analysis varying the data size and the segment size.
p18-1105,2018,6 Conclusion,"we have proposed a method to analyze whether a natural language text follows taylor鈥檚 law, a scaling property quantifying the degree of consistent co-occurrence among words."
p18-1105,2018,6 Conclusion,we showed that a character-based lstm language model generated text with a taylor exponent of 0.5.
p18-1106,2018,5 Discussion,"as such, it is an invaluable complement to those more traditional methodologies."
p18-1106,2018,5 Discussion,"finally, since this framework reduces to niyogi & berwick鈥檚 models in perfectly-mixed populations, it can be used to reason about the formal dynamics of language change as well."
p18-1106,2018,5 Discussion,"if learners are conceived as categorical learners, population size becomes a deciding factor in the path of change."
p18-1106,2018,5 Discussion,"in addition to the core algorithm, the framework offers enough flexibility to represent a wide variety of processes from the highly abstract (e.g., kauhanen (2016)) to those grounded in sociolinguistic and acquisition research (e.g., yang (2009))."
p18-1106,2018,5 Discussion,"in our investigation of kauhanen鈥檚 basic assumptions, we discover how seemingly innocuous decisions about population size and learning conspire to drive simulation results."
p18-1106,2018,5 Discussion,"in our simulation of the spread of the cotcaught merger, we show how a cognitivelymotivated model of acquisition requires a network model in order to represent population-level language change."
p18-1106,2018,5 Discussion,it follows the niyogi and berwick (1996) formalism for language change which presents a clean and modular way of reasoning about the problem and promotes the centrality of language acquisition.
p18-1106,2018,5 Discussion,it is much simpler computationally than previous frameworks because it calculates the statistically expected behavior of each generation analytically and therefore removes the entire inner loop of calculating stochastic inter-agent interactions from the simulation.
p18-1106,2018,5 Discussion,neither factor alone can account for the theoretical or empirically observed patterns.
p18-1106,2018,5 Discussion,"one problem that this line of simulation work has always faced has been the lack of viable comparison between models because every study implements its own learning, network, and interaction models."
p18-1106,2018,5 Discussion,simulations of this kind which explicitly model both simultaneously is well equipped to provide insights that fieldwork and laboratory work cannot.
p18-1106,2018,5 Discussion,"so while the original results are interesting and meaningful, they may only valid for small (on the order of 102 ) populations."
p18-1106,2018,5 Discussion,the algebraic-network framework for modeling population-level language change presented here has substantial practical and theoretical advantages over previous ones.
p18-1106,2018,5 Discussion,the fact that s-curves arise naturally from these networks underscores their centrality to language change.
p18-1106,2018,5 Discussion,the modular nature of our framework advances against this trend since it is now possible to hold the population model constant while slotting in various learning models to test them against one another and vice-versa.
p18-1106,2018,5 Discussion,"the population is represented as a collection of individual clusters based on sociological work, but the clusters themselves are connected randomly."
p18-1106,2018,5 Discussion,"without simulation, it would be difficult or impossible to undercover the interplay between acquisition and social structure on the propagation of language change."
p18-1107,2018,8 Conclusion and Future Work,"like the original gnf transformation for cfgs our construction at most cubes the grammar size, though when applied to the kinds of synchronous grammars used in machine translation the size is merely squared."
p18-1107,2018,8 Conclusion and Future Work,this process is applicable to any scfg which is 蔚- and chain-free.
p18-1107,2018,8 Conclusion and Future Work,we further plan to empirically evaluate our lexicalization on an alignment task and to offer a comparison against the lexicalization due to zhang and gildea (2005).
p18-1107,2018,8 Conclusion and Future Work,we have demonstrated a method for prefix lexicalizing an scfg by converting it to an equivalent stag.
p18-1107,2018,8 Conclusion and Future Work,"we plan to verify whether rank-k pl-rstag is more powerful than rank-k scfg in future work, and to reduce the rank of the transformed grammar if possible."
p18-1108,2018,6 Conclusion,one peculiar aspect of our model is that it predicts split decisions in parallel.
p18-1108,2018,6 Conclusion,"our experiments show that our model can achieve strong performance compare to previous models, while being significantly more efficient."
p18-1108,2018,6 Conclusion,"since the architecture of model is no more than a stack of standard recurrent and convolution layers, which are essential components in most academic and industrial deep learning frameworks, the deployment of this method would be straightforward."
p18-1108,2018,6 Conclusion,"we employ a neural network model that predicts the distances d and the constituent labels c. given the algorithms presented in section 2, we can build an unambiguous mapping between each (d, c, t) and a parse tree."
p18-1108,2018,6 Conclusion,"we presented a novel constituency parsing scheme based on predicting real-valued scalars, named syntactic distances, whose ordering identify the sequence of top-down split decisions."
p18-1109,2018,6 Conclusion,"for each production rule, a lveg defines a continuous weight function over the subtypes of the nonterminals involved in the rule."
p18-1109,2018,6 Conclusion,"the partition function and the expectations of fine-grained production rules in gm-lvegs can be efficiently computed using dynamic programming, which makes learning and inference with gm-lvegs feasible."
p18-1109,2018,6 Conclusion,we empirically show that gm-lvegs can achieve competitive accuracies on pos tagging and constituency parsing.
p18-1109,2018,6 Conclusion,we present latent vector grammars (lvegs) that associate each nonterminal with a latent continuous vector space representing the set of subtypes of the nonterminal.
p18-1109,2018,6 Conclusion,we show that lvegs can subsume latent variable grammars and compositional vector grammars as special cases.
p18-1109,2018,6 Conclusion,we then propose gaussian mixture lvegs (gm-lvegs).which formulate weight functions of production rules by mixtures of gaussian distributions.
p18-1110,2018,7 Conclusion,"not only do contextualized word representations help parsers learn the syntax of new domains with very few examples, but they also work extremely well with parsing models that correspond directly with a granular and intuitive annotation task (like identifying whether a span is a constituent)."
p18-1110,2018,7 Conclusion,recent developments in neural natural language processing have made it very easy to build custom parsers.
p18-1110,2018,7 Conclusion,this allows you to train with either full or partial annotations without any change to the training process.
p18-1110,2018,7 Conclusion,"this work provides a convenient path forward for the researcher who requires a parser for their domain, but laments that 鈥減arsers don鈥檛 work outside of newswire.鈥 with a couple hours of effort (and a layman鈥檚 understanding of syntactic building blocks), they can get significant performance improvements."
p18-1110,2018,7 Conclusion,"we envision an iterative use case in which a user assesses a parser鈥檚 errors on their target domain, creates some partial annotations to teach the parser how to fix these errors, then retrains the parser, repeating the process until they are satisfied."
p18-1111,2018,7 Conclusion,"in the future, we plan to take generalization one step further, and explore the possibility to use the bilstm for generating completely new paraphrase templates unseen during training."
p18-1111,2018,7 Conclusion,"the model differs from previous models by being trained to predict both a paraphrase given a noun-compound, and a missing constituent given the paraphrase and the other constituent."
p18-1111,2018,7 Conclusion,"this results in better generalization abilities, leading to improved performance in two noun-compound interpretation tasks."
p18-1111,2018,7 Conclusion,we presented a new semi-supervised model for noun-compound paraphrasing.
p18-1112,2018,7 Conclusion,"identifying the presence of sentiment words as one key factor for the difference, we propose a novel method sentivec to train word embeddings that are infused with the sentiment polarity of words derived from a separate sentiment lexicon."
p18-1112,2018,7 Conclusion,"the proposed word embeddings show improvements in sentiment classification, while maintaining their performance on subjectivity and topic classifications."
p18-1112,2018,7 Conclusion,"we explore the differences between objective and subjective corpora for generating word embeddings, and find that there is indeed a difference in the embeddings鈥 classification task performances."
p18-1112,2018,7 Conclusion,we further identify two lexical objectives: logistic sentivec and spherical sentivec.
p18-1113,2018,7 Conclusion,"additionally, our experiments demonstrate that using a candidate word output vector instead of its input vector to model the similarity between the candidate word and its context yields better results in the best fit word (the literal counterpart of the metaphor) identification."
p18-1113,2018,7 Conclusion,"cbow and skip-gram do not consider the distance between a context word and a centre word in a sentence, i.e., context word contributes to predict the centre word equally."
p18-1113,2018,7 Conclusion,future work will introduce weighted cbow and skip-gram to learn positional information within sentences.
p18-1113,2018,7 Conclusion,our model outperforms the unsupervised baselines in both sentence and phrase evaluations.
p18-1113,2018,7 Conclusion,"the experiments show that using words鈥 hypernyms and synonyms in wordnet can paraphrase metaphors into their literal counterparts, so that the metaphors can be correctly identified and translated."
p18-1113,2018,7 Conclusion,the interpretation of the identified metaphorical words given by our model also contributes to google and bing translation systems with 11% and 9% accuracy improvements.
p18-1113,2018,7 Conclusion,"to our knowledge, this is the first study that evaluates a metaphor processing method on machine translation."
p18-1113,2018,7 Conclusion,"we believe that compared with simply identifying metaphors, metaphor processing applied in practical tasks, can be more valuable in the real world."
p18-1113,2018,7 Conclusion,we proposed a framework that identifies and interprets metaphors at word-level with an unsupervised learning approach.
p18-1114,2018,6 Conclusion,"in the future, we intend to evaluate our models for some morpheme-rich languages like russian, german and so on."
p18-1114,2018,6 Conclusion,"in this paper, we explored a new direction to employ the latent meanings of morphological compositions rather than the internal compositions themselves to train word embeddings."
p18-1114,2018,6 Conclusion,"on the syntactic analogy as well as the text classification tasks, our models also surpass all the baselines including the emm."
p18-1114,2018,6 Conclusion,the experimental results demonstrate that our models outperform the baselines on five word similarity datasets.
p18-1114,2018,6 Conclusion,the source code of lmms is avaliable at https: //github.com/y-xu/lmm.
p18-1114,2018,6 Conclusion,"three specific models named lmm-a, lmm-s and lmm-m were proposed by modifying the input layer and update rules of cbow."
p18-1114,2018,6 Conclusion,"to test the performance of our models, we chose three word-level word embedding models and implemented an explicitly morpheme-related model (emm) as comparative baselines, and tested them on two basic nlp tasks of word similarity and syntactic analogy, and one downstream text classification task."
p18-1115,2018,7 Conclusion and Future Work,"a latent factor model such as darn (gregor et al., 2014) would consider several sources simultaneously."
p18-1115,2018,7 Conclusion and Future Work,"as this is the first work that systematically considers word-level variation in nmt, there are lots of research ideas to explore in the future."
p18-1115,2018,7 Conclusion and Future Work,"extension to other architectures: introducing latent variables into non-autoregressive translation models such as the transformer (vaswani et al., 2017) should increase their translation ability further."
p18-1115,2018,7 Conclusion and Future Work,"here, we list the three which we believe to be most promising.鈥 latent factor models: our model only contains one source of variation per word."
p18-1115,2018,7 Conclusion and Future Work,our experiments confirm our intuition that modelling variation is crucial to the success of machine translation.
p18-1115,2018,7 Conclusion and Future Work,"richer distributions computed by normalising flows (rezende and mohamed, 2015; kingma et al., 2016) will likely improve our model."
p18-1115,2018,7 Conclusion and Future Work,the proposed model consistently outperforms strong baselines on several language pairs.
p18-1115,2018,7 Conclusion and Future Work,this would also allow us to perform a better analysis of the model behaviour as we could correlate the factors with observed linguistic phenomena.鈥 richer prior and variational distributions: the diagonal gaussian is likely too simple a distribution to appropriately model the variation in our data.
p18-1115,2018,7 Conclusion and Future Work,we have presented a recurrent decoder for machine translation that uses word-level gaussian variables to model underlying sources of variation observed in translation corpora.
p18-1116,2018,6 Conclusion and future work,"as future work, we plan to design some more elaborate structures to incorporate the score layer into the encoder."
p18-1116,2018,6 Conclusion and future work,"compared with conventional string-to-string nmt systems and tree-to-string nmt systems, our framework can utilize exponentially many linearized parsing trees during encoding, without significantly decreasing the efficiency."
p18-1116,2018,6 Conclusion and future work,the experimental results demonstrate the effectiveness of our method.
p18-1116,2018,6 Conclusion and future work,this represents the first attempt to use a forest within the string-to-string nmt framework.
p18-1116,2018,6 Conclusion and future work,"we proposed a new encoding method for nmt, which encodes a packed forest for the source sentence using linear-structured neural networks, such as rnn."
p18-1116,2018,6 Conclusion and future work,we will also apply the proposed linearization method to other tasks.
p18-1116,2018,6 Conclusion and future work,"when introducing packed forest, we confirmed that the score of each edge is indispensable."
p18-1117,2018,7 Conclusions,"future work could also investigate whether context-aware nmt systems learn other discourse phenomena, for example whether they improve the translation of elliptical constructions, and markers of discourse relations and information structure."
p18-1117,2018,7 Conclusions,our analysis has focused on the effect of context information on pronoun translation.
p18-1117,2018,7 Conclusions,we also show that the model induces anaphora relations.
p18-1117,2018,7 Conclusions,"we believe that further improvements in handling anaphora, and by proxy translation, can be achieved by incorporating specialized features in the attention model."
p18-1117,2018,7 Conclusions,we introduced a context-aware nmt system which is based on the transformer architecture.
p18-1117,2018,7 Conclusions,we observe that improvements are especially prominent for sentences containing ambiguous pronouns.
p18-1117,2018,7 Conclusions,"when evaluated on an en-ru parallel corpus, it outperforms both the context-agnostic baselines and a simple context-aware baseline."
p18-1118,2018,7 Conclusion,"for future work, we intend to investigate models which incorporate specific discourse-level phenomena."
p18-1118,2018,7 Conclusion,our model augments the vanilla sentence-based nmt model with external memories to incorporate documental interdependencies on both source and target sides.
p18-1118,2018,7 Conclusion,we have proposed a document-level neural mt model that captures global source and target document context.
p18-1118,2018,7 Conclusion,we show statistically significant improvements of the translation quality on three language pairs.
p18-1119,2018,6 Conclusions and Future Work,"finally, we introduced geovirus, an open-source dataset that helps facilitate geoparsing evaluation across more diverse domains with different lexical-geographic distributions (flatow et al., 2015; dredze et al., 2016)."
p18-1119,2018,6 Conclusions and Future Work,future work may see our methods applied to document geolocation to assess the effectiveness of scaling geodesic vectors from paragraphs to entire documents.
p18-1119,2018,6 Conclusions and Future Work,"geocoding methods commonly employ lexical features, which have proved to be very effective."
p18-1119,2018,6 Conclusions and Future Work,"it is possible, however, to go beyond lexical semantics."
p18-1119,2018,6 Conclusions and Future Work,"locations also have a rich topological meaning, which has not yet been successfully isolated and deployed."
p18-1119,2018,6 Conclusions and Future Work,"mapvec remains effective with various machine learning frameworks (random forest, cnn and mlp) and substantially improves accuracy when combined with other neural models (lstms)."
p18-1119,2018,6 Conclusions and Future Work,our lexical model was the best languageonly geocoder in extensive tests.
p18-1119,2018,6 Conclusions and Future Work,"tasks that could benefit from our methods include social media placing tasks (choi et al., 2014), inferring user location on twitter (zheng et al., 2017), geolocation of images based on descriptions (serdyukov et al., 2009) and detecting/analyzing incidents from social media (berlingerio et al., 2013)."
p18-1119,2018,6 Conclusions and Future Work,"to that end, we introduced mapvec, an algorithm and a container for encoding context locations in geodesic vector space."
p18-1119,2018,6 Conclusions and Future Work,we need a means of extracting and encoding this additional knowledge.
p18-1119,2018,6 Conclusions and Future Work,"we showed how camcoder, using lexical and mapvec features, outperformed both approaches, achieving a new sota."
p18-1120,2018,5 Conclusions and Future Work,challenges also remain in how to evaluate the accuracy of goal knowledge extracted from text corpora.
p18-1120,2018,5 Conclusions and Future Work,"however, this problem is far from solved."
p18-1120,2018,5 Conclusions and Future Work,"in future work, we hope to see if we can take advantage of more contextual information as well as other external knowledge to improve the recognition of goalacts."
p18-1120,2018,5 Conclusions and Future Work,"nevertheless, our work represents a first step toward learning goal knowledge about locations, and we believe that learning knowledge about plans and goals is an important direction for natural language understanding research."
p18-1120,2018,5 Conclusions and Future Work,we demonstrated that our learning algorithm identifies goal-acts for locations more accurately than several baseline methods.
p18-1120,2018,5 Conclusions and Future Work,we introduced the problem of learning prototypical goal activities for locations.
p18-1120,2018,5 Conclusions and Future Work,we obtained human annotations and showed that people do associate prototypical goal-acts with locations.
p18-1120,2018,5 Conclusions and Future Work,we then created an activity profile framework and applied a semi-supervised label propagation algorithm to iteratively update the activity strengths for locations.
p18-1121,2018,8 Conclusions,"different from all the previous acronym disambiguation approaches, our system is capable of accurately resolving acronyms to both enterprise-specific meanings and public meanings."
p18-1121,2018,8 Conclusions,experimental results on microsoft enterprise data demonstrated that our system can effectively construct acronym/meaning repositories from scratch and accurately disambiguate acronyms to their meanings with over 90% precision.
p18-1121,2018,8 Conclusions,"furthermore, our proposed framework can be easily deployed to any enterprises without requiring any domain knowledge."
p18-1121,2018,8 Conclusions,"in this paper, we studied the acronym disambiguation for enterprises problem."
p18-1121,2018,8 Conclusions,our framework takes the enterprise corpus as input and produces a high-quality acronym disambiguation system as output.
p18-1121,2018,8 Conclusions,"the disambiguation models are trained via distant supervised learning, without requiring any manually labeled training examples."
p18-1121,2018,8 Conclusions,"we proposed a novel, end-to-end framework to solve this problem."
p18-1122,2018,7 Conclusion,"analysis shows that matres, albeit crowdsourced, has achieved a reasonably good agreement level, as confirmed by its performance on the gold set (agreement with the authors), the wawa metric (agreement with the crowdsourcers themselves), and consistency with tb-dense (agreement with an existing dataset)."
p18-1122,2018,7 Conclusion,"given the fact that existing schemes suffer from low iaas and lack of data, we hope that the findings in this work would provide a good start towards understanding more sophisticated semantic phenomena in this area."
p18-1122,2018,7 Conclusion,"pilot study by expert annotators shows significant iaa improvements compared to literature values, indicating a better task definition under the proposed scheme."
p18-1122,2018,7 Conclusion,"this further enables the usage of crowdsourcing to collect a new dataset, matres, at a lower time cost."
p18-1122,2018,7 Conclusion,"this paper proposes a new scheme for temprel annotation between events, simplifying the task by focusing on a single time axis at a time."
p18-1122,2018,7 Conclusion,"we have also identified that end-points of events is a major source of confusion during annotation due to reasons beyond the scope of temprel annotation, and proposed to focus on start-points only and handle the end-points issue in further investigation (e.g., in event duration annotation tasks)."
p18-1123,2018,6 Conclusions,in our future work we plan to extend the proposed method to these other applications.
p18-1123,2018,6 Conclusions,"in this work, we propose a deep learning method, exemplar encoder decoder (eed), that given a conversation context uses similar contexts and corresponding responses from training data for generating a response."
p18-1123,2018,6 Conclusions,we further show improvements achieved by the proposed method on a large collection of technical support conversations.
p18-1123,2018,6 Conclusions,we show that by utilizing this information the system is able to outperform state of the art generative models on publicly available ubuntu dataset.
p18-1123,2018,6 Conclusions,"while in this work, we apply the exemplar encoder decoder network on conversational task, the method is generic and could be used with other tasks such as question answering and machine translation."
p18-1124,2018,6 Conclusion,dialsql successfully extracts error spans from queries and offers several alternatives to users.
p18-1124,2018,6 Conclusion,it generates simple questions over a small number of turns without overwhelming users.
p18-1124,2018,6 Conclusion,our results suggest that the accuracy of the generated queries can be improved via real user feedback.
p18-1124,2018,6 Conclusion,the model learns from only simulated data which makes it easy to adapt to new domains.
p18-1124,2018,6 Conclusion,"we demonstrated the efficacy of the dialsql, improving the state of the art accuracy from 62.5% to 69.0% on the wikisql dataset."
p18-1124,2018,6 Conclusion,we further investigate the usability of dialsql in a real life setting by conducting human evaluations.
p18-1125,2018,7 Conclusions and Future Work,a promising line of future work could consider the complementary problem of identifying pragmatic strategies that can help bring uncivil conversations back on track.
p18-1125,2018,7 Conclusions and Future Work,"additionally, since our procedure for collecting and vetting data focused on precision rather than recall, it might miss more subtle attacks that are overlooked by the toxicity classifier."
p18-1125,2018,7 Conclusions and Future Work,"beyond the present binary classification task, one could explore a sequential formulation predicting whether the next turn is likely to be an attack as a discussion unfolds, capturing conversational dynamics such as sustained escalation."
p18-1125,2018,7 Conclusions and Future Work,"finally, our study of derailment offers only one glimpse into the space of possible conversational trajectories."
p18-1125,2018,7 Conclusions and Future Work,"in this work, we started to examine the intriguing phenomenon of conversational derailment, studying how the use of pragmatic and rhetorical devices relates to future conversational failure."
p18-1125,2018,7 Conclusions and Future Work,"indeed, a manual investigation of conversations whose eventual trajectories were misclassified by our models鈥攁s well as by the human annotators鈥攕uggests that interactions which initially seem prone to attacks can nonetheless maintain civility, by way of level-headed interlocutors, as well as explicit acts of reparation."
p18-1125,2018,7 Conclusions and Future Work,"making use of machine learning and crowdsourcing tools, we formulate a tightly-controlled setting that enables us to meaningfully compare conversations that stay on track with those that go awry."
p18-1125,2018,7 Conclusions and Future Work,"noting that our framework is not specifically tied to wikipedia, it would also be valuable to explore the varied ways in which this phenomenon arises in other (possibly noncollaborative) public discussion venues, such as reddit and facebook pages."
p18-1125,2018,7 Conclusions and Future Work,our approach has several limitations which open avenues for future work.
p18-1125,2018,7 Conclusions and Future Work,"our correlational analyses do not provide any insights into causal mechanisms of derailment, which randomized experiments could address."
p18-1125,2018,7 Conclusions and Future Work,"our investigation centers on the particularly perplexing scenario in which one participant of a civil discussion later attacks another, and explores the new task of predicting whether an initially healthy conversation will derail into such an attack."
p18-1125,2018,7 Conclusions and Future Work,"supplementing our investigation with other indicators of antisocial behavior, such as editors blocking one another, could enrich the range of attacks we study."
p18-1125,2018,7 Conclusions and Future Work,the human accuracy on predicting future attacks in this setting (72%) suggests it is feasible at least at the level of human intuition.
p18-1125,2018,7 Conclusions and Future Work,"to this end, we develop a computational framework for analyzing how general politeness strategies and domain-specific rhetorical prompts deployed in the initial stages of a conversation are tied to its future trajectory."
p18-1125,2018,7 Conclusions and Future Work,"we show that our computational framework can recover some of that intuition, hinting at the potential of automated methods to identify signals of the future trajectories of online conversations."
p18-1125,2018,7 Conclusions and Future Work,"while our analysis focused on the very first exchange in a conversation for the sake of generality, more complex modeling could extend its scope to account for conversational features that more comprehensively span the interaction."
p18-1126,2018,7 Conclusion,"quite on the contrary, the better the bleu score, the worse the meaning representation."
p18-1126,2018,7 Conclusion,"we believe that this observation is important for representation learning where bilingual mt now seems less likely to provide useful data, but perhaps more so for mt itself, where the struggle towards a high single-reference bleu score (or even worse, cross entropy) leads to systems that refuse to consider the meaning of the sentence."
p18-1126,2018,7 Conclusion,we evaluated these representations with a number of measures reflecting how well the meaning of the source sentence is captured.
p18-1126,2018,7 Conclusion,"we presented a novel variation of attentive nmt models (bahdanau et al., 2014; vaswani et al., 2017) that again provides a single meeting point with a continuous representation of the source sentence."
p18-1126,2018,7 Conclusion,"while our proposed 鈥渃ompound attention鈥 leads to translation quality not much worse than the fully attentive model, it generally does not perform well in the meaning representation."
p18-1127,2018,9 Conclusion,"as existing methodology ranks system outputs, these shared tendencies bias the validation process."
p18-1127,2018,9 Conclusion,experiments with maege reveal a different picture of metric quality than previously reported.
p18-1127,2018,9 Conclusion,"in this paper, we show how to leverage existing annotation in gec for performing validation reliably."
p18-1127,2018,9 Conclusion,"our analysis suggests that differences in observed metric quality are partly due to system outputs sharing consistent tendencies, notably their tendency to under-predict corrections."
p18-1127,2018,9 Conclusion,"the difficulties in basing validation on system outputs may be applicable to other text-to-text generation tasks, a question we will explore in future work."
p18-1127,2018,9 Conclusion,"we propose a new automatic methodology, maege, which overcomes many of the shortcomings of the existing methodology."
p18-1128,2018,6 Conclusions,we discussed the use of significance testing in nlp.
p18-1128,2018,6 Conclusions,"we hope this paper will serve as a guide for nlp researchers and, not less importantly, that it will encourage discussions and collaborations that will contribute to the soundness and correctness of our research."
p18-1128,2018,6 Conclusions,"we provided the main considerations for significance test selection, and proposed a simple test selection protocol."
p18-1128,2018,6 Conclusions,we then surveyed the state of significance testing in recent top venue papers and concluded with open issues.
p18-1129,2018,6 Conclusion,comparison analysis gives empirically guarantee for our distillation method.
p18-1129,2018,6 Conclusion,experiments on transition-based dependency parsing and machine translation show that our distillation method significantly improves the single model鈥檚 performance.
p18-1129,2018,6 Conclusion,"in this paper, we study knowledge distillation for search-based structured prediction and propose to distill an ensemble into a single model both from reference and exploration states."
p18-1130,2018,5 Conclusion,"another interesting direction is to further improve our model by exploring reinforcement learning approaches to learn an optimal order for the children of head words, instead of using a predefined fixed order."
p18-1130,2018,5 Conclusion,"combining pointer networks with an internal stack to track the status of the top-down, depth-first search in the decoding procedure, the stackptr parser is able to capture information from the whole sentence and all the previously derived subtrees, removing the leftto-right restriction in classical transition-based parsers, while maintaining linear parsing steps, w.r.t the length of the sentences."
p18-1130,2018,5 Conclusion,"experimental results on 29 treebanks show the effectiveness of our parser across 20 languages, by achieving state-ofthe-art performance on 21 corpora."
p18-1130,2018,5 Conclusion,"first, we intend to consider how to conduct experiments to improve the analysis of parsing errors qualitatively and quantitatively."
p18-1130,2018,5 Conclusion,"in this paper, we proposed stackptr, a transition-based neural network architecture, for dependency parsing."
p18-1131,2018,6 Conclusion,aae speakers) are not under-counted or under-represented in results returned to a user or analyst.
p18-1131,2018,6 Conclusion,it remains an open question whether it is better to use a model with a smaller accuracy disparity (e.g.
p18-1131,2018,6 Conclusion,"regardless, the modeling decision should be made in light of the application of interest; for example, applications like opinion analysis and information retrieval may benefit from equal (and possibly weaker) performance between groups, so that concepts or opinions inferred from groups of authors (e.g."
p18-1131,2018,6 Conclusion,"the emerging literature on fairness in algorithms suggests interesting further challenges; for example, kleinberg et al.(2017) and corbettdavies et al.(2017) argue that as various commonly applied notions of fairness are mutually incompatible, algorithm designers must grapple with such trade-offs."
p18-1131,2018,6 Conclusion,"udpipe), or a model with higher average accuracy, but a worse disparity (e.g.deep biaffine)."
p18-1131,2018,6 Conclusion,"while current neural dependency parsers are highly accurate on mae, our analyses suggest that aae text presents considerable challenges due to lexical and syntactic features which diverge systematically from mae."
p18-1131,2018,6 Conclusion,"while the cross-domain strategies we presented can greatly increase accurate parsing of these features, narrowing the performance gap between aae- and mae-like tweets, much work remains to be done for accurate parsing of even linguistically well-documented features."
p18-1132,2018,5 Conclusion,"despite this strong performance, we discover explicit modeling of structure does improve the model鈥檚 ability to discover non-local structural dependencies when determining the distribution over subsequent word generation."
p18-1132,2018,5 Conclusion,"given enough capacity, lstms trained on language modeling objectives are able to learn syntax-sensitive dependencies, as evidenced by accurate number agreement accuracy with multiple attractors."
p18-1132,2018,5 Conclusion,"recurrent neural network grammars (rnngs), which jointly model phrase-structure trees and strings and employ an explicit composition operator, substantially outperform lstm language models and syntactic language models without explicit compositions; this highlights the importance of a hierarchical inductive bias in capturing structural dependencies."
p18-1132,2018,5 Conclusion,"through novel extensions to rnngs that enable the use of left-corner and bottom-up generation strategies, we discover that this is indeed the case: the three rnng variants have different generalization properties for number agreement, with the top-down traversal strategy performing best for cases with multiple attractors."
p18-1132,2018,5 Conclusion,we explore the possibility that how the structure is built affects number agreement performance.
p18-1133,2018,6 Conclusion,experiments show that tscp outperforms the state-ofthe-art baselines in both task accomplishment and language quality.
p18-1133,2018,6 Conclusion,"for our future work, we will consider advanced instantiations for sequicity, and extend sequicity to handle unsupervised cases where information and requested slots values are not annotated."
p18-1133,2018,6 Conclusion,"moreover, our tscp implementation also betters traditional pipeline architectures by a magnitude in training time and adds the capability of handling oov."
p18-1133,2018,6 Conclusion,"one simplistic instantiation of sequicity, called two stage copynet (tscp), demonstrates better effectiveness and scalability of sequicity."
p18-1133,2018,6 Conclusion,such belief spans enable a task-oriented dialogue system to be holistically optimized in a single seq2seq model.
p18-1133,2018,6 Conclusion,such properties are important for real-world customer service dialog systems where users鈥 inputs vary frequently and models need to be updated frequently.
p18-1133,2018,6 Conclusion,"we propose sequicity, an extendable framework, which tracks dialogue believes through the decoding of novel text spans: belief spans."
p18-1134,2018,7 Conclusion,a feature dropout trick is also described and proves to be particularly effective.
p18-1134,2018,7 Conclusion,an e2e dialogue state tracker is introduced based on the pointer network.
p18-1134,2018,7 Conclusion,the model outputs slot values in an extractive fashion similar to the slot filling task in slu.
p18-1134,2018,7 Conclusion,"we also add a jointly trained classification component to combine with the pointer network, forming a hybrid architecture that not only achieves state-of-the-art accuracy on the dstc2 dataset, but also more importantly is able to handle unknown slot values, which is a problem often neglected although particularly valuable in real world situations."
p18-1135,2018,5 Conclusion,"at the core of glad is the global-locally self-attention encoder, whose global modules allow parameter sharing between slots and local modules allow slot-specific feature learning."
p18-1135,2018,5 Conclusion,"glad achieves state-of-theart results of 88.1% goal accuracy and 97.1% request accuracy on the woz dialogue state tracking task, as well as 74.5% goal accuracy and 97.5% request accuracy on dstc2."
p18-1135,2018,5 Conclusion,this allows glad to generalize on rare slot-value pairs with few training data.
p18-1135,2018,5 Conclusion,"we introduced the global-locally self-attentive dialogue state tracker (glad), a new state-ofthe-art model for dialogue state tracking."
p18-1136,2018,7 Conclusion,"in this work, we present an end-to-end trainable memory-to-sequence model for task-oriented dialog systems."
p18-1136,2018,7 Conclusion,mem2seq combines the multi-hop attention mechanism in end-to-end memory networks with the idea of pointer networks to incorporate external information.
p18-1136,2018,7 Conclusion,"mem2seq is fast, general, and able to achieve state-of-the-art results in three different datasets."
p18-1136,2018,7 Conclusion,"we empirically show our model鈥檚 ability to produce relevant answers using both the external kb information and the predefined vocabulary, and visualize how the multihop attention mechanisms help in learning correlations between memories."
p18-1137,2018,6 Conclusion,"experimental results on both specific-requirement (ubuntu data) and diverse-requirement scenarios (stc data) demonstrate that the proposed optimization criteria can meet the corresponding requirement, yielding better performances against traditional seq2seq models in terms of both metric-based and human evaluations."
p18-1137,2018,6 Conclusion,"for the specific-requirement scenario, such as customer service, which requires specific and high quality responses, maximum generated likelihood is used as the objective function instead of the averaged one."
p18-1137,2018,6 Conclusion,"in future work, we plan to further investigate the impact of risksensitive objective functions, including the relations between model robustness and diverse generations."
p18-1137,2018,6 Conclusion,"in this paper, we propose two new optimization criteria for seq2seq model to adapt different conversation scenario."
p18-1137,2018,6 Conclusion,the contribution of this paper is to use tailored seq2seq model for different conversation scenarios.
p18-1137,2018,6 Conclusion,"the study shows that if we want to generate specific responses, it is important to design the model to learn the most significant matching pattern between post and response."
p18-1137,2018,6 Conclusion,"while for the diverse-requirement, such as chatbot, which requires diverse and high quality responses even if for the same post, cvar is used as the objective function for worst case optimization."
p18-1137,2018,6 Conclusion,"while if we want to generate diverse responses, a risk-sensitive objective functions is helpful."
p18-1138,2018,5 Conclusion,"empirical results show the proposed model is able to generate more meaningful and diverse responses, compared with the state-of-the-art baselines."
p18-1138,2018,5 Conclusion,"in future work, we plan to introduce reinforcement learning and knowledge base reasoning mechanisms to improve the performance."
p18-1138,2018,5 Conclusion,"in this paper, we identify the knowledge diffusion in conversations and propose an end-to-end neural knowledge diffusion model to deal with the problem."
p18-1138,2018,5 Conclusion,"the model integrates the dialogue system with the knowledge base through both facts matching and entity diffusion, which enable the convergent and divergent thinking over the knowledge base."
p18-1138,2018,5 Conclusion,"under such mechanism, the factoid question answering and knowledge grounded chitchats can be tackled together."
p18-1139,2018,5 Conclusion,"as for future work, we will investigate how to apply the technique to multi-turn conversational systems, provided that the most proper sentence function can be predicted under a given conversation context."
p18-1139,2018,5 Conclusion,extensive experiments show that our model performs better than several state-of-the-art baselines.
p18-1139,2018,5 Conclusion,the model is thus able to control sentence function and generate informative content simultaneously.
p18-1139,2018,5 Conclusion,"to address the compatibility issue, we devise a type controller to handle function-related and topic words explicitly."
p18-1139,2018,5 Conclusion,"to deal with the global control of sentence function, we utilize a latent variable to capture the various patterns for different sentence functions."
p18-1139,2018,5 Conclusion,we present a model to generate responses with both controllable sentence function and informative content.
p18-1140,2018,9 Conclusion,experiments suggest that incorporating user sentiment is helpful in reducing the dialog length and increasing the task success rate in both sl and rl settings.
p18-1140,2018,9 Conclusion,this work proposed an adaptive methodology to incorporate user sentiment in end-to-end dialog policy learning and showed promising results on a bus information search task.
p18-1140,2018,9 Conclusion,we believe this approach can be easily generalized to other domains given its end-to-end training procedure and task independence.
p18-1140,2018,9 Conclusion,we included sentiment information directly as a context feature in the supervised learning framework and used sentiment scores as immediate rewards in the reinforcement learning setting.
p18-1140,2018,9 Conclusion,we proposed to detect user sentiment from multimodal channels and incorporate the detected sentiment as feedback into adaptive end-to-end dialog system training to make the system more effective and user-adaptive.
p18-1141,2018,5 Summary,our extensive experimental evaluation on crosslingual word similarity and sentiment analysis indicates that concept-based multilingual embedding learning performs better than previous approaches.
p18-1141,2018,5 Summary,"the embedding spaces of the 1259 languages (sample, clique and n(t)) are available: http://cistern.cis.lmu.de/comult/."
p18-1141,2018,5 Summary,we are indebted to michael cysouw for making pbc available to us.
p18-1141,2018,5 Summary,we gratefully acknowledge funding from the european research council (grants 740516 & 640550) and through a zentrum digitalisierung.bayern fellowship awarded to the first author.
p18-1141,2018,5 Summary,we presented a new method for estimating vector space representations of words: embedding learning by concept induction.
p18-1141,2018,5 Summary,we tested this method on a highly parallel corpus and learned semantic representations of words in 1259 different languages in a single common space.
p18-1142,2018,7 Conclusions and Future Work,"a parametrised model could be trained to imitate the operations performed by zhang and shasha (1989)鈥檚 algorithm on multiparallel texts, conditioned on the tree features and previous operations."
p18-1142,2018,7 Conclusions and Future Work,"another possible research direction is learning the mapping between structures from parallel texts jointly with a main task, in the spirit of quasi-synchronous grammars (smith and eisner, 2009)."
p18-1142,2018,7 Conclusions and Future Work,"finally, a wider range of syntactic constructions could be covered by inferring typological strategies from texts (ostling 篓 , 2015; coke et al., 2016)."
p18-1142,2018,7 Conclusions and Future Work,"first, we have provided two measures of anisomorphism based on jaccard distance of morphological feature sets, as well as average tree edit distance of parallel sentences."
p18-1142,2018,7 Conclusions and Future Work,future work will look into automating the tree processing procedure.
p18-1142,2018,7 Conclusions and Future Work,"it boosts the performance of standard frameworks in two downstream applications, obtaining competitive or state-of-art results for 1) nmt on a new dataset of arabic-dutch and indonesian-portuguese and 2) cross-lingual sentence similarity."
p18-1142,2018,7 Conclusions and Future Work,"second, we have proposed a new method for fine-tuning source dependency trees to resemble target language trees in order to reduce anisomorphism."
p18-1142,2018,7 Conclusions and Future Work,"the data for nmt, and the code for our crosslingual sts are available at the following link: github.com/ducdauge/isotransf."
p18-1142,2018,7 Conclusions and Future Work,"the method does not depend on parallel data, and it leverages readily available information in typological databases."
p18-1142,2018,7 Conclusions and Future Work,these can provide reliable indicators for language compatibility for source selection in cross-lingual parsing.
p18-1142,2018,7 Conclusions and Future Work,"this phenomenon, which we have labeled as anismorphism, can challenge the transfer of knowledge from one language to another."
p18-1142,2018,7 Conclusions and Future Work,this variation stems from morphological and syntactic differences across languages.
p18-1142,2018,7 Conclusions and Future Work,we have demonstrated that syntactic structures differ across languages even in well-developed annotation schemes such as universal dependencies.
p18-1142,2018,7 Conclusions and Future Work,"we have proposed novel methodology which reduces the degree of anisomorphism crosslingually 1) by selecting the most compatible languages for transfer, and 2) by editing the syntactic structures (i.e., trees) themselves."
p18-1143,2018,7 Conclusion,"further, we would like to study sampling techniques motivated by natural distributions of linguistic structures."
p18-1143,2018,7 Conclusion,"hence, as a future work, we would like to compare the usefulness of different linguistic theories and different constraints within each theory in our proposed lm framework."
p18-1143,2018,7 Conclusion,"in this paper, we presented a computational method for generating synthetic cm data based on the ec theory of code-mixing, and showed that sampling text from the synthetic corpus (according to the distribution of spf found in real cm data) helps in reduction of ppl of the rnn-lm by an amount which is equivalently achieved by doubling the amount of real cm data."
p18-1143,2018,7 Conclusion,there is no unanimous theory in linguistics on syntactic structure of cm language.
p18-1143,2018,7 Conclusion,this can also provide an indirect validation of the theories.
p18-1143,2018,7 Conclusion,"thus, the linguistic theory based generation is of crucial significance."
p18-1143,2018,7 Conclusion,we also showed that randomly generated cm data doesn鈥檛 improve the lm.
p18-1144,2018,5 Conclusion,"the lattice method is fully independent of word segmentation, yet more effective in using word information thanks to the freedom of choosing lexicon words in a context for ner disambiguation."
p18-1144,2018,5 Conclusion,"we empirically investigated a lattice lstm-crf representations for chinese ner, finding that it gives consistently superior performance compared to word-based and character-based lstm-crf across different domains."
p18-1145,2018,6 Conclusions and Future Work,"because the mismatch between words and extraction units is a common problem in information extraction, we believe our method can also be applied to many other languages and tasks for exploiting inner composition structure during extraction, such as named entity recognition."
p18-1145,2018,6 Conclusions and Future Work,experiment results have shown that our method significantly outperforms conventional methods.
p18-1145,2018,6 Conclusions and Future Work,"this paper proposes nugget proposal networks for chinese event detection, which can effectively resolve the word-trigger mismatch problem by modeling and exploiting character compositional structure of chinese event triggers, using hybrid representation which can summarize information from both characters and words."
p18-1146,2018,5 Conclusion,higher order relation schema induction (hrsi) is an important first step towards building domainspecific knowledge graphs (kgs).
p18-1146,2018,5 Conclusion,"in the second step, tfba solves a constrained clique problem to induce schemata out of multiple binary schemata."
p18-1146,2018,5 Conclusion,"in this paper, we proposed tfba, a tensor factorizationbased method for higher-order rsi."
p18-1146,2018,5 Conclusion,"rather than factorizing a severely sparse higher-order tensor directly, tfba performs back-off and jointly factorizes multiple lower-order tensors derived out of the higher-order tensor."
p18-1146,2018,5 Conclusion,"to the best of our knowledge, this is the first attempt at inducing higher-order (n-ary) schemata for relations from unlabeled text."
p18-1146,2018,5 Conclusion,we are hopeful that the backoff-based factorization idea exploited in tfba will be useful in other sparse factorization settings.
p18-1147,2018,8 Conclusions,"first of all, we plan to explore the use of more advanced forms of entity detection and linking, including propagating features from the edl system forward for both unary and binary deep models."
p18-1147,2018,8 Conclusions,"in addition we plan to exploit unary and binary relations as source of evidence to bootstrap a probabilistic reasoning approach, with the goal of leveraging constraints from the kb schema such as domain, range and taxonomies."
p18-1147,2018,8 Conclusions,in this paper we presented a new methodology to identify relations between entities in text.
p18-1147,2018,8 Conclusions,"our approach, focusing on unary relations, can greatly improve the recall in automatic construction and updating of knowledge bases by making use of implicit and partial textual markers."
p18-1147,2018,8 Conclusions,our method is extremely effective and complement very nicely existing binary relation extraction methods for kbp.
p18-1147,2018,8 Conclusions,"this is just the first step in our wider research program on kbp, whose goal is to improve recall by identifying implicit information from texts."
p18-1147,2018,8 Conclusions,we will also integrate the new triples gathered from textual evidence with new triples predicted from existing kb relationships by knowledge base completion.
p18-1148,2018,5 Conclusion and Future work,"besides, we would like to examine whether the induced latent relations could be helpful for relation extract."
p18-1148,2018,5 Conclusion and Future work,"conceptually, modeling multiple relations is substantially different from simply modeling coherence (as in ganea and hofmann (2017))."
p18-1148,2018,5 Conclusion and Future work,"in future work, we would like to use syntactic and discourse structures (e.g., syntactic dependency paths between mentions) to encourage the models to discover a richer set of relations."
p18-1148,2018,5 Conclusion and Future work,"in this way we also hope it will lead to interesting follow-up work, as individual relations can be informed by injecting prior knowledge (e.g., by training jointly with relation extraction models)."
p18-1148,2018,5 Conclusion and Future work,"our models consider relations as latent variables, thus do not require any extra supervision."
p18-1148,2018,5 Conclusion and Future work,"representation learning was used to learn relation embeddings, eliminating the need for extensive feature engineering."
p18-1148,2018,5 Conclusion and Future work,the experimental results show that our best model achieves the best reported f1 on aida-conll with an improvement of 0.85% f1 over the best previous results.
p18-1148,2018,5 Conclusion and Future work,we also would like to combine ment-norm and relnorm.
p18-1148,2018,5 Conclusion and Future work,we have shown the benefits of using relations in nel.
p18-1149,2018,8 Conclusion,"through extensive experiments on real-world datasets, we demonstrate the effectiveness of neuraldater over existing state-of-theart approaches."
p18-1149,2018,8 Conclusion,"to the best of our knowledge, this is the first application of deep learning techniques for the problem of document dating."
p18-1149,2018,8 Conclusion,we are hopeful that the representation learning techniques explored in this paper will inspire further development and adoption of such techniques in the temporal information processing research community
p18-1149,2018,8 Conclusion,"we propose neuraldater, a graph convolutional network (gcn) based method for document dating which exploits syntactic and temporal structures in the document in a principled way."
p18-1150,2018,7 Conclusion,"allowing high parallelization, the graph encoder is more efficient than the sequence encoder."
p18-1150,2018,7 Conclusion,"compared to sequence-to-sequence models, which require linearization of amr before decoding, a graph lstm is leveraged to directly model full amr structure."
p18-1150,2018,7 Conclusion,"in our experiments, the graph model outperforms a strong sequence-to-sequence model, achieving the best performance."
p18-1150,2018,7 Conclusion,we introduced a novel graph-to-sequence model for amr-to-text generation.
p18-1151,2018,5 Conclusions,"on the gkb dataset, our model outperforms the standard blstm model by up to 15.2%, 20.9%, and 23.1% in these three metrics, respectively."
p18-1151,2018,5 Conclusions,"on the webnlg dataset, our model outperforms the best existing model, the standard blstm model, by up to 17.6%, 6.0%, and 16.4% in terms of bleu, meteor, and ter scores, respectively."
p18-1151,2018,5 Conclusions,our experiments show that gtr-lstm offers a better performance than all the competitors.
p18-1151,2018,5 Conclusions,the proposed model can handle cycles to capture the global information of a knowledge graph and also handle non-predefined relationships between entities of a knowledge graph.
p18-1151,2018,5 Conclusions,the proposed model maintains the structure of input rdf triples as a small knowledge graph to optimize the amount of information preserved in the input of the model.
p18-1151,2018,5 Conclusions,we proposed a novel graph-based triple encoder gtr-lstm for sentence generation from rdf data.
p18-1152,2018,7 Conclusion,human evaluation shows that the quality of the text produced by our model exceeds that of competitive baselines by a large margin.
p18-1152,2018,7 Conclusion,our framework learns a decoding objective suitable for generation through a learned combination of sub-models that capture linguistically-motivated qualities of good writing.
p18-1152,2018,7 Conclusion,"we proposed a unified learning framework for the generation of long, coherent texts, which overcomes some of the common limitations of rnns as text generation models."
p18-1153,2018,5 Conclusion and Future Work,"for future work, we hope to improve the results by using the pun data and design a more proper way to select candidates from associative words."
p18-1153,2018,5 Conclusion and Future Work,"highlight model takes associative words into consideration, which makes the distinct senses more obvious in one sentence."
p18-1153,2018,5 Conclusion and Future Work,"in this paper, we proposed two models for pun generation without using training data of puns."
p18-1153,2018,5 Conclusion and Future Work,"joint model makes use of conditional language model and the joint beam search algorithm, which can assure the assigned senses of target words suitable in one sentence."
p18-1153,2018,5 Conclusion and Future Work,"the produced puns are evaluated using automatic evaluation and human evaluation, and they outperform the sentences generated by our baseline models."
p18-1154,2018,6 Conclusions,a human evaluation study judges outputs from the proposed methods to be as good as human written commentary texts for 鈥楳ove description鈥 subset of the data.
p18-1154,2018,6 Conclusions,an interesting point to explore is whether such pragmatically trained game state representations can be leveraged for the task of game commentary generation.
p18-1154,2018,6 Conclusions,generating commentary for such multi-moves is a potential direction for future work.
p18-1154,2018,6 Conclusions,"in this paper, we curate a dataset for the task of chess commentary generation and propose methods to perform generation on this dataset."
p18-1154,2018,6 Conclusions,our dataset also contains multi-move-single commentary pairs in addition to single movesingle commentary pairs.
p18-1154,2018,6 Conclusions,our proposed method effectively utilizes information related to the rules and pragmatics of the game.
p18-1154,2018,6 Conclusions,"recent work (silver et al., 2016) has proposed reinforcement learning based game-playing agents which learn to play board games from scratch, learning end-to-end from both recorded games and self-play."
p18-1154,2018,6 Conclusions,we anticipate this task to require even deeper understanding of the game pragmatics than the single move-single commentary case.
p18-1155,2018,7 Discussion,"despite the distinct training procedures, both algorithms combine the idea of fine-grained credit assignment and the entropy regularization, leading to positive empirical results."
p18-1155,2018,7 Discussion,"given the numerous potential applications of such an oracle, we believe improving its accuracy will be a promising future direction."
p18-1155,2018,7 Discussion,"however, many problems remain widely open."
p18-1155,2018,7 Discussion,"in particular, the oracle q-function q蠁 we obtain is far from perfect."
p18-1155,2018,7 Discussion,"in this work, motivated by the intriguing connection between the token-level raml and the entropy-regularized rl, we propose two algorithms for neural sequence prediction."
p18-1155,2018,7 Discussion,"we believe the ground-truth reference contains sufficient information for such an oracle, and the current bottleneck lies in the rl algorithm."
p18-1156,2018,7 Conclusion,"in this paper we introduced duorc, a large scale rc dataset of 186k human-generated qa pairs created from 7680 pairs of parallel movie-plots, each pair taken from wikipedia and imdb."
p18-1156,2018,7 Conclusion,"through our experiments, we show how the stateof-the-art rc models, which have achieved near human performance on the squad dataset, perform poorly on our dataset, thus emphasizing the need to explore further avenues for research."
p18-1156,2018,7 Conclusion,"we then showed that this dataset, by design, ensures very little or no lexical overlap between the questions created from one version and segments containing answers in the other version."
p18-1156,2018,7 Conclusion,"with this, we hope to introduce the rc community to new research challenges on qa requiring external knowledge and common-sense driven reasoning, deeper language understanding and multiple-sentence inferencing."
p18-1157,2018,7 Conclusion,"due to the strong connection between the proposed model with memory networks and reasonet, we would like to delve into the theoretical link between these models and its training algorithms."
p18-1157,2018,7 Conclusion,"further, we also would like to explore san on other tasks, such as text classification and natural language inference for its generalization in the future."
p18-1157,2018,7 Conclusion,"the model achieves results competitive with the state-of-the-art on the squad leaderboard, as well as on the adversarial squad and ms marco datasets."
p18-1157,2018,7 Conclusion,"the use of stochastic dropout in training and averaging in test at the answer module leads to robust improvements on squad, outperforming both fixed step memory networks and dynamic step reasonet."
p18-1157,2018,7 Conclusion,we further empirically analyze the properties of san in detail.
p18-1157,2018,7 Conclusion,"we introduce stochastic answer networks (san), a simple yet robust model for machine reading comprehension."
p18-1158,2018,5 Conclusions,"we introduce a novel hierarchical attention network, a state-of-the-art reading comprehension model which conducts attention and fusion horizontally and vertically across layers at different levels of granularity between question and paragraph."
p18-1158,2018,5 Conclusions,"we show that our proposed method is very powerful and robust, which outperforms the previous state-of-the-art methods in various largescale golden mrc datasets: squad, triviaqa, addsent and addonesent."
p18-1159,2018,5 Conclusion,experiments on public open-domain rc datasets quasar-t and searchqa show the necessity of introducing the selection model and the effectiveness of fusing candidates information when modeling.
p18-1159,2018,5 Conclusion,"furthermore, we treat candidate extraction as a latent variable and jointly train these two stages with rl."
p18-1159,2018,5 Conclusion,"in this paper, we formulate the problem of rc as a two-stage process, which first generates candidates with an extraction model, then selects the final answer by combining the information from all the candidates."
p18-1159,2018,5 Conclusion,"moreover, our joint training strategy leads to significant improvements in performance."
p18-1160,2018,6 Conclusion,"first, we studied the minimal context required to answer the question in existing datasets and found that most questions can be answered using a small set of sentences."
p18-1160,2018,6 Conclusion,"in addition, we showed that our approach is more robust to adversarial inputs."
p18-1160,2018,6 Conclusion,"second, inspired by this observation, we proposed a sentence selector which selects a minimal set of sentences to answer the question to give to the qa model."
p18-1160,2018,6 Conclusion,"we achieved the training and inference speedup of up to 15脳 and 13脳, respectively, and accuracy comparable to or better than existing state-of-the-art."
p18-1160,2018,6 Conclusion,we demonstrated the efficiency and effectiveness of our method across five different datasets with varying sizes of source documents.
p18-1160,2018,6 Conclusion,we proposed an efficient and robust qa system that is scalable to large documents and robust to adversarial inputs.
p18-1161,2018,5 Conclusion and Future Work,"in particular, we demonstrate that the performance of our model is hardly compromised when only using a few topselected paragraphs."
p18-1161,2018,5 Conclusion and Future Work,"in the experiments, we show that our models significantly and consistently outperforms state-of-the-art ds-qa models."
p18-1161,2018,5 Conclusion and Future Work,"in the future, we will explore the following directions: (1) an additional answer re-ranking step can further improve our model."
p18-1161,2018,5 Conclusion and Future Work,"in this paper, we propose a denoising distantly supervised open-domain question answering system which contains a paragraph selector to skim over paragraphs and a paragraph reader to perform an intensive reading on the selected paragraphs."
p18-1161,2018,5 Conclusion and Future Work,our model can make full use of all informative paragraphs and alleviate the wrong labeling problem in ds-qa.
p18-1161,2018,5 Conclusion and Future Work,"we will explore how to effectively re-rank our extracted answers to further enhance the performance.(2) background knowledge such as factual knowledge, common sense knowledge can effectively help us in paragraph selection and answer extraction."
p18-1161,2018,5 Conclusion and Future Work,we will incorporate external knowledge bases into our ds-qa model to improve its performance.
p18-1162,2018,8 Conclusion and Future Work,"by orthogonal decomposition, the labor of identifying similar parts and collecting related information in the question body can be well divided in two different alignment matrices."
p18-1162,2018,8 Conclusion and Future Work,empirical results on two community question answering datasets in semeval demonstrate the effectiveness of our model.
p18-1162,2018,8 Conclusion and Future Work,"furthermore, since thread-level features have been explored in previous work (barr贸n-cede帽o et al., 2015; joty et al., 2015, 2016), we will verify their effectiveness in our architecture."
p18-1162,2018,8 Conclusion and Future Work,"in future work, we will try to incorporate more hand-crafted features in our model."
p18-1162,2018,8 Conclusion and Future Work,"to better capture the interaction between the subject-body pair and the question-answer pair, the multi-dimensional attention mechanism is adopted."
p18-1162,2018,8 Conclusion and Future Work,"we propose question condensing networks (qcn), an attention-based model that can utilize the subject-body relationship in community questions to condense question representation."
p18-1163,2018,6 Conclusion,"as our training framework is not limited to specific perturbation types, it is interesting to evaluate our approach in natural noise existing in practical applications, such as homonym in the simultaneous translation system."
p18-1163,2018,6 Conclusion,"experiments on chinese-english, english-german and english-french translation tasks show that the proposed approach can improve both the robustness and translation performance."
p18-1163,2018,6 Conclusion,"it is also necessary to further validate our approach on more advanced nmt architectures, such as cnn-based nmt (gehring et al., 2017) and transformer (vaswani et al., 2017)."
p18-1163,2018,6 Conclusion,the basic idea is to train both the encoder and decoder robust to input perturbations by enabling them to behave similarly for the original input and its perturbed counterpart.
p18-1163,2018,6 Conclusion,we have proposed adversarial stability training to improve the robustness of nmt models.
p18-1163,2018,6 Conclusion,we propose two approaches to construct perturbed data to adversarially train the encoder and stabilize the decoder.
p18-1164,2018,6 Conclusion,"additionally, we also intend to apply these methods to other sequence-to-sequence tasks, including natural language conversation."
p18-1164,2018,6 Conclusion,experiments on chinese to english translation shows that the proposed models can significantly improve the translation quality.
p18-1164,2018,6 Conclusion,"further in-depth analysis demonstrate that our models are able (1) to learn better word alignments than the baseline nmt, (2) to alleviate the notorious problems of over and under translation in nmt, and (3) to learn direct mappings between source and target words."
p18-1164,2018,6 Conclusion,"in future work, we will explore further strategies to bridge the source and target side for sequence-to-sequence and tree-based nmt."
p18-1164,2018,6 Conclusion,the three models seek to shorten the distance between source and target word embeddings along the extensive information procedure in the encoderdecoder neural network.
p18-1164,2018,6 Conclusion,we have presented three models to bridge source and target word embeddings for nmt.
p18-1165,2018,7 Conclusion,"in this work, we sought to find answers to the questions of how cardinal and ordinal feedback differ in terms of reliability, learnability and effectiveness for rl training of nmt, with the goal of improving nmt with human bandit feedback."
p18-1165,2018,7 Conclusion,our experiments show that improvements of over 1 bleu are achievable by learning from a dataset that is tiny in machine translation proportions.
p18-1165,2018,7 Conclusion,"our rating study, comparing 5-point and preference ratings, showed that their reliability is comparable, whilst cardinal ratings are easier to learn and to generalize from, and also more suitable for rl in our experiments."
p18-1165,2018,7 Conclusion,"our work reports improvements of nmt leveraging actual human bandit feedback for rl, leaving the safe harbor of simulations."
p18-1165,2018,7 Conclusion,"since this type of feedback, in contrast to post-edits and references, is fast and cheap to elicit from nonprofessionals, our results bear a great potential for future applications on larger scale."
p18-1166,2018,6 Conclusion and Future Work,extensive experiments on one wmt14 and six wmt17 language pairs demonstrate that the proposed average attention network is able to speed up the transformer鈥檚 decoder by over 4 times.
p18-1166,2018,6 Conclusion and Future Work,"in the future, we plan to apply our model on other sequence to sequence learning tasks."
p18-1166,2018,6 Conclusion and Future Work,"in this paper, we have described the average attention network that considerably alleviates the decoding bottleneck of the neural transformer."
p18-1166,2018,6 Conclusion and Future Work,"our model employs a cumulative average operation to capture important contextual clues from previous target words, and a feed forward gating layer to enrich the expressiveness of learned hidden representations."
p18-1166,2018,6 Conclusion and Future Work,the model is further enhanced with a masking trick and a dynamic programming method to accelerate the transformer鈥檚 decoder.
p18-1166,2018,6 Conclusion and Future Work,we will also attempt to improve our model to enhance its modeling ability so as to consistently outperform the original neural transformer.
p18-1167,2018,5 Conclusion,cnn based models on the other hand can be improved through layer normalization and also feed-forward blocks.
p18-1167,2018,5 Conclusion,"for the data sets we evaluated on, models with self-attention on the encoder side and either an rnn or cnn on the decoder side performed competitively to the transformer model in most cases."
p18-1167,2018,5 Conclusion,"furthermore, we showed that one can successfully combine architectures."
p18-1167,2018,5 Conclusion,"instead of committing to a single architecture, the language allows for combining architectures on a granular level."
p18-1167,2018,5 Conclusion,these variations bring the rnn and cnn based models close to the transformer.
p18-1167,2018,5 Conclusion,using this language we explored how specific aspects of the transformer architecture can successfully be applied to rnns and cnns.
p18-1167,2018,5 Conclusion,we described an adl for specifying nmt architectures based on composable building blocks.
p18-1167,2018,5 Conclusion,we found that rnn based models benefit from multiple source attention mechanisms and residual feed-forward blocks.
p18-1167,2018,5 Conclusion,"we found that self-attention is much more important on the encoder side than it is on the decoder side, where even a model without self-attention performed surprisingly well."
p18-1167,2018,5 Conclusion,we make our implementation available so that it can be used for exploring novel architecture variations.
p18-1167,2018,5 Conclusion,"we performed an extensive evaluation on iwslt en鈫扗e, wmt鈥17 en鈫扗e and lv鈫扙n, reporting both bleu and meteor over multiple runs in each setting."
p18-1168,2018,8 Discussion,"first, we use abstract examples to semiautomatically generate utterance-program pairs that help warm-start our parameters, thereby reducing the difficult search challenge of finding correct programs with random parameters."
p18-1168,2018,8 Discussion,in future work we plan to extend this work and automatically learn such a lexicon.
p18-1168,2018,8 Discussion,"in this paper, we used a manually-built highprecision lexicon to construct abstract examples."
p18-1168,2018,8 Discussion,"in this work we presented the first semantic parser for the cnlvr dataset, taking structured representations as input."
p18-1168,2018,8 Discussion,"our approach dramatically improves performance on cnlvr, establishing a new state-of-the-art."
p18-1168,2018,8 Discussion,"our main insight is that in closed, well-typed domains we can generate abstract examples that can help combat the difficulties of training a parser from delayed supervision."
p18-1168,2018,8 Discussion,"second, we focus on an abstract representation of examples, which allows us to tackle spuriousness and alleviate search, by sharing information about promising programs between different examples."
p18-1168,2018,8 Discussion,this can reduce manual effort and scale to larger domains where there is substantial variability on the language side.
p18-1168,2018,8 Discussion,"this is suitable for well-typed domains, which are ubiquitous in the virtual assistant use case."
p18-1169,2018,7 Conclusion,"finally, our approach to collecting feedback can also be transferred to other domains."
p18-1169,2018,7 Conclusion,"for example, (yih et al., 2016) designed a user interface to help freebase experts to efficiently create queries."
p18-1169,2018,7 Conclusion,"furthermore, we showed that it is essential to obtain reward signals at the token-level in order to learn from partially correct queries."
p18-1169,2018,7 Conclusion,in both cases we show that a strong baseline using a bandit-to-supervised conversion can be significantly outperformed by a combination of a onestep-late reweighting and token-level rewards.
p18-1169,2018,7 Conclusion,"this interface could be reversed: given a question and a query produced by a parser, the interface is filled out automatically and the user has to verify if the information fits."
p18-1169,2018,7 Conclusion,"this scenario is important to avoid complex and costly data annotation for supervise learning, and it is realistic in commercial applications where weak feedback can be collected easily in large amounts from users."
p18-1169,2018,7 Conclusion,we introduced a scenario for improving a neural semantic parser from logged bandit feedback.
p18-1169,2018,7 Conclusion,we presented experimental results using feedback collected from humans and a larger scale setup with simulated feedback.
p18-1169,2018,7 Conclusion,we presented robust counterfactual learning objectives that allow to perform stochastic gradient optimization which is crucial in working with neural networks.
p18-1170,2018,8 Conclusion,"in future work, we will speed it up through the use of pruning techniques."
p18-1170,2018,8 Conclusion,"in particular, advanced methods for alignments, as in lyu and titov (2018), seem promising."
p18-1170,2018,8 Conclusion,overcoming the need for heuristics also seems to be a crucial ingredient for applying our method to other semantic representations.
p18-1170,2018,8 Conclusion,"the am term represents the compositional semantic structure of the amr explicitly, allowing us to use standard treebased parsing techniques."
p18-1170,2018,8 Conclusion,the projective parser currently computes the complete parse chart.
p18-1170,2018,8 Conclusion,"we presented an amr parser which applies methods from supertagging and dependency parsing to map a string into a well-typed am term, which it then evaluates into an amr."
p18-1170,2018,8 Conclusion,we will also look into more principled methods for splitting the amrs into elementary as-graphs to replace our hand-crafted heuristics.
p18-1171,2018,6 Conclusion,"in this paper, we have presented a sequence-toaction-sequence approach for cache transition systems and applied it to amr parsing."
p18-1171,2018,6 Conclusion,"to address the data sparsity issue for neural amr parsing, we show that the transition state features are very helpful in constraining the possible output and improving the performance of sequence-to-sequence models."
p18-1171,2018,6 Conclusion,we also show that the monotonic hard attention model can be generalized to the transitionbased framework and outperforms the soft attention model when limited data is available.
p18-1171,2018,6 Conclusion,"while we are focused on amr parsing in this paper, in future work our cache transition system and the presented sequence-to-sequence models can be potentially applied to other semantic graph parsing tasks (oepen et al., 2015; du et al., 2015; zhang et al., 2016; cao et al., 2017)."
p18-1172,2018,6 Conclusion,"besides, both theoretical analysis and experiments demonstrate that allvec achieves the same time complexity with the classic sgd models."
p18-1172,2018,6 Conclusion,"in contrast with models based on sgd and negative sampling, allvec shows more stable convergence and better embedding quality by the all-sample optimization."
p18-1172,2018,6 Conclusion,"in future, we will extend our proposed all-sample learning scheme to deep learning methods, which are more expressive than the shallow embedding model."
p18-1172,2018,6 Conclusion,"in this paper, we presented allvec, an efficient batch learning based word embedding model that is capable to leverage all positive and negative training examples without any sampling and approximation."
p18-1172,2018,6 Conclusion,"lastly, we are interested in exploring the recent adversarial learning techniques to enhance the robustness of word embeddings."
p18-1172,2018,6 Conclusion,"moreover, we will integrate prior knowledge, such as the words that are synonyms and antonyms, into the word embedding process."
p18-1173,2018,7 Conclusion,"experiments show that spigot achieves stronger performance than baselines under both settings, and outperforms stateof-the-art systems on semantic dependency parsing."
p18-1173,2018,7 Conclusion,our implementation is available at https: //github.com/noahs-ark/spigot.
p18-1173,2018,7 Conclusion,"spigot devises a proxy for the gradients with respect to argmax鈥檚 inputs, employing a projection that aims to respect the constraints in the intermediate task."
p18-1173,2018,7 Conclusion,"we empirically evaluate our method with two architectures: a semantic parser with an intermediate syntactic parser, and a sentiment classifier with an intermediate semantic parser."
p18-1173,2018,7 Conclusion,"we presented spigot, a novel approach to backpropagating through neural network architectures that include discrete structured decisions in intermediate layers."
p18-1174,2018,6 Conclusion,"in this paper, we have proposed a new method for learning active learning algorithms using deep imitation learning."
p18-1174,2018,6 Conclusion,our efficient algorithmic expert provides state-action pairs from which effective active learning policies can be learned.
p18-1174,2018,6 Conclusion,"we formalize pool-based active learning as a markov decision process, in which active learning corresponds to the selection decision of the most informative data points from the pool."
p18-1174,2018,6 Conclusion,"we show that the algorithmic expert allows direct policy learning, while at the same time, the learned policies transfer successfully between domains and languages, demonstrating improvement over previous heuristic and reinforcement learning approaches."
p18-1176,2018,7 Conclusion,"additionally, our perturbation attacks (sections 4.4 and 5.5) serve as empirical validation of attributions."
p18-1176,2018,7 Conclusion,attributions helped us identify weaknesses of these models more effectively than conventional methods (based on validation sets).
p18-1176,2018,7 Conclusion,our attribution-based methods can be directly used to gauge the extent of such problems.
p18-1176,2018,7 Conclusion,under-reliance on important question terms is not safe.
p18-1176,2018,7 Conclusion,we also believe that other qa models may share these weaknesses.
p18-1176,2018,7 Conclusion,we analyzed three question answering models using an attribution technique.
p18-1176,2018,7 Conclusion,we believe that a workflow that uses attributions can aid the developer in iterating on model quality more effectively.
p18-1176,2018,7 Conclusion,"while the attacks in this paper may seem unrealistic, they do expose real weaknesses that affect the usage of a qa product."
p18-1177,2018,7 Conclusion,evaluations with different metrics on the squad machine reading dataset show that our model outperforms state-ofthe-art baselines.
p18-1177,2018,7 Conclusion,"finally, we apply our question generation framework to produce a corpus of 1.26 million questionanswer pairs, which we hope will benefit the qa research community."
p18-1177,2018,7 Conclusion,it would also be interesting to apply our approach to incorporating coreference knowledge to other text generation tasks.
p18-1177,2018,7 Conclusion,the ablation study shows the effectiveness of different components in our model.
p18-1177,2018,7 Conclusion,we propose a new neural network model for better encoding coreference knowledge for paragraphlevel question generation.
p18-1178,2018,6 Conclusion,all these three modules can be trained with different forms of the answer labels and training them jointly can provide further improvement.
p18-1178,2018,6 Conclusion,"in this paper, we propose an end-to-end framework to tackle the multi-passage mrc task ."
p18-1178,2018,6 Conclusion,"the experimental results demonstrate that our model outperforms the baseline models by a large margin and achieves the state-of-the-art performance on two challenging datasets, both of which are designed for mrc on real web data."
p18-1178,2018,6 Conclusion,"we creatively design three different modules in our model, which can find the answer boundary, model the answer content and conduct cross-passage answer verification respectively."
p18-1179,2018,7 Conclusion,"our system is accurate (bleu 68.07), efficient (more than 5 sentences per second on a cpu) and robust (fullcoverage)."
p18-1179,2018,7 Conclusion,"the empirical evaluation confirms the usefulness a dag transducer to resolve nlg, as well as the effectiveness of our design."
p18-1179,2018,7 Conclusion,the key idea is to leverage a declarative programming language to minimize the computation burden of a graph transducer.
p18-1179,2018,7 Conclusion,"to exemplify our design, we develop a practical system for the semantic-graph-to-string task."
p18-1179,2018,7 Conclusion,we extend the work on dag automata in chiang et al.(2018) and propose a general method to build flexible dag transducer.
p18-1179,2018,7 Conclusion,we think may nlp tasks that involve graph manipulation may benefit from this design.
p18-1180,2018,10 Conclusion,"concurrently, we show that derivational transformations can be usefully modeled as nonlinear functions on distributional word embeddings."
p18-1180,2018,10 Conclusion,"in this work, we present a novel aggregation model for derived word generation."
p18-1180,2018,10 Conclusion,"the distributional and orthographic models aggregated contribute orthogonal information to the aggregate, as shown by substantial improvements over state-of-the-art results, and qualitative analysis."
p18-1180,2018,10 Conclusion,"this ameliorates suffix ambiguity and orthographic irregularity, the salient problems of the generation task."
p18-1180,2018,10 Conclusion,this model learns to choose between the predictions of orthographicallyand distributionally-informed models.
p18-1180,2018,10 Conclusion,two ways of incorporating corpus knowledge 鈥 constrained decoding and rescoring 鈥 demonstrate further improvements to our main contribution.
p18-1181,2018,6 Conclusion,"machine-generated generated poems, however, still underperform in terms of readability and emotion."
p18-1181,2018,6 Conclusion,"our research reveals that vanilla lstm language model captures meter implicitly, and our proposed rhyme model performs exceptionally well."
p18-1181,2018,6 Conclusion,"we propose a joint model of language, meter and rhyme that captures language and form for modelling sonnets."
p18-1181,2018,6 Conclusion,"we provide quantitative analyses for each component, and assess the quality of generated poems using judgements from crowdworkers and a literature expert."
p18-1182,2018,Conclusion,"neuralreg decides both on referential form and on referential content in an integrated, end-to-end approach, without using explicit features."
p18-1182,2018,Conclusion,"using a new delexicalized version of the webnlg corpus (made publicly available), we showed that the neural model substantially improves over two strong baselines in terms of accuracy of the referring expressions and fluency of the lexicalized texts."
p18-1182,2018,Conclusion,we introduced a deep learning model for the generation of referring expressions in discourse texts.
p18-1183,2018,7 Conclusion,our comprehensive dataset is publicly available at https://github.com/ yumoxu/stocknet-dataset.
p18-1183,2018,7 Conclusion,"we demonstrated the effectiveness of deep generative approaches for stock movement prediction from social media data by introducing stocknet, a neural network architecture for this task."
p18-1183,2018,7 Conclusion,"we tested our model on a new comprehensive dataset and showed it performs better than strong baselines, including implementation of previous work."
p18-1184,2018,6 Conclusions and Future Work,"in our future work, we plan to integrate other types of information such as user properties into the structured neural models to further enhance representation learning and detect rumor spreaders at the same time."
p18-1184,2018,6 Conclusions and Future Work,results on two public twitter datasets show that our method improves rumor detection performance in very large margins as compared to state-of-the-art baselines.
p18-1184,2018,6 Conclusions and Future Work,"the inherent nature of recursive models allows them using propagation tree to guide the learning of representations from tweets content, such as embedding various indicative signals hidden in the structure, for better identifying rumors."
p18-1184,2018,6 Conclusions and Future Work,we also plan to use unsupervised models for the task by exploiting structural information.
p18-1184,2018,6 Conclusions and Future Work,we propose a bottom-up and a top-down treestructured model based on recursive neural networks for rumor detection on twitter.
p18-1185,2018,6 Conclusions and Future Work,experiments show an absolute 3%-4% f-score gain.
p18-1185,2018,6 Conclusions and Future Work,"for example, an image including a pitcher indicates that the 鈥楪iants鈥 in context should refer to the baseball team 鈥楽an francisco giants鈥."
p18-1185,2018,6 Conclusions and Future Work,"name tagging for more fine-grained types (e.g.soccer team, basketball team, politician, artist) can benefit more from visual features."
p18-1185,2018,6 Conclusions and Future Work,we construct two multimodal datasets from twitter and snapchat.
p18-1185,2018,6 Conclusions and Future Work,we hope this work will encourage more research on multimodal social media in the future and we plan on making our benchmark available upon request.
p18-1185,2018,6 Conclusions and Future Work,we plan to expand our model to tasks such as fine-grained name tagging or entity liking in the future.
p18-1185,2018,6 Conclusions and Future Work,we propose a gated visual attention for name tagging in multimodal social media.
p18-1186,2018,5 Conclusions,"our proposed mned model improves upon the state-of-the-art models by 1) extracting visual contexts complementary to textual contexts, 2) by leveraging lexical embeddings into entity matching which accounts for various surface forms of entities, removing the need for fixed candidates generation process, and 3) by performing entity matching in the distributed knowledge graph embeddings space, allowing for matching of unseen mentions and entities by context resolutions."
p18-1186,2018,5 Conclusions,"we introduce a new task called multimodal named entity disambiguation (mned), which is applied on short user-generated social media posts that are composed of text and accompanying images."
p18-1187,2018,6 Conclusion,"in future work, we are interested in modelling the extent to which a social interaction is caused by geographical proximity (e.g.using user鈥搖ser gates)."
p18-1187,2018,6 Conclusion,we also showed that gcn and dcca are able to perform well under a minimal supervision scenario similar to real world applications by effectively using unlabelled data.
p18-1187,2018,6 Conclusion,"we ignored the context in which users interact with each other, and assumed all the connections to hold location homophily."
p18-1187,2018,6 Conclusion,"we proposed gcn, dcca and mlp-txt+net, three multiview, transductive, semi-supervised geolocation models, which use text and network information to infer user location in a joint setting."
p18-1187,2018,6 Conclusion,"we showed that joint modelling of text and network information outperforms network-only, text-only, and hybrid geolocation models as a result of modelling the interaction between text and network information."
p18-1188,2018,5 Conclusion,"for answer selection, we show that inserting the query with word overlap features using our external attention mechanism outperforms state-of-the-art systems that naturally also have access to this information."
p18-1188,2018,5 Conclusion,"our experiments with extractive document summarization and answer selection tasks validates our model in two ways: first, we demonstrate that external information is important to guide document modeling for natural language understanding tasks."
p18-1188,2018,5 Conclusion,"our model uses image captions and the title of the document for document summarization, and the query with word overlap features for answer selection and outperforms its counterparts that do not use this information."
p18-1188,2018,5 Conclusion,"second, our external attention mechanism successfully guides the learning of the document representation for the relevant end goal."
p18-1188,2018,5 Conclusion,we describe an approach to model documents while incorporating external information that informs the representations learned for the sentences in the document.
p18-1188,2018,5 Conclusion,we implement our approach through an attention mechanism of a neural network architecture for modeling documents.
p18-1189,2018,6 Conclusion,"furthermore, the flexibility of our model enables intriguing exploration of a text corpus on us immigration."
p18-1189,2018,6 Conclusion,"our model demonstrates the tradeoff between perplexity, coherence, and sparsity, and outperforms slda in predicting document labels."
p18-1189,2018,6 Conclusion,we believe that our model and code will facilitate rapid exploration of document collections with metadata.
p18-1189,2018,6 Conclusion,we have presented a neural framework for generalized topic models to enable flexible incorporation of metadata with a variety of options.
p18-1189,2018,6 Conclusion,we take advantage of stochastic variational inference to develop a general algorithm for our framework such that variations do not require any model-specific algorithm derivations.
p18-1190,2018,6 Conclusions,a neural variational framework is introduced to train our model.
p18-1190,2018,6 Conclusions,"motivated by the connections between the proposed method and rate-distortion theory, we inject data-dependent noise into the bernoulli latent variable at the training stage."
p18-1190,2018,6 Conclusions,the effectiveness of our framework is demonstrated with extensive experiments.
p18-1190,2018,6 Conclusions,"this paper presents a first step towards end-to-end semantic hashing, where the binary/discrete constraints are carefully handled with an effective gradient estimator."
p18-1191,2018,8 Conclusion,"finally, we demonstrated that the validation stage of our crowdsourcing pipeline, in combination with our parser tuned for recall, can be used to add new annotations to the dataset, increasing recall."
p18-1191,2018,8 Conclusion,"in this paper, we demonstrated that qa-srl can be scaled to large datasets, enabling a new methodology for labeling and producing predicate-argument structures at a large scale."
p18-1191,2018,8 Conclusion,we demonstrated the utility of this data by training the first parser which is able to produce high-quality qa-srl structures.
p18-1191,2018,8 Conclusion,"we presented a new, scalable approach for crowdsourcing qa-srl, which allowed us to collect qa-srl bank 2.0, a new dataset covering over 250,000 question-answer pairs from over 64,000 sentences, in just 9 days."
p18-1192,2018,6 Conclusion and Future Work,"experimental results show that with the help of deep enhanced representation, our model outperforms the previous state-of-the-art models in both syntaxaware and syntax-agnostic situations."
p18-1192,2018,6 Conclusion and Future Work,"however, maybe we will never reach a satisfying conclusion, as whenever one proposes a syntax-agnostic srl system which can outperform all syntax-aware ones at then, always there comes argument that you have never fully explored creative new method to effectively exploit the syntax input."
p18-1192,2018,6 Conclusion and Future Work,"in addition, we consider the sem-f1/las ratio as a mean of evaluating syntactic contribution to srl, and true performance of srl independent of the quality of syntactic parser."
p18-1192,2018,6 Conclusion and Future Work,"this paper presents a simple and effective neural model for dependency-based srl, incorporating syntactic information with the proposed extended k-order pruning algorithm."
p18-1192,2018,6 Conclusion and Future Work,"though we again confirm the importance of syntax to srl with empirical experiments, we are aware that since (pradhan et al., 2005), the gap between syntax-aware and syntax-agnostic srl has been greatly reduced, from as high as 10% to only 1-2% performance loss in this work."
p18-1192,2018,6 Conclusion and Future Work,"with a large enough setting of k, our pruning algorithm will result in a syntax-agnostic setting for the argument labeling model, which smoothly unifies syntax-aware and syntax-agnostic srl in a consistent way."
p18-1193,2018,9 Discussion,"future modeling work may include using intermediate world states from previous turns in the interaction, which is required for some of the most complex references in the data."
p18-1193,2018,9 Discussion,"however, it is particularly suitable to recovering from biases acquired early during learning, for example due to biased action spaces, which is likely to lead to incorrect blame assignment in neural network policies."
p18-1193,2018,9 Discussion,"one possible direction for future work is to use an estimator to predict rewards for all actions, rather than observing them."
p18-1193,2018,9 Discussion,our learning approach requires additional reward observations in comparison to conventional reinforcement learning.
p18-1193,2018,9 Discussion,we propose a model to reason about contextdependent instructional language that display strong dependencies both on the history of the interaction and the state of the world.
p18-1193,2018,9 Discussion,"we propose to train our model using sestra, a learning algorithm that takes advantage of single-step reward observations to overcome learned biases in on-policy learning."
p18-1193,2018,9 Discussion,"when the domain and model are less susceptible to such biases, the benefit of the additional reward observations is less pronounced."
p18-1194,2018,7 Conclusions,"in this paper, we investigate different ways to combine nns and res for solving typical slu tasks."
p18-1194,2018,7 Conclusions,our experiments demonstrate that the combination clearly improves the nn performance in both the few-shot learning and the full dataset settings.
p18-1194,2018,7 Conclusions,"specifically, we observe that using res to guide the attention module works best for intent detection, and using retags as features is an effective approach for slot filling."
p18-1194,2018,7 Conclusions,"we provide interesting insights on how res of various forms can be employed to improve nns, showing that while simple res are very cost-effective, complex res generally yield better results."
p18-1194,2018,7 Conclusions,"we show that by exploiting the implicit knowledge encoded within res, one can significantly improve the learning performance."
p18-1195,2018,5 Conclusion,"experimental evaluation on image captioning and machine translation demonstrates the complementarity of sequence-level and token-level loss smoothing, improving over both the maximum likelihood and raml."
p18-1195,2018,5 Conclusion,"for the sequencelevel, which is computationally expensive, we introduced an efficient 鈥渓azy鈥 evaluation scheme, and introduced an improved re-sampling strategy."
p18-1195,2018,5 Conclusion,we generalized the sequence-level smoothing raml approach of norouzi et al.(2016) to the tokenlevel by smoothing the ground-truth target across semantically similar tokens.
p18-1195,2018,5 Conclusion,we investigated the use of loss smoothing approaches to improve over maximum likelihood estimation of rnn language models.
p18-1196,2018,7 Conclusion,"finally, we found that using a continuous probability density function can improve prediction accuracy of lms for numbers by substantially reducing the mean absolute percentage metric."
p18-1196,2018,7 Conclusion,"in this paper, we investigated several strategies for lms to model numerals and proposed a novel openvocabulary generative model based on a continuous probability density function."
p18-1196,2018,7 Conclusion,"our approaches in modelling and evaluation can be used in future work in tasks such as approximate information extraction, knowledge base completion, numerical fact checking, numerical question answering, and fraud detection."
p18-1196,2018,7 Conclusion,our code and data are available at: https://github.com/uclmr/ numerate-language-models.
p18-1196,2018,7 Conclusion,"we found that modelling numerals separately from other words through a hierarchical softmax can substantially improve the perplexity of lms, that different strategies are suitable for different contexts, and that a combination of these strategies can help improve the perplexity further."
p18-1196,2018,7 Conclusion,"we provided the first thorough evaluation of lms on numerals on two corpora, taking into account their high out-of-vocabulary rate and numerical value (magnitude)."
p18-1197,2018,8 Conclusion,"in conclusion, we proposed a novel progressive attention model on syntactic structures, and demonstrated its superior performance in semantic relatedness tasks."
p18-1197,2018,8 Conclusion,our work also provides empirical ingredients for potentially profound questions and debates on syntactic structures in linguistics.
p18-1198,2018,6 Conclusion,"for example, we found that bag-of-vectors is surprisingly good at capturing sentence-level properties, thanks to redundancies in natural linguistic input."
p18-1198,2018,6 Conclusion,"in future work, we would like to extend the probing tasks to other languages (which should be relatively easy, given that they are automatically generated), investigate how multi-task training affects probing task performance and leverage our probing tasks to find more linguistically-aware universal encoders."
p18-1198,2018,6 Conclusion,"in particular, we found that bilstm-max embeddings are already capturing interesting linguistic knowledge before training, and that, after training, they detect semantic acceptability without having been exposed to anomalous sentences before."
p18-1198,2018,6 Conclusion,"our results suggest that the encoders are capturing a wide range of properties, well above those captured by a set of strong baselines."
p18-1198,2018,6 Conclusion,"their purpose is not to encourage the development of ad-hoc models that attain top performance on them, but to help exploring what information is captured by different pre-trained encoders."
p18-1198,2018,6 Conclusion,"we further uncovered interesting patterns of correlation between the probing tasks and more complex 鈥渄ownstream鈥 tasks, and presented a set of intriguing findings about the linguistic properties of various embedding methods."
p18-1198,2018,6 Conclusion,"we hope that our publicly available probing task set will become a standard benchmarking tool of the linguistic properties of new encoders, and that it will stir research towards a better understanding of what they learn."
p18-1198,2018,6 Conclusion,we introduced a set of tasks probing the linguistic knowledge of sentence embedding methods.
p18-1198,2018,6 Conclusion,we performed an extensive linguistic evaluation of modern sentence encoders.
p18-1198,2018,6 Conclusion,"we showed that different encoder architectures trained with the same objective with similar performance can result in different embeddings, pointing out the importance of the architecture prior for sentence embeddings."
p18-1199,2018,5 Conclusion,"an important aspect of our work is that our framework does not depend on a specific form of the relation classifier, meaning that it is a plug-and-play technique that could be potentially applied to any relation extraction pipeline."
p18-1199,2018,5 Conclusion,"in experiments, we show that our framework boosts the performance of distant supervision relation extraction of various strong deep learning baselines on the widely used new york times - freebase dataset."
p18-1199,2018,5 Conclusion,"in this work, we propose a deep reinforcement learning framework for robust distant supervision."
p18-1199,2018,5 Conclusion,"more specifically, our goal is to teach the reinforcement agent to optimize the selection/redistribution strategy that maximizes the reward of boosting the performance of relation classification."
p18-1199,2018,5 Conclusion,"the intuition is that, in contrast to prior works that utilize only one instance per entity pair and use soft attention weights to select plausible distantly supervised examples, we describe a policy-based framework to systematically learn to relocate the false positive samples, and better utilize the unlabeled data."
p18-1200,2018,7 Conclusion,"furthermore, we have shown that the autoencoder learns low dimension sparse codings that can be easily explained; the joint training technique drives high-dimensional data toward low dimension manifolds; and the reduction of dimensionality may interact strongly with composition, help discovering compositional constraints and benefit from compositional training."
p18-1200,2018,7 Conclusion,we believe these findings provide insightful understandings of kb embedding models and might be applied to other neural networks beyond the kbc task.
p18-1200,2018,7 Conclusion,we have developed new training techniques and achieved state-of-the-art results on several kbc tasks with strong improvements in mean rank.
p18-1200,2018,7 Conclusion,we have investigated a dimension reduction technique which trains a kb embedding model jointly with an autoencoder.
p18-1201,2018,8 Conclusions and Future Work,"in the future, we will extend this framework to other information extraction problems."
p18-1201,2018,8 Conclusions and Future Work,"in this work, we take a fresh look at the event extraction task and model it as a generic grounding problem."
p18-1201,2018,8 Conclusions and Future Work,"to the best of our knowledge, this work is the first time that zero-shot learning has been applied to event extraction."
p18-1201,2018,8 Conclusions and Future Work,"we propose a transferable neural architecture, which leverages existing humanconstructed event schemas and manual annotations for a small set of seen types, and transfers the knowledge from the existing types to the extraction of unseen types, to improve the scalability of event extraction as well as to save human effort."
p18-1201,2018,8 Conclusions and Future Work,"without any annotation, our approach can achieve performance comparable to state-of-the-art supervised models trained on a large amount of labeled data."
p18-1202,2018,6 Conclusion,"the adaptation takes place in a common relation feature space, which builds the structural correspondences using syntactic relations among the words in each sentence."
p18-1202,2018,6 Conclusion,the model integrates an auxiliary task into representation learning of nodes in the dependency tree.
p18-1202,2018,6 Conclusion,we further develop a joint model to combine rnscn/rnscn+ with a sequential labeling model for terms extraction.
p18-1202,2018,6 Conclusion,"we propose a novel dependency-tree-based rnn, namely rnscn (or rnscn+), for domain adaptation."
p18-1203,2018,4 Conclusion,"as pointed out by sutton and barto (1998), the general problem here is a particular manifestation of the conflict between exploration and exploitation."
p18-1203,2018,4 Conclusion,"compared to previous work, our agent learns in a much more efficient way, using only a small number of real user interactions, which amounts to an affordable cost in many nontrivial domains."
p18-1203,2018,4 Conclusion,"in a planning context, exploration means trying actions that may improve the world model, whereas exploitation means trying to behave in the optimal way given the current model."
p18-1203,2018,4 Conclusion,one interesting topic for future research is exploration in planning.
p18-1203,2018,4 Conclusion,our strategy is based on the deep dyna-q (ddq) framework where planning is integrated into dialogue policy learning.
p18-1203,2018,4 Conclusion,"the effectiveness of ddq is validated by human-in-theloop experiments, demonstrating that a dialogue agent can efficiently adapt its policy on the fly by interacting with real users via deep rl."
p18-1203,2018,4 Conclusion,"to this end, we want the agent to explore in the environment, but not so much that the performance would be greatly degraded."
p18-1203,2018,4 Conclusion,"we need to deal with the challenge of adapting the world model in a changing environment, as exemplified by the domain extension problem (lipton et al., 2016)."
p18-1203,2018,4 Conclusion,we propose a new strategy for a task-completion dialogue agent to learn its policy by interacting with real users.
p18-1204,2018,5 Conclusion and Future Work,"furthermore, the typed decoders are applicable to the settings where word types can be easily obtained, such as in emotional text generation (ghosh et al., 2017; zhou et al., 2018b)."
p18-1204,2018,5 Conclusion and Future Work,"results show that our models can generate more appropriate questions, with richer topics, thereby more likely to elicit further interactions."
p18-1204,2018,5 Conclusion and Future Work,"the decoders firstly estimate a type distribution over word types, and then use the type distribution to modulate the final word generation distribution."
p18-1204,2018,5 Conclusion and Future Work,the detector can be implemented by a classifier or some heuristics.
p18-1204,2018,5 Conclusion and Future Work,the work can be extended to multi-turn conversation generation by including an additional detector predicting when to ask a question.
p18-1204,2018,5 Conclusion and Future Work,"through modeling the word types in language generation, the proposed decoders are able to question with various patterns and address novel yet related transitional topics in a generated question."
p18-1204,2018,5 Conclusion and Future Work,we present two typed decoders to generate questions in open-domain conversational systems.
p18-1205,2018,6 Conclusion & Discussion,"because we collect paraphrases of the profiles, they cannot be trivially matched; indeed, we believe the original and rephrased profiles are interesting as a semantic similarity dataset in their own right."
p18-1205,2018,6 Conclusion & Discussion,"because we have paired human generated profiles and conversations, the data aids the construction of agents that have consistent personalities and viewpoints."
p18-1205,2018,6 Conclusion & Discussion,"furthermore, predicting the profiles from a conversation moves chitchat tasks in the direction of goal-directed dialogue, which has metrics for success."
p18-1205,2018,6 Conclusion & Discussion,"in this work we have introduced the personachat dataset, which consists of crowd-sourced dialogues where each participant plays the part of an assigned persona; and each (crowd-sourced) persona has a word-distinct paraphrase."
p18-1205,2018,6 Conclusion & Discussion,"on the other hand, we show that models trained on persona-chat (with or without personas) are more engaging than models trained on dialogue from other resources (movies, twitter)."
p18-1205,2018,6 Conclusion & Discussion,we believe persona-chat will be a useful resource for training components of future dialogue systems.
p18-1205,2018,6 Conclusion & Discussion,"we hope that the data will aid training agents that can ask questions about users鈥 profiles, remember the answers, and use them naturally in conversation."
p18-1205,2018,6 Conclusion & Discussion,"we test various baseline models on this dataset, and show that models that have access to their own personas in addition to the state of the dialogue are scored as more consistent by annotators, although not more engaging."
p18-1206,2018,6 Conclusions,"in future work, we plan to incorporate various types of context (e.g.anaphora, device-specific capabilities) and dialogue history into a large-scale nlu system."
p18-1206,2018,6 Conclusions,"the model also addresses practical constraints of having a low memory footprint, low latency and being easily parallelized, all of which are important characteristics for real-time production systems."
p18-1206,2018,6 Conclusions,"we have also shown that the model can be extended efficiently and incrementally for new domains, saving multiple orders of magnitude in terms of training time."
p18-1206,2018,6 Conclusions,we have described a neural model architecture to address large-scale skill classification in an ipda used by tens of millions of users every day.
p18-1206,2018,6 Conclusions,we have described how personalization features and an attention mechanism can be used for handling ambiguity between domains.
p18-1207,2018,7 Conclusion,"in this paper, we proposed a deep multimodal architecture with hierarchical attention for sentiment and emotion classification."
p18-1207,2018,7 Conclusion,"our model aligned the text and audio at the word-level and applied attention distributions on textual word vectors, acoustic frame vectors, and acoustic word vectors."
p18-1207,2018,7 Conclusion,our model outperforms the state-ofthe-art methods and provides effective visualization of modality-specific features and fusion feature interpretation.
p18-1207,2018,7 Conclusion,we introduced three fusion strategies with a cnn structure to combine word-level features to classify emotions.
p18-1208,2018,6 Conclusion,"aside analysis of fusion, dfg was trained in the memory fusion network pipeline and showed superior performance in sentiment analysis and competitive performance in emotion recognition."
p18-1208,2018,6 Conclusion,"cmumosei consists of 23,453 annotated sentences from more than 1000 online speakers and 250 different topics."
p18-1208,2018,6 Conclusion,in our studies we investigated the behavior of modalities in interacting with each other using built-in efficacies of dfg.
p18-1208,2018,6 Conclusion,in this paper we presented the largest dataset of multimodal sentiment analysis and emotion recognition called cmu multimodal opinion sentiment and emotion intensity (cmu-mosei).
p18-1208,2018,6 Conclusion,one such study was presented in this paper where we analyzed the structure of multimodal fusion in sentiment analysis and emotion recognition.
p18-1208,2018,6 Conclusion,the dataset expands the horizons of human multimodal language studies in nlp.
p18-1208,2018,6 Conclusion,this was done using a novel interpretable fusion mechanism called dynamic fusion graph (dfg).
p18-1209,2018,6 Conclusion,"furthermore, lmf demonstrates a significant decrease in computational complexity from exponential to linear time."
p18-1209,2018,6 Conclusion,"future work on similar topics could explore the applications of using low-rank tensors for attention models over tensor representations, as they can be even more memory and computationally intensive."
p18-1209,2018,6 Conclusion,"in practice, lmf effectively improves the training and testing efficiency compared to tfn which performs multimodal fusion with tensor representations."
p18-1209,2018,6 Conclusion,"in this paper, we introduce a low-rank multimodal fusion method that performs multimodal fusion with modality-specific low-rank factors."
p18-1209,2018,6 Conclusion,lmf achieves competitive results across different multimodal tasks.
p18-1209,2018,6 Conclusion,lmf scales linearly in the number of modalities.
p18-1210,2018,6 Conclusion,"as the conjunctions chosen by participants convey senses that differ from those of the discourse adverbials, we also provided evidence for the simultaneous availability of multiple coherence relations that arise from both explicit signals and inference."
p18-1210,2018,6 Conclusion,our current experiments have provided some such evidence.
p18-1210,2018,6 Conclusion,"specifically, we have shown that participant responses to systematically manipulated passages involving discourse adverbials can be explained in terms of both the lexical semantics of discourse adverbials and properties of the passages that contain them."
p18-1210,2018,6 Conclusion,"we hope the reader is now convinced that, in both psycholinguistic research on discourse coherence and computational work on discourse parsing, one needs to identify and examine evidence for coherence involving more than one discourse relation."
p18-1210,2018,6 Conclusion,"while our previous work showed that multiple discourse relations can hold between two segments 鈥 relations at the same semantic level, simultaneously available to a reader 鈥 we provided no evidence as to what influences the particular relations that are taken to be available."
p18-1211,2018,5 Conclusion,"however, the method outperforms some previous approaches on document understanding tasks, even while ignoring syntactic structure within sentences."
p18-1211,2018,5 Conclusion,"more generally, similar approaches can explore a wider range of scenarios involving sequences of text."
p18-1211,2018,5 Conclusion,"the ability to visualize learning is a key component of our method, which can find significant applications in data mining and data-discovery in large text collections."
p18-1211,2018,5 Conclusion,"the approach is coarse, and does not have explicit models for important elements such as entities and events in a discourse."
p18-1211,2018,5 Conclusion,we have presented a simple model for extracting and visualizing latent discourse structure from unlabeled documents.
p18-1211,2018,5 Conclusion,"while here our focus was on learning discourse structures at the document level, similar methods can also be used at other scales, such as for syntactic or morphological analysis."
p18-1212,2018,6 Conclusion,"to show the benefit of tcr, we have developed a new dataset that jointly annotates temporal and causal annotations, and then exhibited that tcr can improve both temporal and causal components."
p18-1212,2018,6 Conclusion,"we hope that this notable improvement can foster more interest in jointly studying multiple aspects of events (e.g., event sequencing, coreference, parent-child relations) towards the goal of understanding events in natural language."
p18-1212,2018,6 Conclusion,"we presented a novel joint framework, temporal and causal reasoning (tcr), using ccms and ilp to the extraction problem of temporal and causal relations between events."
p18-1213,2018,9 Conclusion,"importantly, we show that modeling character-specific context and pretraining on freeresponse data can boost labeling performance."
p18-1213,2018,9 Conclusion,this dataset contains over 300k low-level annotations for character motivations and emotional reactions.
p18-1213,2018,9 Conclusion,we present a large scale dataset as a resource for training and evaluating mental state tracking of characters in short commonsense stories.
p18-1213,2018,9 Conclusion,we provide benchmark results on this new resource.
p18-1213,2018,9 Conclusion,"while our work only use information present in our dataset, we view our dataset as a future testbed for evaluating models trained on any number of resources for learning common sense about emotional reactions and motivations."
p18-1214,2018,5 Conclusion,"in the future, we plan to enrich the architecture of dazer to allow few-shot document filtering by incorporating several labeled examples."
p18-1214,2018,5 Conclusion,"in this paper, we propose a novel deep relevance model for zero-shot document filtering, named dazer."
p18-1214,2018,5 Conclusion,the experimental results over two different tasks validate the superiority of the proposed model.
p18-1214,2018,5 Conclusion,"to enable dazer to capture conceptual relevance and generalize well to unseen categories, two kinds of feature interactions, a gated convolutional network and an categoryindependent adversarial learning are devised."
p18-1215,2018,5 Conclusion,"in addition, we also analyze what factors affect the optimal window size of drnn and present an empirical method to search it."
p18-1215,2018,5 Conclusion,"in this paper, we incorporate position-invariance into rnn, so that our proposed model drnn can both capture key phrases and long-term dependencies."
p18-1215,2018,5 Conclusion,"the experimental results show that our proposed model outperforms cnn and rnn models, and achieve the best performance in seven large-scale text classification datasets."
p18-1215,2018,5 Conclusion,we conduct experiments to compare the effects of different recurrent units and pooling operations.
p18-1216,2018,6 Conclusions,"compared with the previous methods, our leam algorithm requires much lower computational cost, and achieves better if not comparable performance relative to the state-of-the-art."
p18-1216,2018,6 Conclusions,"in this work, we first investigate label embeddings for text representations, and propose the label-embedding attentive models."
p18-1216,2018,6 Conclusions,"it embeds the words and labels in the same joint space, and measures the compatibility of word-label pairs to attend the document representations."
p18-1216,2018,6 Conclusions,the learned attention is highly interpretable: highlighting the most informative words in the text sequence for the downstream classification task.
p18-1216,2018,6 Conclusions,the learning framework is tested on several large standard datasets and a real clinical text application.
p18-1217,2018,5 Conclusion,"compared with other word embedding based and neural network based topic models, it overcomes the computation complexity of topic models, and improve the generation of representation over short documents."
p18-1217,2018,5 Conclusion,experimental results demonstrate the effectiveness and efficiency of our models.
p18-1217,2018,5 Conclusion,"for future work, we are interested in various extensions, including combining stc with autoencoding variational bayes (avb)."
p18-1217,2018,5 Conclusion,"in this paper, we propose a novel neural sparsityenhanced topic model nstc, which improves stc by incorporating the neural network and word embeddings."
p18-1217,2018,5 Conclusion,we present three variants of nstc to illustrate the great flexibility of our framework.
p18-1218,2018,7 Conclusion,"our approach has beaten two strong baselines in two downstream applications, concept-project matching and summary-research paper matching."
p18-1218,2018,7 Conclusion,the challenge we address is to bridge the gap between detailed long texts and its abstraction with hidden topics.
p18-1218,2018,7 Conclusion,we incorporate domain knowledge into the matching system to gain further performance improvement.
p18-1218,2018,7 Conclusion,we propose a novel approach to matching documents and summaries.
p18-1219,2018,7 Conclusion and Future Work,"although well defined, predicting the score of these properties for a text is quite challenging."
p18-1219,2018,7 Conclusion and Future Work,"in future, we plan to use use approaches, like multi-task learning (mishra et al., 2018), in estimating gaze features and using those estimated features for text quality prediction."
p18-1219,2018,7 Conclusion and Future Work,"it has been established that cognitive information such as gaze behaviour can help in such subjective tasks (mishra et al., 2013, 2016)."
p18-1219,2018,7 Conclusion and Future Work,"the approach estimates the overall quality on the basis of three properties - organization, coherence and cohesion."
p18-1219,2018,7 Conclusion and Future Work,this indicated that gaze behaviour is more reliable when the reader has understood the text.
p18-1219,2018,7 Conclusion and Future Work,"to evaluate this hypothesis, we collected gaze behaviour data and evaluated the predictions using only the text-based features."
p18-1219,2018,7 Conclusion and Future Work,"to the best of our knowledge, our work is pioneering in using gaze information for predicting text quality rating."
p18-1219,2018,7 Conclusion and Future Work,"we found out that, in all cases, there was an improvement in the agreement scores when the participant who rated the text showed full understanding, as compared to partial understanding, using only the gaze features and the text+gaze features."
p18-1219,2018,7 Conclusion and Future Work,we hypothesized that gaze behavior will assist in predicting the scores of text quality.
p18-1219,2018,7 Conclusion and Future Work,we presented a novel approach to predict reader鈥檚 rating of texts.
p18-1219,2018,7 Conclusion and Future Work,"when we took gaze behaviour into account, we were able to significantly improve our predictions of organization, coherence, cohesion and quality."
p18-1220,2018,6 Conclusions,"an attention-based sequence-to-sequence model is applied for single-input correction, based on which a strategy of multi-input attention combination is designed to correct multiple input sequences simultaneously."
p18-1220,2018,6 Conclusions,"experimental results on historical books and newspapers show that these unsupervised approaches significantly improve ocr accuracy and, when multiple inputs are available, achieve performance comparable to supervised methods."
p18-1220,2018,6 Conclusions,"the proposed strategy naturally incorporates aligning, correcting, and voting among multiple sequences, and is thus effective in improving the correction performance for corpora containing duplicated text."
p18-1220,2018,6 Conclusions,"we have proposed an unsupervised framework for ocr error correction, which can handle both single-input and multi-input correction tasks."
p18-1220,2018,6 Conclusions,we propose two ways of training the correction model without human annotation by exploiting the duplication in the corpus.
p18-1221,2018,6 Conclusion,"applications with lots of named entities, thus, obviously suffer."
p18-1221,2018,6 Conclusion,"in this work, we propose to leverage the type information of such named entities to build an effective language model."
p18-1221,2018,6 Conclusion,language model often lacks in performance to predict entity names correctly.
p18-1221,2018,6 Conclusion,our evaluation and case studies confirm that the type information of the named entities captures inherent text features too which leads to learn intrinsic text pattern and improve the performance of overall language model.
p18-1221,2018,6 Conclusion,"since similar entities have the same type, the vocabulary size of a type based language model reduces significantly."
p18-1221,2018,6 Conclusion,the prediction accuracy of the type model increases significantly with such reduced vocabulary size.
p18-1221,2018,6 Conclusion,"then, using the entity type information as prior we build another language model which predicts the true entity name according to the conditional probability distribution."
p18-1222,2018,6 Conclusion,"for evaluation, paper classification and citation recommendation are conducted on three academic paper datasets."
p18-1222,2018,6 Conclusion,further analyses also demonstrate that possessing the four properties helps h-d2v outperform other models.
p18-1222,2018,6 Conclusion,"in doing so, the learned embeddings satisfy all criteria, which no existing model is able to."
p18-1222,2018,6 Conclusion,results confirm the effectiveness of our approach.
p18-1222,2018,6 Conclusion,"to meet all four criteria, we propose a general approach, hyperdoc2vec, which assigns two vectors to each hyper-doc and models citations in a straightforward manner."
p18-1222,2018,6 Conclusion,we focus on the hyper-doc embedding problem.
p18-1222,2018,6 Conclusion,"we propose that hyper-doc embedding algorithms should be content aware, context aware, newcomer friendly, and context intent aware."
p18-1223,2018,7 Conclusions,edrm inherits entity-oriented search to match query and documents with bag-of-words and bag-of-entities in neural ranking models.
p18-1223,2018,7 Conclusions,"however, the knowledge graph semantics, introduced by the description and type embeddings, provide novel ranking signals that greatly improve the generalization ability of neural rankers in difficult scenarios."
p18-1223,2018,7 Conclusions,it leads to a data-driven combination of entity-oriented search and neural information retrieval.
p18-1223,2018,7 Conclusions,our experiments on the sogou search log and cn-dbpedia demonstrate edrm鈥檚 effectiveness and generalization ability over two state-of-theart neural ranking models.
p18-1223,2018,7 Conclusions,our further analyses reveal that the generalization ability comes from the integration of knowledge graph semantics.
p18-1223,2018,7 Conclusions,the knowledge graph semantics are integrated as distributed representations of entities.
p18-1223,2018,7 Conclusions,the neural model leverages these semantics to help document ranking.
p18-1223,2018,7 Conclusions,"the neural ranking models can effectively model n-gram matches between query and document, which overlaps with part of the ranking signals from entity-based matches: solely adding the entity names may not improve the ranking accuracy much."
p18-1223,2018,7 Conclusions,this paper preliminarily explores the role of structured semantics in deep learning models.
p18-1223,2018,7 Conclusions,"this paper presents edrm, the entity-duet neural ranking model that incorporating knowledge graph semantics into neural ranking systems."
p18-1223,2018,7 Conclusions,"though mainly fouced on search, we hope our findings shed some lights on a potential path towards more intelligent neural systems and will motivate more explorations in this direction."
p18-1223,2018,7 Conclusions,"using user clicks from search logs, the whole model鈥攖he integration of knowledge graph semantics and the neural ranking networks鈥 is trained end-to-end."
p18-1224,2018,6 Conclusions,"our neural-network-based model for natural language inference with external knowledge, namely kim, achieves the state-of-the-art accuracies."
p18-1224,2018,6 Conclusions,"the model is equipped with external knowledge in its main components, specifically, in calculating coattention, collecting local inference, and composing inference."
p18-1224,2018,6 Conclusions,the proposed model of infusing neural networks with external knowledge may also help shed some light on tasks other than nli.
p18-1224,2018,6 Conclusions,we provide detailed analyses on our model and results.
p18-1225,2018,6 Conclusion,"our rule-based generators can be expanded to cover more patterns and phenomena, and the seq2seq generator extended to incorporate per-example loss for adversarial training."
p18-1225,2018,6 Conclusion,"our seq2seq and knowledge-guided example generators, trained in an end-to-end fashion, can be used to make any base entailment model more robust."
p18-1225,2018,6 Conclusion,"the effectiveness of this approach is demonstrated by the significant improvement it achieves on both snli and scitail, especially in the low to medium data regimes."
p18-1225,2018,6 Conclusion,we introduced an adversarial training architecture for textual entailment.
p18-1226,2018,6 Conclusion and Discussions,"both n-grams construct a word vector representation by computing the average of n-grams, and these vectors are trained by subword-level information skip-gram."
p18-1226,2018,6 Conclusion and Discussions,"by leveraging both features, our method produces word vectors reflecting linguistic features effectively, and thus, outperforms previous word-level approaches."
p18-1226,2018,6 Conclusion and Discussions,decomposing korean word into jamo-level or character unigram helps capturing syntactic information.
p18-1226,2018,6 Conclusion and Discussions,"for example, korean words add a character to the root of the word (e.g., 鈥-鞚鈥 subjective case, 鈥-鞐堚 for past tense 鈥-鞁-鈥 for honorific, 鈥-頌鈥 for voice, and 鈥-瓿-鈥 for verb ending form.)"
p18-1226,2018,6 Conclusion and Discussions,"furthermore, sentiment classification results of our work indicate that the representative power of the vectors positively contributes to downstream nlp task."
p18-1226,2018,6 Conclusion and Discussions,"generally, informal korean text contains intentional typos (鈥橂鞛囯嫟鈥榙elicious鈥 with typo鈥), stand-alone jamo as a character, (鈥樸厠銋媗ol鈥) and segmentation errors.(鈥橁皺 鞚搓皜雼も榞o together鈥 without space鈥)."
p18-1226,2018,6 Conclusion and Discussions,"hence, the inter-character jamolevel n-grams also help capture these features."
p18-1226,2018,6 Conclusion and Discussions,"in this paper, we present how to decompose a korean character into a sequence of jamos with empty jongsung symbols, then extract characterlevel n-grams and intercharacter jamo-level ngrams from that sequence."
p18-1226,2018,6 Conclusion and Discussions,"lastly, since our method can capture korean syntactic features through jamo and character n-grams, we can apply the same idea to other tasks such as pos tagging and parsing."
p18-1226,2018,6 Conclusion and Discussions,"meanwhile, we will further train our model over noisy data and investigate how it is dealing with noisy words."
p18-1226,2018,6 Conclusion and Discussions,"on the other hand, larger n-grams such as characterlevel trigram will learn unique meaning of that word since those larger component of the word will mostly occur with that word."
p18-1226,2018,6 Conclusion and Discussions,"prior to evaluating the performance of the vectors, we developed test set for word similarity and word analogy tasks for korean."
p18-1226,2018,6 Conclusion and Discussions,"since korean words are divisible once more into grapheme level, resulting in longer sequence of jamos for a given word, we plan to explore potential applicability of deeper level of subword information in korean."
p18-1226,2018,6 Conclusion and Discussions,"since these errors occur frequently, it is important to apply the vectors in training nlp models over real-word data."
p18-1226,2018,6 Conclusion and Discussions,"specifically, the vectors using both jamo and character-level information can represent syntactic features more precisely even in an agglutinative language."
p18-1226,2018,6 Conclusion and Discussions,"then composed word can be reduced to have fewer characters by transforming jamos, such as 鈥橂悩鞐 雼も to 鈥橂悙雼も."
p18-1226,2018,6 Conclusion and Discussions,we demonstrated the effectiveness of the learned word vectors in capturing the semantic and syntactic information by evaluating these vectors with word similarity and word analogy tasks.
p18-1226,2018,6 Conclusion and Discussions,"we plan to apply these vectors for various neural network based nlp models, such as conversation modeling."
p18-1227,2018,6 Conclusion and Future Work,"in our experiments, our methods achieved promising results and outperformed the state of the art on sememe prediction, especially for low-frequency words."
p18-1227,2018,6 Conclusion and Future Work,"in the future, we will explore methods of exploiting internal information in other languages.(4) we believe that sememes are universal for all human languages."
p18-1227,2018,6 Conclusion and Future Work,"in the future, we will take structured annotations into account.(2) it would be meaningful to take more information into account for blending external and internal information and design more sophisticated methods.(3) besides chinese, many other languages have rich subword-level information."
p18-1227,2018,6 Conclusion and Future Work,"in this paper, we introduced character-level internal information for lexical sememe prediction in chinese, in order to alleviate the problems caused by the exclusive use of external information."
p18-1227,2018,6 Conclusion and Future Work,we evaluated our csp framework on the classical manually annotated sememe kb hownet.
p18-1227,2018,6 Conclusion and Future Work,we proposed a character-enhanced sememe prediction (csp) framework which integrates both internal and external information for lexical sememe prediction and proposed two methods for utilizing internal information.
p18-1227,2018,6 Conclusion and Future Work,we will explore a general framework to recommend and utilize sememes for other nlp tasks.
p18-1227,2018,6 Conclusion and Future Work,"we will explore the following research directions in the future: (1) concepts in hownet are annotated with hierarchical structures of senses and sememes, but those are not considered in this paper."
p18-1228,2018,6 Discussion and Conclusion,"although our work examine communities in reddit, we focus on the difference of the word semantics."
p18-1228,2018,6 Discussion and Conclusion,"despite these limitations, we identify the following key implications."
p18-1228,2018,6 Discussion and Conclusion,"first, semaxis offers a framework to examine texts on diverse semantic axes beyond the sentiment axis, through the 732 systematically induced semantic axes that capture common antonyms."
p18-1228,2018,6 Discussion and Conclusion,"first, we performed the quantitative evaluation only with the sentiment axis, even though we supplemented it with more qualitative examples."
p18-1228,2018,6 Discussion and Conclusion,our study may facilitate further investigations on context-dependent text analysis techniques and applications.
p18-1228,2018,6 Discussion and Conclusion,"second, the unsupervised nature of semaxis provides a powerful way to build lexicons of any semantic axis, including the sentiment axis, for non-english languages, particularly the resourcescarce languages."
p18-1228,2018,6 Discussion and Conclusion,"secondly, gaffney and matias (2018) recently reported the reddit data used in this study is incomplete."
p18-1228,2018,6 Discussion and Conclusion,"the authors suggest using the data with caution, particularly when analyzing user interactions."
p18-1228,2018,6 Discussion and Conclusion,there are two major limitations.
p18-1228,2018,6 Discussion and Conclusion,"thus, we believe the effect of deleted comment would be marginal in our analyses."
p18-1228,2018,6 Discussion and Conclusion,we have also demonstrated that our approach can reveal nuanced context-dependence of words through the lens of numerous semantic axes.
p18-1228,2018,6 Discussion and Conclusion,we have proposed semaxis to examine a nuanced representation of words based on diverse semantic axes.
p18-1228,2018,6 Discussion and Conclusion,we have shown that semaxis can construct good domain-specific sentiment lexicons by projecting words on the sentiment axis.
p18-1228,2018,6 Discussion and Conclusion,we hope that semaxis can facilitate research on other semantic axes so that we will have labeled datasets for other axes as well.
p18-1228,2018,6 Discussion and Conclusion,"we used the sentiment axis because it is better studied and more methods exist, but ideally it would be better to perform evaluation across many semantic axes."
p18-1229,2018,7 Conclusion and Future Work,experiments on two public datasets from different domains show that our approach outperforms state-of-the-art methods significantly.
p18-1229,2018,7 Conclusion and Future Work,"in addition, study on how to effectively encode induction history will be interesting."
p18-1229,2018,7 Conclusion and Future Work,"in the future, we will explore more strategies towards term pair selection (e.g., allow the rl agent to remove terms from the taxonomy) and reward function design."
p18-1229,2018,7 Conclusion and Future Work,the error propagation between two phases is thus effectively reduced and the global taxonomy structure is better captured.
p18-1229,2018,7 Conclusion and Future Work,this paper presents a novel end-to-end reinforcement learning approach for automatic taxonomy induction.
p18-1229,2018,7 Conclusion and Future Work,"unlike previous two-phase methods that treat term pairs independently or equally, our approach learns the representations of term pairs by optimizing a holistic tree metric over the training taxonomies."
p18-1230,2018,5 Conclusions and Future Work,"in the next step, we will consider integrating the rich structural information into the neural network for word sense disambiguation."
p18-1230,2018,5 Conclusions and Future Work,"in this paper, we seek to address the problem of integrating the glosses knowledge of the ambiguous word into a neural network for wsd."
p18-1230,2018,5 Conclusions and Future Work,"in this way, we not only make use of labeled context data but also exploit the background knowledge to disambiguate the word sense."
p18-1230,2018,5 Conclusions and Future Work,results on four english all-words wsd data sets show that our best model outperforms the existing methods.
p18-1230,2018,5 Conclusions and Future Work,there is still one challenge left for the future.
p18-1230,2018,5 Conclusions and Future Work,we further extend the gloss information through its semantic relations in wordnet to better infer the context.
p18-1230,2018,5 Conclusions and Future Work,"we just extract the gloss, missing the structural properties or graph information of lexical resources."
p18-1231,2018,8 Conclusion,"in the future, we will extend our model so that it can project multi-word phrases, as well as single words, which could help with negations and modifiers."
p18-1231,2018,8 Conclusion,"this model requires less parallel data than mt and performs better than other state-of-the-art methods with similar data requirements, an average of 14 percentage points in f1 on binary and 4 pp on 4-class crosslingual sentiment analysis."
p18-1231,2018,8 Conclusion,"we have also performed a phenomena-driven error analysis which showed that blse is better than artetxe and barista at transferring sentiment, but assigns too much sentiment to functional words."
p18-1231,2018,8 Conclusion,"we have presented a new model, blse, which is able to leverage sentiment information from a resource-rich language to perform sentiment analysis on a resource-poor target language."
p18-1232,2018,6 Conclusions,"compared with existing domain-sensitive methods, our model detects domain-common words according to not only similar context words but also sentiment information."
p18-1232,2018,6 Conclusions,"compared with existing sentiment-aware embeddings, our model can distinguish domain-common and domain-specific words with the consideration of varied semantics across multiple domains."
p18-1232,2018,6 Conclusions,"moreover, our learned embeddings considering sentiment information can distinguish words with similar syntactic context but opposite sentiment polarity."
p18-1232,2018,6 Conclusions,the experimental results demonstrate the advantages of our approach.
p18-1232,2018,6 Conclusions,"we have conducted experiments on two downstream sentiment classification tasks, namely review sentiment classification and lexicon term sentiment classification."
p18-1232,2018,6 Conclusions,we propose a new method of learning domainsensitive and sentiment-aware word embeddings.
p18-1233,2018,5 Conclusion,"in contrast with most of the previous methods, which pay more attention to domain invariant information, we showed that domain specific information could also be beneficially used in the domain adaptation task with a small amount of in-domain labeled data."
p18-1233,2018,5 Conclusion,"in this work, we investigated the importance of domain specific information for domain adaptation."
p18-1233,2018,5 Conclusion,sentiment analysis experiments demonstrated the effectiveness of this method.
p18-1233,2018,5 Conclusion,"specifically, we proposed a novel method, based on the cmd metric, to simultaneously extract domain invariant feature and domain specific feature for target domain data."
p18-1233,2018,5 Conclusion,"with these two different features, we performed co-training with labeled data from the source domain and a small amount of labeled data from the target domain."
p18-1234,2018,8 Conclusions and Future Work,"gtru can effectively control the sentiment flow according to the given aspect information, and two convolutional layers model the aspect and sentiment information separately."
p18-1234,2018,8 Conclusions and Future Work,how to leverage large-scale sentiment lexicons in neural networks would be our future work.
p18-1234,2018,8 Conclusions and Future Work,"in this paper, we proposed an efficient convolutional neural network with gating mechanisms for acsa and atsa tasks."
p18-1234,2018,8 Conclusions and Future Work,we prove the performance improvement compared with other neural models by extensive experiments on semeval datasets.
p18-1235,2018,5 Conclusions,"deep neural networks are widely used in sentiment polarity classification, but suffer from their dependence on very large annotated training corpora."
p18-1235,2018,5 Conclusions,"in this paper, we study how to incorporate extrinsic cues into the network, beyond just generic word embeddings."
p18-1235,2018,5 Conclusions,our embeddings and multilingual datasets are freely available from http: //gerard.demelo.org/sentiment/.
p18-1235,2018,5 Conclusions,our experiments show that this can lead to gains across a number of different languages and domains.
p18-1235,2018,5 Conclusions,we have found that this is best achieved using a dual-module approach that encourages the learning of models with favourable generalization abilities.
p18-1236,2018,6 Conclusion,our experimental results show that our model performs significantly better than previous models.
p18-1236,2018,6 Conclusion,"these improvements increase when the level of sparsity in data increases, which confirm that hcsc is able to deal with the cold-start problem."
p18-1236,2018,6 Conclusion,"we propose hybrid contextualized sentiment classifier (hcsc) with a fast word encoder which contextualizes words to contain both short and long range word dependency features, and an attention mechanism called cold-start aware attention (csaa) which considers the existence of the cold-start problem among users and products by using a shared vector and a frequency-guided selective gate, in addition to the original distinct vector."
p18-1237,2018,5 Discussion and Conclusion,"also, our model helps analyzing the influence of user interaction and behavior on the effectiveness of discussion decisions."
p18-1237,2018,5 Discussion and Conclusion,"also, users often attack other turns, instead of considering neutral acts such as clarifications of misunderstandings."
p18-1237,2018,5 Discussion and Conclusion,"besides the model, we created two large-scale corpora: the webis-wikidiscussions-18 corpus, including the entire set of wikipedia discussions (at the time of parsing) with annotated discussion structure and metadata, and the webis-wikidebate-18 corpus, where turns are labeled for their discourse acts, argumentative relations, and frames."
p18-1237,2018,5 Discussion and Conclusion,"finally, we operationalized our wikipedia discussion model in three support vector machine classifiers with tailored features."
p18-1237,2018,5 Discussion and Conclusion,"for example, some wikipedia users focus on the frame 鈥榳ell written鈥 while ignoring others, which may negatively affect the accuracy of an article鈥檚 content."
p18-1237,2018,5 Discussion and Conclusion,"for example, the consistent usage of the most common user tags in wikipedia discussions helps originating concepts manually."
p18-1237,2018,5 Discussion and Conclusion,"for example, when writing a scientific paper, possible frames are the 鈥榳riting quality鈥 or the 鈥榲erifiability of content and citations鈥."
p18-1237,2018,5 Discussion and Conclusion,"in contrast, other metadata might require the use of computational methods, such as clustering, keyphrase extraction, and textual entailment."
p18-1237,2018,5 Discussion and Conclusion,"in future work, we plan to study how to distinguish effective from ineffective discussions based on our model as well as how to learn from the strategies used in successful discussions, in order to predict the best next deliberative move in an ongoing discussion."
p18-1237,2018,5 Discussion and Conclusion,"many categories in our model will apply to deliberative discussions in general, particularly the discourse acts and argumentative relations."
p18-1237,2018,5 Discussion and Conclusion,our experiment results confirm that categories of our model can be predicted successfully.
p18-1237,2018,5 Discussion and Conclusion,"unlike previous approaches to the modeling of discussions on wikipedia, our model decouples the three principle dimensions of discussions: discourse acts, argumentative relations, and frames."
p18-1237,2018,5 Discussion and Conclusion,"we argue that the distinction of these dimensions is key to develop tool support for discussion participants, for example, for recommending the best possible move in an ongoing discussion."
p18-1237,2018,5 Discussion and Conclusion,"we believe that these corpora will help foster research on tasks such as argument mining, among others."
p18-1237,2018,5 Discussion and Conclusion,"we expect the general derivation steps to be the same, whereas the techniques applied within each step may differ depending on the types, frequency, and quality of metadata."
p18-1237,2018,5 Discussion and Conclusion,"while our approach to modeling argumentation strategies in deliberative discussions may seem wikipedia-specific, the derivation of concepts and categories from metadata can be transferred to other online discussion platforms."
p18-1237,2018,5 Discussion and Conclusion,"while the found frames are more wikipedia-specific, similar play a role on collaborative writing platforms."
p18-1238,2018,6 Conclusions,"the results indicate that such models achieve better performance, and avoid some of the pitfalls seen with coco-trained models, such as object hallucination."
p18-1238,2018,6 Conclusions,"we evaluate both the quality of the resulting image/caption pairs, as well as the performance of several image-captioning models when trained on the conceptual captions data."
p18-1238,2018,6 Conclusions,we hope that the availability of the conceptual captions dataset will foster considerable progress on the automatic image-captioning task.
p18-1238,2018,6 Conclusions,"we present a new image captioning dataset, conceptual captions, which has several key characteristics: it has around 3.3m examples, an order of magnitude larger than the coco image-captioning dataset; it consists of a wide variety of images, including natural images, product images, professional photos, cartoons, drawings, etc.; and, its captions are based on descriptions taken from original alt-text attributes, automatically transformed to achieve a balance between cleanliness, informativeness, and learnability."
p18-1240,2018,5 Conclusion,"in this paper, we study how to automatically generate textual reports for medical images, with the goal to help medical professionals produce reports more accurately and efficiently."
p18-1240,2018,5 Conclusion,"on two medical datasets containing radiology and pathology images, we demonstrate the effectiveness of the proposed methods through quantitative and qualitative studies."
p18-1240,2018,5 Conclusion,"our proposed methods address three major challenges: (1) how to generate multiple heterogeneous forms of information within a unified framework, (2) how to localize abnormal regions and produce accurate descriptions for them, (3) how to generate long texts that contain multiple sentences or even paragraphs."
p18-1240,2018,5 Conclusion,"to cope with these challenges, we propose a multi-task learning framework which jointly predicts tags and generates descriptions."
p18-1240,2018,5 Conclusion,we develop a hierarchical lstm network that can more effectively capture long-range semantics and produce high quality long texts.
p18-1240,2018,5 Conclusion,we introduce a co-attention mechanism that can simultaneously explore visual and semantic information to accurately localize and describe abnormal regions.
p18-1241,2018,5 Conclusion,"in this paper, we proposed a novel algorithm, show-and-fool, for crafting adversarial examples and providing robustness evaluation of neural image captioning."
p18-1241,2018,5 Conclusion,"indeed, our show-and-fool algorithm1 can be easily extended to other applications with rnn or cnn+rnn architectures."
p18-1241,2018,5 Conclusion,our extensive experiments show that the proposed targeted caption and keyword methods yield high attack success rates while the adversarial perturbations are still imperceptible to human eyes.
p18-1241,2018,5 Conclusion,our method stands out from the well-studied adversarial learning on image classifiers and cnn models.
p18-1241,2018,5 Conclusion,"the high-quality and transferable adversarial examples in neural image captioning crafted by show-and-fool highlight the inconsistency in visual language grounding between humans and machines, suggesting a possible weakness of current machine vision and perception machinery."
p18-1241,2018,5 Conclusion,"to the best of our knowledge, this is the very first work on crafting adversarial examples for neural image captioning systems."
p18-1241,2018,5 Conclusion,we also show that attacking neural image captioning systems are inherently different from attacking cnn-based image classifiers.
p18-1241,2018,5 Conclusion,"we believe this paper provides potential means to evaluate and possibly improve the robustness (for example, by adversarial training or data augmentation) of a wide range of visual language grounding and other nlp models."
p18-1241,2018,5 Conclusion,we further demonstrate that showand-fool can generate highly transferable adversarial examples.
p18-1242,2018,Conclusion,experimental results have demonstrated the effectiveness of dsmn for geometric reasoning on synthetic data.
p18-1242,2018,Conclusion,"we have developed dsmn, a novel dnn that reasons in the visual space for answering questions."
p18-1242,2018,Conclusion,"we have introduced two synthetic qa datasets, floorplanqa and shapeintersection, that test a system鈥檚 ability to think visually."
p18-1242,2018,Conclusion,we have investigated how to use dnns for modeling visual thinking.
p18-1243,2018,6 Discussion,"all these factors contribute to the increased complexity of the learning task, making it non-trivial and already very challenging to existing approaches as shown by the experimental results."
p18-1243,2018,6 Discussion,"although it might be non-trivial to extend the proposed approach to real natural language directly, we regard this work as an initial step towards this ultimate ambitious goal and our game might shed some light on designing more advanced games or performing real-world data collection."
p18-1243,2018,6 Discussion,experimental results show that the proposed approach is effective for language acquisition with one-shot visual concept learning across several different settings compared with several baseline approaches.
p18-1243,2018,6 Discussion,"for our current design, although it is an artificial game, there is a reasonable amount of variations both within and across sessions, e.g., the object classes to be learned within a session, the presentation order of the selected classes, the sentence patterns and image instances to be used etc."
p18-1243,2018,6 Discussion,"in the current work, we have designed and used a computer game (synthetic task with synthetic language) for training the agent."
p18-1243,2018,6 Discussion,"this is achieved by purely 0 20 40 60 80 100 success rate (%)  reinforce  imitation  imitation+gaussian-rl  proposed -6 -5 -4 -3 -2 -1 0 1 reward  reinforce  imitation  imitation+gaussian-rl  proposed figure 8: test performance for sentence-level task with image variations (variation ratio=0.5).interacting with a teacher and learning from feedback arising naturally during interaction through joint imitation and reinforcement learning, with a memory augmented neural network."
p18-1243,2018,6 Discussion,this is mainly due to the fact that there is no existing dataset to the best of our knowledge that is adequate for developing our addressed interactive language learning and one-shot learning problem.
p18-1243,2018,6 Discussion,we have presented an approach for grounded language acquisition with one-shot visual concept learning in this work.
p18-1243,2018,6 Discussion,we plan to investigate the generalization and application of the proposed approach to more realistic environments with more diverse tasks in future work.
p18-1243,2018,6 Discussion,"while offering flexibility in training, one downside of using a synthetic task is its limited amount of variation compared with real-world scenarios with natural languages."
p18-1244,2018,6 Conclusions,experimental results showed that the model is able to directly convert an input speech mixture into multiple label sequences under the end-to-end framework without the need for any explicit intermediate representation including phonetic alignment information or pairwise unmixed speech.
p18-1244,2018,6 Conclusions,"future work includes data collection and evaluation in a real world scenario since the data used in our experiments are simulated mixed speech, which is already extremely challenging but still leaves some acoustic aspects, such as lombard effects and real room impulse responses, that need to be alleviated for further performance improvement."
p18-1244,2018,6 Conclusions,"in addition, further study is required in terms of increasing the number of speakers that can be simultaneously recognized, and further comparison with the separation-based approach."
p18-1244,2018,6 Conclusions,"in an encoderdecoder network framework, teacher forcing at the decoder network under multiple references increases computational cost if implemented naively."
p18-1244,2018,6 Conclusions,"in this paper, we proposed an end-to-end multispeaker speech recognizer based on permutationfree training and a new objective function promoting the separation of hidden vectors in order to generate multiple hypotheses."
p18-1244,2018,6 Conclusions,"we also compared our model with a method based on explicit separation using deep clustering, and showed comparable result."
p18-1244,2018,6 Conclusions,we avoided this problem by employing a joint ctc/attention-based encoder-decoder network.
p18-1245,2018,8 Conclusion,"as the model鈥檚 rich parameterization prevents tractable inference, we craft a variational inference procedure, based on the wake-sleep algorithm, to marginalize out the latent variables."
p18-1245,2018,8 Conclusion,"experimentally, we provide empirical validation on 23 languages."
p18-1245,2018,8 Conclusion,the model allows us to exploit unlabeled data in the training of morphological inflectors.
p18-1245,2018,8 Conclusion,"we find that, especially in the lower-resource conditions, our model improves by large margins over the baselines."
p18-1245,2018,8 Conclusion,we have presented a novel generative model for morphological inflection generation in context.
p18-1246,2018,6 Conclusions,we presented an approach to morphosyntactic tagging that combines context-sensitive initial character and word encodings with a meta-bilstm layer to obtain state-of-the art accuracies for a wide variety of languages.
p18-1247,2018,7 Conclusion and Future Work,"due to the robustness of the model across languages, we believe it can also be scaled to perform morphological tagging for multiple languages together."
p18-1247,2018,7 Conclusion and Future Work,"in this work, we proposed a novel framework for sequence tagging that combines neural networks and graphical models, and showed its effectiveness on the task of morphological tagging."
p18-1247,2018,7 Conclusion and Future Work,we believe this framework can be extended to other sequence labeling tasks in nlp such as semantic role labeling.
p18-1248,2018,7 Conclusion,"empirical results on a collection of highly non-projective datasets from universal dependencies show improvements in accuracy over the projective approach of shi et al.(2017a), as well as edge-factored maximumspanning-tree parsing."
p18-1248,2018,7 Conclusion,"for this purpose, we have established a mapping between mh 4 items and transition sequences of an underlying non-projective transition-based parser."
p18-1248,2018,7 Conclusion,"the results are on par with the 1-endpoint-crossing parser of pitler (2014) (re-implemented under the same neural framework), but our algorithm is notably simpler and has additional desirable properties: it is purely bottom-up, generalizable to higher coverage, and compatible with transition-based semantics."
p18-1248,2018,7 Conclusion,"to our knowledge, this is the first practical implementation of exact inference for non-projective transition-based parsing."
p18-1248,2018,7 Conclusion,"we have extended the parsing architecture of shi et al.(2017a) to non-projective dependency parsing by implementing the mh 4 parser, a mildly non-projective opnq chart parsing algorithm, using a minimal set of transition-based bi-lstm features."
p18-1249,2018,7 Conclusion,"in particular, we demonstrate state-of-theart parsing results with a novel encoder based on factored self-attention."
p18-1249,2018,7 Conclusion,"in this paper, we show that the choice of encoder can have a substantial effect on parser performance."
p18-1249,2018,7 Conclusion,our results suggest that further research into different ways of encoding utterances can lead to additional improvements in both parsing and other natural language processing tasks.
p18-1249,2018,7 Conclusion,"the gains we see come not only from incorporating more information (such as subword features or externally-trained word representations), but also from structuring the architecture to separate different kinds of information from each other."
p18-1250,2018,6 Conclusion,experiments on chinese language show that our neural model for ecd exceptionally boosts the state-of-the-art detection accuracy
p18-1250,2018,6 Conclusion,"in this paper, we study neural models to detect empty categories."
p18-1250,2018,6 Conclusion,it should be a well-justified solution to identify empty categories as well as to integrate empty categories into syntactic analysis.
p18-1250,2018,6 Conclusion,neural networks have played a big role in multiple nlp tasks recently owing to its nonlinear mapping ability and the avoidance of human-engineered features.
p18-1250,2018,6 Conclusion,"we observe three facts: (1) bilstm significantly advances the pre-parsing ecd.(2) automatic ecd improves the neural dependency parsing quality for overt words.(3) even with a bilstm, syntactic information can enhance the detection further."
p18-1251,2018,5 Future Work,"for future work, other potential bottlenecks could be addressed."
p18-1251,2018,5 Future Work,"for the reduction step, speedups can be achieved if the sort and reduce operations can be merged with the edge expansion part of the method."
p18-1251,2018,5 Future Work,"if the two input transducers contain a large number of states and transitions, the amount of memory needed to track all the states and edges generated will grow significantly."
p18-1251,2018,5 Future Work,"in that case, the reduce operation might not yield significant speedups given the fact that the overhead to compose small transducers is too high when using a gpu architecture."
p18-1251,2018,5 Future Work,it can be doable if the transducers used for the composition are small.
p18-1251,2018,6 Conclusion,our implementation is available as open-source software.
p18-1251,2018,5 Future Work,"previous work (harish and narayanan, 2007) has shown that state queues on the gpu cause a large memory overhead."
p18-1251,2018,5 Future Work,the challenge of merging identical edges during expansion is the auxiliary memory that will be required to store and index intermediate probabilities.
p18-1251,2018,5 Future Work,the largest bottleneck is the queue used on the host to keep track of the edges to expand on the gpu.
p18-1251,2018,5 Future Work,the only challenge of using such a data structure is the memory consumption on the gpu.
p18-1251,2018,5 Future Work,the queue will also require a mechanism to avoid inserting duplicate tuples into the queue.
p18-1251,2018,5 Future Work,"therefore, if state expansion is moved to the gpu, the structures used to keep track of the states must be compressed or occupy the least amount of memory possible on the device in order to allocate all structures required on the device."
p18-1251,2018,6 Conclusion,"this is the first work, to our knowledge, to deliver a parallel gpu implementation of the fst composition algorithm."
p18-1251,2018,6 Conclusion,"this parallel method considers several factors, such as host to device communication using page-locked memory, storage formats on the device, thread configuration, duplicate edge detection, and duplicate edge reduction."
p18-1251,2018,5 Future Work,using a similar data structure on the gpu to keep track of the states to expand would yield higher speedups.
p18-1251,2018,6 Conclusion,we were able to obtain speedups of up to 4.5脳 over a serial openfst baseline and 6脳 over the serial implementation of our method.
p18-1252,2018,6 Conclusions and Future Work,"in future, we would like to advance this work in two directions: 1) proposing more effective conversion approaches, especially by exploring the potential of treelstms; 2) constructing bi-tree aligned data for other treebanks and exploiting all available single-tree and bi-tree labeled data for better conversion."
p18-1252,2018,6 Conclusions and Future Work,"in this work, we for the first time propose the task of supervised treebank conversion by constructing a bi-tree aligned data of over ten thousand sentences."
p18-1252,2018,6 Conclusions and Future Work,"results show that 1) the two approaches achieves nearly the same conversion accuracy; 2) relation labels in the source-side tree are very helpful for both approaches; 3) treebank conversion is more effective in multi-treebank exploitation than multi-task learning, and achieves significantly higher parsing accuracy."
p18-1252,2018,6 Conclusions and Future Work,we design two simple yet effective conversion approaches based on the state-of-the-art deep biaffine parser.
p18-1253,2018,8 Conclusion,experiments on both synthetic and real-world datasets have shown that oonp outperforms several strong baselines by a large margin on parsing fairly complicated ontology
p18-1253,2018,8 Conclusion,"oonp is neural netbased, but equipped with sophisticated architecture and mechanism for document understanding, therefore nicely combining interpretability and learnability."
p18-1253,2018,8 Conclusion,"we proposed object-oriented neural programming (oonp), a framework for semantically parsing in-domain documents."
p18-1254,2018,9 Conclusion,"certainly, the road is now open for more fine-grained investigations of the order and timing of individual parsing operations within the human sentence processing mechanism."
p18-1254,2018,9 Conclusion,"if this one-stage model is cognitively plausible, then its simplicity undercuts arguments for string-based perceptual strategies such as the noun-verb-noun heuristic (for a textbook presentation see townsend and bever, 2001)."
p18-1254,2018,9 Conclusion,"perhaps, as phillips (2013) suggests, these are unnecessary in an adequate cognitive model."
p18-1254,2018,9 Conclusion,"previous work was 鈥渢wo-stage鈥 in the sense that the generative model served to rerank proposals from a conditional model (dyer et al., 2016)."
p18-1254,2018,9 Conclusion,"recurrent neural net grammars indeed learn something about natural language syntax, and what they learn corresponds to indices of human language processing difficulty that are manifested in electroencephalography."
p18-1254,2018,9 Conclusion,"this correspondence, between computational model and human electrophysiological response, follows from a system that lacks an initial stage of purely stringbased processing."
p18-1255,2018,7 Conclusion,"first, we need it to be able to generalize: for instance by constructing templates of the form 鈥淲hat version of are you running?鈥 into which the system would need to fill a variable."
p18-1255,2018,7 Conclusion,"in order to move to a full system that can help users like terry write better posts, there are three interesting lines of future work."
p18-1255,2018,7 Conclusion,one can naturally extend our evpi approach to a full reinforcement learning approach to handle multi-turn conversations.
p18-1255,2018,7 Conclusion,"our model integrates well-known deep network architectures with the classic notion of expected value of perfect information, which effectively models a pragmatic choice on the part of the questioner: how do i imagine the other party would answer if i were to ask this question."
p18-1255,2018,7 Conclusion,our results shows that the evpi model is a promising formalism for the question generation task.
p18-1255,2018,7 Conclusion,"second, in order to move from question ranking to question generation, one could consider sequence-to-sequence based neural network models that have recently proven to be effective for several language generation tasks (sutskever et al., 2014; serban et al., 2016; yin et al., 2016)."
p18-1255,2018,7 Conclusion,"such pragmatic principles have recently been shown to be useful in other tasks as well (golland et al., 2010; smith et al., 2013; orita et al., 2015; andreas and klein, 2016)."
p18-1255,2018,7 Conclusion,"third is in evaluation: given that this task requires expert human annotations and also given that there are multiple possible good questions to ask, how can we automatically measure performance at this task?, a question faced in dialog and generation more broadly (paek, 2001; lowe et al., 2015; liu et al., 2016)."
p18-1255,2018,7 Conclusion,"we have constructed a new dataset for learning to rank clarification questions, and proposed a novel model for solving this task."
p18-1256,2018,8 Conclusion,"additionally, we have presented a novel weighted-pooling attention mechanism which is incorporated into a recurrent neural network model for predicting the presence of an adverbial presuppositional trigger."
p18-1256,2018,8 Conclusion,"in future work, we would like to focus more on designing models that can deal with and be optimized for scenarios with severe data imbalance."
p18-1256,2018,8 Conclusion,"in this work, we have investigated the task of predicting adverbial presupposition triggers and introduced several datasets for the task."
p18-1256,2018,8 Conclusion,"our results show that the model outperforms the cnn and lstm, and does not add any additional parameters over the standard lstm model."
p18-1256,2018,8 Conclusion,this shows its promise in classification tasks involving capturing and combining relevant information from multiple points in the previous context.
p18-1256,2018,8 Conclusion,"we would like to also explore various applications of presupposition trigger prediction in language generation applications, as well as additional attention-based neural network architectures."
P19-1001,2019,7 Conclusions and Future Work,depth of the model comes from stacking multiple interaction blocks that execute representationinteraction-representation in an iterative manner.
P19-1001,2019,7 Conclusions and Future Work,evaluation results on three benchmarks indicate that ioi can significantly outperform baseline methods with moderate depth.
P19-1001,2019,7 Conclusions and Future Work,"in the future, we plan to integrate our ioi model with models like elmo (peters et al., 2018) and bert (devlin et al., 2018) to study if the performance of ioi can be further improved."
P19-1001,2019,7 Conclusions and Future Work,we present an interaction-over-interaction network (ioi) that lets utterance-response interaction in context-response matching go deep.
P19-1002,2019,5 Conclusion and Future Work,"by imitating the real-world human cognitive process, we propose a deliberation decoder to optimize knowledge relevance and context coherence."
P19-1002,2019,5 Conclusion and Future Work,"empirical results show that the proposed model can generate responses with much more relevance, correctness, and coherence compared with the state-of-the-art baselines."
P19-1002,2019,5 Conclusion and Future Work,"in the future, we plan to apply reinforcement learning to further improve the performance."
P19-1002,2019,5 Conclusion and Future Work,"in this paper, we propose an incremental transformer with deliberation decoder for the task of document grounded conversations."
P19-1002,2019,5 Conclusion and Future Work,"through an incremental encoding scheme, the model achieves a knowledge-aware and context-aware conversation representation."
P19-1003,2019,6 Conclusion,"in this paper, we propose improving multi-turn dialogue modelling by imposing a separate utterance rewriter."
P19-1003,2019,6 Conclusion,the rewriter is trained to recover the coreferred and omitted information of user utterances.
P19-1003,2019,6 Conclusion,"the trained utterance rewriter performs remarkably well and, when integrated into two online chatbot applications, significantly improves the intention detection and user engagement."
P19-1003,2019,6 Conclusion,we collect a high-quality manually annotated dataset and designed a transformer-pointer based architecture to train the utterance rewriter.
P19-1003,2019,6 Conclusion,we hope the collected dataset and proposed model can benefit future related research.
P19-1004,2019,5 Conclusion,"by open-sourcing our code, we believe this paradigm of studying model behavior by introducing perturbations that destroys different kinds of structure present within the dialog history can be a useful diagnostic tool."
P19-1004,2019,5 Conclusion,"this work studies the behaviour of generative neural dialog systems in the presence of synthetically introduced perturbations to the dialog history, that it conditions on."
P19-1004,2019,5 Conclusion,we also find subtle differences between the way in which recurrent and transformer-based models use available context.
P19-1004,2019,5 Conclusion,we also foresee this paradigm being useful when building new dialog datasets to understand the kinds of information models use to solve them.
P19-1004,2019,5 Conclusion,we find that both recurrent and transformer-based seq2seq models are not significantly affected even by drastic and unnatural modifications to the dialog history.
P19-1005,2019,5 Conclusion,"our combination of boosting and raml for response generation is novel, and its combination with mmi gives some of the most diversified results."
P19-1005,2019,5 Conclusion,our human evaluation provides evidence that diversified responses by boosting are even more appropriate than those generated from baseline models.
P19-1005,2019,5 Conclusion,quantitative evaluation shows our method can substantially improve the diversity without harming the quality of generated responses.
P19-1005,2019,5 Conclusion,"we investigated the use of boosting to improve the diversity and relevance of dialog response generation, with various training and decoding objectives including mutual-information-based decoding and reward-augmented maximum likelihood learning."
P19-1006,2019,4 Conclusion and Future Work,"in the future, we would like to explore the effectiveness of various attention methods to solve indefinite choices task with interpretive features."
P19-1006,2019,4 Conclusion and Future Work,"in this paper, we proposed an end-to-end spatiotemporal matching model for response selection."
P19-1006,2019,4 Conclusion and Future Work,the model uses a dual stacked gru or pre-trained bert to embed utterances and candidates respectively and apply spatio-temporal matching block to measure the matching degree of a pair of context and candidate.
P19-1006,2019,4 Conclusion and Future Work,"visualization of attention layers illustrates that our model has the good interpretative ability, and has the ability to pick out important words and sentences."
P19-1007,2019,6 Conclusions and Future Work,experimental results show that semantic parsing based on dual learning improves performance across datasets.
P19-1007,2019,6 Conclusions and Future Work,"in the future, we want to incorporate this framework with much refined primal and dual models, and design more informative reward signals to make the training more efficient."
P19-1007,2019,6 Conclusions and Future Work,"in this paper, we develop a semantic parsing framework based on dual learning algorithm, which enables a semantic parser to fully utilize labeled and even unlabeled data through a duallearning game between the primal and dual models."
P19-1007,2019,6 Conclusions and Future Work,"it would be appealing to apply graph neural networks (chen et al., 2018b, 2019) to model structured logical forms."
P19-1007,2019,6 Conclusions and Future Work,"thus, the primal model tends to generate complete and reasonable semantic representation."
P19-1007,2019,6 Conclusions and Future Work,we also propose a novel reward function at the surface and semantic levels by utilizing the prior-knowledge of logical form structures.
P19-1009,2019,8 Conclusion,"for future work, we would like to extend our model to other semantic parsing tasks (oepen et al., 2014; abend and rappoport, 2013)."
P19-1009,2019,8 Conclusion,our model achieves the best performance on two amr corpora.
P19-1009,2019,8 Conclusion,"we are also interested in semantic parsing in cross-lingual settings (zhang et al., 2018; damonte and cohen, 2018)."
P19-1009,2019,8 Conclusion,we proposed an attention-based model for amr parsing where we introduced a series of novel components into a transductive setting that extend beyond what a typical nmt system would do on this task.
P19-1010,2019,6 Conclusions,"experimental results have demonstrated that this approach can achieve competitive results across a diverse set of tasks, while also providing a conceptually simple way to incorporate entities and their relations during parsing."
P19-1010,2019,6 Conclusions,"for future direction, we are interested in exploring constrained decoding, better incorporating pre-trained language representations within our architecture, conditioning on additional relations between entities, and different gnn formulations."
P19-1010,2019,6 Conclusions,"more broadly, we have presented a flexible approach for conditioning on available knowledge in the form of entities and their relations, and demonstrated its effectiveness for semantic parsing."
P19-1010,2019,6 Conclusions,"we have presented an architecture for semantic parsing that uses a graph neural network (gnn) to condition on a graph of tokens, entities, and their relations."
P19-1011,2019,6 Conclusion,"besides, two other model variants with a random projection or pca transformation require no training and demonstrate competitive embedding quality even with relatively small dimensions."
P19-1011,2019,6 Conclusion,experiments on nearest-neighbor sentence retrieval further validate the effectiveness of proposed framework.
P19-1011,2019,6 Conclusion,"notably, a regularized autoencoder augmented with semantic-preserving loss exhibits the best empirical results, degrading performance by only around 2% while saving over 98% memory footprint."
P19-1011,2019,6 Conclusion,this paper presents a first step towards learning binary and general-purpose sentence representations that allow for efficient storage and fast retrieval over massive corpora.
P19-1011,2019,6 Conclusion,"to this end, we explore four distinct strategies to convert pre-trained continuous sentence embeddings into a binarized form."
P19-1012,2019,7 Discussion and Conclusion,and the graph-based parser makes use of far away surface tokens but also structurally related words.
P19-1012,2019,7 Discussion and Conclusion,"evidently, the employment of bilstm feature extractors blurs the difference between the two architectures."
P19-1012,2019,7 Discussion and Conclusion,"finally, the implicit structural information is important for the final parsing decisions: dropping it in ablated models causes their performance to deteriorate."
P19-1012,2019,7 Discussion and Conclusion,"moreover, the transition-based parser does not incorporate structural features through the feature set."
P19-1012,2019,7 Discussion and Conclusion,our transition- and graph-based architectures use the same word representations.
P19-1012,2019,7 Discussion and Conclusion,"specifically, the features drawn from partial subtrees become redundant because the parsing models encode them implicitly."
P19-1012,2019,7 Discussion and Conclusion,the bilstm-based parsers can compensate for the lack of traditional structural features.
P19-1012,2019,7 Discussion and Conclusion,"the classical transition- and graph-based dependency parsers have their strengths and limitations due to the trade-off between the richness of feature functions and the inference algorithm (mcdonald and nivre, 2007)."
P19-1012,2019,7 Discussion and Conclusion,the introduction of bilstms into dependency parsers has an additional interesting consequence.
P19-1012,2019,7 Discussion and Conclusion,the main advantage of bilstms comes with their ability to capture not only surface but also syntactic relations.
P19-1012,2019,7 Discussion and Conclusion,"the one clear advantage of the graphbased parser is that it performs global inference (but exact search algorithms are already being applied to projective (shi et al., 2017) and nonprojective (gomez-rodr ′ ′?guez et al., 2018) transition systems)."
P19-1012,2019,7 Discussion and Conclusion,"therefore, an interesting question is if integrating those two architectures can still be beneficial for the parsing accuracy as in nivre and mcdonald (2008)."
P19-1012,2019,7 Discussion and Conclusion,this structural information is then passed directly (through feature vectors) and indirectly (through bilstms encoding) to mlp and is used for scoring transitions and arcs.
P19-1012,2019,7 Discussion and Conclusion,we examined how the application of bilstms influences the modern transition- and graph-based parsing architectures.
P19-1012,2019,7 Discussion and Conclusion,we leave this question for future work.
P19-1012,2019,7 Discussion and Conclusion,we showed that those representations trained together with the parsers capture syntactic relations in a similar way.
P19-1012,2019,7 Discussion and Conclusion,"when the representations are trained together with a parser they encode structurally-advanced relations such as heads, children, or even siblings and grandparents."
P19-1013,2019,6 Conclusion,"in this work, we have proposed a domain adaptation method for ccg parsing, based on the automatic generation of new ccg treebanks from dependency resources."
P19-1013,2019,6 Conclusion,"remarkably, when applied to our domain adaptation method, the improvements in the latter two domains are significant, with the achievement of more than 5 points in the unlabeled metric."
P19-1013,2019,6 Conclusion,"we have conducted experiments to verify the effectiveness of the proposed method on diverse domains: on top of existing benchmarks on biomedical texts and question sentences, we newly conduct parsing experiments on speech conversation and math problems."
P19-1014,2019,8 Conclusions,"in future work, we’d like to improve this integration in order to gain from training on examples from different domains for tags like ‘name’ and ‘location’."
P19-1014,2019,8 Conclusions,in the case of integrating datasets from the news and medical domains we found the blending task to be difficult.
P19-1014,2019,8 Conclusions,"in the conducted experiments, the proposed model consistently outperformed the baselines in difficult tagging cases and showed robustness when applying a single trained model to varied test sets."
P19-1014,2019,8 Conclusions,"we proposed a tag-hierarchy model for the heterogeneous tag-sets ner setting, which does not require a consolidation post-processing stage."
P19-1015,2019,6 Conclusion,"cross-lingual transfer does not work out of the box, especially when using large numbers of source languages, and distantly related target languages."
P19-1015,2019,6 Conclusion,"in an ner setting using a collection of 41 languages, we showed that simple methods such as uniform ensembling do not work well."
P19-1015,2019,6 Conclusion,"our unsupervised method, beauns, provides a fast and simple way of annotating data in the target language, which is capable of reasoning under noisy annotations, and outperforms several competitive baselines, including the majority voting ensemble, a low-resource supervised baseline, and the oracle single best transfer model."
P19-1015,2019,6 Conclusion,"we also compare our results with bwet (xie et al., 2018), a state-of-the-art unsupervised single source (english) transfer model, and showed that multilingual transfer outperforms it, however, our work is orthogonal to their work in that if training data from multiple source models is created, rare and bea can still combine them, and outperform majority voting."
P19-1015,2019,6 Conclusion,"we proposed two new multilingual transfer models (rare and bea), based on unsupervised transfer, or a supervised transfer setting with a small 100 sentence labelled dataset in the target language."
P19-1015,2019,6 Conclusion,"we show that light supervision improves performance further, and that our second approach, rare, based on ranking transfer models and then retraining on the target language, results in further and more consistent performance improvements."
P19-1016,2019,5 Conclusions and Future Work,experiments on the benchmark data sets in both within-genre and cross-genre settings demonstrate the effectiveness of our model and verify our intuition to introduce reliability signals.
P19-1016,2019,5 Conclusions and Future Work,"our future work includes integrating advanced word representation methods (e.g., elmo and bert) and extending the proposed model to other tasks, such as event extraction and co-reference resolution."
P19-1016,2019,5 Conclusions and Future Work,we also plan to incorporate external knowledge and common sense as additional signals into our architecture as they are important for human readers to recognize names but still absent from the current model.
P19-1016,2019,5 Conclusions and Future Work,we propose a name tagging model that is able to dynamically compose features depending on the quality of input word embeddings.
P19-1017,2019,6 Conclusions and Future Work,"experiments on 20 languages and totally 294 distant language pairs demonstrate that (1) unsupervised pivot translation achieves large improvements over direct unsupervised translation for distant languages; (2) our proposed ltr can select the translation path whose translation accuracy is close to the ground-truth best path; (3) if we leverage supervised translation instead of the unsupervised translation for some popular language pairs in the intermediate hop, we can further boost the performance of unsupervised pivot translation."
P19-1017,2019,6 Conclusions and Future Work,"for further works, we will leverage more supervised translation hops to improve the performance of unsupervised translation for distant languages."
P19-1017,2019,6 Conclusions and Future Work,"in this paper, we have introduced unsupervised pivot translation for distant language pairs, and proposed the learning to route (ltr) method to automatically select a good translation path for a distant language pair."
P19-1017,2019,6 Conclusions and Future Work,we will extend our method to more distant languages.
P19-1018,2019,6 Conclusions,"in this work, we analyze the validity of the orthogonality assumption and show that it breaks for distant language pairs."
P19-1018,2019,6 Conclusions,"on analyzing the model errors, we find that a large fraction of them arise due to polysemy and antonymy (an interested reader can find the details in appendix (§a.2)."
P19-1018,2019,6 Conclusions,"we also find that translating in a common embedding space, as opposed to the target embedding space, obtains orthogonal gains for bli, and plan on investigating this in the semi-supervised setting in future work."
P19-1018,2019,6 Conclusions,we finally propose a semi-supervised framework which combines the advantages of supervised and unsupervised approaches and uses a joint optimization loss to enforce a weak and flexible orthogonality constraint.
P19-1018,2019,6 Conclusions,we motivate the task of semisupervised bli by showing the shortcomings of purely supervised and unsupervised approaches.
P19-1018,2019,6 Conclusions,"we provide two instantiations of our framework, and show that both outperform their supervised and unsupervised counterparts."
P19-1019,2019,6 Conclusions and future work,"finally, we would like to adapt our approach to more relaxed scenarios with multiple languages and/or small parallel corpora."
P19-1019,2019,6 Conclusions and future work,"in addition to that, we use our improved smt approach to initialize a dual nmt model that is further improved through on-the-fly backtranslation."
P19-1019,2019,6 Conclusions and future work,"in addition to that, we would like to incorporate a language modeling loss during nmt training similar to he et al.(2016)."
P19-1019,2019,6 Conclusions and future work,"in the future, we would like to explore learnable similarity functions like the one proposed by (mccallum et al., 2005) to compute the characterlevel scores in our initial phrase-table."
P19-1019,2019,6 Conclusions and future work,"in this paper, we identify several deficiencies in previous unsupervised smt systems, and propose a more principled approach that addresses them by incorporating subword information, using a theoretically well founded unsupervised tuning method, and developing a joint refinement procedure."
P19-1019,2019,6 Conclusions and future work,our code is available as an open source project at https: //github.com/artetxem/monoses.
P19-1019,2019,6 Conclusions and future work,"our experiments show the effectiveness of our approach, as we improve the previous state-of-the-art in unsupervised machine translation by 5-7 bleu points in french-english and german-english wmt 2014 and 2016."
P19-1020,2019,7 Conclusion,"additionally, we confirmed that adversarial regularization techniques effectively worked even if we performed them with the training data increased by a back-translation method."
P19-1020,2019,7 Conclusion,our experimental results demonstrated that applying vat to both encoder and decoder embeddings consistently outperformed other configurations.
P19-1020,2019,7 Conclusion,this paper discussed the practical usage and benefit of adversarial regularization based on adversarial perturbation in the current nmt models.
P19-1020,2019,7 Conclusion,"we believe that adversarial regularization can be one of the common and fundamental technologies to further improve the translation quality, such as model ensemble, byte-pair encoding, and back-translation."
P19-1021,2019,6 Conclusions,"even though we focused on only using parallel data, our results are also relevant for work on using auxiliary data to improve low-resource mt."
P19-1021,2019,6 Conclusions,"our results demonstrate that nmt is in fact a suitable choice in low-data settings, and can outperform pbsmt with far less parallel training data than previously claimed."
P19-1021,2019,6 Conclusions,"our results show that low-resource nmt is very sensitive to hyperparameters such as bpe vocabulary size, word dropout, and others, and by following a set of best practices, we can train competitive nmt systems without relying on auxiliary resources."
P19-1021,2019,6 Conclusions,"recently, the main trend in low-resource mt research has been the better exploitation of monolingual and multilingual resources."
P19-1021,2019,6 Conclusions,"supervised systems serve as an important baseline to judge the effectiveness of semisupervised or unsupervised approaches, and the quality of supervised systems trained on little data can directly impact semisupervised workflows, for instance for the backtranslation of monolingual data."
P19-1021,2019,6 Conclusions,"this has practical relevance for languages where large amounts of monolingual data, or multilingual data involving related languages, are not available."
P19-1022,2019,3 Conclusions,"we demonstrate that ewc effectively regularizes nmt finetuning, outperforming other schemes reported for nmt."
P19-1022,2019,3 Conclusions,"we extend bayesian interpolation with source information and apply it to nmt decoding with unadapted and fine-tuned models, adaptively weighting ensembles to out-perform the oracle case, without relying on test domain labels."
P19-1022,2019,3 Conclusions,we report on training and decoding techniques that adapt nmt to new domains while preserving performance on the original domain.
P19-1022,2019,3 Conclusions,"we suggest our approach, reported for domain adaptation, is broadly useful for nmt ensembling."
P19-1023,2019,5 Conclusions,experimental results show that our proposed model outperforms the existing models by 33.39% and 34.78% in terms of f1 score on the wiki and geo test dataset respectively.
P19-1023,2019,5 Conclusions,"in the future, we plan to explore contextbased similarity to complement the lexical similarity to improve the overall performance."
P19-1023,2019,5 Conclusions,"moreover, we propose a modified beam search and a triple classification that helps the model to generate high-quality triples."
P19-1023,2019,5 Conclusions,our model thus reduces the error propagation between relation extraction and ned that existing approaches are prone to.
P19-1023,2019,5 Conclusions,our proposed n-gram attention model outperforms the other encoder-decoder models by 15.51% and 8.38% in terms of f1 score on the two real-world datasets.
P19-1023,2019,5 Conclusions,these results confirm that our model better captures the multi-word entity names in a sentence.
P19-1023,2019,5 Conclusions,these results confirm that our model reduces the error propagation between ned and relation extraction.
P19-1023,2019,5 Conclusions,"to obtain high-quality training data, we adapt distant supervision and augment it with co-reference resolution and paraphrase detection."
P19-1023,2019,5 Conclusions,we propose an n-gram based attention model that better captures the multi-word entity names in a sentence.
P19-1023,2019,5 Conclusions,we proposed an end-to-end relation extraction model for kb enrichment that integrates the extraction and canonicalization tasks.
P19-1024,2019,5 Conclusion,experimental results show that aggcns achieve state-ofthe-art results on various relation extraction tasks.
P19-1024,2019,5 Conclusion,"one natural question we would like to ask is how to make use of the proposed framework to perform improved graph representation learning for graph related tasks (bastings et al., 2017)."
P19-1024,2019,5 Conclusion,there are multiple venues for future work.
P19-1024,2019,5 Conclusion,"unlike previous approaches, aggcns operate directly on the full tree and learn to distill the useful information from it in an end-to-end fashion."
P19-1024,2019,5 Conclusion,we introduce the novel attention guided graph convolutional networks (aggcns).
P19-1025,2019,6 Conclusion,"in this work, we showed that spatial aggregation of documents leads to discovery of spatially distinct topics."
P19-1025,2019,6 Conclusion,our results show that aggregation is a very powerful and computationally efficient method for discovery of distinct topics.
P19-1025,2019,6 Conclusion,to evaluate the quality of the discovered topics we proposed a metric based on space-time scan statistics.
P19-1025,2019,6 Conclusion,we performed an extensive study on synthetic and real data and demonstrated that spatial and spatio-temporal aggregation indeed leads to discovery of spatial and spatio-temporal distinct topics.
P19-1025,2019,6 Conclusion,"while our study focused on spatial aggregation, aggregation on other types of metadata such as authors, hashtags, or communities is expected to work equally well and discover other types of distinct topics from large collections of documents."
P19-1026,2019,8 Conclusion,"by leveraging the desired properties of dihedral group, relation (skew-) symmetry, inversion, and (non-) abelian compositions are all supported."
P19-1026,2019,8 Conclusion,"finally, we demonstrated that the above g properties can be learned from dihedral by extensive case studies, yielding a substantial increase in interpretability from existing models."
P19-1026,2019,8 Conclusion,our experimental results on benchmark kgs showed that dihedral outperforms existing bilinear form models and even deep learning methods.
P19-1026,2019,8 Conclusion,this paper proposed dihedral for kg relation embedding.
P19-1027,2019,4 Limitations and Conclusions,"as a rule of thumb, a smaller vocabulary size is better for small datasets and larger vocabulary sizes better for larger datasets."
P19-1027,2019,4 Limitations and Conclusions,"bpe vocabulary size has a large effect on model quality, both in monolingual settings and with a large vocabulary shared among 265 languages."
P19-1027,2019,4 Limitations and Conclusions,choose smaller subword vocabulary sizes in low-resource settings.
P19-1027,2019,4 Limitations and Conclusions,combine different subword representations for better results.
P19-1027,2019,4 Limitations and Conclusions,"finally, and while asking the reader to bear above limitations in mind, we make the following practical recommendations for multilingual sequence tagging with subword representations:  choose the largest feasible subword vocabulary size when a large amount of data is available."
P19-1027,2019,4 Limitations and Conclusions,"for high-resource languages, we found that monolingual embeddings and monolingual training perform better than multilingual approaches with a shared vocabulary."
P19-1027,2019,4 Limitations and Conclusions,"in low-resource scenarios, first perform multilingual pretraining with a shared subword vocabulary, then finetune to the language of interest."
P19-1027,2019,4 Limitations and Conclusions,large improvements over monolingual training showed that low-resource languages benefit from multilingual model training with shared subword embeddings.
P19-1027,2019,4 Limitations and Conclusions,limitations.
P19-1027,2019,4 Limitations and Conclusions,monolingual finetuning of a multilingual model improves performance in almost all cases (compare -finetune and +finetune columns in table 9 in appendix f).
P19-1027,2019,4 Limitations and Conclusions,multilingual bert is a robust choice across tasks and languages if the computational requirements can be met.
P19-1027,2019,4 Limitations and Conclusions,"our evaluation did not include other subword representations, most notably elmo (peters et al., 2018) and contextual string embeddings (akbik et al., 2018), since, even though they are languageagnostic in principle, pretrained models are only available in a few languages.conclusions."
P19-1027,2019,4 Limitations and Conclusions,our experiments also show that even a large multilingual contextual model like bert benefits from character embeddings and additional monolingual embeddings.
P19-1027,2019,4 Limitations and Conclusions,"such improvements are likely not solely caused by cross-lingual transfer, but also by the prevention of overfitting and mitigation of noise in small monolingual datasets."
P19-1027,2019,4 Limitations and Conclusions,"the degree to which this sample is representative varies, and low-resource wikipedias in particular contain large fractions of “foreign” text and noise, which propagates into embeddings and datasets."
P19-1027,2019,4 Limitations and Conclusions,this is likely due to the fact that a high-resource language provides large background corpora for learning good embeddings of a large vocabulary and also provides so much training data for the task at hand that little additional information can be gained from training data in other languages.
P19-1027,2019,4 Limitations and Conclusions,"throughout this study, we have used a wikipedia edition in a given language as a sample of that language."
P19-1027,2019,4 Limitations and Conclusions,"we have presented a large-scale study of contextual and non-contextual subword embeddings, in which we trained monolingual and multilingual ner models in 265 languages and pos-tagging models in 27 languages."
P19-1027,2019,4 Limitations and Conclusions,"while extensive, our evaluation is not without limitations."
P19-1027,2019,4 Limitations and Conclusions,"with limited computational resources, use small monolingual, non-contextual representations, such as bpemb combined with character embeddings."
P19-1028,2019,6 Conclusions,"as our experiments showed, our framework allows neural models to benefit from external knowledge during learning and prediction, especially when training data is limited."
P19-1028,2019,6 Conclusions,"in this paper, we presented a framework for introducing constraints in the form of logical statements to neural networks."
P19-1028,2019,6 Conclusions,our experiments were designed to explore the flexibility of our framework with different constraints in diverse tasks.
P19-1028,2019,6 Conclusions,we demonstrated the process of converting first-order logic into differentiable components of networks without extra learnable parameters and extensive redesign.
P19-1029,2019,5 Conclusion,"future research directions will involve the development of reinforcement learning model with multi-dimensional rewards, and modeling explicit credit assignment for improving the capabilities of the regulator to make context-sensitive decisions in mini-batch learning."
P19-1029,2019,5 Conclusion,"the empirical study on interactive nmt with simulated human feedback showed that this selfregulated model finds more cost-efficient solutions than models learning from a single feedback type and uncertainty-based active learning models, also under domain shift."
P19-1029,2019,5 Conclusion,"the proposed framework can naturally be expanded to integrate more feedback modes suitable for the interaction with humans, e.g., pairwise comparisons or output rankings."
P19-1029,2019,5 Conclusion,"we proposed a cost-aware algorithm for interactive sequence-to-sequence learning, with a selfregulation module at its core that learns which type of feedback to query from a human teacher."
P19-1029,2019,5 Conclusion,"while this setup abstracts away from certain confounding variables to be expected in real-life interactive machine learning, it should be seen as a pilot experiment that allows focussing on our central research questions under an exact and noise-free computation of feedback cost and performance gain."
P19-1030,2019,4 Conclusion,"in this paper, we propose tree transformer which successfully encodes natural language grammar trees utilizing the modules designed for the standard transformer."
P19-1030,2019,4 Conclusion,we show that we can effectively use the attention module as the composition function together with grammar information instead of just bag of words and can achieve performance on par with tree lstms and even better performance than the standard transformer.
P19-1031,2019,6 Discussion,"encouragingly, parser improvements on the wsj and ctb treebanks still transfer out-of-domain, indicating that improving results on these benchmarks may still continue to yield benefits in broader domains."
P19-1031,2019,6 Discussion,"neural parsers generalize surprisingly well, and are able to draw benefits both from pre-trained language representations and structured output prediction."
P19-1031,2019,6 Discussion,these properties allow single-model parsers to surpass previous state-of-the-art systems on out-of-domain generalization (table 6).
P19-1031,2019,6 Discussion,"we note that these systems from prior work (choe et al., 2015; petrov and mcdonald, 2012; le roux et al., 2012) use additional ensembling or selftraining techniques, which have also been shown to be compatible with neural constituency parsers (dyer et al., 2016; choe and charniak, 2016; fried et al., 2017; kitaev et al., 2019) and may provide benefits orthogonal to the pre-trained representations and structured models we analyze here."
P19-1032,2019,4 Conclusion,"in this work, we present a novel self-attention layer with an adaptive span."
P19-1032,2019,4 Conclusion,"this mechanism allows for models with longer context, and thus with the capability to catch longer dependencies."
P19-1032,2019,4 Conclusion,we have shown the importantce of this feature in the context of character level modeling where information is spread over great distances.
P19-1033,2019,6 Conclusion,"besides, we propose two methods to fuse long- and short-term user representations, i.e., using long-term user representation to initialize the hidden state of the gru network in short-term user representation, or concatenating both longand short-term user representations as a unified user vector."
P19-1033,2019,6 Conclusion,extensive experiments on a real-world dataset collected from msn news show our approach can effecitively improve the performance of news recommendation.
P19-1033,2019,6 Conclusion,"in addition, we learn short-term representations of users from their recently browsed news via a gru network."
P19-1033,2019,6 Conclusion,"in the news encoder, we learn representations of news from their titles and topic categories, and use an attention network to highlight important words for informative representation learning."
P19-1033,2019,6 Conclusion,"in the user encoder, we propose to learn long-term representations of users from the embeddings of their ids."
P19-1033,2019,6 Conclusion,"in this paper, we propose a neural news recommendation approach which can learn both longand short-term user representations."
P19-1033,2019,6 Conclusion,the core of our model is a news encoder and a user encoder.
P19-1034,2019,6 Conclusion,another interesting topic for future research is the comparison of domain adaptation based on our domain-specific word embeddings vs. based on word embeddings trained on much larger corpora.
P19-1034,2019,6 Conclusion,"in our experiments, we demonstrated that the automatically adapted sentiment dictionary h4nre outperforms the previous state of the art in predicting the financial outcomes excess return and volatility."
P19-1034,2019,6 Conclusion,"in particular, automatic adaptation performs better than manual adaptation."
P19-1034,2019,6 Conclusion,"in the future, we plan to investigate whether there are changes over time that significantly impact the linguistic characteristics of the data, in the simplest case changes in the meaning of a word."
P19-1034,2019,6 Conclusion,"in this paper, we automatically created sentiment dictionaries for predicting financial outcomes."
P19-1034,2019,6 Conclusion,our quantitative and qualitative study provided insight into the semantics of the dictionaries.
P19-1034,2019,6 Conclusion,we found that annotation based on an expert’s a priori belief about a word’s meaning can be incorrect – annotation should be performed based on the word’s contexts in the target domain instead.
P19-1035,2019,8 Conclusion,"a core advantage of our manipulation strategies is that we can work with any given text and thus provide c-tests that do not only have the desired difficulty, but also integrate the learner’s interest or the current topic of a language course."
P19-1035,2019,8 Conclusion,"an important observation is that manipulating the gaps’ size and position does not only influence the c-test difficulty, but also addresses different competencies (e.g., requires more vocabulary knowledge or more grammatical knowledge)."
P19-1035,2019,8 Conclusion,another strand of research will be combining both strategies and deploying the manipulation strategies in a large scale testing platform that allows the system to adapt to an individual learner over time.
P19-1035,2019,8 Conclusion,both strategies automatically predict the difficulty of a test to make informed decisions.
P19-1035,2019,8 Conclusion,both strategies reach close to the desired difficulty level.
P19-1035,2019,8 Conclusion,future manipulation strategies that take the competencies into account have the potential to train particular skills and to better control the competencies required for a placement test.
P19-1035,2019,8 Conclusion,"in this work, we proposed two novel strategies for automatically manipulating the difficulty of c-test exercises."
P19-1035,2019,8 Conclusion,our error analysis points out important directions for future work on detecting ambiguous gaps and modeling gap interdependencies for c-tests deviating from the default generation scheme.
P19-1035,2019,8 Conclusion,"our first strategy selects which words should be turned into a gap, and the second strategy learns to increase or decrease the size of the gaps."
P19-1035,2019,8 Conclusion,"to this end, we reproduced previous results, compared them to neural architectures, and tested them on a newly acquired dataset."
P19-1035,2019,8 Conclusion,we evaluate our difficulty manipulation pipeline in a corpus-based study and with real users.
P19-1035,2019,8 Conclusion,"we show that both strategies can effectively manipulate the c-test difficulty, as both the participants’ error rates and their perceived difficulty yield statistically significant effects."
P19-1036,2019,6 Conclusion,"however, for our application domain, we expect that it may not lead to increased performance as words are used to a large extent with the same sense across the corpus."
P19-1036,2019,6 Conclusion,"in this paper, we present a method for unsupervised text classification based on computing the similarity between the documents to be classified and a rich description of the categories label."
P19-1036,2019,6 Conclusion,the category label enrichment starts with human expert provided keywords but is then expanded through the use of word embeddings.
P19-1036,2019,6 Conclusion,this is certainly an avenue that we seek to explore.
P19-1036,2019,6 Conclusion,we also investigated whether a consolidation step that removes non discriminant words from the label dictionaries could have an effect on performance.
P19-1036,2019,6 Conclusion,"we have not explored whether recent advances in word embeddings from instance elmo (peters et al., 2018) and bert (devlin et al., 2018) could add further benefits."
P19-1037,2019,9 Conclusion + Future Work,"additionally, we could improve the measure of lexical complexity for single and multi word expressions."
P19-1037,2019,9 Conclusion + Future Work,"currently, we are only using frequency as an indicator of lexical complexity, however other factors such as word length, etymology, etc.may be used."
P19-1037,2019,9 Conclusion + Future Work,doctors and patients speak a different language and we hope that our work will help them communicate.
P19-1037,2019,9 Conclusion + Future Work,"finally, we will explore adaptations of our methodology for general (non-medical) domains, e.g., simplified search interfaces (ananiadou et al., 2013) for semantically annotated news (thompson et al., 2017)."
P19-1037,2019,9 Conclusion + Future Work,one clear avenue of future work is to apply this system in a clinical setting and to test the results with actual patients.
P19-1037,2019,9 Conclusion + Future Work,our work has explored the use of neural machine translation for text simplification in the clinical domain.
P19-1037,2019,9 Conclusion + Future Work,we could also look to use parallel simplified medical text to augment the general language parallel text used in the nts system.
P19-1037,2019,9 Conclusion + Future Work,we have shown that general language simplification needs to be augmented with domain specific simplifications and that doing so leads to an improvement in the understandability of the resulting text.
P19-1037,2019,9 Conclusion + Future Work,we will look to develop software that uses nts to identify possible simplifications for a clinician when they are writing a letter for a patient.
P19-1038,2019,8 Conclusion,"even though our work is an application of financial domain, we hope our multimodal learning model can also be useful in other areas (such as social media and customer service) where multimodality data is available."
P19-1038,2019,8 Conclusion,"in this work, we have demonstrated that ceo’s language and voice in company earnings conference calls can be utilized to predict the company financial risk level, as measured by stock price volatility for days following the conference call."
P19-1038,2019,8 Conclusion,predicting financial risks of publicly traded companies is an essential task in financial markets.
P19-1038,2019,8 Conclusion,we propose a bilstm-based multimodal deep regression model that extracts and fuses multimodal features from text transcripts and audio recordings.
P19-1039,2019,"6 Conclusions, Limitations, and FutureDirections","at the time of pouring, there was no secret mechanism in place to inform the particular participant who brought the wine being poured."
P19-1039,2019,"6 Conclusions, Limitations, and FutureDirections","first, more samples and machine learning experiments could have been done, had we had more time, funding, and resources."
P19-1039,2019,"6 Conclusions, Limitations, and FutureDirections","fourth, additional experiments with identified significant acoustic and linguistic features would add more weight to the current paper."
P19-1039,2019,"6 Conclusions, Limitations, and FutureDirections","however, given the fact that (1) it was common knowledge (each one knows it, if each one knows that the others know it, if each one knows that each one knows that the others know it, and so on) that each grape-region combo was assigned to no one or one individual for each session, and (2) all participants were required to bring a wine of the assigned grape-region that they know very well and ensure it of a classical style most representative of the grape and region, the task of detecting self-brought wines becomes trivial to our participants, and therefore the informing mechanism stands."
P19-1039,2019,"6 Conclusions, Limitations, and FutureDirections","in addition, this line of work might inspire new methods for detecting insider trading in financial markets."
P19-1039,2019,"6 Conclusions, Limitations, and FutureDirections","lastly, we look forward to further exploring this line of research by investigating: 1. the individual differences in both detecting concealed information and concealing information, by analyzing the features across groups defined by individual personality traits (fornaciari et al., 2013, an et al., 2018), ethnics, native languages, and different dimensions of professional skills; 2. the result and model robustness by collecting and testing other field data such as board games; 3. the predictive power of phonotactic variation features; 4. the relationship between perceived information concealment and concealing information; 5. how soon can we detect concealed information; 6. how to conduct domain adaptation with regards to detecting concealed information; 7. efficient ways to make the multi-task learning framework scalable."
P19-1039,2019,"6 Conclusions, Limitations, and FutureDirections","our analysis of acousticprosodic and linguistic characteristics of information concealment, contrasted with those of deception, provides insight into the nature of text and speech with or without concealed information."
P19-1039,2019,"6 Conclusions, Limitations, and FutureDirections","second, during the blind tasting games, the identity of each wine during each round was known to one participant by chance, in that, every potential grape-region combo in a total pool of 38 combos was randomly assigned to one participant beforehand without replacement."
P19-1039,2019,"6 Conclusions, Limitations, and FutureDirections",the current study is by no means perfect.
P19-1039,2019,"6 Conclusions, Limitations, and FutureDirections","third, more analyses of acoustics such as pitch and tonal contour, phonotactic variations could be incorporated to further explore the space of information concealment in speech."
P19-1039,2019,"6 Conclusions, Limitations, and FutureDirections","we acknowledge that a cleaner and cleverer design could have been implemented to randomly assign and secretly inform participants, however, that would require hiring more independent administrators and sacrificing some participants’ practice opportunities, which were the reasons why we settled for the current setting."
P19-1039,2019,"6 Conclusions, Limitations, and FutureDirections",we developed a hybrid multi-task learning model that outperforms baseline models by 11.48% and human domain experts by 15.23%.
P19-1039,2019,"6 Conclusions, Limitations, and FutureDirections",we have also evaluated the performance of several machine learning classification methods to the critical problem of detecting concealed information in technical settings.
P19-1039,2019,"6 Conclusions, Limitations, and FutureDirections",we have presented a study of concealed information in text and speech.
P19-1040,2019,5 Conclusions and Future Work,"for example, rather than considering a claim as being true or false, (chen et al., 2019) suggests that one needs to view a claim from a diverse, yet comprehensive, set of perspectives."
P19-1040,2019,5 Conclusions and Future Work,"our framework can be extended to deal with sources that generate a spectrum of perspectives, each with a stance relative to claim and with evidence supporting it."
P19-1040,2019,5 Conclusions and Future Work,the sources make direct claims or indirect claims by generating evidence that implies these claims.
P19-1040,2019,5 Conclusions and Future Work,this paper studied the problem of estimating the trustworthiness of given information sources.
P19-1040,2019,5 Conclusions and Future Work,"we evaluated jelta over both synthetic and real datasets, and our results show significant improvements over baselines."
P19-1040,2019,5 Conclusions and Future Work,we leave this for future work.
P19-1040,2019,5 Conclusions and Future Work,"we proposed a probabilistic framework, jelta, which jointly considers both kinds of assertions to better estimate claims’ veracity and sources’ trustworthiness."
P19-1040,2019,5 Conclusions and Future Work,"while we presented the framework here as applying to claims with two truth values, we believe that this framework can apply more broadly."
P19-1041,2019,5 Conclusion and Future Work,bao et al.(2019) have successfully shown that the syntax and semantics of a sentence can be disentangled from each other.
P19-1041,2019,5 Conclusion and Future Work,both qualitative and quantitative experiments show that the latent space is indeed separated into style and content parts.
P19-1041,2019,5 Conclusion and Future Work,"in this paper, we propose an effective approach for disentangling style and content latent spaces."
P19-1041,2019,5 Conclusion and Future Work,"non-categorical styles cannot be easily handled by fixed style embeddings or style-specific decoders (fu et al., 2018)."
P19-1041,2019,5 Conclusion and Future Work,"our approach can be naturally extended to noncategorical styles, because our style feature is encoded from the input sentence."
P19-1041,2019,5 Conclusion and Future Work,"our method achieves high style-transfer strength, high content-preservation scores, as well as high language fluency, compared with previous work."
P19-1041,2019,5 Conclusion and Future Work,the disentangled space can be directly applied to text style-transfer tasks.
P19-1041,2019,5 Conclusion and Future Work,"we systematically combine multi-task and adversarial objectives to separate content and style from each other, where we also propose to approximate content information with bag-of-words features of style-neutral, non-stopword vocabulary."
P19-1042,2019,7 Conclusion,"on the conll-2014 benchmark test set, when using larger ensembles and integrating bert during rescoring, our final crosssentence model achieves 57.30 f0.5 score which is higher than all prior published f0.5 scores at the time of paper submission, when trained using the same datasets."
P19-1042,2019,7 Conclusion,our cross-sentence models show significant gains over strong sentencelevel baselines.
P19-1042,2019,7 Conclusion,we also demonstrate the ability of our models to exploit wider contexts adequately and correct errors on a synthetic test set of crosssentence verb tense errors.
P19-1042,2019,7 Conclusion,"we present the first approach to cross-sentence gec, building on a convolutional encoderdecoder architecture."
P19-1043,2019,7 Conclusions and Future Work,"in the future, we would like to generalize it to multiple domains and datasets."
P19-1043,2019,7 Conclusions and Future Work,"in this paper, we introduce the task of email subject line generation."
P19-1043,2019,7 Conclusions and Future Work,our model outperforms several competitive baselines and approaches human-level performance.
P19-1043,2019,7 Conclusions and Future Work,"we are also interested in generating more effective and appropriate subjects by incorporating prior email conversations, social context, the goal and style of emails, personality, among others."
P19-1043,2019,7 Conclusions and Future Work,we build a benchmark dataset (aeslc) with crowdsourced human annotations on the enron corpus and evaluate automatic metrics for this task.
P19-1043,2019,7 Conclusions and Future Work,we propose our model of subject generation by multi-sentence selection and rewriting with email subject quality estimation reward.
P19-1044,2019,7 Conclusions and future work,"finally, we evaluated on a small, handcrafted set of change and stable words and found that sgns with temporal referencing gives the largest separation between words that undergo semantic change and those that stay stable over time."
P19-1044,2019,7 Conclusions and future work,"in comparison, for the ppmi model where no alignment is needed, temporal referencing also significantly reduced the noise level, but to a lesser extent."
P19-1044,2019,7 Conclusions and future work,"in particular, we observe a similar behavior between this smaller testset and the synthetic sense injection, supporting our sense injection method as a good proxy for isolating and studying lexical semantic change."
P19-1044,2019,7 Conclusions and future work,"in the future, we plan to evaluate temporal referencing against the related dynamic embedding models on an annotated empirical lexical change dataset with multiple languages."
P19-1044,2019,7 Conclusions and future work,"in this paper, we have empirically tested the temporal referencing method for lexical semantic change."
P19-1044,2019,7 Conclusions and future work,next we evaluated whether the noise reduction carries over performance on a synthetic lexical semantic change detection task.
P19-1044,2019,7 Conclusions and future work,"our results support the following conclusion; trained on a diachronic corpus, sgns with temporal referencing will capture more true semantic change."
P19-1044,2019,7 Conclusions and future work,sgns with temporal referencing outperforms the other methods in correctly classifying the words to the two classes (change vs. stable).
P19-1044,2019,7 Conclusions and future work,"we also plan on testing how well temporal referencing deals with corpora that are too small for alignment-based methods, hopefully opening new avenues of quantitative research."
P19-1044,2019,7 Conclusions and future work,"we compare two commonly used models, namely ppmi and sgns because of their properties; the ppmi model is count-based and does not require alignment across time, while the sgns model has shown state-of-the-art results in previous work."
P19-1044,2019,7 Conclusions and future work,we find that the sgns model trained with temporal referencing contains significantly less noise than the standard sgns for which an alignment is necessary.
P19-1044,2019,7 Conclusions and future work,"we simulated change in a controlled and semantically principled way, using sense injection and showed that words with semantically related and unrelated semantic change can be differentiated from control (stable) words that are not sense injected, but increase in frequency in the same way as the changed words."
P19-1044,2019,7 Conclusions and future work,"we train one vector space model over the whole corpus, and thus share information of the context words while training individual vectors for each target word and time period."
P19-1045,2019,5 Conclusion,aan takes the advantages from both adversarial learning and attention mechanism by conducting adversarial learning between two attention layers in order to learn better weighted information in a given text.
P19-1045,2019,5 Conclusion,empirical evaluation on emobank reader’s and writer’s threedimensional emotion regression tasks shows the superiority of the proposed model with better performance over several state-of-the-art baselines.
P19-1045,2019,5 Conclusion,"however, our proposed aan still has several limitations."
P19-1045,2019,5 Conclusion,"in our future work, we would like to improve the model structure and the adversarial learning algorithm."
P19-1045,2019,5 Conclusion,"in this paper, we propose an adversarial attention network (aan) for multi-dimensional emotion regression."
P19-1045,2019,5 Conclusion,"last but not least, we would like to apply our approach to other heterogeneous texts-concerned nlp tasks."
P19-1045,2019,5 Conclusion,"moreover, we would like to seek a stable and controllable way to conduct adversarial learning among more than two objects."
P19-1045,2019,5 Conclusion,this indicates the effectiveness of the proposed adversarial learning approach to multidimensional emotion regression.
P19-1046,2019,5 Conclusion,hffn learns local interactions at each local chunk and explores global interactions by conveying information across local interactions using abs-lstm that integrates two levels of attention mechanism.
P19-1046,2019,5 Conclusion,"in future work, we intend to explore multiple local fusion methods within our framework."
P19-1046,2019,5 Conclusion,our fusion strategy is generic for other concrete fusion methods.
P19-1046,2019,5 Conclusion,"we propose an efficient and effective framework hffn that adopts a novel fusion strategy called ‘divide, conquer and combine’."
P19-1047,2019,6 Conclusions and future work,"in this work we (a) correlate pragmatic features of analysts’ questions with the pre-call judgment of the questioner, (b) explore the influence of market, semantic and pragmatic features of earnings calls on analysts’ subsequent decisions."
P19-1047,2019,6 Conclusions and future work,"promising directions for future research include examining additional features and feature representations: pragmatic features such as formality (pavlick and tetreault, 2016) or politeness (danescu-niculescu-mizil et al., 2013); acousticprosodic features from earnings call audio; more sophisticated semantic representations such as claims (lim et al., 2016), automatically induced entity-relation graphs (bansal et al., 2017) or question-answer motifs (zhang et al., 2017) (these representations are non-trivial to construct because a single turn may contain many questions or answers); or even discourse structures."
P19-1047,2019,6 Conclusions and future work,"the models used in this work aim to be just complex enough to determine whether useful signals exist for this task; future modeling work could include training a complete end-to-end system such as a hierarchical attention network (yang et al., 2016), or building industry-specific models."
P19-1047,2019,6 Conclusions and future work,we also demonstrate earnings calls are moderately predictive of changes in analysts’ forecasts.
P19-1047,2019,6 Conclusions and future work,"we show that bullish analysts are more likely to ask slightly more positive and concrete questions, talk less about the past, and be called on earlier in a call."
P19-1048,2019,5 Conclusion,"in addition, imn is able to learn from multiple training data sources, allowing fine-grained token-level tasks to benefit from document-level labeled corpora."
P19-1048,2019,5 Conclusion,"the proposed architecture can potentially be applied to similar tasks such as relation extraction, semantic role labeling, etc."
P19-1048,2019,5 Conclusion,"the proposed imn introduces a novel message passing mechanism that allows informative interactions between tasks, enabling the correlation to be better exploited."
P19-1048,2019,5 Conclusion,"we propose an interactive multi-task learning network imn for jointly learning aspect and opinion term co-extraction, and aspect-level sentiment classification."
P19-1049,2019,5 Conclusion,"in this work, we have presented an approach for linking premises and conclusions that uses the similarity of target concepts and aspects, and the agreement between the opinions on target concepts and aspects of edus."
P19-1049,2019,5 Conclusion,it would also be nice to explore about more fine-grained functional components and grammatical entities in the future works.
P19-1049,2019,5 Conclusion,"not only does our dam approach outperform the current state of the art, most importantly, it is shown to work without modification across heterogeneous corpora (aaec, amt and us2016g1tv) which are substantially different in kind."
P19-1049,2019,5 Conclusion,"this generality is an important milestone in the development of argument mining techniques and suggests that a combination of structural and distributional techniques, as employed here, offers the potential for robust, domain-independent performance in this extremely demanding task."
P19-1049,2019,5 Conclusion,"we have demonstrated that the argument relations existing between propositions are largely dependent on the relations existing between the individual components (target concepts, aspects, opinions on target concepts and opinions on aspects) of the propositions."
P19-1050,2019,7 Conclusion,"additionally, we also provide the features used in our baseline experiments."
P19-1050,2019,7 Conclusion,"building upon this dataset, future research can explore the design of efficient multimodal fusion algorithms, novel erc frameworks, as well as the extraction of new features from the audio, visual, and textual modalities."
P19-1050,2019,7 Conclusion,"in this work, we introduced meld, a multimodal multi-party conversational emotion recognition dataset."
P19-1050,2019,7 Conclusion,"meld contains raw videos, audio segments, and transcripts for multimodal processing."
P19-1050,2019,7 Conclusion,we believe this dataset will also be useful as a training corpus for both conversational emotion recognition and multimodal empathetic response generation.
P19-1050,2019,7 Conclusion,"we described the process of building this dataset, and provided results obtained with strong baseline methods applied on this dataset."
P19-1051,2019,5 Conclusion,"model analysis reveals that the main performance improvement comes from the span-level polarity classifier, and the multi-target extractor is more suitable for long sentences."
P19-1051,2019,5 Conclusion,"moreover, we find that the pipeline model consistently surpasses both the joint model and the collapsed model."
P19-1051,2019,5 Conclusion,"on top of it, we design a multi-target extractor for proposing multiple candidate targets with an heuristic multispan decoding algorithm, and introduce a polarity classifier that predicts the sentiment towards each candidate using its summarized span representation."
P19-1051,2019,5 Conclusion,our approach firmly outperforms the sequence tagging baseline as well as previous stateof-the-art methods on three benchmark datasets.
P19-1051,2019,5 Conclusion,the framework contains a pre-trained transformer encoder as the backbone network.
P19-1051,2019,5 Conclusion,"we re-examine the drawbacks of sequence tagging methods in open-domain targeted sentiment analysis, and propose an extract-then-classify framework with the span-based labeling scheme instead."
P19-1052,2019,6 Conclusion,experiments on two semeval datasets demonstrate that transcap outperforms the stateof-the-art baselines by a large margin.
P19-1052,2019,6 Conclusion,"in order to solve the problem of lacking aspect-level labeled data, we wish to utilize the abundant document-level labeled data."
P19-1052,2019,6 Conclusion,"in this paper, we present a novel transfer capsule network (transcap) model for aspect-level sentiment classification."
P19-1052,2019,6 Conclusion,we develop a transfer learning framework to transfer knowledge from the document-level task to the aspect-level task.
P19-1052,2019,6 Conclusion,"we implement it with a carefully designed capsule network, which mainly consists of the aspect routing and dynamic routing approaches."
P19-1053,2019,6 Conclusion and Future Work,"in this paper, we have explored how to automatically mine supervision information for attention mechanisms of neural asc models."
P19-1053,2019,6 Conclusion and Future Work,our method is general for attention mechanisms.
P19-1053,2019,6 Conclusion and Future Work,"then, we propose a novel approach to automatically and incrementally mine attention supervision information for neural asc models."
P19-1053,2019,6 Conclusion and Future Work,these mined information can be further used to refine the model training via a regularization term.
P19-1053,2019,6 Conclusion and Future Work,"through indepth analyses, we first point out the defect of the attention mechanism for asc: a few frequent words with sentiment polarities are tend to be over-learned, while those with low frequency often lack sufficient learning."
P19-1053,2019,6 Conclusion and Future Work,"thus, we plan to extend our approach to other neural nlp tasks with attention mechanisms, such as neural document classification (yang et al., 2016) and neural machine translation (zhang et al., 2018)."
P19-1053,2019,6 Conclusion and Future Work,"to verify the effectiveness of our approach, we apply our approach into two dominant neural asc models, where experimental results demonstrate our method significantly improves the performance of these two models."
P19-1054,2019,6 Conclusion,a good argument similarity function is only the first step towards argument clustering.
P19-1054,2019,6 Conclusion,"arguments can address multiple aspects and therefore belong to multiple clusters, something that is not possible to model using partitional algorithms."
P19-1054,2019,6 Conclusion,"as the annotation showed, about 16% of the pairs were noisy and did not address the target topic."
P19-1054,2019,6 Conclusion,"further, more realistic datasets, that allow end-to-end evaluation, are required."
P19-1054,2019,6 Conclusion,future work should thus study the overlapping nature of argument clustering.
P19-1054,2019,6 Conclusion,"in this publication, we annotated similar argument pairs that came from an argument search engine."
P19-1054,2019,6 Conclusion,"open-domain argument search, i.e.identifying and aggregating arguments for unseen topics, is a challenging research problem."
P19-1054,2019,6 Conclusion,"previous datasets on argument similarity used curated lists of arguments, which eliminates noise from the argument classification step."
P19-1054,2019,6 Conclusion,"previous methods achieved low f1-scores in a crosstopic scenario, e.g., stab et al.(2018b) achieved an f1-score of .27 for identifying pro-arguments."
P19-1054,2019,6 Conclusion,the first challenge is to identify suitable arguments.
P19-1054,2019,6 Conclusion,"the main performance gain came from integrating topic information into the transformer network of bert, which added 13pp compared to the setup without topic information."
P19-1054,2019,6 Conclusion,the second challenge we addressed is to decide whether two arguments on the same topic are similar.
P19-1054,2019,6 Conclusion,"unsupervised methods on argument similarity showed rather low performance scores, confirming that fine-grained semantic nuances and not the lexical overlap determines the similarity between arguments."
P19-1054,2019,6 Conclusion,we could significantly improve this performance to .53 by using contextualized word embeddings.
P19-1054,2019,6 Conclusion,we evaluated the agglomerative clustering algorithm in combination with our similarity function and identified it as a new source of errors.
P19-1054,2019,6 Conclusion,"we were able to train a supervised similarity function based on the bert transformer network that, even with little training data, significantly improved over unsupervised methods."
P19-1054,2019,6 Conclusion,"while these results are very encouraging and stress the feasibility of open-domain argument search, our work also points to some weaknesses of the current methods and datasets."
P19-1055,2019,7 Conclusions,"as our experiments show, modular learning can be used with weak supervision, using examples annotated with partial labels only."
P19-1055,2019,7 Conclusions,"our models, inspired by cognitive neuroscience findings (jacobs et al., 1991; eysenck and keane, 2005) and multitask learning, suggest a functional decomposition of the original task into two simpler sub-tasks."
P19-1055,2019,7 Conclusions,"the modular approach also provides interesting directions for future research, focusing on alleviating the supervision bottleneck by using large amount of partially labeled data that are cheaper and easy to obtain, together with only a handful amount of annotated data, a scenario especially suitable for low-resource languages."
P19-1055,2019,7 Conclusions,"we evaluate different methods for sharing information and integrating the modules into the final decision, such that a better model can be learned, while converging faster5 ."
P19-1055,2019,7 Conclusions,we experiment with several sentiment analysis tasks.
P19-1055,2019,7 Conclusions,we present and study several modular neural architectures designed for a novel learning scenario: learning from partial labels.
P19-1056,2019,5 Conclusion,experimental results on three benchmark datasets verified the effectiveness of doer and showed that it significantly outperforms the baselines on aspect term-polarity co-extraction.
P19-1056,2019,5 Conclusion,"in this paper, we introduced a co-extraction task involving aspect term extraction and aspect sentiment classification for aspect-based sentiment analysis and proposed a novel framework doer to solve the problem."
P19-1056,2019,5 Conclusion,the framework uses a joint sequence labeling approach and focuses on the interaction between two separate routes for aspect term extraction and aspect sentiment classification.
P19-1056,2019,5 Conclusion,"to enhance the representation of sentiment and alleviate the difficulty of long aspect terms, two auxiliary tasks were also introduced in our framework."
P19-1058,2019,6 Conclusion,evaluation on ctdb shows that our proposed ttn model significantly outperforms several state-of-the-art baselines in both micro and macro f1-scores.
P19-1058,2019,6 Conclusion,"in addition to using a gcnbased encoder to obtain the sentence-level argument representations, we train a stm to infer the latent topic distribution as the topic-level representations."
P19-1058,2019,6 Conclusion,"in the future work, we will focus on how to mine different representations for different discourse relation types and apply the topic information to other languages."
P19-1058,2019,6 Conclusion,"in this paper, we propose a topic tensor network ttn to recognize implicit discourse relations in chinese with both the sentence-level and topiclevel representations."
P19-1058,2019,6 Conclusion,"moreover, we feed the two pairs of representations to two ftns, respectively, to model the sentence-level interactions and topic-level relevance among arguments."
P19-1059,2019,5 Conclusion,future work should provide further exploration of the data regime in which pragmatic learning is most beneficial and its correspondence to realworld language use.
P19-1059,2019,5 Conclusion,"in particular, the argument in our introduction suggests that especially frequent objects and low-cost utterances are the seed from which pragmatic inference can proceed over more complex language and infrequent objects."
P19-1059,2019,5 Conclusion,our experiments provide evidence that using pragmatic reasoning during training can yield improved neural semantics models.
P19-1059,2019,5 Conclusion,"overall, we have shown that pragmatic reasoning regarding alternative utterances provides a useful inductive bias for learning in grounded language understanding systems—leveraging inferences over what speakers choose not to say to reduce the data complexity of learning."
P19-1059,2019,5 Conclusion,"this asymmetry in object reference rates is expected for longtail, real-world regimes consistent with zipf’s law (zipf, 1949)."
P19-1059,2019,5 Conclusion,this might include scaling with linguistic complexity and properties of referents.
P19-1059,2019,5 Conclusion,"this was true in the existing color reference corpus, where we achieved state-of-the art results, and even more so in the new color-grid corpus."
P19-1059,2019,5 Conclusion,"we thus found that pragmatic training is more effective when data is relatively sparse and the domain yields complex, high-cost utterances and low-cost omissions over which pragmatic inferences might proceed."
P19-1060,2019,6 Conclusion,"as part of future work, we would like to investigate the use of contextualized embeddings (e.g., bert, devlin et al.(2018)) for coherence assessment – as such representations have been shown to carry syntactic information of words (tenney et al., 2019) – and whether they allow multi-task learning frameworks to learn complementary aspects of language."
P19-1060,2019,6 Conclusion,"we assessed the extent to which our framework generalizes to different domains and prediction tasks, and demonstrated its effectiveness against a number of baselines not only on standard binary evaluation coherence tasks, but also on tasks involving the prediction of varying degrees of coherence, achieving a new state of the art."
P19-1060,2019,6 Conclusion,we have presented a hierarchical multi-task learning framework for discourse coherence that takes advantage of inductive transfer between two tasks: predicting the gr type of words at the bottom layers of the network and predicting a document-level coherence score at the top layers.
P19-1061,2019,6 Conclusions and Future Work,"in future work, we will enrich our weak supervision system by giving the lfs access to more sophisticated contexts that take into account global structuring constraints in order to see how they compare to exogenous decoding constraints applied in (muller et al., 2012; perret et al., 2016)."
P19-1061,2019,6 Conclusions and Future Work,still it is clear that we must further investigate the interaction of the generative and discriminative models in order to eventually leverage the power of generalization a discriminative model is supposed to afford.
P19-1061,2019,6 Conclusions and Future Work,"the results of the model from snorkel’s generative step surpass those of a standard supervised learning approach, showing it to be a promising method for generating a lot of annotated data in a very short time relative to what is needed for a traditional approach: from (asher et al., 2016) we infer that the stac corpus took at least 4 years to build; we created and refined our label functions in 2 months."
P19-1061,2019,6 Conclusions and Future Work,"we have compared a weak supervision approach, as implemented in snorkel, with a standard supervised model on the difficult task of discursive attachment."
P19-1061,2019,6 Conclusions and Future Work,we hope such experiments with the weak supervision paradigm will eventually lead us to understand how weakly supervised methods might effectively capture the global structural constraints on discourse structures without decoding or more elaborate learning architectures.
P19-1062,2019,6 Conclusion,"in this paper, we evaluate structured attention in document representations as a proxy for discourse structure."
P19-1062,2019,6 Conclusion,"nevertheless, calculating statistics on these trees and comparing them to parsed rst trees shows they still contain no meaningful discourse structure."
P19-1062,2019,6 Conclusion,"we first find structured attention at the document level is largely unhelpful, and second it instead captures lexical cues resulting in vacuous trees with little structure."
P19-1062,2019,6 Conclusion,we propose several principled changes to induce better structures with comparable performance.
P19-1062,2019,6 Conclusion,"we theorize some amount of supervision, such as using ground-truth discourse trees, is needed for guiding and constraining the tree induction."
P19-1063,2019,6 Conclusion,"the decoder is based on a pragmatic listener that has hidden beliefs about a referent’s category, which leads the pragmatic speaker to use fewer nouns when being uncertain about this category."
P19-1063,2019,6 Conclusion,"we have presented a pragmatic approach to modeling zero-shot reference games, showing that bayesian reasoning inspired by rsa can help decoding a neural generator that refers to novel objects."
P19-1063,2019,6 Conclusion,"while some aspects of the experimental setting are, admittedly, simplified (e.g.compilation of an artificial test set, uncertainty estimation), we believe that this is an encouraging result for scaling models in computational pragmatics to realworld conversation and its complexities."
P19-1064,2019,5 Conclusion,another interesting direction would be introducing intermediate step rewards for each action to better guide the behaviour of the rl agent.
P19-1064,2019,5 Conclusion,experiments on the english ontonotes benchmark demonstrate that our full model integrated with entropy regularization significantly outperforms previous coreference systems.
P19-1064,2019,5 Conclusion,our model transforms the supervised higher order coreference model to a policy gradient model that can directly optimizes coreference evaluation metrics.
P19-1064,2019,5 Conclusion,"there are several potential improvements to our model as future work, such as incorporating mention detection result as a part of the reward."
P19-1064,2019,5 Conclusion,we present the first end-to-end reinforcement learning based coreference resolution model.
P19-1065,2019,6 Conclusion and Future Work,"in this paper, we proposed a novel pipeline specifically designed for implicit discourse relation identification in open-domain dialogue."
P19-1065,2019,6 Conclusion and Future Work,more sophisticated dialogue features and classification algorithms are needed for the discourse relation identification task in addition to a larger more balanced corpus.
P19-1065,2019,6 Conclusion and Future Work,our experiments show that dialogue intent and entities types play important roles and dialogue features can increase the performance of the discourse relation identification model.
P19-1065,2019,6 Conclusion and Future Work,"since implicit discourse relation identification is a key task for dialogue systems, there are still many approaches worth investigating in future work."
P19-1065,2019,6 Conclusion and Future Work,"we constructed a novel dataset of discourse relation pairs for dialogue conversations, and utilized unique dialogue features to enhance the performance of a state-of-the-art classifier."
P19-1066,2019,8 Conclusion,"furthermore, we can consider more structured representations of entities that reflect entity attributes and inter-entity relations."
P19-1066,2019,8 Conclusion,here we implemented this idea by summing all mention representations within a cluster.
P19-1066,2019,8 Conclusion,in the future we plan to further enrich these representations by considering information from across the document.
P19-1066,2019,8 Conclusion,in this work we presented a new state-of-the-art coreference resolution system.
P19-1066,2019,8 Conclusion,key to our approach is the idea that each mention should contain information about all its coreferring mentions.
P19-1067,2019,6 Conclusion,experimental results on a wide range of tasks and datasets demonstrate that the proposed model outperforms previous state-of-the-art methods significantly and consistently on both domain-specific and open-domain datasets.
P19-1067,2019,6 Conclusion,"in this paper, we examined the limitations of two general frameworks for coherence modelling; i.e., passage-level discriminative models and generative models."
P19-1067,2019,6 Conclusion,we propose a simple yet effective local discriminative neural model which retains the advantages of generative models while addressing the limitations of both kinds of models.
P19-1068,2019,7 Conclusion,"although romanian and moldavian are supposed to be hard to discriminate, since romania and the republic of moldova share the same literary standard (minahan, 2013), the empirical results seem to point in the other direction, to our surprise."
P19-1068,2019,7 Conclusion,another option for improving performance is to combine string kernels and neural networks into an ensemble model.
P19-1068,2019,7 Conclusion,"first of all, the text samples are formed of 309 tokens on average, being at least an order of magnitude longer than samples in typical dialectal corpora (ali et al., 2016; samardziˇ c et al.′ , 2016)."
P19-1068,2019,7 Conclusion,"however, we should note that the high accuracy rates attained by the proposed classifiers can be explained through a combination of two factors."
P19-1068,2019,7 Conclusion,"in future work, we aim to determine if the same level of accuracy can be obtained when single sentences will be used as samples for training and testing."
P19-1068,2019,7 Conclusion,"in this context, we acknowledge that better accuracy rates can be obtained by combining string kernels using a range of n-grams, as we have already shown for other dialects and tasks in our previous works (butnaru and ionescu, 2018; cozma et al., 2018; ionescu and butnaru, 2017, 2018)."
P19-1068,2019,7 Conclusion,"in this paper, we presented a novel and large corpus of moldavian and romanian dialects."
P19-1068,2019,7 Conclusion,our intention was not that of providing top accuracy rates on the moroco corpus.
P19-1068,2019,7 Conclusion,"second of all, the text samples can be discriminated in large part due to different word choices, as shown in the analysis of the most discriminative features provided in section 5."
P19-1068,2019,7 Conclusion,"we also introduced squeeze-and-excitation networks to the nlp domain, performing comparative experiments using shallow and deep state-of-the-art baselines."
P19-1068,2019,7 Conclusion,we leave these ideas for future exploration.
P19-1068,2019,7 Conclusion,we would like to stress out that the methods presented in this paper are only provided as baselines in order to enable comparisons in future work.
P19-1068,2019,7 Conclusion,word preference seems to become easily distinguishable when news samples of around 309 tokens (multiple sentences) are used.
P19-1069,2019,6 Conclusions,"as future work we plan to exploit a subset of the wikipedia categories as coarse-grained sense inventory and enrich our dataset with coarser labels, hence enabling wsd at different granularities."
P19-1069,2019,6 Conclusions,"furthermore, onesec scales to multiple languages without any additional human effort."
P19-1069,2019,6 Conclusions,"in this paper we presented onesec, a novel method for the automatic creation of multilingual sense-annotated corpora on a large scale."
P19-1069,2019,6 Conclusions,"indeed, our approach also proved to be capable of producing high-quality training data for low-resourced languages, leading a wsd supervised model to achieve state-of-the-art results on all the datasets of the multilingual wsd tasks."
P19-1069,2019,6 Conclusions,"moreover, we take a further step towards removing any dependency on a semantic-network structure by exploiting only wikipedia categories and a sparse vector representation of concepts for creating our datasets."
P19-1069,2019,6 Conclusions,"onesec outperforms its automatic and semi-automatic alternatives on the english wsd task, and achieves results in the same ballpark as those attained when manually-curated corpora are used for training."
P19-1069,2019,6 Conclusions,"our approach relieves the burden of human intervention, hence mitigating the knowledge acquisition bottleneck besetting wsd training data."
P19-1069,2019,6 Conclusions,"we release more than one million tagged sentences for english, spanish, italian, french and german at http://trainomatic.org/onesec."
P19-1070,2019,5 Conclusion,"by systematically evaluating cle models for many language pairs on bli and three downstream tasks, we shed new light on the ability of current cutting-edge cle models to support cross-lingual nlp."
P19-1070,2019,5 Conclusion,"cle models are commonaly evaluated only in bilingual lexicon induction (bli), and even the bli task includes a variety of evaluation setups which are not directly comparable, hindering our ability to correctly interpret the key results."
P19-1070,2019,5 Conclusion,"in particular, we have empirically proven that the quality of cle models is largely task-dependent and that overfitting the models to the bli task can result in deteriorated performance in downstream tasks."
P19-1070,2019,5 Conclusion,"in this work, we have made the first step towards a comprehensive evaluation of cles."
P19-1070,2019,5 Conclusion,rapid development of cross-lingual word embedding (cle) methods is not met with adequate progress in their fair and systematic evaluation.
P19-1070,2019,5 Conclusion,"we have highlighted the most robust supervised and unsupervised cle models and have exposed the need for reassessing existing baselines, as well as for unified and comprehensive evaluation protocols."
P19-1070,2019,5 Conclusion,we hope that this study will encourage future work on cle evaluation and analysis and help guide the development of new cle models.
P19-1071,2019,8 Conclusion,"after that, we demonstrate the potential of using sp to represent commonsense knowledge, which can be beneficial for the acquisition and application of commonsense knowledge."
P19-1071,2019,8 Conclusion,"compared with other evaluation methods, sp-10k has much larger coverage and can better represent ground truth sp."
P19-1071,2019,8 Conclusion,"in the end, we demonstrate the importance of the two-hop relations with a subset of the winograd schema challenge."
P19-1071,2019,8 Conclusion,"in this work, we present sp-10k, a large-scale human-labeled evaluation set for selectional preference."
P19-1071,2019,8 Conclusion,two novel two-hop sp relations ‘dobj amod’ and ‘nsubj amod’ are also introduced.
P19-1071,2019,8 Conclusion,we evaluate three representative sp acquisition methods with our dataset.
P19-1072,2019,7 Conclusion,the diachronic and synchronic evaluation tasks we introduced were solved with impressively high performance and robustness.
P19-1072,2019,7 Conclusion,"the overall best performing approach on both data suggests to learn vector representations for different time periods (or domains) with sgns, to align them with an orthogonal mapping, and to measure change with cosine distance."
P19-1072,2019,7 Conclusion,we carried out the first systematic comparison of a wide range of lsc detection models on two datasets which were reliably annotated for sense divergences across corpora.
P19-1072,2019,7 Conclusion,we further improved the performance of the best approach with the application of mean-centering as an important pre-processing step for rotational vector space alignment.
P19-1072,2019,7 Conclusion,"we introduced word injection to overcome the need of (post-hoc) alignment, but find that orthogonal procrustes yields a better performance across vector space types."
P19-1074,2019,7 Conclusion,"experiments show that human performance is significantly higher than re baseline models, which suggests ample opportunity for future improvement."
P19-1074,2019,7 Conclusion,"to promote re systems from sentence level to document level, we present docred, a large-scale document-level re dataset that features the data size, the requirement for reading and reasoning over multiple sentences, and the distantly supervised data offered for facilitating the development of weakly supervised document-level re."
P19-1075,2019,6 Conclusion,all these findings indicate that the corpus may be a proper benchmark for chinese cloze test and worth further research.
P19-1075,2019,6 Conclusion,"firstly, we analyze how the embedding similarity correlates with synonymity and near-synonymity of chinese idiom, and find that the difficulty level of chinese cloze test with idiom correlates positively with the method of choosing candidate choices."
P19-1075,2019,6 Conclusion,"in this paper, we propose a large-scale chinese cloze dataset (chid) which contains 581k passages and 729k queries from news, novels, and essays, covering 3,848 chinese idioms."
P19-1075,2019,6 Conclusion,"secondly, we find that idiom representation is a key factor to the success of reading comprehension models in this task due to the common non-compositionality and metaphorical meaning of chinese idiom."
P19-1075,2019,6 Conclusion,the corpus provides a benchmark to evaluate the ability of chinese cloze test with idiom.
P19-1075,2019,6 Conclusion,"thirdly, we evaluate three stateof-the-art cloze test models on this corpus, and observe that existing model performance is still much worse than human performance."
P19-1076,2019,7 Conclusion,"we apply this human evaluation to a wide variety of models, and find that topic switch percent (or switchp) correlates well with this human evaluation."
P19-1076,2019,7 Conclusion,"we develop a novel crowdsourcing task, which we call topic-word matching, to illicit human judgments of local topic model quality."
P19-1076,2019,7 Conclusion,"we propose that this new metric, which we colloquially refer to as consistency, be adopted alongside evaluations of global topic quality for future work with topic model comparison."
P19-1077,2019,9 Discussion and Conclusions,"about 5,000 sentences were annotated by regular (i.e., not paid) players in this initial development phase, but we expect the game will be able to collect a comparable amount of judgments as for phrase detectives once it’s fully operational and properly advertised."
P19-1077,2019,9 Discussion and Conclusions,"anda gamified interface such as tileattack can be beneficial even for projects who just use microtask crowdsourcing, as it has been shown that gamified hits are more popular (morschheuser et al., 2017)."
P19-1077,2019,9 Discussion and Conclusions,"as a rule of thumb, of the two best-known forms of crowdsourcing, microtask crowdsourcing using platforms such as amazon mechanical turk is best to label small to medium size amounts of data in a short time, and for labelling data of no intrinsic interest."
P19-1077,2019,9 Discussion and Conclusions,data from automatic mention detectors and players are then aggregated using a probabilistic aggregation method choosing the most likely interpretation in a nominal head-centered cluster.
P19-1077,2019,9 Discussion and Conclusions,"in this paper, we presented a hybrid mention detection method combining state-of-the-art automatic mention detectors with a gamified, twoplayer interface to collect markable judgements."
P19-1077,2019,9 Discussion and Conclusions,one example in point is the phrase detectives annotation effort.
P19-1077,2019,9 Discussion and Conclusions,"the approach to mention detection proposed in this paper was developed in support of games such as phrase detectives, thus a gwap or at least gamified approach as exemplified by tileattack was deemed more appropriate even if the judgments used in this paper were collected using amazon mechanical turk for speed."
P19-1077,2019,9 Discussion and Conclusions,the integration takes place by using the automatic mention detectors as ‘players’ in the game.
P19-1077,2019,9 Discussion and Conclusions,"the latest release of these data (poesio et al., 2019) contains 2.2m judgments, around 4 times the number of judgments collected for ontonotes."
P19-1077,2019,9 Discussion and Conclusions,"these results suggest that it may be possible to gamify not just the task of annotating coreference, but also the prerequisite steps to that."
P19-1077,2019,9 Discussion and Conclusions,"we also test the approach in genres outside those in which the automatic pipelines were trained, showing that high accuracy can be achieved in these as well."
P19-1077,2019,9 Discussion and Conclusions,"we showed that using this combination we can achieve, in the news domain, an accuracy at mention identification that is almost three percentage points higher than that obtained with an automatic domain-trained mention detector, and over seven percentage points higher than that obtained with a domain-independent one."
P19-1077,2019,9 Discussion and Conclusions,"whereas crowdsourcing with gameswith-a-purpose is best in cases when the objective is to collect very large amounts of labels, so that the initial costs for setting up the game can be offset by the reduced costs of labelling (poesio et al., 2013)."
P19-1078,2019,7 Conclusion,"in future work, transferring knowledge from other resources can be applied to further improve zero-shot performance, and collecting a dataset with a large number of domains is able to facilitate the application and study of metalearning techniques within multi-domain dst."
P19-1078,2019,7 Conclusion,"moreover, domain sharing enables trade to perform zero-shot dst for unseen domains and to quickly adapt to few-shot domains without forgetting the learned ones."
P19-1078,2019,7 Conclusion,trade shares all of its parameters across multiple domains and achieves stateof-the-art joint goal accuracy and slot accuracy on the multiwoz dataset for five different domains.
P19-1078,2019,7 Conclusion,"we introduce a transferable dialogue state generator for multi-domain dialogue state tracking, which learns to track states without any predefined domain ontology."
P19-1079,2019,7 Conclusions,experimental results on a large-scale alexa dataset show the effectiveness of adding task group encoders into both parallel and serial mtl networks.
P19-1079,2019,7 Conclusions,our mtl models obtain state-of-the-art performance on the atis and snips datasets for intent classification and slot filling.
P19-1079,2019,7 Conclusions,we further explored learning these features in parallel and serial mtl architectures.
P19-1079,2019,7 Conclusions,"we proposed a series of end-to-end multi-task learning architectures, in which task, task group and task universe features are learned nonredundantly."
P19-1080,2019,7 Conclusions,we also propose a constrained decoding technique that leverages tree-structured mrs to exert precise control over the discourse structure and semantic correctness of the generated text.
P19-1080,2019,7 Conclusions,"we release a challenging new dataset for the weather domain and an enriched e2e dataset that include tree-structured mrs. our experiments show that constrained decoding, together with tree-structured mrs, can greatly improve semantic correctness as well as enhance data efficiency and generalizability."
P19-1080,2019,7 Conclusions,we show that using rich tree-structured meaning representations can improve expressiveness and semantic correctness in generation.
P19-1081,2019,6 Conclusions,"for this study, we collect a newly annotated dialog ?"
P19-1081,2019,6 Conclusions,kg parallel corpus of 15k humanto-human dialogs which includes ground-truth annotation of each dialog turn to its reasoning reference in a large-scale common fact kg.
P19-1081,2019,6 Conclusions,"our proposed dialkg walker model improves upon the state-of-the-art knowledge-augmented conversation models by 1) a novel attention-based graph decoder that penalizes decoding of unnatural paths which effectively prunes candidate entities and paths from a large search space (1.1m facts), 2) a zeroshot learning model that predicts its relevance score in the kg embeddings space, combined score of which is used for candidate ranking."
P19-1081,2019,6 Conclusions,"the empirical results from in-domain, cross-domain, and transfer learning evaluation demonstrate the efficacy of the proposed model in domain-agnostic conversational reasoning."
P19-1081,2019,6 Conclusions,"we study conversational reasoning grounded on knowledge graphs, and formulate an approach in which the model learns to navigate a largescale, open-ended kg given conversational contexts."
P19-1082,2019,8 Conclusion,"in this paper, we present an approach which combines a context-aware retrieval model and model-agnostic meta-learning (maml) to utilize multiple retrieved examples for context-dependent semantic parsing."
P19-1082,2019,8 Conclusion,our approach achieves thestate-of-the-art performances and outperforms tworetrieve-and-edit baselines.
P19-1082,2019,8 Conclusion,we show that both context-awareretriever and maml are useful on concodeand csqa datasets.
P19-1083,2019,8 Conclusion,"as a result, any knowledge resource presented in the format of triplets, the most widely used entry format for kg, can be consumed in our model with a proposed attention module."
P19-1083,2019,8 Conclusion,experimental results on two different corpora from two domains demonstrate the superiority of the proposed model to all baselines.
P19-1083,2019,8 Conclusion,"in this paper, we explore how to build a knowledge-aware pronoun coreference resolution model, which is able to leverage different external knowledge for this task."
P19-1083,2019,8 Conclusion,"moreover, as our model learns to use knowledge rather than just fitting the training data, our model achieves much better and more robust performance than state-ofthe-art models in the cross-domain scenario."
P19-1083,2019,8 Conclusion,"the proposed model is an attempt of the general solution of incorporating knowledge (in the form of kg) into the deep learning based pronoun coreference model, rather than using knowledge as features or rules in a dedicated manner."
P19-1084,2019,8 Conclusion,biases in annotations are a major source of concern for the quality of nli datasets and systems.
P19-1084,2019,8 Conclusion,"furthermore, we performed several analyses into the interplay of our methods with known biases in nli datasets, the effects of stronger bias removal, and the possibility of fine-tuning on the target datasets."
P19-1084,2019,8 Conclusion,"our methodology can be extended to handle biases in other tasks where one is concerned with finding relationships between two objects, such as visual question answering, story cloze completion, and reading comprehension."
P19-1084,2019,8 Conclusion,we demonstrated that this discourages the hypothesis encoder from learning the biases to instead obtain a less biased representation.
P19-1084,2019,8 Conclusion,we hope to encourage such investigation in the broader community.
P19-1084,2019,8 Conclusion,we presented a solution for combating annotation biases by proposing two training methods to predict the probability of a premise given an entailment label and a hypothesis.
P19-1084,2019,8 Conclusion,"when empirically evaluating our approaches, we found that in a synthetic setting, as well as on a wide-range of existing nli datasets, our methods perform better than the traditional training method to predict a label given a premise-hypothesis pair."
P19-1085,2019,6 Conclusion,"in the future, we would like to design a multi-step evidence extractor and incorporate external knowledge into our framework."
P19-1085,2019,6 Conclusion,the framework is proven to be effective and our final pipeline achieves significant improvements.
P19-1085,2019,6 Conclusion,"the framework utilizes the bert sentence encoder, the evidence reasoning network (ernet) and an evidence aggregator to encode, propagate and aggregate information from multiple pieces of evidence."
P19-1085,2019,6 Conclusion,we propose a novel graph-based evidence aggregating and reasoning (gear) framework on the claim verification subtask of fever.
P19-1086,2019,8 Conclusion,"although sherliic’s creation is entirely data-driven, it shows a large variety of linguistic challenges for nli, ranging from lexical relations like troponymy, synonymy or morph.derivation to typical actions and common sense knowledge (cf.table 1)."
P19-1086,2019,8 Conclusion,the distributional similarity of both positive and negative examples makes sherliic a promising benchmark to track future nli models’ ability to go beyond shallow semantics relying primarily on distributional evidence.
P19-1086,2019,8 Conclusion,"the large unlabeled resources, sherliic-infcands and sherliic-teg, are potentially useful for further linguistic analysis (as we showed in § 6), as well as for data-driven models of lexical semantics, e.g., techniques such as representation learning and domain adaptation."
P19-1086,2019,8 Conclusion,the restriction to nes (as opposed to common nouns) allowed us to harness more event-like relations than previous similar collections as these naturally occur more often with nes.
P19-1086,2019,8 Conclusion,we hope that sherliic will foster better modeling of lexical inference in context as well as progress in nli in general.
P19-1086,2019,8 Conclusion,"we presented sherliic, a new challenging testbed for liic and nli, based on typed textual relations between named entities (nes) from a kg."
P19-1086,2019,8 Conclusion,we showed that existing rule bases are complementary to sherliic and that current semantic vector space models as well as sota neural nli models cannot achieve at the same time high precision and high recall on sherliic.
P19-1087,2019,7 Conclusions,"although the sa-t model was developed to infer symptoms and status in the clinical domain, the formulation is general and can be applied to any domain."
P19-1087,2019,7 Conclusions,"as an alternative, our seq2seq model is designed to infer symptom labels when the evidence is scattered across multiple turns in a dialog and is not easily associated with a specific word span."
P19-1087,2019,7 Conclusions,"our analysis show that the sa-t model has higher precision while seq2seq model has higher recall, thus the two models compliment each other."
P19-1087,2019,7 Conclusions,the first stage alleviates data sparsity by pooling all spans of interest.
P19-1087,2019,7 Conclusions,the performance of our models is significantly better than baseline systems and range from an f-score of 0.5 to 0.8 depending on the condition.
P19-1087,2019,7 Conclusions,the sa-t model breaks up the task into identifying spans of interests and then classifying the span with richer contextual representations.
P19-1087,2019,7 Conclusions,"this paper describes a novel information extraction task, that of extracting the symptoms mentioned in clinical conversations along with their status."
P19-1087,2019,7 Conclusions,training the model on asr transcripts or on both asr and manual transcripts does not help bridge the performance gap.
P19-1087,2019,7 Conclusions,"we describe our corpus, the annotation paradigm, and tailored evaluation metrics."
P19-1087,2019,7 Conclusions,we plan to investigate the impact of combining the two models.
P19-1087,2019,7 Conclusions,we proposed a novel span-attribute tagging (sat) model and a variant of sequence-to-sequence model to solve the problem.
P19-1087,2019,7 Conclusions,"when the label space naturally partitions into separate categories, the second stage can be broken up further into separate prediction tasks and reduces data splitting."
P19-1087,2019,7 Conclusions,"when the models are trained on manual transcripts and applied on asr transcripts, the performance degrades considerably compared to applying them on manual transcripts."
P19-1088,2019,8 Conclusions,"finally, an important contribution of this work is the dataset collected, which is publicly available at http://lit.eecs.umich.edu/downloads.html."
P19-1088,2019,8 Conclusions,"however, the general trend is to discuss topics related to family and social interactions, regardless of the counseling quality."
P19-1088,2019,8 Conclusions,"in contrast, during low-quality conversations counselors tend to speak more than their clients thus making it difficult to understand their needs."
P19-1088,2019,8 Conclusions,"in the future, we plan to build upon the acquired knowledge and the developed classifiers to generate systems able to provide actionable feedback on how to achieve high-quality counseling."
P19-1088,2019,8 Conclusions,linguistic alignment: good counselors mirror the language of their clients as high-quality interactions showed higher levels of linguistic alignment.
P19-1088,2019,8 Conclusions,"our experimental results showed that the proposed features can provide comparable performance to manually coded features for this task, thus potentially bypassing the need for manual annotations, which are usually costly and time-consuming."
P19-1088,2019,8 Conclusions,our main findings are summarized below.
P19-1088,2019,8 Conclusions,"sentiment: good counselors tend to express more positive sentiment than less successful counselors, which suggest that they focus on the positive aspects of the conversations rather than on the negative aspects."
P19-1088,2019,8 Conclusions,such systems can aid the process of acquiring or improving counseling skills for both novice and experienced mi counselors.
P19-1088,2019,8 Conclusions,"the results of our analyses were used to build accurate counseling quality classifiers that rely on linguistic aspects, with accuracies of up to 88% as compared to a majority baseline of 60%."
P19-1088,2019,8 Conclusions,"this is an important finding as an open problem in the counseling field is the need for computational tools that allow scaling-up the evaluation of the quality of mi interventions (atkins et al., 2014)."
P19-1088,2019,8 Conclusions,this trend is more noticeable at turn-by-turn level where steady and increased levels of linguistic mirroring were observed.
P19-1088,2019,8 Conclusions,"this was also confirmed by our analysis of counseling micro-skills, which showed that good counselors use more reflective listening, thus suggesting that they speak less and listen more."
P19-1088,2019,8 Conclusions,topics: good counselors discuss topics related to behavior change and commitment whereas their counterparts focus more on resistance and persuasion.
P19-1088,2019,8 Conclusions,"turn-by-turn interaction: during high-quality counseling, counselors achieve a more balanced word exchange with clients as the conversation progresses."
P19-1088,2019,8 Conclusions,we presented an extensive analysis of linguistic aspects of the collaboration process during counseling conversations in relation to counseling quality.
P19-1088,2019,8 Conclusions,"we specifically analyzed participants’ turn-byturn interaction, linguistic alignment, and topics discussed, as well as the sentiment expressed by the counselor during the conversation."
P19-1089,2019,7 Discussion and Future Work,"computational tools could help us understand how people acquire conversational skills, and may eventually assist in their development."
P19-1089,2019,7 Discussion and Future Work,"for instance, increased diversification may point to flexibility and specialization that are beneficial, but might also signal detrimental deviations from good counseling practices."
P19-1089,2019,7 Discussion and Future Work,"furthermore, while our approach captures linguistic changes in aggregate, a complementary line of work could explore the trajectories of individuals, and probe the factors determining whether particular individuals diversify or stagnate."
P19-1089,2019,7 Discussion and Future Work,future work could adapt our framework to examine more complex forms of conversational development.
P19-1089,2019,7 Discussion and Future Work,"future work could more directly model how counselors respond to texter behaviors, hence gauging the extent to which counselors evolve in their interactional practices."
P19-1089,2019,7 Discussion and Future Work,"having good conversations is challenging and hard to teach, especially in domains where the stakes are high and where feedback is scarce."
P19-1089,2019,7 Discussion and Future Work,"in order to derive such prescriptive recommendations, further work is needed to causally connect our signals of linguistic experience with actual expertise and skill, as reflected in conversational outcomes."
P19-1089,2019,7 Discussion and Future Work,"in particular, we observed that counselors diversify linguistically, perhaps signaling a beneficial increase in flexibility that enables them to better address the specific concerns of a texter."
P19-1089,2019,7 Discussion and Future Work,"in this work we provide an initial and limited case study in a highly consequential domain, showing that we can track diversification in the linguistic practices of mental health counselors."
P19-1089,2019,7 Discussion and Future Work,it could assist in identifying counselors who acclimatize quickly and those that require more guidance.
P19-1089,2019,7 Discussion and Future Work,"other approaches, such as qualitative labeling by domain experts, could examine whether such changes in language use also result in better conversations."
P19-1089,2019,7 Discussion and Future Work,our approach is necessarily limited in scope.
P19-1089,2019,7 Discussion and Future Work,"our methodology could also be extended to examine other conversational contexts such as academic advising or business interactions, where individuals are expected to learn from experience."
P19-1089,2019,7 Discussion and Future Work,"such domains may contain crucial differences that motivate extensions to our framework; for example, feedback might be more readily available and conversational partners may recur, both of which can interact with experience to further shape linguistic development."
P19-1089,2019,7 Discussion and Future Work,the framework we have started to design could eventually help platforms provide counselors with personalized feedback on their development.
P19-1089,2019,7 Discussion and Future Work,understanding how different components in a conversation change with experience could also inform the particular aspects to focus their training on.
P19-1090,2019,6 Conclusion and Future Work,"a next step would be comprehensive error analysis for a better understanding of where the models succeed or fail in capturing semantic information, particularly for the low-resource languages."
P19-1090,2019,6 Conclusion and Future Work,"gaining access to data was subject to ethical clearance and data anonymization, due to the highly sensitive nature of the content and the vulnerability of individuals involved."
P19-1090,2019,6 Conclusion and Future Work,"in the case where the human does not agree with any of the suggested answers, the option can remain for the human operator to manually select the correct standardized response, as is currently done."
P19-1090,2019,6 Conclusion and Future Work,"lstm networks trained end-to-end outperformed all the other models tested, achieving accuracies of about 62% and 56% on the full and low-resource test sets, respectively."
P19-1090,2019,6 Conclusion and Future Work,"such a model can serve in a semiautomated answer selection process, with a human in the loop to choose the final answer."
P19-1090,2019,6 Conclusion and Future Work,the best lstm further achieved a recall@5 of almost 90%.
P19-1090,2019,6 Conclusion and Future Work,there is also scope to develop language identification tools using the unreliable language labels as noisy priors.
P19-1090,2019,6 Conclusion and Future Work,"this could assist with training separate models for the low-resource languages, or provide an answer in the same language as the question."
P19-1090,2019,6 Conclusion and Future Work,"this could significantly reduce the burden of the current staffing compliment, if approximately 70% of the queries can be dealt with in a semi-automated manner."
P19-1090,2019,6 Conclusion and Future Work,"this feedback can help improve the automated response service, and assist future research tasks."
P19-1090,2019,6 Conclusion and Future Work,we are also working to include into our models the long tail in the distribution of template answers.
P19-1090,2019,6 Conclusion and Future Work,"we considered various approaches to the answer selection problem in a noisy, multilingual, low-resource setting."
P19-1090,2019,6 Conclusion and Future Work,we described the first steps towards automating a multilingual digital helpdesk for pregnant and breastfeeding mothers in south africa.
P19-1090,2019,6 Conclusion and Future Work,"we further intend to explore transfer learning techniques (zhang et al., 2017) as well as deep architectures designed specifically for answer selection (lai et al., 2018)."
P19-1091,2019,5 Conclusion,"for future work, we plan to investigate the model on other related tasks such as relation extraction, normalization as well as the use of advanced conditional models."
P19-1091,2019,5 Conclusion,in this paper we have shown that named entity and negation assertion can be modeled in a multitask setting.
P19-1091,2019,5 Conclusion,"joint learning with shared parameters provides better contextual representation and helps in alleviating problems associated with using neural networks for negation detection, thereby achieving better results than the rule-based systems."
P19-1091,2019,5 Conclusion,our proposed conditional softmax decoder achieves best results across both tasks and is robust to work well in extreme low data settings.
P19-1092,2019,5 Conclusion,in this work we have used it as a closed qa dataset (the potential answers are used as input to determine the right one).
P19-1092,2019,5 Conclusion,"nothing prevents to use the dataset in an open setting, where the system is given no clue about the possible answers."
P19-1092,2019,5 Conclusion,"such questions correspond to examinations to access specialized positions in the spanish healthcare system, and require specialized knowledge and reasoning to be answered."
P19-1092,2019,5 Conclusion,"this would require to think as well whether widely used metrics such as bleu (papineni et al., 2002) or exact match could be appropriate for this particular problem."
P19-1092,2019,5 Conclusion,"to check its complexity, we then tested different state-of-the-art models for open-domain and multi-choice questions."
P19-1092,2019,5 Conclusion,we also believe there is room for alternative challenges in head-qa.
P19-1092,2019,5 Conclusion,we hope this work will encourage research on designing more powerful qa systems that can carry out effective information extraction and reasoning.
P19-1092,2019,5 Conclusion,"we presented a complex multi-choice dataset containing questions about medicine, nursing, biology, pharmacology, psychology and chemistry."
P19-1092,2019,5 Conclusion,"we show how they struggle with the challenge, being clearly surpassed by a non-machine learning model based on information retrieval."
P19-1093,2019,7 Discussion and future work,"a possibility that we did not expand on in this paper is to pre-train one leg of the network on an argument detection data set, like the one of shnarch et al.(2018)."
P19-1093,2019,7 Discussion and future work,"argument detection concerns itself with the binary classification of a single text into argument and non-argument, and not the more subjective notion of convincingness."
P19-1093,2019,7 Discussion and future work,but we nonetheless observed in previous experiments significant improvements when initializing the siamese network with weights learned on this task.
P19-1093,2019,7 Discussion and future work,"in addition, more careful design of the architecture details, which was not the focus of this work, will probably yield better results yet, e.g., contextualized word embeddings (peters et al., 2018), batch normalization (ioffe and szegedy, 2015; cooijmans et al., 2017), deeper networks and other architecture practical heuristics."
P19-1093,2019,7 Discussion and future work,"in addition, we suggest our version of a siamese network for the task, which outperforms state of the art methods."
P19-1093,2019,7 Discussion and future work,"in the future we aim to test and adapt other improvements in the learning to rank field to our task, hoping for further improvement by those models (burges, 2010; severyn and moschitti, 2015)."
P19-1093,2019,7 Discussion and future work,"in this work we proposed a focused view for the task of argument convincingness, constructed a new data set for it, and presented its advantages."
P19-1093,2019,7 Discussion and future work,we believe that it is useful to evaluate methods for identifying the more convincing argument on this more challenging data set.
P19-1093,2019,7 Discussion and future work,"we could not reproduce these improvements here, but our previous efforts relied on far fewer training pairs: an explanation could be that pretraining is most helpful when faced with a low amount of training data."
P19-1094,2019,8 Conclusion,debate topic expansion can enhance the coverage of existing argument mining methods by matching relevant arguments that do not mention the given topic explicitly.
P19-1094,2019,8 Conclusion,debate topic expansion may be highly valuable for argumentation mining.
P19-1094,2019,8 Conclusion,"for instance, topicrelated argument mining has many potential use cases, such as helping individuals and organizations make better decisions, enhancing civic discourse by identifying arguments raised in the media, and promoting critical thinking among students."
P19-1094,2019,8 Conclusion,"in addition, distinguishing consistent and contrastive expansions may improve argument stance classification."
P19-1094,2019,8 Conclusion,"the best results are obtained by combining diverse methods and techniques: pattern-based extraction, a novel set of filters and classification features, and a distantly-supervised neural network."
P19-1094,2019,8 Conclusion,"this work introduced a new task, debate topic expansion, along with a corresponding benchmark dataset, which we plan to make publicly available."
P19-1094,2019,8 Conclusion,we plan to pursue these research directions in future work.
P19-1094,2019,8 Conclusion,we presented a working solution for this challenging task that achieved promising empirical results.
P19-1095,2019,5 Conclusions,"for such a scenario, our hierarchical multimodal model outperforms the state-of-theart score with the aid of modality- and contextbased attention mechanisms."
P19-1095,2019,5 Conclusions,our multi-view setting has shown that acoustic models can still benefit from lexical information over models that have been exclusively trained on acoustic features.
P19-1095,2019,5 Conclusions,the first approach assumes that lexical information is always available when the speech signal is being processed.
P19-1095,2019,5 Conclusions,the second approach adapts to a more realistic scenario where lexical data may not be available for evaluation.
P19-1095,2019,5 Conclusions,we presented multimodal and multi-view approaches for emotion recognition.
P19-1096,2019,6 Conclusions and Future Work,"based on a benchmark ece corpus, we construct a corpus suitable for the ecpe task."
P19-1096,2019,6 Conclusions and Future Work,"in the future work, we will try to build a one-step model that directly extract the emotion-cause pairs in an end-to-end fashion."
P19-1096,2019,6 Conclusions and Future Work,"in this paper, we propose a new task: emotioncause pair extraction, which aims to extract potential pairs of emotions and corresponding causes in text."
P19-1096,2019,6 Conclusions and Future Work,"on the one hand, its goal is not direct."
P19-1096,2019,6 Conclusions and Future Work,"on the other hand, the mistakes made in the first step will affect the results of the second step."
P19-1096,2019,6 Conclusions and Future Work,the experimental results prove the effectiveness of our method.
P19-1096,2019,6 Conclusions and Future Work,the two-step strategy may not be a perfect solution to solve the ecpe problem.
P19-1096,2019,6 Conclusions and Future Work,"to deal with this task, we propose a two-step method, in which we first extract both emotions and causes respectively by multi-task learning, then combine them into pairs by applying cartesian product, and finally employ a filter to eliminate the false emotion-cause pairs."
P19-1097,2019,8 Conclusion,"we formalized the notion of commonplace principled arguments, and suggested a concrete and diverse taxonomy for them."
P19-1097,2019,8 Conclusion,we presented a novel framework – 689 controversial motions with a variety of topics and actions – in which the argument invention task can be formalized and assessed.
P19-1097,2019,8 Conclusion,"while this taxonomy can certainly be expanded and refined, it nonetheless has the basic desired properties: most motions in our framework belong to it, annotators tend to agree on copa-motion matching, this matching can be done automatically with reasonable success, and human debaters tend to allude to the ascribed arguments when debating these motions."
P19-1098,2019,7 Conclusion,both aspects should be modelled in calculating pairwise sentence similarity.
P19-1098,2019,7 Conclusion,our system yields competitive results on benchmark datasets surpassing strong summarization baselines.
P19-1098,2019,7 Conclusion,we show that redundant sentences not only have common words but they can be semantically similar with little word overlap.
P19-1098,2019,7 Conclusion,we strengthen a dpp-based multi-document summarization system with improved similarity measure inspired by capsule networks for determining sentence redundancy.
P19-1099,2019,9 Conclusion,"our methods outperformed the conventional methods in terms of both rouge, while maintaining the ability to generate a summary within a length constraint."
P19-1099,2019,9 Conclusion,the post-edit evaluation with automatically generated summaries showed that in-length summaries contributed to approximately 30% to 40% improved post-editing time compared with use of the baselines.
P19-1099,2019,9 Conclusion,we also demonstrated the importance of the generation of summaries in a length constraint for real use.
P19-1099,2019,9 Conclusion,we proposed a global optimization method for neural text summarization under a length constraint.
P19-1100,2019,5 Conclusion,"in this paper, we seek to better understand how neural extractive summarization systems could benefit from different types of model architectures, transferable knowledge, and learning schemas."
P19-1100,2019,5 Conclusion,our detailed observations can provide more hints for the follow-up researchers to design more powerful learning frameworks.
P19-1101,2019,4 Conclusion and Future Work,a summarizer intends to extract or generate a summary maximizing θi .
P19-1101,2019,4 Conclusion and Future Work,"by aggregating over different people but in one domain, one can uncover a domain-specific k. similarly, by aggregating over many topics for one person, one would find a personalized k. these consistute promising research directions for future works."
P19-1101,2019,4 Conclusion and Future Work,"by aggregating over many users and many topics one can find a generic k: what, on average, people consider as known when summarizing a document."
P19-1101,2019,4 Conclusion and Future Work,can we automatically learn good approximations?
P19-1101,2019,4 Conclusion and Future Work,"characters, character n-grams, morphemes, words, n-grams, phrases, and sentences do not actually qualify as semantic units."
P19-1101,2019,4 Conclusion and Future Work,"conceptually, it is straightforward to build a system out of θi once a semantic units representation and a k have been chosen."
P19-1101,2019,4 Conclusion and Future Work,"even though previous works who relied on information theoretic motivation (lin et al., 2006; haghighi and vanderwende, 2009; louis and nenkova, 2013; peyrard and ecklekohler, 2016) used some of them as support for probability distributions, they are neither atomic nor independent."
P19-1101,2019,4 Conclusion and Future Work,"for the background knowledge k, a promising direction would be to use the framework to actually learn it from data."
P19-1101,2019,4 Conclusion and Future Work,"however, they might serve as convenient approximations."
P19-1101,2019,4 Conclusion and Future Work,importance arises as the notion unifying these concepts.
P19-1101,2019,4 Conclusion and Future Work,"in a framework rooted in information theory, we formalized several summary-related quantities like: redundancy, relevance and informativeness."
P19-1101,2019,4 Conclusion and Future Work,"in particular, one can apply supervised techniques to automatically search for k, α and β: finding the values of these parameters such that θi has the best correlation with human judgments."
P19-1101,2019,4 Conclusion and Future Work,"in this work, we argued for the development of theoretical models of importance and proposed one such framework."
P19-1101,2019,4 Conclusion and Future Work,it is mainly because they are surface forms whereas semantic units are abstract and operate at the semantic level.
P19-1101,2019,4 Conclusion and Future Work,"more generally, importance is the measure that guides which choices to make when information must be discarded."
P19-1101,2019,4 Conclusion and Future Work,"n-grams are known to be useful, but other granularities have rarely been considered together with information-theoretic tools."
P19-1101,2019,4 Conclusion and Future Work,our experiments already hint that strong summarizers can be developed from this framework.
P19-1101,2019,4 Conclusion and Future Work,the introduced quantities generalize the intuitions that have previously been used in summarization research.
P19-1101,2019,4 Conclusion and Future Work,"then, interesting research questions arise like which granularity offers a good approximation of semantic units?"
P19-1101,2019,4 Conclusion and Future Work,they are design choices which can be explored empirically by subsequent works.
P19-1101,2019,4 Conclusion and Future Work,"this fits within the general optimization framework for summarization (mcdonald, 2007; peyrard and eckle-kohler, 2017b; peyrard and gurevych, 2018) the background knowledge and the choice of semantic units are free parameters of the theory."
P19-1101,2019,4 Conclusion and Future Work,"thus, we investigated a theoretical formulation of the notion of importance."
P19-1102,2019,8 Conclusion,"additionally, we introduce an end-to-end model which incorporates mmr into a pointer-generator network, which performs competitively compared to previous multi-document summarization models."
P19-1102,2019,8 Conclusion,in the future we plan to explore interactions among documents beyond concatenation and experiment with summarizing longer input documents.
P19-1102,2019,8 Conclusion,"in this paper we introduce multi-news, the first large-scale multi-document news summarization dataset."
P19-1102,2019,8 Conclusion,we also benchmark methods on our dataset.
P19-1102,2019,8 Conclusion,we hope that this dataset will promote work in multi-document summarization similar to the progress seen in the single-document case.
P19-1103,2019,6 Conclusion,comparison with existing baselines shows the advantage of our method.
P19-1103,2019,6 Conclusion,"experiments show that pwws can greatly reduce the text classification accuracy with a low word substitution rate, and such perturbation is hard for human to perceive."
P19-1103,2019,6 Conclusion,"in the future, we would like to evaluate the attacking effectiveness and efficiency of our methods on more datasets and models, and do elaborate human evaluation on the similarity between clean texts and the corresponding adversarial examples."
P19-1103,2019,6 Conclusion,our work demonstrates the existence of adversarial examples in discrete input spaces and shows the vulnerability of nlp models using neural networks.
P19-1103,2019,6 Conclusion,"pwws also exhibits a good transferability, and by performing adversarial training we can improve the robustness of the models at test time."
P19-1103,2019,6 Conclusion,pwws introduces a new word substitution order determined by the word saliency and weighted by the classification probability.
P19-1103,2019,6 Conclusion,we propose an effective method called probability weighted word saliency (pwws) for generating adversarial examples on text classification tasks.
P19-1104,2019,6 Conclusion,"our study opens up interesting avenues for future research: obfuscation by addition instead of by reduction, development of more powerful, targeted paraphrasing operators, and, theoretical analysis of the search space properties."
P19-1104,2019,6 Conclusion,"we consider heuristic search-based obfuscation a key enabling technology that, combined with tailored deep generative models for paraphrasing, will yield better and stronger obfuscations."
P19-1104,2019,6 Conclusion,"we identified and addressed the following challenges: measuring style similarity in a manner that is agnostic to state-of-the-art verifiers, identifying those parts of a text that have the highest impact on style, and devising and analyzing a search heuristic amenable for informed search."
P19-1104,2019,6 Conclusion,we introduced a promising new paradigm for authorship obfuscation and implemented a first fully functional prototype.
P19-1105,2019,5 Conclusion,"future work will include: (i) incorporating lexical semantics such as named entities for further improvement, (ii) comparing our model to other deep contextualized word representation such as elmo and bert, and (iii) applying the method to other domains for quantitative evaluation."
P19-1105,2019,5 Conclusion,"moreover, we found that our model works well, especially for the categorization of documents with multi-label."
P19-1105,2019,5 Conclusion,the comparative results with the baselines showed that our model is competitive as the improvement was 1.49% ～ 9.49% by micro-f1 and 1.79% ～ 15.23% by macro-f1.
P19-1105,2019,5 Conclusion,we empirically examined that predominant sense identification helps to improve the overall performance of text categorization in the framework on multi-task learning.
P19-1105,2019,5 Conclusion,we have presented an approach to text categorization by leveraging a predominant sense of a word depending on the domain.
P19-1106,2019,7 Conclusion,"here in this work, we show that the reviewer sentiment information embedded within peer review texts could be leveraged to predict the peer review outcomes."
P19-1106,2019,7 Conclusion,"however, considering the sensitivity of the topic, we would like to further dive deep into exploring the subtle nuances that leads into the grading of peer review aspects."
P19-1106,2019,7 Conclusion,"however, we are in consensus that scholarly language processing is not straightforward."
P19-1106,2019,7 Conclusion,"our deep neural architecture makes use of three information channels: the paper full-text, corresponding peer review texts and the sentiment within the reviews to address the complex task of decision making in peer review."
P19-1106,2019,7 Conclusion,we aim to include review reliability prediction in the pipeline of our future work.
P19-1106,2019,7 Conclusion,we found that review reliability prediction should prelude these tasks since not all reviews are of equal quality or are significant to the final decision making.
P19-1106,2019,7 Conclusion,we intend to work upon those and also explore more sophisticated techniques for sentiment polarity encoding.
P19-1106,2019,7 Conclusion,"we need stronger, pervasive models to capture the high-level interplay of the paper and peer reviews to decide the fate of a manuscript."
P19-1106,2019,7 Conclusion,"with further exploration, we aim to mould the ongoing research to an efficient ai-enabled system that would assist the journal editors or conference chairs in making informed decisions."
P19-1107,2019,7 Conclusion,"by incorporating external embeddings with gating mechanism, our model can achieve further improvement with better conversational-context representation."
P19-1107,2019,7 Conclusion,"moreover, our gated network can incorporate effectively with text-based external resources, word or sentence embeddings (i.e., fasttext, bert) within an end-to-end framework and so that the whole system can be optimized towards our final objectives, speech recognition accuracy."
P19-1107,2019,7 Conclusion,our model was shown to outperform standard end-to-end speech recognition models trained on isolated sentences.
P19-1107,2019,7 Conclusion,"this work is easy to scale and can potentially be applied to any speech related task that can benefit from longer context information, such as spoken dialog system, sentimental analysis."
P19-1107,2019,7 Conclusion,"unlike prior work, our model is trained on conversational datasets to predict a word, conditioning on multiple preceding conversational-context representations, and consequently improves recognition accuracy of a long conversation."
P19-1107,2019,7 Conclusion,"we evaluated the models on the switchboard conversational speech corpus and show that our proposed model using gated conversational-context embedding show 15%, 5% relative improvement in wer compared to a baseline model for switchboard and callhome subsets respectively."
P19-1107,2019,7 Conclusion,we have introduced a novel method for conversational-context aware end-to-end speech recognition based on a gated network that incorporates word/sentence/speech embeddings.
P19-1108,2019,8 Conclusions,"also, a similar application to improve disaster mention detection could be useful (for figurative sentences such as ‘my heart is on fire’)."
P19-1108,2019,8 Conclusions,"other ways of combining the two modules, more sophisticated classifiers for both phm detection and figurative usage detection, are possible directions of future work."
P19-1108,2019,8 Conclusions,"our observations demonstrate the promise of using figurative usage detection for phm detection, while highlighting that a simple pipeline-based approach may not work."
P19-1108,2019,8 Conclusions,"the output of this method was combined with classifiers for detecting phms in two ways: (1) a simple pipeline-based approach, where the performance of phm detection degraded; and, (2) a feature augmentation-based approach where the performance of phm detection improved."
P19-1108,2019,8 Conclusions,we employed a state-of-the-art method in figurative usage detection to improve the detection of personal health mentions (phms) in tweets.
P19-1109,2019,5 Conclusions,"finally, we plan to investigate alternative methods to modelling phrase and multi-word expression complexity."
P19-1109,2019,5 Conclusions,"in this paper, we address the limitations of the existing cwi systems."
P19-1109,2019,5 Conclusions,"it is able to take context into account and classify both words and phrases in a unified framework, without the need for expensive feature engineering."
P19-1109,2019,5 Conclusions,our future research will focus on the relative nature of complexity judgements and will use the seq model to predict complexity on a scale.
P19-1109,2019,5 Conclusions,our seq model relies on sequence labelling and outperforms state-of-the-art systems with a one-model-fits-all approach.
P19-1109,2019,5 Conclusions,we will also investigate whether the seq model may benefit from sources of information other than word embeddings and character-level morphology.
P19-1110,2019,4 Conclusion,extensive experiments on a real-world dataset validate the effectiveness of our approach.
P19-1110,2019,4 Conclusion,"in addition, we propose a user encoder to learn representations of users from their browsed news, and use attention network to select important news."
P19-1110,2019,4 Conclusion,"in addition, we propose to train a topic-aware news encoder via jointly training it with an auxiliary topic classification task to extract the topic information in news."
P19-1110,2019,4 Conclusion,"in our approach we propose a new encoder to learn news representations from news titles, and use attention network to select important words."
P19-1110,2019,4 Conclusion,"in this paper, we propose a neural news recommendation approach with topic-aware news representations."
P19-1111,2019,4 Conclusion,"from a pedagogical perspective, it will be beneficial for learners of the language to look into the prose of the verses for an easier comprehension of the concepts discussed in the verse."
P19-1111,2019,4 Conclusion,"in this work, we attempt to address the poetry to prose conversion problem by formalising it as an lm based word linearisation task."
P19-1111,2019,4 Conclusion,"though tremendous progress has been made in digitising texts in sanskrit, they still remain inaccessible largely due to lack of specific tools that can address linguistic peculiarities exhibited by the language (krishna et al., 2017)."
P19-1111,2019,4 Conclusion,we find that kavya guru ˉ outperforms the state of the art models in word linearisation for the task.
P19-1112,2019,7 Conclusion,"as future work, we plan to investigate emphasis selection on a larger and more diverse dataset."
P19-1112,2019,7 Conclusion,its goal is to develop models that suggest which part of the text to emphasize.
P19-1112,2019,7 Conclusion,"to tackle the subjective nature of the task, we propose a sequence labeling architecture that optimizes the model to learn label distributions by capturing the inter-subjectivity within the audience."
P19-1112,2019,7 Conclusion,"we also plan to investigate the role of word sentiment and emotion intensity as well as more advanced language models such as bert (devlin et al., 2018) in modeling emphasis."
P19-1112,2019,7 Conclusion,"we introduced a new task, emphasis selection in short text instances."
P19-1112,2019,7 Conclusion,we provide comparisons to models trained with other objective functions where the ground truth probabilities are mapped to binary labels and show that ldl is more effective in selecting the emphasis.
P19-1113,2019,5 Conclusion,the experiments on two datasets show that our proposed model outperforms the state-ofthe-art rumor detection approaches.
P19-1113,2019,5 Conclusion,"this model incorporates the user credibility information into the rumor detection layer, and uses attention mechanism in the rumor detection process."
P19-1113,2019,5 Conclusion,we proposed a multi-task learning approach for rumor detection and stance classification tasks.
P19-1114,2019,6 Conclusions and Future Work,"after collecting more labeled data, and tuning our model using anomaly detection techniques like isolation forests (liu et al., 2008), we hope to expand this study to the stage where we are able to use unbalanced data sets."
P19-1114,2019,6 Conclusions and Future Work,"finally, in this study, we tested our model on a balanced data set."
P19-1114,2019,6 Conclusions and Future Work,"for more evaluation, we examined the recall rate of models in 85% and 90% of precision."
P19-1114,2019,6 Conclusions and Future Work,"given that, in our future work, we will be investigating those false positive cases with our collaborators to assess what the correct label for these ads should be."
P19-1114,2019,6 Conclusions and Future Work,"however, in the real world, the number of trafficking ads is always far lower than the number of non-trafficking ones."
P19-1114,2019,6 Conclusions and Future Work,"in this paper, we introduced different models based on different text featurizations where the main goal was to engineer features that allowed for understanding the context of sexual ads and remove the restriction of using keywords."
P19-1114,2019,6 Conclusions and Future Work,it also should be noted that our non-trafficking examples may still contain some trafficking ads.
P19-1114,2019,6 Conclusions and Future Work,"lda+awv+bert, outperformed others as it indicated that having comprehensive features may be conveying more information about the advertisements."
P19-1114,2019,6 Conclusions and Future Work,"moreover, since the proposed full feature set involves hundreds of features we plan to increase our sample size to have a better estimation of the performance of our final predictor."
P19-1114,2019,6 Conclusions and Future Work,"the full feature set, i.e."
P19-1114,2019,6 Conclusions and Future Work,"thus, we can significantly increase the ppv of our model while maintaining a high recall rate."
P19-1114,2019,6 Conclusions and Future Work,"we also envision that by including other underlying components from these advertisements, we can assist law enforcement officers with an automated framework to sift millions of sexual advertisements and spend time on especially suspicious activities."
P19-1114,2019,6 Conclusions and Future Work,we have proposed a composite model and compared its performance with other simpler models.
P19-1114,2019,6 Conclusions and Future Work,we thus note with caution that the false positives in our model may not be truly false.
P19-1115,2019,7 Conclusion,experiments in a speech translation task show that our method outperforms previous approaches and is much faster than rnn-based alternatives in both training and inference settings.
P19-1115,2019,7 Conclusion,promising future work includes extension to tree-structured inputs and application to other tasks.
P19-1115,2019,7 Conclusion,"this work extended existing sequential selfattentional models to lattice inputs, which have been useful for various purposes in the past."
P19-1115,2019,7 Conclusion,we achieve this by introducing probabilistic reachability masks and lattice positional encodings.
P19-1116,2019,8 Conclusions,"our human study on an english–russian dataset identifies deixis, ellipsis and lexical cohesion as three main sources of inconsistency."
P19-1116,2019,8 Conclusions,"to build context-aware machine translation systems, such targeted test sets should prove useful, for validation, early stopping and for model selection."
P19-1116,2019,8 Conclusions,we analyze which phenomena cause otherwise good context-agnostic translations to be inconsistent when placed in the context of each other.
P19-1116,2019,8 Conclusions,we consider a novel and realistic set-up where a much larger amount of sentence-level data is available compared to that aligned at the document level and introduce a model suitable for this scenario.
P19-1116,2019,8 Conclusions,we create test sets focusing specifically on the identified phenomena.
P19-1116,2019,8 Conclusions,"we show that our model effectively handles contextual phenomena without sacrificing general quality as measured with bleu despite using only a small amount of document-level data, while a naive approach to combining sentence-level and document-level data leads to a drop in performance."
P19-1116,2019,8 Conclusions,we show that the proposed test sets allow us to distinguish models (even though identical in bleu) in terms of their consistency.
P19-1117,2019,7 Conclusion,"based on the representor architecture, we then propose three language-specific modules dealing with embedding, attention and language discrimination respectively, in order to enhance the multilanguage translation model with the ability of distinguishing among different languages."
P19-1117,2019,7 Conclusion,"in this paper, we have proposed a compact and language-sensitive method for multilingual translation."
P19-1117,2019,7 Conclusion,"moreover, our method is proved to be especially helpful in the low-resource and zero-shot translation scenarios."
P19-1117,2019,7 Conclusion,the empirical experiments demonstrate that our proposed methods can outperform strong standard multilingual translation systems on one-to-many and many-to-many translation tasks.
P19-1117,2019,7 Conclusion,we first introduce a representor for replacing both encoder and decoder so as to fully explore the commonality among languages.
P19-1118,2019,6 Conclusions,"in contrast to previous unsupervised work, we also extracted sentences from real world comparable corpora and showed better translation performance when using these sentence pairs, opening up new possibilities for using small amounts of parallel data in purely unsupervised mt approaches."
P19-1118,2019,6 Conclusions,"in this work, we proposed a fully unsupervised system for parallel sentence extraction."
P19-1118,2019,6 Conclusions,most previous work tackled this problem using supervised techniques which made their applicability problematic.
P19-1118,2019,6 Conclusions,our analysis showed that both handling of named entities and the fluency of sentences improved.
P19-1118,2019,6 Conclusions,parallel sentence extraction is important for providing an additional bilingual signal for many downstream tasks in low resource setups.
P19-1118,2019,6 Conclusions,to overcome this problem we introduced the detection of continuous parallel segments based on word alignments.
P19-1118,2019,6 Conclusions,we filter candidates having too short segments and weight the similarity score of the rest based on segment lengths.
P19-1118,2019,6 Conclusions,we publicly release our system to support mt communities especially for low-resource setups.
P19-1118,2019,6 Conclusions,"we showed that a previous unsupervised system, which only relies on word similarity in source and target language sentences, often mines false positives because not all sentences having similar words are parallel."
P19-1118,2019,6 Conclusions,we showed that using our method better performance could be achieved on the bucc 2017 parallel sentence extraction task compared to previous work.
P19-1119,2019,8 Conclusion,"based on this finding, we proposed two joint learning methods to train unmt with ubwe agreement."
P19-1119,2019,8 Conclusion,empirical results on several language pairs show that the proposed methods can mitigate the decrease in ubwe accuracy and significantly improve the performance of unmt.
P19-1119,2019,8 Conclusion,"in previous methods, the pre-trained ubwe is only used to initialize the word embedding of unmt."
P19-1119,2019,8 Conclusion,"in this study, we found that the performance of unmt is significantly affected by the quality of ubwe, not only in the initialization stage, but also during unmt training."
P19-1119,2019,8 Conclusion,ubwe is a fundamental component of unmt.
P19-1120,2019,7 Conclusion,"5 as for future work, we will test our methods in the nmt transfer where the target language is switched."
P19-1120,2019,7 Conclusion,"all three methods significantly improve over plain transfer learning with a total gain of up to +5.1% bleu in our experiments, consistently outperforming multilingual joint training."
P19-1120,2019,7 Conclusion,"firstly, we transform monolingual embeddings of the new language into the embedding space of the parent nmt model."
P19-1120,2019,7 Conclusion,"for the first time, we show a practical usage of artificial noises to regularize an nmt model."
P19-1120,2019,7 Conclusion,"in this paper, we address the problem of transferring an nmt model to unseen, unrelated language pairs."
P19-1120,2019,7 Conclusion,"lastly, we reuse parallel data of the parent language pair in the child training phase to avoid an abrupt change of the training data distribution."
P19-1120,2019,7 Conclusion,our implementation of the proposed methods is available online.
P19-1120,2019,7 Conclusion,"our methods do not require retraining of a shared vocabulary or the parent model, enabling an incremental transfer of the same parent model to various (possibly unrelated) languages."
P19-1120,2019,7 Conclusion,"secondly, we randomize the word orders in the parent model training to avoid overfitting to the parent source language."
P19-1120,2019,7 Conclusion,this accomplishes an effective transition of vocabularies on the embedding level.
P19-1120,2019,7 Conclusion,this makes it easier for the encoder to adapt to the new language syntax.
P19-1120,2019,7 Conclusion,"we also plan to compare different algorithms for learning the cross-lingual mapping (artetxe et al., 2018a; xu et al., 2018; joulin et al., 2018) to optimize the transfer performance."
P19-1120,2019,7 Conclusion,we propose three novel techniques to improve the transfer without vocabulary sharing between parent and child source languages.
P19-1121,2019,7 Conclusion,"experiments on the europarl, iwslt and multiun corpora show that our proposed methods significantly improve the vanilla zero-shot nmt and consistently outperform the pivot-based methods."
P19-1121,2019,7 Conclusion,"in this paper, we analyzed the issue of zero-shot translation quantitatively and successfully close the gap of the performance of between zero-shot translation and pivot-based zero-resource translation."
P19-1121,2019,7 Conclusion,we proposed two simple and effective strategies for zero-shot translation.
P19-1122,2019,7 Conclusions & Future Work,controlled experiments show that synst outperforms competing non- and semi-autoregressive approaches in terms of both bleu and wall-clock speedup on en-de and en-fr language pairs.
P19-1122,2019,7 Conclusions & Future Work,"finally, we hope that future work in this area will follow our lead in using carefully-controlled experiments to enable meaningful comparisons."
P19-1122,2019,7 Conclusions & Future Work,"we propose synst, a variant of the transformer architecture that achieves decoding speedups by autoregressively generating a constituency chunk sequence before non-autoregressively producing all tokens in the target sentence."
P19-1122,2019,7 Conclusions & Future Work,"while our method is currently restricted to languages that have reliable constituency parsers, an exciting future direction is to explore unsupervised tree induction methods for low-resource target languages (drozdov et al., 2019)."
P19-1123,2019,6 Conclusion,"in future, we would like to extend the method to handle more than two curricula objectives."
P19-1123,2019,6 Conclusion,"we further refine the co-curriculum with an em-style optimization procedure and show its effectiveness, in particular on small-capacity models."
P19-1123,2019,6 Conclusion,"we present a co-curricular learning method to make domain-data selection work better on noisy data, by dynamically composing it with clean-data selection."
P19-1123,2019,6 Conclusion,we show that the method improves over either constituent selection and their static combination.
P19-1124,2019,6 Conclusions and Future Work,"in particular, it quantitatively analyzes that alignment errors which are likely to lead to translation errors at word level measured by different metrics."
P19-1124,2019,6 Conclusions and Future Work,"in the future, we believe more work on improving cfs alignment is potential to improve translation quality, and we will investigate on using source context and target history context in a more robust manner for better predicting cfs and cft words."
P19-1124,2019,6 Conclusions and Future Work,it firstly reveals that attention may not capture word alignment for an nmt model with multiple attentional layers.
P19-1124,2019,6 Conclusions and Future Work,then it suggests prediction difference is better for understanding nmt and visualizes nmt from word alignment induced by prediction difference.
P19-1124,2019,6 Conclusions and Future Work,"therefore, it proposes two methods (explicit model and prediction difference) to acquire word alignment which are agnostic to specific nmt models."
P19-1124,2019,6 Conclusions and Future Work,this paper systematically studies the word alignment from nmt.
P19-1125,2019,7 Conclusion,another direction is to apply the proposed imitation learning framework to similar scenarios such as simultaneous interpretation.
P19-1125,2019,7 Conclusion,"as a future work, we can try to improve the performance of the nmt by introducing more powerful demonstrator with different structure (e.g.right to left)."
P19-1125,2019,7 Conclusion,"as a result, imitate-nat leads to remarkable improvements and largely closes the performance gap between nat and at on several benchmark datasets."
P19-1125,2019,7 Conclusion,"specifically, we propose to employ a knowledgeable at demonstrator to supervise every decoding state of nat across different time steps and layers."
P19-1125,2019,7 Conclusion,we propose an imitation learning framework for non-autoregressive neural machine translation to bridge the performance gap between nat and at.
P19-1126,2019,6 Conclusion,"along the way, we contributed latency-augmented training and a differentiable latency metric."
P19-1126,2019,6 Conclusion,it is particularly useful for extending the length of the region on the latency curve where we do not yet incur a major reduction in bleu.
P19-1126,2019,6 Conclusion,this allowed us to build a simultaneous nmt system that is trained jointly with its adaptive schedule.
P19-1126,2019,6 Conclusion,"we have presented monotonic infinite lookback (milk) attention, an attention mechanism that uses a hard, monotonic head to manage the reading of the source, and a soft traditional head to attend over whatever has been read."
P19-1126,2019,6 Conclusion,we have shown milk to have favorable quality-latency trade-offs compared to both wait-k and to earlier monotonic attention mechanisms.
P19-1128,2019,6 Conclusion and Future Work,"in this work, we demonstrate its effectiveness in predicting the relationship between entities in natural language and bag-level and show that by considering more hops in reasoning the performance of relation extraction could be significantly improved."
P19-1128,2019,6 Conclusion and Future Work,"our model can also be considered as a more generic framework for graph generation problem with unstructured input other than text, e.g.image, video, audio."
P19-1128,2019,6 Conclusion and Future Work,"our proposed model, gp-gnn, solves the relational message-passing task by encoding natural language as parameters and performing propagation from layer to layer."
P19-1128,2019,6 Conclusion and Future Work,we addressed the problem of utilizing gnns to perform relational reasoning with natural languages.
P19-1129,2019,7 Conclusion,"in this paper, we propose a multi-turn question answering paradigm for the task of entity-relation extraction."
P19-1129,2019,7 Conclusion,we achieve new state-of-the-art results on 3 benchmark datasets.
P19-1129,2019,7 Conclusion,we also construct a new entity-relation extraction dataset that requires hierarchical relation reasoning and the proposed model achieves the best performance.
P19-1130,2019,5 Conclusion,"experiment results on ace 2005 chinese and english corpus show that our proposed approach can successfully address the data imbalance problem and significantly improve the performance, outperforming the state-of-the-art models in terms of f1-score."
P19-1130,2019,5 Conclusion,"in this paper, we focus on the relation extraction task with an imbalanced corpus."
P19-1130,2019,5 Conclusion,"moreover, we introduce the embeddings of characterwise/word-wise bio tag from the named entity recognition task to enrich the input representation."
P19-1130,2019,5 Conclusion,"particularly, we find bio tag embeddings very effective, which we believe could be used as a general part of character/word representation."
P19-1130,2019,5 Conclusion,"to mitigate the problem of having too many negative instances, we propose a multi-task architecture which jointly trains a model to perform the relation identification task with cross-entropy loss and the relation classification task with ranking loss."
P19-1131,2019,6 Conclusion,"compared with existing joint methods, it provides a new way to capture the interactions on multiple entity types and relation types explicitly in a sentence."
P19-1131,2019,6 Conclusion,experiments on ace05 dataset show the effectiveness of the proposed method.
P19-1131,2019,6 Conclusion,we propose a novel and concise joint model based on gcn to perform joint type inference for entity relation extraction task.
P19-1132,2019,5 Conclusion,"in summary, we propose a first-of-its-kind solution that can simultaneously extract multiple relations with one-pass encoding of an input paragraph for mre tasks."
P19-1132,2019,5 Conclusion,"in the future work, we will explore the usage of this method with other applications."
P19-1132,2019,5 Conclusion,"our idea of encoding a passage regarding multiple entities has potentially broader applications beyond relation extraction, e.g., entity-centric passage encoding in question answering (song et al., 2018a)."
P19-1132,2019,5 Conclusion,"with the proposed structured prediction and entity-aware self-attention layers on top of bert, we achieve a new state-of-the-art results with high efficiency on the ace 2005 benchmark."
P19-1133,2019,5 Conclusion,"future work will investigate more complex and recent neural network models such as devlin et al.(2018), as well as alternative losses."
P19-1133,2019,5 Conclusion,"in particular, we were able to successfully train a deep neural network classifier that only performed well in a supervised setting so far."
P19-1133,2019,5 Conclusion,"in particular, while forcing an uniform distribution with the distance loss ld might be meaningful with a low number of predicted clusters, it might not generalize to larger number of relations."
P19-1133,2019,5 Conclusion,"in this paper, we show that discriminative relation extraction models can be trained efficiently on unlabeled datasets."
P19-1133,2019,5 Conclusion,"preliminary experiments seem to indicate that this can be addressed by replacing the uniform distribution in equation 5 with the empirical distribution of the relations in the validation set, or any other appropriate law if no validation set is available."
P19-1133,2019,5 Conclusion,unsupervised relation extraction models tends to produce impure clusters by enforcing a uniformity constrain at the level of a single sample.
P19-1133,2019,5 Conclusion,we demonstrated the effectiveness of our reldist losses on three datasets and showcased its effect on cluster purity.
P19-1133,2019,5 Conclusion,we proposed two losses (named reldist) to effectively train expressive relation extraction models by enforcing the distribution over relations to be uniform – note that other target distributions could be used.
P19-1134,2019,7 Conclusion,"because of its generic architecture, distre allows for integration of additional contextual information, e.g.background knowledge about entities and relations, which could also prove useful to further improve performance."
P19-1134,2019,7 Conclusion,"in contrast to reside, which uses explicitly provided side information and linguistic features, our approach only utilizes features implicitly captured in pre-trained language representations."
P19-1134,2019,7 Conclusion,"in future work, we want to further investigate the extent of syntactic structure captured in deep language language representations."
P19-1134,2019,7 Conclusion,"similarly, our approach predicts a larger set of distinct relation types with high confidence among the top predictions."
P19-1134,2019,7 Conclusion,"this allows for an increased domain and language independence, and an additional error reduction because pre-processing can be omitted."
P19-1134,2019,7 Conclusion,"we proposed distre, a transformer which we extended with an attentive selection mechanism for the multi-instance learning scenario, common in distantly supervised relation extraction."
P19-1134,2019,7 Conclusion,"while distre achieves a lower precision for the 300 top ranked predictions, we observe a state-of-the-art auc and an overall more balanced performance, especially for higher recall values."
P19-1135,2019,6 Conclusion,"a bootstrap learning procedure is built to iteratively improve the model, training data and trustable pattern set."
P19-1135,2019,6 Conclusion,"in addition, we publish a better manually labeled test set for sentence-level evaluation."
P19-1135,2019,6 Conclusion,"in the future, we hope to improve our work by the utilization of better model-based pattern extractor, and resorting to latent variable model (kim et al., 2018) for jointly modeling instance selector."
P19-1135,2019,6 Conclusion,"thus, we design attention regulation to help the model learn the locating of relation patterns."
P19-1135,2019,6 Conclusion,we find relation pattern is an important feature but is rarely captured by the previous model trained on noisy data.
P19-1135,2019,6 Conclusion,"we propose arnor, an attention regularizationbased noise reduction framework for distant supervision relation classification."
P19-1135,2019,6 Conclusion,"what is more, we also hope to verify the effectiveness of our method on more tasks, including open information extraction and event extraction, and also overlapping relation extraction models (dai et al., 2019)."
P19-1135,2019,6 Conclusion,"with a more interpretable model, we then conduct noise reduction by evaluating how well the model explains the relation of an instance."
P19-1135,2019,6 Conclusion,"with a very simple pattern extractor, we outperform several strong rl-based baselines, achieving significant improvements on both relation classification and noise reduction."
P19-1136,2019,6 Conclusion,"furthermore, we introduce a novel relation-weighted gcn that considers interactions between named entities and relations."
P19-1136,2019,6 Conclusion,implicit features among all word pairs of the text are also considered in our approach.
P19-1136,2019,6 Conclusion,"in this paper, we present graphrel, an end-toend relation extraction model which jointly learns named entities and relations based on graph convolutional networks (gcn)."
P19-1136,2019,6 Conclusion,the results show that our method outperforms previous work by 3.2% and 5.8% and achieves a new state-of-the-art for relation extraction.
P19-1136,2019,6 Conclusion,we combine rnn and gcn to extract not only sequential features but also regional dependency features for each word.
P19-1136,2019,6 Conclusion,we evaluate the proposed method on the nyt and webnlg datasets.
P19-1136,2019,6 Conclusion,"we predict relations for each word pair, solving the problem of entity overlapping."
P19-1137,2019,5 Conclusion and Future Work,"coupled with the wlf model, diag-nre can produce denoised labels to retrain a better nre model."
P19-1137,2019,5 Conclusion and Future Work,"diag-nre not only eases the hard patternwriting work of human experts by generating patterns automatically, but also enables the quick generalization to new relation types by only requiring a small number of human annotations."
P19-1137,2019,5 Conclusion and Future Work,extensive experiments with comprehensive analyses demonstrate that diag-nre can contribute both significant and interpretable improvements.
P19-1137,2019,5 Conclusion and Future Work,"for the future work, we plan to extend diagnre to other ds-based applications, such as question answering (lin et al., 2018), event extraction (chen et al., 2017), etc."
P19-1137,2019,5 Conclusion and Future Work,"in this paper, we propose a neural pattern diagnosis framework, diag-nre, to diagnose and improve nre models trained on ds-generated data."
P19-1138,2019,5 Conclusions,experimental results show that mgner is able to achieve state-of-the-art results on both nested ner task and traditional non-overlapping ner task.
P19-1138,2019,5 Conclusions,"in this work, we propose a novel neural framework named mgner for multi-grained named entity recognition where multiple entities or entity mentions in a sentence could be non-overlapping or totally nested."
P19-1138,2019,5 Conclusions,mgner is framework with high modularity and each component in mgner can adopt a wide range of neural networks.
P19-1139,2019,5 Conclusion,"accordingly, we propose the knowledgeable aggregator and the pre-training task dea for better fusion of heterogeneous information from both text and kgs."
P19-1139,2019,5 Conclusion,"in this paper, we propose ernie to incorporate knowledge information into language representation models."
P19-1139,2019,5 Conclusion,the experimental results demonstrate that ernie has better abilities of both denoising distantly supervised data and fine-tuning on limited data than bert.
P19-1139,2019,5 Conclusion,"there are three important directions remain for future research: (1) inject knowledge into feature-based pre-training models such as elmo (peters et al., 2018); (2) introduce diverse structured knowledge into language representation models such as conceptnet (speer and havasi, 2012) which is different from the world knowledge database wikidata; (3) annotate more real-world corpora heuristically for building larger pre-training data."
P19-1139,2019,5 Conclusion,these directions may lead to more general and effective language understanding.
P19-1140,2019,7 Conclusions,extensive experiments on five publicly available datasets and further analysis demonstrate the effectiveness of our method.
P19-1140,2019,7 Conclusions,"in future, we are interested in introducing text information of entities for alignment by considering word ambiguity (cao et al., 2017b); and meanwhile, through cross-kg entity proximity (cao et al., 2015)."
P19-1140,2019,7 Conclusions,"in this paper, we propose a novel multi-channel graph neural network model, mugnn, which learns alignment-oriented kg embeddings for entity alignment."
P19-1140,2019,7 Conclusions,it is able to alleviate the negative impacts caused by the structural heterogeneity and limited seed alignments.
P19-1140,2019,7 Conclusions,"through two channels, mugnn not only explicitly completes the kgs, but also pruning exclusive entities by using different relation weighting schemes: kg selfattention and cross-kg attention, showing robust graph encoding capability."
P19-1141,2019,4 Conclusion and Future Work,"although we specifically investigated the ner task for chinese in this work, we believe the proposed model can be extended and applied to other languages, for which we leave as future work."
P19-1141,2019,4 Conclusion and Future Work,"based on the proposed multi-digraph structure, we show that our model is better at resolving entity-matching conflicts."
P19-1141,2019,4 Conclusion and Future Work,the ablation study confirms that a suitable combination of gazetteers is essential and our model is able to make good use of the gazetteer information.
P19-1141,2019,4 Conclusion and Future Work,"through extensive experiments, we have demonstrated that our approach outperforms the state-of-the-art models and previous methods for incorporating gazetteers into a chinese ner system."
P19-1141,2019,4 Conclusion and Future Work,we present a novel neural multi-digraph model for performing chinese named entity recognition with gazetteers.
P19-1142,2019,8 Conclusion,"also, it will be worthwhile to ascertain the efficacy of pdr for language models using transformers and in combination with frage embeddings."
P19-1142,2019,8 Conclusion,future work includes exploring the application of pdr to other seq2seq models that have a similar input-output symmetry.
P19-1142,2019,8 Conclusion,we empirically show reductions in perplexity on several benchmark datasets as compared to strong highly regularized baseline models.
P19-1142,2019,8 Conclusion,we propose a new past decode regularization (pdr) method for language modeling that exploits the input-output symmetry in each step to decode the last token in the context from the predicted next token distribution.
P19-1143,2019,4 Conclusion,"a technical challenge for learning these models is that a given string can have multiple segmentations, and one needs to marginalize over the set of segmentations."
P19-1143,2019,4 Conclusion,"in this paper, we study the problem of hybrid language modeling, where models can predict ngrams, instead of unigrams only."
P19-1143,2019,4 Conclusion,"using this approach, we improve the state of the art on the mwc and wikitext2 datasets, used to evaluate hybrid language models."
P19-1143,2019,4 Conclusion,"we introduce a simple technique to do so, allowing to apply dynamic programming for learning and inference."
P19-1144,2019,7 Conclusion,"experiments showed that our model successfully learned approximate phrase-level knowledge, including segmentation and headwords, without any annotation."
P19-1144,2019,7 Conclusion,"in future work, we aim to capture better structural information and possible connections to unsupervised grammar induction."
P19-1144,2019,7 Conclusion,"in this work, we improved state-of-the-art language models by aligning context and induced phrases."
P19-1144,2019,7 Conclusion,the model generates phrase embeddings with headword attentions.
P19-1144,2019,7 Conclusion,we defined syntactic heights and phrase segmentation rules.
P19-1144,2019,7 Conclusion,we improved the awd-lstm and transformer-xl language models on different data sets and achieved state-of-the-art performance on the wikitext-103 corpus.
P19-1145,2019,6 Conclusion,across all data sets the quaternion model achieves comparable performance while reducing parameter size.
P19-1145,2019,6 Conclusion,"all in all, we demonstrated the utility and benefits of incorporating quaternion algebra in state-of-theart neural models."
P19-1145,2019,6 Conclusion,"more concretely, we proposed two models - quaternion attention model and quaternion transformer."
P19-1145,2019,6 Conclusion,our tensor2tensor implementation of quaternion transformers will be released at https://github.com/ vanzytay/quaterniontransformers.
P19-1145,2019,6 Conclusion,this paper advocates for lightweight and efficient neural nlp via quaternion representations.
P19-1145,2019,6 Conclusion,we believe that this direction paves the way for more efficient and effective representation learning in nlp.
P19-1145,2019,6 Conclusion,we evaluate these models on eight different nlp tasks and a total of thirteen data sets.
P19-1146,2019,6 Conclusion and Future Work,"a natural next step is to apply entmax to self-attention (vaswani et al.,2017)."
P19-1146,2019,6 Conclusion and Future Work,"given the ubiquity of softmax in nlp, entmax has many potential applications."
P19-1146,2019,6 Conclusion and Future Work,"in a different vein, the strong morphological inflection results point to usefulness in other tasks where probability is concentrated in a small number of hypotheses, such as speech recognition."
P19-1146,2019,6 Conclusion and Future Work,"our approach yielded consistent improvements over dense models on morphological inflection and machine translation, while inducing interpretability in both attention and output distributions."
P19-1146,2019,6 Conclusion and Future Work,sparse output layers also provide exactness when the number of possible hypotheses does not exhaust beam search.
P19-1146,2019,6 Conclusion and Future Work,we proposed sparse sequence-to-sequence models and provided fast algorithms to compute their attention and output transformations.
P19-1147,2019,9 Conclusions,future work includes developing a adversarial training scheme as well as devising a more robust architecture based on our findings.
P19-1147,2019,9 Conclusions,"we provide theoretical explanations regarding why the self-attention structure leads to better robustness, in addition to illustrative examples that visualize the model’s internal variations."
P19-1147,2019,9 Conclusions,"we show that self-attentive models are more robust to adversarial attacks than recurrent networks under small input perturbations on three nlp tasks, i.e., sentiment analysis, entailment, and translation."
P19-1148,2019,6 Conclusion,we expand the hard-attention neural sequenceto-sequence model of wu et al.(2018) to enforce monotonicity.
P19-1148,2019,6 Conclusion,"we isolate the effect of monotonicity in a controlled experiment and show monotonicity is a useful hard constraint for three tasks, and speculate previous underperformance is due to a lack of joint training."
P19-1148,2019,6 Conclusion,"we show, empirically, that enforcing monotonicity in the alignments found by hard attention models helps significantly, and we achieve state-of-the-art performance on the morphological inflection using data from the conllsigmorphon 2017 shared task."
P19-1149,2019,7 Conclusion and Future Work,experiments on six different nlp tasks show that lrn achieves competitive performance against existing recurrent units.
P19-1149,2019,7 Conclusion and Future Work,"in the future, we are interested in testing lowlevel optimizations of lrn, which are orthogonal to this work, such as dedicated cudnn kernels."
P19-1149,2019,7 Conclusion and Future Work,"it is simple, effective and reaches better trade-off among parameter number, running speed, model performance and generalization."
P19-1149,2019,7 Conclusion and Future Work,lrn has a strong correlation with selfattention networks.
P19-1149,2019,7 Conclusion and Future Work,theoretical and empirical analysis shows that the input and forget gate in lrn can learn long-range dependencies and avoid gradient vanishing and explosion.
P19-1149,2019,7 Conclusion and Future Work,"this paper presents lrn, a lightweight recurrent network that factors matrix operations outside the recurrence and enables higher parallelization."
P19-1150,2019,5 Conclusion,"in particular, we extended existing capsule networks into a new framework with advantages concerning scalability, reliability and generalizability."
P19-1150,2019,5 Conclusion,"in the future, we plan to apply capsule networks to even more challenging nlp problems such as language modeling and text generation."
P19-1150,2019,5 Conclusion,"in this work, we have addressed the latter issue."
P19-1150,2019,5 Conclusion,making computers perform more like humans is a major issue in nlp and machine learning.
P19-1150,2019,5 Conclusion,our experimental results have demonstrated its effectiveness on two nlp tasks: multi-label text classification and question answering.
P19-1150,2019,5 Conclusion,"this not only includes making them perform on similar levels (hassan et al., 2018), but also requests them to be robust to adversarial examples (eger et al., 2019) and generalize from few data points (ruckl ¨ e′ et al., 2019)."
P19-1150,2019,5 Conclusion,"through our modifications and enhancements, we hope to have made capsule networks more suitable to large-scale problems and, hence, more mature for real-world applications."
P19-1151,2019,7 Conclusion,"in this paper, we study the limitations of hardparameter sharing in sparse transfer learning."
P19-1151,2019,7 Conclusion,we have demonstrated the effectiveness and flexibility of our softcode approaches in extensive evaluations over mtl and cll scenarios.
P19-1151,2019,7 Conclusion,we propose soft-code approaches to avoid the sparseness observed in mtl and cll.
P19-1152,2019,5 Discussion and Future Work,"from a theoretical perspective, there exists a trade-off between the cost of computation and the tightness of approximation."
P19-1152,2019,5 Discussion and Future Work,"in addition, the tensor rank can (far) exceed the maximum dimension, and a low-rank approximation for tensors may not even exist (de silva and lim, 2008)."
P19-1152,2019,5 Discussion and Future Work,"we acknowledge that there are other alternative methods to upper bound the true rank of a tensor (alexeev et al., 2011; atkinson and lloyd, 1980; ballico, 2014)."
P19-1152,2019,5 Discussion and Future Work,"while our tensor rank regularization method seems to work well empirically, there is definitely room for a more thorough theoretical analysis of constructing and regularizing tensor representations for multimodal learning."
P19-1153,2019,4 Conclusion & Future Work,"capitalizing on our model’s architecture, we showed our method to perform well on sentiment analysis and more precisely its advantage when classifying sentiment trees."
P19-1153,2019,4 Conclusion & Future Work,"continuing in the direction of training our model on different nlp tasks, we would like our representations to generalize well on downstream tasks while maintaining their reconstruction property."
P19-1153,2019,4 Conclusion & Future Work,"decoding from a single embedding and working with a 337k vocabulary, we manage to get near perfect reconstruction for sequences of up to 40 length and very good reconstruction for longer sequences."
P19-1153,2019,4 Conclusion & Future Work,"finally, we would like to learn our sentence embeddings’ latent space, similarly to subramanian et al.(2018)’s method, so as to leverage our autoencoder’s strong reconstruction ability and generate very long sequences of text."
P19-1153,2019,4 Conclusion & Future Work,"in this paper, we introduced a recursive autoencoder method for generating sentence and subsentence representations."
P19-1153,2019,4 Conclusion & Future Work,we would also like to further explore the usage of sub-sentence representations in natural language processing.
P19-1154,2019,8 Conclusion,"in addition, we propose an online vocabulary updating algorithm for further performance enhancement by tracking users behavior effectively."
P19-1154,2019,8 Conclusion,the evaluation on the standard linguistic corpus and true inputting history show the proposed methods indeed greatly improve user experience in terms of diverse metrics compared to commercial ime and state-of-the-art traditional model.
P19-1154,2019,8 Conclusion,this paper presents the first neural p2c converter for pinyin-based chinese ime with open vocabulary learning as to our best knowledge.
P19-1154,2019,8 Conclusion,we adopt an online working-style seq2seq model for the concerned task by formulizing it as a machine translation from pinyin sequence to chinese character sequence.
P19-1155,2019,9 Conclusion,"phonotactic acquisition can be accomplished without external, prior knowledge of distinctive features; indeed, according to our results, this knowledge may be a slight hindrance rather than a help."
P19-1155,2019,9 Conclusion,"though segment-level phonotactic inference may still benefit from access to a finer-grained phonetic specification of the speech stream, a predetermined encoding of this input in terms of distinctive features does not appear to be required for this purpose."
P19-1156,2019,7 Conclusion,"finally, we show that these gains can also be projected across closely related languages by sharing morphological annotations."
P19-1156,2019,7 Conclusion,"furthermore, we observe this gain even when the morphological annotations and language modeling data are disjoint, providing a simple way to improve language modelsing without requiring additional annotation efforts."
P19-1156,2019,7 Conclusion,our analysis finds that the addition of morphology benefits inflected forms more than uninflected forms and that training our clms on additional language modeling data does not diminish these gains in bpc.
P19-1156,2019,7 Conclusion,we conclude that this multitasking approach helps the clms capture morphology better than the lm objective alone.
P19-1156,2019,7 Conclusion,we incorporate morphological supervision into character language models via multitask learning and find that this addition improves bpc on 24 languages.
P19-1157,2019,5 Conclusions,"in our error analysis, we, in addition, observe that reinforcement learning is particularly beneficial for long words and unseen words, which are probably the hardest challenges in historical text normalization."
P19-1157,2019,5 Conclusions,"our experiments show that across several languages, policy gradient fine-tuning outperforms maximum likelihood training of sequence-tosequence models for historical text normalization."
P19-1157,2019,5 Conclusions,"since historical text normalization is a characterlevel transduction task, it is feasible to experiment with reinforcement learning, and we believe our results are very promising."
P19-1158,2019,7 Conclusion,"in this paper, we introduced stochastic tokenization for text classification with a neural network."
P19-1158,2019,7 Conclusion,our model also updates the language model depending on the sampled tokenizations in the training phase.
P19-1158,2019,7 Conclusion,our model differs from previous methods in terms of sampling tokenization that considers all possible words under the maximum length limitation.
P19-1158,2019,7 Conclusion,this results in improved performance for sentiment analysis tasks on japanese and english datasets and chinese datasets with a larger cache.
P19-1158,2019,7 Conclusion,"to embed various tokens, we proposed the cache mechanism for frequent words."
P19-1158,2019,7 Conclusion,we expect our model contributes to improved performance of other complex state-of-the-art encoding architectures for text classification.
P19-1158,2019,7 Conclusion,we find that the proposed model of tokenization provides an improvement in the performance of text classification with a simple lstm classifier.
P19-1158,2019,7 Conclusion,"with the updated language model, the proposed model can tokenize the test dataset considering recently used tokenization in the training phase."
P19-1159,2019,5 Conclusion and Future Directions,"a few interdisciplinary studies (herbelot et al., 2012; avin et al., 2015; fu et al., 2016; schluter, 2018) have emerged, and we urge more interdisciplinary discussions in terms of gender bias."
P19-1159,2019,5 Conclusion and Future Directions,"additionally, mitigating gender bias in nlp is both a sociological and an engineering problem."
P19-1159,2019,5 Conclusion and Future Directions,"approaches from other technical fields may improve current debiasing methods in nlp or inspire the development of new, more effective methods even if the properties of the data or problem are different across fields."
P19-1159,2019,5 Conclusion and Future Directions,"as mentioned in section 1, gender bias is not a problem that is unique to nlp; other fields in computer science such as data mining, machine learning, and security also study gender bias (calders and verwer, 2010; feldman et al., 2015; hardt et al., 2016; misra et al., 2016; kleinberg et al., 2016; pleiss et al., 2017; beutel et al., 2017; kilbertus et al., 2017)."
P19-1159,2019,5 Conclusion and Future Directions,"below, we identify a few future directions."
P19-1159,2019,5 Conclusion and Future Directions,"besides, we refer the readers to hovy and spruit (2016) for a more general discussion of ethical concern in nlp."
P19-1159,2019,5 Conclusion and Future Directions,discussions between computer scientists and sociologists may improve understanding of latent gender bias found in machine learning data sets and model predictions.
P19-1159,2019,5 Conclusion and Future Directions,"finally, hand-craft debiasing approaches may unintentionally encode the implicit bias of the developers."
P19-1159,2019,5 Conclusion and Future Directions,"first, the majority of debiasing techniques focus on a single, modular process of an end-to-end nlp system."
P19-1159,2019,5 Conclusion and Future Directions,future work can look to apply existing methods or devise new techniques towards mitigating gender bias in other languages as well.
P19-1159,2019,5 Conclusion and Future Directions,gender debiasing methods in nlp are not sufficient to debias models end-to-end for many applications.
P19-1159,2019,5 Conclusion and Future Directions,"however, in languages such as spanish, each noun has its own gender and corresponding modifiers of the noun need to align with the gender of the noun."
P19-1159,2019,5 Conclusion and Future Directions,"however, such a task is not trivial."
P19-1159,2019,5 Conclusion and Future Directions,"however, we recognize that different applications may require different metrics and there are trade-offs between different notions of biases (barocas et al., 2018; chouldechova and roth, 2018)."
P19-1159,2019,5 Conclusion and Future Directions,"in this paper, we summarize recent literature about recognizing and mitigating gender bias in nlp."
P19-1159,2019,5 Conclusion and Future Directions,it remains to be discovered how these individual parts harmonize together to form an ideally unbiased system.
P19-1159,2019,5 Conclusion and Future Directions,many of these technical methods could be applicable to nlp yet to our knowledge have not been studied.
P19-1159,2019,5 Conclusion and Future Directions,methods such as gender-swapping are relatively easy in english because english does not distinguish gender linguistically.
P19-1159,2019,5 Conclusion and Future Directions,mitigating gender bias in languages beyond english.
P19-1159,2019,5 Conclusion and Future Directions,"non-binary genders (richards et al., 2016) as well as racial biases have largely been ignored in nlp and should be considered in future work."
P19-1159,2019,5 Conclusion and Future Directions,"other important aspects such as model/data transparency (mitchell et al., 2019; bender and friedman, 2018) and privacy preservation (reddy and knight, 2016; elazar and goldberg, 2018; li et al., 2018) are also not covered in this literature survey."
P19-1159,2019,5 Conclusion and Future Directions,"second, most gender debiasing methods have only been empirically verified in limited applications (zhang et al., 2018; zhao et al., 2017), and it is not clear that these methods can generalize to other tasks or models."
P19-1159,2019,5 Conclusion and Future Directions,"similar issues of algorithmic bias have also been discussed extensively in artificial intelligence, machine learning, data mining, and several other application fields (e.g., (calders and verwer, 2010; feldman et al., 2015; hardt et al., 2016; misra et al., 2016; kleinberg et al., 2016; pleiss et al., 2017; beutel et al., 2017; misra et al., 2016))."
P19-1159,2019,5 Conclusion and Future Directions,the study of gender bias in nlp is still relatively nascent and consequently lacks unified metrics and benchmarks for evaluation.
P19-1159,2019,5 Conclusion and Future Directions,"there is a long history of gender stereotype study in law, psychology, media study, and many other disciplines which we do not discuss."
P19-1159,2019,5 Conclusion and Future Directions,"third, we note that certain debiasing techniques may introduce noise into a nlp model, causing performance degradation."
P19-1159,2019,5 Conclusion and Future Directions,"to completely debias effectively, it is important to understand how machine learning methods encode biases and how humans perceive biases."
P19-1159,2019,5 Conclusion and Future Directions,"to perform gender-swapping in such languages, besides swapping those gendered nouns, we also need to change the modifiers.non-binary gender bias."
P19-1159,2019,5 Conclusion and Future Directions,we acknowledge that the scope of this paper is limited.
P19-1159,2019,5 Conclusion and Future Directions,we note the following limitations of current approaches.
P19-1159,2019,5 Conclusion and Future Directions,we urge researchers in related fields to work together to create standardized metrics that rigorously measure the gender bias in nlp applications.
P19-1159,2019,5 Conclusion and Future Directions,"with few exceptions (manzini et al., 2019), work on debiasing in nlp has assumed that the protected attribute being discriminated against is binary."
P19-1159,2019,5 Conclusion and Future Directions,"with few exceptions (vanmassenhove et al., 2018; prates et al., 2018), prior work has focused on mitigating gender bias in the english language."
P19-1160,2019,5 Conclusion,"experimental results on multiple benchmark datasets demonstrate that the proposed method can accurately debias pre-trained word embeddings, outperforming previously proposed debiasing methods, while preserving useful semantic information."
P19-1160,2019,5 Conclusion,"in future, we plan to extend the proposed method to debias other types of demographic biases such as ethnic, age or religious biases."
P19-1160,2019,5 Conclusion,we proposed a method to remove gender-specific biases from pre-trained word embeddings.
P19-1161,2019,7 Conclusion,"as a result, there is no existing annotated corpus of paired sentences that can be used as “ground truth.” despite this limitation, we evaluated our approach both intrinsically and extrinsically, achieving promising results."
P19-1161,2019,7 Conclusion,"finally, we also identified avenues for future work, such as the inclusion of co-reference information."
P19-1161,2019,7 Conclusion,"for example, we demonstrated that our approach reduces gender stereotyping in neural language models."
P19-1161,2019,7 Conclusion,"to do this, we introduced a markov random field with an optional neural parameterization that infers the manner in which a sentence must change to preserve morpho-syntactic agreement when altering the grammatical gender of particular nouns."
P19-1161,2019,7 Conclusion,"to the best of our knowledge, this task has not been studied previously."
P19-1161,2019,7 Conclusion,we presented a new approach for converting between masculine-inflected and feminine-inflected noun phrases in morphologically rich languages.
P19-1162,2019,6 Conclusion,"finally, our framework and metric reveal transparent analysis of the unintended bias hidden in word embeddings."
P19-1162,2019,6 Conclusion,for this work our scope was limited to unfair biases with respect to negative sentiment.
P19-1162,2019,6 Conclusion,"in our framework, we train a classifier on an unbiased positive/negative word sentiment dataset initialized with biased word embeddings."
P19-1162,2019,6 Conclusion,our metric has a direct connection to discrimination and can evaluate any number of demographics in a protected group.
P19-1162,2019,6 Conclusion,"previous metrics and analysis into unintended bias in word embeddings rely on vector space arguments for only two demographics at a time, which does not lend itself well to evaluating real world discrimination."
P19-1162,2019,6 Conclusion,"this allows us to observe clearer signals of bias in our metric, relative negative sentiment bias (rnsb)."
P19-1162,2019,6 Conclusion,"this way, we can observe the unfairness in the word embeddings at the ml prediction level."
P19-1162,2019,6 Conclusion,we presented a transparent framework for evaluating unintended demographic bias in word embeddings.
P19-1163,2019,6 Conclusion,"finally, we introduce dialect and race priming, two ways to reduce annotator bias by highlighting the dialect of a tweet in the data annotation, and show that it significantly decreases the likelihood of aae tweets being labelled as offensive."
P19-1163,2019,6 Conclusion,"we analyze racial bias in widely-used corpora of annotated toxic language, establishing correlations between annotations of offensiveness and the african american english (aae) dialect."
P19-1163,2019,6 Conclusion,we find strong evidence that extra attention should be paid to the confounding effects of dialect so as to avoid unintended racial biases in hate speech detection.
P19-1163,2019,6 Conclusion,"we show that models trained on these corpora propagate these biases, as aae tweets are twice as likely to be labelled offensive compared to others."
P19-1164,2019,5 Conclusions,our data and code are publicly available at https://github.com/ gabrielstanovsky/mt_gender.
P19-1164,2019,5 Conclusions,"we presented the first large-scale multilingual quantitative evidence for gender bias in mt, showing that on eight diverse target languages, all four tested popular commercial systems and two recent state-of-the-art academic mt models are significantly prone to translate based on gender stereotypes rather than more meaningful context."
P19-1165,2019,6 Conclusions,"finally, the introduction of an output layer which predicts pretrained embeddings enables us to use larger vocabularies instead of using the slower softmax."
P19-1165,2019,6 Conclusions,"firstly, we have shown that our semantic representations are capable to properly reflect the similarity between word and sense representations, showing state-of-the-art performance in the sense-aware tasks of word-to-sense similarity and most frequent sense induction."
P19-1165,2019,6 Conclusions,our model shows potential for further applications.
P19-1165,2019,6 Conclusions,"secondly, our approach is also able to attain high performance in standard word-based semantic evaluations, namely, synonym recognition and outlier detection."
P19-1165,2019,6 Conclusions,trying more complex networks is also within our scope and is left as future work.
P19-1165,2019,6 Conclusions,"we did, in fact, explore alternative configurations, for instance, using several layers or replacing the lstms with gated recurrent units (cho et al., 2014) or the transformer architecture (vaswani et al., 2017)."
P19-1165,2019,6 Conclusions,we draw three main findings.
P19-1165,2019,6 Conclusions,"we presented lstmembed, a new model based on a bidirectional lstm for learning embeddings of words and senses jointly, and which is able to learn semantic representations on a par with, or better than, state-of-the-art approaches."
P19-1165,2019,6 Conclusions,we release the word and sense embeddings at the following url: http: //lcl.uniroma1.it/lstmembed.
P19-1166,2019,7 Conclusion,"for example, by contriving the attribute sets for weat, virtually any word can be classified as gender-biased relative to another."
P19-1166,2019,7 Conclusion,"however, for words that are gender-biased or gender-specific by definition, sgns amplifies the genderedness in the corpus."
P19-1166,2019,7 Conclusion,"in this paper, we answered several open questions about undesirable word associations in embedding spaces."
P19-1166,2019,7 Conclusion,"using ripa, we found that sgns does not, on average, make most words any more gendered in the embedding space than they are in the training corpus."
P19-1166,2019,7 Conclusion,"we found that for any embedding model that implicitly does matrix factorization (e.g., sgns, glove), debiasing with the subspace projection method is, under certain conditions, equivalent to training on a corpus that is unbiased with respect to the words defining the bias subspace."
P19-1166,2019,7 Conclusion,"we proved that weat, the most common test of word embedding association, has theoretical flaws that cause it to systematically overestimate bias."
P19-1166,2019,7 Conclusion,we then derived a new measure of association in word embeddings called the relational inner product association (ripa).
P19-1167,2019,5 Conclusion and Limitations,"first, we ignore demographics (e.g., age, gender, location) of the speaker, even though such demographics are likely influence word choice."
P19-1167,2019,5 Conclusion and Limitations,"for example, positive adjectives used to describe women are more often related to their bodies than adjectives used to describe men."
P19-1167,2019,5 Conclusion and Limitations,"in addition, depictions of men and women have certainly changed over the period covered by our corpus; indeed, underwood et al.(2018) found evidence of such a change for fictional characters."
P19-1167,2019,5 Conclusion and Limitations,"in future work, we intend to conduct a diachronic analysis in english using the same corpus, in addition to a cross-linguistic study of gendered language."
P19-1167,2019,5 Conclusion and Limitations,our study has a few limitations that we wish to highlight.
P19-1167,2019,5 Conclusion and Limitations,"second, we ignore genre (e.g., news, romance) of the text, even though genre is also likely to influence the language used to describe men and women."
P19-1167,2019,5 Conclusion and Limitations,"via our experiments, we found evidence in support of common gender stereotypes."
P19-1167,2019,5 Conclusion and Limitations,"we introduced a generative latent-variable model that jointly represents adjective (or verb) choice, with its sentiment, given the natural gender of a head (or dependent) noun."
P19-1167,2019,5 Conclusion and Limitations,"we presented an experimental framework for quantitatively studying the ways in which the language used to describe men and women is different and, moreover, different in a positive or negative way."
P19-1168,2019,5 Conclusion,it may be of interest to extend these techniques to embed knowledge graph elements.
P19-1168,2019,5 Conclusion,"they work better than recent embedding transfer methods, and give benefits even with contextual embeddings."
P19-1168,2019,5 Conclusion,we introduced one regularization and one sourceselection method for adapting word embeddings from a partly useful source corpus to a target topic.
P19-1169,2019,5 Conclusion and Future Work,"additionally, the mapping technique used for relation-aware semantic projection can be further improved to model different linguistic properties of lexical relations (e.g., the “one-to-many” mappings for meronymy)."
P19-1169,2019,5 Conclusion and Future Work,experiments over four benchmark datasets and cogalex-v show spherere outperforms state-of-the-art methods.
P19-1169,2019,5 Conclusion and Future Work,"in the future, we will improve our model to deal with datasets containing a relatively large number of lexical relation types and random term pairs."
P19-1169,2019,5 Conclusion and Future Work,"in this paper, we present a representation learning model to distinguish lexical relations based on hyperspherical relation embeddings (spherere)."
P19-1169,2019,5 Conclusion and Future Work,it learns representations of lexical relation triples by mapping them to the hyperspherical embedding space.
P19-1169,2019,5 Conclusion and Future Work,the lexical relations between term pairs are predicted using neural networks over the learned embeddings.
P19-1170,2019,5 Conclusion,"finally, the preliminary results we have shown on aligning more than two languages at the same time provide an exciting path for future research."
P19-1170,2019,5 Conclusion,the resulting embeddings also significantly increase the precision of sentence retrieval in multilingual settings.
P19-1170,2019,5 Conclusion,"we have introduced a cross-lingual embedding alignment procedure based on a probabilistic latent variable model, that increases performance across various tasks compared to previous methods using both nearest neighbour retrieval, as well as the csls criterion."
P19-1170,2019,5 Conclusion,we have shown that the resulting embeddings in this aligned space preserve their quality by presenting results on tasks that assess word and sentence-level monolingual similarity correlation with human scores.
P19-1171,2019,6 Conclusion,"according to the information-theoretic perspective of monaghan et al.(2011), an optimal lexicon has an arbitrary mapping between form and meaning."
P19-1171,2019,6 Conclusion,"an avenue for future work is connecting our discovered phonesthemes to putative meanings, as done by abramova et al.(2013) and abramova and fernandez ′ (2016)."
P19-1171,2019,6 Conclusion,"if this is true, then a large amount of these benefits do accrue to language; that is, given the small degree of systematicity, we lose little of the benefit."
P19-1171,2019,6 Conclusion,the low uncertainty reduction suggests that the lexicon is still largely arbitrary.
P19-1171,2019,6 Conclusion,"we find evidence in 87 of 106 languages for a significant systematic pattern between form and meaning, reducing approximately 5% of the phone-sequence uncertainty of german lexicons and 2.5% in english and dutch, when controlling for part of speech."
P19-1171,2019,6 Conclusion,"we have framed the question on informationtheoretic grounds, estimating entropies by state-ofthe-art neural language modeling."
P19-1171,2019,6 Conclusion,"we have identified meaningful phonesthemes according to our operationalization, and we have good precision—all but two of our english phonesthemes are attested in prior work."
P19-1171,2019,6 Conclusion,we have revisited the linguistic question of the arbitrariness—and the systematicity—of the sign.
P19-1172,2019,5 Conclusion,"using iterative discovery and robust ensembling of multiple high-performance morphological learning algorithms to yield standalone target language systems, we achieve doubledigit relative error reductions in both lemmatization and morphosyntactic feature analysis over a strong initial system, evaluated on modern test vocabulary in all 26 languages."
P19-1172,2019,5 Conclusion,"using no target-language training data, we successfully transferred multiple fine-grained annotations on 27 different english bible editions to 26 diverse target languages."
P19-1172,2019,5 Conclusion,we have presented a method for learning morphosyntactic feature analyzers and lemmatizers from iterative annotation projection.
P19-1173,2019,8 Conclusions and Future Work,"future work includes joint and cross-dialectal lemmatization models, in addition to further extension to other dialects."
P19-1173,2019,8 Conclusions and Future Work,in this paper we presented a model for joint morphological modeling of the features in morphologically rich dialectal variants.
P19-1173,2019,8 Conclusions and Future Work,"joint modeling for the features within each dialect performs consistently better than having separate models, and joint cross-dialectal modeling performs better than dialect-specific models."
P19-1173,2019,8 Conclusions and Future Work,"our models result in state-of-the-art results for both msa, and egy."
P19-1173,2019,8 Conclusions and Future Work,we also presented several extensions for cross-dialectal modeling.
P19-1173,2019,8 Conclusions and Future Work,"we also used adversarial training to facilitate a knowledge-transfer scheme, providing the best result for egy, especially in lower-resource cases."
P19-1173,2019,8 Conclusions and Future Work,"we showed that having separate embedding models, but shared output layers, performs the best."
P19-1174,2019,7 Conclusion and Future Work,a reordering embedding was learned by considering the relationship between the positional embedding of a word and that of the entire sentence.
P19-1174,2019,7 Conclusion and Future Work,experiments showed that our method can significantly improve the performance of transformer.
P19-1174,2019,7 Conclusion and Future Work,"however, it has not been extensively studied in nmt."
P19-1174,2019,7 Conclusion and Future Work,"in future work, we will further explore the effectiveness of the reordering mechanism and apply it to other natural language processing tasks, such dependency parsing (zhang et al., 2016; li et al., 2018), and semantic role labeling (he et al., 2018; li et al., 2019)."
P19-1174,2019,7 Conclusion and Future Work,"in this paper, we proposed a reordering mechanism to capture knowledge of reordering."
P19-1174,2019,7 Conclusion and Future Work,the proposed reordering embedding can be easily introduced to the existing transformer translation system to predict translations.
P19-1174,2019,7 Conclusion and Future Work,word ordering is an important issue in translation.
P19-1175,2019,7 Conclusion,"compared to previous approaches to incorporate tm information into mt systems, nfr does not require different nmt architectures or algorithms, but relies solely on input preprocessing, and can thus be used in combination with any existing nmt system or toolkit."
P19-1175,2019,7 Conclusion,"in a next step, we plan to compare the performance of nfr to other approaches to tm-nmt integration, for example by carrying out evaluations on the jrc-acquis corpus (gu et al., 2018; koehn and senellart, 2010a; zhang et al., 2018)."
P19-1175,2019,7 Conclusion,"in addition, it would be informative to carry out a qualitative analysis of the nfr output in terms of how and to what extent the information contained in the fuzzy matches is used in the final translation, in comparison with the nmt baseline."
P19-1175,2019,7 Conclusion,"tests on two language pairs (en-nl and en-hu) showed that this method can achieve substantial gains in estimated translation quality compared to a range of baseline systems, even for relatively small training set sizes."
P19-1175,2019,7 Conclusion,"the approach also needs to be tested on data sets with a lower frequency of repeated sentences, other language pairs as well as different domains, ultimately also involving human evaluation (both in term of perceived quality and post-editing time)."
P19-1175,2019,7 Conclusion,"the tm-nmt integration approach presented in this paper, neural fuzzy repair, makes use of data augmentation to help improve machine translation quality using information retrieved from a translation memory."
P19-1175,2019,7 Conclusion,"we also intend to carry out further tests to potentially improve the quality of the output, for example by testing different match metrics and retrieval methods, nmt architectures (e.g.transformer), ways to include alignment information and by applying additional morphological preprocessing."
P19-1175,2019,7 Conclusion,we believe that the ease of implementation of nfr could lead to the wider adoption of tm-nmt integration.
P19-1176,2019,8 Conclusion,"also, its model size is 1.6x smaller."
P19-1176,2019,8 Conclusion,experimental results show that our thin-but-deep encoder can match or surpass the performance of transformer-big.
P19-1176,2019,8 Conclusion,"in addition, it requires 3x fewer training epochs and is 10% faster for inference."
P19-1176,2019,8 Conclusion,it is the deepest encoder used in nmt so far.
P19-1176,2019,8 Conclusion,"moreover, we proposed an approach based on a dynamic linear combination of layers and successfully trained a 30-layer transformer system."
P19-1176,2019,8 Conclusion,"we have shown that the deep transformer models can be easily optimized by proper use of layer normalization, and have explained the reason behind it."
P19-1176,2019,8 Conclusion,we have studied deep encoders in transformer.
P19-1177,2019,5 Analysis and Conclusion,"by examining the results, we found the syntaxbased model tends to produce one translation in active voice and another in passive voice."
P19-1177,2019,5 Analysis and Conclusion,"in contrast, the translation results sampled with the syntactic coding model have drastically different grammars."
P19-1177,2019,5 Analysis and Conclusion,"table 2 gives samples of the candidate translations produced by the models conditioning on different discrete codes, compared to the candidates produced by beam search."
P19-1177,2019,5 Analysis and Conclusion,the source code for extracting discrete codes from parse trees will be publicly available.
P19-1177,2019,5 Analysis and Conclusion,"to summarize, we show a diverse set of translations can be obtained with sentence codes when a reasonable external mechanism is used to produce the codes."
P19-1177,2019,5 Analysis and Conclusion,we can see that the candidate translations produced by beam search has only minor grammatical differences.
P19-1177,2019,5 Analysis and Conclusion,"when a good syntax parser exists, the syntax-based approach works better in terms of diversity."
P19-1178,2019,5 Conclusions and Future Work,"as future work, we will apply our methodology to domain adaptation."
P19-1178,2019,5 Conclusions and Future Work,"existing approaches make use of huge amounts of monolingual (～100 m, references in table 1) or comparable (～10 m, this work) sentences and these numbers are still far from what one can gather in a truly low-resource scenario."
P19-1178,2019,5 Conclusions and Future Work,"in the same vain as unsupervised mt, we want to continue our research by using back translation for rejected pairs and dealing with phrases instead of full sentences."
P19-1178,2019,5 Conclusions and Future Work,"in this setting, word embeddings and hidden layers are already initialised via standard nmt training on parallel data and training is continued with an in-domain monolingual or comparable corpus."
P19-1178,2019,5 Conclusions and Future Work,our architecture is also useful for data selection in data rich language pairs and we will perform experiments on cleaning noisy parallel corpora.
P19-1178,2019,5 Conclusions and Future Work,that will allow us to extract more parallel text from a corpus and facilitate using these approaches for low-resourced languages.
P19-1178,2019,5 Conclusions and Future Work,the key point of our approach is the combination of a margin-based score with the intersection of sentence representations for filtering the input corpus.
P19-1178,2019,5 Conclusions and Future Work,this is a form of self-supervision alternating between two tasks that support each other in an incremental fashion.
P19-1178,2019,5 Conclusions and Future Work,"we focus on data representation, an adequate function for the selection process, and studying how to avoid additional hyperparameters that depend on the input corpus."
P19-1178,2019,5 Conclusions and Future Work,we present a joint architecture to select data and train nmt systems simultaneously using the emerging nmt system itself to select the data.
P19-1179,2019,6 Conclusion,"our method does not introduce additional parameters: we hope to motivate future work on learning speech representations, with continued performance on lowerresource settings if additional parameters are introduced."
P19-1179,2019,6 Conclusion,previous work on end-to-end speech translation has used frame-level speech features.
P19-1179,2019,6 Conclusion,"we compared two input representations for two unrelated languages pairs, and a variety of differently-resourced conditions, using both a supervised alignment method and a cross-lingual method for our low-resource case."
P19-1179,2019,6 Conclusion,we have shown that a na¨?ve method to create higher-level speech representations for translation can be more effective and efficient than traditional frame-level features.
P19-1180,2019,5 Discussion,"along the way, the model acquires estimates of word concreteness."
P19-1180,2019,5 Discussion,"automatically acquiring such inductive biases from data remains challenging (kemp et al., 2006; gauthier et al., 2018)."
P19-1180,2019,5 Discussion,"finally, it may be possible to extend our approach to other linguistic tasks such as dependency parsing (christie et al., 2016b), coreference resolution (kottur et al., 2018), and learning pragmatics beyond semantics (andreas and klein, 2016)."
P19-1180,2019,5 Discussion,"first, vg-nsl matches text embeddings directly with embeddings of entire images."
P19-1180,2019,5 Discussion,"in our experiments, we find that this approach to grounded language learning produces parsing models that are both accurate and stable, and that the learning is much more data-efficient than a state-of-the-art text-only approach."
P19-1180,2019,5 Discussion,"in particular, the current approach has thus far been applied to understanding grounded texts in a single domain (static visual scenes for vg-nsl)."
P19-1180,2019,5 Discussion,"its applicability could be extended by learning shared representations across multiple modalities (castrejon et al., 2016) or integrating with pure text-domain models (such as prpn, shen et al., 2018a)."
P19-1180,2019,5 Discussion,"its performance may be boosted by considering structured representations of both images (e.g., lu et al., 2016; wu et al., 2019) and texts (steedman, 2000)."
P19-1180,2019,5 Discussion,"second, thus far we have used a shared representation for both syntax and semantics, but it may be useful to disentangle their representations (steedman, 2000)."
P19-1180,2019,5 Discussion,the results suggest multiple future research directions.
P19-1180,2019,5 Discussion,there are also limitations to the idea of grounded language acquisition.
P19-1180,2019,5 Discussion,"third, our best model is based on the head-initial inductive bias."
P19-1180,2019,5 Discussion,vg-nsl jointly learns parse trees and visually grounded textual representations.
P19-1180,2019,5 Discussion,"we have proposed a simple but effective model, the visually grounded neural syntax learner, for visually grounded language structure acquisition."
P19-1181,2019,6 Conclusion,"for example, future extensions of vln will likely involve games (baldridge et al., 2018) where the instructions being given take the agent around a trap or help it avoid opponents."
P19-1181,2019,6 Conclusion,"furthermore, our findings suggests ways that future datasets and metrics for judging agents should be constructed and set up for evaluation."
P19-1181,2019,6 Conclusion,future agents will need to make effective use of language and its connection to the environment to both drive cls up and bring ne down in r4r.
P19-1181,2019,6 Conclusion,"in such scenarios, going straight to the goal could be literally deadly to the robot or agent."
P19-1181,2019,6 Conclusion,"keeping in mind that humans have an average navigation error of 1.61 in r2r (anderson et al., 2018b), the average navigation error of 8.08 meters for r4r by our best agent leaves plenty of headroom."
P19-1181,2019,6 Conclusion,"similar constraints could hold in search-and-rescue human-robot teams (kruijff et al., 2014; kruijff-korbayov et al., 2016), where the direct path could take a rolling robot into an area with greater danger of collapse."
P19-1181,2019,6 Conclusion,"the cls metric, r4r, and our experiments provide a better toolkit for measuring the impact of better language understanding in vln."
P19-1181,2019,6 Conclusion,the r4r data itself clearly still has considerable headroom: our reimplementation of the rcm model gets only 34.6 cls on paths in r4r’s validation unseen houses.
P19-1181,2019,6 Conclusion,"we expect path fidelity to not only be interesting with respect to grounding language, but to be crucial for many vln-based problems."
P19-1182,2019,6 Conclusion,"based on automatic and human evaluations, our relational speaker model improves the ability to capture visual relationships."
P19-1182,2019,6 Conclusion,"for future work, we are going to further explore the possibility to merge the three datasets by either learning a joint image representation or by transferring domain-specific knowledge."
P19-1182,2019,6 Conclusion,"in this paper, we explored the task of describing the visual relationship between two images."
P19-1182,2019,6 Conclusion,we are also aiming to enlarge our image editing request dataset with newly-released posts on reddit and zhopped.
P19-1182,2019,6 Conclusion,"we collected the image editing request dataset, which contains image pairs and human annotated editing instructions."
P19-1182,2019,6 Conclusion,we designed novel relational speaker models and evaluate them on our collected and other public existing dataset.
P19-1183,2019,6 Conclusion,an attentive interactor and a diversity loss were proposed to learn the complicated relationships between the instance proposals and the sentence.
P19-1183,2019,6 Conclusion,extensive experiments showed the effectiveness of our model.
P19-1183,2019,6 Conclusion,"in this paper, we introduced a new task, namely weakly-supervised spatio-temporally grounding natural sentence in video."
P19-1183,2019,6 Conclusion,"it takes a sentence and a video as input and outputs a spatio-temporal tube from the video, which semantically matches the sentence, with no reliance on spatio-temporal annotations during training."
P19-1183,2019,6 Conclusion,"moreover, we contributed a new dataset, named as vid-sentence, which can serve as a benchmark for the proposed task."
P19-1183,2019,6 Conclusion,we handled this task based on the multiple instance learning framework.
P19-1184,2019,8 Conclusion,"in future work, the data can be used to further investigate common ground and conceptual pacts; be extended through manual annotations for a more thorough linguistic analysis of co-reference chains; exploit the combination of vision and language to develop computational models for referring expression generation; or use the photobook task in the parlai framework for turing-test-like evaluation of dialogue agents."
P19-1184,2019,8 Conclusion,our results suggest that more sophisticated models are needed to fully exploit shared linguistic history.
P19-1184,2019,8 Conclusion,"the collected dialogues exhibit a significant shortening of utterances throughout a game, with final referring expressions starkly differing from both standard image captions and initial descriptions."
P19-1184,2019,8 Conclusion,"the current paper showcases only some of the aspects of the photobook dataset, which we hereby release to the public (https:// dmg-photobook.github.io)."
P19-1184,2019,8 Conclusion,"through the data collection’s task setup, participants repeatedly refer to a controlled set of target images, which allows them to improve task efficiency if they utilise their developing common ground and establish conceptual pacts (brennan and clark, 1996) on referring expressions."
P19-1184,2019,8 Conclusion,"to illustrate the potential of the dataset, we trained a baseline reference resolution model and showed that information accumulated over a reference chain helps to resolve later descriptions."
P19-1184,2019,8 Conclusion,"we have presented the first large-scale dataset of goal-oriented, visually grounded dialogues for investigating shared linguistic history."
P19-1185,2019,8 Conclusion,"next, we introduced a novel paradigm of transfer learning by combining architecture search with continual learning to avoid catastrophic forgetting."
P19-1185,2019,8 Conclusion,we also explore multi-task cell learning for generalizability.
P19-1185,2019,8 Conclusion,we first presented an architecture search approach for text classification and video caption generation tasks.
P19-1186,2019,5 Conclusion,"for training, we adopt a variational inference technique based on the variational autoencoder."
P19-1186,2019,5 Conclusion,"in empirical evaluation over a multi-domain sentiment dataset and seven language identification benchmarks, our models outperform strong baselines, across varying data conditions, including a setting where no target domain data is provided."
P19-1186,2019,5 Conclusion,"in this paper, we have proposed two models— dsda and csda—for multi-domain learning, which use a graphical model with a latent variable to represent the domain."
P19-1186,2019,5 Conclusion,our proposed models have broad utility across nlp applications on heterogenous corpora.
P19-1186,2019,5 Conclusion,"we propose models with a discrete latent variable, and a continuous vectorvalued latent variable, which we model with beta or dirichlet priors."
P19-1187,2019,6 Conclusions,"however, we believe that the two sources of information are likely to be complementary."
P19-1187,2019,6 Conclusions,in the future work we would like to consider setups where human-annotated data is combined with naturally occurring one (i.e.distantly-supervised one).
P19-1187,2019,6 Conclusions,in this paper we proposed a weakly-supervised model for entity linking.
P19-1187,2019,6 Conclusions,it would also be interesting to see if mistakes made by fully-supervised systems differ from the ones made by our system and other wikipediabased linkers.
P19-1187,2019,6 Conclusions,"our model substantially outperforms previous methods, which used the same form of supervision, and rivals fullysupervised models trained on data specifically annotated for the entity-linking problem."
P19-1187,2019,6 Conclusions,the model was trained on unlabeled documents which were automatically annotated using wikipedia.
P19-1187,2019,6 Conclusions,"this result may be interpreted as suggesting that humanannotated data is not beneficial for entity linking, given that we have wikipedia and web links."
P19-1188,2019,9 Conclusion,"as a result, much of ai and nlp community has focused on making larger and larger datasets, but we believe it is equally important to go the other direction and explore methods that help performance with little data."
P19-1188,2019,9 Conclusion,"inspired by the idea that it is easier to map language to pre-linguistic concepts, we show that when grounding language to actions in an environment, pre-learning representations of actions can help us learn language from fewer languageaction pairings."
P19-1188,2019,9 Conclusion,it is well known that neural methods do best when given extremely large amounts of data.
P19-1188,2019,9 Conclusion,this work introduces one such method.
P19-1189,2019,6 Conclusion,"ablation studies on model convergence, selection numbers, as well as distribution visualizations further confirmed the validity and effectiveness of our approach."
P19-1189,2019,6 Conclusion,"experimental results from three nlp tasks, i.e., pos tagging, dependency parsing, and sentiment analysis, demonstrate that our models outperform various baselines across domains, especially (in most cases) the same predictor trained on all source data."
P19-1189,2019,6 Conclusion,"in this paper, we proposed a general tds framework for domain adaptation via reinforcement learning, which matches the representations of the selected data from the source domain and the guidance set from the target domain and pass the similarity at different steps as rewards to guide a selection distribution generator."
P19-1189,2019,6 Conclusion,"through the generator, different instances from the source domain are selected to train a task-specific predictor."
P19-1189,2019,6 Conclusion,"to this end, not only those data relevant to the target domain are selected, but also task- and domain-specific representations are learned for them."
P19-1190,2019,6 Conclusion,"as future work, we will consider integrating more kinds of syntactic features from linguistic analysis such as dependency parsing."
P19-1190,2019,6 Conclusion,"the results have demonstrated the effectiveness of our model in terms of overall generation quality, aspect coverage, and fluency."
P19-1190,2019,6 Conclusion,this paper presented a novel review generation model using an aspect-aware coarse-to-fine generation process.
P19-1190,2019,6 Conclusion,"unlike previous methods, our model decomposed the generation process into three stages focusing on different goals."
P19-1190,2019,6 Conclusion,we constructed extensive experiments on three real-world review datasets.
P19-1191,2019,5 Conclusions and Future Work,automatic evaluations and human turing tests both demonstrate her promising performance.
P19-1191,2019,5 Conclusions and Future Work,"conducting experiments is beyond her scope, and each of her current components still requires human intervention: constructed knowledge graphs cannot cover all technical details, predicted new links need to be verified, and paper drafts need further editing."
P19-1191,2019,5 Conclusions and Future Work,"in the future, we plan to develop techniques for extracting entities of more fine-grained entity types, and extend paperrobot to write related work, predict authors, their affiliations and publication venues."
P19-1191,2019,5 Conclusions and Future Work,paperrobot is merely an assistant to help scientists speed up scientific discovery and production.
P19-1191,2019,5 Conclusions and Future Work,"we build a paperrobot who can predict related entities for an input title and write some key elements of a new paper (abstract, conclusion and future work) and predict a new title."
P19-1192,2019,5 Conclusion and Future work,experiments show that our model outperforms state-of-the-art approaches and that our model can effectively generate poetry with convincing metaphor and personification.
P19-1192,2019,5 Conclusion and Future work,"in the future, we will investigate the possibility of incorporating additional forms of rhetoric, such as parallelism and exaggeration, to further enhance the model and generate more diverse poems."
P19-1192,2019,5 Conclusion and Future work,"in this paper, we propose a rhetorically controlled encoder-decoder for modern chinese poetry generation."
P19-1192,2019,5 Conclusion and Future work,our model utilizes a continuous latent variable to capture various rhetorical patterns that govern the expected rhetorical modes and introduces rhetoric-based mixtures for generation.
P19-1193,2019,6 Conclusion,a series of evaluation metrics are also developed to comprehensively assess the quality of the generated essays.
P19-1193,2019,6 Conclusion,extensive experimental results show that the proposed method can outperform competitive baselines by a large margin.
P19-1193,2019,6 Conclusion,"further analysis demonstrates that with external commonsense knowledge and adversarial training, the generated essays are more novel, diverse, and topic-consistent."
P19-1193,2019,6 Conclusion,"in addition, the adversarial training based on a multi-label discriminator is employed to further enhance topic-consistency."
P19-1193,2019,6 Conclusion,the proposed model integrates commonsense from the external knowledge base into the generator through a dynamic memory mechanism to enrich the source information.
P19-1193,2019,6 Conclusion,this work presents a memory-augmented neural model with adversarial training for automatic topic-to-essay generation.
P19-1194,2019,6 Conclusion,"in order to train the proposed model without any parallel data, we design a cycle reinforcement learning algorithm."
P19-1194,2019,6 Conclusion,"in this paper, we focus on solving the finegrained text sentiment transfer task, which is a natural extension of the binary sentiment transfer task but with more challenges."
P19-1194,2019,6 Conclusion,"we apply the proposed approach to the yelp review dataset, obtaining state-of-the-art results in both automatic evaluation and human evaluation."
P19-1194,2019,6 Conclusion,we propose a seq2sentiseq model to achieve the aim of controlling the fine-grained sentiment intensity of the generated sentence.
P19-1195,2019,8 Conclusions,"extensive automatic and human evaluation on two benchmarks, rotowire and the newly created mlb, show that our model outperforms competitive baselines and manages to generate plausible output which humans find coherent, concise, and factually correct."
P19-1195,2019,8 Conclusions,"however, we have only scratched the surface; future improvements involve integrating content planning with entity modeling, placing more emphasis on play-by-play, and exploiting dependencies across input tables."
P19-1195,2019,8 Conclusions,in this work we presented a neural model for datato-text generation which creates entity-specific representations (that are dynamically updated) and generates text using hierarchical attention over the input table and entity memory.
P19-1196,2019,5 Conclusion and Future Work,"aside from such syntax templates, in the future, we aim to explore how semantic templates contribute to type description generation."
P19-1196,2019,5 Conclusion and Future Work,experimental results demonstrate that our method achieves state-of-the-art performance over baselines on both datasets while ensuring data fidelity and readability in generated type descriptions.
P19-1196,2019,5 Conclusion and Future Work,"further experiments regarding the effect of templates show that our model is not only controllable through templates, but resilient against wrong templates and able to correct itself."
P19-1196,2019,5 Conclusion and Future Work,"in this paper, we propose a head-modifier template-based type description generation method, powered by a copy mechanism and context gating mechanism."
P19-1196,2019,5 Conclusion and Future Work,we also propose a larger dataset and two metrics designed for this task.
P19-1197,2019,5 Conclusions,"besides, we propose a method to construct a pseudo parallel dataset for the surface realization model, without the need of any structured table."
P19-1197,2019,5 Conclusions,"experiments show that our proposed model can achieve 27.34 bleu score on a biography generation dataset with only 1, 000 parallel data."
P19-1197,2019,5 Conclusions,"in this work, we focus on the low resource tableto-text generation, where only limited parallel data is available."
P19-1197,2019,5 Conclusions,"we separate the generation into two stages, each of which is performed by a model trainable with only a few annotated data."
P19-1198,2019,7 Conclusion,a novel training scheme is proposed which allows the model to perform content reduction and lexical simplification simultaneously through our proposed losses and denoising.
P19-1198,2019,7 Conclusion,experiments were conducted for multiple variants of our system as well as known unsupervised baselines and supervised systems.
P19-1198,2019,7 Conclusion,"in future, we would like to improve the system further by incorporating better architectural designs and training schemes to tackle complex simplification operations."
P19-1198,2019,7 Conclusion,"in this paper, we made a novel attempt towards unsupervised text simplification."
P19-1198,2019,7 Conclusion,"qualitative and quantitative analysis of the outputs for a publicly available test data demonstrate that our models, though unsupervised, can perform better than or competitive to these baselines."
P19-1198,2019,7 Conclusion,we gathered unlabeled corpora containing simple and complex sentences and used them to train our system that is based on a shared encoder and two decoders.
P19-1199,2019,6 Conclusion,experimental results demonstrate the incorporation of syntactic trees is helpful for reconstruction and grammar of generated sentences.
P19-1199,2019,6 Conclusion,"in addition, sivae can perform unsupervised paraphrasing with different syntactic tree templates."
P19-1199,2019,6 Conclusion,"the first version of sivae exploits the dependencies between two latent spaces, while the second version enables syntactically controlled sentence generation by assuming the two priors are independent."
P19-1199,2019,6 Conclusion,"the new lower bound objective accommodates two latent spaces, for jointly encoding and decoding sentences and their syntactic trees."
P19-1199,2019,6 Conclusion,"we present sivae, a novel syntax-infused variation autoencoder architecture for text generation, leveraging constituency parse tree structure as the linguistic prior to generate more fluent and grammatical sentences."
P19-1200,2019,6 Conclusion,"a hierarchy of stochastic layers is employed, where the priors of the latent variables are learned from the data."
P19-1200,2019,6 Conclusion,"consequently, more informative latent codes are manifested, and the generated samples also exhibit superior quality relative to those from several baseline methods."
P19-1200,2019,6 Conclusion,it consists of a multi-level lstm generative network to model the semantic coherence at both the wordand sentence-levels.
P19-1200,2019,6 Conclusion,we introduce a hierarchically-structured variational autoencoder for long text generation.
P19-1201,2019,6 Conclusion,"conala (yin et al., 2018a) and staqc (yao et al., 2018)."
P19-1201,2019,6 Conclusion,extensive experiments demonstrate the effectiveness of our proposed methods.
P19-1201,2019,6 Conclusion,"however, these datasets are noisy and it is hard to train robust models out of them."
P19-1201,2019,6 Conclusion,"in the future, we will further apply dim to learn semantic parser and nl generator from the noisy datasets."
P19-1201,2019,6 Conclusion,"in this work, we propose to jointly train the semantic parser and nl generator by exploiting the structural connections between them."
P19-1201,2019,6 Conclusion,"to overcome the issue of poor labeled corpus for semantic parsing, some automatically mined datasets have been proposed, e.g."
P19-1201,2019,6 Conclusion,we further extend supervised dim to semi-supervised scenario (semidim).
P19-1201,2019,6 Conclusion,"we introduce the method of dim to exploit the duality, and provide a principled way to optimize the dual information."
P19-1202,2019,7 Conclusion,"as a result, our model outperformed the existing models in all evaluation measures."
P19-1202,2019,7 Conclusion,"in this research, we proposed a new data-to-text model that produces a summary text while tracking the salient information that imitates a humanwriting process."
P19-1202,2019,7 Conclusion,we also explored the effects of incorporating writer information to data-to-text models.
P19-1202,2019,7 Conclusion,"with writer information, our model successfully generated highest quality summaries that scored 20.84 points of bleu score."
P19-1203,2019,6 Conclusion,"besides, it would also be interesting to consider using linguistic knowledge such as named entities or part-of-speech tags to improve the coherence of the conversation."
P19-1203,2019,6 Conclusion,"in the future, we would like to explore how to better select the rationale for each question."
P19-1203,2019,6 Conclusion,"in this paper, we introduce the task of conversational question generation (cqg), and propose a novel framework which achieves promising performance on the popular dataset coqa."
P19-1203,2019,6 Conclusion,"moreover, we use the quality of the answers predicted by a qa model as rewards and fine-tune our model via reinforcement learning."
P19-1203,2019,6 Conclusion,we incorporate a dynamic reasoning procedure to the general encoder-decoder model and dynamically update the encoding representations of the inputs.
P19-1204,2019,5 Conclusion,"in the future, we plan to study the effect of other video modalities on the alignment algorithm."
P19-1204,2019,5 Conclusion,we hope our method and dataset will unlock new opportunities for scientific paper summarization.
P19-1204,2019,5 Conclusion,"we propose a novel automatic method to generate training data for scientific papers summarization, based on conference talks given by authors."
P19-1204,2019,5 Conclusion,"we show that the a model trained on our dataset achieves competitive results compared to models trained on human generated summaries, and that the dataset quality satisfies human experts."
P19-1205,2019,5 Conclusion,"finally, an inference algorithm produces the abstractive summaries."
P19-1205,2019,5 Conclusion,"first, the stacked encoder with focus-attention mechanism captures long-term dependencies and local context of input document comprehensively."
P19-1205,2019,5 Conclusion,"in this paper, we propose a novel framework for abstractive document summarization with extended transformer model."
P19-1205,2019,5 Conclusion,our experiments show that the proposed model achieves a significant improvement for abstractive document summarization over previous state-of-the-art baselines.
P19-1205,2019,5 Conclusion,the proposed model consists of a concise pipeline.
P19-1205,2019,5 Conclusion,then the decoder with saliency-selection network distinguishes and condenses the salient information into the output.
P19-1206,2019,6 Conclusion,"in future work, we will make comparisons with those of a humanannotated dataset."
P19-1206,2019,6 Conclusion,"in particular, for relatively long reviews, our model achieved competitive or better performance compared to supervised models."
P19-1206,2019,6 Conclusion,"in this work, we proposed a novel unsupervised end-to-end model to generate an abstractive summary of a single product review while inducing a latent discourse tree."
P19-1206,2019,6 Conclusion,"our model can also be applied to other applications, such as argument mining, because arguments typically have the same discourse structure as reviews."
P19-1206,2019,6 Conclusion,our model can not only generates the summary but also identifies the argumentative structures.
P19-1206,2019,6 Conclusion,the experimental results demonstrated that our model is competitive with or outperforms other unsupervised approaches.
P19-1206,2019,6 Conclusion,"the induced tree shows that the child sentences present additional information about their parent, and the generated summary abstracts the entire review."
P19-1206,2019,6 Conclusion,"unfortunately, we cannot directly compare our induced trees with the output of a discourse parser, which typically splits sentences into elementary discourse units."
P19-1207,2019,6 Conclusion,"an ablation study validates the role of the bi-directional selective layer, and a human evaluation further proves that our model can generate informative, concise, and readable summaries."
P19-1207,2019,6 Conclusion,"extensive evaluations were conducted on a standard benchmark dataset and experimental results show that our model can quickly pick out high-quality templates from the training corpus, laying key foundation for effective article representations and summary generations."
P19-1207,2019,6 Conclusion,"in this paper, we presented a novel bi-directional selective encoding with template (biset) model for abstractive sentence summarization."
P19-1207,2019,6 Conclusion,the enhanced article representations are expected to contribute towards better summarization eventually.
P19-1207,2019,6 Conclusion,the results also show that our model outperforms all the baseline models and sets a new state of the art.
P19-1207,2019,6 Conclusion,"to counteract the verbosity and insufficiency of training data, we proposed to retrieve high-quality existing summaries as templates to assist with source article representations through an ingenious bidirectional selective layer."
P19-1207,2019,6 Conclusion,we also developed the corresponding retrieval and re-ranking modules for obtaining quality templates.
P19-1208,2019,7 Conclusion and Future Work,"another interesting direction is to apply our rl approach on the microblog hashtag annotation problem (wang et al., 2019; gong and zhang, 2016; zhang et al., 2018b)."
P19-1208,2019,7 Conclusion and Future Work,"as a result, it can more robustly evaluate the quality of generated keyphrases."
P19-1208,2019,7 Conclusion and Future Work,empirical studies on real data demonstrate that our deep reinforced models consistently outperform the current state-of-the-art models.
P19-1208,2019,7 Conclusion and Future Work,"in addition, we propose a novel evaluation method which incorporates name variations of the ground-truth keyphrases."
P19-1208,2019,7 Conclusion and Future Work,"in our rl approach, we introduce an adaptive reward function rf1, which encourages the model to generate both sufficient and accurate keyphrases."
P19-1208,2019,7 Conclusion and Future Work,"in this work, we propose the first rl approach to the task of keyphrase generation."
P19-1208,2019,7 Conclusion and Future Work,"one potential future direction is to investigate the performance of other encoder-decoder architectures on keyphrase generation such as transformer (vaswani et al., 2017) with multi-head attention module (li et al., 2018; zhang et al., 2018a)."
P19-1209,2019,6 Conclusion,our framework is founded on the human process of selecting one or two sentences to merge together and it has the potential to bridge the gap between compression and fusion studies.
P19-1209,2019,6 Conclusion,our method provides a promising avenue for domain-specific summarization where content selection and summary generation are only loosely connected to reduce the costs of obtaining massive annotated data.
P19-1209,2019,6 Conclusion,we present an investigation into the feasibility of scoring singletons and pairs according to their likelihoods of producing summary sentences.
P19-1210,2019,5 Conclusions and Future Work,"in the future, we plan to further integrate higher level participant interactions, such as gestures, face expressions, etc."
P19-1210,2019,5 Conclusions and Future Work,"we also plan to construct a larger multimedia meeting summarization corpus to cover more diverse scenarios, building on our previous work (bhattacharya et al., 2019)."
P19-1210,2019,5 Conclusions and Future Work,we develop a multi-modal summarizer to generate natural language summaries for multi-person meetings.
P19-1210,2019,5 Conclusions and Future Work,"we present a multi-modal hierarchical attention mechanism based on vfoa estimation and topic segmentation, and the experiments demonstrate its effectiveness."
P19-1211,2019,6 Summary,"and with very limited target domain labels for fine-tuning, our model performed better than fine-tuning a model trained on the source domain."
P19-1211,2019,6 Summary,"in the future, we would like to understand the usefulness of artificial titles for training the decoder relative to other factors that may impact performance, e.g., how similar the true titles or summaries are in the different domains."
P19-1211,2019,6 Summary,our experiments show that our proposed approach performed best when compared to baseline adaptation techniques when unsupervised.
P19-1211,2019,6 Summary,we investigated unsupervised domain adaptation methods for an encoder-decoder model.
P19-1211,2019,6 Summary,we proposed the use of artificial titles for training a decoder to the target domain vocabulary and style and sequential adversarial domain adaptation to minimize rapid changes of the encoder embedding space.
P19-1212,2019,6 Conclusion,bigpatent can enable future research to build robust systems that generate abstractive and coherent summaries.
P19-1212,2019,6 Conclusion,salient content from the bigpatent summaries is more evenly distributed in the input.
P19-1212,2019,6 Conclusion,"we present the bigpatent dataset with humanwritten abstractive summaries containing fewer and shorter extractive phrases, and a richer discourse structure compared to existing datasets."
P19-1213,2019,6 Conclusions,our proposed task and collected data can therefore be a valuable resource for future extrinsic evaluations of nli models.
P19-1213,2019,6 Conclusions,"we addressed the issue of factual errors in abstractive summaries, a severe problem that we demonstrated to be common even with state-of-the-art models."
P19-1213,2019,6 Conclusions,"while entailment predictions should help with this issue, out-of-the-box nli models do not perform well on the task."
P19-1214,2019,4 Conclusion,"especially, through the switch pre-training task, the model even outperforms the state-of-theart method neusum (zhou et al., 2018)."
P19-1214,2019,4 Conclusion,"experiments on the cnn/dm verify that through the way of pre-training on our proposed tasks, the model can perform better and converge faster when learning on the summarization task."
P19-1214,2019,4 Conclusion,"further analytic experiments show that the document context learned by the document-level self-attention module will benefit the model in summarization task, and the model is not so sensitive to the hyperparameter of the probability to switch sentences."
P19-1214,2019,4 Conclusion,"in this paper, we propose three self-supervised tasks to force the model to learn about the document context, which will benefit the summarization task."
P19-1215,2019,6 Conclusion,"in future work, we intend to examine multitask approaches combining question summarization and question understanding."
P19-1215,2019,6 Conclusion,we also explored data augmentation methods and studied the behavior of abstractive models on this task.
P19-1215,2019,6 Conclusion,"we studied consumer health question summarization and introduced the meqsum corpus of 1k consumer health questions and their summaries, which we make available in the scope of this paper4 ."
P19-1216,2019,6 Conclusion,"furthermore, we presented an approach for the evaluation of context-awareness which may shed light on automatic evaluation for quality of sentence by virtue of pre-trained models."
P19-1216,2019,6 Conclusion,"in the future, we will consider extending the current approach to the single document or multiple document summarization."
P19-1216,2019,6 Conclusion,"in this work, we propose a coarse-to-fine rewriter for multi-sentence compression with a specific focus on improving the quality of compression."
P19-1216,2019,6 Conclusion,"the experimental results show that the proposed method produced more grammatical sentences, meanwhile introducing novel words in the compression."
P19-1217,2019,5 Conclusions and Future Works,"besides, a terminated gate was presented to dynamically determine the uncertain reasoning depth and a reinforcement method was used to train the network."
P19-1217,2019,5 Conclusions and Future Works,experiments on 3 popular data sets demonstrated the efficiency of the approach.
P19-1217,2019,5 Conclusions and Future Works,"for example, given the why question, reasoning process should be stopped when unrelated relation is met, such as transitional relation."
P19-1217,2019,5 Conclusions and Future Works,"in the future, we will expand it to support the questions on text span selection by using the relation type rather than the option as the terminated condition."
P19-1217,2019,5 Conclusions and Future Works,multiple cells were recursively linked to produce an evidence chain in a multi-hop manner.
P19-1217,2019,5 Conclusions and Future Works,such approach is mainly applied to multiple-choice questions now.
P19-1217,2019,5 Conclusions and Future Works,"we decomposed the inference problem into a series of atomic steps, where each was executed by the operation cell designed with prior structure."
P19-1217,2019,5 Conclusions and Future Works,"we have proposed a network to answer generic questions, especially the ones needed reasoning."
P19-1218,2019,6 Conclusion,experimental results showed state-of-the-art performance.
P19-1218,2019,6 Conclusion,"in this paper, we proposed a new dynamic selfattention (dynsa) architecture, which dynamically determinates what tokens are important for constructing intra-passage or cross-passage tokenlevel semantic representations."
P19-1218,2019,6 Conclusion,"the proposed approach has the advantages in remaining fine-grain semantic information meanwhile reaching a balance between time, memory and accuracy."
P19-1218,2019,6 Conclusion,"we showed the effectiveness of the proposed method in handling multi-passage reading comprehension using three benchmark datasets including searchqa, quasar-t, and wikihop."
P19-1219,2019,7 Conclusion,"experimental results show that kar is not only comparable in performance with the state-of-the-art mrc models, but also superior to them in terms of both the hunger for data and the robustness to noise."
P19-1219,2019,7 Conclusion,"in the future, we plan to use some larger knowledge bases, such as conceptnet and freebase, to improve the quality and scope of the general knowledge."
P19-1219,2019,7 Conclusion,"in this paper, we innovatively integrate the neural networks of mrc models with the general knowledge of human beings."
P19-1219,2019,7 Conclusion,"specifically, inter-word semantic connections are first extracted from each given passage-question pair by a wordnet-based data enrichment method, and then provided as general knowledge to an end-to-end mrc model named as knowledge aided reader (kar), which explicitly uses the general knowledge to assist its attention mechanisms."
P19-1220,2019,7 Conclusion,"in particular, the capability of copying words from the question and passages can be shared among the styles, while the capability of controlling the mixture weights for the generative and copy distributions can be acquired for each style."
P19-1220,2019,7 Conclusion,it achieved stateof-the-art performance on the q&a task and the q&a + nlg task of ms marco 2.1 and the summary task of narrativeqa.
P19-1220,2019,7 Conclusion,our future work will involve exploring the potential of our multi-style learning towards natural language understanding.
P19-1220,2019,7 Conclusion,"our proposed model, masque, is based on multi-source abstractive summarization and learns multi-style answers together."
P19-1220,2019,7 Conclusion,the key to its success is transferring the style-independent nlg capability to the target style by use of the question-passages reader and the conditional pointer-generator decoder.
P19-1220,2019,7 Conclusion,this study sheds light on multi-style generative rc.
P19-1221,2019,6 Conclusion,"future work will concentrate on designing a fast neural pruner to replace the ir-based pruning component, developing better end-to-end training strategies, and adapting our approach to other datasets such as natural questions (kwiatkowski et al., 2019)."
P19-1221,2019,6 Conclusion,re3qa outperforms the pipelined baseline with faster inference speed and achieves state-of-the-art results on four challenging reading comprehension datasets.
P19-1221,2019,6 Conclusion,we design three components for each subtask and show that an end-to-end training strategy can bring in additional benefits.
P19-1221,2019,6 Conclusion,"we present re3qa, a unified network that answers questions from multiple documents by conducting the retrieve-read-rerank process."
P19-1222,2019,6 Concluding Remarks,"a promising direction could be a multi-task approach, in which both single- and multi-hop datasets are learned jointly."
P19-1222,2019,6 Concluding Remarks,an interesting improvement to our approach would be to allow the retriever to automatically determine whether or not more retrieval iterations are needed.
P19-1222,2019,6 Concluding Remarks,"one difficulty in the open-domain multi-hop setting is the lack of supervision, a difficulty that in the singlehop setting is alleviated to some extent by using distant supervision."
P19-1222,2019,6 Concluding Remarks,we hope to tackle this problem in future work to allow learning more than two retrieval iterations.
P19-1222,2019,6 Concluding Remarks,"we present muppet, a novel method for multihop paragraph retrieval, and show its efficacy in both single- and multi-hop qa datasets."
P19-1223,2019,5 Conclusion,"e 3 achieved a new state-of-the-art result on the sharc cmr dataset, outperforming existing systems as well as a new extractive qa baseline based on bert."
P19-1223,2019,5 Conclusion,"in addition to achieving strong performance, we showed that e 3 provides a more explainable alternative to prior work which do not model document structure."
P19-1223,2019,5 Conclusion,"we proposed the entailment-driven extract and edit network (e 3 ), a conversational machine reading model that extracts implicit decision rules from text, computes whether each rule is entailed by the conversation history, inquires about rules that are not entailed, and answers the user’s question."
P19-1224,2019,8 Conclusion,"additionally, we hope that our specificity-labeled reading comprehension dataset is useful in other applications such as 1) finer control over question generation systems used in education applications, curiositydriven chatbots and healthcare (du et al., 2017)."
P19-1224,2019,8 Conclusion,"we believe squash is a challenging text generation task and we hope the community finds it useful to benchmark systems built for document understanding, question generation and question answering."
P19-1224,2019,8 Conclusion,we present and evaluate a system which leverages existing reading comprehension datasets to attempt solving this task.
P19-1224,2019,8 Conclusion,"we propose squash, a novel text generation task which converts a document into a hierarchy of qa pairs."
P19-1225,2019,7 Conclusion,our adaptive termination contributes to the exact matching and the precision score of the evidence extraction.
P19-1225,2019,7 Conclusion,"regarding rc, we confirmed that the architecture with qfe, which is a simple replacement of the baseline, achieved state-of-the-art performance in the task setting."
P19-1225,2019,7 Conclusion,"regarding rte, we confirmed that, compared with competing models, the architecture with qfe has a higher evidence extraction score and comparable label prediction score."
P19-1225,2019,7 Conclusion,the ablation study showed that the replacement of the evidence extraction model with qfe improves performance.
P19-1225,2019,7 Conclusion,the difficulty of the questions for qfe depends on the number of the required evidence sentences.
P19-1225,2019,7 Conclusion,this study is the first to base its experimental discussion on hotpotqa.
P19-1225,2019,7 Conclusion,this study is the first to show a joint approach for rc and fever.
P19-1225,2019,7 Conclusion,"we consider that the main contributions of our study are (1) the qfe model that is based on a summarization model for the explainable multihop qa, (2) the dependency among the evidence and the coverage of the question due to the usage of the summarization model, and (3) the state-ofthe-art performance in evidence extraction in both rc and rte tasks."
P19-1226,2019,6 Conclusion,"this paper introduces kt-net for mrc, which enhances bert with structured knowledge from kbs and combines the merits of the both."
P19-1226,2019,6 Conclusion,"this work demonstrates the feasibility of further enhancing advanced lms with knowledge from kbs, which indicates a potential direction for future research."
P19-1226,2019,6 Conclusion,"we learn embeddings for the two kbs, select desired embeddings from them, and fuse the selected embeddings with bert hidden states, so as to enable context- and knowledge-aware predictions.our model achieves significant improvements over previous methods, becoming the best single model on record and squad1.1 benchmarks."
P19-1226,2019,6 Conclusion,we use two kbs: wordnet and nell.
P19-1227,2019,7 Conclusion,"in this paper, we discuss the problem of crosslingual open-domain question answering, and present a novel dataset xqa, which consists of a total amount of 90k question-answer pairs in nine languages."
P19-1227,2019,7 Conclusion,the experimental results show that multilingual bert achieves the best result in almost all target languages.
P19-1227,2019,7 Conclusion,"the performance of translation-based methods can be increased by applying machine translation system that better translates name entities, while the multilingual bert model may be improved by incorporating parallel data with monolingual data."
P19-1227,2019,7 Conclusion,we further examine the performance of two translation-based methods and one zero-shot cross-lingual method on the xqa dataset.
P19-1227,2019,7 Conclusion,we hope our work could contribute to the development of cross-lingual openqa systems and further promote the research of overall cross-lingual language understanding.
P19-1228,2019,7 Conclusion,learning deep generative models which exhibit such conditional markov properties is an interesting direction for future work.
P19-1228,2019,7 Conclusion,the collapsed amortized variational inference approach is general and can be used for generative models which admit tractable inference through partial conditioning.
P19-1228,2019,7 Conclusion,"the latent vector induces marginal dependencies beyond the traditional first-order context-free assumptions within a tree-based generative process, leading to improved performance."
P19-1228,2019,7 Conclusion,"this work explores grammar induction with compound pcfgs, which modulate rule probabilities with per-sentence continuous latent vectors."
P19-1229,2019,6 Conclusions,"meanwhile, more source-domain labeled data usually leads to higher and more consistent improvement, especially when the scale of the targetdomain training data is small."
P19-1229,2019,6 Conclusions,"moreover, detailed analysis shows that enlarging the target-domain labeled data is most effective in boost cross-domain parsing performance."
P19-1229,2019,6 Conclusions,"our proposed semi-supervised domain adaptation approach leads to absolute las improvement of 16.15% (77.16 vs. 61.01) and 15.56% (75.11 vs. 59.55) on pb/zx-test respectively, over the non-adapted parser trained on the source bc-train."
P19-1229,2019,6 Conclusions,"this work addresses the task of semi-supervised domain adaptation for chinese dependency parsing, based on our two newly-annotated large-scale domain-aware data, i.e., pb and zx."
P19-1229,2019,6 Conclusions,"to utilize unlabeled target-domain data, we further propose an effective two-stage approach based on the recently proposed contextualized word representations (elmo)."
P19-1229,2019,6 Conclusions,we propose a simple domain embedding approach with corpus weighting to effectively combine both the sourceand target-domain training data.
P19-1230,2019,6 Conclusions,"despite the usefulness of hpsg in practice and its theoretical linguistic background, our model achieves new state-of-the-art results on both chinese and english benchmark treebanks of both parsing tasks."
P19-1230,2019,6 Conclusions,"our experiments show that joint learning of constituent and dependency is indeed superior to separate learning mode, and combining constituent and dependency score in joint training to parse a simplified hpsg can obtain further performance improvement."
P19-1230,2019,6 Conclusions,this paper presents a simplified hpsg with two different decode methods which are evaluated on both constituent and dependency parsing.
P19-1230,2019,6 Conclusions,"thus, this work is more than proposing a high-performance parsing model by exploring the relation between constituent and dependency structures."
P19-1231,2019,6 Conclusion,and we argue that it can greatly reduce the requirement on sizes of the dictionaries.
P19-1231,2019,6 Conclusion,extensive experimental studies on four ner datasets validate its effectiveness.
P19-1231,2019,6 Conclusion,"in this work, we introduce a novel pu learning algorithm to perform the ner task using only unlabeled data and named entity dictionaries."
P19-1231,2019,6 Conclusion,we prove that this algorithm can unbiasedly and consistently estimate the task loss as if there is fully labeled data.
P19-1232,2019,5 Conclusion,"in our experiments, our multi-task reinforcement ips model achieves a new state of the art for three sdp formalisms."
P19-1232,2019,5 Conclusion,"moreover, we show that fine-tuning with reinforcement learning learns an easy-first strategy and some syntactic features."
P19-1232,2019,5 Conclusion,"we apply multi-task learning to learn general representations of parser configurations, and use reinforcement learning for task-specific fine-tuning."
P19-1232,2019,5 Conclusion,we propose a novel iterative predicate selection (ips) parsing model for semantic dependency parsing.
P19-1233,2019,7 Conclusion,empirical studies on two standard datasets suggest that gcdt outperforms previous state-ofthe-art systems substantially on both conll03 ner task and conll2000 chunking task without additional corpora or task-specific resources.
P19-1233,2019,7 Conclusion,"furthermore, by leveraging bert as an external resource, we report new state-of-the-art f1 scores of 93.47 on conll03 and 97.30 on conll2000."
P19-1233,2019,7 Conclusion,"in the future, we would like to extend gcdt to other analogous sequence labeling tasks and explore its effectiveness on other languages."
P19-1233,2019,7 Conclusion,"we propose a novel hierarchical neural model for sequence labeling tasks (gcdt), which is based on the deep transition architecture and motivated by global contextual representation at the sentence level."
P19-1234,2019,7 Conclusion,"ablation further shows a positive effect of normalizing flow, context embeddings and proposed regularizers."
P19-1234,2019,7 Conclusion,labeled and unlabeled evaluation shows that the pcfg induction model with normalizing flow and context embeddings produces grammars with state-of-the-art accuracy on a variety of different languages.
P19-1234,2019,7 Conclusion,linguistically motivated similarity penalty and categorical distance constraints are also imposed on the inducer as regularization.
P19-1234,2019,7 Conclusion,results show consistent and meaningful use of labels at phrasal and lexical levels by the flow-based model.
P19-1234,2019,7 Conclusion,"this work proposes a neural pcfg inducer which employs context embeddings (peters et al., 2018) in a normalizing flow model (dinh et al., 2015) to extend pcfg induction to use semantic and morphological information."
P19-1235,2019,9 Conclusion,analyses show that vas seems to separate content words from function words in natural language grammars and better arrange words with different frequencies into different classes that are more consistent with these linguistic distinctions.
P19-1235,2019,9 Conclusion,experiments with several linguistically- and psycholinguistically-motivated predictors on a large multilingual data set show that variance of average surprisal (vas) is highly predictive of parsing performance.
P19-1235,2019,9 Conclusion,further evidence shows vas to be a better candidate than data likelihood for predicting word-order typology.
P19-1235,2019,9 Conclusion,this work explores the non-optimality of data likelihood for model selection in unsupervised grammar induction.
P19-1235,2019,9 Conclusion,using it as the criterion for model selection outperforms data likelihood significantly.
P19-1236,2019,5 Conclusion,"experiments on three datasets show that our method is highly effective among supervised domain adaptation methods, while allowing zeroshot learning in unsupervised domain adaptation."
P19-1236,2019,5 Conclusion,"for this goal, cross-domain language modeling is conducted through a novel parameter generation network, which decomposes domain and task knowledge into two sets of embedding vectors."
P19-1236,2019,5 Conclusion,we considered ner domain adaptation by extracting knowledge of domain differences from raw text.
P19-1237,2019,6 Conclusions,"by recursively aggregating the neighbors’ information, our parser can obtain node representation that incorporates high-order features to improve performance."
P19-1237,2019,6 Conclusions,experiments on ptb and ud2.2 datasets show the effectiveness of our proposed method.
P19-1237,2019,6 Conclusions,we propose a novel and efficient dependency parser using the graph neural networks.
P19-1238,2019,6 Conclusion,another important and optimistic result of this investigation is that minimalist grammar parsing is not as slow as may have been expected given its worst case time complexity.
P19-1238,2019,6 Conclusion,"first, the accuracy on recovering syntactic and semantic dependencies predicted by the minimalist syntax is reasonable considering the higher complexity of the mechanisms behind minimalism compared to other formalisms."
P19-1238,2019,6 Conclusion,"however, the gap will likely narrow as the size and quality of mgbank improves and as better probabilistic models are developed enabling these systems to parse a higher number of sentences."
P19-1238,2019,6 Conclusion,"in comparison to ccg, a formalism with a much longer history of widecoverage parsing, performance currently lags behind."
P19-1238,2019,6 Conclusion,"our results show that the combination of a good neural probabilistic model and a* search, together with a strong formal grammar, makes minimalist parsing practical for the majority of sentences."
P19-1238,2019,6 Conclusion,the results of this initial attempt are optimistic.
P19-1238,2019,6 Conclusion,we have presented the first ever wide-coverage transformational grammar parser.
P19-1238,2019,6 Conclusion,worst case complexity results are sometimes raised as a criticism of tg theories.
P19-1239,2019,7 Conclusion and Future Work,evaluation results demonstrate the effectiveness of our proposed model and the usefulness of the three modalities.
P19-1239,2019,7 Conclusion and Future Work,"in future work, we will incorporate other modality such as audio into the sarcasm detection task and we will also investigate to make use of common sense knowledge in our model."
P19-1239,2019,7 Conclusion and Future Work,"in this paper we propose a new hierarchical fusion model to make full use of three modalities (images, texts and image attributes) to address the challenging multi-modal sarcasm detection task."
P19-1240,2019,6 Conclusion and Future Work,"also, our topic-aware neural keyphrase generation model can be investigated in a broader range of text generation tasks."
P19-1240,2019,6 Conclusion and Future Work,"experimental results on three newly constructed social media datasets show that our model significantly outperforms state-of-the-art methods in keyphrase prediction, meanwhile produces more coherent topics."
P19-1240,2019,6 Conclusion and Future Work,further analysis interprets our superiority to discover key information from noisy social media data.
P19-1240,2019,6 Conclusion and Future Work,"in the future, we will explore how to explicitly leverage the topic-word distribution to further improve the performance."
P19-1240,2019,6 Conclusion and Future Work,we have presented a novel social media keyphrase generation model that allows the joint learning of latent topic representations.
P19-1241,2019,8 Conclusion and Future Work,"an extensive comparison shows the merit of using medium-specific language models based on an awd-lstm architecture, along with an augmented vocabulary which is capable of representing deep linguistic subtleties in text that pose challenges to the complex task of detecting sexual harassment disclosure."
P19-1241,2019,8 Conclusion and Future Work,"in this work, we proposed a social media language model, a three part ulmfit architecture, for the task of analyzing disclosures of sexual harassment on social media."
P19-1241,2019,8 Conclusion and Future Work,"on a manually annotated real-world dataset, created in two steps to capture a large demographic, our systems could often achieve significant performance improvements over systems that rely on handcrafted and textual features and generic deep learning based systems."
P19-1241,2019,8 Conclusion and Future Work,our future agenda includes exploring the applicability of our analysis and system for identifying patterns and potential prevention.
P19-1241,2019,8 Conclusion and Future Work,we also hope this study enables further research in terms of how people seek support online on sexual harassment and mental health-related problems.
P19-1241,2019,8 Conclusion and Future Work,we also plan to use this model to solve other downstream medium-specific tasks pertaining to mental health and welfare.
P19-1242,2019,8 Conclusion,"although we focused on english hashtags, our pairwise ranking approach is language-independent and we intend to extend our toolkit to languages other than english as future work."
P19-1242,2019,8 Conclusion,we also constructed a larger and more curated dataset for analyzing and benchmarking hashtag segmentation methods.
P19-1242,2019,8 Conclusion,we demonstrated that hashtag segmentation helps with downstream tasks such as sentiment analysis.
P19-1242,2019,8 Conclusion,we proposed a new pairwise neural ranking model for hashtag segmention and showed significant performance improvements over the state-of-theart.
P19-1243,2019,7 Conclusions and Future Work,"despite this limitation, we find that these models are expressive enough to analyze entity portrayals in in-domain data, allowing us to examine different portrayals of men and women."
P19-1243,2019,7 Conclusions and Future Work,"however, we further expose several limitations to this method, specifically that contextualized word embeddings are biased towards representations from their training data, which limits their usefulness in new domains."
P19-1243,2019,7 Conclusions and Future Work,our results are easy to interpret and readily generalize to a variety of research questions.
P19-1243,2019,7 Conclusions and Future Work,"we leave alternative solutions for future work, including training embeddings from scratch or fine-tuning on the target corpus (however, these ideas are only feasible with a large target corpus, and the need for fine-tuning reduces the usefulness of pre-trained embeddings)."
P19-1243,2019,7 Conclusions and Future Work,"we propose a method for incorporating contextualized word embeddings into entity-centric analyses, which has direct applications to numerous social science tasks."
P19-1243,2019,7 Conclusions and Future Work,"while we explore masking target words as a possible solution to this problem, we find that masking significantly decreases performance."
P19-1244,2019,6 Conclusions and Future Work,"for the future work, beyond what we have mentioned, we plan to examine our model on different information sources."
P19-1244,2019,6 Conclusions and Future Work,our model strengthens the evidence representations by attending on the sentences that are not only topically coherent but can also semantically infer the target claim.
P19-1244,2019,6 Conclusions and Future Work,the results on three public benchmark datasets confirm the advantages of our method.
P19-1244,2019,6 Conclusions and Future Work,we propose a novel neural end-to-end framework for claim verification by learning to embed sentence-level evidence with a hierarchical attention mechanism.
P19-1244,2019,6 Conclusions and Future Work,"we will also try to incorporate relevant metadata into it, e.g., author profile, website credibility, etc."
P19-1245,2019,5 Conclusions,"in this paper, we addressed the task of predicting human activities from user-generated content."
P19-1245,2019,5 Conclusions,"this is evidence that the task is feasible, but very difficult, and it could benefit from further investigation."
P19-1245,2019,5 Conclusions,"using sentence embedding models, we projected activity instances into a vector space and perform clustering in order to learn about the high-level groups of behaviors that are commonly mentioned online."
P19-1245,2019,5 Conclusions,"we collected a large twitter dataset consisting of posts from more than 200,000 users mentioning at least one of the nearly 30,000 everyday activities that we explored."
P19-1245,2019,5 Conclusions,"we make the activity clusters, models, and code for the prediction task available at http://lit.eecs.umich.edu/downloads.html"
P19-1245,2019,5 Conclusions,"we trained predictive models to make inferences about the likelihood that a user had reported to have done activities across the range of clusters that we discovered, and found that these models were able to achieve results significantly higher than random guessing baselines for the metrics that we consider."
P19-1245,2019,5 Conclusions,"while the overall prediction scores are not very high, the models that we trained do show that they are able to generalize findings from one set of users to another."
P19-1246,2019,9 Conclusion,"first, as has been noted (plank et al., 2016), neural networks can perform well with relatively small datasets, in this case proving competitive with the sparse models that are usually favoured in author profiling (malmasi et al., 2017; basile et al., 2018)."
P19-1246,2019,9 Conclusion,"first, we wish to investigate the role of additional variables (such as age and gender)."
P19-1246,2019,9 Conclusion,"from a methodological point of view, we can draw two conclusions from this work."
P19-1246,2019,9 Conclusion,"in contrast, syntactic information is almost as predictive and is a much better signal for stylistic variation."
P19-1246,2019,9 Conclusion,"inspired by labov and encouraged by recent interest in computational sociolinguistics, we developed accurate neural models to predict socioeconomic status from text."
P19-1246,2019,9 Conclusion,"nevertheless, there are limitations of distant labelling and social media data — with issues related specifically to the language of food (askalidis and malthouse, 2016) — that we will take into account in future work."
P19-1246,2019,9 Conclusion,"second, distant supervision with proxy labels for socio-economic status yields useful insights and is validated externally via readability scores."
P19-1246,2019,9 Conclusion,"second, we will take steps to mitigate the risk of fake reviews and validate the distant labelling with human annotation."
P19-1246,2019,9 Conclusion,this is encouraging for further studies in computational social science in ecologically valid and relatively labour-free settings.
P19-1246,2019,9 Conclusion,"while lexical information is highly predictive, it is restricted to topic."
P19-1247,2019,7 Conclusion,in this paper we follow the intuition that the political perspectives expressed in news articles will also be reflected in the way the documents spread and the identity of the users who endorse them.
P19-1247,2019,7 Conclusion,modeling the broader context in which text is consumed is a vital step towards getting a better understanding of its perspective.
P19-1247,2019,7 Conclusion,we also study this approach in the supervised setting and show that it can significantly enhance a text-only classification model.
P19-1247,2019,7 Conclusion,"we intend to study fine-grained political perspectives, capturing how different events are framed."
P19-1247,2019,7 Conclusion,"we suggest a gcn-based model capturing this social information, and show that it provides a distant supervision signal, resulting in a model performing comparably to supervised text classification models."
P19-1248,2019,6 Conclusions and Future Work,"for example, revising spoiler contents in a ‘non-spoiler’ way would be an interesting language generation task."
P19-1248,2019,6 Conclusions and Future Work,"in addition to review semantics, syntax information could be incorporated in a spoiler language model."
P19-1248,2019,6 Conclusions and Future Work,models and knowledge learned on this dataset could be transferred to other corpora where spoiler annotations are limited or unavailable (e.g.detecting spoilers from tweets).
P19-1248,2019,6 Conclusions and Future Work,"our new dataset, analysis of spoiler language, and positive results facilitate several directions for future work."
P19-1248,2019,6 Conclusions and Future Work,the goodreads dataset may also serve as a powerful spoiler source corpus.
P19-1249,2019,5 Conclusion,"in future work, we plan on improving the corpus by incorporating verified accounts from other social networks, and, by inferring new labels for as of yet unlabeled celebrities through link prediction."
P19-1249,2019,5 Conclusion,its generalizability qualities for gender prediction have been demonstrated using state-of-the-art approaches.
P19-1249,2019,5 Conclusion,"its quality is due to twitter’s verification process, wikidata’s accuracy, and our low-error linking strategy between the two sites."
P19-1249,2019,5 Conclusion,"our corpus formed the basis for the first celebrity profiling competition, organized as part of the pan evaluation lab (wiegmann et al., 2019)."
P19-1249,2019,5 Conclusion,"the traits studied were the degree of fame, occupation, age, and gender, introducing fame and occupations as novel, celebrity-specific profiling traits, and revisiting the well-known traits age and gender."
P19-1249,2019,5 Conclusion,"this paper introduces the webis celebrity corpus 2019, the first corpus of its kind comprising a total of 71,706 celebrity profiles, 239 profilingrelevant labels, and 3 billion words."
P19-1250,2019,5 Conclusion,experimental results suggested that pairwise ranking models work well with the variation of comments rather than articles.
P19-1250,2019,5 Conclusion,our future work will include efficiently labeling promising comments via active learning.
P19-1250,2019,5 Conclusion,we created a new labeled dataset for ranking constructive comments.
P19-1251,2019,4 Conclusions and Discussions,"after filtering irrelevant tweets, a cnn derives a feature vector for each tweet with max-over-time pooling."
P19-1251,2019,4 Conclusions and Discussions,"finally, the corpus representation can be taken into account to predict air quality with historical measurements."
P19-1251,2019,4 Conclusions and Discussions,"in this paper, we propose a novel framework for leveraging social media and nlp to air quality prediction."
P19-1251,2019,4 Conclusions and Discussions,the results of extensive experiments show that our proposed approach significantly outperforms two comparative baseline methods across both balanced and unbalanced datasets for different locations in the usa.
P19-1251,2019,4 Conclusions and Discussions,"this is because: (1) most noisy and irrelevant tweets are effectively filtered in the stage of tweet filtering; (2) the convolutional neural network and the proposed max-over-tweets are able to extract essential knowledge about air quality prediction from myriad tweets in social media; (3) there are some limitations on only using historical measurements, such as the capability of recognizing real-world events."
P19-1251,2019,4 Conclusions and Discussions,we also propose the novel max-over-tweet pooling to aggregate the feature vectors of all tweets over numerous hidden topics.
P19-1252,2019,5 Conclusion and Future Work,"directions of future research include adaptation of our methods to a large scale, sparsely connected social network."
P19-1252,2019,5 Conclusion and Future Work,"one might also want to investigate the inductive settings of gcn (hamilton et al., 2017) to predict demographic information of a user from outside the black network."
P19-1252,2019,5 Conclusion and Future Work,previous works have used tweets or a fraction of the network information to extract features for occupation classification.
P19-1252,2019,5 Conclusion and Future Work,"to analyze the importance of network information, we extended an existing twitter dataset for a user’s social media connections (follow information)."
P19-1252,2019,5 Conclusion and Future Work,"we showed that by using only follow information as an input to graph convolutional networks, one can achieve a significantly higher accuracy on the prediction task as compared to the existing approaches utilizing tweet-only information or partial network structure."
P19-1253,2019,7 Conclusion and Future Work,daml is an effective and robust method for training dialog systems with low-resources.
P19-1253,2019,7 Conclusion and Future Work,daml reaches the state-of-the-art performance in entity f1 compared with a zero-shot learning method and a transfer learning method.
P19-1253,2019,7 Conclusion and Future Work,"the daml also provides promising potential extension, such as applying daml on reinforcement learning-based dialog system."
P19-1253,2019,7 Conclusion and Future Work,we also construct an end-to-end trainable dialog system that utilizes a two-step gradient update to obtain models that are more sensitive to new domains.
P19-1253,2019,7 Conclusion and Future Work,we also plan to adapt daml to multi-domain dialog tasks.
P19-1253,2019,7 Conclusion and Future Work,we evaluate our model on a simulated dataset with multiple independent domains.
P19-1253,2019,7 Conclusion and Future Work,we propose a domain adaptive dialog generation method based on meta-learning(daml).
P19-1254,2019,8 Conclusion,we proposed an effective method for writing short stories by separating the generation of actions and entities.
P19-1254,2019,8 Conclusion,we show through human evaluation and automated metrics that our novel decomposition improves story quality.
P19-1255,2019,8 Conclusion,"an argument generation component then employs a text planning decoder to conduct content selection and specify a suitable language style at sentence-level, followed by a content realization decoder to produce the final argument."
P19-1255,2019,8 Conclusion,"automatic evaluation and human evaluation indicate that our model generates more proper arguments with richer content than nontrivial comparisons, with comparable fluency to human-edited content."
P19-1255,2019,8 Conclusion,"given an input statement, it first retrieves arguments of different perspectives from millions of high-quality articles collected from diverse sources."
P19-1255,2019,8 Conclusion,"we present a novel counter-argument generation framework, candela."
P19-1256,2019,4 Discussion,additional modules for lexical choices should be expected for a refined system.
P19-1256,2019,4 Discussion,"an example case is in table 6, where the original pricerange=20-25 is refined to be pricerange=moderate, which enhances the correspondence between the mr and the text but sidesteps the lexical choice for numbers which requires localised numerical commonsense."
P19-1256,2019,4 Discussion,"however, the currently presented approach still has two clear limitations."
P19-1256,2019,4 Discussion,"in this paper, we present a simple recipe to reduce the hallucination problem in neural language generation: introducing a language understanding module to implement confidence-based iterative data refinement."
P19-1256,2019,4 Discussion,"one is that this simple approach is implicitly built on an assumption of a moderate level of noise in the original data, which makes it possible to bootstrap a well-calibrated nlu module."
P19-1256,2019,4 Discussion,"the other limitation of this preliminary work is that it currently overlooks the challenges of lexical choices for quantities, degrees, temporal expressions, etc, which are rather difficult to learn merely from data and should require additional commonsense knowledge."
P19-1256,2019,4 Discussion,"we are still on the way to find out solutions for cases with huge noise (perezbeltrachini and lapata, 2018; wiseman et al., 2017), where heavy manual intervention or external knowledge should be desperately needed."
P19-1256,2019,4 Discussion,we find that our proposed method can effectively reduce the noise in the original mr-text pairs from the e2e dataset and improve the content coverage for standard neural surface realisation (no focus on content selection).
P19-1257,2019,6 Conclusion,experimental results show that our approach can substantially outperform various competitive baselines.
P19-1257,2019,6 Conclusion,"further analysis demonstrates that with multiple modal information and co-attention, the generated comments are more diverse and informative."
P19-1257,2019,6 Conclusion,"furthermore, an effective co-attention model is presented to capture the intrinsic interaction between multiple modal contents."
P19-1257,2019,6 Conclusion,"in this paper, we propose the task of cross-modal automatic commenting, which aims at enabling the ai agent to make comments by integrating multiple modal contents."
P19-1257,2019,6 Conclusion,we construct a largescale dataset for this task and implement plenty of representative neural models.
P19-1258,2019,4 Conclusion,"finally, the improved performance on two task-oriented datasets demonstrates the contributions from the separated storage and the reasoning ability of working memory."
P19-1258,2019,4 Conclusion,"first, the storage separation of the dialog history and kb information is very important and we explore two context-sensitive perceptual processes for the word-level representations of the dialog history."
P19-1258,2019,4 Conclusion,our future work will focus on how to transfer the long-term memory across different tasks.
P19-1258,2019,4 Conclusion,"second, working memory is adopted to interact with the long-term memories and then generate the responses."
P19-1258,2019,4 Conclusion,we leverage the knowledge from the psychological studies and propose our wmm2seq for dialog response generation.
P19-1259,2019,6 Discussion and Conclusion,"benefiting from the explicit structure in the cognitive graph, system 2 in cogqa has potential to leverage neural logic techniques to improve reliability."
P19-1259,2019,6 Discussion and Conclusion,"finally, we believe that our framework can generalize to other cognitive tasks, such as conversational ai and sequential recommendation."
P19-1259,2019,6 Discussion and Conclusion,"moreover, we expect that prospective architectures combining attention and recurrent mechanisms will largely improve the capacity of system 1 by optimizing the interaction between systems."
P19-1259,2019,6 Discussion and Conclusion,multiple future research directions may be envisioned.
P19-1259,2019,6 Discussion and Conclusion,"our implementation based on bert and gnn obtains state-of-art results on hotpotqa dataset, which shows the efficacy of our framework."
P19-1259,2019,6 Discussion and Conclusion,"the reasoning process is organized as cognitive graph, reaching unprecedented entity-level explainability."
P19-1259,2019,6 Discussion and Conclusion,we present a new framework cogqa to tackle multi-hop machine reading problem at scale.
P19-1260,2019,5 Conclusion,"evaluated on wikihop, our end-to-end trained single neural model delivers competitive results while our ensemble model achieves the state-of-the-art performance."
P19-1260,2019,5 Conclusion,"in the future, we would like to investigate explainable gnn for this task, such as explicit reasoning path in (kundu et al., 2018), and work on other data sets such as hotpotqa."
P19-1260,2019,5 Conclusion,"we introduce the hde graph, a heterogeneous graph for multiple-hop reasoning over nodes representing different granularity levels of information."
P19-1260,2019,5 Conclusion,we propose a new gnn-based method for multihop rc across multiple documents.
P19-1260,2019,5 Conclusion,"we use co-attention and self-attention to encode candidates, documents, entities of mentions of candidates and query subjects into query-aware representations, which are then employed to initialize graph node representations."
P19-1261,2019,6 Conclusion,"on medhop, our system outperforms all previously published models on the leaderboard test set."
P19-1261,2019,6 Conclusion,"on wikihop, our system outperforms all published models on the dev set, and achieves results competitive with the current stateof-the-art on the test set."
P19-1261,2019,6 Conclusion,we also presented multiple reasoning-chain recovery tests for the explainability of our system’s reasoning capabilities.
P19-1261,2019,6 Conclusion,"we presented an interpretable 3-module, multihop, reading-comprehension system ‘epar’ which constructs a ‘reasoning tree’, proposes an answer candidate for every root-to-leaf chain, and merges key information from all reasoning chains to make the final prediction."
P19-1262,2019,8 Conclusion,"in this work, we identified reasoning shortcuts in the hotpotqa dataset where the model can locate the answer without multi-hop reasoning."
P19-1262,2019,8 Conclusion,"overall, we hope that these insights and initial improvements will motivate the development of new models that combine explicit compositional reasoning with adversarial training."
P19-1262,2019,8 Conclusion,"trained on the regular data, this 2-hop model is more robust against the adversary than the baseline; and after being trained with adversarial data, this model achieved further improvements on the adversarial evaluation and also outperforms the baseline."
P19-1262,2019,8 Conclusion,"we constructed adversarial documents that can fool the models exploiting the shortcut, and found that the performance of a state-of-the-art model dropped significantly under our adversarial examples."
P19-1262,2019,8 Conclusion,we next proposed to use a control unit that dynamically attends to the question to guide the bi-attention in multi-hop reasoning.
P19-1262,2019,8 Conclusion,we showed that this baseline can improve on the adversarial evaluation after being trained on the adversarial data.
P19-1263,2019,7 Conclusion,"importantly, we illustrate how our model can explain its reasoning via explicit paths extracted across multiple passages."
P19-1263,2019,7 Conclusion,"we present a novel, path-based, multi-hop reading comprehension model that outperforms previous models on wikihop and openbookqa."
P19-1263,2019,7 Conclusion,"while we focused on 2-hop reasoning required by our evaluation datasets, the approach can be generalized to longer chains and to longer natural language questions."
P19-1264,2019,7 Conclusion,"moreover, we find these metrics can be used to generate text; summaries generated with sms as a reward are of better quality than ones generated with rougel, according to both automatic and human evaluations."
P19-1264,2019,7 Conclusion,the metrics’ gain over rouge-l is consistent across word embedding types; there is no significant difference between type-based and contextual embeddings.
P19-1264,2019,7 Conclusion,"we find including sentence embeddings in automatic metrics significantly improves scores’ correlation with human judgments, both on automatically generated and human-authored texts."
P19-1264,2019,7 Conclusion,"we present sms and s+wms, sentence mover’s similarity metrics for automatically evaluating multi-sentence texts."
P19-1265,2019,6 Conclusion,"based on our experiments on training suggestion models, we propose for future annotation studies that annotation suggestions can be given after having annotated only a small amount of data (in our case 70 texts), which ensures a sufficient model performance (0.5 macro-f1)."
P19-1265,2019,6 Conclusion,"if computational resources are an important factor, we propose the usage of inc training with a bundle size of 30 or higher to optimise performance and training time."
P19-1265,2019,6 Conclusion,"if model performance is more important, we recommend cum training using a small bundle size of 10 or 20 to improve suggestions in short intervals."
P19-1265,2019,6 Conclusion,"in our model adjustment experiments, we used gold annotations."
P19-1265,2019,6 Conclusion,"our results show that even mediocre suggestion models have a positive effect in terms of agreement between annotators and annotation speed, while annotation biases are negligible."
P19-1265,2019,6 Conclusion,"since the exact number of texts required to reach sufficient model performance depends on the task, we suggest using continuous model adjustments from the start, ensuring flexibility as to when to start giving suggestions (namely whenever sufficient performance is achieved)."
P19-1265,2019,6 Conclusion,"to create them on the fly, annotation aggregation methods for sequence labelling (simpson and gurevych, 2018) can be used."
P19-1265,2019,6 Conclusion,"we expect our work to have a large impact on future work requiring expert annotations, in particular regarding new tasks with no or little available data, for example for legal (nazarenko et al., 2018), chemical (guo et al., 2014), or psychiatric (mieskes and stiegelmayr, 2018) text processing."
P19-1265,2019,6 Conclusion,"we presented the first study of annotation suggestions for discourse-level sequence labelling requiring expert annotators, using the hard task of epistemic activity identification as an example."
P19-1266,2019,7 Conclusions,"having released our code, we hope this will become a new evaluation standard in the nlp community."
P19-1266,2019,7 Conclusions,we analyzed the extensive experimental setup of reimers and gurevych (2017a) and demonstrated the effectiveness of our proposed test.
P19-1266,2019,7 Conclusions,we considered the comparison of two dnns based on their test-set score distribution.
P19-1266,2019,7 Conclusions,"we defined three criteria for a high quality comparison method, demonstrated that previous methods do not meet these criteria and proposed to use the recently proposed test for almost stochastic dominance that does meet these criteria."
P19-1267,2019,5 Conclusions,"in the meantime, we must depend on system comparison, backed by statistical best practices and error analysis, to make forward progress on this task."
P19-1267,2019,5 Conclusions,it is said that statistical praxis is of greatest import in those areas of science least informed by theory.
P19-1267,2019,5 Conclusions,"we demonstrate that standard practices in system comparison, and in particular, the use of a single standard split, may result in avoidable type i error."
P19-1267,2019,5 Conclusions,we suggest that practitioners who wish to firmly establish that a new system is truly state-ofthe-art augment their evaluations with bonferronicorrected random split hypothesis testing.
P19-1267,2019,5 Conclusions,"while linguistic theory and statistical learning theory both have much to contribute to part-ofspeech tagging, we still lack a theory of the tagging task rich enough to guide hypothesis formation."
P19-1268,2019,6 Conclusion,"in order to encourage the development of models that perform well on difficult items, we propose to use non-obvious f1 scores (f1n) as a complementary ranking metric for model evaluation."
P19-1268,2019,6 Conclusion,recently proposed models perform significantly worse on nonobvious cases compared to obvious cases.
P19-1268,2019,6 Conclusion,we also recommend publishing prediction files along with models to facilitate error analysis.
P19-1268,2019,6 Conclusion,we find that more than 50% of cases in current datasets are relatively obvious.
P19-1268,2019,6 Conclusion,we present an automated criterion for automatically distinguishing between easy and difficult items in text pair similarity prediction tasks.
P19-1269,2019,6 Conclusion,"we show that contextual embeddings are very useful for evaluation, even in simple untrained models, as well as in deeper attention based methods."
P19-1269,2019,6 Conclusion,"when trained on a larger, much noisier range of instances, we demonstrate a substantial improvement over the state of the art."
P19-1270,2019,6 Conclusion,a novel neural framework is proposed for learning the interactions between two source of information.
P19-1270,2019,6 Conclusion,experimental results on two large-scale datasets from twitter and reddit show that our model with bi-attention yields better performance than the previous state of the art.
P19-1270,2019,6 Conclusion,further discussions show that the model learns meaningful representations from conversation context and user history and hence exhibits consistent better performance given varying lengths of context or history.
P19-1270,2019,6 Conclusion,"our proposed model is observed to outperform humans, benefiting from its effective learning from large-scaled data."
P19-1270,2019,6 Conclusion,we also conduct a human study on the first re-entry prediction task.
P19-1270,2019,6 Conclusion,we study the joint effects of conversation context and user chatting history for re-entry prediction.
P19-1271,2019,5 Conclusion,"although a fast and effective responding mechanism can benefit from an automatic generation system, the lack of large datasets of appropriate counter-narratives hinders tackling the problem through supervised approaches such as deep learning."
P19-1271,2019,5 Conclusion,"as future work, we intend to continue collecting more data for islam and to include other hate targets such as migrants or lgbt+, in order to put the dataset at the service of other organizations and further research."
P19-1271,2019,5 Conclusion,"as online hate content rises massively, responding to it with counter-narratives as a combating strategy draws the attention of international organizations."
P19-1271,2019,5 Conclusion,"finally, we expanded the dataset through translation and paraphrasing."
P19-1271,2019,5 Conclusion,"in this paper, we described conan: the first large-scale, multilingual, and expert-based hate speech/counter-narrative dataset for english, french, and italian."
P19-1271,2019,5 Conclusion,"moreover, as a future direction, we want to utilize conan dataset to develop a counter-narrative generation tool that can support ngos in fighting hate speech online, considering counter-narrative type as an input feature."
P19-1271,2019,5 Conclusion,the dataset consists of 4078 pairs over the 3 languages.
P19-1271,2019,5 Conclusion,"together with the collected data we also provided several types of metadata: expert demographics, hate speech sub-topic and counter-narrative type."
P19-1272,2019,8 Conclusions,"future work will look deeper into using the similarity between the content of the text and image (leong and mihalcea, 2011), as the text task results showed room for improvements."
P19-1272,2019,8 Conclusions,"the frequency of use is influenced by the age of the poster, with younger users employing images with a more prominent role in the tweet, rather than just being redundant to the text or as a means of illustrating it."
P19-1272,2019,8 Conclusions,we defined and analyzed quantitatively and qualitatively the semantic relationships between the text and the image of the same tweet using a novel annotated data set.
P19-1272,2019,8 Conclusions,"we developed models that use both text and image features to classify the text-image relationship, with especially high performance (f1 = 0.81) in identifying if the image is redundant, which is immediately useful for downstream applications that maximize screen estate for users."
P19-1272,2019,8 Conclusions,"we envision that our data, task and classifiers will be useful as a preprocessing step in collecting data for training large scale models for image captioning (feng and lapata, 2010) or tagging (mahajan et al., 2018) or for improving recommendations (chen et al., 2016) by filtering out tweets where the text and image have no semantic overlap or can enable new tasks such as identifying tweets that contain creative descriptions for images."
P19-1272,2019,8 Conclusions,"we studied the correlation between the content in the text and relation with the image, highlighting a differentiation between relationship types, even if only using the text of the tweet alone."
P19-1273,2019,7 Conclusion,"a carefully laid out task analysis, as put forward in this paper, provides the basis for exploring more interactive “mixed methods” frameworks (see the discussion in kuhn (to appear)): computational models for a given set of claim categories can feed semi-automatic corpus annotation through manual post correction of predictions."
P19-1273,2019,7 Conclusion,"finally, an interleaved cross-disciplinary collaboration may support the future research process further: the claim ontology for a new field of debate could be constructed in a bootstrapping process, combining the political scientists’ analytical insights with (preliminary) predictions of computational seed models from partially overlapping fields."
P19-1273,2019,7 Conclusion,"in our collaboration, systematic tool support has already made the process of codebook development considerably more effective."
P19-1273,2019,7 Conclusion,"in the broader picture of a project that derives its motivation both from nlp and from css, scaling the computational component is an important objective, but one that should never come at the cost of reliability of the analytical components and methodological validity from the point of view of political science."
P19-1273,2019,7 Conclusion,"in this paper, we have sketched the way towards a computational social science (css) framework for the construction of discourse networks (claims and actors) from news coverage of political debates, which has great potential for expanding the empirical basis for research in political science."
P19-1273,2019,7 Conclusion,"the complexity of the scenario (fine-grained categories, multi-category claims, complex relations, aggregation) suggests that an attempt at automating the construction in its entirety is currently not realistic at a quality that makes it useful for political scientists."
P19-1274,2019,7 Conclusions,"even when tested on held-out user accounts, our predictive model of owner tweets reaches an average performance of .741 auc."
P19-1274,2019,7 Conclusions,"future work could study other types of accounts with similar posting behaviors such as organizational accounts, explore other sources for ground truth tweet identity information (robinson, 2016) or study the effects of user traits such as gender or political affiliation in tweeting signed content."
P19-1274,2019,7 Conclusions,"past research on predicting and studying twitter account characteristics such as type or personal traits (e.g., gender, age) assumed that the same person is authoring all posts from that account."
P19-1274,2019,7 Conclusions,this study introduced a novel application of nlp: predicting if tweets from an account are attributed to their owner or to staffers.
P19-1274,2019,7 Conclusions,"using a novel data set, we showed that owner attributed tweets exhibit distinct linguistic patterns to those attributed to staffers."
P19-1275,2019,6 Conclusion,"we account for discrepancy by reference to the different type of sarcasm captured by the two labelling methods, attributed by previous research in linguistics and psycholinguistics (rockwell and theriot, 2001b; pexman, 2005) to socio-cultural differences between the author and the audience."
P19-1275,2019,6 Conclusion,we studied the predictive power of user embeddings in textual sarcasm detection across datasets labelled via both manual labelling and distant supervision.
P19-1275,2019,6 Conclusion,we suggest a future research direction in sarcasm detection where the two types of sarcasm are treated as separate phenomena and socio-cultural differences are taken into account.
P19-1275,2019,6 Conclusion,"we suggested several neural models to build user embeddings, achieving state-of-the-art results for distant supervision, but not for manual labelling."
P19-1276,2019,6 Conclusion,"a novel latent variable neural model was investigated, which explores latent event type vectors and entity mention redundancy."
P19-1276,2019,6 Conclusion,"in addition, gnbusiness dataset, a largescale dataset annotated with diverse event types and explainable event schemas, is released along with this paper."
P19-1276,2019,6 Conclusion,"to our knowledge, we are the first to use neural latent variable model for inducing event schemas and extracting events."
P19-1276,2019,6 Conclusion,"we presented the task of open domain event extraction, extracting unconstraint types of events from news clusters."
P19-1277,2019,6 Conclusions,"experiments have demonstrated the effectiveness of our proposed model, which achieves state-of-theart performance on the fewrel dataset."
P19-1277,2019,6 Conclusions,"finally, a learnable mlp matching function is employed to calculate the class matching score between the query instance and each candidate class."
P19-1277,2019,6 Conclusions,"first, the query and support instances are encoded interactively via local matching and aggregation."
P19-1277,2019,6 Conclusions,"furthermore, an additional objective function is designed to improve the consistency among the vector representations of all support instances in a class."
P19-1277,2019,6 Conclusions,"in this paper, a neural network with multi-level matching and aggregation has been proposed for few-shot relation classification."
P19-1277,2019,6 Conclusions,studying few-shot relation classification with data generated by distant supervision and extending our mlman model to zero-shot learning will be the tasks of our future work.
P19-1277,2019,6 Conclusions,"then, the support instances in a class are further aggregated to form the class prototype and the weights are calculated by attention-based instance matching."
P19-1278,2019,11 Conclusion and Future Work,"in this paper, we introduce an effective method to quantify the relation similarity and provide analysis and a survey of applications."
P19-1278,2019,11 Conclusion and Future Work,"we note that there are a wide range of future directions: (1) human prior knowledge could be incorporated into the similarity quantification; (2) similarity between relations could also be considered in multi-modal settings, e.g., extracting relations from images, videos, or even from audios; (3) by analyzing the distributions corresponding to different relations, one can also find some “meta-relations” between relations, such as hypernymy and hyponymy."
P19-1279,2019,6 Conclusion and Future Work,"in addition, we show how the new model is particularly effective in low-resource regimes, and we argue that it could significantly reduce the amount of human effort required to create relation extractors."
P19-1279,2019,6 Conclusion and Future Work,"in future work, we plan to work on relation discovery by clustering relation statements that have similar representations according to bertem+mtb."
P19-1279,2019,6 Conclusion and Future Work,in this paper we study the problem of producing useful relation representations directly from text.
P19-1279,2019,6 Conclusion and Future Work,"this is inspired by recent work in knowledge base embedding (bordes et al., 2013; nickel et al., 2016)."
P19-1279,2019,6 Conclusion and Future Work,this would take us some of the way toward our goal of truly general purpose relation identification and extraction.
P19-1279,2019,6 Conclusion and Future Work,"we describe a novel training setup, which we call matching the blanks, which relies solely on entity resolution annotations."
P19-1279,2019,6 Conclusion and Future Work,we will also study representations of relations and entities that can be used to store relation triples in a distributed knowledge base.
P19-1279,2019,6 Conclusion and Future Work,"when coupled with a new architecture for fine-tuning relation representations in bert, our models achieves state-ofthe-art results on three relation extraction tasks, and outperforms human accuracy on few-shot relation matching."
P19-1280,2019,8 Conclusion,we presented a novel semantic framework for modeling fine-grained temporal relations and event durations that maps pairs of events to realvalued scales for the purpose of constructing document-level event timelines.
P19-1280,2019,8 Conclusion,"we used this dataset to train models for jointly predicting fine-grained temporal relations and event durations, reporting strong results on our data and showing the efficacy of a transfer-learning approach for predicting standard, categorical timeml relations."
P19-1280,2019,8 Conclusion,we used this framework to construct the largest temporal relations dataset to date – uds-t – covering the entirety of the ud-ewt.
P19-1281,2019,6 Conclusions,"although we have demonstrated that our algorithms perform well on a complex model selection problem typical of nlp, there is still work to be done to create algorithms more suited to these problems."
P19-1281,2019,6 Conclusions,"crucially, our research further calls into question the current practice in nlp evaluation as used in the literature and international competitions such as semeval."
P19-1281,2019,6 Conclusions,"future research directions include making selection routines more robust to evaluation outliers, relaxing our gaussian assumptions and developing more effective batch strategies."
P19-1281,2019,6 Conclusions,"our algorithms adaptively allocate resources to evaluate promising models, basing evaluations across multiple random seeds and train-test splits."
P19-1281,2019,6 Conclusions,"the aim of this paper has been to propose three algorithms for model selection in nlp, providing efficient and reliable selection for two distinct realistic scenarios: fixed confidence and fixed budget model selection."
P19-1281,2019,6 Conclusions,we demonstrate that this allows significant computational savings and improves reliability over current model selection approaches.
P19-1282,2019,7 Related and Future Work,"finally, another direction for future work would be to extend the importance-ranking comparisons that we deploy here for evaluation purposes into a method for deriving better, more informative rankings, which in turn could be useful for the development of new, more interpretable models."
P19-1282,2019,7 Related and Future Work,"for example, recent work has focused on which training instances (koh and liang, 2017) or which human-interpretable features were most relevant for a particular decision (ribeiro et al., 2016; arras et al., 2016)."
P19-1282,2019,7 Related and Future Work,"others have explored alternative ways of comparing the behavior of proposed explanation methods (adebayo et al., 2018)."
P19-1282,2019,7 Related and Future Work,"we have adopted an erasure-based approach to probing the interpretability of computed attention weights, but there are many other possible approaches."
P19-1282,2019,7 Related and Future Work,"yet another line of work focuses on aligning models with human feedback for what is interpretable (fyshe et al., 2015; subramanian et al., 2017), which could refine our idea of what defines a highquality explanation derived from attention."
P19-1283,2019,6 Conclusion,a toolkit with the implementation of our methods is available at https://github.com/gchrupala/ursa.
P19-1283,2019,6 Conclusion,"our results on arithmetic expressions confirm that both versions of structured rsa capture correlations between different representation spaces, while providing complementary insights."
P19-1283,2019,6 Conclusion,"the proposed methods are general and applicable not just to constituency trees, but given a similarity metric, to any symbolic representation of linguistic structures including dependency trees or abstract meaning representations."
P19-1283,2019,6 Conclusion,"we apply the same techniques to english sentence embeddings, and show where and to what extent each representation encodes syntactic information."
P19-1283,2019,6 Conclusion,we plan to explore these options in future work.
P19-1283,2019,6 Conclusion,"we present two rsa-based methods for correlating neural and syntactic representations of language, using tree kernels as a measure of similarity between syntactic trees."
P19-1284,2019,8 Conclusions,"apart from extracting rationales, we showed that hardkuma has further potential uses, which we demonstrated on premise-hypothesis attention in snli."
P19-1284,2019,8 Conclusions,to allow for reparameterized gradient estimates and support for binary outcomes we introduced the hardkuma distribution.
P19-1284,2019,8 Conclusions,we leave further explorations for future work.
P19-1284,2019,8 Conclusions,"we presented a differentiable approach to extractive rationales, including an objective that allows for specifying how much text is to be extracted."
P19-1285,2019,5 Conclusions,"transformer-xl obtains strong perplexity results, models longer-term dependency than rnns and transformer, achieves substantial speedup during evaluation, and is able to generate coherent text articles."
P19-1285,2019,5 Conclusions,"we envision interesting applications of transformer-xl in the fields of text generation, unsupervised feature learning, image and speech modeling."
P19-1286,2019,5 Conclusion,further analysis show that our method is effective in fine-tuning a pre-trained nmt model to correctly translate unknown words when switching to new domains.
P19-1286,2019,5 Conclusion,"in this paper, we propose a data-based, unsupervised adaptation method that focuses on domain adaption by lexicon induction (dali) for mitigating unknown word problems in nmt."
P19-1286,2019,5 Conclusion,we conduct extensive experiments to show consistent improvements of two popular nmt models through the usage of our proposed method.
P19-1287,2019,6 Conclusion and Future Work,"in the future, we would like to investigate the feasibility of our methods on non-recurrent nmt models such as transformer (vaswani et al., 2017)."
P19-1287,2019,6 Conclusion and Future Work,"in this work, we propose two models to improve the translation quality of nmt inspired by the common human behaviors, paraphrasing and consulting."
P19-1287,2019,6 Conclusion and Future Work,"moreover, we are also interested in incorporating discourse-level relations into our models."
P19-1287,2019,6 Conclusion and Future Work,the monolingual model simulates the paraphrasing process by utilizing the global source information while the bilingual model provides a referable target word based on other sentence pairs in the training corpus.
P19-1287,2019,6 Conclusion and Future Work,"we conduct experiments on chinese-english and english-german tasks, and the experimental results manifest the effectiveness and efficiency of our methods."
P19-1288,2019,6 Conclusion,"first, exploiting other sequencelevel training objectives like bag-of-words (ma et al., 2018)."
P19-1288,2019,6 Conclusion,"firstly, we propose a sequence-level training method based on a novel reinforcement algorithm for nat (reinforce-nat), which significantly improves the performance of nat models without decelerating the decoding speed."
P19-1288,2019,6 Conclusion,"in the future, we plan to investigate better methods to leverage the sequential information."
P19-1288,2019,6 Conclusion,"in this paper, we aim to retrieve the sequential information for nat models to enhance their translation ability while preserving fast-decoding property."
P19-1288,2019,6 Conclusion,"second, using sequential information distilled from the autoregressive teacher model to guide the training of the student nonautoregressive model."
P19-1288,2019,6 Conclusion,"secondly, we propose an innovative transformer decoder named fs-decoder to fuse the target sequential information into the top layer of the decoder, which achieves comparable performance to the transformer and still maintains substantial speedup."
P19-1288,2019,6 Conclusion,we believe that the following two directions are worth study.
P19-1289,2019,8 Conclusions,this prefixto-prefix architecture has the potential to be used in other sequence tasks outside of mt that involve simultaneity or incrementality.
P19-1289,2019,8 Conclusions,"we have presented a prefix-to-prefix training and decoding framework for simultaneous translation with integrated anticipation, and a wait-k policy that can achieve arbitrary word-level latency while maintaining high translation quality."
P19-1289,2019,8 Conclusions,"we leave many open questions to future work, e.g., adaptive policy using a single model (zheng et al., 2019)."
P19-1290,2019,6 Conclusion,"in this work, we proposed a hard-attention based nmt model which focuses solely on a few relevant source sequence tokens for each target token to effectively handle long sequence translation."
P19-1290,2019,6 Conclusion,our model sets new state-of-the-art results on en-de and en-fr translation tasks.
P19-1290,2019,6 Conclusion,we train our model by designing an rl algorithm with the reward shaping strategy.
P19-1291,2019,5 Conclusion,"in this paper, we propose to use both textual and phonetic information in nmt by combining them in the input embedding layer of neural networks."
P19-1291,2019,5 Conclusion,"our experimental results clearly show that both textual and phonetical information are important, and the best choice is to rely mostly on phonetic information."
P19-1291,2019,5 Conclusion,"such combination not only makes nmt models much more robust to homophone noises, but also improves their performance on clean datasets."
P19-1291,2019,5 Conclusion,"we also augment the training dataset by adding homophone noises, and our experiments demonstrate that this is very useful in improving the robustness of nmt models to homophone noises."
P19-1292,2019,5 Conclusion and Future Work,"by adding artificial data, we obtain a new state of the art in ape."
P19-1292,2019,5 Conclusion and Future Work,"in future work, we would like to do an extensive analysis on the capabilities of bert and transfer learning in general for different domains and language pairs in ape."
P19-1292,2019,5 Conclusion and Future Work,"using a small dataset, our results are competitive with systems trained on a large amount of artificial data, with much faster training."
P19-1292,2019,5 Conclusion and Future Work,we explored various ways for coupling bert in the decoder for language generation.
P19-1292,2019,5 Conclusion and Future Work,we found it beneficial to initialize the context attention of the decoder with bert’s self-attention and to tie together the parameters of the self-attention layers between the encoder and decoder.
P19-1292,2019,5 Conclusion and Future Work,we proposed a transfer learning approach to ape using bert pre-trained models and careful parameter sharing.
P19-1293,2019,5 Conclusion,"the pipeline does not require retraining the neural translation model when adapting to new source languages, which makes its application to new languages extremely fast and easy."
P19-1293,2019,5 Conclusion,the pipeline only requires a comprehensive source-to-target dictionary.
P19-1293,2019,5 Conclusion,"we obtain better or comparable quality translation results on high-resource languages than previously published unsupervised mt studies, and obtain good quality results for ten other languages that have never been used in an unsupervised mt scenario."
P19-1293,2019,5 Conclusion,we propose a two step pipeline for building a rapid unsupervised neural machine translation system for any language.
P19-1293,2019,5 Conclusion,we show how to easily obtain such a dictionary using off-the shelf tools.
P19-1293,2019,5 Conclusion,we use this system to translate test texts from 14 languages into english.
P19-1294,2019,4 Conclusion,"in contrast to constrained decoding, we have also observed that the method exhibits flexible use of terminology as in some cases the terms are used in their provided form while other times inflection is performed.6 to our knowledge there is no existing work that has a better speed vs performance trade-off than our method in the space of constrained decoding algorithms for neural mt, which we believe makes it particularly suitable for production environments."
P19-1294,2019,4 Conclusion,"we performed experiments in a zero-shot setting, showing that the copy behaviour is triggered at test time with terms that were never seen in training."
P19-1294,2019,4 Conclusion,"while most of previous work on neural mt addressed the integration of terminology with constrained decoding, we proposed a black-box approach in which a generic neural mt architecture is directly trained to learn how to use an external terminology that is provided at run-time."
P19-1295,2019,6 Conclusion,experimental results on various machine translation tasks demonstrate the effectiveness of the proposed model.
P19-1295,2019,6 Conclusion,"in this study, we propose to integrate the local and global information for enhancing the performance of sans."
P19-1295,2019,6 Conclusion,the extensive analyses verify that: 1) fully leveraging both of the local and global information is beneficial to generate a meaningful representation; and 2) different types of representations indeed have distinct requirements with respect to the local and global information.
P19-1295,2019,6 Conclusion,the proposed method gives highly effective improvements in their integration.
P19-1295,2019,6 Conclusion,we further empirically compare the two kinds of contextual information for different types of representations and probing tasks.
P19-1296,2019,5 Conclusion,"at the same time, we can utilize this information to enhance source representation."
P19-1296,2019,5 Conclusion,"in future work, we intend to apply these methods to other natural language tasks."
P19-1296,2019,5 Conclusion,"in this work, we have presented a sentence-level agreement method for nmt."
P19-1296,2019,5 Conclusion,our goal is to bring the sentence representation of the source-side and the target-side closer together.
P19-1296,2019,5 Conclusion,our study suggests the source-totarget sentence-level relationship is very useful for translation.
P19-1297,2019,7 Conclusion,"in future, we would like to explore other languages with diverse linguistic characteristics."
P19-1297,2019,7 Conclusion,"in this paper, we propose a multilingual unsupervised nmt framework to jointly train multiple languages using a shared encoder and languagespecific decoders."
P19-1297,2019,7 Conclusion,our approach is based on denoising autoencoding of all languages and backtranslating between english and non-english languages.
P19-1297,2019,7 Conclusion,our approach shows consistent improvement over the baselines in all the translation directions with a maximum improvement of 1.48 bleu points.
P19-1297,2019,7 Conclusion,this is due to the ability of the shared encoder in our proposed network to generate languageindependent representation.
P19-1297,2019,7 Conclusion,we also observe that the network learns to translate between unseen language pairs.
P19-1298,2019,6 Conclusions,"different from velickovi ˇ c et al.′ (2017), our work also provides an attempt to encode a simple labeled graph into transformer and can be used in any tasks which need transformer encoder to learn sequence representation."
P19-1298,2019,6 Conclusions,experimental results in two datasets on word-level and subword-level respectively validate the effectiveness of the proposed approaches.
P19-1298,2019,6 Conclusions,"in this paper, we have proposed two methods to incorporate lattice representations into transformer."
P19-1299,2019,5 Conclusion,"following earlier work, the shared features are learned via languageadversarial training (chen et al., 2016)."
P19-1299,2019,5 Conclusion,"for future work, we plan to apply man-moe to more challenging languages for tasks such as syntactic parsing, where multilingual data exists (nivre et al., 2017)."
P19-1299,2019,5 Conclusion,"furthermore, even considering methods with strong cross-lingual supervision, man-moe is able to match or outperform these models on closer language pairs."
P19-1299,2019,5 Conclusion,"furthermore, man-moe is a purely model-based transfer method, which does not require parallel data for training, enabling fully zero-resource mltl when combined with unsupervised cross-lingual word embeddings."
P19-1299,2019,5 Conclusion,"furthermore, we would like to experiment with multilingual contextualized embeddings such as the multilingual bert (devlin et al., 2018)."
P19-1299,2019,5 Conclusion,"in this paper, we propose man-moe, a multilingual model transfer approach that exploits both language-invariant (shared) features and language-specific (private) features, which departs from most previous models that can only make use of shared features."
P19-1299,2019,5 Conclusion,man-moe significantly outperforms all cross-lingually unsupervised baselines regardless of task or language.
P19-1299,2019,5 Conclusion,"on the other hand, the private features are extracted by a mixture-of-experts (moe) module, which is able to dynamically capture the relation between the target language and each source language on a token level."
P19-1299,2019,5 Conclusion,"our claim is supported by a wide range of experiments over multiple text classification and sequence tagging tasks, including a large-scale industry dataset."
P19-1299,2019,5 Conclusion,"this is extremely helpful when the target language is similar to a subset of source languages, in which case traditional models that solely rely on shared features would perform poorly."
P19-1299,2019,5 Conclusion,this makes man-moe more widely applicable to lower-resourced languages.
P19-1299,2019,5 Conclusion,"when transferring to distant languages such as chinese or japanese (from european languages), where the quality of cross-lingual word embeddings are unsatisfactory, man-moe remains highly effective and substantially mitigates the performance gap introduced by cross-lingual supervision."
P19-1300,2019,6 Conclusion,"by analysing the nearest neighbour graphs of monolingual word embeddings, we have verified that word embeddings are far from being isomorphic when they are trained on small data, explaining why existing unsupervised methods did not perform well on the lowresource condition."
P19-1300,2019,6 Conclusion,"in this paper, we proposed a new unsupervised multilingual word embedding approach."
P19-1300,2019,6 Conclusion,it would be also interesting to investigate how our approach compares to the baselines given a large amount of data such as wikipedia.
P19-1300,2019,6 Conclusion,"our experiments on word alignment tasks have demonstrated that our proposed model substantially outperforms the existing cross-lingual and multilingual unsupervised models under resource-poor conditions, namely when only small data are available or when domains of corpora are different across languages."
P19-1300,2019,6 Conclusion,our future work is to exploit character and subword information in our model and see how those information affect the performance in each language pair.
P19-1300,2019,6 Conclusion,"under the first condition, our model even outperforms supervised methods trained with 500 bilingual pairs of words."
P19-1300,2019,6 Conclusion,"we have also found that the performance of our model is closely related to word order rules, and our model can align words very well when they are used in a similar order across different languages."
P19-1300,2019,6 Conclusion,"whereas conventional methods aim to map pre-trained word embeddings into a common space, ours jointly generates multilingual word embeddings by extracting a common language structure among multiple languages."
P19-1301,2019,8 Conclusion,"for machine translation, entity linking, part-of-speech tagging, and dependency parsing, we train ranking models to predict the most promising transfer languages to use given a task language."
P19-1301,2019,8 Conclusion,"through analyzing the learned ranking models, we also gain some insights on the types of features that are most influential in selecting transfer languages for each of the nlp tasks, which may inform future ad hoc selection even without useing our method."
P19-1301,2019,8 Conclusion,we formulate the task of selecting the optimal transfer languages for an nlp task as a ranking problem.
P19-1301,2019,8 Conclusion,"we show that by taking multiple dataset statistics and language attributes into consideration, the learned ranking models recommend much better transfer languages than the ones suggested by considering only single language or dataset features."
P19-1302,2019,7 Conclusions,"identifying cognates based on orthography for words written in 35 different writing systems, as opposed to phonetic data, made the problem statement novel with respect to existing research in cognate identification."
P19-1302,2019,7 Conclusions,"in this paper, we have demonstrated a general method for building a cognate database using existing wordnet resources."
P19-1302,2019,7 Conclusions,"the evaluation showed that the resource has promisingly high quality, with precision and recall adjustable through the algorithm parameters."
P19-1302,2019,7 Conclusions,"the resource has been made available online, together with a graphical webbased tool for the exploration of cognate data, our hope being to attract both linguists and computer scientists as potential users."
P19-1302,2019,7 Conclusions,"the use of a large-scale cross-lingual database and a combination of linguistic, semantic, etymological, and geographic evidence resulted in what in our knowledge is the largest cognate database both in terms of the number of concepts and of the writing systems covered."
P19-1303,2019,7 Conclusions,our approach also demonstrated significant improvement over existing work on romance languages.
P19-1303,2019,7 Conclusions,"we design the model and training procedure following fundamental principles of decipherment from historical linguistics, which effectively guide the decipherment process without supervision signal."
P19-1303,2019,7 Conclusions,"we evaluate our approach on two lost languages, ugaritic and linear b, from different linguistic families, and observed substantially high accuracy in cognate identification."
P19-1303,2019,7 Conclusions,we proposed a novel neural decipherment approach.
P19-1303,2019,7 Conclusions,"we use a neural sequence-tosequence model to capture character-level cognate generation process, for which the training procedure is formulated as flow to impose vocabularylevel structural sparsity."
P19-1304,2019,5 Conclusions,experimental results on the benchmark datasets show that our model significantly outperforms existing baselines.
P19-1304,2019,5 Conclusions,"for example, the metric learning based few-shot knowledge base completion (xiong et al., 2018) can be directly formulated as a similar graph matching problem in this paper."
P19-1304,2019,5 Conclusions,"for this purpose, we further propose a graph matching model which induces a graph matching vector by jointly encoding the entitywise matching information."
P19-1304,2019,5 Conclusions,"in the future, we will explore more applications of the proposed idea of attentive graph matching."
P19-1304,2019,5 Conclusions,"previous cross-lingual knowledge graph alignment methods mainly rely on entity embeddings that derived from the monolingual kg structural information, thereby may fail at matching entities that have different facts in two kgs."
P19-1304,2019,5 Conclusions,"to address this, we introduce the topic entity graph to represent the contextual information of an entity within the kg and view this task as a graph matching problem."
P19-1305,2019,5 Conclusion,"experiments show that our method performs significantly better than several baselines, and is able to significantly reduce the performance gap between the cross-lingual assum and the monolingual assum over the benchmark datasets."
P19-1305,2019,5 Conclusion,"in this paper, we propose a teacher-student framework together with the back-translation procedure to deal with the zero-shot challenge of cross-lingual assum, which has no parallel data for training."
P19-1305,2019,5 Conclusion,"one is the summary word generation probabilities, the other is the attention weights."
P19-1305,2019,5 Conclusion,two properties of the teacher are proposed to teach the student.
P19-1305,2019,5 Conclusion,we also propose attention relay mechanism to form the attention weights of the teacher.
P19-1305,2019,5 Conclusion,"we use monolingual assum which has large-scale training resources as the teacher, and set the cross-lingual assum as the student."
P19-1306,2019,5 Conclusion,"by aligning word embedding spaces for multiple languages, the model can be directly applied under a zero-shot transfer setting when no training data is available for another language pair."
P19-1306,2019,5 Conclusion,we believe the idea of combining bilingual document representations using cross-lingual word embeddings can be generalized to other models as well.
P19-1306,2019,5 Conclusion,we propose to improve cross-lingual document retrieval by utilizing bilingual query-document interactions and learning to rerank from a small amount of training data for low-resource language pairs.
P19-1307,2019,6 Conclusion,"in the future, we will investigate whether our method helps other downstream tasks."
P19-1307,2019,6 Conclusion,our method improves word translation accuracy of different mapping-based clwe algorithms across languages.
P19-1307,2019,6 Conclusion,"we identify two conditions that make cross-lingual orthogonal mapping easier: length-invariance and center-invariance, and provide a simple algorithm that transforms monolingual embeddings to satisfy both conditions."
P19-1308,2019,5 Conclusion,"in this work, we present a morphology-aware alignment model for unsupervised bilingual lexicon induction."
P19-1308,2019,5 Conclusion,the proposed model is able to alleviate the adverse effect of morphological variation by introducing grammatical information learned from pre-trained denoising language model.
P19-1308,2019,5 Conclusion,"the results show that our approach can achieve better performance than several state-of-the-art unsupervised systems, and even achieves competitive performance compared to supervised methods."
P19-1309,2019,5 Conclusions and future work,"finally, we show that our improvements also carry over to downstream machine translation, as we obtain 31.2 bleu points for englishgerman newstest2014 training on our filtered version of paracrawl, an improvement of more than one point over the best performing official release."
P19-1309,2019,5 Conclusions and future work,"in addition, our method obtains up to 85% precision at reconstructing the 11.3m sentence pairs from the un corpus, improving over the similarly motivated method of guo et al.(2018) by more than 30 points."
P19-1309,2019,5 Conclusions and future work,"in this paper, we propose a new method for parallel corpus mining based on multilingual sentence embeddings."
P19-1309,2019,5 Conclusions and future work,our experiments show large improvements over previous methods.
P19-1309,2019,5 Conclusions and future work,"our system obtains the best published results on the bucc mining task, outperforming previous systems by more than 10 f1 points for all the four language pairs."
P19-1309,2019,5 Conclusions and future work,"the code of this work is freely available as part of the laser toolkit, together with an additional single encoder which covers 93 languages."
P19-1309,2019,5 Conclusions and future work,"we use a sequence-to-sequence architecture to train a multilingual sentence encoder on an initial parallel corpus, and a novel marginbased scoring method that overcomes the scale inconsistencies of cosine similarity."
P19-1310,2019,5 Conclusion,jw300 is freely available for all noncommercial use as per terms of the data owner.
P19-1310,2019,5 Conclusion,"our two experiments show that even with simple models jw300 offers top performance in crosslingual word embedding induction and multilingual projection for part-of-speech tagging, where we reach or even surpass more advanced models."
P19-1310,2019,5 Conclusion,"we introduced jw300, a large collection of parallel texts that spans over more than 300 languages, and offers 54 thousand pairs of alignments, each with roughly 100 thousand parallel sentences on average."
P19-1310,2019,5 Conclusion,we posit that the dataset would prove immensely useful for a wide variety of research in cross-lingual processing.
P19-1311,2019,6 Conclusion,"in this work, we focus on transfer to distant languages for pos tagging and dependency parsing, and propose to learn a structured flow model in a cross-lingual setting."
P19-1311,2019,6 Conclusion,"through learning a new latent embedding space as well as languagespecific knowledge with unlabeled target data, our method proves effective at transferring to distant languages."
P19-1312,2019,4 Conclusion and future work,"as a future work, we would like to study, for training bwe, the impact of the use of synthetic parallel data generated by unsupervised nmt, or of a different nature, such as translation pairs extracted from monolingual corpora without supervision."
P19-1312,2019,4 Conclusion and future work,"our approach has, however, a higher computational cost due to the need of generating synthetic parallel data, while generating more data would also improve the vocabulary coverage."
P19-1312,2019,4 Conclusion and future work,our experiments also highlight the robustness of joint training that can take advantage of bilingual contexts even from very noisy synthetic parallel data.
P19-1312,2019,4 Conclusion and future work,"since our approach works on top of unsupervised mapping for bwe and uses synthetic data generated by unsupervised mt, it will directly benefit from any future advances in these two types of techniques."
P19-1312,2019,4 Conclusion and future work,"such translation pairs are, in general, more fluent but potentially much less accurate."
P19-1312,2019,4 Conclusion and future work,we show in several cross-lingual nlp tasks that unsupervised joint bwe achieved better results than unsupervised mapped bwe.
P19-1313,2019,5 Conclusion,"by computing a joint embedding of all terms that best explains the extracted hearst patterns, we can then exploit these properties for improved hypernymy prediction."
P19-1313,2019,5 Conclusion,"experimentally, we show that our embeddings achieve state-of-the-art performance on a variety of commonly-used hypernymy benchmarks."
P19-1313,2019,5 Conclusion,"for this purpose, we combine hearst patterns with hyperbolic embeddings which allows us to set appropriate constraints on the distributional contexts and to improve the consistency in the embedding space."
P19-1313,2019,5 Conclusion,"in this work, we have proposed a new approach for inferring concept hierarchies from large text corpora."
P19-1313,2019,5 Conclusion,the natural hierarchical structure of hyperbolic space allows us also to learn very efficient embeddings that reduce the required dimensionality substantially over svd-based methods.
P19-1313,2019,5 Conclusion,"to improve optimization, we have furthermore proposed a new method to compute entailment cones in the lorentz model of hyperbolic space."
P19-1314,2019,5 Conclusion,"building upon these findings, we show that word-based models’ inferiority is due to the sparseness of word distributions, which leads to more out-of-vocabulary words, overfitting and lack of domain generalization ability."
P19-1314,2019,5 Conclusion,"in this paper, we ask the fundamental question of whether word segmentation is necessary for deep learning of chinese representations."
P19-1314,2019,5 Conclusion,"we benchmark such word-based models against char-based models in four end-to-end nlp tasks, and enforce apples-to-apples comparisons as much as possible."
P19-1314,2019,5 Conclusion,we hope this paper will foster more discussions on the necessity of the long-existing task of cws in the community.
P19-1314,2019,5 Conclusion,we observe that char-based models consistently outperform word-based models.
P19-1315,2019,6 Conclusion,"first, we provided a formal proof of the pennington et al.(2014) conjecture, the intuitive explanation of this phenomenon."
P19-1315,2019,6 Conclusion,"in this paper, we explained why word analogies can be solved using vector arithmetic."
P19-1315,2019,6 Conclusion,"most importantly, we provided empirical support of our theory and avoided making the strong assumptions in past work, making our theory a much more tenable explanation."
P19-1315,2019,6 Conclusion,"second, we provided novel justification for the addition of sgns word vectors by showing that it automatically down-weights the more frequent word, as weighting schemes do ad hoc."
P19-1315,2019,6 Conclusion,"third, we provided the first rigorous explanation of why the euclidean distance between word vectors is a good proxy for word dissimilarity."
P19-1315,2019,6 Conclusion,"we proved that an analogy holds in an sgns or glove embedding space with no reconstruction error iff the co-occurrence shifted pmi is the same for every word pair and across any two word pairs, provided the row vectors of those words in the factorized word-context matrix are coplanar.this had three implications."
P19-1316,2019,6 Conclusion,4 two directions for future work are (i) to extend our approach to other languages by using multilingual resources or translation data; and (ii) to explore various compositionality functions to combine the words’ representation on the basis of their grammatical function within a phrase.
P19-1316,2019,6 Conclusion,"in this paper, we present a novel straightforward method for estimating degrees of compositionality in noun phrases."
P19-1316,2019,6 Conclusion,our pretrained embeddings and the source codes are publicly available.
P19-1316,2019,6 Conclusion,the method is mixing hypernymy and distributional information of the noun phrases and their constituent words.
P19-1316,2019,6 Conclusion,"to encode hypernymy information, we use poincare embed- ′ dings, which – to the best of our knowledge – are used for the first time to accomplish the task of compositionality prediction."
P19-1316,2019,6 Conclusion,"while these hyperbolic embeddings trained on hypernym pattern extractions are not a good signal on their own for this task, we observe that mixing distributional and hypernymy information via euclidean and hyperbolic embeddings helps to substantially and significantly improve the performance of compositionality prediction, outperforming previous state-ofthe-art models."
P19-1317,2019,5 Conclusion,"alternatively, sequence encoding models such as gru, cnn, transformer, or even encoders with contextualized word embeddings like bert (devlin et al., 2018), or elmo (peters et al., 2018) can be used to replace this bilstm, however, with additional computation cost."
P19-1317,2019,5 Conclusion,"by learning to encode names of the same concepts into similar representations, while preserving their conceptual and contextual meanings, our encoder is able to extract meaningful representations for unseen names."
P19-1317,2019,5 Conclusion,"in implementation, we use the simple aggregation of pre-trained embeddings."
P19-1317,2019,5 Conclusion,the core unit of our encoder (in this work) is bilstm.
P19-1317,2019,5 Conclusion,the experiment results show that this approach is both efficient and effective.
P19-1317,2019,5 Conclusion,we also discuss different ways of representing the contextual and conceptual information in our framework.
P19-1318,2019,7 Conclusions,"for future work, we would be interested in further exploring the behavior of neural architectures for nlp tasks which intuitively would benefit from having access to relational information, e.g., text classification (espinosa anke and schockaert, 2018; camacho-collados et al., 2019) and other language understanding tasks such as natural language inference or reading comprehension, in the line of joshi et al.(2019)."
P19-1318,2019,7 Conclusions,"our model is intended to capture knowledge which is complementary to that of standard similarity-centric embeddings, and can thus be used in combination."
P19-1318,2019,7 Conclusions,"parting ways from previous approaches where relational information was either encoded in terms of relation vectors (which are highly expressive but can be more difficult to use in applications), represented by transforming standard word vectors (which capture relational information only in a limited way), or by taking advantage of external knowledge repositories, we proposed to learn an unsupervised word embedding model that is tailored specifically towards modelling relations."
P19-1318,2019,7 Conclusions,the evaluation indicates that our proposed method indeed results in representations that capture relational knowledge in a more nuanced way.
P19-1318,2019,7 Conclusions,"we have introduced the notion of relational word vectors, and presented an unsupervised method for learning such representations."
P19-1318,2019,7 Conclusions,"we tested the complementarity of our relational word vectors with standard fasttext word embeddings on several lexical semantic tasks, capturing different levels of relational knowledge."
P19-1319,2019,6 Conclusion,"in addition, we proposed to pre-train this network, relying on the claim that two antonyms of the same word tend to be synonyms, through a siamese network; and a relaxed version of the contrastive loss function."
P19-1319,2019,6 Conclusion,"the proposed method is based on algebraic properties of synonyms and antonyms, principally in the transitivity of synonymy and the antitransitivity of antonymy."
P19-1319,2019,6 Conclusion,"we evaluated our approach using a publicly available dataset and word vectors, obtaining encouraging results."
P19-1319,2019,6 Conclusion,we presented a supervised approach to distinguish antonyms and synonyms using pre-trained word embeddings.
P19-1319,2019,6 Conclusion,"we proposed a new siamese inspired model to deal with antitransitivity, the parasiamese network."
P19-1320,2019,10 Conclusion,"in this paper, we have proposed syngcn, a graph convolution based approach which utilizes syntactic context for learning word representations."
P19-1320,2019,10 Conclusion,syngcn overcomes the problem of vocabulary explosion and outperforms state-of-the-art word embedding approaches on several intrinsic and extrinsic tasks.
P19-1320,2019,10 Conclusion,the combination of syngcn and semgcn gives the best overall performance.
P19-1320,2019,10 Conclusion,"we also propose semgcn, a framework for jointly incorporating diverse semantic information in pre-trained word embeddings."
P19-1320,2019,10 Conclusion,we make the source code of both models available to encourage reproducible research.
P19-1321,2019,5 Conclusions,"in contrast to our method, these models are based on topic models (e.g.they typically model documents as a multinomial distribution over topics)."
P19-1321,2019,5 Conclusions,"in this paper, we analyzed the effect of adding a prior to the glove word embedding model, encoding the intuition that words can be organized in various natural groupings."
P19-1321,2019,5 Conclusions,"most notably, our model substantially outperforms standard word embedding models in analogy tasks that focus on syntactic/morphological relations, although this comes at the cost of lower performance in semantically oriented tasks such as measuring word similarity."
P19-1321,2019,5 Conclusions,"somewhat surprisingly, perhaps, this leads to a word embedding model which behaves substantially differently from existing methods."
P19-1321,2019,5 Conclusions,"surprisingly, we found that the document representations learned by our model outperform these topic modelling-based approaches, even those that rely on pre-trained word embeddings and thus have an added advantage, considering that our model in this setting is only learned from the (often relatively small) given document collection."
P19-1321,2019,5 Conclusions,this allowed us to compare our model with existing approaches that use von mises-fisher distributions for document modelling.
P19-1321,2019,5 Conclusions,"this finding puts into question the value of document-level topic distributions, which are used by many document embedding methods (being inspired by topic models such as lda)."
P19-1321,2019,5 Conclusions,we also found that the model performs better than standard word embedding models when it comes to modelling rare words.
P19-1321,2019,5 Conclusions,"word embedding models can also be used to learn document embeddings, by replacing word-word co-occurrences by document-word cooccurrences."
P19-1322,2019,5 Conclusion,"because delta embedding learning is an incremental process, it is possible to learn from a sequence of tasks, essentially “continuous learning” (parisi et al., 2018) of word semantics."
P19-1322,2019,5 Conclusion,it is an interesting future work and will make learning word embeddings more like human learning a language.
P19-1322,2019,5 Conclusion,"we proposed delta embedding learning, a supervised embedding learning method that not only improves performance in nlp tasks, but also learns better universal word embeddings by letting the embedding “grow” under supervision."
P19-1323,2019,6 Conclusion and future work,"future work will offer a more principled account of aspectual classification for specific verb classes, among them speech act and communication verbs (e.g., promise or call) that occur frequently in corpora but have hitherto been neglected in aspectual analyses."
P19-1323,2019,6 Conclusion and future work,"on a more general scale, we envisage examining the interplay of verb class (e.g., the classes of levin 1993), verb sense, and aspectual class, with the purpose of estimating the influence of the sentential context on the aspectual value of the predicate."
P19-1323,2019,6 Conclusion and future work,"the annotated corpus, the source code for the annotation tool, and the annotation guidelines are available at https://github.com/wroberts/ annotator."
P19-1323,2019,6 Conclusion and future work,"we also intend to develop a more principled treatment for the aspectual classification of metaphors, which are frequent in other corpora."
P19-1323,2019,6 Conclusion and future work,we present the first aspectually annotated resource for german verb tokens.
P19-1323,2019,6 Conclusion and future work,"we report substantial interannotator agreement, and validate our resource by training automatic aspectual classifiers, permitting favourable comparisons to prior work."
P19-1324,2019,4 Future work,"though we focused on lstm lms for english, this method can be applied to other architectures, objective tasks, and languages; possibilities to explore in future work."
P19-1324,2019,4 Future work,"we also plan to carry out further analyses aimed at individuating factors that challenge the resolution of lexical ambiguity (e.g., morphosyntactic vs. semantic ambiguity, frequency of a word or sense, figurative uses), as well as clarifying the interaction between prediction and processing of words within neural lms."
P19-1324,2019,4 Future work,we introduced a method to study how deep learning models of language deal with lexical ambiguity.
P19-1325,2019,7 Conclusion,"besides, we integrated it into a graph-based wsd algorithm, showing that its vectorized counterpart yields comparable f1 scores on three datasets."
P19-1325,2019,7 Conclusion,"our model, path2vec, relies on both global and local information from the graph and is simple, effective, and computationally efficient."
P19-1325,2019,7 Conclusion,path2vec enables a speed-up of up to four orders of magnitude for the computation of graph distances as compared to ‘direct’ graph measures.
P19-1325,2019,7 Conclusion,structured knowledge contained in language networks is useful for nlp applications but is difficult to use directly in neural architectures.
P19-1325,2019,7 Conclusion,"thus, our model is simple and general, hence it may be applied to any graph together with a node distance measure to speed up algorithms that employ graph distances."
P19-1325,2019,7 Conclusion,"we demonstrated that our approach generalizes well across graphs (wordnet, freebase, and dbpedia)."
P19-1325,2019,7 Conclusion,we proposed a way to train embeddings that directly represent a graph-based similarity measure structure.
P19-1326,2019,6 Conclusion and Future Work,experimental results show that kg2vec achieves state-ofthe-art results on the card-660 dataset.
P19-1326,2019,6 Conclusion and Future Work,future research directions include a theoretical explanation of kg2vec and applications to downstream nlp tasks.
P19-1326,2019,6 Conclusion and Future Work,"in this paper, we introduce kg2vec, a graph neural network based approach for embedding imputation of oov words which makes use of grounded language information."
P19-1326,2019,6 Conclusion and Future Work,"using publicly available information sources like wikipedia and wiktionary, kg2vec can effectively impute embeddings for rare or unseen words."
P19-1327,2019,7 Conclusions and Future Work,"interestingly, a manual error analysis and metrics taken from wordnet suggests each paradigm models different types of hypernymy."
P19-1327,2019,7 Conclusions and Future Work,"our results show that hybrid models of even simple systems are able to perform surprisingly well, consistently outperforming more robust single strategy models."
P19-1327,2019,7 Conclusions and Future Work,"we conclude that further work in hypernym discovery should utilize signals taken from both historical paradigms of hypernymy modeling, not only to improve confidence in answers but also to capture both direct and indirect hypernym relationships."
P19-1327,2019,7 Conclusions and Future Work,we studied the impact of utilizing a hybrid of pattern-based and distributional models for hypernym discovery by hybridizing simple models from each paradigm.
P19-1328,2019,4 Conclusion,"experiments in ls07 and ls14 benchmark datasets show that our proposed embedding dropout for partially masking the target word is helpful for bert to propose substitute candidates, and that analyzing a sentence’s contextualized representation before and after the substitution can largely improve the results of lexical substitution."
P19-1328,2019,4 Conclusion,"in our work, we propose an end-to-end lexical substitution approach based on bert, which can propose and validate substitute candidates without using any annotated data and manually curated resources."
P19-1329,2019,6 Conclusion,current nlp systems rely heavily on word embeddings.
P19-1329,2019,6 Conclusion,"in this work we demonstrate that three popular embedding models are inadequate at dealing precisely with numbers, in two aspects: magnitude and numeration."
P19-1329,2019,6 Conclusion,this work also raises important questions about other categories of word-like tokens that need to be treated like special cases.
P19-1329,2019,6 Conclusion,we hope the community will carefully consider that distributed word representations cannot be relied upon in all scenarios.
P19-1329,2019,6 Conclusion,"we hope this work will promote a more careful treatment of language, and serve a cautionary purpose against using word embeddings in downstream tasks without recognizing their limitations."
P19-1330,2019,7 Conclusion and Future Work,"also, we will explore ways of automating parts of the process, e.g.the highlight annotation."
P19-1330,2019,7 Conclusion and Future Work,"finally, the highlights could also be used as training signal, as it offers content saliency information at a finer level than the single reference typically used."
P19-1330,2019,7 Conclusion and Future Work,"in future work, we would like to extend our framework to other variants of summarization e.g.multi-document."
P19-1330,2019,7 Conclusion and Future Work,in this paper we introduced the highlightbased reference-less evaluation summarization (highres) framework for manual evaluation.
P19-1330,2019,7 Conclusion and Future Work,"our experiments show that highres lowers the variability of the judges’ content assessment, while helping expose the differences between systems."
P19-1330,2019,7 Conclusion and Future Work,the proposed framework avoids reference bias and provides absolute instead of ranked evaluation of the systems.
P19-1330,2019,7 Conclusion and Future Work,we also showed that by evaluating clarity we are able to capture a different dimension of summarization quality that is not captured by the commonly used fluency.
P19-1330,2019,7 Conclusion and Future Work,"we believe that our highlight-based evaluation is an ideal setup of abstractive summarization for three reasons: (i) highlights can be crowd sourced effectively without expert annotations, (ii) it avoids reference bias and (iii) it is not limited by n-gram overlap."
P19-1331,2019,6 Conclusion,"compared to the black-box mt-based systems, our model is more interpretable by providing generated edit operation traces, and more controllable with the ability to prioritize different simplification operations."
P19-1331,2019,6 Conclusion,"our model outperforms previous state-of-the-art machine translation-based ts models in most of the automatic evaluation metrics and human ratings, demonstrating the effectiveness of learning edit operations explicitly for sentence simplification."
P19-1331,2019,6 Conclusion,"we propose an npi-based model for sentence simplification, where edit-labels are predicted by the programmer and then executed into simplified tokens by the interpreter."
P19-1332,2019,5 Conclusion,"in the future, we plan to investigate more efficient methods of unsupervised domain adaptation with decomposition mechanism on other nlp tasks."
P19-1332,2019,5 Conclusion,"in this paper, we have proposed a neural paraphrase generation model, which is equipped with a decomposition mechanism."
P19-1332,2019,5 Conclusion,the qualitative experiments demonstrate that the generation of our model is interpretable and controllable.
P19-1332,2019,5 Conclusion,"the quantitative experiment results show that our model has competitive in-domain performance compared to the state-of-the-art models, and outperforms significantly upon other baselines in domain adaptation."
P19-1332,2019,5 Conclusion,"we construct such mechanisms by latent variables associated with each word, and a couple of transformer models with various inductive biases to focus on paraphrase patterns at different levels of granularity."
P19-1332,2019,5 Conclusion,we further propose a fast and incremental method for unsupervised domain adaptation.
P19-1333,2019,7 Conclusion,"in a comparative analysis, we demonstrated that our ts approach achieves the highest scores on all three simplification corpora with regard to samsa (0.67, 0.57, 0.54), and comes no later than a close second in terms of samsaabl (0.84, 0.84, 0.84), two recently proposed metrics targeted at automatically measuring the syntactic complexity of sentences."
P19-1333,2019,7 Conclusion,"in addition, the extrinsic evaluation that was carried out based on the task of open ie verified that downstream semantic applications profit from making use of our proposed structural ts approach as a preprocessing step."
P19-1333,2019,7 Conclusion,"in the future, we plan to investigate the constituency type classification and rhetorical relation identification steps and port this approach to languages other than english."
P19-1333,2019,7 Conclusion,"these findings are supported by the other scores of the automatic evaluation, as well as the manual analysis."
P19-1333,2019,7 Conclusion,we presented a recursive sentence splitting approach that transforms structurally complex sentences into a novel hierarchical representation in the form of core sentences and accompanying contexts that are semantically linked by rhetorical relations.
P19-1334,2019,9 Conclusions,"however, these models performed significantly better on both hans and on a separate structure-dependent dataset when their training data was augmented with hans-like examples."
P19-1334,2019,9 Conclusions,"overall, our results indicate that, despite the impressive accuracies of state-of-the-art models on standard evaluations, there is still much progress to be made and that targeted, challenging datasets, such as hans, are important for determining whether models are learning what they are intended to learn."
P19-1334,2019,9 Conclusions,statistical learners such as neural networks closely track the statistical regularities in their training sets.
P19-1334,2019,9 Conclusions,this process makes them vulnerable to adopting heuristics that are valid for frequent cases but fail on less frequent ones.
P19-1334,2019,9 Conclusions,"to evaluate whether nli models do behave consistently with these heuristics, we have introduced the hans dataset, on which models using these heuristics are guaranteed to fail."
P19-1334,2019,9 Conclusions,"we find that four existing nli models perform very poorly on hans, suggesting that their high accuracies on nli test sets may be due to the exploitation of invalid heuristics rather than deeper understanding of language."
P19-1334,2019,9 Conclusions,we have investigated three such heuristics that we hypothesize nli models are likely to learn.
P19-1335,2019,8 Conclusion,a strong baseline is proposed by combining powerful neural reading comprehension with domainadaptive pre-training.
P19-1335,2019,8 Conclusion,future variations of the task could incorporate nil recognition and mention detection (instead of mention boundaries being provided).
P19-1335,2019,8 Conclusion,the candidate generation phase leaves significant room for improvement.
P19-1335,2019,8 Conclusion,"the dataset can be used as a shared benchmark for entity linking research focused on specialized domains where labeled mentions are not available, and entities are defined through descriptions alone."
P19-1335,2019,8 Conclusion,we also expect models that jointly resolve mentions in a document would perform better than resolving them in isolation.
P19-1335,2019,8 Conclusion,"we introduce a new task for zero-shot entity linking, and construct a multi-world dataset for it."
P19-1336,2019,5 Conclusion,extensive experiments show the superiority of datnet over existing models and it achieves significant improvements on conll and wnut ner benchmark datasets.
P19-1336,2019,5 Conclusion,"in this paper we develop a transfer learning model datnet for low-resource ner, which aims at addressing representation difference and resource data imbalance problems."
P19-1336,2019,5 Conclusion,"to improve model generalization, we propose dual adversarial learning strategies, i.e., at and grad."
P19-1336,2019,5 Conclusion,"we introduce two variants, datnet-f and datnet-p, which can be chosen according to cross-language/domain user case and target dataset size."
P19-1337,2019,6 Conclusion,"in this paper, we introduce a distilled syntax-aware lstm (dsa-lstm), which combines scalability with structural biases."
P19-1337,2019,6 Conclusion,"our approach is a general one that can be applied to other student model architectures, such as transformers (vaswani et al., 2017)."
P19-1337,2019,6 Conclusion,"these findings suggest that the question of structural biases continues to be relevant for improving syntactic competence, even in scalable architectures that can benefit from evergrowing amounts of training data."
P19-1337,2019,6 Conclusion,we achieve this by distilling the predictions about upcoming words in a large training corpus made by a (computationally complex) hierarchical language model trained on a small subset of the data.
P19-1337,2019,6 Conclusion,"while we find that lstm language models achieve better syntactic generalisation than previously thought, on targeted syntactic evaluations our approach improves over this strong baseline, yields a new state of the art, compares favourably to a language model trained on much more data, and results in a language model that encodes hierarchical information to a large extent despite its sequential architecture."
P19-1338,2019,5 Conclusion,"in future work, we would like to combine more potential parsers—including chartstyle parsing and shift-reduce parsing—and transfer knowledge from one to another in a co-training setting."
P19-1338,2019,5 Conclusion,the tree-lstm’s policy is then refined towards a semantic objective.
P19-1338,2019,5 Conclusion,we achieve a new state-of-the-art result of unsupervised parsing on the nli dataset.
P19-1338,2019,5 Conclusion,we proposed a novel imitation learning approach to unsupervised parsing.
P19-1338,2019,5 Conclusion,we start from the differentiable prpn model and transfer its knowledge to a tree-lstm by step-by-step imitation learning.
P19-1339,2019,5 Conclusion,"male syntax, on the other hand, is parsed or tagged best when sufficient male-authored data is available in the training set."
P19-1339,2019,5 Conclusion,our experiments show that women’s syntax displays resilience: pos taggers and dependency parsers trained on any data perform well when tested on female writings.
P19-1339,2019,5 Conclusion,"the gender annotated wsj data provides a starting point for leveraging gendered grammatical differences and the development of better and fairer models and tools for syntax annotation, as well as for the many nlp down-stream tasks that use syntax in their models."
P19-1339,2019,5 Conclusion,the wsj author gender information is publicly available from http://lit.eecs.umich.edu/downloads.html.
P19-1339,2019,5 Conclusion,this suggests that men “lucked out” with respect to the gender imbalance in the wsj training data: a more balanced or more female-heavy data set could have caused significant drops in the performance of automatic syntax analysis for male writers.
P19-1340,2019,6 Conclusion,"as pre-trained models become increasingly higher-capacity, joint multilingual training is a promising approach to scalably providing nlp systems for a large set of languages."
P19-1340,2019,6 Conclusion,the remarkable effectiveness of unsupervised pretraining of vector representations of language suggests that future advances in this area can continue improving the ability of machine learning methods to model syntax (as well as other aspects of language).
P19-1341,2019,6 Conclusion,"by creating a multilingual bpe embedding space for 1500+ languages, we successfully transfer sentiment to each language without language-dependent preprocessing."
P19-1341,2019,6 Conclusion,"extensive intrinsic and extrinsic, automatic and human evaluations on 95 languages confirm the correctness and good quality of our lexicons."
P19-1341,2019,6 Conclusion,"in such cases, the pbc+ zs lexicons can be utilized because they also have high quality."
P19-1341,2019,6 Conclusion,solving this problem is non-trivial as many low-resource languages have a limited amount of written text in electronic form (and in any form).
P19-1341,2019,6 Conclusion,"to address the fact that the small-size zs lexicons are specific to pbc+’s domain, we conduct domain adaptation and induce large-size generic da (domain-adapted) lexicons for 200 languages."
P19-1341,2019,6 Conclusion,"to induce generic lexicons, our approach requires generic embeddings, which are not always available for low-resource languages."
P19-1341,2019,6 Conclusion,we created 1593 zs (zero-shot) sentiment lexicons and showed for a subset that they are highly consistent with gold lexicons.
P19-1341,2019,6 Conclusion,we make our code and lexicons freely available to the community.
P19-1341,2019,6 Conclusion,we proposed a universal approach for sentiment lexicon induction.
P19-1342,2019,7 Conclusion,"both gcns and grns are explored and compared, with grns showing better accuracies."
P19-1342,2019,7 Conclusion,"results on standard benchmark show that graph nns give better results compared to bi-directional tree lstms, providing more consistent predictions over phrases in one sentence."
P19-1342,2019,7 Conclusion,"to our knowledge, we are the first to leverage graph neural network structures for enhancing tree-lstms, and the first to discuss tree-level sentiment consistency using a set of novel metrics."
P19-1342,2019,7 Conclusion,we additionally propose a novel time-wise attention mechanism to further improve grns.
P19-1342,2019,7 Conclusion,"we investigated two tree communication models for sentiment analysis, leveraging recent advances in graph neural networks for information exchange between nodes in a baseline tree-lstm model."
P19-1343,2019,5 Conclusion,augmenting scarce natural text with synthetic text improves sentiment detection accuracy.
P19-1343,2019,5 Conclusion,code-mixing is an important and rapidly evolving mechanism of expression among multilingual populations on social media.
P19-1343,2019,5 Conclusion,"monolingual sentiment analysis techniques perform poorly on codemixed text, partly because code-mixed text often involves resource-poor languages."
P19-1343,2019,5 Conclusion,"starting from sentiment-labeled text in resource-rich source languages, we propose an effective method to synthesize labeled code-mixed text without designing switching grammars."
P19-1344,2019,5 Conclusion and Future Work,"furthermore, we find that each word’s adjacent words and its own word representation are key factors for its label, and we propose a paa and gun model to incorporate two kinds of information into our model."
P19-1344,2019,5 Conclusion and Future Work,"in our future work, we plan to apply our approach to other sequence labeling tasks, such as named entity recognition, word segmentation and so on."
P19-1344,2019,5 Conclusion and Future Work,"in this paper, we propose a sequence-to-sequence learning based approach to address the ate task."
P19-1344,2019,5 Conclusion and Future Work,our proposed method can take full advantage of the meaning of the whole sentence and the previous label during the decoding process.
P19-1344,2019,5 Conclusion and Future Work,the experimental results demonstrate that our approach can achieve comparable performances on ate task.
P19-1345,2019,7 Conclusion,empirical studies show that our proposed approach significantly outperforms several state-ofthe-art baselines in the task of asc-qa.
P19-1345,2019,7 Conclusion,"furthermore, we would like to explore the effectiveness of our approach to asc-qa in other languages."
P19-1345,2019,7 Conclusion,"in our future work, we would like to solve other challenges in asc-qa such as data imbalance and negation detection to improve the performance."
P19-1345,2019,7 Conclusion,"in this paper, we propose a new task, i.e., aspect sentiment classification towards question answering (asc-qa)."
P19-1345,2019,7 Conclusion,"specifically, we first build a high-quality human annotated benchmark corpus."
P19-1345,2019,7 Conclusion,"then, we design a reinforced bidirectional attention network (rban) approach to address ascqa."
P19-1346,2019,7 Conclusion,"proposed models are far from human performance, in part due to the inability to exploit the long full web text."
P19-1346,2019,7 Conclusion,"we hope eli5 will inspire future work in all aspects of long-form qa, from the information extraction problem of obtaining information from long, multi-document input to generating more coherent and accurate paragraph-length answers."
P19-1346,2019,7 Conclusion,we introduce the first large-scale long form question answering dataset of open-ended queries with explanatory multi-sentence answers.
P19-1346,2019,7 Conclusion,we show that abstractive models generate coherent answers and are competitive with extractive models in human evaluation.
P19-1347,2019,6 Conclusion,"in this paper, we proposed two novel methods to solve a realistic task, tqa dataset."
P19-1347,2019,6 Conclusion,our method also demonstrates state-of-the-art results.
P19-1347,2019,6 Conclusion,we believe that our work can be a meaningful step in realistic multimodal qa and solving the out-of-domain issue.
P19-1347,2019,6 Conclusion,we extract knowledge features with the proposed f-gcn and conduct self-supervised learning to overcome the out-of-domain issue.
P19-1348,2019,5 Conclusion,"in particular, we design the training algorithm to only update the network parameters in the optimization process when the caption generation and vqa tasks agree on the direction of change."
P19-1348,2019,5 Conclusion,"in particular, we present a model which jointly generates question-related captions and uses them to provide additional information to aid vqa."
P19-1348,2019,5 Conclusion,"in this work, we have explored how generating question-relevant image captions can improve vqa performance."
P19-1348,2019,5 Conclusion,our single model joint system outperforms the current state-of-the-art single model for vqa.
P19-1348,2019,5 Conclusion,"this approach only utilizes existing image-caption datasets, automatically determining which captions are relevant to a given question."
P19-1349,2019,4 Conclusion,it involves both word-object grounding and sentence-image association to capture different degrees of granularity and interpretability of the images.
P19-1349,2019,4 Conclusion,this paper proposes a multi-grained attention mechanism.
P19-1349,2019,4 Conclusion,visualizations of object-level attention show a clear improvement in the ability of the model to attend to small details in complicated scenes.
P19-1350,2019,6 Conclusion,"more importantly, we show that the order in which models learn tasks is important, wh→y/n facilitates continual learning more than the opposite order, thereby confirming psycholinguistic evidence."
P19-1350,2019,6 Conclusion,"most importantly, our study revealed that differences in the inherent difficulty of the tasks at hand can have a strong impact on continual learning."
P19-1350,2019,6 Conclusion,"our error analysis highlights the importance of taking the kind of mistakes made by the models into account: a model that does not detect task a after having been exposed to task b should be penalized more than a model that answers task a with wrong task-related labels, but is still capable of identifying the task."
P19-1350,2019,6 Conclusion,"our results show that dramatic forgetting is at play in vqa, and for the tested task pairs we empirically found rehearsal to work better than a regularization-based method (ewc)."
P19-1350,2019,6 Conclusion,"regularization-based methods like ewc appear to work less well when applied to tasks with different levels of difficulty, as in our experiments."
P19-1350,2019,6 Conclusion,we assessed to what extent a multimodal model suffers from catastrophic forgetting in a vqa task.
P19-1350,2019,6 Conclusion,we built two tasks involving different linguistic characteristics which are known to be learned sequentially by children and on which multimodal models reach different performance.
P19-1350,2019,6 Conclusion,we reserve a deeper investigation of this aspect to future research.
P19-1351,2019,6 Conclusion,"in future work, we are investigating whether the vtqa model can be jointly trained with the paragraph captioning model."
P19-1351,2019,6 Conclusion,"we presented a vtqa model that combines visual and paragraph-captioning features to significantly improve visual question answering accuracy, via a model that performs early, late, and later fusion."
P19-1351,2019,6 Conclusion,"while our model showed promising results, it still used a pre-trained paragraph captioning model to obtain the textual symbolic information."
P19-1352,2019,5 Conclusion,"according to the degree of relevance between a parallel word pair, the word pairs are categorized into three different groups and the number of shared features is different."
P19-1352,2019,5 Conclusion,each word embedding is composed of shared and private features.
P19-1352,2019,5 Conclusion,"in this work, we propose a novel sharing technique to improve the learning of word embeddings for nmt."
P19-1352,2019,5 Conclusion,"meanwhile, the private features enable the words to better capture the monolingual characteristics, result in an improvement of the overall translation quality."
P19-1352,2019,5 Conclusion,our experimental results show that the proposed method outperforms the strong transformer baselines while using fewer model parameters.
P19-1352,2019,5 Conclusion,the shared features act as a prior alignment guidance for the attention model to improve the quality of attention.
P19-1353,2019,7 Conclusion,the relatively straightforward application of our model to the analysis of authorial prestige shows how identifying realis events can help to uncover some important and overlooked aspects of novelistic narrative.
P19-1353,2019,7 Conclusion,"to the best of our knowledge, no previous technical or theoretical work has specifically examined the function that events with asserted realis play in the structure of literary fiction."
P19-1353,2019,7 Conclusion,we hope this initial application inspires further research by literary scholars and computational humanists in the future.
P19-1353,2019,7 Conclusion,we present in this work a new dataset for the representation of events in literary texts in order to bridge the gap between previous efforts to represent fact-based accounts in news (along with contemporary models trained on that data) and the demands of literary scholars for the computational analysis of the micro-narratives that comprise plot.
P19-1353,2019,7 Conclusion,"yet simply by analyzing the ratio of realis events, one can capture a meaningful distinction between novels written by authors whose works are reviewed by elite literary journals and those written by authors whose work is not."
P19-1354,2019,6 Conclusion,"as our approach is not limited to the nmt encoders, it is also interesting to explore how do the models trained on other nlp tasks learn word order information."
P19-1354,2019,6 Conclusion,"however, there is no evidence that san learns less word order information under the machine translation context."
P19-1354,2019,6 Conclusion,"in this paper, we introduce a novel word reordering detection task which can probe the ability of a model to extract word order information."
P19-1354,2019,6 Conclusion,our further analyses for the encoders pretrained on the nmt data suggest that 1) the learning objective sometimes plays a crucial role on learning a specific feature (e.g.word order) in a downstream nlp task; 2) modeling recurrence is universally-effective to learn word order information for san; and 3) rnn is more sensitive on erroneous word order noises in machine translation system.
P19-1354,2019,6 Conclusion,the results reveal that rnn and disan exactly perform better than san on extracting word order information in the case they are trained individually for our task.
P19-1354,2019,6 Conclusion,"these observations facilitate the understanding of different tasks and model architectures in finer-grained level, rather than merely in overall score (e.g.bleu)."
P19-1354,2019,6 Conclusion,"with the help of the proposed task, we evaluate rnn, san and disan upon transformer framework to empirically test the theoretical claims that san lacks the ability to learn word order."
P19-1355,2019,5 Conclusions,a government-funded academic compute cloud would provide equitable access to all researchers.
P19-1355,2019,5 Conclusions,academic researchers need equitable access to computation resources.
P19-1355,2019,5 Conclusions,"an additional avenue through which nlp and machine learning software developers could aid in reducing the energy associated with model tuning is by providing easyto-use apis implementing more efficient alternatives to brute-force grid search for hyperparameter tuning, e.g.random or bayesian hyperparameter search techniques (bergstra et al., 2011; bergstra and bengio, 2012; snoek et al., 2012)."
P19-1355,2019,5 Conclusions,an effort can also be made in terms of software.
P19-1355,2019,5 Conclusions,"at that cost, the hardware required to develop the model in our case study (approximately 58 gpus for 172 days) would cost $145,000 usd plus electricity, about half the estimated cost to use on-demand cloud gpus."
P19-1355,2019,5 Conclusions,authors should report training time and sensitivity to hyperparameters.
P19-1355,2019,5 Conclusions,"for example, an off-the-shelf gpu server containing 8 nvidia 1080 ti gpus and supporting hardware can be purchased for approximately $20,000 usd."
P19-1355,2019,5 Conclusions,integrating these tools into the workflows with which nlp researchers and practitioners are already familiar could have notable impact on the cost of developing and tuning in nlp.
P19-1355,2019,5 Conclusions,"limiting this style of research to industry labs hurts the nlp research community in many ways.first, it stifles creativity."
P19-1355,2019,5 Conclusions,more explicit characterization of tuning time could also reveal inconsistencies in time spent tuning baseline models compared to proposed contributions.
P19-1355,2019,5 Conclusions,most of the models studied in this paper were developed outside academia; recent improvements in state-of-the-art accuracy are possible thanks to industry access to large-scale compute.
P19-1355,2019,5 Conclusions,our experiments suggest that it would be beneficial to directly compare different models to perform a cost-benefit (accuracy) analysis.
P19-1355,2019,5 Conclusions,"realizing this will require: (1) a standard, hardwareindependent measurement of training time, such as gigaflops required to convergence, and (2) a standard measurement of model sensitivity to data and hyperparameters, such as variance with respect to hyperparameters searched."
P19-1355,2019,5 Conclusions,recent advances in available compute come at a high price not attainable to all who desire access.
P19-1355,2019,5 Conclusions,researchers should prioritize computationally efficient hardware and algorithms.
P19-1355,2019,5 Conclusions,"researchers with good ideas but without access to large-scale compute will simply not be able to execute their ideas, instead constrained to focus on different problems."
P19-1355,2019,5 Conclusions,"second, it prohibits certain types of research on the basis of access to financial resources."
P19-1355,2019,5 Conclusions,there is already a precedent for nlp software packages prioritizing efficient models.
P19-1355,2019,5 Conclusions,"third, the prohibitive start-up cost of building in-house resources forces resource-poor groups to rely on cloud compute services such as aws, google cloud and microsoft azure."
P19-1355,2019,5 Conclusions,"this even more deeply promotes the already problematic “rich get richer” cycle of research funding, where groups that are already successful and thus well-funded tend to receive more funding due to their existing accomplishments."
P19-1355,2019,5 Conclusions,"this is likely because their interoperability with popular deep learning frameworks such as pytorch and tensorflow is not optimized, i.e.there are not simple examples of how to tune tensorflow estimators using bayesian search."
P19-1355,2019,5 Conclusions,"this will enable direct comparison across models, allowing subsequent consumers of these models to accurately assess whether the required computational resources are compatible with their setting."
P19-1355,2019,5 Conclusions,"to address this, when proposing a model that is meant to be re-trained for downstream use, such as retraining on a new domain or fine-tuning on a new task, authors should report training time and computational resources required, as well as model sensitivity to hyperparameters."
P19-1355,2019,5 Conclusions,"unlike money spent on cloud compute, however, that invested in centralized resources would continue to pay off as resources are shared across many projects."
P19-1355,2019,5 Conclusions,"we recommend a concerted effort by industry and academia to promote research of more computationally efficient algorithms, as well as hardware that requires less energy."
P19-1355,2019,5 Conclusions,"while software packages implementing these techniques do exist,10 they are rarely employed in practice for tuning nlp models."
P19-1355,2019,5 Conclusions,"while these services provide valuable, flexible, and often relatively environmentally friendly compute resources, it is more cost effective for academic researchers, who often work for nonprofit educational institutions and whose research is funded by government entities, to pool resources to build shared compute centers at the level of funding agencies, such as the u.s. national science foundation."
P19-1356,2019,8 Conclusion,"finally, we have shown that bert’s internal representations reflect a compositional modelling that shares parallels with traditional syntactic analysis."
P19-1356,2019,8 Conclusion,it would be interesting to see if our results transfer to other domains with higher variability in syntactic structures (such as noisy user generated content) and with higher word order flexibility as experienced in some morphologically-rich languages.
P19-1356,2019,8 Conclusion,our results therefore confirm those of goldberg (2019); hewitt and manning (2019); liu et al.(2019); tenney et al.(2019) on bert who demonstrated that span representations constructed from those models can encode rich syntactic phenomena.
P19-1356,2019,8 Conclusion,we have also shown that bert requires deeper layers to model long-range dependency information.
P19-1356,2019,8 Conclusion,we have shown that phrasal representations learned by bert reflect phraselevel information and that bert composes a hierarchy of linguistic signals ranging from surface to semantic features.
P19-1356,2019,8 Conclusion,"with our experiments, which contribute to a currently bubbling line of work on neural network interpretability, we have shown that bert does capture structural properties of the english language."
P19-1357,2019,5 Conclusion,abusive behavior online affects a substantial amount of the population.
P19-1357,2019,5 Conclusion,"first, expanding our purview for abuse detection to include both extreme behaviors and the more subtle— but still offensive—behaviors like microaggressions and condescension."
P19-1357,2019,5 Conclusion,"here, we propose a new strategy for nlp to tackling online abuse in three ways."
P19-1357,2019,5 Conclusion,"second, nlp must develop methods that go beyond reactive identifyand-delete strategies to one of proactivity that intervenes or nudges individuals to discourage harm before it occurs."
P19-1357,2019,5 Conclusion,"the nlp community has proposed computational methods to help mitigate this problem, yet has also struggled to move beyond the most obvious tasks in abuse detection."
P19-1357,2019,5 Conclusion,"third, the community should contextualize its effort inside a broader framework of justice—explicit capabilities, restorative justice, and procedural justice—to directly support the end goal of productive online communities."
P19-1358,2019,6 Future Work,"in actuality, there are even more ways a model could learn to improve itself—for example, learning which question to ask in a given context to receive the most valuable feedback."
P19-1358,2019,6 Future Work,"in this way, a dialogue agent could both improve its dialogue ability and its potential to improve further."
P19-1358,2019,6 Future Work,"in this work we learned from dialogue using two types of self-feeding: imitation of satisfied user messages, and learning from the feedback of unsatisfied users."
P19-1358,2019,6 Future Work,"one could even use the flexible nature of dialogue to intermix data collection of more than one type— sometimes requesting new feedback examples, and other times requesting new satisfaction examples (e.g., asking “did my last response make sense?”)."
P19-1358,2019,6 Future Work,we leave exploration of this metalearning theme to future work.
P19-1359,2019,5 Conclusion,"an emotion classifier is also used to guide the response generation process, which ensures that a specific emotion is appropriately expressed in the generated texts."
P19-1359,2019,5 Conclusion,"experimental results with both automatic and human evaluations demonstrated that emods outperformed the baselines in bleu, diversity and the quality of emotional expression with a significant margin, highlighting the potential of the proposed architecture for practical dialog systems."
P19-1359,2019,5 Conclusion,"observing that emotional states can be expressed with language by explicitly using strong emotional words or by forming neutral word in distinct patterns, we proposed a novel emotional dialog system (emods) that can express the desired emotions in either way, and at the same time generate the meaningful responses with a coherent structure."
P19-1359,2019,5 Conclusion,the sequence-to-sequence framework has been extended with a lexicon-based attention mechanism that works by seamlessly “plugging” emotional words into the texts by increasing their probability at the right time steps.
P19-1359,2019,5 Conclusion,"to our knowledge, this study is among the first ones to build an interactive machine capable of expressing the specific emotions either in an explicit (if possible) or implicit (when necessary) way."
P19-1360,2019,7 Conclusion and Future Work,"currently, our proposed method only considers the supervised setting where we have annotated dialog acts, and we have not investigated the situation where such annotation is not available."
P19-1360,2019,7 Conclusion and Future Work,"in the future, we intend to infer the dialog acts from the annotated responses and use such noisy data to guide the response generation."
P19-1360,2019,7 Conclusion and Future Work,"in this paper, we propose a new semanticallycontrolled neural generation framework to resolve the scalability and generalization problem of existing models."
P19-1361,2019,8 Conclusion,"as the usage grows, ids will cumulate more and more knowledge over time."
P19-1361,2019,8 Conclusion,experiments show that ids is robust to new user actions.
P19-1361,2019,8 Conclusion,"importantly, with humans in the loop, ids requires no data for initialization and can update itself online by selecting the most valuable data."
P19-1361,2019,8 Conclusion,"in this paradigm, users are not expected to follow any definition, and ids has potential to handle new situations."
P19-1361,2019,8 Conclusion,"this paper presents a novel incremental learning framework to design dialogue systems, which we call ids."
P19-1361,2019,8 Conclusion,"to simulate new user actions after deployment, we propose a new dataset consisting of five different subsets."
P19-1362,2019,5 Conclusion,"for example, some topical information can be introduced to make the detected relevant contexts more accurate."
P19-1362,2019,5 Conclusion,"furthermore, our further analysis show that the relevant contexts detected by our model are significantly coherent with humans’ judgements."
P19-1362,2019,5 Conclusion,"in addition, the detailed content information can be considered in the relevant contexts to further improve the quality of generated response."
P19-1362,2019,5 Conclusion,"in future work, we plan to further investigate the proposed recosa model."
P19-1362,2019,5 Conclusion,"in this paper, we propose a new multi-turn dialogue generation model, namely recosa."
P19-1362,2019,5 Conclusion,our core idea is to utilize the self-attention mechanism to effectively capture the long distant dependency relations.
P19-1362,2019,5 Conclusion,the experimental results show that our model significantly outperforms existing hred models and its attention variants.
P19-1362,2019,5 Conclusion,"the motivation comes from the fact that the widely used hred based models simply treat all contexts indiscriminately, which violate the important characteristic of multi-turn dialogue generation, i.e., the response is usually related to only a few contexts."
P19-1362,2019,5 Conclusion,"therefore, we obtain the conclusion that the relevant contexts can be useful for improving the quality of multiturn dialogue generation, by using proper detection methods, such as self-attention."
P19-1362,2019,5 Conclusion,"though some researchers have considered using the similarity measure such as cosine or traditional attention mechanism to tackle this problem, the detected relevant contexts are not accurate, due to either insufficient relevance assumption or position bias problem."
P19-1362,2019,5 Conclusion,we conduct extensive experiments on both chinese customer services dataset and english ubuntu dialogue dataset.
P19-1363,2019,6 Conclusion,"future work may also incorporate contradiction information into the dialogue model itself, and extend to generic contradictions."
P19-1363,2019,6 Conclusion,"in this paper, we demonstrated that natural language inference can be used to improve performance on a downstream dialogue task."
P19-1363,2019,6 Conclusion,"the dataset offers a new domain for natural language inference models, and suggests avenues such as devising alternative methods for using natural language inference components in downstream tasks."
P19-1363,2019,6 Conclusion,"to do so, we created a new dialogue-derived dataset called dialogue nli, a re-ranking method for incorporating a dialogue nli model into a dialogue task, and an evaluation set which measures a model’s persona consistency."
P19-1364,2019,4 Conclusion,another interesting direction is to design a trainable budget scheduler.
P19-1364,2019,4 Conclusion,"compared to previous work, our approach can better utilize the limited real user interactions in a more efficient way in the fixed budget setting, and its effectiveness was demonstrated in the simulation evaluation, human evaluation, including human-in-theloop experiments."
P19-1364,2019,4 Conclusion,"in future, we plan to investigate the effectiveness of our method on more complex task-oriented dialogue datasets."
P19-1364,2019,4 Conclusion,"in this paper, the budget scheduler was created independently of the dialogue policy training algorithm, so a trainable budget scheduler may incur additional cost."
P19-1364,2019,4 Conclusion,"one possible solution is transfer learning, in which we train the budget scheduler on some welldefined dialogue tasks, then leverage this scheduler to guide the policy learning on other complex dialogue tasks."
P19-1364,2019,4 Conclusion,we presented a new framework bcs-ddq for task-oriented dialogue policy learning.
P19-1365,2019,8 Conclusion,"due to the computational expense of running large beam searches, we recommend using random-sampling to over-sample."
P19-1365,2019,8 Conclusion,"in this work, we perform an analysis of posttraining decoding strategies that attempt to promote diversity in conditional language models."
P19-1365,2019,8 Conclusion,"the ability to effectively generate a diverse set of responses while not degrading quality is extremely important in a variety of generation tasks, and is a crucial component to harnessing the power of state-of-the-art generative models."
P19-1365,2019,8 Conclusion,"the relative effectiveness of the various decoding strategies differs for the two tasks we considered, which suggests that choice of optimal diverse decoding strategy is both task-specific and dependent on one’s tolerance for lower quality outputs."
P19-1365,2019,8 Conclusion,we show how over-sampling outputs then filtering down to the desired number is an easy way to increase diversity.
P19-1365,2019,8 Conclusion,"while we have focused on evaluating each decoding strategy under the specifics reported to be the best in the original, further work is necessary to conclude whether observed differences in quality and diversity may simply be due to each work’s chosen hyperparameters."
P19-1366,2019,6 Conclusion and Future Work,"experiments show that the reat method significantly improves the quality of the generated responses, which demonstrates the effectiveness of this approach."
P19-1366,2019,6 Conclusion and Future Work,"in addition, we will also explore how to integrate external knowledge in other formats, like the knowledge graph, into adversarial training so that the quality could be further improved."
P19-1366,2019,6 Conclusion and Future Work,"in contrast to existing approaches, our reat method directly uses response candidates from retrieval-based systems to improve the discriminator in adversarial training."
P19-1366,2019,6 Conclusion and Future Work,"in future research, we will further investigate how to better leverage larger training data to improve the reat method."
P19-1366,2019,6 Conclusion and Future Work,"therefore, it can benefit from the advantages of retrieval-based response candidates as well as neural responses from generation-based systems."
P19-1366,2019,6 Conclusion and Future Work,we propose a retrieval-enhanced adversarial training method for neural response generation in dialogue systems.
P19-1367,2019,6 Conclusion and Future Work,experiments demonstrate that the proposed method is remarkably better than strong baselines on both automatic and manual evaluations.
P19-1367,2019,6 Conclusion and Future Work,"in the future, there are some promising explorations in vocabulary pyramid networks.1) we will further study how to obtain multi-level vocabularies, such as employing other clustering methods and incorporating semantic lexicons like wordnet; 2) we also plan to design deep-pass encoding and decoding for vpn; 3) we will investigate how to apply vpn to other natural language generation tasks such as machine translation and generative text summarization."
P19-1367,2019,6 Conclusion and Future Work,"in this study, we tackle the issues of one fixed vocabulary and one-pass decoding in response generation tasks."
P19-1367,2019,6 Conclusion and Future Work,"to this end, we have introduced vocabulary pyramid networks, in which dialogue input and output are represented by multi-level vocabularies and then processed by multi-pass encoding and decoding, where the multi-level vocabularies are obtained from hierarchical clustering of raw words."
P19-1367,2019,6 Conclusion and Future Work,we conduct experiments on english twitter and chinese weibo datasets.
P19-1368,2019,6 Conclusion,"finally, we showed sgnn++ fast latency on pixel phone."
P19-1368,2019,6 Conclusion,"overall, our sgnn++ approach outperformed all baselines from 14 to 41%, improved upon state-of-the-art on-device work (ravi and kozareva, 2018) with up to 5%, and also outperformed non-on-device neural approaches (hakkani-tur et al., 2016; liu and lane, 2016; dzendzik et al., 2017; elfardy et al., 2017)."
P19-1368,2019,6 Conclusion,"through multiple ablation studies, we showed the impact of partitioned projections on accuracy and the impact of model size on accuracy."
P19-1368,2019,6 Conclusion,"we conducted experiments on wide range of nlp applications such as dialog acts, intent prediction and customer feedback."
P19-1368,2019,6 Conclusion,"we evaluated the approach on four languages, showing the language agnostic capability of our on-device sgnn++ model."
P19-1368,2019,6 Conclusion,we proposed embedding-free on-device neural network that uses joint structured and context partitioned projections for short text classification.
P19-1368,2019,6 Conclusion,we trained quantized versions of sgnn++ showing that we can further reduce the model size while preserving quality.
P19-1368,2019,6 Conclusion,"we used the same model architecture and parameter settings across all languages and tasks, which demonstrates the generalizability of this approach compared to prior work that built custom models."
P19-1369,2019,7 Conclusion,"each dialog in duconv is created by two crowdsourced workers, where one acts as the conversation leader and the other acts as the follower."
P19-1369,2019,7 Conclusion,experimental results show that dialogue models that plan over knowledge graph can make more full use of related knowledge to generate more diverse conversations.
P19-1369,2019,7 Conclusion,"in this paper, we build a human-like conversational agent by endowing it with the ability of proactively leading the conversation."
P19-1369,2019,7 Conclusion,"our dataset and proposed models are publicly available, which can be used as benchmarks for future research on constructing knowledge-driven proactive dialogue systems."
P19-1369,2019,7 Conclusion,"the leader is provided with a knowledge graph and asked to sequentially change the discussed topics following the given conversation goal, and meanwhile, keep the dialogue as natural and engaging as possible."
P19-1369,2019,7 Conclusion,"to achieve this goal, we create a new dataset named duconv."
P19-1369,2019,7 Conclusion,we establish baseline results on duconv using several state-of-the-art models.
P19-1370,2019,6 Conclusions,empirical studies on two public data sets show that the proposed approach can generally improve the performance of existing matching models.
P19-1370,2019,6 Conclusions,the learning approach lets two matching models teach each other and evolve together.
P19-1370,2019,6 Conclusions,we propose learning a matching model for response selection under a general co-teaching framework with three specific teaching strategies.
P19-1371,2019,6 Conclusion,"compared with retrieval-augmented models, our model makes improvements on the quality, relevance and informativeness of response generation."
P19-1371,2019,6 Conclusion,"in this paper, we propose a novel memoryaugmented generative model for response generation."
P19-1371,2019,6 Conclusion,"it clusters the training corpus into multiple groups, extracts and memorizes common characteristics of each group for generation."
P19-1371,2019,6 Conclusion,it leverages groups of query-response pairs to augment generative models instead of the individual retrieval results.
P19-1372,2019,6 Conclusion and future work,directions of future work may be pursuing betterdefined features and easier training strategies.
P19-1372,2019,6 Conclusion and future work,experimental results illustrate the superior performance of the proposed model in generating diverse and appropriate responses compared to previous representative approaches.
P19-1372,2019,6 Conclusion and future work,"however, the modeling of the common and distinct features of responses in our method is currently implicit and coarse-grained."
P19-1372,2019,6 Conclusion and future work,"in this paper, we tackle the one-to-many queryresponse mapping problem in open-domain conversation and propose a novel two-step generation architecture with the correlation of multiple valid responses considered."
P19-1372,2019,6 Conclusion and future work,"jointly viewing the multiple responses as a response bag, the model extracts the common and distinct features of different responses in two generation phases respectively to output multiple diverse responses."
P19-1373,2019,7 Conclusion and Future Work,evaluation of the learned representations on four downstream dialog tasks shows strong performance improvement over randomly initialized baselines.
P19-1373,2019,7 Conclusion and Future Work,"finally, the addition of word-level pretraining methods to improve the dialog context representations should be explored."
P19-1373,2019,7 Conclusion and Future Work,"first, the models proposed here should be pretrained on larger external dialog datasets."
P19-1373,2019,7 Conclusion and Future Work,"in this paper, unsupervised pretraining has been shown to learn effective representations of dialog context, making this an important research direction for future dialog systems."
P19-1373,2019,7 Conclusion and Future Work,it proposes two novel pretraining objectives: masked-utterance retrieval and inconsistency identification which better capture both the utterance-level and context-level information.
P19-1373,2019,7 Conclusion and Future Work,"second, it would be interesting to test the representations learned using unsupervised pretraining on less-related downstream tasks such as sentiment analysis."
P19-1373,2019,7 Conclusion and Future Work,these results open three future research directions.
P19-1373,2019,7 Conclusion and Future Work,"this paper proposes several methods of unsupervised pretraining for learning strong and general dialog context representations, and demonstrates their effectiveness in improving performance on downstream tasks with limited fine-tuning data as well as out-of-domain data."
P19-1374,2019,6 Conclusion,"conversation disentanglement has been understudied because of a lack of public, annotated datasets."
P19-1374,2019,6 Conclusion,"the models we develop have already enabled new directions in dialogue research, providing disentangled conversations for dstc 7 track 1 (gunasekara et al., 2019; yoshino et al., 2018) and will be used in dstc 8."
P19-1374,2019,6 Conclusion,"this work fills a key gap that has limited research, providing a new opportunity for understanding synchronous multiparty conversation online."
P19-1374,2019,6 Conclusion,"using our data, we perform the first empirical analysis of lowe et al.(2015, 2017)’s widely used data, finding that only 20% of the conversations their method produces are true prefixes of conversations."
P19-1374,2019,6 Conclusion,we also show that diversity is particularly important for the development of robust models.
P19-1374,2019,6 Conclusion,"we introduce a new corpus that is larger and more diverse than any prior corpus, and the first to include context and adjudicated annotations."
P19-1375,2019,5 Conclusion,"empirically, our method advances the previous state-of-the-art dialogue systems in both opendomain and task-oriented scenarios."
P19-1375,2019,5 Conclusion,"in this paper, we introduce a self-supervised task, inconsistent order detection, to explicitly capture the order signal of the dialogue."
P19-1375,2019,5 Conclusion,"theoretically, we believe this self-supervision can be generalized to other types of temporal order in different nlp tasks."
P19-1375,2019,5 Conclusion,we also show how our ssn can contribute to real dialogue learning.
P19-1375,2019,5 Conclusion,"while previous methods suffer from forgetfulness problem when modeling dialogue history, we further propose a sampling-based self-supervised network ssn , to approximately encoding the dialogue history and highlight the order signal."
P19-1376,2019,6 General discussion and conclusions,"although it learns some of the relevant features anyway, it would be interesting to see whether its behaviour becomes more human-like if the correct features are provided in the input."
P19-1376,2019,6 General discussion and conclusions,"although our paper has revealed a number of weaknesses of the ed model, we do agree with k&c that neural network-based cognitive models of inflection deserve re-evaluation in light of recent technical advances."
P19-1376,2019,6 General discussion and conclusions,"as noted by seidenberg and plaut (2014), models’ failures as well as successes can be informative, and we hope that our detailed exploration of the ed model’s behaviour will inspire future developments in these models, both for cognitive modelling and nlp."
P19-1376,2019,6 General discussion and conclusions,"here, we see from both our model’s output and its internal representations that it has correctly identified the necessary voicing distinctions and that nonce words trigger similar representations and behaviour to real words."
P19-1376,2019,6 General discussion and conclusions,"in future, a stricter test might use nonce words that are intentionally less similar to real words (e.g., the example from prasada and pinker (1993): to out-gorbachev)."
P19-1376,2019,6 General discussion and conclusions,"it is also worth pointing out that the ed model, unlike a&h’s model and many earlier connectionist models, is fed raw phonemes (rather than the phonemes’ distinctive features) as input."
P19-1376,2019,6 General discussion and conclusions,"on the other hand, when the model outputs something other than the regular form, its choices are plausible."
P19-1376,2019,6 General discussion and conclusions,"one issue in particular seems to be overproduction of irregulars, which the model consistently prefers to regulars for four verbs (7% of considered nonce verbs), while humans nearly always prefer the regular form."
P19-1376,2019,6 General discussion and conclusions,"our results confirm that, unlike earlier neural net models, the ed model has no trouble learning the past tense forms of verbs it is trained on."
P19-1376,2019,6 General discussion and conclusions,"there are many other potential architectures and modelling decisions that could be explored, as well as other behavioural data such as developmental patterns (blything et al., 2018; ambridge, 2010) and inflection in other languages (e.g., clahsen et al., 1992; ernestus and baayen, 2004)."
P19-1376,2019,6 General discussion and conclusions,"this was an issue with earlier neural net models as well (plunkett and juola, 1999)."
P19-1376,2019,6 General discussion and conclusions,"this was not true for earlier models: plunkett and juola’s model often chose the wrong regular suffix (with incorrect voicing in the final phoneme), and rumelhart and mcclelland’s (1986) model failed to produce regular endings for nonce verbs (prasada and pinker, 1993; marcus, 1998)."
P19-1376,2019,6 General discussion and conclusions,"we found, however, that its behaviour on nonce verbs does not correlate with human experimental data as well as k&c’s results implied, and indeed not as well as that of a&h’s much earlier rule-based model."
P19-1377,2019,6 Conclusion,"finally, our models strongly outperform the state-of-the-art measures in automatic assessment of conceptual complexity."
P19-1377,2019,6 Conclusion,"from the sa perspective, we showed that measures that account for relevance of relations and nodes make a significant impact, and that targeted search in the close proximity of the seeds performs best."
P19-1377,2019,6 Conclusion,our results confirmed the hypothesis that texts are simpler when the concepts therein are highly active at the end of their corresponding sentences.
P19-1377,2019,6 Conclusion,"we introduced a framework for tracking the conceptual complexity of texts during sequential reading, by mimicking human memory processes such as forward and backward semantic priming through spreading activation, sentence wrap-up and forgetting, and implemented a series of unsupervised models within it."
P19-1378,2019,6 Conclusion,"another interesting direction is to explore combining different semantic similarity measures (lin et al., 2015) for our task."
P19-1378,2019,6 Conclusion,"in future work, we will explore ensemble learning."
P19-1378,2019,6 Conclusion,"our error analysis demonstrates that when the predictions of our two models are the same, the prediction is more accurate with high precision, suggesting the idea of combining them."
P19-1378,2019,6 Conclusion,our models achieve state-of-the-art performance on three public datasets.
P19-1378,2019,6 Conclusion,"the breakdown analysis of vua demonstrates that the improvements of our models derive from the problematic instances for our baselines, e.g., conversation articles and adverb metaphors."
P19-1378,2019,6 Conclusion,"the performances of the two models are close in terms of f1 score, as their linguistic fundamentals, mip and spv are similar in principle."
P19-1378,2019,6 Conclusion,"we proposed two metaphor identification models based on metaphor identification procedure (group, 2007; steen et al., 2010) and selectional preference violation (wilks, 1975, 1978)."
P19-1379,2019,5 Conclusion and Future Work,"furthermore, we model the word change from an ecological viewpoint, and sketch two interesting sense behaviors in language evolution, i.e.sense competition and sense cooperation."
P19-1379,2019,5 Conclusion and Future Work,"in addition to tracking the language evolvement in the history, we believe it is promising future work to use deep contextual embeddings in predicting the future change or trend, as well as detecting novel senses that are not included in existing dictionaries."
P19-1379,2019,5 Conclusion and Future Work,"overall, our study sheds some light on diachronic language study with deep contextualized embeddings."
P19-1379,2019,5 Conclusion and Future Work,the experiment shows that our framework is effective in representing word senses and detecting word change.
P19-1379,2019,5 Conclusion and Future Work,"the sense modeling data we built may serve as a basis for further and deeper analysis of linguistic regularities, as well as an important reference of sense granularities for lexicographers6 ."
P19-1379,2019,5 Conclusion and Future Work,this paper proposes a sense representation and tracking framework based on deep contextualized embeddings.
P19-1379,2019,5 Conclusion and Future Work,"with our method, we can find out not only what and when, but also how the word meaning changes from a fine-grained sense level."
P19-1381,2019,5 Conclusion,"compared to the rnns previously tested in the literature, the out-of-the-box fairseq cnn architecture reaches dramatically better performance on the scan compositional generalization tasks."
P19-1381,2019,5 Conclusion,"concerning the comparison with rnns, the best lstm architecture of lake and baroni has two 200-dimensional layers, and it is consequently more parsimonious than our best cnn (1/4 of parameters)."
P19-1381,2019,5 Conclusion,fully understanding generalization of deep seq2seq models might require a less clear-cut view of the divide between statistical pattern matching and symbolic composition.
P19-1381,2019,5 Conclusion,"in future work, we would like to further our insights on the cnn aspects that are crucial for the task, our preliminary analyses of kernel width and attention."
P19-1381,2019,5 Conclusion,"in informal experiments, we found shallow cnns incapable to handle even the simplest random split."
P19-1381,2019,5 Conclusion,"on the other hand, it is hard to train very deep lstms, and it is not clear that the latter models need the same depth cnns require to “view” long sequences."
P19-1381,2019,5 Conclusion,"the cnn is however not learning rule-like compositional generalizations, as its mistakes are nonsystematic and they are evenly spread across different commands."
P19-1381,2019,5 Conclusion,"thus, the cnn achieved a considerable degree of generalization, even on an explicitly compositional benchmark, without something akin to rule-based reasoning."
P19-1381,2019,5 Conclusion,we leave a proper formulation of a tighter comparison to future work.
P19-1382,2019,6 Conclusions,"additionally, our approach outperforms strong baselines for prediction of typological features."
P19-1382,2019,6 Conclusions,"we defined the notion of probabilistic implications, and presented a computational model which successfully identifies known universals, including greenberg universals, but also uncovers new ones, worthy of further investigation by typologists."
P19-1384,2019,4 Conclusions,"finally, we have glossed over potential differences in embedding preferences stemming from differences in types of dependencies (e.g.center vs. tail embedding) and their linearizations (e.g.headinitial vs. head-final), although these differences are likely to play an important role."
P19-1384,2019,4 Conclusions,"however, surprisingly, when sentence complexity (in number of clauses) is accounted for, there is no clear bias against deeper embeddings."
P19-1384,2019,4 Conclusions,"in future work, we will extend our set of languages, aiming at more typological variety (indoeuropean languages are greatly over-represented in our current data)."
P19-1384,2019,4 Conclusions,"large corpora and automated annotation tools are crucial to address this question, as deep embeddings are expectedly rare."
P19-1384,2019,4 Conclusions,"more deeply embedded sentences appear to be shorter (in number of words), and this is in accordance with the hypothesis that they impose a heavier processing load than shallower clauses."
P19-1384,2019,4 Conclusions,"moreover, our results rely on automated annotation, and we have no estimate of the extent to which they are affected by annotation error."
P19-1384,2019,4 Conclusions,our results confirm that there is no sharp boundary on maximum embedding depth.
P19-1384,2019,4 Conclusions,this is a first large quantitative exploration of the matter.
P19-1384,2019,4 Conclusions,"we empirically addressed one central issue in theoretical linguistics, namely the nature and distribution of nested clausal embeddings in natural languages."
P19-1385,2019,6 Conclusions & Future work,"in the future, we aim to incorporate more elaborate linguistic resources (e.g.knowledge bases) and to investigate the performance of our methods on more complex nlp tasks, such as named entity recognition and sequence labelling, where prior knowledge integration is an active area of research."
P19-1385,2019,6 Conclusions & Future work,our approach can be applied to any rnn-based architecture as a extra module to further improve performance with minimal computational overhead.
P19-1385,2019,6 Conclusions & Future work,"our methods are simple, yet effective, achieving consistent performance improvement for all datasets."
P19-1385,2019,6 Conclusions & Future work,we introduce three novel attention-based conditioning methods and compare their effectiveness with traditional concatenation-based conditioning.
P19-1386,2019,5 Conclusion,"in the future, we wish to study the use of knowref to improve performance on general coreference resolution tasks (e.g., the conll 2012 shared tasks)."
P19-1386,2019,5 Conclusion,"our corpus contains difficult problem instances that require a significant degree of common sense and world knowledge for accurate coreference link prediction, and is larger than previous similar datasets."
P19-1386,2019,5 Conclusion,"using a task-specific metric, consistency, we demonstrate that training coreference models on knowref improves their ability to build better representations of the context."
P19-1386,2019,5 Conclusion,we also plan to develop new models on knowref and transfer them to difficult common sense reasoning tasks.
P19-1386,2019,5 Conclusion,"we also show that progress in this capability is linked to reducing gender bias, with our proposed model setting the state of the art on gap."
P19-1386,2019,5 Conclusion,"we present a new corpus and task, knowref, for coreference resolution."
P19-1387,2019,7 Conclusion,in this paper we proposed a novel deep learning based model stre for quality prediction of edits in wikipedia.
P19-1387,2019,7 Conclusion,one of the remarkable findings of this study is that only 20% of training data is able to boost the performance of the model by a significant margin.
P19-1387,2019,7 Conclusion,our model combines word level as well as character level signals in the orthography of wikipedia edits for extracting a rich representation of an edit.
P19-1387,2019,7 Conclusion,"to the best of our knowledge, this is the first work which attempts to predict edit quality of a page by learning signals from similar category pages as well as cross category pages."
P19-1387,2019,7 Conclusion,we believe this work will usher considerable interest in understanding linguistic patterns in wikipedia edit history and application of deep models in this domain.
P19-1387,2019,7 Conclusion,we further show applications of recent advances in transfer learning in this problem and obtain significant improvements in accuracy without compromising training times.
P19-1387,2019,7 Conclusion,we validate our model on a novel data set comprising millions of edits and show efficacy of our approach compared to approaches that utilize handcrafted features and event based modelling.
P19-1388,2019,6 Conclusion and Discussion,a more subtle form of bias is due to attribution.
P19-1388,2019,6 Conclusion and Discussion,"a somewhat different bias is shown in figure 3c; although the temperatures are an adequate representation of the cyclic year, it is highly biased towards the northern hemisphere, a result of the english web source data."
P19-1388,2019,6 Conclusion and Discussion,"as we do not rely on explicit comparisons between objects, but only on co-occurrences with numeric measurements, we can automatically infer relationships post-facto."
P19-1388,2019,6 Conclusion and Discussion,"below, we discuss a few interesting issues brought up by the data collection process that should be addressed in future work."
P19-1388,2019,6 Conclusion and Discussion,figure 1 in the appendix shows the induced distributions of length for these three senses of bat.
P19-1388,2019,6 Conclusion and Discussion,"for example, ‘bat’ can refer to the animal, a baseball bat or a cricket bat."
P19-1388,2019,6 Conclusion and Discussion,"for example, when comparing the size of alfalfa with the size of watermelons as shown in figure 4, we see that alfalfa is mostly talked about in quantities in which it is harvested (order of tons) rather than individual units (grams)."
P19-1388,2019,6 Conclusion and Discussion,"it is interesting to note that in the case of temperatures, both in the u.s states case (figure 3) and the world case (figure 2 in the appendix), the exaggeration is towards hot temperatures, and not cold ones."
P19-1388,2019,6 Conclusion and Discussion,"one form of reporting bias we observe is that people are more likely to discuss objects when they are exceptional, or they exaggerate measurements for rhetorical effect, leading to long tails for some distributions (see “slowest car” in figure 2 and extreme temperatures in figure 3)."
P19-1388,2019,6 Conclusion and Discussion,"polysemy we have not systematically explored how our resource performs on polysemous words and their senses, although our overall results indicate that in most cases the relatively biased distribution of polysemous senses render this a non-problem."
P19-1388,2019,6 Conclusion and Discussion,"reporting bias and exaggeration although reporting bias (gordon and van durme, 2013) would seem to be a problem for a corpus-driven approach, in practice, doq is quite resilient to it due to the usage of very big web corpora and the collection method."
P19-1388,2019,6 Conclusion and Discussion,this kind of bias cannot be identified as easily as the attribution bias discussed in sec.3.3.
P19-1388,2019,6 Conclusion and Discussion,"this paper develops an unsupervised method for collecting quantitative information from a large web corpus, and uses it to create doq, a very large resource consisting of distributions over physical quantities associated with nouns, adjectives, and events."
P19-1388,2019,6 Conclusion and Discussion,we have also observed that in some cases the data itself can help disambiguate between different word senses.
P19-1388,2019,6 Conclusion and Discussion,we have evaluated doq on multiple existing and new datasets and showed that it compares favorably with other methods that require more resources and lack coverage relative to doq.
P19-1388,2019,6 Conclusion and Discussion,"while the distributions for “baseball bat” (which measure about 1m) and “cricket bat” (which may be no more than 956mm) are correct, the distribution for ‘bat’ is probably a consolidation of these, the animal bat that can measure from 15cm to almost 1.7m in length, and some attribution noise (e.g.the distance the bat flew)."
P19-1389,2019,8 Conclusions,experimental results show that the use of sentence function can help improve both types of conversation models in terms of response relevance and informativeness.
P19-1389,2019,8 Conclusions,"this work introduces the stc-sefun dataset, which consists of short-text conversation pairs with their sentence functions manually annotated."
P19-1389,2019,8 Conclusions,"using the large automatically annotated conversation corpus, we train and evaluate both ir-based and generative conversation models, including baselines and improved variants considering the modeling of sentence function in different ways."
P19-1389,2019,8 Conclusions,"we first show that classifiers trained on the stcsefun dataset can be used to automatically annotate a large conversation corpus with highly reliable sentence functions, as well as to estimate the proper response sentence function for a test query."
P19-1390,2019,5 Conclusion,"since progress in dimension-specific essay scoring research is hampered in part by the scarcity of annotated corpora, we designed rubrics for manually scoring 1021 essays along a fundamental yet unexplored dimension of essay quality, thesis strength, as well as the attributes that could impact strength."
P19-1390,2019,5 Conclusion,we believe our annotations will be a valuable resource to the nlp community.
P19-1390,2019,5 Conclusion,we chose to annotate the essays in icle that have previously been scored along multiple dimensions in order to facilitate future developments of joint models that can capture the interactions among different dimensions.
P19-1391,2019,5 Conclusion,"emotions vary substantially in their properties, both linguistic and extra-linguistic, which affects both annotation and modeling, while there is high consistency across the language pair english– german."
P19-1391,2019,5 Conclusion,our modeling experiment shows that the straightforward application of machine translation for model transfer to another language does not lead to a drop in prediction performance.
P19-1391,2019,5 Conclusion,"our two-phase annotation setup shows that perceived emotions can be different from expressed emotions in such eventfocused corpus, which also affects classification performance."
P19-1391,2019,5 Conclusion,"we presented (a) deisear, a corpus of 1001 event descriptions in german, annotated with seven emotion classes; and (b) enisear, a companion english resource build analogously, to disentangle effects of annotation setup and english when comparing to the original isear resource."
P19-1392,2019,5 Conclusions and Further Work,"as we provide resources for three languages (and with different dependency relations), the corpora can be also useful to verify whether some methods behave similarly or not in each language and syntactic pattern."
P19-1392,2019,5 Conclusions and Further Work,"each collocation candidate was revised by three language experts, and those which were dubious were corrected by the whole team of annotators."
P19-1392,2019,5 Conclusions and Further Work,in further work we plan to carry out a multilingual alignment of the collocations in each language.
P19-1392,2019,5 Conclusions and Further Work,"the resource contains 155k tokens and 1, 526 collocations classified into 60 lexical functions."
P19-1392,2019,5 Conclusions and Further Work,"this dataset can serve as a basis to evaluate systems designed to automatically extract collocations and identify their lexical functions, which in turn may be useful for different nlp and corpus linguistics tasks."
P19-1392,2019,5 Conclusions and Further Work,"this paper presented a multilingual corpus with manual annotation of collocations and their lexical functions in english, portuguese, and spanish."
P19-1392,2019,5 Conclusions and Further Work,"this process, also enlarged with other multilingual equivalents, will generate a new dataset for evaluating the automatic translation of this type of multiword expressions."
P19-1392,2019,5 Conclusions and Further Work,we release both the final corpus of each annotator as well as the gold-standard resource in .conllu format.
P19-1393,2019,5 Conclusion,"results show that sense making remains a technical challenge for such models, whereas inference is a key factor that is missing."
P19-1393,2019,5 Conclusion,"we created a benchmark for directly evaluating whether a system has the capability of sense making and explanation, evaluating models trained over the large raw text as well as a common sense database on the test set."
P19-1394,2019,6 Conclusion and Future Work,"in the future, we aim to analyze how changes in the training procedure and hyperparameters of ulmfit affect resulting model performance."
P19-1394,2019,6 Conclusion and Future Work,"in this paper, we introduced a publicly available dataset for humor recognition in russian that exceeds in size all previous public datasets."
P19-1394,2019,6 Conclusion and Future Work,"on top of that, we hope to improve model generalization by augmenting negative examples with a split of jokes into setups and punchlines, as they should not be funny by themselves."
P19-1394,2019,6 Conclusion and Future Work,we also plan to reproduce the experiment on english data.
P19-1394,2019,6 Conclusion and Future Work,"we compared the performance of a baseline svm method and a more sophisticated ulmfit method on this dataset, with the latter yielding favorable results."
P19-1396,2019,5 Conclusion,"for future study, we would like to investigate how to automatically learn the number of latent clusters with nonparametric bayesian methods."
P19-1396,2019,5 Conclusion,miga is able to aggregate short text documents into latent clusters by leveraging meta-info.
P19-1396,2019,5 Conclusion,miga takes advantages of previous models which perform aggregation according to either content or meta-info in short texts.
P19-1396,2019,5 Conclusion,"the experimental results have shown that miga achieves improved performance on document clustering, topic coherence, as well as appealing interpretability."
P19-1396,2019,5 Conclusion,the proposed framework can be easily extended with hierarchical meta-info and word embeddings.
P19-1396,2019,5 Conclusion,"we have presented a new aggregation framework, miga, for short text topic analysis."
P19-1397,2019,6 Conclusion,an ensemble of the encoder and the inverse of the decoder gives even better performance when the invertible constraint is applied on the decoder side.
P19-1397,2019,6 Conclusion,"analyses show that the invertible constraint enforced on the decoder encourages the usual encoder and the invertible decoder to learn from the other one during learning, and provides improved encoding functions after learning."
P19-1397,2019,6 Conclusion,"furthermore, by comparing with prior work, we argue that learning from unlabelled corpora indeed helps to improve the sentence representations, although the current way of utilising corpora might not be optimal."
P19-1397,2019,6 Conclusion,future work could potentially expand our work into an end-to-end invertible model that is able to produce high-quality representations by omnidirectional computations.
P19-1397,2019,6 Conclusion,"the experiments and comparisons are conducted on two large unlabelled corpora, and the performance on the downstream tasks shows the high usability and generalisation ability of the decoders in testing."
P19-1397,2019,6 Conclusion,"the proposed method in our implementation doesn’t provide state-of-the-art performance on the downstream tasks, but we see our method as an opportunity to fuse all possible components in a model, even a usually discarded decoder, to produce sentence representations."
P19-1397,2019,6 Conclusion,"two types of decoders, including an orthonormal regularised linear projection and a bijective transformation, whose inverses can be derived effortlessly, are presented in order to utilise the decoder as another encoder in the testing phase."
P19-1397,2019,6 Conclusion,"we view our work as unifying the generative and discriminative objectives for unsupervised sentence representation learning, as the decoder is trained with a generative objective which when inverted can be seen as creating a discriminative target."
P19-1398,2019,5 Conclusion,"our method, context vector data description (cvdd), enables contextual anomaly detection and has strong interpretability features."
P19-1398,2019,5 Conclusion,"we demonstrated the detection performance of cvdd empirically and showed qualitatively that cvdd is well capable of learning distinct, diverse contexts from unlabeled text corpora."
P19-1398,2019,5 Conclusion,"we presented a new self-attentive, multi-context one-class classification approach for unsupervised anomaly detection on text which makes use of pretrained word models."
P19-1399,2019,6 Conclusion,"empirical results demonstrate that hnn effectively reduces hubs, and can outperform nn, isf and other state-ofthe-art."
P19-1399,2019,6 Conclusion,"future works include applying the method to more language pairs and more domain-specific lexicon induction, e.g., terminologies."
P19-1399,2019,6 Conclusion,"hnn connects to nn, and also sheds light on a recent hubness-preventing method called inverted softmax (isf)."
P19-1399,2019,6 Conclusion,the hubless nearest neighbor (hnn) is proposed by assuming an “equal preference” constraint.
P19-1399,2019,6 Conclusion,"this paper studies how to reduce hubness during retrieval, a crucial step for bilingual lexicon induction (bli)."
P19-1400,2019,7 Conclusions,"besides, we will consider additional knowledge bases (e.g.yago and wikidata)."
P19-1400,2019,7 Conclusions,"experimental results showed that our models substantially outperform the heuristic baseline, and, for certain categories, they approach the model estimated with supervised learning."
P19-1400,2019,7 Conclusions,"in future work we will aim to improve candidate selection (including different strategies to select candidate lists e+, e?)."
P19-1400,2019,7 Conclusions,"the classifier lets us disregard noisy labels, resulting in a more accurate entity linking model."
P19-1400,2019,7 Conclusions,"we introduced the first approach to entity linking which neither uses annotated texts, nor assumes that entities are associated with textual documents (e.g., wikipedia articles)."
P19-1400,2019,7 Conclusions,"we learn the model using the mil paradigm, and introduce a novel component, a noise detecting classifier, estimated jointly with the el model."
P19-1400,2019,7 Conclusions,we will also use extra document information and jointly predict entities for different mentions in the document.
P19-1401,2019,7 Conclusion,"our approach is the first study to interleave (i) the wake phase, where the al policy is exploited to improve the student learner and (ii) the dream phase, where the student learner in turn acts as an imperfect annotator to enhance the al policy."
P19-1401,2019,7 Conclusion,"this allows the learning of a policy from scratch, or adapt a pretrained al policy on the target task, without requiring additional annotation budget."
P19-1401,2019,7 Conclusion,we have introduced a dream-based approach to directly learn pool-based al query strategies on the target task of interest.
P19-1401,2019,7 Conclusion,"we provide comprehensive experimental results, comparing our method to strong heuristic-based and al policy learningbased methods on several classification and sequence learning tasks, showing the effectiveness of our proposed method."
P19-1402,2019,5 Conclusion,experiments on both benchmark corpus and downstream tasks demonstrate the superiority of hice over existing approaches.
P19-1402,2019,5 Conclusion,we formulated the problem as a k-shot regression problem and proposed a hierarchical context encoder (hice) architecture that learns to predict the oracle oov embedding by aggregating only k contexts and morphological features.
P19-1402,2019,5 Conclusion,we further adopt maml for fast and robust adaptation to mitigate semantic gap between corpus.
P19-1402,2019,5 Conclusion,we studied the problem of learning accurate embedding for out-of-vocabulary word and augment them to a per-trained embedding by only a few observations.
P19-1403,2019,6 Conclusion,"our experiments on six corpora covering two languages show that there are shifts in word usage and context over time, and that it is useful to explicitly account for these shifts in representations of words and documents."
P19-1403,2019,6 Conclusion,"we have presented a new method for constructing diachronic word embeddings as well as a new model for document classification, which are both shown to be effective for temporality adaptation."
P19-1403,2019,6 Conclusion,we open source our code.
P19-1404,2019,6 Conclusion & Future Work,"finally, we demonstrated the efficacy of the proposed approach for cross-domain classification on different datasets."
P19-1404,2019,6 Conclusion & Future Work,"in the cross-domain task, the common part of the representation performs best when it is isolated from the source specific part."
P19-1404,2019,6 Conclusion & Future Work,"on the contrary, both the source specific and common parts of the representation come along for efficient performance in the source domain task."
P19-1404,2019,6 Conclusion & Future Work,one part captures the source specific characteristics that are discriminating for learning in the source domain.
P19-1404,2019,6 Conclusion & Future Work,the major contribution of this work is to learn the common shared representation between domains by explicitly disentangling the source specific characteristics so as not to detract the capabilities of common representation for the crossdomain task.
P19-1404,2019,6 Conclusion & Future Work,the other part captures the common representation between the source and target domain pair which contributes to both source domain learning as well as generalizes to the unlabelled target domain task.
P19-1404,2019,6 Conclusion & Future Work,the paper proposed a novel neural network learning algorithm based on the principle of learning a two-part representation where each part optimizes for different objective.
P19-1405,2019,6 Conclusions,"finally, three experiments on chunking tasks are performed to illustrate the validity of the bayes test."
P19-1405,2019,6 Conclusions,"for nlp practitioners, we recommend here three guidelines: (1) a t-test should be avoided in a comparison of two nlp models on the basis of the precision, recall and f1 measure.(2) the 3 × 2 bcv could be preferred to evaluate the performance of an nlp model in the task of model comparison.(3) the bayes test on the basis of the 3 × 2 bcv could provide informative and fine-grained measures of the differences of precisions, recalls and f1 measures of two nlp models, and the measures could help practitioners to make a reasonable decision."
P19-1405,2019,6 Conclusions,"in the future, we will refine the bayes test of p, r, and f1 in an m × 2 bcv and provide accurate interval estimation of other evaluation metrics on the basis of the confusion matrix."
P19-1405,2019,6 Conclusions,"in this study, we obtained accurate posterior distributions of p, r, and f1 on the basis of a 3×2 bcv, which is an essential part in conducting the comparison of two nlp models."
P19-1405,2019,6 Conclusions,obtaining the posterior distribution of an evaluation metric of a model is still a key problem in this valuable research area.
P19-1405,2019,6 Conclusions,"on the basis of the posterior distributions, a bayes test is proposed, which provides the probabilities of the hypotheses and help users to make a reasonable decision."
P19-1406,2019,6 Conclusions,"in this paper, we propose a general inference algorithm for text infilling."
P19-1406,2019,6 Conclusions,"the results show that the proposed method is an effective and efficient approach for fill-in-theblank tasks, consistently outperforming all baselines."
P19-1406,2019,6 Conclusions,"to the best of our knowledge, the method is the first inference algorithm that does not require any modification or training of the model and can be broadly used in any sequence generative model to solve the fill-in-theblank tasks."
P19-1406,2019,6 Conclusions,we compare the proposed method and several strong baselines on three text infilling tasks with various mask ratios and different mask strategies.
P19-1407,2019,7 Conclusion,"by letting the decoder ’keep notes’ on the encoder, or said another way, re-encode the input at every decoding step, the scratchpad mechanism effectively guides future generation."
P19-1407,2019,7 Conclusion,"in addition, our approach decreases training time and model complexity compared to other leading approaches."
P19-1407,2019,7 Conclusion,"in this paper, we introduce the mechanism, a novel write operator, to the sequence to sequence framework aimed at addressing many of the common issues encountered by sequence to sequence models and evaluate it on a variety of standard conditional natural language generation tasks."
P19-1407,2019,7 Conclusion,"our success on such a diverse set of tasks, input data, and volumes of training data underscores the generalizability of our approach and its conceptual simplicity make it easy to add to any sequence to sequence model with attention."
P19-1407,2019,7 Conclusion,"the scratchpad mechanism attains state of the art in machine translation, question generation, and summarization on standard metrics and human evaluation across multiple datasets."
P19-1408,2019,8 Conclusions,a future direction is to investigate the effect of using mina spans not only in evaluation but also for training existing coreference resolvers.
P19-1408,2019,8 Conclusions,"based on our analysis on the muc and arrau datasets, extracted minimum spans are compatible with those that are manually annotated by experts."
P19-1408,2019,8 Conclusions,coreference evaluation based on maximum spans directly penalizes coreference resolvers because of parsing complexities and also small noises in mention boundary detection.
P19-1408,2019,8 Conclusions,"for instance, detecting the coreference relation of the two nested mentions in “[a deutsche mark based currency board where we have a foreign governor on [the board](1)](1)” 13 would be more straightforward knowing that the minimum span of the first mention is “a currency board”."
P19-1408,2019,8 Conclusions,"if we provide automatically extracted minimum spans alongside maximum spans to the annotators, the annotation of coreference relations may get easier."
P19-1408,2019,8 Conclusions,"in addition to coreference evaluation, automatically extracted minimum spans can benefit the annotation process of new corpora."
P19-1408,2019,8 Conclusions,"in this paper, we propose the mina algorithm to automatically extract minimum spans without introducing additional annotation costs."
P19-1408,2019,8 Conclusions,"investigating the use of mina in other nlp areas, e.g., evaluating spans in named entity recognition or reading comprehension, is another future line of work."
P19-1408,2019,8 Conclusions,maximum spans are recoverable given the mina spans and their corresponding parse trees.
P19-1408,2019,8 Conclusions,mina automatically extracts corresponding minimum spans for both gold and system mentions and uses the resulting minimum spans in the standard evaluation metrics.
P19-1408,2019,8 Conclusions,"minimum span annotation is expensive, and therefore it is not a scalable solution for large coreference corpora."
P19-1408,2019,8 Conclusions,"our results show that the use of minimum spans in coreference evaluation is of particular importance for cross-dataset settings, in which the detected maximum boundaries are noisier."
P19-1408,2019,8 Conclusions,the incorporation of automatically extracted minimum spans reduces the effect of maximum boundary detection errors in coreference evaluation and results in a fairer comparison.
P19-1408,2019,8 Conclusions,"therefore, we can use mina spans for training and testing coreference models and then retrieve their corresponding maximum spans for evaluation."
P19-1408,2019,8 Conclusions,this is a known problem that is addressed by manually annotating minimum spans as well as maximum spans in several corpora.
P19-1409,2019,8 Conclusion,"future directions include investigating ways to minimize the pipeline errors from the extraction of predicate-argument structures, and incorporating a mention prediction component, rather than relying on gold mentions."
P19-1409,2019,8 Conclusion,"our model achieves state-of-the-art results, outperforming previous models by 10.5 conll f1 points on events, and providing the first cross-document entity coreference results on ecb+."
P19-1409,2019,8 Conclusion,we presented a neural approach for resolving cross-document event and entity coreference.
P19-1409,2019,8 Conclusion,"we represent a mention using its text, context, and— inspired by the joint model of lee et al.(2012)— we make an event mention representation aware of coreference clusters of entity mentions to which it is related via predicate-argument structures, and vice versa."
P19-1410,2019,6 Conclusions,"also, in the future, we would like to investigate how our approach can be generalized to other discourse frameworks such as the penn discourse treebank (pdtb)."
P19-1410,2019,6 Conclusions,"apart from the effectiveness, our system is 6 times faster than the fastest available system."
P19-1410,2019,6 Conclusions,"based on what we have done so far, it is natural for us to move our focus from sentence-level to document-level rst parsing."
P19-1410,2019,6 Conclusions,"both our segmenter and parser achieve state-of-the-art results outperforming existing systems by a wide margin, without using any handcrafted features."
P19-1410,2019,6 Conclusions,we also train the segmenter and the parser jointly through the encoder-decoder architecture and improve the results further.
P19-1410,2019,6 Conclusions,we have proposed a unified framework for sentence-level discourse analysis based on pointer networks that constructs a discourse tree in linear time.
P19-1411,2019,5 Conclusion,"in the future work, we plan to extend the idea of multi-task learning/transfer learning with label embeddings to the problems in information extraction (e.g., event detection, relation extraction, entity mention detection) (nguyen and grishman, 2015a,b, 2016d; nguyen et al., 2016a,b,c; nguyen and nguyen, 2018b, 2019)."
P19-1411,2019,5 Conclusion,"in these problems, the labels are often organized in the hierarchies (e.g., types, subtypes) and the label embeddings can exploit such hierarchies to transfer the knowledge between different label-specific prediction tasks."
P19-1411,2019,5 Conclusion,"our proposed model features the embeddings of the implicit connectives and discourse relations, and the three penalization terms to encourage the knowledge sharing between the prediction tasks."
P19-1411,2019,5 Conclusion,we achieve the state-of-the-art performance on different settings for the popular dataset pdtb for idrr.
P19-1411,2019,5 Conclusion,we present a novel multi-task learning model for idrr with deep learning.
P19-1412,2019,6 Conclusion,"both models are able to identify the polarity of commitment, but cannot capture its gradience."
P19-1412,2019,6 Conclusion,"however, they are not able to generalize to other linguistic environments such as conditional, modal, and neg-raising, which display inference patterns that are important for information extraction."
P19-1412,2019,6 Conclusion,"in the long run, to perform robust language understanding, models will need to incorporate more linguistic foreknowledge and be able to generalize to a wider range of linguistic constructions."
P19-1412,2019,6 Conclusion,"our evaluation of two soa models for speaker commitment on the commitmentbank shows that the models perform better on sentences with negation, and with nonfactive embedding verbs."
P19-1412,2019,6 Conclusion,"the rulebased model, outperforming the bilstm models on the full commitmentbank, shows that a linguistically-informed model scales more successfully to challenging naturalistic data."
P19-1413,2019,5 Summary,"in the future we would like to expand this direction, and find ways to connect event and relation representation, learning and inference in a unified framework."
P19-1413,2019,5 Summary,this work lays the foundation for reasoning over narratives and explaining how sentences combine to form them.
P19-1413,2019,5 Summary,"we consider the problem of learning relationaware event embeddings for commonsense inference, which can account for different relations between events, beyond simple event similarity."
P19-1413,2019,5 Summary,"we evaluated and compared two models, eventtranse and eventtransr, on several narrative cloze and relation-specific tasks, and showed the learned embedding can capture relation-specific information as well as improve performance for a downstream task."
P19-1413,2019,5 Summary,"we include several event relations, identifying, for example, the causes for them."
P19-1413,2019,5 Summary,"we show that weak supervision, provided by a rule-based annotator is enough for training our models."
P19-1414,2019,6 Conclusion and Future Work,"as a future work, we plan to introduce a wider range of background knowledge including another type of event causality (hashimoto et al., 2012, 2014, 2015; kruengkrai et al., 2017)."
P19-1414,2019,6 Conclusion and Future Work,"in our why-qa method, causality expressions extracted from the web were used as background knowledge for computing causality-attention/embeddings."
P19-1414,2019,6 Conclusion and Future Work,it employed adversarial learning to generate vector representations of reasons or true answers from answer passages and exploited the representations for judging whether the passages are proper answer passages to the given whyquestions.
P19-1414,2019,6 Conclusion and Future Work,"through experiments using japanese why-qa datasets, we showed that this idea improved why-qa performance."
P19-1414,2019,6 Conclusion and Future Work,we also showed that our method improved the performance in a distantly supervised open-domain qa task.
P19-1414,2019,6 Conclusion and Future Work,we proposed a method for why-question answering (why-qa) that used an adversarial learning framework.
P19-1415,2019,5 Conclusions,a pair-to-sequence model is introduced in order to capture the interactions between question and paragraph.
P19-1415,2019,5 Conclusions,"as for future work, we would like to enhance the ability to utilize antonyms for unanswerable question generation by leveraging external resources."
P19-1415,2019,5 Conclusions,both automatic and human evaluations show that the proposed model consistently outperforms the sequence-tosequence baseline.
P19-1415,2019,5 Conclusions,"in this paper, we propose to generate unanswerable questions as a means of data augmentation for machine reading comprehension."
P19-1415,2019,5 Conclusions,the results on the squad 2.0 dataset show that our generated unanswerable questions can help to improve multiple reading comprehension models.
P19-1415,2019,5 Conclusions,we also present a way to construct training data for unanswerable question generation models.
P19-1415,2019,5 Conclusions,we produce relevant unanswerable questions by editing answerable questions and conditioning on the corresponding paragraph.
P19-1416,2019,6 Conclusions,"for example, we fail to retrieve the paragraph about “bonobo apes” in figure 1, because the question does not contain terms about “bonobo apes.” table 5 shows that the model achieves 39.12 f1 given 500 retrieved paragraphs, but achieves 53.12 f1 when additional two gold paragraphs are given, demonstrating the significant effect of failure to retrieve gold paragraphs."
P19-1416,2019,6 Conclusions,"for example, we found 35% of bridge questions are currently single-hop but may become multi-hop when combined with stronger distractors (section 4.1)."
P19-1416,2019,6 Conclusions,"however, as we demonstrate in section 5, selecting strong distractors for rc questions is non-trivial."
P19-1416,2019,6 Conclusions,"in particular, table 5 shows that single-paragraph bert achieves 53.12 f1 even when using 500 distractors (rather than eight), indicating that 500 distractors are still insufficient."
P19-1416,2019,6 Conclusions,"in summary, we demonstrate that question compositionality is not a sufficient condition for multi-hop reasoning."
P19-1416,2019,6 Conclusions,"in this context, we suggest that future work can explore better retrieval methods for multi-hop questions."
P19-1416,2019,6 Conclusions,"in this end, future multi-hop rc datasets can develop improved methods for distractor collection."
P19-1416,2019,6 Conclusions,"instead, future datasets must carefully consider what evidence they provide in order to ensure multi-hop reasoning is required."
P19-1416,2019,6 Conclusions,open-domain questions our single-hop model struggles in the open-domain setting.
P19-1416,2019,6 Conclusions,retrieving strong distractors another way to ensure multi-hop reasoning is to select strong distractor paragraphs.
P19-1416,2019,6 Conclusions,there are at least two different ways to achieve this.
P19-1416,2019,6 Conclusions,we largely attribute this to the insufficiencies of standard tfidf retrieval for multi-hop questions.
P19-1416,2019,6 Conclusions,we suspect this is also due to the insufficiencies of standard tfidf retrieval for multi-hop questions.
P19-1417,2019,5 Conclusion,"in future work, we will extend the proposed idea to other qa tasks with evidence of multimodality, e.g.combining with symbolic approaches for visual qa (gan et al., 2017; mao et al., 2019; hu et al., 2019)."
P19-1417,2019,5 Conclusion,"the results show that (1) with the graph attention technique, we can efficiently and accurately accumulate question-related knowledge for each kb entity in one-pass of the kb sub-graph; (2) our designed gating mechanisms could successfully incorporate the encoded entity knowledge while processing the text documents."
P19-1417,2019,5 Conclusion,"we present a new qa model that operates over incomplete kb and text documents to answer opendomain questions, which yields consistent improvements over previous methods on the webqsp benchmark with incomplete kbs."
P19-1418,2019,5 Conclusion,"our method does not need expert knowledge of intermediate structures of the target sequences, and achieves stronger results than the existing neural semantic parsers."
P19-1418,2019,5 Conclusion,we present the adansp that adaptively searches the corresponding computation structure of rnns for semantic parsing.
P19-1419,2019,8 Conclusion,"distinguishing features between legal and illegal texts are consistent enough between domains, so that a classifier trained on drug-related websites can be straightforwardly ported to classify legal and illegal texts from another darknet domain (forums)."
P19-1419,2019,8 Conclusion,"in this paper we identified several distinguishing factors between legal and illegal texts, taking a variety of approaches, predictive (text classification), application-based (named entity wikification), as well as an approach based on raw statistics."
P19-1419,2019,8 Conclusion,"our results also show that in terms of vocabulary, legal texts and illegal texts are as distant from each other, as from comparable texts from ebay."
P19-1419,2019,8 Conclusion,"our results revealed that legal and illegal texts on the darknet are not only distinguishable in terms of their words, but also in terms of their shallow syntactic structure, manifested in their pos tag and function word distributions."
P19-1419,2019,8 Conclusion,"we conclude from this investigation that onion pages provide an attractive testbed for studying distinguishing factors between the text of legal and illegal webpages, as they present challenges to offthe-shelf nlp tools, but at the same time have sufficient self-consistency to allow studies of the linguistic signals that separate these classes."
P19-1420,2019,6 Conclusion,"in this paper, we explored automated cta transcript parsing, which is a challenging task due to the lack of direct supervision data and the requirement of document level understanding."
P19-1420,2019,6 Conclusion,our evaluation on manually labeled test set shows the effectiveness of our framework.
P19-1420,2019,6 Conclusion,we noticed the importance of context in the cta parsing task and exploited model variants to make use of context information.
P19-1420,2019,6 Conclusion,we proposed a weakly supervised framework to utilize the full information in data.
P19-1421,2019,6 Conclusions and Future Work,experimental results on online courses with different domains validate the effectiveness of the proposed method.
P19-1421,2019,6 Conclusions and Future Work,"moreover, we design a gamebased mechanism to subtly involve human efforts in model optimization."
P19-1421,2019,6 Conclusions and Future Work,"promising future directions would be to investigate how to utilize user interaction in moocs more adequately, as well as how attributes of course concepts can help expanding."
P19-1421,2019,6 Conclusions and Future Work,we conducted a new investigation on automatically course concept expansion in moocs.
P19-1421,2019,6 Conclusions and Future Work,we precisely define the problem and propose an active model to search external knowledge base for candidate concepts and detect high-quality ones with a classifier.
P19-1423,2019,6 Conclusion,a gcnn model is employed to encode the graph structure and mil is incorporated to aggregate the multiple mention-level pairs .
P19-1423,2019,6 Conclusion,"although the model is applied to biochemistry corpora for inter-sentence re, our method is also applicable to other relation extraction tasks."
P19-1423,2019,6 Conclusion,analysis showed that all edge types are effective for inter-sentence re.
P19-1423,2019,6 Conclusion,"as future work, we plan to incorporate joint named entity recognition training as well as sub-word embeddings in order to further improve the performance of the proposed model."
P19-1423,2019,6 Conclusion,the graph is constructed with words as nodes and multiple intra- and inter-sentence dependencies between them as edges.
P19-1423,2019,6 Conclusion,we proposed a novel graph-based method for inter-sentence re using a labelled edge gcnn model on a document-level graph.
P19-1423,2019,6 Conclusion,we show that our method achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets.
P19-1423,2019,6 Conclusion,we tuned the number of labelled edges to maintain the number of parameters in the labelled edge gcnn.
P19-1424,2019,7 Limitations and Future Work,"attention scores (fig.1) provide some indications of which parts of the texts affect the predictions most, but are far from being justifications that legal practitioners could trust; see also jain and wallace (2019)."
P19-1424,2019,7 Limitations and Future Work,"finally, we plan to adapt bespoke models proposed for the chinese criminal court (luo et al., 2017; zhong et al., 2018; hu et al., 2018) to data from other courts and explore multitask learning."
P19-1424,2019,7 Limitations and Future Work,"providing valid justifications is an important priority for future work and an emerging topic in the nlp community.8 in this direction, we plan to expand the scope of this study by exploring the automated analysis of additional resources (e.g., relevant case law, dockets, prior judgments) that could be then utilized in a multi-input fashion to further improve performance and justify system decisions."
P19-1424,2019,7 Limitations and Future Work,"the neural models we considered outperform previous feature-based models, but provide no justification for their predictions."
P19-1424,2019,7 Limitations and Future Work,"we also plan to apply neural methods to data from other courts, e.g., the european court of justice, the us supreme court, and multiple languages, to gain a broader perspective of their potential in legal justice prediction."
P19-1425,2019,6 Conclusion,experimental results on chinese-english and english-german translation tasks demonstrate the capability of our approach to improving both the translation performance and the robustness.
P19-1425,2019,6 Conclusion,"in future work, we plan to explore the direction to generate more natural adversarial examples dispensing with word replacements and more advanced defense approaches such as curriculum learning (jiang et al., 2018, 2015)."
P19-1425,2019,6 Conclusion,"in this work, we have presented an approach to improving the robustness of the nmt models with doubly adversarial inputs."
P19-1425,2019,6 Conclusion,we have also introduced a white-box method to generate adversarial examples for nmt.
P19-1426,2019,6 Conclusion,"compared to word-level oracle, sentence-level oracle can further equip the model with the ability of overcorrection recovery."
P19-1426,2019,6 Conclusion,the end-to-end nmt model generates a translation word by word with the ground truth words as context at training time as opposed to the previous words generated by the model as context at inference.
P19-1426,2019,6 Conclusion,"the predicted words, referred to as oracle words, can be generated with the wordlevel or sentence-level optimization."
P19-1426,2019,6 Conclusion,"to make the model fully exposed to the circumstance at reference, we sample the context word with decay from the ground truth words."
P19-1426,2019,6 Conclusion,"to mitigate the discrepancy between training and inference, when predicting one word, we feed as context either the ground truth word or the previous predicted word with a sampling scheme."
P19-1426,2019,6 Conclusion,we also conclude that the sentence-level oracle show superiority over the word-level oracle.
P19-1426,2019,6 Conclusion,"we verified the effectiveness of our method with two strong baseline models and related works on the real translation tasks, achieved significant improvement on all the datasets."
P19-1427,2019,9 Conclusion,"our analysis also shows that using this metric eases optimization and the translations tend to be richer in correct, semantically important words."
P19-1427,2019,9 Conclusion,"this is the first time to our knowledge that a continuous metric of semantic similarity has been proposed for nmt optimization and shown to outperform sentence-level bleu, and we hope that this can be the starting point for more research in this direction."
P19-1427,2019,9 Conclusion,"we have found that simile not only outperforms bleu on automatic evaluations, it correlates better with human judgments as well."
P19-1427,2019,9 Conclusion,"we have proposed simile, an alternative to bleu for use as a reward in minimum risk training."
P19-1428,2019,7 Conclusions and future work,"another issue to research will be the use of regression models to estimate the expected fitness of a pipeline given its features, as illustrated in section 6."
P19-1428,2019,7 Conclusions and future work,"as future work, we plan to study the introduction of high-level knowledge to deal with the issue of invalid pipelines and improve the performance of the optimization process."
P19-1428,2019,7 Conclusions and future work,"finally, by modifying the grammar, this strategy can be extensible to other machine learning challenges."
P19-1428,2019,7 Conclusions and future work,"in addition, the data gathered during the optimization provided insights about the optimal settings to deal with the challenge faced."
P19-1428,2019,7 Conclusions and future work,our proposal involves an optimization process which explores a large space of possible pipelines and chooses the best performing ones automatically.
P19-1428,2019,7 Conclusions and future work,"the evaluation was performed on a complex scenario of the tass 2018 ehealth-kd challenge, where the best pipeline discovered improves over the stateof-the-art by combining features, decisions and strategies from different authors."
P19-1428,2019,7 Conclusions and future work,"therefore, we plan to explore this line of research in the future, to compare our proposal with other automl frameworks in standard benchmarks."
P19-1428,2019,7 Conclusions and future work,these results show that an automl strategy based on grammatical evolution is effective for optimizing machine learning pipelines to solve knowledge discovery challenges from natural language text.
P19-1428,2019,7 Conclusions and future work,"this addition would support meta-learning algorithms, allowing to reduce the optimization time and increase its performance by learning from past executions."
P19-1428,2019,7 Conclusions and future work,this knowledge can be in the form of explicit rules that guarantee the validity of the pipelines sampled from the grammar; and in the form of statistical information extracted from similar challenges that helps pre-defining a probabilistic model.
P19-1428,2019,7 Conclusions and future work,this paper presents an automatic machine learning strategy based on probabilistic grammatical evolution to extract knowledge from health documents in spanish language.
P19-1429,2019,6 Conclusions,experimental results demonstrate the effectiveness of our method.
P19-1429,2019,6 Conclusions,"for future work, we plan to investigate new auxiliary ?-learning algorithms using our ?-learning framework."
P19-1429,2019,6 Conclusions,"representation learning is a fundamental technique for nlp tasks, especially for resolving the ambiguity and the diversity problem of natural language expressions."
P19-1429,2019,6 Conclusions,"specifically, two effective ?-learning algorithms are proposed to distill discrimination and generalization knowledge independently, and a lexical gate mechanism is designed to fuse different knowledge adaptively."
P19-1429,2019,6 Conclusions,"this paper proposes a new representation learning framework – ?-learning, which can distill both discrimination and generalization knowledge for event detection."
P19-1430,2019,5 Conclusion and Future Work,"although we have used word, sense and character information in our work, more level of information can be incorporated into the mg lattice."
P19-1430,2019,5 Conclusion and Future Work,"from coarse to fine, sememe-level information can be intuitively valuable."
P19-1430,2019,5 Conclusion and Future Work,"from fine to coarse, sentences and paragraphs should be taken into account so that a border range of contextual information can be captured."
P19-1430,2019,5 Conclusion and Future Work,"here, sememe is the minimum semantic unit of word sense, whose information may potentially assist the model to explore deeper semantic features."
P19-1430,2019,5 Conclusion and Future Work,"in the future, we will attempt to improve the ability of the mg lattice to utilize multi-grained information."
P19-1430,2019,5 Conclusion and Future Work,"in this paper, we propose the mg lattice model for chinese relation extraction."
P19-1430,2019,5 Conclusion and Future Work,"the model incorporates word-level information into character sequences to explore deep semantic features and avoids the issue of polysemy ambiguity by introducing external linguistic knowledge, which is regarded as sense-level information."
P19-1430,2019,5 Conclusion and Future Work,"the results show that our model significantly outperforms other proposed methods, reaching the state-of-the-art results on all datasets."
P19-1430,2019,5 Conclusion and Future Work,we comprehensively evaluate our model on various datasets.
P19-1431,2019,5 Conclusion,"future research will look into applying such methods to reason jointly about text and kg, by attending to textual mentions of entities in addition to graph (verga et al., 2016)."
P19-1431,2019,5 Conclusion,the model has attractive properties as it is interpretable and its number of parameters do not depend on the size of entity neighborhoods.
P19-1431,2019,5 Conclusion,the model performs favorably when compared with state-of-theart models for kg completion.
P19-1431,2019,5 Conclusion,"we proposed a2n, an attention-based model for learning query-dependent entity embeddings based on graph neighborhood."
P19-1432,2019,5 Conclusion & Future Work,"consequently, in the future work, we plan to develop methods that can automatically induce the sentence structures for efp."
P19-1432,2019,5 Conclusion & Future Work,one potential issue with the current approach is that it is dependent on the existence of the highquality dependency parser.
P19-1432,2019,5 Conclusion & Future Work,"unfortunately, such parser is not always available in different domains and languages."
P19-1432,2019,5 Conclusion & Future Work,we achieve the state-ofthe-art performance over several efp datasets.
P19-1432,2019,5 Conclusion & Future Work,we present a graph-based deep learning model for efp that exploits both syntactic and semantic structures of the sentences to effectively model the important context words.
P19-1433,2019,4 Conclusion,"in this paper, we propose a framework to learn temporally-aware timex embeddings from synthetic data."
P19-1433,2019,4 Conclusion,"through experiments on two datasets, we show that incorporating these embeddings in deep temporal models leads to an improvement in the overall temporal classification performance."
P19-1434,2019,5 Conclusion,further qualitative analysis of memory contents after learning confirms that such good performance comes from its ability to retain important instances for future qa tasks.
P19-1434,2019,5 Conclusion,"to handle this problem, we proposed episodic memory reader (emr), which is basically a memory-augmented network with rl-based memory-scheduler, that learns the relative importance among memory entries and replaces the entries with the lowest importance to maximize the qa performance for future tasks."
P19-1434,2019,5 Conclusion,"we proposed a novel problem of question answering from streaming data, where the model needs to answer a question that is given after reading through unlimited amount of context (e.g.documents, videos) that cannot fit into the system memory."
P19-1434,2019,5 Conclusion,"we validated emr on three qa datasets against rule-based memory scheduling as well as an rlbaseline that does not model relative importances among memory entries, which it significantly outperforms."
P19-1435,2019,7 Conclusion,"for example, we still do not know the details of dataset preparations of many other nlsm datasets, and we can not say to what extent the assumptions in section 4 hold in quoraqp and what is the relationship between the leakage-neutral distribution and the real-world distribution."
P19-1435,2019,7 Conclusion,"furthermore, they could reveal the more detailed strategy of sample selection, and might publish some official weights to eliminate the bias."
P19-1435,2019,7 Conclusion,"however, there is still much to do to form a clearer scope of this problem."
P19-1435,2019,7 Conclusion,"in this paper, we take a close look at the selection bias of nlsm datasets and focus on the selection bias embodied in the comparing relationships of sentences."
P19-1435,2019,7 Conclusion,"to mitigate the bias, we propose an easy-adopting method for leakage-neutral learning and evaluations."
P19-1435,2019,7 Conclusion,"we suggest for future nlsm datasets, the providers should pay more attention to this problem."
P19-1436,2019,7 Conclusion,future work includes better phrase representation learning to close its accuracy gap with qa models with query-dependent document encoding.
P19-1436,2019,7 Conclusion,"on squad-open, our experiments show that our model can read words 6k times faster under a controlled environment and 68 times faster in a real setup than drqa while achieving 3.8% higher em."
P19-1436,2019,7 Conclusion,"our phrase representations leverage sparse and dense vectors to capture lexical, semantic, and syntactic information."
P19-1436,2019,7 Conclusion,utilizing the phrase index as an external memory for an interaction with text-based knowledge is also an interesting direction.
P19-1436,2019,7 Conclusion,we believe that even further speedup and larger coverage of documents can be done with a similarity search package for dense+sparse vectors.
P19-1436,2019,7 Conclusion,we introduce a model for real-time open-domain question answering by learning indexable phrase representations independent of the query.
P19-1437,2019,7 Conclusion,experiments demonstrate that our method improves language modeling on new corpus in terms of both convergence speed and perplexity.
P19-1437,2019,7 Conclusion,"in this work, we aim to improve language modeling with shared grammar."
P19-1437,2019,7 Conclusion,our framework contains two probabilistic components: a constituency parser and a joint generative model.
P19-1437,2019,7 Conclusion,"the parser encodes grammar knowledge in natural language, which helps language model quickly land on new corpus."
P19-1437,2019,7 Conclusion,we also propose algorithms for jointly training the two components to fine-tune the language model on new corpus without parsing annotations.
P19-1438,2019,7 Conclusion,our framework is designed to allow the definition of new domains and collecting examples with minimal effort.
P19-1438,2019,7 Conclusion,our new parser substantially outperforms the original parser and we further show that each of our zeroshot components is vital for this improvement.
P19-1438,2019,7 Conclusion,promising future directions include experimenting with our zero-shot adaptation methods in the context of neural semantic parsing (after increasing the number of examples per domain) and extending the dataset to include more complicated applications and multi-utterance instructions.
P19-1438,2019,7 Conclusion,we hope this work will inspire readers to use our framework for collecting a larger dataset and experimenting with more approaches.
P19-1438,2019,7 Conclusion,"we presented a novel task of zero shot semantic parsing for instructions, and introduced a new dataset."
P19-1438,2019,7 Conclusion,"we proposed a new training algorithm as well as features and filtering logic that should enhance zero-shot learning, and integrated them into the fparser (pasupat and liang, 2015)."
P19-1439,2019,8 Conclusions,intermediate training of language models can yield modest further gains.
P19-1439,2019,8 Conclusions,multitask pretraining can produce results better than any single task can.
P19-1439,2019,8 Conclusions,multitask pretraining fails to reliably produce models better than their best individual components.
P19-1439,2019,8 Conclusions,"our primary results are perhaps unsurprising: lm works well as a pretraining task, and no other single task is consistently better."
P19-1439,2019,8 Conclusions,"target task performance continues to improve with more lm data, even at large scales, suggesting that further work scaling up lm pretraining is warranted."
P19-1439,2019,8 Conclusions,"target tasks differ significantly in the pretraining tasks they benefit from, with correlations between target tasks often low or negative."
P19-1439,2019,8 Conclusions,these trends suggest that improving on lm pretraining with current techniques will be challenging.
P19-1439,2019,8 Conclusions,we also observe several worrying trends.
P19-1439,2019,8 Conclusions,we present a systematic comparison of tasks and task combinations for the pretraining and intermediate fine-tuning of sentence-level encoders like those seen in elmo and bert.
P19-1439,2019,8 Conclusions,"when trained on intermediate tasks like mt that are highly different than its original training task, bert shows signs of catastrophic forgetting."
P19-1439,2019,8 Conclusions,"while further work on language modeling seems straightforward and worthwhile, we believe that the future of this line of work will require a better understanding of the settings in which target task models can effectively utilize outside knowledge and data, and new methods for pretraining and transfer learning to do so."
P19-1439,2019,8 Conclusions,"with nearly 60 pretraining tasks and task combinations and nine target tasks, this represents a far more comprehensive study than any seen on this problem to date."
P19-1440,2019,5 Conclusion,"experiments show that compared to several previous systems, hsp effectively improves performance."
P19-1440,2019,5 Conclusion,"in this work, we propose a novel hierarchical semantic parsing (hsp) model based on sequenceto-sequence paradigm."
P19-1440,2019,5 Conclusion,we also design a neural generative question decomposer which achieves much higher performance than splittingbased question decomposition approach.
P19-1441,2019,5 Conclusion,"at last, we also would like to verify whether mt-dnn is resilience against adversarial attacks (glockner et al., 2018; talman and chatzikyriakidis, 2018; liu et al., 2019)."
P19-1441,2019,5 Conclusion,in this work we proposed a model called mtdnn to combine multi-task learning and language model pre-training for language representation learning.
P19-1441,2019,5 Conclusion,mtdnn also demonstrates an exceptional generalization capability in domain adaptation experiments.
P19-1441,2019,5 Conclusion,"mt-dnn obtains new state-ofthe-art results on ten nlu tasks across three popular benchmarks: snli, scitail, and glue."
P19-1441,2019,5 Conclusion,"there are many future areas to explore to improve mt-dnn, including a deeper understanding of model structure sharing in mtl, a more effective training method that leverages relatedness among multiple tasks, for both fine-tuning and pre-training (dong et al., 2019), and ways of incorporating the linguistic structure of text in a more explicit and controllable manner."
P19-1442,2019,9 Conclusion,a dataset for this task is easy to collect relative to other supervised tasks.
P19-1442,2019,9 Conclusion,"compared to unsupervised methods that train on a full corpus, our method yields more targeted and faster training."
P19-1442,2019,9 Conclusion,"encouragingly, the model trained on discourse marker prediction achieves comparable generalization performance to other state of the art models."
P19-1442,2019,9 Conclusion,it provides cheap and noisy but strong training signals.
P19-1442,2019,9 Conclusion,we fine-tune larger models on this task and achieve state-of-the-art on the pdtb implicit discourse relation prediction.
P19-1442,2019,9 Conclusion,we present a discourse marker prediction task for training and fine-tuning sentence embedding models.
P19-1442,2019,9 Conclusion,we train our model on this task and show that the resulting embeddings lead to high performance on a number of established tasks for sentence embeddings.
P19-1443,2019,7 Conclusion,"in this paper, we introduced sparc, a largescale dataset of context-dependent questions over a number of databases in different domains annotated with the corresponding sql representation."
P19-1443,2019,7 Conclusion,it also introduces unique challenge in mapping context-dependent questions to sql queries in unseen domains.
P19-1443,2019,7 Conclusion,the dataset features wide semantic coverage and a diverse set of contextual dependencies between questions.
P19-1443,2019,7 Conclusion,"the dataset, baseline implementations and leaderboard are publicly available at https:// yale-lily.github.io/sparc."
P19-1443,2019,7 Conclusion,"the model accuracy is far from satisfactory and stratifying the performance by question position shows that both models degenerate in later turns of interaction, suggesting the importance of better context modeling."
P19-1443,2019,7 Conclusion,we experimented with two competitive context-dependent semantic parsing approaches on sparc.
P19-1444,2019,6 Conclusion,experimental results on the challenging spider benchmark demonstrate the effectiveness of irnet.
P19-1444,2019,6 Conclusion,"we present a neural approach semql for complex and cross-domain text-to-sql, aiming to address the lexical problem and the mismatch problem with schema linking and intermediate representation."
P19-1445,2019,5 Conclusion,"a detailed experimental validation of eigensent, is performed on three public datasets, of varying degrees of complexity and purpose, and against algorithms which are both state-of-the-art and diverse, to formulate general conclusions about eigensent.4."
P19-1445,2019,5 Conclusion,"in cases where dynamics alone does not capture the essence of a sentence, our embeddings may be concatenated with those obtained via word vector averaging to obtain state-of-the-art results."
P19-1445,2019,5 Conclusion,"in this paper, we have proposed a novel method to construct sentence embeddings, by exploiting the dynamic properties of a sequence of word vectors that the sentence is made up of."
P19-1445,2019,5 Conclusion,the main contributions of the paper are: 1.
P19-1445,2019,5 Conclusion,"the rationale and intuition behind using the said method to capture the dynamic properties of a sentence are motivated, and the mathematical preliminaries of hodmd in the context of constructing sentence embeddings are clearly delineated.3."
P19-1445,2019,5 Conclusion,"thorough empirical validation of the proposed method, which we call eigensent, against known state-of-the-art methods shows the promise of this technique in capturing the dynamics of a word vector sequence to distill sentence embeddings, which may be concatenated with word vector average embeddings to state-ofthe-art performance."
P19-1445,2019,5 Conclusion,"we do this using a spectral decomposition method rooted in fluid-dynamics, known as higher-order dynamic mode decomposition, which is known to capture the fundamental transition dynamics of a multidimensional signal."
P19-1445,2019,5 Conclusion,"we postulate, and later observe, that our method can successfully capture the dynamics present in a sentence."
P19-1445,2019,5 Conclusion,"we use signal summarization as an approach for creating sentence embeddings, a first to the best of our knowledge, using an algorithm from fluid dynamics called higher-order dynamic mode decomposition (hodmd).2."
P19-1446,2019,4 Conclusion,"in practice, vertex mappings can be identified by simply considering the vertex labels, and the labels of their neighbors, through the ngrams in which they appear."
P19-1446,2019,4 Conclusion,"sembleu can be potentially used to compare other types of graphs, including cyclic graphs."
P19-1446,2019,4 Conclusion,the improvement in correlation with human judgments comes from the fact that sembleu considers larger fragments of the input graphs.
P19-1446,2019,4 Conclusion,the improvement in speed comes from avoiding the search over mappings between the two graphs.
P19-1446,2019,4 Conclusion,"while one might expect a trade-off between speed and correlation with human judgments, sembleu appears to outperform smatch in both dimensions."
P19-1447,2019,4 Conclusion,in the future we plan to apply the reranker to other parsers and more benchmark datasets.
P19-1447,2019,4 Conclusion,"we proposed a feature-based reranker for neural semantic parsing, which achieved strong results on three semantic parsing and code generation tasks."
P19-1447,2019,4 Conclusion,we will also attempt to jointly train the base semantic parser and the reranker by using the reranker’s output as supervision to fine tune the base parser.
P19-1448,2019,6 Conclusion,"we demonstrate the effectivness of this method on spider, a dataset that contains complex schemas which are not seen at training time, and show substantial improvement over current state-of-the-art."
P19-1448,2019,6 Conclusion,"we present a semantic parser that encodes the structure of the db schema with a graph neural network, and uses this representation to make schema-aware decisions both at encoding and decoding time."
P19-1449,2019,5 Conclusion,"but we also note that, when trained in low-resource settings, bert’s performance falls considerably."
P19-1449,2019,5 Conclusion,"given these results, and the continued difficulty neural methods have with the winograd schema challenge, we argue that future work on glue-style sentence understanding tasks might benefit from a focus on learning from smaller training sets."
P19-1449,2019,5 Conclusion,"in work subsequent to the main results of this paper, we have prepared such a benchmark in the glue follow-up superglue (wang et al., 2019a)."
P19-1449,2019,5 Conclusion,this paper presents a conservative estimate of human performance to serve as a target for the glue sentence understanding benchmark.
P19-1449,2019,5 Conclusion,we find that state-of-the-art models like bert are not far behind human performance on most glue tasks.
P19-1449,2019,5 Conclusion,we obtain this baseline with the help of crowdworker annotators.
P19-1450,2019,5 Conclusion,"furthermore, one limitation of our approach is that the latent am dependency trees are determined by heuristics, which must be redeveloped for each new graphbank."
P19-1450,2019,5 Conclusion,"in the future, we would like to extend our approach to sembanks which are annotated with different types of semantic representation, e.g."
P19-1450,2019,5 Conclusion,"sql (yu et al., 2018) or drt (abzianidze et al., 2017)."
P19-1450,2019,5 Conclusion,we achieve this by training a compositional neural parser on graphbank-specific tree decompositions of the annotated graphs and combining it with bert and multi-task learning.
P19-1450,2019,5 Conclusion,we have shown how to perform accurate semantic parsing across a diverse range of graphbanks.
P19-1450,2019,5 Conclusion,we will explore latent-variable models to learn the dependency trees automatically.
P19-1451,2019,5 Conclusions,our main contribution augments training with policy learning by priming samples that are more suitable for the evaluation metric.
P19-1451,2019,5 Conclusions,our unlabeled smatch score (achieving the best graph structure) suggests that a new strategy to predict labels may reach even higher numbers.
P19-1451,2019,5 Conclusions,we perform an in-depth ablation experiment that shows the impact of each of our contributions.
P19-1451,2019,5 Conclusions,we report modifications in a competitive amr parser achieving one of the best results in the task.
P19-1452,2019,5 Conclusion,"this provides new evidence corroborating that deep language models can represent the types of syntactic and semantic abstractions traditionally believed necessary for language processing, and moreover that they can model complex interactions between different levels of hierarchical information."
P19-1452,2019,5 Conclusion,we employ the edge probing task suite to explore how the different layers of the bert network can resolve syntactic and semantic structure within a sentence.
P19-1452,2019,5 Conclusion,"we find that while this traditional pipeline order holds in the aggregate, on individual examples the network can resolve out-oforder, using high-level information like predicateargument relations to help disambiguate low-level decisions like part-of-speech."
P19-1452,2019,5 Conclusion,"we present two complementary measurements: scalar mixing weights, learned from a training corpus, and cumulative scoring, measured on an evaluation set, and show that a consistent ordering emerges."
P19-1453,2019,5 Conclusion,"moreover by using the bitext directly, our approach also produces strong paraphrastic cross-lingual representations as a byproduct."
P19-1453,2019,5 Conclusion,our approach is much faster than comparable methods and yields stronger performance on cross-lingual and monolingual semantic similarity and cross-lingual bitext mining tasks.
P19-1453,2019,5 Conclusion,we have shown that using automatic dataset preparation methods such as pivoting or backtranslation are not needed to create higher performing sentence embeddings.
P19-1454,2019,6 Conclusion,our code is publicly available at https://github.com/ wangxinyu0922/second_order_sdp
P19-1454,2019,6 Conclusion,our experimental results show that our model outperforms previous state-of-the-art model and has higher accuracies especially on out-of-domain data and long sentences.
P19-1454,2019,6 Conclusion,we constructed an end-to-end neural network that uses a trilinear function to score second-order parts and finds the highest-scoring parse graph by either mean field variational inference or loopy belief propagation algorithms unfolded as recurrent neural network layers.
P19-1454,2019,6 Conclusion,we proposed a novel graph-based second-order parser for semantic dependency parsing.
P19-1455,2019,7 Conclusion and Future Work,"advanced techniques to learn multimodal relationships could incorporate better relationship modeling (majumder et al., 2018), and exploit models that provide gesture, facial and pose information about the people in the scene (cao et al., 2018)."
P19-1455,2019,7 Conclusion and Future Work,another direction could be to create fusion strategies that can better model incongruity among modalities to identify sarcasm.
P19-1455,2019,7 Conclusion and Future Work,"as a consequence, in our initial experiments, we noticed that svm classifiers perform better than their neural counterparts, such as cnns."
P19-1455,2019,7 Conclusion and Future Work,"as gesture and facial expressions are important features for sarcasm analysis, we believe the capability for models to identify the speakers in the multiparty videos is likely to be beneficial for the task."
P19-1455,2019,7 Conclusion and Future Work,"by showing multiple examples from our curated dataset, we demonstrate the need for multimodal learning for sarcasm detection."
P19-1455,2019,7 Conclusion and Future Work,"consequently, we developed models that leverage three different modalities, including text, speech, and visual signals."
P19-1455,2019,7 Conclusion and Future Work,considering these factors can improve context modeling necessary for sarcasm detection in conversational context.
P19-1455,2019,7 Conclusion and Future Work,"finally, we believe the resource introduced in this paper has the potential to enable novel research in multimodal sarcasm detection."
P19-1455,2019,7 Conclusion and Future Work,"future work could investigate advanced spatiotemporal fusion strategies (e.g., tensor-fusion (zadeh et al., 2017), cca (hotelling, 1936)) to better encode the correspondence between modalities."
P19-1455,2019,7 Conclusion and Future Work,future work should try to leverage these factors to improve the baseline scores reported in this paper.
P19-1455,2019,7 Conclusion and Future Work,"future work should try to overcome this issue with solutions involving pre-training, transfer learning, domain adaption, or low-parameter models."
P19-1455,2019,7 Conclusion and Future Work,"in a dialogue, to classify an utterance at time t, the preceding utterances at time < t can be considered as its context."
P19-1455,2019,7 Conclusion and Future Work,"in multiple evaluations, the multimodal variants were shown to significantly outperform their unimodal counterparts, with relative error rate reductions of up to 12.9%."
P19-1455,2019,7 Conclusion and Future Work,"in this paper, we provided a systematic introduction to multimodal learning for sarcasm detection."
P19-1455,2019,7 Conclusion and Future Work,"in this work, although we utilize conversational context, we ignore modeling various key conversation specific factors such as interlocutors’ goals, intents, dependency, etc.(poria et al., 2019)."
P19-1455,2019,7 Conclusion and Future Work,main speaker localization: we currently extract visual features ubiquitously for each frame.
P19-1455,2019,7 Conclusion and Future Work,"moreover, the occurrence of sarcastic utterances itself is scanty."
P19-1455,2019,7 Conclusion and Future Work,"moreover, while conducting this research, we identified several challenges that we believe are important to address in future research work on multimodal sarcasm detection."
P19-1455,2019,7 Conclusion and Future Work,"multimodal fusion: so far, we have only explored early fusion for multimodal classification."
P19-1455,2019,7 Conclusion and Future Work,multiparty conversation: the dialogues represented in our dataset are often multi-party conversations.
P19-1455,2019,7 Conclusion and Future Work,"neural baselines: as we strove to create a highquality dataset with rich annotations, we had to trade-off corpus size."
P19-1455,2019,7 Conclusion and Future Work,sarcasm detection in conversational context: our proposed mustard is inherently a dialogue level dataset where we aim to classify the last utterance in the dialogue.
P19-1455,2019,7 Conclusion and Future Work,the results of the baseline experiments supported the hypothesis that multimodality is important for sarcasm detection.
P19-1455,2019,7 Conclusion and Future Work,"this, however, arises the problem of over-fitting in complex neural models."
P19-1455,2019,7 Conclusion and Future Work,"to enable research on this topic, we introduced a novel dataset, mustard, consisting of sarcastic and non-sarcastic videos drawn from different sources."
P19-1455,2019,7 Conclusion and Future Work,"to focus on effects induced by multimodal experiments, we chose a balanced version of the dataset with a limited size."
P19-1455,2019,7 Conclusion and Future Work,we also experimented with the integration of context and speaker information as additional input for our models.
P19-1456,2019,6 Conclusion,"besides, developing techniques to incorporate the claim stance and specificity detection models in argument generation to generate more coherent and consistent arguments is another interesting research direction to be explored."
P19-1456,2019,6 Conclusion,"for future work, it may be interesting to understand which other models would be effective in claim specificity and stance detection tasks."
P19-1456,2019,6 Conclusion,"we find that it is easier to predict stance for claims that have a parent-child relationship, where as relative specificity is more difficult to predict in the same case."
P19-1456,2019,6 Conclusion,"we present a new dataset of manually curated argument trees, which can open interesting avenues of research in argumentation."
P19-1456,2019,6 Conclusion,we use this dataset to study methods for determining claim stance and relative claim specificity for complex argumentative structures.
P19-1457,2019,6 Conclusion,"by using emlo embeddings, our final model improves fine-grained accuracies by 1.3 points compare to the current best result."
P19-1457,2019,6 Conclusion,"we presented a range of sentiment grammars for using neural networks to model sentiment composition explicitly, and empirically showed that explicit modeling of sentiment composition with fine-grained sentiment subtypes gives better performance compared to state-of-the-art neural network models in sentiment analysis."
P19-1458,2019,9 Discussion and Future Considerations,"elmo and ulmfit perform just as well trained on small corpora, but bert performs worse since it is designed to be trained on msm and nsp tasks."
P19-1458,2019,9 Discussion and Future Considerations,"finally, domain adaptation always improves the performance of ulmfit."
P19-1458,2019,9 Discussion and Future Considerations,"in the future, we will investigate other nlp tasks such as named entity recognition (ner), question answering (qa) and aspect-based sentiment analysis (absa) (pontiki et al., 2016) to see whether results we saw in sentiment analysis is consistent across these tasks."
P19-1458,2019,9 Discussion and Future Considerations,it is important to note that we did not perform k-fold validation due to their high computational cost.
P19-1458,2019,9 Discussion and Future Considerations,this research is a work in progress and will be regularly updated with new benchmarks and baselines.
P19-1458,2019,9 Discussion and Future Considerations,we believe that our ablation study and the release of pre-trained models will be particularly useful in japanese text classification.
P19-1458,2019,9 Discussion and Future Considerations,we hope that our experimental results inspire future research dedicated to japanese.
P19-1458,2019,9 Discussion and Future Considerations,"we showed that with only 1 3 of the total dataset, transfer learning approaches perform better than previous state-of-the-art models."
P19-1459,2019,8 Conclusion,analysis of easy to classify data points showed reliance on a lower proportion of the strongest cue word than the bov and bilstm - i.e.
P19-1459,2019,8 Conclusion,arct provides a fortuitous opportunity to see how stark the problem of exploiting spurious statistics can be.
P19-1459,2019,8 Conclusion,"as our learners get stronger, controlling for spurious statistics becomes more important in order to have confidence in their apparent performance."
P19-1459,2019,8 Conclusion,bert has learned when to ignore the presence of “not” and focus on different cues.
P19-1459,2019,8 Conclusion,"due to our ability to eliminate the major source of these cues, we were able to show that bert’s maximum performance fell from just three points below the average untrained human baseline to essentially random."
P19-1459,2019,8 Conclusion,"however, our investigations confirmed that bert is indeed a very strong learner."
P19-1459,2019,8 Conclusion,"taken with a growing body of previous work, our results indicate the need for further research into the extent of this problem in nlp more generally."
P19-1459,2019,8 Conclusion,the adversarial dataset should be adopted as the standard in future work on arct.
P19-1459,2019,8 Conclusion,this indicates an ability to exploit much more subtle joint distributional information.
P19-1459,2019,8 Conclusion,to answer our question in the introduction: bert has learned nothing about argument comprehension.
P19-1459,2019,8 Conclusion,we hope that providing a more robust evaluation will help to spur more productive research on this problem.
P19-1460,2019,5 Conclusion and Future Work,a quantitative analysis shows the effectiveness of rcn in recognising stance (dis)agreement on various topics.
P19-1460,2019,5 Conclusion and Future Work,a visualisation analysis further illustrates the ability of rcn to discover useful reason aspects for the stance comparison.
P19-1460,2019,5 Conclusion and Future Work,"finally, it would be insightful to further visualise the reasons in the embedded space with more advanced visualisation tools."
P19-1460,2019,5 Conclusion and Future Work,"first, it is necessary to evaluate our model on more stance data with different linguistic properties (e.g., the much longer and richer stance utterances in posts or articles)."
P19-1460,2019,5 Conclusion and Future Work,"in the future, this work can be progressed in several ways."
P19-1460,2019,5 Conclusion and Future Work,"in this paper, we identify (dis)agreement between stances expressed in paired utterances."
P19-1460,2019,5 Conclusion and Future Work,"second, it is important to show how the learned embedded reasons can help downstream applications such as stance detection."
P19-1460,2019,5 Conclusion and Future Work,we exploit the reasons behind the stances and propose a reason comparing network (rcn) to capture the reason information to infer the stance (dis)agreement.
P19-1461,2019,7 Conclusion,"nevertheless, there is still a substantial performance gap between humans and automatic detectors."
P19-1461,2019,7 Conclusion,the result showed that predictive models can strongly benefit from out-of-domain instances.
P19-1461,2019,7 Conclusion,we aimed at understanding why a writer of a text holds a particular sentiment and proposed a task of human motive detection as an essential building block to this end.
P19-1461,2019,7 Conclusion,we empirically verified this by transferring learned parameters across domains.
P19-1461,2019,7 Conclusion,we evaluated the performance of baseline predictive models on this dataset.4 one interesting property is that the same underlying motives can appear in different domains even though their distribution may differ.
P19-1461,2019,7 Conclusion,"we presented a taxonomy of motives derived from a psychology study and annotated 1,600 restaurant and laptop reviews with six motives."
P19-1462,2019,4 Conclusion,experimental results demonstrated the effectiveness and robustness of the proposed method on two benchmark datasets over the task of tabsa.
P19-1462,2019,4 Conclusion,"hence, the interdependence among specific target, corresponding aspect, and the context can be extracted to generate superior embedding."
P19-1462,2019,4 Conclusion,"in future works, we will explore the extension of this approach for other tasks."
P19-1462,2019,4 Conclusion,"in this paper, we proposed a novel method for refining representations of targets and aspects."
P19-1462,2019,4 Conclusion,the proposed method is able to select a set of highly correlated words from the context via a sparse coefficient vector and then adjust the representations of targets and aspects.
P19-1463,2019,5 Conclusion,"for future work, we plan to i) automatically predict relations between argument components in the uselecdeb60to16 dataset, and ii) propose a new task, i.e., fallacy detection so that common fallacies in political argumentation (zurloni and anolli, 2010) can be automatically identified, in line with the work of (habernal et al., 2018)."
P19-1463,2019,5 Conclusion,"we highlighted the strengths (e.g., satisfactory performances on different oratory styles across time and topics) and weaknesses (e.g., no argument boundaries detection on a clause level, the context of the whole debates is not considered)."
P19-1463,2019,5 Conclusion,"we investigated the detection of argument components in the us presidential campaign debates: i) providing a manually annotated resource of 29k argument components, and ii) evaluating featurerich svm learners and neural networks on such data (achieving ～90% w.r.t.human performance)."
P19-1464,2019,5 Conclusion and Future work,"another direction is to explore span representations in several related tasks such as rst-style discourse parsing or new span-related argumentation mining tasks (trautmann et al., 2019)."
P19-1464,2019,5 Conclusion and Future work,one interesting line of our future work is to investigate the performance of our model in an endto-end setting (including ac segmentation).
P19-1464,2019,5 Conclusion and Future work,"specifically, we have investigated (i) an lstm-minus-based span representation originally developed for other nlp tasks and (ii) a task-specific extended representation capturing the am/ac distinction for asp."
P19-1464,2019,5 Conclusion and Future work,the empirical analysis has showed that there is room for improvement in the li for deeper-level adus.
P19-1464,2019,5 Conclusion and Future work,the experimental results have demonstrated the effects of these representations in asp and that the span representation capturing the am/ac distinction achieves state-of-the-art results for three subtasks.
P19-1464,2019,5 Conclusion and Future work,this work has studied span representations for asp.
P19-1465,2019,5 Conclusion,"due to its fast speed and strong performance, the model is quite suitable for a wide range of related applications."
P19-1465,2019,5 Conclusion,it achieves the performance on par with the state-of-the-art on four well-studied datasets across three different text matching tasks with only a small number of parameters and very high inference speed.
P19-1465,2019,5 Conclusion,"it highlights three key features, namely previous aligned features, original point-wise features, and contextual features for inter-sequence alignment and simplifies most of the other components."
P19-1465,2019,5 Conclusion,"we propose a highly efficient approach, re2, for general purpose text matching."
P19-1466,2019,5 Conclusion and Future Work,"additionally, we generalize and extend graph attention mechanisms to capture both entity and relation features in a multihop neighborhood of a given entity."
P19-1466,2019,5 Conclusion and Future Work,"in the future, we intend to extend our method to better perform on hierarchical graphs and capture higher-order relations between entities (like motifs) in our graph attention model."
P19-1466,2019,5 Conclusion and Future Work,"in this paper, we propose a novel approach for relation prediction."
P19-1466,2019,5 Conclusion and Future Work,our approach improves over the state-of-the-art models by significant margins.
P19-1466,2019,5 Conclusion and Future Work,our detailed and exhaustive empirical analysis gives more insight into our method’s superiority for relation prediction on kgs.
P19-1466,2019,5 Conclusion and Future Work,our proposed model learns new graph attentionbased embeddings that specifically cater to relation prediction on kgs.
P19-1466,2019,5 Conclusion and Future Work,"the proposed model can be extended to learn embeddings for various tasks using kgs such as dialogue generation (he et al., 2017; keizer et al., 2017), and question answering (zhang et al., 2016; diefenbach et al., 2018)."
P19-1467,2019,7 Conclusion,"in future work, we intend to refine these alignment boundaries and to optimize the alignment procedure for speed."
P19-1467,2019,7 Conclusion,"it is able to align arbitrarily long phrases, automatically discovering the best phrase length, from individual words to full sentences, at which to align a given input sentence pair, and it significantly outperforms existing phrase-based aligners at aligning long phrases with high semantic similarity but low lexical overlap."
P19-1467,2019,7 Conclusion,our system achieves high recall but suffers from imprecise alignment boundaries.
P19-1467,2019,7 Conclusion,this pointeraligner uses an lstm language model to compose the embeddings of words in a chunk into a chunk embedding and and then aligns these chunks.
P19-1467,2019,7 Conclusion,we have presented a pointer-network-based system for aligning longer paraphrases.
P19-1467,2019,7 Conclusion,we hope that this work will raise more interest in developing alignment systems for longer paraphras.
P19-1468,2019,7 Conclusion,our experiments show that the entailment graphs built by our proposed score outperform previous state-of-the-art results because link prediction is effective in filtering noise and adding new facts.
P19-1468,2019,7 Conclusion,our results show that the two tasks can benefit from each other.
P19-1468,2019,7 Conclusion,the score is computed by estimating transition probabilities between the relation states.
P19-1468,2019,7 Conclusion,"we have additionally considered the reverse problem, i.e., using the learned entailment graphs to improve link prediction."
P19-1468,2019,7 Conclusion,we have introduced a new score for entailment detection by performing link prediction on predicateargument structures extracted from text.
P19-1468,2019,7 Conclusion,we have shown that link prediction and entailment graph induction are complementary tasks.
P19-1468,2019,7 Conclusion,we reform the normal knowledge graph representation into a markov chain with relations and entity-pairs as its states.
P19-1469,2019,6 Conclusion,"although the current model makes fairly plausible sentences, it tends to prefer relatively short and safe sentences, as the main goal of the training is to accurately predict the relationship between sentences."
P19-1469,2019,6 Conclusion,"due to the use of crosssentence generation, the generative model and the discriminative classifier interacts more strongly, and from experiments we empirically proved that the cs-lvm outperforms other models by a large margin."
P19-1469,2019,6 Conclusion,"for future work, we plan to focus on generating more realistic text and use the generated text in other tasks e.g.data augmentation, addressing adversarial attack."
P19-1469,2019,6 Conclusion,"given a pair of text sequences and the corresponding label, it uses one of the sequences and the label as input and generates the other sequence."
P19-1469,2019,6 Conclusion,"in this work, we proposed a cross-sentence latent variable model (cs-lvm) for semi-supervised text sequence matching."
P19-1469,2019,6 Conclusion,"we also defined multiple semantic constraints to further regularize the model, and observed that fine-tuning with them gives additional increase in performance."
P19-1469,2019,6 Conclusion,we expect the model could perform more natural generation via applying recent advancements on deep generative models.
P19-1470,2019,7 Conclusion,comet is a framework for adapting the weights of language models to learn to produce novel and diverse commonsense knowledge tuples.
P19-1470,2019,7 Conclusion,"empirical results on two commonsense knowledge bases, atomic and conceptnet, show that comet frequently produces novel commonsense knowledge that human evaluators deem to be correct."
P19-1470,2019,7 Conclusion,"these positive results point to future work in extending the approach to a variety of other types of knowledge bases, as well as investigating whether comet can learn to produce openie-style knowledge tuples for arbitrary knowledge seeds."
P19-1470,2019,7 Conclusion,we introduce commonsense transformers (comet) for automatic construction of commonsense knowledge bases.
P19-1471,2019,7 Contributions,"our error analysis indicates that having two event hierarchies in the same sentence is a major problem, as well as having significant separation between a parent and child event."
P19-1471,2019,7 Contributions,"our model involves several novel discourse and narrative features, as well as a small number of feature modifications."
P19-1471,2019,7 Contributions,"we present a model to detect subevent relation in news articles which outperforms the two prior approaches by 15 and 5 percentage points, respectively."
P19-1472,2019,7 Conclusion,"by constructing the dataset through adversarial filtering, combined with state-of-the-art models for language generation and discrimination, we produced a dataset that is adversarial to the most robust models available – even when models are evaluated on items from the training distribution."
P19-1472,2019,7 Conclusion,"in this paper, we presented hellaswag, a new dataset for physically situated commonsense reasoning."
P19-1472,2019,7 Conclusion,"in turn, we provided insight into the inner workings of pretrained models, and suggest a path for nlp progress going forward: towards benchmarks that adversarially co-evolve with evolving state-of-the-art models."
P19-1473,2019,7 Conclusions and Future Work,an interesting direction would be to enable domain experts to identify and actively request for program annotations given the knowledge shared by other domains.
P19-1473,2019,7 Conclusions and Future Work,"in this work, we addressed the challenge of training a semantic parser for multiple domains without strong supervision i.e.in the absence of ground truth programs corresponding to input utterances."
P19-1473,2019,7 Conclusions and Future Work,the resultant multi-domain semantic parser is compact and more precise as demonstrated on the overnight dataset.
P19-1473,2019,7 Conclusions and Future Work,this would further make it feasible to perform transfer learning on a new domain.
P19-1473,2019,7 Conclusions and Future Work,we also plan to investigate the possibility of augmenting the parallel corpus by bootstrapping from shared templates across domains.
P19-1473,2019,7 Conclusions and Future Work,we believe that this proposed framework has wide applicability to any sequence-to-sequence model.
P19-1473,2019,7 Conclusions and Future Work,we plan to explore if further fine-tuning using denotations based training on the distilled model can lead to improvements in the unified parser.
P19-1473,2019,7 Conclusions and Future Work,we propose a novel unified neural framework using multi-policy distillation mechanism with two stages of training through weak supervision from denotations i.e.final answers corresponding to utterances.
P19-1473,2019,7 Conclusions and Future Work,we show that a small parallel corpus with annotated programs boosts the performance.
P19-1473,2019,7 Conclusions and Future Work,we would also like to explore if guiding the decoder through syntactical and domain-specific constraints helps in reducing the search space for the weakly supervised unified parser.
P19-1474,2019,6 Conclusion,a prominent direction for future work is using the hyperbolic embeddings as the sole signal for taxonomy extraction.
P19-1474,2019,6 Conclusion,"since distributional and hyperbolic embeddings cover different relations between terms, it may be interesting to combine them."
P19-1474,2019,6 Conclusion,they consistently yield im- ′ provements over strong baselines and in comparison to word2vec as a representative for distributional vectors in the euclidean space.
P19-1474,2019,6 Conclusion,"this observation confirms the theoretical capability of poincare embeddings to learn ′ hierarchical relations, which enables their future use in a wide range of semantic tasks."
P19-1474,2019,6 Conclusion,we further showed that poincare embeddings can be effi- ′ ciently created for a specific domain from crawled text without the need for an existing database such as wordnet.
P19-1474,2019,6 Conclusion,we presented a refinement method for improving existing taxonomies through the use of hyperbolic poincare embeddings.
P19-1475,2019,6 Conclusion,"in this paper, we propose that margin-loss in contrast to log-loss is a more plausible training objective for copa-style plausibility tasks."
P19-1475,2019,6 Conclusion,"this intuition was shown to lead to a new state-of-the-art in the original copa task, based on a margin-based loss."
P19-1475,2019,6 Conclusion,"through adversarial construction we illustrated that a logloss approach can be driven to encode plausible statements (neutral hypotheses in nli) as either extremely likely or unlikely, which was highlighted in contrasting figures of per-premise normalized hypothesis scores."
P19-1476,2019,4 Conclusion,glen displays robust graded le performance and yields massive improvements over state-of-the-art in cross-lingual le detection.
P19-1476,2019,4 Conclusion,the learned le-specialization function is then applied to vectors of words (1) unseen in constraints and (2) from different languages.
P19-1476,2019,4 Conclusion,"unlike existing le-specialization models (nguyen et al., 2017; vulic and mrk ′ siˇ c′, 2018), glen learns an explicit specialization function using linguistic constraints as training examples."
P19-1476,2019,4 Conclusion,we make glen (code and resources) available at: https://github.com/codogogo/glen.
P19-1476,2019,4 Conclusion,"we next plan to evaluate glen on multilingual and cross-lingual graded le datasets (vulic et al.′ , 2019) and release a large multilingual repository of le-specialized embeddings."
P19-1476,2019,4 Conclusion,"we presented glen, a general framework for specializing word embeddings for lexical entailment."
P19-1477,2019,5 Conclusion,"attracted by the success of recently proposed language representation model bert, in this paper, we introduce a simple yet effective reimplementation of bert for commonsense reasoning."
P19-1477,2019,5 Conclusion,"future work will entail adaption of the attentions, to further improve the performance."
P19-1477,2019,5 Conclusion,"however, although bert seems to implicitly establish complex relationships between entities facilitating tasks such as coreference resolution, the results also suggest that solving commonsense reasoning tasks might require more than leveraging a language model trained on huge text corpora."
P19-1477,2019,5 Conclusion,"specifically, we propose a method which exploits the attentions produced by bert for the challenging tasks of pdp and wsc."
P19-1477,2019,5 Conclusion,the experimental analysis demonstrates that our proposed system outperforms the previous state of the art on multiple datasets.
P19-1478,2019,6 Summary and Outlook,"furthermore, to further improve the results on wsc273, data-filtering procedures may be introduced to find harder wsc-like examples."
P19-1478,2019,6 Summary and Outlook,"in future work, other uses and the statistical significance of maskedwiki’s impact and its applications to different tasks will be investigated."
P19-1478,2019,6 Summary and Outlook,the consistent improvement of several language models indicates the robustness of this method.
P19-1478,2019,6 Summary and Outlook,"the previous sota results on wsc273 and wnli are improved by 8.8% and 9.6%, respectively."
P19-1478,2019,6 Summary and Outlook,"this is particularly surprising, because previous work (opitz and frank, 2018) implies that generalizing to wsc273 is hard."
P19-1478,2019,6 Summary and Outlook,this work achieves new sota results on the wsc273 and wnli datasets by fine-tuning the bert language model on the wscr dataset and a newly introduced maskedwiki dataset.
P19-1478,2019,6 Summary and Outlook,"to our knowledge, this is the first model that beats the majority baseline on wnli."
P19-1478,2019,6 Summary and Outlook,"we show that by fine-tuning on wsc-like data, the language model’s performance on wsc consistently improves."
P19-1479,2019,5 Conclusion,experiment results show that our model can generate more coherent and informative comments.
P19-1479,2019,5 Conclusion,"in the future, we would like to explore how to introduce external knowledge into the graph to make the generated comments more logical."
P19-1479,2019,5 Conclusion,"in this paper, we propose to automatically generate comment of articles with a graph-to-sequence model that organizes the article into a topic interaction graph."
P19-1479,2019,5 Conclusion,"our model can better understand the structure of the article, thus capturing the main point of the article."
P19-1479,2019,5 Conclusion,we observe that there are still some comments conflicting with the world knowledge.
P19-1480,2019,7 Conclusion and Future Work,experiments show that our proposed framework achieves the best performance in automatic and human evaluations.
P19-1480,2019,7 Conclusion and Future Work,"first, the presented system is still contingent on highlighting answer-like nuggets in the declarative text."
P19-1480,2019,7 Conclusion and Future Work,"however, questions can be raised by either part in real scenario."
P19-1480,2019,7 Conclusion and Future Work,"in this paper, we study the problem of questionanswering style conversational question generation (cqg), which has never been investigated before."
P19-1480,2019,7 Conclusion and Future Work,integrating answer span identification into the presented system is a promising direction.
P19-1480,2019,7 Conclusion and Future Work,"second, in our setting, the roles of the questioner and the answerer are fixed."
P19-1480,2019,7 Conclusion and Future Work,the conversation flow modeling builds a coherent conversation between turns.
P19-1480,2019,7 Conclusion and Future Work,the coreference alignment enables our framework to refer back to the conversation history using coreferences.
P19-1480,2019,7 Conclusion and Future Work,there are several future directions for this setting.
P19-1480,2019,7 Conclusion and Future Work,we propose an end-to-end neural model with coreference alignment and conversation flow modeling to solve this problem.
P19-1481,2019,7 Conclusion,"however, large training sets are not available for most languages."
P19-1481,2019,7 Conclusion,"in future work, we will explore the use of cross-lingual embeddings to further improve performance on this task."
P19-1481,2019,7 Conclusion,"neural models for automatic question generation using the standard sequence to sequence paradigm have been shown to perform reasonably well for languages such as english, which have a large number of training instances."
P19-1481,2019,7 Conclusion,"to address this problem, we present a crosslingual model that leverages a large qg dataset in a secondary language (along with monolingual data and parallel data) to improve qg performance on a primary language with a limited number of qg training pairs."
P19-1482,2019,7 Conclusions,experimental results show the effectiveness of our method to address the challenges of existing methods.
P19-1482,2019,7 Conclusions,"to make the hrl training more stable, we provide an efficient mask-based inference algorithm that allows for single-option trajectory during training."
P19-1482,2019,7 Conclusions,"we identify three challenges of existing seq2seq methods for unsupervised text style transfer and propose point-then-operate (pto), a sequence operation-based method within the hierarchical reinforcement learning (hrl) framework consisting of a hierarchy of agents for pointing and operating respectively."
P19-1482,2019,7 Conclusions,"we show that the key aspects of text style transfer, i.e., fluency, style polarity, and content preservation, can be modeled by comprehensive training objectives."
P19-1483,2019,7 Conclusions,"lastly, we show that parent is comparable to the best existing metrics when references are elicited by humans on the webnlg data."
P19-1483,2019,7 Conclusions,"we also perform the first empirical evaluation of information extraction based metrics (wiseman et al., 2017), and find rg-f to be effective."
P19-1483,2019,7 Conclusions,"we propose a new metric, parent, which shows the highest correlation with humans across a range of settings with divergent references in wikibio."
P19-1483,2019,7 Conclusions,we study the automatic evaluation of table-to-text systems when the references diverge from the table.
P19-1484,2019,6 Conclusion,"however, we note that whilst our results are encouraging on this relatively simple qa task, further work is required to handle more challenging qa elements and to reduce our reliance on linguistic resources and heuristics."
P19-1484,2019,6 Conclusion,"in this work, we explore whether it is possible to to learn extractive qa behaviour without the use of labelled qa data."
P19-1484,2019,6 Conclusion,"we find that it is indeed possible, surpassing simple supervised systems, and strongly outperforming other approaches that do not use labelled data, achieving 56.4% f1 on the popular squad dataset, and 64.5% f1 on the subset where the answer is a named entity mention."
P19-1485,2019,7 Conclusions,"bert representations improve generalization, but their effect is moderate when the source of the context is web snippets compared to wikipedia and newswire."
P19-1485,2019,7 Conclusions,in this work we performed a thorough empirical investigation of generalization and transfer over 10 rc datasets.
P19-1485,2019,7 Conclusions,it also leads to substantial reduction in the number of necessary training examples for a fixed performance.
P19-1485,2019,7 Conclusions,performance over an rc dataset can be improved by retrieving web snippets for all questions and adding them as examples (context augmentation).
P19-1485,2019,7 Conclusions,training the high-capacity bert-large representations over multiple rc datasets leads to good performance on all of the trained datasets without having to fine-tune on each dataset separately.
P19-1485,2019,7 Conclusions,we characterized the factors affecting generalization and obtained several state-ofthe-art results by training on 375k examples from 5 rc datasets.
P19-1485,2019,7 Conclusions,"we highlight several practical take-aways: pre-training on multiple source rc datasets consistently improves performance on a target rc dataset , even in the presence of bert representations."
P19-1485,2019,7 Conclusions,"we open source our infrastructure for easily performing experiments on multiple rc datasets, for the benefit of the community."
P19-1486,2019,5 Conclusion,our proposed ial-cpg model achieves stateof-the-art performance on the challenging narrativeqa benchmark.
P19-1486,2019,5 Conclusion,"we conduct extensive ablation studies and qualitative analysis, shedding light on the task at hand."
P19-1486,2019,5 Conclusion,we proposed curriculum learning based pointergenerator networks for reading long narratives.
P19-1486,2019,5 Conclusion,we show that sub-sampling diverse views of a story and training them with a curriculum scheme is potentially more effective than techniques designed for open-domain question answering.
P19-1487,2019,7 Conclusion and Future Work,explanations must be carefully monitored to ensure that they do not reinforce negative or otherwise harmful reasoning that might then propagate into downstream models.
P19-1487,2019,7 Conclusion and Future Work,"for example, in cqa we observed significant gender disparity and bias with higher proportion of female pronouns used in negative contexts."
P19-1487,2019,7 Conclusion and Future Work,they might also be extended to a broader set of tasks.
P19-1487,2019,7 Conclusion and Future Work,this kind of bias has inevitably propagated into cose and advise these datasets and trained models be used with that in mind.
P19-1487,2019,7 Conclusion and Future Work,we also performed comprehensive error analyses of language model explanations and evaluated explanation transfer to out-of-domain datasets.
P19-1487,2019,7 Conclusion and Future Work,we also proposed the novel commonsense auto-generated explanations (cage) framework that trains a language model to generate useful explanations when finetuned on the problem input and human explanations these explanations can then be used by a classifier model to make predictions.
P19-1487,2019,7 Conclusion and Future Work,"we empirically show that such an approach not only results in state-of-the-art performance on a difficult commonsense reasoning task, but also opens further avenues for studying explanation as it relates to interpretable commonsense reasoning."
P19-1487,2019,7 Conclusion and Future Work,we introduced the common sense explanations (cos-e) dataset built on top of the existing commonsenseqa dataset.
P19-1487,2019,7 Conclusion and Future Work,"while cage focuses on generating explanations prior to predicting an answer, language models for explanation might also be jointly trained to predict the answer."
P19-1487,2019,7 Conclusion and Future Work,"with a sufficient dataset of explanations (analogous to cos-e) for many tasks, it might be possible to fine-tune a more general explanatory language model that generates more useful explanations for unseen tasks."
P19-1487,2019,7 Conclusion and Future Work,"with deferral of explanation to neural models, it will be crucial in the future to study the ethical implications of biases that are accumulated during pretraining or fine-tuning."
P19-1488,2019,7 Conclusions,"based on the outcomes of our experiments, we recommend the ip method for the textkbqa model, rather than the model’s self-explanatory attention mechanism or lime."
P19-1488,2019,7 Conclusions,"the evaluated methods are attention, lime and input perturbation."
P19-1488,2019,7 Conclusions,"to compare their performance, we introduced an automatic evaluation paradigm with fake facts, which does not require manual annotations."
P19-1488,2019,7 Conclusions,we performed the first evaluation of different explanation methods for a qa model working on a combination of kb and text.
P19-1488,2019,7 Conclusions,"we validated the ranking obtained with this paradigm through an experiment with human participants, where we observed the same ranking."
P19-1489,2019,7 Discussion: What Modularity Can andCannot Do,"additionally, we believe that modularity could serve as a useful prior for the algorithms that learn cross-lingual word embeddings: during learning prefer updates that avoid increasing modularity if all else is equal."
P19-1489,2019,7 Discussion: What Modularity Can andCannot Do,"consider the following cross-lingual word embedding “algorithm”: for each word, select a random point on the unit hypersphere."
P19-1489,2019,7 Discussion: What Modularity Can andCannot Do,"future work should investigate how to combine techniques that use both word meaning and nearest neighbors for a more robust, semisupervised cross-lingual evaluation."
P19-1489,2019,7 Discussion: What Modularity Can andCannot Do,"nevertheless, this representation will have very low modularity."
P19-1489,2019,7 Discussion: What Modularity Can andCannot Do,"nevertheless, we recognize limitations of modularity."
P19-1489,2019,7 Discussion: What Modularity Can andCannot Do,this is a horrible distributed representation: the position of words’ embedding has no relationship to the underlying meaning.
P19-1489,2019,7 Discussion: What Modularity Can andCannot Do,this work focuses on modularity as a diagnostic tool: it is cheap and effective at discovering which embeddings are likely to falter on downstream tasks.
P19-1489,2019,7 Discussion: What Modularity Can andCannot Do,"thus, practitioners should consider including it as a metric for evaluating the quality of their embeddings."
P19-1489,2019,7 Discussion: What Modularity Can andCannot Do,"thus, while modularity can identify bad embeddings, once vectors are well mixed, this metric—unlike qvec or qvec-cca—cannot identify whether the meanings make sense."
P19-1490,2019,5 Conclusion and Future Work,"in the future, we will work on cross-lingual extensions of monolingual hyperbolic embedding models (nickel and kiela, 2017; ganea et al., 2018)."
P19-1490,2019,5 Conclusion and Future Work,we have demonstrated its robustness and usefulness for graded and ungraded le in monolingual and cross-lingual settings.
P19-1490,2019,5 Conclusion and Future Work,"we have proposed a novel graded cross-lingual lexical entailment (le) task, introducing new monolingual and cross-lingual graded le datasets that hold promise to support future research on this topic."
P19-1490,2019,5 Conclusion and Future Work,we have then proposed a transfer-based method that can reason over graded le across languages.
P19-1490,2019,5 Conclusion and Future Work,"we will also experiment with other sources of bilingual information (e.g., cross-lingual word embeddings) and port the transfer approach to more language pairs, with a particular focus on resource-poor languages."
P19-1491,2019,7 Conclusion,"however, a language’s vocabulary size and the length in characters of its sentences were well-correlated with difficulty on our large set of languages."
P19-1491,2019,7 Conclusion,"our language difficulty estimates were largely stable across datasets and language model architectures, but they were not significantly predicted by linguistic factors."
P19-1491,2019,7 Conclusion,"our mixed-effects approach could be used to assess other nlp systems via parallel texts, separating out the influences on performance of language, sentence, model architecture, and training procedure."
P19-1491,2019,7 Conclusion,there is a real danger in cross-linguistic studies of over-extrapolating from limited data.
P19-1491,2019,7 Conclusion,"we reevaluated the conclusions of cotterell et al.(2018) on a larger set of languages, requiring new methods to select fully parallel data (§4.2) or handle missing data."
P19-1491,2019,7 Conclusion,we showed how to fit a paired-sample multiplicative mixed-effects model to probabilistically obtain language difficulties from at-least-pairwise parallel corpora.
P19-1492,2019,6 Conclusions and future work,"in particular, we would like to explore new methods to jointly learn cross-lingual embeddings on monolingual corpora."
P19-1492,2019,6 Conclusions and future work,"in this work, we compare the properties of crosslingual word embeddings trained through joint learning and offline mapping on parallel corpora."
P19-1492,2019,6 Conclusions and future work,"this analysis calls for further research on alternatives to current mapping methods, which have been very successful on unsupervised settings."
P19-1492,2019,6 Conclusions and future work,"we observe that, under these ideal conditions, joint learning yields to more isomorphic embeddings, is less sensitive to hubness, and obtains stronger results in bilingual lexicon induction, concluding that current mapping methods have strong limitations."
P19-1493,2019,6 Conclusion,"as to why m-bert generalizes across languages, we hypothesize that having word pieces used in all languages (numbers, urls, etc) which have to be mapped to a shared space forces the co-occurring pieces to also be mapped to a shared space, thus spreading the effect to other word pieces, until different languages are close to a shared space."
P19-1493,2019,6 Conclusion,"in this work, we showed that m-bert’s robust, often surprising, ability to generalize crosslingually is underpinned by a multilingual representation, without being explicitly trained for it."
P19-1493,2019,6 Conclusion,it is our hope that these kinds of probing experiments will help steer researchers toward the most promising lines of inquiry by encouraging them to focus on the places where current contextualized word representation approaches fall short.
P19-1493,2019,6 Conclusion,"the model handles transfer across scripts and to code-switching fairly well, but effective transfer to typologically divergent and transliterated targets will likely require the model to incorporate an explicit multilingual training objective, such as that used by lample and conneau (2019) or artetxe and schwenk (2018)."
P19-1494,2019,6 Conclusions and future work,"in addition to that, we would like to integrate our induced dictionaries in other downstream tasks like unsupervised cross-lingual information retrieval (litschko et al., 2018)."
P19-1494,2019,6 Conclusions and future work,"in the future, we would like to further improve our method by incorporating additional ideas from unsupervised machine translation such as joint refinement and neural hybridization (artetxe et al., 2019)."
P19-1494,2019,6 Conclusions and future work,"our approach does not require any additional resource besides the monolingual corpora used to train the embeddings, and outperforms traditional retrieval techniques by a substantial margin."
P19-1494,2019,6 Conclusions and future work,our code is available at https://github.com/ artetxem/monoses.
P19-1494,2019,6 Conclusions and future work,"we propose a new approach to bli which, instead of directly inducing bilingual dictionaries from cross-lingual embedding mappings, uses them to build an unsupervised machine translation system, which is then used to generate a synthetic parallel corpus from which to extract bilingual lexica."
P19-1494,2019,6 Conclusions and future work,"we thus conclude that, contrary to recent trend, future work in bli should not focus exclusively in direct retrieval approaches, nor should bli be the only evaluation task for cross-lingual embeddings."
P19-1495,2019,7 Conclusions & Future Work,a predictive model for identification of complaints is useful to companies that wish to automatically gather and analyze complaints about a particular event or product.
P19-1495,2019,7 Conclusions & Future Work,all data and code is available to the research community to foster further research on complaints.
P19-1495,2019,7 Conclusions & Future Work,"another research direction is to study the role of complaints in personal conversation or in the political domain, e.g., predicting political stance in elections (tsakalidis et al., 2018)."
P19-1495,2019,7 Conclusions & Future Work,"in the future, we plan to identify the target of the complaint in a similar way to aspect-based sentiment analysis (pontiki et al., 2016)."
P19-1495,2019,7 Conclusions & Future Work,"then, we built predictive models of complaints in tweets using a wide range of features reaching up to 79% macro f1 (0.885 auc) and conducted experiments using distant supervision and domain adaptation to boost predictive performance."
P19-1495,2019,7 Conclusions & Future Work,this would allow them to improve efficiency in customer service or to more cheaply gauge popular opinion in a timely manner in order to identify common issues around a product launch or policy proposal.
P19-1495,2019,7 Conclusions & Future Work,"to this end, we introduced the first data set consisting of english twitter posts annotated with complaints across nine domains."
P19-1495,2019,7 Conclusions & Future Work,we analyzed the syntactic patterns and linguistic markers specific of complaints.
P19-1495,2019,7 Conclusions & Future Work,we plan to use additional context and conversational structure to improve performance and identify the sociodemographic covariates of expressing and phrasing complaints.
P19-1495,2019,7 Conclusions & Future Work,"we presented the first computational approach using methods from computational linguistics and machine learning to modeling complaints as defined in prior studies in linguistics and pragmatics (olshtain and weinbach, 1987)."
P19-1495,2019,7 Conclusions & Future Work,we studied performance of complaint prediction models on each individual domain and presented results with a domain adaptation approach which overall improves predictive accuracy.
P19-1496,2019,6 Conclusion,"specifically, we find that qa on social media requires systems to comprehend social media specific linguistic patterns like informality, hashtags, usernames, and authorship."
P19-1496,2019,6 Conclusion,the proposed dataset informs us of the distinctiveness of social media from formal domains in the context of qa.
P19-1496,2019,6 Conclusion,these distinguishing linguistic factors bring up important problems for the research of qa that currently focuses on formal text.
P19-1496,2019,6 Conclusion,we present the first dataset for qa on social media data by leveraging news media and crowdsourcing.
P19-1496,2019,6 Conclusion,we see our dataset as a first step towards enabling not only a deeper understanding of natural language in social media but also rich applications that can extract essential real-time knowledge from social media.
P19-1497,2019,5 Conclusion and Future Work,"besides, how to better leverage prior knowledge during openqg (like human often do) is also interesting."
P19-1497,2019,5 Conclusion and Future Work,experiments show that our model outperforms commonly-used text generation methods.
P19-1497,2019,5 Conclusion and Future Work,"finally, combining openqg with its reverse task, openqa, is also worth exploration."
P19-1497,2019,5 Conclusion and Future Work,"first, we will explore more powerful qg structure to deal with the huge difference between the length of input and output texts."
P19-1497,2019,5 Conclusion and Future Work,"for question generation, we propose a model based on cgan using reinforcement learning to introduce information from our evaluation model."
P19-1497,2019,5 Conclusion and Future Work,"in this paper, we take the first step on teaching machines to ask open-answered questions from news for open discussion."
P19-1497,2019,5 Conclusion and Future Work,there are many future works to be done.
P19-1497,2019,5 Conclusion and Future Work,"these conclusions help us to build question evaluation models, and can also used to compare results of different question generation models."
P19-1497,2019,5 Conclusion and Future Work,"to generate high-qualified questions, we analysis how language use affects the number of answers that a question receives based on oqrand, a dataset created by variable control."
P19-1498,2019,5 Conclusion,"in this work, we explored a few variants of lstm cells for rumor-veracity and stance learning tasks in social-media conversations."
P19-1498,2019,5 Conclusion,"using a human labeled dataset with rumor-veracity labels for source posts and stance labels for replies, we evaluated the proposed models and compared their strengths and weaknesses."
P19-1498,2019,5 Conclusion,we also experimented with different types of features and find that skipthoughts and bert are competitive features while skipthoughts have slight advantage for rumor-veracity prediction task.
P19-1498,2019,5 Conclusion,we also proposed a new binarized constituency tree structure to model social-media conversations.
P19-1498,2019,5 Conclusion,we find that using convolution unit in lstms is useful for both stance and rumor classification.
P19-1499,2019,5 Conclusions,adding the large open-domain dataset to pre-training leads to even better performance.
P19-1499,2019,5 Conclusions,"in the future, we plan to apply models to other tasks that also require hierarchical document encodings (e.g., document question answering)."
P19-1499,2019,5 Conclusions,the core part of a neural extractive summarization model is the hierarchical document encoder.
P19-1499,2019,5 Conclusions,we are also interested in improving the architectures of hierarchical document encoders and designing other objectives to train hierarchical transformers.
P19-1499,2019,5 Conclusions,we proposed a method to pre-train document level hierarchical bidirectional transformer encoders on unlabeled data.
P19-1499,2019,5 Conclusions,"when we only pre-train hierarchical transformers on the training sets of summarization datasets with our proposed objective, application of the pre-trained hierarchical transformers to extractive summarization models already leads to wide improvement of summarization performance."
P19-1500,2019,6 Conclusions,experimental results show that our model produces summaries which are both fluent and informative outperforming competitive systems by a wide margin.
P19-1500,2019,6 Conclusions,in the future we would like to apply our hierarchical transformer to question answering and related textual inference tasks.
P19-1500,2019,6 Conclusions,in this paper we conceptualized abstractive multidocument summarization as a machine learning problem.
P19-1500,2019,6 Conclusions,we have also demonstrated the importance of a learning-based approach for selecting which documents to summarize.
P19-1500,2019,6 Conclusions,"we proposed a new model which is able to encode multiple input documents hierarchically, learn latent relations across them, and additionally incorporate structural information from well-known graph representations."
P19-1501,2019,7 Conclusion and Future Work,"even though deep learning approaches have been widely used in abstractive ts, it is evident that their combination with semantic-based or structure-based methodologies needs to be more thoroughly studied."
P19-1501,2019,7 Conclusion and Future Work,"finally, the distinct semantic representation of each word could further enhance the performance of the deep learning model."
P19-1501,2019,7 Conclusion and Future Work,"in this direction, the proposed novel framework combines deep learning techniques with semantic-based content methodologies so as to produce abstractive summaries in generalized form, which, in turn, are transformed into the final summaries."
P19-1501,2019,7 Conclusion and Future Work,"moreover, as the ambiguity is a challenging problem in natural language processing, it would be interesting to capture the particular meaning of each word in the text so that our methodology manages to uncover the specific semantic meaning of words."
P19-1501,2019,7 Conclusion and Future Work,"of course, certain aspects of the proposed methodology could be extended."
P19-1501,2019,7 Conclusion and Future Work,"since currently only nouns are considered for generalization, an expansion to verbs could result in additional improvement."
P19-1501,2019,7 Conclusion and Future Work,the experimental results have demonstrated that the followed approach enhances the performance of deep learning models.
P19-1501,2019,7 Conclusion and Future Work,the obtained results show that the proposed approach is an effective methodology of handling oov or rare words and it improves the performance of text summarization.
P19-1501,2019,7 Conclusion and Future Work,the positive results may be attributed to the optimization of the parameters of the deep leaning model and the ability of the method to handle oov and very low frequency words.
P19-1502,2019,5 Conclusion,"by collecting human judgments in this specific range, we could identify the best ones using standard meta-evaluation techniques."
P19-1502,2019,5 Conclusion,evaluation metrics behave similarly on the average scoring range covered by existing human judgment datasets.
P19-1502,2019,5 Conclusion,"indeed, since metrics strongly disagree in the high-scoring regime, at least some of them are deviating largely from humans."
P19-1502,2019,5 Conclusion,such annotations would also be greatly beneficial to improve summarization systems and evaluation metrics alike.
P19-1502,2019,5 Conclusion,this casts some doubts on the evaluation methodologies in summarization and calls for the collection of human annotations for high-scoring summaries.
P19-1502,2019,5 Conclusion,this disagreement is strong enough that there is no common trend which could be captured by reporting improvements across several metrics.
P19-1502,2019,5 Conclusion,"thus, we cannot clearly decide which one is the best."
P19-1502,2019,5 Conclusion,"yet, we showed that they will promote very different summaries in the highscoring range."
P19-1503,2019,5 Conclusion,future work could be comparing language models of different types and scales in this direction.
P19-1503,2019,5 Conclusion,previous neural unsupervised works mostly adopt complex encoder-decoder frameworks.
P19-1503,2019,5 Conclusion,we achieve good generation qualities and competitive evaluation scores.
P19-1503,2019,5 Conclusion,we also demonstrate a new way of utilizing pre-trained generic language models for contextual matching in untrained generation.
P19-1503,2019,5 Conclusion,we propose a novel methodology for unsupervised sentence summarization using contextual matching.
P19-1504,2019,6 Conclusions,comparison of our model against competitive singlesequence decoders shows that structured decoding yields summaries with better content coverage.
P19-1504,2019,6 Conclusions,our decoder is aware of which topics to mention in a sentence as well as of its position in the summary.
P19-1504,2019,6 Conclusions,we introduced a novel structured decoder module for multi-document summarization.
P19-1505,2019,6 Conclusion,"a canonical example is the past participle of stride (e.g., ?strode/ ?stridden/ ?strided)."
P19-1505,2019,6 Conclusion,"adopting the terminology of yang (2002), we can say that low frequency forms free-ride on the higher frequency members of the lexeme."
P19-1505,2019,6 Conclusion,"by measuring which words or lexemes are most predictable from one another, a general picture of morphological relatedness within a language can be built in a bottom-up way."
P19-1505,2019,6 Conclusion,"focusing on a subset of the languages for which the model was able to recover the correct inflected forms at a high rate (§5.2.2), we showed that average irregularity varies a good deal between languages."
P19-1505,2019,6 Conclusion,"in §5.2.1, we showed that this measure produces results that are consistent with human judgements."
P19-1505,2019,6 Conclusion,"in these cases, the problem seems to be that the irregularity of the overall lexeme is known, but the particular word form has never been observed."
P19-1505,2019,6 Conclusion,"in this paper, we have introduced a measure of irregularity based on wug-testing a model of morphological inflection."
P19-1505,2019,6 Conclusion,"more generally, we observe that our wug-test techniques provides a general way of studying regularity and predictability within languages and may prove useful for attacking other difficult problems in the literature, such as detecting inflectional classes."
P19-1505,2019,6 Conclusion,our results provide further support for the view that inflected forms represent surface exponence of common underlying morphological objects.
P19-1505,2019,6 Conclusion,perhaps of greater interest than this positive result is the difference in the strength of the correlation between the level of individual forms and the level of lexemes.
P19-1505,2019,6 Conclusion,"such models seem necessary to account for paradigmatic structure cross linguistically and to deal with phenomena such as the existence of defective paradigms—the phenomenon whereby certain inflected forms of a word seem to be impossible for speakers (baerman et al., 2010)."
P19-1505,2019,6 Conclusion,the main novel empirical result of our paper was presented in §5.4 which showed that irregularity is correlated with frequency both at the level of individual forms as well as at the level of lexemes.
P19-1505,2019,6 Conclusion,"this difference appears to be driven by the fact that, in many cases, lexemes that contain high-frequency forms will also contain a few low frequency forms as well."
P19-1505,2019,6 Conclusion,this finding lends credence to models of linguistic structure which group words together by their lexeme or stem.
P19-1505,2019,6 Conclusion,this result is consistent with the findings of cotterell et al.(2018a) which gave large scale empirical evidence of a tradeoff between the size of morphological paradigms and the predictability of individual forms within each paradigm.
P19-1505,2019,6 Conclusion,"to our knowledge, this is the first large-scale empirical demonstration of this piece of linguistic folk wisdom and provides evidence relevant to recent proposals questioning this generalization (fratini et al., 2014; yang, 2016)."
P19-1506,2019,5 Discussion and Conclusions,"a theory of human language processing might distinguish between symbolic language knowledge and processes that implement compositionality to produce semantics on the one hand, and implicit processes that leverage sequences and associations to produce expectations."
P19-1506,2019,5 Discussion and Conclusions,"bert is pre-trained on a very large corpus, but it still picked up a performance improvement when finetuned on the visual context and language, as compared to the corpus language signal alone."
P19-1506,2019,5 Discussion and Conclusions,how these predictions are formed is unclear.
P19-1506,2019,5 Discussion and Conclusions,"however, we can show that an architecture that predicts linguistic input well learns better when its input mimics that of a human language learner."
P19-1506,2019,5 Discussion and Conclusions,"if even static visual input alone improves language acquisition, then what could a sensorily rich environment achieve?"
P19-1506,2019,5 Discussion and Conclusions,"in the garden path sentence “the horse raced past the barn fell”, the final word “fell” forces the reader to revise their initial interpretation of “raced” as the active verb (bever, 1970)."
P19-1506,2019,5 Discussion and Conclusions,"more generally, the idea of predictive coding holds that the mind forms expectations before perception occurs (see clark, 2013, for a review)."
P19-1506,2019,5 Discussion and Conclusions,"neural language models, as used here, are contenders as cognitive and psycholinguistic models of the non-symbolic, implicit aspects of language representation."
P19-1506,2019,5 Discussion and Conclusions,"predictive language models trained with a generic neural architecture, without specific linguistic universals, are a reasonable candidate for a model of predictive coding in language."
P19-1506,2019,5 Discussion and Conclusions,"specifically, augmenting a predictive language model with images that illustrate the sentences being learned enhances its next-word or masked-word prediction ability."
P19-1506,2019,5 Discussion and Conclusions,"the near state-of-the-art language model, using bert, reflects the case of human language acquisition less than do the other models, which were trained “ab initio” in a situated context."
P19-1506,2019,5 Discussion and Conclusions,"the performance improvement persists even in situations devoid of visual input, when the model is used as a pure language model."
P19-1506,2019,5 Discussion and Conclusions,"the surprisal of a word or phrase refers to the degree of mismatch between what a human listener expected to be said next and what is actually said, for example, when a garden path sentence forces the listener to abandon a partial, incremental parse (ferreira and henderson, 1991; hale, 2001)."
P19-1506,2019,5 Discussion and Conclusions,there is a great deal of evidence that something like a predictive language model exists in the human mind.
P19-1506,2019,5 Discussion and Conclusions,"this does not imply neuropsychological realism of the low-level representations or learning algorithms, and we cannot advocate for a specific neural architecture as being most plausible."
P19-1506,2019,5 Discussion and Conclusions,training with perceptual context improves multimodal neural models compared to training on language alone.
P19-1506,2019,5 Discussion and Conclusions,"we do not expect this to be a ceiling for visual augmentation: in the world of training lms, the ms coco corpus is, of course, a small dataset."
P19-1506,2019,5 Discussion and Conclusions,"when a multi-modal learner is considered, then, perhaps, the language acquisition stimulus that has been famously labeled to be rather poor (chomsky, 1959; berwick et al., 2013), is quite rich after all."
P19-1506,2019,5 Discussion and Conclusions,"with respect to acquiring the latter, implicit and predictive model, we note that children are exposed to a rich sensory environment, one more detailed than what is provided to our model here."
P19-1507,2019,6 Conclusion,"encouraged by these findings, we use deep networks to generate synthetic brain data to show that it helps in improving accuracy in a subsequent stimulus decoding task."
P19-1507,2019,6 Conclusion,"in particular, most models are able to predict activity in the left temporal region of the brain with high accuracy."
P19-1507,2019,6 Conclusion,"in this paper, we study the relationship between sentence representations learned by deep neural network models and those encoded by the brain."
P19-1507,2019,6 Conclusion,representations learned by bert are the most effective in predicting brain activity.
P19-1507,2019,6 Conclusion,such data augmentation approach is very promising as actual brain data collection in large quantities from human subjects is an expensive and labor-intensive process.
P19-1507,2019,6 Conclusion,this brain region is also known to be responsible for processing syntax and semantics for language understanding.
P19-1507,2019,6 Conclusion,"to the best of our knowledge, this is the first work showing that the meg data, when reading a word in a sentence, can be used to distinguish earlier words in the sentence."
P19-1507,2019,6 Conclusion,we are hopeful that the ideas explored in the paper will promote further research in understanding relationships between representations learned by deep models and the brain during language processing tasks.
P19-1507,2019,6 Conclusion,"we encode simple sentences using multiple deep networks, such as elmo, bert, etc."
P19-1507,2019,6 Conclusion,we make use of meg brain imaging data as reference.
P19-1508,2019,8 Conclusion,"in our work, we show for the first time that sensorimotor and to some extent language-related brain regions that correlate with distributional semantic models of action verbs may be impacted by negation."
P19-1508,2019,8 Conclusion,our work paves the way towards understanding the extent to which human meaning representation is impacted by negation.
P19-1508,2019,8 Conclusion,"this finding can in turn inform the design of distributional models dealing with verb negation, for instance when modelling negation as a space of alternative meanings."
P19-1508,2019,8 Conclusion,we also show that this effect may extend to more complex compositional models (in motor brain regions).
P19-1509,2019,5 Discussion,"a better understanding of what are the “innate” biases of standard models in highly controlled setups, such as the one studied here, should complement large-scale simulations, as part of the effort to develop new methods to encourage the emergence of more human-like language."
P19-1509,2019,5 Discussion,"for example, our results suggest that current neural networks, as they are not subject to human-like least-effort constraints, might not display the same trend towards efficient communication that we encounter in natural languages."
P19-1509,2019,5 Discussion,"for example, they exhibit a preference for a backward order, and there are only weak signs of a trade-off between different ways to encode constituent roles, with redundant solutions often being preferred."
P19-1509,2019,5 Discussion,how to incorporate “effort”-based pressures in neural networks is an exciting direction for future work.
P19-1509,2019,5 Discussion,"in other ways, our agents depart from typical human language patterns."
P19-1509,2019,5 Discussion,"it has been observed that the communication protocol emerging in such simulations is very different from human language (e.g., kottur et al., 2017; lewis et al., 2017; bouchacourt and baroni, 2018)."
P19-1509,2019,5 Discussion,the research direction we introduced might lead to a better understanding of the biases that affect the linguistic behaviour of lstms and similar models.
P19-1509,2019,5 Discussion,"this could help current efforts towards the development of artificial agents that communicate to solve a task, with the ultimate goal of developing ais that can talk with humans."
P19-1509,2019,5 Discussion,"we found that some trends follow natural patterns, such as the tendency to limit word order to few configurations, and long-distance dependency minimization."
P19-1509,2019,5 Discussion,we studied whether word-order constraints widely attested in natural languages affect learning and diachronic transmission in seq2seq agents.
P19-1510,2019,5 Summary,"additionally, our annotations are built on top of the ptb, so that the nne dataset will allow joint learning models to take advantage of semantic and syntactic annotations, and ultimately to understand and exploit the true structure of named entities."
P19-1510,2019,5 Summary,"we are optimistic that nne will encourage the development of new ner models that recognize structural information within entities, and therefore understand finegrained semantic information captured."
P19-1510,2019,5 Summary,"we present nne, a large-scale, nested, finegrained named entity dataset."
P19-1511,2019,6 Conclusions and Future Work,"as the head-driven structures are widely spread in natural language, the solution proposed in this paper can also be used for modeling and exploiting this structure in many other nlp tasks, such as semantic role labeling and event extraction."
P19-1511,2019,6 Conclusions and Future Work,experiments show that arns achieve the state-of-theart performance on all three benchmarks.
P19-1511,2019,6 Conclusions and Future Work,"furthermore, we also propose bag loss to train arns in an end-to-end manner without using any anchor word annotation."
P19-1511,2019,6 Conclusions and Future Work,"specifically, an anchor detector is first used to detect the anchor words of entity mentions and then a region recognizer is designed to recognize the mention boundaries centering at each anchor word."
P19-1511,2019,6 Conclusions and Future Work,"this paper proposes anchor-region networks, a sequence-to-nuggets architecture which can naturally detect nested entity mentions by modeling and exploiting head-driven phrase structures of entity mentions."
P19-1512,2019,6 Conclusion,"compared with existing solutions, the attention mechanisms employed by our gane model enjoys the following benefits: (i) it is naturally sparse and self-normalized, (ii) it is a global sequence matching scheme, and (iii) it can capture long-term interactions between two sentences."
P19-1512,2019,6 Conclusion,"looking forward, our attention mechanism can also be applied to tasks such as relational networks (santoro et al., 2017), natural language inference (maccartney and manning, 2009), and qa systems (zhou et al., 2015)."
P19-1512,2019,6 Conclusion,these claims are supported by experimental evidence from link prediction and multi-label vertex classification.
P19-1512,2019,6 Conclusion,we have proposed a novel and principled mutualattention framework based on optimal transport (ot).
P19-1513,2019,8 Conclusions,"also the work reported in this paper is based on a small tdm taxonomy, we plan to construct a tdm knowledge base and provide an applicable system for a wide range of nlp papers."
P19-1513,2019,8 Conclusions,experiments show that our model outperforms the baselines by a large margin in the identification of tdm triples.
P19-1513,2019,8 Conclusions,"in the future, more effort is needed to extract the best score."
P19-1513,2019,8 Conclusions,"in this paper, we have reported a framework to automatically extract tasks, datasets, evaluation metrics and scores from a set of published scientific papers in pdf format, in order to reconstruct the leaderboards for various tasks."
P19-1513,2019,8 Conclusions,"our first model extracts <task, dataset, metric> (tdm) triples, and our second model associates the best score reported in the paper to the corresponding tdm triple."
P19-1513,2019,8 Conclusions,we created two datasets in the nlp domain to test our system.
P19-1513,2019,8 Conclusions,"we have proposed a method, inspired by natural language inference, to facilitate learning similarity patterns between labels and the content words of papers."
P19-1514,2019,7 Conclusion,"even if the attribute size reaches tens of thousands or even millions, our approach only trains a single model for all attributes instead of building one specific model for each attribute."
P19-1514,2019,7 Conclusion,"experiments on a large dataset prove that this model is able to scale up to thousands of attributes, and outperforms state-of-the-art ner tagging models."
P19-1514,2019,7 Conclusion,"to extract product attribute values in e-commerce domain, previous sequence tagging models face two challenges, i.e., the huge amounts of product attributes and the emerging new attributes and new values that have not been seen before."
P19-1514,2019,7 Conclusion,"to tackle the above issues, we present a novel architecture of sequence tagging with the integration of attributes semantically."
P19-1514,2019,7 Conclusion,"when labeling new attributes that have not encountered before, by leveraging the learned information from existing attributes which have similar semantic distribution as the new ones, this model is able to extract the new values for new attributes."
P19-1515,2019,6 Conclusion,"in particular, we incorporate the linguistic constraints of keyphrases into the basic seq2seq network, and employ multi-task learning framework to enhance generation performance."
P19-1515,2019,6 Conclusion,"in this study, we propose the parallel seq2seq network with the coverage attention to alleviate the overlapping problem (including sub-phrase and super-phrase problems) in existing keyphrase generation methods."
P19-1515,2019,6 Conclusion,"the experimental results show that the proposed method can significantly outperform the state-ofthe-art copyrnn on scientific datasets, and is also effective in news domain."
P19-1516,2019,7 Conclusion,"in future, we would like to assist the model with multiple linguistic aspects of social media text like figurative languages."
P19-1516,2019,7 Conclusion,"in this paper, we have proposed an end-to-end multi-task framework that provides a unified solution for pharmacovigilance mining."
P19-1516,2019,7 Conclusion,our results demonstrate the capability of our model across all the datasets.
P19-1516,2019,7 Conclusion,we evaluated this framework on three benchmark pharmacovigilance datasets.
P19-1516,2019,7 Conclusion,"we have utilized an adversarial training based multi-task framework, which ensures that task-specific and task shared features are not contaminated."
P19-1517,2019,4 Conclusion and Future Work,"despite its simplicity, it yields better performance."
P19-1517,2019,4 Conclusion and Future Work,"in the future, we would also like to investigate better models that are capable to address general arithmetic word problems, including addition, subtraction, multiplication and division."
P19-1517,2019,4 Conclusion and Future Work,this work proposes the quantity tagger that regards solving addition-subtraction problem as a sequence labeling task by introducing the quantity span for each quantity.
P19-1518,2019,6 Conclusion,experimental results show that our seq2set model can outperform competitive baselines by a large margin.
P19-1518,2019,6 Conclusion,further analysis demonstrates that our approach can effectively reduce the sensitivity to the label order.
P19-1518,2019,6 Conclusion,"in this work, we present a simple but effective sequence-to-set model based on reinforcement learning, which aims to reduce the stringent requirements of the sequence-to-sequence model for label order."
P19-1518,2019,6 Conclusion,"the proposed model not only captures high-order correlations between labels, but also reduces the dependence on the order of output labels."
P19-1519,2019,6 Conclusions,a re-routing schema is proposed to further synergize the slot filling performance using the inferred intent representation.
P19-1519,2019,6 Conclusions,experiments on two real-world datasets show the effectiveness of the proposed models when compared with other alternatives as well as existing nlu services.
P19-1519,2019,6 Conclusions,"in this paper, a capsule-based model, namely capsule-nlu, is introduced to harness the hierarchical relationships among words, slots, and intents in the utterance for joint slot filling and intent detection."
P19-1519,2019,6 Conclusions,the learned word-level slot representations are futher aggregated to get the utterancelevel intent representations via dynamic routingby-agreement.
P19-1519,2019,6 Conclusions,"unlike treating slot filling as a sequential prediction problem, the proposed model assigns each word to its most appropriate slots in slotcaps by a dynamic routing-by-agreement schema."
P19-1520,2019,5 Conclusion and Future Work,"for future work, we plan to apply the main idea of our approach to other tasks."
P19-1520,2019,5 Conclusion and Future Work,"in this paper, we present an approach to improve the performance of neural aspect and opinion term extraction models with automatically mined rules."
P19-1520,2019,5 Conclusion and Future Work,the effectiveness of this approach is verified through our experiments.
P19-1520,2019,5 Conclusion and Future Work,"the mined rules are used to annotate a large unlabeled dataset, which is then used together with a small set of human annotated data to train better neural models."
P19-1520,2019,5 Conclusion and Future Work,we propose an algorithm to mine aspect and opinion term extraction rules that are based on the dependency relations of words in a sentence.
P19-1521,2019,6 Conclusions,experiments show that our methods significantly improve the performance of neural network event detection models.
P19-1521,2019,6 Conclusions,"in this paper, we propose cost-sensitive regularization for neural event detection, which introduces a cost-weighted term of mislabeling likelihood to enhance the training procedure to concentrate more on confusing type pairs."
P19-1522,2019,6 Conclusion and Discussion,and co-occurring roles tend to hold a tight relation.
P19-1522,2019,6 Conclusion and Discussion,events of the same type often share similarity.
P19-1522,2019,6 Conclusion and Discussion,experimental results show that the quality of generated data is competitive and incorporating them with existing corpus can make our proposed event extractor to be superior to several state of the art approaches.
P19-1522,2019,6 Conclusion and Discussion,"in addition, although our generation method can control the number of generated samples and filter with quality, it still suffers the deviation of roles alike with distant supervision."
P19-1522,2019,6 Conclusion and Discussion,"in this paper, we present a framework to promote event extraction by using a combination of an extraction model and a generation method, both of which are based on pre-trained language models."
P19-1522,2019,6 Conclusion and Discussion,it also benefits from the scoring mechanism which is able to quantify the quality of generated samples.
P19-1522,2019,6 Conclusion and Discussion,"on the other hand, there are still limitations in our work."
P19-1522,2019,6 Conclusion and Discussion,"such features are ignored in our model, but they deserve more investigation for improving the extraction model."
P19-1522,2019,6 Conclusion and Discussion,then it exploits the importance of roles to re-weight the loss function.
P19-1522,2019,6 Conclusion and Discussion,"therefore, for the future work, we will incorporate relation between events and relation between arguments into pre-trained language models, and take effective measures to overcome the deviation problem of roles in the generation."
P19-1522,2019,6 Conclusion and Discussion,this event generation method can produce controllably labeled samples through argument replacement and adjunct tokens rewriting.
P19-1522,2019,6 Conclusion and Discussion,"to perform event generation, we present a novel method that takes the existing events as prototypes."
P19-1522,2019,6 Conclusion and Discussion,"to solve the roles overlap problem, our extraction approach tries to separate the argument predictions in terms of roles."
P19-1523,2019,5 Conclusion,an error analysis is performed to shed light on possible future directions.
P19-1523,2019,5 Conclusion,"iteratively optimizing the loss function enables the model to incrementally learn from trial and error, yielding substantial improvement."
P19-1523,2019,5 Conclusion,we propose a binary classification loss function to calibrate confidences in open ie.
P19-1524,2019,4 Discussion,"also, we would like to further explore the possibility to use domain-specific gazetteers or dictionaries to boost the performance of ner in various domains (shang et al., 2018), beyond the standard corpora."
P19-1524,2019,4 Discussion,experimental results demonstrate the usefulness of gazetteer knowledge and show some promising results from our initial attempt to make use of gazetteer information.
P19-1524,2019,4 Discussion,"future directions will include trying similarly enhanced modules on other different types of segmental models (kong et al., 2016; liu et al., 2016; zhuo et al., 2016; zhai et al., 2017; sato et al., 2017), along with richer representations for further gain."
P19-1524,2019,4 Discussion,"in summary, we show that gazetteer-enhanced modules could be useful for neural ner models."
P19-1524,2019,4 Discussion,"table 5 lists some examples that the baselines failed to recognize as a complete entity name, while the sub-tagger enhanced system managed to do it."
P19-1524,2019,4 Discussion,the gazetteer possesses an abundance of similar terms that enables generalization to out-ofgazetteer items.
P19-1524,2019,4 Discussion,the sub-tagger has an advantage over hard matching with the capability of recognizing entity names not appearing in but being similar to those contained in the gazetteer.
P19-1524,2019,4 Discussion,"we checked a few cases for which only the sub-tagger enhanced model got correct predictions, and found terms with similar patterns from the gazetteer while not in training data as in table 6."
P19-1525,2019,7 Conclusions,"in contrast with existing token-level models: our model is able to use span-specific features, allows for overlapping entity mentions and does not use sequential decoding."
P19-1525,2019,7 Conclusions,our proposed model achieves a new state-of-the-art re performance on the ace2005 dataset.
P19-1525,2019,7 Conclusions,the gains are driven by improvements in recall for both tasks.
P19-1525,2019,7 Conclusions,we present a neural span-level end-to-end model for joint entity mention detection and relation extraction.
P19-1526,2019,6 Conclusion,d-ndmv extends neural dmv by parsing a sentence using grammar rule probabilities that are computed based on global information of the sentence.
P19-1526,2019,6 Conclusion,"in this way, d-ndmv breaks the context-free independence assumption in generative dependency grammars and is therefore more expressive."
P19-1526,2019,6 Conclusion,our extensive experimental results show that our approach achieves competitive accuracy compared with state-of-the-art parsers.
P19-1526,2019,6 Conclusion,"we propose d-ndmv, a novel unsupervised parser with characteristics from both generative and discriminative approaches to unsupervised parsing."
P19-1527,2019,6 Conclusions,"the lstm-crf modeling of ne multilabels is better suited for putatively less-nested and flat corpora, while the sequence-to-sequence architecture captures more complex relationships between nested and complicated named entities and surpasses the current state of the art in nested ner on four nested ne corpora."
P19-1527,2019,6 Conclusions,we also report surpassing state-of-theart results with the recently published contextual word embeddings on both nested and flat ne corpora.
P19-1527,2019,6 Conclusions,we presented two neural architectures for nested named entities and a simple encoding algorithm to allow the modeling of multiple ne labels in an enhanced bilou scheme.
P19-1528,2019,7 Conclusion,"building off of our previous work, we have considered the problem of incrementally computing the infix probabilities of each prefix of a given string."
P19-1528,2019,7 Conclusion,"furthermore, we solve the open problem of computing the infix probabilities of each prefix of a stream of characters."
P19-1528,2019,7 Conclusion,the problem of adapting this approach to higher order statistical language models (such as pcfgs) remains open.
P19-1528,2019,7 Conclusion,we provide an improved analysis of our incremental algorithm that leads to an asymptotic speedup.
P19-1529,2019,6 Conclusion and Future Work,"for example, is it better for all the models in an ensemble to use the same external information, or is it more effective if they make use of different kinds of information?"
P19-1529,2019,6 Conclusion and Future Work,in future work we will explore how external information is best used in ensembles of models for srl and other tasks.
P19-1529,2019,6 Conclusion and Future Work,our best system sets a new state-of-theart for non-ensemble srl systems on in-domain data.
P19-1529,2019,6 Conclusion and Future Work,"this paper evaluated three different ways of representing external syntactic parses, and three different ways of injecting that information into a stateof-the-art srl system."
P19-1529,2019,6 Conclusion and Future Work,"using the external syntactic information as input features was far more effective than a multi-task learning approach, and just as effective as an auto-encoder approach."
P19-1529,2019,6 Conclusion and Future Work,we showed that representing the external syntactic information as constituents was most effective.
P19-1529,2019,6 Conclusion and Future Work,we will also investigate whether the choice of method for injecting external information has the same impact on other nlp tasks as it does on srl.
P19-1530,2019,5 Conclusion,our proposed conversion method can be easily combined with other tree-based parsers.
P19-1530,2019,5 Conclusion,"this paper proposes a conversion of ptb graphs into ptb augmented trees, which enables us to reduce nonlocal dependency identification and constituency parsing into single parsing."
P19-1530,2019,5 Conclusion,we can expect that the evolution of tree-based parsing technology makes our approach improve the accuracy of nonlocal dependency identification.
P19-1531,2019,5 Conclusion,"it combines multi-task learning, auxiliary tasks, and sequence labeling parsing, so that constituency and dependency parsing can benefit each other through learning across their representations."
P19-1531,2019,5 Conclusion,source code will be released upon acceptance.
P19-1531,2019,5 Conclusion,we have described a framework to leverage the complementary nature of constituency and dependency parsing.
P19-1531,2019,5 Conclusion,"we have shown that mtl models with auxiliary losses outperform single-task models, and mtl models that treat both constituency and dependency parsing as main tasks obtain strong results, coming almost at no cost in terms of speed."
P19-1532,2019,5 Conclusion,"as a result, our model outperforms the baseline by a clear margin and the ablation analysis proves the effectiveness of our method."
P19-1532,2019,5 Conclusion,"as a side effect, this module also improves the interpretability of models."
P19-1532,2019,5 Conclusion,"considering reinforce method is used in training, a novel method is introduced to reduce the variance of rewards."
P19-1532,2019,5 Conclusion,"in this paper, we propose a prism module to reduce the noise of word embeddings by selectively replacing some words with task-related semantic aspects."
P19-1532,2019,5 Conclusion,"since our prism module can be easily integrated into existing models, it can be applied in a wide range of neural architectures."
P19-1532,2019,5 Conclusion,we also introduce a structure to train this prism module jointly with existing model and no extra data is needed.
P19-1533,2019,7 Conclusion,future work will consider problems where more challenging forms of neighbor manipulation are necessary for prediction.
P19-1533,2019,7 Conclusion,"we have also proposed an approach to sequence label prediction in the presence of retrieved neighbors, which allows for discouraging the use of many distinct segments in a labeling."
P19-1533,2019,7 Conclusion,"we have proposed a simple label-agnostic sequence-labeling model, which performs nearly as well as a standard sequence labeler, but improves on zero-shot transfer tasks."
P19-1567,2019,7 Conclusion,we hope that this work will raise more interest in developing alignment systems for longer paraphrase.
P19-1568,2019,7 Conclusion and Future Work,another potential future work would be to explore other ways of providing rich supervision from textual descriptions as targets.
P19-1568,2019,7 Conclusion and Future Work,ewise improves state-of-the-art results on standardized benchmarks for wsd.
P19-1568,2019,7 Conclusion and Future Work,ewise uses sense embeddings as targets instead of discrete sense labels.
P19-1568,2019,7 Conclusion and Future Work,"our modular architecture opens up various avenues for improvements in few-shot learning for wsd, viz., context encoder, definition encoder, and leveraging structural knowledge."
P19-1568,2019,7 Conclusion and Future Work,"this helps the model gain zero-shot learning capabilities, demonstrated through ablation and detailed analysis."
P19-1568,2019,7 Conclusion and Future Work,this paper should serve as a starting point to better investigate wsd on out-of-vocabulary words.
P19-1568,2019,7 Conclusion and Future Work,we are releasing ewise code to promote reproducible research.
P19-1568,2019,7 Conclusion and Future Work,"we have introduced ewise, a general framework for learning wsd from a combination of senseannotated data, dictionary definitions and lexical knowledge bases."
P19-1569,2019,7 Future Work,in future work we plan to use multilingual resources (i.e.embeddings and glosses) for improving our sense embeddings and evaluating on multilingual wsd.
P19-1569,2019,7 Future Work,"we expect our sense embeddings to be particularly useful in downstream tasks that may benefit from relational knowledge made accessible through linking words (or spans) to commonsense-level concepts in wordnet, such as natural language inference."
P19-1569,2019,7 Future Work,"we’re also considering exploring a semi-supervised approach where our best embeddings would be employed to automatically annotate corpora, and repeat the process described on this paper until convergence, iteratively fine-tuning sense embeddings."
P19-1570,2019,8 Conclusion and future work,a natural extension to this work would be to capture the sense distribution of sentences using the same framework.
P19-1570,2019,8 Conclusion and future work,"further, the construction provides a natural mechanism to refine the representation of a word in a short context by disambiguating its senses."
P19-1570,2019,8 Conclusion and future work,this will make our model more comprehensive by enabling the embedding of words and short texts in the same space.
P19-1570,2019,8 Conclusion and future work,we demonstrated that such interpretable embeddings can be competitive with dense embeddings like w ord2vec on similarity tasks and can capture entailment effectively.
P19-1570,2019,8 Conclusion and future work,we have demonstrated the effectiveness of such contextual representations.
P19-1570,2019,8 Conclusion and future work,"we motivated an efficient unsupervised method to embed words, in and out of context, in a way that captures their multiple senses in a corpus in an interpretable manner."
P19-1571,2019,6 Conclusion and Future Work,"in experiments, our proposed sememe-incorporated models achieve impressive performance gain on both intrinsic and extrinsic evaluations in comparison with baseline methods without considering external knowledge."
P19-1571,2019,6 Conclusion and Future Work,"in the future, we will explore the following directions: (1) context information is also essential to mwe representation learning, and we will try to combine both internal information and external context information to learn better mwe representations; (2) many mwes lack sememe annotation and we will seek to calculate an mwe’s scd when we only know the sememes of the mwe’s constituents; (3) our proposed models are also applicable to the mwes with more than two constituents and we will extend our models to longer mwes; (4) sememe is universal linguistic knowledge and we will explore to generalize our methods to other languages."
P19-1571,2019,6 Conclusion and Future Work,"in this paper, we focus on utilizing sememes to model semantic compositionality (sc)."
P19-1571,2019,6 Conclusion and Future Work,"then we make the first attempt to employ sememes in a typical sc task, namely mwe representation learning."
P19-1571,2019,6 Conclusion and Future Work,we first design an sc degree (scd) measurement experiment to preliminarily prove the usefulness of sememes in modeling sc.
P19-1572,2019,6 Conclusion,"finally, we plan to further extend and use the humour dataset to investigate open questions on the linguistics of humour, such as what relationships hold between a pun’s phonology and its “successfulness” or humorousness (lagerquist, 1980; hempelmann and miller, 2017)."
P19-1572,2019,6 Conclusion,"for humorousness, we have provided a new set of crowdsourced pairwise comparisons, while for metaphor novelty we extracted pairwise labels from existing best–worst scaling data."
P19-1572,2019,6 Conclusion,"given that our model achieves good results with rudimentary, task-agnostic linguistic features, in future work we plan to investigate the use of humourand metaphor-specific features, including some of those used in past work (see §2) as well as those inspired by the prevailing linguistic theories of humour (attardo, 1994) and metaphor (black, 1955; lakoff and johnson, 1980)."
P19-1572,2019,6 Conclusion,"our experiments showed that gppl outperforms bws at ranking instances in the training set when few pairwise labels are available, and generalises well to ranking test instances that were not compared in the training set."
P19-1572,2019,6 Conclusion,"the benefits of including word and bigram frequency also point to possible further improvements using n-grams, tf–idf, or other task-agnostic linguistic features."
P19-1572,2019,6 Conclusion,this paper has introduced new tasks for evaluating the degree of humorousness of a short text and the novelty of a metaphor within a short text.
P19-1572,2019,6 Conclusion,"we have introduced a bayesian approach, gaussian process preference learning, that can use sparse pairwise annotations to estimate humorousness or novelty scores given word embeddings and linguistic features."
P19-1573,2019,7 Conclusion,"aside from combo embeddings, linguistic information is retained most exactly in the recently proposed laser sentence embeddings, provided by an encoder designed with a relatively simple bilstm architecture, but estimated on tremendous multilingual data."
P19-1573,2019,7 Conclusion,"further research is required to find out in what lies the success of laser embeddings: in the embedding size, in the magnitude of training data, or maybe in the multitude of used languages."
P19-1573,2019,7 Conclusion,"in the downstream-based scenario, the publicly available datasets for semantic relatedness and entailment were used."
P19-1573,2019,7 Conclusion,"in the probing-based scenario, a set of language-independent tests was designed and probing datasets were generated for two contrasting languages – english and polish."
P19-1573,2019,7 Conclusion,it is thereby universal for all languages with a ud treebank on which a natural language pre-processing system can be trained.
P19-1573,2019,7 Conclusion,the procedure of generating probing datasets is based on the universal dependency schema.
P19-1573,2019,7 Conclusion,we found out that the combo-based embeddings designed to convey morphosyntax encode linguistic information in the most accurate way.
P19-1573,2019,7 Conclusion,"we performed a series of probing and downstream experiments with three types of sentence embeddings in the senteval environment, followed by a thorough analysis of the linguistic content of sentence embeddings."
P19-1573,2019,7 Conclusion,we presented a methodology of empirical research on retention of linguistic information in sentence embeddings using probing and downstream tasks.
P19-1574,2019,6 Discussion and Conclusion,"a summary of our findings is as follows.(i) we can build a classifier that, with high accuracy, correctly predicts whether an embedding represents an ambiguous or an unambiguous word.(ii) we show that semantic classes are recognizable in embedding space – a novel result as far as we know for a real-world dataset – and much better with a nonlinear classifier than a linear one.(iii) the standard word embedding models learn embeddings that capture multiple meanings in a single vector well – if the meanings are frequent enough.(iv) difficult cases of ambiguity – rare word senses or words with numerous senses – are better captured when the dimensionality of the embedding space is increased."
P19-1574,2019,6 Discussion and Conclusion,"but this comes at a cost – specifically, cosine similarity of embeddings (as, e.g., used by knn, §5.2) becomes less predictive of s-class.(v) our diagnostic tests show that a uniform-weighted sum of the senses of a word w (i.e., unifς) is a high-quality representation of all senses of w – even if the word embedding of w is not."
P19-1574,2019,6 Discussion and Conclusion,"however, to achieve high-performance natural language understanding at the human level, our models also need to be able to have access to rare senses – just like humans do."
P19-1574,2019,6 Discussion and Conclusion,only then will we be able to show the benefit of word representations that represent rare senses accurately.
P19-1574,2019,6 Discussion and Conclusion,results suggest that embeddings with frequency-based weighting of meanings work better for these tasks.
P19-1574,2019,6 Discussion and Conclusion,this indicates that currently used tasks rarely need rare senses – they do fine if they have only access to frequent senses.
P19-1574,2019,6 Discussion and Conclusion,"this suggests again that the main problem is not ambiguity per se, but rare senses.(vi) rare senses are badly represented if we use explicit frequency-based weighting of meanings (i.e., wghtς) compared to word embedding learning models like skipgram."
P19-1574,2019,6 Discussion and Conclusion,"to relate these findings to sentence-based applications, we experimented with a number of public classification datasets."
P19-1574,2019,6 Discussion and Conclusion,"we applied these probing tasks on wiki-pse, a large new resource for analysis of ambiguity and word embeddings."
P19-1574,2019,6 Discussion and Conclusion,we conclude that we need harder nlp tasks for which performance depends on rare as well as frequent senses.
P19-1574,2019,6 Discussion and Conclusion,"we did so by designing two probing tasks, s-class prediction and ambiguity prediction."
P19-1574,2019,6 Discussion and Conclusion,we quantified how well multiple meanings are represented in word embeddings.
P19-1574,2019,6 Discussion and Conclusion,we used s-classes of wikipedia anchors to build our dataset of word/s-class pairs.
P19-1574,2019,6 Discussion and Conclusion,we view s-classes as corresponding to senses.
P19-1574,2019,6 Discussion and Conclusion,weighting all meanings equally means that a highly dominant sense (like “time” for “friday”) is severely downweighted.
P19-1575,2019,6 Conclusions,"by abstracting away from individual neurons and combining linear probes, task knowledge, and correlation techniques, insight into the knowledge learned by the neural models have been made more transparent."
P19-1575,2019,6 Conclusions,future work can improve this method further by examining the effects of different dimensionality reduction methods with varying properties on extracting the most informative pathways from the activations.
P19-1575,2019,6 Conclusions,"in this paper, we have demonstrated an approach for neural interpretation using neural pathways on recognizing textual entailment and named entity recognition."
P19-1575,2019,6 Conclusions,"this general interpretation method draws similar conclusions to highly domain-specific analyses, and while it will not replace the need for deep analysis, it provides a much simpler starting point for a broad class of models."
P19-1576,2019,5 Conclusions and Future Work,"another exciting avenue would involve exploring cross-lingual transfer of lfs, taking advantage of recent development in unsupervised cross-lingual embedding learning (artetxe et al., 2017; conneau et al., 2017)."
P19-1576,2019,5 Conclusions and Future Work,"however, even with these improvements, categorizing lfs proves to be a difficult task."
P19-1576,2019,5 Conclusions and Future Work,"in addition, we have used the diffvec (vylomova et al., 2016) dataset to provide a frame of reference, as this dataset has been extensively studied in the distributional semantics literature, mostly for evaluating the role of vector difference."
P19-1576,2019,5 Conclusions and Future Work,"in the future, we would like to experiment with more data, so that enough training data can be obtained for less frequent lfs."
P19-1576,2019,5 Conclusions and Future Work,"in this paper, we have discussed the task of distributional collocation classification."
P19-1576,2019,5 Conclusions and Future Work,"to this end, we could benefit from the supervised approach proposed in (rodr′?guez-fernandez et al.′ , 2016), and then filter by pairwise correlation strength metrics such as pmi."
P19-1576,2019,5 Conclusions and Future Work,"we found that, despite this operation being the go-to representation for lexical relation modeling, concatenation works as well or better, and clear improvements can be obtained by incorporating explicitly learned relation vectors."
P19-1576,2019,5 Conclusions and Future Work,"we have used a set of collocations categorized by lexical functions, as introduced in the meaning text theory (mel’cuk ˇ , 1996), and evaluated a wide range of vector representations of relations."
P19-1577,2019,5 Conclusion,"in future, we plan to develop an automatic procedure of finding thesaurus regularities in dbag of problematic words, which can make more evident what kind of relations or senses are missed in the thesaurus."
P19-1577,2019,5 Conclusion,in this paper we discuss the usefulness of applying a checking procedure to existing thesauri.
P19-1577,2019,5 Conclusion,it is possible to find a lot of unexpected knowledge about the language and the thesaurus.
P19-1577,2019,5 Conclusion,the procedure is based on the analysis of discrepancies between corpus-based and thesaurus-based word similarities.
P19-1577,2019,5 Conclusion,"we applied the procedure to more than 30 thousand words of russian wordnet ruwordnet, classified sources of differences between word similarities and found some serious errors in word sense description including inaccurate relationships and missing senses for ambiguous words."
P19-1577,2019,5 Conclusion,we highly recommend using this procedure for checking wordnets.
P19-1578,2019,4 Discussion and Future Work,"besides the spelling errors mentioned above, grammar errors are also common in our chinese writing, which requires us to correct the erroneous sentence by insertion, deletion and even re-ordering."
P19-1578,2019,4 Discussion and Future Work,"for the future work, we hope to extend this idea proposed in this paper to train a model capable of handling different types of errors through the generative model since it can generate different lengths of results."
P19-1578,2019,4 Discussion and Future Work,"however, our model is unable to handle such errors in that we limit the length of the generated sentence to be same to that of the input sentence in order to incorporate confusionsets into our model as a guiding resource."
P19-1578,2019,4 Discussion and Future Work,"in our everyday chinese writing, there exist a variety of problematic usage of language, one of which is the spelling error referred in this paper."
P19-1578,2019,4 Discussion and Future Work,one concern is that we need to reconsider how to incorporate confusionsets into the encoder-decoder architecture.
P19-1578,2019,4 Discussion and Future Work,"such spelling errors are mainly generated due to the similarity of chinese characters in terms of sound, shape, and/or meaning, and the task is to detect the misspelled words and then replace them with their corresponding correct ones."
P19-1578,2019,4 Discussion and Future Work,"take as an example “我真不不明白，为啥他要自 杀。” (translation: i really don’t understand why he committed suicide.), we need to delete the character in red in order to guarantee the correctness of the sentence."
P19-1579,2019,7 Conclusion,"in future work, we will explore methods for controlling the induced dictionary quality to improve word substitution as well as m-umt."
P19-1579,2019,7 Conclusion,"we propose a generalized data augmentation framework for low-resource translation, making best use of all available resources."
P19-1579,2019,7 Conclusion,we propose an effective two-step pivoting augmentation method to convert hrl parallel data to lrl.
P19-1579,2019,7 Conclusion,we will also attempt to create an end-toend framework by jointly training m-umt pivoting system and low-resource translation system in an iterative fashion in order to leverage more versions of augmented data.
P19-1580,2019,8 Conclusions,"important heads have one or more interpretable functions in the model, including attending to adjacent words and tracking specific syntactic relations."
P19-1580,2019,8 Conclusions,"in future work, we would like to investigate how our pruning method compares to alternative methods of model compression in nmt."
P19-1580,2019,8 Conclusions,"moreover, the vast majority of heads, especially the encoder self-attention heads, can be removed without seriously affecting performance."
P19-1580,2019,8 Conclusions,"to determine if the remaining less-interpretable heads are crucial to the model’s performance, we introduce a new approach to pruning attention heads."
P19-1580,2019,8 Conclusions,we evaluate the contribution made by individual attention heads to transformer model performance on translation.
P19-1580,2019,8 Conclusions,"we observe that specialized heads are the last to be pruned, confirming their importance directly."
P19-1580,2019,8 Conclusions,we use layer-wise relevance propagation to show that the relative contribution of heads varies: only a small subset of heads appear to be important for the translation task.
P19-1581,2019,4 Conclusions,"although oovs can be represented in nmt systems, translation is difficult."
P19-1581,2019,4 Conclusions,in this paper we proposed a method for better translation of oovs.
P19-1581,2019,4 Conclusions,our approach relies on bilingual word embeddings based dictionaries which are simple to construct but cover a large vocabulary.
P19-1581,2019,4 Conclusions,our method of term mining followed by back-translation and fine-tuning can easily be applied to any nmt task including non-domainadaptation tasks.
P19-1581,2019,4 Conclusions,our results showed that having both source oovs and their translations in the sentence pairs results in improvements in bleu.
P19-1581,2019,4 Conclusions,using this noisy synthetic parallel data we fine-tune the initial nmt system.
P19-1581,2019,4 Conclusions,we mine targetlanguage sentences containing the 5?best translations of oovs according to our bwes.we then back-translate.
P19-1581,2019,4 Conclusions,we showed the performance of our approach on the translation of medical terms using a system trained on europarl parallel data.
P19-1582,2019,6 Conclusions,the model trained with this method can learn a flexible policy for simultaneous translation and achieve better translation quality and lower latency compared to previous methods.
P19-1582,2019,6 Conclusions,we also designed a restricted dynamic oracle for the simultaneous translation problem and provided a local training method utilizing this dynamic oracle.
P19-1582,2019,6 Conclusions,we have presented a simple model that includes a delay token in the target vocabulary such that the model can apply both read and write actions during translation process without a explicit policy model.
P19-1583,2019,4 Conclusion,tcs brings up to 2 bleu improvements over strong baselines with only slight increase in training time.
P19-1583,2019,4 Conclusion,"we propose target conditioned sampling (tcs), an efficient data selection framework for multilingual data by constructing a data sampling distribution that facilitates the nmt training of lrls."
P19-1584,2019,10 Conclusions & Future Work,a further possible extension is a dynamic noise level in the representation model that depends on the lstm output instead of being a trained weight.
P19-1584,2019,10 Conclusions & Future Work,"a model trained on our automatically pseudonymized data with n = 100 neighbors loses around one percentage point in f1 score when compared to the non-private system, scoring 96.75% on the i2b2 2014 test set."
P19-1584,2019,10 Conclusions & Future Work,"as precursors to our adversarial representation approach, we developed a deep learning model for de-identification that does not rely on explicit character features as well as an automatic word-level pseudonymization approach."
P19-1584,2019,10 Conclusions & Future Work,"eventually, improved de-identification classifiers could help enable large-scale medical studies that eventually improve public health."
P19-1584,2019,10 Conclusions & Future Work,"for an invariance criterion of up to n = 500 neighbors, training is stable and adversaries cannot beat the random guessing accuracy of 50%."
P19-1584,2019,10 Conclusions & Future Work,"further, we presented an adversarial learning based private representation of medical text that is invariant to any phi word being replaced with any of its embedding space neighbors and contains a random element."
P19-1584,2019,10 Conclusions & Future Work,future work: the automatic pseudonymization approach could serve as a data augmentation scheme to be used as a regularizer for deidentification models.
P19-1584,2019,10 Conclusions & Future Work,"in adversarial learning with the three-part training procedure, it might be possible to tune the λ parameter and define a better stopping condition that avoids the unstable characteristics with high values for n in the invariance criterion."
P19-1584,2019,10 Conclusions & Future Work,"in contrast, the automatic pseudonymization approach only reaches an f1 score of 95.0% at n = 500."
P19-1584,2019,10 Conclusions & Future Work,our adversarial representation approach enables cost-effective private sharing of training data for sequence labeling.
P19-1584,2019,10 Conclusions & Future Work,"our approach is based on adversarial learning, which yields representations that can be distributed since they do not contain private health information."
P19-1584,2019,10 Conclusions & Future Work,our implementation and experimental data are publicly available6 .
P19-1584,2019,10 Conclusions & Future Work,pooling of training data for de-identification from multiple institutions would lead to much more robust classifiers.
P19-1584,2019,10 Conclusions & Future Work,private character embeddings that are learned from a perturbed source could be an interesting extension to our models.
P19-1584,2019,10 Conclusions & Future Work,the representation acts as a task-specific feature extractor.
P19-1584,2019,10 Conclusions & Future Work,the representation allows training a de-identification model while being robust to adversaries trying to re-identify protected information or building a lookup table of representations.
P19-1584,2019,10 Conclusions & Future Work,the setup is motivated by the need of deidentification of medical text before sharing; our approach provides a lower-cost alternative than manual pseudonymization and gives rise to the pooling of de-identification datasets from heterogeneous sources in order to train more robust classifiers.
P19-1584,2019,10 Conclusions & Future Work,this might allow using lower amounts of noise for certain inputs while still being robust to the adversary.
P19-1584,2019,10 Conclusions & Future Work,"training a model on a combination of raw and pseudonymized data may result in better test scores on the i2b2 test set, possibly improving the state of the art."
P19-1584,2019,10 Conclusions & Future Work,"using the adversarially learned representation, de-identification models reach an f1 score of 97.4%, which is close to the non-private system (97.67%)."
P19-1584,2019,10 Conclusions & Future Work,we extended existing adversarial representation learning approaches by using two adversaries that discriminate real from fake sequence pairs with an additional sequence input.
P19-1584,2019,10 Conclusions & Future Work,we introduced a new approach to sharing training data for de-identification that requires lower human effort than the existing approach of document-coherent pseudonymization.
P19-1584,2019,10 Conclusions & Future Work,"when more training data from multiple sources become available in the future, it will be possible to evaluate our adversarially learned representation against unseen data."
P19-1585,2019,7 Conclusion,"despite being trained only for ner, the architecture provides intuitive embeddings for a variety of multi-word entities, a step which we suggest could prove useful for a variety of downstream tasks, including entity linking and coreference resolution."
P19-1585,2019,7 Conclusion,"the architecture performs strongly on the task of nested ner, setting a new state-of-the-art f1 score by close to 8 f1 points, and is also competitive at flat ner."
P19-1585,2019,7 Conclusion,"we have presented a novel neural network architecture for smoothly merging token embeddings in a sentence into entity embeddings, across multiple levels."
P19-1586,2019,7 Conclusion,"although our transfer learning alone did not suffice to construct a reliable and stable entity resolution system, it contributed to faster convergence and stable performance when used together with active learning."
P19-1586,2019,7 Conclusion,our frameworks of transfer and active learning for deep learning models are potentially applicable to low-resource settings beyond entity resolution.
P19-1586,2019,7 Conclusion,these results serve as further support for the claim that deep learning can provide a unified data integration method for downstream nlp tasks.
P19-1586,2019,7 Conclusion,"we presented transfer learning and active learning frameworks for entity resolution with deep learning and demonstrated that our models can achieve competitive, if not better, performance as compared to state-of-the-art learning-based methods while only using an order of magnitude less labeled data."
P19-1587,2019,5 Conclusion,"experiment results show that our model performs better than inference-time approaches at several precision levels, especially for longer mentions."
P19-1587,2019,5 Conclusion,the proposed model offers promising future extensions in terms of directly optimizing other metrics such as recall and fβ.
P19-1587,2019,5 Conclusion,this work also opens up a range of questions from modeling to evaluation methodology
P19-1587,2019,5 Conclusion,"to our best knowledge, it is the first training-time model for high precision structured prediction."
P19-1587,2019,5 Conclusion,we proposed a semi-markov ssvm model for high-precision ner.
P19-1588,2019,6 Conclusion,"in the future, first, we try to utilize more eyetracking corpus and estimate more features of reading behavior."
P19-1588,2019,6 Conclusion,"in this paper, we consolidate the neural network keyphrase extraction algorithm with human attention represented by total reading time (trt) estimated from geco eye-tracking corpus."
P19-1588,2019,6 Conclusion,"moreover, human attention is also effective on unsupervised models."
P19-1588,2019,6 Conclusion,the proposed models yield a better performance on two twitter datasets.
P19-1588,2019,6 Conclusion,"then, we will attempt to analyze real human reading behavior on social media and thereby explore more specific human attention features on social media."
P19-1589,2019,5 Conclusion and Future Work,"in future work, we want to extend this approach to other natural language processing tasks."
P19-1589,2019,5 Conclusion and Future Work,"we show that the performance of supervised relation classification models can be improved, even with limited supervision at training time, by framing relation classification as an instance of metalearning, and proposed a model-agnostic learning protocol for training relation classifiers with enhanced predictive performance in limited supervision settings."
P19-1590,2019,9 Conclusions,"in settings where bert cannot easily be used, either due to computational limitations, or because an appropriate pretrained model in the relevant language does not exist, vampire offers a competitive lightweight alternative for pretraining from unlabeled data in the low-resource setting."
P19-1590,2019,9 Conclusions,"in this paper, we confirm that these models are useful for text classification when the number of labeled instances is small, but demonstrate that fine-tuning to in-domain data is also of critical importance."
P19-1590,2019,9 Conclusions,"the emergence of models like elmo and bert has revived semi-supervised nlp, demonstrating that pretraining large models on massive amounts of data can provide representations that are beneficial for a wide range of nlp tasks."
P19-1590,2019,9 Conclusions,"when working with limited amounts of labeled data, we achieve superior performance to baselines such as self-training, or using word vectors pretrained on out-of-domain data, and approach the performance of elmo trained only on in-domain data at a fraction of the computational cost."
P19-1591,2019,6 Conclusions,"in future work we would like to develop more sophisticated trl algorithms, for both in-domain and domain adaptation nlp setups."
P19-1591,2019,6 Conclusions,"moreover, we would like to establish the theoretical groundings to the improved stability achieved by trl, and to explore this effect beyond domain adaptation."
P19-1591,2019,6 Conclusions,our trl algorithms are tailored to the pblm representation learning model of zr18 and aim to provide more effective training for this model.
P19-1591,2019,6 Conclusions,the resulting pblm-cnn model improves both the accuracy and the stability of the original pblm-cnn model where pblm is trained without trl.
P19-1591,2019,6 Conclusions,we proposed task refinement learning algorithms for domain adaptation with representation learning.
P19-1592,2019,6 Conclusion,"additionally, we hope to apply stance to a wider-range of entity resolution tasks, for which string similarity is a component of model that considers additional features such as the natural language context of the entity mention."
P19-1592,2019,6 Conclusion,"in future work, we hope to further study the connections between our optimal transport-based alignment method and methods based on attention."
P19-1592,2019,6 Conclusion,"in this work, we present stance, a neural model of string similarity that is trained end-to-end."
P19-1592,2019,6 Conclusion,"the main components of our model are: a characterlevel bidirectional lstm for character encoding, a soft alignment mechanism via optimal transport, and a powerful cnn for scoring alignments."
P19-1592,2019,6 Conclusion,"we also hope to consider connections to work on probabilistic latent representation of permutations and matchings (mena et al., 2018; linderman et al., 2018)."
P19-1592,2019,6 Conclusion,we also show that using stance improves upon state of the art performance in cross-document coreference in the twitter at the grammy’s dataset.
P19-1592,2019,6 Conclusion,we analyze our trained model and show that its optimal transport component helps to filter noise and that is has the capacity to learn non-standard similarity-preserving string edit patterns.
P19-1592,2019,6 Conclusion,we evaluate our model on 5 datasets created from publicly available knowledge bases and demonstrate that it outperforms the baselines in almost all cases.
P19-1593,2019,5 Conclusion,"a key question for future work is the performance on longer texts, such as the full-length news articles encountered in ontonotes."
P19-1593,2019,5 Conclusion,"another direction is to further explore semi-supervised learning, by reducing the amount of training data and incorporating linguistically-motivated constraints based on morphosyntactic features."
P19-1593,2019,5 Conclusion,"this enables semisupervised learning from a language modeling objective, which substantially improves performance."
P19-1593,2019,5 Conclusion,"this paper demonstrates the viability of incremental reference resolution, using an end-to-end differentiable memory network."
P19-1594,2019,5 Conclusions,"for example, we are interested in word-level language models that make use of character-level pnfa to compute expectations, which is useful to make predictions on words and substrings which do not appear in training."
P19-1594,2019,5 Conclusions,"given that we obtain similar results than feed-forward nn and some rnn, this suggests that some forms of non-linearities can be approximated by linear models, with the advantage that some computations (mainly, expectations) can be done exactly."
P19-1594,2019,5 Conclusions,in this paper we presented experiments using character-based spectral ngram language models.
P19-1594,2019,5 Conclusions,it is also interesting to consider a pnfa as a special case of an rnn which uses linear transitions.
P19-1594,2019,5 Conclusions,the ability of the spectral method for pnfa to estimate substring expectations can be exploited in other contexts.
P19-1594,2019,5 Conclusions,we combine two key ideas: a) modeling of longrange dependencies via the basis selection of long substring moments by quattoni et al.(2017); and b) efficient optimization of arbitrary prediction losses (e.g.cross-entropy) via a loss refinement step.
P19-1594,2019,5 Conclusions,"with these two ideas, we can improve the performance of spectral learning for pnfa, and bring the results of spectral models closer to the state-of-the-art."
P19-1595,2019,6 Discussion and Conclusion,"achieving robust multi-task gains across many tasks has remained elusive in previous research, so we hope our work will make multi-task learning more broadly useful within nlp."
P19-1595,2019,6 Discussion and Conclusion,"however, with the exception of closely related tasks with small datasets (e.g., mnli helping rte), the overall size of the gains from our multi-task method are small compared to the gains provided by transfer learning from self-supervised tasks (i.e., bert)."
P19-1595,2019,6 Discussion and Conclusion,it remains to be fully understood to what extent “self-supervised pre-training is all you need” and where transfer/multi-task learning from supervised tasks can provide the most value.
P19-1595,2019,6 Discussion and Conclusion,we have shown that single→multi distillation combined with teacher annealing produces results consistently better than standard single-task or multi-task training.
P19-1596,2019,6 Conclusions,"finally, we are interested in reproducing our corpus generation method on various other domains to allow for the creation of numerous useful datasets for the nlg community."
P19-1596,2019,6 Conclusions,"for future work, we plan on exploring other models for nlg, and on providing models with a more detailed input representation in order to help preserve more dependency information, as well as to encode more information on syntactic structures we want to realize in the output."
P19-1596,2019,6 Conclusions,"the dataset is unique in its huge range of stylistic variation and language richness, particularly compared to existing parallel corpora for nlg."
P19-1596,2019,6 Conclusions,"this paper presents the yelpnlg corpus, a set of 300,000 parallel sentences and mr pairs generated by sampling freely available review sentences that contain attributes of interest, and automatically constructing mrs for them."
P19-1596,2019,6 Conclusions,"we are also interested in including richer, more semanticallygrounded information in our mrs, for example using abstract meaning representations (amrs) (dorr et al., 1998; banarescu et al., 2013; flanigan et al., 2014)."
P19-1596,2019,6 Conclusions,"we train different models with varying levels of information related to attributes, adjective dependencies, sentiment, and style information, and present a rigorous set of evaluations to quantify the effect of the style markup on the ability of the models to achieve multiple style goals."
P19-1597,2019,5 Conclusion and Future Work,and unsupervised approaches to leverage massive chess comments in social media is also worth exploring.
P19-1597,2019,5 Conclusion and Future Work,another interesting direction is to extend our models to multimove commentary generation tasks.
P19-1597,2019,5 Conclusion and Future Work,"by introducing a compatible chess engine to comment generation models, we get models that can mine deeper information and ground more insightful comments to the input boards and moves."
P19-1597,2019,5 Conclusion and Future Work,comprehensive experiments demonstrate the effectiveness of our models.
P19-1597,2019,5 Conclusion and Future Work,in this work we propose a new approach for automated chess commentary generation.
P19-1597,2019,5 Conclusion and Future Work,our experiment results show the direction to further developing the state-of-the-art chess engine to improve generation models.
P19-1597,2019,5 Conclusion and Future Work,"we come up with the idea that models capable of playing chess will generate good comments, and models with better playing strength will perform better in generation."
P19-1598,2019,7 Conclusions and Future Work,"by relying on memorization, existing language models are unable to generate factually correct text about real-world entities."
P19-1598,2019,7 Conclusions and Future Work,"in our evaluation, we showed that by utilizing this graph, the proposed kglm is able to generate higher-quality, factually correct text that includes mentions of rare entities and specific tokens like numbers and dates."
P19-1598,2019,7 Conclusions and Future Work,"in particular, they are unable to capture the long tail of rare entities and word types like numbers and dates."
P19-1598,2019,7 Conclusions and Future Work,"in this work, we proposed the knowledge graph language model (kglm), a neural language model that can access an external source of facts, encoded as a knowledge graph, in order to generate text."
P19-1598,2019,7 Conclusions and Future Work,linked wikitext-2 is freely available for download at: https://rloganiv.github.io/ linked-wikitext-2.
P19-1598,2019,7 Conclusions and Future Work,"our distantly supervised approach to dataset creation can be used with other knowledge graphs and other kinds of text as well, providing opportunities for accurate language modeling in new domains."
P19-1598,2019,7 Conclusions and Future Work,our implementation is available at: https://github.com/rloganiv/ kglm-model.
P19-1598,2019,7 Conclusions and Future Work,"the limitations of the kglm model, such as the need for marginalization during inference and reliance on annotated tokens, raise new research problems for advancing neural nlp models."
P19-1598,2019,7 Conclusions and Future Work,this work lays the groundwork for future research into knowledge-aware language modeling.
P19-1598,2019,7 Conclusions and Future Work,"we also introduced linked wikitext2 containing text that has been aligned to facts in the knowledge graph, allowing efficient training of the model."
P19-1599,2019,8 Conclusion,further analysis shows the model has learned both interpretable and disentangled representations.
P19-1599,2019,8 Conclusion,the proposed approaches do not rely on a test-time parser or tagger and outperform our baselines.
P19-1599,2019,8 Conclusion,we also proposed a variational model accompanied with a neural component and multiple multi-task training objectives for addressing this task.
P19-1599,2019,8 Conclusion,"we proposed a novel setting for controlled text generation, which does not require prior knowledge of all the values the control variable might take on."
P19-1600,2019,6 Conclusion and Future Work,"in the future, we will explore the representation for the implicit information like whether a man is retired or not or how long a sportsman’s career is given starting and ending years, in the table by including some inference strategies."
P19-1600,2019,6 Conclusion and Future Work,"richness-oriented reinforcement learning is proposed to cover more informative contents in source tables, which help the generator to generate informative and accurate descriptions."
P19-1600,2019,6 Conclusion and Future Work,the experiments on the wikibio and wb-filter datasets show the merits of our model.
P19-1600,2019,6 Conclusion and Future Work,"to achieve these goals, we propose force-attention method, which encourages the generator to pay more attention to previous uncovered attributes to avoid potential key attribute missing."
P19-1600,2019,6 Conclusion and Future Work,"we set up 3 goals for comprehensive description generation for attribute-value factual tables: accurate, informative and loyal."
P19-1601,2019,5 Conclusions and Future W,"especially, because our proposed approach doesn’t assume a disentangled latent representation for manipulating the sentence style, our model can get better content preservation on both of two datasets."
P19-1601,2019,5 Conclusions and Future W,experimental results on two text style transfer datasets have shown that our model achieved a competitive or better performance compared to previous state-of-the-art approaches.
P19-1601,2019,5 Conclusions and Future W,how to combine the back-translation with our training algorithm is also a good research direction that is worth to explore.
P19-1601,2019,5 Conclusions and Future W,"in the future, we are planning to adapt our style transformer to the multiple-attribute setting like lample et al.(2019)."
P19-1601,2019,5 Conclusions and Future W,"in this paper, we proposed the style transformer with a novel training algorithm for text style transfer task."
P19-1601,2019,5 Conclusions and Future W,"on the other hand, the backtranslation technique developed in lample et al.(2019) can also be adapted to the training process of style transformer."
P19-1602,2019,5 Conclusion,experiments show that dss-vae outperforms the vae baseline in reconstruction and unconditioned language generation.
P19-1602,2019,5 Conclusion,"in both experiments, dss-vae achieves promising results."
P19-1602,2019,5 Conclusion,"in this paper, we propose a novel dss-vae model, which explicitly models syntax in the distributed latent space of vae and enjoys the benefits of sampling and manipulation in terms of the syntax of a sentence."
P19-1602,2019,5 Conclusion,"we further make use of the sampling and manipulation advantages of dss-vae in two novel applications, namely unsupervised paraphrase and syntax-transfer generation."
P19-1603,2019,6 Conclusion and Future Work,experiments show the effectiveness of the proposed framework to control the sentiment intensity on both automatic evaluation and human evaluation.
P19-1603,2019,6 Conclusion and Future Work,"future work can combine the analyzer and generator via joint training, hopefully to achieve better results."
P19-1603,2019,6 Conclusion and Future Work,"in this paper, we make the first endeavor to control the fine-grained sentiment for story ending generation."
P19-1603,2019,6 Conclusion and Future Work,"the proposed framework is generic and novel, and does not need any human annotation of story dataset."
P19-1604,2019,5 Conclusions and Future Work,"furthermore, a qualitative assessment indicated improvements in terms of relevance and answerability."
P19-1604,2019,5 Conclusions and Future Work,"the best performance is obtained when using the three mechanisms altogether, reaching an improvement of almost one bleu4 point (and of 0.5 for rouge-l) over the current state-of-the-art approaches."
P19-1604,2019,5 Conclusions and Future Work,"the results obtained show the contribution of auxiliary techniques such as copying mechanism, placeholding, and contextualized embeddings, which complement each other."
P19-1604,2019,5 Conclusions and Future Work,"we are extending the proposed approach to other qa datasets, and adapting it to use pretrained language models such as bert (devlin et al., 2018), to evaluate the consistency of the mechanisms introduced."
P19-1604,2019,5 Conclusions and Future Work,we have described a preliminary study on the adaptation of transformer architectures to neural question generation.
P19-1605,2019,6 Conclusion,access to parallel data is therefore still advantageous for paraphrase generation and our monolingual method can be a helpful resource for languages where such data is not available.
P19-1605,2019,6 Conclusion,bilingual settings considered supervised and unsupervised translation.
P19-1605,2019,6 Conclusion,monolingual settings considered autoencoders trained on unlabeled text and introduced continuous residual connections for discrete autoencoders.
P19-1605,2019,6 Conclusion,"overall, we showed that monolingual models can outperform bilingual ones for paraphrase identification and data-augmentation through paraphrasing."
P19-1605,2019,6 Conclusion,this method is advantageous over both discrete and continuous auto-encoders.
P19-1605,2019,6 Conclusion,we also reported that generation quality from monolingual models can be higher than model based on unsupervised translation but not supervised translation.
P19-1605,2019,6 Conclusion,we compared neural paraphrasing with and without access to bilingual data.
P19-1606,2019,6 Conclusions,our main focus in this paper is instilling structure learnt from fsms in neural models for sequential procedural text generation with multimodal data.
P19-1606,2019,6 Conclusions,the first model imposes structure on the decoder and the second model imposes structure on the loss function by modeling it as a hierarchical multi-task learning problem.
P19-1606,2019,6 Conclusions,we gather a dataset of 16k recipes where each step has text and associated images.
P19-1606,2019,6 Conclusions,we plan on exploring backpropable variants as a scaffold for structure and also extend the techniques to other how-to domains in future.
P19-1606,2019,6 Conclusions,we plan to explore explicit evaluation of the latent structure learnt.
P19-1606,2019,6 Conclusions,we propose two ways of imposing structure from phases and states of a recipe derived from fsm.
P19-1606,2019,6 Conclusions,we setup a baseline inspired from the best performing model in vist.
P19-1606,2019,6 Conclusions,we show that our proposed approach improves upon the baseline and achieves a meteor score of 0.31.
P19-1607,2019,5 Conclusion,experimental results on english text simplification and formality transfer indicated that the proposed method consistently improved the quality of paraphrase generation for both rnn and san models across styles or domains.
P19-1607,2019,5 Conclusion,our proposed method deleted complex or informal words appearing in source sentences and promoted the addition of simple or formal words to paraphrased sentences.
P19-1607,2019,5 Conclusion,"to improve the conservative rewriting of the paraphrase generation model, we proposed the identification of words to be paraphrased and the addition of negative lexical constraints on beam search."
P19-1608,2019,6 Conclusion,further work on these inductive biases could help understand how a pretrained transfer learning model can be adapted in the most optimal fashion to a given target task.
P19-1608,2019,6 Conclusion,"in this work, we have presented various ways in which large-scale pretrained language models can be adapted to natural language generation tasks, comparing single-input and multi-input solutions."
P19-1608,2019,6 Conclusion,"this comparison sheds some light on the characteristic features of different types of contextual inputs, and our results indicate that the various architectures we presented have different inductive bias with regards to the type of input context."
P19-1610,2019,6 Conclusion,"as post-processing is required to remove semantically dissimilar paraphrased questions, there is scope for developing better techniques for semantic similarity scoring."
P19-1610,2019,6 Conclusion,"in addition, we have only considered paraphrasing the question in this paper."
P19-1610,2019,6 Conclusion,"in this paper, we propose a novel approach to train a neural paraphrasing network to paraphrase questions utilizing paraphrase suggestions."
P19-1610,2019,6 Conclusion,our experiments highlight the need for separate adversarial testing and the importance of improving the robustness of qa models to question paraphrasing for better reliability when tested on future unseen test questions.
P19-1610,2019,6 Conclusion,"paraphrasing the context is another area to explore but poses significant technical challenge, since it requires altering words over multiple sentences while still retaining the original meaning of the context."
P19-1610,2019,6 Conclusion,there are several possible future directions stemming from this work.
P19-1610,2019,6 Conclusion,"there is also scope for better techniques to generate more coherent question paraphrasing when significant question re-writing is required, such as for the situation when we want to paraphrase the question using context words."
P19-1610,2019,6 Conclusion,we also create an adversarial paraphrased test set to test models’ reliance on string matching.
P19-1610,2019,6 Conclusion,"we also show that a completely automatic approach to augment the training data can improve the robustness of the qa models to the paraphrased questions, while still retaining performance on the original questions."
P19-1610,2019,6 Conclusion,we show that all three state-of-the-art qa models give poorer performance on the first test set and drastically reduced performance on the second test set.
P19-1610,2019,6 Conclusion,we use the approach to construct a test set of paraphrased squad questions containing questions similar to the original to test models’ robustness to question paraphrasing.
P19-1611,2019,6 Conclusion,"altogether, rankqa provides a new, strong baseline for future research on neural qa."
P19-1611,2019,6 Conclusion,here answer re-ranking is responsible for bolstering the overall performance considerably: our rankqa represents the state-of-the-art system for 3 out of 4 datasets.
P19-1611,2019,6 Conclusion,our experiments confirm the effectiveness of a three-stage architecture in neural qa.
P19-1611,2019,6 Conclusion,this performance was even rendered possible with a light-weight architecture that allows for the efficient fusion of information retrieval and machine comprehension features during training.
P19-1611,2019,6 Conclusion,"when comparing it to corresponding two-staged architecture, answer re-ranking can be credited with an average performance improvement of 4.9 percentage points."
P19-1612,2019,11 Conclusion,"experiments show that learning to retrieve is crucial when the questions reflect an information need, i.e.the question writers do not already know the answer."
P19-1612,2019,11 Conclusion,this is made possible by pre-training the retriever using an inverse cloze task (ict).
P19-1612,2019,11 Conclusion,"we presented orqa, the first open domain question answering system where the retriever and reader are jointly learned end-to-end using only question-answer pairs and without any ir system."
P19-1613,2019,5 Conclusion,"decomprc achieved the state-of-the-art on hotpotqa distractor setting and full wiki setting, while providing explainable evidence for its decision making in the form of sub-questions and being more robust to adversarial settings than strong baselines."
P19-1613,2019,5 Conclusion,"moreover, decomprc achieved further gains from the decomposition scoring step."
P19-1613,2019,5 Conclusion,"we proposed decomprc, a system for multihop rc that decomposes a multi-hop question into simpler, single-hop sub-questions."
P19-1613,2019,5 Conclusion,"we recasted sub-question generation as a span prediction problem, allowing the model to be trained on 400 labeled examples to generate high quality sub-questions."
P19-1614,2019,5 Conclusion,at other times a more involved knowledge about actions and properties is needed.
P19-1614,2019,5 Conclusion,automatic extraction of the needed commonsense knowledge is a major obstacle in solving the winograd schema challenge.
P19-1614,2019,5 Conclusion,it is a general approach may be applied to other commonsense reasoning tasks which require the both the knowledge embedded in the pre-trained language models and more involved knowledge about actions and properties.
P19-1614,2019,5 Conclusion,"so, in this work we utilized the knowledge embedded in the pretrained language models and developed a technique to automatically extract the more involved commonsense knowledge from text repositories."
P19-1614,2019,5 Conclusion,the experimental results show that the combined approach possesses the benefits of both the approaches and achieves the state-ofthe-art accuracy on the wsc.
P19-1614,2019,5 Conclusion,then we defined an approach to combine the two kinds of knowledge in a probabilistic soft logic based framework to solve the winograd schema challenge (wsc).
P19-1614,2019,5 Conclusion,this work presents an approach to combine the ideas of knowledge hunting and language modeling to perform commonsense reasoning.
P19-1614,2019,5 Conclusion,we observed that sometimes the needed knowledge can be retrieved from the pre-trained neural language models.
P19-1615,2019,6 Conclusion,"all techniques are shown to improve on the performance of the final task of qa, but there is still a long way to reach human performance."
P19-1615,2019,6 Conclusion,"for the natural language abduction, the heuristic technique performs better than the supervised techniques."
P19-1615,2019,6 Conclusion,"in this work, we have pushed the current state of the art for the openbookqa task using simple techniques and careful selection of knowledge."
P19-1615,2019,6 Conclusion,"nevertheless, our overall system improves on the state of the art by 11.6%."
P19-1615,2019,6 Conclusion,"our analysis also shows the limitations of bert based mcq models, the challenge of learning natural language abductive inference and the multiple types of reasoning required for an openbookqa task."
P19-1615,2019,6 Conclusion,we analyzed the performance of various components of our qa system.
P19-1615,2019,6 Conclusion,we have provided two new ways of performing knowledge extraction over a knowledge base for qa and evaluated three ways to perform abductive inference over natural language.
P19-1616,2019,7 Conclusion,"in this paper, we discuss unseen relation detection in kbqa, where the main problem lies in the learning of representations."
P19-1616,2019,7 Conclusion,"similar problems may exist in other nlp tasks, which will be interesting to investigate in the future."
P19-1616,2019,7 Conclusion,"we emphasize that for any other tasks which contain a large number of unseen samples, training, fine-tuning the model according to the performance on the seen samples alone is not fair."
P19-1616,2019,7 Conclusion,"we re-organize the simplequestion dataset as simplequestionbalance to reveal and evaluate the problem, and propose an adapter which significantly improves the results."
P19-1617,2019,5 Conclusion,"besides, our analysis shows dfgn can produce reliable and explainable reasoning chains."
P19-1617,2019,5 Conclusion,"different from previous approaches in qa, dfgn is capable of predicting the sub-graphs dynamically at each reasoning step, and the entity-level reasoning is fused with token-level contexts."
P19-1617,2019,5 Conclusion,"in the future, we may incorporate new advances in building entity graphs from texts, and solve more difficult reasoning problems, e.g.the cases of comparison query type in hotpotqa."
P19-1617,2019,5 Conclusion,"specifically, we propose a dynamic fusion reasoning block based on graph neural networks."
P19-1617,2019,5 Conclusion,we evaluate dfgn on hotpotqa and achieve leading results.
P19-1617,2019,5 Conclusion,we introduce dynamically fused graph network (dfgn) to address multi-hop reasoning.
P19-1618,2019,6 Discussion and Future Work,"additionally, it would be interesting to study the behavior of nlprolog in the presence of multiple wikihop query predicates."
P19-1618,2019,6 Discussion and Future Work,"for instance, a prover for temporal logic (orgun and ma, 1994) would allow to model temporal dynamics in natural language."
P19-1618,2019,6 Discussion and Future Work,"to this end, we proposed to combine a symbolic prover with pretrained sentence embeddings, and to train the resulting system using backpropagation."
P19-1618,2019,6 Discussion and Future Work,"we are also interested in incorporating future improvements of symbolic provers, triple extraction systems and pretrained sentence representations to further enhance the performance of nlprolog."
P19-1618,2019,6 Discussion and Future Work,"we evaluated nlprolog on two different qa tasks, showing that it can learn domainspecific rules and produce predictions which outperform those of the two strong baselines bidaf and fastqa in most cases."
P19-1618,2019,6 Discussion and Future Work,"we proposed nlprolog, a system that is able to perform rule-based reasoning on natural language, and can learn domain-specific rules from data."
P19-1618,2019,6 Discussion and Future Work,"while we focused on a subset of first order logic in this work, the expressiveness of nlprolog could be extended by incorporating a different symbolic prover."
P19-1619,2019,5 Conclusion,"in this paper, we introduce a group attention method which can reinforce the capacity of model to grab various types of mwps specific features."
P19-1619,2019,5 Conclusion,"plus, our ablation study demonstrates the effectiveness of each group attention mechanism."
P19-1619,2019,5 Conclusion,"we conduct experiments on two benchmarks and show significant improvements over a collection of competitive baselines, verifying the value of our model."
P19-1620,2019,5 Conclusion,"we additionally proposed a possible direction for formal grounding of this method, which we hope to develop more thoroughly in future work."
P19-1620,2019,5 Conclusion,we presented a novel method to generate synthetic qa instances and demonstrated improvements from this data on squad2 and on nq.
P19-1621,2019,5 Discussion,"however, data augmentation has its limitations: it may add new biases to data, and it cannot cover all the different implications or ways of writing questions."
P19-1621,2019,5 Discussion,"ideally, we want models to be able to reason that “what color is the rose?"
P19-1621,2019,5 Discussion,"ideally, we want models to be able to reason that “what color is the rose? red” implies “is the rose red?"
P19-1621,2019,5 Discussion,"the results of this approach are promising: consistency evaluation reveals gaps in models, and augmenting training data produces models that are more consistent even in human generated implications."
P19-1621,2019,5 Discussion,"to support such endeavours, generated implications for vqa and squad, along with the code to generate them and for evaluating consistency of models, is available at https://github.com/marcotcr/qa consistency."
P19-1621,2019,5 Discussion,"we argued that evaluation of qa systems should take into account the relationship between predictions rather than each prediction in isolation, and proposed a rule-based implication generator which we validated in crowdsourcing experiments."
P19-1621,2019,5 Discussion,"we hope that our work persuades others to consider the importance of consistency, and initiates a body of work in qa models that achieve real understanding by design."
P19-1621,2019,5 Discussion,"yes” without needing to add every possible implication or rephrasing of every (q, a) to the training data."
P19-1622,2019,4 Conclusion,"by integrating cnn with rnn, fusing 1d and 2d convolutions, extending causal convolution to 2d, our model achieves the best results among published models on the coqa dataset without fine-tuning bert."
P19-1622,2019,4 Conclusion,"in this paper, we introduce multi-perspective convolutional cube (mc2 ), a novel model for conversational machine reading comprehension."
P19-1622,2019,4 Conclusion,the cube is viewed from different perspectives to fully understand the history of conversation.
P19-1622,2019,4 Conclusion,we will study further the capability of our approaches on other datasets and tasks in the future work.
P19-1623,2019,5 Conclusion,contrastive examples are automatically constructed by omitting words from the groundtruth translations.
P19-1623,2019,5 Conclusion,experiments show that our approach significantly reduces omission errors and improves translation performance on three language pairs.
P19-1623,2019,5 Conclusion,our approach is model-agnostic and can be applied to arbitrary nmt models.
P19-1623,2019,5 Conclusion,we have presented contrastive learning for reducing word omission errors in neural machine translation.
P19-1624,2019,5 Conclusion,experimental results on wmt14 benchmarks show that exploiting sentential context improves performances over the state-of-theart transformer model.
P19-1624,2019,5 Conclusion,"in this work, we propose to exploit sentential context for neural machine translation."
P19-1624,2019,5 Conclusion,linguistic analyses reveal that the proposed approach indeed captures more linguistic information as expected.
P19-1624,2019,5 Conclusion,"specifically, the shallow and the deep strategies exploit the top encoder layer and all the encoder layers, respectively."
P19-1625,2019,5 Conclusion,all data and code are made available at https://blablablab.si.umich.edu/projects/naija/.
P19-1625,2019,5 Conclusion,"however, as no one factor was sufficient for predicting code switching, our results point to the need for holistically modeling the social context when examining factors influence code-switching behavior."
P19-1625,2019,5 Conclusion,"notably, we find that topical modulation has the largest effect on switching to naija, with use of emotion surpassing the ef- ′ fect for a few topics."
P19-1625,2019,5 Conclusion,"this work provides the first computational examination of code switching behavior in naija through ′ introducing a large corpora of articles in naija′ and nigerian standard english, along with comments to these articles."
P19-1625,2019,5 Conclusion,"through examining code switching in an intersectional social context, our analysis provides evidence of complementary social factors influencing switching."
P19-1625,2019,5 Conclusion,we develop new methods for distinguishing these two languages and identify over 24k instances of code switching in the comments.
P19-1626,2019,6 Conclusion,3 future work includes the fusion of additional operations in neural models.
P19-1626,2019,6 Conclusion,both implementations outperform different parallel baselines.
P19-1626,2019,6 Conclusion,"in this work, we introduce two parallel methods for sparse computations found in nmt."
P19-1626,2019,6 Conclusion,matrix operations form the largest bottleneck in deep learning.
P19-1626,2019,6 Conclusion,"the first operation is the sparse multiplication found in the input layer, and the second one is a fused softmax and top-n."
P19-1626,2019,6 Conclusion,"the fusion of these three operations requires a different implementation of the matrix multiplication, and shared memory usage."
P19-1626,2019,6 Conclusion,the last affine transformation in deep neural models can be fused with our softmax and top-n methods.
P19-1626,2019,6 Conclusion,"we obtained speedups of up to 7× for the sparse affine transformation, and 50× for the fused softmax and top-n task."
P19-1627,2019,5 Conclusion,"at the same time, new models, allowing for a consistent handling of multi-state characters and a direct handling of partial cognates, could be added to our fast bayesian phylogenetic inference approach."
P19-1627,2019,5 Conclusion,"both methods are not only very fast, but – as our tests show – also quite accurate in their performance, when compared to similar, much slower, algorithms proposed in the past."
P19-1627,2019,5 Conclusion,both methods can be further improved in multiple ways.
P19-1627,2019,5 Conclusion,"here, we believe that the new framework can provide considerable help to future research, specifically also, because it does not not require the technical support of high-end clusters."
P19-1627,2019,5 Conclusion,"in combination, the methods can be used to assess preliminary phylogenies from linguistic datasets of more than 100 languages in less than half an hour on an ordinary single core machine."
P19-1627,2019,5 Conclusion,in this paper we proposed an automated framework for very fast and still highly reliable phylogenetic reconstruction in historical linguistics.
P19-1627,2019,5 Conclusion,our cognate detection method’s weak performance on south-east asian languages could be addressed by enabling it to detect partial cognates instead of complete cognates.
P19-1627,2019,5 Conclusion,our framework introduces two new methods.
P19-1627,2019,5 Conclusion,our methods are best used for the purpose of exploratory analysis on larger datasets which have so far not yet been thoroughly studied.
P19-1627,2019,5 Conclusion,the bipskip approach uses bipartite networks of soundclass-based skip-grams for the task of automatic cognate detection.
P19-1627,2019,5 Conclusion,the maple approach makes use of simulated annealing technique to infer a map tree for linguistic evolution.
P19-1627,2019,5 Conclusion,"we are well aware that our framework is by no means perfect, and that it should be used with a certain amount of care."
P19-1628,2019,6 Conclusions,experimental results on three news summarization datasets demonstrated the superiority of our approach against strong baselines.
P19-1628,2019,6 Conclusions,"in the future, we would like to investigate whether some of the ideas introduced in this paper can improve the performance of supervised systems as well as sentence selection in multi-document summarization."
P19-1628,2019,6 Conclusions,"in this paper, we developed an unsupervised summarization system which has very modest data requirements and is portable across different types of summaries, domains, or languages."
P19-1628,2019,6 Conclusions,we employed bert to better capture sentence similarity and built graphs with directed edges arguing that the contribution of any two nodes to their respective centrality is influenced by their relative position in a document.
P19-1628,2019,6 Conclusions,we revisited a popular graph-based ranking algorithm and refined how node (aka sentence) centrality is computed.
P19-1629,2019,8 Conclusions,"experimental results on two benchmarks show that our parser is able to obtain reasonably accurate sentence- and document-level discourse representation structures (77.85 and 66.56 exact-f1, respectively)."
P19-1629,2019,8 Conclusions,"in the future, we would like to more faithfully capture the semantics of documents by explicitly modeling entities and their linking."
P19-1629,2019,8 Conclusions,in this work we proposed a novel semantic parsing task to obtain discourse representation tree structures and introduced a general framework for parsing texts of arbitrary length and granularity.
P19-1630,2019,6 Conclusions,"an interesting challenge, and open research question, concerns the extent to which synthetic training impacts the overall model generalizability."
P19-1630,2019,6 Conclusions,document structure was shown to be particularly useful for long documents.
P19-1630,2019,6 Conclusions,evaluation further showed that models trained on synthetic data generalize to natural test documents.
P19-1630,2019,6 Conclusions,"our models induce latent document structure, to identify aspect-relevant segments of the input document."
P19-1630,2019,6 Conclusions,sizable datasets of documents paired with aspect-specific summaries do not exist and are expensive to create.
P19-1630,2019,6 Conclusions,"the aspects considered in this work, as well as the creation process of synthetic data by interleaving documents which are maximally distinct with respect to the target aspects leave room for refinement."
P19-1630,2019,6 Conclusions,the latent document structure is induced jointly with the summarization objective.
P19-1630,2019,6 Conclusions,"this paper presented the task of aspect-based summarization, where a system summarizes a document with respect to a given input aspect of interest."
P19-1630,2019,6 Conclusions,treating document structure as latent allows for efficient training with no need for subdocument level topic annotations.
P19-1630,2019,6 Conclusions,"we believe that training models on heuristic, but inexpensive data sets is a valuable approach which opens up exciting opportunities for future research."
P19-1630,2019,6 Conclusions,we demonstrated the benefit of document structure aware models for summarization through a diverse set of evaluations.
P19-1630,2019,6 Conclusions,"we introduced neural models for abstractive, aspect-driven document summarization."
P19-1630,2019,6 Conclusions,"we proposed a scalable synthetic training setup, adapting an existing summarization data set to our task."
P19-1631,2019,6 Conclusion and Future Work,"additionally, we show that model can be also forced to focus on pre-determined tokens."
P19-1631,2019,6 Conclusion and Future Work,"another avenue is to incorporate different model attribution strategies such as deeplrp (bach et al., 2015) into the objective function."
P19-1631,2019,6 Conclusion and Future Work,applying model attribution as a fine-tuning step on a trained classifier makes it converge to a more debiased classifier in just a few epochs.
P19-1631,2019,6 Conclusion and Future Work,"finally, it would be worthwhile to invest in a technique to extract problematic terms from the model automatically rather than providing prescribed identity or toxic terms."
P19-1631,2019,6 Conclusion and Future Work,"in this paper, we proposed actionability on model explanations that enable ml practitioners to enforce priors on their model."
P19-1631,2019,6 Conclusion and Future Work,"our experiments indicate that the models trained jointly with cross-entropy and prior loss do not suffer a performance drop on the original task, while achieving a better performance in fairness metrics on the template-based dataset."
P19-1631,2019,6 Conclusion and Future Work,our method incorporates path integrated gradients attributions into the objective function with the aim of stopping the classifier from carrying along false positive bias from the data by punishing it when it focuses on identity words.
P19-1631,2019,6 Conclusion and Future Work,our technique can be applied to implement a more robust model by penalizing the attributions falling outside of tokens annotated to be relevant to the predicted class.
P19-1631,2019,6 Conclusion and Future Work,there are several avenues we can explore as future research.
P19-1631,2019,6 Conclusion and Future Work,we apply this technique to model fairness in toxic comment classification.
P19-1632,2019,6 Conclusion,"in the experiments, our proposed approaches significantly outperformed an extensive range of state-of-theart schemes, including both term-based and deepmodel-based text matching algorithms."
P19-1632,2019,6 Conclusion,results suggest that the proposed graphical decomposition and the structural transformation by gcn layers are critical to the performance improvement in matching article pairs.
P19-1632,2019,6 Conclusion,"we created two new datasets for long document matching with the help of professional editors, consisting of about 60k pairs of news articles, on which we have performed extensive evaluations."
P19-1632,2019,6 Conclusion,"we propose the concept interaction graph to organize documents into a graph of concepts, and introduce a divide-and-conquer approach to matching a pair of articles based on graphical decomposition and convolutional aggregation."
P19-1633,2019,5 Conclusions and Future Work,"as future work, we will investigate approaches to hyperparameter tuning to find better model architectures for hierarchical multi-label text classification tasks."
P19-1633,2019,5 Conclusions and Future Work,"finally, we show improvement over the state of the art multi-label model by using optimized class weights obtained when training the binary classifiers."
P19-1633,2019,5 Conclusions and Future Work,"furthermore, we show that binary classifiers greatly outperform multi-label models."
P19-1633,2019,5 Conclusions and Future Work,"in this work, we propose htrans, a hierarchical transfer learning-based strategy to train binary classifiers for categories in a taxonomy."
P19-1633,2019,5 Conclusions and Future Work,our approach relies on re-using model parameters trained at upper levels in the taxonomy and fine-tuning them for classifying categories at lower levels.
P19-1633,2019,5 Conclusions and Future Work,our experiments on the rcv1 dataset show that classifiers of categories with less training examples benefit using pre-trained model parameters from upper level categories.
P19-1634,2019,6 Conclusion,another important take-away message is that model authors in authorship verification need to be extra careful about their feature selection.
P19-1634,2019,6 Conclusion,"as long as much larger corpora remain out of reach for lack of a sufficient source of monographs, extra care needs to be taken in preparing the data, as exemplified for our corpus."
P19-1634,2019,6 Conclusion,"finally, in a spin-off study on unmasking, we generalized the algorithm to work on short, essaylength texts (bevendorff et al., 2019): it achieves an accuracy of 0.73, an f1 of 0.69, and a precision of 0.82, marking the first baseline for our corpus."
P19-1634,2019,6 Conclusion,"fortunately, this will come naturally to researchers in the field as they are already trained to avoid features that encode topic rather than style."
P19-1634,2019,6 Conclusion,"in particular, we strongly suggest that future evaluations should adopt a stateless one-case-at-a-time test policy."
P19-1634,2019,6 Conclusion,"in shared tasks, sometimes basic approaches outperform more sophisticated ones."
P19-1634,2019,6 Conclusion,"in the case of authorship verification as per pan, this was a major part of the problem."
P19-1634,2019,6 Conclusion,inadvertent properties of the data act as confounders that a learning algorithm will gladly fit onto if they are not controlled.
P19-1634,2019,6 Conclusion,this is frequently the case when machine learning meets small data.
P19-1635,2019,6 Conclusion,an experiment on an application scenario of exaggerated numeral detection suggests the capability of the proposed nn models.
P19-1635,2019,6 Conclusion,"in future work, we plan to extend our work to further applications such as detecting exaggerated statements by investors in social media data."
P19-1635,2019,6 Conclusion,the experimental results show that nn models can learn the proper range for a target numeral from contextual information.
P19-1635,2019,6 Conclusion,"we present a novel task of learning numeracy with the numeracy-600k,2 including the market comments and the ariticle titles."
P19-1636,2019,6 Limitations and Future Work,"finally, we plan to investigate generalized zero-shot learning (liu et al., 2018)."
P19-1636,2019,6 Limitations and Future Work,"furthermore, experimenting with more datasets e.g., rcv1, amazon-13k, wiki-30k, mimic-iii will allow us to confirm our conclusions in different domains."
P19-1636,2019,6 Limitations and Future Work,"moreover, rnn (and gru) based methods have high computational cost, especially for long documents."
P19-1636,2019,6 Limitations and Future Work,"one major limitation of the investigated methods is that they are unsuitable for extreme multi-label text classification where there are hundreds of thousands of labels (liu et al., 2017; zhang et al., 2018; wydmuch et al., 2018), as opposed to the lmtc setting of our work where the labels are in the order of thousands."
P19-1636,2019,6 Limitations and Future Work,we also plan to experiment with hierarchical flavors of bert to surpass its length limitations.
P19-1636,2019,6 Limitations and Future Work,we leave the investigation of methods for extremely large label sets for future work.
P19-1636,2019,6 Limitations and Future Work,"we plan to investigate more computationally efficient methods, e.g., dilated cnns (kalchbrenner et al., 2017) and transformers (vaswani et al., 2017; dai et al., 2019)."
P19-1637,2019,6 Conclusion,"an important question for future models, especially interactive ones, is how to signal to the user when their desires do not comport with reality."
P19-1637,2019,6 Conclusion,"in such cases, control may not be a desired property of interactive systems."
P19-1637,2019,6 Conclusion,"informed prior models provide an effective way to incorporate different feedback into topic models, improving user control and topic coherence, while constraints yield higher quality topics, but with less control."
P19-1637,2019,6 Conclusion,interactive models—by design—are balancing user insight with the truth of the data (and thus the world).
P19-1637,2019,6 Conclusion,"while we simulate user behavior for good and random users, future work should compare these systems with end users, as well as compare end user ratings of control with our proposed automated metrics."
P19-1638,2019,5 Conclusions and Future work,"future work can investigate other embedding methods with a richer set of probe tasks, or explore a wider range of downstream tasks."
P19-1638,2019,5 Conclusions and Future work,"in this paper, we evaluate a state-of-the-art paragraph embedding model, based on how well it captures the sentence identity within a paragraph."
P19-1638,2019,5 Conclusions and Future work,"our results indicate that the model is not fully aware of this basic property, and that implementing a simple objective to fix this issue improves classification performance, training speed, and generalization ability."
P19-1639,2019,4 Conclusion,"as the motivating task is clir, we evaluated the ranking effectiveness of our proposed architecture."
P19-1639,2019,4 Conclusion,"in the future, we hope to learn models that are able to sample search queries or information needs from sentences and use the output of that model to get relevant documents."
P19-1639,2019,4 Conclusion,one big challenge in this landscape is to sample meaningful queries from sentences as sentences do not directly convey information need.
P19-1639,2019,4 Conclusion,we present a multi-task learning architecture to learn nmt for search query translation.
P19-1639,2019,4 Conclusion,we used sentences from the target side of the parallel corpus as queries to retrieve relevant document and use terms from those documents to train a word embedding model along with nmt.
P19-1640,2019,7 Conclusion and Future Work,a future direction is to improve the gan-based training of w-lda.
P19-1640,2019,7 Conclusion and Future Work,another future direction is to experiment with more complex priors than the dirichlet prior.
P19-1640,2019,7 Conclusion and Future Work,"for example, the nested chinese restaurant process can be used as a nonparametric prior to induce hierarchical topic models (griffiths et al., 2004)."
P19-1640,2019,7 Conclusion and Future Work,the w-lda framework that we have proposed offers the flexibility of matching more sophisticated prior distributions via mmd or gan.
P19-1640,2019,7 Conclusion and Future Work,"to measure topic diversity, we have proposed a topic uniqueness measure in addition to the widely used npmi for coherence."
P19-1640,2019,7 Conclusion and Future Work,"unlike existing neural network based models, w-lda can directly enforce dirichlet prior, which plays a central role in the sparse mixed membership model of lda."
P19-1640,2019,7 Conclusion and Future Work,"we believe these discoveries are of independent interest to the broader research on mmd, gan and wae."
P19-1640,2019,7 Conclusion and Future Work,"we further make two novel discoveries: first, mmd out-performs gan in matching high dimensional dirichlet distributions; second, carefully adding noise to the encoder output can significantly boost topic coherence without harming diversity."
P19-1640,2019,7 Conclusion and Future Work,"we have proposed w-lda, a neural network based topic model."
P19-1640,2019,7 Conclusion and Future Work,we report significant improvement of topic quality in both coherence and diversity over existing topic models.
P19-1640,2019,7 Conclusion and Future Work,"while we were not successful in training wlda using the gan-based method, we acknowledge that many new formulations of gan have been proposed to overcome mode collapse and vanishing gradient such as (arjovsky et al., 2017; gulrajani et al., 2017)."
P19-1641,2019,5 Conclusion,"in this paper, we propose a framework for procedure extraction and captioning modeling in instructional videos."
P19-1641,2019,5 Conclusion,our model use narrated transcripts of each video as the supplementary information and can help to predict and caption procedures better.
P19-1641,2019,5 Conclusion,"the extensive experiments demonstrate that our model achieves state-of-the-art results on the youcookii dataset, and ablation studies indicate the effectiveness of utilizing transcripts."
P19-1642,2019,5 Conclusions and Future work,"importantly, our models compare favourably to the state-of-theart."
P19-1642,2019,5 Conclusions and Future work,"in future work we will explore other generative models for multi-modal mt, as well as different ways to directly incorporate images into these models."
P19-1642,2019,5 Conclusions and Future work,"we also show that in the absence of enough data to train a more complex inference network a simple fixed prior suffices, whereas when more training data is available (even noisy data) a conditional prior is preferable."
P19-1642,2019,5 Conclusions and Future work,"we are also interested in modelling different views of the image, such as global vs. local image features, and also in using larger image collections and modelling images directly, i.e.pixel intensities."
P19-1642,2019,5 Conclusions and Future work,we have proposed a latent variable model for multimodal neural machine translation and have shown benefits from both modelling images and promoting use of latent space.
P19-1643,2019,7 Conclusion,a distinctive aspect of this work is that we label actions in videos based on the language that accompanies the video.
P19-1643,2019,7 Conclusion,"in future work, we plan to explore additional representations and architectures to improve the accuracy of our model, and to identify finer-grained alignments between visual actions and their verbal descriptions."
P19-1643,2019,7 Conclusion,"in this paper, we address the task of identifying human actions visible in online videos."
P19-1643,2019,7 Conclusion,the dataset and the code introduced in this paper are publicly available at http://lit.eecs.umich.edu/downloads.html.
P19-1643,2019,7 Conclusion,"this has the potential to create a large repository of visual depictions of actions, with minimal human intervention, covering a wide spectrum of actions that typically occur in everyday life."
P19-1643,2019,7 Conclusion,"we describe and evaluate several text-based and video-based baselines, and introduce a multimodal neural model that leverages visual and linguistic information as well as additional information available in the input data."
P19-1643,2019,7 Conclusion,"we focus on the genre of lifestyle vlogs, and construct a new dataset consisting of 1,268 miniclips and 14,769 actions out of which 4,340 have been labeled as visible."
P19-1643,2019,7 Conclusion,we show that the multimodal model outperforms the use of one modality at a time.
P19-1644,2019,8 Conclusion,"our analysis shows that the language contains a wide range of linguistic phenomena including numerical expressions, quantifiers, coreference, and negation."
P19-1644,2019,8 Conclusion,"our focus on visually complex, natural photographs and human-written captions aims to reflect the challenges of compositional visual reasoning better than existing corpora."
P19-1644,2019,8 Conclusion,procedures for evaluating on the unreleased test set and a leaderboard are available at http://lic.nlp.cornell.edu/nlvr/.
P19-1644,2019,8 Conclusion,these results and our analysis exemplify the challenge that nlvr2 introduces to methods for visual reasoning.
P19-1644,2019,8 Conclusion,this demonstrates how our focus on complex visual stimuli and data collection procedure result in compositional and diverse language.
P19-1644,2019,8 Conclusion,"we experiment with baseline approaches and several methods for visual reasoning, which result in relatively low performance on nlvr2."
P19-1644,2019,8 Conclusion,we introduce the nlvr2 corpus for studying semantically-rich joint reasoning about photographs and natural language captions.
P19-1644,2019,8 Conclusion,"we release training, development, and public test sets, and provide scripts to break down performance on the 800 examples we manually analyzed (section 4) according to the analysis categories."
P19-1645,2019,9 Conclusion,"the development of these categories likely interact with the development of the lexicon and acquisition of semantics (feldman et al., 2013; fourtassi and dupoux, 2014), and thus subsequent work should seek to unify more aspects of the acquisition problem."
P19-1645,2019,9 Conclusion,"while this work unifies several components that had previously been studied in isolation, our model assumes access to phonetic categories."
P19-1645,2019,9 Conclusion,"while work studying the problem in isolation has provided valuable insights (showing both what data is sufficient for word discovery with which models), this paper shows that neural models offer the flexibility and performance to productively study the various facets of the problem in a more unified model."
P19-1645,2019,9 Conclusion,word discovery is a fundamental problem in language acquisition.
P19-1646,2019,8 Conclusion,in this paper we propose a model for goal-oriented visual question generation using two different approaches that leverage information gain with reinforcement learning.
P19-1646,2019,8 Conclusion,our algorithm achieves improved accuracy and qualitative results in comparison to existing state-of-the-art models on the guesswhat?!dataset.
P19-1646,2019,8 Conclusion,our results indicate that rig is a more promising approach to build betterperforming agents capable of displaying strategy and coherence in an end-to-end architecture for visual dialogue.
P19-1646,2019,8 Conclusion,we also discuss the innovative aspects of our model and how performance could be increased.
P19-1647,2019,6 Conclusion,"for one, it makes it infeasible to carry out an apples-to-apples comparison with a pipeline architecture."
P19-1647,2019,6 Conclusion,going forward we would like to go beyond matching tasks and evaluate the impact of an explicit speech-to-text decoder as an auxiliary task.
P19-1647,2019,6 Conclusion,"limitations and future work our current model does not include an explicit speech-to-text decoder, which limits the types of analyses we can perform."
P19-1647,2019,6 Conclusion,this would be especially interesting given that one motivation for a visually-supervised end-to-end approach is the un-availability of large amounts of transcribed speech in certain circumstances.
P19-1647,2019,6 Conclusion,via controlled experiments and analyses we show evidence that this is due to the role of inductive bias on the learned encoder representations.
P19-1647,2019,6 Conclusion,we are also planning to investigate how sensitive our approach is to amount of data for the auxiliary task.
P19-1647,2019,6 Conclusion,we show that the speech/text task leads to substantial performance improvements when compared to training the speech/image task in isolation.
P19-1648,2019,5 Conclusion,experiments on the visdial dataset validate the effectiveness of the proposed approach.
P19-1648,2019,5 Conclusion,"this iterative reasoning process enables model to achieve a finegrained understanding of multimodal context, thus boosting question answering performance over state-of-the-art methods."
P19-1648,2019,5 Conclusion,"we have presented recurrent dual attention network (redan), a new multimodal framework for visual dialog, by incorporating image and dialog history context via a recurrently-updated query vector for multi-step reasoning."
P19-1649,2019,5 Conclusions,"in general, the lattice transformer can increase the metric bleu for translation tasks by a significant margin over many baselines."
P19-1649,2019,5 Conclusions,"in this paper, we propose a novel lattice transformer architecture with a controllable lattice attention mechanism that can consume a word lattice and probability scores from the asr system."
P19-1649,2019,5 Conclusions,the proposed approach is naturally applied to both the encoder self-attention and encoder-decoder attention.
P19-1649,2019,5 Conclusions,"we mainly validate our lattice transformer on speech translation task, and additionally demonstrate its generalization to text translation on the wmt 2017 chinese-english translation task."
P19-1650,2019,5 Conclusion,"additionally, our model learns to estimate entity and object label coverage, which can be used at inference time to further boost the generated caption’s informativeness without hurting its fluency."
P19-1650,2019,5 Conclusion,"our human evaluations validate that training a model against ground-truth captions containing fine-grained labels (but without the additional help for fine-grained label identification), leads to models that produce captions of inferior quality."
P19-1650,2019,5 Conclusion,"the results indicate that the best configuration is one in which fine-grained labels are precomputed by upstream models, and handled by the captioning model as types, with additional significant benefits gained by boosting the coverage of the finegrained labels via a coverage control mechanism."
P19-1650,2019,5 Conclusion,"we present an image captioning model that combines image features with fine-grained entities and object labels, and learns to produce fluent and informative image captions."
P19-1651,2019,7 Conclusion,"in this paper, we introduce codraw: a collaborative task designed to facilitate learning of effective natural language communication in a grounded context."
P19-1651,2019,7 Conclusion,long-term planning and contextual reasoning as two key challenges for this task that our models only begin to address.
P19-1651,2019,7 Conclusion,the models we present in this paper show levels of task performance that are still far from what humans can achieve.
P19-1651,2019,7 Conclusion,"the task combines language, perception, and actions while permitting automated goaldriven evaluation both at the end and as a measure of intermediate progress."
P19-1651,2019,7 Conclusion,"we hope that the grounded, goal-driven communication setting that codraw is a testbed for can lead to future progress in building agents that can speak more naturally and better maintain coherency over a long dialog, while being grounded in perception and actions."
P19-1651,2019,7 Conclusion,"we introduce a dataset and models for this task, and propose a crosstalk training + evaluation protocol that is more generally applicable to studying emergent communication."
P19-1652,2019,5 Conclusion and Future Work,a novel two-step approach has been proposed to construct the vocabulary considering both visual information and relations among words.
P19-1652,2019,5 Conclusion and Future Work,"experiments on two public datasets, namely, ms coco and flickr30k, show that image-grounded vocabulary is able to enhance the quality of image captions compared to existing state-of-the-art approaches."
P19-1652,2019,5 Conclusion and Future Work,"furthermore, it is also interesting to design a mutual reinforcement mechanisms between the vocabulary constructor and the text generator to improve both components simultaneously."
P19-1652,2019,5 Conclusion and Future Work,"in future, we plan to study more effective ways to construct the image-grounded vocabulary."
P19-1652,2019,5 Conclusion and Future Work,"in this paper, we have proposed a novel framework which constructs an image-grounded vocabulary to leverage the image semantics for image captioning in order to tackle the problem of generating irrelevant n-grams."
P19-1652,2019,5 Conclusion and Future Work,reinforcement learning has been adopted for the training of the generator to encourage it to only choose words from the image-grounded vocabulary.
P19-1652,2019,5 Conclusion and Future Work,two strategies have then been explored to utilize the constructed vocabulary via hard constraints and soft constraints.
P19-1653,2019,6 Conclusions,"adding visual information, and in particular structural representations of this information, proved beneficial when input text contains noise and the language pair requires substantial restructuring from source to target."
P19-1653,2019,6 Conclusions,our code and pre-processing scripts are available at https:// github.com/imperialnlp/mmt-delib.
P19-1653,2019,6 Conclusions,our findings suggest that the combination of a deliberation approach and information from additional modalities is a promising direction for machine translation that is robust to noisy input.
P19-1653,2019,6 Conclusions,our results show that further exploring textual context through deliberation networks already leads to better results than the previous state of the art.
P19-1653,2019,6 Conclusions,"we have proposed a novel approach to multimodal machine translation which makes better use of context, both textual and visual."
P19-1654,2019,5 Conclusions,"further, the metric relies on off-the shelf object detectors and word-embeddings and computes the scores in a semantic space."
P19-1654,2019,5 Conclusions,"our analysis on two of the most widely used datasets for metric comparison shows that our metric correlates well with human judgments, and is particularly well suited when few or no reference description is available."
P19-1654,2019,5 Conclusions,our hypothesis is that the use of image information provides a more reliable pathway for measuring the fidelity of a description for a given image.
P19-1654,2019,5 Conclusions,the metric performs comparatively for gold and predicted annotations on objects and is lightweight in terms of dependency on linguistic resources.
P19-1654,2019,5 Conclusions,we have introduced a new metric for image description evaluation that goes beyond comparing descriptions to human references and is explicitly based on object-level image information.
P19-1655,2019,6 Discussion,"for example, if an instruction says turn left at the couch, and the route structure only allows the agent to turn left at a single location, it may not need to perceive the couch."
P19-1655,2019,6 Discussion,"notably, an agent in the r2r environment is only able to move to a discrete set of locations in the environment, and at each point in time it only has a small number of actions available, determined by the environment’s connectivity graph (i.e., moving to the adjacent locations)."
P19-1655,2019,6 Discussion,"other instructions, such as go straight for 5 meters and stop may also be carried out without access to visual perception."
P19-1655,2019,6 Discussion,"our intuition is that, while language has rich, high-level symbolic meaning, which can be easily matched to the modality of the route structures, pixel-based visual representations, even those extracted via cnns, are a lowerlevel modality which require more data to learn, and so a model trained on both modalities may learn to mostly rely on the route structure."
P19-1655,2019,6 Discussion,"the “rn+obj” model (table 2, line 8) has access to the same information as our best result in table 3, line 25, but obtains much lower success rate (39.5% vs. 51.9%)."
P19-1655,2019,6 Discussion,the improvement of our mixture-of-experts approach over single models suggests that it is challenging to learn to ground language into multiple modalities in one model.
P19-1655,2019,6 Discussion,"the success of non-visual versions of two recent state-of-the-art vln models, often outperforming their vision-based counterparts in unseen environments on the benchmark r2r dataset, shows that these models do not use the visual inputs in a generalizable way."
P19-1655,2019,6 Discussion,these constraints on possible routes help explain our findings that language in the vln instructions often grounds into geometric route structure in addition to visual context along the route.
P19-1655,2019,6 Discussion,"this is also supported by the results in table 3 (line 23 vs. line 20), where adding higher-level object representations improves the success rate by 2.6%."
P19-1655,2019,6 Discussion,"thus, splitting the prediction task across several models, where each has access to a different input modality, is an effective way to inject an inductive bias that encourages the model to ground into each of the modalities."
P19-1656,2019,5 Discussion,"at the heart of mult is the crossmodal attention mechanism, which provides a latent crossmodal adaptation that fuses multimodal information by directly attending to low-level features in other modalities."
P19-1656,2019,5 Discussion,"empirically, we show that mult exhibits the best performance when compared to prior methods."
P19-1656,2019,5 Discussion,"in the paper, we propose multimodal transformer (mult) for analyzing human multimodal language."
P19-1656,2019,5 Discussion,"we believe the results of mult on unaligned human multimodal language sequences suggest many exciting possibilities for its future applications (e.g., visual question answering tasks, where the input signals is a mixture of static and time-evolving signals)."
P19-1656,2019,5 Discussion,"we hope the emergence of mult could encourage further explorations on tasks where alignment used to be considered necessary, but where crossmodal attention might be an equally (if not more) competitive alternative."
P19-1656,2019,5 Discussion,"whereas prior approaches focused primarily on the aligned multimodal streams, mult serves as a strong baseline capable of capturing long-range contingencies, regardless of the alignment assumption."
P19-1657,2019,6 Conclusion,"extensive quantitative and qualitative experiments demonstrated that the proposed cmas not only could generate meaningful and fluent reports, but also could accurately describe the detected abnormalities."
P19-1657,2019,6 Conclusion,"in this paper, we proposed a novel framework for accurately generating chest x-ray imaging reports by exploiting the structure information in the reports."
P19-1657,2019,6 Conclusion,the entire system was trained with reinforce algorithm.
P19-1657,2019,6 Conclusion,"we explicitly modeled the between-section structure by a two-stage framework, and implicitly captured the within-section structure with a novel co-operative multi-agent system (cmas) comprising three agents: planner (pl), abnormality writer (aw) and normality writer (nw)."
P19-1658,2019,6 Conclusion,"vist-edit, the first dataset for human edits of machine-generated visual stories, is introduced."
P19-1658,2019,6 Conclusion,"we argue that human editing on machinegenerated stories is unavoidable, and such edited data can be leveraged to enable automatic postediting."
P19-1658,2019,6 Conclusion,"we have established baselines for the task of visual story post-editing, and have motivated the need for a new automatic evaluation metric."
P19-1659,2019,6 Conclusions,"in the future, we would like to extend this work to generate multidocument (multi-video) summaries and also build end-to-end models directly from audio in the video instead of text-based output from pretrained asr."
P19-1659,2019,6 Conclusions,our presented models include a video-only summarization model that performs competitively with a text-only model.
P19-1659,2019,6 Conclusions,"we define and show the quality of a new metric, content f1, for evaluation of the video summaries that are designed as teasers or highlights for viewers, instead of a condensed version of the input like traditional text summaries."
P19-1659,2019,6 Conclusions,we present several baseline models for generating abstractive text summaries for the open-domain videos in how2 data.
P19-1660,2019,6 Conclusion,"an orthogonal line of future work might involve using a visual question answering (vqa) task (such as in krishna et al.(2017)), either on its own replacing the captioning task, or in conjunction with the captioning task with a multi-task learning objective."
P19-1660,2019,6 Conclusion,"in this work, we propose a novel task of weaklysupervised relation prediction, with the objective of detecting relations between entities in an image purely from captions and object-level bounding box annotations without class information."
P19-1660,2019,6 Conclusion,"one possible line of work involves removing the requirement of ground truth bounding boxes altogether by leveraging a recent line of work that does weakly-supervised object detection (such as (oquab et al., 2015; bilen and vedaldi, 2016; zhang et al., 2018; bai and liu, 2017; arun et al., 2018))."
P19-1660,2019,6 Conclusion,our proposed approaches thus allow weakly-supervised relation detection.
P19-1660,2019,6 Conclusion,"our proposed method builds upon top-down attention (anderson et al., 2018), which generates captions and grounds word in these captions to entities in images."
P19-1660,2019,6 Conclusion,there are several interesting avenues for future work.
P19-1660,2019,6 Conclusion,this would reduce the amount of supervision required even further.
P19-1660,2019,6 Conclusion,"we leverage this along with structure found from the captions by the stanford scene graph parser (schuster et al., 2015) to allow for the classification of relations between pairs of objects without having ground truth information for the task."
