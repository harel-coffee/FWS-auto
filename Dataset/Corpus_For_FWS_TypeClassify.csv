id,label,text
D00-1302,1,we hope that other learning algorithms can benefit from the ideas presented here and that the idea of learning rres can be generalized to allow other learners to incorporate more powerful features as well.
D00-1302,1,we plan to use argumentative zoning as a first step for ir and shallow document understanding tasks like summarization.
D00-1303,1,"of course, we have to be careful in introducing such constraints, and they should be learned from existing corpus."
D00-1303,1,integration with other simple models.
D00-1303,1,error-driven data selection.
D00-1303,1,"then, by analyzing held-out training data and selecting the features that affect the parsing accuracy."
D00-1303,2,we can use the system to output some redundant parsing results and use only those results for the positive and negative examples.
D00-1303,3,we can start with a small size of training data with a small size of feature set.
D00-1303,1,"for future research, to reduce the computational overhead, we will work on methods for sample selection as follows: introduction of constraints on nondependency."
D00-1304,3,we also intend to investigate the effects that other decision tree growth and smoothing techniques may have on continued refinement of the converted rule list.
D00-1304,3,"future research will include testing the behavior of the system under adaboost (freund and schapire, 1997)."
D00-1306,1,the current approach uses uncertainty-based evaluation functions; we hope to consider other factors such as confidence about the parameters of the grammars and domain knowledge.
D00-1306,1,we also plan to focus on the constituent units within a sentence as training examples.
D00-1306,3,"first, since the ideas behind the proposed evaluation fimctions are general and independent of formalisms, we would like to empirically determine their effect on other parsers."
D00-1306,3,"next, we shall explore alternative formulations of evaluation functions for the single-learner system."
D00-1306,3,another area of interest is to experiment with committee-based sample selection using multiple learners.
D00-1306,4,"finally, we are interested in applying sample selection to other natural language learning algorithms that have been limited by the sparsity of annotated data."
D00-1307,1,"For future work, we plan to use derivation trees to train LTAG parsers directly and use LexTract to add semantic information to the Penn Treebank."
D00-1308,5,in the future we hope to explore automatically discovering information sources that can be profitably incorporated into maximum entropy part-of-speech prediction.
D00-1309,1,"for future work, we will explore the effectivessness of considering even more contextual information on approximation of p(t~""ig ~) by using the forward-backward algorithm (rabiner 1989) while currently we only consider the contextual information of current location and previous location."
D00-1312,1,we would like to extend the model to allow phrase generation in the query generation process.
D00-1312,5,future improvements in cross-lingual ir will come by attacking the incompleteness of bilingual dictionaries and by improved query expansion and context-dependent translation.
D00-1312,5,we also wish to explore techniques to extend bilingual lexicons.
D00-1313,1,we are currently exploring an algorithm to generate the matrices more efficiently and the selection of coefficients in formula (6) also needs further research.
D00-1314,2,we should extend our method to the large corpus of other domains without lost much accuracy.
D00-1320,1,"although it has been very useful to work with the bbn model, we are currently implementing and hope to augment a more state-of-the-art model, vzz., models 2 and 3 of (collins, 1997)."
D00-1320,1,"we would also like to explore the use of a more radical model, where nonterminals only have synsets as their heads, and words are generated strictly at the leaves."
D00-1320,1,"we would also like to incorporate long-distance context in the model as an aid to wsd, a demonstrably effective feature in virtually all the recent, statistical wsd work."
D00-1320,1,"also, as mentioned earlier, we believe there are several features that would allow significant parsing improvement."
D00-1320,1,"finally, we would like to investigate the incorporation of unsupervised methods for wsd, such as the heuristically-based methods of (stetina and nagao, 1997) and (stetina et al., 1998), and the theoretically purer bootstrapping method of (yarowsky, 1995)."
D00-1320,1,"bolstered by the success of (stetina and nagao, 1997), (lin, 1997) and especially (stetina et al., 1998), we believe there is great promise the incorporation of word-sense into a probabilistic parsing model."
D00-1321,5,"first, studies on recovery mechanisms for unsafe segmentation before parsing seem necessary since ungafe segmentation may cause parsing failures."
D00-1321,5,"second, parsing control mechanisms should be studied that exploit the characteristics of segmentation positions and the parallelism among segments."
D00-1322,1,"since most of the knowledge learned from a domain is not useful when changing to a new domain, further investigation is needed on tuning strategies, specially on those using non-supervised algorithms."
D00-1322,1,"further work on web-based ne acquisition could take advantage of machine learning techniques as used for wrapper induction (kushmerick et al., 1997)."
D00-1322,3,extensively evaluate lazyboosting on the wsd task.
D00-1324,1,"another task for the near future is, as mentioned in the previous section, to add an ordering mechanism on binary conjuncts in order to ensure that the more restrictive node pairs are searched for first."
D00-1324,2,"first, the set of queries the tool can process needs to be extended to all queries allowed in the query language."
D00-1324,6,"further, the design of a graphical user-interface to enter the queries is planned, allowing to specify queries by drawing partial trees instead of typing in the expressions in the query language."
D00-1324,6,"finally, we also want to implement a web-based userinterface for the query tool."
D00-1325,1,"however, it seems from our experiments that it would be better to avoid hypothesis tests that make use of the unconditional distribution."
D00-1325,1,"one possibility is to put more effort into the estimation of pe, and to avoid use of the unconditional distribution for this."
D00-1325,2,further work on handling low frequency data in scf acquisition is warranted.
D00-1326,1,"we plan to devise ways to integrate genre/topic parameters into the word sense disambiguation models, and to apply them on a system to acquire training examples automatically."
D00-1326,3,"further work will focus on evaluating the separate weight of genre and topic in word sense disambiguation performance, and on studying the behavior of each particular word and features through genre and topic variations."
D00-1327,1,"if filtering based on relative frequencies still achieves better results, it would be worth investigating ways of handling the low frequency data for integration with this method."
D00-1327,1,"this will involve (i) defining the set of semantic verb classes across the lexicon, (ii) obtaining back-off estimates for each verb class, and (iii) implementing a method capable of automatically classifying verbs to semantic classes."
D00-1327,1,"while this approach proved satisfactory, our future work will include investigating ways of addressing the problem of polysemy better."
D00-1327,1,"in addition to refining the filtering method, our future work will focus on integrating this approach with large-scale scf acquisition."
D01-0502,1,"better understanding of methods for thresholding the probability distributions that the classifiers output, as well as principled ways to order them are also among the future directions of this research."
D01-0503,3,and then using the model for apractical application such as n-best list rescoring.
D01-0503,3,more work remains to be done to evaluate now well the semantic coherence measure improves the baseline language model.
D01-0503,3,this evaluation can be performed by incorporating the semantic coherence features into an exponential language model
D01-0504,5,the heart of the problem still seems to befinding the overall best translation for all thewords rather than advanced word sense disambiguation task of finding the right translationin a given context.
D01-0504,6,this is even more true forlow density languages.where less resources areavailable.
D01-0505,1,we expect that the alignment can be more accurate and efficient by combining the structural features with translation axicon in the future.
D01-0506,3,"finally, it would be interesting to compare the performance of the stacked generalization approach to other multiclassifier methods, such as boosting (schapire & singer, 2000)."
D01-0506,3,"furthermore, we would like to evaluate other classifiers in the role of the president."
D01-0506,1,"a larger variety of  classifiers is expected to lead the president to  more informed decisions, resulting in further  improvement of the filters performance."
D01-0506,1,"in particular, we are  interested in combining more classifiers, such as  decision trees (quinlan, 1993) and support  vector machines (drucker, et al.1999), within  the stacking framework."
D01-0508,1,"we hope to use query expansion, analogous to the way it was used with bining to improve information retrieval in(umemura and church.2000).to boost results further."
D01-0508,1,even without conducting full experiments examination of term weights can determine which features are important.
D01-0508,1,"also we believe that it might be better to compute term weights for individual words when possible, and so back off to the bin only when necessary to her words."
D01-0508,1,"in the future, we hope to test more binning features."
D01-0510,3,"as for using the slm as the language understanding component of a speech driven application, such as mipad, it would be interesting to evaluate the impact of incorporating the semantic constraints on the word-level accuracy of the system."
D01-0510,1,another possible research direction is to modify the framework such that it finds the most likely semantic parse given the acoustics -- thus treating the word sequence as a hidden variable.
D01-0511,5,we also plan to tackle the problem of noun compounds containing more than two terms.
D01-0511,1,in future we plan to train the algorithm to allow different levels for each noun in the compound.
D01-0511,1,"we also plan to compare the results to the tree cut algorithm reported in (li and abe, 1998), which allows different levels to be identified for different subtrees."
D01-0514,3,"finally, a lsa procedure for computing document specihc similarity values will be evaluated."
D01-0514,5,future work will focus on document specifica and the terminations algorithm.
D01-0514,1,wethe threshold selection method has to be modified.
D01-0514,1,"in terms of clustering, dynamic programming approaches (ponte and croft, 1997:utiyama and isahara, 2001, for example)will be examined."
D01-0515,2,"we will again start by investigating data, in particular the distribution of symbol tuples."
D01-0515,3,"however, we need to compare our system with other methods and the whole issue of evaluation of large scale language processing tools needs much more attention."
D01-0516,6,"examples are : 	mechanisms for processing these abbreviations, which tend to occur in informal text such as email, chat rooms, or customer service call records, are the subject of ongoing research in our project."
D01-0517,2,"in the future, we have to automatically construct dialog style tobi labeled corpus to verify our prediction model more thoroughly since current mbcnewsdb has biased tone labels because it is a reading corpus for broadcast newsscripts."
D01-0520,3,we are shus assessing the ability of our grammars to map between surface string and some sort of meaning representation which is exactly what grammar is generally assumed to do.
D01-0521,2,cross corpus experiments could reveal whether these clusters uncover generally applicable semantic categories for the parser's use.
D01-0521,4,"an area for future work is investigation of the degree to which such features apply across corpora, or, on the other hand, further tune the parser to the peculiarities of the wall street journal."
D01-0521,1,of particular interest are the automatic clustering of lexical cooccurrences used in charniak (1997) and magerman (1995).
D02-1003,1,"notice however that the chief advantage of decision lists over linear models is their compact size and understandability, and our techniques simultaneously improve those aspects; adding additional splits will almost certainly lead to larger models, not smaller."
D02-1003,1,"it would also be interesting to try more sophisticated smoothing techniques, such as those of yarowsky."
D02-1005,1,"finally, since the mixture model and its improvements performed well on two major tasks and several multilingual data sets, we believe that they can be productively applied to other related high-dimensionality lexical classification problems, including named-entity classification, topic classification, and lexical choice in machine translation."
D02-1006,1,we will investigate the effect of more elaborate feature selection schemes on the performance of different learning algorithms for wsd in future work.
D02-1008,4,"in addition, although we use ripper as the underlying learning algorithm in our coreference system, we expect that the techniques described in this paper can be used in conjunction with other learning algorithms."
D02-1008,6,we plan to explore this possibility in future work.
D02-1009,1,we expect to further pursue transformation models (and simpler variants that are easier to estimate) within this flexible finite state framework.
D02-1011,5,"hence, we believe that an important next step is the identification of features indicating whether sentences are on topic (which is a kind of coreference problem); we look forward to addressing this challenge in future work."
D02-1014,1,"we are currently working on a version of the lr parser for a subclass of tags, the tree insertion grammars (schabes and waters, 1995), for which efficient true lr parsers can be obtained."
D02-1017,1,"in future work, we hope to investigate other types of syntactic structures that may be used to identify semantically related terms, and other types of heuristics that can reveal specific semantic relationships."
D02-1019,1,"our ultimate goal, towards which this work is the first step, is to construct loss functions that take advantage of linguistic structures such as syntactic dependencies found through monolingual analysis of the sentences to be aligned."
D02-1019,1,in future work we will investigate loss functions that incorporate french and english parse tree information into the alignment decoding process.
D02-1020,5,we feel that the idea of creating explicit user models to guide the behaviour of interactive systems is likely to have applications in areas of nlp apart from translators’ tools.
D02-1020,1,other possibilities include virtually any application where a human and a machine communicate through a language rich interface.
D02-1024,1,"in particular, we plan to investigate semiautomatic methods for extracting ontological knowledge from existing webpages and databases."
D02-1024,4,"in our current work, we are focusing on expanding system coverage to other domains."
D02-1025,3,future work would include experiments using larger scale test collections in various domains.
D02-1026,3,"(iii) evaluating the manipulating data approach using automatically generating hierarchies(sanderson and croft, 1999)."
D02-1026,1,"future work includes (i) extracting features which discriminate between categories within the same top level category,"
D02-1026,1,"(ii) investigating other machine learning techniques to obtain further advantages in efficiency in the manipulating data approach, and"
D02-1029,2,this includes extending these experiments to an even larger corpus with the hope of establishing the cross over point for thesaurus extraction.
D02-1029,5,"finally, although wider machine learning research uses large ensembles, many nlp ensembles use only a handful of classifiers."
D02-1029,5,"we would like to further investigate the relationship between contextual complexity, data sparseness, noise and learner bias on very large corpora."
D02-1029,1,it would be very interesting to experiment with a large number of classifiers using bagging and boosting techniques on very large corpora.
D02-1030,5,"if web counts correlate reliable with smoothed counts, then this provides further evidence for our claim that the web can be used to overcome data sparseness."
D02-1030,1,"in future work, we plan to compare web counts for unseen bigrams with counts recreated using standard smoothing algorithms, such as similarity based smoothing (dagan et al., 1999) or class based smoothing (resnik, 1993)."
D02-1031,1,we are currently extending the almost parsing superarv lm to a full parser based lm.
D02-1032,4,other areas include the application of the proposed model to a wider variety of test corpora and to related tasks.
D02-1032,1,one area of future work is therefore to reduce the model size.
D02-1039,3,we are interested in examining different language pairs as the opportunity arises.
D02-1039,5,"in the future, we plan to explore this hypothesis in an actual translation system."
D02-1040,3,we are curious to investigate further cheap features and compare them to what could be obtained when taking domain or world knowledge into account.
D02-1041,2,an investigation of how to combine these sources of information is left for future research.
D03-1003,1,"we are developing maximum entropy models to more effectively combine the multiple information sources we have used in our experiments, and expect to report the results in the near future."
D03-1007,4,"in future work, we intend to apply the lessons learned here to the problem of frame element identification."
D03-1007,6,gildea and jurafsky have shown that improvements in identification can be had by more closely integrating the task with classification (they report an f-score of .719 using an integrated model).
D03-1007,1,initial results show that significant improvements can be had using techniques similar to those described above.
D03-1007,1,we are currently exploring a me approach which integrates these two tasks under a tagging framework.
D03-1008,1,we would expect a higher performance for the ccg-based system if the analyses in ccgbank resembled more closely those in propbank.
D03-1009,1,finding applications of these results is the most important direction for future research.
D03-1021,5,there are still many interesting problems in applying the neural network enhenced slm to real applications.
D03-1021,1,"in particular, if we use separate mapping matrices for word/nt/pos at different positions in the context, we may be able to learn very different representations of the same word/nt/pos."
D03-1021,1,"interpreting the word representations learned in this framework: for example, word clustering, context clustering, etc."
D03-1021,1,"among those, we think the following are of most of interest: speeding up the stochastic gradient descent algorithm for neural network training: since training the neural network models is very time-consuming, it is essential to speed up the training in order to carry out many more interesting experiments."
D03-1027,1,the combined approach may yield better results with a small number of labeled examples.
D03-1027,1,"in the future, it would be interesting to employ virtual examples with methods to use both labeled and unlabeled examples (e.g., (blum and mitchell, 1998; nigam et al., 1998; joachim's, 1999))."
D03-1027,1,we believe we can use prior knowledge on these tasks to create effective virtual examples.
D03-1027,5,"another interesting direction would be to develop methods to create virtual examples for the other tasks (e.g., named entity recognition, pos tagging, and parsing) in nlp."
D03-1028,3,future work will examine alternative approaches to evaluation.
D03-1028,3,"one possibility for a more liberal evaluation could be to use human evaluators with real information needs, as done by turney (2000)."
D03-1028,5,"future work should also go in the direction of generating (as opposed to extracting) keywords, by for example exploring potential knowledge provided by a thesaurus."
D03-1028,5,"this would hopefully lead to a better precision, while recall probably would be affected negatively; the importance of recall would then need to be reconsidered."
D03-1028,1,"another possibility would be to let several persons index each document, thus getting a larger set of acceptable terms to choose from."
D04-3201,5,"this trade-off between the complexity, accuracy and efficiency of a parsing model is an important area of future research."
D04-3203,1,we plan to add a beam search to explore the speed-accuracy tradeoff.
D04-3203,1,"improvements in the state representation are possible, particularly along the lines of linguistically-motivated treebank transformations, as in klein and manning (2003)."
D04-3203,1,adding a lexical component to the model is another extension we intend to investigate.
D04-3204,2,"in addition, we will give preference to multiwords that contain the target word when choosing the relatives."
D04-3204,3,"now that the monosemous corpus is available for all nouns, we would also like to test the system on the all-words task."
D04-3204,1,"finally, more sophisticated methods to acquire examples are now available, like exretriever (fernandez et al., 2004), and they could open the way to better examples and performance."
D04-3204,1,"for the future we also want to test the performance of more powerful machine learning methods, explore feature selection methods for each individual word, and more sophisticated ways to combine the examples from the web corpus with those of semcor or senseval."
D04-3205,5,"we hope to open the way to inferring implied, but not stated assertions and to benefit applications such as question answering, information retrieval, and summarization."
D04-3205,1,further work may refine extraction methods and further process the mined semantics to derive other relations such as entailment.
D04-3205,1,"another possibility would be to use more relaxed patterns when the part of speech confusion is not likely (e.g. ""eat” is a common verb which does not have a noun sense, and patterns need not protect against noun senses when testing such verbs).our approach can potentially be extended to multiword paths."
D04-3205,1,"one avenue would be to automatically learn or manually craft more patterns and to extend the pattern vocabulary (when developing the system, we have noticed that different registers and verb types require different patterns)."
D04-3205,1,there are several ways to improve the accuracy of the current algorithm and to detect relations between low frequency verb pairs.
D04-3206,2,in future work we also plan to find the valid contexts for entailment relations.
D04-3206,2,in future work we aim to improve the yield by increasing the size of the sample-corpus in a qualitative way.
D04-3206,6,"like (lin and pantel, 2001), learning the context for which entailment relations are valid is beyond the scope of this paper."
D04-3206,1,"in future work we aim to improve the yield by increasing the size of the sample-corpus in a qualitative way, as well as precision, using statistical methods such as supervised learning for better anchor set identification and cross-correlation between different pivots."
D04-3206,1,"we also plan to support noun phrases as input, in addition to verb phrases."
D04-3206,1,"finally, we would like to extend the learning task to discover the correct entailment direction between acquired templates, completing the knowledge required by practical applications."
D04-3208,1,"finally, we would like to suggest that bootstrapping can in the future be used in conjunction with other sentence or word alignment learning methods to provide better mining results."
D04-3209,1,"in future work we hope to determine how the individual qualitative differences of the two models (estimation methods, model structure, etc.)contribute to the observed differences in results."
D04-3209,1,we also plan to investigate using a conditional random field (crf) model.
D04-3209,1,"to improve results overall, we plan to explore features that combine multiple knowledge sources, as well as approaches that model recognition uncertainty in order to mitigate the effects of word errors."
D04-3210,4,"in the future, we plan to apply our model to new domains (e.g., broadcast news or scientific papers), to non-indo-european languages such as arabic and chinese, and to machine generated texts."
D04-3211,3,another area for future research involves the estimation of class probabilities.
D04-3211,4,further research is also required to determine how rfs generalize to new genres.
D04-3213,6,"we also hope to integrate some processing of adjunct roles, rather than limiting ourselves to the specified arguments."
D04-3215,2,we will extend the question corpus to other question types.
D04-3215,1,"we are also continuing to develop the super tagger, which we have demonstrated is central to efficient portable wide-coverage ccg parsing."
D04-3216,1,"finally, we are looking to incorporate the results of this model into a real system."
D04-3217,1,results from this study will be instrumental in shaping the future of the plot analysis system in story station and the expansion of the current system into a general purpose plot analysis system for other writing tasks.
D04-3219,3,we also plan to pursue better (automated) metrics for paraphrase evaluation.
D04-3219,1,"this will we hope, eventually allow us to address such issues as paraphrase identification for ir."
D04-3219,1,"although we have not attempted to address the issue of paraphrase identification here, we are currently exploring machine learning techniques, based in part on features of document structure and other linguistic features that should allow us to bootstrap initial alignments to develop more data."
D04-3219,1,we will be experimenting with more sophisticated decoder models designed to handle reordering and mappings to discontinuous elements.
D04-3219,1,"to exploit richer data sets, we will also seek to address the monotone limitation of our decoder that further limits the complexity of our paraphrase output."
D04-3219,1,"while the alternations our system produces are currently limited in character, the field of smt offers a host of possible enhancements—including reordering models—affording a natural path for future improvements."
D04-3220,1,"most importantly, more specific use could be made of scf information besides modeling its joint distribution with sense, for example conditioning on headwords of (perceived) arguments, especially particles and prepositions."
D04-3220,1,"one way this could be done would be to use a parser only to estimate the probability of the sequence of word tags (i.e., parts of speech) in the sentence, then to use a sense-specific lexicon to estimate the probability of finding the words under the tags."
D04-3220,1,"it may also be possible to improve parsing accuracy on verb phrases or other phrases, by simultaneously resolving word sense ambiguities, as attempted unsuccessfully by bikel (2000)."
D04-3220,1,"third, we could hope to get some improvement from changing our model structure to address the issue of double generation of words discussed in section 3."
D04-3220,1,"second, although we made some attempt at extracting the “underlying” scf of verbs by analyzing passive constructions separately, similar analysis of other types of movement such as relative clauses may also be useful."
D04-3221,1,we plan to extend this work both by refining our notion of attribute and by using more sophisticated patterns working off the output of a parser.
D04-3222,1,we also plan to explore annotation with more features from hpsg signs.
D04-3222,1,"in future work, we aim to explore the definition of new string kernels that are more suitable for this particular application and apply these ideas to penn treebank parse trees."
D04-3223,1,further improvements of efficiency of grafting are possible by applying zhou et al.’s (2003) technique of restricting feature selection in each step to the top ranked features from previous stages.
D04-3224,5,"of course, there is still much more analysis, hypothesizing, testing and extrapolation to be done."
D04-3224,1,a thorough study of the highest-entropy distributions should reveal new ways in which to use grammar transforms or develop features to reduce the entropy and increase parse accuracy.
D04-3224,1,"a closer look at the low-entropy distributions may reveal additional reductions in the size of the model, and, perhaps, a way to incorporate hard constraints without disturbing the more ambiguous parts of the model more suited to machine learning than human engineering."
D04-3227,1,"our future work is to explore alternatives such as the reranking work in (collins, 2002) and include more knowledge such as syntax information in rescoring the phrase translation pairs."
D04-3229,2,"finally, we would like to extend our work to slavic languages for which there are even fewer available resources than russian, such as belarusian, since this was the original motivation for undertaking the work in the first place."
D04-3229,1,"we are seeking the right way to operationalize this intuition in our system, bearing in mind that we want a sufficiently general algorithm to make the method portable to other languages, for which we assume we have neither the time nor the expertise to undertake knowledge-intensive work."
D04-3230,5,"for this challenge, mccallum proposes an interesting research avenue to explore (mccallum, 2003)."
D04-3230,5,there exist some phenomena which cannot be analyzed only with bi-gram features in japanese morphological analysis.
D04-3230,6,"however, the numbers of features and nodes in the lattice increase exponentially as longer contexts are captured."
D04-3230,1,"to deal with longer contexts, we need a practical feature selection which effectively trades between accuracy and efficiency."
D04-3230,1,"to improve accuracy, tri-gram or more general n-gram features would be useful."
D04-3230,1,crfs have capability of handling such features.
D04-3233,5,"while the reranking is very efficient in the classification phase, training a support vector reranking system is computationally very expensive."
D04-3235,5,are there situations for which we must expect a significance difference between the two decision rules?
D04-3235,5,can we come up with a training criterion tailored to the symbol error rate?
D04-3235,5,is that an experimental coincidence?
D04-3235,5,"we speculate that the two decision rules could always have similar performance if the error rates are small.2) ideally, the training criterion should be closely related to the error measure used in the decision rule."
D04-3235,5,is it possible to derive closed-form bayes decision rules (or suitable analytic approximations) for these error measures?
D04-3235,6,1) the error rates for the two decision rules are comparable.
D04-3235,6,"right now, we have used the training criteria that had been developed in the past and that had been (more or less) designed for the string error rate as error measure."
D04-3237,5,"another interesting research direction is to explore the usefulness of the map adaptation of maxent models for other problems among which we wish to include language modeling, part-of-speech tagging, parsing, machine translation, information extraction, text routing."
D04-3237,5,"as future work we plan to investigate the best way to blend increasing amounts of less-specific background training data with specific, indomain data for this and other problems."
D04-3238,2,"a direction that we plan to investigate is the adaptation of such a technique to the general purpose spelling correction, by using statistics from both query-logs and large office document collections."
D04-3239,4,"we would like to apply our method to other applications where instances are represented in a tree and their subtrees play an important role in classifications (e.g., parse re-ranking (collins and duffy, 2002) and information extraction)."
D04-3240,4,this suggests that useful task-tracking tools could be constructed based on automatic classifiers—a potentially important practical application.
D04-3240,5,"we showed that entity extraction and part of speech tagging improves classifier performance, but leave open the question of whether other types of linguistic analysis would be useful."
D04-3241,3,"another important prediction of the entropy rate principle remains to be evaluated in future work: for out-of-context sentences, there should be a correlation between sentence position and processing effort."
D04-3241,3,this prediction can be tested by obtaining reading times for sentences sampled from a corpus and read by experimental subjects in isolation.
D04-3242,5,it will be very interesting to see how our approach performs in a longer history than the trigram.
D04-3242,6,"since our current rf models uses kn smoothing exclusively in lower order probabilities, 3for the * -test, we used the standard sclite’s statistical system comparison program from nist with the option “maps we”, which means the test is the matched pairs sentence segment word error test.it may not be adequate when we apply it to higher order  -gram models."
D04-3242,1,one possible solution is to use rf models for lower order probabilities as well.
D04-3242,1,higher order rfs will be grown based on lower order rfs which can be recursively grown.
D04-3244,5,"second, as noted in section 5.2.1, the dimensionality reduction required for linguistic data may constrain the performance of the metric distance."
D04-3244,6,"the latter obviates the problem of dimensionality, while it restricts the usage to a situation where the kernel-based approach is available."
D04-3244,1,"to alleviate this problem, simultaneous dimensionality reduction and metric induction may be necessary, or the same idea in a kernel-based approach is worth considering."
D04-3244,1,"first, as we stated in section 4.3, the effect of a cluster weighted generalized metric must be investigated and optimal weighting must be induced."
D04-3245,1,as well as some improvements in the search algorithms to reduce the computational cost of finding a path in the word graph with the minimum edit cost.
D04-3245,1,"finally, the introduction of morpho-syntactic information or bilingual categories in finite-state transducers, are topics that leave an open door to future research."
D04-3246,2,we intend to investigate this problem for hebrew in the future.
D04-3246,1,"another example is the case of morphological disambiguation in languages with non-trivial morphology, which can be viewed as a pos tagging problem with a large number of tags on which structure can be imposed using the various morphological and morphosyntactic features that morphological analyzers produce."
D04-3248,2,we also notice that ner performance over the source language can be improved using bilingual knowledge.
D04-3248,1,we may need some manually-generated rules to fix this.
D04-3249,3,"as far as the evaluation of dre is concerned, for the moment we have tested its usefulness in the context of a wsd task, but we are going deeper, considering a pure tc framework."
D04-3251,1,more effort is required in order to better integrate the cluster-specific models.
D04-3251,1,strategy overlap analysis and refinement of local optimization criteria has the potential to improve overall performance under time constraints.
D04-3252,4,"an important aspect of textrank is that it does not require deep linguistic knowledge, nor domain or language specific annotated corpora, which makes it highly portable to other domains, genres, or languages."
D04-3253,6,the approach presented here is flexible and suggests promising avenues of further investigation.
D04-3254,2,"we also plan to expand our data sets to more texts, in order to investigate the presence and distribution of factoids, types of factoids and relations between factoids in summaries and summary collections."
D04-3254,2,we now plan to elicit the help of new annotators to increase our data pool.
D04-3254,5,another pressing line of investigation is reducing the cost of factoid analysis.
D04-3254,5,the flrst reason why this analysis is currently expensive is the need for large summary bases for consensus summaries.
D04-3254,1,"all in all, the use of factoid analysis and weighted factoid score, even though initially expensive to set up, provides a promising alternative which could well bring us closer to a solution to several problems in summarization evaluation."
D04-3255,4,we can apply this method to each submodel of machine translation.
D05-1001,1,"further work will include, in addition to extending the set of documents and testing the system with other collections, evaluating the improvement to be achieved by adding a proper noun resolution algorithm to guitar."
D05-1002,2,we also want to benefit from our experience with the czech data in order to create an english corpus annotated with information structure.
D05-1002,1,"issues for further research include, on the one hand, a deeper investigation of the topic-focus articulation in the prague dependency treebank of czech, by improving the feature set, considering also the distinction between contrastive and noncontrastive t items and, most importantly, by investigating how we can use the t/f annotation in pdt (and respectively our results) in order to detect the topic/focus partitioning of the whole sentence."
D05-1003,4,"this approach could be applied more broadly, to different nlp tasks, and also more deeply, going beyond the simple one-and-a-halfiteration procedure we present here."
D05-1003,4,we also intend to extend our method both to cross-document relation detection and to event detection.
D05-1005,1,this will result in a more generative and less extractive approach to summarization - indeed the case for generative approaches to summarization is more convincing when the input is noisy.
D05-1005,1,"in the future, we plan to consider other types of constituents, such as correcting errors in verb groups, and in the argument structure of verbs."
D05-1007,2,"in future work, we plan to run our experiments on other datasets when they become available to us."
D05-1007,2,"in particular, we want to experiment with multi-topic audio documents where we expect more marked advantages for windowing and alternative aggregation schemes like max and 3max."
D05-1007,2,"some named entities could have high semantic similarity with the text if they are frequently mentioned in the same contexts in the web corpus, but some names could be common to many contexts."
D05-1007,3,"a final direction for research is to conduct experiments with human subjects, to evaluate the degree to which filtered transcripts are better than unfiltered ones for tasks like browsing, gisting and searching audio clips."
D05-1007,4,we plan to explore ways to scale up other corpusbased semantic similarity measures to large terabyte corpora.
D05-1007,1,"we plan to explore more approaches to detecting semantic outliers, for example clustering or lexical chains (hirst and st-onge, 1997)."
D05-1007,1,another future direction will be to actually correct the errors instead of just filtering them out.
D05-1007,1,"the most promising direction is to combine our method with confidence measures that use internal information from the asr system (although the internal information is hard to obtain when using an asr as a black box, and it could be recognizer specific)."
D05-1007,1,to increase recall we can also identify named entities and not filter them out.
D05-1007,1,"a combination is likely to improve the performance, with the pmi-based measure contributing at the high-precision end and the internal asr measure contributing to the high-recall end of the spectrum."
D05-1007,1,"for example, we might look at the top n speech recognizer hypotheses (for a fairly large n like 1000) and choose the one that maximizes semantic cohesion."
D05-1009,2,we are also planning to investigate whether neuralign helps when the individual aligners are trained using more data.
D05-1009,3,we also intend to evaluate the effectiveness of our improved alignment approach in the context of machine translation and cross-language projection of resources.
D05-1009,3,we will do additional experiments to observe the effects of varying the size of the annotated data while learning neural nets.
D05-1009,1,"we will extend our combination approach to combine word alignment systems based on different models, and investigate the effectiveness of our technique on other language pairs."
D05-1013,1,"we intend to explore more complex features for resolving pronouns, and to incorporate these features into our current model."
D05-1013,1,"we also intend to explore more complex models for automatically extracting knowledge from data that can help with this task and applying this technique to a real application, such as summarization."
D05-1016,4,in the future ned can also be extended to other interesting domains like scientific literature to detect the emerge of new topics and interests.
D05-1016,5,the reason for this superior performance over other kernels needs to be investigated.
D05-1016,6,a classifier with rbf kernel with γ set to one exhibited the best performance.
D05-1016,1,engineering of better features is also a definite priority.
D05-1017,1,"furthermore, it may be very interesting to explore optimal combinations of intensional and extensional supervision, provided by the user in the forms of seed features and labeled examples."
D05-1017,1,future work is needed to investigate optimal procedures for collecting seed features and to find out whether additional seeds might still contribute to better performance.
D05-1019,4,we are now planning to apply our method to an evaluation of machine translation.
D05-1019,4,we believe that our method will also be useful for other natural language generation tasks.
D05-1020,2,"of course, using only few test cases (topics sets and collections) is a limitation of this current study, which we are going to address in our future research."
D05-1020,4,"our approach can be also used as an explorative tool in order to identify important relevance-indicating features, which can be later modeled analytically."
D05-1020,4,"we believe that our work and the ones referred in this paper may bring many of the achievements made in a more general area of classification and machine learning closer to the task of rank ordered information retrieval, thus making retrieval engines more helpful in reducing the information overload and meeting people’s needs."
D05-1021,1,future work will build on these simple structures to produce more powerful models of word and phrase movement in translation.
D05-1024,3,"finally, we will evaluate the improved alignments in the context of an end-to-end application, such as machine translation."
D05-1024,5,"whether alp improves the statistical alignment systems when they are trained on more data is an interesting research problem, which we plan to tackle in future."
D05-1026,1,"this is similar to boosting techniques (freund, 1995) which build sequentially classifiers that focus on examples wrongly classified by the preceding one."
D05-1026,1,one could also imagine to associate a probability to each training example and to use these probabilities to weight the random sampling.
D05-1026,1,several extensions of the learning algorithm itself are promising.
D05-1026,1,these probabilities would be updated after each epoch.
D05-1026,1,we are in particular interested in smarter ways to select different subsets from the large corpus at each epoch (instead of a random choice).
D05-1026,1,"one possibility would be to use active learning, i.e. focusing on examples that are most useful to decrease the perplexity."
D05-1027,5,"however, due to the lack of theoretical underpinnings, we are unable to prove that msr will always succeed."
D05-1028,1,our future work will specifically investigate how to combine information from multiple sources in salience modeling and how to apply the salience models in different early stages of processing.
D05-1029,1,our next efforts will focus on using reinforcement learning to automatically derive the error recovery policies.
D05-1030,1,"finally, it may also be useful to integrate the prosodic events directly into the pcfg, in addition to their use in reranking."
D05-1030,1,"one could combine ip and prosodic break features (so far explored separately), find new combinations of prosody and syntactic structure, and/or incorporate other prosodic events."
D05-1030,1,"in addition to assessing the impact of prosody in a fully automatic system, other avenues for future work include improving feature extraction."
D05-1033,1,"an important future direction lies in extending our model to the document-level and the assignment of rhetorical relations, thus going beyond the basic nucleus-satellite distinction."
D05-1033,1,"our results indicate that a modular approach to discourse processing (i.e., treating segmentation as separate from labelling) could increase performance."
D05-1033,1,"in the future, we plan to investigate how to combine our chunker with models like spade for improved prediction on both local and global levels."
D05-1034,4,"one direction of future research is to apply this technique to an incremental learning scenario, i.e., to incrementally build models using incoming data for adaptation, taking all previously available data as background corpus."
D05-1036,6,"it produces code that is slower than hand-crafted code but acceptably fast for our nlp research, where it has been extremely helpful."
D05-1036,1,"we hope it will facilitate emnlp research, just as fs toolkits have done for the fs case."
D05-1039,1,"to improve the performance on “what-is” questions, we could divide “what-is” questions into finer classes such as organization, location, disease, and general substance, and process them specifically."
D05-1039,2,"another possible improvement is to generalize using automatically derived word clusters, which provide semantic information."
D05-1041,5,we have already pointed out the need to improve the positive precision of the training examples.
D05-1041,1,"finally, our method can be improved by including attributes for the layout and authority of web pages."
D05-1041,1,one way may be to combine our similarity method with cui et al.’s centroids.
D05-1041,1,we also plan to study the effect of including more automatically acquired patterns and using more training target terms.
D05-1042,5,"an interesting question is how to integrate our component into a generation pipeline, using feedback from other components to guide collective content selection."
D05-1042,5,"ideally, we would like to express more complex relations between items."
D05-1042,5,"for instance, we may want to represent disjunctive constraints, such as “at least one of the defense players should be mentioned in the summary.”"
D05-1042,1,another promising approach is the combination of our automatically acquired cross-entity links with domain knowledge.
D05-1042,6,"for instance, we may want to represent disjunctive constraints, such as “at least one of the defense players should be mentioned in the summary.” such dependencies can be efficiently handled in a collective classification framework by using approximate probabilistic inference (taskar et al., 2002)."
D05-1042,1,"currently, we consider a limited set of contextual dependencies based on attribute similarity."
D05-1042,1,"in the future, we plan to explore how to integrate more refined discourse models in the content selection process."
D05-1045,1,the probability or the strength of an opinion expression may also play a useful role in encouraging or suppressing source extraction.
D05-1045,1,"for example, the fact that a coreferring noun phrase was marked as a source in one sentence could be a useful clue for extracting the source from another sentence."
D05-1045,1,"directions for future work include trying to increase recall by identifying relationships between opinions and sources that cross sentence boundaries, and relationships between multiple opinion expressions by the same source."
D05-1049,1,we believe that the methods employed in this work show much potential for improving the state-of-the-art in computational semantic inference.
D05-1051,5,the challenge lies in producing pseudowords that better model real words.
D05-1051,1,"however, the size of modern ir test collections dictates that future studies will need to rely more heavily on simulation."
D05-1051,1,"therefore, until such time that a significant manually disambiguated ir collection exists pseudowords remain an interesting way to explore the effects of ambiguity within a large collection."
D05-1053,1,we will do further work to ascertain the best method for quantifying “substantial change”.
D05-1053,1,we also intend to exploit the automatic ranking to obtain information on sense frequency distributions (rather than just predominant senses) given the genre as well as the domain of the text.
D05-1053,1,"we plan to combine this with local context, using collocates of neighbors in the thesaurus, for contextual wsd."
D05-1054,1,"so in the future work, we will be focusing more on recognizing abbreviated ons."
D05-1056,2,"as future work, experiments should be expanded to include additional entity types and other types of informal text, such as blogs and forum postings."
D05-1057,4,our methods also generalize well across languages since there are no language specific techniques employed.
D05-1057,4,"we can improve and apply our methods to other domains like switchboard data (godfrey , 1992)."
D05-1059,1,"a natural extension of this work is to replace the maximum entropy modeling, which was used as the local classifiers, with other machine learning algorithms."
D05-1059,1,support vector machines with appropriate kernels is a good candidate because they have good generalization performance as a single classifier.
D05-1059,1,"although svms do not output probabilities, the easiest-first method would be easily applied by considering the margins output by svms as the confidence of local classification."
D05-1061,3,"we also would like to test our approach on more standard test sets, and compare the performance with other systems."
D05-1061,4,"in the future we plan to try more flexible translation candidate selection methods, and apply them to other language pairs."
D05-1063,1,"our future plans are to overcome some of the limitations in this study, specifically using more than a single (although standard and very diverse) collection and study other experimental setups, such as document retrieval, text categorization, or topic detection and tracking."
D05-1064,2,"another area for future research is to investigate the use of unlabeled data within the approach, for example by making use of clusters derived from large amounts of unlabeled data (e.g., see miller (2004))."
D05-1064,4,"finally, future work may apply the models to nlp tasks other than parsing."
D05-1064,1,"future work may consider the use of hidden– value domains with mixed contents, such as a domain that contains 3 refinement–oriented lexical values and 3 clustering–oriented part–of–speech values."
D05-1064,1,these mixed values would allow the hidden– variable model to exploit interactions between clustering and refinement at the level of words and dependencies.
D05-1065,2,more data should be annotated to create a treebank of morphological analyses.
D05-1065,2,guidelines should be developed for the manual annotation of data in order to make it less dependent on the annotators intuitions.
D05-1065,1,"given such a treebank, the parser could be trained on labeled data or on a combination of labeled and unlabeled data, which presumably would further increase the parsing accuracy."
D05-1067,5,"given that automatic humor-recognition is a rather understudied problem, we believe that this is an important result, as it provides insights into potentially productive directions for future work."
D05-1067,5,we plan to address these aspects in future work.
D05-1067,1,"the flattened shape of the curves toward the end of the learning process suggests that rather than focusing on gathering more data, future work should concentrate on identifying more sophisticated humor-specific features, e.g. semantic oppositions, ambiguity, and others."
D05-1069,2,"secondly, we could try this approach on other language pairs, japanese-english, for example."
D05-1069,6,there are a number of future directions that we could investigate.
D05-1069,1,"in other words, when a chinese translation of an english sense is still ambiguous, we could try to collect sense examples using translation in a third language, japanese, for instance."
D05-1069,1,"thirdly, it would be interesting to try to tackle the problem of chinese wsd using sense examples built using english, the reverse process to the one described in this paper."
D05-1069,1,this is also a possible solution to the problem that ambiguity may be preserved between chinese and english.
D05-1069,1,"firstly, instead of using a bilingual dictionary to translate chinese text snippets back to english, we could use machine translation software."
D05-1071,2,"while our experiments have used the web corpus, our approach transfers readily to other large corpora; experimentation with other corpora is another topic for future work."
D05-1074,3,"further investigation is needed to look at data not clicked, which is a critical step to see whether the improvement on prediction accuracy of user preference will help the system serve the user better in a real system."
D05-1075,6,We plan to investigate these problems in the future.
D05-1078,4,work to extend this technique to propbank annotation is underway.
D05-1078,1,"since function labels describe dependence relations between the predicative head and its complements, whether they be arguments or adjuncts, this paper suggests that a left-corner parser and its probabilistic model, which are defined entirely on configurational criteria, can be used to produce a dependency output."
D05-1078,6,consequences of this observation will be explored in future work.
D05-1079,3,"thus, although test suites establish a controlled way of assessing textual entailment detection systems, the importance of being able to predict textual entailment in nlp might be better justified using task-based evaluation."
D05-1079,1,this can be achieved by incorporating them in qa or summarization systems.
D05-1080,1,"in the future, we plan to enhance the system to also deal with verbs, adjectives, and adverbs, as well as compound nouns."
D05-1084,1,another interesting research direction that our study suggests is the combination of syntactic and semantic models in co-training.
D05-1084,1,the next step would then be to construct a combined meta-model that describes the behavior of systems with both syntactic and semantic features.
D05-1084,1,"co-training can be sensibly applied only when conditional independence holds for the two target functions and the distribution (blum and mitchell, 1998), i.e. when it uses two independent views on the instance set."
D05-1086,3,"preliminary results suggest that sentence retrieval can be used to improve document retrieval, but we plan a more extensive investigation of evaluating document similarity and relevance based on sentence-level similarity."
D05-1086,1,"we used this model because it had been shown effective in document retrieval, and was easily incorporated in the query-likelihood framework, but we intend to explore more sophisticated translation models, and better alignment mechanisms."
D05-1087,1,"in the absence of methods that work well for a wide range of operating points, we need training procedures that can be made sensitive to rare cases depending on the particular demands of the application."
D05-1088,5,"the accuracy ratio for this latter task is already fairly acceptable (86.26%), but it still needs to be enhanced in order to guarantee an optimal detection of subordinating intensional contexts (recall examples 1-2)."
D05-1088,1,both lines of work will involve the exploration and use of word sense disambiguation techniques.
D05-1088,1,"further work on evita will be focused on two main areas: (1) improving the sense disambiguation of candidates to event nominals by experimenting with additional learning techniques, and"
D05-1088,1,(2) improving event classification.
D05-1090,4,"beyond new-information detection, the idea of tracking context with a surface means like the focus variable is worth exploring in other tasks, including summarization and question-answering."
D05-1090,4,"in addition, the work here suggests three directions for future work: adapt the features used here to some of the newer probabilistic formalisms, like conditional random fields."
D05-1090,5,"high precision is very difficult to obtain, and every point in precision costs too much in recall."
D05-1090,5,"further exploration is needed to determine whether linguistic knowledge will help, and whether state-of-the-art tools are powerful enough to improve performance."
D05-1090,1,try to identify all nominal references to canonical forms.
D05-1090,1,try full segmentation of the input documents rather than treat the sentences as a sequence.
D05-1091,1,"recent research (roth and yih, 2004) indicates that integrating entity recognition with relation extraction in a global model that captures the mutual influences between the two tasks can lead to significant improvements in accuracy."
D05-1091,1,a natural extension is to automatically extract both the entities and their relationships.
D05-1093,1,"we plan to optimize the blanc parameters for different criteria in addition to incorporating syntactic and semantic features (e.g. ngrams, word classes, part of speech)."
D05-1093,4,"in the future, we plan to investigate the stability and performance of blanc and also apply it to automatic summarization evaluation."
D05-1094,1,"our results suggest that off-the-shelf nlp tools will need not only to provide a single-best prediction, but also to be engineered so that they can easily communicate distributions over predictions to models for higher-level tasks."
D05-1095,6,this is the line of research that we intend to pursue in the near future.
D05-1095,1,"in other words, it’s not how many bi-phrases you have, it’s how good they are."
D05-1099,4,"a third possibility is to optimize the definition of the shallow-parse phrase types themselves, for use in other applications."
D05-1099,1,future work will also include continued exploration of possible features that can be of use for either shallow parsing models or context-free parsing models.
D05-1099,1,"in addition, we intend to investigate ways in which to encode approximations to context-free parser derived features that can be used within finite state models, thus perhaps preserving finite-state efficiency while capturing at least some of the accuracy gain that was observed in this paper."
D05-1099,1,we intend to explore including features from the context-free parser output in our perceptron model to improve shallow parsing accuracy.
D05-1099,1,another possibility is to look at improving context-free parsing accuracy.
D05-1100,5,"some preliminary investigation of this suggests that we see much higher gains when using generic features than these more specific ones, but we leave a thorough investigation of this to future work."
D05-1100,5,we are curious to know the extent to which a close analysis of the dependency errors made by the baseline parser can be corrected by the development of features tailored to addressing these problems.
D05-1100,1,"finally, we would like to use the spanish parser in an application such as machine translation."
D05-1100,1,"another avenue for future investigation is to try using a more sophisticated baseline model such as collins’ model 2, which incorporates both subcategorization and complement/adjunct information."
D05-1101,3,further experimental evaluation needs to be carried out on multilingual corpora in order to asses the practical impact of these findings.
D05-1103,3,extending our experiments to the question types that we have not yet assessed is an important next step.
D05-1103,3,"finally, we need to assess questions generated on word lists with different characteristics."
D05-1103,3,"in addition, we want to assess questions individually, evaluating their use of distractors."
D05-1103,5,"another is using other resources such as text collections to enable us to generate more questions per word, especially for the cloze questions."
D05-1103,5,"in addition, we are looking at ways to predict word knowledge using confidence ratings and morphological and semantic cohorts in situations where we cannot perform a standard assessment or cannot test all the vocabulary words we would like to."
D05-1103,5,one is the creation of new question types to test other aspects of word knowledge.
D05-1103,6,there are also a number of ongoing extensions to this project.
D05-1104,5,"we leave it to future work to explain why adaptation is much stronger in co-ordination: is co-ordination special because of extra constrains (i.e., some kind of expected contrast/comparison between co-ordinate sisters) or because of fewer constraints (i.e., both co-ordinate sisters have a similar grammatical role in the sentence)?"
D05-1105,1,in future work we intend to explore better-motivated evidence combination algorithms and to apply the approach to other nlp problems.
D05-1106,1,"finally, since we only require shallow syntactic analysis (in terms of np chunking), our approach might be well suited to be easily portable to other domains."
D05-1107,4,one is to explore ways of applying the proposed approach to other learning models.
D05-1107,5,"finally, we will investigate whether the proposed approach can be adapted to more complex tasks in which the output is not a class label but a structure (e.g.parsing)."
D05-1107,1,another is to compare against other methods of combining evidences from multiple learners.
D05-1108,3,"an important direction for future work lies in the assessment of more shallow syntactic information (i.e., chunks) which can be obtained more easily for new languages, and generally in the integration of more linguistic knowledge to guide projection."
D05-1108,1,"finally, we will incorporate into our projection approach automatic semantic role annotations for the source language and investigate the potential of the projected annotations for training semantic parsers for the target language."
D05-1109,2,post-processing can be used to reduce the training time and improve recognition accuracy by aiding generation of more training data once basic recognition capability is in place.
D05-1113,3,"in future, we will evaluate the effectiveness of the techniques developed in this paper for applications like machine translation."
D05-1113,4,we will also extend our approach to other types of mwes and to the mwes of other languages (work on hindi is in progress).
D05-1114,2,"in the future, we may extend our work by using more datasets to empirically evaluate this feature clustering algorithm."
D05-1114,4,"this semi-supervised feature clustering framework is quite general, which can be applied to other nlp tasks, for example, text categorization."
D05-1117,1,a much shorter experimental cycle will allow researchers to explore different techniques and receive immediate feedback on their effectiveness.
D05-1123,4,"it would be an interesting challenge to apply the cp framework further for other tasks, possibly with more practical flavor, such as comparing and detecting commonalities between commercial products and firms, identifying equivalencies and precedents in legal cases and so on."
D05-1124,5,"a broader area of investigation are other problems in language processing that can benefit from structured multilabel classification, e.g., ambiguities in language often result in multiple acceptable parses for sentences."
D05-1124,6,"in further work, the classification threshold should also be learned to achieve the desired balance between precision and recall."
D05-1124,1,it would also be useful to investigate methods for combining these models with standard sequential tagging models to get top performance on simple segmentations as well as on overlapping or non-contiguous ones.
D05-1124,1,it may be possible to extend the algorithms presented here to learn to distinguish all acceptable parses from unacceptable ones instead of just finding a single parse when many are valid.
D05-1125,3,"we plan next to further improve our system by evaluating a number of novel pattern classification techniques to increase accuracy and user-independence, and to introduce additional vocal characteristics (possibilities include vibrato, degree of nasality, rate of change of any of the above as an independent parameter) to increase the available simultaneous degrees of freedom controllable via the voice."
D05-1125,1,"moreover, we plan to develop algorithms to decouple unintended user correlations of these parameters, and to further advance both our adaptation and acceleration algorithms."
D05-1126,4,"the methodology proposed here is not so dependent on the domains, thus applicable to many other tasks of this category."
D05-1127,4,even with the lack of convergence our approach could be applied to more complicated domains in order to learn an effective dialog policy.
D05-1127,4,our approach would be especially useful in situations where there are no existing corpora of human-human interactions for the domain or as a way to provide a check against a policy based on human intuition.
D06-1601,6,"more work needs to be done to resolve particular classes of errors; for example, the one reported above for the verb work."
D06-1602,3,"finally, we would like to investigate the extent to which existing nlp systems (such as open-domain qa systems) can benefit from a detailed analysis of superlatives."
D06-1602,5,"also, only one of the possible types of superlative was considered, namely the attributive case."
D06-1602,5,"another aspect which we have neglected in this study but want to consider in future work is the interaction between superlatives and focus (heim, 1999; gawron, 1995)."
D06-1602,1,"one obvious improvement is to amend ccgbank in order to avoid the need for postprocessing rules, thereby also allowing the creation of more accurate language models."
D06-1602,1,"in future work we will consider the interpretation of predicative and adverbial superlatives, as well as comparative expressions."
D06-1603,5,unpaired tuples that are semantically redundant should also be regarded as insignificant.
D06-1603,1,another direction is to detect semantic redundancy in a sentence.
D06-1603,1,"while we continue to explore more suitable representation of unpaired predicate argument tuples, we plan to augment the similarity measure for phrasal units to reduce the error rate in the first component."
D06-1604,3,"in future work, we hope to further validate this paradigm by constructing additional semantic filters that detect other types of errors."
D06-1604,5,such a corpus could be used to re-train a statistical parser to improve its performance.
D06-1604,1,"if semantic filters become sufficiently accurate, they could rule out enough erroneous parses that the parser is left with just the correct one."
D06-1604,1,"beyond that, we plan to embed semantic filtering into the parser itself."
D06-1604,1,we also plan to use semantic filters such as woodward to build a large-scale corpus of automatically-parsed sentences that has higher accuracy than can be achieved today.
D06-1605,1,part of our future work will be to try an intermediate degree of coarseness (still much coarser than wordnet) by using the paragraph subdivisions of the thesaurus instead of its categories to see if this gives even better results.
D06-1606,1,this correlation suggests that we can continue to use the bleu metric to further improve our models and systems.
D06-1608,5,we challenge others who are conducting research on syntactically-informed smt to verify whether or to what extent their systems are sensitive to parse quality.
D06-1609,1,"as further research, we would like to add extra features to the smr system, and study new types of classes for the reordering task."
D06-1610,5,"from examining the paraphrase extraction process, it is unclear how to relate translation probabilities and confidences with semantic closeness."
D06-1610,5,we plan to explore the parallels between the two to enable a weighted implementation of paraeval.
D06-1611,3,"in the future, we plan to test the generality of our paradise model on other corpora and to compare models built using our interaction parameters against models based on parameters commonly used in previous work (moller, 2005a)."
D06-1611,3,"we also want to see if our results hold for performance metrics based on user satisfaction questionnaires; in the new itspoke corpus we are currently annotating, each student also completed a user satisfaction survey (forbes-riley and litman, 2006) similar to the one used in the darpa communicator multi-site evaluation (walker , 2002)."
D06-1611,5,testing if our results generalize to a human annotation of the discourse structure and automated models of certainty and correctness is also of importance.
D06-1612,5,"for example, an entity that is linked in wordnet (within a given depth) and/or framenet to a previously introduced one is more likely to be mediated than new."
D06-1612,1,"additionally, we will attempt to exploit dialogue turns, since knowing which speaker said what is clearly very valuable information."
D06-1612,1,"in particular, we plan to use additional lexical and relational features derived from knowledge sources such as wordnet (fellbaum, 1998) and framenet (baker et al., 1998) which should be especially helpful in distinguishing mediated from new entities, the most difficult decision to make."
D06-1612,1,we also plan to run experiments on the automatic classification of old and mediated subtypes (the finer-grained classification) that is included in the corpus but that we did not consider for the present study (see section 2.1).
D06-1612,1,"in a similar vein, we will experiment with distance measures, in terms of turns, sentences, or even time, for determining when an introduced entity might stop to be available."
D06-1613,1,"we are currently performing an experiment to see if citation processing can increase performance in a large-scale, real-world information retrieval task, by creating a test collection of researchers queries and relevant documents for these (ritchie et al., 2006a)."
D06-1614,1,"with this additional information, we expect considerable improvement in grammatical function assignment for the functions subject, accusative object, and dative object, which are marked by nominative, accusative, and dative case, respectively."
D06-1614,1,additional experiments with the tuba-d/z treebank are planned in future work.
D06-1615,4,one of our next goals is to apply scl directly to parsing.
D06-1615,5,"we are also focusing on other potential applications, including chunking (sha and pereira, 2003), named entity recognition (florian, 2004; ando and zhang, 2005b; daume iii and marcu, 2006), and speaker adaptation (kuhn, 1998)."
D06-1615,1,"finally, we are investigating more direct ways of applying structural correspondence learning when we have labeled data from both source and target domains."
D06-1618,1,"finally, our training curves suggest that  future research in this area should focus primarily on identifying more discriminative features."
D06-1619,1,we hope that this research provides a novel approach to deterministic parsing in which only lexical selection and little phrasal information without packed representations dominates the parsing strategy.
D06-1621,1,"in the long run, we believe that the availability of such datasets will facilitate improved models that consider the various sub-cases of lexical reference, as well as applying supervised learning to optimize model combination and performance."
D06-1622,1,"future work will investigate using more features,  new heuristics and/or other ml approaches to  improve the performance of instance-based  learning algorithms at the srl task."
D06-1623,3,"we will build on the existing formal framework (fikes , 2003) for the verification of ordering consistency."
D06-1623,4,we are also interested in expanding our framework for global inference to other temporal annotation schemes.
D06-1623,1,"in the future, we will explore a richer set of constraints on the topology on the ordering graph."
D06-1623,3,"we will build on the existing formal framework (fikes et al., 2003) for the verification of ordering consistency."
D06-1623,1,"given a richer set of temporal relations, the benefits from global inference can be even more significant."
D06-1624,2,"then, high level knowledges, such as the dialog context, can also be included as the features of topic and semantic classifiers."
D06-1624,3,the future work includes further evaluation of our approach in other application domains and languages.
D06-1624,1,we also plan to integrate this understanding system into a whole dialog system.
D06-1624,1,"then, it is worthwhile to investigate how to appropriately define topics and the probability of exploiting the sentence clustering techniques to  facilitate the topic (frame) designment."
D06-1625,2,we also like to investigate if our findings generalize to other types of corpora besides tv-show dialogs.
D06-1625,1,"in the future, we plan to explore more sophisticated semantic and pragmatic features such as incongruity, ambiguity, expectation-violation etc."
D06-1626,4,we are planning to integrate the distributed lm in the statistical machine translation decoder in the near future.
D06-1626,1,we will investigate different relevance weighting schemes to better combine n-gram statistics from different data sources.
D06-1628,6,"for example, we might consider aeps that include larger chunks of phrase structure, or we might consider aeps that contain more detailed information about the relative ordering of modifiers."
D06-1628,6,future work may also consider expanded definitions of aeps.
D06-1628,6,there is certainly room for improvement in the accuracy with which aeps are predicted in our data; the feature-driven approach allows a wide range of features to be tested.
D06-1629,1,"future work will include collecting impression keywords automatically, and adapting the language model to the category of source words."
D06-1630,1,one particular area that we will continue to work on is phonetic distance. the work we report is ongoing and is part of a larger project on multilingual named entity recognition and transliteration.
D06-1631,1,we are presently exploring algorithms to normalize foreign words in arabic text.
D06-1631,1,this will allow us to identify normalized forms for foreign words and use a single consistent version for indexing and retrieval.
D06-1632,2,"in the future, we also want to experiment with a larger data set for determining whether discourse cues really do not correlate with paragraph boundaries."
D06-1632,4,"then, we will move on towards multi-document summarization, the application which motivates the research described here."
D06-1635,1,"for longer sequences, we will take advantage of the fact that cky is easily parallelizable, since any operation which combines the entries of two cells chart and chart is completely independent of other parts of the chart."
D06-1635,1,"we also plan to adapt this technique to other, more realistic, representations of proteins, and to longer sequences."
D06-1638,1,"when modeling the lexical rule p(x[]  w), we could use features that consider the spelling of the word w in conjunction with the value of ."
D06-1639,1,"while such functionality is well beyond the scope of our current study, we are optimistic that we can develop methods to exploit additional types of relationships in future work."
D06-1640,1,we plan to investigate algorithms that can directly optimize for complex measures (such as b3)for the problem of partially supervised clustering.
D06-1641,4,"our model can be easily extended to opinion retrieval, if the opinion retrieval is defined as retrieving sentences or documents that contain either positive or negative sentiments."
D06-1641,6,this issue is worth pursuing in future work.
D06-1641,1,"approaches considering polarity strength or continuous values for the polarity specification, rather than using {-1,1} can also be considered in future work."
D06-1644,5,"furthermore, we hope to improve the convergence properties of the dy- namic adaptation scheme at the start of lectures and across topic transitions."
D06-1644,1,"lastly, we would like to extend the lda framework to support speaker-specific adaptation and apply the resulting topic distributions to lecture segmentation."
D06-1645,1,sentence selection can then be improved by considering the composite entropy for all the models.
D06-1645,1,we are exploring the use of ranking to reorder the data such that the sequential selection process gives better results.
D06-1645,1,another idea we are currently investigating is to use multiple instances of the selection algorithm with different initial language models pinit generated by bagging.
D06-1647,2,future work includes a more detailed analysis of transductive learning in this domain and possible solutions to alleviating error propagation.
D06-1648,2,"also, determining the size of a sufficiently large corpus to generate a correction dictionary and to train a language model is desirable."
D06-1648,5,"finally, word prediction might prove useful for cases where ocr grossly misrecognized words."
D06-1648,1,"for future work, a factor language model  might prove beneficial to incorporate morphological information and other factors such as part  of speech tags while overcoming training data  sparseness problems."
D06-1650,4,"it is our hope through this work to shed some light onto what people find helpful in usersupplied reviews and, by automatically ranking them, to ultimately enhance user experience."
D06-1653,4,this implies that a large amount of work in the field of ir could be imported into cf.
D06-1653,6,this would be interesting to investigate in future work.
D06-1654,1,"an interesting direction would be the development of algorithms that allowed the incremental application of weights, perhaps by re-weighting vectors when a new context is learned."
D06-1657,3,"furthermore, more sophisticated character sequence kernels can be evaluated, such as mismatch string kernels used in bioinformatics, where mutations in the sequences are allowed (leslie , 2004)."
D06-1657,3,"in future work it would be useful to appraise composite kernels (joachims , 2001) in order to combine character and word sequence kernels."
D06-1657,1,"if the two kernel types use (partly) complementary information, better performance could be achieved."
D06-1659,1,"in our future work, we plan to focus on generalizing the approach for targeting more nlp problems."
D06-1660,1,"in the future, wed like to focus on further exploring more effective methods to adapt ner model to a new domain with much less efforts, time and performance degrading."
D06-1662,4,"in future, we may explore the role of term-level or word-level features, e.g., proper nouns, in the ordering of summary sentences."
D06-1662,5,one specific task is how to generate coreference among sentences in summaries.
D06-1662,1,"to make summaries more coherent and  readable, we may also need to discover how to  detect and control topic movement automatic summaries."
D06-1662,1,"in  addition, we will also try other semi-supervised  classification methods, and other evaluation  metrics, etc."
D06-1665,4,"these relations can be applied in many other tasks, such as machine translation, word sense disambiguation / discrimination, and so on."
D06-1665,6,these are some interesting research work in the future.
D06-1667,1,"moreover, instead of cosine similarity measure to calculate the distance between context vectors, we will try other distributional similarity measures to see whether the performance of relation extraction can be improved."
D06-1667,1,in the future we will further explore other semantic information to assist the relation extraction problem.
D06-1667,1,"in addition, if we can find an effective unsupervised way to filter out unrelated entity pairs in advance, it would make our proposed method more practical."
D06-1669,1,"we would also like to integrate different kinds of information, specially the local or syntactic features so successfully used by supervised systems, but also more heterogeneous information from knowledge bases."
D06-1669,1,"for the future, we would like to look more closely the micro-senses induced by hyperlex, and see if we can group them into coarser clusters."
D06-1671,5,"one shortcoming of this approach is that fields are not allowed to belong to multiple records, because the partitioning algorithm returns nonoverlapping clusters."
D06-1671,1,another avenue of future research is to consider syntactic information in the compatibility function.
D06-1671,1,exploring overlapping clustering techniques is an area of future work.
D06-1672,1,we plan to further investigate the relationship between the local and global approaches to complex learning problems in natural  language.
D06-1673,1,we are proposing a general method to deal with all multi-stage algorithms.
D07-1001,3,"in general, we will assess the impact of discourse information more systematically by incorporating it into generative and discriminative modelling paradigms."
D07-1001,1,"in the future, we will interface our compression model with sentence extraction."
D07-1001,3,we also plan to study the effect of global discourse structure (daume iii and marcu 2002) on the compression task.
D07-1002,3,"important future directions lie in evaluating the contribution of alternative semantic role frameworks (e.g., propbank) to the answer extraction task and developing models that learn semantic roles directly from unannotated text without the support of framenet annotations (grenager and manning, 2006)."
D07-1002,4,"beyond question answering, we also plan to investigate the potential of our model for shallow semantic parsing since our experience so far has shown that it achieves good recall."
D07-1004,2,"moreover, applying the u-svm to qa systems in other languages, like english and japanese, will also be included in our future work."
D07-1005,1,"in future work, we will consider several extensions to this framework that lead to more powerful system combination strategies using multiple bridge languages."
D07-1007,3,"it would also be interesting to assess whether a more grammatically structured statistical mt model that is less reliant on an n-gram language model, such as the syntactic itg based grammatical channel translation model of (wu and wong, 1998), could make more effective use of wsd predictions."
D07-1008,2,"secondly, the present paper used a relatively simple feature set."
D07-1008,6,future research will follow three directions.
D07-1008,1,"first, we will extend the framework to incorporate position dependent loss functions."
D07-1008,1,examples include the hamming distance or more sophisticated functions that take the tree structure of the source and target sentences into account.
D07-1008,1,such functions can be supported by augmenting our generation algorithm with a beam search.
D07-1008,1,our intention was to examine our models performance without extensive feature engineering.
D07-1008,1,"nevertheless, improvements should be possible by incorporating features defined over n-grams and dependencies (mcdonald, 2006)."
D07-1009,1,we are also interested in further developing our system for automatic update of wikipedia pages.
D07-1010,1,"finally, jointly modeling propbank and the pdtb is another interesting area we plan toinvestigate, something to which the head-basedapproach and dependency parse representation we advocate here would be well-suited."
D07-1011,5,"another possible limitation of iapart is that, despite strong evidence for overspecification, complex domains could yield very lengthy outputs."
D07-1011,5,"strategies to avoid them include the utilisation of other boolean operators like negation (the desks which are not red) (horacek, 2004)."
D07-1011,1,these issues are open to future empirical research.
D07-1012,5,"our plan is to investigate why all methods perform poorly on missing word errors, to extend the error creation procedure so that it includes a wider range of errors, to try the deep approach with other parsers, to integrate additional features from stateof-the-art shallow techniques and to repeat the experiments for languages other than english."
D07-1016,4,future work will also involve determining the english inclusion classifiers merit when applied to rule-based parsing.
D07-1016,6,this provides an upper bound on the performance we can expect from a parser that uses inclusion detection.
D07-1016,1,"finally, our results indicate that future work could improve parsing performance for inclusions further: we found that parsing the inclusion set is still harder than parsing a randomly sampled test set, even for our best-performing model."
D07-1017,4,it is our hope that methods such as the one proposed in this paper may one day be used to harness the richness of automatically created inference rule resources within large-scale nlp applications.
D07-1018,4,"third, the application of the acquired information to broader nlp tasks."
D07-1018,6,future work involves three main lines of research.
D07-1018,1,"first, the refinement of the classification itself, based on the results of the experiments presented."
D07-1018,1,"second, the use of additional linguistic evidence that contributes to the semantic class distinctions (e.g., selectional restrictions)."
D07-1019,4,"for example, we can work on page ranking information of returning pages, because trusted or well-known sites with high page rank generally contain few wrong spellings."
D07-1019,4,there is still further potential useful information that should be studied in this direction.
D07-1019,1,"in addition, the term cooccurrence statistics on the returned snippet text  are also worth deep investigation."
D07-1025,4,"we believe this versatility will lead to other successful applications of the idea, both within computational linguistics and in other fields involving sequential learning."
D07-1026,4,"finally, we are going to explore more elaborated kernel functions to recognize lexical entailment and more efficient learning strategies to apply our method to web-size corpora."
D07-1026,4,"for the future, we plan to apply our instance based approach to a wide variety of tasks, e.g., lexical substitution, word sense disambiguation and information retrieval."
D07-1026,1,"in addition, we plan to exploit our lexical entailment as a subcomponent of a more complex system to recognize textual entailment."
D07-1027,1,"we also plan to adapt other nld recovery methods (jijkoun and rijke, 2004; schmid, 2006) to chinese and compare them with the current results."
D07-1027,1,"we will investigate ways of closing the gap between the performance of gold-standard and parser output trees, including improving parsing result for chinese."
D07-1027,1,"in future work, we will refine and extend the conditioning features in our models to discriminate subcat frames and explore the possibilities to use the chinese propbank and hownet to supplement our automatically acquired subcat frames."
D07-1028,1,in the future we will experiment with increasing conditioning context further and using more sophisticated smoothing techniques to avoid sparse data problems when conditioning is increased.
D07-1030,1,"for example, we will investigate  to train chinese-to-english smt system based on  natural english and rbmt-generated synthetic  chinese."
D07-1030,1,"in the future work, we will investigate the possibility of training a reverse smt system with the  rbmt systems."
D07-1032,1,"our future work involves incorporating ellipsis resolution to develop an integrated model for syntactic, case, and ellipsis analysis."
D07-1033,1,we would like to investigate various types of new nonlocal features using the proposed algorithm in future work.
D07-1035,4,"we hope to explore our future work in several areas, such as further consolidating the new ground-truth from different points of view and verifying the effectiveness of low-quality review detection with other applications."
D07-1036,1,introducing language model optimization  into our system might further improve translation  performance.
D07-1036,1,it might work better by trying other sophisticated similarity measure models or using some  optimization algorithms to determine submodels  weights.
D07-1037,2,we intend to extend the hybrid indexing approach by considering more vocabulary subsets.
D07-1037,1,"syntactic similarity is more appropriate for verbs, for example, than co-occurrence."
D07-1037,1,"as a next step, we intend to embed verbs using syntactic similarity."
D07-1037,1,it would also be interesting to use lexical chains for proper names and learn the weights for different similarity scores.
D07-1039,4,"we hope to use these models in tasks such as diathesis alternation detection (mccarthy, 2000; tsang and stevenson, 2004) and contrast with wordnet models previously used for this purpose."
D07-1040,5,"given the increasingly large number of books available in electronic format, and correspondingly the growing need for tools for book summarization, we believe that the topic of automatic book summarization will become increasingly important."
D07-1040,5,we hope that this paper will encourage and facilitate the development of an active line of research concerned with book summarization.
D07-1042,5,how much noise this approximation introduces when finer role sets are used is an open research question.
D07-1042,1,"excluding the direct use of role-annotated corpora like framenet for coverage reasons, the most promising strategy is to extend our present scheme of approximating semantic relations by grammatical realizations."
D07-1045,5,"in particular, the problem of generalization to new translations seems to be promising to us."
D07-1045,1,"despite these encouraging results, we believe that additional research on improved estimation of probabilities in n-gram- or phrase-based statistical machine translation systems is needed."
D07-1046,5,"finally, we are puzzled by the differences between hebrew and arabic (for which the baseline and the current state of the art are significantly higher) on this task."
D07-1046,5,we intend to investigate the linguistic sources for this puzzle in the future.
D07-1046,1,"we also believe that further linguistic exploration, based on deeper error analysis, will result in more hard constraints which can reduce the error rate of the combination module."
D07-1049,5,we are currently implementing kneser-ney smoothing within the proposed framework.
D07-1049,1,"we hope the present work will, together with talbot and osborne (2007), establish the bloom filter as a practical alternative to conventional associative data structures used in computational linguistics."
D07-1051,5,further research is needed to investigate whether the problem class (classification with a fixed and moderate number of classes vs. ranking large numbers of possible candidates) is responsible for limited data reusability.
D07-1051,1,in future work we will directly compare qbc and uncertainty sampling with respect to data reusability.
D07-1053,6,this is one of the main aims of the recently started esac_imc project.
D07-1053,1,for  this reason we will perform an extended qualitative analysis of the presented methods with persons  who use our aac system sibylle.
D07-1055,5,whether this pays off in terms of translation quality is left open for future work.
D07-1055,1,we think that the n-gram level computation has certain advantages: the n-gram posterior probabilities could be computed from a word graph which would result in more reliable estimates.
D07-1056,1,"in the future work, we plan to improve the reordering model by introducing n-best syntax trees  and exploiting richer syntactic knowledge."
D07-1057,4,"in future, we plan to apply our model directly on machine-generated parse trees."
D07-1057,1,we also plan to classify non-coreferential zero pronouns into the six categories.
D07-1058,1,"the exact relation of p-dop to other dop models, including s-dop (bod, 2003), backoff-dop (simaan and buratto, 2003), dop* (zollmann and simaan, 2005) and ml-dop (bod, 2006; based on expectation maximization) and not dissimilar automatic enrichment models such as (petrov et al., 2006), remains a topic for future work."
D07-1060,4,"cross-lingual dpcs also have tremendous potential in tasks inherently involving more than one language, such as machine translation and multilanguage multi-document summarization."
D07-1060,5,our future work will explore other tasks such as information retrieval and text categorization.
D07-1061,2,"in future work, we hope to integrate other lexical resources such as wikipedia into the walk."
D07-1061,2,incorporating more types of links from more resources will underline the importance of determining appropriate relative weights for all of the types of edges in the walks matrix.
D07-1061,4,"even for wordnet, we believe that certain link types, such as antonyms, may be more or less appropriate for certain tasks and should weighted accordingly."
D07-1063,4,"for example, (zero-) anaphora resolution is considered as a good candidate task for application."
D07-1063,4,we are also planning to apply the proposed method to other tasks which need to construct tree structures.
D07-1063,1,the features used by sassano (2004) are promising as well.
D07-1063,1,"by extracting new features that are more suitable for the ancestor-descendant relation, we can further improve our method."
D07-1064,1,the similarity of two words derived from an external knowledge base can be assigned to a substitute node at a corresponding location in the state space in a straightforward manner.
D07-1064,1,we are also planning to reimplement our algorithms using crfs instead of the averaged perceptron algorithm.
D07-1064,6,This is a topic we are currently working on.
D07-1065,1,"in addition, we will investigate methods for automatically inferring patterns from a treebank corpus to support fast porting of our approach to other languages with treebanks."
D07-1065,1,"in future work, we will investigate methods for adding lexical information to our model in order to improve the performance on whadvps and whpps."
D07-1067,3,it would be interesting to compare this method to latent semantic analysis approaches for text segmentation as studied for example in bestgen (2006) and the references thereof.
D07-1067,4,"by defining appropriate features, we can use our method immediately for text and discourse segmentation."
D07-1067,1,"as future work, we plan on implementing 3 features in order to perform an accuracy/time analysis."
D07-1068,2,"for nlp applications such as qa, ne dictionary with finegrained label sets will be a useful resource."
D07-1068,2,"in future research, we plan to explore ne categorization with more fine-grained label set."
D07-1068,5,"however, generally, classification with statistical methods becomes difficult in case that the label set is large, because of the insufficient positive examples."
D07-1068,6,it is an issue to be resolved in the future.
D07-1069,4,we are currently working on applying our methods to the us house of representatives and other records of parliamentary speech from the united kingdom and australia.
D07-1069,5,"we are interested in dynamic mavenrank to go further with the idea of tracking how ideas get propagated through a network of debates, including congressional records, blogs, and newsgroups."
D07-1073,1,exploiting wikipedia structures such as disambiguation pages and link structures will be the key in that case as well.
D07-1073,1,we thus would like to incorporate a disambiguation technique into our method in future work.
D07-1075,5,"first, it would be interesting to see if the use of richer features can improve classifier performance, and if that in turn improves the performance of the ie system."
D07-1075,1,"finally, other techniques for learning semantically appropriate extraction patterns could be investigated."
D07-1076,5,"finally, we will study how to resolve the data imbalance and sparseness issues from the learning algorithm viewpoint."
D07-1076,6,our preliminary work of including the entity type information significantly improves the performance.
D07-1076,1,"moreover,  we will explore more entity-related information in the  parse tree."
D07-1076,1,"for the future work, we will focus on improving  the context-sensitive convolution tree kernel by exploring more useful context information."
D07-1077,1,"we plan to output reordered lattices in the future, so that the approach would be more robust to errors made during parsing/reordering."
D07-1080,2,future work involves scaling up to larger data and more features.
D07-1082,5,"in the future work, we will study how to exactly identify these borderline samples thus they are not firstly selected in active learning procedure."
D07-1082,1,the borderline instances can be detected using the  concept of tomek links (tomek 1976).
D07-1082,1,"it is also  worth studying cost-sensitive learning for active  learning with imbalanced data, and using such  techniques for wsd."
D07-1083,1,"in future we intend to investigate more appropriate model and feature design for unlabeled data, which may further improve the performance achieved in our experiments."
D07-1085,3,"for future work, we plan to test our proposed method on english speech corpora, and with largerscale retrieval tasks involving more queries and more documents."
D07-1085,4,"we would like to extend our method to other speech processing tasks, such as spoken document classification and example-based spoken document retrieval as well."
D07-1086,1,the next step in this research is to directly investigate how query segmentation affects search performance.
D07-1088,1,more effort will also  be put into the sentence-level analysis to reduce  error propagations.
D07-1088,1,"in addition, ontology based  knowledge inference strategies might be useful to  validate attributes in single record and in turn help  data record extraction."
D07-1088,1,"for the future, we plan to explore additional feature types and feature selection strategies to determine what is good for unstructured record templates to improve our results."
D07-1088,1,the last thing under our  direction is to explore new models if applicable.
D07-1091,1,"moreover, we expect to overcome the constraints of the currently implemented synchronous factored models by developing a more general asynchronous framework, where multiple translation steps may operate on different phrase segmentations (for instance a part-of-speech model for large scale reordering)."
D07-1091,1,"we are currently exploring these possibilities, for instance use of syntactic information in reordering and models with augmented input information."
D07-1092,5,"first, we are investigating why our approach remains silent for some words or phrases."
D07-1092,5,"second, we are investigating how a systematic enrichment of a phrase-transfer table will impact a phrase-based statistical machine translation engine."
D07-1092,1,"last, we want to investigate the training of a model that can learn regularities from the analogies we are making."
D07-1092,1,this will allow us to better characterize the limitations of analog and will hopefully lead us to design a better strategy for identifying the stems of a given word or phrase.
D07-1092,6,this work is currently being developed in several directions.
D07-1093,4,"we believe that the probabilistic framework we have introduced for diachronic phonology is promising, and scaling it up to richer phylogenetic may indeed reveal something insightful about language change."
D07-1096,1,"thus, finding ways of reusing already invested development efforts by adapting the outputs of existing systems to new requirements, without substantial loss in accuracy, seems to be another line of research that may be worth pursuing."
D07-1100,5,improving dependency relation labeling is left for future work.
D07-1101,5,"thus, a promising line of research is the investigation of methods to efficiently incorporate higher-order relations in discriminative parsing."
D07-1103,4,"for example, skip-ngrams (n-grams that allow for gaps of fixed or variable size) may be studied better using this approach leading to insight about methods that weakly approximate patterns."
D07-1103,4,"the code base for taking a list of n, m-grams and computing the required frequencies for signifance evaluation can be applied to related problems."
D07-1103,6,there are a number of important ways that this work can and will be continued.
D07-1104,4,"this model would bring statistical machine translation closer to convergence with so-called example-based translation, following current trends (marcu, 2001; och, 2002)."
D07-1104,6,we intend to explore these ideas in future work.
D07-1106,1,"first, the  method explored here can be extended as an alterative way to support such mt subtasks as back  transliteration (knight and graehl 1998) and noun  phrase translation (koehn and knight 2003)."
D07-1106,6,many opportunities exist for future research and improvement of the proposed approach.
D07-1107,5,we hope these new lexical resources will be useful for nlp applications that require a coarser-grained sense hierarchy than that already found in wordnet.
D07-1108,4,it would be interesting to see how the feature can help on wsd of other languages and other natural language processing tasks such as named-entity recognition.
D07-1109,1,"similarly, any topic based information retrieval scheme could employ topics that include semantically relevant (but perhaps unobserved) terms."
D07-1109,1,"incorporating this model in a larger syntactically aware model, which could benefit from the local context as well as the document level context, is an important component of future research."
D07-1110,6,the statistical measures are currently only used in a preprocessing step to filter the non-mwes for the lexical type predictor.
D07-1110,1,"alternatively, the statistical outcomes can be incorporated more tightly, i.e.to combine with the lexical type predictor and give confidence scores on the resulting lexical entries."
D07-1110,6,These possibilities will be explored in future work.
D07-1111,2,"as mentioned in section 5, the addition of a backward svm model did improve accuracy on the turkish set significantly, and it is likely that improvements would also be obtained in other languages."
D07-1111,6,"of course, the use of different approaches used by different groups in the conll 2006 and 2007 shared tasks represents great opportunity for parser ensembles."
D07-1111,1,one of the simplest improvements to our approach is simply to train more models with no other changes to our set-up.
D07-1111,1,"in addition, other  learning approaches, such as memory-based language processing (daelemans and van den bosch,  2005), could be used."
D07-1111,1,"for example, using mstparser  (mcdonald and pereira, 2005), a large-margin all pairs parser, in our domain adaptation procedure  results in significantly improved accuracy (83.2  las)."
D07-1111,1,"a  similar idea that may be more effective, but requires more effort, is to add parsers based on different approaches."
D07-1111,5,"a drawback of adding more  models that became obvious in our experiments  was the increased cost of both training (for example, the svm parsers we used required significantly longer to train than the maxent parsers) and  run-time (parsing with mbl models can be several  times slower than with maxent, or even svm)."
D07-1112,4,"in the future, we would like to model predictive opinions in other domains such as the real estate market and the stock market which would require further exploration of system design and data collection."
D07-1117,1,"in future efforts, we plan to extract additional reranking features utilizing more explicitly the characteristics of mandarin."
D07-1117,4,"we also plan to extend our work to speech transcripts for broadcast news and broadcast conversation corpora, and explore semisupervised training methods for reranking."
D07-1119,1,"since the technique is applicable to any parser,  we plan to test it also with more accurate english  parsers."
D07-1121,1,"future work will focus on tuning the many parameters our system has, as well as on experimenting with different types of constraints to supplement or replace one or more of the three types used in this study."
D07-1123,1,"while the parser is outperformed by a system based on local classifiers, we still hope that the parsing and training combination described here opens new ways in parser design and eventually leads to the improvement of parsing performance."
D07-1126,4,future work will also be focused on extending our method to a version of using semi-supervised learning that can efficiently be learnt by using labeled and unlabeled data.
D07-1126,4,we hope that the application of the pa algorithm to other nlp problems such as semantic parsing will be explored in future work.
D07-1129,1,"future work includes use of more sophisticated features such as pos and other morphological features, possibly a joint domain adaptation of pos tagging and dependency parsing for unlabeled data as well as re-examination of pivot features."
D07-1131,1,"in this way, the parse reranking  algorithm can be adopted to further improve the  performance."
D07-1131,1,"also, we are investigating how to convert  the shift-reduce parser into approximate n-best  parser efficiently."
D07-1131,1,"in the future, we plan to report the actual performance with replacing the mfn-svm by the  polynomial kernel svm."
D08-1001,6,"finally, the framework presented in this paper opens up exciting possibilities for future work."
D08-1001,6,such tasks are greatly facilitated by the explicit knowledge gained during structure recognition.
D08-1001,1,"in particular, we aim at automatically transforming report dictations into properly formatted and rephrased reports that conform to the requirements of the relevant domain."
D08-1002,3,"we believe that these lessons are broadly applicable, but verification of this claim is a topic for future work."
D08-1004,4,"our new method, being essentially bayesian inference, is potentially extensible to many other situations—other tasks, classifier architectures, and more complex features."
D08-1004,5,"in future, we are interested in new domains that can adaptively solicit rationales for some or all training examples."
D08-1006,1,"therefore, in future work we plan to experiment with classifiers whose decision function is cheaper to compute, such as neural networks and decision trees."
D08-1006,1,"another possible direction would be using the recently proposed deep belief network formalism (hinton et al., 2006)."
D08-1006,1,"in future work we plan to experiment with richer representations, e.g. including long-range n-grams (rosenfeld, 1996), class n-grams (brown et al., 1992), grammatical features (amaya and benedy, 2001), etc."
D08-1007,4,"it would be interesting to expand our cooccurrence features, including co-occurrence counts across more grammatical relations and using counts from external, unparsed corpora like the world wide web."
D08-1007,4,"also, like other models of sp, our technique can also be used for sense disambiguations: the weightings on our semantic class features indicate, for a particular noun, which of its senses (classes) is most compatible with each verb."
D08-1007,4,"it would be interesting to expand our cooccurrence features, including co-occurrence counts across more grammatical relations and using counts from external, unparsed corpora like the world wide web. also, like other models of sp, our technique can also be used for sense disambiguation: the weightings on our semantic class features indicate, for a particular noun, which of its senses (classes) is most compatible with each verb."
D08-1009,3,"in the future, we plan more extensive tests to characterize when holmes’s inference is helpful."
D08-1009,5,we also hope to examine in what cases jointly performing extraction and inference (as opposed to performing them separately) is feasible at scale.
D08-1009,1,"finally, we plan to examine methods for holmes to learn both rule weights and new inference rules."
D08-1010,3,"moreover, we will test the performance of the mers model on large scale corpus."
D08-1010,1,"in future, we will explore more sophisticated features for the mers model."
D08-1013,4,"we would expand the idea in this paper into other models, such as semi-crfs and hierarchical-crfs."
D08-1013,1,"for our future work, we will explore other hierarchical models for sentimental strength rating because the experiments presented in this paper prove this hierarchical frame is effective for ordinal regression."
D08-1014,1,"in the future, we plan to explore additional language-specific clues, and integrate them into the subjectivity classifiers."
D08-1015,1,we plan to combine these two methods together.
D08-1015,1,we would also like to integrate emotion ranking into information retrieval.
D08-1015,1,"an observation is that plm exploits pairwise order information, whereas edr exploits emotional distribution information."
D08-1015,1,another research direction is to improve edr by finding better features.
D08-1016,1,"we are interested in extending these ideas to phrase-structure and lattice parsing, and in trying other higher-order features, such as those used in parse reranking (charniak and johnson, 2005; huang, 2008) and history-based parsing (nivre and mcdonald, 2008)."
D08-1018,4,another future work will be to apply our work to chart parsing.
D08-1018,4,"it is known that binarization is also essential for an o(n3) complexity of chart parsing, where dotted rules are used to binarize the grammar implicitly from left."
D08-1018,4,therefore chart parsing can use multiple binarizations.
D08-1018,4,we expect that a better binarization will also help improve the efficiency of chart parsing.
D08-1018,4,"as shown in charniak et al.(1998), we can binarize explicitly and use intermediate symbols to replace dotted rules in chart parsing."
D08-1018,1,one future work will be relaxing the assumption and finding a better approach.
D08-1019,1,to generate coherent texts we plan to move beyond sentence generation and add discourse constraints to our system.
D08-1021,5,"syntactic constraints significantly improve the quality of this paraphrasing method, and their use opens the question about whether analogous constraints can be usefully applied to paraphrases generated from purely monolingual corpora."
D08-1022,4,"for future work we would like to apply this approach to other types of syntax-based translation systems, namely the string-to-tree systems (galley , 2006) and tree-to-tree systems."
D08-1023,2,"with further work on scaling these models to large data sets, and engineering high performance features, we believe this research has the potential to provide significant increases in translation quality."
D08-1025,4,"although the noisy channel has been in use for many years in spelling correction, our model could be used more generally for grammar corrections, including insertions, deletions, and (with new noise functions) potentially changes in word order."
D08-1025,4,"finally, we note that the model here could potentially find practical application in grammar correction."
D08-1026,1,incorporating eye gaze with automatic word acquisition provides another potential approach to improve the robustness of human machine conversation.
D08-1029,2,this suggests both that a document-level ie system operating over a large corpus text can improve its accuracy with information that it learns from the corpus; and also that integrating an ie system more closely with a source of world knowledge (e.g.a knowledge base) could improve extraction accuracy.
D08-1030,1,"for future work, we intend to investigate the use of automatic feature selection methods on the same data."
D08-1032,5,"in future, we have the plan to decompose the complex questions into several simple questions before measuring the similarity between the document sentence and the query sentence."
D08-1032,5,"we expect that by decomposing complex questions into the sets of subquestions that they entail, systems can improve the average quality of answers returned and achieve better coverage for the question as a whole."
D08-1034,5,what if we could extend the idea of hierarchical architecture to the single semantic role level?
D08-1034,5,would that help the improvement of src?
D08-1035,1,"in the future, we hope to explore the use of similar bayesian techniques for hierarchical segmentation, and to incorporate additional features such as prosody and speaker change information."
D08-1036,5,"inspired by this, one can devise hybrid strategies that interleave blocked and pointwise sampling; these might perform better than both the blocked and pointwise samplers described here."
D08-1037,1,"we plan to investigate using more constraints within this framework, such as soft constraints which can penalize unlikely local decisions while not completely eliminating the entire solution."
D08-1039,2,"further experiments will be conducted, especially on large tasks such as the nist chinese-english and arabicenglish task."
D08-1039,2,training on these huge databases will only be possible with an appropriate selection of promising triplets.
D08-1039,1,"for the inverse model, p(e|f), an integration into the search is directly possible."
D08-1039,1,future work will address an integration into the decoder since the performance of the current rescoring framework is limited by the quality of the nbest lists.
D08-1040,5,by opening the systems code and giving others the opportunity of adding their own modules and changes we hope to solve remaining problems.
D08-1040,1,we assume that the affect recognition mentioned above will help us to achieve this goal in near future and this is our next step.
D08-1042,4,"the dependency-based word subsequence kernel could be tested on other tasks which require computing similarity between sentences or texts, like text classification, paraphrasing, summarization etc."
D08-1042,6,this will generalize the kernel and make it more robust to data sparsity.
D08-1042,6,we believe this kernel will help improve performance on those tasks.
D08-1042,1,"in future, the dependency-based word subsequence kernel could be extended to incorporate word classes like the kernels presented in (bunescu and mooney, 2005a; zelenko et al., 2003)."
D08-1042,1,"it should be possible to achieve this by incorporating matches between word classes in addition to the exact word matches in the kernel computations similar to the way in which the word subsequence kernel was extended to incorporate word classes in (bunescu and mooney, 2005b)."
D08-1043,3,future work will focus on testing the effectiveness of the proposed method on a larger set of qa collections with broader domains.
D08-1043,1,"since the proposed approach cannot handle many-to-one or one to-many word transformations, we also plan to investigate the effectiveness of phrase-based translation models in closing gaps between queries and questions for further enhancement of qa retrieval."
D08-1045,5,these phenomena are not observed in newspaper articles but cannot be ignored in web texts.
D08-1045,1,"in the future, we will work on these phenomena. hence in the future, we aim to use the proposed method to improve the quality of these applications."
D08-1047,1,"a natural extension of this study is to handle multiple regions of changes for morphologically rich languages (e.g. german) and to handle changes at the phrase/term level (e.g., “estrogen receptor” and “receptor of oestrogen”)."
D08-1047,1,another direction would be to incorporate the methodologies for semisupervised machine learning to accommodate situations in which positive instances and/or unlabeled strings are insufficient.
D08-1048,3,"as future work, we will evaluate new types of spaces (e.g.dimensionality reduction methods) to improve the generalization capabilities of the space models."
D08-1048,1,"we will also address the data sparseness issue, by testing smoothing techniques to better model low frequency lus."
D08-1048,1,"finally, we will implement the presented models in a complex architecture for semi-supervised framenets development, both for specializing the existing english framenet in specific domains, and for creating new framenets in other languages."
D08-1050,5,"the utility of measures such as unknown word rate (which can be performed with unlabelled data) and unknown pos n-gram rate (which can be performed with only pos tags) is not yet sufficiently clear to rely on them as predictive measures, but it seems a fruitful avenue for future work to investigate the importance of such measures for parser domain adaptation."
D08-1054,1,"as future work, we plan to compare the htm model with other existing models, to develop learning and inference methods for handling extremely large-scale data sets, and to combine the current method with a keyphrase extraction method for extracting keyphrases from web pages."
D08-1055,1,"in future, we will use richer constraints and research better ways of distinguishing whether or not cases are obligatory."
D08-1056,5,one interesting question for future research is whether different game architectures might be better suited to certain kinds of data.
D08-1056,5,"there are two significant difficulties: keeping the game fun, and making sure the collected data is not too noisy."
D08-1056,1,probably the most interesting direction for future work is trying to increase the complexity of the data collected from a game.
D08-1057,5,"in future work, we intend to explore other string matches corresponding to variations due to paraphrases and synonymy."
D08-1057,5,we would also like to study the effects of corpus size when learning schematic patterns.
D08-1057,1,"finally, we are currently investigating the use of machine learning methods to combine the best of the salience and schemata models in order to provide a single model for use in decoding."
D08-1058,2,"in future work, more additional english resources will be used to further improve the results."
D08-1058,4,we will also apply the idea to supervised chinese sentiment analysis.
D08-1059,4,"the idea of combining different approaches to the same problem using beam-search and a global model could be applied to other parsing tasks, such as constituent parsing, and possibly other nlp tasks."
D08-1060,1,"in future, we plan to incorporate features from target-side syntactic information, and connect them with the source information explored in this paper, to model long-distance reordering for better translation quality."
D08-1061,2,combine information from more data sources to answer the question of whether more data or diverse sources are more effective in increasing precision and coverage.
D08-1061,4,apply similar ideas to other information extraction tasks such as relation extraction.
D08-1061,1,"1.encode richer relationships between nodes, for example instance-instance associations and other types of nodes.2."
D08-1062,3,"of the parameters not considered in section 4.2, we would like to further investigate the benefits of chunking entities on the resulting base relations, experimenting with different measures of collocation."
D08-1062,1,"first, we would like to generalize the scope of our discovery pipeline beyond binary relations and with richer considerations of context, even across sentences."
D08-1062,1,"in future work, we seek to expand upon our rd methods in three directions."
D08-1062,1,"finally, we intend to induce entire concept maps from text using the discovered relations to bootstrap an re phase, where the underlying problem is not just of inferring multiple types of relations, but to have sufficient co-ordination among the discovered relations to ensure connectedness among the resulting concepts."
D08-1062,1,"second, we hope to achieve greater tunability of performance, to account for additional discovery metrics besides precision."
D08-1063,3,"the experiments are conducted on clearly specified partitions of the ace 2007 data set, so future comparisons against the presented work can be correctly and accurately made."
D08-1063,1,"as future work, we plan to extend this work to use semisupervised and unsupervised approaches that can make use of cross-language information propagation."
D08-1065,4,"we hope that our approach will provide some insight into the design of lattice-based search procedures along with the use of non-linear, global loss functions such as bleu."
D08-1066,3,"firstly, we plan to explore our estimator on other language pairs in order to obtain more evidence on its behavior."
D08-1066,5,"finally, it would be interesting to study properties of the penalized deleted estimation used in this paper."
D08-1066,1,"apart from a new decoder, it will be worthwhile adapting the prior probability in our model to allow for consistent estimation."
D08-1066,1,"secondly, as (blunsom et al., 2008) show, marginalizing out the different segmentations during decoding leads to improved performance."
D08-1066,1,we plan to build our own decoder (based on itg) where different ideas can be tested including tractable ways for achieving a marginalization effect.
D08-1068,4,"future directions include incorporating additional knowledge, conducting joint entity detection and coreference resolution, and combining coreference resolution with other nlp tasks."
D08-1070,3,"in future work we plan to empirically evaluate ner with an approximate version of the full model m2 which, while more demanding in terms of time complexity, could lead to even more significant gains in accuracy."
D08-1070,3,we also intend to comprehensively evaluate the proposed scheme for computing probabilities by experimenting with alternative normalization functions.
D08-1071,4,"in addition to an analysis of the theoretical properties of the algorithm presented, the most compelling avenue for future work is to apply this framework to other task pairs."
D08-1071,1,"with a little thought, one can imagine formulating compatibility functions between tasks like discourse parsing and summarization (marcu, 2000), parsing and word alignment, or summarization and information extraction."
D08-1072,5,"additionally, how can prior knowledge about domain similarity be included into the combination methods?"
D08-1072,5,this work also raises some questions about learning on large numbers of disparate domains: can a hierarchical online clustering yield a better representation than just selecting between k shared parameters?
D08-1073,4,we expect such an improved ordering classifier to be used to improve the performance of tasks such as summarization and question answering about the temporal nature of events.
D08-1073,1,"further progress in improving global constraints will require new methods to more accurately identify unknown events, as well as new approaches to create implicit constraints over the ordering."
D08-1074,4,in the future we will attempt to solve this problem along these lines and work toward a system that can be used in practical applications.
D08-1075,3,"secondly, the system should be tested in different types of biomedical texts, like full papers or medical reports to check its robustness."
D08-1075,6,further research is possible in several directions.
D08-1075,1,"in the first place, other machine learning algorithms could be integrated in the system in order to optimize performance."
D08-1075,1,"finally, the postprocessing algorithm could be improved by using more sophisticated sequence classification techniques (dietterich, 2002) ."
D08-1076,5,our future work will therefore focus on how much system combination and syntax-augmented machine translation can benefit from lattice mert and to what extent feature function weights can robustly be estimated using the suggested method.
D08-1077,2,"in future, we hope to study translations from other languages into english to study the role of deletions in such cases."
D08-1078,5,"however, we suspect that the conclusions would be similar for most statistical machine translation models because of their dependence on automatic alignments."
D08-1079,1,"in future work, we will exploit this kind of subtopic-level information to further improve the summarization performance."
D08-1080,1,future work includes testing the spreading activation and page ranking method in the context of the update summarization task and exploring methods of extracting related concepts from the full text of wikipedia articles.
D08-1081,2,"we are also investigating domain adaptation techniques; for example, we hypothesize that the relatively well-resourced domain of meetings can be leveraged to improve email results, and preliminary findings are encouraging."
D08-1081,4,"we are currently working on extending our system to other conversation domains such as chats, blogs and telephone speech."
D08-1082,4,we are also interested in investigating ways to apply the generative model to the inverse task: generation of a nl sentence that explains a given mr structure.
D08-1082,1,"in future, we would like to extend the current model to have a wider range of support of mr formalisms, such as the one with lambda-calculus support."
D08-1083,1,future research includes an approach that learns the compositional inference rules from data.
D08-1085,4,"methods such as these should also be useful for natural language decipherment problems such as character code conversion, phonetic decipherment, and word substitution ciphers with applications in machine translation (knight , 2006)."
D08-1085,1,"obtaining optimal keys according to such models will permit the automatic decipherment of shorter ciphers, but this requires more specialized search than what is provided by general integer programming solvers."
D08-1087,1,"thus, for future work, we plan to investigate the use of the initial recognition hypotheses as the development set, as well as manually transcribing a subset of the test set utterances."
D08-1093,6,there are many avenues for future work opened up by the work presented here.
D08-1093,1,the accuracy of the predictor can be further improved by incorporating more complex syntax-based features and multiple agreement features.
D08-1093,1,"moreover, rather than predicting an intrinsic metric such as the parseval fscore, the metric that the predictor learns to predict can be chosen to better fit the final metric on which an end to-end system is measured, in the style of (och, 2003)."
D08-1094,1,"we will explore the usability of vector space models of word meaning in nlp applications, formulated as the question of how to perform inferences on them in the context of the textual entailment task (dagan et al., 2006)."
D08-1095,1,we are also interested in exploring a possible relation between the path-constrained walk approach and reinforcement learning.
D08-1096,5,"future work will examine how to best treat this challenge, e.g., by using an estimation of density instead of the simplistic “1 nearest neighbor” distance used here."
D08-1096,5,we also want to address that the model as it currently stands is trained under the false assumption that the training input is grammatical.
D08-1096,1,"in future work, we intend to investigate the influence of noise and ambiguity on the quality of the representations in order to characterize when higher order representations improve generalization and exemplar-theoretic inference."
D08-1096,1,the most important future work concerns class based language models.
D08-1098,4,we will also investigate other applications of our cotraining framework to tasks such as sentiment analysis in community question answering and similar social media content.
D08-1098,1,we also plan to explore related variants of semi-supervised learning such as co-boosting methods to further improve classification performance.
D08-1098,1,"in the future we plan to explore more sophisticated features such semantic concepts and relationships (e.g., derived from wordnet or wikipedia), and richer syntactic and linguistic information."
D08-1099,2,"in future experiments, we will investigate its performance on question answering tasks in languages such as chinese and japanese."
D08-1099,2,"in future experiments, we will investigate other methods of merging answer candidates, such as taking the union of answers from both systems."
D08-1099,4,"we would also like to emphasize that the se approach is entirely language independent, and thus can be readily applied to answer candidates in other languages."
D08-1099,5,"we expect further improvements from adding candidates that are found only by the qa system, but it is unclear how the confidence measures from the two systems can be combined effectively."
D08-1099,1,"in the future, we will investigate how to utilize more answer candidates from the qa system and determine the minimal quality of those candidates required for se approach to make an improvement. in future experiments, we will investigate other methods of merging answer candidates, such as taking the union of answers from both systems."
D08-1101,4,future work could focus on applying ranking statistics to techniques for mining and tracking temporal and time-changing parameters in conjunction with techniques like (agrawal and srikant 1995; pratt 2001; last et al 2001).
D08-1101,4,"our approach is also suited for the analysis of large streams of real time conversations, and this is a very important area of focus as presently more and more conversational data gets generated through channels like chat, mobile telephony, voip etc."
D08-1101,1,another area of possible future work is the detection and separation of multiple underlying trends in dialogs.
D08-1102,4,we also want to investigate the integration of our approach to multilingual language models and move beyond cs to address other deeper linguistic phenomena.
D08-1102,5,some directions for future work include: exploring the extent to which our results can be improved by including a multi-word expression recognition system.
D08-1102,1,"lastly, we would like to explore similar approaches in other popular language combinations."
D08-1103,4,our future goals include porting this approach to a cross-lingual framework in order to determine antonymy in a resource-poor language by combining its text with a thesaurus from a resource-rich language.
D08-1103,4,"we also intend to use the approach proposed here in tasks where keyword matching is especially problematic, for example, separating paraphrases from contradictions."
D08-1103,1,we will use antonym pairs to identify contrast relations between sentences to in turn improve automatic summarization.
D08-1104,4,"at any rate, our idiom corpus will play an important role in the development of unsupervised or semi-supervised methods, and the experimental results obtained in this study will be a good reference point to evaluate those methods."
D08-1107,1,we believe that query grammar can be further exploited to increase query understanding and that this understanding can improve the overall search experience.
D08-1110,4,an interesting line of future work would be to explore if the method presented here can be adapted to different language combinations.
D08-1110,5,"moreover, multilingual communities will code-switch among more than two codes and this poses fascinating research challenges as well."
D08-1111,2,"moreover, we will try to relabel sighan corpus on our three labels, and do experiments on them, which will be more convenient to compare with other segmentation methods."
D08-1111,1,"besides, we will carry out more experiments to search the effectiveness of our segmentation method to cir."
D08-1111,1,"as future work, we would search another more encouraging method to make a segmentation decision from the ranking result."
D08-1113,1,"in future work, we would like to identify a set of features, latent variables, and training methods that port well across languages and string-transduction tasks."
D08-1114,5,more empirical comparative analysis between the two algorithms of this sort will appear in future work.
D08-1114,5,"we see, moreover, qualitative differences in the solutions as well – e.g., ams solution for the pendant node 5 is less confident than is lps solution."
D09-1001,1,"directions for future work include: better handling of antonyms, subsumption relations among expressions, quantifier scoping, more complex lambda forms, etc.; use of context and discourse to aid expression clustering and semantic parsing; more efficient learning and inference; application to larger corpora; etc."
D09-1002,5,an interesting question is whether the incorporation of wordnet-based similarity would lead to similar improvements in our case.
D09-1002,1,an obvious direction for future work concerns improving our scoring function.
D09-1003,1,"the current model uses the context in a very straightforward way, i.e. the two words left and right of the current word, but in the future we would like to explore more advanced methods to improve the similarity estimates."
D09-1003,1,"in the future we would like to improve the described automatic expansion methods, since we feel that their full potential has not yet been reached."
D09-1003,1,more specifically we plan to experiment with more advanced methods to decide whether some automatically generated examples should be added to the training set.
D09-1008,1,we will employ a gaussian model to unify various linguistic and contextual features.
D09-1008,1,we will also improve the dependency-to dependency method with a better bi-lingual parser.
D09-1008,6,"in future, we will continue this work in two directions."
D09-1010,1,we are investigating if this is a valid algorithm for two general directed acyclic graphs.
D09-1011,4,"our modeling approach is potentially applicable to a wide range of other tasks, including transliteration, phonology, cognate modeling, multiplesequence alignment and system combination."
D09-1011,1,our work ties into a broader vision of using algorithms like belief propagation to coordinate the work of several nlp models and algorithms.
D09-1012,1,"in our future work, we plan to widen the list of covered tasks and to extend our algorithm to cope with different kernel families, such as the partial tree kernel and kernels defined over pairs of trees, e.g. the ones used for textual entailment in (moschitti and zanzotto, 2007)."
D09-1012,1,"we also plan to move from mining fragments to mining classes of fragments, i.e.to identify prototypical fragments in the fragment space that generalize topological sub-classes of the most relevant fragments."
D09-1013,1,"for further improvement, we need to analyze the cause of these false negatives more deeply, and design a more discriminative feature space."
D09-1014,4,"in the future, we plan to surpass supervised accuracy by applying our method to millions of parallel record-text pairs collected automatically using matching."
D09-1014,1,we also want to explore the addition of markov dependencies into our alignment model and other constraints such as monotonicity and one-to-one correspondence.
D09-1015,5,"while most ner corpus designers have defenestrated embedded entities, we hope that in the future this will not continue, as large amounts of information are lost due to this design decision."
D09-1017,2,the future work focuses in two directions: (1) building a relational database from the summaries and ratings and using it to enhance users’ experiences in a multimodal spoken dialogue system; and
D09-1017,4,(2) applying our techniques to other domains to demonstrate generality.
D09-1018,1,the implementation and comparison of the two methods under full automation is the focus of our future work.
D09-1019,5,"although there are some special conditional sentences that do not use easily recognizable conditional connectives and identifying them are useful, such sentences are very rare and spending time and effort on them may not be cost-effective at the moment."
D09-1019,5,"in our future work, we will further improve the classification accuracy and study related problems, e.g., identifying topics/features."
D09-1020,2,"we will manually annotate a moderate number of strategically chosen words, namely frequent ones which are highly ambiguous."
D09-1020,5,we plan several future directions which promise to further increase the impact of swsd on subjectivity and sentiment analysis.
D09-1020,1,"in addition, we will add features to the swsd system reflecting the subjectivity of the surrounding context."
D09-1020,1,"finally, there are more sophisticated strategies to explore for improving subjectivity and sentiment analysis via swsd than the simple, intuitive rules we began with in this paper."
D09-1021,1,a key area for future work will be further development of the discriminative dependency model (section 4.1).
D09-1022,1,"for the trigger-based model, we plan to investigate more model variants."
D09-1022,1,"in future work, we plan to extend the discriminative word lexicon model in two directions: extending context to the document level and feature engineering."
D09-1022,1,"it might be interesting to look at cross-lingual trigger models such as p(f|e, f&apos;) or constrained variants like p(f|e, e&apos;) with pos(e&apos;) &lt; pos(e), i.e. the second trigger coming from the left context within a sentence which has already been generated."
D09-1023,1,we believe one cause for this performance gap is the generation of the lattice and plan to address this in future work by allowing the phrase table to inform lattice generation.
D09-1025,2,"on entity extraction, exploring more knowledge extractors from different sources (such as the html tables and query log sources used for our features) is promising."
D09-1025,5,"other feature types may potentially capture other aspects of the semantics of entities, such as wordnet and search engine click logs."
D09-1025,6,There is ample directions for future work.
D09-1026,1,"because labeled lda is a graphical model in the lda family, it enables a range of natural extensions for future investigation."
D09-1027,3,"investigate the performance of the method onother types of documents, such as long articles, product reviews and news;"
D09-1027,1,investigate the feasibility of combining cooccurrence-based and wikipedia-based relatedness for clustering; 3.
D09-1027,1,"the solution of using frequent word list for filtering out too common single-word keyphrases is undoubtedly simple, and we plan to make a better combination of the clustering-based method with traditional frequency-based methods for keyphrase extraction."
D09-1028,4,"automated acquisition of spatial data can significantly help many nlp tasks, e.g., question answering."
D09-1028,1,"we would also like to incorporate some patterns based on (egenhofer and shariff, 1998), such as ‘crosses’, ‘goes through’ or ‘runs into’, which may allow automated acquisition of complex spatial relationships."
D09-1028,1,"finally, we would like to incorporate in our framework mod-ules which may allow recognition of structured data, like those developed by (schockaert et al., 2008)."
D09-1029,2,"anyway, the english version has proved to be more precise, while the resource for new languages would require a more accurate revision."
D09-1029,2,the resource can be produced and made available with a reduced effort for every language in wikipedia.
D09-1029,2,the retrieved sentences will be made available as training or annotation material.
D09-1029,3,"in the next research step, we plan to carry out an extended evaluation process in order to compute inter-annotator agreement and eventually point out validation problems."
D09-1029,4,"then, we want to extend the mapping and the data extraction process to all (f, l) pairs in framenet (about 10,000)."
D09-1029,6,"besides, we want to create an online resource where the links between (f, l) pairs and wikipages are made explicit and where users can browse the retrieved sentences."
D09-1033,1,"while our initial studies in this direction were negative, careful feature engineering might lead to better results."
D09-1033,1,"while this is already pretty good, a more sophisticated model might lead to further improvements."
D09-1033,1,"for example, one could experiment with linguistically more informed features."
D09-1033,1,"future work should look at improving the supervised classifier, which so far has an accuracy of 90%."
D09-1037,1,"a second avenue is to develop better means of inference under the grammar, in order to ensure faster mixing and a means to escape from local optima."
D09-1037,1,with these extensions we expect that our model of grammar induction has the potential to greatly improve translation output.
D09-1037,1,"finally, we wish to develop a method for decoding under the full bayesian model, instead of the current beam search."
D09-1037,1,"one avenue is to develop a more sophisticated prior over rules, e.g., one that recognizes common types of rule via the shape of the tree and ordering pattern in the target."
D09-1038,1,we think that the cost of a binary rule can be better estimated by taking the rules with different source-sides into account.
D09-1043,1,"finally, the perceptron model paves the way for exploring the utility of richer feature spaces in statistical realization, including the use of linguistically-motivated and non-local features, a topic which we plan to investigate in future work."
D09-1045,1,we also plan to investigate more sophisticated composition models that take syntactic structure into account.
D09-1045,1,"an interesting future direction would be to optimize the vector components of the probabilistic model over a suitable training corpus, in order to derive a vector model of semantics adapted specifically to the task of composition."
D09-1046,1,"second, the vector space model we used was very simple; it might be worthwhile to test more sophisticated one-class classifiers (marsland, 2003; schoellkopf et al., 2000)."
D09-1046,1,in the future we plan to investigate features that are more informative for making graded judgments.
D09-1048,3,it would be interesting to test our algorithm on other domains and other languages to conclusively establish the effectiveness of parameter projection for multilingual wsd.
D09-1048,5,it would also be interesting to analyze the contribution of corpus and wordnet parameters independently.
D09-1049,3,"methods for gathering enough positive instances of such mwes will be useful for testing the methods proposed here, as well as for general mwe research."
D09-1049,1,another direction is a more sophisticated corpus sampling algorithm.
D09-1049,1,"future research should experiment with nonverbal mwes, since our features are not specific to verbal mwe types."
D09-1052,1,we plan to extend these ideas to structured problems with exponentially many labels and develop methods that efficiently model label correlations.
D09-1053,1,robustness deserves more investigation and forms one area of our future work.
D09-1053,1,"another family of model adaptation methods that we have not studied in this paper is transfer learning, which has been well-studied in the machine learning community (e.g., caruana, 1997; marx et al., 2008)."
D09-1054,1,"our future work includes: (a) to summarize threads and represent the forum threads in question-context-answer triple, which will change the organization of online forums; and"
D09-1054,1,"(b) to enhance qa services (e.g., yahoo!answers) by the contents extracted from online forums."
D09-1055,4,we are currently extending our work to a variety of exact match features and different sources of clickthrough logs.
D09-1061,4,"second, since none of the steps in our method is specifically designed for sentiment classification, we plan to apply it to other non-topic-based text classification tasks."
D09-1061,1,"for instance, instead of having the user construct a relevant feature space from scratch, she can simply extend the set of informative features identified for the user-selected dimension."
D09-1061,1,"first, we plan to use our user-feedback method in combination with existing methods (e.g., bekkerman et al.(2007)) for improving its performance."
D09-1061,1,"in future work, we plan to explore several extensions to our proposed method."
D09-1062,1,the positive results from our experiments encourage further research for lexical resource adaptation techniques.
D09-1063,2,"theoretically, a much larger turney–littman lexicon can be created even though it may be computationally intensive when working with 100 billion words.we are also developing methods to leverage the information in an english thesaurus to create semantic orientation lexicons for a low-resource language through the use of a bilingual lexicon and a translation disambiguation algorithm."
D09-1065,2,"and as the discriminative accuracy of crosstraining techniques improves, further insights into the relative validity of corpus representations will be attainable."
D09-1065,2,"in future research, we plan to extract richer models from larger corpora."
D09-1066,4,"in our own future work, we are especially interested in using higher-order windowless association measures for retrieving paradigmatic relations as well as exploring their use in various nlp applications."
D09-1067,3,"finally, we plan to conduct a bigger experiment with a larger number of verbs, and conduct evaluation in the context of practical application tasks."
D09-1067,1,"in addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring sps for verb classification."
D09-1067,1,"the number and type (and combination) of grs for which sps can be reliably acquired, especially when the data is sparse, requires also further investigation."
D09-1067,1,"in addition, we plan to investigate other potentially useful features for verb classification (e.g. named entities and preposition classes) and explore semi-automatic ml technology and active learning for guiding the classification."
D09-1068,1,"we are currently investigating document-side analysis to complement the queryside work, and believe that this will further boost the retrieval accuracy; we hope to report on this in a follow-up study."
D09-1069,4,we hope that our answer promotes research on phoneme-based english-to-chinese transliteration.
D09-1070,1,"in particular, we will move from the use of document boundaries to a flexible notion of textual distance to estimate likelihood of morphological relatedness."
D09-1071,4,"on the implementation side, it would be interesting to see how our methods scale in a distributed map-reduce architecture where network communication overhead becomes an issue."
D09-1071,1,we will need new breakthroughs to unleash the full potential of unsupervised learning for nlp.
D09-1072,5,how the system can be adapted to work for other languages and 2) how to automatically obtain the knowledge of functional elements.
D09-1072,6,we are certainly aware that our work does not yet address two problems: 1).
D09-1073,1,it is able to work with any long phrase pairs with gap of any length in-between.
D09-1073,1,we would also like to use one individual tree kernel for one partition in a structured feature space.
D09-1074,1,pushing the weight estimation at the alignment link level will alleviate this problem and will make the discriminative training more targeted.
D09-1075,1,"we also used the most simple word-alignment model, but more complex word alignment models could be incorporated into our bilingual model."
D09-1077,4,future work will explore ways to apply additional features of these systems or other sources of information to account for the remainder of the performance gap.
D09-1078,3,"as our next steps, first, we need to verify that the results obtained on a moderate-sized training corpus are repeatable on much larger corpora."
D09-1078,1,"second, we plan to extend this work to incorporate language model size reduction by word clustering, which has been shown by goodman and gao (2000) to produce additional gains when combined with previous methods of language model pruning."
D09-1079,5,"for example, can we use small randomised representations called sketches to compactly represent side-information on the stream telling us which aspects of it we should insert into our data?"
D09-1079,5,how can we efficiently deal with smoothing in this setting?
D09-1080,3,future investigators should evaluate the gains possible by integrating this information into the features and ideas presented here.
D09-1081,1,"we are now interested in improving semantic distance measures for verb–verb, adjective– adjective, and cross-part-of-speech pairs, by exploiting specific information pertaining to these parts of speech in lexical resources in addition to purely co-occurrence information."
D09-1082,2,"furthermore, since the pas is usually a bag of unconnected graphs, we could find a way to joint them together, in order to consider both inter- and intra- sentential inferences based on it."
D09-1082,1,"even more, we also plan to compare/combine it with other methods which are not based on overlapping information between t and h."
D09-1082,1,"for future work, we would like to see whether the pas can help the second-stage classification as well, e.g. the semantic dependency of negation (am-neg) could be helpful for the contraction recognition."
D09-1083,4,"in the future, we plan to explore more vector operations other than the inner-product (i.e., cosine) as well as different functional forms of the termweighting function (e.g.log-linear instead of linear)."
D09-1083,4,"in terms of applications, we would like to apply tweak in other problems such as paraphrase recognition and nearduplicate detection."
D09-1086,1,we are exploring methods that incorporate a packed parse forest on the source side and similar representations of uncertainty about alignments.
D09-1087,2,"in future work, we plan to scale up the training process with more unlabeled training data (e.g., gigaword) and investigate automatic selection of materials that are most suitable for self-training."
D09-1087,1,"finally, it is also important to explore other ways to exploit the use of unlabeled data."
D09-1087,4,we also plan to investigate domain adaptation and apply the model to other languages with modest treebank resources.
D09-1088,1,"in particular we wish to examine whether parsing different languages should be pursued by different models, or whether the rr strategy can effectively cope with different languages types."
D09-1088,1,in the future we plan to investigate how the different models fare against one another in parsing different languages.
D09-1088,1,"finally, we wish to explore the implications of rr modeling for applications that consider the form of expression in multiple languages, for instance statistical machine translation (smt)."
D09-1093,3,"furthermore, future work will involve evaluating the performance of the system for these language on real typed data."
D09-1093,6,it may be beneficial to perform more tuning in future.
D09-1095,1,"since selectional preferences acquired from sense-untagged corpora have worked well for the metonymy resolution task, we plan to push further towards unsupervised metonymy resolution, putting to use the lessons learned from unsupervised wsd."
D09-1095,1,"we plan to expand on this, and find methods to extract more such features automatically, without manually provided clues."
D09-1096,1,"in future work, we plan to determine the effectiveness of using a sequential learning algorithm like conditional random fields (crf)."
D09-1097,1,future work will be to filter out difficult hypernyms for hyponymy extraction process to achieve higher precision.
D09-1100,1,"in the future we hope to explore ways to interleave semantic analysis with exploration of the learning domain, by using the environment as a supervision signal for linguistic analysis."
D09-1103,1,"in particular, we will further employ the centering theory in pronoun resolution from both grammatical and semantic perspectives on more corpora."
D09-1103,1,"for future work, we will explore more kinds of semantic information and structured syntactic information in pronoun resolution."
D09-1105,3,"it would be interesting to compare the speed and accuracy of our dynamic-programming localsearch method with an exact algorithm for solving the lop, such as integer linear programming with branch and bound (cf.charon and hudry (2006))."
D09-1105,1,"that possibility remains future work, but it is likely to lead to further improvements, because it allows the translation system to consider multiple possible reorderings under the model, as well as to tune the weight of the model relative to the other parts of the system during mert."
D09-1106,4,we expect that these syntax-based systems could benefit more from our approach.
D09-1106,4,"another interesting direction is applying our approach to extracting translation rules with hierarchical structures such as hierarchical phrases (chiang, 2007) and tree-to-string rules (galley , 2006; liu , 2006)."
D09-1106,1,we believe that better estimation of alignment distributions will result in more significant improvements.
D09-1109,4,another issue which is worth a deeper investigation is the application of fourier transform methods which offer tools for studying the periodic structure of the temporal sequences.
D09-1109,6,this agrees with the intuition that time provides a fundamental semantic dimension possibly orthogonal to broad topical classification.
D09-1109,6,this issue however deserves further investigation.
D09-1110,4,"our efficient representation of the consequent search space opens the way to future investigation of the benefit of larger-scale rule chaining, and to the development of efficient search strategies required to support such inferences."
D09-1111,1,"next, we plan to combine our models with a classifier that makes such a decision, allowing us to integrate transliteration into smt for other language pairs."
D09-1112,1,"additionally, term similarity kernels, e.g.(basili et al., 2005; bloehdorn et al., 2006), will be likely improve our models, especially when combined syntactic and semantic kernels are used, i.e.(bloehdorn and moschitti, 2007a; bloehdorn and moschitti, 2007b)."
D09-1112,1,"kernel methods show that combinations of feature vectors, sequence kernels and other structural kernels, e.g. on shallow or deep syntactic parse trees, appear to be a promising future research line3."
D09-1112,1,"in the future we would like to extend this research by focusing on advanced shallow semantic approaches such as predicate argument structures, e.g.(giuglea and moschitti, 2004; moschitti and cosmin, 2004; moschitti et al., 2008)."
D09-1113,5,"therefore, an interesting topic is how to further reduce the inconsistency between skipabove pairs and human labeling so that such data may also be useful for task-specific ranking."
D09-1114,4,"in the future, we will explore more possibilities for feature subspaces selection and experiment with our method in a word-level system combination model."
D09-1116,2,"finally, we plan to apply the technique to other languages with treebanks, such as chinese and arabic."
D09-1116,3,"we also plan on evaluating the models performance on other genres of speech, as well as in other tasks such as machine translation."
D09-1116,6,we intend to release the source code of our model within several months of this publication.
D09-1116,1,"we plan to investigate how parser accuracy and data selection strategies, e.g., based on parser confidence scores, impact the performance of our model."
D09-1116,1,we are also working on scaling our model further to accommodate amounts of data typical for modern large-scale ngram models.
D09-1117,1,in the future we would like to explore the possibilities created by more tightly coupling the forward and reverse components of the bidirectional decoder.
D09-1117,1,"scores from partial hypotheses of both processes could be combined and used at each step of the decoding, making the search more informed."
D09-1117,1,"furthermore, forward partial hypotheses and reverse hypotheses would ‘meet’ during decoding (when one decoding direction has covered words in the source that the other has yet to cover), and provide paths for each other to a final state in the search."
D09-1118,1,we will also experiment with using sequential models in order to try to exploit any sequential ordering patterns in the occurrence of the ddas.
D09-1118,1,"in future work, we plan to extend the decision discussion annotation scheme and try to extract supporting arguments for decisions."
D09-1120,1,"indeed, we suspect that further improving the syntactic and semantic modules in our system may produce greater error reductions than any other route forward."
D09-1120,1,"of course, a system which is rich in all axes will find some advantage over any simplified approach."
D09-1121,1,"with the obtained robust information, we will explore rewarding ways for parser improvements."
D09-1121,1,"in our future work, we will improve the performance of our approaches by adding more patterns for the descriptive approach and by handling uncorrectable errors for the empirical approach."
D09-1123,1,future work will attempt further extensions of our ddtm system to allow for the exploitation of long-range aspects of the dependency structure.
D09-1123,1,we will work on expanding the features set of ddtm system to leverage features from the constructed dependency structure itself.
D09-1123,1,"finally, we will work on enabling the deployment of source side dependency structures to influence the construction of the target dependency structure based on a bilingually enabled dependency parsing mechanism using the discriminative modeling capabilities."
D09-1127,4,"furthermore, we believe this bilingual-monolingual approach can easily transfer to shift-reduce constituency parsing (sagae and lavie, 2006)."
D09-1127,1,"so we will engineer more such features, especially with lexicalization and soft alignments (liang et al., 2006), and study the impact of alignment quality on parsing improvement."
D09-1127,1,"from a linguistics point of view, we would like to see how linguistics distance affects this approach, e.g., we suspect english french would not help each other as much as english-chinese do; and it would be very interesting to see what types of syntactic ambiguities can be resolved across different language pairs."
D09-1129,6,in this way we will be able to correct grammar errors too.
D09-1129,1,"we also plan more experiments using the 5-grams, but backing off to 4-grams and 3-grams when needed."
D09-1129,1,"in future work, we plan to extend our method to allow for deleted or inserted words, and to find the corrected strings in the google web 1t n-grams."
D09-1130,5,"in the future, we plan to explore other related problems such as adjacency pairs (levinson, 1983) and discourse parsing (soricut and marcu, 2003) for large-scale online forum data."
D09-1131,1,"now we define these functions intuitively based on linguistic rules, but learning methods like regression will be investigated in the future."
D09-1131,1,examining the interaction of cues from word and sentence levels on the opinion sentence extraction and the opinion polarity detection is our next goal.
D09-1132,4,"we are, thus, experimenting with the possibility of obtaining short definitions from the web, using the system we presented."
D09-1134,4,"in the future, we would like to see the application of our approach to other tasks such as (li , 2009)."
D09-1136,1,"because our bayesian model employs a very simple prior, more sophisticated generative models provide a possible direction for further experimentation."
D09-1138,4,"one possible direction for this research topic would be to use our model for the semi-automatic construction of verb lexicons, with the help of human curation."
D09-1138,1,"however, there is also a demand for exploring other types of features that can discriminate among confusing classes."
D09-1139,2,"in our future work, we plan to extend our analysis to other test collections and to query expansion methods in order to generalize our conclusions."
D09-1139,5,"as the problem of language ambiguity has a high impact on the use of sr measures, we will also consider word sense disambiguation in our future experiments."
D09-1140,4,we plan to apply the methods described here to these other applications in the near future.
D09-1142,5,"it would be interesting to verify whether gender recognition can be boosted by using lexical resources that capture the semantics of the words, such as wordnets or knowledge extracted from wikipedia, and verify whether similarities from a semantic point of view are also responsible for gender assignments in various languages."
D09-1142,5,"we plan to look closer at cases where we obtain different predictions based on the word ending and the full form of the word, and use boosting to learn weights for classifiers based on different parts of the word to see whether we can further improve the results."
D09-1143,5,"regarding future work, there are many research line that may be followed: i) capturing more features by employing external knowledge such as ontological, lexical resource or wordnet-based features (basili , 2005a; basili , 2005b; bloehdorn , 2006; bloehdorn and moschitti, 2007) or shallow semantic trees, (giuglea and moschitti, 2004; giuglea and moschitti, 2006; moschitti and bejan, 2004; moschitti , 2007; moschitti, 2008; moschitti , 2008)."
D09-1143,1,"from constituent trees, we can extract subtrees constituted by non-terminal symbols (grammar symbols), which provide a better generalization (with a risk of underfitting).iii) design a new kernel which can integrate the advantages of the constituent and dependency tree."
D09-1143,1,from dependency trees we can extract more precise but also more sparse relationships (which may cause overfit).
D09-1143,1,"the new tree kernel should inherit the benefits of the three available tree kernels: st, sst or pt."
D09-1143,5,"regarding future work, there are many research line that may be followed: i) capturing more features by employing external knowledge such as ontological, lexical resource or wordnet-based features (basili et al., 2005a; basili et al., 2005b; bloehdorn et al., 2006; bloehdorn and moschitti, 2007) or shallow semantic trees, (giuglea and moschitti, 2004; giuglea and moschitti, 2006; moschitti and bejan, 2004; moschitti et al., 2007; moschitti, 2008; moschitti et al., 2008).ii) design a new tree-based structures, which combines the information of both constituent and dependency parses."
D09-1144,2,then we intend to compare output results of the procedure with the human responses.
D09-1144,2,"in the future we plan to experiment with large role-annotated corpora for english such as propbank (approx.300 000 words, (palmer , 2005)) and the framenet-annotated corpus provided by the fn project (more than 135 000 annotated sentences, (ruppenhofer , 2006))."
D09-1144,2,"in the future we plan to experiment with large role-annotated corpora for english such as propbank (approx.300 000 words, (palmer et al., 2005)) and the framenet-annotated corpus provided by the fn project (more than 135 000 annotated sentences, (ruppenhofer et al., 2006))."
D09-1144,1,in the future we plan to implement a procedure making use of the extracted ap-relations which would automatically extend phrases containing implicit predicates.
D09-1144,1,"additionally, a study of a possible correspondence between human agreement on associated predicates and a semantic type of an argument (e.g. concrete/abstract, natural kind/artifact) should be performed on more test arguments."
D09-1146,4,"certainly, there are many other possible applications of this model, including product comparison, media bias detection, and interdisciplinary literature analysis."
D09-1146,4,"cultural awareness is also important in marketing and we can use this model to investigate, for example what products and what aspects of life people in different regions focus on."
D09-1146,1,"in future work, we would like to enrich the model and/or feature set to move beyond the limitations of a bag-of-words analysis."
D09-1146,1,"for example, by considering negation and word polarity, we can better capture the opinions of the authors, which is an important component of such cultural analysis."
D09-1148,4,"similar methods could also be used to personalize search (teevan , 2008); for queries that mean different things to different people, the yarowsky method could be applied to variables such as user, time and place, so the results reflect what a particular user intended in a particular context."
D09-1148,6,"in addition to click type, there are many other features in the logs that could prove useful for classifying queries by intent, e.g., who issued the query, when and where."
D09-1148,1,"similar methods could be applied in future work to many other applications such labeling queries and urls by: language, market, location, time, intended for a search vertical (such as medicine, recipes), intended for a type of answer (maps, pictures), as well as inappropriate intent (porn, spam)."
D09-1150,4,"more applications also will be explored, such as emotional summarization, emotional question answering; emotional topic discovering."
D09-1150,5,"at the same time, new research problems will arise, for examples, how to acquiring more emotional words and to generate their emotional vectors automatically; how to generate emotional vectors for sentences, paragraphs and documents with known emotional elements in them?"
D09-1151,1,"as future work, we plan to consider referential properties of noun phrases in associative anaphora resolution."
D09-1152,5,we will need to handle more kinds of quantifiers in our mln model.
D09-1152,5,integrating our technique for qsd with discourse processing is a major challenge that we hope to address.
D09-1154,1,"our next goal is to use and evaluate the term variation collected by the proposed method in an actual search scenario, as well as improving the performance of our classifier by using individual, character-dependent edit operations as features in classification."
D09-1157,2,"also, the predictions with high confidence can be used as seed training material for automatically harvesting more training data."
D09-1157,1,"nevertheless, we combined the two methods in a rather crude way, leaving ample room for exploring better strategies in the future."
D09-1160,5,"we will in the future consider an issue of speeding up decoding with structured models (lafferty , 2001; miyao and tsujii, 2002; sutton , 2004)."
D09-1161,1,"in the future, we will explore more features and study the forest-based combination methods for syntactic parsing."
D09-1162,2,"in future work, we will try other word combinations and investigate better ways to represent the chinese text."
D09-1162,1,"in addition, we will explore how to utilize the better chinese sentence-level novelty mining result to improve the detection performance on documents."
D09-1163,3,we are planning to explore these methods to compare their performance.
D09-1163,1,"future work will also include the comparison between our methods with other related approaches, such as kurland and lee’s cluster based approach (kurland and lee, 2006).we are planning to explore these methods to compare their performance."
D09-1163,1,also direct re-ranking can be used to improve automatic query expansion since better ranking in top retrieved documents can be expected to improve the quality of the augmented query.
D10-1002,5,it would also be interesting to determine whether further increasing the accuracy of the model used for automatically labeling the unlabeled data can enhance performance even more.
D10-1002,6,"finally, for this work, we always used products of 10 grammars, but we sometimes observed that subsets of these grammars produce even better results on the development set."
D10-1002,1,"in future work, we plan to investigate additional methods for increasing the diversity of our self trained models."
D10-1002,1,finding a way to select grammars from a grammar pool to achieve high performance products is an interesting area of future study.
D10-1002,1,one possibility would be to utilize more unlabeled data or to identify additional ways to bias the models.
D10-1002,1,a simple but computationally expensive way to do this would be to parse the data with an sm7 product model.
D10-1004,1,"other parsing formalisms can be handled with the inventory of factors shown here鈥 among them, phrase-structure parsing."
D10-1004,1,"recent progress in message-passing algorithms yield 鈥渃convexified鈥 bethe approximations that can be used for marginal inference (wainwright et al., 2005), and provably convergent max-product variants that solve the relaxed lp (globerson and jaakkola, 2008)."
D10-1004,6,There are several possible directions for future work.
D10-1005,5,how sentiment prediction impacts the implicit wsd is left to future work.
D10-1005,1,"better capturing local syntax and meaningful collocations would also improve the model鈥檚 ability to predict sentiment and model multilingual topics, as would providing a better mechanism for representing words not included in our bridges."
D10-1005,1,we intend to develop such models as future work.
D10-1006,4,"one of the future directions we plan to explore is to use this model to help sentence-level extraction of specific opinions and their targets, which previously was only tackled in a fully supervised manner."
D10-1006,1,another direction is to extend the model to support polarity classification.
D10-1010,5,the questions that are posted on community qa sites often contain spelling or grammatical errors.
D10-1010,1,"consequently, we will work on interfacing the question ranking system with a separate module aimed at fixing orthographic and grammatical errors."
D10-1010,1,we also plan to make the dependency tree matching more flexible in order to account for paraphrase patterns that may differ in their syntactic structure.
D10-1010,1,we plan to integrate context dependent word similarity measures into a more robust question utility function.
D10-1011,4,"in future work, we intend to expand our analysis of the distribution of peco elements to a larger number of citations."
D10-1011,1,"one way to do that would be to automatically extract pubmed citations that contain structural markers associated to peco categories (chung, 2009)."
D10-1012,1,we also aim to implement a number of word sense induction algorithms and compare them in the same evaluation framework with more web search and web clustering engines.
D10-1012,1,"as regards future work, we intend to combine our clustering algorithm with a cluster labeling algorithm."
D10-1012,1,"finally, it should be possible to use precisely the same approach presented in this paper for document clustering, by grouping the contexts in which the target query occurs 鈥 and we will also experiment on this in the future."
D10-1013,3,"however, in future work we plan to substitute our own statistical mt systems, which will permit us to experiment across a range of translation model and language model lm training set sizes, and therefore to vary quality while keeping other system details constant."
D10-1013,4,"finally, we intend to explore the application of our approach in scenarios involving less-common languages, by using a more common language as a pivot or bridge (habash and hu, 2009)."
D10-1013,5,"one important step will be to better characterize the relationship between cost and quality in quantitative terms: how much does it cost to obtain how much quality improvement, and how does that compare with typical professional translation costs of $0.25 per word?"
D10-1013,5,"we hope to explore whether the targeted paraphrasing translation pipeline can improve the productivity of post-editing by bilinguals, making it easier to move toward the upper bound in a cost-effective way."
D10-1013,6,these initial studies leave considerable room for future work.
D10-1013,1,"we plan to implement a fully automatic targeted paraphrasing translation pipeline, using the automated methods discussed when introducing the pipeline in section 2, including translation of targeted paraphrase lattices (cf.(max, 2010; du et al., 2010))."
D10-1014,4,we will continue this line of research and exploit better ways to learn syntax and apply syntactic constraints to machine translation.
D10-1015,5,"we think we have built a solid framework for the second challenge, and we plan to extend it further."
D10-1016,2,"since our goal is not to be literal, but to obtain a satisfactory compromise between form and meaning, it would clearly be beneficial to augment target phrases with synonyms and paraphrases, or to allow for words to be dropped or added."
D10-1021,6,this again will be an interesting future work.
D10-1021,1,"also, efs being a useful method for feature selection in machine learning, it would be useful to perform further experiments to investigate how well it performs on a variety of classification datasets."
D10-1022,6,"finally, we would like to point out that it is conceivable that negative training data could still be useful in many cases."
D10-1022,1,an interesting direction to explore is to somehow combine the extracted reliable negative data from the unlabeled set and the existing negative training data to further enhance learning algorithms.
D10-1022,1,"in our future work, we plan to do more comprehensive experiments to compare the classic supervised learning and pu learning techniques with different kinds of settings, for example, by varying the ratio between positive and negative examples, as well as their sizes."
D10-1022,1,it is also important to explore how to catch the best iteration of the svm/nb classifier in the iterative running process of the algorithms.
D10-1024,5,"as the quality of models increases on these noisy datasets, we anticipate a consequent rise in their usefulness to researchers and historians as browsing the data and mining it for useful patterns becomes more efficient and profitable."
D10-1025,4,"as future work, dda could be applied to mapping documents in many languages simultaneously."
D10-1025,1,a similar model could be used for cplsa: future work will show whether such a model can outperform opca.
D10-1027,4,for future work we would like to apply this algorithm to forest-based translation and hierarchical system by pruning the first-pass 鈭扡m forest.
D10-1027,1,"we would also combine cube pruning with our incremental algorithm, and study its performance with higher-order language models."
D10-1028,3,"we plan to investigate this further, and a natural follow-on would be to experiment with adaptation for this variety of latent structure, to produce an adapted lspm-like model analogous to adaptive naive bayes."
D10-1028,1,another promising direction for this work is the application of adaptor grammar models as a way to capture both lexical and grammatical aspects of framing in a unified model.
D10-1029,2,"one source of information that has not previously been exploited is the lexical fixedness (fazly , 2009) of noncompositional prefix verbs."
D10-1029,6,"on the other hand, marry again is relatively frequent, indicating that remarry is compositional."
D10-1029,1,incorporation of these and other n-gram counts could further improve classification accuracy.
D10-1029,1,there are also other n-gram-derived features that warrant further investigation.
D10-1032,2,an obvious direction for future work is to expand the annotated corpus and improve the algorithm by experimenting with additional features.
D10-1032,2,"in fact, tsd can assist textual entailment as well, since the sense of a tense form may provide substantial information about the relations entailed from the sentence."
D10-1032,1,using tsd in such applications is a major direction for future work.
D10-1033,2,"chief among directions for further work is to continue to improve performance on noisy data, and to strengthen our findings via larger data sets."
D10-1033,4,"additionally, we look forward to expanding analysis to different types of imperfect input, such as machinetranslation output, different types of mark-up, and different genres of real data."
D10-1034,1,"for the future work, it is possible to adapt our one-level clustering-based sampling to the multilevel one, where for every stratum it is still possible to divide it into lower sub-strata for further stratified sampling in order to make the seeds better represent the true distribution of the data."
D10-1035,6,there are numerous avenues for further examination.
D10-1035,1,"we intend to use sophisticated clustering methods, such as cbc (pantel, 2003), to identify multiple negative categories across the target categories in a single iteration."
D10-1035,1,we would also like to explore the suitability of neg-finder for relation extraction.
D10-1036,3,we will investigate the influence of corpus selection in training lda for keyphrase extraction using tpr.
D10-1036,5,"in fact, the learned topics are highly dependent on the learning corpus."
D10-1036,1,"we design to obtain topics using other machine learning methods and from other knowledge bases, and investigate the influence to performance of keyphrase extraction.2."
D10-1036,1,"we plan to consider topic information in other graph-based ranking algorithms such as hits (kleinberg, 1999).3."
D10-1038,4,"we plan to work on both synchronous (e.g., chats, meetings) and asynchronous (e.g., blogs) domains."
D10-1038,4,we are also interested in the near future to transfer our approach to other similar domains by hierarchical bayesian multi-task learning and other domain adaptation methods.
D10-1038,4,"as a next step for this research, we will investigate how to exploit these features in our methods."
D10-1041,1,"secondly, we plan to experiment with another feature function in the log-linear model to discount words derived from paraphrases, and use mert to assign an appropriate weight to this feature function."
D10-1041,1,"as for future work, firstly we plan to propose a pruning algorithm for the duplicate paths in the lattice, which will track the edge generation with respect to the path span, and thus eliminate duplicate paths."
D10-1043,3,"in the future, we are interested in testing our algorithm at forest-based tree sequence to tree sequence translation."
D10-1044,1,"finally, we intend to explore more sophisticated instance weighting features for capturing the degree of generality of phrase pairs."
D10-1044,1,"in future work we plan to try this approach with more competitive smt systems, and to extend instance weighting to other standard smt components such as the lm, lexical phrase weights, and lexicalized distortion."
D10-1044,1,we will also directly compare with a baseline similar to the matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.
D10-1047,1,we also plan to investigate using other metrics in training in order to reduce redundant information in the summaries.
D10-1047,1,we will investigate various ways of incorporating these global features into our a* search.
D10-1047,1,however this will incur an additional computational cost over a purely local feature model and therefore may necessitate using an approximate beam search.
D10-1047,1,"in the future we plan to expand this feature set with global features, especially ones measuring lexical diversity in the summaries to reduce the redundancy in them."
D10-1048,4,our code is publicly released5 and can be used both as a stand-alone coreference system and as a platform for the development of future systems.
D10-1050,4,"beyond summarization, we would also like to apply our model to other generation tasks, such as paraphrasing and text simplification."
D10-1050,1,"currently we consider deletions, reordering and insertions."
D10-1050,1,"ideally, we would also like to model arbitrary substitutions between words but also larger constituents (e.g., subclauses, sentence aggregation)."
D10-1050,1,"In the future, we plan to explore how to integrate more sophisticated QG rules in the generation process."
D10-1051,6,"rather than translating the source text, a program may instead use the source text for inspiration."
D10-1051,6,"such a hybrid translation/generation program would not be bound to translate every word, but rather it could more freely combine lexical material from its translation tables with other grammatical and lexical resources."
D10-1051,6,"interestingly, human translators sometimes work this way when they translate poetry any excellent works have been produced by people with very little knowledge of the source language."
D10-1051,1,an appealing future direction is to combine translation and generation.
D10-1054,3,"in the future, we will carry out experiments on deeper features and evaluate the effects of different feature sets."
D10-1055,1,we expect the corpus to also prove useful for feature engineering and error analysis in developing better realization models.
D10-1055,1,"in future work, the performance of the ter family of metrics on this data might be improved by optimizing the edit weights used in computing its scores, so as to avoid over-penalizing punctuation movements or under-penalizing agreement errors, both of which were significant sources of ranking errors."
D10-1056,1,"these promising results suggest a new direction for future research: improving pos induction by developing methods targeted towards extracting better prototypes, rather than focusing on improving clustering of the entire data set."
D10-1057,1,"our methods are promising as tools to accompany the deployment of domain adaptation algorithms, so that a complete system can first identify when a domain shift has occurred before automatically adapting to the new domain."
D10-1059,1,"a possible direction to investigate in the future consists in generalizing this hybrid strategy and combining random samples where the probability distribution induced on the lattice by the current parameters is scaled by a further temperature parameter q: p鈥(e, a|f) a p(e, a|f),3 (7) where for q = 1 the random samples used in this paper are obtained, for q tending to infinite the distribution becomes peaked around the single best path, thus producing samples similar to n-best lists, and samples from other real values of the temperature can be combined."
D10-1060,1,"in future, we will explore various learning methods for better estimation of families, templates and lexical items."
D10-1060,1,the target linguistic knowledge that we used in this paper will provide a nice starting point for unsupervised learning algorithms.
D10-1060,1,features defined on templates and families will have good generalization capability.
D10-1060,1,we will also try to further exploit the factorized representation with discriminative learning.
D10-1061,4,"we are now actively exploring the possibility of linking the sample selection front-end to a crowd-sourcing backend, in order to obtain 鈥渘on-expert鈥 translations using a platform such as the amazon mechanical turk."
D10-1062,5,"as always, preprocessing the corpus to address a certain problem in machine translation is less principled than tackling the problem head on by integrating it into the machine translation system itself."
D10-1062,5,"however, there might be some empty elements which are not annotated but nevertheless helpful for improving machine translation."
D10-1062,5,there are several other issues we may consider when recovering empty categories that are missing in the target language.
D10-1062,5,we only considered empty categories that are present in treebanks.
D10-1062,1,"it may be beneficial to include consideration for empty elements in the decoding process, so that it can benefit from interacting with other elements of the machine translation system."
D10-1064,2,"second, we will combine paraphrases obtained via different techniques and resources, which will allow us to also learn translation distributions for phrases absent from the original corpus."
D10-1064,1,"our future work includes three main areas: first, we want to improve the modeling of context, by notably working on techniques inspired from information retrieval to quickly access contextually-similar examples of source phrases in bilingual corpora."
D10-1064,1,"lastly, we want to also exploit paraphrases for the additional translations that they propose (such as e4 on figure 3) and that would be contextually similar in the target language to other existing translations of a given phrase or that could even represent a new sense of the original phrase."
D10-1067,4,"another direction is to apply zl to other nlp tasks and ml areas, supervised and unsupervised."
D10-1067,1,developing a quality assessment algorithm for dependency trees will allow us to apply confidence based zl to unsupervised dependency parsing.
D10-1067,1,"future research should focus on the development of more accurate estimators of parser output quality, and experimentation with different corpora, languages and parsers."
D10-1067,1,"particularly, it will enable us to explore the combination of the methods proposed in (piatkowski et al., 2010) with zl for the dmv model and to integrate the pupa score into their bootstrapping algorithm."
D10-1068,3,"alternatively, it would be interesting so see how much difference it makes to train the tagger on one set of data, and use that to tag a model training set from a different domain."
D10-1068,5,"finally, a user study involving a grammar engineer working on a new language would be useful to validate the results we found here and confirm whether they are indeed helpful in bootstrapping a new grammar."
D10-1068,5,"the issues surrounding what makes a good tagger for this purpose, and how can we best learn one without gold training data, would be one possibly fruitful avenue for further exploration."
D10-1068,5,another interesting slant would be to investigate domain effects of the tagger.
D10-1068,6,there are plenty of directions for further work arising from these results.
D10-1068,1,other methods of incorporating the tagger output could also be investigated.
D10-1070,1,"for the future work, we will explore tree kernel based methods to further improve the performance of scope learning in better capturing the structural information, and apply our parsing approach to other kinds of scope learning."
D10-1071,4,"the current accuracy of of our model, getting the correct answer among the top two choices 96.2% of the time is high enough to be highly useful for tasks such as aiding the manual annotation of arabic text; a more complete automation would require that accuracy for the single top choice."
D10-1073,3,"finally in terms of evaluation, our future work also focuses on evaluating hrgs using a finegrained sense inventory, extending the evaluation on the semeval-2010 wsi task dataset (manandhar , 2010) as well as applying hrgs to other related tasks such as taxonomy learning."
D10-1073,5,this consensus tree might be able to express a larger amount of topological features of the initial undirected graph.
D10-1073,1,"moreover, following the work in (clauset et al., 2008), we are also working on using mcmc in order to sample more than one dendrogram at equilibrium, and then combine them to a consensus tree."
D10-1073,1,"our future work focuses on using different feature types, e.g. dependency relations, second-order cooccurrences, named entities and others to construct our undirected graphs and then applying hrgs, in order to measure the impact of each feature type on the induced hierarchical structures within a wsd setting."
D10-1074,2,a more systematical approach to collect and create a larger set of data is crucial.
D10-1074,4,"finally, as the technology in conversation entailment is developed, its applications in nlp problems should be explored."
D10-1074,6,these applications may provide new insights on the nature of the conversation entailment problem and its potential solutions.
D10-1074,1,"as more techniques in semantic processing (e.g., semantic role) become available, future work should also capture deeper semantics, address pragmatics, and incorporate richer world knowledge."
D10-1074,1,"example applications include information extraction, question answering, summarization from conversation scripts, and modeling of conversation participants."
D10-1074,1,"innovative community-based approaches (e.g., through web) for data collection and annotation can be pursued in the future."
D10-1076,1,our future work will thus aim at studying the connections between our empirical observations and the deep learning framework.
D10-1078,6,investigating this and related options is left for future work.
D10-1078,1,"this would not necessarily preclude the use of an iteration-dependent scaling factor, which would achieve the goal of gradually forcing the tagging to become deterministic."
D10-1078,1,"an alternative would be to estimate the variance for each gaussian separately, as is usually done in em for gaussian mixtures."
D10-1079,4,"finally, future work includes the application of the syromorph methodology to other under-resourced semitic languages."
D10-1079,5,future work will require addressing issues encountered in this corpus.
D10-1079,6,"in addition, there is much to do in getting the overall tag accuracy closer to the accuracy of individual decisions."
D10-1079,1,we leave further feature engineering for the stem tagger and the exploration of possible new morphological tagging techniques for future work.
D10-1080,3,"in future work, we will evaluate the use of break indexes for tagging when there is lexical error."
D10-1080,1,"we would also apply the nbest rescoring method to exploit break indexes in the hmm-la bidirectional model, as this would likely produce further improvements."
D10-1081,1,"as the proposed approach provides significant performance improvements, it could be utilized in the development of more sophisticated novel word induction schemes, e.g. ensemble models trained independently with different data."
D10-1081,1,"of course, we are also going to explore the model鈥檚 potential in the setting of semi-supervised morphological analysis."
D10-1083,1,"we hypothesize that modeling morphological information will greatly constrain the set of possible tags, thereby further refining the representation of the tag lexicon."
D10-1083,1,a promising direction for future work is to explicitly model a distribution over tags for each word type.
D10-1084,1,"in future work, we plan to investigate methods for automatically cleansing the data to remove typos, and taking account of temporal gaps that can sometimes arise in online chats (e.g.in table 2, there is a time gap between c:u22 brb in 1 min and c:u23 thank you for waiting)."
D10-1086,2,"finally, we will devote more on further developing our corpus, with the ultimate mission of annotating all the documents in cbt 6.0."
D10-1086,6,"in the future work, we will consider other situations."
D10-1088,5,"therefore, it is possible that a word could have a problematic lexical entry even if it only occurs in sentences which are assigned a full-span parse."
D10-1088,1,"however, it would be useful to extend it, so it can work with longer n-grams."
D10-1088,1,"for example, a given word could have some reading which is not yet handled in the lexicon only within a particular bi- or trigram."
D10-1090,2,"another alternative is to collect parallel texts against multiple foreign languages, e.g., using europarl (koehn, 2005)."
D10-1090,3,we intend to obtain and evaluate paraphrases generated from real pg systems and compare their performances in a follow-up study.
D10-1090,6,We leave this for future work.
D10-1091,3,"in our future work, we aim at using this ilp framework to systematically assess various search configurations."
D10-1091,1,"we are also concerned by determining how tight is our approximation of the bleu4 score is: to this end, we intend to compute the best bleu-4 score within the n-best solutions of the oracle decoding problem."
D10-1091,1,we plan to explore how replacing non-reachable references with high-score pseudo-references can improve discriminative training of pbts.
D10-1092,1,future work includes extension of the method so that it can outperform conventional methods even for similar language pairs.
D10-1093,2,"also, we plan to experiment our approach on others languages."
D10-1093,3,"in the future, we will consider the experimentation of the developed tool in the generation of various personalized views both in virtual and materialized versions."
D10-1093,4,"also, we plan to put up our system on the web."
D10-1094,6,"improving the system is also in our future work, but orthogonal to the current contribution."
D10-1094,1,we would also like to compare the proposed methods to the quality of a model trained on error-tagged data.
D10-1100,1,we will also investigate the relation between classes of social events and their syntactic realization.
D10-1100,1,"in the future, we will use other parsers (such as semantic parsers) and explore new types of linguistically motivated structures and transformations."
D10-1101,1,"for future work, we might investigate how machine learning algorithms, which are specifically designed for the problem of domain adaptation (blitzer et al., 2007; jiang and zhai, 2007), perform in comparison to our approach."
D10-1102,4,"our approach can, in principle, be applied to any classification task that is well modeled by jointly solving an extraction subtask."
D10-1104,2,"in the future, we plan to automatically construct confusion sets, expand our approach to more verbs and test our approach on a larger size of real data."
D10-1104,1,we also plan to further validate the effectiveness of the srl-derived features under other learning methods like svms.
D10-1104,1,we will try to combine the outputs of several srl systems to make our system more robust.
D10-1105,5,"future work would address on how to systematically predict and recommend the bursty queries using online media, as well as a reasonable evaluation metrics upon it."
D10-1106,1,"directions for future work include inducing longer inference rules, investigating better methods for combining the rules, allowing deeper inferences across multiple rules, evaluating our system on other corpora and devising better techniques for handling word sense ambiguity."
D10-1107,3,our future work will include an evaluation of tarec in the context of textual inference applications.
D10-1108,1,"however, further research is required to develop methods that reliably (a) identify the number of independent perspectives a concept can take (or seems to take in the domain text), and (b) classify any harvested term into one or more of them."
D10-1108,1,"another promising line of research would investigate the combination of the two styles of taxonomization algorithms: first, the one described here to produce an initial (set of) taxonomies, and second, the term-insertion algorithms developed in prior work."
D10-1109,1,"also, since questions on online communities are classified into categories by topic, we plan to perform joint question type inference on function-based taxonomy as well as topic based taxonomy by markov logic."
D10-1109,1,the model will not only capture the relation between patterns and types but also the relation between types in different taxonomy.
D10-1111,1,"finally, in the future we plan to extend our model to perform joint modeling and summarization of ideological discourse."
D10-1112,6,we see two main possibilities of improvement.
D10-1112,1,"this would require a larger, more heterogeneous set of training material for the latter in order to avoid overfitting."
D10-1112,1,"second, a mixed approach could combine the benefits of the word based model with the n-gram model."
D10-1112,1,additional training data could be extracted from the web and automatically annotated with the current model in a semi-supervised approach.
D10-1112,1,"first, the rule base can be extended to better account for lexical exceptions, orthographic variation and irregular morphology."
D10-1113,1,we plan to further investigate how to select or to better aggregate the entire set of features extracted from a context.
D10-1113,1,"ideally, we would like to compute the collective influence of several context words on the target."
D10-1114,3,"towards this end we are currently evaluating two classes of approaches for setting pruning parameters per-word instead of globally: (1) subspace clustering, i.e.unsupervised feature selection (e.g., parsons , 2004) and (2) multiple clustering, i.e.finding feature partitions that lead to disparate clusterings (e.g., shafto , 2006)."
D10-1114,5,with this data it would be interesting to validate the hypothesis that the percentage of features allocated to the background cluster is correlated with the degree of homonymy.
D10-1114,1,"furthermore, it is straightforward to extend the model to a two tier, two-clustering structure capable of additionally accounting for commonalities between arguments."
D10-1114,1,"the basic tiered clustering can be extended with additional background tiers, allocating more expressivity to model background feature variation."
D10-1114,1,applying more principled feature selection approaches to vector-space lexical semantics may yield more significant performance gains.
D10-1114,1,"(future work) the word similarity experiments can be expanded by breaking pairs down further into highly homonymous and highly polysemous pairs, using e.g. wordnet to determine how closely related the senses are."
D10-1114,5,"this class of models covers the spectrum between a pure 6e.g., pad麓o et al.(2007) report p鉁0.515 on the same data. topic model (all background tiers) and a pure clustering model and may be reasonable when there is believed to be more background structure (e.g. when jointly modeling all verb arguments)."
D10-1115,5,it remains to be seen if the approach we proposed will scale up to such challenges.
D10-1115,1,"ultimately, we want to compose larger and larger constituents, up to full sentences."
D10-1116,3,"in addition, it is crucial to have a full evaluation of the linguistic stegosystem in terms of imperceptibility and payload capacity so we can know how much data can be embedded before the cover text reaches its maximum distortion which is tolerated by a human judge."
D10-1116,1,"for future work, we would like to explore more linguistic transformations that can meet the requirements of linguistic steganography 鈥 retaining the meaning, grammaticality and style of the original text."
D10-1117,1,"we envisage that in future many grammar formalisms that have been shown to be effective in supervised parsing, such as categorial, unification and tree adjoining grammars, will prove amenable to unsupervised induction using the hierarchical nonparametric modelling approaches we have demonstrated in this paper."
D10-1118,6,the ibm approach demonstrated the benefit of a gradual increase of model complexity.
D10-1118,1,"furthermore, an alignment-based parsing method is expected to integrate well with smt bi-lingual alignment models and may,"
D10-1118,1,"therefore, be suitable for combined models which use parse trees to improve word alignment (e.g., burkett et al.2010)."
D10-1118,1,it would be interesting to see if the two approaches could be successfully combined.
D10-1119,1,"such an approach would complement ideas for using high-order unification to model a wider range of language phenomena, such as vp ellipsis (dalrymple et al., 1991)."
D10-1119,1,"we are also interested in developing similar grammar induction techniques for context-dependent understanding problems, such as the one considered by zettlemoyer &amp; collins (2009)."
D10-1120,1,"2) lexicalizing the model, and"
D10-1120,1,"in future work we intend to study ways to bridge this gap by 1) incorporating more sophisticated linguistically-driven grammar rulesets to guide induction,"
D10-1120,1,"3) combining our constraint-based approach with richer unsupervised models (e.g., headden iii et al.(2009)) to benefit from their complementary strengths."
D10-1122,2,"as future work, we would like to explore the possibility of learning hash functions using 1) bilingual and monolingual data together and 2) multiple conjugate languages."
D10-1123,6,future work: functionality is one of the several properties a relation can possess.
D10-1123,6,these properties are very useful in increasing our understanding about these open ie relation strings.
D10-1123,6,"we believe that the general principles developed in this work, for example, connecting the open ie knowledge with an existing knowledge resource, will come in very handy in identifying these other properties."
D10-1123,6,"others include selectional preferences, transitivity (schoenmackers , 2008), mutual exclusion, symmetry, etc."
D10-1124,2,"in some regions, estimates of many of these factors may be obtained by cross-referencing geography with demographic data."
D10-1124,6,We hope to explore this possibility in future work.
D10-1125,4,"finally, our approach could be used with other structured learning algorithms, e.g.meshi (2010)."
D10-1125,1,"in addition, our dual decomposition approach is well-suited to parallelization. finally, our approach could be used with other structured learning algorithms, e.g.meshi et al.(2010)."
D10-1125,1,"as described in section 7.7, the algorithms can be easily modified to consider projective structures by replacing y with the set of projective trees, and then using first-order dependency parsing algorithms in place of mst decoding."
D10-1125,1,"this method could be used to derive parsing algorithms that include higher-order features, as an alternative to specialized dynamic programming algorithms."
D10-1125,4,"the general approach should be applicable to other lexicalized syntactic formalisms, and potentially also to decoding in syntax-driven translation."
D11-1001,2,"in the future we want to integrate weak supervision techniques to train extractors with existing biomedical databases, such as kegg, and only minimal amounts of annotated text."
D11-1002,1,"in future work, we plan to delve further into dependency parsing, looking specifically at the implications of multi-headedness and disconnected subgraphs on dependency parsing."
D11-1002,1,"we also intend to carry out meta-classification, combining the predictions of crfsgd and maltparser."
D11-1005,1,"future work might consider exploiting a larger number of treebanks, and more powerful techniques for combining models than simple local mixtures."
D11-1009,3,"apart from obtaining and experimenting with larger collections of paraphrasing rules, it would be interesting to evaluate our method in vivo, for example by embedding it in question answering systems (to paraphrase the questions), in information extraction systems (to paraphrase extraction templates), or in natural language generators (to paraphrase template-like sentence plans)."
D11-1009,1,"we also plan to investigate the possibility of embedding our svr ranker in the sentence paraphraser we compared against, i.e., to rank candidates produced by using several machine translation systems and pivot languages, as in zhao-eng."
D11-1010,4,"in future work, we plan to extend our system to fully automatic collocation correction that involves both identification and correction of collocation errors."
D11-1011,4,"in the future, we want to apply our approach to web-based taxonomy induction, which according to (kozareva and hovy, 2010) is stifled due to the lacking relations between the instances and the classes, and the classes themselves."
D11-1015,1,the future work will be focused on (1) integrating more semantic and syntactic information in proposed unsupervised method; (2) extending our method to inter-sentence level and then jointly modeling intrasentence level and inter-sentence level discourse constraints on polarity to reach a global optimal inference for polarity classification.
D11-1016,1,also we plan to consider other ways to initialize the matrix-space model.
D11-1016,1,"the other possible direction is to use existing sentiment lexicons and employing a “curriculum learning” strategy (bengio et al., 2009; kumar et al., 2010) for our learning problem."
D11-1016,1,"one interesting direction to explore might be to use non-negative matrix factorization (lee and seung, 2001), co-clustering techniques (dhillon, 2001) to better initialize words that share similar contexts."
D11-1020,1,"in our future works, we will exploit the semantic information encoded in the dependency structures which is expected to further improve the translations, and replace 1-best dependency structures with dependency forests so as to alleviate the influence caused by parse errors."
D11-1022,5,"non-logical constraints may also yield efficient subproblems, e.g., the length constraints in summarization and compression (clarke and lapata, 2008; martins and smith, 2009; bergkirkpatrick , 2011)."
D11-1022,1,"finally, dd-admm can be adapted to tighten its relaxations towards exact decoding, as in sontag et al.(2008) and rush and collins (2011)."
D11-1022,1,"dd-admm may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by liang et al.(2011)."
D11-1022,6,We defer this for future work.
D11-1023,4,"association scores and counts from sketch can be used for more nlp tasks like small-space randomized language models, word sense disambiguation, spelling correction, relation learning, paraphrasing, and machine translation."
D11-1026,1,it is likely that dictionary-based or corpus-based similarity measures would yield a major improvement in performance.
D11-1026,1,we consider two main directions for future work: using more informative similarity metrics and making the process of segmentation hierarchical.
D11-1027,1,we could also use discourse relations to aid in extracting other semantic relations between events.
D11-1027,2,"there are several interesting directions for future work, including the incorporation of other knowledge sources such as coreference and semantic class predictions, which were shown to be potentially important in our error analysis."
D11-1030,1,"in future work, we plan to integrate distance metric learning into our approach, allowing some features to be weighted more heavily than others."
D11-1031,4,"although we have focused on ccg parsing in this work, we expect our methods to be equally applicable to parsing with other grammar formalisms including context-free grammar or ltag."
D11-1031,1,in future work we plan to scale our exact loss functions to larger settings and to explore training with loss functions within loopy belief propagation.
D11-1039,4,an important area for future work is to consider how this learning can be best integrated into a complete dialog system.
D11-1043,3,"further study needs to analyze more summarization metrics such as those proposed at the recent nist evaluation of automatic metrics, automatically evaluating summaries of peers (aesop) (nat, 2010)."
D11-1044,1,"our framework allows integrating together all of these and other types of structures, with the ultimate goal of combining the strengths of multiple approaches to translation in a single model."
D11-1044,1,"the obvious next step for our framework is to include bilingual rules that include source syntax (quirk et al., 2005), target syntax (shen et al., 2008), and syntax on both sides."
D11-1045,2,"in future work we plan to explore using more data from automatic alignments, perhaps by considering a joint model for aligning and reordering."
D11-1045,5,we also would like to explore whether the use of scores from our reordering model directly in machine translation systems can improve performance relative to using just the single best reordering.
D11-1045,1,we would like to investigate the use of other loss functions and their effect on reordering performance.
D11-1045,1,"we would also like to explore doing away with the requirement of having a pos tagger, using completely unsupervised methods to class words."
D11-1047,3,"we still have a lot to do with respect to improving our implementation, exploring the different possibilities offered by this framework and proceeding to more experiments."
D11-1049,6,"first, inference starting from both the query nodes and target nodes (richards and mooney, 1992) can be much more efficient in discovering long paths than just inference from the query nodes."
D11-1049,6,there are several prominent directions for future work.
D11-1049,1,"second, inference starting from the target nodes of training queries is a potential way to discover specialized paths (with grounded nodes)."
D11-1049,1,"third, generalizing inference paths to inference trees or graphs can produce more expressive random walk inference models."
D11-1050,1,"future work will focus on ways to enhance the noun vector representations through additional contextual features, to make them denser and more articulated in structure."
D11-1052,1,"another direction of research is the connection between lengthening and other orthographic conventions associated with sentiment and emphasis, such as emoticons, punctuation, and capitalization."
D11-1052,1,"finally, we plan to integrate lengthening and its related phenomena into an accurate, twitter-specific, sentiment classifier."
D11-1056,1,in the future we will assign a pos tag to each word in order to use segmented noun phrases in morphological analysis.
D11-1056,1,we assume that the meaning of constituents in a noun phrase rarely depends on outer context.
D11-1059,1,"since it is so easy to add extra features to our model, one direction for future work is to explore other possible features."
D11-1059,1,"we are also interested in improving our morphology features, either by considering other ways to extract features during preprocessing (for example, including prefixes or not concatenating together all suffixes), or by developing a joint model for inducing both morphology and syntactic classes simultaneously."
D11-1059,1,"for example, it could be useful to add dependency features from an unsupervised dependency parser."
D11-1060,4,"in future work, we plan to apply our framework to the remaining relations in the inventory of levi (1978), and to release the resulting dataset to the research community."
D11-1066,1,"in future work, we plan to extend the current research by investigating models capable of exploiting predicate argument structures for question classification and answer reranking."
D11-1066,1,"the use of syntactic/semantic kernels is a promising research direction (basili et al., 2005; bloehdorn and moschitti, 2007a; bloehdorn and moschitti, 2007b)."
D11-1067,1,we will investigate automatic conversion of these treebanks (by flattening mwe bracketing) for mwe identification.
D11-1068,1,we will also develop algorithms for argument identification.
D11-1068,1,"in future, we will explore further features for connective disambiguation as well as connective-specific classification, combined with semi-supervised algorithms to alleviate data sparseness."
D11-1069,1,"ultimately, we would like to identify the speech act expressions themselves because some sentences contain speech acts as well as factual information."
D11-1069,1,"in future work, we believe that segmenting sentences into clauses may help to train classifiers more precisely."
D11-1072,1,"our future work will consider additional semantic properties between entities (types, member of/part of, etc.)for further enhancing the coherence algorithm."
D11-1074,1,"as our future work, we plan to explore how to further incorporate such world knowledge into our model in a principled way."
D11-1075,3,"an alternative is to consider an approximate solution that evaluates, for instance, only the top few label assignments that are likely to maximize the likelihood of our observations."
D11-1075,6,this remains as an interesting future work of this study.
D11-1077,2,we also plan to apply the methodology to languages other than hebrew.
D11-1077,3,"in the future, we intend to extend the evaluation to longer n-grams."
D11-1078,3,future experiments include contrasting these results with other dictionaries and language pairs.
D11-1081,1,"finally, we hope to exploit more features such as reordering features and syntactic features so as to further improve the performance."
D11-1082,3,"future work will assess its impact on translation for the other language pairs, as well as its impact on other tasks, such as named entity projection."
D11-1084,5,"in the future, we will further explore how to reflect document divergence during training and dynamically adjust cache weights according to different documents."
D11-1085,2,"in future work, we plan to consider lowresource test domains and language pairs like urduenglish, where bilingual data for novel domains is sparse."
D11-1089,4,"although our investigation was restricted to katakana noun compounds, one might expect that a similar approach would be useful for splitting other types of noun compounds (e.g., german noun compounds), or for identifying general word boundaries, not limited to those between nouns, in asian languages."
D11-1089,6,we think these are research directions worth exploring in the future.
D11-1091,1,"to this end, we plan to develop a model that simultaneously induces predicates and learns coercions, using knowledge of a predicate’s coerciveness to inform the induction mechanism."
D11-1092,3,"we will also design methods for assessing the quality of mapping, and analyze their correlations with the sr methods."
D11-1092,5,"third, analyses will be carried out to uncover the differences between feature combination and integration that have led to different accuracies."
D11-1094,3,"secondly, we would like to subject our approach to further evaluation, in particular on a number of different evaluation tasks, such as semantic compositionality."
D11-1094,1,"and thirdly, we would like to transfer the general idea of the approach presented in this paper to a tensor based framework (which is able to capture the multiway co-occurrences of words, together with their window-based and dependency-based context features, in a natural way) and investigate whether such a framework proves beneficial for the modeling of word meaning in context."
D11-1094,1,the results might improve further if window-based context and dependency-based context are combined in an optimal way.
D11-1095,2,"as for the constrained version of hgfc, we will conduct a larger scale experiment on the verbnet data to investigate what kind of upper level hierarchy it can propose for this resource (which currently has over 100 top level classes)."
D11-1095,4,"in addition, we plan to apply the unconstrained hgfc to specific domains to investigate its capability to learn novel, previously unknown classifications."
D11-1095,1,"finally, we plan to compare hgfc to other hierarchical clustering methods that are relatively new to nlp but have proved promising in other fields, including bayesian hierarchical clustering (heller and ghahramani, 2005; teh et al., 2008) and the method of azran and ghahramani (2006a) based on spectral clustering."
D11-1097,4,"we intend to adapt our approach for word sense disambiguation as well as related domain-specific tasks such as gene name normalisation (morgan , 2008)."
D11-1097,1,"a further, more speculative direction for future research is to investigate more richly structured models of context, for example capturing correlations between words in a text within a framework similar to the correlated topic model of blei and lafferty (2007) or more explicitly modelling polysemy effects as in reisinger and mooney (2010)."
D11-1097,4,"in future work, 7the overall average gap for thater et al.(2010) does not appear in their paper but can be calculated from the score and number of instances listed for each pos. we intend to adapt our approach for word sense disambiguation as well as related domain-specific tasks such as gene name normalisation (morgan et al., 2008)."
D11-1100,2,"using a bi-lingual dictionary which maps wordnet across languages, such a machine translation sub-system can be avoided."
D11-1100,6,more sophisticated features which include the two need to be explored.
D11-1100,1,another line of work is in the context of cross-lingual sentiment analysis.
D11-1103,4,"as part of the future work, we plan to extend this technique for hypergraphs and lattices in re-scoring mt outputs with complex and long span language models."
D11-1105,2,"second, we may explore more domain knowledge to improve the quality of aspect-oriented summaries."
D11-1105,5,currently the sentence compression algorithm may generate meaningless subtrees.
D11-1105,5,it is relatively hard to decide which clause is redundant in terms of summarization.
D11-1105,1,"first, we can possibly apply more linguistic knowledge to improve the quality of sentence compression."
D11-1105,1,there are a number of directions we plan to pursue in the future in order to improve our method.
D11-1105,1,"third, we want to extend our event-aspect model to simultaneously find topics and aspects."
D11-1105,1,"for example, we know that the “who-affected” aspect is related to person, and “when, where” are related to time and location. we can import name entity recognition to annotate these phrases and then help locate relevant sentences."
D11-1106,6,"first, a large scale language model can be incorporated into our model in the search algorithm, or through reranking."
D11-1106,6,potential improvements to our system can be made in several areas.
D11-1106,1,"future work also includes integration with an smt system, where content word selection will be applicable."
D11-1106,1,"second, a heuristic future cost (e.g."
D11-1106,2,"varges and mellish (2010)) can be considered for each hypothesis so that it also considers the words that have not been used, leading to better search."
D11-1107,1,in future work we plan to identify further features that will allow us to inform this choice so that we can move towards this level of performance.
D11-1113,1,"future work on parse correction might focus on developing specialized models for other difficult attachment types, such as verb-phrase attachment (verb dependents account for around 15% of incorrect attachments across all four parsers)."
D11-1115,5,"we have shown through adapting to the question domain that it is possible to make focused improvements when we can identify the gaps in coverage (as in wh-question words), but in order to address the challenge of automatic lexicon extension fully, quite different techniques for generalising lexical entries for seen words will be require."
D11-1116,5,it would also be interesting to examine the impact on final parsing accuracy of the various differences between our dependency conversion and stanford’s.
D11-1116,1,we would like to change our handling of coordinating conjunctions to treat the coordinating conjunction as the head because this has fewer ambiguities than the current approach and also add the ability to produce traces for wh- words.
D11-1118,4,and exploring other uses of soft clustering algorithms — perhaps as inputs to part-of-speech disambiguators — may be another fruitful research direction.
D11-1119,1,"we will investigate extending, to other applications, this general methodology combining distributional, semantic and syntactic information with language models."
D11-1120,4,"in future work, we will explore how well such models carry over to gender identification in other informal online genres such as chat and forum comments."
D11-1120,1,"furthermore, we have been able to assign demographic features beside gender, including age and location, to our twitter dataset."
D11-1121,1,"in future work we plan to improve the features of the svm classifier, and further investigate the usefulness of our approach for trading."
D11-1126,1,in future work we will continue to investigate methods to mitigate quality loss.
D11-1128,4,"for future work, graph reinforcement could be extended to mt to improve the coverage of aligned phrase tables."
D11-1128,1,"in doing so, it is reasonable to assume that there are multiple ways of expressing a singular concept and hence multiple translations are possible."
D11-1128,1,using graph reinforcement can help discover such translation though they may never be seen in training data.
D11-1129,3,"the advantages and disadvantages of this method and comparisons with other systems, in particular ccg, constitutes ongoing work."
D11-1131,4,"in addition, we are thinking about using an extension of the pcfg formalism that allows for some kind of ‘featurepassing’ which could lead to much smaller and more general grammars."
D11-1131,1,"in future research, we plan to address the limitation of our model to a finite set of meaning representations, in particular through the use of nonparametric bayesian models such as the infinite pcfg model of liang et al.(2007) and the infinite tree model of finkel et al.(2007); both allow for a potentially infinite set of non-terminals, hence directly addressing this problem."
D11-1134,4,one additional area for future research is to extend ontext to discover new categories in addition to new relations.
D11-1137,4,"in the future, we plan to investigate more appropriate generative models for reranking."
D11-1137,1,"as we mentioned in section 5.4, we also plan to incorporate semi-supervised learning into our framework, which may potentially improve our reranking performance."
D11-1137,1,"we plan to examine to model such a complex structure (granduncle) (goldberg and elhadad, 2010) or higher-order structure than third-order for reranking which is computationally expensive for a baseline parser."
D11-1142,6,the error analysis in section 5.2 also suggests natural directions for future work.
D11-1142,1,"for instance, since many of reverb’s errors are due to incorrect arguments, improved methods for argument extraction are in order."
D11-1143,3,we plan to confirm these results by using estimates of quality in the future.
D11-1146,1,we will ensemble wtm with other content based and collaboration-based methods to build a practical social tag suggestion system.3.
D11-1146,1,wtm and ewtm can only suggest the tags that have appeared in translation models.
D11-1146,1,"we can exploit other word alignment methods like log-linear models (liu et al., 2010a) for social tag suggestion.2."
D11-1146,1,"in future, we plan to incorporate keyphrase extraction in social tag suggestion to make it suggest more appropriate tags not only from translation models but also from the resource descriptions."
D11-1147,1,"as our future work, we aim to build a system that employs our findings in this paper and the emergent patterns in the re-tweet network topology to identify whether a new trending topic is a rumor or not."
D11-1148,1,"one is the use of feature schema instances that did not appear in the largely grammatical wsj; another is the extension of feature schemas; and a third is the use of a parser that does not enforce linguistic constraints such as the berkeley parser (petrov et al., 2006)."
D12-1002,5,"while this is an important question, in this paper, we are primarily interested in showing the importance of handling language specific phenomenon in the bridge language approaches."
D12-1002,1,"in future, we would like to study the appropriateness of ipa vs. english as the bridge language and also the generalizability of our technique to other scenarios."
D12-1003,3,"we are also planning an end-toend evaluation, for instance, by employing the extracted bilingual lexicon into an mt system."
D12-1003,5,"we are planning to investigate the following open problems in future work: word sense disambiguation and translation of compound words as described in (daille and morin, 2005; morin , 2007)."
D12-1003,1,"we will utilize their random walk approach or other graph-based techniques such as modified adsorption (talukdar and crammer, 2009) for generating seed distributions."
D12-1004,2,we are currently collecting and transcribing additional stories from the two groups which we would like to use as a definitive test set to verify the stability of our findings.
D12-1004,1,"we plan to explore syntactic and coherence models to analyze the stories, as well as emotion analysis of the narratives."
D12-1005,5,"finally, we assumed here strictly count-based features; streaming log-counting methods, tailored bloom-filters for binary feature storage, and other related topics are assuredly applicable, and should give rise to many interesting new results."
D12-1007,1,it could be improved to incorporate noise resulting from the decisions made by the semantic parser.
D12-1007,1,the effects of a dependence assumption between the different utterances occurring in a single user turn under the act model can also be explored.
D12-1007,1,"another possible improvement is to explore the effects of introducing dependency between the slots in the user goal, which would enforce more plausible values pairings and would potentially improve the simulator鈥檚 performance."
D12-1007,1,we would also like to use our simulator to train a pomdp-based dialog manager using a form of reinforcement learning.
D12-1007,1,our model could be extended in a number of ways.
D12-1008,4,"finally, we can confirm the human results obtained from an overhearer-style evaluation in a real interactive setting and explicitly extend our language model to discourse phenomena such as pauses or hesitations to take them into account in measuring id."
D12-1008,1,"given that id is a measure influencing human language production, we could replace our template-based surface realizer by an agent that optimizes the information density of its output."
D12-1008,1,"currently we learn the agent鈥檚 behavior offline, before the interaction, and then execute it statistically."
D12-1008,1,more adaptivity towards individual users and situations could be achieved if the agent was able to learn from ongoing interactions.
D12-1008,6,Future work can take several directions.
D12-1009,5,a drawback is that the time complexity of inference as presented here is quadratic in the number of classes rather than linear.
D12-1009,5,improving this may be the subject of future research.
D12-1009,1,"another potential avenue of future work is to model transitions such that a dirichlet prior for the class distribution of a block, rather than the class distribution itself, depends on the previous class assignments."
D12-1009,1,"this would yield a model that more closely resembles lda, but with topic priors that encode sequence information."
D12-1011,5,"the initial success of these open-db ned approaches indicates that this task is a promising area for future research, including exciting extensions that link large numbers of domainspecific databases to text."
D12-1012,2,"other interesting directions for future work are introducing more constructs in our framework, and applying our techniques to other languages."
D12-1012,1,"therefore, we believe more work is required to devise a more comprehensive quantitative measure for interpretability, and refine our techniques in order to increase the interpretability of induced rules."
D12-1013,4,"for the future work, we are interested in applying our co-selecting approach to active learning for other imbalanced classification tasks, especially those with much higher imbalanced ratio."
D12-1015,1,"the immediate extension of our work is to polish each component of this framework, such as improving the accuracy of query expansion and pseudo context acquisition, using other effective polarity computing methods for each context and so on."
D12-1015,1,"in addition, we will explore other query expansion strategies to generate more effective contexts."
D12-1016,1,"using this set of alignments, we will then proceed to exploit contextual information in order to learn a semantic model for discourse coherence in argument structure realization."
D12-1016,1,"in future work, we will enhance our model by incorporating more refined similarity measures including discourse-based criteria."
D12-1016,1,"we will further explore tuning techniques, e.g., a more suitable preselection method for edges in graph construction, in order to increase either precision or recall."
D12-1019,1,"we are also working on extending it to learn more powerful grammars e.g. split head-automata grammars (shag) (eisner and satta, 1999)."
D12-1019,1,"as future work, we are working on building an end to-end parser which would involve coming up with a spectral version of the inside-outside algorithm for our setting."
D12-1021,4,we also expect that expanding our sampler beyond strict binary sampling may allow us to explore the space of hierarchical word alignments more quickly allowing for faster mixing.
D12-1021,1,future work includes the obvious extension to learning scfgs that contain multiple nonterminal instead of a single nonterminal grammar.
D12-1021,1,we expect with these extensions our model of grammar induction may further improve translation output.
D12-1022,1,"in the future, we also plan to test the ability of the model to adapt to other multi-document summarization tasks, where the location of summary information is not as regular as it is in news articles."
D12-1022,1,we would also like interface our model with sentence ordering and more generally with some notion of the coherence of the generated summary.
D12-1023,5,"interestingly, even with the richest model, in some cases we found that the dependency length feature still appears to go too far in minimizing dependency length, suggesting that further counter-balancing features specially ones for the sentence-initial position (filippova and strube, 2009) arrant investigation in future work."
D12-1024,5,"we intend to employ the rouge score as the score function in future work, and obtain the parameters of the state value function."
D12-1024,1,"using these results, we will attempt to obtain a single learned policy by employing the rouge score or human evaluations as rewards."
D12-1024,1,we also intend to consider efficient features and a score to achieve stable convergence.
D12-1024,1,"in addition, we plan to use other methods of function approximation, such as rbf networks."
D12-1025,2,we will continue to work in scenarios where large amount of monolingual data is readily available.
D12-1025,2,"in the future, we will work with more language pairs, especially those with significant word re-orderings."
D12-1026,1,"in the future work, we will focus on modeling intratense variation according to specific sentence types and using more features to improve it."
D12-1027,2,"finally, we plan experiments with other language pairs and application to other linguistic problems."
D12-1027,5,"in future work, we would like to add word deletion, insertion, splitting, and concatenation as allowed editing operations."
D12-1027,1,we further want to explore tighter integration of word-based and phrase based paraphrasing.
D12-1028,1,we would also like to employ lexicalized models that should help in situations in which the pos tags are too coarse.
D12-1028,1,"furthermore, we would like to get rid of manually designed pos tags and use some kind of unsupervised clusters in order to have all the annotation process completely unsupervised."
D12-1028,1,"finally, we would like to move towards deeper syntactic structures, where the tree would be formed only by content words and the function words would be treated in a different way."
D12-1028,1,"in future work, we would like to estimate the hyperparameters automatically."
D12-1029,5,"other research directions involve investigating the set of non-projective arcs allowed by non-projective buffer transitions, defining different variants of buffer transitions (such as nonprojective buffer transitions that work with nodes located deeper in the buffer) or using projective and non-projective buffer transitions at the same time."
D12-1029,1,"therefore, future work will include an evaluation of the impact of buffer transitions on more transition based parsers."
D12-1032,1,"for names, this could include learning common nicknames (nonparametrically); explicitly modeling abbreviation processes such as initials; conditioning on name components such as title and middle name; and transliterating across languages.14another future direction would be to incorporate the context of tokens in order to help reconstruct which tokens are coreferent."
D12-1032,1,one direction for future work would be more sophisticated transduction models than the one we developed in 搂4.
D12-1032,1,"for example, we might extend the generative story to generate a context for token (e, w) conditioned on e. combining contextual similarity with string similarity has previously proved very useful for identifying cognates (schafer and yarowsky, 2002; schafer, 2006b; bergsma and van durme, 2011)."
D12-1033,1,"however, further experimentation is required on other measures of syntactic complexity (e.g. dlt, gibson (2000)) as well as other levels of representation such as the semantic level."
D12-1035,5,"first, questions with aggregations cannot be handled at this point."
D12-1035,5,"second, queries sometimes return empty answers although they perfectly capture the original question, but the underlying data sources are incomplete or represent the relevant information in an unexpected manner."
D12-1035,1,"we plan to extend our approach of combining structured data with textual descriptions, and generate queries that combine structured search predicates with keyword or phrase matching."
D12-1035,1,future work includes relaxing some of the limitations that our current approach still has.
D12-1036,6,this will be done by incorporating more nlp features in salience and coherence weights estimation.
D12-1036,1,"in the future, we will explore the more sophisticated nlp features to improve the proposed framework."
D12-1037,2,we will test our method on other translation models and larger training data.
D12-1037,1,"in the future work, we will further investigate the local training method, since there are more room for improvements as observed in our experiments."
D12-1038,1,"as future works, we will investigate the acceleration of the iterative training and the weight parameter tuning, and extend the optimized annotation transformation strategy to joint chinese word segmentation and pos tagging, parsing and other nlp tasks."
D12-1039,2,"in the future, we intend to improve our dictionary by leveraging the constantly-growing volume of microblog data, and considering alternative ways to combine distributional and string similarity."
D12-1039,4,"in addition to direct evaluation, we also want to explore the benefits of applying normalisation for downstream social media text processing applications, e.g.event detection."
D12-1040,2,"in the future, we would like to develop a better lexicon learner since our pcfg approach critically relies on the quality of the learned lexicon."
D12-1040,4,"in addition, we want to investigate the use of discriminative reranking (collins, 2000), which has proven effective in various other nlp tasks."
D12-1040,1,"we would expect the final mr output to improve if a discriminative model, which uses additional global features, is used to rerank the top-k parses produced by our generative pcfg model."
D12-1040,1,"particularly, we would like to investigate how syntactic information (such as part-of-speech tags induced using unsupervised learning) could be used to improve semantic-lexicon learning."
D12-1041,5,"for example, fdts can be used in a prereordering framework."
D12-1041,5,several potential research topics can be explored in future.
D12-1041,1,we also plan to adapt our fdt-based model training approach to scfg-based and traditional left-to-right phrase-based systems.
D12-1043,3,some further experiments are thus needed to investigate which of these facts better account for our findings on the semantic features.
D12-1043,5,"further investigation on this issue would definitely be worthwhile, since several facts could explain these contradictory findings."
D12-1043,1,"a last avenue of research worth mentioning would be to develop the family of specific-to-ffl predictors, to determine whether taking into account the impact of a given l1 language on the readability of l2 texts would increase performance over a generic model enough so that tuning efforts are worthwhile."
D12-1045,1,"for example, document clustering and coreference resolution can be solved jointly, which we expect would improve both tasks."
D12-1045,1,"furthermore, our iterative coreference resolution procedure (algorithm 1) could be modified to account for mention ordering and distance, which would allow us to include pronominal resolution in our joint model, rather than addressing it with a separate deterministic sieve."
D12-1045,1,"however, our model can be improved."
D12-1046,1,we will also investigate methods for joint learning as well as ways to speed up the joint decoding algorithm.
D12-1046,1,"in the future work, we will compare this joint model to the pipeline approach that uses multiple candidates or soft decisions in the early modules."
D12-1047,4,"first, we will utilize our approach to mine large-scale corpora by distributed infrastructure system, and investigate the use of our approach for other domains, such as speech translation system."
D12-1047,1,"second, the significant improvement of lm adaptation based on cross-lingual data selection is exciting, so it will be instructive to explore other knowledge based cross-lingual data selection for lm adaptation, such as latent semantic model."
D12-1049,1,"the current gibbs sampler is slower than regular lda, so future work is to speed up the algorithm."
D12-1050,5,the problem of finding the right methods of vector composition cannot be pursued independent of the choice of lexical representation.
D12-1050,1,"having tested many model combinations, we argue that in a good model of distributive semantics representation and composition must go hand in hand, i.e., they must be mutually learned."
D12-1051,4,"secondly, we plan to apply our approach to other joint segmentation and labeling tasks, such as clause identification and named entity recognition."
D12-1051,1,"firstly, we will explore applying external information, such as semantic knowledge, to represent the chunk-level features, and then incorporate them into our model to improve the performance."
D12-1053,4,"furthermore, we also want to investigate the use of srl approaches for high-relational domains, and make a clear comparison with related techniques."
D12-1053,1,"in future work, we intend to explore additional ways to incorporate background knowledge in a declarative way, since it renders the language learning problem more intuitive and gives a better understanding of feature contribution."
D12-1056,1,integrating data driven error correction feature with these advanced features for the benefit of users is the challenge we face in the next step.
D12-1057,4,we will investigate additional applications of excitation in future work.
D12-1057,1,"for instance, we expect that excitation and its related semantic knowledge acquired in this study will improve the performance of why-qa system like the one proposed by oh et al.(2012)."
D12-1058,3,"finally, the developed paraphrase collection will be attested through applications, such as sentence compression (cohn and lapata, 2008; ganitkevitch , 2011) and machine translation (callison-burch , 2006; marton , 2009)."
D12-1058,5,"other interesting questions related to the work presented here are, as mentioned in section 4.2, exploitation of patterns with more than one variable, learning curve experiments with different amounts of monolingual data, and comparison of in-domain and general-purpose monolingual corpora."
D12-1058,1,we will therefore investigate similarity metrics in our future work.
D12-1058,1,"second, we have an interest in exploiting sophisticated paraphrase patterns; for instance, by inducing patterns hierarchically (recursively) and incorporating lexical resources such as those exemplified in (4)."
D12-1059,1,"in future, we plan to exploit this feature: when estimating the category-based alignment, we can interpolate predictions of multiple categories to which a word belongs, weighted by its probabilities associated with membership in each category."
D12-1060,3,"finally, as all our conclusions have been drawn on a data set of 12 domain pairs, we plan to increase a number of domains to verify our findings on larger data sets."
D12-1060,6,as a result of this research we have identified the following future directions.
D12-1060,1,another research direction will focus on the integration of sfa into the similarity measure to overcome the problem of lexical discrepancy in the source and target domains.
D12-1060,1,we expect that this modification will diminish the number of 鈥渂ad鈥 neighbors and allow us to reveal a dependency of similarity threshold on some domain properties.
D12-1060,1,"first, we plan to improve the rank performance by choosing the number of neighbors on the basis of the document similarity threshold which we set equal for both in-domain and cross-domain neighbors."
D12-1062,1,"while our experiments suggest that the temporal classifiers can potentially help enhance the performance of event coreference, in future work we would like to investigate into coupling event coreference with other components in a global inference framework."
D12-1063,1,dbms are a step in the direction towards modeling constituent boundaries jointly with head dependencies.
D12-1063,1,"further steps must involve more tightly coupling the two frameworks, as well as showing ways to incorporate both kinds of information in other state-of-the art grammar induction paradigms."
D12-1064,1,"future work will involve a broader exploration of the parameter space of the adaptor grammars, in particular the number of topics and the value of 伪; a look at other non-parametric extensions of pcfgs, such as infinite pcfgs (liang et al., 2007) for finding a set of non-terminals permitting more fine-grained topics; and an investigation of how the approach can be extended to semi-supervised learning to take advantage of the vast quantity of texts with errors available on the web."
D12-1065,1,it would be interesting to extend our model to structures beyond linear chains to trees and other structures.
D12-1066,4,"lastly, we plan to apply such knowledge in text-to-text applications."
D12-1066,6,our avenues for future work lie in three main areas.
D12-1066,1,"the first one is to continue our current line of work and study the impact of additional individual acquisition techniques and better characterizations of paraphrases in context, in tandem with working on identifying parallel text pairs in large corpora."
D12-1066,1,another avenue is to start from the output of high recall techniques and to attempt to characterize the contexts of possible substitution for candidate paraphrases from large corpora as a means to acquire precise paraphrases.
D12-1068,1,"similarly, to extend the diagonal mahalanobis matrix to the general covariance matrix is also desirable."
D12-1068,1,"last but not least, to find a more systematical way to determine the optimal k in the proposed method is also our possible future work."
D12-1068,1,it would be interesting to consider the pure unsupervised tasks that have no any target annotations.
D12-1068,1,"besides, to develop some better ways for document-level representation, e.g., incorporating the domain knowledge, also deserves our attentions."
D12-1069,1,"in the future, we plan to increase the expressivity of our parser鈥檚 meaning representation to capture more linguistic and semantic phenomena."
D12-1069,1,"in this fashion, we can make progress toward broad coverage semantic parsing, and thus natural language understanding."
D12-1070,4,"even though the objective of our work is for speech recognition, our proposed cross-lingual language modeling can be easily applied to speech translation of other language pairs for efficient direct decoding from source speech to target text."
D12-1071,3,"and evaluate the resulting resolver on standard evaluation corpora such as muc, ace, and ontonotes."
D12-1071,1,"in addition, we plan to integrate our resolver into a general-purpose coreference system"
D12-1072,3,"for the task more broadly, it would be beneficial to compare methods of finding indirect and mixed quotes, and to evaluate how well quote attribution performs on those quotes as opposed to just direct quotes."
D12-1072,1,we will also explore other approaches to representing quote attribution with a crf.
D12-1072,1,"in future work, we intend to further explore the sequence features that have a large impact on accuracy, and to find similar features or proxies for the sequence features that would be beneficial."
D12-1073,4,"meanwhile we will also apply sshlda to other media forms, such as image, to solve related problems in these areas."
D12-1073,1,"in the future, we will continue to explore novel topic models for hierarchical labeled data to further improve the effectiveness;"
D12-1076,1,"in addition, graphical model, which has been studied in academic author disambiguation, may be a good choice to cope with the noises and non-standard forms in web data."
D12-1076,1,jointly modeling entity linking and person (entity) disambiguation tasks will be an interesting direction where the two tasks are closely related and usually need to be considered at the same time.
D12-1076,1,"investigating the person name disambiguation task in different web applications will also be of great importance, e.g., disambiguating a name in streaming data or during knowledge base construction."
D12-1078,1,"in future, we will explore more alternatives in integrating parsing information and alignment information, such as discriminative word alignment using a lot of features from parser."
D12-1080,1,a prime direction for future work is combining our model with a probabilistic relation extraction system.
D12-1080,1,"furthermore, the consistency component can be extended with new question types to incorporate non-temporal constraints as well."
D12-1080,1,this could be accomplished by using the marginal probabilities on the extracted relations and multiplying them with the probabilities from the classifier and consistency components.
D12-1080,1,inference would require an additional step which could add or drop candidate fluents.
D12-1081,4,"we consider that the notion of constancy will even be beneficial in acquiring world knowledge, other than relations between entities, from text; we aim at extending the notion of constancy to other types of knowledge involving real-world entities, such as concept-instance relations."
D12-1081,1,"we also plan to start a spin-off research that acquires paraphrases by grouping values of arg2s for each value of arg1 in a constant, unique relation."
D12-1081,1,we will utilize the identified properties of the relations to adopt an appropriate strategy to compile their instances.
D12-1082,5,"for detecting new entities, we are interested in seeing if timestamped twitter data could be analyzed to increase both recall and precision."
D12-1082,1,an area we are continuing to improve the system on is textual ambiguity.
D12-1082,1,"last, we would like to feed back our system output to improve system performance."
D12-1082,1,"for predicting semantic types, (kozareva et al., 2011) proposed additional techniques which we have not fully explored."
D12-1082,1,"also, we can incorporate additional signals such as shared term heads when they are available, in order to help find terms that are likely to share types."
D12-1082,1,"we have ideas for how to detect ambiguous entities using mutual exclusion (carlson, 2010) and functional relations."
D12-1082,1,"we also plan to continue improving our techniques, as there is still plenty of room for improvement on both subtasks."
D12-1082,1,"for example, non-entity noun phrases that make it to the typing step might lead to particular predicted type distributions that indicate an error occurred earlier in the process."
D12-1083,3,"in ongoing work, we plan to generalize our dcrf-based parser to multi-sentential text and also verify to what extent parsing and segmentation can be jointly performed."
D12-1083,3,"once we achieve similar performance on graph structures, we will perform extrinsic evaluation to determine their relative utility for various nlp tasks."
D12-1083,1,"a longer term goal is to extend our framework to also work with graph structures of discourse, as recommended by several recent discourse theories (wolf and gibson, 2005)."
D12-1084,4,"for future work, we plan to use msa to align single clauses rather than whole sentences."
D12-1084,5,"in a long-term view, it would be interesting to see how aligned discourse trees could help to extract paraphrases from arbitrary parallel text."
D12-1084,1,"additionally, we plan to generalize the method for other parallel texts by preprocessing them with a temporal classifier."
D12-1084,1,"in a more advanced step, we will also use the aligned paraphrases to help resolving discourse structure, e.g. for coreference resolution, which could lead to a high-performance bootstrapping system."
D12-1087,1,"for applications in which a human end-user will interact with learned topics, the flexibility of lda and the coherence advantages of lda warrant strong consideration."
D12-1088,1,"as future work, we would like to combine our approach with significance pruning, since both approaches are orthogonal and address different issues."
D12-1088,1,we also plan to improve the pruning step of our algorithm to find the optimal set of phrase pairs to prune given the pruning threshold.
D12-1089,4,"the entropy pruning criterion could be applied to hierarchical machine translation systems (chiang, 2007)."
D12-1089,1,we could also include other phrase models such as p( 锟絝|e) and the language model.
D12-1089,1,"we might obtain a better estimate by also considering the distortion costs, which penalize reordering."
D12-1092,1,"in future work, we will focus on how to introduce the discourse information into the individual classifiers to capture those long-distance features and joint learning of subtasks in chinese event extraction."
D12-1093,1,"second, relation paths that contain constant nodes (lexicalized features) and conjunction of random walk features are potentially very useful for extraction tasks."
D12-1093,1,"first, bidirectional search from both query and target nodes can be an efficient way to discover long paths."
D12-1097,5,how to use lexical cohesion devices appropriately instead of frequently is thus an important issue to tackle before we can adopt them in mt and mt evaluation by a suitable means.
D12-1097,5,"our future work will continue to explore the relationship of lexical cohesion to translation quality, so as to identify, apart from its use frequency, other significant aspects for mt evaluation at the document level."
D12-1098,4,"moreover, we will apply graph theoretic models on graphs constructed using flag for solving a large variety of nlp applications."
D12-1098,1,"in future, we will apply flag to construct graphs using several kinds of contexts like lexical, semantic, syntactic and dependency relations or a combination of them."
D12-1099,3,"first, we plan to compare our system with supervised systems to identify the gap between the two systems."
D12-1099,1,"second, as in (poon and domingos, 2010), we plan to explore a joint learning method to combine the tasks of tokenization, forming the main concept cluster and forming the attribute clusters; these tasks depend on the outputs of one another."
D12-1099,1,"to improve our system further, we plan the following works."
D12-1099,1,"finally, we plan to explore that external knowledge resources such as dbpedia (auer et al., 2007) and freebase (bollacker et al., 2008) can be used to further improve performance."
D12-1100,5,"however, it may be challenging to combine this with conservative update."
D12-1100,1,"future work is to reduce the bit size of each counter (instead of the number of counters), as has been tried for other summaries (talbot and osborne, 2007; talbot, 2009; van durme and lall, 2009a) in nlp."
D12-1101,4,"motivated by our positive results, we will also study the application of this approach to other approximate inference techniques, such as belief propagation and variational inference."
D12-1101,1,"in particular, we will explore dynamic sampling, in which we sample fewer factors during the initial, burn in phase, but sample more factors as we get close to convergence."
D12-1101,1,"based on the ideas presented in the paper, we will consider additional sampling strategies."
D12-1101,1,"since training is often a huge bottleneck for information extraction, we will also explore its applications to parameter estimation."
D12-1102,6,one direction of future work is to take advantage of the fact that the inference problem can be split into smaller sub-problems.
D12-1104,1,"we would also like to investigate the enhanced interplay of information extraction and pattern extraction, and possible applications for question answering."
D12-1104,1,an interesting future direction is to study this generalized setting.
D12-1106,5,we also want to analyze genre differences to understand if the strength of these coherence dimensions varies with genre.
D12-1106,6,We plan to explore these ideas in future work.
D12-1108,6,"we believe that smt research has reached a point of maturity where discourse phenomena should not be ignored any longer, and we consider our decoder to be a step towards this goal."
D12-1110,1,"it generalizes several models in the literature, can learn propositional logic, accurately predicts sentiment and can be used to classify semantic relationships between nouns in a sentence."
D12-1113,6,addressing the aforementioned challenges is a subject for future work.
D12-1114,1,we will also attempt to introduce more predicates and transform structure learning techniques for mln into coreference problems.
D12-1114,1,in the future we will try to design more global constraints and explore deeper relations between training instances generation and mention clustering.
D12-1115,3,"second, we will experiment with a two-stage resolution approach."
D12-1115,4,"finally, during annotation, we noted a number of issue patterns (e.g., an open question is x, x is under debate); a possible extension is extracting issues and problems from text using these patterns as seed patterns."
D12-1115,6,a number of extensions are planned for this work.
D12-1115,1,"first, we will extend the work to resolve other abstract anaphors (e.g., this decision, this problem)."
D12-1115,1,"third, we would like to explore the effect of including serious discourse structure features in our model.(the feature sets sc and c encode only shallow discourse information.)"
D12-1118,2,"in addition, using larger corpora would allow us to have more comprehensive doubly-hierarchical language models (wood and teh, 2009)."
D12-1118,1,"we are also interested in adding richer models of opponents to the state space that would adaptively adjust strategies as it learned more about the strengths and weaknesses of its opponent (waugh et al., 2011)."
D12-1119,5,can we learn class-bias for unsupervised domain adaptation?
D12-1119,5,limiting the multi-domain improvements to a small set of parameters raises an interesting question: can these parameters be adapted to a new domain without labeled data?
D12-1120,4,"important considerations for future work include identifying further effective and tractable biases, and extending beyond sequence-labeling to other types of nlp tasks."
D12-1121,1,"for future work, we plan to combine unambiguity regularization with other types of priors and regularizations for unsupervised grammar learning, to apply it to more advanced grammar models, and to explore alternative formulations of unambiguity regularization."
D12-1122,4,"also, we will apply our model to additional opinion analysis tasks such as fine-grained opinion summarization and relation extraction."
D12-1122,1,"in future work, we hope to explore better ways of utilizing parsing information with less cost."
D12-1123,3,and we think that it鈥檚 useful to add some prior knowledge of opinion words (sentiment lexicon) in our model for estimating candidate opinion relevance.
D12-1123,1,"meanwhile, we will add some syntactic information into wtm to constrain the word alignment process, in order to identify opinion relations between words more precisely."
D12-1123,1,"in future work, we plan to use other word alignment methods, such as discriminative model (liu et al., 2010) for this task."
D12-1123,1,"moreover, we believe that there are some verbs or nouns can be opinion words and they may be helpful for opinion target extraction."
D12-1124,2,"more broadly, this paper is an example of using extrinsic variables to drive model-building for linguistic data, and future work might explore richer extrinsic variables toward a goal of task-driven notions of semantics."
D12-1126,1,"we would also like to investigate these features in more applications of natural language processing, such as name entity recognition, information extraction, etc."
D12-1126,1,"for future works, we plan to improve our approximate tagging algorithm to reduce error propagation."
D12-1126,1,"in addition, we will refer to an english dictionary to generate some useful features to distinguish between 鈥淣r鈥 and 鈥淣n鈥 in chinese-english mixed texts and add some statistical features derived from english resources, such as the most common tag of each english word."
D12-1127,4,"it would be very interesting and perhaps necessary to incorporate this additional data in order to tackle challenges that arise across a larger number of language types, specifically non-european languages."
D12-1128,4,"in addition, we plan in the very near future to generalize our multilingual joint approach and apply it to high-end tasks such as multilingual textual entailment (mehdad , 2011) and sentiment analysis (lu , 2011) 鈥 so as to provide a general framework for knowledge-rich multilingual nlp."
D12-1129,4,"additionally, our framework can be applied to any language of interest, provided enough glossaries are available online, by simply translating the keywords used for our queries."
D12-1130,1,"a natural avenue for future work would be to develop semantic representation models that exploit perceptual data that is both naturally occurring and easily accessible (e.g., images, physical simulations)."
D12-1130,1,in the future we plan to experiment with feature selection methods in an attempt to represent perceptual information more succinctly.
D12-1131,1,"in future work, we intend to explore efficient techniques for joint parameter learning for both the global mrf and the local models."
D12-1133,1,in future work we intend to explore joint models that incorporate not only basic part-of-speech tags but also more fine-grained morphological features.
D12-1134,2,3) modeling multi-modality data.
D12-1134,3,4) evaluation of the identified bursts.
D12-1134,1,1) variable-length context.
D12-1134,1,2) incorporation of more useful features.
D12-1136,1,our analysis (entropy estimates along with upper-bound numbers observed from experiments) suggest that there can be interesting future work to explore the contextual information provided by the stimulus more effectively and further improve the response completion task.
D12-1138,3,"we also verify that through the use of rich features, we can further improve the accuracy of our query spelling correction system."
D13-1002,5,"in future work, we will like to explore how to better exploit the various discourse analysis frameworks for temporal classification."
D13-1002,5,we believe it is interesting to examine how such information can help.
D13-1002,1,"we are also interested to apply discourse features in the context of a global inferencing system (yoshikawa et al., 2009; do et al., 2012), as we think such analyses will also benefit these systems as well."
D13-1003,1,we think that the good results motivate research into more integrated combinations of noise reduction approaches.
D13-1005,1,"the model’s correspondence with human behavioral results is by no means exact, but we believe these kinds of predictions might help guide future research on infant phonetic and word learning."
D13-1006,4,"our method has the added advantage of interpretability, which we believe will be useful when using it as a component in a larger system."
D13-1007,4,future work may consider whether sequential monte carlo can offer similar advantages in other unsupervised nlp tasks.
D13-1007,4,"an additional benefit of our joint statistical approach is that it may be combined with other downstream language processing tasks, such as part-of-speech tagging (gimpel , 2011) and named entity resolution (liu , 2012b)."
D13-1008,4,"in future work, we shall attempt to build normalizations for other languages."
D13-1008,1,"we shall also attempt to learn an unsupervised normalization model with only monolingual data, similar to the work for mt in (ravi and knight, 2011)."
D13-1009,4,"we also will investigate non-technical areas, where there might be no strongly distinct notion of experts and non-experts."
D13-1009,5,"in the future, we would like to explore predicting question difficulty from the text of question descriptions."
D13-1011,1,"in the future, we plan to investigate alternative training data selection techniques, disfluency handling strategies, search heuristics, and novel transduction grammar induction models."
D13-1012,4,"in future work, the proposed framework could readily be extended to model other aspects of scientific influence, such as the effects of authors and journals on topical influence, and to exploit the context in which citations occur.we envision that this line of work will also be useful for building visualization tools to help researchers explore scientific corpora."
D13-1013,1,"another challenge is related to the parser speed, since the number of candidates and features are much greater than the number used in classical dependency parsers."
D13-1013,1,"in future work, we will explore different learning algorithms which can help us address the sparsity problem and improve the model accuracy."
D13-1014,3,"• experiments on other semantics tasks, such as paraphrase detection, word sense induction, and word meaning in context."
D13-1014,5,• explore further the potential synergy between distributional semantics and the generative lexicon.
D13-1014,1,"• experiments on other semantics tasks, such as paraphrase detection, word sense induction, and word meaning in context.• extension to more holistic sentence-level composition using a matrix-vector recursive framework like (socher et al., 2012).• explore further the potential synergy between distributional semantics and the generative lexicon."
D13-1015,5,"in the meantime, we hope that the results we reported here provide convincing evidence of the usefulness of compositional distributional semantics in tackling topics, such as recursive adjectival modification, that have been of traditional interest to theoretical linguists from a new perspective."
D13-1015,1,"in our future work, we would like to develop an order model that exploits semantic, metrical and lexicalization features jointly for maximal classification accuracy."
D13-1017,3,we are comparing the performance of a single measure (say spmid or pmis) against the best measure for each task.
D13-1018,3,"as future work, we plan to study the ability of protodog to acquire domain glossaries at different levels of specificity (i.e., domains vs. subdomains)."
D13-1018,4,"finally, we will adapt protodog to other languages, by translating the glossary keyword used in step (2), along the lines of (de benedictis , 2013)."
D13-1019,4,"in the future, we plan to apply this technology to develop asrs for more languages."
D13-1020,2,if mctest is used we will collect more story sets and will continue to refine the collection process.
D13-1020,3,"we plan to use this dataset to evaluate approaches for machine comprehension, but are making it available now so that others may do the same."
D13-1020,1,one interesting research direction is ensuring that the questions are difficult enough to challenge state-of the-art techniques as they develop.
D13-1020,1,removing such questions may increase the difficulty for machines as well.
D13-1020,1,we will also experiment with timing the raters as they answer questions to see if we can find those that are too easy for people to answer.
D13-1020,1,one idea for this is to apply existing techniques automatically during story set creation to see whether a question is too easily answered by a machine.
D13-1020,1,"by requiring authors to create difficult questions, each data set will be made more and more difficult (but still answerable by humans) as the state-of-the-art methods advance."
D13-1020,1,"additionally, any divergence between how easily a person answers a question vs. how easily a machine does may point toward new techniques for improving machine comprehension; we plan to conduct research in this direction as well as make any such data available for others."
D13-1021,4,future work will investigate the proposed method in other domains and language pairs.
D13-1021,4,it is also worth extending the approach into word alignment in statistical machine translation.
D13-1023,1,"while dalm has outperformed state-of-the-art language model implementations methods in our experiments, we should continue to consider ways to optimize the method for higher-order ngrams."
D13-1023,1,"in future work, we will develop a faster algorithm for building double-array structures."
D13-1025,2,"as the translator works and corrects the proposed translations, the translation engine will be able to make better predictions."
D13-1025,3,"specifically, measures that estimate the cognitive load involve in reading, understanding and detecting an error in a translation (foster , 2002), in contrast ksmr simply considers a constant cost."
D13-1025,1,"more recently, ortiz-martinez et al.(2010) described a set of techniques to obtain an incrementally updateable imt system, solving technical problems encountered in previous works.• more sophisticated measures to estimate the human effort."
D13-1025,1,• adaptive translation engines that take advantage of the user’s corrections to improve its statistical models.
D13-1026,1,"we would like to investigate whether further improvement can be achieved by incorporating such features, especially the context model (shen et al., 2009) in the future."
D13-1026,1,"because our proposed model is quite general, we are also interested in applying this method to induce linguistically motivated synchronous grammars for syntax-based smt."
D13-1029,1,"finally, we would also like to explore the extent to which a joint probabilistic model (e.g., (durrett and klein, 2013)) might be used to learn how to best make this tradeoff."
D13-1030,2,"first, we plan to use both kinds of data, csn and asn antecedent data, which will give us a basis for developing a better performing asn resolver."
D13-1030,1,"that said, for the same nouns, the antecedents were in the first four ranks about 76% to 81% of the times, suggesting that in future research, our models can be used as base models to reduce the large search space of asn antecedent candidates."
D13-1030,1,"finally, we will examine whether a model trained for one shell noun can be generalized to other shell nouns from the same semantic category."
D13-1030,1,"we also plan to incorporate contextual features (e.g., right-frontier rule (webber, 1991) and context ranking (eckert and strube, 2000))."
D13-1031,4,"as our future work, we plan to apply our method to other natural language processing tasks, such as text chunking."
D13-1033,5,making use of the improved morphology in the dependency parser is not straight-forward and requires more investigation in the future.
D13-1036,3,in future work we want to explore in more detail the differences in performance of the different contingency measures.
D13-1036,1,"in future work we also want to explore ways of inducing larger event structures than event pairs, such as the causal chains, scripts, or narrative schemas of previous work."
D13-1040,1,"an appealing direction would be to learn these automatically e.g., via a procedure that optimizes some clustering objective."
D13-1040,1,"in the future, we would like to explore additional types of rules such as seed rules, which would assign tuples complying with the “seed” information to distinct relations."
D13-1040,1,"aside from devising new rule types, an obvious next step would be to explore different ways of extracting the rule set based on different criteria (e.g., the most general versus most specific rules)."
D13-1040,1,"finally, it should be interesting to use some form of distant supervision (mintz et al., 2009) either as a means of obtaining useful rules or to discard potentially noisy or uninformative rules."
D13-1046,2,"equally important, the size of the domain can be adapted so as to find enough context for all the words in domain reference lists."
D13-1046,1,"given that comparability of article versions in the source and the target language varies, we will evaluate algorithms for filtering out concepts from the target language that have low alignment with their source language versions."
D13-1046,1,"second, given a word in a context, we currently exploit all similar concepts from the target language."
D13-1046,1,"first, we will pursue the integration of our method, notably through comparable corpora creation using the data driven domain delimitation technique described in subsection 3.5."
D13-1046,1,a final line of work is constituted by the use of distributional properties of texts in order to automatically rank parts of concept descriptions (i.e. articles) by their relatedness to the candidate word.
D13-1046,1,"similar to the second direction, this process involves finding comparable text blocks but rather at a paragraph or sentence level than at the article level."
D13-1047,3,"and report results using multiple evaluation metrics (nenkova , 2007; louis and nenkova, 2012) as well as performing human evaluations."
D13-1047,1,"in future, we would like to further explore the reinforcement relationship between keywords and summaries (wan et al., 2007),"
D13-1047,1,"improve the readability of the sentences generated from the guided compression system,"
D13-1049,1,"promising directions for future work are joint parsing and reordering models, and measuring the influence of parsing accuracy on preordering and final translation quality."
D13-1050,1,"In the future, we plan to explore more approaches for phrase table pruning."
D13-1051,1,"though, our best system may not overpass he and toutanova (2009) who combine all the modules into a unified training procedure, we believe our method could boost many work on the higher modules of the pipeline to obtain a further improvement to match their work."
D13-1053,1,the general framework that uses an external reordering model in hierarchical models via features can also be naturally extended to use multiple reordering models.
D13-1053,1,"it also might be beneficial to look beyond syntactic constituent pairs when modeling reordering, given that phrasal cohesion does not always hold in translation."
D13-1054,4,"finally, it is possible to apply our method to other phrase-based and even syntax-based systems."
D13-1054,1,another problem with our system is that the decoding speed is much slower than the baseline system because of the computational overhead introduced by raes.
D13-1054,1,it is necessary to investigate more efficient decoding algorithms.
D13-1054,1,"first, replacing the maxent classifier with a neural one redefines the conditions for risk-free hypothesis recombination."
D13-1054,1,"therefore, we plan to use forest reranking (huang, 2008) to alleviate this problem."
D13-1054,1,"second, it is interesting to follow socher et al.(2013) to combine linguistically-motivated labels with recursive neural networks."
D13-1055,2,"a larger amount of labeled data would certainly help to improve the classifier performance for weak categories (e.g.vandalism and paraphrase) and sparse categories (e.g.template-d, markup-m)."
D13-1055,2,"with respect to future work, we plan to include more resources, e.g. the pan-wvc10 (potthast and holfeld, 2011) or wicopaco (max and wisniewski, 2010) to increase the size of training data."
D13-1055,1,"based on our trained classifier, annotating more examples can be alleviated with the help of active learning."
D13-1056,1,future work includes aligning discontinuous (gappy) phrases and integrating alignment more closely in nlp applications.
D13-1060,4,we look forward to generalizing our approach to other types of noncompositional phrases.
D13-1061,1,"(2) incorporate some common techniques, such as cascading, voting, and ensemble;"
D13-1061,1,and (3) use the special network architecture tailored for the tasks of interest.
D13-1061,1,"although we focus on the question of how far we can go for chinese word segmentation and pos tagging without using the extra task-specific features in this study, there are at least three ways to further improve the performance of the networks, which are worthy to be explored in the future: (1) introduce specific linguistic features (e.g. gazetteer features) that are helpful for the tasks;"
D13-1062,2,"besides, we would also like to investigate for other nlp tasks which have different annotation-style corpora."
D13-1062,1,"in the future, we will continue to refine the proposed model in two ways: (1) we wish to use the unsupervised method to extract the loose mapping relation between the different annotation standards, which is useful to the corpora without loose mapping guideline.(2) we will analyze the shared information (weights of the features derived from the tags which have the mapping relation) in detail and propose a more effective model."
D13-1064,2,it would also be interesting to experiment with more diverse languages types.
D13-1064,1,"embedding cluster-identifiers in a logical form allows us to also model logical operators, such as negation and quantifiers, which may help to improve the translation of these."
D13-1064,1,"as we use a flat clustering of relations, we are only able to model synonyms and not hypernyms."
D13-1064,1,there is much potential for future extensions to address the limitations of the process described here.
D13-1067,1,"in the future work, we will explore more kinds of social context information and investigate better ways of incorporating them into profile summarization and a wider range of social network mining."
D13-1069,2,"however, the data type is also important to impact the summary length."
D13-1069,2,"in addition, in the experiments, we only use the imbalanced datasets as the example that intuitively needs varying the summary length."
D13-1069,3,"in future we may consider more human factors, and prove the summary length determined by our system agrees with human preference."
D13-1069,6,"in future, we may extend the work by studying more cases that need varying summary length."
D13-1070,3,"in the future we would like to experiment further with refining the sentence selection method so as to consider criteria for local cohesion, such as lexical chains."
D13-1070,3,we also would like to perform a human based evaluation of coherence and explore the full potential of these summaries as alternatives to author-written abstracts.
D13-1070,3,we would also like to perform comparisons with automatically induced content models and check their viability for scientific articles.
D13-1071,1,we are also interested in exploring a* heuristics to further speed up our dp best-first parsing.
D13-1071,1,for future work we would like to improve the performance of the probabilistic models that is required by the best-first search.
D13-1072,4,"in the future, we want to further investigate the problem of domain adaptation when applying general language models to a new image dataset."
D13-1072,1,this problem can be integrated into the energy-based model during the training phase.
D13-1072,1,we plan to extend work on human action recognition by including the relative position between the human and object in the images.
D13-1073,5,"we hope to tackle the annotation bottleneck in future work on definition extraction, common in many data-driven learning fields."
D13-1073,1,"we can use the lexicon obtained from different years to carry out trend prediction, which we have illustrated here."
D13-1073,1,we think future work could pursue more in-depth analysis of the distributional and demographic properties of automatically extracted lexica.
D13-1073,1,"downstream systems may predict which term will become popular, or could alert an author if their definition of a term significantly differs from the original source."
D13-1073,1,"in addition, with respect to modeling, although we showed that doing definition classification before term classification does not improve over our single stage classifier, we hope to study whether suitable joint inference models can benefit from the interaction between the two classification processes."
D13-1073,1,"we plan to explore iterative, semi-supervised methods to best manage human effort to maximize the effectiveness of future annotation."
D13-1078,5,"but given an expression like monday, it would still be impossible to decide whether it refers to the future or the past, since the surrounding context, e.g.tense of the governing verb, is needed for such a judgment."
D13-1078,1,"in future work, we plan to replace the heuristic for selecting between ambiguous parses with a more principled approach."
D13-1078,1,"it would be a simple extension to support a probabilistic grammar, as in (angeli et al., 2012)."
D13-1078,1,a more promising approach would be to train a classifier that selects between the ambiguous parses based on features of the surrounding context.
D13-1079,5,"specifically, we call for the standardization of an ie rule language and outline an ambitious research agenda for nlp researchers who wish to tackle research problems of wide interest and value in the industry."
D13-1081,2,"• to replace the exhaustive translation rule setwith a compact meta grammar that can create and parameterize new translation rules dynamically, which is the ultimate goal of this line of work."
D13-1081,1,"to try other prior distributions to generate the number of source tokens. unsupervised and semi-supervised learning of hidden models. to incorporate rich models into the generative process, e.g.reordering, non-terminals, structural information and lexical models. to improve the posterior model with better parameter estimation, e.g. bayesian methods."
D13-1082,3,"our future work is to compare our conversion method with that of (arsoy , 2013)."
D13-1084,5,"future work could focus on cases with more than two languages, and languages that are typologically less distinct from each other or dialects (trieschnigg , 2012)."
D13-1087,3,"in addition to improving search, large-scale restarts can also provide a novel perspective when performing exploratory analysis, here letting us argue in support for the hypothesis that zodiac 340 is not a row-major homophonic cipher."
D13-1090,4,"these ideas may have applicability in other semantic similarity tasks, and we are also eager to apply them to new, large-scale automatically-induced paraphrase corpora (ganitkevitch , 2013)."
D13-1091,1,"in future work, we intend to design new similarity measurements that can make best of the advantages of twitter data."
D13-1095,1,"as future work, we plan to improve the author/reader detection model to improve the zero reference resolution."
D13-1096,1,this dataset will be valuable for both training and testing automatic response models for short texts.
D13-1098,4,we also intend to adapt the described system to other languages than english.
D13-1098,1,"there is no doubt that reaction detection can be improved a lot, by going beyond simple lexical features and discovering specific patterns."
D13-1099,1,our future work will focus on the pruning algorithm for the syntactic structures and analyzing errors in depth in order to get more effective features for the scope detection on different corpora.
D13-1100,1,"we will consider incorporating periodicities in other applications, such as topic models."
D13-1100,1,"in future work, we aim to model time continuously and to perform discriminative clustering in order to make better use of the learned periodicities."
D13-1101,3,we will also evaluate the effect of adding a decoding step to the constituent approach.
D13-1101,4,this work provides an accurate and complete quotation extraction and attribution system that can be used for a wide range of tasks in information extraction and opinion mining.
D13-1101,1,"future work will include extending these methods to extract all attributions, i.e. beliefs, eventualities, and facts, as well as the source spans."
D13-1103,1,"in the future, we plan to extend this work to more precisely pinpoint the answer location on a page, and consequently incorporate searcher behavior into subsequent answer extraction and ranking stages of question answering."
D13-1103,1,"we also plan to further investigate the examination data to better understand how searchers find correct (and incorrect) answers using both general web search engines and qa systems – in order to inform and further improve query suggestion, result snippet generation, and result ranking algorithms."
D13-1104,2,for the latter we have already started collecting parallel text in russian and english.
D13-1104,5,"for the future work we plan to use the corpus as a research tool to tackle the following problems: (i) automatic part of speech tagging, (ii) morphological disambiguation, (iii) statistical machine translation."
D13-1105,4,"we are interested in extending our approach to languages with different morphological systems, e.g., agglutinative or reduplicative."
D13-1105,1,we will explore ideas from unsupervised morphology learning to minimize the need for morphological annotations.
D13-1106,5,"we would also like to apply recent advances in tackling the vanishing gradient problem (pascanu , 2013) using a regularization term to maintain the magnitude of the gradients during back propagation through time."
D13-1106,1,"thus, the model itself can decide which encoding is best for the task."
D13-1106,1,"finally, we would like to integrate the recurrent model directly into first-pass decoding, a straightforward extension of lattice rescoring using the algorithm we developed."
D13-1106,1,"we also plan to change the cross entropy objective to a bleu-inspired objective in a discriminative training regime, which we hope to be more effective."
D13-1106,1,in future work we plan to directly learn representations of the source-side during training of the joint model.
D13-1107,4,"furthermore, since our approach is a general training method, we may also combine this approach with other domain adaptation methods to get more performance improvement."
D13-1107,1,"for other texts, we use the general system to translate them."
D13-1107,1,"for those domains that are identified with high confidence, we use the domain specific system to translate the texts."
D13-1107,1,"in the future, we will pre-define more popular domains and develop automatic domain classifiers."
D13-1108,1,"since constituency tree binarization can lead to more constituency-to-string rules and syntactic phrases in rule extraction and decoding, which improve the performance of constituency-to-string systems, for future work, we would like to do research on encoding binarized constituency trees to dependency trees to improve translation performance."
D13-1109,2,"recent work has identified nts words in new-domain corpora (carpuat , 2013b), and in future work we plan to incorporate discovered translations for such words into mt."
D13-1109,5,"although the use of marginal matching is, to the best of our knowledge, novel in mt, there are related threads of research that might inspire future work."
D13-1110,2,it has been shown to be useful for hiero in some languages therefore it is promising to improve translation quality in lrhiero which suffers from lack of modeling power of non-gnf target side rules.
D13-1110,1,"in future work, we plan to apply lexicalized reordering models to lr-hiero."
D13-1110,1,we also plan to extend the glue rules in lr-hiero to provide a better reordering model.
D13-1111,4,"future work could replace m-best lists with diverse lists in these and related tasks, whether for mt or other areas of structured nlp."
D13-1112,1,"for future work, we will consider other translation paradigms such as hierarchical phrase-based or syntax-based mt."
D13-1114,4,"we suspect that this result will generalize to the inference of other demographic characteristics (e.g., age and political orientation), though this must be explored in future work."
D13-1114,5,"this stands out as a clear direction for future work, particularly since apparent issues with the large number of unigrams used by japanese will create issues for handling (mandarin) chinese, the world’s most-spoken language."
D13-1114,1,"though there has been relatively little investigation into latent attribute inference outside of english language content, we consider it both a fruitful and important area for future research."
D13-1114,1,identifying and leveraging such features will be an interesting and fruitful direction for future work.
D13-1114,1,the positive results suggest that latent attribute inference in the non-english context as a research direction worthy of further attention.
D13-1116,2,"as well as repeating these experiments for languages which rely more on function annotation, we also plan to apply our method to other types of annotations, e.g.more linguistically motivated binarization strategies or – of particular interest to us – annotation of empty elements."
D13-1117,5,investigating how to better optimize this non-convex regularizer online and convincingly scale it to the semisupervised setting seem to be promising future directions.
D13-1120,5,"finally, we would like to explore just how much the statistic properties of our data dictate the success of the model by looking at related problems like morphological analysis of unsegmented languages such as japanese."
D13-1120,1,"analysis of the tagging errors still being made have suggested some possibly avoidable inconsistencies in the grammar and treebank, which have been fed back to the developers, hopefully leading to even better results in the future."
D13-1120,1,"in future work, we will investigate more advanced smoothing methods to try and boost the uber tagging accuracy."
D13-1120,1,"we also intend to more fully explore the domain adaptation potentials of the lexical model that have been seen in other parsing setups (see rimell and clark (2008) for example), as well as examine the limits on the effects of more training data."
D13-1121,4,"thus, although this paper focused on the activepassive alternation in japanese, our framework is applicable to the other types of case alternation and to other languages, especially similar languages such as korean."
D13-1121,4,we plan to apply our framework to other types of case alternation such as case alternation between intransitive and transitive verbs.
D13-1122,1,"for future work, we would like to explore knowledge from more sources to enhance our model, such as semantic thesauri and info boxes in encyclopedias."
D13-1126,2,our next step is to try these same techniques with spoken questions and spoken answers in a low-resource language using the test collection that is being developed for the mediaeval 2013 question answering(qa) for the spoken web task.
D13-1126,5,"in the long term, many of the questions we are exploring will also has implications for open-domain web search in other hands- or eyes-free applications such as driving a car or operating an aircraft."
D13-1126,1,another potentially productive direction for future work would be to somehow filter the queries in ways that improve the rankings.
D13-1127,2,"as a future work, we plan to adapt parameters automatically on the basis of different types of datasets."
D13-1127,4,"furthermore, our framework is universal, so that the media other than text and image can be adopted as well."
D13-1127,1,improving the layout quality of story map by concerning the interactivity of different media (e.g. images order) is also significant.
D13-1128,1,"future work will focus on improvements to the image parser, on exploring this representation in open domain data sets, and on using the output of an object detector to obtain a fully automated model."
D13-1129,4,"the idea described in this paper is general and can be applied to other nlp applications, such as part-of-speech tagging and chinese word segmentation, in future work."
D13-1130,1,"further work can be considered to improve segmentation of documents characterized by small segments and few words repetitions, such as using semantic relations or vectorization techniques to better exploit implicit relations not considered by lexical reoccurrence."
D13-1131,1,"in the future, we would like to investigate other methods for generating robust inter-feature laplacians that include deep syntactic and semantic features."
D13-1132,1,"for future work, we will consider incorporating a trend detection component into our method, which can be more flexible to adapt to various trend signals."
D13-1132,1,we can also refine the method of the product keyword extraction by using more principled solutions.
D13-1133,2,"more data (e.g.kosinski (2013)) and more sophisticated models (e.g.supervised lda, blei and mcauliffe (2008), and extensions such as nguyen (2013)) will be the key to further progress."
D13-1133,1,"more data (e.g. kosinski et al.(2013)) and more sophisticated models (e.g. supervised lda, blei and mcauliffe (2008), and extensions such as nguyen et al.(2013)) will be the key to further progress."
D13-1134,3,and evaluate whether this actually improves task performance.
D13-1134,4,"the immediate next step for future research is to extend our model to an implemented end-to-end situated nlg system for the give challenge,"
D13-1134,1,we will furthermore improve pobs by switching to a more temporally dynamic probability model.
D13-1136,4,"our modeling approach is general and should apply to other settings, e.g.for the task of entity linking."
D13-1137,1,"as future work, we plan to combine deep learning models with richer information such as predicate argument structures."
D13-1138,4,in future work we plan to apply our method to a wider range of languages.
D13-1139,4,future work will investigate our methods usefulness on various language datasets.
D13-1139,1,"we plan to study more general methods that use word alignments to embed swap information in trees (galley et al., 2006)."
D13-1141,1,"further, our results offer suggestive evidence that bilingual word embeddings act as high-quality semantic features and embody bilingual translation equivalence across languages."
D13-1142,1,"based on evaluation results and despite differences between the evaluators with background knowledge and the crowds, we can conclude that that our approach for automatic construction of in-text links rivals manual creation by professional writers and bloggers and is thus a promising direction for further research."
D13-1144,4,"the approach can generalize to any task involving structural and local similarity, and arbitrary node similarity measures."
D13-1145,3,and evaluate it on more varied sentence types.
D13-1145,1,we expect that semi-supervised learning techniques could better recover the missing labels and boost overall performance.
D13-1145,1,"we also think it should be possible to scale the detection approach, perhaps with automatic dictionary definition discovery,"
D13-1146,3,"we further plan to test our method on a wider range of datasets, allowing a more direct comparison with other approaches."
D13-1146,4,"in future work, we plan to broaden the scope of this work to other steps in document preparation,such as normalization of punctuation, and their interaction with segmentation."
D13-1146,1,"finally, we plan to explore the possibility of a statistical universal segmentation model for mutliple languages and domains."
D13-1149,3,"future work will explore using an individual annotators history across trials to weight that users contributions, something that verbcorner was specifically designed to allow (see above). future research will be needed to assess this tradeoff."
D13-1152,1,"in future, we are interested in training parsers favoring the dynamic feature selection setting, for example, parsers that are robust to missing features, or parsers optimized for different stages."
D13-1156,4,"as graph cut is a general method, applying it to other binary structured learning tasks is also an interesting direction."
D13-1156,4,"the other is to adapt it to social media text summarization task, where text is much more noisy."
D13-1156,1,"one idea is to apply it to the language model based compression method (clarke and lapata, 2008)."
D13-1156,1,there are several possibilities for further research involving our graph cut algorithms.
D13-1157,4,"in the future, we would like to tackle more challenging domains, such as nfl recaps, financial articles and biographies (howald , 2013; schilder , 2013)."
D13-1157,1,"our models could also benefit from the development of more sophisticated planners either via grammar refinement or more expressive grammar formalisms (cohn et al., 2010)."
D13-1159,1,"in the future, we plan to employ document summarization techniques to shorten the depth of cets."
D13-1159,1,"we also aim to incorporate semantic analysis and normalize named entities to canonical entities, which make cet more suitable for practical use."
D13-1160,4,"learning these composite predicates would drastically increase the possible space of logical forms, but we believe that the methods proposed in this paper— alignment via distant supervision and bridging—can provide some traction on this problem."
D13-1161,4,"although we focused exclusively on qa applications, the general two-stage analysis approach should allow for the reuse of learned grammars across a number of different domains, including robotics or dialog applications, where data is more challenging to gather."
D13-1163,1,"in the future, we would like to use lexical chains to identify coherence and incorporate both cohesion and coherence into document-level machine translation."
D13-1164,5,future work will consider ways to speed up our algorithm and extensions of the method to more complex alignment models.
D13-1165,4,"ultimately, we would like to apply these methods to the normalization of social media text, especially to find alternative spellings based on alternative pronunciations."
D13-1166,4,"as for future work, we aim to investigate the application of this procedure to the regression model of grefenstette (2013)."
D13-1167,2,"for instance, the knowledge encoded by mrlsa can be enriched by adding more relations from a variety of linguistic resources, including the co-occurrence relations from large corpora."
D13-1167,1,"following the strategy of using siamese neural networks to enhance pilsa (yih et al., 2012), training mrlsa with a multi-task discriminative learning setting can be a promising approach as well."
D13-1167,1,"on model refinement, we notice that mrlsa can be viewed as a 3-layer neural network without applying the sigmoid function."
D13-1167,1,"for future work, we plan to explore directions that aim for improving both the quality and word coverage of the model."
D13-1168,3,"finally, we also plan to test the robustness of our fully corpus-based bootstrapping approach by porting it to more language pairs."
D13-1168,5,"furthermore, we plan to study other confidence functions and explore if asymmetric translation candidates could also contribute to the bootstrapping method."
D13-1168,1,"in future work, we will investigate other models of similarity besides topicbc and responsebc (e.g., the method from (haghighi et al., 2008)) that could be used as preliminary models for constructing an initial bilingual vector space."
D13-1171,3,"in future work, we hope to explore further methods for teasing apart sentiment polarity expressed towards a target."
D13-1172,1,"in our future work, we plan to incorporate aspect specific sentiments in the mclda model."
D13-1174,1,"therefore, an important extension of our work is to explore the interaction of our approach with more sophisticated language models that more directly model morphology, e.g., the models of bilmes and kirchhoff (2003), or, alternatively, ways to incorporate target language context in the inflection model."
D13-1175,5,"while this technique is useful to develop patent retrieval systems, it would be interesting to see if our results transfer to patent retrieval scenarios where full patent documents are used instead of only abstracts, or to standard clir scenarios that use short search queries in retrieval."
D13-1177,1,"last, we would like to integrate our method into qa systems and allow non-factoid questions that require deeper reasoning to be answered by matching the questions against the learned process structures."
D13-1177,1,"in future work, we want to perform trigger identification jointly with extraction of event event relations."
D13-1177,1,"alternatively, we can search on the web for redundant descriptions of the same process and use this redundancy to improve classification."
D13-1178,1,our future plans are to build upon our event schemas to create an open-domain event extractor.
D13-1180,1,"because of the diversification of writing features of essays associated with different prompts, a viable approach is to explore more generic writing features that can well reflect the writing quality."
D13-1180,1,"in our future work, we plan to continue the research on generic rating model."
D13-1182,1,"ultimately, we would like to correlate patterns in physician communication (as gleaned from the model) with objective, measured health outcomes (e.g., patient satisfaction and adherence to arvs)."
D13-1182,1,we also plan on extending this model to investigate qualitative questions surrounding patient physician communication quantitatively.
D13-1182,1,"to explore this, we can add additional components to the transition probability terms corresponding to different hospitals and doctors."
D13-1183,2,"in order to spur future research, we are releasing an annotated corpus of time-stamped news articles and our harvested relation clusters."
D13-1184,1,"one possibility for future work is to supply this framework with a richer set of relations from the text, such as verbal relations."
D13-1184,1,it will also be interesting to incorporate high-level typed relations and relax the relation arguments to be general concepts rather than only named entities.
D13-1185,1,there is ample room for improvement and future research in event schema induction.
D13-1187,1,"in future work we plan to develop new models for joint modeling of personalized sentiment, user demographics e.g., age and user preferences e.g., political favorites in social media."
D13-1189,2,"in future work, we will try to collect and annotate data for microblogs in other languages to test the robustness of our method."
D13-1189,1,the repost and reply messages can also be integrated into our graph model to help improve the results.
D13-1192,1,"furthermore, the event-topic association inferred by our model can help an event recommendation task and organize events by topics."
D13-1194,3,studies of this kind may also inform future data annotation efforts in that certain ways of anchoring the elements of a comparison linguistically may be more helpful than others.
D13-1194,3,we also want to further study the different possible linguistic anchorings of comparisons and their effect on classification performance.
D13-1194,1,"to address the inherent diversity of expressions typical for user generated content, we want to employ generalization techniques, e.g., to detect product names."
D13-1194,1,"for future work, we plan to include features that have been tailored specifically to the task of detecting product comparisons."
D13-1194,1,"we also believe that the explicit modeling of different types (equative, superlative, non-equal gradable) of comparisons will have a positive effect on performance."
D13-1196,1,"we plan to experiment next with more linguistically motivated ways to adapt the latter to recursive composition, including hybrid methods where ans and nns are treated differently."
D13-1196,1,"we would also like to consider more sophisticated semantic plausibility measures (e.g., supervised ones), and apply them to other ambiguous syntactic constructions."
D13-1198,5,"in the future, we plan to investigate this finding in the context of other, similar ranking problems in natural language processing."
D13-1200,3,"as future work, we plan to evaluate how the quality is impacted by the time dimension (adaptation delay, cache reset,etc.)."
D13-1202,3,"collecting our own fmri data will also allow us to move beyond exploratory analysis, to test sharper predictions about distributional models and their brain area correlates."
D13-1202,3,in future experiments it may prove valuable to configure a fmri stimulus set where text-based and image-based interrelationships are maximally different.
D13-1202,1,"there are also many opportunities for focusing analyses on different subsets of brain regions, with the semantic system identified by binder et al.(2009) in particular presenting one interesting avenue for investigation."
D13-1202,1,"this not only presents an optimistic outlook for the future use of image-based models as an interpretative tool to explore issues of cognitive grounding, but also demonstrates that they are capturing useful additional aspects of meaning to the text models, which are likely relevant for computational semantic tasks."
D13-1204,1,"furthermore, we are optimistic that both count transforms and model recombination could be usefully incorporated into sampling methods: although symmetrized models may have higher cross-entropies, hence prone to rejection in vanilla mcmc, they could work well as seeds in multi-chain designs;"
D13-1204,1,"future parsing models, in grammar induction, may benefit by modeling head-dependent relations separately from direction."
D13-1204,1,"existing algorithms, such as mcmcmc (geyer, 1991), which switch contents of adjacent chains running at different temperatures, may also benefit from introducing the option to combine solutions, in addition to just swapping them."
D14-1002,2,we believe that the prior browsing and interaction history recorded in the session provides additional signals for predicting interestingness.
D14-1002,1,"one area of future work is to extend our method to model interestingness given an entire user session, which consists of a sequence of browsing events."
D14-1002,1,"one potentially effective model for such a purpose is based on the architecture of recurrent neural networks (e.g., mikolov et al.2010; chen and deng, 2014), which can be incorporated into the deep semantic model proposed in this paper."
D14-1002,1,"to capture such signals, our model needs to be extended to adequately represent time series (e.g., causal relations and consequences of actions)."
D14-1003,1,"besides its good performance in practice, the bidirectional architecture is of theoretical interest as it allows the exact modeling of posterior probabilities."
D14-1004,3,we also want to further investigate the advantages and disadvantages of having different embedding matrices for different argument positions in our multi-way neural network.
D14-1004,3,"we want to investigate the benefit of our approach, compared to a model that shares the distributed word representation among different argument positions."
D14-1004,5,"first of all, we would like to investigate how our neural network approach might be improved by incorporating information from other sources."
D14-1004,1,"in particular, we think of initializing our embedding matrices with distributed representations that come from a large-scale neural language model (mikolov et al., 2013)."
D14-1004,1,"finally, we want to investigate more advanced neural network architectures for the acquisition of selectional preferences."
D14-1004,6,we conclude with a number of issues for future work.
D14-1005,1,"in future work, we will investigate whether our system can be further improved by including concreteness information or a substitute metric such as image dispersion, as has been suggested by other work on multi-modal semantics (kiela et al., 2014)."
D14-1005,1,"furthermore, a logical next step to increase performance would be to jointly learn multi-modal representations or to learn weighting parameters."
D14-1005,1,"another interesting possibility would be to examine multi-modal distributional compositional semantics, where multi-modal representations are composed to obtain phrasal representations."
D14-1006,2,"for future work, we plan to extend our studies to larger corpora, to integrate our classifiers in writing environments, and to investigate their effectiveness for supporting students."
D14-1009,1,"in future work we plan to include probabilistic and distributional features from a top-down incremental parser e.g. roark et al.(2009), and use stir distributional features to classify repair type."
D14-1010,1,in future work we will examine partial-label learning with this more enforced lattice constraint in depth.
D14-1011,1,another future research is to speed-up our model.
D14-1012,1,"for future work, we are exploring a novel and a theoretically more sounding approach of introducing embedding kernel into the linear models."
D14-1015,1,"in the future, we will explore more features to refine the model and try to utilize contextual information in target sentences."
D14-1016,1,"in the future, we will refine the method by considering neighbor words and alignments when discarding noisy links."
D14-1017,1,"as our future work, we will investigate more precise methods for deciding function words and content words for better alignment and translation qualities."
D14-1018,3,"in future work, we intend to obtain training data from multiple domains that enables us to verify cross-domain scalability of pos-mtus."
D14-1018,1,another future direction for this work is leveraging sentence level translation direction detection to improve statistical machine translation output quality.
D14-1018,1,"in addition, observing linguistic phenomena that occur in one translation direction but not the other can be very informative in improving statistical machine translation quality."
D14-1018,1,"finally, further investigation of the linguistic interpretation of individual feature that are most discriminating between opposite translation directions can lead to discovery of new linguistic phenomena that occur during the translation process."
D14-1019,1,another interesting direction is to determine the appropriate number of clusters for each corpus and the initialization method for clustering.
D14-1019,1,we expect to make a new objective that includes the terminal symbols and the reordering of nonterminal symbols that were ignored in this work.
D14-1019,1,future work involves improving the optimization criterion.
D14-1023,4,the connecting phrase-based method can also be applied to lm adaptation.
D14-1024,4,"further, we will apply our linguistically informed method to other phenomena which cause similar issues for smt."
D14-1024,4,"in future research, we will extend our method to other language pairs which exhibit the same type of translation asymmetries when it comes to pvs."
D14-1024,1,"when it comes to mwes, we will pay special attention to the compositionality aspect since it seems to have contributed most to the good performance achieve by our method in the study presented here."
D14-1026,2,"the size and diversity of the topics in the corpus, along with its relatively high annotation quality (measured by iaa scores) makes it a useful resource for future research on arabic mt."
D14-1026,1,"moreover, the strong performance of our albleu metric is a positive indicator for future exploration of richer linguistic information in evaluation of arabic mt."
D14-1027,1,"this would also include more semantic information, e.g., in the form of brown clusters or using semantic similarity between the words composing the structure calculated with latent semantic analysis (saleh et al., 2014b)."
D14-1027,1,"in the future, we plan to work towards our long term goal, i.e., including more linguistic information in the skl framework and showing that this can help."
D14-1027,1,"we further want to show that the proposed framework is flexible and can include information in the form of quality scores predicted by other evaluation metrics, for which a vector of features would be combined with the structured kernel."
D14-1030,1,future work the work described here is our first attempt along the promising endeavor of matching complex computational models of language with brain processes using brain recordings.
D14-1030,1,we plan to extend our efforts by (1) collecting data from more subjects and using various types of text and (2) make the brain data help us with training better statistical language models by using it to determine whether the models are expressive enough or have reached a sufficient degree of convergence.
D14-1032,1,"this may involve filtering the perceptual input stream for concepts according to concreteness, and possibly more elaborate model architectures that facilitate distinct representational frameworks for abstract and concrete concepts."
D14-1032,1,"in future we will address the question of whether type iii concepts can ever be enhanced via multi-modal learning, and investigate multimodal models that optimally learn concepts of each type."
D14-1034,1,"we will also adapt it to a multilingual setup, aiming to model a wide range of languages."
D14-1034,1,in the future we intend to improve our model by encoding additional information in it.
D14-1035,1,"having established this procedure and its relative tolerance for low amounts of data, we would like to extend the model to make use of partial bracketing information instead of complete trees, perhaps in the form of fragmentary unlabeled dependency grammar annotations (schneider et al., 2013)."
D14-1038,1,"in the future, we plan to extend renoun to extract triples whose components are not limited to freebase ids."
D14-1038,1,"as an example, extending renoun to handle numerical or typed attributes would involve extending our extraction pattern learning to accommodate units (e.g., kilograms) and other special data formats (e.g., addresses)."
D14-1039,1,such methods could be combined with hierarchical classification to yield further gains.
D14-1039,1,an interesting extension of this work is to rely upon the natural clustering of related documents.
D14-1040,1,it is also worth studying other models that induce latent semantic concepts from multilingual data (see sect.2) within this framework of context-sensitive clss modeling.
D14-1040,1,"one may also investigate a similar approach to context sensitive clss modeling that could operate with explicitly defined concept categories (gabrilovich and markovitch, 2007; cimiano et al., 2009; hassan and mihalcea, 2009; hassan and mihalcea, 2011; mccrae et al., 2013)."
D14-1043,2,we would like to annotate more games to improve our dataset.
D14-1043,1,"we did not attempt to learn this information in our process, but it is likely that modeling the event transition probabilities could provide better results."
D14-1043,1,a larger future work would extend the method outlined herein to produce templates for automated commentary generation.
D14-1043,1,we could improve our model by encoding the dynamics of the environment.
D14-1045,4,"we anticipate that this line of research will also be of interest for a range of related tasks beyond traditional srl, including predicate-argument structure alignment (roth and frank, 2012) and implicit argument linking (gerber and chai, 2012)."
D14-1045,1,"in future work, we plan to apply more sophisticated models of compositionality to better represent predicate-argument structures and to guide classification decisions towards outcomes that are semantically more plausible."
D14-1046,4,"we are planning to extend this work to include domain clusters to improve the domain assignment results, namely in terms of recall."
D14-1047,2,"as future work, we plan to use these filters to build thesauri from larger corpora."
D14-1047,3,"finally, we would like to compare the proposed heuristics with more sophisticated filtering strategies like singular value decomposition (landauer and dumais, 1997) and non-negative matrix factorization (van de cruys, 2009)."
D14-1047,4,we would like to generalize our findings to other syntactic configurations (e.g.noun-adjective) as well as to other similarity and informativeness measures.
D14-1048,4,"as the proposed system learns the alignments automatically using very little domain knowledge, it can be applied in any domain and for any language with minor adaptations."
D14-1048,4,"hence, a natural extension to this work will be automatically parsing english sentences into amr and generating english sentences from amr."
D14-1048,1,computing the alignments between english sentences and amr graphs is a first step for extraction of semantic interpretation and generation rules.
D14-1049,1,future work should explore further approaches to parse partial syntactic structure specific to some target semantic relations.
D14-1050,5,"in the future, we plan to investigate the use of semantic similarity from distributional and other sources (mihalcea , 2006; pado and lapata, 2007), e.g., wikipedia (strube and ponzetto, 2006; mihalcea and csomai, 2007), wiktionary (zesch , 2008), wordnet (pedersen , 2004; agirre , 2009), framenet, verbnet (shi and mihalcea, 2005), babelnet (navigli and ponzetto, 2010), and lsa, and for different domains."
D14-1050,2,"in the future, we plan to investigate the use of semantic similarity from distributional and other sources (mihalcea et al., 2006; pad麓o and lapata, 2007), e.g., wikipedia (strube and ponzetto, 2006; mihalcea and csomai, 2007), wiktionary (zesch et al., 2008), wordnet (pedersen et al., 2004; agirre et al., 2009), framenet, verbnet (shi and mihalcea, 2005), babelnet (navigli and ponzetto, 2010), and lsa, and for different domains."
D14-1051,3,"in a future work, we plan to evaluate this new representation of textual documents in other information retrieval tasks, such as keyword extraction or automatic summarization systems."
D14-1052,3,"in the future, we intend to test our model on sentiment classification at the sentence-level, based only on document-level supervision ."
D14-1052,1,"in the longer term, we plan to investigate new methods to estimate instance weights at prediction time, and to evaluate the impact of assigned weights on sentence ranking, segmentation or summarization."
D14-1052,1,"moreover, we will experiment with other model settings, such as regularization norms other than e2 and feature spaces other than bow or tf-idf."
D14-1054,4,"in the future, we plan to apply the joint model on other domains, such as movie/product reviews."
D14-1055,2,"in future work, we will discuss the application of our proposed method in the massive dataset."
D14-1056,1,"our next planned task is clustering different shell nouns based on the kind of complements they take in different usages similar to verb clustering (merlo and stevenson, 2000; schulte im walde and brew, 2002)."
D14-1057,3,"since the number of clusters has an influence on the quality of the ensuing semantic classification, we will also be running our experiments with different settings of k to explore whether this also influences the overall results of our evaluation."
D14-1058,2,"in addition, we plan to focus on incrementally collecting domain knowledge to deal with missing information gaps."
D14-1058,1,another possible direction is to improve parsing and coreference resolution.
D14-1061,1,"given the positive results we obtain by using the joint approach to improve word alignment, we are inspired to apply this approach to help find translations for out of vocabulary words, and to explore other possible ways to improve machine translation with decipherment."
D14-1062,1,"in future work, we plan to explore generative bayesian models as well as discriminative learning approaches with different ways for estimating the latent domain relevance models."
D14-1064,4,another possible future endeavor is to extend these ideas to (i) other query translation approaches and (ii) document translation.
D14-1064,5,"while the exact same problem can be formulated for learning to translate documents effectively, a more complicated infrastructure and longer running times are two challenges that need to be considered."
D14-1064,1,"finally, we hope this to be a significant step towards more context-dependent and robust clir models, by taking advantage of modern translation technologies, as well as machine learning techniques."
D14-1065,4,our tensor-based representation of topically-segmented multilingual documents can also be applied to cross-lingual information retrieval or multilingual document categorization.
D14-1066,1,"furthermore, we want to incorporate features using n-grams computed on a corpus from the domain and include co-occurrence features."
D14-1066,1,"in future work, we will work on integrating dts using other context features, as we could see an impact of using two different dts."
D14-1068,1,"finally, our future work will explore kle and homoglyph correction bidirectionally, as opposed to the unidirectional approach explored in this work."
D14-1070,2,"a promising avenue for future work would be to incorporate wikipedia data into qanta by transforming sentences to look like quiz bowl questions (wang , 2007) and to select relevant sentences, as not every sentence in a wikipedia article directly describes its subject."
D14-1070,2,"syntax-specific annotation (sayeed , 2012) may help in this regard."
D14-1070,1,"having learned many facts about entities that occur in question text, a dt-rnn could add new facts to a knowledge base or check existing relationships."
D14-1070,1,"finally, we could adapt the attribute space learned by the dt-rnn to use information from knowledge bases and to aid in knowledge base completion."
D14-1071,1,"for our future work, we will build concept-level context embeddings by leveraging latent meanings of nles rather than their surface n-grams with the aligned logical features on kb."
D14-1073,2,"and of course it would be better if this standard dataset was multilingual instead of billingual, for obvious reasons."
D14-1073,2,further we would also need sets of human gold-standard query biased summaries in l1 and l2.
D14-1073,3,"these standards and data would allow us to compare method-to-method across different languages, while simultaneously allowing us to tease apart other variables such as: when and what to translate, translation quality, methods for biasing, and type of summarization strategy (sentences, words, etc)."
D14-1074,4,"finally, we hope that some of the work described here might be of relevance to other generation tasks such as summarization, conceptto-text generation, and machine translation."
D14-1074,6,"we would like to generate poems across different languages and genres (e.g., engish sonnets or japanese haiku)."
D14-1074,1,we would also like to make the model more sensitive to line-to-line transitions and stylistic conventions by changing its training objective to a combination of cross-entropy error and bleu score.
D14-1075,3,"therefore we intend to pursue future research in utilizing word-sense disambiguation and synonyms, as well as other techniques for furthering reaper query similarity metrics in order to improve its rouge and human evaluation scores on query-focused tasks."
D14-1077,2,"as future work, we plan to extend the data set with more clusters and more reference summaries, as well as to develop sentence compression methods for turkish mds."
D14-1080,1,"additionally, alternative notions of depth that are orthogonal to stacking, as in pascanu et al.(2013) can be investigated for this task."
D14-1080,1,another direction is to investigate the impact of finetuning the word vectors during supervised training.
D14-1080,1,one potential future direction is to explore the effects of pre-training on the architecture.
D14-1082,1,"there is still room for improvement in our architecture, such as better capturing word conjunctions, or adding richer features (e.g., distance, valency)."
D14-1082,1,an interesting line of future work is to combine our neural network based classifier with search based models to further improve accuracy.
D14-1087,4,"future work includes supporting multi-argument templates, disambiguating headwords of category names and applying our approach to general short text template mining."
D14-1088,4,"in the future, we will apply the proposed taxonomy construction method to other domains such as biomedicine and integrate it into other frameworks such as ontology authoring."
D14-1089,2,"in addition, we intend to expand the bootstrapping experiments with variations over the training data."
D14-1089,3,"in the future, we intend to assess how specific slots are affected by recall and search space tradeoff, and perform evaluation over all slot types: names, values and strings."
D14-1090,4,(2) transfer the results of this investigation to other complex nlp tasks that can potentially benefit from joint inference;
D14-1090,1,"and (3) develop scalable inference and learning algorithms (ahmadi et al., 2013)."
D14-1090,1,"in future work, we plan to (1) improve our joint model by incorporating co-reference information and developing model ensembles;"
D14-1096,2,"in future work, we would like to improve the performance of dpm by collecting more parallel data."
D14-1096,2,we would like to experiment with different source languages other than english.
D14-1096,2,duong (2013a) pointed out that using a different source language can greatly alter the performance of the target language pos tagger.
D14-1100,4,"in our future research, we will exploit the proposed framework to resolve other parsing difficulties in chinese, e.g., n-n combination."
D14-1100,1,"finally, for real world knowledge learning, we will continue to learn more useful knowledge by auto-parsing to improve the parsing performance."
D14-1100,1,we will also extend the semantic type predication algorithm (figure 2) to deal with all chinese words.
D14-1101,1,we may study the integration of these embeddings into our approach as future work.
D14-1103,1,the different granularities of the hierarchy induced by split-merge training are potentially useful.
D14-1103,1,"we would like to use additional information (e.g., from the dependency trees) to identify useless splits."
D14-1103,1,we think that coupling parents and children in the tag hierarchy might be one way to force a consistent hierarchy.
D14-1104,4,"for future work we plan to extend this work to further weight functions, data sets and nlp tasks."
D14-1105,2,"we plan to continue our work in that direction, specifically for conversational text in social media in a multilingual context."
D14-1105,5,"the challenges and issues identified in this study are likely to hold for many other languages as well, which makes this a very important and globally prevalent problem."
D14-1106,5,"finally, we plan to investigate why our system performed so much better on enni than on nsr."
D14-1106,5,"second, we hope to explore features that could be useful for identifying grammatical errors in multiple data sets."
D14-1106,6,there are several key areas we plan to investigate in the future.
D14-1106,1,"first, we would like to explore different update functions for the parser; the predicted error codes are a byproduct of parsing, but we do not care what the parse itself looks like."
D14-1107,2,"one interesting angle would be to increase the amount of information in ccgbank lexical entries, to further reduce the search space for the parser."
D14-1107,1,"incorporating specific models for such decisions may improve accuracy, while still allowing fast and exact search鈥攆or example, we intend to try including coppola et al.(2011)鈥檚 model for prepositional phrase attachment."
D14-1110,5,2.we can explore other wsd methods based on sense vectors to improve our performance.
D14-1110,5,"there are still several open problems that should be investigated further: 1.because the senses of words change over time (new senses appear), we will incorporate cluster-based methods in our model to find senses that are not in the sense inventory."
D14-1110,1,"for example, (li et al., 2010) used lda to perform data-driven wsd in a manner similar to our model."
D14-1110,1,we may integrate the advantages of these models and our model together to build a more powerful wsd system.
D14-1110,1,"3.to learn better sense vectors, we can exploit the semantic relations (such as the hypernym and hyponym relations defined in wordnet) between senses in our model."
D14-1111,1,"further numerical techniques for improving the estimation of the class decision boundary, and consequently the f-score, will also constitute future work."
D14-1112,4,"we plan to apply our similarity method on a corpus of spoken language, and to extend our analysis to other languages as well, as we gain access to available resources."
D14-1112,1,we further intend to combine our orthographic approach with syntactic and semantic evidence for a wider perspective on language similarity.
D14-1113,1,in future work we plan to use the multiple embeddings per word type in downstream nlp tasks.
D14-1114,1,"there are several directions for future work, including using both types and domains in freebase schema, diving into refiners and looking for a proper weighting method, developing a query recommendation framework based on the intent topic graph and user interest modeling."
D14-1116,4,"and d) the learning system has the advantage of being easily adapted to new settings, and we plan to extend it to other domains and languages (liang and potts, 2014)."
D14-1116,5,"in the future, we plan to address the following limitations that still exist in the current system: a) numerous hand-labeled data are required for training the mln, and we could use a latent form of semantic item query graphs (liang , 2013);"
D14-1116,1,b) more robust solutions can be developed to find the implicit relations in questions;
D14-1118,4,2) apply rcm to non-technical domains.
D14-1118,4,it is worthy investigating whether rcm still works on such domains.
D14-1118,5,"we would like to investigate how to deal with the bottleneck, e.g., via parallel or distributed computing."
D14-1118,1,"as future work, we plan to 1) enhance the efficiency and scalability of rcm."
D14-1121,4,"given that manual lexica are already extensively employed in social sciences such as psychology, economics, and business, using lexical representations of data-driven models allows the utility of our models to extend beyond the borders of the field of nlp."
D14-1124,2,"finally, we plan to apply our novel feature set to other corpora (e.g., argue) in order to study the utility of these features across genres and with respect to the accuracy of the discourse parser."
D14-1124,5,"furthermore, we will also explore how to create a discourse tree from the thread structure of a conversation (instead of from its temporal structure), and verify whether this improves the accuracy of the relation graphs, especially when the temporal structure is not representative of the reply-to relationships."
D14-1124,1,"additionally, we will incorporate generalized dependency and pos features (abbott et al., 2011), which were not used in this analysis due to the very small number of training samples in our dataset."
D14-1124,1,"in future work, we will improve sentiment features by considering methods to detect opinion topic pairs in conversation, similar to somasundaran and wiebe (2009)."
D14-1124,1,"the fragment quotation graph features did not perform as well as we expected, and in future work we would like to investigate this further."
D14-1125,1,it would be promising to combine our method with other methods to enable it to find +effect and -effect senses that are outside the coverage of wordnet.
D14-1128,2,"in future work, we would like to show that our findings generalize from the case of 鈥渉ard鈥 to the entire sentiment lexicon."
D14-1129,3,"besides, we will evaluate the effectiveness of our mined data on mt or other applications."
D14-1129,1,"in the future work, we will study some method on extracting parallel resource from existing parallel page pairs, which are challenging tasks due to the diversity of page structures and styles."
D14-1131,1,"there are a number of directions that we intend to investigate to speed up our decoder, such as: (1) error-safe pruning based on search error bounds; (2) use of reinforcement learning to guide the decoder in choosing which n-gram contexts to extend; and (3) grouping edges into partial edges, effectively reducing the size of the hypergraph and ultimately computing inside weights in less time."
D14-1133,2,another avenue for improvement lies in the possibility to perform the training of our rewriter by providing it with more reference translations.
D14-1133,2,"first, we could use a larger set of rewriting operations (langlais , 2007), including the rewrite (sic) operation introduced in (marie and max, 2013) that paraphrases source phrases and then translates them."
D14-1133,5,"it is furthermore worth noticing that our work proposes a potential answer to an original question: contrarily to typical works on sub-sentencial mt confidence estimation, which predict whether a word or phrase is correct or not, our rewriter system could be used to determine automatically whether a rewriting system could (if asked to) attempt to improve locally a translation, or whether a human post-editor should already tackle working on improving it."
D14-1133,1,"we could also possibly consider any phrase segmentation compatible with a specific word alignment rather than rely on specific phrase segmentations. more features could also be used, for instance to model more fine-grained syntax (post, 2011) or document-level lexical coherence (hardmeier et al., 2012)."
D14-1133,1,"contrarily to (madnani and dorr, 2013), we could bias the paraphrasing table so that it only contains paraphrases that correspond to target phrases of high confidence values, which would add new n-grams likely of being produced by rewriter."
D14-1133,1,"however, anticipating that some features might be very expensive to compute, we could adapt our procedure to work in several passes: initial passes would tend to restrict the search space more and more using an initial set of features, before a more expensive pass would concentrate on a limited number of hypotheses."
D14-1133,1,"as these are typically not readily available, we could resort to targeted paraphrasing (madnani and dorr, 2013) to rewrite reference translations into acceptable paraphrases that reuse n-grams from the best hypotheses of the system so far."
D14-1133,6,Our work could be extended in several directions.
D14-1134,1,"in the future, we wish to study various aspects of learning more robust lexicons."
D14-1134,1,"for example, in our current approach, words not appearing in the training set are treated as unknown and ignored at inference time."
D14-1134,1,"to alleviate this problem, we intend to further explore learning novel lexical templates."
D14-1134,1,we would like to study the benefit of using large amounts of unlabeled text to allow the model to better hypothesize the meaning of such previously unseen words.
D14-1135,1,"we would also like to study how to generalize these gains to languages other than english, by inducing more of the syntactic structure."
D14-1136,1,"in the future, we intend to extend this model for interpreting requirements in un-restricted, or less restricted, english, endowed with a more sophisticated discourse interpretation function."
D14-1137,4,"furthermore, as a general string-to-tree structured prediction model, this work may find applications in other areas within nlp."
D14-1137,1,"being able to efficiently exploit features defined over individual words, our model also opens up the possibility for us to exploit alternative representations of words for learning (turian et al., 2010), or to perform joint learning of both distributional and logical semantics (lewis and steedman, 2013)."
D14-1137,1,"future works include development of efficient algorithms for feature-based semantic parsing with alternative loss functions (zhou et al., 2013), development of feature-based language generation models (lu et al., 2009; lu and ng, 2011) and multilingual semantic parsers (jie and lu, 2014), as well as the development of efficient semantic parsing algorithms for optimizing the performance of certain downstream nlp tasks with less supervision (clarke et al., 2010; liang et al., 2013)."
D14-1138,1,"finally, visualizable spaces offer the potential to produce interactive environments for semisupervised topic reconstruction."
D14-1140,4,"a natural next step is expanding this work to other languages, such as japanese, which not only has sov word order but also requires tokenization and morphological analysis, perhaps requiring sub-word prediction."
D14-1142,1,"to gain additional insights into why this technique is working well, the features selected by the classifier as being more discriminating can be analyzed in future work. these features would offer insights into two kinds of language transfer effects, namely word choice (lexical transfer) and morphological differences."
D14-1143,1,"for our future work, because the graph used in this paper was constructed manually, we plan to automatically create a graph suitable for active learning and classification."
D14-1143,1,"there are several algorithms that create graphs from feature-based representations of words, but these have never been used for active learning of this task."
D14-1144,1,the use of unsupervised learning methods such as bayesian mixture models may be appropriate here.
D14-1144,1,"another avenue is to implement weight-based ranking methods to further refine and re-rank the lists, potentially by incorporating the measures mentioned in section 2 to assign weights to features."
D14-1144,1,the first relates to clustering the data within the lists.
D14-1144,1,"in addition to these further technical investigations, we see as a particularly useful direction the development of an sla research tool to conduct a large sla study with a wide range of experts."
D14-1144,1,"finally, the use of other linguistic features such as context-free grammar phrase structure rules or tree substitution grammars could provide additional insights."
D14-1144,1,"our intuition is that there might be coherent clusters of related features, with these clusters characterizing typical errors or idiosyncrasies, that are predictive of a particular l1."
D14-1144,1,"as the corpus we used includes learner proficiency metadata, it may also be possible to create proficiency-segregated models to find the features that characterize errors at each language proficiency level."
D14-1144,1,"for parse features, tree kernels could help measure similarity between the trees and fragments (collins and duffy, 2001)."
D14-1145,2,"we are planning to extend our study to dialects in other immigrant settings (e.g., turkish in germany) and to other types of multiword expressions (e.g., [n n] compounds)."
D14-1147,6,we are going to study on these issues in the future.
D14-1147,1,potential future works may include using semi-supervised methods to incorporate unlabeled data and design reasonable features from large corpora.
D14-1149,1,"another avenue for future research would be to use mixed membership community detection (gopalan and blei, 2013)."
D14-1149,1,future work could seek to combine graph-theoretic notions of centrality and intuitions about the defining features of term clusters.
D14-1149,1,"using cooccurrence networks to extract clusters of specialist terms, though an important task, is perhaps only a starting point for exploring the observed lexicon."
D14-1150,2,"although we illustrated the benefits of leveraging inter-linked document networks for keyphrase extraction from scientific documents, the proposed model can be extended to other types of documents such as webpages, emails, and weblogs."
D14-1150,2,another aspect of future work would be the use of external sources to better identify candidate phrases.
D14-1152,1,"as a direction for further research, it is interesting and important to provide more analysis on the expanded words via the continuous vector representations of words."
D14-1153,1,"another interesting experiment we plan to carry out in the future is to use the ngram classes along with the traditional stylistic features such as the vocabulary richness, average sentence length, etc."
D14-1153,1,"as future work, it would be interesting to combine the most precise classes of different n-gram lengths in order to improve the precision."
D14-1153,1,it would be important as well to try other segmentation strategies and postprocessing techniques in order to improve the granularity.
D14-1156,1,"as to future work, we would like to investigate jointly integrating proximity and other different kinds of relevance and lexical/semantic information cues into the process of feedback document selection so as to improve the empirical effectiveness of such query modeling."
D14-1157,1,"in future work, we will explore a model that captures individuals inherent topic shift propensities, while also capturing their fluctuations due to social factors."
D14-1160,3,"finally, we plan to investigate the impact of using sensory information for metaphor detection and interpretation based on our observations during the evaluation."
D14-1160,1,"for instance, the synesthetic metaphor bittersweet symphony could be detected by determining the sensorial characterizations of its components."
D14-1161,4,"second, we will apply the method to other tasks that require completing a word relatedness matrix."
D14-1161,1,"possible additional perspective slices include lsa for topic relatedness, and corpus occurrences in engineered or induced semantic patterns."
D14-1161,1,one straight-forward idea is that the dot product of perspective vectors pk 路 pl should be a measurement of correlation between perspectives.
D14-1161,1,"we evaluated the performance of our model on creating / recreating one perspective of word relatedness: antonymy. perhaps using vectors generated from many kinds of perspectives would improve the performance on other nlp tasks, such as term matching employed by textual entailment and machine translation metrics."
D14-1161,1,"third, if our model does learn the relation between semantic similarities and distributional similarities, there may be fruitful information contained in the vectors vi and pk that can be explored."
D14-1161,1,"first, in this model we only use a three-way tensor with two slices, while more relations may be able to add into it directly."
D14-1161,1,"for future works, we will extend the model and its applications in three main directions."
D14-1166,1,we believe that such kernels could improve the relatively low recall obtained so far by weakly supervised method for relation extraction.
D14-1166,1,"we would like to explore the use of kernels, such as the ones introduced by zelenko et al.(2003), culotta and sorensen (2004) and bunescu and mooney (2005), in future work."
D14-1168,1,"in addition, we plan to develop and evaluate an end to-end system, in which the aspect extraction and polarity estimation of aspects are automated."
D14-1168,1,"in future work, we plan to extend the microplanning phase by taking advantage of the highly weighted rhetorical relations between the aspects and select connective phrases based on the discourse relations specified in the aspects tree."
D14-1170,2,"in future work, we will make use of citation sentences to improve our system."
D14-1171,4,"another line of future research is to use the pm algorithm in other nlp tasks, where finding the pairs having some particular elements in common is necessary: for example, comparing parsing trees or dependency trees."
D14-1171,4,we think that pm can be used in other nlp tasks as well and we hope the community can take advantage of it.
D14-1173,1,"in the future, we plan to explore combining multiple source words which are aligned to the same target words."
D14-1174,1,"In the future, we plan to explore more effective approaches."
D14-1175,1,"future work will investigate the benefits of coupling our bnn models with target language models that also exploit abstract word representations, such as botha and blunsom (2014) and auli et al.(2013)."
D14-1176,1,"in this paper, we have focused on rather elementary dependency relations, which we are planning to expand on in future work."
D14-1176,1,"in particular, we are interested in exploring ways to better capture the notion of syntactic cohesion in translation (fox, 2002; cherry, 2008) within our framework."
D14-1177,3,"it would be interesting to investigate the contribution of different clues for various experimental parameters, e.g., domain, distance of languages, types of comparable corpora."
D14-1177,1,"as future work, we plan to improve the quality of the extracted dictionary further by exploiting additional translation signals."
D14-1180,1,"another interesting extension for our methods would be to also define types for the edge variables, and then sample both cut and edge types jointly."
D14-1180,1,one possible solution would be using better averaging methods instead of simply averaging over every few iterations.
D14-1183,4,the proposed method is also useful for many classification problems in natural language processing that require large-scale data.
D14-1186,1,"in the future, we plan to explore how to combine more features such as part-of-speech tags into our model."
D14-1190,1,"as it was discussed in section 4.3, we plan to further explore the possibility of using nongaussian likelihoods with the gp models."
D14-1190,1,"another research avenue we intend to explore is to employ multiple layers of metadata, similar to the model proposed by cohn and specia (2013)."
D14-1191,5,future works should also investigate how fully unsupervised methods can be extended to match our performance.
D14-1195,5,"as several recent efforts have focused on extracting large-scale parallel corpus for sentence compression (filippova and altun, 2013), we would like to study how larger corpora can affect tree transduction and our joint decoding solution."
D14-1195,5,"meanwhile, we would like to explore on how other text-rewriting problems can be formulated as a joint model and be applicable to similar strategies described in this work."
D14-1199,1,"as future work, we will reconsider the architecture of the neural network and we will refocus on creating a deep learning model while taking advantage of a larger set of types of information such as syntactic information, following (levy and goldberg, 2014), or semantic information, following (yu and dredze, 2014)."
D14-1200,4,"as future work, we plan to apply this approach to other relation extraction tasks and explore more suitable search orders for relation extraction tasks."
D14-1200,4,we also plan to investigate the potential of this table representation in other tasks such as semantic parsing and co-reference resolution.
D14-1201,1,we plan to investigate the use of ore in improving syntactic analysis in future work.
D14-1203,1,"future work will explore the use of nel in distantly supervised relation extraction further, tuning a confidence parameter for the nel system, and determining whether different confidence parameters should be used for training and extraction."
D14-1203,1,another possible direction is interleaving nel with relation extraction by using newly extracted facts to try to improve nel performance.
D14-1204,1,our future work will include exploring ways to improve automatic eventuality type and modality labeling accuracy to further improve tense inference accuracy.
D14-1205,5,"for future work, we will address the nil issue of el where we currently assume all entities should be linked to a kb."
D14-1205,1,"it would be also interesting to jointly model the two subtasks through structured learning, instead of joint inference only."
D14-1207,2,we also would like to investigate time-stamped corpora of finer-grained granularity such as day.
D14-1207,5,"for future work, we would like to investigate how our method can be improved to dp better at detecting fact end times."
D14-1208,1,"in the future, we would like to experiment with other constraints like modeling the selectional preferences of entity types."
D14-1208,1,"as part of immediate future work, we would like to improve the system recall."
D14-1208,1,our ilp formulation provides a good framework to add new type of constraints to the problem.
D14-1210,1,"for future work, we will seek to relax this consideration and jointly reason about non-terminal categories and derivation structures."
D14-1211,4,we expect our corpus to be a rich resource for social scientists interested in the effect of power and gender on language use.
D14-1211,5,"we will investigate several other sociolinguistic-inspired research questions; for example, do the strategies managers use for effectiveness of communication differ based on gender environments?"
D14-1211,1,"in future work, we will explore machine learning algorithms which capture the interactions between features better than our svm with quadratic kernel."
D14-1211,4,"while our findings pertain to the enron data set, we believe that the insights and techniques from this study can be extended to other genres in which there is an independent notion of hierarchical power, such as moderated online forums."
D14-1212,4,we also plan to apply the proposed method to content recommendations and trend analysis in twitter to investigate this method further.
D14-1212,1,"in future work, we plan to extend the proposed method to capture the birth and death of topics along the timeline with a variable number of topics, such as the model proposed by ahmed (ahmed and xing, 2010)."
D14-1213,1,"third, we can look at the relationship between self-disclosure behavior and general online social network usage beyond conversations."
D14-1213,1,"first, we can improve our modeling for higher accuracy and better interpretability."
D14-1213,1,"for instance, sdtm only considers first-person pronouns and topics. second, the number of topics for each level is varied, and so we can explore nonparametric topic models (teh et al., 2006) which infer the number of topics from the data."
D14-1215,5,future investigation into the causal effects of these interactions could lead to a better understanding of the role of figurative language in persuasion and rhetorics.
D14-1215,1,this suggests that future work on automatic detection of figurative language should consider contextual parameters such as the topic and community where the content appears.
D14-1217,1,an interesting avenue for future research is to automatically learn how to parse text describing scenes into formal representations by using more advanced semantic parsing methods.
D14-1217,1,"another interesting line of future work would be to explore the influence of object identity in determining when people use ego-centric or object centric spatial reference models, and to improve resolution of spatial terms that have different interpretations (e.g., 鈥渢he chair to the left of john鈥 vs 鈥渢he chair to the left of the table鈥)."
D14-1217,1,we can also improve the representation used for spatial priors of objects in scenes.
D14-1217,1,"finally, a promising line of research is to explore using spatial priors for resolving ambiguities during parsing."
D14-1217,1,we can improve the representation by modeling whether a surface is an interior or exterior surface.
D14-1219,4,"in the future, we would like to apply our reranker to the document-level parses."
D14-1219,1,"however, this will require a better hypotheses generator."
D14-1220,4,"our future work will focus on extending discourse-level distributed presentations to related tasks, such as implicit discourse relation identification or dialogue analysis."
D14-1222,1,"furthermore, better methods to model the semantics of the specific context need to be explored in the future."
D14-1223,1,"as a future work, we would like to adapt our model for different languages and include other features from multi modality including gesture or geo-location."
D14-1224,2,"in the future work, we will focus on enlarging the scale of the corpus annotation and developing a complete chinese discourse parser."
D14-1225,4,"future work should apply this general idea to other natural language processing tasks including dependency parsing (nivre , 2007) and information extraction (li , 2013)."
D14-1225,1,"we would expect more beneficial behavior with the pruning constraints for problems with large action sets (e.g., labeled dependency parsing)."
D14-1225,1,"it would be interesting and useful to generalize this approach to search spaces where there are multiple target paths from the initial state to the terminal state, e.g., as in the easy-first framework."
D14-1226,6,"as future work, we plan to explore various other forum specific features such as user reputation and quality of content to improve summarization performance."
D15-1001,1,future directions include tackling high-level planning and strategy learning to improve the performance of intelligent agents.
D15-1002,5,"finally, it is still not clear how to extend the current approach beyond words and phrases directly denoting an entity (amur) to other kinds of definite descriptions (this dog)."
D15-1003,3,"in future work, we will test our system on such context-specific examples, using contextualised vector representations such as the ones proposed by e.g.erk and pad´o (2008) and dinu and lapata (2010)."
D15-1004,1,"although experiments show significant improvements over baselines, our model has limitations that can be avenues for future work."
D15-1005,1,"there are various direction we would like to explore, the most obvious of which are integrating the learned reordering with other feature functions in a discriminative setting, and extending the model to deal with non-contiguous minimal phrases."
D15-1006,5,"while there exist approaches that extract syntactic tree transformation rules automatically, one of the difficulties is that most parallel corpora is dominated by lexical paraphrasing instead of syntactic paraphrasing."
D15-1009,1,"to further improve the prediction accuracy, in the future, we will extend our current study by incorporating new features such as the properties of a brand as well social influence from people in one’s social network."
D15-1010,6,"as future work, we aim to improve the efficiency of our entire workflow, such that the annotation can become an end-to-end service."
D15-1010,1,"we also aim to improve the context similarity between entities and the topic, for example by using a deeper distributional semantics-based method, instead of language models as in our current work."
D15-1012,5,it is interesting to explore how will the performance be affected if we are only provided with parallel sentences and then alignments can only be derived using an independent aligner.
D15-1016,4,"2) apply this framework to other cross lingual tasks such as paraphrase detection, question answering, aspect based opinion mining etc"
D15-1016,1,learning different weight matrices at different nodes to capture complex relations between words and phrases.
D15-1020,6,"moreover, joint detection of events from both sides is our ultimate goal, however, we need to explore the mapping among events from both text and visual sides, and automatic detection of a wide range of objects and events from news video itself is still challenging."
D15-1022,1,"future work could include nonprepositional terms like verbs, having prepositions modify verbs, adding word2vec embeddings to the structured prediction model, and providing stronger features – whether textual, visual or geometric."
D15-1023,5,"for further research, we note that picking the optimal hi,j,k is an open question, so provably finding and justifying the choice is one topic of interest."
D15-1025,4,"in the future, we plan to extend these models to other tasks such as syntactic parsing and machine translation."
D15-1025,1,"moreover, we will also investigate other architectures to infer word embeddings from the character level."
D15-1025,1,"for instance, preliminary experiments show that bidirectional recurrent network can achieve very competitive and promising results."
D15-1029,3,"in future work, we can assess the impact of this model on a wider array of feed-forward embedding-based neural network models, such as the dssm (huang , 2013)."
D15-1030,1,"in future work, we plan to engage in further model analysis and comparison, to explore alterations to model structure, e.g. introducing hierarchical topic models, to use other clustering methods to obtain priors, and to explore the value of predicted links for downstream tasks such as friend recommendation (pennacchiotti and gurumurthy, 2011) and inference of user attributes (volkova et al., 2014)."
D15-1033,5,"third, while we have shown that distributional initialization improves the quality of representations of rare words, we did not investigate whether distributional initialization for rare words has any adverse effect on the quality of representations of frequent words for which one-hot initialization is applied."
D15-1033,1,"since rare and frequent words are linked in the mixed model, this possibility cannot be dismissed and we plan to investigate it in future work."
D15-1033,1,"it remains to be investigated whether there are interactions between these two properties of our model, e.g., a high rare-frequent separator may work well for words whose corpus frequency is much smaller than the separator."
D15-1034,6,we leave the study of more general paths to future work.
D15-1037,1,personalized topic modeling is also an interesting future direction in which the model will generate a personalized topic structure based on the user’s preferences or interests.
D15-1037,1,"for all these applications, an efficient learning algorithm is a crucial prerequisite."
D15-1039,2,"future work should consider application of the method to a broader set of languages,"
D15-1039,4,and application of the method to transfer of information other than dependency structures.
D15-1040,4,"in future work, we plan to extend joint training to several languages, and further explore the idea of learning and exploiting crosslingual embeddings."
D15-1042,1,"in the future, we are planning to experiment with more “interesting” paraphrasing models which translate the input not into a zero-one sequence but into words."
D15-1043,4,future work includes application of the system on text-to-text problem such as machine translation.
D15-1044,1,both pose additional challenges in terms of efficient alignment and consistency in generation.
D15-1044,1,"as a next step we would like to further improve the grammaticality of the summaries in a data-driven way, as well as scale this system to generate paragraph-level summaries."
D15-1047,3,"in our future work, we plan to verify the soundness of the results by applying our method on large volume corpus of both english and chinese."
D15-1047,1,"in addition, we will investigate other ways of computing the word coupling matrices, such as incorporating word coherency or semantics, and develop efficient merging strategies which can be used for training classification models, as well as for building graphs."
D15-1048,2,"this is relevant, if a technique like filtering is used to include more relevant class examples in a dataset than provided with an original sample – a necessary step to realize a labeled dataset for model learning of a rare-class task."
D15-1048,6,there are two main topics for our future work.
D15-1048,1,"first, we will investigate the performance of models generated with biased datasets on unfiltered datasets."
D15-1048,1,"second, we will work on using novel features for the creation of generalized models."
D15-1048,1,"one example is the utilization of the semantic web to generate abstract features, utilizing a technique called semantic abstraction (schulz et al., 2015a)."
D15-1050,2,"expanding to additional corpora will probably require development of additional features, to capture signals unique to each corpus."
D15-1050,5,"this poses additional challenges in gathering labeled data, as it will require a mechanism to decide which documents to label per claim and will probably increase the number of documents to be labeled."
D15-1051,1,we also plan to investigate the prototype directly in query expansion as part of the search backend.
D15-1051,1,"for this, we feel that adding full word level models will help to overcome the somewhat limited context present in our character sequence based models."
D15-1051,1,We plan to further invest in improving precision.
D15-1053,1,"while we were able to demonstrate the viability of our hybrid model when using only simple surface statistics of text, future work shall include application of our models to more semantic-oriented representations, such as those leveraged in building log-linear language models (mikolov et al., 2013)."
D15-1054,2,one of these directions is to extend the vocabulary by identifying significant phrases (as well as words) before training word vectors.
D15-1054,5,"in case the computational cost of such methods are too high to be practical for sponsored search, we can employ them only for a small fraction of ads filtered by faster methods."
D15-1054,6,there are multiple interesting research directions for future work.
D15-1054,1,we also like to investigate more structured embedding methods such as rnns (probably for ad descriptions).
D15-1055,1,"in future work, we will address the task of temporal expression normalization."
D15-1056,1,this could alleviate ner errors and enable experimentation with other relationship types.
D15-1056,1,"in future work, more robust entity-linking approaches, as proposed by hoffart et al.(2011), could be included in our pre-processing pipeline."
D15-1059,2,"therefore, another line of future work is to also learn from these unlabeled documents."
D15-1059,1,as future work we would also explore the feasibility of learning preconditions of verbs from wikipedia revisions.
D15-1061,4,"while these initial results are encouraging, we hope to apply entice on other knowledge graphs,"
D15-1061,1,and also experiment with other normalization and entity linking algorithms as part of future work.
D15-1063,2,"in the future, we thus plan to constantly update heideltime’s automatically created resources."
D15-1066,1,"for future work, we wish to explore more useful document-type features and apply more proper combination strategies to improve the latent document type model."
D15-1067,1,"for future work, it would be interesting to see if more sophisticated dnn training techniques (e.g. unsupervised pre-training and different optimization algorithms) would yield a better performance."
D15-1068,3,"we further plan to test our framework on other cqa datasets, including on other languages."
D15-1068,1,"in future work, we would like to improve the pairwise classifiers with richer features, as this is currently the bottleneck for improving the performance in the global model."
D15-1068,1,"last but not least, we are interested in extending this research with even more global information, e.g., by modeling global decision consistency across multiple threads."
D15-1069,6,there are several directions we can explore in our future research.
D15-1069,1,we would also investigate domain adaptation techniques to learn key concept identification models from other data sources.
D15-1069,1,"firstly, our key concept identification methods are not optimized for the retrieval results, but for the identification subtask only."
D15-1069,1,we hypothesize that directly optimizing the key concept identifier for retrieval would lead to better performance.
D15-1071,3,"for future research, it would be interesting to validate the results by conducting the study on a larger scale."
D15-1071,4,"the approach could also be used for other document types, for example analyst reports or internal memos, or in other industries."
D15-1072,1,"in future sentiment analysis approaches, sentiment flows may therefore rather serve as pivot features for domain adaptation."
D15-1075,4,we hope that snli presents valuable training data and a challenging testbed for the continued application of machine learning to semantic representation.
D15-1076,4,"finally, future work will also explore applications of our annotation."
D15-1076,1,"alternatively, the annotation could be used for active learning: we envisage a scheme where parsers, when faced with ambiguous attachment decisions, can generate a human-readable question whose answer will resolve the attachment."
D15-1076,1,"a joint syntactic and semantic parser, such as that of lewis et al.(2015), could be trained directly on the annotations to improve both the syntactic and semantic models, for example in domain transfer settings."
D15-1076,1,"most obviously, the annotation can be used for training question-answering systems, as it directly encodes question-answer pairs."
D15-1076,1,"more ambitiously, the annotation has the potential to be used for training parsers."
D15-1077,1,"in the future, we are interested in refining the prior estimation by using the ontology and extending this work to detect the target entities that are not in a list while performing the disambiguation task."
D15-1081,1,we are also experimenting with our collective validation algorithm to incorporate the impact of more distant kb entities other than just the neighbors.
D15-1081,1,"in the future, we plan to improve the source document processing such that the system can better extract the mention context without involving extensive linguistic knowledge."
D15-1082,5,"in future, we will explore the following research directions: (1) this paper only considers the inference patterns between direct relations and relation paths between two entities for learning."
D15-1082,1,"(2) there are some extensions for transe, e.g., transh and transr."
D15-1082,1,"it is non-trivial for them to adopt the idea of ptranse, and we will explore to extend ptranse to these models to better deal with complicated scenarios of kbs."
D15-1082,1,we may take advantages of first-order logic to encode these inference patterns for representation learning.
D15-1084,1,"we plan to further exploit sense-enhanced unified representations of relations in various ways: providing an ontological structure for the unified kb, exploring complementary approaches for capturing semantic relation alignments, and incorporating multilinguality."
D15-1086,1,"in future work, the proposed approach could be combined with other approaches to solve typical issues arising in the context of distant supervision, such as dealing with overlapping relations (hoffmann et al., 2011), improving heuristic labelling of sentences (takamatsu et al., 2012) or dealing with incomplete knowledge bases (min et al., 2013)."
D15-1087,3,"to facilitate comparison with future work on this task, we released the source code of our spatial relation extraction system."
D15-1089,1,future work might involve designing new kernels for syntactic parse trees with appropriate similarity measures between non-terminal nodes as well as exploring recently proposed phrase embeddings for more accurate phrase kernels.
D15-1090,1,"further, we plan to build on simmr to get closer to the milk representation."
D15-1090,1,"we also plan on parsing a large corpus of text recipes to provide structural features on a large scale that will allow us to discover new patterns of similarity across and within cuisines, as well as generate new recipes."
D15-1091,1,we will also explore how to incorporate prior knowledge about topic relations (such as causation and correlation) into topic modeling.
D15-1091,1,"in future work, we will study how to discover overlapping clusters, i.e., allowing one document to be grouped into multiple topic clusters."
D15-1092,1,"in future work, we would like to investigate the other gating mechanisms for better modeling the feature combinations."
D15-1093,3,"for quantitative evaluation, we need to apply our mehtod to other data such as acm-dl and a large, heterogeneous collection of web content in addition to the experiment to examine the performance agasint the ratio between same-period and diff-period training data."
D15-1093,1,"in the future, we will try to extend the framework to address this issue."
D15-1093,3,"for quantitative evaluation, we need to apply our method to other data such as acm-dl and a large, heterogeneous collection of web content in addition to the experiment to examine the performance against the ratio between same-period and diff-period training data."
D15-1095,2,"besides, we will build upon the outcome of this study by extracting the event sequence in a persons life starting from the complete biographies retrieved from wikipedia."
D15-1095,3,"in the future, we plan to compare our approach based on section titles with more sophisticated approaches considering also the sections’ content, to assess whether the latter improves over our simple methodology."
D15-1096,3,"besides, we would like to compare our algorithm with the algorithms designed for specific word problems, such as (hosseini , 2014)."
D15-1096,4,"our future work will focus on studying the performance of applying nonlinear kernel function to the qp problem (3),"
D15-1096,1,"and using the word embedding vector (bengio et al., 2003; mikolov et al., 2013) to replace current lexicalized features."
D15-1098,1,"for the future work, we plan to devise embedding models based together on the composition of component-character and of character-word."
D15-1098,1,the two types of compositions will serve in a coordinate fashion for the distributional representations.
D15-1100,1,"for future work, we plan on employing alternative comparison criteria in our model such as those derived from named entity recognition and paraphrase detection."
D15-1102,1,"future work include explorations of efficient algorithms for other information extraction tasks, such as joint mention and relation extraction (li and ji, 2014) and event extraction (li et al., 2013)."
D15-1104,1,"for future works, we would like to study how to leverage existing partial labeled data, either for ner or for linking only, in joint optimization, and incorporate more nlp tasks together for multitasks joint optimization."
D15-1105,1,"for example, the hmm alignment model cannot “cross off” a source word and stop trying to translate it."
D15-1105,1,"also possible are phrase-based translation, neural nets, or as-yet-unanticipated pattern finding algorithms."
D15-1105,1,"first, we would like to develop and exploit better predictive translation modeling."
D15-1108,1,"we could also explore other serial update schemes, which generally speed up message-passing algorithms over parallel update."
D15-1110,4,"our next step is to apply the method to other corpora and to more complex text, where the identification of non-participating segments (which are irrelevant for the argumentation) needs to be accounted for."
D15-1110,1,"furthermore, we plan to investigate structured models that not only jointly predict but jointly learn the different aspects of the argumentation graph."
D15-1114,4,"we also plan to use our techniques to support related tasks, such as instructional recipe generation."
D15-1114,1,"future work includes learning a more comprehensive model of locations (e.g., identifying nested locations such as an oven and a pan in the oven), enriching action graphs with greater semantic coverage (e.g., durations, tools, amounts), and training and evaluating on larger datasets."
D15-1115,6,"in the future, we are planning to design a reading comprehension task where we use this framework for answering comparison questions from a paragraph containing various interrelated comparisons."
D15-1116,3,"in future, we plan to perform additional experiments to study the issue of noisy data."
D15-1116,5,"we hope that the release of our datasets will stimulate other studies related to the sarcasm detection problem, including addressing the issue of noisy data."
D15-1116,1,we want to follow their experiments to study whether parameter tuning in pmi based disambiguation can improve its performance.
D15-1116,1,we also plan to study the effect of hyperparameters in designing the dsms.
D15-1116,1,"recently, levy et al.(2015) have argued that parameter settings have a large impact on the success of word embedding models."
D15-1117,1,"we will also enhance the accuracy measure of trustiness, based on the observation that some untrusted sites copy information from other sites to make them look more trustful."
D15-1117,1,"as future work, we will investigate into the task of automatically constructing patterns for the pattern matching methods in sections 3.3 and 3.4, to improve coverage."
D15-1119,1,we are investigating further properties of seg rev and plan to extend it to achieve greater stability and efficiency.
D15-1122,1,our future work aims to incorporate syntactic or semantic information into our paraphrasing framework.
D15-1125,1,"as future work, it would be interesting to explore various distributed word representations for quality estimation and joint models that look at both the source and the target sentences simultaneously."
D15-1128,1,"looking to the future, one important benefit of taking a hierarchical approach is that the re-ordering process is made explicit, and in further research we wish to explore the possibility of introducing of new interpretation-oriented rules into the stream decoding process."
D15-1129,1,"in future work, we will use different techniques to improve the diversity of the string-to-tree rules considered during decoding in our system."
D15-1130,6,"overall, however, we are encouraged to pursue our goal of personalized machine translation."
D15-1130,1,"we would also like to understand the true relationship between linguistic features and traits across languages, along with how native speakers naturally observe these traits."
D15-1131,4,"we are still exploring promising properties of the generated vectors and their applications in other nlp tasks (sentiment analysis, ner...)."
D15-1134,1,"in future work, we intend to extend the model to learn larger templates that include syllable structure and phonological tiers (goldsmith, 1976)."
D15-1135,4,we hope to extend our techniques to handling general math word problems and to other domains (like physics and chemistry) in the future.
D15-1137,5,our future work is to focus on how to choose/combine different ways of computation.
D15-1137,1,"for instance, we might replace the max pooling by different pooling operations such as mean pooling, k-max pooling (kalchbrenner et al., 2014), and stochastic pooling (zeiler and fergus, 2013)."
D15-1138,4,"future work might extend this approach to tasks like question answering, where logicbased approaches have been successful."
D15-1139,5,"in future work, it may be worthwhile to study the impact of alignment techniques on overall system performance in other string transduction problems such as transliteration, lemmatization, and spelling error correction."
D15-1141,1,"in future work, we would like to adopt the bidirectional recurrent neural network (schuster and paliwal, 1997) to process the sequence in both directions."
D15-1144,1,it is also interesting to develop consistency-aware training algorithms for word alignment.
D15-1144,1,"in the future, we plan to apply our approach to syntax-based models (galley et al., 2006; liu et al., 2006; shen et al., 2008) and include the constituency constraint in the optimization objective."
D15-1145,4,"in the future, we plan to further improve our model by capturing semantic relatedness among source words."
D15-1145,1,"additionally, we also want to jointly model different levels of context information in a unified framework for smt."
D15-1146,4,"additionally, we also want to apply our model to other bilingual tasks, e.g., learning bilingual terminology or paraphrases."
D15-1146,1,"in the future, we would like to derive more features from bcorrrae, e.g., consistency/inconsistency scores of bilingual phrases, to further enhance smt."
D15-1148,2,another important future direction lies in text simplification.
D15-1148,4,a similar analysis using ideas from this work can be useful in identifying sentences that needs simplification in the first place.
D15-1150,4,"in the near future, this technique will enable us to release a suite of pos taggers for hundreds of lowresource languages."
D15-1152,6,we also plan to host a demo and make our system available through the website of the computational approaches to modeling language (camel) lab: www.camel-lab.com.
D15-1152,1,"in the future, we plan to investigate the development of joint morphological disambiguation and syntactic parsing models."
D15-1152,1,other possible directions include using more sophisticated machine learning techniques and richer lexical features.
D15-1152,1,we will also work on improving the quality of arabic parsing which is behind many of the errors according to our error analysis.
D15-1154,4,"in future, we are interested in extending our parser to higher-order factorization by increasing horizontal context (e.g., from siblings to trisiblings) and vertical context (e.g., from siblings to grand-siblings) and validating its effectiveness via a wide range of nlp applications."
D15-1156,3,other future work could examine the effectiveness of the approach in the opposite direction (japanese to english) or on other language pairs.
D15-1156,6,we would like to report the results in the future version of this paper.
D15-1156,1,"since the accuracy of the empty category detection implemented as a post-process highly depends on that of the underlying parser, we want to explore models that can solve them jointly, such as the lattice parsing approach of (cai et al., 2011)."
D15-1157,3,our next steps include learning error suffixes during a prior tagging phase and experimenting with the french social media bank.
D15-1160,4,"lastly, it would be interesting to adapt fusion to other structured prediction tasks where n-best lists are available."
D15-1160,1,we also intend to explore how to better apply fusion to converted dependencies from constituency parsers.
D15-1160,1,"future work includes applying fusion to n-best dependency parsers and additional (parser, language) pairs."
D15-1162,1,in future work we plan to combine such neural network models with a version of our parser that incorporates a much larger set of nonmonotonic parsing transitions.
D15-1163,2,"in future work, we would like to include translations for infrequent phrases which are not oovs."
D15-1163,1,we would like to explore new propagation methods that can directly use confidence estimates and control propagation based on label sparsity.
D15-1164,1,we also would like to extend our work by using more contextual lexical information to derive semantic vectors for nonterminals.
D15-1167,5,how to simultaneously learn document structure and composition function is an interesting future work.
D15-1168,4,"in the future, we would like apply our models to other fine-grained opinion mining tasks including opinion expression detection and characterizing the intensity and sentiment of the opinion expressions."
D15-1168,1,we would also like to explore to what extent these tasks can be jointly modeled in an rnn-based multi-task learning framework.
D15-1170,5,"therefore, another direction of our future work is to explore specific problems that will emerge when employing tree-based smt systems to semantic parsing, and provide solutions to them."
D15-1171,2,and scaling up the dataset.
D15-1171,1,"future work includes expanding the geometry language and the reasoning to address a broader set of geometry questions, reducing the amount of supervision, learning the relevant geometry knowledge,"
D15-1177,3,"as a future step, we aim to test the proposed models on a convolutional compositional architecture, similar to that of kalchbrenner (2014)."
D15-1178,3,"we also plan to evaluate the usefulness of conversation trees in tasks such as predicting if a thread is resolved, and user expertise."
D15-1178,1,a main goal for future work is to incorporate further domain specific constraints on the models to improve parsing speed and at the same time allow more flexible trees.
D15-1179,1,future work on these models could explore methods to fit non-common topics for both collections.
D15-1181,4,future work could extend this model to related tasks including question answering and information retrieval.
D15-1183,3,"through learning such latent factors, important summary information of documents would be acquired, which are useful in various applications."
D15-1183,1,"in the future work, we will incorporate global latent factors into this generative model, such as topics, sentiments, or writing styles, and develop more elaborate models of documents."
D15-1184,1,"one possible direction of future work would explore ways to fully represent the rich information in texts by extending the text features and language representations like continuous bag of-words (cbow) models (mikolov et al., 2013) or global vectors for word representation (glove) (pennington et al., 2014)."
D15-1187,1,"therefore, in future work, we will focus on investigating an effective method to integrate the local lexical/syntactic information and the global contextual discourse information."
D15-1189,1,"there is significant room for future work to improve the results, including jointly modeling the factuality of multiple events and integrating factuality models into information extraction and question answering systems."
D15-1190,3,we are considering an extrinsic evaluation for these data such as the rte test in future research.
D15-1191,3,"we would also like to test the feasibility of other such strategies, e.g., initializing sme by transe, so as to combine the benefits of both models."
D15-1191,1,"moreover, our approach actually reveals the possibility of a broad idea, i.e., initializing an embedding model by another embedding model."
D15-1191,1,"as future work, we plan to 1) investigate the efficacy of longer ccps (i.e. knowledge paths with lengths longer than 5). 2) design a joint model that encodes lcps and ccps simultaneously."
D15-1192,4,"in general, the proposed approach may prove beneficial for additional tasks that model word meaning in context, such as lexical substitution and sense induction."
D15-1192,1,"we are further interested in creating specialized models that fit different word classes, e.g., of particular part-of-speech."
D15-1193,4,promising future directions would be applying the framework of refd to other contexts such as measuring the prerequisite relations or reading orders between papers and textbooks.
D15-1193,5,also it would be meaningful to explore ranking different prerequisites of a concept.
D15-1193,1,"in addition, refd can be incorporated into existing supervised models for a more accurate measure."
D15-1194,1,"for future work, we aim to investigate the modelling of learned vector representation, such as cbow and glove, within a phrase-based mt model when normalizing medical terms."
D15-1196,3,(2) we will evaluate the performance of our oiwe models in various nlp applications.
D15-1196,1,"(3) we will also investigate possible extensions of our oiwe models, including multiple-prototype models for word sense embeddings (huang et al., 2012; chen et al., 2014), semantic compositions for phrase embeddings (zhao et al., 2015) and knowledge representation (bordes et al., 2013; lin et al., 2015)."
D15-1196,1,"in future, we will explore the following research issues: (1) we will extensively investigate the characteristics of oiwe with respect to various hyperparameters including dimension numbers."
D15-1202,5,"hence a similar approach can be targeted towards algebra word problems, a direction we wish to investigate in the future."
D15-1205,4,"moreover, as the model provides a general idea for representing both sentences and sub-structures in language, it has the potential to contribute useful components to various tasks, such as dependency parsing, srl and paraphrasing."
D15-1205,4,we plan to explore the above applications of fcm in the future.
D15-1205,4,"also as kindly pointed out by one anonymous reviewer, our fcm can be applied to the tac-kbp (ji , 2010) tasks, by replacing the training objective to a multi-instance multilabel one (e.g.surdeanu (2012))."
D15-1205,4,"also as kindly pointed out by one anonymous reviewer, our fcm can be applied to the tac-kbp (ji et al., 2010) tasks, by replacing the training objective to a multi-instance multilabel one (e.g. surdeanu et al.(2012))."
D15-1205,1,our next steps in improving fcm focus on enhancements based on task-specific embeddings or loss functions as in hashimoto et al.(2015; dos santos et al.(2015).
D15-1208,3,"we also plan to examine closer the differences between perceived human and fictional personality, and the relationship between the personality of the reader and the characters."
D15-1208,1,"in the future we aim on collecting a more detail and rigorous gold standard through gamification and expanding our work on all five personality traits from the five factor model and their facets, and ultimately extend our system to a semi-supervised model dealing with notably larger amount of data."
D15-1209,5,it is a interesting question why giza++ achieved competitive bleu scores though its alignment accuracy measured by f1 was substantially lower.
D15-1209,1,our future work will focus on increasing the gains in end to-end translation quality through the proposed leave-one-out aligner.
D15-1209,1,"in addition, we plan to improve the proposed method by integrating kneser-ney smoothing."
D15-1210,1,"in the future, we plan to investigate more loss functions to account for syntactic constraints."
D15-1212,1,"as suggested in this paper, future work for parsing morphologically rich languages will require to focus both on feature selection and on the interface between syntax and morphology, which means in our case the interface between the segmenter, the tagger and the parser."
D15-1215,4,"in future work, we would also like to extend our grnns for the other nlp tasks."
D15-1216,1,"in future work, we plan to perform ttp analysis in the case of real users and to optimize the hand-crafted rules introduced here to operate the floor management in the system (when to take/give the floor and according to which ttp scheme) by using reinforcement learning (sutton and barto, 1998; lemon and pietquin, 2012)."
D15-1220,4,"in future work, we intend to extend our study to compressive summarization."
D15-1221,2,"in future work, we plan to use more data to train our model, making it easier for our system to actually identify rhyming pairs and use them in new contexts."
D15-1221,3,"in terms of evaluation, we hope to incorporate some method to evaluate the fluency of generated lyrics (addanki and wu, 2014)."
D15-1221,1,"lastly, to further avoid over-fitting to the training data and reproducing lyrics with a high similarity, we plan to use weight noise (jim et al., 1996) to regularize our model."
D15-1221,1,"furthermore, we plan to generate lyrics from artists with a varying vocabulary size to see if it is easier to generate lyrics for an artist with a smaller vocabulary."
D15-1221,1,we also plan to encode phoneme features of words to improve rhyme discovery.
D15-1222,3,an effective and accurate automatic evaluation measure will be a big boon to our quest for better text summarization systems.
D15-1223,4,"in the future, we intend to apply the mdl method to keyword extraction, headline generation, and other related tasks."
D15-1224,3,"in future, we will compare existing reg algorithms on our dataset, in a similar experiment to mitchell (2013b)."
D15-1224,1,"then, we will extend existing algorithms to take into account other properties such as material (e.g. ""wooden”), components of the referred object (e.g. ""balconies”) etc."
D15-1224,1,"finally, we will incorporate such an algorithm in interactive settings to investigate the influence of user dialogue behavior and the influence of visual features, such as salience (clarke et al., 2013), in order to improve the fit of our predictive model."
D15-1225,5,"moreover, we will also explore the impact of different scale of the dependencies from historical epochs on the distributions of the current epoch."
D15-1225,1,"in future work, we will consider modelling background topics explicitly and investigating more principled ways in setting the weight parameters of the statistics gathered in the historical epochs."
D15-1227,2,"we also deployed coursemirror in a statistics class in spring 2015 and have created gold-standard summaries, which will allow us to both replicate the intrinsic evaluation of this paper with a new and larger dataset as well conduct an extrinsic evaluation beyond rouge scores."
D15-1227,3,"in the future, we plan to have additional annotation to evaluate the relative importance using the student coverage numbers."
D15-1227,4,"finally, we are interested in applying our summarization approach to other types of user-generated content from mobileapplications (e.g., review comments)."
D15-1230,1,"in future work, we intend to combine this discourse-based view of coherence with a content based view to create a unified statistical discourse planner."
D15-1230,1,"in addition, we will explore additional stochastic models of discourse that look at other, non-sequential collocational information."
D15-1232,4,this direction should be useful for query-focused summarization tasks.
D15-1232,1,"in future research, we will explore other scaling functions suitable for our problem or different problems."
D15-1232,1,a promising direction is to consider a relative scaling function to extract a biased summary of a document.
D15-1234,2,"finally, it is worth remembering that language provides only a partial description of depicted characters, so we should aim to augment with aural/visual information."
D15-1234,2,we also intend to collect more movie and character level metadata to be used in analysis.
D15-1234,3,"future work will include the use of further metrics, with those describing emotions being the first candidates."
D15-1239,1,future work involves improving the classification algorithm by using new approaches to learning about rare events.
D15-1244,1,"future work will focus on incorporating a robust model of lexical knowledge (lewis and steedman, 2013; tian et al., 2014) to our framework."
D15-1247,2,one can further ensure the advantage of the joint model using a larger corpus.
D15-1247,1,another future work is to incorporate other components of events into the model.
D15-1247,1,"one could leverage them as other learning targets or constraints, and investigate further benefits of joint modeling."
D15-1247,1,"our preliminary experiment on the ace 2005 corpus shows that due to its larger document size and event types, one will need to reduce training time by a distributed learning algorithm such as mini-batches (zhao and huang, 2013)."
D15-1254,4,"in future work, we would like to explore more user-specific information for dialect classification, apply the classifier for arabic-to-english mt systems, and extend the approach to a larger family of languages and dialects."
D15-1256,4,"another crucial task is to expand this investigation beyond the united states, as the varying patterns of use for social media across countries (pew research center, 2012) implies that the findings here cannot be expected to generalize to every international context."
D15-1257,6,we hope this work will encourage others to further investigate the most reportable event.
D15-1257,1,"in future work we hope to be able to generate a text description the full mre, which would be better suited to summarization or generating headlines, rather than identifying sentences that refer to it."
D15-1258,5,analysis of classification errors highlighted the challenges associated with the task.
D15-1258,5,"furthermore, the relation between sentiments and suggestions seem to be worth investigating."
D15-1258,1,"the classification results have scope for improvement, and therefore the task calls for advanced semantic features and dedicated models, which will be our future direction."
D15-1260,1,"as future work, we are planning to use commonsense knowledge, such as causality (hashimoto et al., 2014) and script-like knowledge (sano et al., 2014), that has been automatically acquired from big data for accurate subject sharing recognition to improve intersentential zero anaphora resolution for cases not focused on in this work."
D15-1262,4,"in future work, we first plan to extend our comparative framework to a larger set of relations and to other languages."
D15-1262,1,"we also want to explore methods for learning embeddings that are directly related to the task of discourse relation classification, potentially using existing embeddings as initialization (labutov and lipson, 2013)."
D15-1268,3,"to this end, we are planning to organize an evaluation workshop on dialogue breakdown detection."
D15-1268,6,"to accurately detect dialogue breakdowns, dialogue systems researchers will need to collaborate."
D15-1268,1,"for future work, we plan to consider ways to improve systems on the basis of our findings and also verify the generality of the results on data using other systems."
D15-1269,1,"in future work, we aim to implement a nonparametric bayes model that will be able to estimate the number of concepts automatically."
D15-1274,1,"since creating such tools is a labor-intensive task, we expect our diacritization approach to promote the development of speech recognizers for other languages and dialects."
D15-1274,1,"in future work, we intend to incorporate our discretization system in a speech recognizer."
D15-1276,4,"we also intend to apply our method to unsegmented languages other than japanese, such as chinese and thai."
D15-1276,1,"in the future, we will design features derived from rnnlm models, and integrate them into a unified learning framework."
D15-1277,1,"it is interesting to apply the symbol grounding results to an embedding model-based word segmentation approach (ma and hinrichs, 2015)."
D15-1277,1,it is also interesting to extend our method to deal with other types of non-textual information such as images and economic indices.
D15-1277,1,"as future work, we will apply other deep neural network models to our approach."
D15-1280,1,"in future work, we would like to investigate the other feedback mechanism between the short-term and long-term memories."
D15-1281,5,"in order to develop a fully automated deception deception system, our future work will address the use of automatic gesture and facial expression identification and automated speech transcription."
D15-1281,1,our goal is to move forward towards a real-time deception detection system.
D15-1283,1,"in the future, we plan to extend co-training to include active learning for more robust classification."
D15-1283,1,"moreover, it would be interesting to extend the co-training approach to multi-views that could potentially handle more than two feature spaces, e.g., it could include topics by latent dirichlet allocation (blei et al., 2003) as an additional view."
D15-1284,1,"in the future, we would like to step further into the discovery of humor characteristics and apply our findings to the process of humor generation."
D15-1287,1,"as future work, we will fully integrate our model into a pbsmt decoder and evaluate it on other language pairs with different reordering distributions."
D15-1289,1,"in our future work, we will consider to use the ontology matching approach to the matching between different nlp-oriented ontologies such as wordnet, freebase, yago, etc."
D15-1293,1,another interesting possibility is to improve auditory representations by training a neural network classifier on the audio files and subsequently transferring the hidden representations to tasks in semantics.
D15-1293,1,"in future work, it would be interesting to investigate different sampling strategies for the early fusion joint-learning approach and to investigate more sophisticated mixing strategies for the middle and late fusion models, e.g. using the “audio dispersion” of a word to determine how much auditory input should be included in the multi-modal representation (kiela et al., 2014)."
D15-1294,1,"finally, the next steps in this line of research are to integrate the aspectual information attributed to clauses by our model into models of temporal discourse structure, which in turn are useful for information extraction and text understanding tasks in general."
D15-1294,1,costa and branco (2012) are the first to show that aspectual information is relevant here; we hope to show in the future that temporal processing profits from integrating more fine-grained aspectual information.
D15-1295,5,"in the future, we would like to investigate how our approach generalizes across languages and tasks."
D15-1299,2,1.increase the size of the dataset.
D15-1299,5,2.discuss the issue of unbalanced dataset and text classification.
D15-1299,1,3.extend the generated method either automated or manually.
D15-1303,1,our future work will focus on extracting more relevant features from the visual modality.
D15-1303,1,we will employ deep 3d convolutional neural networks on this modality for feature extraction.
D15-1303,1,we will use a feature selection method to obtain key features; this will ensure the scalability as well as stability of the framework.
D15-1303,1,"we will continue our study of reasoning over text (jimenez et al., 2015; pakray et al., 2011; sidorov et al., 2014; sidorov, 2014) and in particular of concept based sentiment analysis (poria et al., 2014b)."
D15-1304,3,we also plan to study the cases where english and arabic translations have different sentiments due to cultural differences.
D15-1304,4,"additionally, the work will be extended to the arabic dialects for which aramorph-like morphological analyzers are available."
D15-1304,1,the future plans include manually correcting slsa to reach a nearly 100% accuracy.
D15-1305,4,"in future, we will explore the applications of dependence distributed representation."
D15-1308,2,we also plan to explore larger datasets and features based on network structures.
D15-1308,6,"future research might diverge to other types of online collective action, such as online petitions and open source communities."
D15-1309,2,future work could use a larger training set from multiple online sites to analyze the patterns of online abuse discourse across varied forums.
D15-1311,1,an interesting direction for future work would be adding non-textual features.
D15-1311,1,"for example, the rumour diffusion pattern (lukasik et al., 2015) may be a useful cue for judgement classification."
D16-1001.pdf,1,"in the future,  we hope to achieve still better results using beam search, which is relatively straight-forward given that the parsing system already uses a fixed number of actions."
D16-1002.pdf,3,"regarding the qa application, there are two nat-ural extensions that we want to address, namely todevelop general and automatic entity and predicatelinking mechanisms for large knowledge bases, andto test our approach in datasets that require higherlevels of compositionality such as the qald challenges (unger , 2015) or those datasets pro-duced by wang (2015)."
D16-1002.pdf,1,"another extension would be to make the rule extraction more robust against parsing errors, using pairs of forests instead of pairs of trees, similarly as in liuet al. (2009)."
D16-1002.pdf,1,one step further in the generalization of the rule ex-traction is to remove the necessity of explicitly pro-viding  cost  functions  such  as  word-to-word  hard-alignments  or  costs  between  tree  fragments
D16-1004.pdf,4,"in particular, it would be fruitful to apply our idea into constituent structure induction for which, to our knowledge, there has been no successful pcfg-based learning algorithm."
D16-1004.pdf,4,future work includes applying our dep constraint into other pcfg-based grammar induction tasks beyond dependency grammars.
D16-1008.pdf,1,"we note that an extension similar to semi-markov or weak semi-markov  (muise  and  lu,  2016)  is  possible  for  our models. we leave this for future investigations."
D16-1010.pdf,1,"in  the  future, we  aim  to  analyze  the  role  of  fuller  syntactic  distributions in restricting overgeneralization patterns."
D16-1010.pdf,1,we aim to further analyze manual and automated methods for semantic feature extraction in future work.
D16-1011.pdf,1,"we could also apply variance reduction techniques to increase stability  of  stochastic  training (cf.   (weaver  and  tao,2001; mnih et al., 2014; ba et al., 2015; xu et al.,2015))."
D16-1011.pdf,1,the encoder and generator  can  be  realized  in  numerous  ways  with-out changing the broader algorithm.
D16-1012.pdf,1,"in future work, we would like to investigate the other sharing mechanisms of neural network based multi-task learning."
D16-1013.pdf,3,"in future work, we plan to test our framework with alternative models for natural language inference (e.g., wang and jiang(2016)), and explore the effect of pretraining such a model specifically on an inference task."
D16-1014.pdf,3,"extending this work beyond causality, we hypothesize that additional embedding spaces customized to the different information needs of questions would allow for robust performance over a larger variety of questions, and that these customized embedding models should be evaluated both directly and indirectly to accurately characterize their performance."
D16-1015.pdf,1,"in the future, we would like to investigate how the abstraction and explicit type inference can be incorporated in the early stage of semantic parsing for generating better candidate logical forms."
D16-1017.pdf,3,"in future work, we intend to test the potential contribution of this model when applied to larger tasks such as entailment and inference tasks as well as semantic surprisal-based prediction tasks."
D16-1018.pdf,3,"finally, we plan to evaluate our model with more nlp tasks."
D16-1018.pdf,1,"also,  we  will  study unsupervised methods to link the learned senses to existing inventories and to automatically determine the numbers of senses."
D16-1018.pdf,1,"besides, we plan to use shared senses instead of lexemes in our model to improve the  generality  of  our  model."
D16-1018.pdf,1,"for the future work, we plan to try learning our model with soft em."
D16-1019.pdf,1,"for future work, we would like to (i) investigate the efficacy of incorporating other types of logical rules such as∀x, y, z: (x,capital-of, y)⇒¬(x,capital-of, z). (ii) investigate the possibility of modeling logical rules using only relation embeddings as suggested by demeester et al. (2016),e.g., modeling the above rule using only the embed-ding associated with capital-of.  this avoids grounding, which might be time and space inefficient especially for complicated rules. (iii) investigate the use of automatically extracted rules which are no longer hard rules and tolerant of uncertainty."
D16-1020.pdf,1,"future work includes extending the representations to new contexts – such as the alternative lexicalization an-notated in the pdtb, the modals or some adverbs– using more sophisticated weighting schemes (le-bret  and  collobert,  2014)  and  testing  this  strategy for other languages and domains."
D16-1024.pdf,3,the sentiment lexicon is also another kind of useful resource for classification.
D16-1024.pdf,3,we will explore how tomake full usages of these resources in the proposed framework.
D16-1024.pdf,3,"in future work, we will evaluate the performance of our model on more datasets and more language pairs."
D16-1025.pdf,1,"while nmt proved superior to pbmt with respect to all error types that were investigated, our analysis also pointed out some aspects of nmt that deserve further work, such as the handling of long sentences and the reordering of particular linguistic constituents requiring a deep semantic understanding of text."
D16-1026.pdf,3,"we expect even higher improvement with more languages, but it must be tested thoroughly in the future."
D16-1026.pdf,6,"considering that this is the first attemptat such zero-resource, or extremely low-resource, translation using neural machine translation, we expect a large progress in near future."
D16-1029.pdf,4,"we  hope  the  proposed  approach  can  shed light on how to leverage data on the web, and eventually  improves  other  semantic  parsing  tasks  suchas knowledge base question answering and mappingnatural instructions to actions."
D16-1032.pdf,1,"the neural checklist model can also be adapted to handle multiple checklists,  such as checklists over composite entities created over the course of a recipe(see kiddon (2016) for an initial proposal)."
D16-1032.pdf,1,"the neural check-list model is sensitive to hyperparameter initialization,  which  should  be  investigated  in  future  work."
D16-1035.pdf,1,we will try extending attention mechanism to obtain the representationof  a  text  span  by  referring  to  another  text  span  at minimal additional cost.
D16-1036.pdf,1,we will extend our framework to response generation approaches in our future work.
D16-1036.pdf,1,we believe it will help construct a  better  representation  of  context  in  the  encoding phrase of dnn-based generation model and thus improve the performance.
D16-1037.pdf,4,"furthermore,  we  are  also  interested  in  adapting  our model to other similar tasks, such as nature language inference."
D16-1037.pdf,1,"in the future, we would like to exploit the utilization of discourse instances with explicit relations for implicit drr."
D16-1037.pdf,1,for this we can start from two directions:  1)  converting  explicit  instances  into  pseudo implicit instances and retraining our model;  2) developing  a  semi-supervised  model  to  leverage  semantic information inside discourse arguments.
D16-1038.pdf,1,"one of the key research directions is to extend this unsupervised approach to a range of other relations among events, including temporal and causality relations, as is (do et al., 2011; do etal., 2012)."
D16-1040.pdf,4,"as part of future work, we hope to analyze sictf further, as-421_x000D_ sign labels to induced categories, and also apply the model to more domains."
D16-1041.pdf,4,for future work we are planning to apply this strategy to learn large-scale semantic relations beyond hypernymy.
D16-1041.pdf,4,"finally, we see potential in the domain clustering approach for improving graph-based taxonomy learning systems, asit can serve as a weighting measure as to how pertinent a given set of concepts in a taxonomy are for a specific domain."
D16-1041.pdf,1,"as mentioned in section 5.2.2, we are also planning  to  combine  our  distributional  approach  with rule-based heuristics, following the line of work introduced by shwartz et al. (2016)."
D16-1041.pdf,1,"in the context of semantic web, we would like to include semantic parsers and distant supervision to our algorithm in order to capture n-ary relations between pairs of concepts to further create and im-prove existing kbs."
D16-1042.pdf,1,another  idea  we  want  to  explore  is  to  use different  distributions  as  a  prior  to  multinomials.
D16-1043.pdf,1,we hope that this work may be used as a reference in determining some of the choices that can be made when developing multi-modal models.
D16-1046.pdf,1,more efforts are also needed in developing an effective yet robust method for multi-task learning.
D16-1046.pdf,1,"our work also points out some future directions of research, for example, we would like to analyze the effect of different mult strategies."
D16-1047.pdf,1,"clearly we would expect this model to be more effective in languages with richer morphological structure than english, and we plan to explore this possibility in future work."
D16-1047.pdf,1,"if we follow tsvetkovet al. (2015) in the argument that word embeddings should correspond to lexical semantic features, then an  inventory  of  such  features  could  be  used  as  a source of partial supervision,  thus locking dimensions of the word embeddings to specific semantic properties."
D16-1048.pdf,4,"hard similarization could be more suitable for cross-lingual applications, and we leave thisaspect for future research."
D16-1048.pdf,1,"the cross-lingual similarization in this paper is still soft similarization, it is worth to investigate the hard similarization, where the  syntactic  structures  are  totally  isomorphic  between two languages."
D16-1048.pdf,1,"of course, in such syntactic structures, the syntactic nodes should be super-node, that is, a graph containing one or more basic syntac-tic nodes."
D16-1049.pdf,3,"although we evaluated the correlation between the estimated system performance scores and the wmt official scores, other evaluation procedures might also be considered."
D16-1049.pdf,1,it will be required to investigate correlation between the estimates and expert decisions.
D16-1049.pdf,1,"in  the  future  work,  we  will  incorporate  active learning  to  the  proposed  method  so  that  we  could reduce the total number of comparisons to obtain final results."
D16-1050.pdf,1,"in  the  future,  since  the  latent  variable  in  our model is at the sentence level,  we want to explore more  fine-grained  latent  variables  for  neural  machine translation, such as the recurrent latent vari-able model(chung et al., 2015)."
D16-1050.pdf,4,We are also interested in applying our model to other similar tasks.
D16-1053.pdf,1,"in the future, we hope to develop more linguistically plausible neural architectures able to reason over nested structures and neural models that learn to discover compositionality with weak or indirect supervision."
D16-1055.pdf,2,another extension of this work would be to apply the same translation for translating answers into the question language (in addition to question translation).
D16-1055.pdf,1,"one potential next step is to learn bilingual embeddings directly for the task of qa,  for  which  we  have  started  adapting  some  re-lated work (bai et al., 2010)."
D16-1055.pdf,1,"finally,  since one of the take-away messages of our work is that a deeper understanding of linguistic context can improve qa effectiveness via more sophisticated question translation, we are hoping to see even more improvements by creating features based on word embeddings."
D16-1055.pdf,1,more sophisticated merging of multiple ranked lists of answers need to be explored.
D16-1055.pdf,1,learning  to  rank  between  answers  from  different languages might be more effective than heuristics.
D16-1057.pdf,4,"in the future our method could also be integrated with supervised domain-adaption (e.g.,yang andeisenstein, 2015) to further improve these domain-specific results."
D16-1057.pdf,1,we  hope  these tools will facilitate future quantitative studies on the domain-dependency of sentiment.
D16-1058.pdf,1,"as future work, an interesting and possible direction would be to model more than one aspect simultaneously with the attention mechanism."
D16-1061.pdf,2,"in future work, we will investigate the efficiency ofthe proposed approach in other datasets and exploreother methods in capturing the inter-relations of e-motions."
D16-1062.pdf,3,future work can adapt this analysis to create evaluation mechanisms for other nlp tasks.
D16-1065.pdf,1,"in addition, we are interested in further incorporating the incremental semantic role labeling into our incremental  framework  to  allow  bidirectional  information flow between the two closely related tasks."
D16-1065.pdf,1,"in  future  work,  we  plan  to  improve  the  parsing performance  by  exploring  more  features  from  the coreference resolution,  word sense disambiguation system  and  other  external  semantic  resources."
D16-1067.pdf,1,"in  future  research, we aim to automatically estimate the number  of  topics  to be  used  in  the  eup  models."
D16-1067.pdf,1,we also  plan  to  explore  the  use  of  more  external  resources and  novel  latent  semantic  models to  enhance performance.
D16-1069.pdf,1,"in the future, we plan to integrate the  dynamic  dictionary  into  the  term  construction model in information retrieval."
D16-1071.pdf,4,"as morphological analyzers are becoming more widely available, our method – which is easy to implement, only requiring running the analyzer should become applicable to more and more languages."
D16-1073.pdf,4,"in addition, we plan to apply our approach to other unsupervised tasks such as word alignment and sentence cluster-ing."
D16-1073.pdf,1,"for future work, we plan to extend our approach in  learning  lexicalized  dmv  models."
D16-1075.pdf,4,"in the future, we plan to generalize the stream summarization problem to various streams such associal (e.g., twitter), image (e.g., imgur) and evenvideo streams (e.g., youtube), which would yield many interesting and practical applications (lu etal., 2016) to deal with the information overload challenge in the big data era."
D16-1076.pdf,1,"moving  forward,  we  plan  to  explore  additional mechanisms for exploiting supervision at lower levels in neural architectures."
D16-1076.pdf,1,"furthermore, we believe an alternative approach may be a hybrid of the at-cnn  and  ra-cnn  models,  wherein  an  auxiliary loss  might  be  incurred  when  the  attention  mechanism output disagrees with the available direct supervision on sentences."
D16-1078.pdf,1,this  suggests our future direction to extend the model from token level  to  parse  tree  level in  better  capturing long-distance syntactic dependency and to address the cross-domain adaptation issue.
D16-1082.pdf,1,these results suggest that the structured prediction models are good directions for improving the exact phrase extraction for clinical entities.
D16-1083.pdf,1,"in future work, we plan to explore a more effective way to learn the embeddingsof review text."
D16-1084.pdf,5,future work will investigate further the challenge of stance detection for tweets which do not contain explicit mentions of the target.
D16-1085.pdf,5,"in the future, we plan to investigatethe non-consecutive architecture on other problemssuch as relation extraction or slot filling."
D16-1086.pdf,3,"in the future, studies targeting less similar languages could further evaluatethe portability of props."
D16-1086.pdf,2,"directions for future work on propsde are extensions of the rule set to better cover  complex  coordination  constructions,  nested sentences and nominal predicates."
D16-1088.pdf,4,"while our approach is certainly tailored to the civil unrest domain, we believe that this method is applicable to many other domains within the scope of news reports, including health, economics and even politics, where reporters overwhelmingly rely on outside opinion to present the facts of the story and provide the summary themselves."
D16-1090.pdf,6,"one possibility includes identifying the premise entailedin a question, as opposed to just stating true- or false-premise."
D16-1092.pdf,5,our work thus poses an interesting question for future work – what is the right semantic space in which it is meaningful to talk about necessary and sufficient attention maps for humans?
D16-1093.pdf,4,"as to future work, we plan to apply residual learning to other sequence tasks such as language modeling, and rnn based neural machine translation (sutskever ,2014) (cho , 2014)."
D16-1094.pdf,1,"in  future  work, we will integrate our language model into the moses machine translation pipeline to intrinsically measure its impact on translation qualities, which is of particular use for out-of-domain scenario."
D16-1095.pdf,1,in  future  work  we could  also  seek  to  find  methods  that  can  auto-matically  optimize  these  parameters.
D16-1097.pdf,1,"in future work, we hope to develop models to exploit this."
D16-1099.pdf,1,"this interpretation suggests that it might be interesting to investigate hybrid models that use different processing models — or at least different parameterizations — for different frequency ranges, and for different data sizes."
D16-1100.pdf,1,"as future work, we would like to 1) investigate more complicate composition manners among radicals, characters, and words, e.g., a hierarchical structure of them. 2) explore the semantic composition of higher level language units such as phrases, sentences, and even documents."
D16-1101.pdf,4,"in future work, we will plan to apply numerically grounded models to other tasks, such as numeric error correction."
D16-1101.pdf,1,"for sec, a trainable  hypothesis generator can potentially improve the coverage of the system."
D16-1101.pdf,1,"we will explore alternative ways for deriving the numeric representations, such as  accounting  for  verbal  descriptions  of  numbers."
D16-1104.pdf,1,"our word embedding-based features may work better if the similarity scores are computed for a subset of words in the sentence, or using weighting  based  on  syntactic  distance  instead  of  linear distance as in the case of our weighted similarity-based features."
D16-1105.pdf,1,"thus,  automatic  detection  of  targets  and  inferring the  stance  towards  all  of  the  targets  is  the  next step toward creating a practical weakly-supervised stance classifier."
D16-1106.pdf,4,"applying core nlp technologies to local news reports of gun violence couldtransform raw text into structured, queryable datathat public health researchers can use."
D16-1107.pdf,1,"planned future work includes  a  more  flexible  decoupling  of  the  language and structure fits (in light of section 5),  and moving from pre-existing lid systems to joint models where lid scores are directly informed by structural information."
D16-1111.pdf,1,"in the future, we plan to create cross-lingual models by applying multilingual word embeddings (ammar et al., 2016)."
D16-1113.pdf,1,"in future work, by taking advantage of the promising results of robust gram, we intend to explore the model’s  behavior  in  various  settings."
D16-1113.pdf,1,"we also plan to enhance the underlying optimization by designing elastic constraints (zouand hastie, 2005) specialized for word embeddings."
D16-1113.pdf,1,"in  particular, we plan to model various corpora, i.e. predictive modeling of sequentially arriving network packages."
D16-1113.pdf,1,"another  future  direction  might  be  encoding  avail-able domain knowledge by additional regularization terms, for instance, knowledge on synonyms can be used to reduce the degrees of freedom of the optimization."
D16-1116.pdf,1,a natural extension would also be a formulation where semi-supervised training was performed in both x and y.
D16-1117.pdf,5,"our future work will focus on lifting this restriction,in order to allow relations expressed across multiple sentences and multiple relations expressed in thesame sentence."
D16-1118.pdf,1,"another extension of the current work is to investigate a similar approach for other modality markers such as nouns (e.g., possibility, chance), adjectives (e.g. necessary, probable, ) and certain verbs(e.g., claim, suggests)."
D16-1118.pdf,1,"we believe better results could be obtained by incorporating features capturing knowledge in the context of the modal construction, including other clauses in the same sentence, and the previous and next sentences."
D16-1120.pdf,1,n  interesting  direction for future research would be to combine distant supervision with unsupervised linguistic models to au-tomatically uncover such underrecognized dialectal language.
D16-1121.pdf,5,"how do such preferences vary across topics, users and other multilingual communities?"
D16-1121.pdf,5,how representative of the society is this kind of social media study?
D16-1121.pdf,5,"is it the case that english being the language of aspiration in india, it is preferred for positive expression?"
D16-1121.pdf,5,"or is it because hindi is specifically preferred for swearing and therefore, is the language of preference for negative emotion?"
D16-1121.pdf,5,this raises some intriguing socio-linguistic questions.
D16-1121.pdf,6,we plan to explore some of these questions in the future.
D16-1122.pdf,4,"we anticipate that capsule, and ourvisualization tool, will be useful for historians, political scientists, and journalists who wish to explore and understand large corpora of documents."
D16-1123.pdf,1,"since cnns are quite different in nature, we believe that a fruitful line of future research could focus  on  integrating  the  convolutional  layer  into  are current  structure  for  language  modeling,  as  well as other sequential problems, perhaps capturing the best of both worlds."
D16-1124.pdf,4,"as the framework discussed here is general, it is also possible that they could be used in other tasks that perform sequential prediction of words such asneural machine translation (sutskever , 2014)or dialog response generation (sordoni , 2015)."
D16-1124.pdf,1,"in  addition,  given  the  positive  results  using  block dropout for hybrid models, we plan to develop more effective  learning  methods  for  mixtures  of  sparse and dense distributions."
D16-1125.pdf,4,we believe that this general strategy of using reasoning to obtain novel contextual behavior from neural decoding models might be more broadly applied.
D16-1126.pdf,1,"we hope that future work will provide more discourse structure and function to automatic poetry, while  maintaining  the  syntax,  semantics,  and  creative phrasing we observe."
D16-1128.pdf,2,"in this paper, we have only focused on generating the first sentence and we will tackle the generation oflonger biographies in future work."
D16-1128.pdf,3,a loss function that could assess factual accuracy would certainly improve sentence generation by avoiding such mistakes.
D16-1131.pdf,2,"one potential path would be to leverage data where discourse relations are explicitly annotated, such as that in the penn discourse treebank (prasad , 2008)."
D16-1131.pdf,5,"in closing, we would like to return to the larger question of effectively handling ellipsis."
D16-1131.pdf,1,"in future work, we hope to improve the performance of several of our features."
D16-1131.pdf,1,"in addition, although our content and correlate features were not useful alongside the  others,  we  hope  that  more  refined  versions  of those could provide some assistance."
D16-1131.pdf,1,"we also noted that  our  performance  was  impacted  by wh-types, and therefore it might be helpful to learn differen tfeatural weights per type."
D16-1132.pdf,4,"as future work, we plan to use our mcnn architecture for inter-sentential zero an aphora resolution and develop highly accurate nlp applications using our intra-sentential subject zero an aphora resolution method."
D16-1133.pdf,4,"finally, we will investigate downstream applications of our alignment methods, in the areas of both language documentation and speech translation."
D16-1133.pdf,1,"we plan to try incorporating ibm model 3 or the hmm alignment model(vogel  et  al.,  1996)  instead."
D16-1134.pdf,1,"we  believe that hume, and a future automated version of hume, allows for a finer-grained analysis of translation quality, and will be useful in informing the development of a more semantically aware approach to mt."
D16-1136.pdf,1,"our  combination  techniques during training, especially using regularization, are highly effective and could be used to im-prove monolingual word embeddings."
D16-1137.pdf,2,future work will examine scaling this approach to much larger datasets.
D16-1138.pdf,1,for future work we would like to incorporate attention-based models to our framework to enable such models to process data online.
D16-1139.pdf,4,we anticipate that methods described in this paper can be used to similarly train smaller models in other domains.
D16-1142.pdf,1,"in future, we shall look into more linguistic evidences to improve our model."
D16-1143.pdf,4,"this framework has good scalability and can be applied on other concrete features, such as users book reading behaviors and music listening behaviors."
D16-1144.pdf,1,"the proposed objective function is general and can be considered to incorporate various language features, to conduct integrated modeling of multiple sources, and to be extended to distantly-supervised relation extraction."
D16-1144.pdf,1,"as future work,  it would be interesting to study topical features as the context cues of the entity mentions, to leverage multi-sensing embedding to represent linguistic features with multiple senses, and to exploits other effective modeling methods to inject type hierarchy information."
D16-1145.pdf,1,"in the future, we are interested in exploring mining formulas directly in the distributional spaces which may resolve the sparsity of formulas."
D16-1146.pdf,1,"in future work,  we want to extend the proposed ideas  beyond  implications  towards  general  first-order logic rules."
D16-1146.pdf,1,"furthermore, we want to integrate these ideas into neural methods beyond matrix factorization approaches."
D16-1146.pdf,1,"we believe that supporting conjunctions, disjunctions and negations would enable to debug and improve representation learning based knowledge base completion."
D16-1147.pdf,4,"these models could be applied to storingand reading memories for other tasks as well, andfuture work should try them in other domains, such as in a full dialog setting."
D16-1149.pdf,1,"our  current  plans  include  continued corpus development (recall section 4.4), and using more sophisticated methods than dyad averaging(e.g., using weighting based on team process measures) to move from dyads to teams."
D16-1152.pdf,3,"we would also like to investigate other metadata attributes that are relevant to the task, such as spatial and temporal signals."
D16-1152.pdf,6,"social networks arise in other settings besides microblogs, such as webpages and academic research articles; exploiting these networks is a possible direction for future work."
D16-1153.pdf,4,we believe these results to be encouraging and look forward to replicating these results on more language pairs in different language families and further automating the process of obtaining phonological character representations.
D16-1154.pdf,4,"this work, however, did not investigate the nature of the long and short context encodedby this network or its possible applications for other nlp tasks."
D16-1155.pdf,1,our future work will  explore  incremental  learning  through  human-agent dialogue to acquire grounded task structures.
D16-1156.pdf,4,"while we have demonstrated our approach ona task involving simultaneous reasoning about language and vision, our approach is general and canbe used for other applications."
D16-1158.pdf,5,"another future area is to see if the techniques formaking bayesian networks discriminative can fix the length bias of encoder decoder networks (peharz etal., 2013; guo , 2012)."
D16-1158.pdf,1,an interesting future work is to explore if the ed model can be used to generate a candidate set of responses which are then reranked by our globally conditioned model.
D16-1160.pdf,4,"we also plan to apply our methodsin other languages, especially for low-resource languages."
D16-1160.pdf,1,"in  the  future,  we  would  like  to  design  smarter mechanisms to distinguish real data from synthetic data in self-learning algorithm, and attempt to pro-pose better models for handling source-side mono-lingual  data."
D16-1162.pdf,2,"for future work, we are interested in conductingthe experiments on larger-scale translation tasks."
D16-1162.pdf,3,"we also plan to do subjective evaluation, as we expectthat improvements in content word translation arecritical to subjective impressions of translation re-sults."
D16-1162.pdf,1,"finally, we are also interested in improve-ments to the linear method where λ is calculated based on the context, instead of using a fixed value."
D16-1164.pdf,2,"considering the usage of other kinds ofdata (e.g., tags, comments) as additional “spaces” to extend the cqa clustering problem is an interestingdirection for future work."
D16-1164.pdf,1,the extension of the formulation to include a weight learning step may be appropriate for scenarios  where  prior  information  on  the  relative  importance of the different spaces is not available.
D16-1165.pdf,4,"we further want to apply a similar network to other semantic similarity problems, such as textual entailment."
D16-1165.pdf,1,"in future work, we plan to use the labels for sub-tasks a and b, which are provided in the datasets in order to pre-train the corresponding components of the full network for answer ranking."
D16-1166.pdf,4,in the future we want extend our system to multi-relation questions.
D16-1167.pdf,3,"while the experiments only focused on sequence-to-sequence decoding, our preliminary experiments with otherhigh-capacity deep neural nets seem promising."
D16-1167.pdf,1,another future work direction is to derive efficient mechanisms to guide the humans that are creating the data generation programs.
D16-1168.pdf,2,"future work could improve the thematicity andsolvability components by incorporating domain-specific and commonsense knowledge, leveraginginformation extraction."
D16-1168.pdf,1,"finally, we in tend to incorporate the generated problems in educational technology and tutoring systems."
D16-1168.pdf,1,"additionally,  we  plan  to study rewriting in other domains such as children's short stories and extend the model to generate math word problems directly from equations."
D16-1171.pdf,2,we will take advantages of those information in sentiment analysis infuture.
D16-1171.pdf,1,we will explore the effectiveness of our model on aspect level sentiment classification.
D16-1172.pdf,4,"for future work, we are going to design a strategyto dynamically adjust the forgetting rates for fine-grained document-level sentiment analysis."
D16-1173.pdf,4,"in the future work, we plan to apply our framework too ther text and vision applications."
D16-1174.pdf,3,we also plan to evaluate the performance of our approach with other decay functions as well as with other initial word representations.
D16-1174.pdf,4,we intend to apply our technique to the task of harmonizing biomedical terms in the phenebank project.
D16-1174.pdf,1,"as future work, we plan to investigate the possibility of using larger semantic networks, such as free-base and babelnet,  which would also allow us to apply the technique to languages other than english."
D16-1175.pdf,5,we further more plan to investigate whether the number of neighbours required for improving elementary vector representations remains as high for other compositional tasks and longer phrases as in this study.
D16-1175.pdf,1,in future work we aim to scale our approach to semantic composition with distributional inference to longer phrases and full sentences.
D16-1176.pdf,1,"in future work, we would like to incorporate some gating strategies into the depth dimension of our proposed models, like highway or residual network, to enhance the interactions between depth and other dimensions thus training more deep and powerful neural networks."
D16-1177.pdf,1,"in future work, we intend to further revise and extend these protocols as well as produce novel protocols aligned with decomp."
D16-1179.pdf,1,we also seek to improve the antecedent identification approach by extracting stronger features.
D16-1179.pdf,1,"in  future  work,  we  would  like  to  further  investigate  other  margin-based  optimizations  similar  to mira,  but  perhaps  even  more  resilient  to  over-fitting."
D16-1181.pdf,1,"for future work, a natural directions to explore integrated super tagging and parsing ina single neural model (zhang and weiss, 2016)."
D16-1182.pdf,1,"while there are some previous works on  fixing  the  unnecessary  words  in  the  literature(xue and hwa, 2014), it is worthy to develop better nlp methods for catching and mitigating the missing word errors prior to parsing."
D16-1186.pdf,1,"since there was no assistant system to check the  quality  for  many  years  (hussain  et  al.,  2007)we  plan  to  extend  the  provided  system  in  order to  provide  some  quality  analysis  of  the  extracted information."
D16-1186.pdf,1,"based on these insights, we want  to  implement  a  system  for  the  resolution  of vagueness and incompleteness of nl requirements."
D16-1187.pdf,1,"for future work, we plan to create much more ground truth review data from other review domains and different applications like forums or microblogs, and further test our proposed models for deceptive opinion spam detection."
D16-1187.pdf,1,"we also plan to incorporate our model into a practical opinion mining system, in this way, more reliable opinion and sentiment analysis results can be then expected."
D16-1188.pdf,5,"future work could involve a more thorough investigation on how to create and cluster graphs, i. e.covering weighted and/or signed cases."
D16-1188.pdf,1,"additionally, we plan on exploring alternative regularization algorithms diverging from the group-lasso method."
D16-1188.pdf,4,"gaussian mixture models (mclachlan and basford, 1988) could be applied in order  to  capture  overlapping  groups  at  the  cost  of high complexity."
D16-1188.pdf,1,"furthermore, topical word embeddings (liu et al.,  2015) can be considered for regularization."
D16-1189.pdf,1,"it would also be of interest to explore implementations of backtracking in the sub-action space (incurring accost),  in  order  to  recommend comments  that  were not selected earlier but have become highly popular."
D16-1190.pdf,2,"finally,analyzing further languages and data sets helps tofurther complete our findings."
D16-1190.pdf,5,whether learning edit scriptson such intricate transformations is possible is anopen question and valuable future research.
D16-1192.pdf,2,"in future work, we plan to increase the numberof sentences in our data set, so that additional morefine-grained features might be considered."
D16-1192.pdf,1,"finally, we are interested in the contribution of context in understanding the meaning of an unknown word."
D16-1192.pdf,1,another goal is to obtain absolute difficulty labels for sentences by calibrating ordinal ranges based on the relative ranking.
D16-1192.pdf,1,"our use of the crowd-based labels was intended to reduce noise in the ranking analysis, but we also intend to use the pairwise predictions produced by the logistic model as the input to the aggregation model, so that rankings can be obtained for previously unseen sentences in operational settings."
D16-1199.pdf,3,we plan to test our models ability to generalize to other typesof infobox-like tables on the web.
D16-1203.pdf,5,"our motive is simply to better understandcurrent generation models via their behaviors, anduse these observations to guide future choices – dowe need novel model classes? or dataset with dif-ferent biases? etc."
D16-1207.pdf,4,"for future work, we plan to apply our regular-ization method to other models and tasks to deter-mine how generally applicable our method is."
D16-1207.pdf,1,"also,we will explore methods for more realistic linguisticnoise, such as lexical, syntactic and semantic noise,to develop models that are robust to the kinds of dataoften encountered at test time."
D16-1210.pdf,1,"in future work, we plan to incorporate a phrasalitg (cherry and lin, 2007) instead of a bracketingitg to efficiently handle many-to-many alignments."
D16-1212.pdf,1,this method can also be used in other probabilistic methods.
D16-1213.pdf,3,we hope that this contribution will bring brain imaging tests to the masses and encourage discussion around the testing of ds models against brain imaging data.
D16-1214.pdf,1,our ongoing work explores creating a syntax-semantics loop where each benefits the other with no human (annotation) in the loop.
D16-1215.pdf,4,"although tested on english, the proposedmethodology can be applied to all languages in the ppdbeven to the ones that do not dispose of a de-pendency parser (as shown by the high performanceof the bow.vec models)."
D16-1215.pdf,1,"we consider using a vector space  model  of  semantic  composition  to  calculate the meaning of longer candidate paraphrases (dinuet al., 2013; paperno et al., 2014) and select appropriate substitutes for phrases in context."
D16-1217.pdf,5,our findings suggest that further work isneeded to understand when automatic translation oflanguage-based models will lead to competitive sen-timent translation on social media and how suchtranslations can be improved.
D16-1217.pdf,5,"we expect that language indicators of personality and emotion willsimilarly translate poorly, but that remains to bestudied."
D16-1218.pdf,5,"many questions remain; in providing a quantitative descriptionof this dataset, we hope to highlight its potential foranalysis, and encourage other work in this domain."
D16-1222.pdf,1,"also, we plan to extend the constrained decoding idea to slot tagging with neural networks (kim et al., 2016), which achieved gains over crfs."
D16-1222.pdf,1,"one of the future directions of research is to extend the same idea to the intent modeling, where we can re-use intent data built for different applica-tions and domains for a new domain."
D16-1223.pdf,1,leveraging our encoder-labeler lstm approach in joint training should be worth trying.
D16-1225.pdf,4,"future work includes applications in informal texts, such as tweets andshort messages (muis and lu, 2016)."
D16-1228.pdf,1,"we  believe that, for future work, fluency measures could be further improved with other methods, such as using existing gec systems to detect errors, or even using an ensemble of systems to improve coverage (indeed, ensembles have been useful in the gec task itself  (susanto  et  al.,  2014))."
D16-1230.pdf,1,future work should examine how retrieving additional responses affects the correlation with word-overlap metrics.
D16-1231.pdf,5,"hence, the prediction ofwhether torespondin a multi-party conversation would be animportant next challenge."
D16-1231.pdf,5,our future objective to tackle the task of predict-ingwhether to respondto a particular utterance.
D16-1232.pdf,1,"this empirical evidence suggests that the value-based formulation is a promising approach for arbitrary slot filling tasks, which is worth exploring further in future work."
D16-1233.pdf,3,it would be very helpful if the same comparison could be conducted inother application domains such as machine transla-tion or image caption generation so that a wider viewof the effectiveness of these approaches can be as-sessed.
D16-1233.pdf,5,"furthermore, removing slot-value delexical-isation and learning confirmation behaviour in noisy speech conditions are also main research problems from the system development prospective."
D16-1235.pdf,5,in future work we aim to apply suchmethods to the task of verb acquisition.
D16-1236.pdf,2,"future work includes jointly processing all 61 languages inthe corpus, rather than considering them pair-wise, to build a resource for all languages."
D16-1239.pdf,2,"as a first step towards this goal, we plan to carry out an in-depth analysis of disagreement in the collected data, characterize the main sources of inconsistent annotation and subsequently formulate further strategies for improving annotation accuracy."
D16-1239.pdf,3,"we believe that better understanding of human disagreements and their relation to disagreements between humans and parsers will also contribute to advancing evaluation methodologies for pos tagging and syntactic parsing in nlp, an important topic that has received only limited attention thus far (schwartz , 2011; plank , 2015)."
D16-1239.pdf,1,"at the same time, it is expected to achieve annotation quality comparable to human-based annotation by avoiding parser specific bias, which plays a pivotal role in the reduced quality of single-parser reviewing pipelines."
D16-1242.pdf,1,"the attractiveness of a logic-based system is that it is highly modular and can be extended with other components such as a robust knowledge base (lewis and steed-man, 2013; beltagy et al., 2013; bjerva et al., 2014).such an extension will be a focus of future work."
D16-1243.pdf,1,"another extension we plan to investigate is modeling of context, to capture how people describe colors differently to contrast them with other colors via pragmatic reasoning (devault and stone, 2007; gol-land et al., 2010; monroe and potts, 2015)."
D16-1243.pdf,1,"one  natural  extension  is  theuse of character-level sequence modeling to capture complex  morphology  (e.g.,  “-ish”  in  “greenish”)."
D16-1247.pdf,1,"it is possible to adopt existing models of creating vector representation of dependency sub-trees such as the model using recursive neural net-works (liu et al., 2015) and convolutional neural networks (mou et al., 2015)."
D16-1248.pdf,1,"in future work, we hope to determine how other  parts  of  the  translator  work,  especially  with reference to grammatical structure and transformations."
D16-1250.pdf,1,"in the future, we would like to study non-linear mappings (lu et al., 2015) and the additional techniques in (lazaridou et al., 2015)."
D16-1252.pdf,1,we hope that future open ie systems can make use of this new resource to easily and objectively measure and compare their performance.
D16-1253.pdf,2,"since the lack of labeled data is a major challenge for implicit discourse relation classification, our proposed bisyndata can enrich the training data and then benefit future work."
D16-1254.pdf,4,we expect that heuristic backtracking could be applied to any other transition-based parser with similar benefits.
D16-1254.pdf,1,"we plan on experimenting with various heuristics and cutoff models, such as adapting the attention based models of bahdanau et al. (2014) to act as a guide for both the heuristic search and cutoff."
D16-1257.pdf,1,we also wish to develop a complete parsing model using the lstmlm framework.
D16-1257.pdf,1,"we suspect building large models with character embeddings would lead to further improvement as in language modeling (kim et al., 2016; jozefowicz et al., 2016)."
D16-1258.pdf,4,"we hope to scale these methods to large unlabelled corpora or other languages, to provide data for re-training parsers."
D16-1258.pdf,5,"future work will explore the possibility of asking questions about other types of parsing uncertainties, such as nominal and adjectival argument structure, and a more thorough treatment of prepositionalphrase attachment, including distinctions between arguments and adjuncts."
D16-1260.pdf,2,"as future work: (1) we will incorporate the valid time of facts. (2) some time-sensitive facts lack temporal information in yago2, we will mine such temporal information from texts."
D16-1262.pdf,4,"in future work, we will apply our approach to other structured prediction tasks, where neural networks—and greedy beam search—have become ubiquitous."
D16-1263.pdf,1,"future work should use a universal phoneme recognizer or acoustic model of a similar language, thus making a step towards true generalizability"
D17-1001,4,"in addition, we will apply our method to parallel parsing and other grammar, e.g., projective dependency trees."
D17-1001,1,we plan to extend our method to align comparable paraphrases that are partially paraphrase sentences.
D17-1001,1,"furthermore, we will apply such syntactic paraphrases to phrase embedding."
D17-1002,1,"two possibilities are to create even better training methods, and to find some way to extend our run-time improvements to other transition systems."
D17-1002,1,it would also be interesting to further investigate relationships between graph-based and dependency-based parsing.
D17-1003,1,it would be interesting to extend these lines of work to decrease the complexity of our quartic algorithm.
D17-1005,1,another is to incorporate openie methods to automatically find domain specific patterns and generate pattern-based labeling functions.
D17-1005,1,one is to apply transfer learning techniques to handle label distributions’ difference between training set and test set.
D17-1005,6,There exist several directions for future work.
D17-1007,1,"in this work we directly use the default ranker in lucene for similar sentences, which can be improved in future work."
D17-1008,3,following bennett (2016) we will also experiment on more realistic estimates of p(s|w) in formula 5 as well as other assumptions made in our work.
D17-1008,1,"as future work we plan to extend our approach to verbs, adjectives and adverbs."
D17-1009,5,"in the future, we would like to explore how this framework can benefit applications such as summarization (liu et al., 2015) and machine reading (sachan and xing, 2016)."
D17-1010,1,"in this paper, the mimick model was trained using characters as input, but future work may consider the use of other subword units, such as morphemes, phonemes, or even bitmap representations of ideographic characters (costa-jussa et al.` , 2017)."
D17-1011,1,"creating a common map of tense along the lines of figure 1, but unifying the three tenses addressing shortcomings of the way we compute alignments: (i) generalizing character n-grams to more general features, so that templates in templatic morphology, reduplication and other more complex manifestations of linguistic features can be captured;"
D17-1011,1,"(ii) use n-gram features of different lengths to account for differences among languages, e.g., shorter ones for chinese, longer ones for english;"
D17-1011,1,"(iii) segmenting verses into clauses and performing alignment not on the verse level (which caused many errors in our experiments), but on the clause level instead;"
D17-1011,1,"(iv) using global information more effectively, e.g., by extracting alignment features from automatically induced bi- or multilingual lexicons."
D17-1012,4,"in future work, we investigate the effectiveness of our approach in different types of target tasks."
D17-1013,1,"in the future, it might be helpful to analyze the benefits of jointly learning other related tasks together with machine translation, to provide further control of the learning process."
D17-1014,5,"this work raises several compelling possibilities which we intend to address in future work, such as improving decoding speed, integrating additional constraints such as word coverage and fertility into decoding, and applying our method to other intractable structured prediction problems."
D17-1015,1,our dataset can be used to investigate future models that expand to handle relativization and other recursive phenomena in language.
D17-1016,4,"in this work, we used the bivariate gaussian distribution in the mdn’s mixture leaving the use of other distributions which might better suit the geolocation task for future research."
D17-1020,1,"in the future work, we will focus on the self transition of adjustable affinity preserving random walk, which could be used to remove the redundancy between summary sentences."
D17-1021,1,"in future work we will design such a model, and offer it candidates chosen not only from sentences containing the antecedent, but the larger context."
D17-1026,1,our hope is that these results can enable learning paraphrastic sentence embeddings with powerful neural architectures across many languages and domains.
D17-1027,6,there could be several directions to be explored for future work.
D17-1027,1,the structure of chinese characters and the positions of components in the character may be considered to fully leverage the component information of chinese characters.
D17-1027,1,"first, we use the average operation to integrate the sub character components as the context to predict the target word."
D17-1027,1,"to solve this problem, attention models may be used to adaptively assign weights to word context, character context, and subcharacter context."
D17-1027,1,"second, for any target word, we simply use word context, character context, and subcharacter context to predict it and do not distinguish compositional words and non-compositional words."
D17-1029,1,"future work includes applying the proposed method to other aspects of nominal semantics, such as understanding compound nouns in other languages, and to explore the compositionality of words and compounds."
D17-1030,3,"we would like to systematically evaluate, in particular, how fast the system can gain an understanding of a concept which is fully equivalent to a vector built from big data."
D17-1030,3,"in order to validate our system as a suitable, generic solution for word learning, we will have to test it on various data sizes, from the type of low- to middle-frequency terms found in e.g.the rare words dataset (luong , 2013), to highly frequent words."
D17-1030,5,our future work will thus investigate how to capture and measure informativeness.
D17-1030,1,we believe that both quality and speed of learning will be strongly influenced by the ability of the algorithm to detect what we called informative sentences.
D17-1032,2,"in future work, we wish to explore embedding signatures that leverage richer knowledge of the practitioners corpus and task."
D17-1032,1,"finally, we hope to extend vecshare’s embedding selection methods to consider syntheses of multiple distinct embedding sets, tailored to the practitioner’s task and corpus."
D17-1033,4,we also intend to extend the experiments to other nlp tasks in addition to word similarity such as word analogies.
D17-1033,1,"in future work we intend to extend the experiments to include other original pre-trained embeddings, and other algorithms for manifold learning."
D17-1034,1,"in the future, we plan to investigate reinforcement learning methods to incorporate multi-sense word representations for downstream nlp tasks."
D17-1035,5,"as future work, we will investigate if those methods are either less dependent on the hyperparameters or are less dependent on the random seed value, e.g.if they avoid converging to bad local minima.thus, there is a good chance that these configurations require less tuning when applied to new tasks or domains."
D17-1037,1,such studies would further extend the potential of word embedding methods.
D17-1037,1,"one possibility is to explore extending other embedding methods such as glove (pennington et al., 2014) to incremental algorithms."
D17-1037,6,the success of this work suggests several research directions to be explored in the future.
D17-1040,3,"we also encourage evaluating our models on other tasks that must deal with long sequences but have compact representations, such as summarization and question-answering, and further exploration of their effect on memory and training speed."
D17-1040,1,we encourage future work that explores the optimal values of k for various language tasks and examines whether or not it is possible to predict k based on the task at hand.
D17-1041,3,"second, we will investigate the interpretability of hidden structures of neural networks for nlp tasks such as (yang , 2016; li , 2016), when the rotated word vectors are used as an embedding layer."
D17-1041,6,we will examine the issues in future work.
D17-1041,1,"first, we apply target rotation (harman, 1960; browne, 1972a,b) to incorporate prior knowledge when constructing the rotated word vector representations."
D17-1041,1,"based on the meanings, we can remove irrelevant dimensions for a specific task of interest, in order to secure more efficient storage of the vectors and decrease the complexity of downstream nlp models."
D17-1041,6,We plan to explore following issues.
D17-1042,1,an interesting application would be to infer dependencies between textual and image features in image-to-text prediction (e.g. image captioning).
D17-1042,1,"also, we used a vae-based sampling for object perturbations but other approaches are possible depending on the nature of the domain or data."
D17-1043,4,"future work should explore other natural language processing tasks, where the data is likely to arise from complex, multi-modal latent factors."
D17-1044,1,"in our future work, we intend to also explore how to complement `1 penalties with terms penalizing more explicitly the processing time; we also wish to study how these ideas can be used in combination with neural models."
D17-1045,3,"our future work is to test this approach on systems with expensive communication cost, such as multi-node environments."
D17-1046,1,"we would like to build a deeper understanding of which aspects of an unsupervised objective, near uniform initialization, and non-identifiability contribute to these issues and to discover other learning problems that may share these issues."
D17-1047,1,we may also try other memory weighting strategies to distinguish multiple targets in one comment more clearly.
D17-1047,1,"therefore, we need a mechanism to stop the attention process automatically if no more useful information can be read from the memory."
D17-1048,2,future work includes using other eye-tracking information such as saccade and fixation.
D17-1048,2,the incorporation of other information such as user-product information can also be explored.
D17-1050,4,"thirdly, and perhaps most usefully for future work by others, this feedbackbased dataset will be made available for use by other researchers and in other evaluations."
D17-1051,1,another direction would be to use topic models and see whether reviewers are more inclined to compare different types of references when talking about certain aspects of restaurants or other products.
D17-1051,1,a different approach to identifying helpful reviews would be to create entertaining and informative summaries.
D17-1053,1,"in future work, we will investigate using more advanced document embedding techniques (e.g., cnn, rnn) to directly model document-level sentiment information."
D17-1053,4,We will also extend our model to other languages.
D17-1056,3,future work will evaluate the proposed method on another datasets.
D17-1056,3,more experiments will also be conducted to provide more in-depth analysis.
D17-1057,1,"in future, we would like to extend our work by creating an end to end stock prediction system where the system would predict the future stock prices based on the sentiment score and stock value of the company."
D17-1060,4,"also, to address the problematic scenario when the kg does not have enough reasoning paths, we are interested in applying our rl framework to joint reasoning with kg triples and text mentions."
D17-1060,1,"for future studies, we plan to investigate the possibility of incorporating adversarial learning (goodfellow et al., 2014) to give better rewards than the human-defined reward functions used in this work."
D17-1060,1,"instead of designing rewards according to path characteristics, a discriminative model can be trained to give rewards."
D17-1061,2,"(2) using information from modalities other than text,"
D17-1061,1,and (3) better reinforcement learning algorithms for a partially-observable environment.
D17-1061,1,"in the future, more research is necessary in the directions of (1) iterative reformulation under the proposed framework."
D17-1062,4,"beyond sentence simplification, the reinforcement learning framework presented here is potentially applicable to generation tasks such as sentence compression (chopra , 2016), generation of programming code (ling , 2016), or poems (zhang and lapata, 2014)."
D17-1062,1,"in the future, we would like to explicitly model sentence splitting and simplify entire documents (rather than individual sentences)."
D17-1064,2,another direction for future work concerns the exploitation of the extended webnlg corpus.
D17-1064,2,"we plan to exploit this extended corpus to make available a correspondingly extended websplit corpus, to learn optimised split-and-rephrase models and to explore sentence fusion (converting a sequence of sentences into a single complex sentence)."
D17-1064,5,"in future work, it would be interesting to see whether and if so how, sentence splitting can be learned in the absence of explicit semantic information in the input."
D17-1065,1,further explorations of gan-based techniques to model contextual information in dialogue problems will be addressed in our future research.
D17-1066,4,in future work we plan to apply our vae model to semi-supervised nlp tasks and experiment with conditioning generation on text attributes such as sentiment and writing style.
D17-1068,1,"for example, future work might aim at identifying strategies for tuning the parameter n to account for the different degrees of selectivity of each verb-specific role."
D17-1068,1,"another possible extension would be the inclusion of a mechanism for updating the role prototypes depending on how the other roles are filled, which would be the key for a more realistic and dynamic model of thematic fit expectations (lenci, 2011)."
D17-1071,4,"given this interpretative ability, we believe that our logic-based system may also be of benefit to other natural language processing tasks, such as question answering and text summarization."
D17-1071,4,"in future work, we will refine our system so that it can be applied to other tasks such as question answering."
D17-1072,3,"we believe there are many exploration directions for this new task, among which we are particularly interested in three in the near future: 1) improving our benchmark approaches by considering task-specific features and neural network architectures, 2) verifying the usefulness of mws to highlevel applications such as mt, 3) integrating mws with syntactic parsing in some way by exploiting existing treebanks."
D17-1073,4,"finally, we aim at applying our models to arabic dialects and other languages."
D17-1073,1,"we expect that character-level embeddings will have a bigger role in scenarios with noisy input, such as non-standard spontaneous orthography used in social media."
D17-1073,1,"we also intend to further investigate the role of syntax features in morphological disambiguation, and explore additional techniques for more accurate tagging."
D17-1073,1,"future directions include exploring additional deep learning architectures for morphological modeling and disambiguation, especially joint and sequence-to-sequence models."
D17-1077,6,"in providing a labeled dataset for others to use, we hope to encourage other work that reasons about the structure present in alternative representations (such as images) as well."
D17-1078,3,"future work should focus on extending the neural morphological tagger to a joint lemmatizer (muller ¨ , 2015) and evaluate its functionality in the low-resource setting."
D17-1079,2,"it can also be useful for other nlp tasks with small labeled training data, but a large unlabeled data."
D17-1080,1,"as an anonymous reviewer suggested, a possible direction of future work is to leverage another word segmentation approach which uses linguistic features, such as the stanford word segmenter (tseng et al., 2005) with k-best segmentations."
D17-1081,2,"while this paper focused on harvesting geometry axioms from textbooks as a case study, it can be extended to obtain valuable structured knowledge from textbooks in areas such as science, engineering and finance."
D17-1082,1,we hope this dataset will stimulate the development of more advanced machine comprehension models.
D17-1084,4,"additionally, we plan to apply our findings to generating math problem."
D17-1084,1,we would like to leverage semantic parsing and transform math problems to a more structured representation.
D17-1084,1,then we can reason with small equation units to generate a final equation in testing.
D17-1084,1,"for long tail problems with a template size less 2, we want to utilize the fine grained expressions we have learned and decompose equations for learning."
D17-1084,1,"based on our error analysis, we plan to improve our model by detecting quantity types more accurately, learning relations and incorporating commonsense knowledge."
D17-1085,1,"2) another direction of research is to incorporate sest with deeper neural networks such as vd-cnn (conneau et al., 2017) to improve learning capacity for syntactic embedding."
D17-1085,1,"since there are no structures in the bidaf models to specifically optimize for syntactic information, an attention mechanism that is designed for to utilize syntactic information should be studied."
D17-1085,1,3) tree structured information such as knowledge graphs and ontology structure should be studied and improve question answering tasks using similar techniques to the ones proposed in the paper.
D17-1086,2,"for future work, we plan to examine the effects of other knowledge sources."
D17-1086,2,we plan to explore the idea of external knowledge integration further in future research.
D17-1086,1,"thus, one potential direction for future work would be to incorporate both relational information and lexical definitions."
D17-1088,1,"as future work, we plan to extend our model to be able to generate equation systems and nonlinear equations."
D17-1089,4,"further, we would like to see how laser-qa generalizes to beyond text; one immediate scenario of interest is to explore how pictures and multimedia within qas may be leveraged within laser-qa."
D17-1089,1,future work: a study on the correlation between the knns in the laser-qa embedded space and the original question and answer spaces would provide insights into the extent of correlation between manifolds in the original spaces.
D17-1091,4,"the framework can be used for other natural language processing tasks which are sensitive to the variation of input (e.g., textual entailment or summarization)."
D17-1091,1,"we would also like to explore more advanced paraphrase scoring models (parikh et al., 2016; wang and jiang, 2016) as well as additional paraphrase generators since improvements in the diversity and the quality of paraphrases could also enhance qa performance."
D17-1095,6,we are looking forward to try out richer and more suitable features for multimodal translation (ie.dense captioning features).
D17-1095,1,another interesting approach would be to use visually grounded word embeddings to capture visual notions of semantic relatedness.
D17-1096,1,we hope our work motivates further efforts on understanding and relating onomatopoeia words to “regular” words.
D17-1098,1,"in future work we hope to use more powerful image taggers, and to consider the use of constrained beam search within an expectation maximization (em) algorithm for learning better captioning models from weakly supervised data."
D17-1099,2,"third, future research on action attributes should ideally include videos to better capture attributes that require temporal signals."
D17-1099,6,several possibilities remain open for future work.
D17-1099,1,"in particular, since our experiments show that unsupervised word embeddings significantly help performance, it might be desirable to learn data-driven attributes in an end to-end fashion directly from a large corpus or from dictionary definitions."
D17-1099,1,"first, more attributes could be collected and evaluated, possibly integrating other linguistic theories about verbs, with more accurate modeling of context."
D17-1102,4,"we hope our new dataset can encourage further multilingual, multimodal research."
D17-1103,4,"in future work, we are applying our entailment-corrected rewards to other directed generation tasks such as image captioning and document summarization (using the new multi-domain nli corpus (williams , 2017))."
D17-1105,1,"in future work, we will conduct a more systematic study on the impact that synthetic back translated data brings to multi-modal nmt, and run an error analysis to identify what particular types of errors our models make (and prevent)."
D17-1107,2,another untapped source of information is the acoustic signal in the reading aloud trial.
D17-1107,5,in future work we also plan to explore the connection between eye movements and reading comprehension.
D17-1107,1,"one avenue for future research will be to design more sophisticated ways of incorporating linguistic information into the eye-tracking model, especially features that take into account context, rather than operating at the single word level."
D17-1108,4,we plan to build on the notable improvements shown here and expand this study to deal with additional temporal reasoning problems in natural language text.
D17-1109,3,existing evaluation metrics for tasks with a generation component— such as summarization or dialogue—leave much to be desired.
D17-1111,4,"as future work we plan to apply the gnr to other question answering datasets such as ms marco (nguyen , 2016) or newsqa (trischler , 2016a), as well as investigate the applicability and benefits of type swaps to other tasks like named entity recognition, entity linking, machine translation, and summarization."
D17-1111,5,"finally, we believe there a broad range of structured prediction problems (code generation, generative models for images, audio, or videos) where the size of original search space makes current techniques intractable, but if cast as learning-to-search problems with conditional computation, might be within reach."
D17-1112,2,we plan to conduct more detailed parameter tuning in the acoustic domain and to segment the xitsonga dataset supplied with the zerospeech 2015 challenge.
D17-1112,6,in the future we hope to pursue a number of lines of inquiry.
D17-1112,1,and we plan to explore clustering techniques that would allow our system to discover categories in addition to probable segmentation points.
D17-1112,1,"we also intend to introduce additional layers into the autoencoder network so as to allow for joint acquisition of phone-like, morph-like, and/or word-like units in the acoustic signal; this may benefit from the alternate model structure of chung et al.(2017)."
D17-1113,6,an interesting avenue for future work would be to investigate the variance of results amongst individual participants (figure 1).
D17-1113,1,"however, understanding how individual variations in participants can impact modeling decisions would be of great value to the computational semantics community."
D17-1114,2,"we also plan to enlarge ourmms dataset, specifically to collect more videos."
D17-1116,4,"and, naturally, applying the same frame-work to other annotation tasks."
D17-1117,2,"we plan to consider user-specific information(e.g., ratio of comments rejected in the past)(cheng , 2015; waseem and hovy, 2016)"
D17-1117,1,"we  plan  to  consider  user-specific  information (e.g.,   ratio  of  comments  rejected  in  the  past) (cheng  et  al.,  2015;  waseem  and  hovy,  2016) and explore character-level rnns orcnns (zhanget al., 2015), e.g., as a first layer to produce embeddings of unknown words from characters (dossantos  and  zadrozny,  2014;  ling  et  al.,  2015), which  would  then  be  passed  on  to  our  current methods that operate on word embeddings."
D17-1118,5,"it remains for future research to establish whether other approaches to word representation, e.g. (blei , 2003; mikolov ,2013), have inherent biases."
D17-1120,3,"as future work, we plan to extend our evaluation to larger sense-annotated corpora (raganato, 2016) as well as to different sense inventories and different languages."
D17-1120,1,"we also plan to exploit  the  flexibility  of  our  models  by  integrating them into downstream applications,  such as machine translation and information extraction."
D17-1122,3,"in the future work, wewill improve the inter-weighted layer with moresophisticated module and evaluate our model onother large scale datasets."
D17-1124,4,"in future work, we would like to investigate more complicated idiom-enriched nlp tasks, suchas machine translation."
D17-1126,1,future work could include expanding into many different languages present in social media and developing language-independent automatic paraphrase identification models.
D17-1132,1,"future  works  should  also consider incorporating word-sense frequencies or developing  word-sense  agnostic  features,  with  a particular focus on asymmetric features."
D17-1133,2,"in the future, we would like to improve parsing accura-cy by leveraging unlabeled text rather than relying exclusively on human annotated training data."
D17-1140,1,"considering other criteria, and also more sophisticated search strategies  to  explore  the  huge  space  of  possible patterns, is left for future work."
D17-1141,5,"also, we will further explore whether thereare important types of evidence in editorials andsimilar texts that we have not considered in thispaper so far, such as analogies."
D17-1141,1,"in future work, we plan to investigate argumentation strategies across different genres and provenances."
D17-1142,5,"for  future  work,  an  immediate  next  step  is to  explore  the  usage  of  automatically  extracted arguments  in  helpful  reviews  identification:   in this work,  all argument-based features are based on manually annotated arguments;  deep-learning based argument mining (li et al., 2017; eger et al.,2017)  has  produced  some  promising  results  recently, and we plan to investigate whether the automatically extracted arguments can be used to identify helpful reviews, and how the errors made in  the  argument  extraction  stage  will  influence the performance of helpful reviews identification."
D17-1142,4,We also plan to investigate the effectiveness ofargument-based features in other domains.
D17-1143,1,"lastly,  future  work  can  integrate  subtasks  1  and 4  into  the  model."
D17-1144,3,"in future work, we plan to test our model oncorpora such as the language of opposition from aifdb4(by converting the finer-grained relationtypes used in this corpus to argumentative relations of the kind we considered), on datasets proposed for different tasks."
D17-1144,5,it would be interesting to see whether any corpora for identifying discourse relations could be useful for furthering our experimentation.
D17-1144,1,"finally, we plan to incorporate an attention-based mechanism as well as additional features (e.g. extracted through sentiment analysis) to determine which parts of the texts are most relevant in identifying attack and support."
D17-1145,1,"promising avenues for future work are investigating  consensus  networks  (mangu  et  al.,  2000)for potential gains in terms of speed or quality as compared to lattice inputs, explicitly dealing with rare or unknown words in the lattice, and facilitating gpu training via auto batching (neubig et al.,2017b)."
D17-1146,1,"future  work  will  investigate  better  model-memory integration, e.g., by joint training."
D17-1148,1,"in the future, we plan to focus on integration of phrasal components into nmt training, including better coverage constraints, as well as methods for context-dependent translation override within our hybrid search algorithm."
D17-1152,1,our framework is also easily extendable to incorporate more bilingual and auxiliary signals of translation equivalence.
D17-1153,5,"anobvious question is whether we could extend ourframework to model individual annotator preferences (passonneau and carpenter, 2014) or learnpersonalized models (mirkin , 2015; rabi-novich , 2017), and handle heteroscedasticnoise (park, 1966; kersting , 2007; antos, 2010)."
D17-1153,1,"another direction is to apply active learning techniques to reduce the sample complexity required to improve the systems or to extend to richer action spaces for problems like simultaneous translation, which requires prediction (gris-som ii et al., 2014) and reordering (he et al., 2015)among other strategies to both minimize delay and effectively translate a sentence (he et al., 2016)."
D17-1154,1,"in  the  future,  we  plan  to  integrate weight quantization into our method."
D17-1155,1,"in  the  future,  we  will  try to study a specific sentence weighting method for nmt domain adaptation."
D17-1156,4,"our techniques are not specific to neural machine translation, and we propose that they could be also tried for other neural network architectures and other tasks."
D17-1157,1,"future work should consider integration of the method with more recent models, in particular neural translation models."
D17-1158,6,"while initial results showonly moderate improvements over the baseline andfall short against using synthetic parallel data, we believe there is value in pursuing this line of research further to simplify training procedures."
D17-1159,4,"more generally, given simplicity of gcns andtheir applicability to general graph structures (notnecessarily trees), we believe that there are many nlp tasks where gcns can be used to incorporate linguistic structures (e.g., syntactic and semantic representations of sentences and discourse parsesor co-reference graphs for documents)."
D17-1160,1,analyzing the errors made by our parser suggests that improving entity linking and using the table structure are two directions for future work.
D17-1161,2,"for example, this work ignores modifiers such as always and usually, which often carry valuable information that could be incorporated via model expectation constraints."
D17-1161,2,future work can also incorporate other modes of supervision from language.
D17-1161,1,this improvement seems to indicate that a system which would compute named entities in parallel with normalization would be useful.
D17-1161,1,"as hidden markov models have been used both for name finding (bikel et al.(1997)) and tokenization (cutting et al.(1992)), this seems to be a promising research possibility."
D17-1163,4,"finally, it may also be possible to adapt our model to extract other types of social behavior events."
D17-1163,4,"furthermore, our dataset could be used to contribute to communication studies, by exploring research questions about the dynamics of media attention (for example, the effect of race and geography on coverage of police killings), and discussions of historical killings in news—for example, many articles in 2016 discussed michael brown’s 2014 death in ferguson, missouri."
D17-1163,1,"our model allows for the use of mention level semantic parsing models; systems with explicit trigger/agent/patient representations, more like traditional event extraction systems, may be useful, as would more sophisticated neural network models, or attention models as an alternative to disjunction aggregation (lin et al., 2016)."
D17-1163,1,"one goal is to use our model as part of a semi-automatic system, where people manually review a ranked list of entity suggestions."
D17-1164,1,"from a technical standpoint, future work could also augment the representation of questions and answers presently used in our framework, beyond our heuristic of using root arcs without noun phrases."
D17-1165,1,including a ‘speaker’ level to know which parliamentarians discuss which topics is another interesting path to follow.
D17-1165,1,"also, adding a ‘bill’ level could be beneficial as speeches about the same bill should share the same high-level topic."
D17-1165,1,"in future work, we plan to improve the model through complex priors or semantic similarity strategies."
D17-1167,1,"in the future, we will explore effective ways to model discourse relations among clauses and develop a qa system which can directly output the cause of emotions as answers."
D17-1168,1,"while this is the best-performing model till date on this task, our analysis indicates a need for even deeper analysis of human behavior and societal norms to further improve our understanding."
D17-1169,4,we release our pretrained deepmoji model with the hope that other researchers will find good use of them for various emotion-related nlp tasks4.
D17-1172,6,this work could benefit from several extensions.
D17-1172,1,"high-order scores on arcs like grandparent or siblings can be handled in subproblem p2 with the algorithms described in (koo et al., 2010)."
D17-1172,1,bigram scores on spines could be added at the expense of a third subproblem in the dual objective.
D17-1173,1,future work will involve exploring ways of augmenting the parser with a more innovative architecture than the relatively simple one used in current neural graph-based parsers.
D17-1175,4,"in the future, we plan to apply our approach in more languages and other transition-based system, such as arc-eager or arc-hybrid."
D17-1175,1,another direction we are interested in is to train our model with complex training approaches proposed in weiss et al.(2015) and andor et al.(2016).
D17-1176,2,"in the future, we plan to study higher degrees of lexicalization or full lexicalization, as well as even larger training corpora (such as the wikipedia corpus)."
D17-1176,1,we would also like to experiment with other grammar induction approaches with lexicalization and big training data.
D17-1178,6,we look forward to exploring these directions in future work.
D17-1178,1,"we believe that further refinements of our search procedure can continue to push the bar higher, such as the use of a learned heuristic function for forward score estimation, or a more sophisticated approximate decoding scheme making use of specific properties of the model."
D17-1180,4,"these positive results suggest that tag can provide the foundation of nlp systems for tasks requiring deeper analysis than current dependency parsers provide, and we will apply our parser to such tasks in the future."
D17-1180,5,"nonetheless, a large discrepancy remains in parser performance with gold supertags and predicted supertags, indicating that supertagging is still a bottleneck."
D17-1180,1,we will explore ways to leverage our super tagger's high β-pruning accuracy in parsing.
D17-1181,3,"furthermore, we plan to verify our results on other datasets in future work."
D17-1181,1,an interesting future direction is the extension of the linear-chain crf to jointly normalize all predictions for table filling in a single model pass.
D17-1183,3,"we hope to extend kgeval to incorporate varying evaluation cost, and also explore more sophisticated evaluation aggregation."
D17-1184,1,"these findings suggest a rich area of future research, determining new strategies to extend embeddings to cope with sparse and unreliable data."
D17-1184,1,"three promising approaches include revising the closed-world assumption frequently used in training embeddings, combining embeddings and collective probabilistic models that perform well on extracted kgs, and devising an optimization approach for embeddings that exploits confidence from knowledge extraction systems."
D17-1185,1,"as future work, we plan to develop similar models based on explicit specialization tensors for detecting symmetric relations (e.g., synonymy, antonymy)."
D17-1185,1,"we will also seek to exploit the dual tensor model in different downstream tasks, e.g., hypernymy detection for taxonomy induction (faralli et al., 2017) or recognizing textual entailment."
D17-1186,1,"in the future, we will explore the following directions: (1) we will explore the combination of relation paths from both plain texts and kbs for relation extraction."
D17-1186,1,"(2) we may take advantages of probabilistic graphical model or recurrent neural network to encode more complicated correlations between relation paths, e.g. multi-step relation paths, for relation extraction."
D17-1188,1,it would be possible to replace their sentence-level feature extractor with our model.
D17-1188,1,"for instance, lin et al.(2016) extract sentence-level features and then combine features from multiple sentences with a selective attention mechanism."
D17-1188,1,our approach can be easily applied to other types of relation extraction models as well.
D17-1189,3,"in the future, we plan to develop a new measurement for the reliability of certain distantly supervised label by evaluating the corresponding similarity between certain instance and the potential correctly labeled instances instead of using heuristically set confidence vector."
D17-1189,1,"in addition, we tend to find a more suitable sentence encoder rather than piece-wise cnn for our soft-label method."
D17-1190,1,"in the future, we will extend our sequential models to predict temporal relations for event pairs spanning across multiple sentences, for instance by incorporating discourse relations between sentences in a sequence."
D17-1192,1,"in the future work, we plan to develop more sophisticated noise models for features and labels separately, and try to explore logical information, particularly in this context of nonparametric noise modeling, for further benefiting this task."
D17-1193,1,unsupervised relation classification is a very challenging task for several reasons.
D17-1193,1,"moreover, semantic relations differ with respect to the semantic constraints they put on their arguments."
D17-1193,1,"on the other hand, contextual relation instances tend to have relation-specific indicators when they co-occur, but their individual vectors will not reveal this information (unless they co-occur very often)."
D17-1193,1,"any future development towards an automated unsupervised classification needs to take these aspects into account and work towards a hybrid solution by separating relations with semantically constrained arguments from free ones, as well as adapting the clustering method to handle outliers."
D17-1193,1,"some relation instances are lexical by nature and, therefore, can be expected to show up in the same cluster based on distributional cues."
D17-1194,3,"we observed this performance drop in our experiments, and it would be interesting to know more about the regularities governing this deterioration."
D17-1194,1,"also, for the particular task of analyzing armed conflicts, we plan to research ways of improving accuracy in predicting completely new armed groups not present in the training data, and the methods of filtering out locations not involved in armed conflicts."
D17-1194,1,"in terms of future work, we plan to trace how quickly incremental updates to the model ‘dilute’ the projections, rendering them useless with time."
D17-1196,1,we are still working on these improvements and we will hope to get a complete model soon.
D17-1196,1,"with this contribution we would like to raise also some interest in the community to analyze and develop more effective techniques, both on the modelling and minimization/learning sides, to allow to build real world application based on this framework."
D17-1196,1,qmt and its probability calculus seem to be promising methodologies to enhance the performances of our systems in nlp and certainly deserve further investigations.
D17-1197,3,"in task oriented dialogues, we can also try human evaluation to see if the model can reply users’ query accurately."
D17-1197,1,it is also interesting to use reinforcement learning to learn the actions in each step in coref based lm.
D17-1199,1,"morphological segmentation is a noteworthy alternative to syllabification: a simple morpheme-aware model which sums morpheme embeddings looks promising, and its study is deferred to future work."
D17-1200,5,in future work we plan to explore joint learning of more and different views.
D17-1200 ,3,we invite the community to join us in exploring the full space of opportunities and evaluating induced representations holistically in the future.
D17-1200 ,4,"another interesting avenue for future work would be to apply the framework to questions arising in the digital humanities, e.g., to extract different views from news articles."
D17-1200 ,1,"in the future we plan to define more targeted input, e.g., by using semantic and syntactic information from dependency parses."
D17-1201,4,it still requires further exploration to apply our method to fields beyond nlp.
D17-1203,1,"we plan to merge tree prior construction and the topic modeling into a unified framework (teh et al., 2007; gor¨ ur and teh ¨ , 2009; hu et al., 2013)."
D17-1207,3,future work should also evaluate the earth movers distance between more languages to assess its quality as language distance.
D17-1208,1,"unfolding and shrinking diverse networks could be possible, for example by applying the technique only to the input and output layers or by some other scheme of finding associations between units in different models, but we leave this investigation to future work as models in nmt ensembles in current research usually have the same topology (bojar et al., 2016; sennrich et al., 2016a; chung et al., 2016; neubig, 2016; wu et al., 2016; durrani et al., 2017)."
D17-1209,1,"since gcns are capable of encoding any kind of graph-based structure, in future work we would like to go beyond syntax, by using semantic annotations such as srl and amr, and co-reference chains."
D17-1210,4,"the proposed trainable greedy decoding is a generic idea that can be applied to any recurrent language modeling, and we anticipate future research both on the fundamentals of the trainable decoding as well as on the applications to more diverse tasks such as image caption generating and dialogue modeling."
D17-1211,2,we plan to go beyond the binary classification and explore satire degree estimation.
D17-1211,1,"we will investigate efforts to model satire at the paragraph level following our conclusion and theoretical backgrounds, such as (ermida, 2012)."
D17-1211,1,"we will generalize our approach to reveal characteristics of figurative language (joshi et al., 2016), where different paragraphs or sentences may reflect different degrees of sarcasm, irony, and humor."
D17-1212,1,as future work we foresee a larger ground-truth and more robust approaches which take into account factors such as a reference being irrelevant to a citing paragraph and cases where the evidence for a paragraph is implied rather than explicitly stated in the reference.
D17-1213,2,"we are now deploying the same edit intention taxonomy for italian wikipedia, and plan to apply it to other low resourced languages in wikipedia."
D17-1213,1,"finally, beyond the context of wikipedia, similar taxonomies can be designed for analyzing the collaboration and interaction happened in other online contexts such as academic writing (e.g., google docs or sharelatex, etc)."
D17-1214,1,"however, these methods require careful configuration and tuning to succeed, and making them more robust presents an excellent opportunity for future work."
D17-1215,1,we hope that our work will motivate the development of more sophisticated models that understand language at a deeper level.
D17-1216,1,"currently, there are little labeled training instances for commonsense machine comprehension, for future work we want to address this issue by developing semi-supervised or unsupervised approaches."
D17-1218,1,"to further overcome the problem of domain dependence, multitask learning is a framework that could be explored (sgaard and goldberg, 2016) for different conceptualizations of claims."
D17-1219,5,it would also be interesting to see if the generated questions can be used to help improve question answering systems.
D17-1219,6,"in future work, we would like to investigate approaches to identify question-worth concepts rather than question-worthy sentences."
D17-1220,1,"as future work we suggest injection of structured and unstructured external knowledge (ahn et al., 2016) and explicit modeling of references (yang et al., 2016)."
D17-1224,5,"in future work, we will investigate supervised learning methods for passage ranking and selection, and try to paraphrase the selected passages."
D17-1226,1,"in the future, we plan to continue to investigate the distinct characteristics of wd and cd coreferent event mentions in order to further improve event coreference performance."
D17-1226,1,"especially, we are interested in including additional discourse-level features for improving wd coreference merge performance, such as, features indicating the distance between two event mentions in a document."
D17-1229,4,"in the future, we plan to combine our strategies with more complex neural architectures, and explore their application to other sequence-labeling problems."
D17-1230,6,exploring this relationship further is a focus of our future work.
D17-1231,4,"in our future work, we plan to apply these approaches to other tasks, such as intent recognition and slot filling in language understanding."
D17-1232,1,these findings in addition to the fact that many function words are not filtered out as background may help inform future model design.
D17-1234,4,we expect to deploy our proposed framework in real-world scenarios collaborating with real human teachers to verify the results presented in this paper and discover more potential challenges of on-line dialogue system development.
D17-1234,6,so future work is needed to take more qualities into consideration to achieve better user experience.
D17-1234,6,There are several directions for our future work.
D17-1235,4,"while the focus of this work has been on conversation modeling, we expect some of these results to carry over to other sequence-to-sequence settings, such as machine translation or image-captioning."
D17-1237,5,but more complex tasks might require multiple levels of hierarchy.
D17-1237,5,this challenge calls for future work on automatic learning of hierarchies for complex dialogue tasks.
D17-1237,1,"thus, it is valuable to extend our approach to handle such deep hierarchies, where a subtask can invoke another subtask and so on, taking full advantage of the options framework."
D17-1237,6,the promising results suggest several directions for future research.
D17-1239,1,"future work on this task might include approaches that process or attend to the source records in a more sophisticated way, generation models that attempt to incorporate semantic or reference-related constraints, and approaches to conditioning on facts or records that are not as explicit in the box- and line-scores."
D17-1240,2,"finally, we would also like to incorporate our findings into other standard tasks of multilingual ir and multilingual speech synthesis (for example to render the appropriate native accent to the borrowed word)."
D17-1240,5,future directions: it would be interesting to understand and develop theoretical justification for the metrics.
D17-1240,1,"further, it would be useful to study and classify various other linguistic phenomena closely related to core borrowing, such as: (i) loanword, where a form of a foreign word and its meaning or one component of its meaning gets borrowed, (ii) calques, where a foreign word or idiom is translated into existing words of native language, and (iii) semantic loan, where the word in the native language already exists but an additional meaning is borrowed from another language and added to existing meaning of the word."
D17-1241,3,"we expect that with more training data, the performance of spe-based methods can be further improved."
D17-1242,5,"we regard this as a first step toward demographic-aware nlp, and in future work we plan to address other more advanced nlp tasks while accounting for demographics."
D17-1243,3,"recently, a dataset with dialogue act annotations on reddit discussions is published and can be used for a dialogue act prediction task (zhang , 2017)."
D17-1243,4,a potential future direction is to explore whether the comment embeddings derived from the unsupervised factored neural model can be useful across multiple tasks.
D17-1243,5,"identifying or ranking persuasive arguments in the changemyview subreddit (as studied in (tan , 2016; wei , 2016)) and asking for favors in the randomactsofpizza subreddit (used in (althoff , 2014)) are also interesting for future work."
D17-1243,5,"identifying or ranking persuasive arguments in the changemyview subreddit (as studied in (tan et al., 2016; wei et al., 2016)) and asking for favors in the randomactsofpizza subreddit (used in (althoff et al., 2014)) are also interesting for future work."
D17-1244,5,"the latter would allow us to, for example, analyze how the relationship between two individuals changes over time, and determine which events make a relationship go from superficial to intense and vice versa."
D17-1244,1,"our future plans include studying values of dimensions for selected relationships (e.g., coworker), and investigating changes on the dimensions of the relationship over time."
D17-1245,2,"among them, we are currently extending the dataset of annotated tweets both in terms of annotated tweets per topic, and in terms of addressed topics (e.g."
D17-1245,2,"brexit after the referendum, trump), in order to have more instances of facts and sources."
D17-1245,3,"on such extended dataset, we plan to run experiments using the three modules of the system as a pipeline."
D17-1245,4,"moreover, we plan to extend our pipeline by considering also the links provided in the tweets to verify their sources, i.e., if a tweet claims to report information from the cnn but the link actually redirects towards an advertisement website the source is not indubitably the cnn."
D17-1245,5,"being it a work in progress, several open issues have to be considered as future research."
D17-1246,5,we are also interested in finding the meanings of the detected non-standard usages.
D17-1246,1,we are interested in expanding our method for detecting words that have non-standard usages.
D17-1248,2,"in addition to methodology, future work will also need to take into account ethical implications of this personalization."
D17-1248,6,"as humans, we automatically perform this adaptation through multiple additional channels: speech tone, frequency, facial expression; which the agent can not alter."
D17-1248,1,"in future work, we will experiment with automatically altering or generating text while keeping topic constant, as our current results are in part topically driven."
D17-1249,2,"preliminary results on the 2016 us campaign tweets show that the topics used by both candidates were different from their speech: the tweets, being shorter than speeches, emphasize more oversimplified criticism of the opponent rather than justified political ideas."
D17-1250,3,"consumers of comments, typically, desire information that ultimately leads to real utility benefits, and this domain is not the only one where objective quality can be obtained: for example, one could: (1) ask consumers of restaurant reviews to indicate if one convinces them to go to the restaurant and then follow up on their experience, (2) consider evaluating research paper quality – reviewer ratings versus citation count (influence), (3) determine whether the answer to the question about how to drive to conserve fuel lead to the reader actually using less gas?"
D17-1251,5,the future work includes the reduction or estimation of the computation time and memory usage.
D17-1251,5,"the proposed method should be used with effective methods of identifying auto-generated contents, bots, and spammers in microblogs."
D17-1251,1,"they increase as the target document set grows or, specifically the number of occurrences of bursty n-grams increases."
D17-1252,4,improving mmrn for dealing with large search space is an important future direction as well.
D17-1252,1,"in the future, we plan to apply maximum margin reward networks on other structured learning tasks."
D17-1253,1,"while the noise from mining errors might qualify some of our findings, we also expect that larger corpora will allow us to discover more reliable and discriminative patterns."
D17-1254,3,"in future work, we aim to use more advanced cnn architectures (conneau , 2016) for learning generic sentence embeddings."
D17-1255,1,there are several avenues for future work including the extent to which our rbf model and its kernels could be combined with curriculum learning or leitner system to either predict easiness of novel training instances to inform curriculum learning or incorporate leitner’s queueing mechanism to the rbf model.
D17-1255,1,"other directions include extending rbf to dynamically learn the recall confidence parameter with respect to network behavior, or developing more flexible delay functions with theoretical analysis on their lower and upper bounds."
D17-1257,1,we hope researchers will take advantage of the code for further improvements and applications to other tasks.
D17-1259,4,"there remains much potential for future work, particularly in exploring other reasoning strategies, and in improving the diversity of utterances without diverging from human language."
D17-1259,5,"we will also explore other negotiation tasks, to investigate whether models can learn to share negotiation strategies across domains."
D17-1261,1,future work can leverage optimal inputs to create a language model that can become an automated debate agent.
D17-1261,1,"however, since the input is partially based on the knowledge of talking points, there is a potential for an information retrieval-based task to provide the talking points for the debate agent (if one desires a fully-automated system than can work without the presence of introductory remarks, from which talking points are currently extracted)."
D17-1261,1,"finally, future work can also examine the trained model itself in further detail, seeking to understand the debate strategy."
D17-1263,2,but there are also several ways to improve and expand upon our challenge set approach itself.
D17-1263,3,"first, though our human judgments of output sentences allowed us to precisely assess the phenomena of interest, this approach is not scalable to large sets, and requires access to native speakers in order to replicate the evaluation."
D17-1263,3,the existence of human judgments for this set provides a goldstandard by which proposed automatic judgments may be meta-evaluated.
D17-1263,5,a method to automatically create such a challenge set for a new language pair would be extremely useful.
D17-1263,5,"it is our hope that the insights derived from our challenge set evaluation will help inspire future mt research, and call attention to the fact that even “easy” language pairs like english–french still have many linguistic issues left to be resolved."
D17-1263,5,it would be interesting to see whether similar scores could be achieved through automatic means.
D17-1263,5,localizing a divergence within a difficult sentence pair would be another useful subtask.
D17-1263,1,"finally, we would like to explore how to train an mt system to improve its performance on these divergence phenomena."
D17-1263,1,"this could take the form of designing a curriculum to demonstrate a particular divergence to the machine, or altering the network structure to capture such generalizations."
D17-1263,1,"second, the construction of such a challenge set requires in-depth knowledge of the structural divergences between the two languages of interest."
D17-1263,1,"one could imagine approaches that search for divergences, indicated by atypical output configurations, or perhaps by a system’s inability to reproduce a reference from its own training data."
D17-1264,4,"first, due to advances in methods for extracting general purpose knowledge (mitchell , 2015; nakashole , 2013; wijaya , 2014), the use of semantic knowledge has been explored for several natural language tasks (nakashole and mitchell, 2015; yang and mitchell, 2017)."
D17-1264,4,"second, although we focus on bilingual dictionary induction, our knowledge distillation training objective that enables seamless use of paths of rich resource languages as teachers of low resource languages is general and can be applied to problems such as multilingual tagging and parsing."
D17-1264,1,"however, for bilingual dictionary induction, and more generally, machine translation, the role of semantic knowledge has not been fully explored."
D17-1265,2,possible improvements include improving the choice of linguistic information used and using this work to explore how tqs’ functions are portrayed in languages other than english.
D17-1265,3,"interesting future work would be to compare the opposite approach to the task; augmenting source sentences with disambiguating information prior to translation, particularly within an nmt framework, which has good potential for handling nonlocal context and integrating extra features."
D17-1265,5,"as a stylistic aspect, its prediction and evaluation are complex and should be further explored."
D17-1268,5,"we hope that this study will encourage additional use of typological features in downstream nlp tasks, and inspire further techniques for missing knowledge prediction in under-documented languages."
D17-1271,2,"one idea for future work is thus to leverage additional projected annotations from similar phenomena in additional languages, possibly improving overall performance by combining complementary information."
D17-1271,5,"hence, telicity recognition is also relevant for machine translation research and could be a useful component in computer aided language learning systems, helping learners to select appropriate aspectual forms."
D17-1271,1,"when translating, telic and atelic constructions also require different lexical choices and appropriate selection of aspectual markers."
D17-1271,1,clustering more than two languages may also enable us to induce clusters corresponding to the different usages of imperfective verbs in czech.
D17-1271,1,we plan to leverage similar ideas as presented in this paper to create temporal discourse parsing models for such languages.
D17-1272,1,"since our objectives are agnostic of the choice of the underlying model πw, it is also possible to transfer our techniques to non-linear models such as neural machine translation."
D17-1273,1,future work includes: i) improving our work for short text knowledge extraction and ii) designing a general framework for cross-lingual ugc relation extraction.
D17-1274,1,"in the future, we will combine additional rules, patterns, and constraints with dnn techniques to further improve slot filling."
D17-1276,4,"future work includes further investigations on how to apply the multigraph approach to other structured prediction tasks, as well as applications of the proposed model in other related nlp tasks that involve the prediction of overlapping structures, such as equation parsing (roy , 2016)."
D17-1277,1,"in the future, we would like to extend this system to perform nil detection, coreference resolution and mention detection."
D17-1279,2,"therefore, adding more in-domain unlabeled data may help when combined with selection schemes such as the ulm algorithms proposed here."
D17-1279,6,"other future work includes leveraging global context, information of citation network."
D17-1279,1,it would be useful to assess the impact of matched unlabeled data for the physics and material science domain.
D17-1281,4,we hope that this first application of asynchronous deep rl algorithms will open up more adoption of such techniques in the nlp community.
D17-1282,5,"while avoiding them is one of the strengths of our model, generating more consistent parses to reduce this kind of named entities would be one possible direction for future research."
D17-1283,4,"in the future we hope to extend this work to nlp tasks with richer structured output, such as parsing."
D17-1284,1,"further research will include encoding more structured knowledge about the entities, such as their relations to other entities, to make their representations semantically richer."
D17-1284,1,"we will investigate how we can use unstructured resources, such as the corpus of unlabeled webpages used by plato, and noisy supervision from the wikilinks corpus (singh et al., 2012) in order to further improve the model."
D17-1284,1,"we will also evaluate our approach on substantially varied domains, such as discussion forums, and social media posts."
D17-1287,1,"however, when used alone, the character features still yield reasonable performance, which suggests that more meaningful character-based features could lead to story detectors with even better performance."
D17-1287,1,In future work we plan to develop richer character-based features.
D17-1289,5,"while promising, our model still cannot explicitly handle multidefendant cases, and there is also a clear gap between our model and the upper bound improvement that relevant articles can achieve."
D17-1289,6,we will leave these challenges for future work.
D17-1291,5,"we hope this work provides insights for further studies on outlier document texts in specific domains, and in more challenging settings such as detecting outliers from crowdsourced data."
D17-1292,3,"in our following work, we collect a sequence of causal chains verified by domain experts for more solid evaluation of generating explanations."
D17-1293,1,"for instance, we can adopt the proposed algorithm to streaming documents, e.g. webpage click streams, since our method can model the document-document sequences."
D17-1294,5,planned future work for this project will include the creation of a browser plug-in to present opt-out hyperlinks to internet users.
D17-1301,1,"our models benefit from larger contexts, and would be possibly further enhanced by other document level information, such as discourse relations."
D17-1301,1,we propose to study such models for full length documents with more linguistic features in future work.
D17-1304,1,"in the future, we will try to exploit a general framework for utilizing richer syntax knowledge."
D17-1305,5,another issue is the use of a commercial image search api as a black box to retrieve images.
D17-1305,1,"our future work is to assess the applicability of our approach into other rte problems such as the rte challenges, snli (bowman et al., 2015) and multinli (williams et al., 2017) datasets and further investigate what syntactic or semantic units can be best represented using visual denotations."
D17-1306,5,"in this way sequence bias effects can be considered as independent noise sources, rather than a systematic bias, and consequently the aggregate results over several workers will remain unbiased."
D17-1306,5,"another important question for future work is whether sequence bias is detectable in expert annotators, not just crowd workers."
D17-1307,5,"by revealing that 95% of real use cases fit into this paradigm, we hope to convince the reader that this is a valuable problem that requires more attention."
D17-1307,5,"while firstorder qa might seem like a solved problem, there is clearly still room for improvement."
D17-1307,1,"while an ablation study revealed the importance of both entity detection and relation prediction, we are hoping to further study the degree of which improvements in either component affect qa accuracy."
D17-1307,1,"even though deep learning has opened the potential for more generic solutions, we believe that taking advantage of problem structure yields a more accurate and efficient solution."
D17-1310,1,a novel lstm unit with extended memories is developed for memory interactions.
D17-1311,5,"one major question left open by this analysis is what happens when multiple transformations are applied hierarchically, and future work might focus on extending the techniques in this paper to explore recursive structure."
D17-1312,4,future work includes performing further investigations to better understand and to visualize what types of information has been transferred across domains and how such information influence different types of down-stream nlp tasks.
D17-1312,1,it is also important to understand how such an approach will work on other types of models such as neural networks based nlp models.
D17-1313,2,"finally, we will incorporate additional biological constraints (e.g., focus on pathways that exist in specific species) into the search itself."
D17-1313,1,there are many exciting directions in which to take this work.
D17-1313,1,"second, we can expand focused reading to efficiently search for multiple paths between s and d."
D17-1313,1,"first, more of the focused reading algorithm can be subject to rl, with the chooseendpoints policy being the clear next candidate."
D17-1314,1,"this will enable the system to learn by self to achieve continual or lifelong learning (chen and liu, 2016)."
D17-1314,1,"we also plan to improve model performance during testing (shu et al., 2017)."
D17-1314,1,"in our future work, we plan to improve the cumulative or incremental learning method in (fei et al., 2016) to learn new classes without training on all past and new classes of data from scratch."
D17-1315,2,an obvious extension to our work is to try similar models on multiple languages.
D17-1316,2,"as our data comes from a vulnerable population, obtaining a larger data set is challenging, but essential for future work."
D17-1316,2,"in fact, two of the authors are in the process of collecting data from a total of 120 chr individuals."
D17-1316,1,"this would enable a more thorough investigation of a larger and more sophisticated suite of linguistic features, and especially a more fine grained analysis of the interaction of metaphor and emotional language in schizophrenia."
D17-1318,2,"on the computational side, we will to extend our approach to crosslingual data, in order to enable computer-assisted political analysis across different languages."
D17-1318,1,"in the future, we hope that the pipeline presented in this paper will support political science researchers in studying topics such as party polarization through the analysis and comparison of electoral manifestos, parliamentary proceedings and campaign speeches."
D17-1319,2,"zipporah currently selects sentences based on the individual quality only, and we plan in future work to also consider other factors, e.g.encourage selection of a subset that has a better n-gram coverage."
D17-1321,5,"our goal is simply to improve understanding and interpretability of invented languages in multiagent dialog, place recent work in context, and inform fruitful directions for future work."
D17-1323,4,"future work also includes applying bias reducing methods in other structured domains, such as pronoun reference resolution (mitkov, 2014)."
D17-1323,1,"more extensive analysis could explore the interaction among predictor, bias measurement, and bias de amplification method."
D17-1323,1,our work is the first to demonstrate structured prediction models amplify bias and the first to propose methods for reducing this effect but significant avenues for future work remain.
D18-1005,4,"however, we hope to test generalizability in future work by applying our methods to other gang-related corpora, because there is variation in language, local concepts, and behavior across gangs."
D18-1005,1,"in the future, we are also interested in further experimenting with the context features introduced in this work; for instance, by extending our pairwise interaction features to take into account direction between users."
D18-1005,1,"finally, we intend to explore other types of context, such as reference to specific events that may trigger the emotions of either aggression or loss."
D18-1008,1,future work might further extend the recovery of analogy as part of information extraction.
D18-1011,1,we believe that one of the promising future directions is to learn from how humans learn and store semantic word representations to build a more effective computational model.
D18-1012,5,"we also plan to explore advanced video features such as activity recognition, person identification, etc."
D18-1012,5,"in future work, we plan to investigate the effects of multiple users, i.e., the multi-party aspect of this dataset."
D18-1014,1,"in future work, we are interested in merging our model with memory-based fusion methods since they have complementary strengths as discussed in subsection 5.1."
D18-1019,4,"we anticipate this model could be leveraged in other similar sequence modeling tasks that involve predicting overlapping structures such as recognizing overlapping and discontinuous entities (muis and lu, 2016) which frequently exist in the biomedical domain."
D18-1021,2,"in the future, we will enrich semantics of lowresourced languages by cross-lingual linking to rich-resourced languages, and extend more crosslingual words and entities to multi-lingual settings."
D18-1022,2,"as our airline-product results indicate, increasing the domain gap harms our results, and we expect the same with more diverse language pairs."
D18-1022,2,"in future work we wish to improve our results for large domain gaps and for more dissimilar languages, particularly in the important lazy setup."
D18-1023,2,"in the future, we will further extend our approach to multi-lingual multimedia common semantic space construction."
D18-1024,4,"for future work, we plan to investigate how our method can be extended to work with other bwe frameworks, in order to overcome the instability issue of lample (2018b)."
D18-1025,2,"a newly collected dataset for evaluating bilingual contextual word similarity is presented, which provides potential research directions for future work."
D18-1026,2,"we also plan to extend the method to asymmetric relations (e.g., hypernymy) and to more target (resource-lean) languages."
D18-1026,1,"in future work, we will explore more sophisticated adversarial models such as cycle-gan (zhu et al., 2017)."
D18-1026,1,"moreover, we will experiment with bootstrapping approaches to extract new lexical constraints from post-specialized embeddings."
D18-1029,2,we hope that nlp/lm practitioners will find the datasets for 50 languages put forth in this work along with benchmarked lms useful for future developments in (language-agnostic as well as typologically-informed) multilingual language modeling.
D18-1029,1,this study calls for next-generation solutions that will additionally leverage typological knowledge for improved language modeling.
D18-1032,5,"furthermore, how to iteratively discover new entity alignments in the framework of our approach is another interesting direction that we will study in the future."
D18-1032,1,"in the future work, we will explore more advanced gcn models for kg alignment task, such as relational gcns (schlichtkrull et al., 2017) and graph attention networks (gats) (velickovic et al., 2017)."
D18-1033,4,"(2) in fact, our framework for cross-lingual lexical sememe prediction can be transferred to other cross-lingual tasks."
D18-1033,1,"in the future, we will explore the following research directions: (1) in this paper, for simplification, we ignore the rich hierarchy information in hownet and also ignore the fact that a word may have multiple senses."
D18-1033,1,we will explore the effectiveness of our model in these tasks such as cross-lingual information retrieval.
D18-1033,1,we will extend our models to consider the structure information of sememe and multiple senses of words;
D18-1035,1,"as our model is agnostic to both the model architecture and the target metric, we see the exploration of more diverse and ambitious model target metric pairs as a clear avenue for future work."
D18-1037,4,"we plan to explore the applications of sync in nmt with more structured attention mechanisms, and potentially a hybrid phrase-based nmt systems with sync, in which the model can benefit from sync to be more extensible when handling larger lexicons."
D18-1038,4,we believe our method can be further extended into a general purpose multi-lingual transfer framework to resolve other nlp matching or classification problems.
D18-1039,1,"in the future, we want to extend the concept of the contextual parameter generator to more general settings, such as translating between different modalities of data (e.g., image captioning)."
D18-1039,1,"furthermore, based on the discussion of section 3.3, we hope to develop an adaptable, never-ending learning (mitchell et al., 2018) nmt system."
D18-1041,4,"besides, our method is also general to other nmt models."
D18-1041,4,"therefore, we plan to apply our method to the nmt with complex architectures, for example, lattice-to-sequence nmt (su , 2017), hierarchy-to-sequence nmt (su , 2018), nmt with context-aware encoder (zhang , 2017) and transformer (vaswani , 2017) and so on."
D18-1041,4,"therefore, we plan to apply our method to the nmt with complex architectures, for example, lattice-to-sequence nmt (su et al., 2017), hierarchy-to-sequence nmt (su et al., 2018), nmt with context-aware encoder (zhang et al., 2017) and transformer (vaswani et al., 2017) and so on."
D18-1041,1,"in the future, we would like to extend the proposed word-level cost weighting strategy to source words."
D18-1044,5,"second, explore new method for knowledge distillation."
D18-1044,1,"first, use object function beyond maximum likelihood to improve the modeling of long-distance dependencies."
D18-1044,1,"in the future, we plan to investigate better methods for training the sat to further shrink the performance gap between the sat and the transformer."
D18-1044,1,"we also plan to extend the sat to allow the use of different group sizes k at different positions, instead of using a fixed value."
D18-1045,1,"in future work, we would like to investigate an end to-end approach where the back-translation model is optimized to output synthetic sources that are most helpful to the final forward model."
D18-1046,1,"performing model combination, either by developing hybrid transliteration models (nicolai et al., 2015) or by ensembling (finch et al., 2016), can further improve low resource transliteration."
D18-1046,1,"jointly leveraging similarities between related languages, such as writing systems or phonetic properties (kunchukuttan et al., 2018), also shows promise for low-resource settings."
D18-1047,1,"a promising line of future work is to explore a best of both worlds approach, combining neighborhood sensitivity with the methods that achieve superior performance on nearby languages."
D18-1048,1,"more specifically, we can extend the policy network to choose the backward decoding except for forward decoding and halting."
D18-1048,1,one promising direction is to incorporate the backward decoding into our architecture.
D18-1049,2,"in the future, we plan to further validate the effectiveness of our approach on more language pairs."
D18-1053,5,"as future works, we plan to further improve paragraph ranker based on the researches on learning to rank."
D18-1054,1,"future work can focus for example on designing an inexpensive preprocess layer, and other strategies for improved performance on answer generation."
D18-1056,5,"understanding when biases induce highly non-convex landscapes, and how to make adversarial training less sensitive to such scenarios, remains an open problem, which we think will be key to the success of unsupervised machine translation and related tasks."
D18-1057,1,"as future works, we suggest further investigating the characteristics of context overlap in diversified ways."
D18-1058,4,"in our future work, we will apply word-pair embeddings from nlra to various downstream tasks related to lexical relational information."
D18-1062,3,"future directions include validate our model on more realistic scenarios (dinu , 2015) as well as combine our algorithms with more sophiscated adversarial networks (arjovsky , 2017; gulrajani , 2017)."
D18-1062,3,"future directions include validate our model on more realistic scenarios (dinu et al., 2015) as well as combine our algorithms with more sophiscated adversarial networks (arjovsky et al., 2017; gulrajani et al., 2017)."
D18-1065,1,"an interesting direction for future work is to extend beam-joint to multi-head attention architectures as in (vaswani et al., 2017; xu chen, 2018)."
D18-1073,3,"also, we plan to look at specialized tasks that naturally evaluate the influence of external knowledge, to help the model to generate diverse responses."
D18-1073,6,"in future work, we aim to add more experiments with dialogue tasks that require understanding a supplementary source of knowledge to solve the task."
D18-1074,1,"in future work, we will investigate other external sources of knowledge, such as acoustic cues and videos to further improve the performance of the model."
D18-1078,5,other types of questions can be formulated over this data in following work.
D18-1080,2,this suggests that future improvements may come from finding other such sources of data as much as from modeling.
D18-1082,5,"in future work, we would like to devise more sophisticated solutions to bridge the semantic gap in rg and explore linguistic patterns in conversations like what has been done in discourse analysis (lei , 2018) ."
D18-1084,4,"in future work, we hope to further address the language issues of paragraph generation as well as extend this simple approach to other tasks requiring long-form text or paragraph generation."
D18-1085,1,"since there is not such dataset at the time of writing this paper, we can continue building on this work by using model summaries, which are abstractive in nature, as a proxy."
D18-1086,1,in future work we aim to improve summarization performance by jointly training the guiding process with the amr-based summarization process.
D18-1087,3,"to broadly substantiate our findings, we propose future work that would follow our assessment methodology over test samples from current datasets (e.g.cnn/dailymail), judging performance of current systems and utilizing current manual evaluation protocols."
D18-1088,1,"in the future, we plan to explore ways to train compression models tailored to our summarization task."
D18-1090,1,one of the future directions can be exploring other kinds of scoring actions than classification under the reinforcement learning framework.
D18-1092,4,"in the future, we are interested in extending this approach to more natural language tasks."
D18-1093,1,"future work will include: (i) incorporating lexical semantics such as named entities and domain specific senses for further improvement, (ii) extending the method to utilize label dependency constraints (bi and kwok, 2011), and (iii) improving the accuracy of the top ranking categories to deal with p@1 and ndcg@1 metrics."
D18-1094,1,"as a future direction, we will advance our model to automatically construct the hierarchical taxonomy in order to improve text classification with a large number of classes."
D18-1097,2,we are also interested in applying our approach to other languages and dialects.
D18-1098,3,as future work we intend to explore ways that combine both the topic coherence and topic intrusion for topic model evaluation.
D18-1099,5,"further, we have planned to generate titles for prose text essentially creating micro-summaries of text, a relatively unexplored area."
D18-1099,5,"in the future, we intend to build upon our methods and enable automatic detection of subheaders."
D18-1108,1,"our methods generality opens up several avenues for future work: since it supports any structure for which map inference is available (e.g., matchings, alignments), and we have no restrictions on the downstream p(y | h, x), we may design latent versions of more complicated state-of-the-art models, such as esim for nli (chen et al., 2017)."
D18-1109,4,"in the future, we would like to investigate the effectiveness of rnfs on a wider range of nlp tasks, such as natural language inference and machine translation."
D18-1113,3,"in future work, we plan to investigate to what extent these methods can be used to support the automatic generation of grammar exercises."
D18-1119,1,"in future work, we would like to encourage the development of more natural word meanings by enforcing the agent representations to stay more faithful to the perceptual input they receive."
D18-1119,1,"moving ahead, it is fundamental to design setups where agents would have stronger reasons to develop human-like communication strategies."
D18-1120,4,"besides, dynamic routing could also be useful to improve other natural language processing tasks such as the sequenceto-sequence task and so on."
D18-1120,5,"in the future, we tend to resolve the situation of how to assign predicted relationship to multi entity pairs when two entities have multi-relations by utilizing prior knowledge such as entity type and joint training with named entity recognition."
D18-1120,1,"we will also try to optimize the model in terms of speed and focus on other problems by leveraging class ties between labels, specially on multilabel learning problems."
D18-1121,4,"lme may also be extended to other tasks that also suffer from noises and incompleteness of ds, such as relation extraction (takamatsu , 2012; ritter , 2013; lin , 2016)."
D18-1121,1,"for example, (1) how to train a language model that is sensitive with incorrect labels; (2) how to combine meaning of labels with the hierarchical structure of types; (3) how to find the optimal  easily for a new dataset."
D18-1123,2,"for future work, we aim to expand our experimental study to a larger scale."
D18-1123,5,we further consider extracting publication strings from academic homepages of the same organization.
D18-1123,1,we also plan to investigate adaptive alternating model training schemes as well as external memory mechanism such as memory networks.
D18-1124,4,"another direction that we plan to work on is to apply this model to recognizing overlapping and entities that involve discontinuous spans (muis and lu, 2016) which frequently exist in the biomedical domain."
D18-1125,4,"in the future, we will extend our framework to cross-lingual and cross-domain information extraction tasks."
D18-1128,3,future work involves closing the loop and evaluating the extent to which improved human performance at fp and kp translates to improved success of human-ai teams at accomplishing a shared goal.
D18-1131,5,"one idea is to try source-pivot-target transfer, similarly to the way this is done for machine translation (wu and wang, 2007)."
D18-1131,5,"another promising direction is to have an attention mechanism (luong , 2015) for question similarity which can be adapted across domains."
D18-1131,1,"in future work, we plan to develop better methods for adversarial adaptation based on these observations."
D18-1134,1,"specifically, we will look at how to better model the interaction between the reader and the relation embedding model and how to improve the relation embedding model itself by adopting ideas from the relation extraction literature (miwa and bansal, 2016; peng et al., 2017; ammar et al., 2017)."
D18-1134,1,"in the future, we would like to invest in building better sequential question answering models that push the accuracy beyond the presented baselines."
D18-1136,4,it is also interesting to see whether such parameterized cnn architecture could benefit other natural language processing tasks involving text pairs like question answering task.
D18-1139,5,"furthermore we observed consistently better performance of cnn architectures in otherwise comparable scenarios, which suggests that cnns cope better with the irregularities of user-written texts on social media, a research question we leave to future work."
D18-1144,1,"in the future, we will develop a model which uses a series of posts as a feature."
D18-1146,4,it would be interesting to quantify the synergy in the learning process; 5. exploring ml applications to other aspects of the service component.
D18-1146,4,"future work could improve on: 1. fine-tuning the open-domain question answering for wine knowledge; 2. connecting our hslda or hierarchical multilabel classification to robotic sensors to fully mimic the blind tasting task; 3. exploring other simpler and more efficient ml models for pairing tasks; 4. training a joint multi-task model, since it is accepted in the industry that a solid knowledge of wine theory helps immensely in blind tasting and wine service."
D18-1147,2,"given our initial success, in the future, it would be valuable to construct a large health-related dataset to cover other types of emotions, e.g., anger or fear."
D18-1148,1,"future work in this area can look at adjusting models to account for other meta-data such as temporal variation and diversity and to adjust for selection biases present in social media, where the user base on social media is not representative of the population of the community (greenwood et al., 2016)."
D18-1149,1,"lastly, further work on sequence-to-sequence model architectures could yield better results in non-autoregressive sequence modeling."
D18-1151,1,"we hope that our data set, and future extensions to other phenomena and languages, will make it possible to measure progress in syntactic language modeling and will lead to better understanding of the syntactic generalizations captured by language models."
D18-1153,4,"In the future, we plan to apply LD-Net to other applications."
D18-1154,1,"another direction of study is large-scale semantic relation discovery, for example, frames and scripts, with a focus on salient discourse units."
D18-1154,6,"In the future, we plan to investigate these complex settings."
D18-1155,1,"as a direction for future work, it would be very interesting to extend the current models, diving further into direct time-line models, and learn to predict absolute time-lines, i.e. making the time-lines directly mappable to calendar dates and times, e.g.by exploiting complementary data sources such as the event times corpus (reimers et al., 2016) and extending the current loss functions accordingly."
D18-1156,1,"in the future, we plan to exploit the information of one argument which plays different roles in various events to do better in event extraction task"
D18-1159,4,"in the future, we will explore the possibility of removing the projective constraint and the tree requirement, extending the applicability of valency patterns to other tasks such as semantic role labeling."
D18-1160,6,"future work might explore more sophisticated invertible projections, or recurrent projections that jointly transform the entire input sequence."
D18-1163,2,"we are also interested in experimenting with richer families of permutation distributions, as well as conservative distributions that tend to prefer the original source order."
D18-1163,1,"we would also like to consider more sensitive divergence measures that go beyond bigrams, for example using recurrent neural network language models (rnnlms) for q and p."
D18-1163,1,"we could use entropy regularization (grand valet and bengio, 2005) to encourage more deterministic patterns of realization in the synthetic languages."
D18-1165,3,"it would also be interesting to compare the effectiveness of the opportunistic active learning framework, as well as the policy learning, across a variety of applications."
D18-1165,4,"it would be interesting to examine how a policy learned using a dataset such as visual genome generalizes to a different domain such as images captured by a robot operating in an indoor environment, possibly with some fine-tuning using a smaller in-domain dataset."
D18-1165,1,"the simulation could also potentially be improved using positive unlabeled learning methods (liu et al., 2002; li and liu, 2003) instead of assuming that an object or attribute not labeled in an image region is not present in the image."
D18-1166,2,"in the future, we also intend to extend the dataset by collecting natural language questions-answer pairs via crowdsourcing."
D18-1166,4,we also hope that recipeqa will serve other purposes for related research problems on cooking recipes as well.
D18-1167,2,"thus, future work also includes integrating better temporal cues."
D18-1167,6,"additionally, temporal reasoning is crucial for answering the tvqa questions."
D18-1167,6,"another direction is to exploit human-object relations in the video and subtitle, as we observe that a large number of questions involve such relations."
D18-1167,6,"to narrow the gap, one possible direction is to enhance the interactions between videos and subtitles to improve multimodal reasoning ability."
D18-1167,1,we hope this novel multimodal dataset and the baselines will encourage the community to develop stronger models in future work.
D18-1168,1,"additionally, in table 5, even when the mllc model can properly localize context, it does not always properly localize temporal sentences indicating that improved temporal reasoning can also improve our results."
D18-1168,1,"we believe our dataset, analysis, and method are an important step towards better moment retrieval models that effectively reason about temporal language."
D18-1169,5,"the significant gap between state of the art and iaa (around 50.0) encourages future research to take this dataset as a challenging, yet reliable, evaluation benchmark."
D18-1171,1,"for example, we want to extend the notion of pom to nouns and adjectives, and investigate other a priori measures for metaphor novelty."
D18-1171,1,"further, we want to jointly detect metaphors and score their novelty."
D18-1171,1,another interesting direction is to investigate the correlation between perceived novelty and the existence of dictionary definitions for metaphoric senses of a token or expression.
D18-1171,1,"in future work, we want to develop more sophisticated methods to detect and distinguish novel metaphors."
D18-1173,1,"in the future work, we will explore more effective memory addressing and updating approaches to boost the few-shot representation learning."
D18-1175,2,"second, there are many intuitively promising sources of information that we have not explored, such as coreference."
D18-1175,1,"and third, our models rely on pairwise similarity-based coherence learning, which leads to the natural question of whether structured prediction would improve performance."
D18-1176,3,"in addition, it would be interesting to explore how the attention weights change during training, and if, e.g., introducing entropy regularization (or even negative entropy) might improve results or interpretability further."
D18-1177,1,how to incorporate visual domain knowledge more explicitly into the model would be an interesting direction for future research.
D18-1177,1,"while we build on skip-gram, the idea of pixie could be extended to other word embedding models, e.g., glove (pennington et al., 2014), elmo (peters et al., 2018), etc."
D18-1178,4,"in future work, we plan to incorporate other nlp tasks defined over these frameworks to learn nouncnoun compound interpretation using tl and mtl."
D18-1178,1,"such tasks include semantic role labeling of nominal predicates in nombank annotations as well as verbal predicates in propbank (kingsbury and palmer, 2002)."
D18-1179,5,one open question is to what extent can the quality of bilm representations be improved by simply scaling up model size or data size?
D18-1179,1,"as our results have show that computationally efficient architectures also learn high quality representations, one natural direction would be exploring the very large model and data regime."
D18-1179,1,an alternate direction for future work combines the purely unsupervised bilm training objective with existing annotated resources in a multitask or semisupervised manner.
D18-1181,2,"firstly, more work is needed to evaluate the representations on other languages and tasks."
D18-1181,5,"secondly, solving downstream tasks requires representations for the inflected words as well."
D18-1181,1,"to address it in future work, we might want to split word representations into a lexical and a morphological part."
D18-1183,1,"in future, we plan to extend this work in several aspects: (1) enrich the simile component structure by adding shared properties or events so that the extracted structures would be more useful for metaphor processing;"
D18-1183,1,(2) improve representation learning for recognition by incorporating external knowledge;
D18-1183,1,(3) apply simile recognition to study the use of figurative language in writings.
D18-1184,1,"first, the success of span comparisons over word-level comparisons suggests that it may be advantageous to include such comparisons in more complex models, either for comparing two sentences directly, or as intermediate parts of models for more complex tasks, such as reading comprehension."
D18-1184,1,we hope to find a more efficient way to accelerate this dynamic programming method on a gpu.
D18-1184,1,"second, our models ability to infer trees from a semantic objective is intriguing, and suggestive of future opportunities in grammar induction research."
D18-1184,1,the use of the inside-outside algorithm unavoidably renders the full model er (by 5c 8 times) compared to the decomposable attention model.
D18-1186,4,"in future work, we hope to improve the extensibility of cin and apply it to other nlp tasks, such as machine comprehension."
D18-1188,1,"in future work, we would like to make use of table information and external knowledge to improve our qg model."
D18-1188,4,We also plan to apply the approach to other tasks.
D18-1190,2,"in future work, we would like to automatically learn a delexicalizer from data, tackle zero-shot parsing when the structure distribution in the target domain is very different from the source domains, and apply our framework to datasets where only denotations are provided."
D18-1191,3,"since the investigation on the characteristics of the representations can lead to interesting findings, it is worthwhile evaluating them intrinsically and extrinsically."
D18-1191,1,the use of frame knowledge to reduce these confusions is a straightforward approach.
D18-1191,1,an interesting direction for future work concerns evaluating span representations from our span-based model.
D18-1191,1,another promising direction is to explore methods of incorporating frame knowledge into srl models.
D18-1194,1,"the representations for cross-lingual decompositional semantics, the evaluation metric, and the evaluation dataset provided in this work will be beneficial to the increasing interests in semantic analysis and cross-lingual applications."
D18-1195,1,"we hope that the problem we introduce in this work, together with the dataset that we release, inspires the community to develop models that can learn language (e.g., semantic parsers) through flexible natural language conversation with end users."
D18-1196,2,"this includes applying more sophisticated tagging techniques such as bidirectional lstms, attention, and dynamic oracles, but most importantly developing new data and tasks to which the approach can be applied."
D18-1196,1,we hope deepcx will inspire further work on scl.
D18-1198,1,"in the future, we would also like to investigate if it is possible for alignments to be treated as latent variables, which can be learned in a joint manner within the current framework."
D18-1201,2,we also plan to explore multilingual applications.
D18-1201,1,"our model is capable of scoring bundles of new edges, and in future work, we plan to explore the possibility of combining m3gm with a search algorithm, to automatically extend existing knowledge graphs by linking in one or more new entities."
D18-1204,2,"we also would like to take the citation sentences of each reference into consideration, which is another concise and universal data source for scientific summarization (chen and hai, 2016; cohan and goharian, 2017)."
D18-1204,5,"in future work, an appealing direction is to organize the selected sentences in a logical fashion, e.g., by leveraging a topic hierarchy tree to determine the arrangement of the related work section (cong and kan, 2010)."
D18-1204,1,"at the end of this paper, we believe that extractive methods are by no means the final solutions for literature review generation due to plagiarism concerns, and we are going to put forward a fully abstractive version in further studies."
D18-1206,1,"in the future, we would like to create more linguistically-aware encoders and decoders incorporating co-reference and entity linking."
D18-1207,5,including mechanisms to promote paraphrase generation in the summary generator could be an interesting direction.
D18-1207,1,"future work could be done on closing the gap to match human levels of abstraction, which is still very far ahead from our model in terms of novel n-grams."
D18-1209,1,"another interesting direction is to learn binary and compact network embedding, which could be more efficient in terms of both computation and memory, relative to its continuous counterpart (shen et al., 2018b)."
D18-1209,1,"in future work, we aim to leverage abundant unlabeled text data to abstract more informative sentence representations (dai and le, 2015; zhang et al., 2017; shen et al., 2017; tang and de sa, 2018) ."
D18-1213,2,"as part of future work, we would like to incorporate external knowledge as a side information for improved time-stamping of documents."
D18-1215,1,future directions include: combining dpl with deep generative models; exploring alternative optimization strategies; applications to other domains.
D18-1219,5,"in the future, we will study how to integrate context outside of nps for the task of choosing antencedents for bridging anaphors."
D18-1219,1,also we hope that our word representation resource will facilitate other related research problems such as semantic role labeling.
D18-1220,5,in the future we will study how this commonsense reasoning technique can contribute to solving edge cases and difficult examples in more general coreference tasks.
D18-1221,4,"the reasonable next step will be to extend our methods for modelling the relations (edges) of a kb graph, which will allow applications in tasks such as link prediction and kb completion."
D18-1221,6,"furthermore, having a mechanism that translates arbitrary text to points in a continuous space creates many opportunities for interesting research."
D18-1221,2,the exciting question of how can we exploit this extra informationfor instance in order to enrich the knowledge base with new dataconstitutes one of our future directions.
D18-1222,1,"in our future work, we will explore the following research directions: (1) sphere is a simple model to represent a concept in semantic space, but it still have some limits since it is too naive. we will try to find a more expressive model instead of spheres to represent concepts.(2) a concept may have different meanings in different triples."
D18-1222,1,we will try to use several typical vectors of instances as a concepts centers to represent different meanings of a concept.
D18-1223,1,our future work might consider incorporating external text data and also enhancing our model to make better use of multiple training examples in the few-shot learning case.
D18-1224,5,one direction of future work is to use our framework to diagnose when a re is broken over a text stream.
D18-1225,1,"in future, we would like to incorporate type consistency information to further improve our model and also integrate hyte with open-world knowledge graph completion (shi and weninger, 2018)."
D18-1225,1,we are hopeful that our proposed temporal representation learning algorithm will motivate further research on temporal kg embedding learning.
D18-1226,1,"future directions include investigations on employing alternative neural architectures such as convolutional neural networks (cnns) as adaptation layers, as well as on how to learn the optimal value for from the data automatically rather than regarding it as a hyper-parameter."
D18-1227,2,"in the future, we plan to extract more patterns to obtain more comparative sentences, so that we may more accurately demonstrate how useful performing comparative analysis after linking the business mentions can be."
D18-1229,6,"assessing the effectiveness of visual supervision for these tasks is outside of the scope of this paper, however, and we will consider these studies in future work."
D18-1230,4,"also, the proposed framework can be further extended to other sequence labeling tasks, such as noun phrase chunking."
D18-1230,4,"in future, we plan to further investigate the power and potentials of the auto ner model with tie or break scheme in different languages and domains."
D18-1230,1,"moreover, going beyond the classical ner setting in this paper, it is interesting to further explore distant supervised methods for the nested and multiple typed entity recognitions in the future."
D18-1232,4,"we also plan to further study the biased dis- tillation problem and explore the compatibility of our approach in other nlp tasks such as natural language inference (bowman , 2015), answer sentence selection (yang , 2015) and so on."
D18-1232,1,"in future work, we will explore new distillation methods that have better compression capabilities for mrc tasks, such as distilling knowl- edge from a single model instead of the ensemble without lossing performances, adding weights on knowledge based on the distilling quality and so on."
D18-1235,1,"in the future, we plan to design models which could generate all possible answers for a single question."
D18-1236,4,"lastly, one can also interested in developing task-oriented rewards for adapting the agent to a specific task, for example, the answer generation task for open-domain kbqa system."
D18-1236,6,"our work suggests at least three future research topics: firstly, one can enrich the theoretical study of the duality between the oie and oin tasks."
D18-1236,1,"secondly, one can investigate how to conquer the barrier of the absence of an extensive collection of reasonable sets of open-domain facts and incorporate unsupervised information into this logician orator dual learning structures for further improvement."
D18-1237,1,future work includes developing a scalable read/write accessing mechanism to handle a large-scale external memory to reason over multiple documents.
D18-1238,4,"while our proposed encoder demonstrates promise on reasoning and understanding natural language, we believe that our encoder is generalizable to other domains beyond reading comprehension."
D18-1239,4,"future work should explore scaling the model to other question answering tasks, using more general composition modules, and introducing additional module types."
D18-1240,1,"there is significant room for further elaboration of this approach, for example, by expanding feature spaces with more syntactic and semantic features, employing new types of kernels for measuring the inter-question/answer pair similarity or trying to implement the same idea in dnn architectures."
D18-1242,1,possible future work includes supporting more complex semantics like implicit time constraints.
D18-1243,1,"furthermore, we will conduct research in how to utilize entity information to assign more appropriate initial parameters of the relation extractor."
D18-1243,1,"in the future, we will incorporate the sdp and stp to obtain more precise shortened sentences."
D18-1245,2,"in future work, we will build a domain-specific distant supervision dataset with a higher ratio of multiple instances and compare our system with others."
D18-1245,1,"furthermore, we will consider not using rnns or cnns, but a deeper neural networks with only attentions for distantly supervised re, similar to the work in vaswani et al.(2017)."
D18-1246,2,"for future work, we consider adding coreference information as an entity mention can have coreferences, which help on information collection."
D18-1246,1,"not only content words, but also propositions can introduce word sense problem (gong et al., 2018)."
D18-1246,1,confusing caused by word senses can be a severe problem.
D18-1246,1,Another possible direction is including word sense information.
D18-1247,2,"in the future, we plan to explore the following directions: (1) it will be promising to adopt extra information to help train more efficient models for solving the long-tail relation problem."
D18-1247,1,(2) we may also combine our attention method with recent denoising methods to further improve model performance.
D18-1248,2,"in the future, we plan to utilize more information in knowledge graphs to improve the distant supervision signal."
D18-1248,1,"for instance, the reasoning path can introduce new prior knowledge, which is a key direction in current works of kg."
D18-1250,5,"our experiments also highlighted the existing challenges for neural relation classification models, including cross-sentence relations and imbalanced data."
D18-1253,4,"third, we would like to generalize sdn to a wide range of complex goal-oriented tasks beyond dialogue, such as the particularly challenging atari game of montezumas revenge (kulkarni , 2016)."
D18-1253,1,"first, we want to integrate subgoal discovery into dialogue policy learning rather than treat them as two separate processes."
D18-1253,1,"second, we would like to extend sdn to identify multi-level hierarchical structures among subgoals so that we can handle more complex tasks than those studied in this paper."
D18-1254,1,"several future research directions are enabled by our study, ranging from the use of neural clustering models to the application of our models to fast and semi-automatic prototyping of dialog systems."
D18-1255,1,"going forward, we would like to build models which are a hybrid of span prediction models and generation models."
D18-1256,6,"we hope this work will spur more research in hybrid approaches that can work in open-ended, goal-oriented settings."
D18-1257,1,"secondly, to use the generated questions more effectively, the representative-based semisupervised approach might be improved by techniques studied in active learning and hard example mining (settles, 2009; shrivastava et al., 2016; chang et al., 2017)."
D18-1257,1,"in our future work, we would like to design algorithms to better model a long context, to utilize external knowledge, and to explore more effective semi-supervised learning approaches."
D18-1257,1,"firstly, we would like to investigate efficient ways of utilizing external knowledge such as paraphrasing and semantic concepts like prior works (dong et al., 2017; dasigi et al., 2017)."
D18-1260,3,"we leave closing this gap for future research, and illustrate, via oraclestyle experiments, the potential of better retrieval and reasoning on this task."
D18-1263,4,"future work can employ this linearization function within downstream applications, as was done with syntactic linearization, or extend this framework with other graph-based representations, such as universal dependencies (nivre , 2016) or amr (banarescu , 2013)."
D18-1265,1,"future work includes exploring alternative approaches such as transition-based methods (nivre et al., 2006; chen and manning, 2014) for semantic parsing with latent dependencies, applying our dependency-based hybrid trees on other types of logical representations (e.g., lambda calculus expressions and sql (finegan-dollak et al., 2018)) as well as multilingual semantic parsing (jie and lu, 2014; susanto and lu, 2017a)."
D18-1266,4,"for the future, we plan to apply the proposed learning framework to more semantic parsing tasks and consider new methods for policy shaping."
D18-1268,2,"for future work, we would like to extend this work in the semi-supervised setting where insufficient bilingual dictionaries are available."
D18-1269,1,"we found that cross-lingual encoder baselines provide an encouraging and efficient alternative, and that further work is required to match the performance of translation based methods."
D18-1270,4,similar techniques can be applied to other information extraction tasks like relation extraction to extend them to multilingual settings.
D18-1270,5,"for all xel approaches, the task of candidate generation is currently limited by existence of a target language wikipedia and remains a key challenge."
D18-1270,1,"a joint inference framework which enforces coherent predictions (cheng and roth, 2013; globerson et al., 2016; ganea and hofmann, 2017) could also lead to further improvements for xel."
D18-1271,3,"for future work, we plan to 1) conduct more experiments and analyses following this preliminary study to verify our approachs effectiveness for more languages and domains (e.g., social stream vs news stream);"
D18-1271,4,3) apply our approach to real-time coordinated text streams for never-ending knowledge mining and use the mined knowledge to improve the downstream applications.
D18-1271,3,"for future work, we plan to 1) conduct more experiments and analyses following this preliminary study to verify our approach effectiveness for more languages and domains (e.g., social stream vs news stream); 2) attempt to use word embedding (e.g., word2vec (mikolov et al., 2013), glove (pennington et al., 2014) and elmo (peters et al., 2018)) for local context encoding and use it as a clue for decipherment; 3) apply our approach to real-time coordinated text streams for never-ending knowledge mining and use the mined knowledge to improve the downstream applications."
D18-1272,6,we also focus on automatically generating homographic puns.
D18-1272,1,"in future work, we would like to find an appropriate way in incorporating the external linguistic knowledge to improve the performance of homographic puns recognition."
D18-1273,1,"in this paper, we do not put too many efforts into model design for csc, which we leave as potential future work."
D18-1277,2,future work on dataset creation can include generating similar challenge datasets for different key ambiguities in nlp.
D18-1277,1,a third direction for research would be using this dataset to evaluate different contextual modeling approaches and investigate the creation and using such context sensitive dataset to create simpler and smaller models that can capture a lot of contextual word representation.
D18-1277,1,"future work can include exploring ways to incorporate more context into the tagger, possibly by using information from dependency tree."
D18-1277,1,"also investigating more downstream tasks and explore if this dataset can be used directly in downstream tasks in a way similar to what have been done in swayamdipta et al.(2017) and (eriguchi et al., 2017; niehues and cho, 2017; kiperwasser and ballesteros, 2018) for injecting syntax in semantic role labeling and translation tasks."
D18-1279,4,"in the future, we would like to explore using the intnet model for other nlp tasks."
D18-1280,4,"in the future, we plan to test icon on other relevant dialogue-based applications and also use it for empathetic dialogue generation."
D18-1282,1,"encouraged by our findings, future work includes the exploration of the model and its learned frame-semantic representations for tasks such as the interpretation of multimodal scenes and stories and referring expressions."
D18-1283,1,our future work will extend the model and findings from this work to vision processing that will not only identify commonsense evidence but also explain where and how in the perceived environment the evidence is gathered.
D18-1284,4,"because we use short text and can leverage domain knowledge, we believe future work could use our models for applications such as personalized dialogue systems."
D18-1285,1,"in the future, we intend to add a generative model along with a physical simulation allowing the learner to imagine scenarios where a predicate might not hold."
D18-1285,1,we intend to combine the weakly supervised parser with an unsupervised parser and learn to determine whether a sentence should be grounded visually during training.
D18-1286,6,"in future work, we are interested in investigating mechanisms to improve generalization to new environments."
D18-1286,1,"for example, pointer and graph networks (vinyals et al., 2015; defferrard et al., 2016) are a promising direction to help supervise translation models and predict motion behaviors."
D18-1287,5,"another important open question concerns automated evaluation, which remains especially challenging when instructions do not only specify goals, but also constraints on how to achieve them."
D18-1287,6,achieving this while retaining an interpretable goal representation that clearly determines the execution is an important direction for future work.
D18-1290,1,"relevant directions include improving the transition logic of our parser, the bilstm ne model and the interactions between the two models."
D18-1290,1,in future work we intend to explore methods for closing the performance gap our algorithms still have for msg queries (both uas and segmentation f1) and for ssg queries (uas only).
D18-1291,1,"in future work it would be interesting to investigate whether the patterns observed here also hold true for other types of models in dependency parsing; possible variations to examine include alternative character models such as convolutional neural networks, joint tagging-parsing models, and graph-based parsers."
D18-1293,5,"finally, extending this method to work for pcfgs and more complex probabilistic models is an important open problem."
D18-1298,1,"as pretraining leads to a considerable improvement in performance, future work could be done fine-tuning this model for various dialog systems."
D18-1298,1,future work may also entail building more advanced strategies to select a limited number of personas for each user while maximizing the prediction performance.
D18-1299,1,"our future work is to evaluate the performance of our models in the scenario where there are new slots and more unobserved slot values, and to evaluate the domain-transferring ability of our models."
D18-1302,4,"although our work is preliminary, we hope that our work can further develop the discussion of evaluating nlp systems in different directions, not merely focusing on performance metrics like accuracy or auc."
D18-1302,4,"although this work focuses on gender terms, the methods we proposed can easily be extended to other identity problems like racial and to different tasks like sentiment analysis by following similar steps, and we hope to work on this in the future."
D18-1303,2,"in future work, we hope to experiment with the transferability of our model to other datasets to encompass the diverse mediums through which these personal stories are shared."
D18-1306,5,one interesting problem that our models do not account for is the existence of overlapping and non-continuous entity spans.
D18-1308,2,"in future work we thus hope to apply this model to larger datasets, and to address the efficiency issue."
D18-1308,5,"another promising direction would be to move to convolutional encoder and decoder architectures, designing the latter in a way similarly capitalizes on the label space tree structure."
D18-1309,1,we also consider modeling the dependencies between regions.
D18-1309,6,"For future work, we would like to investigate the use of region-level information."
D18-1311,2,"as for future work, we are testing on languages other than english."
D18-1312,5,it would be interesting to have a similar measure for headmarking situations with dependencies marked on the governor.
D18-1316,1,we hope our work encourages researchers to pursue improving the robustness of dnns in the natural language domain.
D18-1320,2,"the proposed approaches can also be applied to other languages such as min nan or hakka, which are spoken languages that are even less well-documented than cantonese."
D18-1320,1,"for future work, recursive neural network (tai et al., 2015) can be used as it is better suited for the hierarchical logographic decomposition."
D18-1320,1,"besides, incorporating more detailed relationship between radicals (e.g.(zhuang et al., 2017)) can help improve the model."
D18-1322,3,"for future work, we would like to evaluate our approaches in more applications."
D18-1322,1,"in addition, we would like to continue improving the marginal estimation by experimenting with recent density estimation techniques such as nade (uria et al., 2016)."
D18-1322,1,"for example, we can use the marginal statistics for information extraction, or to detect and remove abnormal phrases in text generation."
D18-1325,1,"in future work, we plan to explicitly model discourse connections with the help of annotated data, which may further improve translation quality."
D18-1326,4,"for future work, we plan to extend our strategies on many-to-many multilingual translation scenarios, and explore other effective strategies to balance parameter sharing."
D18-1327,1,"in the future, we will explore adding back translated (sennrich et al., 2016a) or copied (currey et al., 2017) target data to our multi-source system."
D18-1328,3,"in addition, we would like to measure the performance of our model after applying subword tokenisation, as well as using multiple lstm layers, a technique well known to capture hierarchical structure in the context of mt."
D18-1328,1,"we plan to use our model to predict sentence embeddings over monolingual corpora, allowing to collect parallel pairs through vector similarity measures."
D18-1329,4,"we will also apply this evaluation method to other tasks that use additional context, e.g.images in visualquestion answering, or part-of-speech tags in neural machine translation."
D18-1329,1,future work includes augment existing multimodal translation models with an additional adversarial objective that forces the model to perform better in the presence of the congruent image than a random incongruent image.
D18-1331,1,"in the future, we hope to find out more patterns and generalized rules to explain the models learning of the temperature."
D18-1332,1,in the future we would like to apply local optimizers in distributed setting where the communication latency between local and remote devices varies significantly we could use local optimizers to synchronize remote models less often.
D18-1333,1,"in future work we plan to: 1) build a fully end to-end nmt model for dp translation, which does not depend on any external component (i.e."
D18-1333,1,"dpp predictor); 2) exploit cross-sentence context (wang et al., 2017) to further improve dp translation; 3) investigate a new research strand that adapts our model in an inverse translation direction by learning to drop pronouns instead of recovering dps."
D18-1334,3,"in the future, we would like to conduct further manual evaluation on the translations to further analyze the differences with the baseline system."
D18-1334,1,"furthermore, we aim to experiment with other ways of integrating speaker information."
D18-1335,2,we consider the results promising and try more language pairs and fine-tune the hyperparameters.
D18-1335,1,"as future work, we aim to develop a bidirectional 2dlstm and consider stacking up 2dlstms for a deeper model."
D18-1336,1,"another direction of improving the model might be efficient implementation of beam search which can contain rescoring using an external language model as often done in speech recognition (graves et al., 2013)."
D18-1336,1,"as a future work, we can try to improve the performance of the model by iterative denoising as done by lee et al.(2018) while keeping the nonauto regressive nature of the decoder."
D18-1339,6,we leave as future work a deeper analysis of the level of character-awareness encoded in representations of the bpe segments as a byproduct of training.
D18-1341,6,future work includes inducing macro-actions composed of simpler building block actions.
D18-1344,1,"as future work, it will be interesting to explore techniques that can generate artificial cm data following the lexical, semantic and pragmatic constraints, or develop novel embedding techniques that can appropriately interpolate between real and artificial cm data to learn collocations that arise due to not only syntactic but also lexical, semantic and pragmatic aspects of code-mixing."
D18-1347,5,"this problem is treated in recent work on pos-tagging for social media text (owoputi , 2013; gimpel , 2011), but these issues have yet to be fully explored in code-switched contexts."
D18-1347,5,"this problem is treated in recent work on pos-tagging for social media text (owoputi et al., 2013; gimpel et al., 2011), but these issues have yet to be fully explored in code-switched contexts."
D18-1349,1,"we plan to make use of the rest unannotated abstracts or full texts to pre-train our model and then fine tune it to the target annotated datasets inspired by the work from (howard and ruder, 2018) so that the performance can be further boosted."
D18-1352,1,"for our method to be useful for human coders, it is important to develop an accurate novelty detector."
D18-1352,1,"if we can take advantage of all this structured and unstructured information via methods such as transfer learning or multi-task learning, then we may be able to predict infrequent labels better.2."
D18-1352,1,"we plan to study methods for determining if an instance contains an infrequent label and if it does, how many infrequent labels it should be annotated with."
D18-1353,5,"can we quantify some other intractable criteria, e.g, poeticness?"
D18-1353,5,how to design the methods of communication among many generators?
D18-1353,5,would the collaboration of more learners lead to better results?
D18-1354,3,"in future works, we will explore the use of other attributes of responses such as part-of-speech (pos) tags and chunking sequences as additional conditions for better response generation."
D18-1355,3,"in future, we would like to investigate deeper into the different effects of additional memory and internal memory."
D18-1358,1,"in the future, we will utilize more sophisticated models to leverage the hrs information, e.g, (1) utilize the embeddings of the three layers in a more sophisticated way instead of sum them together; (2) determine the number of relation clusters and sub-relations automatically instead of manually."
D18-1360,4,we also plan to extend our multi-task framework to information extraction tasks in other domains.
D18-1360,1,future work includes improving the performance using semisupervised techniques and providing in-domain features.
D18-1361,1,"in this way, we can further build an end to-end framework for the large-scale q20 games in the real world."
D18-1361,1,"as for the future work, we plan to explore methods to use machine reading to automatically construct the state transition dynamics from corpora like wikipedia."
D18-1362,1,"in future work, we would like to investigate learnable reward shaping and action dropout schemes and apply model-based rl to this domain."
D18-1367,1,"as a future work, we plan to investigate if our qq-based models are enhanced by extra-linguistic knowledge, and to incorporate contextual, multimodal features along semantic ones."
D18-1368,4,"in the future, we plan to incorporate se type information in various downstream applications, e.g., many information extraction applications that require distinguishing specific fact descriptions from generic statements."
D18-1370,4,"in the future work, we would like to extend the collection of scientific text to other fields."
D18-1370,1,"next, we intend to explore a wider range of mtl models, especially those involving more than two tasks."
D18-1370,1,"having annotated argumentative relations, we will work on models for their automated identification in scientific publications."
D18-1371,1,"therefore, in future work we plan to develop joint models that simultaneously extract events and time expressions, and parse their temporal dependency structure."
D18-1374,5,this could also be explored for better feature representation in our problem.
D18-1374,1,"many avenues for further work can be identified, including: finding a better metric capturing novelty of entities, analyzing the influence of the size of the input passage to quality of predictions, and experimenting with new models."
D18-1374,1,"moreover, recent work has considered incorporating knowledge graph information for better use of entity features (dalton et al., 2014; liu et al., 2018)."
D18-1375,1,"as future work, we will extend the proposed method to include components that learn weights for individual feature elements and not only the entire feature type."
D18-1378,1,"in future work, we plan to extend limbic to capture long-distance discourse relations and the influence decay of discourse relations between seus as their distance increases."
D18-1379,1,"in the future, we will explore the possibility of learning a topic model and an emotion ranking function simultaneously in a unified framework."
D18-1382,6,"future direction of work also include adding more dimensions, e.g.emotion analysis & intensity prediction."
D18-1382,1,"in future, we would like to investigate new techniques, and explore the ways to handle implicit sentiment and sarcasm."
D18-1383,4,"the proposed framework could be potentially adapted to other domain adaptation tasks, which is the focus of our future studies."
D18-1389,2,we are also interested in characterizing the factuality of reporting for media in other languages.
D18-1389,1,"in future work, we plan to address the task as ordinal regression, and further to model the interdependencies between factuality and bias in a joint model."
D18-1390,4,"in the future, we will seek to explore the following directions: (1) we will explore more ljp subtasks and more scenarios of cases such as multiple defendants and charges to investigate the effectiveness of topjudge."
D18-1390,1,"(2) we will explore how to incorporate into ljp the temporal factors, which are not considered in this work."
D18-1395,2,"we would also like to test the classifiers defined here in the more challenging scenario of smaller text chunks (e.g., 10c20 sentences rather than the 100-sentence text chunks we used here)."
D18-1395,1,"furthermore, we plan to devise unsupervised approaches to the identification of native language with the same dataset."
D18-1395,1,"finally, we are currently experimenting with adversarial learning models for this task."
D18-1396,2,"for future works, we will study more left/rightbranching languages as well as other languages that have no obvious branching characteristics."
D18-1396,4,"we will also investigate how language branching influences other natural language tasks, especially for neural networks based models."
D18-1399,2,"in the future, we would like to extend our approach to semi-supervised scenarios with small parallel corpora, which we expect to be particularly helpful for tuning purposes."
D18-1399,1,"moreover, we would like to try a hybrid approach with nmt, using our unsupervised smt system to generate a synthetic parallel corpus and training an nmt system over it through iterative backtranslation."
D18-1400,1,"we also want to change the visual pre-training model from an image classification dataset to other datasets that have both objects and actions, to further improve translation performance."
D18-1400,1,"in the future, we will continue exploring different methods to ground the visual context into the translation model, such as learning a multimodal shared space across image, source language text, as well as target language text."
D18-1401,3,"besides, we would like to test the effectiveness of the proposed approach to qa-style sentiment classification in some other languages."
D18-1401,6,"in the future, we would like to investigate some other network structures to explore deeper information in each qa text pair."
D18-1402,2,"furthermore, we are experimenting with language adaptation and plan to extend the tool to the german language."
D18-1402,1,we also intend to investigate methods for grouping similar arguments.
D18-1403,1,we would also like to develop methods for abstractive opinion summarization using weak supervision signals.
D18-1403,1,"in the future, we plan to develop a more integrated approach where aspects and sentiment orientation are jointly identified, and work with additional languages and domains."
D18-1404,3,we plan to employ transfer learning methods with the proposed enriched patterns and test on other emotion-related problems such as sentiment classification and sarcasm detection.
D18-1404,4,the proposed methodology is also being expanded to support spanish and japanese emotion recognition tasks.
D18-1404,6,"in the future work, we aim to investigate the graph-based patterns more in-depth and provide a more comprehensive and advanced theoretical discussion of how they are constructed."
D18-1404,1,we also hope to keep improving the pattern weighting mechanism so as to improve the overall performance on emotion recognition tasks and minimize trade-off between pattern coverage and performance.
D18-1409,6,"in future work, we will explore the direction of adding an extra coherence reward (wu and hu, 2018) to improve the quality of extracted summaries in terms of sentence discourse relation."
D18-1410,4,"for future work, we would like to extend our lexicon to cover specific domains, different target users and languages."
D18-1411,4,"for future study, one could collect large-scale data and texts in other domains where more complex grounding on phrases such as increasing trends should be done."
D18-1411,1,"to enhance modeling power, unsupervised discriminative models that utilize rich features (berg-kirkpatrick et al., 2010) could also be explored."
D18-1411,1,we are also interested in collecting more high-quality parallel data to induce grounded compositional logic representations.
D18-1413,1,"though the proposed model works well empirically, understanding exactly what is learned in the latent variables is non trivial, and is a possible direction for future work."
D18-1417,1,"in future works, we plan to improve our model by introducing extra knowledge."
D18-1421,4,"in the future, we plan to apply the framework and training techniques into other tasks, such as machine translation and dialogue."
D18-1422,2,"we will also extend the set of operations to accommodate historical data, graph data and detect the unsupported facts in the generation within the single framework."
D18-1422,1,"as applying operations on a large number of records greatly increases the search space for the attention mechanism, we will extend our model to automatically detect the relevant operations to reduce computing complexity."
D18-1424,1,"our future work lands in the following directions: incorporate rich features, such as pos and entity, in input passages; directly optimize sequence-level metrics with policy gradient; relax the constraint on answer to accept abstractive answers; jointly model question generation and question answering; ask multiple questions simultaneously with diverse perspectives."
D18-1426,1,"in future work, we will focus on the semi-supervised approach to make the dae also suitable for problems where instead of all, only a subset of the structured information should be included in the output."
D18-1429,3,"as future work, we would like to design better metrics for answerability and check if a non-linear combination of different elements in the q-metric leads to better correlation with human judgments."
D18-1430,6,"for future works, we will consider adopting the mutual information regularization for other text generation tasks which encourage stylistic generation or diversity to improve their performances."
D18-1430,1,"besides, our model simplifies the prior of style distribution as a uniform distribution."
D18-1430,1,another intriguing direction is to refine our model for more task-specific scenarios.
D18-1432,3,we also plan to verify the results with a human evaluation study.
D18-1432,4,this will allow us to use extend the notion of coherence to account for phenomena such as topic shifts.
D18-1432,1,"in future work, we plan to replace the glove based measure of coherence with a trained discriminator that distinguishes between coherent and incoherent responses (li and jurafsky, 2017)."
D18-1433,2,"in future work, we are increasing the size of dataset and exploring other knowledge-centric metrics for this task."
D18-1434,4,we also aim to consider the generalisation of this approach to other vision and language tasks.
D18-1434,1,"in future, we would like to analyze means of obtaining composite embeddings."
D18-1435,1,we will also make further research on context-aware fine-grained entity typing to train a better template generator.
D18-1435,1,another research direction based on this work is to develop an end to-end neural architecture to make the model more flexible without generating a template in the middle.
D18-1435,1,"in the future, we will expand the entity-aware model to incorporate the relations between candidates when the model fills in the slots, which can avoid the cases such as cristiano ronaldo of barcelona."
D18-1439,1,"our future work will focus on two areas: investigation on multi-document keyphrase generation, and incorporation of structure or syntax information in keyphrase generation."
D18-1443,4,"preliminary work that investigates similar bottom-up approaches in other domains that require a content selection, such as grammar correction, or data-to-text generation, have shown some promise and will be investigated in future work."
D18-1445,3,"on the experimental side, a logical next step is to implement an interactive user interface for april and conduct a larger evaluation study comparing the summary quality before and after the interaction."
D18-1445,4,"we also plan to apply april to more nlp applications, including machine translation, information exploration and semantic parsing."
D18-1445,1,"on the technical side, we plan to employ more advanced apl and rl algorithms in april, such as sample-efficient bayesian-based apl algorithms (e.g., simpson and gurevych, 2018) and neural rl algorithms (e.g.mnih et al., 2015) to further reduce the sample complexity of april."
D18-1447,1,"for future work, we will 1) leverage unlabeled data to study domain adaptation or transfer learning for keyphrase generation; and 2) investigate novel models to improve absent keyphrase generation when limited labeled data is available based on semi-supervised learning."
D18-1449,1,"for example, future research includes applying learning-to-rank regarding all outputs as features, conducting active learning to select a new model setting online, and developing boosting-like-ensemble based on the bagging of training data."
D18-1451,1,"in future work, we hope to use extra discriminators to control the style and sentiment of the generated summaries."
D18-1452,2,"it would be also interesting to extend the framework to a cross-domain (shah , 2018) or a cross-language setting (da san martino , 2017; joty , 2017)."
D18-1452,1,trying an ensemble of neural networks with different initial seeds is another possible research direction.
D18-1452,1,"from a modeling perspective, we want to strongly couple crf and dnn, so that the global errors are backpropagated from the crf down to the dnn layers."
D18-1452,1,"in future work, we plan to model text complexity (mihaylova et al., 2016), veracity (mihaylova et al., 2018), speech act (joty and hoque, 2016), user profile (mihaylov et al., 2015), trollness (mihaylov et al., 2018), and goodness polarity (balchev et al., 2016; mihaylov et al., 2017)."
D18-1455,1,"current directions for future work include c (1) extending graft-nets to pick spans of text as answers, rather than only entities and (2) improving the subgraph retrieval process."
D18-1456,6,employing such a nil-aware answer span extractor in practical ir-style qa tasks will be interesting future work.
D18-1457,3,"future directions include validating our approach on other architectures such as rnn (bahdanau , 2015) or cnn (gehring , 2017) based nmt models, as well as combining with other advanced techniques (shaw , 2018; shen , 2018; yang , 2018; li , 2018) to further improve the performance of transformer."
D18-1457,3,"future directions include validating our approach on other architectures such as rnn (bahdanau et al., 2015) or cnn (gehring et al., 2017) based nmt models, as well as combining with other advanced techniques (shaw et al., 2018; shen et al., 2018; yang et al., 2018; li et al., 2018) to further improve the performance of transformer."
D18-1459,3,"in the future, we will continue to examine the effectiveness of atr on different neural models for nmt, such as the hierarchical nmt model (su , 2018b) as well as the generative nmt model (su , 2018a)."
D18-1459,4,"we are also interested in adapting our atr to summarization, semantic parsing etc."
D18-1462,5,"however, even with the best human evaluation results, the error analysis shows that there are still many challenges in narrative story generation, which we would like to explore in the future."
D18-1467,1,"the existence of semantic neighbors occurring in similar contexts (e.g., the in- fluence of standard intensifier very on nonstandard intensifier af) may prevent a new word from reaching widespread popularity (grieve, 2018)."
D18-1467,1,future work should also investigate more semantically-aware definitions of linguistic dissemination.
D18-1468,6,"in the future, we would like to investigate the inferred trees in detail.10 the source code is publicly available at https://github.com/murawaki/ lattyp."
D18-1471,1,"as two of the most used functions of vulgar words relate to expressing sentiment or emotions, we will also explore collecting sentiment annotations for joint sentiment and vulgar word function inference and use this to improve the task of sentiment analysis using multi-task methods."
D18-1471,1,"future work will use this linguistic information to inform more complex machine learning models, e.g., deep neural networks, in an attempt to increase predictive gains."
D18-1475,1,"it is also interesting to combine with other techniques (shaw et al., 2018; shen et al., 2018a; dou et al., 2018; li et al., 2018) to further improve the performance of transformer."
D18-1475,1,"another promising direction is to design more powerful localness modeling techniques, such as incorporating linguistic knowledge (e.g. phrases and syntactic categories)."
D18-1481,2,"in the future, we will adapt our mean-max aae to other low-resource languages for learning universal sentence representations."
D18-1484,4,"in future work, we would like to explore other learning layers and generalize mtle to address other nlp tasks, for example, sequence labeling and sequence-to-sequence learning."
D18-1486,6,"in future work, we would like to investigate the relations of various tasks in multi-task learning by exploiting the potential of capsule network."
D18-1490,4,it might also be useful in very different applications such as image processing.
D18-1490,5,"in future work, we also intend to investigate other applications of the auto-correlational kernel."
D18-1491,4,"in future, we plan to study the performance of prus on different tasks, including machine translation and question answering."
D18-1491,1,"in addition, we will study the performance of the pru on language modeling with more recent inference techniques, such as dynamic evaluation and mixture of softmax."
D18-1493,4,"as for future work, we plan the following research directions: (1) in language modeling, given a sequence of words, a sequence of corresponding sememes can also be obtained."
D18-1493,1,we will extend our model with the hierarchical sememe tree for more accurate relations between words and their sememes.(3) it is imaginable that the performance of sdlm will be significantly influenced by the annotation quality of sememe knowledge.
D18-1493,1,"we will also devote to further enrich the sememe knowledge for new words and phrases, and investigate its effect on sdlm."
D18-1493,1,we will utilize the context sememe information for better sememe and word prediction.(2) structural information about sememes in hownet is ignored in our work.
D18-1494,6,"with the development of deep learning techniques, we also plan to de-emphasize irrelevant words with an attention mechanism."
D18-1494,1,"for future work, we plan to speed-up the training process of sltm by gpus and distributed algorithms."
D18-1495,1,our following work will consider adopting fastgcn to speed up the process.
D18-1497,1,"going forward, disentangled representations may afford additional advantages in nlp, e.g., by facilitating transfer (zhang et al., 2017), or supporting aspect-focused summarization models."
D18-1499,4,the psycholinguistic plausibility of these models can be tested in future work.
D18-1499,1,"for example, the model of jaech and ostendorf (2018) adapts to environmental factors, so it could potentially draw on independent experiences with female speakers and with lawyer speech in order to initialize a model of adaptation to a new female lawyer (see also mikolov and zweig, 2012; kleinschmidt, 2018)."
D18-1500,1,future work can investigate why this is the case and how we can leverage this information to improve model performance and interpretability.
D18-1505,1,future work includes a fine-grained quantitative study of elmo word vectors for logically complex sentences along the lines of peters et al.(2018b).
D18-1508,1,"as for future work, we plan to apply our label wise attention mechanism to understand other interesting linguistic properties of human-generated text in social media, and other multi-class or multilabel classification problems."
D18-1509,5,the interesting result that syntax-free trees outperform their syntax-driven counterparts elicits a natural question for future work: how do we better model syntactic structure in these models?
D18-1509,1,it would also be interesting to study the effect of using source-side syntax together with the target-side syntax supported by trdec.
D18-1512,2,"we expect that this will require further efforts in creating document-level training data, designing appropriate models, and supporting research with discourse-aware automatic metrics."
D18-1512,5,it will be interesting to explore to what extent existing and future techniques for document-level machine translation can narrow this gap.
D18-1514,6,future researches may consider incorporating commonsense knowledge or improved causal modules.
D18-1518,4,"it worth exploring if our model can improve the performance of other natural language processing applications whose inputs contain multiple sentences, for example, reading comprehension, dialog generation, and sentiment analysis."
D18-1519,4,"future work includes relation discovery, which is to identify new relations besides hypernyms in an unsupervised manner."
D18-1520,2,it would also be vital for future work to explore efficient combinations with other refinement methods using language resources.
D18-1520,1,"in future work, we have to modify the refinement method by relevance propagation to be more effective by exploring the mechanism of how the internal knowledge of word vectors is extracted by multilayer neural networks and examining the effectiveness of other relevance propagation methods."
D18-1521,1,future directions include extending the proposed approach to model other properties of words such as sentiment and generalizing our analysis beyond binary gender.
D18-1522,2,our future plans include exploring the value of the proposed methodology with other languages and additional properties.
D18-1524,4,"we also intend to evaluate on additional benchmark tasks such as glue (wang , 2018a), explore using the learned word representations as contextualized embeddings and perform downstream fine-tuning."
D18-1524,1,"in future work, we would like to explore using contextualized word embeddings, such as cove (mccann et al., 2017) and elmo (peters et al., 2018), as input to our models as opposed to noncontextualized representations."
D18-1525,1,"the next step for this work is to evaluate the performance of the proposed loss functions in state of-the-art models for the nlp tasks that leverage sentence-level semantic representation, such as the ones we explored in the present study."
D18-1529,2,"in this work, we showed that further research in chinese segmentation must overcome two key challenges: (1) rigorous tuning and testing of deep learning architectures and (2) more effort should be made on exploring resources for further performance gain."
D18-1530,1,"as future work, we intend to tackle the harder samasa problem which requires semantic information of a word in addition to the characters context."
D18-1533,1,future work could extend the fst to model orthographic changes suggested by an error analysis of the current models predictions (see appendix a.6).
D18-1536,4,"compared with existing corpora, it is of high quality and challenging, and is hopefully useful for research on ssei, crosslingual and cross-domain learning."
D18-1537,4,our future work will extend our study to consider other nlp tasks and models with the goal of producing useful insights for further improving these models.
D18-1538,1,semi-supervised training from the scratch and examination of semi-supervised setting on large dataset remains as part of the future work.
D18-1539,6,another area of future research is how the underlying system should recover when domain-adjacent instances are detected.
D18-1539,1,future work includes exploring alternative ways of incorporating information outside of the given training set and experimenting with various combinations of semantic parsers and upstream domain-adjacency models.
D18-1541,6,"among possible future work is using generative adversarial networks as corruption engines, and developing better sequence alignment methods."
D18-1541,1,"some preliminary results with simple corruptions using word substitution and word dropout (iyyer et al., 2015) appear to be promising, and may feature as components of a future corruption system."
D18-1543,2,"in future work, we plan to investigate what happens when training on more than 2 languages."
D18-1543,2,it would be interesting to experiment with using datasets that are not balanced with respect to size.
D18-1543,1,it would be interesting to experiment with different parsing architectures as well as varying those hyperparameters.
D18-1545,3,"furthermore, we plan to extend this work by evaluating the augmentation on other nlp benchmarks such as language modeling, dependency parsing and semantic role labeling."
D18-1545,1,"following these encouraging results, method can be improved by (1) considering the preferred chunk order of the language during rotation, (2) taking language specific flexibilities into account (e.g., spanish typically allows free subject inversion (unlike object))."
D18-1548,4,"future work will explore improving lisas parsing accuracy, developing better training techniques and adapting to more tasks."
D18-1549,5,"its an open question whether there are more effective instantiations of these principles or other principles altogether, and under what conditions our iterative process is guaranteed to converge."
D18-1549,1,Future work may also extend to the semisupervised setting.
D19-1001,4,"in the future, we would like to apply our approach to other sequence generation tasks."
D19-1001,1,"additionally, we wonder if a further performance increase could be achieved if the pre-training of bert would employ our placeholder strategy."
D19-1002,4,"additional future directions for this line of work include application on other tasks such as sequence modeling and multi-document analysis (nli, qa); extension to languages other than english; and adding a human evaluation for examining the level of agreement with our measures."
D19-1002,6,we view the conditions under which adversarial distributions can actually be found in practice to be an important direction for future work.
D19-1003,5,together these findings raise serious concerns regarding the efficacy of active learning in practice.
D19-1005,2,future work will involve incorporating a diverse set of domain specific kbs.
D19-1006,1,another direction for future work is generating static word representations from contextualized ones.
D19-1006,1,"therefore, adding an anisotropy penalty to the language modelling objective – to encourage the contextualized representations to be more isotropic – may yield even better results."
D19-1007,1,"moreover, our framework models different semantic change scenarios, and future work could focus on approaches that are able to distinguish between these different scenarios."
D19-1009,4,"thanks to the flexibility and scalability of our model, as future work we plan to explore in depth its use in different tasks, such as the creation of sentence (document) embeddings and lexical substitution."
D19-1009,4,"in fact, we believe that using disambiguated sense vectors, as shown in the contextsensitive embeddings and paraphrase detection studies, can offer a more accurate representation and improve the quality of downstream applications such as sentiment analysis and text classification (see, e.g., (pilehvar , 2017)), machine translation and topic modelling."
D19-1009,1,"encouraged by the good results achieved in our exploratory studies, we plan to develop a new model for contextualized word embeddings based on a gametheoretic framework."
D19-1011,5,"in the future, we will study how to solve the logical consistency problem between utterances and candidate responses to improve selection performance."
D19-1012,6,"one of the possible extensions of this work would be incorporating it with persona (zhang , 2018a) and task-oriented dialogue systems (gao , 2018; madotto , 2018; wu , 2019, 2017, 2018a; reddy , 2018; raghu , 2019)."
D19-1012,6,"one of the possible extensions of this work would be incorporating it with persona (zhang et al., 2018a) and task-oriented dialogue systems (gao et al., 2018; madotto et al., 2018; wu et al., 2019, 2017, 2018a; reddy et al., 2018; raghu et al., 2019)."
D19-1012,1,"having a persona would allow the system to have more consistent and personalized responses, and combining open-domain conversations with task-oriented dialogue systems would equip the system with more engaging conversational capabilities, hence resulting in a more versatile dialogue system."
D19-1014,2,"in the future, we plan to collect a fashion retrieval visual dialog dataset which simulates a realistic application for multi-modal dialog systems."
D19-1014,4,"furthermore, we will explore additional task-oriented settings where we can decouple task accomplishment from language generation to evaluate the extent our framework can generalize to other conversational tasks."
D19-1014,1,"to address the limitation of a high image retrieval rate with just the use of captions from the visdial dataset, we plan to format a challenging candidate image pool in which images are visually similar to each other."
D19-1015,2,"future works will focus on incorporating multimodal information into dialoguegcn, speaker-level emotion shift detection, and conceptual grounding of conversational emotion reasoning."
D19-1015,1,we also plan to use dialoguegcn in dialogue systems to generate affective responses.
D19-1016,1,"in addition, given that nrc vad is the only emotion-specific component, our model can be adapted as a generic model for conversation analysis."
D19-1025,1,"in the future, we are interested in incorporating entity structural knowledge to enhance text representation (cao et al., 2017, 2018b), or transfer learning (sun et al., 2019) to deal with massive rare words and entities for low-resource name tagging, or introduce external knowledge for further improvement."
D19-1026,1,the scalability of dca based models make it possible to handle large scale data with long documents.
D19-1027,1,"in future work, we will explore incorporating external knowledge (e.g. word relatedness contained in word embeddings) into the learning framework for event extraction."
D19-1027,1,"besides, exploring nonparametric neural event extraction approaches and detecting the evolution of events over time from news articles are other promising future directions."
D19-1029,1,"in the future work, we will explore the use of the structured tuples to bridge the gap between text content and knowledge-based applications, such as knowledge-based scientific literature search."
D19-1030,1,in the future we will explore more language-universal representations such as visual features from topically-related images and videos and external background knowledge.
D19-1032,1,"as this work shows promising results for the end to-end dee, expanding the inputs of doc2edag from pure text sequences to richly formatted ones (wu et al., 2018) is appealing, and we leave it as future work to explore."
D19-1033,2,"in future work, we will conduct experiments on more languages with and without explicit word delimiters."
D19-1033,1,"in addition, we will try developing a dynamic mechanism to selectively consider the sense-level information rather than take all the senses of characters and words into account."
D19-1034,1,"for future work, we consider to model the dependencies among entity regions explicitly and improve the performance of boundary detection module which is important for entity categorical label prediction."
D19-1039,2,"interesting future work includes: (1) given that information from 2-hop ds is redundant and noisy, we can explore smarter sampling and/or better bag-level aggregation methods to capture the most representative information.(2) metadata in web tables like headers and column names also contain rich information, which can be incorporated to further improve re performance."
D19-1040,6,"as shown by our experimental results, the contextualized entity encoder benefits more from this hyperlink-based training objective, suggesting future works to prioritize encoding entity description from its mention context."
D19-1041,2,"since a better event model is generally helpful for relation extraction, another promising direction would be to incorporate multiple datasets to enhance the performance of our event extraction systems."
D19-1041,6,"future research can focus on creating more robust structured constraints between events and relations, especially considering event types, to improve the quality of global assignments using ilp."
D19-1042,2,"for future work, we will explore the effectiveness of the proposed framework on other base models and forms of data (e.g., images)."
D19-1042,1,we will introduce more losses covering other aspects in the objective function to further improve the performance of our framework.
D19-1043,4,"in future, we would like to investigate the application of our theory in various tasks including reading comprehension and machine translating."
D19-1045,2,"in the future, we will contribute new text dataset to few-shot learning, explore better feature extractor networks and do some industrial application."
D19-1046,5,another promising direction is to understand whether more concentrated important features (lower entropy) lead to better human performance in supporting decision making.
D19-1047,3,"in future work, we plan to validate its effectiveness for multi-label classification."
D19-1047,1,"besides, we are interested in incorporating more powerful unsupervised methods into our architecture."
D19-1048,3,"future work will explore the performance of latent generative classifiers in other challenging experimental conditions, including testing robustness to data shift and adversarial examples as well as zero-shot learning."
D19-1048,1,"another thread of future work is to explore the performance of discriminative models with latent variables, and investigate combining pretrained representations with both generative and discriminative classifiers."
D19-1049,5,other efforts could involve the use of a variable number of research topics for each reviewer and exploring ways to render reviewer profiles human interpretable.
D19-1049,5,"such efforts could consider, for instance, the temporal variation of research interests in order to capture the relevance of a given reviewer to a given topic based on the recency of the contributions to a given area."
D19-1049,6,a concrete direction for future work would be to consider enhancements in representing reviewers’ profiles.
D19-1050,1,"future work should investigate how more complex transforms linking brain and machine can reveal parallel structure between the two systems. future work should integrate brain images derived from different behavioral tasks, and study which model–brain relationships are conserved across these behaviors."
D19-1051,2,"we hope that this critique provides the summarization community with practical insights for future research directions that include the construction of datasets, models less fit to a particular dofigure 2: pairwise similarities between model outputs computed using rouge."
D19-1053,3,"in future work, we plan to avoid the need for costly human references in the evaluation of text generation systems, and instead base evaluation scores on source texts and system predictions only, which would allow for ‘next-level’, unsupervised (in a double sense) and unlimited evaluation (louis and nenkova, 2013; bohm ¨ , 2019)."
D19-1056,1,further challenges for this novel architecture are to extend it to joint predicate and role labeling for more than one predicate at a time.
D19-1056,1,"in future work we also aim to make the system more flexible, by extending it to few-shot or zero shot learning, to alleviate the need for an initial big annotated set, and thus to be able to generate srl data for truly resource-poor languages."
D19-1057,2,"apart from our tentative experiments on the english dataset, applying the approach to other languages will be an interesting research direction to work on in the future."
D19-1058,1,"we also plan to extend our methodology to nouns and adjectives, in a similar fashion to (o’gorman et al., 2018) and connect the resulting frames to those in verb atlas."
D19-1058,1,"as future work, we plan to take full advantage of the novel semantic features available in verbatlas, such as wide-coverage selectional preferences and synset-level information, by exploiting them in multilingual srl and word sense disambiguation tasks."
D19-1058,1,"our plans include integrating the selectional preferences from syntagnet (maru et al., 2019), a new, large-scale lexical-semantic combination resource."
D19-1059,1,"in future work, we plan to consider subwords into the model and explore more geometric structures in sentences."
D19-1060,1,"while our learning criteria showed benefit on certain classes of tasks, our hope is that the discoeval evaluation suite can inspire additional research in capturing broad discourse context in fixed-dimensional sentence embeddings."
D19-1063,3,we are also exploring ways to deploy and evaluate our methods on real-world platforms.
D19-1063,1,"future work aims to provide more natural, linguistically realistic interaction between the agent and humans (e.g., providing the agent the ability ask a natural question rather than just signal for help), and to establish a theoretical framework for modeling human assistance."
D19-1064,2,"as future work, we plan to use visual information to specifically target complex downstream tasks requiring commonsense and reasoning such as question answering or visual dialogue."
D19-1065,1,we hope that continued study of this area will produce models that can aid humans in critical domains like citizen science.
D19-1066,2,the results of our questionnaire and system evaluation suggest the need for future work on supporting complex forms of reference within el systems; the datasets we provide can be used to evaluate such approaches.
D19-1066,3,"another important direction is to either reach a consensus on the el task (perhaps in a similar style to muc-6 for ner), or define protocols for evaluation in the absence of such a consensus; our fuzzy recall/f1 metrics are concrete steps in this direction."
D19-1068,2,"in the future, we plan to investigate more language-independent patterns in crosslingual transfer to circumvent this dependency."
D19-1069,1,these results indicate that end to-end models might be a promising alternative to the traditional pipeline approach.
D19-1071,4,"in the future, we may apply our pre-training method to other language pairs and delve into the performance of the pre-trained encoders on other nlp tasks, such as name entity recognition."
D19-1072,1,further extensions are needed to tackle tree-structured syntax information.
D19-1073,4,"as our approach is transparent to model architectures, we plan to further verify the effectiveness of our approach on other downstream applications of nmt such as post-editing and interactive mt in the future."
D19-1075,1,"for future work, we plan to explore more powerful kg embedding methods."
D19-1075,1,and we also have the idea of using categorical attributes or hierarchical types to guide the negative sampling process.
D19-1076,4,this approach has applications beyond the scope of this paper and can be used in any applications requiring solving opp or gpp.
D19-1077,1,"with such strong cross-lingual nlp performance, it would be interesting to prob mbert from a linguistic perspective in the future."
D19-1078,4,"finally, we will apply our framework into other translation models (bahdanau , 2015; su , 2018; song , 2019), so as to verify the generality of our framework."
D19-1078,1,"besides, how to leverage monolingual sentences of different domains to refine our proposed framework."
D19-1078,4,"finally, we will apply our framework into other translation models (bahdanau et al., 2015; su et al., 2018; song et al., 2019), so as to verify the generality of our framework."
D19-1078,1,"In the future, we plan to extend our framework to multi-domain NMT."
D19-1079,6,"in the future, we will focus on training with more agents by translating apart of one sentence considering its advantage for each agent, rather than translating the whole sentence."
D19-1080,1,"future work will be zero-shot translation without step-wise pre-training, i.e., combining individually pre-trained encoders and decoders freely for a fast development of nmt systems for a new non-english language pair."
D19-1081,2,"while in the current work we used text fragments of 4 sentences, in future work we would like to consider longer contexts."
D19-1082,4,"as our approach is not limited to specific tasks, it is interesting to validate the proposed model in other tasks, such as reading comprehension, language inference, and sentence classification."
D19-1083,4,"in the future, we would like to extend our model to other sequence-to-sequence tasks, such as summarization and dialogue generation, as well as adapt the idea to other generative architectures (zhang et al., 2016, 2018).an open question is whether there are other structural issues that limit the benefits of increasing the depth of the transformer architecture, or whether the benefit of very deep models is greater for other tasks and dataset."
D19-1084,2,"future work will explore different sets of languages paired with new, higher-quality elicited alignments of existing bitext."
D19-1084,2,we expect that asking annotators to only align named entities (or some other token type) will greatly increase the annotation speed.
D19-1084,4,these experiments will extend beyond ner to other tokenlevel tasks such as coreference resolution and event detection.
D19-1084,1,"we will also explore architecture modifications to allow for task-specific annotation, training, and inference for a given ie task, where only certain spans are of interest (i.e. training our aligner only on ner-span alignments)."
D19-1085,3,"first, we will evaluate our method on other implication phenomena (or called unaligned words (takeno , 2017)) such as tenses and article words for nmt."
D19-1085,1,"second, we will investigate the impact of different context aware models on zp translation, including multiattention (jean et al., 2017b) and context-aware transformer(voita et al., 2018)."
D19-1087,3,we will also investigate how to make better evaluation metrics with the help of error detection.
D19-1087,1,we will also study how to improve machine translation model through error detection.
D19-1087,1,"for example, how to improve mt models according to different class of errors."
D19-1087,1,"in the future, we will explore more advanced techniques such as neural networks for error detection."
D19-1088,4,"our approach can benefit from advanced exploitation of the gradients or other useful intermediate information, which we leave to the future work."
D19-1089,2,we will consider more languages (hundreds or thousands) to study our methods in larger scale setting.
D19-1089,2,"on the other hand, we will also consider other pre-training methods (song , 2019) for multilingual and low-resource nmt."
D19-1089,3,"for future work, we will test our methods for many-to-many translation."
D19-1089,5,"we will also study how to obtain language embeddings from monolingual data, which will make our method scalable to cover those languages with little or without bilingual data."
D19-1091,1,"language-specific or language-family-specific improvements (i.e. proper dealing with different alphabets, or using an adversarial language discriminator) could potentially further boost performance."
D19-1092,1,"our method is complementary with several other methods for cross-lingual transfer, such as annotation projection, and thus can be further integrated with these methods."
D19-1094,1,"we would like to apply the proposed model in low-resource settings, e.g., to transfer roles from english to another language via annotation projection or to learn an srl model from weak supervision where only annotations for dependency labels are available."
D19-1099,1,"for example, we can take an inspiration from either declarative constraints used in the previous work (punyakanok et al., 2008) or from literature on lexical semantics of verbs, studying patterns of event and argument realization (e.g., levin 1993)."
D19-1099,1,"for the future work, the structured refinement network can be further improved."
D19-1100,1,"in the future, we would like to explore ways to extend our method to languages not supported by google translate through the use of pivot languages."
D19-1101,1,"in future work, we plan to adapt active learning methods for easy deployment on crowdsourcing platforms, and to investigate techniques for automatically selecting good hyperparameters without recourse to a development set, which is often unavailable at the start of a crowdsourcing process."
D19-1102,1,"finally, while the techniques presented in this paper might be applicable to other low-resource languages, we want to also highlight the importance of understanding the characteristics of languages being studied. different language pairs might benefit from other types of similarity (e.g., morphological) and investigating this would be another interesting future work for low-resource dependency parsing."
D19-1103,2,"in the future, we plan to study the design and incorporation of fine-grained constraints considering multipule languages for cross-lingual transfer."
D19-1103,1,"we also plan to adapt this constrained inference framework to other cross-lingual structured prediction problems, such as semantic role labeling."
D19-1104,1,"some future directions include exploring the effects of contents in the memory, automating memory extraction from dataset, and improving the encoder."
D19-1104,1,we found that the bilstm encoders are likely bottlenecks for the model.
D19-1106,1,the original methodology presented in this paper paves the way for further computational work in quantifying parsing complexity and thus fine-grained modelling of human sentence processing.
D19-1108,4,as future work we will apply the framework for other text reading tasks.
D19-1108,1,another promising direction is to explore the benefits of text classification model in an edge-device setting.
D19-1109,3,"we also see potential benefit in the development of a more expansive set of evaluation methods for commonsense knowledge mining, which would strengthen the validity of our conclusions."
D19-1109,4,"in the future, we hope to explore whether this approach can be extended to mining facts that are not commonsense and to generating new commonsense knowledge outside of any given database of candidate triples."
D19-1110,1,"we hope our method will facilitate the development of “thin” nlp models, that are faster, consume less memory, and are interpretable (schwartz et al., 2019)."
D19-1112,1,future directions include integrating more sophisticated training strategies of meta-learning algorithms as well as validating our algorithms on other datasets.
D19-1113,4,"we seek to extend the use of par to other contextualized embeddings (devlin , 2019; mccann , 2017) in future work."
D19-1115,1,"while our work shows the potential for high-quality efficient steganography and the realizable optimality of arithmetic coding, future advancements in language modeling can push steganographic performance even further."
D19-1117,3,"besides, we will also conduct experiments on several other current summarization datasets like new york times (nyt) (paulus , 2018) and newsroom (grusky , 2018)."
D19-1117,1,further exploration on this and combination with other approaches like rl remains as our future exploration.
D19-1120,3,we plan to explore weighting methods to better evaluate the importance of translation pairs.
D19-1120,5,we will also study how to improve topic transformation with the topic link weight matrices.
D19-1121,6,"our next steps are to further study the correlation with time lags, and to incorporate more sophisticated event extraction techniques."
D19-1125,3,"in the future, we plan to experiment with other intermediate signals like dialogue acts."
D19-1125,1,"further improvements could potentially be obtained from employing more advanced regularization losses (oliver et al., 2018)."
D19-1128,4,"in the future, we would like to extend our negative sampling strategies to other tasks."
D19-1130,1,"a few directions for improvement have also been identified: 1) improving the performance on non-critical slots, 2) tuning the decoder with rl, 3) text generation from gcas."
D19-1132,4,"motivated by the promising success of our model in this short paper, we will apply it to other diverse nlp tasks in future work."
D19-1133,1,"we believe that the method can be extended and improved by examining other dimensionality reduction methods and alternatives to gmm, and by introducing other heuristics into the feature vector, such as sentence length and punctuation marks5."
D19-1138,4,"future directions include adding the languageadversarial task during bert pre-training on the multilingual wikipedia corpus, which may further improve zero-resource performance, and finding better stopping criteria for zero-resource crosslingual tasks besides using the english dev set."
D19-1139,4,"in future work, we plan to extend this method to unsupervised nmt (sun , 2019) and other natural language processing tasks, such as dependency parsing (li , 2018b) and reading comprehension (zhang , 2018b)."
D19-1139,4,"in future work, we plan to extend this method to unsupervised nmt (sun et al., 2019) and other natural language processing tasks, such as dependency parsing (li et al., 2018b) and reading comprehension (zhang et al., 2018b)."
D19-1140,4,"in future work, we will target new application scenarios, covering multi-class classification and regression tasks."
D19-1144,1,this algorithm has the potential in other incremental tasks such as streaming asr and incremental tts.
D19-1145,1,"furthermore, the structural position encoding can be also applied to the decoder with rnn grammars (dyer et al., 2016; eriguchi et al., 2017), which we leave for future work."
D19-1145,1,"future directions include inferring the structure representations from the amr (song et al., 2019) or the external smt knowledge (wang et al., 2017)."
D19-1146,3,"in the future, we will explore the utility of multistage fine-tuning for many-to-one and many-tomany nmt."
D19-1146,3,"we will also try to explicitly determine the impact of corpora sizes, language similarity, and domain on our approach, and propose further improvements according to our findings."
D19-1147,1,future work include designing more sophisticated architectures and combination strategies as well as validating our model on other language pairs and datasets.
D19-1148,4,"in future work, we plan to investigate the effectiveness of our approach in other types of induction tasks."
D19-1152,1,"our code will be publicly released, and we hope this will serve as a robust base for furthering progress on training visual dialog agents with rl for other multi-agent grounded language games, adapting to learn to talk about novel visual domains, etc."
D19-1153,5,this suggests an interesting future research direction toward data selection for cross-lingual transfer learning problems.
D19-1156,4,our method can be extended to evaluate other text generation tasks.
D19-1158,2,"finally, we also strive to acquire larger datasets."
D19-1158,1,"this might be achieved by using a composition sub-word representation for modifiers, such as character-level encoding."
D19-1158,1,"in future work, we intend to develop more accurate modifier representations to allow for better generalization to unseen modifiers."
D19-1159,1,"as both the components of press can be easily integrated, future models can consider building upon them as a strong baseline system."
D19-1160,5,"we obtain modest but positive improvements, which opens the question about how to increase the leverage of eye-tracking or other complementary data that is only available during training or comes from a different dataset."
D19-1161,1,"the results indicate that grammar induction with types is viable using recent neural network-based models, and our analysis warrants further exploration in this area."
D19-1162,2,"in future work, we hope to expand the size of convbank to improve its utility to researchers in training parsing models and studying conversational systems."
D19-1162,1,we also hope to improve the performance of our parsing system by jointly training dependency parsing with semantic role labeling.
D19-1163,1,"apart from standard accuracy improvement techniques such as better token embeddings and ensembling, possible future directions include a more fine-grained control of the recall trade-off, modeling the tokens outside the non-terminals instead of ignoring them, incorporating the parent’s embedding in edge scores, and a more efficient or approximate decoder similar to the greedy decoder from stern et al.(2017)."
D19-1165,1,we hope that this work would motivate further research into massively multitask and universal translation models.
D19-1166,1,our datasets and models thus provide a foundation to investigate strategies for a tighter control on output complexity in future work.
D19-1167,4,with this work we hope to inspire future work on understanding multitask and multilingual nlp models.
D19-1168,1,"in our future work, we plan to enrich our hm-gdc model to solve discourse phenomena such as (zero) anaphora."
D19-1169,5,"future studies on cross-lingual machine reading comprehension will focus on 1) how to utilize various types of english reading comprehension data; 2) cross-lingual machine reading comprehension without the translation process, etc."
D19-1169,1,experiments on two chinese machine reading comprehension datasets indicate that the proposed model could give consistent and significant improvements over various state-of-the-art systems by a large margin and set baselines for future research on clmrc task.
D19-1170,6,"as future work, we would like to consider handling additional types such as sorting or multiplication/division."
D19-1170,1,we also plan to explore more advanced methods for performing complex numerical reasoning.
D19-1171,2,"for instance, future work could extend upon this by using our methods to obtain more training data in cross-lingual qa setups (joty , 2017; ruckle , 2019b), or by combining them with other training strategies, e.g., using our methods for pre-training."
D19-1171,2,"for instance, future work could extend upon this by using our methods to obtain more training data in cross-lingual cqa setups (joty et al., 2017; ruckl ¨ e et al.´ , 2019b), or by combining them with other training strategies, e.g., using our methods for pre-training."
D19-1172,1,"in the future, we plan to improve our dataset by including more complex ambiguous situations for both single-turn and multi-turn questions, such as multi-hop questions, aggregation questions, etc."
D19-1172,1,"we also plan to integrate the clarification-based models into existing kbqa system, and study how to iteratively improve the model based on human feedback."
D19-1174,1,"directions for future work include devising approaches that perform sexism classification more accurately, enhancing the categorization scheme, and developing other ways to help counter sexism."
D19-1177,2,"enhancing the prediction of helpful reviews with unlabeled data: as a small proportion of reviews could be heuristically regarded as helpful or unhelpful, it thus becomes a promising study to automatically predict the helpfulness of online reviews based on the small amount of labeled data and a vast amount of unlabeled data."
D19-1177,4,"cross-domain helpfulness prediction of online reviews (chen , 2018b): given that it costs a lot on manually annotating a sufficient number of helpful reviews in a new domain, we should explore effective approaches on transferring useful knowledge from limited labeled samples in another domain."
D19-1178,2,"an immediate extension is to further scale up the experiments to web-scale datasets consisting of millions of users, as has been successfully done for face recognition (kemelmachershlizerman , 2016)."
D19-1178,4,we are also considering further applications of the proposed approach beyond those in this paper.
D19-1178,5,"it would also be interesting to explore community composition on the basis of the proposed embeddings (newell , 2016; waller and anderson, 2019)."
D19-1179,5,do architectures which transfer well across one style variable (e.g gender) generalize to other style variables (e.g age)?
D19-1179,5,studying these and other aspects of the content-style relationship in pastel could be an interesting direction.• does any external variable co-varying with the text qualify to be a style variable/facet?
D19-1179,5,what are the categories of style variables/facets?
D19-1180,2,we will also scale the tripod dataset and move to a multi-modal setting where tps are identified directly in video data.
D19-1180,1,"in future work, we will investigate the usefulness of tps for summarization and question answering."
D19-1182,2,"in future work, we aim to expand this study to multiple languages."
D19-1183,3,"apart from that, we will consider alternative evaluation criteria to account for rich surface variability of natural speech."
D19-1183,1,we will also explore ways to enable more efficient copying from the input which is crucial for correctly handling entities and therefore attaining high goal oriented performance of the system.
D19-1183,1,"in our own future work, we will try and find ways to improve the unsupervised representation (shi et al., 2019) in order to increase the transfer potential."
D19-1184,4,a particularly interesting application would be to generalize this method to language generation tasks.
D19-1184,4,"first, this method is general and broadly applicable, which suggests that it may improve performance on other tasks and domains."
D19-1184,6,"third, while this paper focuses on capturing multiple representations at different levels of granularity, it would be interesting to generalize mgt to learning multiple representations along several different axes (e.g., domains, styles, intents, etc.)."
D19-1184,1,"second, a useful improvement on top of mgt would be a more sophisticated method of combining the multiple granularities of representations."
D19-1186,1,"instead of specifying a dialog act manually, the most appropriate one can be decided automatically."
D19-1186,1,"In the future work, we will explore the act interactions with HRG."
D19-1188,4,we would like to explore the effectiveness of the approach regarding other structures in future work.
D19-1190,2,"in future work, we will use this technique to distill information from other nonparallel datasets, such as external informative text (qin , 2019; galley , 2019)."
D19-1193,1,"in the future, we will explore models to make better use of dialogue partners’ persona for response selection."
D19-1196,1,"due to the flexibility of our hierarchical encoder decoder framework and the cmr decoder, abundant future research direction remains as applying the transformer structure, incorporating open vocabulary and copy mechanism for explicit unseen words generation, and inventing better dialogue history access mechanism to accommodate efficient inter-turn reasoning."
D19-1199,1,"in the future, we will further investigate better utterance models associated with additional information such as topics or knowledge."
D19-1199,1,"with the help of learned addressee structure, we can build a general structural dialogue system for complex multiparty conversations."
D19-1200,5,"however, the error analysis shows that there are still challenges in dialogue generation, which we would like to explore in the future."
D19-1203,5,a potential direction for future work is to study how different game objectives interact with each other.
D19-1203,3,"future work should evaluate if our training scheme generalizes to a fully open-ended recommendation system, thus making our task not only useful for research and model development, but a useful end-product in itself."
D19-1204,5,its language and discourse diversity and crossdomain setting raise exciting open problems for future research.
D19-1204,5,"future work as discussed in section 5, some examples in cosql include ambiguous and unanswerable user questions and we do not study how a system can effectively clarify those questions or guide the user to ask questions that are answerable."
D19-1204,6,"we urge the community to investigate these problems in future work in order to build practical, robust and reliable conversational natural language interfaces to databases."
D19-1204,5,"its language and discourse diversity and cross domain setting raise exciting open problems for future research. future work as discussed in section 5, some examples in cosql include ambiguous and unanswerable user questions and we do not study how a system can effectively clarify those questions or guide the user to ask questions that are answerable. we urge the community to investigate these problems in future work in order to build practical, robust and reliable conversational natural language interfaces to databases."
D19-1206,1,"in the future we plan to include more domain using various domain-adaptive methods (qian and yu, 2019; tran and nguyen, 2018; gasi ˇ c et al.´ , 2017) to support multi-domain dialog system research, and incorporate our work into more and more standardized dialog system platforms (lee et al., 2019)."
D19-1210,1,"future work could incorporate better models of sequence within document context (kim et al., 2015; alikhani and stone, 2018)."
D19-1213,4,"furthermore, the proposed asgn has a good flexibility which can be employed to the other language and vision fields, such as image captioning, visual question answering, and so on."
D19-1215,2,"in future versions, talk2car will be expanded to include the above annotations and dialogues."
D19-1216,2,"in future work, we plan to extend the dataset with more examples, to try other features, e.g., from social media and from metadata,15 and to adapt the system to work with other languages."
D19-1216,3,we further plan experiments with fact-checking claims about videos.
D19-1218,6,an important direction for future work is to remove our full observability assumption.
D19-1218,1,"other future directions include experimenting with using the interaction history, expanding the learning example aggregation to error cases beyond incorrect start positions, and making agent reasoning interpretable to reduce user frustration"
D19-1220,4,"in our future work, we plan to extend the metric to other visualtext generation tasks such as storytelling."
D19-1220,1,the improvements with grounding models focusing on the latent correspondence between object-level image regions and descriptions will allow tiger to be further improved in the future.
D19-1221,1,"to enhance both the interpretability, as well as the attack stealthiness, future research can find grammatical triggers that work anywhere in the input. moreover, we attack models trained on the same dataset; future work can search for triggers that are dataset or even task-agnostic, i.e., they cause errors for seemingly unrelated models. in future work, we aim to both attribute and defend against errors caused by adversarial triggers."
D19-1223,1,further speedups are possible by leveraging more parallelism in the bisection algorithm for computing α-entmax.
D19-1225,1,"in future work, it may be worth extending this model to learn a latent manifold on content as well as style, which could allow for reconstruction of previously unseen character types, or generalization to other domains where the notion of content is higher dimensional."
D19-1226,1,"furthermore, our method is amenable be extended to contextualized embeddings (lauscher et al., 2019) and/or other wordnet lexical relations such as hypernyms and hyponyms."
D19-1227,1,"our results support the hypothesis on the interaction of metaphor and emotion, and suggest that it may be beneficial to incorporate a model of metaphor into emotion- and sentiment-related nlp applications in the future."
D19-1229,6,future work will investigate generation and validation of unseen color terms.
D19-1229,1,"finally, given that the divide between basic and secondary color terms is so blurred, future computational models of these should employ models of graded membership, such as fuzzy set theory."
D19-1230,1,"in the future, we intend to review the annotated guidelines for negative focus with the problematic annotation cases mentioned in this paper, and explore the ways to combine additional patterns and constraints from negative focus definition with neural network techniques to further improve negative focus detection."
D19-1234,3,"in future work, we will enrich our weak supervision system by giving the lfs access to more sophisticated contexts that take into account global structuring constraints in order to see how they compare to the simple, exogenous decoding constraints like mst."
D19-1234,1,we think we can put much more sophisticated decoding constraints that don’t just work off local probabilities of arcs but on the full information about the arc in its discourse context.
D19-1236,1,"in future experiments, more attention to the contribution of each author of the article might lead to further improvements in the prediction performance."
D19-1239,1,"the next step would be to interpret the semantics of the extracted slang adrs, and linking them to medical ontologies to allow for further structured analysis."
D19-1240,2,it would be also interesting to adopt rlta for other types of data.
D19-1240,5,one future direction is to generate privacy preserving text rather than embeddings.
D19-1240,1,a future direction is to apply different rl algorithms and investigate how it impacts results.
D19-1240,1,we also adopt deep q-learning to train the agent.
D19-1243,1,the substantial headroom (25.6%) between the best model performance and human encourages future research on contextual commonsense reasoning.
D19-1246,1,"other future directions include reducing the increased parameters in the proposed block circulant matrices, such as by using multiplicative l1 regularization for complex (manabe et al., 2018)."
D19-1247,3,"for future work, we investigate error cases by analyzing the error distributions of 100 examples.it implies that it is still intractable to evaluate generated questions."
D19-1247,3,"although we additionally evaluate on predicate identification and answer coverage, these metrics may be coarse and deserve further study."
D19-1248,2,"in the future, we will test our proposed framework on more datasets and investigate potential approaches to handle spurious logical forms for weakly-supervised kb-qa."
D19-1249,2,"first, we would like to create more parallel triples by adding more novels to make instances more balanced between the two languages."
D19-1249,5,"second, we want to create questions with non-extractive answers."
D19-1249,5,"third, we are also interested in adding multi-passage questions or questions based on the entire novels to bipar."
D19-1250,3,"moreover, assessing multi-token answers remains an open challenge for our evaluation setup."
D19-1250,1,"in addition to testing future pretrained language models using the lama probe, we are interested in quantifying the variance of recalling factual knowledge with respect to varying natural language templates."
D19-1250,1,"this suggests that while relation extraction performance might be difficult to improve with more data, language models trained on ever growing corpora might become a viable alternative to traditional knowledge bases extracted from text in the future."
D19-1251,5,how to incorporate more sophisticated symbolic reasoning abilities into mrc systems is also a valuable future direction.
D19-1251,1,"in the future, we will explore the following directions: (1)as we use a pre-defined reasoning graph in our model, it is incapable of handling reasoning process which involves intermediate numbers that not presented in the graph."
D19-1251,1,"therefore, how to combine both of their abilities to develop a more powerful numerical mrc model is an interesting future direction.(3) symbolic reasoning plays a crucial role in human reading comprehension."
D19-1251,1,"how to incorporate dynamic graph into our model is an interesting problem.(2) compared with methods proposed for arithmetic word problems (awps), our model has better natural language understanding ability."
D19-1251,1,"our work integrates numerical reasoning, which is a special case of symbolic reasoning, into traditional mrc systems."
D19-1255,1,"in future work, we will explore even tighter integration of symbolic knowledge and stronger reasoning methods."
D19-1257,2,"a promising future direction would be to include additional external knowledge such as commonsense and world knowledge, and learn all annotations jointly with the downstream task."
D19-1258,1,the work can give general guidelines on mrs modeling and inspire future research on the relationship between semantic retrieval and downstream comprehension in a joint setting.
D19-1259,1,learning a harder but more informative auxiliary task of long answer generation might lead to further improvements.
D19-1259,1,(2) we use binary bow statistics prediction as a simple demonstration for additional supervision of long answers.
D19-1259,5,"there are several interesting future directions to explore on pubmedqa, e.g.: (1) about 21% of pubmed qa contexts contain no natural language descriptions of numbers, so how to properly handle these numbers is worth studying;"
D19-1263,5,"in future work, we plan to add support for other complex questions whose queries require union, group by, or numerical comparison."
D19-1263,1,"also, we are interested in mining natural language expressions for each query substructures, which may help current parsing approaches."
D19-1264,4,"we would also like to research how to utilize gat, msr and mrr into other kg related tasks, such as kg representation, relation clustering and kb-qa."
D19-1264,1,"in the future, we are interested in utilizing multi-task learning, to enable the model to learn reasoning paths for several query relations simultaneously."
D19-1265,5,"in the future, we will try to investigate how to update kgs when entities are involved in several successive events."
D19-1267,1,"in the future, we will further investigate the effect of the network depth on sentence matching and explore introducing external knowledge, such as pre-trained language model bert (devlin et al., 2018) and paraphrase database (ganitkevitch et al., 2013), to help learning more accurate and robust sentence representation."
D19-1268,1,"(2) we will explore other techniques to fuse the ordered relation information from different paths (liu et al., 2019)."
D19-1268,4,"in the future, we will explore the following research directions: (1) we will study the applications of the proposed models in various domains, like personalized recommendation (liu et al., 2018);"
D19-1272,3,"lastly, our human evaluation is limited in size and a larger and more diverse participant pool is needed."
D19-1272,1,"potential directions for future work include adding specific methods to control sentiment, and fine-tuning smerti for preservation of persona or personality."
D19-1272,1,"experimenting with other text infilling models (e.g. fine-tuning bert (devlin et al., 2019)) is also an area of exploration."
D19-1273,2,"in addition, we plan to create larger human-curated datasets with variable size scenarios."
D19-1273,1,"our current model sometimes gets misled by superficially similar sentences, and it will be an important future direction to move towards deeper reasoning for the task."
D19-1274,1,"besides, iteratively discovering new entity alignments based on the framework of kecg is another interesting direction."
D19-1274,1,"for future work, we will extend the knowledge embedding model of kecg to other kg representation learning methods, such as transd (ji et al., 2015), to gain a stronger ability of modeling relationships."
D19-1276,2,"one possibility that does not require supervised data is to create artificial tasks, such as reproducing the input sentence or predicting missing parts of the input (such as affixes and function words)."
D19-1277,1,"in a broader perspective, we hope that future studies on dependency parsing will take the results obtained here into account and extend them by investigating other parsing approaches and neural network architectures."
D19-1277,1,"indeed, given the rapid development of new representations and architectures, future work should include analyses of how all components in neural parsing architectures (embeddings, encoders, decoders) contribute to distinct error profiles (or lack thereof)."
D19-1278,1,"we also plan to explore modelling alternatives, such as taking different graph generation orders into account (bottom-up vs. top-down) as well as predicting the components of a fragment (type, number of edges, edge labels) separately."
D19-1278,1,"future work should focus on extending our parser to other formalisms (amr, mrs, etc.)."
D19-1280,5,"future work: based on our present efforts to tackle qa it, we propose the following directions for future work. sometimes attributes require long procedures to verify, and thus, we believe that denser rewards would help with this problem."
D19-1280,1,one possible solution is to provide intermediate rewards whenever the agent achieves a sub-task.
D19-1281,4,"this work opens up the possibility of focusing on other kinds of knowledge gaps and extending this approach to other datasets and tasks (e.g., span prediction)."
D19-1282,1,"future directions include better question parsing methods to deal with negation and comparative question answering, as well as incorporating knowledge to visual reasoning."
D19-1285,4,"in parallel, the present work could be extended to other gas and threshold functions."
D19-1285,5,"in the long term, we aim to move to natural images."
D19-1285,1,"moreover, it might be worth exploring whether equipping models with similar inductive biases as those leading speakers of any language to develop abstract, compositional representations of size adjectives is needed to properly handle these expressions."
D19-1285,1,"an interesting open question, which we plan to explore in future work, is whether training models to jointly learn superlative, comparative, and positive gas (similarly to how pezzelle et al.(2018) did for quantities), or framing the task in a dialogue setting (as monroe et al.(2017) did for colors) could lead to more compositional models."
D19-1287,5,it remains an open question why all but one of the models tested were unable to leverage the numerous examples of gender agreement seen in various contexts during training to drive correct subject/predicate expectations.
D19-1289,6,"third, in addition to understanding what goes into an explanation, we need to understand what makes an explanation effective."
D19-1289,1,"second, it is important to connect the words in explanations that we investigate here to the structure of explanations in psychology (lombrozo, 2006)."
D19-1289,1,"a better understanding of explanations not only helps develop explainable ai, but also informs the process of collecting explanations that machine learning systems learn from (hancock et al., 2018; rajani et al., 2019; camburu et al., 2018)."
D19-1289,1,"first, although /r/changemyview has the useful property that its explanations are closely connected to its explananda, it is important to further investigate the extent to which our findings generalize beyond /r/changemyview and reddit and establish universal properties of explanations."
D19-1290,4,future work on framing will focus on its application to down-stream argument mining tasks such as analyzing argument quality.
D19-1290,5,especially interesting is whether specific frames are expected to persuade an audience.
D19-1290,5,a clear problem here is how to label a frame given its arguments in order to deliver short labels for the user.
D19-1290,5,a follow-up question will be whether frames in an argumentative text should be delivered in a specific sequence to achieve the persuasion of an audience.
D19-1291,2,"in the future, we plan to experiment further with language model fine-tuning on other sources of data."
D19-1291,1,"as the rst parser is not perfect, we want to incorporate other features from these trees that allow us to better recover from errors."
D19-1291,1,we also plan to investigate whether this approach can be extended to comparing programs: in this casenon-alpha numeric symbols would be candidates for fingerprinting.
D19-1292,1,we hope that the findings and metrics that we present in this paper help continue the discussion on improving the robustness of models and enable better comparisons between adversarial attacks to be made that consider instance correctness.
D19-1293,1,"therefore, we leave it as future work to develop a method that is effective even when workers are aware of quality control methods that employ nonsensical reasons, possibly by making the nonsensical reasons change automatically between annotations."
D19-1294,4,we would also like to extend the work to other discourse phenomena.
D19-1294,6,"in future work, we want to handle cases where multiple pronouns are equally suitable in a given context."
D19-1295,1,"in the future, we will identify new types of commonsense knowledge for further improving the performance of discourse parsing."
D19-1295,1,"for example, antonyms (e.g., warm vs. cold) can directly indicate a contrast relation between two situations, and this type of knowledge has potential to further improve the performance on comparison discourse relations."
D19-1298,3,"as for evaluation, we would like to elicit human judgments, for instance by inviting authors to rate the outputs from different systems, when applied to their own papers."
D19-1298,5,"more long term, we will study how extractive/abstractive techniques can be integrated; for instance, the output of an extractive system could be fed into an abstractive one, training the two jointly."
D19-1298,6,"furthermore, we would like to explore more sophistical structure of documents, like discourse tree, instead of rough topic segments."
D19-1298,1,"more generally, we plan to combine of traditional and neural models, as suggested by our results."
D19-1298,1,"for future work, we initially intend to investigate neural methods to deal with redundancy."
D19-1298,1,"then, it could be beneficial to integrate explicit features, like sentence position and salience, into our neural approach."
D19-1299,2,"besides, we are also interested in integrating external knowledge into other types of datasets beyond wikipedia infobox-to-text datasets."
D19-1299,4,it will be worthwhile especially when we extend the task to multiple sentence generation.
D19-1299,5,the main challenge in integrating multi-hops knowledge graph is the large search space.
D19-1299,1,"in the future, we plan to investigate integrating multi-hops knowledge graph behind the data which has potential to further improve the inference ability of neural models."
D19-1299,1,we plan to employ reinforcement learning based techniques to allow the model to search the optimal inference paths by trial and error.
D19-1303,4,"future work can be improving the sensationalism scorer and investigating the applications of dynamic balancing methods between rl and mle in textgan(yu , 2017)."
D19-1303,5,"our work also raises the ethical questions about generating sensational headlines, which can be further explored."
D19-1305,1,"in future work, we plan to explore the development of a joint model that simultaneously handles morphological realization and word ordering while using finer grained word representations, such as fasttext embeddings (bojanowski et al., 2017) or byte pair encoding (bpe; gage, 1994; sennrich et al., 2016)."
D19-1307,4,"for future work, we plan to test our method in other summarisation tasks (e.g.multi-document summarisation) and downstream tasks of summarisation (e.g.investigating whether users can correctly answer questions by reading our summaries instead of the original documents)."
D19-1307,4,"also, we believe the “learning reward from human judgements” idea has potential to boost the performance of rl in other natural language generation applications, e.g. translation, sentence simplification and dialogue generation."
D19-1308,4,future work involves incorporating selector for other generation tasks such as diverse image captioning.
D19-1309,4,"in future work, we intend to accelerate the training of our encoders and decoders with the techniques in (huo , 2018) and apply our architecture and training techniques to other nlp tasks."
D19-1312,1,one future direction is to model larger contexts with this approach.
D19-1312,1,"it would also be interesting to integrate our model into an existing language generation system (e.g., as part of an abstractive summarization system)."
D19-1313,3,"in the future, we would like to apply our framework to increase reference texts for automatic evaluation or augment training data for text classification."
D19-1314,1,"in the future, we will consider integrating deep generative graph models to express probabilistic dependencies among amr nodes and edges."
D19-1315,4,"as a future work, we would like to explore whether our method is applicable to other tasks such as multi-task learning for object detection and image caption generation."
D19-1316,3,we will also investigate how we can efficiently evaluate generation results.
D19-1316,5,"in future work, we will investigate how we can generate more flexible feedback comments."
D19-1316,1,"one way of achieving it is to apply a more sophisticated retrieval-based method such as (hashimoto et al., 2018) to this task."
D19-1317,5,"for future work, there are some interesting dimensions to explore, such as difficulty levels (gao , 2019a), paragraph-level information (zhao , 2018) and conversational question generation (gao , 2019c)."
D19-1317,5,"for future work, there are some interesting dimensions to explore, such as difficulty levels (gao et al., 2019a), paragraph-level information (zhao et al., 2018) and conversational question generation (gao et al., 2019c)."
D19-1319,5,"another direction of the future research, however, lies in developing novel methods that distinguish the type of reviews described in the paper and organic reviews."
D19-1319,6,"as a part of the future work, we would like to improve the review generation process in a way that could receive several key words from users as input and generate reviews based on these prior information."
D19-1321,4,our planning-based model may be inspiring to other long text generation tasks such as long text machine translation and story generation.
D19-1326,1,"as a future work, we would like to extend refnet to have the ability to decide whether a refinement is needed on the generated initial draft."
D19-1327,2,different domains of datasets except for news articles pose new challenges to the appropriate design of summarization systems.
D19-1327,3,similar studies can be applied to other aspects and their combinations in various systems and different domains of corpora.• one can repeat our bias study on evaluation metrics.
D19-1327,6,"a more theoretical study of summarization, and the various aspects, is required."
D19-1327,1,developing such bias-free or robust models can be very important for future directions.• nobody has clearly defined the deeper nature of meaning abstraction yet.
D19-1330,2,"for the future work, we plan to extend our method on more than two target languages and explore other effective interactive approaches to improve the translation quality further."
D19-1332,6,we hope that this study will inspire further research on temporal commonsense.
D19-1332,1,our analysis sheds light on the capabilities as well as limitations of current models.
D19-1332,1,"we find that systems equipped with state-of-the-art language models such as elmo and bert are still far behind humans, thus motivating future research in this area."
D19-1333,1,"in the future, we will investigate more methods for reducing the limitations of qa infomax and improving the capability of generalization in qa systems."
D19-1335,5,"with the release of an increasing number of finegrained inference tasks aimed at these abilities (roemmele , 2011; morgenstern , 2016; wang , 2018; rashkin , 2018; mccann , 2018), the issue of experimental validity in csr will also become even more important."
D19-1335,5,"with the release of an increasing number of finegrained inference tasks aimed at these abilities (roemmele et al., 2011; morgenstern et al., 2016; wang et al., 2018; rashkin et al., 2018; mccann et al., 2018), the issue of experimental validity in csr will also become even more important."
D19-1336,4,"pun-gan is generic and flexible, and may be extended to other constrained text generation tasks in future work."
D19-1337,4,"in future work, we will adopt the auxiliary language modeling task to other neural generation systems to test its generalization ability."
D19-1339,1,"in this paper, we use manually selected keywords and phrases to generate text, which, while an appropriate scope to quantify the biases that appear in nlg systems, could be expanded to more automatic methods and help generalize our findings."
D19-1340,4,"since our approach is implemented as a data pre-processing step, it can potentially help any neural method that learns from text and is likely to be used out of domain."
D19-1341,2,"moving forward, we suggest using our symmetric dataset in addition to the current retrieval-based fever evaluation pipeline."
D19-1343,1,future work will conduct more detailed analysis to continue enhancing the proposed method.
D19-1344,1,"in the future, we will go deeper into the taxonomy and try to explore the hierarchical relations between labels and improve the scalability of models for finer grained label sets."
D19-1346,4,"in the future, we plan using the same approach to tackle similar problems, including lexical selection in machine translation, or word sense disambiguation (lala and specia, 2018)."
D19-1346,4,"tasks that lie at the intersection of computer vision and nlp, such as the challenges posed in the new breakingnews dataset (popularity prediction, automatic text illustration) could also benefit from our results (ramisa , 2018)."
D19-1346,4,"we believe our approach could also be useful in multimodal machine translation, where an image caption must be translated using not only the text but also the image content (barrault , 2018)."
D19-1348,2,"performance improvements are expected as these features are incorporated, and as additional labeled data is created."
D19-1348,4,"visual region detection can serve as a precursor to numerous existing information extraction techniques or adaptations of them, including parsing of reference (lammey, 2015), tables (rastan , 2015), and equations (smithies , 2001), as well as data extraction from figures (tummers, 2006)."
D19-1348,5,and so it is valuable to all these efforts to be able to accurately and quickly segment document pages into regions of interest.
D19-1348,1,alternative detection frameworks are also being explored.
D19-1348,1,"in ongoing work, additional contextual features are being explored, including roi oversampling (i.e. looking at a region’s surroundings), random feature map sampling (i.e. looking at patterns in a whole page or whole article), and all-region positional information (i.e. the position of other predicted rois in an article)."
D19-1349,2,"for future work, we intend to work with larger datasets to investigate neural solutions to combine features from different metrics, as well as to apply our findings to other variants of lda models trained on low-resource languages, where high-quality external corpora are usually not available (hao , 2018)."
D19-1350,1,"in future work, we will explore extending our model for temporal topic modelling."
D19-1351,4,"although our method has been explored in the context of systematic reviews, it could be applied to a variety of other document retrieval problems."
D19-1352,3,"important future work includes more detailed analyses of these transfer effects, as well as a closer look at the contributions of document-level and sentence-level scores."
D19-1353,1,adapting neural models for use in an mt-ir setting and addressing the properties of mt output most harmful to their performance are promising avenues for future work to enable low-resource clir.
D19-1356,2,"for future work, we will further improve the quality of phrasing knowledge or incorporate other concept-level knowledge in text-to-sql."
D19-1357,1,"in our future work, we will extend our methods for phrase-level definition generation (ishiwatari et al., 2019)."
D19-1358,1,"thus, it would be an attractive direction to take as future work developing an end to-end system."
D19-1359,3,iii) establish a common evaluation framework to compare the contribution of lexical-semantic combination resources;
D19-1359,4,"iv) employ and assess syntagnet in other nlp tasks, such as word and sense similarity (navigli and martelli, 2019) or semantic role labeling (where a newly released wordnet-linked resource, verbatlas (di fabio , 2019), would greatly benefit from collocational information)."
D19-1359,1,"as future work, we plan to: i) enrich syntagnet with more combinations, so as to surpass supervised english wsd;"
D19-1359,1,ii) include information from adjectives;
D19-1362,4,"further, the feature-dependent confusion matrices are taskindependent and could be used for other nlp tasks, which is one possible direction of future research."
D19-1363,1,a possible future research direction would involve finding efficient optimization methods where the orthogonality constraint could be slightly relaxed.
D19-1366,3,"also, designing proper evaluation metrics is still an open problem for text style transfer."
D19-1366,1,"although our model achieves better content preservation, the general quality of our transferred sentences can be further improved."
D19-1367,4,we plan to consider the network density problem in search and apply i-darts to more tasks in our future study.
D19-1368,1,"applying jobi to non-bilinear models is also possible, but left for future work."
D19-1371,2,"because these language models are costly to train, we aim to build a single resource that’s useful across multiple domains."
D19-1371,2,"for future work, we will release a version of scibert analogous to bert-large, as well as experiment with different proportions of papers from each domain."
D19-1375,1,we believe our method can also be applied to slu systems with other semantic representations (e.g. semantic frame).
D19-1377,1,"our dataset can serve as a starting point for further research on this task, which can be beneficial to the investigation of chinese qa and dialogue models."
D19-1379,4,another line of our future work is to apply these transductive methods to various nlp tasks and investigate their performance.
D19-1379,4,one interesting line of future work is to explore effective transductive methods for task-dependent (neural) layers.
D19-1379,1,"for instance, as some unsupervised domain adaptation methods can be applied to transudative learning, integrating them with transudative lm fine-tuning may further improve their performance."
D19-1381,4,"in addition, it can also be applied to other dag structures such as nested/discontiguous entities (muis and lu, 2016; ju , 2018)."
D19-1381,5,these results set the first focussed benchmark of our model and next steps include applying it to other event datasets in the biomedical and general domain.
D19-1381,1,further analyses on the development set revealed some desirable characteristics of the model such as its computational efficiency while yielding higher f1-score performance.
D19-1382,2,it also leaves considerable headroom as a new challenging benchmark to drive multilingual research on the problem of paraphrase identification.
D19-1383,1,"for future work, we would like to explore methods for better encoding long sequences using pretrained language models."
D19-1384,5,"the setting in this paper included a very simple vision task, and we suspect emergent linguistic phenomena would be more pronounced and even more interesting to study in more sophisticated settings."
D19-1384,1,"in future work, it would be useful to investigate more complicated environments and more complex agent interactions."
D19-1386,1,"our experimental results demonstrate that the topic and partial summary help the extractive and abstractive models, but the task remains a significant challenge with room for improvement in future work."
D19-1387,1,"although we mainly focused on document encoding for summarization, in the future, we would like to take advantage the capabilities of bert for language generation."
D19-1390,1,"by integrating off-the-shelf knowledge bases to clearly model the transition relation embedding, it should further improve the interpretability and might be especially helpful under low-resource settings, which we leave for future work."
D19-1392,3,"in the future, we hope to explore its potential in crossframework and cross-lingual semantic parsing."
D19-1393,6,it is interesting to explore the use of some revision mechanisms when the initial steps go wrong.
D19-1395,2,"in the future, we will explore other useful information (e.g., correlations among the relations from kgs) available to improve the label embeddings."
D19-1397,6,all of these findings support our argument that shifted label distribution can severely hinder model performance and should be handled properly in future research.
D19-1397,1,we hope that the analysis presented will provide new insights into this long-overlooked factor and encourage future research of creating models robust to label distribution shift.
D19-1397,1,"based on these observations, we suggest that in addition to label noise, more attention be paid to the shifted label distribution in distantly supervised relation extraction research."
D19-1397,1,we also hope that methods such as threshold techniques and bias adjustment become useful tools in future research.
D19-1399,1,"as statistics shows that most of the entities form subtrees under the dependency trees, future work includes building a model for joint ner and dependency parsing which regards each entity as a single unit in a dependency tree."
D19-1400,2,"in future work, we would like to generalize our approach to the multilingual case of multiple languages and a multi-class target task, explore applications beyond text classification, and study transformations that eliminate the need for a translation system."
D19-1405,4,"our framework is easily extensible to other domains with rich output structure, e.g., entity relation extraction, and multilabel classification."
D19-1409,1,this may affect the design of objectives for the next generation of nlg models.
D19-1412,4,"for future work, we would like to validate our model on other tasks such as response generation, explore more effective unsupervised sequence-tosequence pre-training methods, and handle crosslingual tasks such as machine translation."
D19-1413,2,"in the future, we would like to abstract multi-view data from multi-lingual and multi-modal sources and investigate the effectiveness of av-kmeans on a wider range of tasks in the multi-lingual or multi-modal settings."
D19-1415,1,"argumentation theory, applied to the active landmark semantics and the source input example as captured by the kernel, provides a very rich framework to design future and more complex justification mechanisms."
D19-1416,1,"one research direction is to explore how to generate long-form text with deep generative models organized in multiple layers of latent variables, due to natural language characterizing a hierarchical structure."
D19-1417,1,"to list a few: (a) a deeper look into the nature of sampled data - their distribution in the feature space, as well as their importance for the task at hand; (b) the creation of surrogate datasets for a variety of applications, including hyperparameter and architecture search, etc; (c) an extension to other deep models (beyond ftz) and beyond classification models; and, (d) an extension to semi-supervised, online and continual learning."
D19-1418,4,"future work includes learning to automatically detect dataset bias, which would allow our method to be applicable with less specific prior knowledge."
D19-1421,4,further research should investigate the potential gains of using γ-divergences for appropriate downstream tasks.
D19-1423,1,"we hope that our work is a stepping stone towards models that are robust against an even wider, harder-to characterize space of possible attacks."
D19-1424,5,"a further understanding of how multi-task training with bert (liu , 2019b) improves fine-tuning, and how it affects the geometry of loss surfaces are worthy of exploration, which we leave to future work."
D19-1424,1,"in addition, we would like to apply the proposed methods for other pretrained models."
D19-1424,1,"moreover, the results motivate us to develop fine-tuning algorithms that converge to wider and more flat optima, which would lead to better generalization on unseen data."
D19-1425,4,"we plan to evaluate our proposed solution on other tasks where topics can be latent confounds, like predicting gender bias (voigt , 2018)."
D19-1426,4,"future work can also extend the approach to other supervised learning tasks, as well as bootstrap from natural dialog data."
D19-1427,3,"our proposed evaluation framework and dataset will facilitate future work by providing the ability to meaningfully compare the performance of different methods to each other, an ability that was sorely missing in previous work."
D19-1430,4,"for future work, we would like to verify our training strategy on more language pairs and other sequence-to-sequence tasks."
D19-1430,1,"furthermore, we are interested in studying our noised training with other data augmentation approaches."
D19-1431,2,we may consider obtaining more valuable information about sparse entities in few-shot link prediction in kgs in the future.
D19-1433,4,"we are also interested to more thoroughly explore how to combine domain-adaptive and task-specific fine-tuning within the framework of continual learning (yogatama , 2019), with the goal of balancing between these apparently conflicting objectives."
D19-1434,5,"with the methods presented here, an interesting direction for future work is to further examine why certain examples are more difficult than others."
D19-1434,3,"having difficulty and ability estimates for machine learning data sets and models can lead to very interesting work around such areas as active learning, curriculum learning, and meta learning."
D19-1435,4,in the future we plan to apply the pie model to more ambitious transduction tasks like translation.
D19-1437,1,one potential direction for future work is to leverage iterative refinement techniques such as masked language models to further improve translation quality.
D19-1437,1,"another exciting direction is to, theoretically and empirically, investigate the latent space in flowseq, hence providing deep insights of the model, even enhancing controllable text generation."
D19-1438,4,"we hope this work opens a path to encourage machines learn quickly and generalize widely like humans do, and to make machines more helpful in various tasks."
D19-1439,2,we hope that the community will make use of our released wikicrem dataset to further improve the pronoun resolution task.
D19-1440,1,"future work will concentrate on extending the model to cope with relative attributes, the inclusion of additional data sources to increase model coverage, such as large-scale definition sets."
D19-1441,4,we will also investigate patient-kd in more complex settings such as multi-task learning and meta learning.
D19-1441,1,designing more sophisticated distance metrics for loss functions is another exploration direction.
D19-1441,1,"for future work, we plan to pre-train bert from scratch to address the initialization mismatch issue, and potentially modify the proposed method such that it could also help during pre-training."
D19-1443,4,we hope our empirical study may potentially allow others to design better attention mechanisms given their particular applications.
D19-1445,2,"one of the directions for future research would be to study self-attention patterns in different languages, especially verb-final langauges and those with free word order."
D19-1446,2,"besides, we will study using weakly paired documents from other data resources, such as news websites."
D19-1446,2,"for future work, we will apply our method to more other language pairs."
D19-1446,1,"furthermore, we will investigate better ways to utilize such weakly paired documents, going beyond mining sentence pairs and aligning topic distributions."
D19-1449,1,"future work should move beyond the restrictive assumption by exploring new methods that can, e.g., 1) increase the isomorphism between monolingual spaces (zhang et al., 2019) by distinguishing between language-specific and language-pair invariant subspaces;"
D19-1449,1,2) learn effective non-linear or multiple local projections between monolingual spaces similar to the preliminary work of nakashole (2018);
D19-1449,1,3) similar to vulic and korhonen ´ (2016) and lubin et al.(2019) “denoisify” seed lexicons during the self-learning procedure.
D19-1449,3,"finally, this paper demonstrates that, in order to enable fair comparisons, future work on clwes should focus on evaluating the clwe methods’ constituent components (e.g, components c1-c3 from this work) instead of full-blown composite systems directly."
D19-1450,1,extending our concept-based cross-lingual mapping to contextualized word representations will be the focus of our future work.
D19-1451,2,"in the future, we would like to apply our methods to other multilingual datasets such as yago and babelnet."
D19-1451,1,"also, since literal descriptions of entities are not always available, we will investigate alternative ways to design graph-based models that can better capture structured knowledge for this task."
D19-1452,4,"in future, we will extend our language adaptation model to document retrieval and check-worthy claim detection tasks."
D19-1453,1,"to train our model in a single run, we would like to investigate a training method which alternates between alignment extraction and model training."
D19-1456,5,this notion of fairness allowed us to rigorously pose the question of whether specific nli models can learn to do robust natural logic reasoning.
D19-1457,5,"in addition, given additional hand-labeled training data, a system might learn to directly predict dependency links (instead of deriving them heuristically from state changes)."
D19-1458,1,"we hope that by using this benchmark suite, progress can be made in building more compositional, modular, and robust nlu systems."
D19-1460,2,the data collection and annotation methodology that we use to gather multidogo can efficiently scale across languages.
D19-1460,3,several pilot experiments aimed at collecting spanish dialogues in the same domains have shown preliminary success in quality assessment.
D19-1461,4,"future work could consider classes of offensive language separately (zampieri , 2019), or explore other dialogue tasks, e.g.from social media or forums."
D19-1461,1,"another interesting direction is to explore how our build it, break it, fix it strategy would similarly apply to make neural generative models safe (henderson et al., 2018)."
D19-1462,4,we would also like to adapt the proposed methods to document-level neural machine translation in the future.
D19-1462,1,we will further improve our model by incorporating syntactic and location information.
D19-1464,6,"second, domain knowledge can be incorporated."
D19-1464,1,we plan to design a specific graph neural network that takes into consideration the edge labels.
D19-1464,1,"last but not least, the asgcn model may be extended to simultaneously judge sentiments of multiple aspects by capturing dependencies between the aspects words."
D19-1466,4,"in the future, the proposed sal method can be potentially extended to other domain adaptation methods and applied to more general sequence labeling tasks including named entity recognition (zhou , 2019c), part-of-speech tagging (zhou , 2019a), etc."
D19-1468,4,"in future work, we plan to extend our framework to multi-task settings, and to incorporate interaction to learn better seed words."
D19-1469,2,"of course our new dataset and the baseline classifier models are just a preliminary effort, and future work will need to examine larger datasets, consider additional data such as hashtags, richer classification schemes, and more sophisticated classifiers.expanding our taxonomies with richer sets like these is an important goal."
D19-1469,1,"nonetheless, the fact that we found multimodal classification to be most helpful in cases where the image and text diverged semiotically points out the importance of these complex relations, and our taxonomies, dataset, and tools should provide impetus for the community to further develop more complex models of this important relationship."
D19-1471,1,there are several important directions remain for future research: (1) the fusion mechanism of private and shared features; (2) how to represent meta-data of fake news better to integrate into inputs.
D19-1472,3,"future research should evaluate the appropriateness of the framework to probing moral change from a diverse range of cultures and linguistic backgrounds, and the extent to which moral sentiment change interacts and crisscrosses with linguistic meaning change and lexical coinage."
D19-1473,3,"we will then be able to accurately identify inappropriate causal language use in weaker study design types, and also to compare among countries and languages in each specific study design types."
D19-1473,1,"toward this new goal, we also plan to develop a classification method to identify study types in other domains such as psychology and education."
D19-1473,1,"in the future we will further our investigation by developing prediction models to automatically identify study designs in more fine-grained categories, such as cross-sectional, case-control, retrospective cohort, and prospective cohort."
D19-1473,4,We also plan to extend our work to other domains in the future.
D19-1474,2,"the different annotation labels and comparable corpora would help us perform transfer learning and investigate how multimodal information on the tweets, additional unlabeled data, label transformation, and label information sharing may boost the classification performance in the future."
D19-1477,1,"in future work, we plan to perform a deeper investigation of cases in which social information does not prove beneficial, and to assess the ability of our model to dynamically update the representation of the same author in different contexts, a task that, due to the nature of the data, was not possible in present work."
D19-1480,1,we plan to incorporate temporal states to capture location changes in future work.
D19-1480,1,"for future work, we would like to explore ways to combine graph-level classification methods and our user-level learning model."
D19-1480,1,"besides, our model assumes each post of one user all comes from one single home location but ignores the dynamic user movement pattern like traveling."
D19-1481,5,"it would be exciting to consider finer-grained forecasting of conversational trajectories, accounting for the natural—and sometimes chaotic—ebband-flow of human interactions."
D19-1482,1,we intend to make our dataset freely available to facilitate further exploration of hate speech intervention and better models for generative intervention.
D19-1484,2,our future plans include extending this dataset with conversational context and investigating additional aspects of online written multilingual discourse in richer contexts.
D19-1484,5,"the released dataset suggests a wealth of both intra- and inter-sentential code-switched utterances, and investigation of sociolinguistic differences between the two introduces an interesting research question."
D19-1485,2,"in future work, we shall explore to incorporate external context (derczynski , 2017; popat , 2018), and extend our model to multi-lingual scenarios (wen , 2018)."
D19-1485,1,"moreover, we shall investigate the diffusion process of rumors from social science perspective (vosoughi et al., 2018), draw deeper insights from there and try to incorporate them into the model design."
D19-1485,2,"in future work, we shall explore to incorporate external context (derczynski et al., 2017; popat et al., 2018), and extend our model to multi-lingual scenarios (wen et al., 2018)."
D19-1486,4,we also plan to apply our model in dialogue systems for low-resource languages such as cantonese.
D19-1486,1,one is to customize our model for few-shot intent classification.
D19-1486,1,another is to extend our framework to deal with multiple-intent classification.
D19-1487,4,"as future work, we will work along this line and investigate the design interpretable solutions to domain adaptation for person-job fit."
D19-1487,1,we will also consider how to model the domain relationship for effectively transferring information across domains.
D19-1491,1,the novel architecture performs simplification recursively and can be adapted to different levels of language complexity in the future;
D19-1491,3,"in the future, we plan to extend this work with a syntactic simplification component and test the architecture extrinsically with non-native readers."
D19-1492,1,in the future we would also like to explore using both i2b2 and clvc together to see if a multi-task learning approach would provide additional improvements.
D19-1493,5,"the second one is how to incorporate the activities of users on social media platforms, since the opinions, experiences and events shared by users on social media are usually useful clues to indicate their interests."
D19-1493,1,"the first one is how to incorporate the interactions between different kinds of user behaviors, since the relatedness of them may be useful for modeling the interest evolution of users."
D19-1493,1,"the third one is how to incorporate language models to generate context-aware word embeddings, which may enhance the representation learning of contexts in news and other kinds of user generated content."
D19-1494,2,"second, since there are also other kinds of user and item information, we will explore how to incorporate heterogeneous user and item information to benefit recommendation."
D19-1494,1,"third, we will work on how to reduce the computational cost of our approach, which can improve the applicability of our approach."
D19-1494,1,"first, since we only use the local neighbors of nodes in the user-item graph, we will explore the use of other kinds of graph neural networks to further enhance the learning of users and items."
D19-1497,1,"as future work, we will consider how to extend our work by modeling sentiments of review text."
D19-1498,4,we hope that this study will inspire the community to further investigate the usage of edgeoriented models on re and other related tasks.
D19-1498,1,"as future work, we plan to improve the inference mechanism and potentially incorporate additional information in the document-graph structure."
D19-1500,4,"we hope this contribution leads to multidisciplinary efforts to precisely understand user intent and reconcile it with information in policy documents, from both the privacy and nlp communities."
D19-1501,6,"in the near future, we aim to fully prevent the generation of unfaithful descriptions and bring fpdg online."
D19-1504,1,"we hope that our work, built upon the theoretical insights on spectral inference, provide a complete guide to correlated topic modeling for both researchers and practitioners."
D19-1505,4,"we also hope to apply our modeling approach to practical downstream applications, including: i) detecting completed to-dos based on related edits; ii) localizing the paragraphs that could be edited to address a given comments; iii) summarizing document revisions."
D19-1505,1,"in future work we plan to explore sequences of revisions through the lifecycle of a document from creation to completion, with the ultimate goal of modeling document evolution."
D19-1506,4,"in the future, we want to extend this approach to more natural language tasks and languages."
D19-1507,4,they both are also possible applications of the subword-level modeling.
D19-1507,4,"query suggestion (sordoni et al., 2015; dehghani et al., 2017) and query reformulation (nogueira and cho, 2017) are related to qac and well-established problems."
D19-1507,1,a student model can learn to match an estimation of query probability with that of a teacher model.
D19-1507,1,"another interesting research direction is learning segmentation jointly with language model (kawakami et al., 2019; grave et al., 2019) rather than using fixed pretrained segmentation algorithms. proposed method can be extended to wide range of tasks."
D19-1508,1,"in the future, we will build a larger symptom graph and use external medical information to further improve the performance of symptom diagnosis on dialogues."
D19-1510,1,"in future work, we would like to experiment with more light-weight tagging architectures (andor et al., 2016) to better understand the trade-off between inference time and model accuracy."
D19-1511,4,"furthermore, our work of utilizing rl and gan with answer information is general, and it can be easily extended to existing work."
D19-1516,2,"further case studies also demonstrate that jointly using visual information and contextual information is an essential path for fully understanding human language, especially dialogues."
D19-1517,1,we will make the dataset publicly accessible in order to support the research investigation in fine-grained semantic comprehension.
D19-1518,1,"moving forward, we are going to narrow the gap between top-down and bottom-up models, and design a hybrid framework exploring both choices."
D19-1519,1,one solution to overcome it is to apply meta learning.
D19-1519,1,"in future, we plan to extend our framework into an iterative setting, similar to those boosting algorithms."
D19-1519,1,"in this way, we can identify label mistakes more accurately and obtain a series of weighted models at the end."
D19-1519,1,we can first train a meta model and only fine-tune on different training data on each fold.
D19-1520,1,"in future, to account for different levels of annotator expertise, we plan to combine proactive learning (li et al., 2017) with our proposed method."
D19-1521,4,"in the future, we plan to extend openkp with more annotated documents and connect it with downstream applications."
D19-1522,1,future work might include exploring how to incorporate background knowledge on individual relation properties into the existing model.
D19-1523,3,"lastly, we consider evaluating on other datasets and other advanced architectures beneficial future work as it may reveal further interesting qualities of the explanation methods."
D19-1524,1,"for future work, we will try using pre-trained bert for scientific domain such as the scibert (beltagy et al., 2019) and explore using our frameworks to help more tasks such as the evaluation, prediction and knowledge graph construction for scientific resources."
D19-1525,4,"we recommend future work to study the scope of adversarial reprogramming for other nlp applications such as machine translation, text to speech synthesis and text to image synthesis where the input space is discrete."
D19-1525,5,"furthermore, due to the threat presented by adversarial reprogramming, we recommend future work to study defenses against such attacks."
D19-1527,4,"in the future, we will try to move the state-of-the-art frontier further, not only for top-1 mips but also for top-n, n > 1, mips results."
D19-1527,4,it would be interesting to adopt these measures in nlp tasks.
D19-1527,4,"another promising direction is to adopt a gpu-based system for fast anns or mips, which has been shown highly effective for generic anns tasks (li , 2012; johnson , 2017; zhao , 2019)."
D19-1527,1,developing gpu-based algorithms for mips is still a topic which has not been fully explored.
D19-1527,1,"besides of metric measures (e.g., ‘2distance and cosine similarity) and inner product, more complicated measures has been studied, for example (tan et al., 2019)."
D19-1530,2,future work could extend the names intervention to names from other languages beyond the usbased gazetteer used here.
D19-1531,4,directions for future work include testing monolingual and bilingual word embeddings on downstream tasks like machine translation to measure bias and also test the performance for mitigation methods.
D19-1531,1,"moreover, the number of noun classes for other languages with grammatical gender ranges from two to several tens (corbett, 1991) and future work can extend methods proposed in this paper to address grammatical gender in languages with more gender forms."
D19-1534,5,"there are still many fruitful areas for future research, including discovering why numeracy naturally emerges in embeddings, and what other properties are similarly emergent."
D19-1535,4,"for future work, we may extend our method to other natural language tasks."
D19-1536,4,"in the future, we would like to investigate how such an approach can be applied to more complicated math word problems, like algebra word problems where a problem usually maps to an equation set."
D19-1536,5,another interesting direction is to investigate how to incorporate world knowledge into the graph-based approach to boost the performance.
D19-1539,1,"in particular, we had initial success sharing the parameters of the two towers which allows training much deeper models without increasing the parameter count."
D19-1539,1,"In future work, we will investigate variations of our architecture."
D19-1541,1,"moreover, the comparison of sentence performance indicates that there is still a lot of work to do to better integrate syntactic information and bert representation."
D19-1542,4,"additionally, we will apply our model to improve the alignment quality of the phrase alignment model."
D19-1542,1,"in the future, we plan to investigate the effects of our method on different sizes of bert models."
D19-1543,4,"in addition, we also plan to extend our approach to the tasks with question-denotation pairs as supervision."
D19-1543,1,"in the future, we plan to improve the performance of the anonymization model by exploring more efficient expected reward."
D19-1544,4,"in future work, we would like to extend the approach to modeling interaction between multiple predicate-argument structures in a sentence as well as to other semantic formalisms (e.g., abstract meaning representations (banarescu , 2013))."
D19-1547,1,we outline several future directions to further improve misp-sql and develop misp systems for other semantic parsing tasks: improving agent components.
D19-1547,1,"one can also use learning-based approaches, such as a reinforced decision policy (yao et al., 2019), to increase the rate of identifying wrong and solvable predictions."
D19-1547,1,"one can augment the probability-based error detector in misp-sql with probability calibration, which has been shown useful in aligning model confidence with its reliability (guo et al., 2017)."
D19-1547,1,"learning from user feedback is a promising solution for lifelong semantic parser improvement (iyer et al., 2017; padmakumar et al., 2017; labutov et al., 2018)."
D19-1547,1,"in the context of dialog systems, padma kumar et al.(2017) suggests that this effect can be mitigated by jointly updating the dialog policy and the semantic parser batchwisely."
D19-1547,1,"to this end, one can improve misp from at least three aspects: (1) using more intelligent interaction designs (e.g., free-form text as user feedback) to speed up the hypothesis space searching globally, (2) strengthening the world model to nail down a smaller set of plausible hypotheses based on both the initial question and user feedback, and (3) training the agent to learn to improve the parsing accuracy while minimizing the number of required human interventions over time."
D19-1547,1,Lifelong Learning for Semantic Parsing.
D19-1548,1,"in future work, we would like to do semi-supervised learning and use silver data to test how much improvements could be further achieved."
D19-1549,1,we would also like to combine such a graph-based model with a sequence-based model to avoid potential noise from dependency parsing errors.
D19-1549,1,"since this work only uses the dependency graph and ignores various types of relations in the graph, we plan to incorporate dependency relation types into our model and take part-of-speech tagging into consideration as well in the future."
D19-1549,1,future work could consider using an attention mechanism to focus on important words in the aspect.
D19-1550,4,future work includes exploring approaches to capture explicit and implicit structural information to other sentiment analysis tasks and other structured prediction problems.
D19-1551,4,"in the future, our theory can be generalized to other tasks that highly depends on the attention mechanism."
D19-1551,4,"For example, reading comprehension and machine translating."
D19-1554,5,"first, to address the problem of low-quality phrases in generated text, we will explore how to keep the syntactic correctness of the generated text."
D19-1554,1,"second, we would like to figure out the detailed effects of the generated examples on the robustness of sentiment classification models."
D19-1554,1,"by removing useless examples, we can obtain higher training speed."
D19-1555,1,"one direction is to learn disentangled latent representations for attributes in neural network’s space, such as for disentangling aspects (jain et al., 2018), text style (john et al., 2019), and syntax and semantics (chen et al., 2019; bao et al., 2019)."
D19-1555,1,"another direction is to develop a content-based recommender based on trait, since it provides an effective unsupervised solution for generating profiles based on different attributes."
D19-1556,1,future work will explore the use of structured output learning methods dedicated to the opinion structure.
D19-1557,4,"due to size and memory complexities of available implementation of large scale language models such as bert, an easy integration with our proposed adaptation framework does not currently exist."
D19-1557,1,"as future work, we will interface the adaptation layer for use recently introduced large scale language models such as bert."
D19-1560,4,"furthermore, we would like to apply our hrl approach to other sentiment analysis tasks, such as aspect and opinion co-extraction, and dialog-level sentiment analysis."
D19-1560,4,"in our future work, we would like to solve other challenges in dasc, e.g., negation detection problem, to further improve the performance."
D19-1561,1,"while baselines are provided only for detecting gp-claims in spoken content, future work should aim to solve the problem as a whole - either by developing algorithms that determine relevance and stance of gp-claims to given motions, or by forgoing these stages, and successfully deciding whether a claim was mentioned in a speech, without first focusing on relevant claims."
D19-1563,4,"in the future, we will exploit how to incorporate discourse parse tree or discourse relations into emotion cause analysis task to further improve the performance."
D19-1564,1,"in future work we intend to further investigate this approach, as well as explore in more detail the low fraction of cases where these two schemes led to clearly different results."
D19-1565,6,"in the mid-term, we plan to build an online platform where professors in relevant fields (e.g., journalism, mass communication) can train their students to recognize and annotate propaganda techniques."
D19-1566,2,"In future, we would like to extend our work towards the multi-party dialogue."
D19-1571,1,channel models are particularly effective with large context sizes and an interesting future direction is to iteratively refine the output while conditioning on previous contexts.
D19-1573,1,"in the future, we will focus on designing new architectures and training methods for nart models to achieve comparable accuracy as art models."
D19-1574,1,"ultimately, we would like to design models that can directly leverage typological compositionality for distant languages."
D19-1575,5,"for future work, we are interested in unsupervised cross-lingual alignment, inspired by prior success on static embeddings (lample , 2018; alvarez-melis and jaakkola, 2018), which demands a deeper understanding to the geometry of the multilingual contextualized embedding space."
D19-1576,1,"while in this work we follow previous work and perform unlexicalized parsing, the proposed model can be extended for lexicalized parsing by replacing pos tag embeddings with cross-lingual word embeddings, which we leave for future work."
D19-1577,5,we leave the investigation of less frequent inanimate nouns for future work.
D19-1578,4,"future work could employ our method to study various group biases such as nationality, caste, and religion, since person names may function as significant markers for many such demographic associations."
D19-1578,4,"our method could also be easily extended to other kinds of nlp models (beyond classification) as well as other types of entities (locations, organizations etc.)."
D19-1579,3,"finally, we plan to continue widening the scope of our study – for example, expanding our methods to include non-binary gender identities, evaluating changes in gender norms over time, and spreading to more domains, such as the political sphere."
D19-1579,4,"in future work, we hope to use our findings to improve performance on tasks such as abusive language detection."
D19-1579,1,"we also hope to delve into finer-grained analyses, exploring how language around gender interacts with other variables, such as sexual orientation or profession (e.g. actresses versus female athletes)."
D19-1583,1,the next challenge is to incorporate this into actual noisy ie extraction pipelines.
D19-1584,4,"in the future, we will further explore to leverage other kinds of inductive bias from human experience to improve extensive tasks with our modular networks ."
D19-1585,4,future work could extend the framework to other nlp tasks and explore other approaches to model higher-order interactions like those present in event extraction.
D19-1586,5,"it would be interesting to see whether it has the same effect on implicit discourse relation classification task, we’d like to leave that in the future work."
D19-1586,1,future work should explore the joint representation of discourse expectations through implicit representations that are learned during training and the inclusion of external knowledge.
D19-1589,3,"a potential direction for future study is to directly couple them together and see whether one form contains the other, or vice versa."
D19-1589,1,another direction is to check their effectiveness on top of the recent pre-trained language models.
D19-1590,1,"as future work, we are examining the possibility of using an adversarial learning framework (goodfellow et al., 2014), which was recently used in the why-qa task (oh et al., 2019) for causality recognition."
D19-1591,5,it would be meaningful to study how the neurons are trained to encode label-specific information and what exactly the information is.
D19-1591,1,"it would be interesting to see what will happen if they are removed, and furthermore, how this can help model compression and hidden size selection.(3) transformers (vaswani et al., 2017; devlin et al., 2019) have been increasingly popular in nlp, and it would be important to extend our work to understanding these architecture."
D19-1596,2,"as future work, we plan to look into improving our automatically generated consistent qa pairs using external knowledge-bases."
D19-1597,6,"in future work, we will work towards more effective solutions to meet the challenges."
D19-1598,1,"in future work, we aim at developing models to solve the newly proposed tasks."
D19-1598,3,"therefore, solving the tasks we propose can only be considered a prerequisite for a fully functional theory of mind which will ultimately have to be evaluated in real-world scenarios."
D19-1599,5,"in future, we plan to consider inter-correlation among passages for open-domain question answering (wang , 2018b; song , 2018)."
D19-1600,5,"we hope the release of this dataset could bring language diversity in machine reading comprehension task, and accelerate further investigation on solving the questions that need comprehensive reasoning over multiple clues."
D19-1603,1,"to handle larger tables, we will investigate sharding the table row-wise, running the model on all the shards first, and then on the final table which combines all the answer rows."
D19-1603,1,"as future work, we plan to expand our model with pre-trained language representations (e.g., bert (devlin et al., 2018)) in order to improve performance on initial queries and matching of queries to table entries."
D19-1604,4,"as future work, we plan to further investigate the generality of the approach with other tasks and base models."
D19-1605,1,"an interesting avenue for future work is to incorporate deeper context, either from other modalities (das et al., 2017) or from other dialog comprehension tasks (sun et al., 2019)."
D19-1605,5,a natural question is whether reinforcement learning could learn to retain the necessary context to rewrite questions in cqa.
D19-1605,3,"this dataset helps start that conversation; the next steps are developing and evaluating models that efficiently decide when to ask for human assistance, and how to best use this assistance."
D19-1609,1,"we plan to generalize this model to more complex and compositional answers, with better searching and pruning strategies of the derivations."
D19-1610,4,"in addition, we plan to apply our self-attention memory network on other sentence matching tasks such as natural language inference, paraphrase identification, or measuring semantic relatedness."
D19-1610,1,"in the future, we plan to investigate more transfer learning techniques for utilizing the large volume of existing cqa data."
D19-1613,6,"our future work includes generating structured representations of recipes to handle ingredient properties, as well as accounting for references to collections of ingredients (e.g.dry mix)."
D19-1616,2,"as the next step, we plan to investigate: i) synthetic summarization data, and ii) applying transfer learning on text summarization for the multilingual low resource data set with little or no ground truth summaries (keneshloo , 2018)."
D19-1617,3,"furthermore, conducting a study just on the evaluation itself makes it possible for us to evaluate the adequacy of the used evaluation metric in evaluating computer generated poetry."
D19-1618,3,a concrete estimation of performance in this setting will be part of future work.
D19-1618,4,"it could thus be applied to other text generation tasks, such as natural language generation and sentence compression."
D19-1618,1,"future work involves extending the sum-qe model to capture content-related aspects, either in combination with existing evaluation metrics (like pyramid and rouge) or, preferably, by identifying important information in the original text and modelling its preservation in the proposed summaries."
D19-1618,3,"finally, sum-qe could serve to assess the quality of other types of texts, not only summaries."
D19-1620,6,we will also carry out deeper study of the properties of dearly and dlate type documents and use them to inform new solutions.
D19-1620,1,"while our auxiliary loss method achieves significant improvement, we note that there is a large gap which better methods can hope to bridge in the future."
D19-1620,1,"one approach, building on ours, is to examine other ways to combine loss signals (finn et al., 2017), and to encourage exploration (haarnoja et al., 2018)."
D19-1628,4,future directions include trying our method on different tasks and different relevant data.
D19-1630,3,"there is much room for improvement, and the cb dataset will be a useful testbed to assess models’ progress on such reasoning."
D19-1631,4,"with the emergence of knowledge graphs in different domains, the proposed approach can be tried out in other domains as well for future exploration."
D19-1633,5,"while there are still open problems, such as the need to condition on the targets length and the dependence on knowledge distillation, our results provide a significant step forward in nonautoregressive and parallel decoding approaches to machine translation."
D19-1633,1,"in a broader sense, this paper shows that masked language models are useful not only for representing text, but also for generating text efficiently."
D19-1636,1,"in the future, we would like to extend this framework for cross-lingual and cross-cultural sarcasm and irony generation."
D19-1637,4,"in the future, we plan to explore: (1) applying the umt model in the tasks where the abstraction levels of source and target languages are different (e.g., unsupervised automatic summarization); (2) improving the quality of generated poems via better structure organization approaches."
D19-1640,1,"in the future, we will investigate more sophisticated methods to improve ss’s performance, e.g., enable self-attention mechanism for contextualized information modelling."
D19-1641,4,"we also plan to utilize fine-grained entity typing results in more downstream applications, such as coreference resolution and event extraction."
D19-1641,1,"in the future, we will further improve the performance of fine-grained types, which is still lower than that of general types due to less training instances and distant supervision noise."
D19-1642,2,"therefore, we should move along that direction to collect more high-quality data, which can facilitate more advanced learning algorithms in the future."
D19-1644,5,"this issue is similar to exposure bias (wiseman and rush, 2016) and it might be beneficial if the classifier of segment merging is exposed to incorrect segments during training."
D19-1649,5,"though achieving promising improvements, these commonly-used techniques are still not the satisfactory solutions for few-shot da and fewshot nota, which requires further explorations in these two real-world challenges."
D19-1651,3,we expect that crowdsourced annotation will also be able to help the training of open ie systems as it has helped their evaluation – we leave the creation of a suitably large crowdsourced training set for open ie to future work.
D19-1652,1,"various other formulations for this task are possible: for example, one might involve marginalizing over the tags and using this for predicting a label; another could perform attention over spans instead of tokens."
D19-1653,1,the annotated corpus and neural models will be applicable for future persuasion reasoning or evaluation of persuasiveness.
D19-1655,5,we believe that learning with noisy labels is a promising direction as it is often easy to collect noisy-labeled training data.
D19-1656,1,interesting future directions for this work include: 1. incorporating common-sense knowledge into emotion analysis to capture semantic context and 2. using few-shot learning to bootstrap and improve performance of underrepresented emotions.
D19-1656,1,"finally, as narrative passages often involve interactions between multiple emotions, one avenue for future datasets could be to focus on the multiemotion complexities of human language and their contextual interactions."
D19-1657,2,"future work includes exploiting unsupervised learning to generate target-related lexicon and incorporating more labels (e.g., emotion classification) for multi-task learning."
D19-1660,2,confirming whether our method works well for other datasets including ones that have only shorter label texts is left for our future work.
D19-1661,5,"we believe could be addressed in future work by placing informative priors on multiple dimension to capture more complex concepts, and move away from simplistic antonym-driven dimension definitions."
D19-1661,1,"finally, while being flexible and easily extended to other probabilistic word embedding models, our approach performs better or on par with sota."
D19-1665,2,"in future work, we will explore how to integrate the textual descriptions of the labels in our our approach which has been shown to be beneficial in the case of large label sets (mullenbach , 2018)."
D19-1665,2,"in future work, we will explore how to integrate the textual descriptions of the labels in our  approach which has been shown to be beneficial in the case of large label sets (mullenbach et al., 2018)."
D19-1666,1,important future directions include examining the temporal aspect of bias as well as developing more precise mention identification techniques.
D19-1668,4,"by open-sourcing pythia, and phi-ml’s processing pipeline, we hope to aid future research and inspire further interdisciplinary work."
D19-1669,1,"it is interesting to further investigate a unified tensor embedding model to combine not only lexical, but also other features that are important for the sense of humor."
D19-1672,2,our experiment on bengali shows that leveraging knowledge from wikipedia will be a promising direction for future research.
D19-1674,1,more efficient use of gpus in performing the dfs procedure is a promising future work.
D19-1676,2,"it is an open question on whether these results generalize to other language pairs.in general, more investigation is needed to assess how well the current generation of multi-lingual document representations supports cross-lingual transfer and if there are differences in how well the representations between languages align."
D19-1678,1,"in the future, we expect to improve more using advanced models for the clinical notes since text summarizes expert knowledge about a patient’s condition."
D19-1680,2,our next step is to annotate more documents and make them available to the research community to foster research in this area.
D19-1680,2,we also plan to add location and time annotation on top of the intervention annotation.
D19-1681,1,"we further intend to add additional signals, for instance coming from vision (c.chem. et al.(2018)), for more accurate localization."
D19-1681,1,"in the future we plan to extend the world-state representation, and enable the model to ground generic and abstract concepts as well."
N00-1001,1,"further work remains to make the parser really efficient, and much work remains to make the language coverage complete within reasonable limits."
N00-1001,5,it is an open question whether the system of this kind will be a preferred way of offering information to the public.
N00-1002,1,"although these languages are not so similar as czech and slovak, we hope that an addition of a simple partial noun phrase parsing might provide results with the quality comparable to the fullfledged syntactic analysis based system ruslan (this is of course true also for the czechoto-slovak translation)."
N00-1004,3,we hope to be able to demonstrate in the near future that a fine-tuned english-chinese translation model can provide query translations for clir with the same quality produced by mt systems.
N00-1006,1,"in addition, we intend to apply other extra-linguistic information to a dialogue translation system."
N00-1006,5,we are considering ways to enable identification of the agent of an action in an utterance and to expand the current framework to improve the level of politeness even more.
N00-1010,1,"the other is that the system needs to be more communicative about its current understanding of the user's goals, even at points in the dialogue at which it might be assumed that user and system were in agreement."
N00-1010,5,one is that the system needs to be better able to recognize and deal with problem situations in which the dialogue is not advancing.
N00-1010,6,"while preliminary, these results point to two directions for future work."
N00-1011,1,"in addition, the design of rees is highly portable for future addition of new relations and events."
N00-1012,5,"however, our aim has been to highlight the problem as one worthy of further exploration within the field of nlp and to establish some baselines (human and algorithmic) against which further work may be compared."
N00-1013,3,"on the other hand, it is computationally expensive to keep the complete previous context in the systems ""working memory"" to evaluate the few presuppositions which may refer back over a large number of questions."
N00-1013,3,solving this problem will likely require comparing a variety of different settings.
N00-1013,5,"a future challenge is to turn dp into a dip (detector of incorrect presuppositions), that is, to reduce the number of reported presuppositions to those likely to be incorrect.is a dip system feasible?"
N00-1013,5,"on the one hand, one could limit the considered context to, say, three questions and risk missing the critical question."
N00-1015,1,"though in its infancy, we believe this approach holds vast potential for sls development."
N00-1015,1,we are developing a system to exploit these characteristics to automatically generate javox grammars from an application's compiled code.
N00-1015,1,the automatically-generated grammars are intended to serve as a starting point for developers - though they may certainly require some hand crafting.
N00-1015,2,it is imaginable that additional data sources - such as a sample corpus - will allow us to more accurately generate grammars for an application.
N00-1017,4,"ultimately, it is hoped that the use of a scheme of this type will permit much more widespread 'plug-and-play' among members of the nlg community."
N00-1018,1,we are now considering the possibility of integrating transcheck in an off-the-shelf text editor to cross the ascii barrier.
N00-1019,1,one obvious direction for future research is to revise our current strategy of decoupling the selection of units from their bilingual context.
N00-1020,1,"currently, we are also considering the effects of a bootstrapping algorithm for multilingual coreference resolution.to be able to develop such a learning approach, we must first develop a method for automatic recognition of multilingual referential expressions."
N00-1020,3,"we intend to analyze the performance of sidizzle when it is used as a module in an ie system, and separately in a question/answering system."
N00-1022,1,a reorganization of the existing three-level category system into a semantically consistent tree structure would allow us to explore the nonterminal nodes of the tree for multi-layered sml.
N00-1022,1,"where system-generated answers are acceptable to customers, a straightforward extension of ice-mail can provide this functionality."
N00-1022,1,further task-specific heuristics aiming at general structural linguistic properties should be defined.
N00-1022,1,this includes heuristics for the identification of multiple requests in a single e-mail that could be based on key words and key phrases as well as on the analysis of the document structure.
N00-1022,3,"our initial experiments with the integration of germanet (hamp and feldweg, 1997), the evolving german version of wordnet, seem to confirm the positive results described for wordnet (de buenaga rodriguez et al., 1997) and will thus be extended."
N00-1022,4,"in a further industrial project with german telekom, the icc-mail technology will be extended to process multi-lingual press releases."
N00-1022,5,this places additional requirements on the knowledge engineering task and thus needs to be thoroughly investigated for pay-off.
N00-1022,5,we intend to explore its use within an information broking assistant in document classification.
N00-1022,6,"technically, we expect improvements from the following areas of future work."
N00-1022,6,"for the application in hand, this was not the case."
N00-1025,1,"the semantic type filter will be greatly improved by the addition of a named entity tagger, but we believe that additional gains can be attained by augmenting named entity identification with information from wordnet."
N00-1025,1,"in addition, we believe that the retrieval and summarization components can be improved by incorporating automatic relevance feedback (buckley, 1995) and coreference resolution."
N00-1025,1,we also plan to reconsider paragraph-based summaries given their coverage on the test corpus.
N00-1025,1,incorporation of such a tagger will be a focus of future work.
N00-1025,1,"the most critical area for improvement, however, is the linguistic filters."
N00-1027,4,"consequently, the methodology is promising and the segmentation system would be helpful for the application system such as machine translation and information retrieval."
N00-1028,1,"future work includes extending the study of corpus-based grammar specialization from firstorder grammar pruning to higher-order grammar pruning, thus extending previous work on explanation-based learning for parsing, aad applying it to the lfg fornaalism."
N00-1029,4,"moreover, these algorithms can be used in any applications, which deal with grammars."
N00-1031,1,"it is a very interesting future research topic to determine the advantages of either of these approaches, to find the reason for their high accuracies, and to find a good combination of both."
N00-1033,1,the core idea is to call a deep parser only to the separated field elements which contain sequences of simple nps and pps (already determined by the shallow parser).
N00-1033,1,"thus seen the shallow parser is used as an efficient preprocessor for dividing a sentence into syntactically valid smaller units, where the deep parser's task would be to identify the exact constituent structure only on demand."
N00-1035,1,"the coverage of the grammar, in particular the treatment of genitive phrases, should also be further developed."
N00-1035,1,"in addition, two aspects of the system that have only be touched on in this paper would be worth further attention: one is the mechanism for the treatment of split-ups and run-ons, which as mentioned earlier is not well-integrated at the moment; the other is the weight adjustment process, which is done manually at the moment, and for which the adoption of a semiautomatic tool could be considered."
N00-1035,3,"therefore, more extensive testing and debugging should also be carried out."
N00-1035,5,"in particular, future development should focus on treating stylistic matters such as capitalisation and punctuation which have not been in focus in the current prototype."
N00-1039,1,"we are working on capturing the rich information about concept classes which is currently returned as part of our pattern discovery procedure, to build up a concept dictionary in tandem with the pattern base."
N00-1039,1,"we are also considering the proper selection of weights and thresholds for controlling the rankings of patterns and documents, criteria for terminating the iteration process, and for dynamic adjustments of these weights."
N00-1039,1,"we feel that the generalization technique in pattern discovery offers a great opportunity for combating sparseness of data, though this requires further research."
N00-1039,1,"lastly, we are studying these algorithms under several unrelated scenarios to determine to what extent scenario-specific phenomena affect their performance."
N00-1039,5,"we are at the early stages of understanding how to optimally tune these techniques, and there are number of areas that need refinement."
N00-1041,1,"another area of future work is to extend the entity-extraction component of the system to handle arbitrary types (mountain ranges, films etc.).it is likely that as more features are added a trainable statistical or machine learning approach to the problem will become increasingly desirable."
N00-1041,2,"this entails developing a training set of question-answer pairs, raising the question of how a relatively large corpus of questions can be gathered and annotated."
N00-1043,1,"in the future, we would like to integrate our sentence reduction system with extraction-based summarization systems other than the one we have developed, improve the performance of the system further by introducing other sources of knowledge necessary for reduction, and explore other interesting applications of the reduction system."
N00-1046,1,"future research should be searching for more such tasks, and developing more general toolkits that support rapid adaptation of multimodal technologies to support them."
N00-2002,5,"our model is still unable to determine correctly how to re-package sentences into paragraphs; a better understanding of the notion of ""paragraph"" is required in order to improve this."
N00-2003,1,"there are two main reasons for this: first, the proposed sortal class annotation scheme needs further work,  second, the relationship between sortal class and pronominalization may well be too intricate to be modelled by the factor class alone."
N00-2003,2,"large corpora can be annotated relatively quickly with this information, which can then be used for statistical pronoun generation."
N00-2004,1,"it would be interesting to corot)are c99 with the multi-source method described in (beeferman et al., 1999) using the tdt corpus."
N00-2004,1,we would also like to develop a linear time and multi-source version of the algorithiil.
N00-2004,3,"although our evaluation scheme is sufficient for this comparative study, further research requires a large scale, task independent benchmark."
N00-2006,2,"the inclusion of this information in a large database which is mainly intended for computational applications can be very useful, mainly because it may help in resolving ambiguities and may be used to draw inferences of different nature."
N00-2007,1,another approach would be to use different classification algorithms and combine the results.
N00-2007,5,we are working on this but we are still to overcome the practical problems which prevent us from obtaining acceptable results with the other learning algorithms.
N00-2012,1,we are currently working on the other kinds of weak verbs: defective and assimilated verbs.
N00-2012,1,"other categories of words can be handled in a similar manner, and we will turn our attention to them next."
N00-2013,1,"we would like to compare more taggers using still other methodologies, especially the mbt tagger, which achieved the best results on slovene but which was not available to us at the time of writing this paper."
N00-2013,1,"obviously, we would also like to use the classifter combination method on them, to confirm the really surprisingly good results on romanian and test it on the other languages as well."
N00-2013,3,we would also like to enrich the best taggers available today (such as the maximum entropy tagger) by using the dictionary information available and compare the results with the exponential featurebased tagger we have been using in the experiments here.
N00-2014,1,"we are currently expanding the number of templates in our grammar in an attempt to obtain full coverage of the rm2 corpus using only templateexpanded rm sentences.we are also developing a stochastic version of cdg that uses a statistical arv, which is similar to a supertag (srinivas, 1996)."
N00-2016,2,a particularly promising research direction for this is to harness knowledge and training resources across languages.
N00-2017,5,in future work we intend to study ways that determine the appropriate confusion set in a way to makes use of the current task properties.
N00-2018,1,"ultimately it is this flexibility that let us try the various conditioning events, to move on to a markov grammar approach, and to try several markov grammars of different orders, without significant programming."
N00-2018,1,"indeed, we initiated this line of work in an attempt to create a parser that would be flexible enough to allow modifications for parsing down to more semantic levels of detail."
N00-2018,6,it is to this project that our future parsing work will be devoted.
N00-2020,1,future work in this area involves modeling the corpora with other probability distributions.
N00-2020,1,the method is very sensitive to the effectiveness of the probability model in modeling the normal elements.
N00-2020,1,extensions to the probability distributions presented here such as adding information about endings of words or using more features could increase the accuracy of the probability distribution and the overall performance of the anomaly detection system.
N00-2020,4,other future work involves applying this method to other marked corpora.
N00-2021,4,"an obvious extension of this work, which we hope will be persued in the future, is to apply these techniques in broadcoverage feature-based tag parsers."
N00-2022,1,"finally, applications like verbmobil favour prioritized best-first rather than all-paths parsing."
N00-2022,1,"firstly, we will enhance the unpacking phase to take advantage of the large number of equivalence packings we observe."
N00-2022,1,"secondly, many application contexts and subsequent layers of semantic processing will not require unfolding the entire parse forest; here, we need to define a selective, incremental unpacking procedure."
N00-2022,1,"using slightly more sophisticated accounting in the agenda, we plan to investigate priority propagation in a best-first variant of our parser."
N00-2022,6,we intend to develop the approach presented in this paper in several directions.
N00-2023,1,"in future work, we plan to modify the forests our system produces so they conform to the penn treebank corpus (marcus et al., 1993) annotation style,"
N00-2023,3,and then do experiments using models built with treebank data.
N00-2024,3,we are preparing the task-based evaluation of the overall system.
N00-2024,3,we also plan to evaluate the portability of the system by testing it on another corpus.
N00-2024,5,we will also extend the system to query-based summarization and investigate whether the system can be modified for multiple document summarization.
N00-2028,1,"finally, we plan to integrate the learned rulesets into the hmihy dialogue system to improve the system's overall performance."
N00-2028,1,in future work we intend to use the (noisy) output from this classifier as input to our problematic dialogue classifier with the hope of improving the performance of the fully automatic feature sets.
N00-2028,3,"in addition, since it is more important to minimize errors in predicting problematic dialogues than errors in predicting tasksuccess dialogues, we intend to experiment with ripper's loss ratio parameter, which instructs ripper to achieve high accuracy for the problematic class, while potentially reducing overall accuracy."
N00-2029,1,our ultimate goal is to provide prosodicallybased mechanisms for identifying and reacting to asr failures in sds systems.
N00-2029,2,"we are examining the w99 corpus, which was collected in a spoken dialogue system that supported registration, checking paper status, and information access for the ieee automatic speech recognition and understanding workshop (asru99) (rahim et al., 1999)."
N00-2029,2,we also are extending our toot corpus analysis to include prosodic analyses of turns in which users become aware of misrecognitions and correct them.
N00-2029,3,we are currently replicating our experiment on a new domain with a new speech recognizer.
N00-2029,5,"in addition, we are exploring whether prosodic differences can help explain the ""goat"" phenomenon -- the fact that some voices are recognized much more poorly than others (doddington et al., 1998; hirschberg et al., 1999)."
N00-2031,1,a more systematic investigation into the advantages of different feature trees would be useful.
N00-2031,1,"we could add to the feature tree the values of other categories of function tag, or the function tags of various tree-relatives (parent, sibling)."
N00-2032,3,"since our method does not use any japanese-dependent heuristics, we also hope to test it on chinese or other languages as well."
N00-2032,3,"in future work, we plan to experiment on japanese sentences with mixtures of character types, possibly in combination with morphological analyzers in order to balance the strengths and weaknesses of the two types of methods."
N00-2033,4,"given the diverse nature of these grammars, we conclude that our techniques based on the lc transform are likely to be applicable to a wide range of cfgs used for natural-language processing."
N00-2034,5,"however, a considerably larger corpus would be required to overcome the sparse data problem for other rsa alternations."
N00-2035,1,there are ways for further improvement of the performance of our system by combining it with a word-based system which encodes specific behavior for individual words.
N00-2035,4,another avenue for further development is to extend the system to other languages.
N00-2036,4,"we perceive that these results can be extended to other language models that properly embed bilexical context-free grammars, as for instance the more general history-based models used in (ratnaparkhi, 1997) and (chelba and jelinek, 1998)."
N00-2037,1,"with better estimates of subject preferences, we can then proceed to our larger goal of comparing the usefulness and user acceptability of spoken language dialogue models with and without acknowledgment behavior (c.f.walker, 1993)."
N00-2037,4,our immediate plans include extending this study to a larger and gender-balanced group of subjects so that we can draw firmer quantitative conclusions about the percentage of people who are likely to prefer this style of interaction.
N00-2037,5,we also plan to explore the effect of the quality of the synthesized voice on the incidence of acknowledgment behavior.
N00-2038,1,"the goal of my current research is to combine the new alignment algorithm with a cognate identification procedure, the alignment of cognates is possible only after the pairs of words that are suspected of being cognate have been identified."
N00-2038,1,"moreover, it is hardly feasible without some kind of pre-alignment between candidate lexemes."
N00-2038,1,"an integrated cognate identification algorithm would take as input unordered wordlists from two or more related languages, and produce a list of aligned cognate pairs as output."
N00-2038,1,such an algorithm would be a step towards developing a fully automated language reconstruction system.
N00-2038,5,"identification of cognates is, however, an even more difficult task than the alignment itself."
N00-2039,1,"it is worth mentioning that our method can be transferred into a two-level transducer setting without major difficulties (walther, 1999, appendix b)."
N00-2040,1,"given the result of roche and schabes (1997), an obvious next step is to compile the induced rules into an actual transducer, and to compose this with the hand-crafted transducer."
N00-2040,1,"it should be noted, however, that the number of induced rules is quite large in some of the experiments, so that the compilation procedure may require some attention."
N00-2041,1,"in some cases, such as with copular constructions and with adjunct prepositional phrases, it would be useful to introduce some non-determinism so that, for example, semantic selectional restrictions between the object of the preposition and the semantic structure that the prepositional phrase is attaching to can more easily play a role in selecting the appropriate semantic relationship."
N00-2041,1,exploring approaches for achieving this non-determinism efficiently is one of our current objectives.
N00-2042,1,"on the other hand, (gardent and webber, january 2000) shows that minimality is also an important tool for disambiguating nouncompounds, logical metonymy and definite descriptions."
N00-2042,1,another area that deserves further investigation concerns the use of minimality for disambiguation.
N00-2042,1,"in further work, we intend to look at other ambiguous natural language constructs and to identify and model the ranking criteria determining their preferred interpretation."
N00-2042,1,plurals are a first obvious choice.
N00-2042,4,this suggests that one of the most promising application of model generators is as a device for developing and testing preference systems for the interpretation of natural language.
N00-2042,5,"but more generally, we hope that looking at a wider range of data will unveil a broader picture of what the general biases are which help determine a preferred reading -- either in isolation, as here, or in context, as in (gardent and webber, january 2000) -- and of how these biases can be modelled using automated reasoning systems."
N00-2042,5,"as the paper shows though, many questions remains open about this use of minimality for disambiguation which are well worth investigating."
N00-3001,3,"our experiment could be extended in many aspects, for example, validating the evaluation function through empirical analysis of human assessments of the generated texts, and experimenting with the interaction between aggregation and referring expression generation."
N00-3001,3,"to generalise our claim, a larger scale experiment is needed."
N00-3001,4,the architecture based on the genetic algorithm can also be used for testing interactions between or within other text generation modules.
N00-3002,1,we are also looking into replacing the fixed pruning and constituent extraction strategy with one learned from training data.
N00-3002,1,we are also investigating ways of learning the number of levels and grammar rule partitioning among levels.
N00-3002,3,there are a number of research issues that we are planning to address soon.
N00-3002,3,"first, a more thorough evaluation is required, as described in the previous section."
N00-3002,3,we are currently looking for ways to perform such an evaluation.
N00-3004,1,we could imagine that rule design process can be partially automated and we intend to pursue some research on developing methods for both assisted rule design and corpus based rule induction.
N00-3005,1,"we plan to extend the system to other qualitatively different types of errors, such as those involving agreement between the main components of the sentence, which is very rich in basque, errors due to incorrect use of subcategorization and errors in postpositions."
N00-3005,1,adding new kinds of errors.
N00-3005,3,"in any case, it must be examined whether automatic methods reach the high precision and reliability obtained by hand-coded rules."
N00-3005,5,"another interesting aspect is the reusability of the linguistic patterns: in the process of treating errors in dates some patterns describe general linguistic facts that can be reused, while others pertain to idiosyncratic facts of dates."
N00-3005,1,Automatic acquisition of error detecting patterns.
N00-3006,4,"there is certainly a great deal of room for further investigation into the use of metadata in spelling correction in general, however."
N00-3007,3,"the next step in our work will be to evaluate our wsd algorithm against the manually sensetagged semcor corpus for validation, and then integrate our wsd algorithm into cindor's processing and evaluate directly the impact on retrieval performance."
N00-3007,3,and also extend our evaluation to multiple languages and to other parts of speech.
N00-3007,3,we hope to verify that word sense disambiguation leads to improved precision in cross-language retrieval.
N00-3007,5,"in our future work we must tackle issues associated with the fine granularity of some wordnet sense distinctions, synsets which are proper subsets of other synsets and are therefore impossible to distinguish,"
N01-1002,2,"we should also try to increase the size of the annotated corpus for training statistical models, which might help achieving higher accuracy rate."
N01-1002,2,A more fine-grained analysis of the annotated corpus is needed.
N01-1003,1,"in future work, we intend to build on the work reported in this paper in several ways."
N01-1003,1,"these include features based on the discourse context, and features that encode relationships between the sp-tree and the dsynts."
N01-1003,1,"first, we believe that we could utilize additional features as predictors of the quality of a sentence plan."
N01-1003,1,"we will also expand the capabilities of the spg to cover additional sentence planning tasks in addition to sentence scoping, and duplicate the methods described here to retrain spot for our extended spg."
N01-1004,1,the general framework opens up the possibility of assigning weights to more sophisticated lexical questions that is consistent with the popular notion of idf.
N01-1006,1,"furthermore, our algorithm scales better with training data size; therefore the relative speed-up obtained will increase when more samples are available for training, making the procedure a good candidate for large corpora tasks."
N01-1010,1,"for other future work, we plan to investigate an automatic way of detecting and filtering unrelated relations."
N01-1010,4,"for those tasks, we anticipate that our extraction methods may be useful in deriving characteristics of the domains or given corpus, as well as customizing the lexical resource."
N01-1010,3,"we are also planning to compare our sense partitions with the systematic disagreement obtained by (wiebe, et al., 1998) automatic classifier."
N01-1014,4,"however, in spite of the fact that the main focus of this paper is diachronic phonology, the techniques and findings presented here may also be applicable in other contexts where it is necessary to identify cognates, such as bitext alignment."
N01-1016,1,"also of interest are models that compute the joint probabilities of the edit detection and parsing decisions 鈥 that is, do both in a single integrated statistical process."
N01-1017,2,"in the future, we will address the issue of the amount of training data for the si model."
N01-1017,2,we would also like to investigate the point at which the size of the various kinds of data used for adaptation stops making improvements in recognition accuracy.
N01-1017,1,we believe that using such confidence measures can improve the generation of semi-literal transcriptions considerably.
N01-1017,1,"also, current atrs system does not take advantage of various confidence scores available in leading recognition engines."
N01-1020,2,"one possible area of investigation is to use larger dictionaries and assess how much better stochastic transducers, and distance metrics derived from them, perform with more training data."
N01-1020,1,the first is whether there exists a better string transformation algorithm to use in the induction step.
N01-1020,1,"another option is to investigate the use of multi-vowel or multi-consonant compounds which better reflect the underlying phonetic units, using an more sophisticated edit distance measure."
N01-1020,5,it is an interesting research question as to whether we can augment these methods with translation probabilities estimated from statistical frequency information gleaned from loosely aligned or unaligned bilingual corpora for non-cognate pairs.
N01-1020,1,"various machine learning techniques, including co-training and mutual bootstrapping, could employ these additional measures in creating better estimates."
N01-1024,4,"we also believe that some findings of this work can benefit other areas of linguistic induction, such as part of speech."
N01-1024,4,"for the future, we expect improvements could be derived by coupling this work, which focuses primarily on inducing regular morphology, with that of yarowsky and wicentowski (2000), who assume some information about regular morphology in order to induce irregular morphology."
N01-1027,1,"the next steps, developing techniques to interpret these turns more accurately and to use correction prediction to drive modifications in dialogue strategy, are both subjects of our future research."
N01-1028,3,"more work needs to be done to determine, if possible, under which conditions such improvements can be obtained."
N01-1029,3,in the future we plan to evaluate full em reestimation of the models on the trainset using the formulas given in this paper.
N01-1030,5,"it is unclear to us how large the cumulative effect may become when detailed modelling of many such phenomena is added to an atheoretical coarse-grained language model, and it is seems plausible that it could be considerably greater than that produced by the single phenomenon we have investigated here."
N01-1030,1,it is feasible to investigate these questions empirically by modifying large-scale linguistically motivated systems like the psa grammar described in this paper; we hope to present further results in due course.
N01-1030,5,"looking further ahead, it is also important to remember that grammatical agreement is only one of a large range of linguistic phenomena currently ignored by most practical implementors."
N01-1031,1,"based on the promising results on classification of text, we are looking at ways of incorporating this with existing techniques for identifying non-native speakers based on acoustic features, with the goal of obtaining even greater reliability from combining the two."
N01-1031,5,thus we would also like to explore minimizing the number of words needed to be spoken before the system can identify nativeness.
N01-1031,4,"finally, it may be helpful to apply some of the features discovered in this work to the problem of identifying native and non-native writing, which may be of relevance to writer identification."
N03-1005,4,the long-term objective of this work is to learn the pronunciations and spellings of general oov data in spoken dialogue systems on domains where oov proper nouns are prevalent.
N03-1005,2,future experiments will involve general classes of unknown words such as names of geographical locations or businesses.
N03-1007,5,"work also needs to be done on using information given by answers, not just questions in recognizing clarification dialogue and on coping with the cases in which clarification dialogue recognition is not enough to retrieve an answer and where other, more complex, techniques need to be used."
N03-1007,1,future work will be directed on extending wordnet in this direction and in providing other useful semantic relationships.
N03-1011,1,"thus the learning procedure and the validation procedure are generally applicable and we intend to use the method for the detection of other semantic relations such as manner, influence, and others."
N03-1011,5,the inconvenience of the method is that for a very precise learning the number of examples (both positive and negative) should be very large.
N03-1011,1,we also intend to automate the detection of lexicosyntactic patterns and to discover constraints for all the part-whole patterns.
N03-1012,4,"currently, we are also beginning to investigate whether the proposed method can be applied to scoring sets of potential candidates for resolving the semantic interpretation of ambiguous, polysemous and metonymic language use."
N03-1012,1,"as future work we will examine how the computation of a discourse dependent semantic coherence score, i.e. how well a given srh fits within domain model with respect to the previous discourse, can improve the overall score."
N03-1012,1,"additionally, we intend to calculate the semantic coherence score with respect to individual domains of the system, thus enabling domain recognition and domain change detection in complex multi-modal and multi-domain spoken dialogue systems."
N03-1013,3,"additionally, we are interested in measuring the applied contribution of using the catvar in natural-language applications such as information retrieval."
N03-1013,1,"and finally, we intend to incorporate catvar into new applications such as parallel corpus word alignment."
N03-1013,2,future work includes improving the word-cluster ratio and absorbing more of the single-word clusters into existing clusters or other single-word clusters.
N03-1013,2,we are also considering enrichment of the clusters with types of derivational relations such as “nominal-event” or “doer” to complement part-of-speech labels.
N03-1013,1,"other lexical semantic features such telicity, sentience and change of-state can also be induced from morphological cues (light, 1996)."
N03-1018,1,we are also exploring the possibility of tuning a statistical machine translation model to be used with our model to exploit parallel text.
N03-1018,2,"finally, we plan to challenge our model with other languages, starting with arabic, turkish, and chinese."
N03-1018,1,"if a translation of the ocr’d text is available, a translation model can be used to provide us with a candidate-word list that contains most of the correct words, and very few irrelevant words."
N03-1018,2,chinese will require more work due to the size of its character set.
N03-1018,1,"we are optimistic that the power and flexibility of our modeling framework will allow us to develop the necessary techniques for these languages, as well as many others."
N03-1018,2,"arabic and turkish have phonetic alphabets, but also pose the problem of rich morphology."
N03-1021,4,"in future work, we hope to extend the formalism to cover some of the aspects that would not raise the computational complexity of its recognition, such as discontinuous and/or phrasal terminals."
N03-1021,1,"concurrently, we shall explore the empirical properties of mtg, by inducing stochastic mtgs from real multitexts."
N03-1023,1,"ther less studied single-view weakly supervised algorithms in the nlp community such as co-training with different learning algorithms (goldman and zhou, 2000) and graph mincuts (blum and chawla, 2001) can be similarly applied to these problems to further test our original hypothesis."
N03-1024,1,we also wish to extract more abstract paraphrase patterns from the current representation.
N03-1024,1,"such patterns are more likely to get reused – which would help us get reliable statistics for them in the extraction phase, and also have a better chance of being applicable to unseen data."
N03-1024,1,"in our future work, we wish to experiment with more flexible merging algorithms and to integrate better the top-down and bottom-up processes that are used to induce fsas."
N03-1025,5,"we are currently investigating more challenging problems like multiple category classification using the reuters-21578 data set (lewis, 1992) and subjective sentiment classification (turney, 2002)."
N03-1028,5,"full discriminative parser training faces significant algorithmic challenges in the relationship between parsing alternatives and feature values (geman and johnson, 2002) and in computing feature expectations."
N03-1031,1,we hope these studies will reveal ways to combine the strengths of co-training and active learning to make better use of unlabeled data.
N03-1031,3,"finally, we are conducting experiments to compare corrected co-training with other active learning methods."
N03-1034,3,a key area for future work is to devise a truly reusable qa evaluation infrastructure.
N03-1035,1,we will also study the impact of semantic differences between terms on user preferences and investigate whether terms which are preferred for information access are equally suitable for other nlp tasks.
N03-1037,5,headline generation is another task that we can approach equipped with our large restructured web the performance is reported on four metrics.
N03-2004,1,"given sufficient training, we might even take question features into account, learning that certain systems are better at certain types of questions."
N03-2004,2,we would like to pursue the use of these and other evidence sources in the future.
N03-2007,1,"further investigation is needed into the best way to measure overall labeling effort, and into refinements of the active learning process to optimize that labeling effort."
N03-2008,1,"by treating semantic classification as a single tagging problem, we hope to create a unified, practical, and high performance system for frame element tagging."
N03-2008,1,"in future work, we will extend the strategies outlined here to incorporate frame element identification into our model."
N03-2013,3,we are planning to integrate this method into a form of automatic evaluation.
N03-2014,1,"ongoing work includes improving the mention detection and mention tracking by adding morphological, syntactic (derived from parse trees) and semantic (e.g. wordnet) information streams, and extracting relations between the detected entities using statistical models."
N03-2016,1,"in the future, we plan to develop a method of incorporating the cognate information directly into the training algorithm."
N03-2018,1,"we will next explore the utility of a wider variety of features representing many knowledge sources (including acoustic, prosodic, lexical, syntactic, semantic, discourse, and local and global contextual dialogue features), using ablation studies."
N03-2018,2,"we will perform our learning using and comparing large corpora of both human-human and human computer data for training and testing, and will evaluate our results using a variety of metrics (e.g. recall, precision, and f-measure)."
N03-2018,1,we will also investigate a variety of emotion annotations with the goal of producing a reliable annotation scheme for the emotions associated with our tutoring domain.
N03-2018,2,"previous studies have shown low inter-annotator reliability (around 70%, kappa values around 0.47 (narayanan, 2002)), which originates partly in vague descriptions of the emotions to be labeled."
N03-2019,1,"future work will examine the role of aspectual features, learning from skewed distributions dominated by at (an overwhelming majority of news events occur at the reference times), and the incorporation of unsupervised learning methods."
N03-2027,1,we are currently investigating the performance of other dbn topologies on pos tagging.
N03-2032,4,"map task is appropriate because besides dialogue acts it is annotated for syntactic information, while callhome is not.4) experiment with flsa on other tasks, such as assessing text coherence."
N03-2032,1,"2) include other features in flsa, e.g. syntactic roles."
N03-2032,1,"thus, we will pursue the following directions.1) further investigate the correlation of the performance of (semi)clustered lsa with the size of the corpus and / or of the target classification."
N03-2035,1,our future works will investigate other machine learning techniques such as snow and svm.
N03-2038,1,"first, when modifying the decoder to use gtm, we used viterbi approximation at word boundaries, which means trajectory information is lost upon word transition."
N03-2038,1,"second, we plan to extend gtm to handle deletions in sloppy speech, a major challenge in lvcsr."
N03-3001,6,we believe we have only taken a first step in this direction and much remains to be done as part of future work.
N03-3001,1,"additionally, it has been suggested that statistical models such as the aspect model (hoffman, 1999) and the latent dirichlet allocation (blei et al., 2001) which generate words from a mixture of aspect-models can be exploited by modeling semantic classes as the aspects."
N03-3001,1,we will be studying the applicability of these ideas to the current task as part of our future work.
N03-3001,1,we believe the main contribution of our work lies in our attempt at incorporating semantic information in the language modeling framework and combining scores in a principled way.
N03-3003,3,we have plans to carry out additional reading experiments with good and bad readers to investigate whether the constraints we tighten to adjust the language models for poor readers actually produce more readable results.
N03-3003,2,plans for future work include expanding the size of our corpus analysis and automating at least some of the analysis and data reconfiguration.
N03-3003,1,"further on, we plan to take discourse coherence considerations into account."
N03-3003,1,we plan further development of the micro planner to prevent incoherent solutions being generated.
N03-3003,3,we will measure reading speeds and comprehension as in our preliminary experiment.
N03-3003,1,"we will generate texts under the default “good reader” models and under the constrained, poor reader, models."
N03-3004,1,we are also considering the use of other measures of similarity beyond the matching coefficient and the cosine.
N03-3004,1,"we are also exploring a number of other types of features, as well as varying the formulation of the features we are currently using."
N03-3004,3,"we have already conducted a number of experiments that vary the window sizes employed with bigrams and second order co–occurrences, and will continue in this vein."
N03-3005,1,future work will focus on improving the indexing techniques analyzed in this paper.
N03-3005,1,"possible areas of investigation are substitution tree indexing (graf, 1995) for nonstatistical methods, or restructuring decision trees (utgoff et al., 1997), while trying to maintain index operation costs at a minimum."
N03-3005,1,performance profiling combined with software and database engineering techniques will be used to determine the optimum trade-off between indexing efficiency and implementation cost.
N03-3007,4,"furthermore, we also need to investigate the performance when applying such an approach to the speech recognition results."
N03-3007,1,"finally, a unified framework for word fragment and the disfluency detection is also a future direction of our work."
N03-3009,3,"in future experiments we plan to compare select’s performance on written and spoken news texts with two recently proposed systems, u00 (utiyama 2001) and cwm (choi 2001), which have marginally outperformed the c99 algorithm on choi’s (2000) test corpus."
N03-3010,1,how to build a balanced training set with single finite state machine model will remain our important work in the future.
N03-3010,1,"for the learning mechanism, naïve bayesian learning requires more understanding of different factors’ roles and their importance."
N03-3010,2,"for the statistical learning model, the quality and the different configurations of training set highly affect the performance of models trained and thus their abilities to process sentences."
N03-3010,1,the balance of training set is also a big issue.
N04-1002,1,"second, we intend to extend our work to include new comparison and clustering approaches."
N04-1002,1,we look forward to trying out techniques on that data when it is available.
N04-1002,1,"and the subject information has apparently value in some cases, so we hope to determine how to use the information more broadly."
N04-1002,2,"first, colleagues of ours are working on a more realistic corpus that is not just large but also contains a much richer set of marked up entities."
N04-1002,1,it appears that sentence-based snippets and within document coreference information may provide a small gain.
N04-1003,1,"(3) integration of this work with other aspects of general coreference resolution (e.g., other terms like pronouns that refer to an entity) and named entity recognition (which we now take as given);"
N04-1003,4,(4) scalability issues in applying the system to large corpora.
N04-1003,2,"some of the issues that will be included in future steps are: (1) integration with more contextual information (like time and place) related to the target entities, both to support a better model and to allow temporal tracing of entities;"
N04-1004,1,"one possible future project would be to develop a set of constraints for speech-gesture alignment, and investigate the effect of these constraints on both accuracy and speed."
N04-1004,1,"only the construction of such a comprehensive end to-end system will reveal whether the algorithm and features that we have chosen are sufficient, or whether a more sophisticated approach is required."
N04-1004,1,"in this section, we describe four possible avenues of future work: dynamic programming, deeper syntactic analysis, other anaphora resolution techniques, and user adaptation."
N04-1009,6,"when the results are released to the participants, they would be asked not to look at these equating questions, and not to use them to train their systems in the future."
N04-1009,4,of particular interest is application to technology areas that use metrics other than percent of items processed correctly.
N04-1010,1,"one possible approach will be to combine our methods with alternative techniques, which were actually examined in our experiments."
N04-1010,4,our second goal is to extend our method so that it can handle multi-word hypernyms.
N04-1010,1,the first goal of our future work is to further improve the precision of our method.
N04-1010,2,"if we can obtain a multi-word hypernym such as “automobile manufacturer,” it can provide more useful information to various types of natural language processing systems."
N04-1012,1,"thus, it is likely in such situations that we will be able to develop better evolved models only after the data is annotated and more has been learned about the task."
N04-1012,3,it is then necessary to see whether improved models benefit from the examples selected using al techniques with an earlier model more than they would have if random sampling had been used.
N04-1012,5,"another issue we will explore in future work is that for a scenario in which we label a data set from scratch, it is quite possible that we will not know how best to model the task we are labeling that data for."
N04-1016,1,another possibility that needs further investigation is the combination of web-based models with supervised methods.
N04-1016,1,this can be done with ensemble learning methods or simply by using web-based frequencies (or probabilities) as features (in addition to linguistically motivated features) to train supervised classifiers.
N04-1019,1,"second, creating an initial pyramid is laborious so large-scale application of the method would require an automated or semi-automated approach."
N04-1019,1,"first, pyramid scores ignore interdependencies among content units, including ordering."
N04-1020,1,an important future direction lies in modelling the temporal relations of events across sentences.
N04-1020,5,an important question for further investigation is the contribution of linguistic and extra-sentential information to modelling temporal relations.
N04-1020,3,"apart from taking more features into account, in the future we plan to experiment with models where main and subordinate clauses are not assumed to be conditionally independent and investigate the influence of larger data sets on prediction accuracy."
N04-1022,1,"by extending the search space of the decoder to a much larger space than the -best list, we expect further performance improvements."
N04-1022,3,we expect new automatic mt evaluation metrics to emerge frequently in the future.
N04-1022,1,in future work we plan to extend the search space of mbr decoders to translation lattices produced by the baseline system.
N04-1022,1,we are developing efficient lattice search procedures for mbr decoders.
N04-1022,3,we expect new automatic mt evaluation metrics to emerge frequently in the future.in future work we plan to extend the search space of mbr decoders to translation lattices produced by the baseline system.we are developing efficient lattice search procedures for mbr decoders.
N04-1024,1,"we hope that in further investigation of this richly annotated data set, we will be able to build on the current prototype and develop a full-scale writing instruction capability that provides feedback on the coherence dimensions described in this paper."
N04-1025,5,"the problem of reading difficulty prediction lies in an interesting region between classification and regression, with close connections to ordinal regression (maccullagh, 1980) and discriminative ranking models (crammer and singer, 2001)."
N04-1026,1,"we are also exploring methods of combining information other than by feature level combination, such as data fusion across multiple classifiers (lee et al., 2002; batliner et al., 2003)."
N04-1026,1,"we are currently exploring the use of other emotion annotation schemas for emotion prediction, such as those that incorporate categorizations encompassing multiple dimensions (craggs, 2004; cowie et al., 2001) and those that examine emotions at smaller units of granularity than turns (batliner et al., 2003)."
N04-1026,5,"finally, when itspoke’s evaluation is completed, we will address the same questions for our human-computer dialogues that we have addressed here for our corresponding human-human dialogues."
N04-1026,3,"for evaluation, we would like to see whether the ordering preferences among feature sets (as in figure 5) are the same when recall, precision, and f-measure are plotted instead of accuracy."
N04-1026,5,"furthermore, we are investigating whether greater tutor response to emotions correlates with greater student learning."
N04-1027,1,"such communicative behavior, then has to be implemented in dialogue systems, to make their way of communicating more like that of their human partners."
N04-1027,1,"that is, finding out just how much extra-propositional signaling is needed to guarantee a felicitous dialogue."
N04-1027,1,as future work we propose to take the wizard and operator test paradigm introduced herein and to change and adjust the parameters of the computer-human interaction - while performing subsequent measurements of the ensuing effects - until an acceptable degree of dialogue efficiency is reached.
N04-1027,5,"in our minds, achieving dialogue quality remains an important challenge for the scientific community, but - as we have shown herein and seen in recent evaluations - dialogue efficiency constitutes another necessary condition for achieving dialogue felicity."
N04-1028,1,we will also work on integrating the confirmation prompt generation method proposed in this work with state-of-the-art confidence annotation methods.
N04-1028,1,"in the near future, our priority is to collect more data to improve the acoustic models of the system and address the specific issues related to a general non-native population, which does not share a common native language."
N04-1029,1,"to effectively use information such as user’s familiarity with the topic, the purpose of the user’s search or the user’s genre preferences we need more complex linguistic and stylistic analysis techniques."
N04-1031,3,one of our future work is to examine what kind of spoken language is suitable for such a kind of application that was illustrated in the introduction.
N04-1031,3,"however, there are other kinds of paraphrasing which are necessary in order to paraphrase written language text into spoken language."
N04-1033,5,"also, slightly relaxing the monotonicity constraint in a way that still allows an efficient search is of high interest."
N04-1033,1,"for further improvements, we will investigate the usefulness of additional models, e.g. modeling the segmentation probability."
N04-1033,1,"in spirit of the ibm reordering constraints of the single-word based models, we could allow a phrase to be skipped and to be translated later."
N04-1034,5,lack of parallel corpora is a major bottleneck in mt research.
N04-1035,1,"we suspect that such probabilistic rules could be also used in conjunction with statistical decoders, to increase the accuracy of statistical machine translation systems."
N04-1037,5,"on the other hand, predicate-argument statistics appear to provide a poor substitute for the world knowledge that may be necessary to correctly interpret the remaining cases."
N04-1038,1,"in future work, we plan to follow-up on this approach and investigate other ways that contextual role knowledge can be used."
N04-1039,3,"in the future, we would like to use our technique of examining the distribution of model parameters to see if other problems exhibit other priors besides gaussian and laplacian/exponential, and if performance on those problems can be improved through this observation."
N04-1040,4,"our method can be used as postprocessing to the methods developed by other researchers, such as topic-specific models, to create a system with even better performance."
N04-1042,3,"we also suggest better evaluation metrics to facilitate future research in this task—especially field-f1, rather than word accuracy."
N04-1042,5,fundamental advances in regularization for crfs remains a significant open research area.
N04-1043,1,"similarly, alternative clustering techniques, perhaps based on different contextual features or different distance measures, could further improve performance."
N04-1043,2,"on the application side, it would be interesting to apply the technique to other language problems."
N04-2001,5,"significant amount of research remains in handling spontaneous speech effects and nonverbal sounds, which are common in real world data."
N04-2002,3,it would be interesting to try these approaches for our task and compare them with naive bayes and n-gram approaches discussed here.
N04-2002,1,we intent to incorporate other types of features including context based features with this work.
N04-2003,1,alternative indicator clustering techniques will be explored too. a theoretical description of the relationship between em and soft tagging would potentially be able to identify convergency properties of the bootstrapping framework.
N04-2003,2,"in the future, we will try bootstrapping with bnc data."
N04-2004,3,"lacking quantitative measures currently, the merits of my proposed framework can only be gauged on theoretical grounds and its future potential to better capture a variety of linguistic phenomena."
N04-2005,5,another important issue being examined is how to evaluate the asl animation output of an mt system – in particular one that produces classifier predicates.
N04-2005,5,"currently, this project is finishing the specification of the multi-path design and investigating the following issues: deep generation techniques for creating multiple interrelated classifier predicates, surface generation of individual classifier predicates from compositional rules or parameterized templates, and asl morphological and syntactic representations for the transfer pathway."
N04-2006,3,"we plan to train and test our model on other corpora and, if possible, on writing samples of non-native speakers."
N04-2006,1,"on the other hand, the articles that were already present in the sentence provided strong hints about the correct article; this points to the need for better methods for estimating the underlying confusion matrix of a sentence."
N04-2006,1,"finally, we would like to investigate how well the rules learned by our model generalize to other genres of texts."
N04-2006,6,"for native speakers of languages that do not inflect nouns and verbs, it is a common mistake to use the root forms of nouns and verbs instead of the inflected form."
N04-2006,6,"we have also identified a few other common categories of grammatical mistakes, such as the number of the np head (singular vs. plural), and the verb tenses (present vs. past vs. continuous)."
N04-2006,1,We would like to improve the performance of the article generator.
N04-2007,1,"also, a window of characters will introduce new features to learn from."
N04-2007,2,"since bacchant decision tree learns based on context, greater context may allow for better learning, and a window of characters will expand context."
N04-2007,1,"since a decision tree's features determine how it learns from context, adding better features to the decision tree may help the tree learn better."
N04-2007,1,"as hidden markov models have been used both for name finding (bikel (1997)) and tokenization (cutting (1992)), this seems to be a promising research possibility."
N04-2008,1,"(1) using transitional probability (p(sk+1 |sk)) and mutual information measures over two adjacent segments as cues to the likelihood of word boundaries between those two segments, as suggested in e.g., (brent, 1999a).(2) developing more plausible models for approximating word-length distributions from utterance-length information, distances between stressed vowels, pause information, and other salient cues available to children.(3) incorporating stress cues (as potentially signaling both beginnings and approaching ends of content words) both alone and in combination with segmental cues."
N04-2009,1,the work on the system is ongoing and the efforts are continuing to implement a verb sense disambiguation component and to test the conceptual graph construction module.
N04-2010,5,an interesting new topic for speaker adaptation could be joint structure and parameter adaptation.
N04-3002,1,"our next step will be to modify the dialogue manager inherited from why2-atlas to use new tutorial strategies optimized for speech, and to enhance itspoke to predict and adapt to student emotion."
N04-3006,4,"we believe that the semantic parser will prove useful for a range of language processing applications that require knowledge of text meaning, including word sense disambiguation, information extraction, question answering, machine translation, and others."
N04-4001,3,"finally, the potential impact of our findings with respect to recent evaluation metrics should not be overlooked."
N04-4001,4,"in the future, we wish to apply our method to other corpora, and to explore the extent to which different summarization goals, such as describing an event or providing a biography, affect the degree to which humans employ rewriting as opposed to extraction."
N04-4001,4,"we would like to empirically quantify to what extent current summarization systems reformulate text, by applying the techniques presented in this paper to system output."
N04-4003,1,future work will also focus on the integration of the proposed rescoring formula in the decoding process.
N04-4006,1,"furthermore, it is important to closely match testing conditions for perceptron training."
N04-4008,4,we expect the final bilingual framenet will provide a valuable resource for multi-lingual or cross-lingual natural language processing.
N04-4008,1,"the next step is to automatically extract semantically annotated chinese sentences based on the annotated english sentences in framenet, the aligned framenet lexical entries, and bilingual corpora."
N04-4009,1,"it is thus at least conceivable that a self-trained approach, coupled with a large set of features and a large corpus of raw data, could eventually overtake the performance of the best supervised models."
N04-4011,1,a successful approach will need to consider both algorithmic requirements and technology limitations.
N04-4012,3,"without a gold standard for a generation system for dynamic multimodal user interfaces to qualitatively compare against, controlled user trials will allow us to evaluate the usability of the interfaces we have created."
N04-4012,3,"task completion times, user frustration levels, and user satisfaction can then be used to evaluate the success of this model of multimodal interactions."
N04-4012,5,"as possible examples for future applications, we see a multimodal interface that allows mobile users or users with sensory impairments to traverse information-rich social networks, and a kiosk for multimodal, multilingual access to public transportation options."
N04-4012,3,further evaluation will show whether the fitness function can accurately mirror user satisfaction with a given output variant and whether our form of adaptivity is actually an advantage to users on the go.
N04-4013,1,we are investigating multi-document summarization techniques which might allow users to better pinpoint the category in which a relevant document might be found.
N04-4013,6,we would like users to be able to more readily identify the class into which a relevant document (if one exists) would be found.
N04-4016,4,the creation of rules for correction utterances based on the dialog history could be applicable to dialog systems which use speech recognition or natural language processing and other kinds of rules beyond regular grammars; we plan to study this in future work.
N04-4016,4,"we also plan to apply the proposed method to other types of dialogs, such as user-initiative dialogs and mixed-initiative dialogs."
N04-4016,1,we are also planning to develop an algorithm to improve the precision of corrections that are based on the set of recognition candidates for the correction utterance and an error recovery strategy.
N04-4018,4,we also plan to extend these methods to additional tasks such as disfluency detection.
N04-4018,1,"this will provide opportunities to investigate the interaction of automatic word recognition and structural metadata, hopefully resulting in reduced wer."
N04-4018,1,future work will involve a tighter integration of su detection and word recognition by including su events directly in the recognition lattice.
N04-4019,1,we also plan to refine speech graffiti’s runtime help facilities in order to assist users more effectively in saying the right thing at the right time.
N04-4019,4,"in addition to these core interface goals, we plan to extend the functionality of speech graffiti beyond information access to support the creation, deletion and modification of information in a database."
N04-4019,1,"to improve the habitability of speech graffiti, we plan to explore allowing more natural language-esque interaction while retaining an application-portable structure."
N04-4019,5,how can we help users who are having severe difficulties with an interface learn how to use it better and faster?
N04-4019,4,"for speech graffiti, scores for habitability (represented by statements like “i always knew what to say to the system”) were typically the lowest of any of the six user satisfaction factors, suggesting that this is a prime area for further work."
N04-4019,5,these users have become a motivator of future work: what can be done to make the interface work for them and others like them?(future studies will focus on a broader population of adults.)
N04-4020,1,an implementation of this “sharpening” algorithm is currently under development.
N04-4020,6,"repeating this alternation until the weights show minimal change will minimize the contributions of unreliable annotators and poorly annotated messages to the assignment of labels to messages, thereby increasing confidence in the results."
N04-4021,1,"in addition, we are currently investigating extensions to the model, including context-dependent feature substitutions."
N04-4021,2,we also plan to extend this study to a larger data set and to multi-word utterances.
N04-4021,1,we are currently working on a new formulation in which the synchronization constraints can be trained via em.
N04-4022,1,"instead, the focus of the work is to improve the specific aspects of the asr output that may adversely affect a user-centered task like information retrieval."
N04-4022,3,"while we have not formally evaluated the impact of our error detection and correction on retrieval performance, there is an obvious benefit to correcting misrecognitions of the specific query term that a user is seeking."
N04-4025,5,"however, up to recently, the large amounts of transcript data have limited researchers from performing analyses of team discourse."
N04-4025,5,research into team discourse is a new but growing area.
N04-4025,1,"we are currently exploring two promising avenues to predict performance in real time: integration of speech recognition technology, and inter-turn tag sequences."
N04-4027,1,"we intend to learn predictors for some other thread aspects such as thread category and question-answer pairs, and then use these as input to the sentence extraction procedure."
N04-4027,3,we also intend to perform an evaluation based on human feedback.
N04-4027,1,"we can use this insight to instruct human annotators, and to improve the automatic extraction."
N04-4028,1,"we hypothesize this is because crfs are already discriminative (not joint, generative) models; furthermore, this may suggest that future discriminative parsing methods will also have the benefits of discriminative reranking built-in directly."
N04-4030,1,"in addition, we are currently designing usability studies in which we will present different categorization suggestions to information architects to organize."
N04-4030,6,"their subjective reactions, the amount of time it takes them to create the organizations, and the resulting quality and coverage of the organizations, as measured by users performing navigation tasks using the hierarchies, will be compared to other techniques."
N04-4032,4,"in addition, just as the slm is useful for both parsing and language modeling, it could be used to predict metadata for its own sake or to improve word recognition, with or without the word-based representation."
N04-4032,1,"further research using the structured language model could incorporate these metadata directly into the model, allowing it to take advantage of higher-level metadata without reducing the effective number of words available to the model."
N04-4034,1,latent semantic analysis of stream a might also be usefully employed here.
N04-4034,2,"one can imagine that higher level information (e.g., a dialog or other speech act) about the other speakers might be particularly important."
N04-4034,1,"furthermore, more than one word from stream a can be included in the context to provide additional predictive ability."
N04-4035,1,"we also plan to explore the integration of prosodic and textual features and investigate the identification of more fine-grained sub-topic structure, to provide more focused units for information retrieval, summarization, and anaphora resolution."
N04-4035,1,we would like to integrate speaker identification for normalization and speaker change detection.
N04-4036,5,"unfortunately, meaningful comparisons to these efforts are difficult due to differing evaluation metrics."
N04-4037,4,"although we have flattened the trees in the propbank corpus for our experiments, the proposed language structure supports flat annotation from scratch, which we believe is useful for porting the method to other domains and languages."
N04-4037,1,"while our initial results have been encouraging, this work must be extended and enhanced to produce the quality of semantic parse produced by systems using a full syntactic parse."
N04-4038,2,"we are currently trying to improve the performance of the systems by using additional features, a wider context and more data created semi-automatically using an unannotated large arabic corpus."
N04-4038,1,"in addition, we are trying to extend the approach to semantic chunking by hand labeling a part of arabic treebank with arguments or semantic roles for training."
N04-4039,3,another important direction is to evaluate the effectiveness of agent gestures in actual human-agent interaction.
N04-4039,1,"we expect that if our model can generate gestures with appropriate timing for emphasizing important words and phrases, users can perceive agent presentations as being more alive and comprehensible."
N04-4039,1,"we plan to enhance our model by incorporating more general discourse level information, though the current system exploits cue words as a very partial kind of discourse information."
N04-4039,1,we plan to conduct a user study to examine this hypothesis.
N04-4040,1,"it does appear to be a promising technique for future research however. eventually it may prove that more elaborate acoustic cues will be needed to identify these edits, at which point a model of interruption points could be included as a feature in the rules learned by the system."
N06-1002,1,we plan to consider other approaches for conditioning on context.
N06-1002,1,we hope to replace them with effective models without the brittleness and sparsity issues of heavy lexicalization.
N06-1003,3,"finally, we plan to formalize our targeted manual evaluation method, in the hopes of creating a evaluation methodology for machine translation that is more thorough and elucidating than bleu."
N06-1003,2,"in future work, we plan to determine how much data is required to learn useful paraphrases."
N06-1004,1,"once we have found the best segmentation model, we will improve the system’s current naïve single-word segmentation of the remaining source sentence during decoding, and construct a more accurate future cost function for beam search."
N06-1004,1,"another obvious system improvement would be to incorporate more advanced word-based features in the dts, such as questions about word classes (tillmann and zhang 2005, tillmann 2004)."
N06-1004,1,we also plan to apply scms to rescoring n-best lists from the decoder.
N06-1004,1,"finally, we are contemplating an entirely different approach to dt-based scms for decoding."
N06-1006,1,"in future, we aim to combine more precise modeling of monotonicity effects with better modeling of paraphrase equivalence."
N06-1007,2,"it is very likely that many common event participants appearing in such proximity are referred to by coreferential expressions, and therefore noticeable improvement can be expected from applying coreference resolution to the corpus prior to learning entailment patterns from it."
N06-1011,3,"to this end, we would like to compare the performance of an ner system trained on a corpus tagged using this approach to one trained on a hand-tagged corpus."
N06-1012,1,"in this work, we choose the subsets based on our intuition of which features are complementary for this task, but automatically determining the feature subsets is an interesting area for future work."
N06-1013,5,"however, additional studies are needed to investigate why huge improvements in aer result in relatively smaller improvements in bleu scores."
N06-1014,1,"while aer is only a weak indicator of final translation quality in many current translation systems, we hope that more accurate alignments can eventually lead to improvements in the end to-end translation process."
N06-1017,1,"possibilities include associating items with similar existing senses (widdows, 2003; curran, 2005; burchardt et al., 2005) or clustering them into approximate senses."
N06-1017,6,"once items have been identified as unknown, they are available for further processing: if possible one would like to assign some measure of sense information even to these items."
N06-1017,1,"our immediate goal is to use unknown sense detection in combination with wsd, to filter out items that the wsd system cannot handle due to missing senses."
N06-1018,1,finally we would like to expand our coverage of temporal expressions to include other types of expressions such as recurrence expressions10.
N06-1018,1,we also need to investigate the disambiguation procedure and possibly migrate the functionality into a separate discourse module.
N06-1018,1,"in addition, the co-referencing tendency of noun-modifying expressions could lead to a better way to anchoring this particular type of temporal expressions."
N06-1021,1,in future work we intend to explore the discriminative reranking of n-best lists produced by these parsers and the incorporation of morphological features.
N06-1023,1,"in the future, by incorporating ellipsis resolution, we will develop an integrated model of syntactic, case and ellipsis analysis."
N06-1024,4,"we also expect that productivity of syntactic annotation of further genres of english will be significantly enhanced by the use of this new tool, and hope to have practical evidence of this in the near future."
N06-1024,4,"most importantly from the point of view of the authors, we have constructed a system that recovers sufficiently rich syntactic structure based on the penn treebank to provide rich syntactic guidance for the recovery of predicate-argument structure in the near future."
N06-1025,4,"future work will include turning wikipedia into an ontology with well defined taxonomic relations, as well as exploring its usefulness of for other nlp applications."
N06-1027,1,we are also exploring the application of graph-based algorithms to other structured-objects ranking problems in nlp so as to improve system performance while relieving human costs.
N06-1027,1,the tradeoff and balance between system performance and human cost for different learning algorithms is of great interest.
N06-1027,1,supervised learning is another approach to detecting conversation focus that might be explored.
N06-1028,6,"furthermore we are aware that, maybe with the exception of the classes related to fluency, rate and length, our feature set is as of yet quite rudimentary and will need significant expansion in order to obtain a broader coverage of communicative competence."
N06-1028,1,"in summary, future work will focus on improving speech recognition, and on significantly extending the feature sets in different categories."
N06-1028,1,an important step for future work will be to train the acoustic and language models of the speech recognizer directly from our corpus; we are additionally planning to use automatic speaker adaptation and to evaluate its benefits.
N06-1029,1,we also hope to integrate a richer contextual representation of tone and intonation consistent with phonetic theory within this unsupervised and semisupervised learning framework.
N06-1029,1,"future work will consider a broader range of tone and intonation classification, including the richer tone set of cantonese as well as bantu family tone languages, where annotated data truly is very rare."
N06-1029,1,we will further explore improvements in classification accuracy based on increases in labeled and unlabeled training examples.
N06-1032,1,"future work thus will concentrate on improvements of in-coverage translations e.g., by stochastic generation."
N06-1032,2,"furthermore, we intend to apply our system to other language pairs and larger data sets."
N06-1033,1,"we believe that extensions of our technique to more powerful models such as synchronous tree-adjoining grammar (shieber and schabes, 1990) is an interesting area for further work."
N06-1034,1,in future work we will use such features in our prediction models.
N06-1034,2,"finally, we are also annotating tutor and student dialogue acts and automating the tutor act annotations; when complete we can investigate their usefulness in our prediction models; dialogue acts have also been used in prior paradise applications (moller, 2005a)."
N06-1035,1,"in the short term, we are investigating whether other metrics such as entropy and confidence bounds can better indicate the usefulness of a feature."
N06-1035,1,we are investigating how well an automated certainty and frustration detection algorithm will impact the % policy change.
N06-1035,2,"with respect to future work, we are annotating more human-computer dialogue data and will triple the size of our test corpus allowing us to create more complicated states since more states will have been explored, and test out more complex tutor actions, such as when to give hints and restatements."
N06-1036,5,we have not yet been able to combine the benefits of both an hbm and prosody information.
N06-1037,3,"in the future, we would like to test our algorithm on the other version of the ace corpus and to develop fast algorithm (vishwanathan and smola, 2002) to speed up the training and testing process of convolution kernels."
N06-1038,1,inference in these highly connected models will likely require approximate methods.
N06-1038,1,"additionally, we wish to focus on extracting implicit relations, dealing more formally with the precision-recall trade-off inherent in applying noisy rules to improve extraction."
N06-1038,1,"also, we plan to improve upon iterative database construction by performing joint inference among distant relations in an article."
N06-1038,2,"in the future, we wish to explore extending this methods to larger datasets, where we expect relational patterns to be even more interesting."
N06-1040,4,"more generally, the ideas and method for determining structural zeros (vs. sampling zeros) can be used in other contexts for a variety of other learning tasks."
N06-1045,1,"we plan for our weighted determinization algorithm to be one component in a generally available tree automata package for intersection, composition, training, recognition, and generation of weighted and unweighted tree automata for research tasks such as the ones described above."
N06-1046,5,"another important issue, not addressed in this work, is the interaction of our aggregation method with content selection and surface realization."
N06-1046,1,an appealing future direction lies in integrating learning and inference in a unified global framework.
N06-1047,6,there are likely to be differences regarding usefulness of certain features due to the icsi meetings being relatively unstructured and informal and the ami hub meetings being more structured with a higher information density.
N06-1047,2,"finally, we will apply these methods to the ami corpus [18] and create summaries of comparable length for that meeting set."
N06-1047,5,"of particular interest to us are features relating to speaker status, i.e. features that help us determine who is leading the meeting and who it is that others are deferring to."
N06-1047,5,we would also like to more closely investigate the relationship between areas of high speaker activity and informative utterances.
N06-1047,1,"in the immediate future, we will incorporate these features into a machine-learning framework, building support vector models trained on the extracted and non-extracted classes of the training set."
N06-1050,3,"thus, bpref may offer a solution to the incompleteness problem and we intend to investigate its potential use in our future evaluations."
N06-1050,1,"when finished, we hope our test collection will be a generally useful ir resource."
N06-1050,3,"in particular, we expect the collection to be useful for experimentation with citation information, for which there is currently no existing test collection with the properties that ours offers."
N06-1051,6,these matter any statistical inference on shallow pools.
N06-1051,1,it is necessary to include other metasearch methods for further study.
N06-1051,3,this will allow us to validate not only the impact of the metasearch training principle based on pairwise ranking error rloss but also the capacity of automatic feature selection of the two ranking algorithms used in this paper.
N06-1051,6,"second, some topics might even have no relevant document in shallow pools."
N06-1054,3,"our constraint relaxation method should be tested on problems other than semantic role labeling. the method should also be evaluated on a task with longer sequences: though the finite-state operations we use do scale up linearly with the sequence length, longer sequences have more chance of violating a global constraint somewhere in the sequence, requiring us to apply that constraint explicitly."
N06-1055,5,"our results also show that the k-best propositions produced by the local classifier have a very high oracle score, which perhaps indicates a promising path that deserves further exploration, based on careful analysis of the errors."
N06-1055,1,we intend to continue to experiment with new features and parameters for the reranking algorithm.
N06-1056,1,"in the future, we would like to develop a word-based alignment model that is aware of the mrl syntax, so that better lexicons can be learned."
N06-1057,3,we plan to use paraeval to investigate the impact of these changes on paraphrase quality under the assumption that better paraphrase collections lead to better summary evaluation results.
N06-1057,3,"and with paraeval, it is possible for us to evaluate systems that do incorporate some level of abstraction, especially paraphrasing."
N06-1058,1,"in the future, we would like to incorporate substitutions at the level of phrases and syntactic trees."
N06-1059,1,"(2) tune the bayesian smoothing parameter p to further examine the effect of smoothing,"
N06-1059,1,(3) develop better content generation model and
N06-1059,2,"with the initial success of this study, we would like to: (1) verify the results with other set of data, for example, duc 2003 data,"
N06-1059,1,(4) add synonym and paraphrase matching capability in the future .
N06-1061,4,"possible applications include information retrieval, text clustering/classification and summarization."
N06-1061,1,"based on the success of our model, we will investigate various graph based relationships for explaining semantic structure of text collections in the future."
N06-1062,1,"future work will be the further development of also the grammatical morph-based language models and comparison of that to the current approach, as well as extending this evaluation work to new languages."
N06-2002,1,our future research includes building a corpus navigation system to dynamically explore the full feature space.
N06-2002,2,large scale experiments will be conducted in the near future to see if the success of the smaller experiments will carry over to a larger scale.
N06-2002,1,using machine learning we will use information detected from translated sentences in order to decide what parts of the feature space are redundant and what parts must be explored and translated next.
N06-2002,1,"additionally, we will change from using humans to write sentences and context fields to having them generated by using a natural language generation system (alvarez et al.2005)."
N06-2003,1,we are currently exploring ways of bootstrapping a model from a small amount of hand labeled data in combination with lexical cohesion (tuned for high precision and consequently low recall) and some reliable discourse markers.
N06-2004,1,"in future work, we plan to explore distributional methods for modeling relatedness, as well as the use of text-based information to improve correlations with the human data, as judgments are situated in specific textual contexts."
N06-2007,1,in the future we would like to investigate how to select more useful feature stream and whether feature selection method can improve the performance of our graph based semi-supervised relation extraction.
N06-2008,2,further work is being carried out to extend the system to chinese and arabic.
N06-2008,1,current research is aiming at improving the accuracy of the classifier by using the non-periodic components and improving the combined classification method.
N06-2009,1,"in future work, we are interested in developing effective filtering techniques to reduce our candidate set to a small number of high precision paraphrases, in experimenting with state-of-the-art paraphrasers, and in using paraphrasing to improve the stability of the qa system."
N06-2011,1,"to do this, we could use information from the eigenvalues or the distribution of points in the clusters."
N06-2011,1,we would like to locate and extend this region for spectral clustering.
N06-2011,3,"as future work, we would like to analyze the variation in performance as the amount of data increases."
N06-2011,1,"also, it would be interesting to compare the clusters obtained with spectral clustering and the part of speech tags of the words in the same cluster, especially for languages such as english where good taggers are available."
N06-2011,1,"finally, an important direction of research is in automatically selecting the number of clusters for the clustering algorithm."
N06-2013,6,we plan to study additional variants that these results suggest may be helpful.
N06-2013,1,"in particular, we plan to include more syntactic knowledge and investigate combination techniques at the sentence and subsentence levels."
N06-2014,3,"in addition, it is important to assess the impact of semi-supervised training with recognizer output, where gains from using unlabeled data may be greater than with reference transcripts as in (hillard et al., 2003)."
N06-2015,2,"in intent and in many details, ontonotes is compatible with all these efforts, which may one day all participate in a larger multilingual corpus integration effort."
N06-2016,5,"the challenges of clef cl-sr task will continue to expand in subsequent years as new collections are introduced (e.g., czech interviews in 2006)."
N06-2016,1,in future work we plan to investigate methods of removing or correcting some of the speech recognition errors in the asr transcripts using semantic coherence measures.
N06-2016,1,in ongoing further work we are exploring the relationship between properties of the collection and the weighting schemes in order to better understand the underlying reasons for the demonstrated effectiveness of the mpc.ntn weighting scheme.
N06-2017,1,we will also try to supplement m.nocb with other features of coherence to improve its performance.
N06-2017,3,"in our future work, we intend to directly evaluate their method using a substantially large number of alternative orderings and m.nocb as the baseline."
N06-2019,3,we also plan to evaluating the benefit of disfluency modeling in bleaching speech data for text-based machine translation.
N06-2020,2,in the future we plan to investigate the influence of the context size on sense discrimination performance.
N06-2021,1,"we will investigate other features to improve the classification results, such as name information, acoustic or prosodic features, and speaker clustering results (considering that the same speaker typically has the same role tag)."
N06-2021,3,"we plan to examine the effect of using speech recognition output, as well as automatic speaker segmentation and clustering results."
N06-2021,2,analysis of difference news sources may also reveal some interesting findings.
N06-2022,3,we will then be able to test whether the accuracies we achieve are sufficient and explore methods for improving them.
N06-2022,1,"in future work, we plan to integrate these models in a dialogue system to adapt the system’s language generation;"
N06-2027,2,"future work includes an implementation of a system with full access for alternative devices, expansion of the underlying lexicon for hebrew generation, and adding voice output."
N06-2028,1,"currently, we are performing more sophisticated experiments on different ways to exploit additional audiovisual cues."
N06-2028,3,our next plan is to conduct an extensive comparison between glossex and the proposed scheme.
N06-2028,1,there is also room for improving the calculation of the incentive values of keywords.
N06-2029,1,further investigations should analyze the characteristics of the variant corpora in more detail and focus on the automatic identification of specific linguistic phenomena that could be helpful to measure how good an input sentence is covered by a specific model.
N06-2030,3,"modern yi stands as a single, but questionable, counterexample to this observation, and for it to be visible in sproat’s grid (with writing systems arranged along only the diagonal), one would need an objective and verifiable means of discriminating between consonantal and vocalic scripts."
N06-2031,1,future systems may also align their output with their recognition capabilities and actively align with the user to signal understanding.
N06-2032,2,our further research will address the study of vocal cues to segmentation in arabic bn.
N06-2034,1,"there are many future directions, which include 1) applying other machine learning methods,"
N06-2034,1,2) analyzing discourse relation categorization strategy
N06-2034,2,3) including a longer context beyond two sentences.
N06-2036,6,we intend to explore this possibility in future extensions of this work.
N06-2037,1,we are currently exploring ways to use multiple bagged in-domain language models for the selection process.
N06-2037,1,"instead of sequential scan of the corpus, we are exploring the use of rank-and-select methods to give a better search sequence."
N06-2038,1,"to combine the strengths of different tagging strategies, ensemble meta-strategies utilizing the results of multiple strategies could be explored."
N06-2038,4,future work will be to observe how well these results generalize in the context of other classifiers and other corpora.
N06-2039,1,"in summary, we observe very interesting clusters of verbs which indeed require more in depth lexical semantic study as msa verbs in their own right"
N06-2040,4,"in the future, the generation procedure for our interactive agent will be further developed in areas such as spatial descriptions and surface realization."
N06-2040,1,"we also plan to investigate whether different object types in the domain require differential processing, as prior work on spatial semantics would suggest."
N06-2043,1,"our directions for future research include experimenting with other machine learning techniques, utilizing the newly-gained knowledge of the tickets’ sublanguage grammar, as well as testing sublanguage analysis technology on other types of field service reports."
N06-2044,1,we also plan to investigate other approaches to strategy summarization.
N06-2044,3,"finally, we will evaluate our approach against purely rl-based methods."
N06-2044,1,"in future work, we intend to exploit this generalization feature further by developing systems that require much larger state representations."
N06-2045,1,"there are many things that remained to be done with retriever, including extracting paragraphs from non-html documents, auto hyperlinking topics within retriever pages (as in wikipedia), finding more up-to-date sources for categorization, and verticalizing retriever page generation for different types of topics (e.g. treating movies differently than people and both differently than diseases)."
N06-3001,1,"in the next stage of my research, i will focus on integrating previous efforts into a complete multimodal model for structural event detection."
N06-3001,4,my research is expected to support adding multimodal perception capabilities to current human communication systems that rely mostly on speech.
N06-3001,5,i am also interested in investigating mutual impacts among the structural events.
N06-3001,1,"for example, we will study sus and their relationship to floor control structure."
N06-3001,1,"in my thesis, i hope to better understand the role that the non-verbal cues play in assisting structural event detection."
N06-3001,1,"given progress in structural event detection in human communication, i also plan to utilize the detected structural events to further enhance meeting understanding."
N06-3001,1,i will also investigate alternative integration architectures to the hmm shown in figure 2.
N06-3001,1,"in particular, i will improve current gesture feature extraction, and expand the non-verbal features to include both eye gaze and body posture."
N06-3001,1,"a particularly interesting task is to locate salient portions of a meeting from multimodal cues (chen, 2005) to summarize it."
N06-3002,4,"it is hoped that this research will not only improve processing of spoken natural language, but also enhance our understanding of how speakers use gesture to structure their discourse."
N06-3002,1,"in one case, we are looking for a similarity between the disfluency and the repair point; in the other case, we are looking for similarities across all disfluencies, or across all repair points."
N06-3002,3,"using the subsection of the corpus in which no explanatory aids were provided, i will investigate how to assess the similarity of such dynamic gestures, in the hope that coreference resolution can still benefit from gestural cues in this more general case."
N06-3002,4,disfluency repair is another plausible domain in which gesture might improve performance.
N06-3002,1,"alternatively, gesture could play a pragmatic function, if there are characteristic gestures that indicate restarts or other repairs."
N06-3003,1,we hope to explore the space between batch mode and a fully interactive system to discover the optimal setting which allows the system to only ask the user for further interaction when it cannot determine the appropriate refinement operation or when it would be impossible to correctly refine the grammar and the lexicon automatically.
N06-3003,1,"however, if we can detect such rule dependencies before the refinement process, then we can try to find an optimal ranking, given the current set of cis, which should result in higher translation accuracy, as measured on a test set."
N06-3003,1,"in an interactive mode, the system can use active learning to produce minimal pairs to further investigate which refinement operations are more robust, treating the bilingual speaker as an oracle."
N06-3003,1,another interesting future direction is enhancing the rule refinement system to allow for further user interaction.
N06-3005,3,"the unique sentence level perspective modeling can automatically identify sentences that are strongly representative of the perspective of interest, and we plan to manually evaluate their quality in the future work."
N06-3007,1,our goal is to develop a multilevel measure of document similarity that will be helpful for summarization and information extraction.
N06-3009,4,"in the future, templates that can match long contextual relations and coordinated nes may be applied to ner postprocessing."
N06-3010,1,future work will focus on the interaction structure and construction and testing of the integrated system.
N06-3010,1,through this we hope to improve information retrieval and human computer interaction.
N06-4008,1,"future plans include adding more scalable embedding algorithms, and allowing other output formats."
N07-1001,1,the prosody labels by themselves may or may not improve the translation accuracy but they provide a framework where one can obtain prosody labels in the target language from the speech signal rather than depending on a lexical prosody prediction module in the target language.
N07-1002,3,"finally, prominence prediction classifiers need to be incorporated in a speech synthesis system and their performance should be gauged via listening experiments that test whether the incorporation of prominence leads to improvement in synthesis."
N07-1002,1,thus in future work we plan to incorporate more acoustic and phonological features.
N07-1003,2,"in our future work, we plan to examine initiative conflicts in face-to-face multi-party conversation, such as the icsi corpus (shriberg et al., 2004)."
N07-1003,1,"in our future work, we plan to build an initiative model to capture this negotiation process."
N07-1004,5,another problem that has been pointed out in section 6 and in section 7 is the different functional roles of dm dialogue acts in current annotations.
N07-1005,1,we are also planning to expand the method and improve the accuracy of the automatic sub-goal generation and determination of the rate of accomplishment of sub-goals.
N07-1005,2,"in the near future, we would like to expand the test set to improve the upper bound obtained by our method."
N07-1005,3,we believe that future research would allow us to develop high-quality mt systems by tuning the system parameters based on the automatic mt evaluation measures.
N07-1005,3,further advanced generation and estimation would give us information about the erroneous parts of mt results and their quality.
N07-1005,1,the sub-goals of a given sentence should be generated by considering the complexity of the sentence and the alignment information between the original source-language sentence and its translation.
N07-1006,1,"by examining the contribution of each component metric, we find that metrics showing different properties of a sentence are more likely to make a good combined metric."
N07-1007,1,we plan to extend and generalize the current approach to cover these phenomena in morphologically complex languages in general in the future.
N07-1008,1,"many city names and dates in arabic can not be handled by such blocks and in future work we intend to investigate the utilization of more complex blocks as necessary. however, in future work we intend to investigate feature selection using the language model as a prior which should result in much smaller systems."
N07-1011,1,"additionally, we are investigating more sophisticated inference algorithms that will reduce the greediness of the search procedures described here."
N07-1011,4,future work will extend our approach to a wider variety of tasks.
N07-1012,1,"in addition to using srms for retrieval, we are currently extending the ideas to provide field validation and suggestions for data entry and validation: the same ideas used to find documents with missing field values can also be used to suggest potential values for a field and to identify values that seem inappropriate."
N07-1012,1,our work is continuing by exploring methods for handling fields with incorrect or corrupted values.
N07-1012,5,"the challenge becomes more than just inferring what values might be there; it requires combining likely missing values with confidence in the values already present: if an audience field contains ’undergraduate’, it should be unlikely that ’k-6’ would be a plausible value, too."
N07-1012,1,"we have also begun explorations toward using inferred values to help a user browse when starting from some structured information— e.g., given values for two fields, what values are probable for other fields."
N07-1013,5,"in addition, we will explore the issue of parameter learning, and user feedback (e.g., “this item should be ranked higher.”)."
N07-1013,4,"we also plan to apply grasshopper to a variety of tasks, including information retrieval (for example ranking news articles on the same event as in google news, where many newspapers might use the same report and thus result in a lack of diversity), image collection summarization, and social network analysis for national security and business intelligence."
N07-1013,1,"as future work, one direction is “partial absorption,” where at each absorbing state the random walk has an escape probability to continue the random walk instead of being absorbed."
N07-1014,1,"for further work, an obvious step is to improve the word generator so that it produces morphologically more plausible sequences of letters and to intertwine both generators for the emergence of word categories."
N07-1014,1,"furthermore, it is desirable to embed the random generator in models of communication where speakers parameterize language generation of hearers and to examine, which structures are evolutionary stable (see jäger, 2003)."
N07-1014,6,this would shed light on the interactions between different levels of human communication.
N07-1015,1,"in our future work, we will study how to automatically conduct task-oriented feature search, feature pruning and feature weighting using statistical methods instead of heuristics. in the future, we will study the effectiveness of these global features."
N07-1017,4,"in the future, we plan to compare lsa to other term similarity measures, to train the lsa model on large open domain corpora and to apply our technique to both generic and specific corpora in different domains."
N07-1017,1,"we want also to increase the level of integration of the lsa technique in the espresso algorithm, by using lsa as an alternative reliability measure at each iteration."
N07-1017,1,"we will also explore the domain restriction property of semantic domains to develop open domain ontology learning systems, as proposed in (gliozzo, 2006)."
N07-1017,5,the domain restriction hypothesis has potential to greatly impact many applications where matching textual expressions is a primary component.
N07-1017,1,"it is our hope that by combining existing ranking strategies in applications such as information retrieval, question answering, information extraction and document classification, with knowledge of the coherence of the underlying text, one will see significant improvements in matching accuracy."
N07-1021,6,the lesson from nlu and mt appears to be that higher quality results when the symbolic and statistical paradigms join forces.
N07-1024,1,future work may explore alternative ways to combine these models to make better use of contextual information.
N07-1025,1,"given that wikipedia is growing at a fast pace, the curve suggests that the accuracy of the word sense classifiers built on this data is likely to increase for future versions of wikipedia."
N07-1025,6,another aspect we were interested in was the correlation in terms of sense coverage with respect to other sense annotated data currently available.
N07-1028,1,"as part of future work, we plan to conduct a more elaborate study with more interaction strategies included."
N07-1028,1,better techniques to select effective subqueries are also in the pipeline.
N07-1028,5,"since we used mutual information as the basis for most of our subquery selection procedures, we could not consider sub-queries that comprised of a single term."
N07-1028,6,we plan to address this issue too in future work.
N07-1029,5,it would also be interesting to investigate how much different systems contribute to the overall gain achieved via system combination.
N07-1029,5,the focus of the future work will be to address the individual issues in the combination methods mentioned above.
N07-1030,1,"in future work, we will explore the use of global constraints, similar to those used by (barzilay and lapata, 2006) to improve both precision and recall."
N07-1030,2,"we will also consider linguistic constraints (e.g., restrictions on pronouns) in order to improve precision."
N07-1030,1,"for example, we expect transitivity constraints over coreference pairs, as well as constraints on the entire partition (e.g., the number of entities in the document), to help considerably."
N07-1033,1,a natural approach would start with log-linear models in place of svms.
N07-1033,1,"while our experiments in this paper used a discriminative svm, we plan to explore generative approaches."
N07-1033,1,"we would like to explicitly model rationale annotation as a noisy process that reflects, imperfectly and incompletely, the annotator’s internal decision procedure."
N07-1035,1,"as an extension of this work, we are currently investigating in more detail what makes some mdp’s reliable or unreliable for a certain data size (such as the case where baseline 2 does not converge but a more complicated model does, such as concept repetition)."
N07-1036,5,"during human machine conversation, how is eye gaze aligned with speech production?"
N07-1036,5,are there any other factors such as interface design and visual properties that will affect eye gaze behavior and therefore attention prediction?
N07-1036,5,how reliable is eye gaze for attention prediction?
N07-1036,5,the answers to these questions will affect how eye gaze should be appropriately modeled and used for language processing.
N07-1037,6,we assumed that each word has a semantic orientation.
N07-1037,1,"however, word senses and subjectivity have strong interaction (wiebe and mihalcea, 2006).the value of α must be properly set, because lower α can be better for the seed words added by the classifier, • to address word-segmentation problem discussed in section 5.3, we can utilize the fact that the heads of compound nouns often inherit the property determining the semantic orientation when combined with an adjective.• the semantic orientations of pairs consisting of a proper noun will be estimated from the named entity classes of the proper nouns such as person name and organization."
N07-1038,3,we also plan to theoretically analyze the convergence properties of this and other joint perceptron algorithms.
N07-1038,1,an avenue for future research is to consider the impact of additional rhetorical relations between aspects.
N07-1039,3,"immediate future work includes extending the approach to include other types of appraisal expressions, such as where an attitude is expressed via a noun or a verb."
N07-1039,1,"in this regard, we will be examining extension of existing methods for automatically building lexicons of positive/negative words (turney, 2002; esuli and sebastiani, 2005) to the more complex task of estimating also attitude type and force."
N07-1039,2,"as well, a key problem is the fact that evaluative language is often context-dependent, and so proper interpretation must consider interactions between a given phrase and its larger textual context."
N07-1040,1,"in particular, we plan to investigate the use of scientific attribution information for the citation function classification task."
N07-1042,1,an interesting area of future work is the application of data mining to search for appropriate constraints to integrate into this model.
N07-1043,4,"in future, we intend to apply the proposed method to automatically extract synonyms from the web."
N07-1044,3,"future work must assess whether the models presented in this paper can be extended to alternative sense inventories (e.g., dictionary definitions) that may differ in granularity and structure."
N07-1044,3,an important future direction lies in evaluating the disambiguation potential of our models across domains and languages.
N07-1044,1,we will also experiment with a wider range of lexical association measures for quantifying the similarity of a word and its synonyms.
N07-1045,5,we plan to explore the question of which inventory of near-synonyms or similar words is the most suitable for use in the intelligent thesaurus.
N07-1045,6,"in case the target word selected by the writer has multiple senses, they could trigger several groups of near-synonyms."
N07-1045,6,the system will decide which group represents the most likely senses by computing the semantic coherence scores averaged over the near-synonyms from each group.
N07-1045,1,future work includes a word sense disambiguation module.
N07-1047,6,"it has been reported that syllabification can potentially improve pronunciation performance in english (marchand and damper, 2005)."
N07-1047,2,we are investigating the possibility of integrating syllabification information into our system.
N07-1047,1,"we plan to explore other sequence prediction approaches, such as discriminative training methods (collins, 2004), and sequence tagging with support vector machines (svm-hmm) (altun et al., 2003) to incorporate more features (context information) into the phoneme generation model."
N07-1047,4,We are also interested in applying our approach to other related areas such as morphology and transliteration.
N07-1048,1,"the flms might work well also for the other languages, and in fact, to do justice to the more advanced morph models from later versions of morfessor, flms or some other refined techniques may be necessary as a complement to the currently used standard n-grams."
N07-1049,1,"as an issue for further investigation, we mention that in this framework, asin reranking, it is possible to exploit global features in the revision phase; e.g., semantic features such as those produced by named-entity detection systems."
N07-1052,1,"in the future, we plan to investigate alternative objective functions and error-driven methods for learning heuristic bounds."
N07-1054,1,"we would also like to experiment with combining the two approaches, i.e.by applying the syntactic heuristics to an instance set extracted using topic segmentation constraints."
N07-1054,2,"while the topic-segmentation filtering approach achieves significant improvement and the best results overall, our analysis of the syntactic filtering approach indicates that refined heuristics and a larger set of parsed data can further improve those results."
N07-1055,5,domains with more natural writing styles will make lexical prediction a much more difficult problem.
N07-1055,5,"finding optimal orderings is a difficult task even for short documents, and will become exponentially more challenging in longer ones. multi-paragraph documents also pose a problem for the τ metric itself."
N07-1055,1,"in documents with clear thematic divisions between their different sections, a good ordering metric should treat transposed paragraphs differently than transposed sentences."
N07-1055,1,"on the other hand, the wider variety of grammatical constructions used may motivate more complex syntactic features, for instance as proposed by (siddharthan et al., 2004) in sentence clustering."
N07-1056,4,"the algorithm has been tested on title generation, but the decoder is not specific to this task and can be applied to other generation and summarization applications."
N07-1057,1,"for the former, search facilities could be built over the data that would allow linguists to find syntactically marked up data for a large variety of languages, and could even accommodate cross-linguistic comparisons and analyses."
N07-1057,1,"for the latter, we could automatically discern grammars and transfer rules from the aligned and marked up data, where these computational artifacts could act as bootstraps for the development of additional tools and resources."
N07-1058,1,"ongoing work aims to improve grammar-based readability by reducing noise in training data, automatically creating larger grammar feature sets, and applying more sophisticated modeling techniques."
N07-1059,3,"with the data we have collected and labelled (and the effort is ongoing), it becomes feasible to examine the use of data-driven methods."
N07-1059,1,"in future work, we plan to apply machine learning techniques to this problem."
N07-1059,4,"we also plan to expand our application to many other domains appropriate for language learning, and test the effectiveness of the translation game as a means for language learning."
N07-1060,4,"in future applications, we envision using our automated measure to allow a form of feedback for intelligent language tutors, so that the system can automatically adapt its behavior based on the student’s test responses."
N07-1060,3,"looking beyond definition scoring, we believe automated methods for assessing word learning have great potential as a new scientific tool for language learning researchers, and as a key component of intelligent tutoring systems that can adapt to students."
N07-1061,3,"in future work, we will further investigate the pivot strategies described in this paper to confirm that the phrase translation strategy is better than the sentence translation strategy in the intended situation as well as with the europarl corpus."
N07-1062,3,it would be interesting to investigate if the described techniques and data structures are applicable for reducing the memory requirements of language models.
N07-1063,5,we also hope to address the question of how much search error is tolerable to 3analysis of total lm calls made by each method (not presented here) shows the h.search makes significantly fewer (1/2) total lm calls than cp to achieve each model cost.
N07-1063,3,we plan to evaluate the impact of these more powerful models in future work.
N07-1064,1,"in such a setting, the phrase-based system becomes a sort of combination mt system."
N07-1064,6,we intend to explore such alternatives in the near future as well.
N07-1065,1,"first, we plan to move to a discriminative approach to combining scores and weighting unit features using a small labeled set."
N07-1065,1,"secondly, we will look at incorporating units into the information retrieval process. making the information retrieval process aware of the desired answer types will be an important future direction of qa research."
N07-1066,1,"as our current framework is based on the assumption that each answer is independent, we are building another probabilistic framework which does not require any independence assumption, and uses an undirected graphical model to estimate the joint probability of all answer candidates."
N07-1066,1,we plan to improve our framework by adding regularization and selecting the final answers among candidates returned from all extractors.
N07-1067,5,a major challenge is to ascertain whether the mention of the target is indeed involved in the recognized justice event.
N07-1067,4,"but ultimately, we would like to adapt our system to arbitrary topic areas."
N07-1067,3,"the first test would reasonably be conflict events, for which the ace program has training data."
N07-1067,1,"in addition, we are working to produce answers using text generation, to bring more sophisticated summarization techniques to make a better presentation than an unordered list of sentences."
N07-1067,4,"finally, we will look into applying the techniques used here on other topics."
N07-1067,2,"within the gale program, we are limited to the defined corpus, but in the general case, we could add more varied resources."
N07-1067,1,we are planning to obtain various pieces of information from additional secondary queries to the search engine.
N07-1068,4,"in the future, we plan to integrate the other useful features in videos to support multi-model-based multimedia question answering."
N07-1069,3,"finally, we plan to explore the effect of different thematic role groupings on system performance."
N07-1069,3,"in future work, we will map the propbank-ed brown corpus to verbnet as well, which will allow much more thorough testing of our hypothesis."
N07-1069,1,we will also examine back-off to verb class membership as a technique for improving performance on out of vocabulary verbs.
N07-1071,1,future work in this direction includes further exploration of the appropriate inventory of semantic classes used as sp’s.
N07-1072,1,"we believe this approach can generalize to other domains where phrases, sentences, or other short texts need to be compared."
N07-2001,4,"in our future work, we are going to further investigate whether the trends shown in this paper generalize to on-line mdp policy learning."
N07-2002,1,grammatical features with low precision and recall results (mass and pcomp) show that some more research should be carried out for finding relevant linguistic cues to be used as learning features.
N07-2006,3,it would be interesting to compare the performance if the statistics is done using the metric-best path on a smaller amount of data to the performance if the statistics is done using the model-best path on a larger amount (as there is no reference translation necessary).
N07-2006,1,it could also be beneficial to sequentially prune the phrase pairs and always re-calculate the statistics after removing a certain number of phrase pairs.
N07-2007,3,we also plan to evaluate the influence of our alignment improvement on mt quality.
N07-2007,1,we plan to use more sophisticated machine learning models such as support vector machines for combination and make use of more available parallel data.
N07-2007,1,"in the future, we plan to extend our system with additional models at the phrase and multi-word levels for both alignment and alignment combination improvement."
N07-2008,5,the answer to that question might have implications for the range of text types that ought to be used to comprehensively test parallel document identification systems.
N07-2008,6,"for example, if there were no document in the language b set that shared more than the minimum number of unique words with the document da in language a, then the approach might return no parallel for that document."
N07-2008,6,"a system relying on non-binary word similarity measures rather than on total identity of words would be more complex and slower, but also more robust across different domains of text."
N07-2008,1,this could take the form of establishing a score or significance threshold.
N07-2008,5,"however, it is also a limitation because many cross language cognates are not orthographically identical."
N07-2008,5,"the exact matching of words is a critical feature of our approach, which enables it to perform quick comparisons of documents by representing them as sets of low-frequency words stored in hash tables."
N07-2008,4,"second, it might be revealing to run further tests with this approach on other types of text than parliamentary proceedings."
N07-2008,5,what types of text would require a more sophisticated approach?
N07-2008,6,"first, the problem definition could be expanded to include cases where there is no valid parallel for a given language a document in the language b document set."
N07-2009,1,we are also working on new best-first search generalizations of our depth-first search inference to improve decoding time.
N07-2009,1,"we believe that, in addition to the finite-state transducer approach, a graphical model framework such as ours would be well suited for this scientific and engineering endeavor."
N07-2009,1,"we also plan to design better approximate inference strategies for training highly connected graphs such as ibm models 3 and 4, and some novel extensions."
N07-2009,5,"as there has been increased interest in end to-end task such as speech translation, dialog systems, and multilingual search, a new challenge is how best to combine the complex components of these systems into one framework."
N07-2009,1,"in future work, we intend to implement phrase-based mt models."
N07-2010,4,"in future work we will examine the effects of applying situated models of meaning to other tasks (e.g., machine translation)."
N07-2011,6,"these responses will be based both on our hypotheses about why uncertainty is significantly associated with these contexts, as well as on analyses of human tutor responses in these contexts, using our human tutoring corpus, which was collected with our first itspoke corpus using the same experimental procedure."
N07-2011,2,"we plan to examine more contexts, such as a topic repetition variable that tracks similar questions about a topic (e.g. gravity) across dialogues."
N07-2011,1,our next step will be to use the significant dependencies to develop system responses to uncertain answers in these contexts.
N07-2011,1,"we also plan to investigate context dependencies for other affective states, such as student frustration."
N07-2012,3,"present and future work includes evaluating the method as a component of a real-time dialog system, where its usefulness at decreasing waiting time can be tested."
N07-2012,2,and finally we are experimenting with larger datasets.
N07-2012,1,"we are also working on methods for feature selection and compression to obtain further speedup,"
N07-2013,1,"in addition, the syntactic features will be expanded to include frequent grammatical alternations such as active / passive."
N07-2013,1,extensions of this method can potentially be used to determine if a given essay was written by a native or a non-native speaker.
N07-2013,1,"in the immediate future, we plan to extend the set of features to include non-verbatim similarity, such as synonyms and words derived by lsa-type comparison (landauer et al.1998)."
N07-2014,5,"however, a large portion of the unknown words are in fact foreign words and names, and it is not clear whether the models learned handle such words well."
N07-2017,1,"future work will investigate additional classifiers, classifier combination, and expanded training data."
N07-2017,1,we are also interested in applying a language model to decode an alignment network that has been scored with our classifier.
N07-2018,3,our future work will focus on more thorough evaluation of the algorithm and integrating it into a summarization system.
N07-2019,2,"in future work, we plan to exploit other sources of metadata such as e-mails, as well as the structure of the meetings themselves."
N07-2020,1,"continuing along these lines, we are currently creating two new tests."
N07-2020,3,"we are constructing a new arabic dlpt-star test, tailoring the document selection more specifically for comprehension testing and ensuring texts and tasks are at the intended ilr levels."
N07-2020,3,we intend for both of these tests to be available for a public machine translation evaluation to be conducted in 2007.
N07-2020,3,we are also constructing a mandarin chinese test with similar design specifications.
N07-2020,2,in future tests we wish to include level 1 documents and questions.
N07-2022,1,"in future work, we could consider using various smt features (as would be required for a phrase-based smt system)."
N07-2024,6,our ultimate goal is to identify the errors in the non-native sentences and propose corrections.
N07-2024,1,we also plan to explore techniques for combining large mt training corpora and smaller non-native training corpora.
N07-2026,3,"in addition, we plan to test this framework using automatic speech recognition output, speaker segmentation, and soundbite segment detection."
N07-2026,1,"our future work will focus on exploring more useful features, such as part-of-speech and semantic features."
N07-2027,1,"in future work, we would like to improve individual components of icetagger and icemorphy, with the purpose of further increasing the tagging accuracy."
N07-2030,1,"context-dependent stream weights can also model feature asynchrony to some extent, so that this approach not only improves automatic speech recognition, but might also be an interesting starting point for future work in speaker clustering, speaker identification, or other applications in speech analysis."
N07-2031,5,"finally, while unification grammars are reversible for use in generation, good generation methods remain an open research problem."
N07-2034,1,"in future work we intend to make a deeper study on the performance of the multi-target system as the amount of targets increase, since the amount of parameters to be estimated also increases."
N07-2036,1,i plan to further validate the collection of agreed-upon certainty markers on a much larger dataset and by using the typology as input data to machine learning algorithms for certainty identification and extraction.
N07-2036,1,a baseline for future attempts to improve the calibration of levels and their boundaries was established.
N07-2036,1,"in the future studies, i intend to revise the number of the discrete categories on the epistemic continuum and further re-define certainty levels conceptually."
N07-2037,1,our future work will be directed towards tight integration of all available information by predicting the entire mlpt (besides leaves).
N07-2038,1,"the next step is hence to use the recorded data to train the simulator, and to then retrain the dm policy."
N07-2039,3,"in future work, we plan to evaluate the effectiveness of the model for automatic new word acquisition in spoken dialogue systems."
N07-2040,2,additional experiments would be conducted on a larger data set to extract more robust patterns.
N07-2040,1,we plan to replicate the study with automatic error detection experiment.
N07-2041,1,"we also plan to train a graphical model based on all extracted bp, pl and bpl relations to infer relations from multiple sentences and documents."
N07-2041,1,"in future work, we plan to let the discriminative model take the output of our parser and refine our current results further."
N07-2043,3,"finally, we intend to evaluate the benefit of having a human in the loop in the first few iterations to filter out patterns chosen by the system."
N07-2043,1,we also plan to detail how the various features in our classification model contribute to ranking of candidate patterns.
N07-2043,3,we plan to verify the strength of our approach evaluating against other ground truth data sets.
N07-2043,1,an additional area of envisioned improvement regards the use of a random sub selection of negative candidate patterns as training samples to counteract the presence of sentence fragments among low ranking candidate patterns.
N07-2044,3,we plan to improve word prediction and validate these results using aac users as future work.
N07-2045,1,the success of using a state-of-the-art language model in determiner selection also suggests that one would be helpful in making other decisions in the surface realization stage of text generation.
N07-2046,5,"it may thus be that today’s exciting emerging work in “unsolved” areas – semantics, reference, and learning – could come to play a key role in what is sometimes maligned as yesterday’s boring solved problem."
N07-2047,1,"subset selection techniques could give a solution to this problem, of which we will leave the further exploration to future work."
N07-2048,3,we also plan to appraise our proposal on other languages.
N07-2048,3,"in the future, we will evaluate different choices of words for the sets of positive and negative reference words."
N07-2049,2,"future work will investigate the pitch reset phenomenon in cantonese broadcast news, because cantonese is another major chinese dialect with more complicated tonal characteristics."
N07-2049,1,we also plan to incorporate prosodic cues with lexical cues to further improve performance in chinese story segmentation.
N07-2050,1,"directions for future work include learning the structure of mlns automatically and using mlns for information extraction and statistical relational learning (e.g., entity relation identification)."
N07-2051,1,"in addition, improving sub-sentence root finder and sentence root finder will also be considered in the future."
N07-2052,1,"as the simple pl measure performs remarkably well, we should also consider computing sr based on the wikipedia article graph instead of the category graph."
N07-2052,1,future research should focus on improving the strategies for combining complementary knowledge sources.
N07-2052,3,we also need to evaluate a wider range of measures to validate our findings.
N07-2055,3,we are currently preparing to deploy our design into full scale evaluation exercises.
N07-3001,5,"still, a big challenge will be to work out how to deal with polysemic and homonymic words and words with medical and non-medical facets."
N07-3002,4,"next, i am going to apply my approaches to parse other languages, such as czech, german, spanish and french, and analyze the performance of my parsers on these different languages."
N07-3002,1,"first and most important, i plan to investigate new advanced machine learning methods (e.g., structured boosting or unsupervised / semi-supervised algorithms (xu et al., 2006)) and apply them to the dependency parsing problem generally, since the goal of my research is to learn natural language parsers in an elegant and principled manner."
N07-3002,4,"furthermore, i plan to apply my parsers in other domains (e.g., biomedical data) (blitzer et al., 2006) besides treebank data, to investigate the effectiveness and generality of my approaches."
N07-3003,2,"further work will then concentrate on making this information available to our coreference resolution system, e.g. semantic similarity computation."
N07-3003,3,"finally, since wikipedia is available in many languages, we believe it is worth performing experiments in a multilingual setting."
N07-3003,1,work in the near future will accordingly concentrate on automatically inducing the semantics of the relations between wikipedia categories.
N07-3003,5,"we believe that our work opens up exciting new challenges for the ai and nlp research community, e.g. how to handle the noise included in such knowledge bases and how to fully structure the information given in the form of only partially structured text and relations between knowledge base entries."
N07-3003,1,we aim in the future to induce an ontology from its collaboratively generated categorization graph.
N07-3004,3,"i will build on this work in the coming months as i prepare for two evaluations: a study on the usability of natural language and graphical tools for navigating a knowledge base, and a task-based evaluation on labeled image retrieval."
N07-3004,6,these evaluations should bring closure to the work as a contribution in the field of semantic analysis of text.
N07-3005,3,"i will also describe evaluation criteria used to assess the quality of the automatic summaries, for example informativeness and readability."
N07-3005,3,"next, i will make recommendations for the presentation of summarization evaluation results, based on the knowledge acquired from my analysis of 22 scientific papers, and from previous evaluation campaigns."
N07-3005,3,"in the next couple of months, i plan to analyze evaluation methods identified in my corpora, for example comparing automatic summaries with gold standard or baseline summaries, and asking judges to give their opinion on the quality of automatic summaries."
N07-3007,1,"beyond the simple enumeration of all the published systems, the aim is to create a categorization of dialogue systems according to the tasks they allow and to the type of knowledge they use independent of the used knowledge representation primitives (classes, relations and axioms)."
N07-3007,1,the next step to achieve the main goal of this work is to study the existing dialogue systems with emphasis on the performed tasks and the used knowledge sources.
N07-3008,4,another application of the semantic frames i am interested in is prosody prediction.
N07-3008,1,the project will be further developed by adding to the automatic import program rules discovered though the analysis of the mismatching cases.
N07-3008,2,"within the institute of computer science, i have begin to work at a syntax-prosody interface for romanian based on fdg trees of sentences and other syntactical information to discover the phonological entities underlying the written text and the topic/focus articulation."
N07-3008,1,the algorithm for finding sentence focus uses the semantic roles as a main component.
N07-3009,5,i will investigate how combining evidence sources can increase their applicability to new content domains.
N07-3009,1,"this will include, for example, understanding how (vocabulary independent) std systems can be paired with fixed vocabulary asr."
N07-3009,1,"lastly, i will explore how these new evidence sources may themselves be improved."
N07-3009,1,this will include utilizing temporal domain knowledge for classification and improving the robustness of phone based std systems.
N07-3009,1,i will investigate multiple methods for combining the evidence presented by both std and classification systems with conventional asr output (transcripts or word lattices).
N07-4002,3,the pilot will evaluate the tool’s effectiveness in terms of measurable learning gains in reading comprehension and english language skills.
N07-4008,4,"in the future, we plan to incorporate additional web-based functionality, with the ultimate goal of creating a general-purpose dialog interface to web applications and services."
N09-1001,1,"future work will explore other graph construction methods, such as the use of morphological relations as well as thesaurus and distributional similarity measures."
N09-1001,1,We will also explore other semisupervised algorithms.
N09-1004,2,"continue to build the knowledge base, enlarge the coverage and improve the system performance."
N09-1004,6,"it is unlikely that a reader examines all surrounding words when determining the sense of a word, which calls for a smarter and more selective matching strategy than what we have tried in section 4.1; 3."
N09-1004,3,test our wsd system on fine-grained semeval 2007 wsd task 17.
N09-1004,6,wsd is often an unconscious process for human beings.
N09-1004,2,the experiment results in section 4.2 clearly show that more word instances can improve the disambiguation accuracy and recall scores; 2.
N09-1004,4,"although we only evaluated our approach with coarse-grained senses, our method can be directly applied to fine grained wsd without any modifications."
N09-1005,1,"in future work, we would like to develop more powerful decipherment models and techniques, and we would like to harness the information available from a wide variety of monolingual resources, and use it to further narrow the gap between parallel-trained and non-parallel-trained approaches."
N09-1006,2,we are working on creating a gold standard corpus of children鈥檚 transcripts annotated with pos tags.
N09-1006,2,our current efforts are devoted to improving prediction accuracy by refining our feature set.
N09-1006,2,this data set will help us improve accuracy on our pos based features.
N09-1006,1,"we are also exploring the use of socio-demographic features such as the educational level of parents, the gender of children, and enrollment status on free lunch programs."
N09-1007,4,"the latent variable model handles latent-dependencies naturally, and can be easily extended to other labeling tasks."
N09-1007,2,"since the latent variable model allows a wide range of features, so the future work will consider how to integrate open resources into our system."
N09-1010,5,a future challenge lies in incorporating constraints from additional languages even when parallel text is unavailable.
N09-1012,1,we would also like to explore using our smoothing technique in other models such as hmms.
N09-1012,1,future work includes using lexical information more deeply in the model by conditioning argument words and valence on the lexical head.
N09-1012,1,"finally, we would like to learn the parts-of-speech in our dependency model from text and not rely on the gold-standard tags."
N09-1012,2,we suspect that successfully doing so will require using much larger datasets.
N09-1012,1,"for instance, we could do unsupervised hmm part-of-speech induction by smooth a tritag model with a bitag model."
N09-1013,1,further translation experiments will be carried out.
N09-1014,6,the latter could be quite easily incorporated into a string kernel or the related tree kernel similarity measure.
N09-1014,2,more gains can be expected when using better domain knowledge in constructing the string kernels.
N09-1014,3,"additionally, we will investigate the effectiveness of this approach on larger translation tasks."
N09-1014,1,"future work will include testing different graph construction schemes, in particular better parameter optimization approaches and better string similarity measures."
N09-1014,2,"this may include e.g. similarity measures that accommodate pos tags or morphological features, or comparisons of the syntax trees of parsed sentence."
N09-1015,2,"the most straightforward one is to use our approaches with more different languages, such as chinese and arabic, and incompatible corpora, for example, different segments of europarl."
N09-1015,6,the main focus of such experiments should be verifying the conclusions we had in this paper.
N09-1016,1,"for languages where npi lists are not extensive, one could envision applying an iterative co-learning approach: use the newly-derived de operators to infer new npis, and then discover even more new de operators given the new npi list."
N09-1016,1,"the prospect that our method might potentially eventually be refined in such a way so as to shed at least a little light on linguistic questions is a very appealing one, although we cannot be certain that any progress will be made on that front."
N09-1016,1,"on the other hand, although the results are already quite good for english, it would be interesting to see what improvements could be gained by using more sophisticated syntactic information."
N09-1017,6,"as described in section 5.2, many nominals do not have enough labeled training data to produce accurate argument models."
N09-1017,6,"such inferences would help connect entities and events across sentences, providing a fuller interpretation of the text."
N09-1017,1,the generalization procedures developed by gordon and swanson (2007) for propbank srl and pad麓o et al.(2008) for nombank srl might alleviate this problem.
N09-1017,1,"additionally, instead of ignoring nominals with implicit arguments, we would prefer to identify the implicit arguments using information contained in the surrounding discourse."
N09-1017,6,Our results also suggest interesting directions for future work.
N09-1019,1,"how much improvement is possible in practice, and whether joint inference can also improve named-entity performance, remain interesting questions for future work."
N09-1019,1,this suggests that it should indeed be possible to improve on their coreference results without using a supervised named-entity model.
N09-1020,5,"however wordnet has a low coverage problem, i.e. there are some words in the data which do not exist in it."
N09-1020,1,"one solution to this low coverage problem is to combine trees generated by the clustering algorithms mentioned in this paper and wordnet, which we leave as a future work."
N09-1020,1,"an interesting avenue of research is to construct the vocabulary tree based on wordnet, as a way to inject independent prior knowledge into the model."
N09-1021,1,"second, we might consider training the degradation model in a discriminative framework (e.g., training to optimize a measure that will penalize degradations which cause false alarms, even if they are good candidates from the perspective of mle)."
N09-1021,1,"one solution, for future work, might be to incorporate a false alarm model (e.g., down-weighting putative occurrences which look suspiciously like non-query words)."
N09-1021,6,we hope that the ideas presented in this paper will provide a solid foundation for this future work.
N09-1022,1,"in the future, we also plan to investigate other methods, such as plsi (hofmann, 1999), to deal with data sparseness in computing semantic similarity."
N09-1023,5,"our work raises three important questions: (1) to what extent is the single test collection that we have used representative of the broad range of exact chat applications?,"
N09-1023,5,"our work raises three important questions: (1) to what extent is the single test collection that we have used representative of the broad range of 鈥渢ext chat applications?, (2) to what extent do the measures we have reported correlate to effective performance of downstream tasks such as summarization or automated response?, and (3) can we re-conceptualize the formalized problem in a way that would result in greater inter-annotator agreement, and hence provide scope for further refinements in our technique."
N09-1023,6,These problems will be the focus of our future work.
N09-1024,2,"future directions include applying our model to other inflectional and agglutinative languages, modeling internal variations of morphemes, leveraging parallel data in multiple languages, and combining morphological segmentation with other nlp tasks, such as machine translation."
N09-1027,1,in future work we plan to focus on methods to improve on the integration of the pgyn(d) feature during decoding and techniques that allow us consider more of the search space through less pruning.
N09-1028,1,it would be very interesting to investigate ways to have efficient procedure for training em models and getting word alignments using word lattices on the source side of the parallel data.
N09-1028,1,"along this line of research, we think some kind of tree-to-string model (liu et.al., 2006) could be interesting directions to pursue."
N09-1028,1,"although there is clearly room for improvements, we also feel that using one reordering during training may not be good enough either."
N09-1029,1,"we will investigate more linguistic ways to classify words in future work, especially on target language."
N09-1029,1,"for example, using word hierarchical structures in wordnet (fellbaum, 1998) system provides more linguistic and semantic information than statistically-motivated classification tools."
N09-1030,1,"much work remains in this new research area, including the creation of more types of features."
N09-1030,1,"also, due to the difficulty in obtaining wish annotated training data, we plan to explore semisupervised learning for wish detection."
N09-1032,1,"to further reduce the accuracy loss, we will explore informative sampling to capture fine-grained data difference in the domain transfer."
N09-1033,6,it is our hope that most of the errors identified in this work could be automatically discovered without any manual judgments.
N09-1033,1,a complimentary way to improve performance would be to investigate the addition of relevant candidate expansions that are not already in the initial expansion.
N09-1033,1,we are currently investigating extensions to fmm that can efficiently add new candidate expansions to the set by computing the similarity between modified centroids and all terms occurring in a large body of text.
N09-1033,1,we are also investigating ways to use the findings of this work to a priori remove ambiguous seed instances (or their ambiguous contexts) before running the initial expansion algorithm.
N09-1035,1,"in the future, we plan to explore a hybrid approach, which would benefit from both the generality of linguistic principles and the smooth exception-handling of supervised techniques, in order to make best use of whatever data is available."
N09-1037,1,"in the future, we would like to add other levels of annotation available in the onto notes corpus to our model, including word sense disambiguation and semantic role labeling."
N09-1040,1,rather it may be necessary to model discourse connectors and lexical semantics explicitly.
N09-1040,1,"it is interesting to consider how multi-scale segmentation might be extended to finer-grain segments, such as paragraphs."
N09-1040,5,the development of more comprehensive bayesian models for discourse structure seems an exciting direction for future research.
N09-1040,6,"the lexical counts at the paragraph level will be sparse, so lexical cohesion alone is unlikely to be sufficient."
N09-1043,1,together with more elaborate confidence handling a system could quickly generate hypotheses and then refine the associated confidences over time.
N09-1043,6,We will explore this in future work.
N09-1044,3,"we also plan to explore the impact of integrating language modeling with search, and to examine the impact of these different language modeling approaches on performance of a trainable dialog manager that takes n-best output from the speech recognizer."
N09-1044,1,"in future work, we will optimize the parameters in our algorithm for geo-centric lm computation and merging."
N09-1045,1,"we also will develop a similar approach to handling dialectical arabic speech using the magead morphological analyzer (habash and rambow, 2006)."
N09-1045,1,"a larger goal is to employ the msa and dialectical phone recognizers to aid in spoken arabic dialect identification using phonotactic modeling (see (biadsy et al., 2009))."
N09-1045,5,"in future work, we will address several issues which appear to hurt our recognition accuracy, such as handling the words that mada fails to analyze."
N09-1046,1,"second, incorporating target language information into a segmentation model holds considerable promise for inducing more effective translation models that perform especially well for segmentation lattice inputs."
N09-1047,1,"in future work, we plan to explore selection methods based on potential phrases, adaptive sampling using features other than decoder confidence and the use of features from confidence estimation in mt (ueffing and ney, 2007)."
N09-1050,2,"(3) we will add other standard varieties of english (such as british, canadian, australian, etc) to the training corpus for the reference pronunciation model as the current model is trained on primarily north american english (nae)."
N09-1050,1,"(2) we will investigate other aspects of pronunciation, e.g., consonant quality and word stress;"
N09-1050,1,"we plan to continue our research in the following directions: (1) we will improve the native speech norms for vowel durations, such as using the distribution of vowel durations rather than just the mean of durations in our feature computations;"
N09-1055,2,"additionally, we plan to exploit more information, such as background knowledge, to improve the performance."
N09-1055,6,we will conduct deeper research in this area in future work.
N09-1056,5,"given the diverse range of topics present in our dataset, addressing topic-dependency is also an interesting future research direction."
N09-1056,1,"there are several interesting avenues for future research, including improving the current method for exploiting the site structure."
N09-1057,1,"because we computed opus features for opinionated as well as non-evaluative language in our corpora, obtaining overall positive results, we believe these features may also improve conventional opinion labeling for subjective text."
N09-1057,6,This will be investigated in future work.
N09-1058,5,"we also believe that stream counts can be applied to other problems involving higher order lms such as speech recognition, information extraction, spelling correction and text generation."
N09-1062,6,"while our results on the full treebank are well shy of the best available parsers, we have proposed a number of improvements to the model and the parsing algorithm that could lead to state-of-the art performance in the future."
N09-1066,5,"creating readily consumable surveys is a hard task, especially when using only raw text and simple summarization techniques."
N09-1066,1,"given the overlapping content of abstracts and citation texts, discovered in the current study, it is clear that redundancy detection will be an integral component of this future work."
N09-1066,2,we next plan to generate surveys using both citation texts and abstracts together as input.
N09-1066,1,therefore we intend to combine these summarization and bibliometric techniques with suitable visualization methods towards the creation of iterative technical survey tools systems that present surveys and bibliometric links in a visually convenient manner and which incorporate user feedback to produce even better surveys.
N09-1068,1,"in the future we would like to see if the model could be adapted to improve performance on data from a new domain, potentially by using the top-level weights which should be less domain-dependent."
N09-1070,1,"for the future work, we plan to investigate different weighting algorithms for the graph-based approach."
N09-1070,1,we also need a better way to decide the number of keywords to generate instead of using a fixed number.
N09-1070,1,"furthermore, since there are multiple speakers in the meeting domain, we plan to incorporate speaker information in various approaches."
N09-1070,3,"more importantly, we will perform a more rigorous human evaluation, and also use extrinsic evaluation to see whether automatically generated keywords facilitate tasks such as information retrieval or meeting browsing."
N09-1072,5,"therefore, a wider array of studies across populations and genres would be required before a more general theory of conversational styles is established."
N09-1073,1,"more complicated methods for using both sets that also achieve linear complexity (perhaps with a smaller constant), or that achieve o(nlogn) complexity rather than o(nlog2n), may exist."
N09-1075,1,"first, we make predictions for where local coherences should obtain for an arbitrary scfg, not just one particular class of sentences."
N09-1075,1,such a hypothesis for how comprehends approximate the prior could be tested by manipulating the frequency of the relevant substrings in sentences with local coherences.
N09-1075,4,"while it may be unlikely that they calculate these probabilities for sequences directly from their grammar as we do in this paper, there could be a number of ways to approximate this prior: for example, given a large enough corpus, these probabilities could be approximated for any string of words that appears sufficiently often by merely tracking the structures the string has each time it occurs."
N09-1075,1,"interestingly, the fact that the prior is actually more difficult to compute than the posterior suggests that the only way it would be available more rapidly is if it is precomputed."
N09-1075,1,another possibility relates to the problem of correlations between the different components of the prior and posterior vectors.
N09-1075,4,"this allows the model to scale up for use with a broad coverage grammar and to make predictions for arbitrary sentences, which was not possible with a model such as tabor &amp; hutchins (2004)."
N09-1075,6,"for example, in our small grammar, whenever a root category begins, so does an s, an s-base, and an np-base."
N09-2002,5,currently we use our bigram language model in a brute-force manner: in order to generate the ilp we evaluate the probability of all possible bigrams of english candidate tokens in advance.
N09-2002,5,this leads us to wonder if there may be a way to provide tighter integration of program generation and solving.
N09-2002,5,"one possible solution may be the use of so-called delayed column generation strategies which incrementally add parts of the objective function (and hence the language model), but only when required by the ilp solver."
N09-2002,1,a further extension is to reformulate higher-level mt models (phrase- and syntax-based) within the ilp framework.
N09-2002,5,these representations could be more desirable from a linguistic constraint perspective as the formulation of constraints may be more intuitive.
N09-2002,5,the first issue is that the generation of the ilp programs can take a long time.
N09-2002,5,related to this issue is how to tackle the incorporation of higher order language models.
N09-2002,5,such an integration would avoid the need to query the models in advance for all possible model components the solver may require.
N09-2002,5,these challenges raise some interesting research questions and practical issues one must consider when embarking on exact inference using ilp.
N09-2006,1,"in the future, we plan to integrate domain specific heuristics via approximated derivatives of evaluation metrics or mixture of them to guide the optimizers move toward better solutions for simplex-downhill algorithms."
N09-2007,1,"future work could address some limitations of the present study by using bidirectional translation models, considering other language families and source languages other than english, and applying query expansion techniques."
N09-2009,1,"future research is proposed to increase the method recall via broader coverage lexical reference resources, and to improve its precision through better context models than lsa, which was found rather noisy for quite a few categories."
N09-2011,1,future plans include training svms on the results of the rule-based annotation.
N09-2011,2,we also plan to involve two annotators in order to collect a more robust dataset based on interannotator agreement.
N09-2011,1,further work is also needed in improving image marker referent identification and co-reference resolution.
N09-2012,4,we also plan to use this corpus as test and training data for algorithms to automatically annotate such information.
N09-2012,1,"though currently we are only annotating pursuit with location references, future plans include extending tesla to support the annotation of movement, orientation, and path descriptions."
N09-2013,1,"another important step is to correlate the dialogue profile of each tutoring session, as revealed by the hmm, to learning and affective outcomes of the tutoring session."
N09-2013,1,"in addition, leveraging knowledge of the task state as well as surface-level utterance content below the dialogue act level are promising directions for refining the descriptive and predictive power of these models."
N09-2013,3,future evaluation of the hmm presented here will include comparison with other types of graphical models.
N09-2014,1,"we are currently working on the extensions to the nlu model that will allow for the use of different types of context features, and investigating interesting ways in which agents can take advantage of early interpretations."
N09-2017,1,"in the next step, we will focus on acquisition of adjectives."
N09-2018,1,"however, we expect that the malay glosses will block readings of indonesian classifiers, and classifiers in other languages will require different strategies; we intend to examine this in future work."
N09-2021,3,this will allow us to compare word-based detection to published syllable-based results.
N09-2021,1,"in future work, we will explore a number of techniques to transfer word based predictions to syllables."
N09-2022,1,"however, our current results show that future research on complex spoken dialog systems is enabled to exploit automatically generated frame semantics, which is our very direction."
N09-2023,1,we can also discover hidden links in the graph by exploring new dialog flows with random walks.
N09-2023,1,we can improve the clustering performance by using a distance metric learning algorithm to consider the correlation between features.
N09-2023,6,there are several possible subjects for further research on our approach.
N09-2024,1,we hope that a purely sentence-level processing might result in a more productive pair extraction in future.
N09-2024,1,"this way, we will be able to maintain our search-driven extraction approach."
N09-2024,1,"we are also re-implementing ir-based techniques to preselect translation pairs at the document-level, to gauge the effect of this additional filtering step."
N09-2025,4,"we plan to extend our method to include more complex features, and apply it to structured output learning."
N09-2027,1,"in the near future, we plan to experiment with prediction models of the speaker鈥檚 self reported level of certainty."
N09-2028,1,in future work we will experiment with acoustic and prosodic features and detect disfluencies from the speech recognition output.
N09-2031,1,we will try other thresholds and syntactic parsers to see their effects on dictionary extraction in the future.
N09-2031,5,"besides, because the proposed approach is based on the syntactic analysis of sentences with no more than k words (see section 4.1), the parsing accuracy and the setting of threshold k will affect the correctness of dependency heterogeneity vector learning."
N09-2031,2,"there are several future works under consideration including corpora cleaning, extending the proposed approach from single-noun dictionary extraction to multiword, and adapting the proposed approach to other language pairs."
N09-2033,1,"particularly useful would be for the system to be able to provide feedback, including generating nprs; we have started investigating this reverse problem, of obtaining nprs from pronunciations, and are encouraged by the initial results."
N09-2033,4,"moreover, it appears that novice users don't have much difficulty generating useful nprs on their own; we expect that their skill would increase with use."
N09-2034,1,"furthermore, finding a way to calculate confidence scores of speech understanding results is on our agenda."
N09-2034,3,we will conduct more experiments in other domains or with other resources to evaluate the effectiveness of our framework.
N09-2034,2,we plan to investigate the case in which a smaller amount of the training data is used to estimate the coefficients of the logistic regressions.
N09-2037,3,"in our ongoing work, we plan to conduct a similar error analysis for these problems in order to evaluate the generality of the findings reported here."
N09-2038,1,"we wish to further investigate automatic adaptation based on implicit confidence scores, or even active participation of the user e.g.by marking bad utterance which could be excluded from the adaptation."
N09-2039,2,we plan to increase the number of cases considered in the sample required to delimit the perplexity classes.
N09-2039,1,the equation (2) may be developed further in order to obtain exactly the number of required cases for each perplexity class.
N09-2040,1,"our goals for the future include further development of the answer credibility model to include not only terms from a question context, but terms that can be deduced to be in an answer context."
N09-2041,2,"in future work, we plan to investigate the unexpected drop in hyper tagger performance on our ne-collapsed corpus, which we conjecture may be resolved by taking advantage of vadas and curran鈥檚 (2008) corrections to the ccgbank鈥檚 np structures."
N09-2042,4,we believe this framework can be applied to any category of queries once a query classification and a score detector have been implemented.
N09-2043,1,"in the future, we plan to incorporate our alignment-based soft pattern matching method into the tree kernel method for ie."
N09-2044,5,"many open questions remain, including which other factors can or cannot be captured by our current feature set and classifier, and whether noisy label learning methods could address the problem of uncertainty in the labels for particular features and genres."
N09-2045,4,"as future work, we will demonstrate the impact of our simplification method on other text mining tasks, such as relationship extraction."
N09-2048,3,"we also plan additional experiments on adaptation in let鈥檚 go!, including an analysis of the time course of adaptation and further analyses of the impact of adaptation on asr performance."
N09-2048,2,"in future work, we plan to confirm these results using transcribed data."
N09-2049,1,one solution might be to develop methods for detecting when the problem is in acoustics and to trust the language model more in these regions.
N09-2053,1,"for future work, we intend to explore an approach to conducting cross-lingual event extraction and investigate whether the cross-lingual inference can bootstrap either side when running two language event extraction systems in parallel."
N09-2054,1,"in the future work, we will investigate automatic data selection methods to choose materials that are most suitable for self-training and evaluate the effect of the amount of automatically labeled data."
N09-2055,3,"a future improvement for this would be to integrate the process in the core of the translator鈥檚 workflow, so that on-the-fly evaluation can be made."
N09-2055,1,"in addition, several possible sources of errors have been identified which will help develop future system enhancements."
N09-2056,1,"in order to explore the question of pivot selection further and arrive at firmer conclusions, future work will have to investigate in detail what kind of features are important in selecting a pivot language for a given language pair."
N09-2056,5,"in addition, concerning the question of how the pivot language selection criteria depends on the choice of the pivot translation method, future work will also have to investigate the effects of pivot language selection for the other pivot translation approaches described in section 2."
N09-2056,1,"based on these findings, we plan to determine the contribution of different language characteristics on the system performance automatically to obtain useful indicators that could be used to train statistical classification models to predict the best pivot language for a new language pair and improve the usability of machine translation between under-resourced languages further."
N09-2058,3,"in future work, we plan a human evaluation of our results to see if more features could lead to performance gains."
N09-2059,3,"as future work, we want to compare this approach of estimating entropy with other methods for estimating sense distributions which do not require hand-labelled data or parallel texts."
N09-2059,2,we wish to couple the confidence in the mfs with contextual evidence and investigate application on coarse-grained datasets.
N09-2060,1,"some potential future directions that would make greater use of this flexibility include the following: a simple extension from sense-kernels to word-kernels involves adding word nodes to the wordnet graph, with an edge linking each word to each of its possible senses."
N09-2060,1,"incorporating other wordnet relations such as meronymy and topicality gives a way of kernelizing semantic association or relatedness; one application of this might be in developing supervised methods for spelling correction (budanitsky and hirst, 2006).a wordnet graph can be augmented with information from other sources, such as links based on corpus-derived similarity."
N09-2060,4,"alternatively, the graph-based kernel functions could be applied to graphs constructed from parsed corpora (minkov and cohen, 2008)."
N09-2061,3,future work includes further experiments with structured classification to treat the three classes appropriately.
N09-2063,3,"our study can be extended via further experimentation with the methods we excluded in section 4.1, with other parsers, with other languages and with other zipfian cues of language (e.g."
N09-2065,3,"in future work, we would like to experiment with larger datasets, include semantic features, and trial other learners amenable to structured learning tasks."
N09-2066,1,we expect that further tuning of the method might help reduce these differences.
N09-2067,2,"our future work involves handling audio in foreign languages, that is robust to both asr and machine translation noise."
N09-2068,1,"in the future, other parametric forms can be utilized to better model the posterior score distributions."
N09-2069,2,we plan to collect multiple abbreviations for reference.
N09-2069,1,"after that we are going to combine the abbreviation modeling in the voice search system to alleviate the weakness of speech recognition for unknown abbreviation words, which are unlikely to be correctly recognized due to the out of vocabulary problem."
N09-2071,3,"encouraged by the results of our experiments, we plan to explore other relevance metrics that can encode more sophisticated constraints such as the relative coherence of the terms within a query."
N09-3001,1,"in future, we plan to improve (1) the entity alignment strategy, (2) the majority voting technique by setting a minimum threshold for the majority vote and better tie-breaking, and (3) the boosting algorithm to automatically optimize the parameters that have been manually set in this paper."
N09-3001,3,another possible avenue for future work would be to test these combination techniques with other coreference resolution systems.
N09-3003,1,"besides annotating and using more dialogue data as more people talk to our iqa system, we plan to implement a state-of-the-art topic-shift detection algorithm as proposed in (yang et al., 2006), training and testing it on our own fu q data."
N09-3003,1,we then plan to build dedicated logistic regression models for the different sub-classes of topic continuation fu qs.
N09-3003,1,"we will attempt to improve this system by adding action based features, and then extend it to distinguish three classes: topic shifts, (topic continuation) fu qs that are fully specified, and (topic continuation) context-dependent fu qs."
N09-3003,1,"also, from comparing the different models, we are interested in studying the specific properties of different fu q types."
N09-3003,1,"if each model uses a specific set of predictors, we hope to improve the overall rank of correct answers across the different classes of fu qs."
N09-3007,1,further classification is possible with both manual and automatic methods by utilizing individual contextual features in the optimal model.
N09-3008,1,"similarly, for building models from unaligned sequences, the addition of domain knowledge would likely prove beneficial."
N09-3008,1,"since profile hmm training is highly sensitive to the choice of initial model, we would like to explore more informed methods of constructing the initial model."
N09-3008,1,"we also plan to investigate better pseudo count methods, as well as the possibility of using n-grams as output symbols."
N09-3010,3,"in the future, we will conduct an annotation experiment with real users to evaluate the usefulness of rationales in terms of clock time."
N09-3011,4,our k-best framework can also be easily extended to cases where one name has multiple foreign translations that are equally likely.
N09-3011,1,"in future work, we may imagine penalizing insertions and deletions higher than substitutions and other non-uniform schemes for better transliteration performance."
N09-3013,1,"further work includes the improvement of the polarity classification component by using machine learning over annotated corpora and other techniques, such as anaphora resolution."
N09-3013,5,"moreover, we plan to study the manner in which opinion sentences of blogs/bloggers can be coherently combined."
N09-3014,2,"future work will have to concentrate on this aspect of sentence ordering, as it appears to coincide with the structure of the summaries for the duc 2005 dataset."
N09-3015,1,"along this perspective, da recognition could serve also as a basis for conversational analysis aimed at improving a fine-grained opinion mining in dialogues."
N09-3015,1,"regarding future developments, we will investigate how to include in the framework a wider context (e.g. the previous n utterances), and the introduction of new linguistic markers by enriching the preprocessing techniques."
N09-3015,2,"in particular, it would be interesting to exploit the role of slanted or affective loaded lexicon to deal with the misclassification of opinions as statements."
N09-3016,1,first is the minimum error rate training (mert) and the second is the decoding phase.
N09-3016,1,"we also plan to introduce a decoding scheme similar to the substring based transducer (sherif and kondrak, 2007) to improve the usage of lower order language models."
N09-5001,1,"in addition, we plan to automatically adjust cross-document event aggregation operations according to specific compression ratios provided by the users."
N10-1001,5,"however in many interactions, the social signals are at least as important as the propositional content of the words (pentland, 2008); it is a major challenge to develop meeting interpretation components that can infer and take advantage of such social cues."
N10-1001,1,it is a major challenge to develop algorithms that are unsupervised and adaptive to free us from the need to collect and annotate large amount of data each time we are interested in a new domain.
N10-1003,5,it remains to be seen if a similar approach can be used in other cases where em converges to widely varying local maxima.
N10-1004,1,"additionally, our model separates domain and syntax estimation, but a future direction is to learn these jointly."
N10-1004,5,"applying our methods to other generative parsers (such as (collins, 1999; petrov and klein, 2007)) is trivial, but it is less clear how our methods can be applied to the discriminative reranker component of the two stage parser."
N10-1004,4,2) application of the review summary generation approach in other domains and other languages;
N10-1004,3,3) data collection on user engagement with our dialogue systems involving reviewsummary evaluation.
N10-1006,1,"in our future work, we will investigate probability normalization methods and other techniques for term weighting to cope with these problems."
N10-1008,4,future work will focus on: 1) applying the sentiment scoring model to noun/verb sentiment assessment; 2) application of the review summary generation approach in other domains and other languages; 3) data collection on user engagement with our dialogue systems involving review summary evaluation.
N10-1009,1,further research will also explore using the extracted entities from advertisements to improve downstream sponsored search tasks.
N10-1011,1,future improvements include developing a nonparametric version that jointly learns how many visual terms and topics are optimal. another extension concerns the creation of visual terms.
N10-1011,4,"out with cognitive science, we hope that some of the work described here might be of relevance to more applied tasks such as thesaurus acquisition, word sense disambiguation, multimodal search, image retrieval, and summarization."
N10-1016,1,we would also like to investigate new methods to incorporate automatically learned translation boundaries more efficiently into decoding in an attempt to further improve search in both speed and accuracy.
N10-1016,1,"future work in this direction will involve trying different methods to define more informative translation boundaries, such as a boundary to begin/end a swapping."
N10-1017,4,we intend also to apply htp to learning paraphrases in languages other than english and investigate the impact of the learned paraphrases on resource-sparse machine translation systems.
N10-1017,1,"as future work, we plan to learn the distribution of weights on edges to phrase nodes and feature nodes automatically from data, rather than tuning them manually, and to develop a probabilistic model supporting htp."
N10-1018,1,while it is quite possible that the article correction system presented here can be improved 鈥 we would like to explore improving the system by using a more sophisticated feature set 鈥 we believe that the performance gap due to the error driven training paradigms shown here will remain.
N10-1018,4,"finally, while this study focused on the problem of correcting article mistakes, we plan to apply the proposed training paradigms to similar text correction problems."
N10-1019,1,we believe that the logical next step is to combine more primary models in the meta-classifier.
N10-1019,1,"candidates for additional primary models include (1) more classifiers trained either on different data sets or with a different classification algorithm, and (2) more language models, such as skip models or part-of-speech n-gram language models."
N10-1020,2,"in the future, we wish to scale our models to the full corpus, and extend them with more complex notions of discourse, topic and community."
N10-1020,4,"ultimately, we hope to put the learned conversation structure to use in the construction of a data-driven, conversational agent."
N10-1022,1,we also plan to expand linear interpolation language model scheme to include history specific (context dependent) weights.
N10-1022,1,"as a future work, we plan to use the proposed criterion for adapting log-linear models used in machine translation, conditional random fields (crf) and other applications."
N10-1029,2,"it would therefore be interesting to conduct this study on a larger scale, using more general mwe definitions such as automatically learned collocations (smadja, 1993) or verb-noun constructions (diab and bhutada, 2009)."
N10-1030,1,"in the future, we will use an automatic word sense disambiguation (wsd) system to obtain word senses and study the function of wsd for srl."
N10-1031,4,our extensions are not specific to hter tasks; improved alignments and additional features should improve performance on any task having sufficient tuning data.
N10-1033,1,this suggests that faster composition algorithms that incorporate top down filtering may still be discovered.
N10-1036,3,in the future we aim to set up an online news article analysis system and perform larger and regular utility evaluations.
N10-1041,6,these results provide a starting point for future work in interactive summarization.
N10-1042,1,"for our future work, we plan to investigate other adaption methods, and try to address some of the problems identified in our error analysis."
N10-1043,5,future work in discourse analysis will need to develop better understanding of how the two types of coherence interact.
N10-1050,6,"in the future, we hope that litgs will be a spring board towards full itgs, with more interesting nonterminal than the bitgs seen in the literature so far."
N10-1051,3,one future experiment would be to see whether these results are reliable across other publications in the domain.
N10-1051,3,"another set of experiments would be to determine the optimum number of annotators; we assumed three, but cross-domain results may be more stable with more annotators."
N10-1052,1,using xx rules to model legitimate word insertions is a topic for future work.
N10-1054,2,"future work will widen the study by (i) looking at a wider range of words and languages,"
N10-1054,1,and (iii) integrating unsupervised subjectivity features into cross-lingual lexical substitution.
N10-1056,3,further evaluation could include comparison with machine-translation and paraphrasing algorithms.
N10-1056,1,another idea would be to estimate simplification priors based on a model of inherent lexical complexity; some possible starting points are number of syllables (which is used in various readability formulae) or word length.
N10-1056,4,it would be interesting to use our proposed estimates as initialization for em-style iterative re-estimation.
N10-1057,6,"the next step is to explicitly identify undesired worker behavior, such as not editing the mt output at all, or using the human reference as is instead of editing the mt output."
N10-1059,1,"in future, we will try more syntactic structure generalization strategies."
N10-1062,3,in the near future we also hope to test the online em setup in an application setting such as a computer aided translation or crowdsourced generated streams via amazon鈥檚 mechanical turk.
N10-1063,5,We plan to address this question in future work.
N10-1068,1,"an extension of fsts along the lines of recursive transition networks may be appropriate, but we leave details for future work."
N10-1070,1,"in future work, we would like to explore further enhancements to weighting in lda."
N10-1070,1,there are many variants which can be considered: one example is the incorporation of word order and context through an n-gram model based on conditional probabilities.
N10-1070,3,"we also aim to evaluate lda against lsi with a view to establishing whether one can be said to outperform the other consistently in terms of precision, with appropriate settings held constant."
N10-1070,5,"finally, we would like to determine whether other techniques which have been shown to benefit lsi can also be usefully brought to bear in lda, just as we have shown here in the case of term weighting."
N10-1071,1,"in future work, we would like to extend our approach to other models, e.g., sparse combinations of lexicalized features."
N10-1072,1,"in the future, we will try to develop more sophisticated features in entity linking and design a typical learning to rank method for the entity linking task."
N10-1076,4,"our framework is applicable for any sequence labeling task that can be done at either a word or a sub-word (e.g., character) level."
N10-1076,2,"since obtaining data for these tasks is substantially more expensive, we hope to use active learning to obtain more data."
N10-1076,1,"also, other approximate decoders used in pipeline approaches could be explored as alternatives to the one we used (e.g., finkel et al., 2006)."
N10-1076,1,"finally, for the sake of completeness, we note that more recent work has been done based on our baseline models that has emerged since the preparation of the current work, particularly zitouni et al.(2009) and mohamed et al.(2009)."
N10-1076,2,we wish to address any improvements captured by this more recent work such as the use of different data sets and addressing problems with the hamza to decrease error rates.
N10-1076,4,segmentation and lemmatization are particularly promising tasks to which our approach could be applied.
N10-1076,1,"additionally, we wish to include our model as a stage in a pipeline that segments, diacritizes, and labels morphemes."
N10-1076,1,"In future work, we would like to consider using CRFs in place of MEMMs."
N10-1077,5,such analysis will reveal whether an affix is more inclined towards joining or occurs freely more frequently.
N10-1077,1,in future work bi-gram statistics can be used to merge morphemes.
N10-1077,2,more data can be tagged to find out joining probabilities for the affixes that occur as free morpheme.
N10-1077,2,similarly a corpus can be tagged on compounds.
N10-1078,6,we believe that there is ample opportunity to provide additional assistance and we will explore this in future work.
N10-1080,3,we hope future work on developing new evaluation metrics will explicitly explore the translation quality of models trained to them.
N10-1084,5,"another interesting question that we have not addressed is whether some languages are better suited to linguistic steganography than others, or whether some languages are better suited to particular linguistic transformations than others."
N10-1084,3,how best to evaluate the imperceptibility of such a system we leave to future work.
N10-1085,2,we intend to explore deriving such labels from resources such as wordnet or ontonotes.
N10-1085,2,"finally (and perhaps most importantly), we expect that our model would benefit from additional training data, and plan to train on a larger, automatically-parsed corpus."
N10-1085,1,we also plan to continue exploration of msa training methods.
N10-1087,1,"it might be possible to discover some interesting regularities about the (preferential) uses of terms within semantic domains, as reflected in term network connectivity."
N10-1087,1,"one direction of our ongoing work is to determine this distribution, and to empirically derive its parameters."
N10-1088,1,there is a great deal of room for future work in expanding the ability of gloss extraction systems to extract sense glosses that more closely match the meanings of a word.
N10-1088,1,"an important first step in this direction is to extract relations, rather than ngrams, that make up the definition a word鈥檚 senses."
N10-1089,3,"furthermore, the evaluation results presented in this paper could be strengthened by adding manual multiword expression annotation to some treebank."
N10-1089,1,"thus, unifying multiword expressions longer than two words would potentially contribute more to shallow parsing accuracy."
N10-1089,3,future work will focus in conducting similar experiments for multiword expressions longer than two words.
N10-1089,1,"one would expect that due to their size, a wrong interpretation of their structure would affect the shallow parser output more than it does for multiword expressions consisting of two words."
N10-1092,1,"as future work, we plan to proove the correctness of the modified algorithm and to study the impact of these modifications on the use of sitgs for machine translation, and the estimation of sitgs."
N10-1093,3,"experiments can be done in future, to find out if there is a label bias to the clause boundary, which also helps in reducing the search space for specific labels."
N10-1094,1,"however, our hope, and expectation, is that the vast majority of real-life dtree queries will be local (parent,child,sister) searches on the derivation tree, since each node of the derivation tree already encodes small chunks of structure."
N10-1097,1,"in future work, we plan to further investigate the connection between an initiation-response pairs from multiple dimensions, such as topical coherence, semantic relatedness, conversation acts, etc."
N10-1097,1,"one important current direction is to develop a richer operationalization of the interaction that accounts for the way posts sometimes respond to a user, a collection of users, or a user鈥檚 posting history, rather than specific posts per se."
N10-1099,3,"we plan to evaluate this method on additional data sets, and in the context of automated essay scoring."
N10-1100,3,we are experimenting with using a front-end classifier for producing trending topics within distinct categories and then summarizing around these topics in order to generate an automated real-time newspaper.
N10-1100,1,"presently, we are working on extending our pr algorithm to providing real-time summaries within specific topics."
N10-1102,1,"options to explore include the use of language identification probabilities as features in the transliteration system (li et al., 2007), as well as splitting the data into sets that are not necessarily disjoint, allowing separate transliteration models to learn from potentially useful common information."
N10-1102,1,"in the future, we plan to investigate other methods of incorporating language identification in machine transliteration."
N10-1105,1,"in the future, we will investigate knowledge-richer methods for segmentation."
N10-1105,5,"in particular, we will investigate whether an automatic vocalization step previous to segmentation will improve pos tagging accuracy for unknown words."
N10-1107,1,the algorithm that builds the lexicon creates a more or less hierarchical structure subwords tend to be composed from those extracted at the previous iteration.
N10-1107,4,"our method of subword extraction could also be applied to phrase extraction for machine translation, or in finding subwords for related problems like transliteration."
N10-1108,1,"one way to extend this work is to use the automatically extracted phrase patterns as initial features, and then employ supervised or semi-supervised learning techniques to learn a more discriminative feature set."
N10-1109,4,this technique may be applicable to classifying other phenomena.
N10-1110,3,"in the future, we intend to examine techniques by which these representations could be used to further improve word recognition results."
N10-1111,3,"additionally, various sensitivity, convergence, and robustness properties of the method need to be analyzed."
N10-1111,1,In future work we will investigate our approach in such settings.
N10-1112,1,future work might exploit approximate inference for more expressive cost functions.
N10-1113,4,this corpus will be of use in future research on syntactic role preferences and for the training of monolingual subject-object disambiguates.
N10-1115,5,"we hope that further work on this non-directional parsing framework will pave the way to better understanding of an interesting cognitive question: which kinds of parsing decisions are hard to make, and which linguistic constructs are hard to analyze?"
N10-1116,1,future work could explore unifying these techniques with other state-of-the-art approaches.
N10-1117,4,"in future work we plan to apply relaxed marginal inference to larger joint inference problems within nlp, and test its effectiveness with other marginal inference algorithms as solvers in the inner loop."
N10-1119,5,"a primary question is whether such lexicons improve performance over a translate-to-english strategy (banea et al., 2008)."
N10-1119,2,"in the future we plan to investigate the construction of web-derived lexicons for languages other than english, which is an active area of research (mihalcea et al., 2007; jijkoun and hofmann, 2009; rao and ravichandran, 2009)."
N10-1122,1,"so far, we focused on adjectives as sentiment indicators, however, there have been studies showing that other parts of speech can be very helpful for this task (e.g., pang et al.2002; benamara et al.2007)."
N10-1122,1,"also, it would be interesting to take a closer look at the interactions between aspect and sentiment, especially at a multiple-sentence level (see snyder and barzilay 2007)."
N10-1122,3,"finally, we feel that the true test of the usability of our system should be through an application, and intend to proceed in that direction."
N10-1122,6,this work has opened many avenues for future research and improvements.
N10-1123,4,"directions for future work include: leveraging additional joint-inference opportunities, better integration of syntactic parsing and event extraction, and applying this approach to other extraction tasks and domains."
N10-1124,2,"in a similar perspective, we are continuing our efforts to construct a larger manually annotated collection of abstracts."
N10-1124,1,"as the distribution in abstracts is not the same among pico elements, it is expected that differentiated weighting schemes could result in better retrieval effectiveness."
N10-1124,4,we plan to extend the coverage of queries to other topics in the future.
N10-1126,1,"again, the goal will be to model the most successful wizards, based upon data from recognition results, nlu, and voice search results."
N10-1126,3,our next experiment will collect full dialogues with embedded wizards whose actions will again be restricted through an interface. we will select wizards who perform well during pilot tests.
N10-1126,1,that wizard actions can be modeled using system features bodes well for future work.
N10-1131,2,"in future work, we plan to support more complex transformations, instead of only removing words and experiment with different sizes of training data."
N10-1132,1,"based on the email error analysis, we plan to pursue domain adaptation techniques to improve performance on different types of emails."
N10-1132,4,future work will focus on the generation component and on applying the summarization system to conversations in other modalities such as blogs and instant messages.
N10-1133,1,"in the future, we would like to apply a similar exhaustive search strategy, but this time with different compression ratios, in order to see the impact of compression ratios on the pdf of each domain."
N10-1133,1,such an analysis would allow us to see whether these extracts exhibit certain properties which could be used in training machine learning systems.
N10-1133,1,"furthermore, we would also like to analyze the high scoring extracts found by the exhaustive search, in terms of coherence, position and other features."
N10-1135,1,"we see two main avenues for future work: (1), the construction of properly bilingual models where source language information can also help to further improve the target language model (diab and resnik, 2002); (2), the extension of our cross-lingual mapping for the argument position to mappings that hold across multiple predicates as well as argument dependent mappings like the spanish direct objects, whose realization depends on their animacy."
N10-1137,1,we also aim to embed and test our role induction method within a full srl system that is also concerned with argument identification.
N10-1137,1,"importantly, the framework can incorporate different probabilistic models for detection and canonicalization which we intend to explore in the future."
N10-1137,1,"eventually, we also intend to replace the treebank-trained parser with a chunkier."
N10-1138,4,"our system achieves improvements over the state of the art at each stage of processing and collectively, and is amenable to future extension."
N10-1140,1,"in future work, we plan to extend the parameterization of phrase based lexicalized reordering models to be sensitive to these discontinuities, and we will also consider adding syntactic features to our models to penalize discontinuities that are not syntactically motivated (marton and resnik, 2008; chiang et al., 2009)."
N10-1144,1,we are also in the process of developing a framework within which we can use transfer rules and a bidirectional grammar to automate such complex syntactic reformulation.
N10-2004,1,"we will continuously develop the tools by improving their functionalities through user-testing and feedback, and also by applying them to more languages."
N10-2007,5,"investigating how to score these general aspects, and presenting this information in an intuitive way, are directions for our continued development of this tool."
N10-2007,1,"based on our observations, however, the error analysis support could still be improved by directing users towards features that not only point to differences and similarities between different subsets of instances, but also to more information about how features are being used in the trained model."
N10-2007,1,"this can be implemented either in algorithm-specific ways (such as displaying the weight of features in an svm model) or in more generalizable formats, for instance, through information gain."
N10-2010,1,"future work includes improvements to the client side (e.g., confidence measures as a visual aid, multimodality), as well as exploring other kinds of parsing algorithms for the server side (e.g., adaptative parsing)."
N10-3005,3,"furthermore, we started with evaluation on the task which has been proposed for the evaluation of </bodytext> <page confidence=""0.992678""> 27 </page> <bodytext confidence=""0.999687428571429""> word space models at the level of word meaning."
N10-3005,2,"next steps include, amongst others, evaluating the suggested model with a bigger data corpus as well as using stemming and more sophisticated filling of word matrices, e.g., by introducing advanced weighting schemes into the matrices instead of simple counts."
N10-3005,3,"we need, however, to evaluate the model for the tasks where word order information matters more, e.g. on selectional preferences or paraphrasing."
N10-3005,1,"last but not least, we plan to address the issue of modeling compositional meaning with matrix-based distributional model of meaning."
N10-3007,2,"we are currently working on multiple extensions of this work, including investigating how the results can be applied to other corpora, adding additional features, and finally methods for post-processing extractive summaries."
N10-3009,1,"opinion targets are more difficult to be identified than opinion holders, and deserve more attention in the nlp field, and we also would extend the targets to verb phrases and embedded clauses in addition to noun phrases."
N10-3009,1,"the proposed approach is highly dependent on dependency parser, and we would like to further investigate machine learning approaches (including the crf model) by treating dependency structures as one of the linguistic features, which could be more robust to parsing errors."
N10-3009,1,to explore the effectiveness of our approach with english data such as mpqa is another direction.
N10-3010,1,we will also look into the linguistic clause based reordering features which would help in reordering of distant pair of languages.
N10-3010,3,to further evaluate the approach we would also try the approach on some other distant language pairs.
N10-3010,3,manual evaluation of the output will throw some light on the effectiveness of this system.
N10-3011,5,"we are very excited about the future stages of this study, and its potential contribution to the linguistic perspective of the field of machine translation."
N12-1001,1,"interesting future directions include exploring supervised narrative disentanglement, combining mnd with narrative induction (chambers and jurafsky, 2009) and applying mnd to non-fictional texts."
N12-1002,1,in future work we will explore this direction and go more thoroughly into individual differences in entrainment behavior.
N12-1005,1,"given that moses only implicitly uses ngram based information, adding soul translation models is expected to be even more helpful."
N12-1005,1,"future work will thus aim at introducing them into conventional phrase-based systems, such as moses (koehn et al., 2007)."
N12-1006,2,the gains we observed from using msa morphological segmentation can be further increased with dialect-specific segmenters.
N12-1006,5,topic adaptation is another important problem to tackle if the large msa linguistic resources already developed are to be leveraged for dialectal arabic-english mt.
N12-1006,2,input preprocessing can also be used to decrease the noise of the user-generated data.
N12-1009,1,the next direction in this research is to extract the scope of a given reference as a standalone grammatical sentence.
N12-1009,1,"in many cases, the scope identified by our method can form a grammatical sentence with no or minimal postprocessing."
N12-1009,1,"in other cases, more advanced text regeneration techniques are needed for scope extraction."
N12-1011,6,"for contextual based tasks, the test takers are asked to read or listen to some stimulus material, and answer a question based on this information."
N12-1011,1,we can build models using these materials to check the correctness and relatedness of the spoken responses.
N12-1011,1,we will also explore generating other features measuring the higher-level aspects of the spoken responses.
N12-1011,1,"for example, we can extract features assessing the responses’ relatedness to the stimulus of an opinion-based task."
N12-1011,5,"we will also investigate whether the content features can provide additional information for automated speech scoring, and help build better scoring systems when they are combined with other non-content features, such as the features representing fluency, pronunciation, prosody, vocabulary diversity information."
N12-1011,5,"in our future work, we will compare models trained on human transcripts and on asr outputs, and investigate whether we should use matching data for training and evaluation, or whether we should not introduce noise during training in order to maintain the validity of the models."
N12-1014,4,"more generally, we offer our approach as an intriguing new tool to help semisupervised learning benefit from very large datasets."
N12-1016,4,"in current and future work, we will show how s can be used to analyze hierarchical segmentations, and illustrate how to apply s to linear segmentations containing multiple boundary types."
N12-1020,1,"future work will focus on parsing classical chinese poems of other poets, and on enriching the corpus with semantic information, which would facilitate not only deeper study of parallelism but also other topics such as imagery and metaphorical coherence (zhu and cui, 2010)."
N12-1024,1,"one avenue we are investigating is the use of a non-deterministic g, which would allow us to encode latent variables (dreyer et al., 2008), such as loosely defined “regions” within a string, and to allow for the encoding of alignments between the input strings."
N12-1024,4,"we would also like to extend these methods to other combinatorial optimization problems involving strings, such as inference in graphical models over strings (dreyer and eisner, 2009)."
N12-1025,3,"for future work, we want to test the effect of improved tm in the context of different nlp applications such as mt and cross language ir."
N12-1026,1,"in future work, we would like to investigate other objectives with a more direct task loss, such as maxmargin (taskar et al., 2004), risk (smith and eisner, 2006) or softmax-loss (gimpel and smith, 2010), and different regularizes, such as li-norm for a sparse solution."
N12-1026,1,"instead of n-best approximations, we may directly employ forests for a better conditional log-likelihood estimation (li and eisner, 2009)."
N12-1026,1,"we would also like to explore other mixing strategies for parallel training which can directly minimize the training objectives like those proposed for a cutting-plane algorithm (franc and sonnenburg, 2008)."
N12-1026,1,"in future work, we would like to investigate other objectives with a more direct task loss, such as maxmargin (taskar , 2004), risk (smith and eisner, 2006) or softmax-loss (gimpel and smith, 2010), and different regularizers, such as li-norm for a sparse solution."
N12-1031,3,"we note that this work also experimented with dependency parsing, and then automatically converting the results to ps, a further basis of comparison."
N12-1031,1,"one other aspect of future work is to implement the algorithm in wang and zong (2010), using our own dependency representation, since this would allow a precise investigation of what the phrase structure parser is contributing as compared to our automatic conversion."
N12-1031,3,we will also experiment with dependency parsing for the ptb dependency representation discussed in this paper.
N12-1031,3,"following this, we will then experiment with parsing the arabic dependency representation, converting to phrase structure, and evaluating the resulting phrase structure representation as usual for parsing evaluation."
N12-1031,1,"habash and roth (2009) discuss an already-existing dependency representation of parts of the atb and it will be interesting to compare the conversion accuracy using the different dependency representations, although we expect that there will not be any major differences in the representations."
N12-1036,3,"as for the further work, we intend to evaluate and improve our system further in learner productivity in terms of output quality, typing speed, and the amount of using certain keys such as delete and backspace."
N12-1039,1,"future work will consider unsupervised or semisupervised approaches to word origin recognition for this task, and methods to tune the smoothing weights a at the language rather than the global level."
N12-1041,1,we plan to model interspeaker topics in the graph-based approach in the future.
N12-1043,5,we would like to investigate in future work if the information provided by the two types of classes is indeed largely redundant or if a more sophisticated combination would perform better than the simple linear interpolation we have used here.
N12-1045,1,the model can serve as a basis for several further extensions.
N12-1046,1,we might also be able to productively group similar documents into clusters in which the vocabulary choices are (or should be) mutually reinforcing.
N12-1047,2,"in the future, we intend to investigate improved sentence-bleu approximations to help narrow the gap between mira and the direct optimizers."
N12-1048,1,"we are also exploring new algorithms for performing reordering aware incremental speech-to-speech translation, i.e., translating source phrases such that text-to-speech synthesis can be rendered incrementally."
N12-1049,2,we hope to improve detection and explore system performance on multilingual and complex datasets in future work.
N12-1051,1,"additionally, the taxonomies inferred with the hrg do not currently admit term ambiguity which we could remedy by modifying our technique for constructing a consensus hierarchy to reflect the sampled frequency of observed subtrees. a small number of items, consequently limiting the scope of any computational model based on normed data."
N12-1051,1,"besides exploring the performance of our algorithm on more specialized domains (e.g., mathematics or geography) we would also like to create an incremental version that augments an existing taxonomy with missing information."
N12-1051,6,Avenues for future work are many and varied.
N12-1054,3,possible future work includes experiments using cascades to explore much higher-order models.
N12-1056,4,"one future avenue is to explore the use of this structure in applications such as machine translation, as potentially enabling greater use of long distance dependencies than in prior work, such as by hasan et al.(2008)."
N12-1057,1,we intend to develop a better dialog act tagger which we can use to automatically obtain dialog act labels for odp classification.
N12-1058,1,"in the near future, we will integrate a module which can resolve pronouns and deictics to textual antecedents, including type information provided by indefinite descriptions."
N12-1058,1,this will make the system fully multi-modal.
N12-1058,5,Additionally we intend to study issues of timing.
N12-1059,5,the major open question is how our trait-based combination interacts with multi-system combination.
N12-1059,5,or can you jointly combine all of the trait hypotheses and get an even greater relative gain?
N12-1059,5,"if you independently improve all three using trait-based combination, will the relative gain from multi-system combination be reduced?"
N12-1063,6,future studies in early childhood readability need to take visual content into account.
N12-1066,6,"future research on mixed-initiative user interfaces might try to detect and encourage these kinds of annotator behaviors, and potentially improve interactive machine learning outcomes."
N12-1073,1,future work directions may include improving the detection algorithms by filtering the context sentences more intelligently.
N12-1073,1,"context features may also be used for first filtering citations which have been mentioned only in passing, and then applying context based sentiment classification to the remaining significant citations."
N12-1074,1,"these may include clique-specific language features, more properties of the user’s social network, mentions of named entities and topics of tweets."
N12-1074,6,another direction is to distinguish between replies and retweets and to predict the number of responses and the length of conversations that a tweet may generate.
N12-1074,1,"there is also potential in learning models for the prediction of other measures of impact, such as hashtag adoption and inclusion in “favorites” lists."
N12-1074,1,the natural path for future work is to improve performance using new features.
N12-1075,6,"looking forward, there are multiple directions to explore."
N12-1077,5,"while this result is interesting and encouraging, it also raises several research questions, such as how to enhance the quality of each vector space model and whether the models can be combined more effectively3."
N12-1077,4,"we also would like to study whether similar techniques can be useful when comparing longer text segments like phrases or sentences, with potential applications in paraphrase detection and recognizing textual entailment."
N12-1081,3,"future work could test this claim, compare other graphical models, such as dynamic bns, and aim for a comprehensive human evaluation."
N12-1087,1,"our technique is amenable to be combined with many existing variations of em (berg-kirkpatrick et al., 2010)."
N12-1087,6,We leave this as future work.
N12-1088,1,"but there are alternative approximation techniques that scale well to large data sets (halko et al., 2009)."
N12-1089,1,"additionally, although this work focused solely on dialogue related features, future work may wish to take a closer look at the autocorrection mistakes themselves (e.g., which words are most likely to be mistakenly corrected, etc.)."
N12-1090,3,"second, we will perform an empirical comparison of two approaches to projecting coreference annotations, our translation-based approach and camargo de souza and orasan’s (2011) approach, where annotations are projected via a parallel corpus."
N12-1090,2,"first, we will isolate the impact of each factor that adversely affects its performance, including errors in projection, translation, and coreference resolution in the resource-rich language."
N12-1090,1,"fourth, since the success of our projection approach depends heavily on the accuracies of machine translation as well as coreference resolution in the source language, we will determine whether their accuracies can be improved via an ensemble approach, where we employ multiple mt engines and multiple coreference resolvers."
N12-1090,4,"finally, we plan to employ our approach to alleviate the corpus-annotation bottleneck, specifically by using the annotated data it produces to augment the manual coreference annotations that capture the specific properties of the target language."
N12-1090,2,"third, rather than translate from the target to the source language, we will examine whether it is better to translate all the coreference-annotated data available in the source language to the target language, and train a coreference model for the target language on the translated data."
N12-1090,1,"to gain additional insights into our approach, we plan to pursue several directions."
N12-1092,5,we additionally intend to cast the sentence selection problem as a separate learning problem that can also be trained from human judgments.
N12-1092,1,"in our future work, we hope to expand the set of features as described in section 7."
N12-1093,4,"we would also like to scale our model to more challenging domains (e.g., product descriptions) and to enrich our generator with some notion of discourse planning."
N12-1093,5,an interesting question is how to extend the pcfg-based approach advocated here so as to capture discourse-level document structure.
N12-1093,1,"in the future, we plan to remedy this by using forest reranking, a technique that approximately reranks a packed forest of exponentially many derivations (huang, 2008)."
N12-1095,1,"in doing so, we will develop a system that can automatically induce high quality, human-readable bilingual dictionaries from large corpora using unsupervised learning methods."
N12-1095,1,"in future work, we hope to combine our clustering method with a system for automatically generating translation sets."
N12-1096,1,we hope new opportunities will arise as this work explores a new research area for topic models.
N12-1096,1,"for selectional preference, components could correspond to semantic features that intersect to define semantic classes (gormley et al., 2011)."
N12-2001,3,we will continue this line of work using both technical evaluation measures as well as user focused evaluations.
N12-2001,1,"finally, our gold standard collection penalizes a data-driven approach, which might offer a broader range of experts."
N12-2001,1,"consideration could be given to term dependence and positional models as in metzler and croft (2005), which might improve our proximity-based scoring function."
N12-2001,1,"for example, modelling the user background and interests could increase the system’s effectiveness."
N12-2001,3,some more realistic end-user studies could be used to evaluate the systems.
N12-2001,6,There are a number of directions for future work.
N12-2003,1,"other future work might apply subjectivity features to cluster adjectives into classes pertinent to ao, perhaps in combination with independent distributional measures of semantic similarity."
N12-2003,1,"therefore, beyond the adjustments discussed above, the next stage in this research will evaluate the effects of combining semantic features with direct evidence in a single system."
N12-2005,1,we will also put effort in the automatic extraction of symptom identification rules by analyzing the classification predictions and the corresponding document feature vectors.
N12-2005,1,"as a follow up to this study we will try to generalise this algorithm to a more abstract level so that it can be transferable for the identification of other health conditions, medication etc."
N12-2006,4,"however, we expect the results to generalize at least to similar tasks with other precision grammars, and probably treebank-derived parsers as well."
N12-2006,4,exploration of how well these results hold for other tasks and for other types of parsers is an excellent subject for future research.
N12-2007,3,"future work includes comparing our approach with esa, experimenting on more courses from more universities, and adapting our work to courses in other languages."
N12-2010,3,future work will also include extensive evaluation of our proposed models.
N12-2011,5,investigating how df can be extended and utilized in a multi-lingual environment is an interesting future direction.
N12-2012,6,"we did not find any relevant correlation studies between age, origin, and other attributes with humor, but such research has likely been explored."
N12-3001,1,"as future work we aim at allowing for the comparison at fragment level, where a fragment is considered a part of a function, a group of functions."
N12-3001,2,we plan in the next future to extend its functionality to other common programming languages.
N13-1001,5,it would also be interesting to study whether the insight of using minimal units for modeling and phrase-based search would hold for hierarchical smt.
N13-1001,1,"in future work, we would like to study whether a phrase-based system like moses or phrasal can profit from an osm-style or n-gram style feature."
N13-1001,1,vaswani et al.(2011) recently showed that a markov model over the derivation history of minimal rules can obtain the same translation quality as using grammars formed with composed rules.
N13-1001,1,feng et al.(2010) previously showed that adding a linearized source-side language model in a phrase-based system helped.
N13-1002,1,"we are incorporating the models into first pass decoding, in hopes of even greater gains."
N13-1002,1,"in future work, perhaps we would see larger gains by including additional decomposition orders (e.g., top-down in a dependency tree), and taking this idea deeper into the machine translation model, down to the word-alignment and language-modeling levels."
N13-1002,1,we were surprised to find n-best reranking so effective.
N13-1003,1,"in the future, we would like to investigate how to incorporate useful future cost estimates for our sparse reordering features."
N13-1003,1,"we would also like to investigate features inspired by transition-based parsing, such as features that look further down the reordering stack."
N13-1005,1,"in future research, we plan to explore these issues in more depth to design a more general multi-faceted event recognition system, and we plan to investigate new ways to use these event dictionaries for event extraction as well."
N13-1005,5,an open question for future work is to investigate whether the same multi-faceted approach to event recognition will work well for other types of events.
N13-1005,1,"our belief is that many different types of events have characteristic agent terms, but additional types of facets will need to be defined to cover a broad array of event types."
N13-1005,1,the syntactic constructions used to harvest dictionary items may also vary depending on the types of event information that must be learned.
N13-1006,4,"the final ilp-based bilingual ner tagger with soft constraints is publicly available at: github.com/carfly/bi_ilp future work could apply the bilingual constraint based method to other tasks, such as part-of-speech tagging and relation extraction."
N13-1008,1,in future work we also plan to integrate universal entity types and attributes into the model.
N13-1010,1,"dlt, on the other hand, predicts that there is a storage cost for maintaining unresolved dependencies during a parse (gibson, 2000)."
N13-1010,1,future work could use these sequential cueing operations to investigate further claims of the dynamic recruitment hypothesis.
N13-1010,1,"one of the implications of the hypothesis is that recruitment of resources alleviates the initial encoding cost, which allows the parser to continue on as before the embedding."
N13-1013,1,"the models presented here are very simple, and in future work we would like to explore more complex approaches such as quasi-synchronous grammars (smith and eisner, 2009; li et al., 2012) or automatic treebank transformation (niu et al., 2009)."
N13-1015,1,"there are natural ways to extend the approach to semi-supervised learning; for example the svd step, where representations of outside and inside trees are learned, could be applied to unlabeled data parsed by a first pass parser."
N13-1015,1,"there are a large number of parameters in the model, and we suspect that more sophisticated regularization methods than the smoothing method we have described may improve performance."
N13-1016,1,"other possible extensions to this work include exploring alternative approaches to generating candidate images and developing techniques to automatically identify abstract topics for which suitable images are unlikely to be found, thereby avoiding the problem cases described in section 6."
N13-1018,2,"incorporating a better source of prior knowledge in the generalization phase (e.g., yago or dbpedia) is also an interesting research direction towards a better phrase aggregation step."
N13-1018,1,"on the other hand, we plan to apply a ranking strategy to select the top candidate phrases generated by our framework."
N13-1019,3,"for example, in order to compare with sits, one can make an assumption that each document just has one speaker."
N13-1019,1,"currently, our model uses marginal boundary probabilities to generate the final segmentation."
N13-1019,1,"instead, we could develop a metropolis-hasting sampling algorithm to move one boundary at a time, given the gold-standard number of segments."
N13-1019,1,"in future work, we would like to make the model fully nonparametric and investigate the effects of adding different cues in texts, such as cue phrases, pronoun usage, prosody, etc."
N13-1021,1,"first, word alignment models can be extended to process asr lattices or word confusion networks as part of the unsupervised alignment learning algorithm, and incorporated into our approach."
N13-1021,1,"second, the contextual features can be refined (e.g., concatenated features instead of smoothed features) when large amounts of training data is available."
N13-1021,5,"In future work, we would like to investigate two questions left open by these results."
N13-1023,1,"as part of future work, we plan to extend the framework presented in this work for performing speech-to-speech translation."
N13-1023,5,we also plan to address the challenges involved in s2s translation across languages with very different word order.
N13-1024,1,future work will concentrate on more elaborate selection functions as well as more sophisticated ways to combine the different resources.
N13-1025,1,"in particular, we hope this framework will facilitate incorporation of richer linguistic knowledge into machine translation."
N13-1027,1,"we also believe that there is much work to be carried out and that induction from summaries should be complemented with a process that explores full event reports, in order to reinforce some induced concepts, discard others, and discover additional ones."
N13-1029,3,"in future work, we plan to work towards the development of an automatic stopping criterion, a more principled test for whether each successive iteration of label collapsing provides some useful benefit to the underlying grammar."
N13-1030,3,"similar to the work of hasan et al.(2006) in the machine translation field, we plan to experiment with high order pos language models reranking."
N13-1030,1,"in future work, we intend to examine how grammaticality of the generated compressions can be enhanced."
N13-1031,1,our future aim is to add a semantic level to the annotation.
N13-1032,3,"as future work, we would like to evaluate our algorithm on other language pairs."
N13-1032,1,"also, it would be interesting to model the segmentation explicitly, where the aim would be to first segment the sentence and then use a two level hierarchical reordering model which first reorders these segments and then reorders the words within the segment."
N13-1032,1,we also plan to integrate the score assigned by our model into the decoder to avoid having to do n decoding for every source sentence.
N13-1033,5,we believe that explorations of modern parallel hardware architectures is a fertile area of research: the field has only begun to examine the possibilities and there remain many more interesting questions to tackle.
N13-1033,1,"parallelism is critical not only from the perspective of building real-world applications, but for overcoming fundamental computational bottlenecks associated with models that researchers are developing today."
N13-1036,5,we are interested in studying how our approach can be combined with solutions that simply add more dialectal training data since the two directions are complementary in that they address linguistic normalization and domain coverage.
N13-1036,1,we also plan to automatically learn additional rules from limited available data (da-msa or da-english).
N13-1036,4,"in the future, we plan to extend elissa’s coverage of phenomena in the handled dialects and to new dialects."
N13-1036,3,"we also would like to do additional mt experiments where we use elissa to preprocess the training data, comparable to experiments done by sawaf (2010)."
N13-1038,5,"for future work, we would like to explore parallelized quadratic optimization and larger minibatch sizes, and eventually apply it to machine translation."
N13-1041,5,"as future work, we would like to explore how to incorporate textual contents without opinionated expressions."
N13-1041,1,one possible way is to consider the combination of matrix factorization and topic modeling as studied by wang and blei (2011) where we can use topic modeling to study textual contents.
N13-1043,2,sentimental language is an unexplored avenue for improving natural language systems that operate in situated settings.
N13-1044,1,"in the future, we intend to perform further feature engineering to improve the results of mada-arz, and extend the system to handle other das."
N13-1045,1,it would also be worth investigating the performance of using our sentence-level model to re-rank n-best outputs of a confusion network model.
N13-1045,1,there are several avenues for future work: we have focused on bigram dependencies in our models; extension to more than two dependent elementary trees is straightforward.
N13-1046,4,we plan to expand the features and apply the classifier to new languages.
N13-1046,3,and conduct mt experiments in domains other than news.
N13-1048,1,"we will also explore the use of mrf-based translation models for translation systems that go beyond simple phrases, such as hierarchical phrase based systems (chiang 2005) and syntax-based systems (galley et al.2004)."
N13-1048,1,in future work we strive to fully realize the potential of the mrf model by developing features that can capture more sophisticated translation dependencies that those used in this study.
N13-1050,5,"future work can investigate how to more tightly integrate our beam-search decoder for text normalization with a standard mt decoder, e.g., by using a lattice or an n-best list."
N13-1053,5,"we have also identified other features that vary with the sentiment, such as first person singular use, although further work is required to determine if these differences may be exploited to improve deception detection performance."
N13-1053,1,"indeed, future work may wish to jointly model sentiment and deception in order to better determine the effect each has on language use."
N13-1055,3,"we also plan to extract multiword corrections for other types of errors and to examine the usefulness of including error contexts in our confusion distributions (e.g., preposition confusions following verbs versus those following nouns)."
N13-1055,3,"in future work, we will examine whether the results we obtain for english generalize to other wikipedia languages."
N13-1058,3,this would help to ensure that no unintentional biases have skewed our results.
N13-1058,2,"in future explorations of this topic, we would like to expand the size of the biographical corpus and reaffirm its correctness through the use of cross-validation between multiple annotators."
N13-1058,1,"in addition, we would like to further investigate feature selection to find a best-case subset for performance on the bio corpus."
N13-1060,1,"in the future work, we will extend the pas reordering model to include useful context, e.g., the head words and the syntactic categories of arguments."
N13-1061,1,"for this area, there is some connection to the work of goldberg and elhadad (2010) and dickinson (2010), which are both concerned with examining dependency structures of more than one edge."
N13-1061,4,one main area for future work is the application of this work to parser evaluation as well as iaa.
N13-1061,1,"the connection is that those works are focused on dependency representations, and the kbm system does phrase structure analysis using a tag-like derivation tree, which strongly resembles a dependency tree (rambow and joshi, 1997)."
N13-1063,1,"in the future, we will research on task specific representations for sub-structures, such as phrases and sub-trees based on word embeddings and documents representations (xu et al., 2012)."
N13-1066,4,"In future work, we plan to extend our approach to other Arabic dialects."
N13-1067,4,"for future work, we plan to use the output of this research in several applications such as predicting future prominence of publications, studying the dynamics of research, and designing more accurate bibliometric measures."
N13-1072,1,"in the future we plan to address the related tasks of improving existing respellings, and assisting writers in creating respellings without direct access to the phonemic representations."
N13-1078,2,"more work is clearly needed to improve evaluation: some of our seeds could fall into multiple stylistic categories, so a more detailed annotation would be useful."
N13-1078,3,"beyond bayesian models, vector space and graphical approaches should be compared."
N13-1081,2,"it will also require the creation of new training corpora and related resources, temporarily threatening comparability."
N13-1081,5,one of the risks for the field in proceeding to investigations of how to deal with the question of met subjectivity is one familiar in natural language processing as a whole: there is a strong risk that these techniques will—initially and for a nontrivial quantity of time—cause the incremental performance gains in existing research to be lost or damaged.
N13-1082,5,"in the future, we plan to explore more factors to better understand deletion behavior and regret, including users’ recent posts, historical behavior, and other statistics related to their specific social network."
N13-1084,2,"in future work, we plan to use a development set to determine the optimal number of topical words to select during the topic estimate filtering stage of the pipeline in order to maintain improvements in precision without a loss in recall."
N13-1084,1,"we would also like to investigate using part-of-speech, word sense, and parse information to improve our approaches for both semantic expansion and topic estimation."
N13-1087,2,"as a future work, we will increase the size of the ood data and examine other methods like relative entropy based ood selection."
N13-1088,1,in the future we aim to perform an in-depth analysis of clue words that aid humans in sense disambiguation.
N13-1088,1,the distance of clue words from the target word and their and pattern of occurrence could give us significant insights into building a ‘discrimination net’.
N13-1093,2,"in future, we would like to exploit the scope of more negation cues, apart from the three cues that are used in this study."
N13-1094,1,future work will be aimed towards employing our hitting time based method in combination with a richer feature set.
N13-1098,6,future work may also attempt to address these differences directly.
N13-1098,1,"intentional wizard error could be introduced to frustrate the user into responding as she would to a less accurate system, analogous to intentional errors produced in user simulation in spoken dialogue systems (lee and eskenazi, 2012)."
N13-1100,1,"possible directions for future work include richer representations of discourse structure, and the combination of discourse-level and sentence-level valence and subjectivity shifters."
N13-1104,3,"in future work, we would like to further investigate frame induction evaluation, particularly in evaluating event clustering."
N13-1107,1,"for further work, we intend to improve open ie by tackling the conjunction and apposition structure problem."
N13-1107,1,another direction will be to extract relation words for implicit relations.
N13-1113,4,a clear direction for future research is the design of more fine-grained constraint taxonomies which can enable efficient usage of other constraint types and can result in further improvements in performance.
N13-1118,4,one of our key future research objectives is to investigate the use and adaptation of the created conceptual graph to perform metaphor interpretation.
N13-1118,4,"in addition, we plan to extend this work to cover nominal and adjectival metaphors, by harvesting salient nominal and adjectival features."
N13-1119,6,this would enable to find out advantages and disadvantages of our three variants with respect to an application.
N13-1119,4,"while we found it important to directly evaluate our lexical chaining algorithms on manually annotated data, a natural next step in this line of research is to use our lexical chaining methods as pre-processing steps for applications such as summarization, text segmentation or word sense disambiguation."
N13-1120,1,"third, it is intriguing to see that the directional similarity model based on the rnnlm vectors performs strongly, even though the rnnlm training process is not related to the task of relational similarity."
N13-1120,1,investigating how to select models to combine for each specific relation or relation group individually will be our next step for improving this work.
N13-1120,5,"second, because the labeling process of relational similarity comparisons is inherently noisy, it is unrealistic to request a system to correlate human judgments perfectly."
N13-1120,1,investigating the effects of different vector space models and proposing some theoretical justifications are certainly interesting research topics.
N13-1120,1,conducting some user study to estimate the performance ceiling in each relation category may help us focus on the weaknesses of the final system to enhance it.
N13-1120,1,"first, as shown in our experimental results, the model combination approach does not always outperform individual models."
N13-1120,3,"finally, we would like to evaluate the utility our approach in other applications, such as the sat analogy problems proposed by turney (2006) and question answering."
N13-1120,6,"In the future, we plan to pursue several research directions."
N13-1122,5,there are plenty of research problems left to be addressed.
N13-1122,1,developing a better algorithm for mining contextual words is an important research topic.
N13-1122,1,it would also be interesting to design a method that jointly learns ner models and entity linking models.
N13-1123,5,"we did not study how to summarize the discovered viewpoints in this work, which is also something we will look into in our future work."
N13-1123,1,"in our further work, we plan to consider more accurate methods using deeper linguistic analysis."
N13-1123,1,currently we use a simple heuristic-based classifier to predict interaction polarity.
N13-1123,6,we will consider these strategies in our future work.
N13-1125,5,"in the future, we intend to work toward resolving ecs to their antecedents when ec detection can be done with adequate accuracy."
N13-1125,3,"we also plan to test our approach on the penn (english) treebank, with the first step being converting the penn treebank to a dependency representation with the ecs preserved."
N13-1128,1,"we are also interested in discovery of less rigid dialogue goals, for example, a library patron who would be satisfied by an alternative book, and goals involving information aggregation where user utterances map to sophisticated queries."
N13-1128,5,"questions that arise from this work include how to extend focus discovery and vocabulary selection to numerical databases, how to extract strategies for goals other than tuple-selection from a database, and how to automatically infer intelligible table and attribute labels."
N13-1128,1,"explore more sophisticated models of user domain knowledge,"
N13-1128,4,"future work will also scale to mixed-initiative open dialogue management,"
N13-1128,1,we would like to investigate how optimal policies learned through reinforcement learning vary across domains.
N13-1129,1,"in future work, we will explore additional machine learning models that leverage richer training data, and investigate further the combination of explicit and predictive techniques."
N13-1130,2,"specifically, as future work, we plan to utilize the generated pseudo sense-tagged dataset to perform an in-depth study of different wsd paradigms."
N13-1130,1,We also plan to extend our work to other part-of-speech tags.
N13-1131,5,one major issue to be improved upon in future work is how named entities are handled.
N13-1131,5,"going forward, an issue that needs to be addressed with this method is its dependence on knowing the set of possible languages a priori."
N13-1131,5,"alternatively, the problem of language-independent named entity recognition has received some attention in the past (tjong kim sang and de meulder, 2003), and it may be beneficial to incorporate such a system in a robust word-level language identification system."
N13-1131,5,"because we don’t see an easy way to adapt this method to accurately label words in documents from a possible set of thousands of languages when the document itself may only contain two or three languages, we would propose the following future work."
N13-1131,1,we could simply choose not to evaluate a system on the named entity tokens in a document.
N13-1131,1,"a straightforward way to approach this may be to create another label for named entities, which (for the purposes of evaluation) would be considered not to belong to any of the languages in the document."
N13-1135,3,"we would like to explore other evaluation metrics (e.g., rouge-2, -su4, pyramid (nenkova et al., 2007)) and the human evaluation in future."
N13-1135,1,we will also explore better ways of integrating the sub-event detection and summarization approaches.
N13-1136,4,"we believe this research has applications to other areas of summarization such as update summarization and query based summarization, and we are interested in investigating these topics in future work."
N13-1137,1,"for example, modeling the conditional probability of generating reference for a property vn given the previously generated context p(vn|v1 ... vn−1) may bring us closer to human-like output."
N13-1137,1,we may also be able to build more sophisticated graphical models as larger corpora become available.
N13-1137,5,"there are several additional issues that do not arise in this evaluation, but we expect must be accounted for when referring to naturalistic objects in improves performance."
N13-1139,4,"for future work, we intend to apply the data set selection strategies to other nlp tasks, such as the optimal selection of sentences for tagging and parsing."
N13-1140,4,"in future work, we plan to apply these techniques to languages such as kinyarwanda, a resource-poor but morphologically rich language spoken in rwanda."
N13-2002,5,"at the same time, there are some unique challenges and opportunities that can be further investigated for our annotation scheme on unbalanced data."
N13-2002,5,another open question is how the degree of unbalance between classes in the corpora affects overall annotation quality.
N13-2002,1,"secondly, the features and machine learning algorithms used in semi-supervised annotation are also domain specific."
N13-2002,2,"we suggest that if the data is not unbalanced, the total amount of effort that can be reduced will be lower."
N13-2002,5,"for example, even though the cost matrix method can achieve a high recall for binary classification problem, whether it can be generalized to other tasks (e.g., multi-class classification tasks) is an unanswered question."
N13-2004,1,we are considering the corpus based approach in our future work for the message generation and a more automated approach in knowledge collection.
N13-2005,1,"for instance, since languages in the same family tend to share basic vocabulary, it may have some level of transfer to l2 that could be captured by a synonym-based classifier."
N13-2005,2,"for instance, using wide age ranges as the broader class for classifying age of anonymous authors, or personality prototypes for personality type identification."
N13-2005,4,"in addition, we can further explore the notion of increasing accuracy by applying knowledge of a broader class on the task applied in other stylometry based information extraction tasks."
N13-2005,1,"first, trying new feature sets that may capture other similarities between languages in the same family."
N13-2006,1,"instead of measuring similarity between the vectors directly using cosine, we will investigate the application of esa to calculate the similarities between short texts by taking their linguistic variations into account (aggarwal et al., 2012)."
N13-2007,3,"in the future, we plan to test our approach on other languages that have morphological characteristics similar to arabic."
N13-2012,3,"in our influencing experiments, we will attempt to influence a user to speak in a way that will optimize asr performance simply by changing the system’s own voice."
N13-2012,1,"we will build a framework for implementing the results of our studies of entrainment in human conversations into prediction models, which we hypothesize will improve their accuracy and can be used to improve a system’s performance."
N15-1001,1,"for our model, the reconstruction objective can be easily combined with the likelihood objective, yielding a potentially powerful semi-supervised method."
N15-1002,1,"future work should extend the problem formulation of predicate argument alignment to consider incremental linking: starting with a pair of documents, perform linking, and then continue to add in documents over time."
N15-1003,1,"in future work, we can investigate more sophisticated methods of vector clustering (such as expectation maximization and non-negative matrix factorization), interactions with verb and noun frequency, and interactions with number of word senses from a task-general knowledge-base such as wordnet."
N15-1003,1,"it would be especially useful to evaluate this system of a dataset of human judgements with verbs that systematically vary in polysemy, as this would more clearly expose the general trends we wish to model computationally."
N15-1004,1,"additionally, incorporating such constraints into an interpretable model allows for a deeper exploration of performance in the context of evaluation tasks."
N15-1006,1,"providing information about lexical category probabilities (auli and lopez, 2011) assigned by the super tagger can be useful during parsing."
N15-1006,3,"ultimately, we intend to evaluate the impact of our incremental parser extrinsically in terms of language modeling for smt or asr."
N15-1006,1,we would like to explore the limited use of a beam to handle lexical ambiguity by only keeping analyses derived from distinct lexical categories in the beam.
N15-1007,5,the question is now to establish whether will this be verified in other semantic data sets?
N15-1007,1,"from the parsing of deep syntax treebanks a la meaning text theory (ballesteros et al., 2014), to framenet semantic parsing (das et al., 2014) or data-driven approaches closer to ours (flanigan et al., 2014), it is difficult to know which models will predominate from this bubbling field and what kind of semantic data sets will benefit the most from syntax."
N15-1007,4,"from the parsing of deep syntax treebanks a la meaning text theory (ballesteros , 2014), to framenet semantic parsing (das , 2014) or data-driven approaches closer to ours (flanigan , 2014), it is difficult to know which models will predominate from this bubbling field and what kind of semantic data sets will benefit the most from syntax."
N15-1010,1,the fact that we identified significant differences in performance by selecting n-gram categories that are related to affixation in this poorly inflected language suggests that we may find even larger differences in performance in morphologically richer languages.
N15-1012,1,"for future work, we will study the incorporation of large-scale language models, and the integration of morphology generation and linearization."
N15-1013,3,"in particular, we are very interested to test our system’s performance on automatically generating a technical survey of a scientific paradigm, which thanks to the authors of (mohammad et al., 2009; qazvinian et al., 2013), has been established as a well-defined task with high-quality open data."
N15-1013,5,we also plan to apply our method to the problem of multi-document summarization.
N15-1013,3,"finally, while we have shown that our approach is effective in summarizing a scientific paper’s major contributions using its citation summary text, further experiments are required to test our method’s effectiveness on more generic summarization tasks and texts genres."
N15-1013,1,"in the future, we plan to extend our approach to higher order n-grams and see whether larger information units (phrases) would help boost summarization performance."
N15-1013,3,"finally, while we have shown that our approach is effective in summarising a scientific papers major contributions using its citation summary text, further experiments are required to test our methods effectiveness on more generic summarisation tasks and texts genres."
N15-1014,1,"exploring more advanced features that manage to detect abstract semantic relationships or discourse flows in the compressed article.• complementing our system with a separate translation model capable of transforming to “headlinese” the titles generated with the language used in the bodies of articles.• attempting to achieve a more objective evaluation of our generated headlines, through the use of semantic-level measures."
N15-1015,1,"additionally, we believe that combining visual and linguistic cues may help overcome longstanding challenges to language understanding, such as anaphora resolution and word sense disambiguation."
N15-1015,2,"in the future, we hope to use this labeled corpus to train visual action detectors, which can then be combined with the existing visual object detectors to interpret novel videos."
N15-1016,4,"their incremental nature makes them well-suited for cognitive simulations of grounded language acquisition, an avenue of research we plan to explore further."
N15-1018,4,we hope this work will help computer scientists and social scientists engage in deeper conversations about research reproducibility for large-scale computer-assisted text analysis.
N15-1019,1,"finally, additional research should quantify how responsive label regularization approaches are to the changing linguistic patterns common in online data."
N15-1019,1,most pressing is the need to directly address the sampling bias created when constraints derived from the overall population are applied to online users.
N15-1019,5,we plan to explore alternative optimization strategies to explicitly address this issue.
N15-1020,3,future progress in this area will also greatly benefit from thorough study of automated evaluation metrics.
N15-1020,1,we anticipate that there is much room for improvement if we employ more complex neural network models that take into account word order within the message and context utterances.
N15-1020,1,direct generation from neural network models is an interesting and potentially promising next step.
N15-1021,1,"in addition to improving this model, we are interested in developing systems that can select component words for portmanteaux and reconstruct component words from portmanteaux."
N15-1021,1,we also plan to research other applications for multi-input/output models.
N15-1022,2,future work involves developing text simplification techniques using the introduced datasets.
N15-1022,1,"in addition, we plan to improve our current alignment technique with better text preprocessing (e.g., coreference resolution (hajishirzi et al., 2013)), learning similarities, as well as phrase alignment techniques to obtain better partial matches."
N15-1024,1,future work should focus on additional methods for identifying relevant kb candidates given inaccurate transcriptions of mention strings.
N15-1026,5,"for future work, we plan to explore other strategies and constraints for noise reduction in the document graph."
N15-1028,3,"future work could compare dcca to other nonlinear approaches discussed in §5, compare different languages as multiview context, and extend to aligned phrase pairs, and to unaligned data."
N15-1030,1,we will further explore the feasibility of using neural networks to resolve empty categories: to link ecs to their antecedents.
N15-1031,1,"we leave for future work investigation of a version of the model that can ground language with raw(er) information from the world (e.g., vision information), eliminating the need to determine properties."
N15-1033,5,"can we improve results by considering all languages available (lardilleux and lepage, 2009)?"
N15-1033,5,"can we relax this constraint and use comparable data, or apply mscfgs to pivot translation?"
N15-1033,2,"second, we are currently performing alignment independently for each target."
N15-1034,1,"investigating the impact of richer feature sets, including a combination of phonotactic and morphological features, would be an excellent topic for future work."
N15-1034,4,it would be interesting to extend this approach to a wider range of phonological processes in addition to the word-final /t/ and /d/ deletion studied here.
N15-1034,1,"because this model enumerates the possible surface/underlying/context triples before beginning to search for potential surface and underlying words, its memory requirements would grow dramatically if the set of possible surface/underlying alternations were increased."
N15-1036,2,we believe that typology provides important clues for long-term language change.
N15-1036,2,"the currently available database only contains modern languages, but we expect that data of some ancestral languages could greatly facilitate computational approaches to diachronic linguistics."
N15-1038,1,"we free the neural network from its complex hmm infrastructure, which we view as the first step towards the next wave of advances in speech recognition and language understanding."
N15-1038,4,we hope the simplicity of our approach will facilitate future research in improving lvcsr with ctc-based systems and jointly training lvcsr systems for slu tasks.
N15-1040,1,in future work we plan to continue to perfect our parser via improved learning and decoding techniques.
N15-1043,5,"finally, an open question is whether it is possible to learn the latent domain alignment model in a fully unsupervised style."
N15-1043,1,"another interesting direction might be to integrate our model into advanced mixing multiple translation models, improving smt systems trained on the heterogeneous data (razmara et al., 2012; sennrich et al., 2013; carpuat et al., 2014)."
N15-1043,1,"one obvious direction for future work might be to integrate the model into fertility-based alignment models (brown et al., 1993), as well as other recently advanced alignment frameworks, e.g., (simion et al., 2013; tamura et al., 2014; chang et al., 2014)."
N15-1043,6,this challenge deserves more attention in future work.
N15-1045,1,"further, our results support the suggestion that in order for downstream applications to produce accurate results, in most cases it is necessary to take a broad view of the normalization task the looks beyond simple word replacements."
N15-1046,4,"first, we hope to expand summaries, similarity judgments, and systems to several topics beyond gay marriage."
N15-1046,4,"we believe, for example, that the features and the system we have trained for afs will apply to other domains without retraining, since none of the features are topic specific, but we have not shown that."
N15-1046,1,"in addition, we aim to develop additional features and improve on the results reported here."
N15-1046,1,"for example, we believe that it is possible that other off-the-shelf systems, such as for example one for sentence specificity (louis and nenkova, 2011; louis and nenkova, 2012), might possibly help with aspects of this task."
N15-1046,1,"in addition, in future, we aim to automatically identify central propositions without the mediation of human summarizers and evaluators."
N15-1046,1,"given the summaries that we have collected for each dialog, we plan to examine the relationship between the contributors to the related pyramid and the original source text, to determine whether indeed there are surface features of the source that would allow us to treat central proposition detection as an extractive task."
N15-1046,1,"In future work, we aim to expand on this work in several ways."
N15-1050,1,"this suggests that generating substitute vectors with better language models, such as neural language models, is a potential path to further improvements."
N15-1051,1,"our future work would focus on performing a similar task on bigrams consisting of adverb-adjective pairs (e.g., somewhat unclear < quite hard < very difficult) that exhibit properties of gradability."
N15-1052,5,"moreover, one oft-heralded advantage of the translation-as-annotation scheme (carpuat and wu, 2007) is that it naturally integrates into a machine translation framework, since one is learning to predict precisely what is necessary for successful translation; evaluating whether this hypothesis is true is currently an open question."
N15-1054,1,"in future work, we plan to use information from entity linked documents to improve performance and also explore active leaning, and other humanin-the-loop methods to get more training data."
N15-1058,1,"therefore we are exploring methods for performing discriminative optimization of weights assigned to views, for purposes of task-based customization of learned representations."
N15-1058,1,"we believe that the results could be improved by (1) either using recent methods for handling missing values mentioned in footnote 1 or by using the heuristic count dependent non-linear weighting mentioned by pennington et al.(2014) and that sits well within our framework as exemplified in expression 12 (2) by using even more views, which look at the future words as well as views that contain pmi values."
N15-1059,4,we also intend to use our approach on the task of multilingual word sense disambiguation.
N15-1059,4,"as future work we plan to integrate nasari into babelnet and apply our representation to a multilingual setting, enabling the comparison of pairs of concepts across languages."
N15-1060,3,"future work might usefully explore automated sentence quality estimation, as a component both of grammatical error correction systems and of their evaluation, in order to ameliorate the issue that any set of gold standard references will underspecify the set of possible corrections."
N15-1061,5,a promising next step is to look to techniques from speech retrieval for insights that might be applicable to the zero-resource setting.
N15-1061,1,one possibility in this regard is to explore extending the zero-resource term matching techniques to generate a lattice representation from which expected pseudo-term counts could be computed.
N15-1062,5,can we automatically identify a donor language (or its phonological properties) for a borrowed word?
N15-1062,5,"since languages may borrow from many sources, can jointly modeling this process lead to better performance?"
N15-1062,5,is it possible to monolingually identify borrowed words in a language?
N15-1062,5,can we reduce the amount of language specific engineering required to deploy our model?
N15-1062,5,can we integrate knowledge of borrowing in downstream nlp applications?
N15-1062,6,we intend to address these questions in future work.
N15-1063,1,"another promising direction is to encourage invertibility not only between words, but between their senses and synonyms."
N15-1063,1,"as future work, we plan to apply mir on largescale mt decipherment (ravi and knight, 2011; dou and knight, 2013), where, so far, only a single directional model has been used."
N15-1065,1,designing an overall model to harmonize such heterogeneous lexicons is an important issue.
N15-1065,1,(i) paraphrase lexicons created by different methods and sources have different properties.
N15-1065,4,"(iii) we will apply our method to various languages for demonstrating its applicability, extending it for a wider range of lexical variants depending on the targeted language."
N15-1065,5,"we are therefore interested in determining to what extent our paraphrase lexicons can improve the performance of application tasks such as machine translation, text summarization, and text simplification."
N15-1065,2,(ii) we aim to investigate an extensive collection of corpora: there are far more corpora than those we used in this experiment.
N15-1065,1,(iv) paraphrases are the fundamental linguistic phenomena that affect a wide range of nlp tasks.
N15-1067,1,"besides, taking into account the fast development of the word embedding research (mikolov et al., 2013; pennington et al., 2014), we will try different word embeddings."
N15-1067,1,our future work is to exploit other existing supervised parsers that fit our framework.
N15-1069,4,"by combining feature embeddings with metadata domain attributes, we can perform domain adaptation across a network of interrelated domains, distilling the domain-invariant essence of each feature to obtain more robust representations."
N15-1070,1,"our current approach assumes a fixed ontology, but we hope to explore a more bi-directional relationship between ontology and vsm in future work."
N15-1070,4,we also hope to extend our research to the multi-lingual domain.
N15-1070,2,we are particularly excited by the idea of using multi-lingual wordnets to learn sense specific semantic vectors that generalize across languages.
N15-1070,1,in particularly we envisage simultaneously incrementing ontologies with structure learning in addition to improving vsms.
N15-1070,1,we propose to use sense-specific vectors as features in downstream applications such a word sense disambiguation.
N15-1075,1,"in response to these observations, we would like to investigate more flexible representations, perhaps similar to those of botha and blunsom (2014), who use a linear combination of morpheme vectors to create representations that can generalize across words with similar forms."
N15-1077,2,incorporating reasoning and other sources of indirect supervision.
N15-1077,2,future directions include: pubmed-scale pathway extraction;
N15-1077,1,learning vector-space representations for complex states;
N15-1077,1,incorporating additional complex states to address syntax-semantics mismatch;
N15-1085,4,"in order to fully support users in complex problem-solving dialogues, the field must move toward richer grounding of natural language utterances within complex artifacts across many domains."
N15-1085,4,"additionally, generating specific and tailored dialogue feedback grounded in the artifact is a complementary area of research that holds the potential to increase the effectiveness of dialogue systems for supporting problem solving."
N15-1085,1,it is hoped that this line of investigation will lead to dialogue systems that smoothly support a much broader range of human endeavors.
N15-1086,5,"we consider all acquired relations equally salient, but future work will examine how to rank relation saliency."
N15-1086,4,"our method generalizes to other non-factoid qa tasks which could usefully employ relations, such as arithmetic word problems (hosseini et al., 2014) and biology reading comprehension questions (berant et al., 2014)."
N15-1086,1,"however, our collected corpus of real human-system dialogs can be used to improve our system in further iterations."
N15-1086,1,"we will also examine how dialog features can help distinguish between paraphrase, entailment, and negative relations."
N15-1087,3,"in future work, it would useful to compare the interannotator agreement between trained human annotators to determine an upper bound for the accuracy."
N15-1087,1,future work will look at adapting the standard algorithms to improve performance in the impaired case.
N15-1090,1,"for future work, we would like to maximize other performance measures, such as area under the curve, for information extraction models."
N15-1090,1,"furthermore, we would like to explore our approach for other latent variable models in nlp, such as those in machine translation."
N15-1091,4,"in the future, we plan to apply bi-cnn-mi to sentence matching, question answering and other tasks."
N15-1092,2,"beyond query classification and web search, we believe there are many other knowledge sources (e.g. sentiment, paraphrase) that can be incorporated either as classification or ranking tasks."
N15-1092,6,a comprehensive exploration will be pursued as future work.
N15-1093,4,"in the future, we would like to apply our method to noneuropean languages, with different morphological systems."
N15-1093,1,"we also plan to investigate methods of extracting morphological tags from a corpus, including differentiating syncretic forms in context."
N15-1094,1,"in future, instead of choosing λ, we plan to reduce λ as pep runs."
N15-1094,1,"this serves to gradually refine the approximations, yielding an anytime algorithm whose beliefs approach the bp beliefs."
N15-1094,1,we hope that the introduction of faster inference algorithms will increase the use of graphical models over strings.
N15-1094,1,"as nlp turns its attention to lower-resource languages and social media, it is important to model the rich phonological, morphological, and orthographic processes that interrelate words."
N15-1095,2,"in the future, we would like to further explore the idea of using interlingual representations for transliteration without parallel training data."
N15-1096,1,"in the future we plan to completely automatize the process, by employing segmental durations obtained with signal based methods for speech segmentation."
N15-1096,1,"finally, prosody was introduced here by way of a discrete symbol, forcing us to make a binary decision."
N15-1096,1,"a more integrated model would enable to associate prosodic break with a probability distribution, over acoustic features, thereby achieving the joint learning of segmentation and prosody."
N15-1097,5,future lines of research will explore the contributions that accounting for compatibility can make to these tasks.
N15-1100,1,"as future work, we plan to extend our approaches to obtain word embeddings for other semantic relations (gao et al., 2014)."
N15-1101,2,"these positive results for small datasets suggest vectors derived from slower but more accurate analysis of these resources may be practical for lexical semantic applications, and we hope by providing this result, future researchers may be more aware of the viability of smaller-scale resources like simple english wikipedia (or presumably wikipedia in other languages which are substantially smaller in size than english wikipedia), that can still produce high quality vectors despite a much smaller size."
N15-1102,4,"possible areas for future study include the use of discourse or and other contextual information to determine morphological agreement, application to other languages pairs/morphological agreement types, and learning the annotation rules from data."
N15-1104,1,"nevertheless, locating word vectors on a hypersphere opens a door to study the properties of the word embedding in a space that is yet less known to us."
N15-1109,1,"further technical improvements may be made by having the system automatically attempt to bootstrap the identification of spelling variants, a process that could complement our approach through an active learning setup."
N15-1111,1,"as future work, it might be interesting to explore a more sophisticated model where the regression models in different layers are trained simultaneously by back-propagating the error of the upper-layer, as in neural networks."
N15-1113,6,"finally, our long-term goal is to be able to generate loglines as well as movie plot summaries."
N15-1113,1,"in the future, we plan to explore model performance in a wider range of movie genres as well as its applicability to other nlp tasks (e.g., book summarization or event extraction)."
N15-1113,1,we would also like to automatically determine the compression rate which should presumably vary according to the movie’s length and content.
N15-1114,6,experiments show the approach to be promising and suggest directions for future research.
N15-1115,1,"specifically, we plan to incorporate world knowledge into the framework of discourse role matrix (lin et al., 2011; feng et al., 2014)."
N15-1115,1,"in our future work, we wish to explore the effect of our world knowledge in conjunction with discourse relations."
N15-1115,1,"in addition, we also plan to develop a more sophisticated feature encoding by distinguishing different types of predicates in world knowledge triples."
N15-1116,1,"since the performance of our resolver is limited in part by the errors made by sinocoreferencer's subsystems, we plan to mitigate this problem by performing joint inference for entity coreference, event extraction and event coreference in future work."
N15-1117,1,"systems should be able to distinguish who is likely to marry whom, identify the titles of books from roundabout descriptions, and intuit family relationships from raw text."
N15-1117,5,the next challenge is to incorporate the necessary world knowledge to solve these harder coreference problems.
N15-1118,1,"finally, we will investigate methods to automatically mine commonsense knowledge for injection into embeddings from additional resources such as probase (wu et al., 2012) or directly from text using a semantic parser (zettlemoyer and collins, 2005)."
N15-1119,5,"we have only applied a subset of amr representations to the el task, but we aim to explore how more amr knowledge can be used for other more challenging information extraction and knowledge base population tasks."
N15-1122,4,"we will further address the application of semi-supervised variants of the proposed techniques (e.g., self-training) to other domains, where no sizable corpora of temporally aligned data can be found."
N15-1122,1,"future work will address the extension of the feature set and model, and the application of this model to temporal semantics and planning tasks."
N15-1123,5,"in future work, we would like to address efficiency, e.g.by investigating the possibility of incorporating an inverted index into online applications of forced decoding."
N15-1125,5,does this multilingual leveraging help in a situation where we have large size corpora like europarl corpora? how much of an impact can treatment (morphological or syntactic) of the pivot language help in improving translation quality? can good reordering information be extracted by pivoting? can multi source and multi pivot setting further enhance quality? how can the noise induced by pivoting be controlled by methods other than probability cutoffs and finally? can simpler but more effective methods compared to triangulation be exploited in a multilingual scenario?
N15-1125,5,how can one choose a set of good pivot languages amongst available choices?
N15-1125,5,does this multilingual leveraging help in a situation where we have large size corpora like europarl corpora?c. how much of an impact can treatment (morphological or syntactic) of the pivot language help in improving translation quality?d. can good reordering information be extracted by pivoting?e. can multi source and multi pivot setting further enhance quality?f. how can the noise induced by pivoting be controlled by methods other than probability cutoffs?and finally g. can simpler but more effective methods compared to triangulation be exploited in a multilingual scenario?
N15-1126,5,"in the future, we will aim to explore how to generate a trigger list for a “surprise” new fact type within limited time."
N15-1129,2,"for future work, we plan to improve word alignment and translation quality in a more data restricted case where there are very weak source-pivot resources: for example, word alignment of malagasy english via french, using only a malagasy-french dictionary, or pashto-english via persian."
N15-1130,3,we also plan on testing with different feature vector combinations.
N15-1130,2,"in future, we plan to make a larger dataset of cognacy judgments for other language families in a richer phonetic transcription and integrate articulatory phonetic features into the feature vectors for the purpose of cognate identification."
N15-1131,1,"in the future work, the self-contained knowledge, such as those identified key-phrases, and the external knowledge-base should be integrated to guide topic modeling."
N15-1132,4,"also, we will explore this approach for other languages and for other parts-of-speech."
N15-1132,1,"in future, we plan to improve on the performance of our model for english, even for infrequent words."
N15-1134,5,we are currently exploring the possibility to use it in data-driven parsing.
N15-1136,1,we will study semantic text similarity to improve the sentence similarity matrix.
N15-1136,4,we will then apply the proposed method in query-based multi-document summarization.
N15-1137,3,we also intend to perform task-based evaluation of the manually checked versus automatically generated lexicons.
N15-1137,4,"we are continuing to improve the multilingual semantic taggers and extend them to cover more languages, such as spanish and dutch, aiming to develop a large-scale multilingual semantic annotation and analysis system."
N15-1139,1,"future work will focus on improving svo extraction, especially adding consideration for negations of predicate verbs."
N15-1139,1,"in addition we will analyze other hashtags in use in the trend and perform further analysis of the trend itself, implement advanced text normalization rather than relying on a dictionary, and determine the roles features from linked webpages and framenet or other semantic resources play in making sense of domestic abuse."
N15-1140,4,we are interested in the application of our embeddings to morphological tagging and other tasks.
N15-1140,1,future work will handle such integration of character-level features.
N15-1141,4,our observations suggest that there is an increasing need for such integration in various domains.
N15-1143,1,"· more accurately collecting physical objects, · sense disambiguation of words in clues, · use of superlative sentences, · filtering out descriptions of rare events, · a more effective way of using glosses, · application to other attributes, e.g., weight, · handling idioms."
N15-1144,4,it would be interesting to see if we can apply this approach to other tasks which require generative modeling of textual observations such as language modeling and grammar induction.
N15-1146,2,we believe that the corpus will be a valuable new resource for developing entity/event-level sentiment analysis systems to facilitate nlp applications such as automatic question answering.
N15-1149,1,"for example, a good fit for our task of response-based learning for smt might be bannard and callison-burch (2005)’s approach to paraphrasing via pivoting on smt phrase tables."
N15-1149,1,"in future work, we would like to investigate additional techniques for paraphrasing and synonym extension."
N15-1150,1,"alternatively, we may try different weighting systems depending on whether a token is from the same speaker as the current utterance or a different speaker, since it would generally make more sense for a particular speaker not to repeat him/herself."
N15-1150,1,an improvement is likely to come from attempting different methods to extract the core tokens from the past utterances.
N15-1150,1,"in future work, we hope to look at different types of network information for label propagation, more precise propagation methods to deal with non-local interactions, and also efficient ways of utilizing both textual and network information in a joint model."
N15-1154,3,"in future, we plan to do experiments in a cross domain setup and enhance our algorithm for domain adaptation paraphrase identification."
N15-1156,4,hence our method should be easily extended to german.
N15-1156,2,"other languages are also possible with language resources, in particular english."
N15-1158,1,"further work could look at improving the similarity metric used, applying these sampling techniques to other streaming problems or adding a mention compression component."
N15-1159,1,"in particular, (tang et al., 2014) showed that learning sentiment-specific word embeddings and using them as features can boost the accuracy of existing sentiment classifiers."
N15-1159,2,"in the future, we plan to experiment with constructing ml lexicons from larger twitter corpora also using hashtags."
N15-1160,1,nli is a young but rapidly growing field of research and this study is but a first step in shifting efforts towards a more interpretive approach to the task.
N15-1160,4,we hope that the new dataset and directions presented here will galvanize future work.
N15-1160,2,we believe this will motivate future work by equipping researchers with a large-scale corpus that is highly suitable for nli.
N15-1160,1,"the next phase of this research will focus on developing tools to derive and browse ranked lists of the most discriminative cross-corpus features, which will then be used to formulate sla hypotheses."
N15-1160,1,"subject to availability of data, this could be expanded to a multiple cross-corpus methodology, using three or more corpora."
N15-1160,4,its application to other languages besides english is also of interest.
N15-1161,5,"in addition, the availability of the automatically annotated disfluencies makes it possible to study the variation in rates for different cases and speakers over an extended time period."
N15-1161,1,"possible extensions of this work include exploring graph-based semi-supervised approaches (e.g., (subramanya et al., 2010)) and combining the text-based approach with flexible asr forced alignment allowing optional insertion of filled pauses and words that are common as repetitions."
N15-1163,1,"future work will focus on performance improvements, using the kernel on higher order parts, and integrating the kernel directly into a graph based dependency parser."
N15-1164,3,"furthermore, since we now evaluated extrinsically, it would be useful to devise intrinsic sense-based evaluation schemes, e.g. a sense analogy task similar to the word analogy task used by mikolov et al.(2013)."
N15-1164,5,"in future work, we plan to investigate whether the sense vectors are useful for retrieving rarely occurring senses in corpora."
N15-1165,1,"it would also be interesting to investigate the influence of the different semantic relations in wordnet, either by removing certain relations or by assigning different weights to them."
N15-1165,1,"in fact, we think that our technique opens up exciting opportunities to combine distributional and knowledge based word representations."
N15-1165,1,in the future we want to overcome this sparsity problem by combining both textual and kb based embeddings into a unified model.
N15-1167,1,"furthermore, we would expect that by applying supervised learning or combining with gazetteer-based method, and by extending the current method to recognizing other types of names in the texts, our system can work even better as an automation tool for such an annotation task."
N15-1169,1,"longer-term future work will pursue more sophisticated methods for taxonomy enrichment to improve the quality of integrated content and will aim to integrate additional dictionaries, with a special emphasis on adding domain-specific terminology."
N15-1169,1,immediate future work will add support for including new lemmas as synonyms in existing synsets and linking newly-created synsets with all appropriate types of wordnet semantic relationship.
N15-1171,2,the data set of annotations released with this paper may also prove a valuable resource for future analyses of framing.
N15-1171,3,similarly informative insights could be gained by comparing lay-persons’ annotations with framing experts’.
N15-1171,3,"future work should compare these results with similar analyses of texts containing more explicit framing, such as opinion columns, campaign speeches, or political advertisements."
N15-1171,1,"future work should examine more closely the role that structural features play, or perhaps do not play, in invoking framing."
N15-1171,5,differences in how framing is identified may give important clues to how framing operates in different contexts.
N15-1172,2,"as future work, we will investigate the impact of other phonetic devices such as assonance, consonance and rhythm on persuasiveness."
N15-1172,5,"it would also be interesting to focus on the connection between sound symbolism and persuasiveness, and investigate how the context or domain of persuasive statements interacts with the sounds in those statements."
N15-1173,1,"we will release our caffe-based implementation, as well as the model and generated sentences."
N15-1173,1,"however our approach falls short in better utilizing the temporal information in videos, which is a good direction for future work."
N15-1174,1,"in the future, we would like to develop better content selection models (e.g., identify surprising aspects in a scene) and more accurate grounding strategies (e.g., via discriminative alignment)."
N15-1176,4,"furthermore, we would like to apply redundant bit vectors to other nlp tasks."
N15-1176,1,"in future work, we would like to investigate more elaborate projection schemes that use contextual information from the source side or non-linear projections."
N15-1177,1,"experiments with discriminative joint tagging of mwes and super senses establish a strong baseline for future work, which may incorporate new features, richer models, and indirect forms of supervision (cf.grave et al., 2013; johannsen et al., 2014) for this task."
N15-1177,4,"we also expect future investigations will apply our tagger to a downstream task such as semantic parsing or machine translation (for further discussion of potential applications, see schneider, 2014, pp.179–189)."
N15-1178,1,"in future, we plan to augment our method with more complex levels of detail,"
N15-1178,1,"finally, using act predictions to bootstrap supervised learning could also yield improvements."
N15-1178,3,and evaluate act for more precise and detailed sentiments.
N15-1178,2,"gather more extensive datasets,"
N15-1180,1,we may also investigate new ways to re-rank nbest lists.
N15-1180,1,"language model scoring is a good start, but we may prefer vivid, concrete, or other types of words, or we may use text data associated with the user (papers, emails) for secure yet personalized password generation."
N15-1181,1,"we will also develop an incremental learning algorithm for joint category and feature learning (e.g., using sequential monte carlo methods such as particle filtering)."
N15-1181,4,"finally, the bcf model can be applied to tasks beyond those discussed here."
N15-1181,1,extending the model in a way that allows to learn qualitatively different types of features is desirable from a cognitive perspective.
N15-1181,1,"in addition, it would be interesting to investigate the emergence of feature types with nonparametric bayesian methods."
N15-1181,1,"an interesting direction for future work would be to learn feature types from multiple modalities (not only text) and to investigate how different information sources (e.g., visual or pragmatic input) influence feature learning."
N15-1181,1,"in addition to such descriptive features (e.g., behavior) categories also possess defining features (e.g., animate) which are bound to one particular value."
N15-1181,1,the bcf model learns descriptive feature types represented as a collection of feature values.
N15-1182,3,but future work should test this hypothesis across a wider variety of structures and contexts.
N15-1185,3,"future work should evaluate the capability of this approach to induce asymmetric signed networks, the utility of partial or distant supervision, and applications to non-fictional dialogues."
N15-1186,1,we acknowledge that certain languages exhibit phenomena (such as word-compounds in german) that require a more focused approach for solving them.
N16-1002,1,"we plan to continue working on feature design for insertion position choice, and in the future would like to consider using neural networks for learning these features."
N16-1002,1,"we believe that it is important to continue to explore approaches that exploit more general target-side syntax, faithful to the tree-to-tree translation paradigm."
N16-1003,1,"in addition, softer syntactic constraints that allow annotation of phrases with variables (chiang, 2007) such as “one of the preceding x” are another interesting avenue of future work."
N16-1003,1,"particularly, as the proposed method selected segments that took more time to translate due to technical terms, the combination with methods to harvest unknown words (daume iii and ja- ´ garlamudi, 2011) or optimize the selected segments based on the time required (sperber et al., 2014) is potentially useful."
N16-1005,1,"future work could aim to automatically predict it from the english source text based on textual features such as titles and names, or meta-textual information about the discourse participants."
N16-1009,1,in the future we wish to perform joint extraction and clustering instead of our current pipelined approach.
N16-1010,5,"we will also investigate whether the proposed approach benefits other informal text such as product reviews, social media discussions or spontaneous speech conversations, in which we expect the same sparsity issue occurs and the language expression is diverse."
N16-1010,2,"in the future, we may take advantage of the high quality student responses (luo and litman, 2016) and explore helpfulness-guided summarization (xiong and litman, 2014) to improve the summarization performance."
N16-1011,5,obtaining world knowledge or common-sense rules at high precision and scale continues to be the key nlp challenge in this area.
N16-1013,4,"our treatment of cdus in discourse annotations proposes a new distributional translation of those annotations into dependency graphs, which we think is promising for future work."
N16-1013,5,"this gives us good reason to believe that in subsequent work, we will be able to predict cdus and attack the problem of hierarchical discourse structure seriously."
N16-1014,1,we have focused on the algorithmic dimensions of the problem.
N16-1014,4,"since the challenge of producing interesting outputs also arises in other neural generation tasks, including image-description generation, question answering, and potentially any task where mutual correspondences must be modeled, the implications of this work extend well beyond conversational response generation."
N16-1015,2,"in our future work, we intend to focus on training the generator on the fly with real user feedback during conversation."
N16-1016,1,"as future work we plan to use a virtual agent system to collect a set of human-robot humorous interactions, and adapt our model to predict humor from them."
N16-1017,1,"one could explore more complex representations of talking points and discussion points, for instance using topic models or word embeddings."
N16-1017,1,we expect that improving our retrieval model will also improve the robustness of our idea flow analysis.
N16-1017,3,more explicitly comparing and contrasting monologic and interactive dynamics could lead to better models of conversations.
N16-1017,1,"furthermore, augmenting the flow of content in a conversation with the speakers’ linguistic choices could better capture their intentions."
N16-1017,1,a better model of discussion points could also provide more insight into the role of these points in persuading the audience.
N16-1017,1,"in addition, it would be interesting to study the interplay between our conversational flow features and relatively monologic features that consider the argumentative and rhetorical traits of each side separately."
N16-1017,1,"more explicitly comparing and contrasting monologic and interactive dynamics could lead to better models of conversations.for instance, by using a basic unigrambased definition of discussion points, we do not account for the context or semantic sense in which these points occur."
N16-1019,5,one way to address this is to treat alignment and grounding as a joint problem.
N16-1019,1,causality models for verbs can potentially provide top-down information to guide intermediate representations for visual processing and improve grounded language understanding.
N16-1019,5,"first, the current alignment between a video clip and a sentence is generated by some heuristics which are error-prone."
N16-1019,1,"second, our current visual features have not shown effective especially when they are extracted based on automatic visual processing."
N16-1019,1,"recent advances in object tracking algorithms (yang et al., 2013; milan et al., 2014) together with 3d sensing can be explored in the future to improve visual processing. thus another direction is to systematically study causality of verbs."
N16-1020,1,"multimodal embeddings are also likely to provide useful information for the models of metaphor translation, as they have already proved successful in bilingual lexicon induction more generally (kiela et al., 2015b)."
N16-1020,1,"in the future, it would be interesting to apply multimodal word and phrase embeddings to automatically interpret metaphorical language, e.g.by deriving literal or conventional paraphrases for metaphorical expressions (similarly to the task of shutova (2010))."
N16-1021,4,we also release a new multilingual image caption benchmark (mic benchmark) which will help in further research in this field5.
N16-1025,1,"we expect the general framework will be applicable to models using other types of neural networks such as feed-forward or lstm nets, and to shift-reduce parsers for constituent and dependency parsing."
N16-1026,1,"future work will explore using our parser to recover other representations from ccg, such as universal dependencies (mcdonald et al., 2013) or semantic roles."
N16-1026,1,"we will also explore new a∗ parsing algorithms that explicitly model the global parse structure using neural networks, while maintaining optimality guarantees."
N16-1026,1,the major obstacle is the mismatch between these representations and ccgbank—we will therefore investigate new techniques for obtaining other representations from ccg parses.
N16-1027,5,future work will investigate improving performance on rare categories.
N16-1028,5,we also would like to explore more subsequent tasks beyond sequence labeling problems.
N16-1028,1,"in the future, we can try to sequentially stack two crfs (one for word segmentation and one of subsequent task)."
N16-1029,1,we will also develop methods to make the tagger capable of active self-assessment to produce the best workflow within time bounds.
N16-1029,1,in the future we will exploit broader and deeper entity prior knowledge to improve name identification.
N16-1029,6,we will aim to make the framework more transparent for native speakers so the survey can be done in an automatic interactive question-answering fashion.
N16-1031,4,"in the future, we will experiment our approach on more nlp tasks such as dependency parsing and conference resolution where induced features should play a more critical role."
N16-1033,1,we are also interested in investigating the integration of more sophisticated event-event relation models of causality and temporal ordering.
N16-1033,1,"for future work, we plan to integrate entity and event coreference as additional components into the joint inference framework."
N16-1034,4,"in the future, we plan to apply this joint model on the event argument extraction task of the kbp evaluation as well as extend it to other joint tasks such as mention detection together with relation extraction etc."
N16-1037,1,"first, it would enable the model to scale up to the large datasets needed for competitive language modeling."
N16-1037,5,"future work will investigate the possibility of learning from partially-labeled training data, which would have at least two potential advantages."
N16-1037,2,"second, by training on more data, the resulting vector representations might support even more accurate discourse relation prediction."
N16-1040,2,"in the future, we will explore the uses of the idiom paraphrases in nlp applications such as machine translation and intelligent tutor for second-language learners."
N16-1042,4,"in future work, we would like to explore other ways to address the rare word problem in nmt-based gec, such as incorporating the soft-alignment information generated by the attention-based decoder, or using character-based models instead of word-based ones."
N16-1044,2,"for future works, we plan to apply the structured svm on top of other advanced models."
N16-1047,2,we plan to collect more data on which we can employ richer models that also take into account utterance sequence and dialogue features.
N16-1048,5,"indeed, it would be interesting in future research to explore which factors account for this within-form variation."
N16-1050,5,"in future work we plan to employ our bootstrapping solution in other regression problems, and to further explore potential uses of automatically learned psycholinguistic features."
N16-1051,1,we would also combine this work with more document representations methods as well.
N16-1051,1,further study will investigate the adaptive methods for constructing robust feature spaces.
N16-1052,1,"in future we plan to explore more sophisticated tagging and parsing models like deep neural networks (weiss et al., 2015), recurrent neural networks (dyer et al., 2015), and bi-directional lstms (lewis et al., 2016) for shift-reduce ccg parsing."
N16-1054,1,"in future work we plan to extend stranse to exploit relation path information in knowledge bases, in a manner similar to lin et al.(2015a), garciaduran et al.(2015a) or guu et al.(2015)."
N16-1054,1,thus it is a suitable candidate for serving as future baseline for more complex models in the link prediction task.
N16-1056,1,we believe that adding a probabilistic graphical model framework for structured output prediction would further improve the performance of our system.
N16-1056,3,this experiment remains as our future work.
N16-1058,3,"future work include the testing of this model in a linearization shared task (belz et al., 2011) and investigating the integration of large scale training data (zhang et al., 2012; liu and zhang, 2015)."
N16-1058,2,"future work include the testing of this model in a linearization shared task (belz , 2011) and investigating the integration of large scale training data (zhang , 2012; liu and zhang, 2015)."
N16-1059,4,applying rnns approach to word level qe and studying other ways to make quality vectors better are remained for the future study.
N16-1059,1,our experiments have showed that rnns approach is a meaningful step for qe research.
N16-1060,5,we are also interested in the representation of other word classes such as adverbs for which no evaluation set currently exists.
N16-1060,1,future work includes developing a model that successfully combines the various context types explored in this paper.
N16-1061,1,our future work includes designing a more robust solution that still works well when the number of known classes is small.
N16-1063,1,"our future work also includes setting the initialization weight in a more sophisticated way and combining the proposed method with other nn-based methods (kim, 2014; johnson and zhang, 2015)."
N16-1066,1,future work will focus on building useful dimensional sentiment applications based on the constructed resources.
N16-1067,1,future work should further develop automated methods for participant labelling.
N16-1070,1,"in particular, we plan to develop and evaluate models for idea flow and (dis)agreement, using more advanced features (e.g., from dependency relations and knowledge graphs)."
N16-1070,3,our full control over the game permits manipulation and intervention experiments that can further advance research on teamwork.
N16-1070,4,"in future work, we envision applying our framework to settings where teamwork takes place online, such as open-source software development, wikipedia editing, or massive open online courses."
N16-1070,1,"in future work, improved classifiers could lead to a system that can intervene in non-constructive discussions early on, steering them on track and preventing wasted time."
N16-1070,1,"further improving classification performance on such a difficult task will hinge on better conversation processing tools, adequate for the domain and robust to the informal language style."
N16-1072,1,an immediate future direction following our work is to improve the title candidate generation process so that it can handle the case where the corresponding titles only exist in the english wikipedia.
N16-1073,1,"our method offers search engine providers with a novel method to identify and analyze user task-behavior, and better support task decisions on their platforms."
N16-1073,1,"further, using an embedding based distancing scheme, we offer an improvement in empirical performance over prior clustering approaches that have used either a bag-of-words or tf-idf based solution."
N16-1073,1,"in future work, we intend to consider hierarchical extensions for extracting hierarchies of tasks subtasks."
N16-1074,4,we plan to apply this approach in the future to construct semantic parsers for more challenging tasks.
N16-1075,3,"in the future, we would like to assess the impact of secos in task-based settings as well as apply it to other compounding languages."
N16-1078,1,"in future work, we plan to adapt our methods for learning compounding morphology for languages such as greek, that have a special compound marker."
N16-1079,1,preliminary experiments indicate that the unigram word probabilities are a somewhat strong feature for pun recognition but further work is needed.
N16-1079,1,extending the edit model further is a fruitful area for future work but will likely require additional data. of interest for future work is joint recognition of the pun and its target.
N16-1081,1,in future work we would like to improve the query analysis performance of our algorithms.
N16-1081,3,"in addition, we plan to assess the contribution of query parsing to ir tasks such as document retrieval and query reformulation."
N16-1082,1,"our future work includes using results of the visualization be used to perform error analysis, and understanding strengths limitations of different neural models."
N16-1083,1,"in future, we aim to extend the approach to learn multilingual semantic spaces with more labels/classes."
N16-1084,3,"in this setting, we want to experiment with (i) ranking comments (instead of classifying them), (ii) exploiting the similarities between the new question and the questions in the database and also the relations between comments across different answer-threads."
N16-1084,4,"in the near future, we plan to apply the fccrf model to the full cqa task, i.e., finding good answers to newly-asked questions using previously asked questions and their answer threads."
N16-1085,4,"future work includes the application of the weak semi-crf model to other structured prediction problems, as well as performing investigations on handling other types of informal or noisy texts such as speech transcripts."
N16-1088,2,"an online version of our natural language interface to osm will be enabling for various interesting directions of future research: besides the possibility to gain new and more realistic data which we can use to extend the corpus, the semantic parser can be improved itself by response-based learning, where supervision signals can be extracted from the executed parses of new user queries against the database (kwiatowski et al.(2013), berant et al.(2013), goldwasser and roth (2013), inter alia)."
N16-1090,1,"in the future, we propose to explore other options, such as additional models to combine the contextual information from all siblings in the tree structure, and extending our model to structures beyond trees."
N16-1091,5,"in future work, we will explore the possibility of factoring all information present in an embedding into a dozen or so orthogonal subspaces."
N16-1093,1,"in our future work, instead of selecting aspects based on frequency, we will leverage domain knowledge to improve the selection."
N16-1095,2,we hope these findings will encourage further use of best–worst scaling in linguistic annotation.
N16-1096,2,future work will explore the use of our multilingual verb resource for relation extraction by reading natural language text in multiple languages.
N16-1100,5,"however for more complex optimization scenarios or for problems that require lengthy searches, parallelization might be needed to keep the computations required for optimization in line with what is needed to measure translation speed and quality."
N16-1101,2,"finally, an interesting future work is to use the proposed model to translate between a language pair not included in a set of training corpus."
N16-1101,1,"also, all the other techniques proposed recently, such as ensembling and large vocabulary tricks, need to be tried together with the proposed multilingual model to improve the translation quality even further."
N16-1101,1,more research on this phenomenon in the future will result in further improvements from using the proposed model.
N16-1102,2,"in future work we intend to investigate the model performance on larger-scale datasets, and incorporate further linguistic information, such as morphological representations."
N16-1103,1,we would also like to avoid the entity detection problem by using a deep architecture to both identify entity mentions and identify relations between them.
N16-1103,4,in future work we will apply this model to many more languages and domains besides newswire text.
N16-1106,4,"while we specifically treat lstm in this paper, it should be rather straightforward to adapt the proposed idea to other architectures of recurrent neural networks."
N16-1106,2,"this is an intriguing direction for us, as most nlp tasks lack training data, compared with speech recognition or image classification where neural models have achieved more significant successes."
N16-1107,4,"we intend to explore adaptation to other sts applications and with additional sts features (e.g., word and character ngram overlap) in future."
N16-1107,1,"unsupervised and semisupervised domain adaptation techniques that do not assume the availability of in-domain annotations or that learn effective domains splits (hu et al., 2014) provide another avenue for future research."
N16-1109,2,this work provides a proof of-concept that we hope will spur future work towards solving this important problem in a true low resource language.
N16-1110,1,"as future work, we plan to extend our feature sets to information theoretic aspects of character-level indicators, such as character n-grams frequencies and language models, encoding complexity and surprisal respectively."
N16-1111,1,"another future direction is to investigate cognitive inference (chernov, 2004), which is useful for semantic/syntactic prediction during interpretation (grissom ii et al., 2014; oda et al., 2015)."
N16-1112,1,"in future, we are planning to extend our word based lstm reordering model to phrase-based reordering model, in order to dissolve much more ambiguities and improve reordering accuracy."
N16-1112,1,"furthermore, we are also going to integrate our neural reordering model into neural machine translation systems."
N16-1113,3,"in future work, we plan to extend our work to different genres, languages and other kinds of dropped words to validate the robustness of our approach."
N16-1114,5,"future work will examine improving recall, and more sophisticated approaches to global training."
N16-1116,1,another one is to add more precise or even event-based features to improve the model’s performance.
N16-1116,1,one possible direction for future work is to differentiate more resolution modes.
N16-1118,1,further improvements can often be achieved by combining complementary word embeddings of different context types with the right dimensionality.
N16-1119,4,we argue that our method may be used to improve word embeddings of other language whose internal structure is similar to chinese.
N16-1120,3,we are also currently exploring the usefulness of incremental analysis for psycholinguistic data by switching off the lookahead feature.
N16-1120,2,"we can explore the usefulness of our system on other datasets like onestopenglish (ose) corpus (vajjala and meurers, 2016) or the dataset from xu et al.(2015)."
N16-1121,4,"in our future work, we intend to study how this training strategy behaves for other transition-based systems or, more generally, for other nlp scenarios using partially annotated data."
N16-1124,1,"this value represents a lower bound on what is possible with this technique and in future work we intend to study the introduction of additional features into the log-linear model to encourage or discourage the use of interlocking phrases during decoding, and investigate the effect of increasing the number of interlocked words."
N16-1125,2,"additionally we plan to improve the feature set to take into account phenomena such as early termination, i.e. when an evaluator makes a judgment before finishing reading a translation."
N16-1125,2,"in the future, we plan to extend our experiments to a large set of users and different language pairs."
N16-1125,2,we plan to deepen our analysis to determine what kind of information is being used beyond fluency and adequacy.
N16-1127,1,"we will explore less redundant, more compact representations of the two dimensions since some annotations can be factorized between the two dimensions (e.g."
N16-1127,1,mwes with irregular syntax) and some can easily be induced from others (e.g. sequential linking between lexical units).
N16-1128,4,"we intend to use scl-opp in the following applications: (1) to automatically create a large coverage sentiment lexicon of multi-word phrases and apply it in downstream applications such as sentence-level sentiment classification, and (2) to investigate how the human brain processes sentiment composition."
N16-1131,1,"in future work, our measure could be simplified by implementing the bias as a single scaling parameter."
N16-1133,1,"in future work, we will use the adaptive regularization of weight vectors (arow) algorithm (crammer et al., 2009) instead of the averaged perceptron."
N16-1133,4,"in addition, we will apply the pairwise approach to ranking (herbrich et al., 1999) used in information retrieval to rerank of grammatical error correction."
N16-1136,5,we hope that our framework will help facilitate comparison between results while allowing researchers to focus on targets such as all grammatical problems or all single equation problems.
N16-1136,1,"as current results show, designing general algorithms that can address different problem types while still being robust to the template or lexical variations has remained a challenge."
N16-1137,1,our future research will focus on joint modeling of cross-genre event extraction in the training stage through cross-genre knowledge enrichment.
N16-1138,1,"thus in future work we will explore ways of incorporating large amounts of raw text in training stance classification models, possibly using a neural network architecture."
N16-1138,4,"in future work we will develop methods for the other tasks involved, such as classifying the stance of a whole article towards a claim and truth assessment."
N16-1140,1,"we are interested in building an unsegmented, word-level language model that can provide meaningful estimates for morphological segments, which would improve scoring for out-of-context phrases and incomplete words."
N16-1140,1,"inspired by the disambiguating de segmentation system of el kholy and habash (2012), we would like to extend our system to propose multiple de segmentation candidates for each word, and allow the decoder to select the correct form using its other features."
N16-1142,5,"these could be addressed in future work by adapting existing methods for learning sense-specific representations for dense vectors (jauhar et al., 2015; ettinger et al., 2016; reisinger and mooney, 2010; guo et al., 2014; huang et al., 2012; neelakantan et al., 2015) to our sparse representations, and target cross-lingual textual entailment tasks, which focus on full sentences rather than isolated words."
N16-1142,4,"we also plan to study lexical entailment on more languages and example types, as well as investigate the usefulness of our bilingual representations in higher level multilingual applications such as machine translation."
N16-1142,1,"these could be addressed in future work by adapting existing methods for learning sense-specific representations for dense vectors (jauhar , 2015; ettinger , 2016; reisinger and mooney, 2010; guo , 2014; huang , 2012; neelakantan , 2015) to our sparse representations, and target cross-lingual textual entailment tasks, which focus on full sentences rather than isolated words."
N16-1143,1,to overcome this limitation we are currently designing a classifier based on conditional random fields.
N16-1144,2,future work concerns using cm fusion to  align and canonicalize multiple such knowledge bases to solve the knowledge fusion problem.
N16-1146,1,"in future work, we plan to use the inferred properties to improve affective polarity recognition in similes."
N16-1148,3,we will also conduct real-world experiments to see how this new imt framework works when human translators are actually involved.
N16-1148,1,"further improvement could be achieved by supporting other type of interactions, such as reordering operations, or building the system with stronger statistical models."
N16-1149,1,"our future work will investigate the model performance on a closely-related task, i.e., neural machine translation (sutskever et al., 2014; bahdanau et al., 2015)."
N16-1149,1,"furthermore, we will explore learning methods to combine utterances with and without the auxiliary side information."
N16-1151,1,these results are encouraging; they suggest that useful extensions of current methods are possible with two-phase embeddings.
N16-1152,5,"finally, it would be also very interesting to exploit structural kernels in the network layers."
N16-1152,1,"In the future, we would like to embed CNN similarity in CTKs."
N16-1154,1,"one possible direction for future research is in learning distributed document representations and the ranking simultaneously and applying more sophisticated recurrent models such as long short term memory (lstm) (hochreiter and schmidhuber, 1997) neural networks, that have been shown to be effective in similar tasks (wang and nyberg, 2015; zhou et al., 2015)."
N16-1156,4,"the effectiveness of our approach suggests its potential application to a broader range of nlp tasks that require word-level multilingual transfer, such as multilingual parsing and machine translation."
N16-1159,2,"in the future, we intend to continue creating more annotated code-mixed social media data."
N16-1159,5,we would also like to improve upon the challenging problem of normalization of monolingual social hindi sentences.
N16-1159,1,"also, we would further extend our pipeline and build a full parser which has aplenty applications in nlp."
N16-1161,4,"although we focus on phonology, our approach is general, and can be applied in problems that integrate divergent modalities, e.g., topic modeling, and multilingual tagging and parsing."
N16-1162,1,"while we have focused on models using naturally-occurring training data, in future work we will also consider supervised architectures (including convolutional, recursive and character-level models), potentially training them on multiple supervised tasks as an alternative way to induce the ’general knowledge’ needed to give language technology the elusive human touch."
N16-1163,3,"finally, it will be important to evaluate the effectiveness of the retrofitted word embeddings on extrinsic tasks that require disambiguating word meaning in context."
N16-1163,3,"based on these results, it would be interesting to evaluate further refinements of the sense graph: alignment-based senses could be clustered, or further filtered to reduce the impact of alignment noise; new edges could be added using other multilingual resources."
N16-1165,5,"besides a robust identification of argumentative segments, search engines will also need to decide which arguments are the most relevant to a given query— a very promising future research direction in the field of argumentation mining."
N16-1166,1,"in future work, we will aim to improve the accuracy of the extracted persuasive argumentation features by exploring other methods for identifying persuasive argumentations from text."
N16-1167,4,"also, we see that our adaption of kneser-ney smoothing to graphs may be useful for research in subgraph mining in general."
N16-1167,4,in future work we want to apply lcg to essay scoring as well.
N16-1168,2,"while the corpora used in this study cannot be published because of the lack of required irb, we are starting a user study project (zhang et al., 2016) on the application of our proposed techniques and will publish the data collected from this project."
N16-1168,2,"also, we plan to investigate whether revision dependencies exist in other types of corpora such as wikipedia revisions."
N16-1168,2,"in the future, we plan to investigate whether performance can be further improved when more sentences in the context are included."
N16-1169,1,we believe that more annotations and a learning algorithm that scores jointly all positive interpretations generated from each negation (as opposed to individually) would yield better results.
N16-1170,2,"a future direction would be to incorporate other resources such as the paraphrase database (ganitkevitch et al., 2013) into the learning process so that such prior knowledge can be utilized."
N16-1171,5,learning topics and sub-activities under workplace activities is a promising research direction which we will explore in future work.
N16-1173,2,"if successful, an adaptation using grammatical relations rather than semantic roles represents a promising possibility to create isrl annotation and isrl annotation tools for other languages, as universal dependencies are becoming increasingly available for major and low-resourced languages and can be projected to others."
N16-1181,4,"these observations are not limited to the question answering domain, and we expect that they can be applied similarly to tasks like instruction following, game playing, and language generation."
N18-1001,1,"for future work, we plan to jointly perform ner and entity linking for better cross-specialty media structural information extraction."
N18-1002,4,"considering type information is valuable in various nlp tasks, we can incorporate results produced by our fetc system to other tasks, such as relation extraction, to check our model’s effectiveness and help improve other tasks’ performance."
N18-1002,1,"in addition, tasks like relation extraction are complementary to the task of fetc and therefore may have potentials to be digged to help improve the performance of our system in return."
N18-1002,1,more work can be done to further develop hierarchical loss normalization since currently it’s very simple.
N18-1006,1,"ii) we plan to benefit from other types of information such as syntactic and semantic annotations to boost the decoder, as the table is not limited to morphological information alone and can preserve other sorts of information."
N18-1006,1,as our future work we follow three main ideas. we try to find more efficient ways to supply morphological information for both the encoder and decoder.
N18-1006 ,1,"iii) finally, we target sequence generation for fusional languages."
N18-1007,1,"we thus plan to investigate aligning the treebank and ms-state versions of switchboard for future work. however, it remains an open question as to whether dependency, constituency or other parsing frameworks are better suited to leveraging prosody."
N18-1008,1,"for future work, we aim to extend these methods to settings where we don’t necessarily have sentence triples, but where some audio is only transcribed and some audio is only translated."
N18-1009,4,"one additional interesting application of this work is to bring to the surface occasions where a speaker uses typical applause-seeking devices but does not receive applause (the “please clap” moments); we leave to future work identifying the reverse, when speakers receive applause without invoking common techniques (for example, to identify instances of claques paid to clap)."
N18-1010,5,we leave the question of how to transfer contextual information from the overall discussion as future work.
N18-1011,1,"in future work, integrating insights from theoretical linguistic approaches to focus and the notion of focus projection established there (cf., e.g., de kuthy and meurers 2012) could provide more guidance for ensuring contiguity of focus domains."
N18-1014,1,"since the stylistic data selection noticeably improved the diversity of our system’s outputs, we believe this is a method with future potential, which we intend to further explore."
N18-1015,1,"for future directions, we plan to further extend the proposed model for capturing other aspects of lyrics/melody discourse structure such as repetitions, verse-bridge-chorus structure, and topical coherence of discourse segment."
N18-1017,5,"in the future, we plan to extend our work through 1) investigating more sophisticated structures in the memory such as knowledge graph, 2) solving more complex questions, such as those involving deep reasoning over multiple facts."
N18-1019,1,"we expect syntactic information to further enhance the quality of the proposed substitutions, ensuring the functional similarity of the lexical substitutions to the target word."
N18-1019,1,we plan to extend our method in this direction in future work.
N18-1019,1,we will explore other ways for promoting high-quality substitutions without hurting the overall coverage of the system in the future.
N18-1019,3,"in future work, we plan to experiment with syntactic substitution models and with syntax-based word embeddings like the ones used in the initial addcos implementation (melamud et al., 2015)."
N18-1019,1,"furthermore, we intend to integrate lexical and syntactic simplification, both crucial steps towards text simplification."
N18-1026,1,"as future work, we would like to investigate supervised methods for cross-session task extraction."
N18-1033,5,"future work may investigate better use of already generated candidates since invoking generation for each batch slows down training by a large factor, e.g., mixing with fresh and older candidates inspired by mert (och, 2003)."
N18-1036,6,"first, we have ignored metainformation of the debate participants, such as their overall activity (i.e., whether they are spammers or trolls)."
N18-1036,3,"second, the proposed typology of ad hominem causes has not yet been post-verified empirically."
N18-1036,1,"third, we expect that personality traits of the participants (big5) may also play a significant role in the argumentative exchange."
N18-1037,5,"we also plan to investigate parsing scene graph with cyclic structures, as well as whether/how the image information can help boost parsing quality."
N18-1037,4,"in the future, we plan to tackle more computer vision tasks with this improved scene graph parsing technique in hand, such as image region grounding."
N18-1038,4,"it could well be that our methods are even more suited for more concrete tasks, such as visual question answering, visual storytelling, or image-grounded dialogue— an avenue worth exploring in future work."
N18-1038,1,"in addition, it would be interesting to explore multi-task learning for sentence representations where one of the tasks involves grounding."
N18-1039,2,"for instance, can these methods be successfully applied to datasets of real scenes?"
N18-1039,1,"since linguistic expressions of quantity are grounded on a non-symbolic system, we might expect that a model trained on one modality can be applied to another, at least to some extent."
N18-1039,1,"even further, jointly learning representations from both modalities might represent an even more natural, human-like way to learn and refer to quantities."
N18-1042,3,"in future work, we aim to investigate the effect of adding different contextual information, and we plan to test the resulting models in various applications."
N18-1043,4,"in future we plan to investigate the effectiveness of the joint representation on other nlp tasks like text classification, sentence completion challenge, evaluation of common sense stories etc."
N18-1043,4,the overall aim is to prepare a better generalized representation of words which can be used across languages in different nlp tasks.
N18-1047,2,"future work will test the versatility of our proposals, rntn-diag and rntn-comp, in other tasks that deal with data sets exhibiting carious structures."
N18-1048,1,we will also investigate more sophisticated non-linear functions.
N18-1048,1,"in future work, we plan to extend our approach to specialization for asymmetric relations such as hypernymy or meronymy (glava and ponzetto, 2017; nickel and kiela, 2017; vulic and mrki ′ c′, 2018)."
N18-1049,1,future work could focus on augmenting the model to exploit data with ordered sentences.
N18-1049,1,"furthermore, we would like to investigate the model’s ability to use pre-trained embeddings for downstream transfer learning tasks."
N18-1052,1,"in the future, we will explore the possibility of extending the current framework by detecting emotions at more fine-grained level, for example, emotions associated with specific events reported in text."
N18-1053,4,"in future, we would like to explore the application of proposed method in another aspect level sentiment analysis task known as aspect term extraction or opinion target extraction."
N18-1054,3,"we recommend evaluation with comparable dev and test set sizes for future work, as this enables more reliable evaluation."
N18-1059,1,"in future work, we plan to train our model directly from weak supervision, i.e., denotations, and to extract information not only from the web, but also from structured information sources such as web tables and kbs."
N18-1061,5,we also like to extend our work with the link to more behavioral study and analysis.
N18-1061,1,"acknowledging the possible limitations of this study including the quality of the sentiment classifier and a low recall of the present temporal orientation, in future, we will consider more sophisticated sentiment classifier for better performance and explore more linguistic insight into consideration to improve the performance of the temporal orientation classifier."
N18-1062,1,examining these models on similarity vs. relatedness using our proposed measures will be left for the future.
N18-1069,1,"in future work, we shall enhance this methodology of phrasal axiom injection to predict unseen paraphrases."
N18-1070,1,"in future work, we plan to extend the inference component to select an optimal set of explanations for each prediction, and to explain the model as a whole, not only at the instance level."
N18-1071,4,"in the future, we would like to investigate the effectiveness of sgtb on other information extraction tasks, such as relation extraction and coreference resolution."
N18-1074,5,we believe that fever will provide a stimulating challenge for claim extraction and verification systems.
N18-1075,5,joint embedding with global statistics remains an open problem.
N18-1075,2,another interesting venue for future research is to construct much larger-scale distant supervision datasets to train general-purpose textual relation embedding that can help a wide range of downstream relational tasks such as question answering and textual entailment.
N18-1076,4,"we also plan to investigate how the extracted implicit arguments can be integrated into a downstream task that makes use of event information, in particular we would like to experiment with reading comprehension."
N18-1076,1,we currently use a relatively simple model for local narrative coherence; in the future we will turn to models that can test global coherence for an implicit argument candidate.
N18-1079,1,"in future work, it would be interesting to learn global dependencies between the output labels for such a hypergraph structure and training the model globally."
N18-1079,1,"we can also experiment with different representations such as the one in finkel and manning (2009) and use the recent advances in neural network approaches (vinyals et al., 2015) to learn the constituency parse tree efficiently."
N18-1080,4,"our model also lends itself to other pairwise scoring tasks such as hypernym prediction, co-reference resolution, and entity resolution."
N18-1080,1,"however, this could be ameliorated by integrating our model into open relation extraction architectures such as universal schema (riedel et al., 2013; verga et al., 2016b)."
N18-1081,1,"for example, open ie would directly benefit from an automatic qa-srl extractor, while a more exhaustive or extensive annotation of qamr would improve open ie’s performance on a wider range of predicates."
N18-1082,4,"we expect our vector representations of prepositions to be widely used in more complicated downstream nlp tasks where prepositional role is crucial, including “text to programs” (guu et al., 2017)."
N18-1085,1,"another possible extension would be to allow each step of q to propose a sequence of actions, effectively making the tag set size ∞."
N18-1085,1,"for example, we can learn the generative model and proposal distribution jointly; we can also infuse them with hand-crafted structure, or use more deeply stacked architectures; and we can try training the proposal distribution end to-end (footnote 10)."
N18-1087,2,we also plan to explore knowledge transfer and adaptation models for more dialects with limited resources.
N18-1087,1,"Future directions include exploring additional deep learning architectures for morphological modeling and disambiguation, especially joint and multitasking architectures."
N18-1092,4,we are currently expanding the notion of distributional context to multiple auxiliary foreign languages at once.
N18-1092,1,another direction worth exploring is to extend the model’s hierarchy with respect to how parallel sentences are generated.
N18-1092,1,"for example, modelling sentence level latent variables may capture global constraints and expose additional correlations to the model."
N18-1093,1,"besides, we plan to study how to leverage other information to facilitate the training of word embeddings under the low-resource setting."
N18-1093,2,"in the future, we would like to conduct experiments on other languages where available text corpora are relatively hard to obtain."
N18-1093,5,"we are also interested in applying the proposed approach to domains, such as legal documents and clinical notes, where the amount of accessible data is small."
N18-1094,5,"furthermore, it would be interesting to study how people’s prior beliefs affect their other activities on the website and the language they use while interacting with people with the same and different prior beliefs."
N18-1094,5,"finally, one could also try to understand in what aspects and how the language people with different prior beliefs/backgrounds use is different."
N18-1097,3,"however, we expect that for tasks where individual words are not predictive, the current evaluation methods and local explanation approaches may not be sufficient."
N18-1098,5,"in future work, we foresee to investigate learning dynamics in variable number of topics over time."
N18-1098,5,it would also be an interesting direction to investigate the effect of the skewness in the distribution of papers over all years.
N18-1098,4,"further, we see a potential application of the proposed model in learning the time-aware i.e. dynamic word embeddings (aitchison, 2001; basile et al., 2014; bamler and mandt, 2017; rudolph and blei, 2018; yao et al., 2018) in order to capture language evolution over time, instead of document topics."
N18-1099,5,"for example, human judgment is more difficult to measure than in monolingual settings, and it is still an open question on how to design a reliable and accurate survey for multilingual quality judgments."
N18-1099,3,"as the first study on evaluation for multilingual topic models, there is still room for improvement and further applications."
N18-1100,1,"from the linguistic side, we plan to integrate the document structure of discharge summaries in mimic-iii, and to better handle non-standard writing and other sources of out-of-vocabulary tokens."
N18-1100,1,"from the application perspective, we plan to build models that leverage hierarchy of icd codes (choi et al., 2016), and to attempt the more difficult task of predicting diagnosis and treatment codes for future visits from discharge summaries."
N18-1102,1,our model bears not only word pair representations but also dependency path representations as context vectors.
N18-1102,1,"in future work, we will explore unsupervised learning with a neural path encoder."
N18-1102,4,"thus, we intend to apply these representations to various tasks, which path representations contribute to."
N18-1103,1,"we will also extend the model to reason over words unseen in input lexical resources, similar to the recent post-specialization model oriented towards specialization of unseen words for similarity (vulic et al.′ , 2018)."
N18-1103,4,we also plan to test the usefulness of le specialized vectors in downstream natural language understanding tasks.
N18-1103,4,"in future work, we plan to apply a similar methodology to other asymmetric relations (e.g., meronymy), as well as to investigate finegrained models which can account for differing path lengths from the wordnet hierarchy."
N18-1106,4,simple algorithms for lexical and structural alignment establish baselines for the new alignment task; we expect statistical models will be brought to bear on this task in future work.
N18-1107,1,we will explore more applications of our tag parsers in future work.
N18-1108,3,"on the other, we would like to expand our empirical investigation by focusing on other long-distance phenomena, such as overt case assignment (blake, 2001) or parasitic gap licensing (culicover and postal, 2001)."
N18-1108,1,"in future work, we would like to better understand what kind of syntactic information rnns are encoding, and how."
N18-1109,4,"future work includes generalizing our method to non-nlp problems, as well as applying the task-clustering idea to other few-shot learning frameworks (ravi and larochelle, 2017; finn et al., 2017; mishra et al., 2017; cheng et al., 2017)."
N18-1112,2,we also intend to extend the reach of our approach to cross-lingual setups.
N18-1112,1,in future we intend to extend pblm so that it could deal with nlp tasks that require the prediction of a linguistic structure.
N18-1112,1,"for example, we believe that pblm can be smoothly integrated with recent lstm-based parsers (e.g.(dyer et al., 2015; kiperwasser and goldberg, 2016; dozat and manning, 2017))."
N18-1113,1,we also plan to extend our method to multisource classification cases and utilize the multiagent communication environment to boost the classification performance.
N18-1113,1,"for future studies, we will investigate the data selection policies of other semi-supervised methods and try to learn these policies automatically."
N18-1115,1,"in the future, we plan to investigate the effectiveness of carnn units in other sequence modelling tasks."
N18-1117,1,"for the future work, we will combine dense connections with other deep architectures, such as rnns (wu et al., 2016) and self attention networks (vaswani et al., 2017)."
N18-1119,6,we imagine that there are further optimizations in reach that could improve this even further.
N18-1121,6,"we hope that this paper will spark discussion on the topic, and future work will propose even more focused architectures."
N18-1122,1,"in the future, we would like to try multi-adversarial framework which consists of multi discriminators and generators for gan."
N18-1123,1,"for future work, we would like to investigate architectures which allow automatic parameter tying among the tasks (ruder et al., 2017)."
N18-1124,1,"moreover, we will incorporate relative positional information to the attention function."
N18-1124,1,"as future work, we plan to enrich the present attention mechanism with the key-value-prediction technique (daniluk et al., 2016; miller et al., 2016) which was shown to be useful for language modeling."
N18-1125,1,"in future work, it is promising to exploit other target foresight information such as word cluster besides the pos tags in this paper, and it is also interesting to apply this idea on top of other attention models such as the local attention in luong et al.(2015b)."
N18-1134,3,"it would be interesting to evaluate the multimodal architecture on other predicate-argument frameworks, e.g., script knowledge or verbnet style semantic role labeling. more precisely, future work should consider using implicit knowledge not only from images of the participants of the situation, but also from the entire scene in order to directly capture relations between the participants."
N18-1134,1,"this could provide access to a more holistic understanding of the scene. regarding the combination of embeddings from different modalities, we suggest to experiment with different fusion strategies complementing the middle fusion (concatenation) and the mapping (imagined method)."
N18-1136,5,these findings open several avenues for future work: how can we improve divergence detection further?
N18-1136,1,"how do divergent examples impact other applications, including cross-lingual nlp applications and semantic models induced from parallel corpora, as well as tools for human translators and second language learners?"
N18-1136,5,can we characterize the nature of the divergences beyond binary predictions?
N18-1137,1,"it would also make sense for the model to decode hierarchically, taking sequences of words and sentences into account (zhang and lapata, 2014; lebret et al., 2015)."
N18-1137,1,"in the future, it would be interesting to investigate a more sophisticated representation of the input (vinyals et al., 2016)."
N18-1139,1,"given the multilingual nature of the new datasets, as future work, we would like to build models which can jointly learn to generate natural language descriptions from structured data in multiple languages."
N18-1139,6,one idea is to replace the concepts in the input infobox by wikidata concept ids which are language agnostic.
N18-1140,1,"one direction for future research is improving the reading models on the queries that are currently the most challenging, i.e. those requiring world and background domain knowledge."
N18-1140,1,better representing background knowledge by inducing embeddings for entities or otherwise integrating ontological knowledge is in our opinion a promising avenue for future research.
N18-1141,1,another potential direction is to jointly learn the collaboration detector together with the qa and qg models.
N18-1141,5,how to conduct joint inference is an interesting future work. improving the diversity of the samples requires different sampling mechanisms.
N18-1143,1,"in addition, since the original data format of the toefl listening comprehension test is audio instead of text, it is worth trying to initialize the embedding layer of the qacnn with semantic or acoustic word embeddings learned directly from speech (chung and glass, 2018, 2017; chung et al., 2016) instead of those learned from text (mikolov et al., 2013; pennington et al., 2014)."
N18-1143,2,one area of future research will be generalizing the transfer learning results presented in this paper to other qa models and datasets.
N18-1145,4,"besides, we are interested to apply the method of combing topic model and deep learning into some traditional nlp tasks."
N18-1145,3,"in the future, we plan to evaluate more complex networks for recommendation tasks under the framework proposed by ltmf."
N18-1146,1,"this is a promising new direction for nlp research, one that we hope will help computational (and non-computational!)social scientists better interpret linguistic variables and their relation to outcomes."
N18-1146,1,"this includes algorithmic innovation, theoretical bounds for performance, and investigating rich social questions with these powerful new techniques."
N18-1149,3,"more importantly, we hope that other researchers will identify novel opportunities which we have not explored to analyze the peer reviews in this dataset."
N18-1149,5,"as a concrete example, it would be interesting to study if the accept/reject decisions reflect author demographic biases (e.g., nationality)."
N18-1152,1,"furthermore, we want to investigate whether we can reduce the number of required preferences with smarter sampling methods."
N18-1152,2,"in future work, we would like to investigate whether we can use crowd-sourcing platforms to collect pairwise preferences on a large scale."
N18-1154,1,"in the future, we intend to explore more about gbn’s applications as well as its provable computational and statistical guarantees."
N18-1156,1,"in future work, we will explore the extension of our proposed model to cater for varying number of storylines automatically and also better deal with intermittent storylines."
N18-1157,1,"the techniques studied in this field may be useful to develop better algorithms for stkp, which we leave for future work."
N18-1158,1,"in the future we would like to focus on smaller discourse units (mann and thompson, 1988) rather than individual sentences, modeling compression and extraction jointly."
N18-1159,1,"because of this heterogeneity, we argue that future attempts to summarize relationships will likely require a diversity of models and techniques."
N18-1160,1,"in the future, we would like to investigate how attribute-specific features can improve performance compared to our more general feature set which is invariant for each sentence type."
N18-1160,1,it would also be possible to equip the model with a hierarchical decoder which generates a document instead of individual sentences.
N18-1160,2,"finally, we would like to examine whether the content analysis presented here can extend to different types of fiction such as novels or short stories."
N18-1161,1,"instead of learning to predict rouge recall scores, the regress and can simply be exchanged and the rouge precision can be used instead."
N18-1161,6,"for future works, it is very simple to incorporate the findings presented in this paper."
N18-1163,4,"we also plan to extend the work to detect egregious conversations in real time (e.g., for escalating to a human operators), and create log analysis tools to analyze the root causes of egregious conversations and suggest possible remedies."
N18-1163,2,"in this context, future work includes collecting more data and using neural approaches (e.g., rnn, cnn) for analysis, validating our models on a range of domains beyond the two explored here."
N18-1166,5,it is worth conceiving a more general solution to improve the limitations of torder in the future work.
N18-1167,1,we also plan to explore entity embeddings obtained using other graph densifying methods.
N18-1167,1,"in future, we plan to enhance elden using el of pseudo entities to estimate entity prior of entities not present in kg."
N18-1168,1,"in the future: 1) more advanced technologies like reinforcement learning (sutton and barto, 1998) can be introduced to generate latent fact details such as the time of theft more accurately;"
N18-1168,5,"2) in this work, we only generate rationales in court views omitting charge prediction, it is interesting to see whether jointly generating the two parts will benefit both of the tasks;"
N18-1168 ,3,3) studying verification mechanism is meaningful to judge whether generated court views can really be adopted which is important for court-view-gen in practice;
N18-1168 ,2,4) more complex cases with multiple charges and multiple defendants will be considered in the future.
N18-1171,1,"future work involves leveraging raw human annotations to improve sentiment analysis classifiers, and finding ways to better detect and understand the “complicated” property in these samples that cause high annotator disagreement."
N18-1171,4,"additionally, we encourage researchers to use mtsa in the development of other methods for short text sentiment analysis, including unsupervised, lexicon-based, and rule based methods."
N18-1173,4,"since this contribution was restricted to the vad format of emotion representation, in future work we will examine whether mtl yields similar gains for other representational schemes, as well."
N18-1174,2,"in future work, obtaining more human annotations will be useful to build a better human needs categorization system."
N18-1174,5,"in addition, applying and analyzing the human needs of affective events in narrative stories and conversations is a fruitful and interesting direction for future research."
N18-1175,6,"in the short run, we plan to draw more attention to this topic by running a semeval 2018 shared task.12"
N18-1176,3,"finally, we plan to conduct an empirical analysis of deception behavior across personality types."
N18-1176,2,"in future work, we plan to conduct similar analysis in additional deception corpora in other domains, in order to identify consistent domain-independent deception indicators."
N18-1176,3,"in addition, we plan to conduct cross-corpus machine learning experiments, to evaluate the robustness of these and other feature sets in deception detection."
N18-1176,1,"we also would like to explore additional feature combinations, such as adding acoustic-prosodic features."
N18-1178,1,"there are many possible extensions to this work, including: (a) learning multilingual word embeddings with domain information; and (b) modeling other policy related scores from text, such as “support for eu integration”."
N18-1179,2,fully uncovering these factors in current nli datasets is a pre-requisite for the construction of more effective resources in the future.
N18-1179,5,further work is required to understand what these interactions are and how they contribute to performance.
N18-1180,3,this will enable developing novel approaches to language proficiency assessment which will integrate task based performance with real time monitoring of cognitive and linguistic processing.
N18-1180,1,"in future work, we plan to extend the analysis of the validity and consistency of our approach, and further explore its applications for language proficiency evaluation."
N18-1180,6,we will further study the consistency of our scores for repeated tests by the same participants.
N18-1181,2,ultimately we plan to complement our corpus analysis with real-time language production experiments to more thoroughly test hypotheses about speaker choice.
N18-1182,1,"our work can be extended by: (a) utilizing several predictions for each training instance, (b) investigating the extent to which a more sophisticated and effective downstream learner can affect the performance of different spotters, (c) developing models to better distinguish hard genuine instances from spurious ones, and (d) developing ranking algorithms to improve the performance of models on real-world datasets."
N18-1185,2,"for the future work, we will expand the annotation for more entity types and automatically link mentions with respect to their entities using an entity linker."
N18-1186,1,applying to group chatting with more than two speakers and reasoning over emotion embeddings or knowledge vectors included from an external knowledge base/graph are taken as our future directions.
N18-1188,5,"besides, injecting external information during response’s generation would be another challenging work."
N18-1188,1,"in the future studies, we would explore the possibility of promoting diversity on the learning procedure, by directly optimizing diversity loss in the cost function."
N18-1189,1,"In the future, we plan to investigate three limitations of our current model."
N18-1192,1,"and, we will research methods to accelerate the training and reduce the memory consumption during taring."
N18-1192,1,"in the future, we will study how to improve the performance of the bllm model."
N18-1198,1,"future work includes (i) investigating how object category information can be better used or expanded to improve ic; (ii) analyzing end to-end ic systems by using interpretable representations that rely on other explicit detectors (e.g. actions, scenes, attributes)."
N18-1199,3,"our work suggests the following future directions: evaluating algorithms: because concreteness scores are able to predict performance prior to training, evaluations could be reported over concrete and abstract instances separately, as opposed to aggregating into a single performance metric. designing datasets: when constructing a new multimodal dataset, or augmenting an existing one, concreteness scores can offer insights regarding how resources should be allocated. curriculum learning: during training, instances could be up/down-weighted in the training process in accordance with concreteness scores."
N18-1201,5,"another issue we plan to explore is using textual explanations (park et al., 2016) for vqa."
N18-1201,4,"finally, we hope to apply our approach to additional problems beyond vqa."
N18-1201,5,"therefore, in the future, we would like to focus on explaining the results of an ensemble."
N18-2002,6,"we hope that by drawing attention to this issue, future systems will be designed in ways that mitigate gender-based overgeneralization."
N18-2006,3,"for example, it would be interesting to investigate whether standard low-level tasks, such as pos tagging or chunking, are effective for am."
N18-2007,1,"noise in the coreference annotations has a detrimental effect on the performance (figure 3), hence we also aim to explore joint models which learn to do coreference resolution and reading together."
N18-2007,4,"in future work, we aim to apply this model to other problems where long term dependencies at the document level are important."
N18-2012,4,"we also plan to investigate how this method transfers to related tasks, such as evaluating open-domain dialogue responses, e.g.(lowe et al., 2017)."
N18-2015,6,future investigation will be needed to determine whether this is a property inherent to human storytelling or a form of bias introduced during data collection.
N18-2018,3,"in future work, we would conduct more rigorous comparative evaluation with human humor recognition and look into how the humorous texts can be generated using deep learning models as well."
N18-2020,3,future work will conduct user studies to assess the relative importance of different evaluation criteria.
N18-2020,1,a better understanding of how these interact may lead to improved semantic evaluation that will alleviate the need for a high number of references.
N18-2021,4,"in the future, we wish to explore the effectiveness of r-spen on various tasks using domain knowledge functions with varying degrees of complexity."
N18-2022,5,"future work may also compare the catalonian situation with multilingual societies in which a minority language is discouraged (karabakh, 2013), or in which the languages are more equally distributed (blommaert, 2011)."
N18-2023,1,"as future work, amr-covington produces sequences of no-arcs which could be shortened by using non-local transitions (qi and manning, 2017; fernandez-gonz ′ alez and g ′ omez- ′ rodr′?guez, 2017)."
N18-2025,1,the results shed light on future work on language-independent paraphrase identification and multilingual paraphrase acquisition where pretrained word embeddings on large corpora are not readily available in many languages.
N18-2029,1,"as future work, we plan to experiment with more advanced neural architectures and finer-grained relations."
N18-2029,2,we also intend to port the model to more languages.
N18-2030,5,"in the future, we would like to work on bli of multiword translations and compound words."
N18-2034,1,the most natural next step would be to study similar extensions of other representation-learning models.
N18-2037,3,"future work includes investigation of the accuracy of these methods for different clinical conditions, and languages."
N18-2039,5,"is a one-hit accuracy sufficiently informing on success in word analogy, or do we need a softer measure from for example the ranking world?"
N18-2040,2,"in addition, we perform a detailed comparative analysis of the transition distributions for english and chinese as well as errors in chinese amr parsing that we hope will inform future chinese amr parsing research."
N18-2044,5,"in future, we plan to address the implicit and sarcastic medical sentiments that account to the majority of the errors."
N18-2046,3,future work would require a human evaluation effort to draw more conclusions.
N18-2051,1,"for future work, we want to investigate the impact of using pre-trained weights to initialize the lstm cells in the seq2seq model from a language modelling task, as well the grammatical diversity of generated paraphrases."
N18-2053,4,we also plan to extend convkb for a new application where we could formulate data in the form of triples.
N18-2056,5,we also plan to extend the knowledge base cluster to include related entities.
N18-2056,5,"furthermore, runtime and computational complexity of the system should be studied."
N18-2057,1,"as part of future investigation, we will experiment with other types of encoders such as convolutional and recurrent networks."
N18-2057,2,"furthermore, we aim to scale this approach to larger datasets."
N18-2063,3,we intend to infer cognate sets from the combined system and use them to infer phylogenies and evaluate the inferred phylogenies against the gold standard trees.
N18-2063,1,"as future work, we intend to create a cognate identification system that combines the output of different algorithms in a more systematic way."
N18-2064,3,"in future work, we would like to evaluate the impact of annotation conventions on other kind of parsers and to find the properties of a dependency tree that facilitate its prediction."
N18-2064,2,we also plan to find ways to easily annotate sentences with multiple references (e.g.by indicating that the head of word can be chosen arbitrarily) and eliminate the constraint that references should be trees.
N18-2068,1,"we believe that with these extensions our variability measures will offer a unified framework for describing variability profiles of mwes, which should be useful both in theoretical and applied research."
N18-2072,2,future work will explore comparison and combination with other generalization methods exploiting datasets deeply as well as our method.
N18-2073,2,"importantly, we also plan to evaluate our models on standard clir test sets such as trec (schauble and sheridan ¨ , 1997), ntcir (2007), fire (2013) and clef (2016)."
N18-2073,2,"future work includes: (a) expansion of the dataset to more languages, (b) extraction of different types of queries and relevant judgments from wikipedia, and (c) development of other ranking models."
N18-2074,1,we are also interested in nonlinear compatibility functions to combine input representations and edge representations.
N18-2074,1,"for future work, we plan to extend this mechanism to consider arbitrary directed, labeled graph inputs to the transformer."
N18-2075,1,another important direction is developing a structured global model that will take all local predictions into account and then perform a global segmentation decision.
N18-2075,1,"in future work, we will explore richer neural models at the sentence-level."
N18-2077,1,"it would be straightforward to extend the addcos method to handle multi-word paraphrases by training embeddings for multi-word phrases, keeping in mind that longer substitutions might require restructuring the produced sentences to preserve grammaticality."
N18-2078,1,"it would be interesting in future work to both understand when semantics appears beneficial, and also to see which components of semantic structures play a role."
N18-2078,2,experiments on other language pairs are also left for future work.
N18-2079,1,in the future we would like to change the training model to dynamically build the encoder and the attention model in order to match our incremental decoder.
N18-2082,1,"in future work, we would like to study how well nmt encoders capture other semantic phenomena, possibly by recasting other datasets."
N18-2091,4,we hope that these robustness methods are generalizable to other insertion-based adversaries for q&a tasks.
N18-2092,4,we also want to explore how to adapt cloze style pretraining to nlp tasks other than qa.
N18-2092,1,"for future work, we plan to explore the active learning setup for this task – specifically, which passages and / or types of questions can we select to annotate, such that there is a maximum performance gain from fine-tuning."
N18-2093,1,"in this way, we can study the performance of a generalized model on a more realistic text-to-sql task which includes many complex sql and different databases."
N18-2093,2,"in the future, we plan to advance this work by exploring other more complex datasets under the database-split setting."
N18-2095,5,"in the near future, we will look at multi-task helpfulness prediction to further transfer knowledge across domains."
N18-2098,1,"as future work, we propose to tackle general tabular summarization where the schema can vary across tables in the whole dataset."
N18-2100,4,we also plan to use key2vec in other domains such as news articles and extend the methodology for other related tasks like summarization.
N18-2100,3,"in the future, we plan to use other existing procedures for training phrase embeddings and study their effects."
N18-2105,1,"in future work, we would like to apply ranking algorithms that leverage the specific structure of our graph representation, such as the one proposed in (becker, 2013)."
N18-2106,4,"future work can also explore other domains (e.g., newswire and literary fiction) and evaluate character and event alignments between narratives based on established ground truths."
N18-2107,5,"as future work, we plan to extend our models by considering the prediction of more than one emoji per social media post and also considering a bigger number of labels."
N18-2110,1,potential future work includes using more conversational context and implementing multi-class classification to differentiate among stages of ad.
N18-2110,4,"we also plan to apply this generalizable model to other similar neurological diseases, such as diffuse lewy body disease and huntington’s disease (heindel et al., 1989)."
N18-2111,1,"we plan to work on these improvements for future work, and also explore evaluation methods which go beyond language model perplexities, and capture model aspects closer to the task and domain."
N18-2112,1,a promising approach would be to substitute the handcrafted feature functions used in this work by neural feature extractors trained jointly with the policy.
N18-2113,3,"despite these mixed results, we hope that the evaluation guidelines presented here will help promote work in this area, in order to eventually provide better tools for working with historical text collections."
N18-2114,5,"in future work, we would like to investigate the effect of memory mechanism for multi-task learning, which is similar to gate mechanism but more complex."
N18-2115,4,we also would like to understand this approach better by testing it on more natural language processing tasks.
N18-2115,1,"in the future, we plan to explore more variations of the meta-learning setup, such as using different relevance functions, including the ones that are jointly learned."
N18-2120,1,"we plan to train our models on dialogue/image pairs from datasets such as comics (iyyer et al., 2017) and visual storytelling (huang et al., 2016); these models could also help learn powerful joint representations of vision and language to improve performance on downstream prediction tasks."
N18-2122,1,"the techniques explored here could be use to combat adversarial attacks on cnns, detect misclassifications, or possibly guide the improvement of cnn architectures, and eventually help to unite the study of semantics in computer vision and computational linguistics."
N18-3002,2,we are now planning to extend them with image data.
N18-3004,1,"in ongoing work, we explore reinforcement learning techniques to reach the goal state quicker thereby reducing the number of interactions."
N18-3007,2,"furthermore, we plan to develop qe methods for languages other than english, where the amount of training data is much smaller."
N18-3007,4,"as future work, we plan to bring those research and pilot systems into production and gather experience on their use; as well as extending them to multi-class prediction for finer-grained qe, directly predicting the error severity classes (good, p3, p2, p1)."
N18-3009,6,we would also like to utilize human evaluators to judge the rankings produced in the review reranking experiments.
N18-3009,1,"for future work, we wish to investigate alternative ways to aggregate and compute user profiles and compute distances between objects to rank and user profiles."
N18-3012,1,"in the future, we plan transfer these findings to production settings by performing regular nmt model updates with batches of collected user behavior data, especially focusing on improving translation of ambiguous and rare terms based on rewards from implicit partial feedback."
N18-3014,5,we also plan to explore training with low precision for faster experiment turnaround time.
N18-3014,1,in the future we plan to implement a multilayered matrix multiplication that falls back to the naive algorithm for matrix-panel multiplications.
N18-3017,2,"in future work, we plan to apply our approach to further languages and explore bootstrapping new domains for an existing nlu system."
N18-3018,1,"in the future, we plan to explore unsupervised methods for transfer learning and the effect of semantic similarity between source and target tasks."
N18-3021,1,"in the future, we plan to focus on the normalization of heads (e.g.pnl and panal to panel) and parts (e.g.lft valve and left vlv to left valve) from panda extracted results."
N18-3025,5,"we would also like to investigate good-turing frequency estimation (good, 1953)."
N18-3025,1,"for future work we would like to try complex neural network architectures, regularization, and semantic embeddings or other abstracted relations to enhance the signatures."
N18-3027,1,"in future work, we want to extend the weighting scheme by integrating ontology and keyword information in order to improve the similarity search."
N19-1002,5,"in future work, we intend to explore how the encoding pattern we found varies across network architectures and hyperparameters, as well as across languages and domains."
N19-1002,5,we also would like to investigate the time course of emergence of the found behavior over training time.
N19-1002,3,"we conjecture a similar distinction between sr and lr units to be found in the human brain, as well as an interaction between syntax-processing and feature carrying units such as the lr units, and plan to test these in future work."
N19-1002,1,"more generally, we hope that our study will inspire more analyses of the inner dynamics of lstms and other sequence-processing networks, complementing the currently popular “black-box probing” approach."
N19-1003,2,"there are several venues for future work including (a): finer-grained data sampling at queue level,"
N19-1003,1,"(c): applying this model to areas where neural networks have not been investigated, e.g. due to limited availability of labeled data."
N19-1003,1,"b): extending our model to other machine learning algorithms that employ iterative training, such as boosting approaches,"
N19-1005,1,"in future work, we will further explore what information is encoded into the model representations when neural and behavioral data are used to train neural networks, and how these representations differ from the representations in a model trained on language alone."
N19-1008,3,an important next step is to test the system using asr rather than hand transcripts.
N19-1008,4,"finally, we expect that the innovation model of prosody can benefit other nlp tasks, such as sarcasm and intent detection, as well as detecting paralinguist information."
N19-1010,3,we plan to assess the effectiveness of our approach in the near future by integrating it in a heads-up display cai system and performing a user study.
N19-1010,1,"additionally, speech features could be extracted from the source or interpreter audio to reduce the dependence on a strong asr system."
N19-1011,1,"first, we can further expand the scope of audio caps."
N19-1011,1,"second, our model is integrable with speech counterparts to achieve more complete auditory captioning tasks."
N19-1011,1,"additionally, if we can better understand what makes a headline funny, we may be able to automatically generate funny headlines and even personalize them to particular readers."
N19-1012,3,future work with this data could include deeper features for assessing humor.
N19-1013,5,"one significant research challenge in the space of free text generation problems when the set of possible outputs is large, is that of automatic evaluation (lowe et al., 2016): in our results we saw some correlation between human judgments and automatic metrics, but not enough to trust the automatic metrics completely."
N19-1013,2,"following mostafazadeh et al.(2016), we could combine text input with image input in the amazon dataset (mcauley and yang, 2016) to generate more relevant and useful questions."
N19-1013,4,"lastly, we hope to integrate such a question generation model into a real world platform like stackexchange or amazon to understand the real utility of such models and to unearth additional research questions."
N19-1017,4,"however, our new training procedure can be applied to any task, so a future work would be to use it to perform phylogenetic pos tagging."
N19-1017,1,other directions for the future are designing better sampling methods as well as better ways to measure training convergence at each level.
N19-1018,1,"finally, we plan to adapt our system to non-projective dependency parsing and semantic graph parsing."
N19-1019,2,we also plan to see how the different experiments we have made to identify annotation errors and inconsistencies can be used during the annotation process to reduce the workload of annotators and help them creating high-quality corpora.
N19-1026,5,"as a future work, we will be working on devising automated-assisted methods for detection of paraphrasing issues."
N19-1029,3,potential directions may include ranking the subgraph by assigning each edge (relation) a closeness score and evaluating the length of the shortest path between any two path-connected entity nodes.
N19-1029,4,"one could also consider extending our approach to complex questions, e.g., multi-hop questions where more than one supporting facts is required."
N19-1029,1,"in the future work, one could further improve the performance on simple question answering tasks by exploring relation ranking, different embedding strategies and network structures, dealing with open questions and out-of-scope questions."
N19-1030,1,"for future work, we plan to explore the directions of (1) constructing multihop query and (2) developing end to-end retriever reader model via reinforcement learning."
N19-1031,1,we expect in the future that incorporating a beam search may further improve performance.
N19-1032,1,our future work will be making use of more complex relations between entities and building graphs in more general way without candidates.
N19-1034,4,"in the future, we would like to explore the other dimensions to our multi-task framework, e.g., sentiment classification & intensity prediction, emotion classification & intensity prediction and all the four tasks together."
N19-1035,4,"in the future, we will apply this conversion method to other similar tasks."
N19-1036,1,"in the future, we plan to explore better target opinion word extraction approaches to find better “supervision” signals."
N19-1037,1,"in the future, we plan to explore semi-supervised learning methods to address the problem of data scarcity in this task."
N19-1043,4,"in future work, we will investigate the use of differentiable reconstruction from sampled sequences in unsupervised and semi-supervised sequence generation tasks."
N19-1043,2,"in particular, we will exploit monolingual corpora in addition to parallel corpora for nmt."
N19-1044,2,"in the future, we will study how the copy success rate and the bleu scores interact when different sampling strategies are taken to obtain augmented training corpus and when the amount of augmented data grows."
N19-1044,4,"another direction is to validate the performance when applying this approach to language pairs that contain a number of identical letters in their alphabets, such as english to french and english to italian."
N19-1046,1,"in the future, we want to dig deeply into the subspace regularities of the learned representations for more fine-grained understanding."
N19-1048,2,"furthermore, it would be interesting to investigate whether the proposed architecture is also beneficial for languages typologically different from english, e.g., morphologically rich languages."
N19-1048,1,"in future work, one might investigate whether attention mechanisms on the word level (cf.ling et al., 2015) can further improve the model’s performance."
N19-1049,1,"the idea of using both x and x0, akin to how we train automated classifiers of naturalness (section 4.3), can be extended to construct a perplexity-based metric that also takes into account the perplexity of input x."
N19-1049,3,it is worth studying the distinction between style words and content words in the vocabulary of each such dataset.
N19-1049,3,another avenue for future work could be evaluating on datasets with a different style or number of style classes.
N19-1052,4,"finally, future work could expand on our methodology to formulate other more general tasks aiming to understand the reasons why a person is sharing a personal story."
N19-1053,1,"to make use of such systems, one needs to develop mechanisms to recognize valid argumentative structures."
N19-1053,2,"in addition, we ignore trustworthiness and credibility issues, important research issues that are addressed in other works."
N19-1054,1,"as bert pre-training includes a next-sentence prediction task, we expect this model to be effective for modeling argumentative context and to be beneficial for predicting premise or justifications for these claims and the relations between these argumentative components."
N19-1054,1,"in the future, we plan to expand this work beyond single sentences as the data-set for lm fine tuning used in our experiments consists of sentences containing im(h)o without additional context."
N19-1055,1,"as future work, we would like to explore other architectures to directly model dependencies between slot labels and intents."
N19-1055,3,we will also test the proposed approaches against real-world scenarios to understand their generality across various domains.
N19-1056,1,"ultimately, of course, we hope to leverage such corpora to build and apply better models of multimodal communication."
N19-1063,1,"future work may consider revisiting choices like the use of semantically bleached sentence inputs, the aggregation applied to models that represent sentences with sequences of hidden states, and the use of cosine similarity between sentence representations."
N19-1069,3,"future work could investigate the effect of other potential confounding variables in satire detection, such as the distribution of time and region of the articles."
N19-1069,1,"further, we propose to perform more quantitative but also more qualitative analysis to better understand the behavior of the two classifier con- figurations in comparison."
N19-1071,4,"in future work, we plan to explore the potential of seq3 in other tasks, such as unsupervised machine translation (lample et al., 2018a; artetxe et al., 2018) and caption generation (xu et al., 2015)."
N19-1072,1,"in particular, it may be possible to extend our design of crowdsourcing tasks to supply indications for these complementary measurements as well."
N19-1075,2,"in future work, we will explore cross lingual transfer for super tagging and semantic role labeling."
N19-1078,1,future work will therefore examine methods to learn weighted pooling of previous mentions.
N19-1078,4,we will also investigate applicability of our proposed embeddings to tasks beside ner.
N19-1079,4,"although we focused on the task of named entity recognition in this work, we believe the proposed approach may find applications in some other sequence labeling tasks or other more general structured prediction problems where the issue of incomplete annotations is involved."
N19-1082,1,future work includes the exploration of automatically learning the underlying graphical structure of the input data.
N19-1083,1,some future extensions include exploring more advanced graph embedding techniques without modeling entity-specific parameters and using text encoders as additional signals.
N19-1089,2,these experiments suggest plausible pathways to achieving human-level performance on this task that are both challenging and interesting problems for future research.
N19-1091,4,"moreover, studies will be conducted on courtesy transfer for the other domains, and also transfer learning from one domain to the another (like customer care to hospitality)."
N19-1091,1,"in future, along with the opportunity of extending the architectural designs and training methodologies to enhance the performance of our systems, we look forward to designing a specific component to enhance the natural language generation component of an end to end chatbot, by including appropriate mechanisms to interact with all its components (memory, database, and the dialog manager)."
N19-1092,2,"for future work, we will explore techniques to generate metaphors without extracting its fit words in the corpus and improve the quality of generated metaphors."
N19-1093,1,"since the proposed model adopted an extensible structure, one possible future work is to explore the best way to enhance it with more complicated knowledge resources such as knowledge graphs."
N19-1094,2,"in the future work, we will use transformer, which is proved to be more powerful than lstm, as the encoder of our unsupervised deep structured semantic models, and we will collect a larger corpus from common crawl to train our model."
N19-1100,1,a final and fascinating direction of future work is to explain the non-normality of certain types of word vectors (and in particular the presence of outliers) by analysing their training procedures.
N19-1101,1,"in future work we plan to address this by adding explicit cross-sentence semantic knowledge (joshi et al., 2018)."
N19-1103,1,it is worth investigating modeling these interactions while keeping the merit of fast training and evaluation.
N19-1103,1,"as future work, we plan to devise convolutional paradigms that can maximize interactions not only between subject entities and relations, but also between object entities and relations."
N19-1103,3,"in convr, we use 1-to-many scoring to speed up training and evaluation."
N19-1104,1,"in future work, we will extend grank to use more complex patterns."
N19-1105,1,"in the future, we plan to explore the following directions: (1) we will extend our method to further extract event arguments and perform event extraction."
N19-1105,2,"(2) we will develop a large-scale and clean dataset for ed based on our method, which will benefit further research in this field."
N19-1106,1,"moreover, ablation study results suggest that a nonlinear propagation model to reconstruct the full label set may be of benefit."
N19-1106,1,"in future, we can improve our model by using nonlinear training model instead of a simple linear regression model for the selected subset of the labels."
N19-1107,1,"in the future, we plan to further explore the effect of different encoding modules like bi-lstm or self-attention and try to model temporal information with more sophisticated choices."
N19-1108,1,"in the future, we plan to extend our framework to do multi-label classification with a larger amount of data, and also study how semantic units defined by linguists can be used in the zero-shot scenario."
N19-1109,1,"in future, we would like to explore ways of applying a similar graph based formalism for learning vectors for documents."
N19-1110,1,"moreover, we would like to explore a more sophisticated smoothing technique where the number of gaussian components is adapted for each word."
N19-1110,1,"as future work, one can hypothesize that the area a word covers in the mapped space reveals its semantic range."
N19-1110,1,"finally, it may 1060 be helpful to investigate non-linear mappings between semantic spaces using deep neural network architectures."
N19-1110,1,given that gaussian mixture embeddings could capture the uncertainty of a words representation in the semantic space one could also investigate different metrics for measuring the semantic relationship between word pairs that go be? yond their point-wise comparison.
N19-1110,1,"in this direction, a refinement of the semantic anchor selection approach could be explored in an iterative way assuming that the variance of a words gaussian distribution denotes its degree of polysemy (vilnis and mccallum, 2015)."
N19-1115,3,"finally, we plan to look into applying recursive approaches to language modelling as a pre-training step and measure if it has the same impact on downstream tasks as sequential models."
N19-1115,1,"in the future, we would like to explore further relaxation-based techniques for learning the parser, such as rebar (tucker et al., 2017) or relax (grathwohl et al., 2017)."
N19-1116,2,"potential avenues include training larger models over much larger corpora, extra unsupervised or weakly-supervised phrase classification objectives, and other modeling enhancements."
N19-1116,2,we are also eager to apply diora to other domains and languages which do not have rich linguistically annotated training sets.
N19-1117,1,"in future work, we look to model other types of world knowledge beyond named entities using predictive learning and training on large corpora of text without additional information, and to make kalm more robust against corrupted entities."
N19-1119,1,"in the future, we are mainly interested in: (i) exploring more difficulty heuristics, such as measures of alignment between the source and target sentences (kocmi and bojar, 2017), sentence length discrepancies, or even using a pre-trained language model to score sentences, which would act as a more robust replacement of our sentence rarity heuristic, and (ii) exploring more sophisticated competence metrics that may depend on the loss function, the loss gradient, or on the learners performance on held-out data."
N19-1119,4,"furthermore, it would be interesting to explore applications of curriculum learning to multilingual machine translation (e.g., it may be easier to start with high-resource languages and move to low-resource ones later on)."
N19-1119,4,"we would also like to explore the usefulness of our framework in more general machine learning tasks, outside of nmt."
N19-1121,2,"another interesting future direction would be to explore different hand-engineered or learned data representations, which one could use to encourage models to agree on during training (e.g., make translation models agree on latent semantic parses, summaries, or potentially other data representations available at training time)."
N19-1122,1,another promising direction is to directly augment transformer encoder on recurrence modeling without the additional encoder.
N19-1122,4,"Future work includes validating the proposed model in other tasks, such as reading comprehension, language inference, and sentence classification."
N19-1125,6,"in future work, we will provide theoretical justification of the effectiveness of the proposed regularization terms."
N19-1130,3,(2) literary criticism will be used as a testbed for evaluating representations.
N19-1132,3,our future study will further examine the robustness of several existing evaluation metrics and explore new metrics appropriate for cross-corpora and/or cross-domain evaluation.
N19-1133,1,"moreover, we also want to introduce more nlp prior knowledge into the model."
N19-1133,2,"in the future work, we will investigate the ability of star-transformer by unsupervised pre-training on the large corpus."
N19-1135,5,"furthermore, we will expand our work by investigating additionally the criminal offense of incitement to hatred ( 130 stgb) and its implication on the freedom of expression."
N19-1135,3,"in future work, we will investigate the usefulness of layman-annotated data for an automated classification."
N19-1136,2,future studies will explore the possibility of applying the title-guided attention mechanism to other large datasets on major social media platforms.
N19-1136,1,"it is also interesting to see whether the semantic based loss regularizes can be adapted to improve the performance of the recent pre-trained transferable deep learning models, such as the bidirectional encoder representations from transformers (bert) (devlin et al., 2018)."
N19-1136,1,"it is also interesting to see whether the semanticbased loss regularisers can be adapted to improve the performance of the recent pre-trained transferable deep learning models, such as the bidirectional encoder representations from transformers (bert) (devlin , 2018)."
N19-1137,1,"in future, we would like to explore character embedding as this can give us crucial linguistic features from noisy twitter data."
N19-1138,2,possible future research direction might be detecting cyber security related events in different languages.
N19-1143,1,a promising future direction is to use hierarchical clustering to create better cluster-level representations.
N19-1144,2,"we further plan to create similar datasets for other languages, following olids hierarchical annotation scheme."
N19-1145,4,"in the future, we plan to explore a broader range of properties from kb to facilitate biomedical information extraction tasks."
N19-1146,1,"furthermore, the mechanisms that represent linguistic features into symmetric spaces should be analyzed within the context of explainable ai."
N19-1146,2,"in the future, the agreement among modalities concept may be applied to design objective functions for training classifiers in various tasks, and from other data sets (for example, education and occupation modalities for the bank marketing prediction task)."
N19-1147,2,we conjecture that these may serve well for cross-sentence relation extraction in long pieces of texts.
N19-1147,1,"also, we only considered one relation type between each entity and bridge token but it is possible, and very likely that two different relation types may lead to a third relation type."
N19-1147,1,"despite restricting ourselves to sors, it should be noted that the proposed method can be generalized to third and fourth order relations."
N19-1148,4,"we expect to apply our trained model to various text-based psychotherapy applications, such as extracting and summarizing counseling dialogues or using the information to build a model addressing the privacy issue of training data."
N19-1148,1,we hope our categorization scheme and our convmfit model become a stepping stone for future computational psychotherapy research.
N19-1148,1,"convmfit is a seq2seq model for counselor-client conversation, however, another approach would be to model with existing non-goal oriented conversation models incorporating variational autoencoder (vae) (serban et al., 2017; park et al., 2018b; du et al., 2018)."
N19-1149,4,"as a future study, we will explore how these similarity measures predict performance of pretrained models in other nlp tasks."
N19-1150,5,"in future work, routing strategies based on instance difficulty could be further investigated for budget-quality trade-off."
N19-1151,1,"in addition, we also plan to explore the learning of emotional based representations by means of a deep neural network from which we could exploit local invariance properties to model fine-grained emotions."
N19-1152,2,another possibility is the expansion of the test-set for a more accurate capture of the variance in the corpus.
N19-1152,2,"future work includes manually correcting the human phenotype annotations that did not match any hpo identifier, with the potential of expanding the number of human phenotype annotations almost 2-fold and increasing the overall recall."
N19-1152,1,"further, we intend to use semantic similarity to validate the human phenotype gene relations."
N19-1152,4,"finally, the effect of different ner systems applied to the pipeline should be studied."
N19-1152,2,"also, we intend to expand the corpus by identifying more missed gene annotations using pattern matching, which is possible due to our approach being fully automated."
N19-1154,1,"we would also like to explore representations from robustly trained systems, which should improve performance on noisy input (belinkov and bisk, 2018; heigold et al., 2018)."
N19-1154,4,"finally, it would be interesting to study representations in other nlp tasks besides neural machine translation."
N19-1154,1,"in future work, we plan to study how different units affect representation quality in non-recurrent models such as the transformer (vaswani et al., 2017) as well as in convolutional architectures (gehring et al., 2017)."
N19-1160,1,"there are various avenues to improving and extending derivation projection: alignment ambiguity could be handled with a global score, and multiple possible parses could be included in the target-language set, potentially improving the tradeoff between the number of projected derivations and the amount of noise."
N19-1160,1,"to increase the range of structural differences between languages that can be handled, derivation projection could be extended to consider sub-token units and to handle 1:n translation units in addition to n:1 ones."
N19-1161,1,"in the future work, we will integrate gaussian embeddings (vilnis and mccallum, 2015) with our approach."
N19-1166,1,"developing a more expressive algorithm (e.g., one that incorporates reply-structure relationships) could boost predictive performance, and enable textual features to be less brittle."
N19-1166,1,one promising avenue for future work is to examine higher-quality textual representations for conversation trees.
N19-1168,1,"future work might investigate end to-end approaches, or develop alternative approaches that generate titles more similar to how humans write titles."
N19-1170,1,"future work includes optimizing control settings automatically, and building more convincingly human-like chatbots."
N19-1172,1,"language models conflate the two, so developing methods that are nuanced enough to recognize this difference is key to future progress."
N19-1173,1,"in the future, we would like to generalize sumo to abstractive summarization (i.e., to learn latent structure for documents and sentences) and perform experiments in a weakly-supervised setting where summaries are not available but labels can be extrapolated from the articles title or topics."
N19-1174,2,we also plan to improve the dataset by improving our models for contrastive pair prediction to reduce noise.
N19-1174,5,"going forward, we hope to classify the viewpoint of the original claim and then generate a claim with a desired orientation."
N19-1174,6,"furthermore, we hope to improve on the generation task by identifying the types of claims we encounter."
N19-1175,5,"it has the potential of triggering novel research on multimodal deception data, specifically for speech and the dialogue dimension, which should be explored in the future."
N19-1177,1,"in particular, we are looking into the bayesian method of simpson and gurevych (2018), which takes advantage of the sequential dependencies between bio tags, and works more robustly with noisy, subjective data such as ours."
N19-1177,1,"for future work, we are investigating alternatives to mace, which was designed for categorical annotations rather than the sequence labelling of our task."
N19-1178,1,"in the future, we may use more sophisticated ways to encode the entity information into the latent states."
N19-1179,2,"in the future, we will continue to enrich document-level causal structures, e.g., by considering segment-wise topic layouts and rhetorical discourse structures."
N19-1185,1,"in the future, we have a plan to leverage external knowledge and generalize our model for target independent stance detection in the same domain."
N19-1186,3,our future work will improve we wpi to obtain high-quality evaluation scores in combination with other metrics.
N19-1186,1,"moreover, we will use we wpi to improve nmt quality."
N19-1187,3,we plan to further investigate the performance of testing time greedy decoding with beam search optimization during training.
N19-1188,4,"moreover, we have shown that ihs outperforms even supervised models on downstream tasks of multilingual dependency parsing and document classification, and this anomaly requires further investigation in future work."
N19-1193,3,"therefore, evaluating vbsix on other domains is a natural next step for our research."
N19-1195,1,"our future work focuses on further increasing the accuracy of the self-generated advice model, so we can achieve better performance with no human effort."
N19-1196,1,"furthermore, we plan to constrain our model to always predict paths that exist in the graph, as we discussed above."
N19-1198,2,"other future directions of this work include adding temporal attention in order to handle more complicated temporal references and extending this approach to work for longer, and thereby more challenging videos such as movies."
N19-1199,1,"also, our model only uses lexicosyntax features, and ignores acoustic features (e.g., pause duration) which are significant for dementia detection in english."
N19-1199,5,"future work will investigate the use of automatic speech recognition to reduce the need for manual transcripts, which are impractical in a clinical setting."
N19-1202,2,"thanks to a scalable corpus creation procedure initialized with constantly expanding ted talks data, future extensions will increase the coverage of the already present target languages and introduce new ones."
N19-1208,1,"in the future, we intend to improve our approach by eliminating the static exploration schedule and binning strategy, and extend it to handle additional data attributes such as domain, style, and grammatical complexity."
N19-1210,2,"future research should experiment with other datasets (reddits from other domains, other online communities) and also alternative models that address the challenges described here."
N19-1213,4,"in future work, we plan to move from twitter to more generic domains and evaluate our approach to more tasks."
N19-1213,1,"finally, we want to explore approaches for improving the adaptive layer unfreezing process and the contribution of the language model objective (value of ) to the target task."
N19-1213,1,"additionally, we aim at exploring ways for scaling our approach to larger vocabulary sizes (kumar and tsvetkov, 2019) and for better handling of out-of-vocabulary words (oov) (mielke and eisner, 2018; sennrich et al., 2015) in order to be applicable to diverse datasets."
N19-1215,1,we will develop a generic method of inducing personalized word embeddings for any subjective text.
N19-1215,1,we plan to analyze relationships between semantic variations and user factors of writers who used the target words such as age and gender.
N19-1216,5,"we further plan to go beyond left vs. right, which is not universal and can exhibit regional specificity (tavits and letki, 2009), and to model other kinds of biases, e.g., eurosceptic vs. europhile, nationalist vs. globalist, islamist vs. secular, etc."
N19-1216,2,"in future work, we want to try more auxiliary tasks, and to experiment with other languages."
N19-1217,2,"research on puns for other languages such as chinese is still under-explored, which could also be an interesting direction for our future studies."
N19-1217,1,future research includes the investigations on how to make use of richer semantic and linguistic information for detection and location of puns.
N19-1218,4,"a future line of work we are interested in is to test whether the knowledge learned with this dataset could be transferred to real-word actions (i.e. real-domain setups), or if such transfer is not possible and a model needs to be trained from scratch."
N19-1223,1,"in future work, we would like to integrate a semantic parser into our model (yin et al., 2018)."
N19-1223,2,"this opens up scope for data augmentation for downstream nlp tasks, such as machine translation."
N19-1223,1,"in addition, we would be able to generate text-to-text paraphrases by parsing into amr first and then carrying out the paraphrase generation procedure described in this paper (iyyer et al., 2018)."
N19-1223,1,"by integrating a component which parses into amr into our model, we can do semi-supervised learning on plentiful unannotated natural language sentences, and improve our amr generation performance even further."
N19-1231,1,we will consider strategies such as wang et al.(2018a)s shift-reduced-based lstm architecture or sohrab and miwa (2018)s method of modeling the contexts of overlapping potential named entity spans with bidirectional lstms.
N19-1231,1,"additionally, we will expand her to model hierarchically nested entity labels."
N19-1231,3,"in future work, we will investigate sources of noise in performance to see if these are due to gaps in the model, idiosyncrasies of corpora, or both."
N19-1234,1,"furthermore, the powerful transformer (vaswani et al., 2017) can be applied for the auto-encoder which could improve the performance of the proposed approaches."
N19-1234,1,pre-train the auto-encoder before adversarial training can also improve the performance.
N19-1234,1,"rl or self-attention (zhang et al., 2018) techniques can be used as a tool to accommodate this weakness.3."
N19-1235,4,"for future work, we are interested in applying graph-to sequence neural networks (beck et al., 2018; song et al., 2018) to mrs-to-text generation."
N19-1236,5,"additionally, the planning stage allows explicit user-control and generating diverse sentences, to be pursued in future work."
N19-1237,5,"we plan to research over generating questions and using the reward signals to rerank the outputs, thereby including the inductive bias the rewards represent without allowing the model to exploit them."
N19-1238,5,future work could address the problem of repetition and entity coverage in the generated texts.
N19-1239,2,"one interesting direction is to investigate whether neuron can be extended to work on open-domain qa corpus, which may not be restricted to any specific domain."
N19-1243,1,"in future, we plan to explore the option of augmenting falcon with deep learning methods for further improvement in performance specially in entity and relation extraction module."
N19-1245,1,"in future work, we plan to extend our representation language and models to cover currently unsolvable problems, including sequence and high-order polynomial problems."
N19-1247,4,"also, the proposed methods can be further extended to other chinese nlp tasks, such as cws, text classification, and sentiment analysis."
N19-1247,1,"in the future, we plan to further improve and perfect the proposed method, such as exploring some strategies to handle oov words."
N19-1253,1,"in the future, we plan to explore multisource transfer and incorporating prior linguistic knowledge into the models for better cross-lingual transfer."
N19-1255,1,"self-discriminative learning shows potential 2473 for real-world scarcely-labeled scenarios, and our future work will focus on joint training of representations for semi-supervised learning."
N19-1258,4,in future we would like to adapt our method to other domain adaptation tasks and consider more effective alternatives for the generator regularizer.
N19-1259,1,"in addition, an end to-end opinion extractive summary method without given golden targets is also a future work."
N19-1259,1,"in future works, towe could be utilized to further improve the performance on downstream sentiment analysis tasks with building a more interpretable model, such as enhanced-feature or multitask learning."
N19-1261,3,evaluation of these techniques for benchmarking automated summarization systems is one direction for our future research.
N19-1262,4,"on a more general level, the interactive setup and the active learning strategies presented can also be used for other natural language processing tasks, such as question answering, to transfer a model to a new domain or genre."
N19-1262,4,"first, we intend to investigate further applications of our interactive setup, e.g., in movie subtitle compression or television closed captions where there is no sufficient training data to build neural models."
N19-1266,4,"future work will possibly be visualizing the visual and language features encoded by amfe to find more straightforward interpretations, as well as trying our method on more complex structures, discriminative models, and on discriminative tasks such as vqa and visual reasoning."
N19-1269,5,future work could also explore changes that modify existing content rather than simply appending.
N19-1273,5,one potential future work direction is investigating whether they extend to other structured prediction problems beyond semantic parsing.
N19-1275,1,"in future, we will further develop our system using structured attention (kim et al., 2017) and try to improve the accuracy of parsers in multi-tasking scenarios."
N19-1276,1,"in future work, we will explore (1) the relationship between vocabulary coverage and segmentation performance, and (2) the effect of using pre-trained word vectors learned from different domain texts in domain adaptation scenarios."
N19-1277,4,"in the future, we hope to apply our model to other downstream tasks and other logographic writing systems."
N19-1281,4,"furthermore, our tagging approach should be universal and work with other tasks like named entity recognition."
N19-1281,1,using retraining also seems to be a natural extension for this work.
N19-1281,1,"we will consider how to make the model to recognize new words, which is an important feature for a practical analyzer."
N19-1281,1,"it is easy to provide diverse models, required for tri-training, by using different types of encoder and varying network parameters."
N19-1281,1,"a method to incorporate tags with a large number of possible values (like readings and lemmas) without introducing embeddings for them, hence keeping the models small, could also be a useful extension."
N19-1282,1,in future work we hope to integrate syntactic parsing more closely with automatic speech recognition.
N19-1282,1,"it may also be possible to more directly integrate an attention-based syntactic parser with a speech recognizer, perhaps trained in an end to end fashion."
N19-1282,1,"a first step is to develop parsing models that parse asr output, rather than speech transcripts."
N19-1284,1,"for future work, it could be interesting to explore guiding some latent modes with a few examples to pick up specific user features such as personality traits."
N19-1285,2,"in future work, we hope to reduce the dependence on fluent target data during training through decoder pretraining on external non-conversational corpora or multitask learning."
N19-1285,3,"further, standard metrics alone do not tell the full story for this task; additional work on evaluation metrics may better demonstrate the differences between such systems."
N19-1287,2,"in the future work, we will consider to detect events and their sentence-level and document level factuality with a joint framework, and we will also continue to expand the scale of our dlef corpus."
N19-1288,1,to deal with the multi-label problem of relation extraction and to integrate external knowledge into our model will be the tasks of our future work.
N19-1292,1,one interesting future work is to incorporate the similar documents themselves into keyphrase generation.
N19-1294,4,"as a part of future investigation, we plan to apply the approach to other distantly supervised tasks, such as relation extraction."
N19-1295,1,"we also plan to investigate the use of variational inference (jordan et al., 1999) as a means of training our model."
N19-1295,1,using variational inference might improve the stability and performance of our model.
N19-1295,4,"for future work, we plan to apply our model to other nlp tasks such as relation extraction and named entity recognition."
N19-1298,1,we aim to address them and further extensions of our model in future works.
N19-1299,1,"in the future, we would like to explore effective ways of modeling more complex types of constraints (e.g., ordinal, comparison and aggregation)."
N19-1300,6,"future work could include building a document-level version of this task, which would increase its difficulty and its correspondence to an end-user application."
N19-1302,1,"results on two challenging qa datasets, as well as our ablation study, indicate that entailment based qa can achieve state-of-the-art performance and is a promising direction for further research."
N19-1306,4,"in the future, we plan to explore the following directions: (1) we may combine our method with recent denoising methods to further improve performance.(2) we may combine rule mining and reasoning technologies to learn better class embeddings to boost performance.(3) it will be promising to apply our method to zero-shot re and further adapt to other nlp scenarios."
N19-1308,1,future directions include extending the framework to encompass more structural ie tasks such as event extraction.
N19-1309,1,"in the future, we would like to improve extraction by training a model to extract (predicate, object) pairs directly without having to train on particular predicates."
N19-1309,1,"such a model could potentially be based on visual clues common across websites, so a single model could be applied to many sites."
N19-1313,1,"in future work, we plan to dig deeper on the benefits of sparse attention in terms of better interpretability of context aware nmt models."
N19-1315,1,"in future work, it is interesting to use our method in generative adversarial networks to further improve the sentence generation models."
N19-1318,4,we plan to apply our work towards building a notification system for incoming helpful posts.
N19-1320,1,we will explore and incorporate other metrics to improve other aspects of generated texts such as the structural diversity in the future work.
N19-1322,1,"in future, we will focus on each category to devise novel methodology for mitigating the errors of that category and in turn further increase the accuracy of g2p system in bangla."
N19-1326,3,we are going to test deep architectures to combine the n-grams in misspellings to better capture various interdependencies of n-grams and correct versions of words.
N19-1326,3,"finally, we will assess the robustness of both character-based (kim et al., 2016) and context-dependent embeddings (devlin et al., 2018), (peters et al., 2018) with respect to misspellings."
N19-1326,1,"in the future, we will test different ways of training embeddings for misspellings including the extension of the same technique to multi-lingual embeddings."
N19-1326,3,"finally, we will assess the robustness of both character-based (kim , 2016) and context-dependent embeddings (devlin , 2018), (peters , 2018) with respect to misspellings."
N19-1327,1,"as future work, we plan to continue our investigation by extending the method with other ideas."
N19-1327,4,"finally, we plan also to explore the use of analogy embeddings in other tasks, such as question answering and knowledge base population."
N19-1327,1,"for instance, the use of positional embeddings, as well as the use of placeholders replacing the entities in the textual mentions are promising future directions."
N19-1329,1,"a possible direction for future work would be to explore which of these explanations is true, possibly by decorrelating particular aspects of linguistic structure from language modeling representations."
N19-1330,6,"in future work, we would like to obtain a better theoretical understanding of why starting the rnnlm from a zero state forms an effective n-gram regularize."
N19-1330,1,"we would also like to extend our regularization approach to bilstms (peters et al., 2017) and transformers (alec radford and sutskever, 2018; devlin et al., 2018)."
N19-1331,1,"as a future direction, we expect to lift this assumption, for example, by updating the common direction statistics at a sentence level using autoconceptors (jaeger, 2014, section 3.14)."
N19-1331,4,"finally, the continual learning based sentence encoders should be applied to downstream applications in areas such as open domain nlp systems."
N19-1334,5,scaling the gains derived from structural supervision is a challenge for data-scarce nlp and is the basis for future work.
N19-1344,1,"in future work, we plan to consider multiple predicates and event-nouns."
N19-1346,1,"future work includes leveraging external knowledge bases to disambiguate chunks and entities that appear within chinese addresses, as well as designing algorithms that are able to capture longer-range dependencies among chunks using alternative structures."
N19-1347,5,"finally, un-supervised discourse-level structure extraction of fake/real news documents is a worthwhile research topic."
N19-1347,5,"second, investigating the hierarchical structure at the word-level will be an exciting research inquiry."
N19-1347,1,"first, we in-tend to define more advanced properties from the discourse dependency trees."
N19-1348,2,we would like to expand them to more input sentences in future work.
N19-1350,1,"as future work, we plan to modify our model to use multiple contexts in text to improve the quality of descriptions, considering the one sense per discourse hypothesis (gale et al., 1992)."
N19-1351,2,"for instance, we can make use of more lenient patterns that capture an even wider 3485 range of discourse markers, such as multi-word markers."
N19-1351,1,"in future work, we also aim to increase the coverage of our method."
N19-1352,4,"in the future, we plan to investigate broader applications like summarization, translation, question answering, etc."
N19-1354,1,"we would also like to explicitly quantify the uncertainty captured in our framework under different sampling strategies or mcmc-sg methods (e.g., similar to mcclure and kriegeskorte (2016); teye et al.(2018))."
N19-1356,3,"in future work, a human experiment based on the agreement prediction task can help determine whether the difficulty of our languages is consistent across humans and rnns."
N19-1359,1,we expect that the two kinds of approaches can complement each other to further improve the expressiveness of multi-head attention.
N19-1359,1,"future work includes combining our information aggregation techniques together with other advanced information extraction models for multihead attention (li et al., 2018)."
N19-1361,4,another relevant line of work is adapting our model to other domains containing documents with similar linked structured such as wikipedia articles.
N19-1361,5,an interesting line of future work is to explore the design of such tasks or explore the properties or similarities between the auxiliary and the main tasks.
N19-1361,2,future work may benefit from replacing elmo with other types of contextualized representations such as bert in our scaffold model.
N19-1362,1,potential avenues for future work include multitasking bert with pair2vec in order to more directly incorporate reasoning about word pair relations into the bert objective.
N19-1364,1,"in the future, we plan to build a richer taxonomy of persuasion strategies and incorporate additional neural architectures such as variational autoencoders to better represent sentences in each message to further assist the modeling of persuasiveness."
N19-1364,2,"beyond the text, images and even audios may provide additional insights on the successes of persuasive requests."
N19-1364,4,"our model also has important applications to other domains, such as in computational advertisements, micro-funding platforms and political campaigns."
N19-1365,4,"it is our hope that these lessons extend to other richly compositional, context-sensitive language understanding tasks."
N19-1365,2,"we believe exploring more powerful variants of dispatching is an interesting avenue for future work, as is pretraining routing models on language model tasks using large corpora."
N19-1366,1,"further exploration of graph encoders is left to future work, which may result crucial to improve performance further."
N19-1367,2,"future work will involve extending the set of features involved, incorporating data from other languages, and testing whether similar techniques can be effective for detecting earlier stages of cognitive decline, such as mci."
N19-1368,2,"in future work, we will investigate structured and unstructured knowledge sources to find explanations for sentiments and emotions."
N19-1371,5,this motivates a key future research direction: designing more sophisticated attention mechanisms that (conditionally) identify spans of evidence pertinent to a given prompt.
N19-1374,1,future work could include incorporating contextual information that would help emoticons to better capture emotional content.
N19-1376,1,"moving forward, we would like to explore a more realistic multi-modal topic spotting system."
N19-1379,1,"for future work, we consider extending the model to handle unknown words."
N19-1379,1,"also, we want to find a more principled way to down sample the negative exemplars."
N19-1380,1,"this presumably makes sense for the english-thai transfer learning case since these two languages use different writing systems but given the results by lin et al.(2018) and yang et al.(2017), we would expect additional improvements by sharing character embeddings for languages with similar writing systems."
N19-1380,1,"second, one could try to include a specific learning objective to embed translations into a similar vector space as used by yu et al.(2018a) and conneau et al.(2018) for multilingual sentence representations."
N19-1380,1,"despite the range of models that we considered in this paper, we only scratched at the surface of possible cross-lingual (embedding) models, and hence there are many future directions of this work."
N19-1383,1,"in the future, we will further explore selecting the feature-consistent annotations from the source language and add to the target language, and explore unsupervised pretrained cross-lingual language models (peters et al., 2018; radford et al., 2018; devlin et al., 2018; lample and conneau, 2019) for cross-lingual low resource name tagging."
N19-1384,2,"in our future work, we will study the impact of using more partial translations of better quality to train nmt systems."
N19-1384,3,"we will also analyze whether our partial translations are useful because of their noisy nature, since noisy synthetic data have recently been proven useful in some specific configurations (edunov et al., 2018)."
N19-1384,2,we assume that we can collect better partial translations by searching in more monolingual data.
N19-1385,3,future work should investigate the effect of our proposed reordering methods on truly low resource machine translation.
N19-1386,1,"in future work, we plan to incorporate knowledge from the similarity space in our adversarial framework."
N19-1387,1,we would also like to explore alternative methods to address word-order divergence which do not require expensive parsing of the assisting language corpus.
N19-1387,4,"further, use of pre-ordering to address word-order divergence for multilingual training of other nlp tasks can be explored."
N19-1387,2,"while the current work focused on indian languages, we would like to validate the hypothesis on a more diverse set of languages."
N19-1388,1,understanding and improving zero-shot performance in such scenarios is also a promising direction for future work.
N19-1388,1,"there are many possible avenues for future work, including semi-supervised learning in such settings, exploring ways to reduce the performance degradation when increasing the number of languages, or using such models for multilingual transfer learning (mccann et al., 2017; eriguchi et al., 2018; artetxe and schwenk, 2018)."
N19-1390,2,"an interesting direction to pursue would be to use multiple related languages, to aid lexical relation classification in an under resourced language, instead of transferring supervision from a single language (english)."
N19-1390,1,"beyond classification, another direction for future work is to extend our approach to distinguish synonyms and antonyms from unrelated word pairs."
N19-1390,2,"the simplicity of our approach allows to easily incorporate other features (e.g., morphological marking) and it can be extended to further languages or lexical relations."
N19-1391,1,"in future work, we will explore the viability of the sentence mapping approach on other sentence embedding models."
N19-1393,1,"whereas a delexicalized parser offers a simple experimental setup, it impacts parsing performance."
N19-1393,3,"as future work, we plan to study the influence of typological features on each dependency type."
N19-1400,1,"since our task has two elements, summarization and chit-chat, the focus of our future work will be a more sophisticated multitask model that considers these relations."
N19-1400,1,we also plan to improve the proposed method so that it can generate even better initial utterances.
N19-1400,1,"in that case, depending on the users interest, the model needs to determine whether to do a usual chat or talk about the news contents."
N19-1400,1,"as a natural next step, we plan to develop a more sophisticated conversation model, which can not only generate initial utterances but also continue the conversation for the given news contents (yoshino and kawahara, 2014)."
N19-1403,1,"in the future, we intend to generalize our model to other relationships beyond strict entailment and contradiction relations."
N19-1404,3,"for future work, we will extend our study to examine saliency learning on nlp tasks in an active learning setting where real explanations are requested and provided by a human."
N19-1408,3,one potential extension of this work is to conduct a comprehensive ablation study to determine the relative contribution of each of the regularization and optimization techniques.
N19-1408,4,"finally, the examined regularization and optimization methods deserve exploration in other nlp tasks as well."
N19-1409,1,in future research we will investigate ways to improve the decoder with pre-trained representations.
N19-1410,1,"future work might allow finer-grained modeling of the tradeoff between under- and over-informativity within the sequence generation pipeline (e.g., with a learned communication cost model) or explore applications of pragmatics for content selection earlier in the generation pipeline."
N19-1414,4,"going forward, we will test this model on other tasks, diagnostic and otherwise, to see its generalizability."
N19-1417,1,"also, motivated by our findings, we will work on utilizing continuous space representations as side information in sampling the parameters of bkn, i.e. similar to zhao et al.(2018), which potentially can reduce the gap between bkn and neural models."
N19-1419,2,"beyond this actionable insight, we suggest our probe may be useful for testing the existence of different types of graph structures on any neural representation of language, an exciting avenue for future work."
N19-1420,1,"furthermore, a deeper and robust quantum inspired neural architecture in a higher-dimension hilbert space like (zhang et al., 2018b) is also worth to be investigated for achieving stronger performances with better explanatory power."
N19-1420,1,"another possible direction is to borrow other quantum concepts to capture the interaction and non-interaction between word semantics, such as the fock space (sozzo, 2014) which considers both interacting and noninteracting entities in different hilbert spaces."
N19-1420,1,"despite the effectiveness of the current network, we would like to further explore the phase part in complex-valued word embedding to directly link to concrete semantics such as word sentiment or word position."
N19-1422,1,"in the future, we would like to devise models that can learn when and how to integrate multiple modalities by taking care of the complementary and redundant aspects of them in an intelligent way."
N19-1424,2,"finally, although we focus on english, we expect our method will work well for other languages, but leave this direction for future work."
N19-2002,5,"we plan to address several issues, including but not limited to: 1) how can we capture and share knowledge of common patterns of utterances belonging to the same domain but written in different languages across different locales?2) how can we prevent a locale from interfering with other locales using different language for learning linguistic context of utterances?"
N19-2005,4,"furthermore, we plan to extend the graph convolution framework to other tasks in vrds, such as document classification."
N19-2006,1,m-cvae thus opens up new avenues to improve the quality of responses further through personalization and stylization.
N19-2007,2,"in future work, we plan to develop a more comprehensive annotation guide for error analysis in real-world goal-oriented systems."
N19-2010,3,"future research will include verifying how much our headline generation model can affect practical performance indicators, such as click-through rate."
N19-2018,1,we would also like to jointly model the relation extractor and the entity linker to improve the model performance.
N19-2018,3,"in future work, we would like to investigate its effectiveness and robustness in a cross-lingual setting."
N19-2019,1,"as a future research direction for conversational ai, we think that to train and test a k-nn model for predicting which dialog move to take will be beneficial as well."
N19-2020,2,"we share the challenges of annotating a user reported bug dataset with non-technical annotators, as opposed to using annotations from engineers."
N19-2023,2,"in the future, the proposed approach could be applied to other dissimilar language pairs, e.g. english and chinese."
N19-2023,1,other possible extensions include using multi-lingual embeddings that could complement the currently transferred weights.
N19-2025,5,"in future work, we plan to address this through several directions, including end to end de-id (ghannay et al., 2018), lattice-based techniques (ladhak et al., 2016), and diarization and segmentation of the audio as part of the transcription process (cerva et al., 2013)."
N19-2025,1,"in future work, we plan to address this through several directions, including end-to-end de-id (ghannay , 2018), lattice-based techniques (ladhak , 2016), and diarization and segmentation of the audio as part of the transcription process (cerva , 2013)."
P00-1004,1,we expect further improvement by assigning translation scores according to corpus statistics.
P00-1004,6,this will be the main focus for future work.
P00-1005,1,and research on restoring the necessary information in english sentence has to be done.
P00-1005,1,"in the future, it is needed to use the syntactic collocational information of korean to reduce ambiguities in pattern matching step."
P00-1006,1,"finally, i suggest that it may be fruitful to explore the idea of using a memd model for p (w i h, s) as an alternative to the noisy-channel approach to smt."
P00-1007,1,"in a future work, we plan to use the system for providing instance candidates, and disambiguate them using an algorithm more suitable for handling lexical information."
P00-1010,3,"we also hope to handle a wider class of time expressions, as well as further improve our extraction and evaluation of event chronologies."
P00-1010,1,"in the future, we expect to improve the integration of various modules, including tracking the temporal focus in the time resolver, and interaction between the event order and the event-aligner."
P00-1012,1,"furthermore, any realistic dialog system would make use of some limited vocabulary  for which semantic information would be available."
P00-1012,1,"first, while semantic information is not available for all adjectives, it is clearly available for some."
P00-1012,1,future work will pursue at least two directions for improving the results.
P00-1014,1,a promising approach considers the mutual information between the prepositional relationship of candidate attachments and n2.
P00-1015,5,these experiments triggered an interesting future research challenge: how to cluster certain basenp rules into certain identifiers so as to improve the precision of both basenp and pos tagging.
P00-1021,1,"we showed how a set of importance scores and inference rules can be used as the basis for agents with different discourse strategies, and how the discourse control techniques of interruption, abbreviation, repetition and silence can be used not just to moderate the discourse of an individual agent, but also the interaction between agents."
P00-1024,5,"in addition, the difficulty of this problem depends on the number of attributes available for describing an object in the domain; our nominal expression generator has to correctly make four different decisions to achieve an exact match to human performance."
P00-1024,3,"in future work, we plan to perform similar experiments on different corpora with different communications settings and problem types (e.g.planning, scheduling, designing) to determine whether our findings are specific to the genre of dialogues that we examine here, or whether they are more general."
P00-1030,1,"our results show that, of all the collocation measures we investigated, bigram word predictability has the strongest correlation with pitch accent assignment."
P00-1030,1,"however, our combined model performs best of all, suggesting that both contextual and non-contextual features of a word are important in determining whether or not it should be accented."
P00-1031,1,"compared to the baseline of system, our system gets approximate 30% error reduction."
P00-1032,1,the future research may include: pruning the approximate word matching result before they take part in the approximate segmentation.
P00-1038,3,"furthermore, using faqs allows us to assess the effectiveness of applying standard statistical learning machinery maximum-likelihood estimation, the em algorithm, and so on—to the qrs problem."
P00-1038,1,"although this work is meant as an opening salvo in the battle to conquer summarization with quantitative, statistical weapons, we expect in the future to enlist linguistic, semantic, and other non-statistical tools which have shown promise in condensing text."
P00-1041,1,this resulted in a working system that could simultaneously translate and summarize japanese documents.8 the performance of the system could be improved by improving either content selection or linearization.
P00-1050,5,"based on our clarified definition, we can easily see that the alignment problem is essentially the problem of bilingual word similarity."
P00-1051,1,"e.g., we might have a center of coherence, analogous to sidner’s discourse focus, and that can be realized indirectly; and a center of salience, similar to her actor focus, and that can only be realized directly."
P00-1051,1,we believe however more work is still needed to identify a completely satisfactory way of breaking up sentences in utterance units.
P00-1053,1,"by comparing the types of referring expressions for which vt and the stack-based model fail, we also show that vt provides a better model for determining dras."
P00-1058,1,"one, also suggested by (chen and vijay-shanker, 2000), is to group elementary trees into families and relate the trees of a family by transformations."
P00-1058,6,"as for future work, there are still possibilities made available by tag which remain to be explored."
P00-1058,1,another possibility is the use of multiplyanchored trees.
P00-1059,2,"since sparseness of data is a major problem, we intend to train the models on the new brown corpus of 30,000,000 words."
P00-1061,4,"furthermore, it is important to show that embased methods can be applied successfully also to other statistical parsing frameworks."
P00-1068,1,"the tasks to be achieved are: 1. to establish ontology of semantic relationship description, 2. efficient methodology for preparing the lexical items comprising semantic constraints, 3. to communicate semantic contexts and situations to the students through assisting reading the texts by way of bidirectionally linking the text words with an electronic dictionary, 4. to deal with anaphora."
P00-1069,4,"thus, the proposed method seems especially effective and useful for the languages for which a largescale sense-tagged corpus is not available yet."
P00-1070,3,we have analysed and measured the effects of applying pronominal anaphora resolution in qa systems.
P00-1070,4,the analysis of information referenced pronominally in documents has revealed to be important to tasks where high level of recall is required.
P00-1071,5,"it is not sufficient to classify only the types of questions alone, since for the same question the answer may be easier or more difficult to extract depending on how the answer is phrased in the text."
P00-1072,1,"if we make a matrix from any given information once, we can use the reduced matrix for estimating probability."
P00-1078,2,"to develop continuous speech recognition, a large-scale speech corpus is needed."
P01-1003,3,we will also be working on the evaluation of the word-error rate (wer) of the wsme model.
P01-1008,2,"in this paper, we presented a method for corpus based identification of paraphrases from multiple english translations of the same source text."
P01-1014,5,"the larger implication is that we begin to see that there are underlying discourse elements in essays that can be identified, independent of the topic of the test question."
P01-1015,1,"a revision based architecture might require synthetic events to “wake up” a module to do some more work, after it has finished its first pass."
P01-1017,1,we believe that the adaptation of prosodic information to parsing use is a worthy topic for future research.
P01-1017,5,we believe that the resulting text grossly underrepresents the useful grammatical information available to speech-recognition systems.
P01-1017,2,"first, we believe that information about rare or even truly unknown words would be useful."
P01-1017,5,"finally, we have noted two objections to immediate-head language models: first, they complicate left-to-right search (since heads are often to the right of their children) and second, they cannot be tightly integrated with trigram models."
P01-1017,5,the possibility of trigram-less language models makes the second of these objections without force.
P01-1023,3,we also are interested in further evaluating the technique in an unrestricted domain such as the wall street journal (wsj) with shallow semantics such as the wordnet top-category for each np-head.
P01-1023,1,"from our learned results, we have inferred placement constraints of the new information in relation to the previous plan elements without further interviews with experts."
P01-1024,1,"although we provided here an account specific to german, our framework intentionally permits the definition of arbitrary language-specific topologies."
P01-1026,1,"future work would include generating information associated with more complex interrogations, such as ones related to how and why, so as to enhance web based natural language understanding."
P01-1026,1,"in addition, we proposed a question answering system, which answers interrogative questions associated with what, by using a web-based encyclopedia as a knowledge base."
P01-1026,2,"in addition, when we used both resources, the performance was further improved."
P01-1027,1,we believe that by performing a rescoring on translation word graphs we will obtain a more significant improvement in translation quality.
P01-1028,5,"and similarly, whether the description based treatment of discourse parsing sketched in (duchier and gardent, 2001) could be used to generate discourse."
P01-1028,5,"in particular, we intend to investigate whether the dependency grammar presented in (duchier, 1999), once equipped with a semantics, could be used not only for parsing but also for generating."
P01-1031,5,even choosing among the responses in (5) might be a pretty knowledge intensive business.
P01-1034,1,"as a final stage, we may find it useful to follow kasper (1999) and have a ‘fallback’ strategy for failed parses where the best partial analyses are assembled in a robust processing phase."
P01-1034,1,"moreover, we have not yet incorporated any domain specific lexical knowledge from, e.g., umls but we would expect this to contribute to improved performance."
P01-1034,2,"although we are only able to parse between 30 and 40 percent of the corpus, we will be able to improve on that figure quite considerably in the future through continued development of the pre-processing component."
P01-1035,3,another interesting issue is the evaluation method used for taggers.
P01-1040,5,"we are, therefore, at a point where the creation and use of annotated data and concerns about the way it is represented can be treated separately鉂憈hat is, researchers can focus on the question of what to encode, independent of the question of how to encode it."
P01-1041,4,we hope our method can be applicable to other languages.
P01-1043,1,"future improvements of the tool will consist in adding a module to annotate syntactic functions, and complete valency information for verbs, with the help of a lexicon (kinyon, 00)."
P01-1043,5,"finally, from a theoretical point of view, it may be interesting to see if our rules could be acquired automatically from raw text (although this might not be worth it in practice, considering the small number of rules we use, and the fact that acquiring the rules in such a way would most likely introduce errors)."
P01-1048,1,"our future research will focus upon combining these sources of information identifying system errors and user corrections, and investigating strategies to make use of this information, including changes in dialogue strategy (e.g.from user or mixed initiative to system initiative after errors) and the use of specially trained acoustic models to better recognize corrections."
P01-1050,1,"the second sentence is correctly translated only when the system uses a tmem seed; and fortunately, the translation of highest probability is the one obtained using the tmem seed."
P01-1053,4,future work is to apply our method to a variety of other languages.
P01-1056,3,"for this type of problem it is possible to evaluate the generator by the degree to which it matches human performance (yeh and mellish, 1997)."
P01-1057,3,"we hope that more such evaluations are performed in the future, and that their results are reported whether they are positive or negative."
P01-1059,3,"future directions could include improved sentential descriptions as well as further intrinsic and extrinsic evaluations of the summarizer as a whole (i.e., including canned text)."
P01-1060,3,"in collaboration with pranav anand and eric breck, we have incorporated governor markup in the question answering prototype, but not debugged or evaluated it."
P01-1060,4,this idea could be exploited in other markup tasks.
P01-1069,4,"furthermore, the success of regularized winnow in text chunking suggests that the method might be applicable to other nlp problems where it is necessary to use large feature spaces to achieve good performance."
P01-1070,4,"in the longer term, we plan to explore the use of coverage results to enable an enhanced qa system to compose an appropriate answer from information found in the retrieved documents."
P01-1070,1,"we also propose to investigate predictive models which return more informative predictions than those returned by our current model, e.g., a distribution of the probable informational goals, instead of a single goal."
P01-1070,1,we intend to use the insights obtained from this experiment to construct models which can capture probabilistic dependencies among variables.
P02-1001,1,we are particularly interested in the potential for quickly building statistical models that incorporate linguistic and engineering insights.
P02-1001,1,"bringing diverse models into the same declarative framework also allows one to apply new optimization methods, objective functions, and finite-state algorithms to all of them."
P02-1001,1,"for example, it should be possible to do end-to-end training of a weighted relation defined by an interestingly parameterized synchronous cfg composed with tree transducers and then fsts."
P02-1003,1,"in the future, it will first of all be necessary to lift the restrictions we have placed on the tag grammar: so far, the nodes of the elementary trees are only equipped with nonterminal labels and indices, not with general feature structures, and we allow only a restricted form of adjunction constraints."
P02-1003,1,"it should be possible to either encode these constructions directly in the dependency grammar (which allows user-defined features too), or filter out wrong realizations in a post-processing step."
P02-1004,5,it appears that much of the feature extraction and many of the linguistic operations are reusable.
P02-1013,5,"once disjunctive and negative relations are used, interesting questions arise as to how these should be realized."
P02-1013,5,"how should conjunctions, disjunctions and negations be realized within the sentence?"
P02-1013,1,one area that deserves further investigation is the relation to surface realisation.
P02-1014,1,"as noted above, for example, it is important to automate the precision-oriented feature selection procedure as well as to investigate other methods for feature selection."
P02-1014,1,"we also plan to investigate previous work on common noun phrase interpretation (e.g.sidner (1979), harabagiu (2001)) as a means of improving common noun phrase resolution, which remains a challenge for state-of-the-art coreference resolution systems."
P02-1014,6,"nevertheless, there is substantial room for improvement."
P02-1016,3,"we also want to test the algorithms developed here on other domains (e.g., wall street journal corpus)."
P02-1016,1,improving speed of sentence clustering is also worthwhile.
P02-1018,1,there are medications and variations on this algorithm that are worth exploring in future work.
P02-1019,1,a subject of future research is looking for a better way to combine the two error models or building a single model that can recognize whether there is a phonetic or typographic error.
P02-1020,1,we are adapting the gst algorithm to deal with simple rewrites (e.g. synonym substitution) and to observe the effects of rewriting upon finding longest common substrings.
P02-1021,2,"in the future, i am planning to test the assumption that abbreviations and their expansions occur in similar contexts by testing on hand-labeled data."
P02-1021,2,it will also be necessary to extend this approach to other medical and possibly non-medical domains with larger data sets.
P02-1021,1,"finally, i will experiment with combining the umls abbreviations table with the mayo clinic specific abbreviations."
P02-1021,1,i also plan to vary the size of the window used for determining the local context from two words on each side of the expression in question as well as the cutoff used during me training.
P02-1022,2,one future direction is the integration of processing resources which learn in the background while the user is annotating corpora in gates visual environment.
P02-1023,3,"for our future work, more experiments will be performed on other language models such as word-based bigram and trigram for chinese and english."
P02-1023,1,more pruning criteria and their combinations will be investigated as well.
P02-1026,5,"for example, sentences will differ in how much useful contextual information they carry."
P02-1026,5,are there useful generalizations to be made?
P02-1026,5,"e.g., might the previous sentence always be the most useful, or, perhaps, for newspaper articles, the first sentence?"
P02-1026,5,can these measurements detect such already established contextual relations as the given-new distinction?
P02-1026,5,what about other pragmatic relations?
P02-1026,5,All of these deserve further study.
P02-1027,1,"thus, the apparent ability of certain indicators which were developed for one class, and hence one type of thematic assignment, to become useful indicators for other classes seems to suggest that the inventory of thematic roles that we have explored here should be decomposed into finer-grained primitives."
P02-1027,1,"we are currently exploring that approach, along with our on-going investigation of other languages, and additional verb classes."
P02-1028,4,we also would like to apply the proposed method to a word selection task in machine translation.
P02-1028,4,we are planning to extend our dictionary based paraphrasing system to more complicated phrases and sentences.
P02-1029,3,"future work will concern the extension of the clustering experiments to a larger number of verbs, both for the scientific purpose of refining our understanding of the semantic and syntactic status of verb classes and for the more applied goal of creating a large, reliable and high quality lexical resource for german."
P02-1029,1,"for this task, we will need to further refine our verb classes, further develop the repertoire of syntactic frames which we use, perhaps improve the statistical grammar from which the frames were extracted and find techniques which allow us to selectively include such information about selectional preferences as is warranted by the availability of training data and the capabilities of clustering technology."
P02-1030,2,we would like to extend this analysis to at least one billion words for at least the most successful methods and try other tools and parsers for extracting the contextual information.
P02-1032,1,"for example, we would like to be able to interpret titles in terms of semantic relations, for example, transforming congenital anomalies of tracheobronchial branching patterns into a form that allows questions to be answered such as 鈥淲hat kinds of irregularities can occur in lung structure?鈥 we hope that by compositional application of relations to entities, such inferences will be possible."
P02-1037,3,future work will include evaluating performance when scores from the acoustic and/or n-gram models are incorporated for ranking competing candidate parses.
P02-1041,1,"in future work, we intend to combine techniques for building wide-coverage statistical parsers for ccg (hockenmaier and steedman, 2002; clark , 2002) with corpora that have explicitly marked semantic dependency relations (such as the prague dependency treebank and negra) to produce hlds terms as the parse output."
P02-1042,3,in future work we will present an evaluation which teases out the differences in extracted and insitu arguments.
P02-1045,2,"while we restricted ourselves in this work to rather small sets of labeled training data, future work on co-training will include further experiments with larger data sets."
P02-1049,1,"in future work, we hope to improve our results by trying different machine learning methods; including the user dialogue act types as input features; and testing these methods in new domains."
P02-1050,2,"to improve the quality of the input analyses, we are adapting active learning and co-training techniques (hwa, 2000; sarkar, 2001) to exploit the most reliable data."
P02-1051,2,we would like to apply to other languages such as chinese and japanese and to investigate whether the current algorithm would perform as well or whether new algorithms might be needed.
P02-1052,1,"it will be interesting to see how the similarity and clustering method will work in conjunction with other word alignment algorithms, as the dictionary rebuilding algorithm is independent of the actual word alignment method used."
P02-1052,1,"furthermore, we plan to explore ways to improve the similarity scoring algorithm."
P02-1052,1,"in general, we hope to move automated dictionary extraction away from pure surface form statistics and toward dictionaries that are more linguistically motivated."
P02-1055,3,we therefore plan to experiment with a pos tagger and an attenuated words variant that use exactly the same word form information.
P02-1055,1,in addition we also want to pursue using the combined chunker and grammatical function tagger described here as a first step towards grammatical relation assignment.
P02-1058,3,we are analyzing the duc evaluation scores in the hope of suggesting improved and more stable metrics.
P02-1058,1,we would like to apply some compression techniques or use linguistic units smaller than sentences to improve our retention score.
P03-1003,1,"however, building dedicated systems that employ more sophisticated, qa-motivated generative stories is likely to yield significant improvements."
P03-1003,4,"it is remarkable that a statistical machine translation system can do so well in a totally different context, in question answering."
P03-1007,2,further research is warranted to improve the results further.
P03-1008,1,"in the future, we will experiment with combining grammatical features and local/topical cooccurrences."
P03-1009,3,"in the future, we plan to investigate the use of soft clustering (without hardening the output) and develop methods for evaluating the soft output against polysemous gold standards."
P03-1010,6,it has attracted the attention of people both inside and outside the nlp community.
P03-1010,4,"it is therefore reasonable to expect that they can be applied to any language pair and still retain good performance, particularly since their effectiveness has been demonstrated in such a disparate language pair as japanese and english."
P03-1013,2,testing our sister-head model on these languages is a topic for future research.
P03-1013,4,such annotation schemes are often used for languages that (unlike english) have a free or semi-free wordorder.
P03-1013,4,it can be hypothesized that this finding carries over to other treebanks that are annotated with flat structures.
P03-1016,1,our future work will extend synonymous expressions of the collocations to words and patterns besides collocations.
P03-1018,1,"we hope that these preliminary aspects will be initial gains in developing a concrete and effective system for learning, representing and composing aspects of lexical meaning."
P03-1019,1,future work will include the automatic extraction of the bilingual grammar as well as the use of this grammar for the translation process.
P03-1021,5,the following important questions should be answered in the future: how many parameters can be reliably estimated using unsmoothed minimum error rate criteria using a given development corpus size?
P03-1021,5,we expect that directly optimizing error rate for many more parameters would lead to serious overfitting problems.
P03-1021,5,is it possible to optimize more parameters using the smoothed error rate criterion?
P03-1021,5,which error rate should be optimized during training?
P03-1021,5,this relates to the important question of which automatic evaluation measure is optimally correlated to human assessment of translation quality.
P03-1023,1,"in our future work, we intend to adopt a looser filter together with an anaphor city determination module (bean and riloff, 1999; ng and cardie, 2002b)."
P03-1023,1,"furthermore, we would like to incorporate more syntactic features into our feature set, such as grammatical role or syntactic parallelism."
P03-1023,1,"only if an encountered np is determined as an anaphor, we will select an antecedent from the candidate set generated by the looser filter."
P03-1023,1,these features may be helpful to improve the performance of pronoun resolution.
P03-1027,4,"our early exploration of the application of this work for corpus analysis (u.s. state of the union addresses) has produced interesting results, and we expect that the continued development of this resource will be important to the success of future corpus analysis and human-computer interaction projects."
P03-1029,1,"in particular, we would like to relax the restraint that all the fills must be tagged with their proper ne tags by introducing a generic place-holder into the extraction patterns."
P03-1029,1,there are several ways in which our pattern model may be further improved.
P03-1029,1,also patterns with a generic place-holder can be applied to slots that are not names.
P03-1030,3,"in future work, we plan to evaluate the impact of the hellinger metric on recall."
P03-1030,1,"in addition, we plan to use anaphora resolution which was shown to improve recall (pirkola and jrvelin, 1996) to enhance the ned system."
P03-1031,1,"these include the use of statistical information other than the probability of a dialogue act type sequence and the collocation probability of dialogue states and dialogue acts, the optimization of weighting factors α, β, γ, other default parameters that we used in the experiments, and more experiments in larger domains."
P03-1031,5,there still remain several issues that we need to explore.
P03-1031,1,"despite these issues, the present results have shown that our approach is promising."
P03-1038,1,"because all the markov models are represented as decision trees in the framework, the models are hu man readable and we are planning to develop editing tools for self-organizing markov models that help experts to put human knowledge about language into the models."
P03-1038,1,"by adopting x2-test as the criterion for potential improvement, we can control the degree of context extension based on the confidence level."
P03-1040,1,"the next step would be verb clauses, where modeling of the subcategorization of the verb is important."
P03-1040,4,our long term goal is to address additional syntactic constructs in a similarly dedicated fashion.
P03-1046,5,"future work will address the question whether these models can be run with a less aggressive beam search strategy, or whether a different parsing algorithm is more suitable."
P03-1050,2,"we are planning to experiment with different languages, translation model alternatives, and to extend task-based evaluation to different tasks such as machine translation and cross-lingual topic detection and tracking."
P03-1053,1,"future work: though the ldd has been validated against childes data in certain respects, we intend to extend this work by adding distributions to the ldd that correspond to actual distributions of child-directed speech."
P03-1056,1,"in addition to further pcfg refinements, tuning the dependency model may lead to improved performance."
P03-1056,1,"for the future, we believe that there is still room for considerable improvement in ctb parsing under our model."
P03-1060,1,"in the future, we plan to extend the context to improve the performance."
P03-1062,3,"moreover, we plan to investigate the perceptual cost of false insertions and deletions of accents and breaks in experiments with human listeners."
P03-1065,3,"in future research, we plan to extend the successful experiment on phrasal verbs to other types of multi-word expressions and idioms using the same expert lexicon formalism."
P03-1066,1,correlating the accuracy of the dependency parser as a parser vs. its utility in cer reduction may suggest a useful direction for further research.
P03-1066,1,"in particular, as discussed in section 6, syntactic dependency structure is believed to capture useful information for informed language modeling, yet further improvements may be possible by incorporating non-syntax-based dependencies."
P03-1066,6,there are many possibilities for future improvements.
P03-1070,1,one of the most important future directions is to establish a more comprehensive model of face to-face grounding.
P03-1071,1,"in future work, we would like to investigate the effects of adding prosodic features, such as pitch ranges, to our segmenter, as well as the effect of using errorful speech recognition transcripts as opposed to manually transcribed utterances."
P03-2002,1,the next step in our project is the selection of relevant words given the concepts annotating them and the topic segments where they appear.
P03-2002,1,selection will be based on a combination of a probabilistic model taking into account the probability of observing a concept given a word and the probability of observing that concept given a relevant topic.
P03-2004,3,"in particular, we want to use semantic plausibility to rescore/rerank n-best recognition hypotheses."
P03-2004,1,first we want to try to further improve the results presented in this paper by using better optimization methods for the machine learners (e.g. cross-validation optimization to avoid over-fitting on the development data).
P03-2004,5,we also want to do a more thorough investigation of the rule sets generated by ripper to find out which features were most important for classification.
P03-2004,1,"for example, we can add the words in the recognition hypothesis as a set-valued feature when using ripper."
P03-2004,1,further improvement of the results might also be achieved by considering other features for prediction.
P03-2004,1,a long-term goal is to combine the (acoustic) quality prediction with a notion of semantic plausibility in an actual dialog system.
P03-2004,6,Future work aims in two directions.
P03-2005,1,future work will focus on user adaptation and on the user interface to make best use of mmif.
P03-2006,1,"we also plan to develop the approach by using iteration of our non-local relations extraction algorithm, i.e., by running the algorithm, inserting the found non-local dependencies, running it again etc., until no new dependencies are found."
P03-2008,1,"our future work is to improve a model, corpus etc.to improve the ranked retrieval for structured texts."
P03-2009,2,"ongoing work involves seeing how accurately a new corpus can be tagged with discourse chunks, even when the da tags are unknown."
P03-2010,1,"in the future, we will study the korean time adverbials with mane and zero particle."
P03-2010,1,the first temporal marker is believed to signal the telicity of the event and the second appears very frequently in informal discourses.
P03-2011,1,future work will explore the use of the contextual information of the unknown words and the contextual information of the lexicons in the predicted category of the unknown words to boost predictive power.
P03-2015,3,we are currently conducting even more detailed experiments to demonstrate the usefulness of a spoken dialogue interface for television control and to examine problem areas.
P03-2021,3,we plan to conduct a user-based evaluation of the system to compare users鈥 satisfaction with both the automatically generated summaries and summaries produced by ineats.
P03-2021,1,we plan to extend the system by adding temporal visualization that places the documents on a timeline based on the date and time values extracted from the text.
P03-2022,2,future work could focus on texts in fields other than sports that is used in this paper
P03-2025,1,"ongoing research is focused on the integration of other linguistics-based techniques and combination to transliteration for katakana, the special phonetic alphabet to japanese language."
P03-2029,2,we plan to construct an algorithm for an automatic pattern acquisition from large scale corpora based on those biological approaches.
P03-2031,2,"in the future, we plan to apply more sophisticated natural language processing schemes for automatic generation of more accurate ne tagged corpus."
P03-2033,1,"words, pos/labels and features of the subsentences can be clues to infer the causes of parsing errors."
P03-2033,1,"in the future, we will try to modify willex to infer causes of parsing errors (semi-)automatically."
P03-2033,5,"it is difficult to find a point of parsing failure automatically, because subsentences that have no correspondent partial results are not always the failed point."
P03-2033,1,"hence, we will expand willex to find the longest subsentences that are parsed successfully."
P03-2034,1,in the future we will continue tightening the integration of the components of the system and port the interface to phones and palm or pocket pc devices.
P03-2036,3,we will also conduct experiments on trade-offs between the degree of cf approximation and the size of approximated cfgs as in maxwell iii and kaplan (1993).
P03-2036,1,we are going to integrate the advantage of the cf approximation of hpsg into that of ltag in order to establish another cfg filtering for ltag.
P04-1001,5,our future work will address how to extend this approach to optimize the overall interpretation of user multimodal inputs.
P04-1004,1,we also plan to investigate the interaction of modal verbs with the argumentative structure of the proof.
P04-1005,1,it would also be interesting to combine this probabilistic model of speech repairs with the word classifier approach of charniak and johnson (2001).
P04-1005,1,"still, more sophisticated models may yield better performance."
P04-1006,1,"improving the quality of the parses selected by the first stage should reduce the need for generating such a large number of candidates prior to pruning, improving efficiency as well as overall accuracy."
P04-1006,1,"we believe that attention shifting, or some variety of this technique, will be an integral part of efficient solutions for word-lattice parsing."
P04-1009,5,future research will address issues of graceful recovery from recognition error.
P04-1009,3,we plan to compare the efficacy of our language models built from simulated data with those trained from real user data.
P04-1009,1,we believe that the framework of using simulated dialogs possibly with synthesized speech input augmented with controlled levels of additive noise can be an effective way to develop and evaluate error recovery strategies.
P04-1009,1,however further work is needed to incorporate greater configurability to the dialog flow.
P04-1009,1,increased flexibility for customizing the model of the dialog is needed to enable the software to be applied to the development of other kinds of dialog systems.
P04-1010,2,"for advanced speech recognition, we hope to train our asr on new acoustic data."
P04-1010,4,we plan to port the system to a new domain: from telephone banking to information-technology support.
P04-1010,1,"we also plan to expand our dialogue act classification so that the system can recognize more types of acts, and to improve our classification reliability."
P04-1012,3,future work will focus on analyzing the data collected through the evaluations of the complete athosmail system with real users.
P04-1012,1,another future research topic is to apply machine-learning and statistical techniques in the implementation of the user expertise model.
P04-1012,2,through the user studies we will also collect data which we plan to use in re-implementing the dasex decision mechanism as a bayesian network.
P04-1014,1,future work will investigate extending the feature sets used by the log-linear models with the aim of further increasing parsing accuracy.
P04-1015,1,"first, we will look to include more useful features that are difficult for a generative model to include."
P04-1015,1,"secondly, combining with the generative model can be done in several ways."
P04-1015,6,Future research will look in two directions.
P04-1017,5,we would like to investigate the influence of the coreferential factors on general np reference resolution in our future work.
P04-1017,1,"in fact, the coreferential information of candidates is expected to be also helpful for non-pronoun resolution."
P04-1019,1,"current and future work will also include incorporating the methods tested here in an actual anaphora resolution system, the guitar system (poesio and alexandrov-kabadjov, 2004)."
P04-1019,1,"we are also working on methods for automatically recognizing bridging descriptions, and dealing with other types of (non-associative) bridging references based on synonymy and hyponymy."
P04-1021,4,we expect to see the proposed model to be further explored in other related areas.
P04-1022,4,"in future work, we are interested in extending our method to solving the problem of noncompositional collocation translation."
P04-1022,1,we are also interested in incorporating our triple translation model for sentence level translation.
P04-1023,5,we plan to investigate whether it is feasible to use active learning to select which examples will be most useful when aligned at the word-level.
P04-1024,5,"lastly, we will explore whether the proposed romanji to kanji backtransliteration approach applies to other types of names such as place names and study the effectiveness of the approach for backtransliterating romanji names of chinese origin and korean origin to their respective kanji representations."
P04-1024,4,"based on the results of this study, our future work will involve testing the effectiveness of the current method in real clir applications, applying the method to other types of proper names and other language pairs, and exploring new methods for improving precision and recall for romanji name back-transliteration."
P04-1024,1,"to further improve precision and recall, one promising technique is fuzzy matching (meng et al, 2001) for dealing with phonological transformations in name generation that are not considered in our current approach (e.g., “matsuda” vs “matsuta”)."
P04-1025,1,"furthermore, we also intend to reuse the recognition of named entities to extract other, specific types of interactions between biological entities."
P04-1026,2,"finally, with the right types of corpora, the worth of the technique for actual application scenarios could be investigated."
P04-1026,1,"the next step in the investigation of linguistic profiling for this task should be a more exhaustive charting of the parameter space, and especially the search for an automatic parameter selection procedure."
P04-1026,1,another avenue of future research is the inclusion of even more types of features.
P04-1028,1,we plan to continue development of our proof-of-concept system to explore those areas.
P04-1029,2,further work on efficient implementations and data structures is therefore required.
P04-1030,2,"a speedup may be achieved, and additional training data could be used."
P04-1030,1,tuning of parameters using em has lead to improved wer for other models.
P04-1030,1,"an investigation into the relevant importance of each parameter for the speech recognition task may allow a reduction in the size of the parameter space, with minimal loss of recognition accuracy."
P04-1030,4,we encourage investigation of this technique for lexicalized head-driven lattice parsing.
P04-1032,3,"a more detailed evaluation is an important task for future research, but if our “net hypothesis” is true, a system that tests whether all outputs of a grammar are nets (or a formal “safety criterion” that would prove this theoretically) could be a useful tool for developing and debugging grammars."
P04-1033,2,"since our method depends on title words and keywords, we need additional studies about the characteristics of candidate words for title words and keywords according to each data set."
P04-1035,1,"directions for future research include developing parameterselection techniques, incorporating other sources of contextual cues besides sentence proximity, and investigating other means for modeling such information."
P04-1036,3,"in the future, we will perform a large scale evaluation on domain specific corpora."
P04-1036,2,"in particular, we will use balanced and domain specific corpora to isolate words having very different neighbours, and therefore rankings, in the different corpora and to detect and target words for which there is a highly skewed sense distribution in these corpora."
P04-1037,3,we would also like to perform additional validation of the learned secondary language sense inventory.
P04-1037,2,"In future work, we plan to investigate the use of additional monolingual context."
P04-1038,4,future work is to extend our coverage and to apply the semantic taxonomy and the same types of features to supervised wsd in chinese.
P04-1038,1,"since the experimental results suggest that a general semantic taxonomy and more constrained lexical sets are both beneficial for wsd tasks, we will develop automatic methods to build large-scale semantic taxonomies and lexical sets for chinese, which reduce human effort as much as possible but still ensure high quality of the obtained taxonomies or lexical sets."
P04-1039,3,an investigation into the feasibility of combining these different factors with the different attributes of the experimental conditions for salaam to automatically predict when the noisy training data can reliably replace manually annotated data is a matter of future work.
P04-1042,1,the second is to incorporate nonlocal dependency information into the category structure of cf trees.
P04-1042,1,"the third would be to incorporate nonlocal dependency information into the edge structure parse trees, allowing discontinuous constituency to be explicitly represented in the parse chart."
P04-1043,1,other studies may relate to the use of scf to generate verb clusters.
P04-1043,1,"in the future we plan to design other structures and combine them with scf, paf and standard features."
P04-1043,1,in this vision the learning will be carried out on a set of structural features instead of a set of flat features.
P04-1044,3,"future work points in two directions: first, integrating our methodology into working isu-based dialogue systems and determining whether or not they improve in terms of standard dialogue evaluation metrics (e.g. task completion)."
P04-1044,1,"second, it will be interesting to investigate the impact of different dialogue and task features for classification and to introduce a distinction between generic features that are domain independent and application-specific features which reflect properties of individual systems and application scenarios."
P04-1045,5,"finally, we will explore how the recognized emotions can be used to improve system performance."
P04-1045,1,"we continue to manually annotate itspoke data, and are exploring partial automation via semi-supervised machine learning (maeireizo-tokeshi , 2004)."
P04-1045,2,"further manual annotation might also improve reliability, as understanding systematic disagreements can lead to coding manual revisions."
P04-1045,2,"we are also expanding our feature set to include features suggested in prior dialogue research, tutoring-dependent features (e.g., pedagogical goal), and other features available in our logs (e.g., semantic analysis)."
P04-1047,4,we hope to be able to apply our lexical acquisition methodology beyond existing parse-annotated corpora (penn-ii and penniii): new text is parsed by our pcfg-based lfg approximations into f-structures from which we can then extract further semantic forms.
P04-1047,1,"currently, we are migrating the technique to spanish, which has freer word order than english and less morphological marking than german."
P04-1048,1,"in our future work, we will experiment with the more recent release of wordnet (2.0)."
P04-1048,1,"this version provides derivational morphology links between nouns and verbs, which will promote far greater precision in the linking of verb senses based on morphology than was possible in our initial implementation."
P04-1050,4,"moreover, although we consider the parameter configuration of centering used here a plausible choice, we intend to apply our methodology to study different instantiations of the centering parameters, e.g.by investigating whether “indirect realisation” reduces the classification rate for m.nocb compared to “direct realisation”, etc."
P04-1050,3,"In our future work, we would like to experiment with more metrics."
P04-1052,1,"in future work, we plan to use this algorithm as part of a system for generation from a database of user opinions on products which has been automatically extracted from newsgroups and similar text."
P04-1053,1,"in the future, we are planning to discover less frequent pairs of named entities by combining our method with bootstrapping as well as to improve our method by tuning parameters."
P04-1054,1,"the most immediate extension is to automatically learn the feature compatibility function c(vq, vr)."
P04-1054,1,a first approach might use tf-idf to weight each feature.
P04-1054,1,another approach might be to calculate the information gain for each feature and use that as its weight.
P04-1054,5,further investigation is also needed to understand why the sparse kernel performs worse than the contiguous kernel.
P04-1054,1,"it is worthwhile to characterize relation types that are better captured by the sparse kernel, and to determine when using the sparse kernel is worth the increased computational burden."
P04-1055,3,in the future we plan to assess additional relation types.
P04-1056,1,"regarding future work, a richer set of features for the local templates would likely improve performance."
P04-1056,1,"besides exploring improvements to loopy belief propagation that increase computational cost (yedidia , 2000), we intend to examine alternative approximate-inference methods."
P04-1058,5,obvious questions for future work arise: are these two techniques the best way to split non-homogeneous classes into homogeneous ones?
P04-1058,5,is there an optimal splitting?
P04-1063,1,"one possible approach would be to use a dynamic language model which adapts itself for a new domain by re-training itself on data sampled from the web (berger and miller, 1998)."
P04-1063,5,an interesting question that remains to be addressed is how we might deal with translations from a novel domain.
P04-1064,3,"further investigations are required to apply this technique on different association measures, and to measure the influence that onmf may have, eg on a phrase-based machine translation system."
P04-1065,1,there is still room to improve the rwth fsa toolkit.
P04-1066,3,thus the next step in this research must be to test whether the improvements in aer we have demonstrated for model 1 lead to improvements on task-based performance measures.
P04-1072,1,"in the future, our work is directed to fine tune this system and increase its capabilities towards processing questions of higher complexity."
P04-1074,1,we will try to automate or at least semi-automate feature selection process.
P04-1074,1,another future work worth investigating is temporal indicator clustering.
P04-1075,1,"furthermore, we will study how to overcome the limitation of the strategy 1 discussed in section 3 by using more effective clustering algorithm."
P04-1075,5,another interesting work is to study when to stop active learning.
P04-1080,1,it is necessary to incorporate these more structural information to improve the performance of word sense learning.
P04-1081,1,we can thus make use of the vast amounts of cheap unannotated data to augment the model presented in this paper.
P04-1081,2,"given the positive results, we plan next to combine large amounts of unsupervised data with reasonable smaller amounts of supervised data such as the senseval lexical sample."
P04-1081,1,earlier we mentioned that one of the promising advantages of kpca is that it computes the transform purely from unsupervised training vector data.
P04-1084,2,"in future work, we shall explore the empirical properties of gmtg, by inducing stochastic gmtgs from real multitexts."
P04-1085,5,"in future work, we plan to extend our inference process to treat speaker ranking (i.e. ap identification) and agreement/disagreement classification as a single, joint inference problem."
P04-1085,1,contextual information about agreements and disagreements can also provide useful cues regarding who is the addressee of a given utterance.
P04-1085,1,we also plan to incorporate acoustic features to increase the robustness of our procedure in the case where only speech recognition output is available.
P04-1086,1,"in the near future we would like to incorporate reliable acoustic information, controlling for individual speaker difference and also apply different discriminative sequence labeling techniques to pitch accent prediction task."
P04-1087,6,in future work we aim to extend our work in two directions.
P04-1087,1,"firstly, we will consider finer-grained classification tasks, such as learning whether a causal discourse marker introduces a cause or a consequence, e.g.distinguishing because from so."
P04-1087,1,"secondly, we would like to see how far our results can be extended to include adverbial discourse markers, such as instead or for example, by using just features of the clauses they occur in."
P04-2001,1,"in the future, the system will be modified to handle various term formations such as abbreviated form."
P04-2001,1,morphological structure analysis of words is also needed to use the morpheme level information.
P04-2001,4,finally we will apply the proposed methods to terms of other domains and terms in general domains such as wordnet.
P04-2003,1,the main directions for our future work are thorough evaluation of the model and optimization of the parameters.
P04-2005,3,we intend to carry out more extensive evaluation to further explore this new resource’s properties and potential.
P04-2007,1,"on the one hand, the theoretical basis of the manual verb classification suggests that, although the syntactic behavior of verbs is an important criteria for a semantic classification, other properties of the verbs should be taken into account."
P04-2007,3,"for the purpose of evaluation, the gold standard classification could also be organized in the form of similarity rankings, based on the distance between the verbs in the hierarchy."
P04-2007,1,"for this reason, a new approach could be envisaged for this task, in the direction of the work by (weeds and weir, 2003), by building rankings of similarity for each verb."
P04-2007,1,"therefore, the description of verbs could be further enhanced with features that reflect on meaning components and event structure."
P04-2007,3,"the incorporation of name entity recognition in the experiments reported here is a first step in this direction, but it is probably a too sparse feature in the data to make any significant contributions."
P04-2007,3,"then, the rankings for each verb could be evaluated."
P04-2008,3,"in the future, i need to evaluate the quality of the resulting scfs by manual analysis and by using the extended lexicons to improve parsing."
P04-2008,1,"i will investigate other clustering methods such as hierarchical clustering, and use other information for clustering such as semantic preference of arguments of scfs to have more accurate clusters."
P04-2009,1,"next, we will experiment with an algorithm (johnson, 2002) that can insert empty-category information into data from charniak’s parser, allowing replication of features that need this."
P04-2009,3,cross-validation experiments will be performed to negate the effects the small test set may cause.
P04-2010,2,"to examine whether the system would yield comparable results in unrestricted text, it needs to be trained on a more diverse and possibly larger corpus."
P04-2011,5,"in addition to reducing lexical ambiguity, it would be interesting to see if structural ambiguity can be reduced."
P04-2011,1,"an alternative approach is to use an extended model that assigns chunk tags and pos tags simultaneously, as was done for finite verb occurrence and pos tags in the current work."
P04-2011,4,another possible application is tagging of german.
P04-2011,1,"in this way, relations between pos tags and chunk tags can be modeled in both directions."
P04-2012,4,i also plan to apply these techniques to describe the morphologies of a variety of languages beyond english and spanish.
P04-2012,1,i intend to implement the search strategies outlined in this paper.
P04-3002,4,"second, we will use the alignment adaptation results in other applications."
P04-3002,1,"first, we will seek other adaptation methods to further improve the domain-specific word alignment results."
P04-3002,6,Our future work includes two aspects.
P04-3004,2,we could then analyze the data and find useful information for future research.
P04-3004,2,"we are also adding on the basic functions to include a log of user activities, which will record the users’ query behavior and their background."
P04-3012,5,it is part of our future work to confirm that machine learning techniques can really induce syntactic information from such a corpus.
P04-3015,4,"in our future work, we will use our approach for other parts of speech and other types of word."
P04-3015,3,"moreover, we will compare with current alternative approaches such as those based on sentence patterns."
P04-3016,1,results indicate that the classification module has to be improved.
P04-3016,2,"given the highly modular architecture of carpanta, adaptation to other languages has a very low cost of development, provided the required nlp tools are available."
P04-3017,1,one of the future works is to extend pas method to handle events in nominalized forms.
P04-3019,4,"in the future, we will apply our method to more types of collocations, to pave the way for more comprehensive applications."
P04-3022,2,"we believe our approach of combining many kinds of evidence can potentially scale better to problems (like ace), where we have a lot of relation types with relatively small amounts of annotated data."
P04-3023,1,a possible solution to this problem is to consider the shortest string on both sides and have “remainder strings” like we have remainder weights in the weighted case.
P04-3024,3,assessing the error introduced by this approximation is a topic for future work.
P04-3024,1,"in order to keep the computational cost low, we use an approximation instead of the exact kldivergence."
P04-3025,2,"further investigation using larger datasets is necessary for the purposes of fully exploiting topic information where it is available, but the present results suggest that this is a worthwhile direction to investigate."
P04-3026,2,by operating on a part-of-speech tagged corpus those sense distinctions that have an effect on part of speech can be taken into account.
P04-3028,4,"we will also try to address the limitation of noise in our co-training system, and generalize our solution to a corresponding corpus of human computer data (litman and forbes-riley, 2004)."
P04-3028,3,we will also conduct experiments comparing cotraining with other semi-supervised approaches such as self-training and active learning.
P04-3028,2,"in the future, we will verify the generalization of our results to other partitions of our data."
P05-1002,1,"we also plan to develop higher order crfs, using error-correcting codes to curb the increase in complexity."
P05-1002,1,"we plan to apply error-correcting coding to dynamic crfs, which should result in better modelling of naturally layered tasks, while increasing the efficiency and scalability of the method."
P05-1003,1,in future we intend to investigate cooperative training of lop-crf weights and the parameters of each expert in an expert set.
P05-1005,1,"in the future, we hope to improve on these results: instead of using wordnet unique beginners, using more natural semantic classes based on word usage would possibly improve the accuracy, and finding such classes would be a worthwhile area of research."
P05-1005,1,"as seen from our results, selecting correct similarity measure has an impact on the final outcome."
P05-1005,1,we hope to work on similarity measures that are more applicable in our task.
P05-1007,1,"we are pursuing new exciting directions in a new domain, that of basic data structures and algorithms."
P05-1007,5,"we are investigating what distinguishes expert from novice tutors, and we will implement our findings in an its that tutors in this domain."
P05-1008,4,the results reported here also make us think that a possible avenue for future work is to explore the issue of what types of problems the generalization induced by our framework (which will be discussed below) can be applied to.
P05-1011,1,"future work includes the investigation of such features, as well as the abstraction of lexical dependencies like semantic classes."
P05-1014,1,"finally, further investigation of combining the distributional and the co-occurrence pattern-based approaches over the web is desired."
P05-1015,4,"in the future, we would like to apply our methods to other scale-based classification problems, and explore alternative methods."
P05-1017,1,since the importance of each word consisting a gloss depends on its syntactic role.syntactic information in glosses should be useful for classification. another is active learning.
P05-1017,1,"to decrease the amount of manual tagging for seed words, an active learning scheme is desired, in which a small number of good seed words are automatically selected."
P05-1017,1,One is the incorporation of syntactic information.
P05-1018,1,an important future direction lies in augmenting our entity-based model with lexico-semantic knowledge.
P05-1020,1,"previous attempts on bootstrapping coreference classifiers have only been mildly successful (e.g., m¨uller et al.(2002)), and this is also an area that deserves further research."
P05-1021,1,we believe that the semantic information under such a configuration would be even more effective on technical domains where neutral pronouns take the majority in the pronominal anaphors.
P05-1021,4,our future work would have a deep exploration on such domains.
P05-1025,1,"we are currently working on a framework for soft-labeling of grs, which will allow us to manipulate the precision/recall trade-off as discussed in (carroll and briscoe, 2002)."
P05-1028,2,"we will also extend our work to other kinds of information graphics such as line graphs and pie charts, and to complex graphics, such as grouped and composite bar charts."
P05-1028,5,we currently limit ourselves to recognizing what appears to be the primary communicative intention of an information graphic; in the future we will also consider secondary intentions.
P05-1029,2,we also intend to implement multilogue protocols in clarie so it can simulate multilogue.
P05-1029,3,we will then evaluate its ability to process nsus from the bnc.
P05-1029,3,"we plan to carry out more detailed work, both corpus–based and experimental, in order to evaluate the status of mag and, correspondingly to assess just how local acceptance and grounding interaction really are."
P05-1030,2,"in current work, we are studying data collected in a wizard-of-oz study in a multi-modal setting, in order to study clarification behavior in multi-modal dialogue."
P05-1031,1,"in future work, we will try a modified approach, where the detection of fragments is integrated with a classification of utterances as backchannels, fragments, or full sentences, and where the antecedent task only ranks pairs, leaving open the possibility of excluding a supposed fragment by using contextual information."
P05-1031,1,"lastly, we are planning to integrate our classifier into a processing pipeline after the pronoun resolution step, to see whether this would improve both our performance and the quality of automatic meeting summarisations.9"
P05-1033,2,"our primary goal for the future is to move towards a more syntactically-motivated grammar, whether by automatic methods to induce syntactic categories, or by better integration of parsers trained on annotated data."
P05-1033,1,"in any case, future improvements to this system will maintain the design philosophy proven here, that ideas from syntax should be incorporated into statistical translation, but not in exchange for the strengths of the phrase-based approach."
P05-1034,1,"following up on ideas introduced by (cherry &amp; lin, 03) we plan to explore ways to leverage the dependency tree to improve alignment quality."
P05-1036,2,"however, more training data is always the easiest cure to statistical problems."
P05-1036,2,if we can find much larger quantities of training data we could allow for much richer rule paradigms that relate compressed to original sentences.
P05-1036,1,one example of a rule we would like to automatically discover would allow us to compress all of our design goals or in the limit such rules blur the distinction between compression and paraphrase.
P05-1037,1,"in future work, we need to prioritize information according to the perceived knowledgeability of each participant in the discussion, in addition to identifying informative content and recognizing dialogue structure."
P05-1038,2,"in future work, we plan to test this prediction for korean, a flexible word order language whose treebank (penn korean treebank) has a non-flat annotation."
P05-1039,2,"because of the strong impact of pos tagging on parsing results, we conjecture that increasing pos tagging accuracy may be another fruitful area for future parsing research."
P05-1039,1,"furthermore, while pos tagging is highly accurate, the error analysis also shows it does have surprisingly large effect on parsing errors."
P05-1045,4,"in particular, it could in the future be applied to statistical parsing."
P05-1049,1,in the future we may extend the evaluation of lp algorithm and related cluster assumption based algorithms using more benchmark data for wsd.
P05-1049,1,another direction is to use feature clustering technique to deal with data sparseness and noisy feature problem.
P05-1051,1,"also, as information extraction is extended to capture cross-document information, we should expect further improvements in performance of the earlier stages of analysis, including in particular name identification."
P05-1052,2,"to deal with sparse data, we can also use deeper text analysis to capture more regularities from the data."
P05-1052,2,one solution is to use general purpose corpora to create clusters of similar words; another option is to use available resources like wordnet.
P05-1052,1,we can explore other kernel properties to integrate the existing syntactic kernels.
P05-1053,1,"in the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research."
P05-1053,5,"therefore, it would be interesting to see how imperfect edt affects the performance in relation extraction."
P05-1054,3,future work will examine empirical differences in other features such as dialog acts or turntaking.
P05-1056,1,"another future direction is to investigate how to effectively incorporate prosodic features more directly in the maxent or crf framework, rather than using a separate prosody model and then binning the resulting posterior probabilities."
P05-1056,1,"to improve su detection results on the stt condition, we plan to investigate approaches that model recognition uncertainty in order to mitigate the effect of word errors."
P05-1056,1,"in future work, we will examine the effect of viterbi decoding versus forward-backward decoding for the crf approach, since the latter better matches the classification accuracy metric."
P05-1061,1,we also plan on running these algorithms on more data sets to test if the algorithms empirically generalize to different domains.
P05-1061,1,the binary relation classifier we employ is quite simplistic and most likely can be improved by using features over a deeper representation of the data such as parse trees.
P05-1061,1,"other more powerful binary classifiers should be tried such as those based on tree kernels (zelenko et al., 2003)."
P05-1061,6,"as for future work, there are many things that we plan to look at."
P05-1062,1,"as our future works, we will apply fsa or learned rules to improve the precision and recall of some personal detailed information (such as zip code and mobile)."
P05-1062,1,we hope to continue this work in the future by investigating the use of other well researched ie methods.
P05-1063,1,"in addition, we plan to explore the alternative parameter estimation methods described in (roark , 2004a; roark , 2004b), which were shown in this previous work to give further improvements over the perceptron."
P05-1063,1,future work will include a further investigation of parser– derived features.
P05-1064,1,"in monolingual spoken document categorization, we suggest that the semantic domain can be characterized by latent phonotactic features."
P05-1065,2,"further experiments are planned on the generalizability of our classifier to text from other sources (e.g. newspaper articles, web pages); to accomplish this we will add higher level text as negative training data."
P05-1065,2,"we also plan to test these techniques on languages other than english, and incorporate them with an information retrieval system to create a tool that may be used by teachers to help select reading material for their students."
P05-1065,1,"future work includes testing additional classifier features, e.g. parser likelihood scores and features obtained using a syntax-based language model such as chelba and jelinek (2000) or roark (2001)."
P05-1067,1,future work includes a full-fledged version of sdig and a more sophisticated mt pipeline with possibly a tri-gram language model for decoding.
P05-1068,2,"in the near future, we will try to solve the data sparseness problem and to increase the coverage and accuracy of verb-noun collocations."
P05-1077,1,we hope that randomized algorithms will make other nlp tools feasible at the tera scale and we believe that many algorithms will benefit from the vast coverage of our newly created noun similarity list.
P05-2001,1,"first, it will be useful to identify a statistical model that achieves higher precision for disyllabic words, as this seems to be the bottleneck."
P05-2001,1,"it will also be relevant to apply advanced statistical models that can incorporate various useful information to this task, e.g., the maximum entropy model (ratnaparkhi, 1996)."
P05-2001,2,"second, for better evaluation, it would be helpful to use a larger corpus and evaluate the individual models on a held-out dataset, to compare our model with other models on more comparable datasets, and to test the model on other logographic languages."
P05-2001,5,"finally, as part of a bigger project on chinese unknown word resolution, we would like to see how well the general methodology used and the specifics acquired in this task can benefit the identification and sense-tagging of unknown words."
P05-2001,1,"third, some grammatical constraints may be used for the detection and correction of tagging errors in a post-processing step."
P05-2001,6,several avenues can be taken for future research.
P05-2003,2,"in the future, we will focus especially on improving quality of the training and testing data, employing other classification and attribute-selection techniques, and performing experiments on english data."
P05-2003,1,a necessary part of the work will be a rigorous theoretical study of all applied methods and appropriateness of their usage.
P05-2003,4,"finally, we will attempt to demonstrate contribution of collocations in selected application areas, such as machine translation or information retrieval."
P05-2004,1,"a promising approach is to learn the mappings using decision trees or random forests, which has recently achieved good results in a similar problem in language modeling (xu and jelinek, 2004)."
P05-2004,1,"finally, we plan to integrate the tagger/chunker in an end to-end system, such as a factored language model (bilmes and kirchhoff, 2003), to measure the overall merit of joint labeling."
P05-2004,1,"three directions for future research are planned: first, we will augment the fhmm such that its accuracies are competitive with state-of-the-art taggers and chunkers."
P05-2004,1,"this includes adding word features to improve accuracy on oov words, augmenting the context from bigram to trigram, and applying advanced smoothing techniques."
P05-2004,3,"second, we plan to examine the switching fhmm further, especially in terms of automatic construction of the α and q function."
P05-2008,2,"other extensions of this work are to collect more text marked-up with emoticons, and to experiment with techniques to automatically remove noisy examples from the training data."
P05-2008,3,in future work we will perform further tests to determine the nature of dependency in machine learning techniques for sentiment classification.
P05-2010,3,"we can now subject these putative chains to a direct test; in fact, this is the immediate future research direction."
P05-2016,1,"among these are discontinuous constituents, head switching, phrasal translation, english word stemming, and improved modeling of structural changes."
P05-2016,3,our first priority is to complete the final pieces so that we have an end-to-end system to experiment with.
P05-2016,3,"once we are able to evaluate our system output, our first priority will be to analyze the system errors and adjust the model accordingly."
P05-2016,5,"we recognize that our independence assumptions are generally too strong, and improving them is a hight priority."
P05-2016,1,"with this will come sparse data issues, so it will also be important for us to incorporate smoothing into the model."
P05-2016,3,there are many interesting subproblems which deserve attention and we hope to examine at least a couple of these in the near future.
P05-2018,1,we will also include centrality measures as sentence features in producing extractive summaries.
P05-2018,1,"to make this research more robust, we will include reference resolution into our study."
P05-2021,3,"in the near future, we would like to test our approach on agglutinative languages, where the problems with high oov are even more challenging."
P05-2021,1,We would also like to experiment with more complex language models.
P05-2024,1,we expect that the accuracy can be improved through further elaboration of the grammar design and disambiguation method.
P05-2026,1,"to further improve fluency, these could also be combined with a scoring function that takes longer-range dependencies into account, as well as penalizing extraneous content."
P05-2026,1,a weighted average of the dependency score with an n-gram model would already offer improvement.
P05-3003,1,"but the new chart data structure that utool computes is a more explicit packed representation of the possible readings, and still relatively small in practice."
P05-3003,6,thus it could open up avenues for more theoretical future research as well.
P05-3006,1,"to achieve automatic pattern generation, we will try to apply machine learning technique like the boosting algorithm."
P05-3006,3,"finally, we will compare with other systems which participated in trec by translating definitional questions of trec in korean."
P05-3006,1,our further works will concentrate on reducing human efforts for building descriptive patterns.
P05-3016,4,"in future, we are going to study translation qualities, prepare error-handling mechanisms for brittle ocr, mt and its combination, and explore new application areas of language computation."
P05-3018,1,"furthermore, we believe that the aggregate visualization tool will also help us uncover additional characteristics of potentially informative training examples."
P05-3021,2,we are also compiling more annotated data in order to provide more training data for machine learning approaches to tlink extraction.
P05-3021,1,"sputlink currently uses only qualitative temporal infomation, it will be extended to use quantitative information, allowing it to reason over durations."
P05-3021,3,"in the nearby future, we will experiment with more strategies to extract temporal relations from texts."
P05-3026,1,the focus of our future work will thus be on identifying features that support improved hypothesis scoring.
P05-3028,1,we work on support for queries like pairs of referring expressions that are a certain number of referring expressions apart.
P05-3028,1,"we also want to include wild cards and proximity searches, and support for automatic markable creation from query results."
P05-3031,1,"we plan to improve the performance by, for example, using larger amount of training examples."
P05-3031,1,finding other reformatting strategies in addition to the ones proposed in this paper is also important future work.
P06-1001,5,we are especially interested in the relationship between alignment and decoding and the effect of preprocessing scheme on both.
P06-1002,5,future work will involve an investigation into how the phrase extraction and scoring should be adjusted to take the nature of the alignment into account and how the phrase-table size might be reduced without sacrificing the mt output quality.
P06-1003,1,we are currently investigating the addition of non-lexical features as observed outputs in  our unsupervised generative model.
P06-1003,1,"we are also investigating improvements into the lexical model as presented here, firstly via simple techniques such as word stemming and replacement of named entities by generic class tokens (barzilay and lee, 2004); but also via the use of multiple asr hypotheses by incorporating word confusion networks into our model."
P06-1004,1,we will explore how the interaction between the generation and segmentation components can improve the performance of such a system as a whole.
P06-1004,2,our ultimate goal is to automatically generate tables of content for lectures.
P06-1004,1,we plan to investigate strategies for generating titles that will succinctly describe the content of each segment.
P06-1007,3,"in so doing we can find the places where the prediction problem faced both by the hspm and the machines that aspire to emulate it actually warrants the greater power of structurally sensitive models, using this knowledge to mine large corpora for future experiments with human subjects."
P06-1007,1,"one attractive future direction is to carry out simulations that compare the evolution of probabilities in the tagger with that in a theoretically more powerful model trained on the same data, such as an incremental statistical parser (kim , 2002; roark, 2001)."
P06-1008,1,this information will be validated by means of acceptability judgments acquired on the basis of a sparse sampling strategy.
P06-1008,1,our parser will associate a grammatical index to each sentence.
P06-1008,2,the next step of our work will be its validation on large corpora.
P06-1013,2,in the future we plan to integrate more components into our ensembles.
P06-1013,1,"increasing the number of components would allow us to employ more sophisticated combination methods such as unsupervised rank aggregation algorithms (tan and jin, 2004)."
P06-1013,1,"these include not only domain driven disambiguation algorithms (strapparava , 2004) but also graph theoretic ones (mihalcea, 2005) as well as algorithms that quantify the degree of association between senses and their co-occurring contexts (mohammad and hirst, 2006)."
P06-1014,1,"in a future work, we plan to investigate the contribution of coarse disambiguation to such real-world applications."
P06-1014,3,"to this end, we aim to set up an open mind-like experiment for the validation of the entire mapping from wordnet to ode, so that only a minimal error rate would affect the experiments to come."
P06-1015,1,"for the former, we plan to investigate the use of wordnet to automatically learn selectional constraints on generic patterns, as proposed by (girju et al.2006)."
P06-1015,1,there are many avenues of future work both in improving system performance and making use of the relations in applications like question answering.
P06-1015,5,we expect here that negative instances will play a key role in determining the selectional restrictions.
P06-1016,1,"in the future work, we will explore the hierarchical learning strategy using other machine learning approaches besides online classifier learning approaches such as the simple perceptron algorithm applied in this paper."
P06-1016,5,this will be done by integrating the relation extraction system with our previously developed ner system as described in zhou and su (2002).
P06-1016,5,"Therefore, it would be interesting to see how imperfect NER affects the performance in relation extraction."
P06-1017,1,"in the future, we would like to investigate more effective feature set or use feature selection to improve the performance of this graph-based semisupervised relation extraction method."
P06-1018,1,"further investigations must concern the computational properties of pugs, notably restrictions allowing polynomial time parsing."
P06-1020,2,"we also intend to use the turkish treebank (oflazer et al., 2003), as a resource to extract statistical information along the lines of frank et al.(2003) and o鈥橠onovan et al.(2005)."
P06-1020,2,"we also intend to use the turkish treebank (oflazer , 2003), as a resource to extract statistical information along the lines of frank (2003) and o鈥橠onovan (2005)."
P06-1020,2,"our current and future work involves extending the coverage of the grammar and lexicon as we have so far included in the grammar lexicon only a small subset of the root lexicon of the morphological analyzer, annotated with the semantic and subcategorization features relevant to the linguistic phenomena that we have handled."
P06-1022,3,"therefore, we want to examine the effectiveness by conducting the parsing experiment of long sentences in written language such as newspaper articles."
P06-1022,1,"after that, we plan to investigate techniques for identifying the dependency relations over clause boundaries."
P06-1022,1,future research will include making a thorough investigation into the relation between dependency type and the type of clause boundary unit.
P06-1022,4,"furthermore, as the experiment described in this paper has shown the effectiveness of our technique for dependency parsing of long sentences in spoken monologues, so our technique can be expected to be effective in written language also."
P06-1024,3,"we will also carry out a more direct comparison with the hybrid strategies learned by (henderson et al., 2005)."
P06-1024,5,we will continue to investigate whether we can maintain tractability and learn superior strategies as we add incrementally more high-level contextual information to the state.
P06-1024,3,"in the slightly longer term, we will test our learned strategies on humans using a full spoken dialogue system."
P06-1024,3,"cfs subset evaluation (rieser and lemon, 2006)) on in order to determine which contextual features this suggests are important."
P06-1024,1,we also intend to use feature selection techniques (e.g.
P06-1026,5,"in future work, we will address the use of data-driven dialog management to improve slu."
P06-1031,2,We will investigate how to exploit these sources of information in future work.
P06-1032,1,"in particular, we expect to be looking into alternative word alignment models and possibly enhancing our system鈥檚 decoder using some of the richer, more structured language models that are beginning to emerge."
P06-1034,3,"finally, while we believe that this technique will apply across domains, it would be useful to test it on domains such as movie reviews or product reviews, which have more complex domain ontologies."
P06-1034,1,"an alternative would be to leave some sentences unparsed and use them as templates with hybrid generation techniques (white and caldwell, 1998)."
P06-1035,2,"in further work, we plan to improve both the quantity and the quality of the data."
P06-1035,1,"likewise, we plan to develop the method for identifying cognates."
P06-1041,1,"another interesting goal of future work might be to even consider dynamic predictors, which can change their behavior according to text type and perhaps even to text structure."
P06-1044,4,"in addition, we plan to apply similar technology to other interesting domains (e.g. tourism, law, astronomy)."
P06-1044,1,"in the future, we plan to improve the accuracy of automatic classification by seeding it with domain-specific information (e.g. using named entity recognition and anaphoric linking techniques similar to those of vlachos et al.(2006))."
P06-1044,2,we also plan to conduct a bigger experiment with a larger number of verbs and demonstrate the usefulness of the bigger classification for practical bio-nlp application tasks.
P06-1045,2,"although the result also shows the possibility that the bigger the corpus is, the better the performance will be, the contents and size of the corpora we used are diverse, so their relationship, including the effect of the window radius, should be examined as the future work."
P06-1046,2,we intend to use this knowledge to process even larger corpora to produce more accurate results.
P06-1046,1,"having set out to improve the efficiency of distributional similarity searches while limiting any loss in accuracy, we are producing full nearestneighbour searches 18 times faster, with only a 2% loss in accuracy."
P06-1047,5,we are interested in the issue of how to improve an event representation in order to build a more powerful event-based summarization system.
P06-1047,1,we also want to see how concepts rather than sentences are selected into the summary in order to develop a more flexible compression technique and to know what characteristics of a document set is appropriate for applying event-based summarization techniques.
P06-1047,6,This would be one of our future directions.
P06-1048,2,"another important future direction lies in applying the unsupervised model presented here to languages with more flexible word order and richer morphology than english (e.g., german, czech)."
P06-1048,1,finding such a mechanism is an avenue of future work.
P06-1048,1,we would also like to enhance the word based model with more linguistic knowledge; we plan to experiment with syntax-based language models and more richly annotated corpora.
P06-1049,2,"a future direction of this study would be to explore the application of the proposed framework to more generic texts, such as documents without chronological information."
P06-1051,1,"in the future, we would like to study approaches to improve the computational complexity of our kernel function and to design approximated versions that are valid mercer’s kernels."
P06-1053,5,"in the future, we intend to explore the consequences of introducing lexicalization into the parser."
P06-1053,1,future work will also involve the use of smoothing to increase the benefit of priming for parsing accuracy.
P06-1054,1,"for future work, we will further improve the speed and accuracy of our models, and apply them to more chinese and multilingual natural language applications that require high speed and accurate parsing."
P06-1056,2,"in future work we plan to try different representations of the data, to use knowledge of the relations that exists between the partial cognate and the context words, and to run experiments when we iterate the mb and bb steps more than once."
P06-1057,5,"we speculate that there is a great potential for such approaches, and suggest that sense matching may become an appealing problem and possible track in lexical semantic evaluations."
P06-1058,1,"in future work, we will examine the effectiveness of ep-based method in other wsd techniques."
P06-1061,1,"in the future, we intend to design segment retrieval methods that do not require documents to be segmented before retrieval, hence avoiding the possibility of early-stage errors introduced from the text segmentation step."
P06-1061,1,a very promising idea is to adapt a naive bayes ie to perform redundant extractions directly on an entire document to retrieve filler-containing text segments for a segment hmm ie system.
P06-1064,5,"since the tiger corpus contains complete morphological and lemma information for all words, future work will address the question of how to identify and apply a set of (non-recursive) lexical rules (carpenter, 1992) to the extracted ccg lexicon to create a much larger lexicon."
P06-1064,2,"since tiger corpus is of comparable size to the penn treebank, we hope that the work presented here will stimulate research into statistical widecoverage parsing of free word order languages such as german with deep grammars like ccg."
P06-1066,1,"the future work is to investigate other valuable features, e.g. binary features that explain blocks from the syntactical view."
P06-1070,1,for the future we try to explore also the use of a word sense disambiguation all-words system.
P06-1073,1,"as future work, we plan to incorporate buckwalter morphological analyzer information to extract new features that reduce the search space."
P06-1075,5,there are rich features generated during the translation procedure.
P06-1075,5,this question is what we would like to answer in our future work.
P06-1075,5,will such features be helpful to clir?
P06-1076,5,further research is required to understand how well the abstract tested corpus-weighting schemes will perform in a full-text environment.
P06-1082,3,in particular we like to compare the proposed scheme with the famous ibm models.
P06-1082,2,we hope that with a much larger corpus size we shall be able to make the necessary comparisons in near future.
P06-1086,2,we will also investigate ways of using morphologically tagged corpora to assign weights to the arcs in the transducer so that the analyses returned by magead are ranked.
P06-1086,1,"in future work, we will investigate the derivation of words with morphemes from more than one variant (code switching)."
P06-1087,6,We plan to address this issue in future work.
P06-1088,1,"another possibility is to investigate whether similar techniques can improve other tagging tasks, such as named entity recognition."
P06-1088,5,in future work we will investigate maintaining tag ambiguity further down the language processing pipeline and exploiting the uncertainty from previous stages.
P06-1088,1,"in particular, we will incorporate real-valued pos tag and lexical category features in the statistical parsing model."
P06-1093,1,"we would also like to link our model to technical manuals, catalogs, etc. already available on the different topics in the given domain."
P06-1093,1,in future we would like to semantically cluster the topic specific information so that redundant topics are eliminated from the list.
P06-1093,1,"we can use automatic taxonomy generation(atg) algorithms for document summarization (kummamuru , 2004) to build topic taxonomies."
P06-1094,1,"future work will examine whether the model can be used to describe the semantics of nouns (such as corner) that express vague spatial extent, and how the model relates to the functional aspects of spatial reasoning."
P06-1095,2,"future research will investigate this effect further, as well as examine factors that enhance or mitigate this effect in different corpora."
P06-1097,1,additional feature functions will be also investigated that were proved successful for phrase-based models together with feature functions useful for a tree based modeling.
P06-1097,3,"as our future work, we are in the process of experimenting our model for other languages with rich resources, such as chinese and arabic, as well as similar language pairs, such as french and english."
P06-1097,4,we hope that minimum error / maximum likelihood training using the emd algorithm can be used for a wide diversity of tasks where there is not enough labeled data to allow supervised estimation of an initial model of reasonable quality.
P06-1100,1,the induction of conceptual instances has opened the way for many avenues of future work.
P06-1109,3,"since parsers are becoming increasingly important in applications like syntax-based machine translation and structural language models for speech recognition, one way to go would be to compare these different parsing methods by isolating their contribution in improving a concrete nlp system, rather than by testing them against gold standard annotations which are inherently theory-dependent."
P06-1109,5,"the initially disappointing results of inducing trees entirely from raw text was not so much due to the difficulty of the bootstrapping problem per se, but to (1) the poverty of the initial models and (2) the difficulty of finding theoryindependent evaluation criteria."
P06-1110,1,"lastly, we plan to give the model linguistically more sophisticated features."
P06-1114,1,we believe that these results suggest that current supervised machine learning approaches to the recognition of textual entailment may provide open-domain q/a systems with the inferential information needed to develop viable answer validation systems.
P06-1116,1,"a more sophisticated model of syntactically weighted vector space (pado and lapata, 2003) may help improve the lexical acquisition phase."
P06-1120,3,"in future work, we consider investigating other levels of the significance list, extending the evaluation to other languages, comparing against shallow-parsing methods instead of the window method, and performing recall-based evaluation as well."
P06-1120,5,our study finally clear the doubts on the usefulness of parsing for collocation extraction.
P06-1122,2,in future work we will investigate the use of linguistic resources to define feature sets for the mrf prior.
P06-1122,5,"lexical redundancy would ideally be addressed in the context of phrases, however, computation and statistical estimation may then be significantly more challenging."
P06-1124,3,"in the future we plan to study in more detail the differences between our model and the variants of kneser-ney, to consider other approximate inference schemes, and to test the model on larger data sets and on speech recognition."
P06-1124,1,"the hierarchical pitman-yor language model is a fully bayesian model, thus we can also reap other benefits of the paradigm, including having a coherent probabilistic model, ease of improvements by building in prior knowledge, and ease in using as part of more complex models; we plan to look into these possible improvements and extensions."
P06-1128,4,this framework will thus be applicable to other domains such as patent documents.
P06-1128,1,"for example, query expansion and relevancy feedback can be integrated in a straightforward way in order to improve accuracy."
P06-1130,1,"more research is required to implement such a model efficiently using packed representations (carroll and oepen, 2005)."
P06-1134,1,"second, a number of methods for subjectivity or sentiment analysis start with a set of seed words and then search through wordnet to find other subjective words (kamps and marx, 2002; yu and hatzivassiloglou, 2003; hu and liu, 2004; kim and hovy, 2004; esuli and sebastiani, 2005)."
P06-1134,1,"while much work remains to be done, this first attempt has proved the feasibility of correctly assigning subjectivity labels to the fine-grained level of word senses."
P06-1135,5,in particular we feel that much more attention should be paid to the problem of determining if two entities are the same (or “close enough”).
P06-1135,1,"the outcome is a re-ranking of the candidate answers, with the possible insertion of nil (no answer in corpus) as the top answer."
P06-1136,2,"in the future, we are interested in enhancing the centroid learning using human knowledge sources such as encyclopedia."
P06-1138,2,"a linguistic study on corpora might determine what types of elements are actually emancipated and in particular what types of elements can be emancipated simultaneously, i.e.what list of slashed element are possible, given that this is the main factor of complexity of the algorithm (see kiefer 1 for similar heuristic considerations for hpsg parsing)."
P06-1138,1,real values on efficiency will not be available as long as the grammar does not surpass experimental size.
P06-1140,3,"finally, we plan to examine whether gains in quality can be achieved with an off-the-shelf, general purpose voice that are similar to those we have observed using comic's limited domain voice."
P06-1140,5,"we also plan to investigate whether additional features derived from the synthesizer can better detect unnatural pauses or changes in speech rate, as well as f0 contours that fail to exhibit the targeting accenting pattern."
P06-1140,3,"in future work, we intend to verify the results of our cross-validation study in a perception experiment with naive subjects."
P06-1143,1,"pmt is an endeavor to bridge the ethnical, cultural and geographical divisions between the punjabi speaking communities."
P06-1144,2,"in addition, we will study if changing the order of the bilingual and monolingual comparison steps the performance varies significantly for different type of corpus."
P06-1146,1,"in future work, we will investigate minimal tree edit distance (bille, 2005) and related formalisms which are defined on tree structures and can therefore model divergences explicitly."
P06-2001,2,"nevertheless, we think the presented results for the basque language could be improved."
P06-2001,1,"finally, we contemplate building an icall (intelligent computer assisted language learning) system to help learners to put commas correctly."
P06-2001,5,"however, we have not obtained as good results as we hoped in the task of placing commas (we get a precision of 69.6% and a recall of 48.6%)."
P06-2001,3,"these results, anyway, confirm our hypothesis and our diagnosis of the detected problems."
P06-2002,1,"concerning future work, we are currently trying to improve the estimation of the patterns accuracy for the pruning step."
P06-2002,4,we also plan to apply the obtained patterns in a system for automatically generating biographical knowledge bases from various web corpora.
P06-2009,2,"our future work includes trying to generalize this work to non-projective dependency parsing, as well as attempting to incorporate additional sources of information (e.g., shallow parsing information) into the pipeline process."
P06-2009,1,"we have addressed the problem of using learned classifiers in a pipeline fashion, where a task is decomposed into several stages and stage classifiers are used sequentially, where each stage may use the outcome of previous stages as its input."
P06-2009,5,this is a common computational strategy in natural language processing and is known to suffer from error accumulation and an inability to correct mistakes in previous stages.
P06-2010,2,we can use general pur-pose corpus to create clusters of similar words oruse available resources like wordnet.
P06-2010,4,"we can also use the hybrid kernel method into other tasks, such as relation extraction in the future."
P06-2016,1,"further research in both experimental and theoretical aspects of this work is planned, specifically in the area of reconstructing hierarchies where recursive formations are present and formal analysis and testing of techniques."
P06-2018,1,one limitation of our method is the fact that it treats the classification task separately for each target node.
P06-2019,1,an appealing future direction is the incorporation of discourse-based constraints into our models.
P06-2019,2,"we plan to apply our method to languages with more flexible word order than english (e.g., german) and more challenging spoken domains (e.g., meeting data) where parsing technology may be less reliable."
P06-2020,1,we introduced an oracle score based upon the simple model of the probability that a human will choose to include a term in a summary.
P06-2021,4,"in future work we hope to extend this algorithm to provide a more efficient algorithmic implementation, and also to apply the algorithm in areas such as the machine translation of nounnoun compounds, where the identification of semantic relations in compounds is a crucial step in the translation process."
P06-2027,1,our approach features the use of corpus statistics derived from both lexical and syntactic analysis across documents.
P06-2030,1,"future work includes (i) extending the method to an incremental approach for extracting story pairs, (ii) comparing our clustering method with the other existing methods such as x-means(pelleg, 2000), and (iii) applying the method to the tdt4 for quantitative evaluation."
P06-2031,1,we are currently carrying out further work in the aspects of (1) improving the accuracy of source word frame identification and (2) incorporating bilingual frame semantics in a full fledged  machine translation system.
P06-2032,1,"we are currently exploring ways in which the xmg formalism could be extended to automatically enforce linguistically-based wellformedness principles such as for instance, a kind of head feature principle for tag."
P06-2034,1,"experimenting with other effective reranking algorithms, such as svms (joachims, 2002) and maxent (charniak and johnson, 2005), is also a direction of our future research."
P06-2034,1,"we also plan to explore other types of reranking features, such as the features used in semantic role labeling (srl) (gildea and jurafsky, 2002; carreras and m`arquez, 2005), like the path between a target predicate and its argument, and kernel methods (collins, 2002b)."
P06-2035,1,"therefore, it is possible to plug any external and exogenous component in our architecture to improve the overall quality."
P06-2037,1,we believe that coarse-grained is sufficient for the purpose of mt.
P06-2037,3,"a substantial increase is consistently obtained according to standard mt evaluation metrics, which has been shown to be statistically significant in the case of bleu."
P06-2042,2,"we will also study use of information on quotations and inserted clauses to text formatting, such as text summarization."
P06-2042,3,"in the future, we plan to solve the problems found in the experiments and investigate the robustness of our method when the without boundaries of quotations open 81.0% and inserted clauses closed 90.3% with boundaries of quotations and open 81.7% inserted clauses (automatically detected) closed 90.3% with boundaries of quotations open 82.8% and inserted clauses (correct) closed 91.3% results of automatic speech recognition are given as the inputs."
P06-2043,1,"however, approaches that are more complicated still exist theoretically, for instance, some scf types unseen by the hypothesis generator may be recalled by integrating semantic verb classification information into the system."
P06-2045,3,the evaluation is divided to test each of the two processes underlying the framework.
P06-2046,1,"what remains to be done is two things; one is to reveal all the subclasses of class c and all the disambiguation knowledge, and the other is to apply a machine learning technique to disambiguating those cases that the current technique is unable to handle, i.e., cases without visible evidence."
P06-2048,5,"first, the set of features we selected were chosen with simplicity in mind, to see how well a simple and unadorned set of features would work, given our probabilistic model."
P06-2048,6,we can extend this research in multiple directions.
P06-2050,5,"in conclusion, the goal of this research is set to survey the unique characteristics of chinese ideographs."
P06-2054,4,applying and extending our approach to other natural language tasks (which are difficult to apply a parser to) such as information extraction from e-mail data or biomedical named entity recognition is a topic of future work.
P06-2055,2,retrieving related articles from a large collection should increase the likelihood of finding a name instance with a disambiguating context.
P06-2055,2,"finally, our cross-document coreference is currently performed only within the (small) test corpus."
P06-2057,2,"while training, we tried to balance the selection somewhat, but applying the projection methods on other types of parallel corpora (such as novels available in both languages) may produce a better training corpus."
P06-2058,3,further experimentation is clearly needed in this area needs to address the impact of this interdependency.
P06-2058,3,an interesting experiment would be to explore how this approach applies to different types of corpora like email messages.
P06-2061,1,sia uses stochastic word mapping to allow soft or partial matches between the mt hypotheses and the references.
P06-2062,1,"for example, some russian rules only make sense as a part of language-specific modules while some rules that were considered universal at first are not directly applicable to russian."
P06-2062,1,syntactic types and some combination rules are more problematic.
P06-2066,1,"experimental evaluation based on data from two treebanks shows, that a combination of the wellnestedness constraint and parametric constraints on discontinuity (formalized either as gap degree or edge degree) gives a very good fit with the empirical linguistic data."
P06-2067,4,we have also shown that it is feasible to apply the current scf extraction technology to spoken language.
P06-2071,2,"in most corpora used to date for research on illustrated text, word sense is an entirely secondary phenomenon, whereas our data set was collected as to emphasize possible ambiguities associated with word sense."
P06-2071,5,"also, it remains an unsolved problem how to enumerate iconographic senses, and use them in manual annotation and classification."
P06-2071,1,"in particular, we intend to explore the notion of iconographic senses; surprisingly good results on image classification by (chapelle, haffner, and vapnik, 1999) using image features suggest that iconography plays an important role in the semantics of images."
P06-2071,1,"future work will involve learning the algorithms parameters without supervision, and develop a semantically meaningful image taxonomy."
P06-2071,5,"it is remarkable how high purity is, considering that we are using relatively simple image and text representation."
P06-2071,1,an important aspect is to enhance our understanding of the interplay between text and image features for this purpose.
P06-2073,4,"the application of our statistical models to other tasks (like verb mobil (alexanderson et al., 1998)) would allow us to confirm our conclusions and compare results with other works."
P06-2073,4,"the application of our statistical models to other tasks (like verbmobil (alexandersson , 1998)) would allow us to confirm our conclusions and compare results with other works."
P06-2075,1,"the main contribution of this paper is a novel integration of the pattern-based and distributional approaches for lexical semantic acquisition, applied to lexical entailment."
P06-2077,1,we will also explore a method for including instances tagged with question in training data by using the proposed method and bootstrapping.
P06-2077,1,it can reinforce almost any earlier method.
P06-2077,4,"even to handcoded rules, it can be applied as long as they give predictions with their confidences."
P06-2077,1,the second is that the proposed method is unsupervised.
P06-2077,1,This further gives an additional advantage.
P06-2082,2,"first, we shall continue to build dependency-analyzed corpora."
P06-2083,1,"a future direction of this study would be to incorporate other types of relations expressed with parenthesis such as synonym, paraphrase, etc."
P06-2085,1,"the next step towards a rl-based system is to add task-level and reward-level annotations to calculate reward functions, as discussed in (rieser , 2005)."
P06-2088,1,"to realize simultaneous translation our method utilizes the feature that word order is flexible in japanese, and determines the word order of a translation based on dependency structures and japanese dependency constraints."
P06-2089,1,this simple combination of the two models produces an f-score of 90.8% for the standard wsj test set.
P06-2090,1,"from a technical point of view, the inferential model presented in this paper is a simple starting point for reflection on a number of issues in automatic identification of genres in web pages."
P06-2090,1,"although parameters need a better tuning and text type and genre palettes need to be enlarged, it seems that the inferential approach is effective, as shown by the preliminary evaluation reported in section 4.3."
P06-2090,1,"more importantly, this model instantiates a theoretical characterization of genre that includes hybridism and individualization, and interprets these two elements as the forces behind genre evolution."
P06-2092,2,"if possible, we want to conduct experiments that involve further languages and additional kinds of corpus annotation, like e.g. detailed morphological information as annotated e.g. within the croco project (neumann and hansen-schirra, 2005)."
P06-2093,1,"another promising direction that we have not yet explored, is to build long-span lm, i.e. with n much greater than 4."
P06-2093,1,one could expect that the target language model plays a different role in a phrase-based system since the phrases induce some local coherency on the target sentence.
P06-2095,2,"furthermore, dictionaries also tend to translate words using the same pos."
P06-2095,1,"even if the procedure for producing similarity classes does not impose restrictions on pos properties, nevertheless words in the similarity class tend to follow the pos of the original word, because of the similarity of their contexts of use."
P06-2095,1,"currently we are working on an option to identify semantic contexts by means of semantic signatures obtained from a broad-coverage semantic parser, such as usas (rayson , 2004)."
P06-2102,1,"moreover, we are exploring the possibility of improving the gold standard clusters by examining the lexical semantic attributes of the msa verbs."
P06-2102,2,We are in the process of manually cleaning the English translations corresponding to the MSA verbs.
P06-2105,1,we plan to improve our logic prover to detect false entailments even when the two texts have a high word overlap and expand our axiom set.
P06-2106,2,"we start with three asian languages, chinese, japanese and thai, on top of the existing framework which was designed mainly for european languages."
P06-2108,4,"in other directions, we will extend it to other chinese nlp research topics, especially word segmentation, main verb identification and subject-verb-object (svo) auto construction."
P06-2108,5,"although there is room for improvement, we believe it would not produce a noticeable effect as far as the stw accuracy of poly-syllabic words is concerned."
P06-2108,2,"we will continue to improve our wsm to cover more characters of the udn2001 and the as corpus by those word-pairs comprised of at least one mono-syllabic word, such as “我們 (we)-是(are)”."
P06-2109,1,"we presented a maximum entropy model to extend the sentence compression methods described by knight and marcu (knight and marcu, 2000).our proposals are two-fold."
P06-2109,2,"to improve our approaches, we can introduce more feature functions, especially more semantic or lexical features, and to deal with these features, we need a larger corpus."
P06-2111,1,the last thing that remains for future work is to find a more adequate way to combine the  syntax-based and the alignment-based methods.
P06-2112,2,"first, we will further investigate the effect of the size of corpora on the alignment results."
P06-2112,1,"second, we will investigate different parameter combination of the induced model and the original model."
P06-2113,5,"we have also discussed our experience in directly porting a generative model to a conditional model, and demonstrated that it may not be beneficial at all if we still think generatively in conditional modeling; more specifically, replicating the feature set of a generative model in a conditional model may not help much."
P06-2114,4,the proposed g2p conversion mechanism will be useful in various applications in the speech domain.
P06-2114,5,"in this paper, the problem of sinhala graphemeto-phoneme conversion is addressed with a special focus on dealing with the schwa epenthesis."
P06-2116,1,we plan next to extend the model by incorporating ontological and linguistic knowledge for additional disambiguation leverage.
P06-2118,1,"we believe this represents a rich area of exploration and we intend to experiment with more verbs with further customization of features, including experimenting with automatic feature selection."
P06-2119,3,in the forthcoming work we will investigate their validity in the lexical task of senseval-3.
P06-2122,1,initializing the model with good dependency parameters is a possible adjustment.
P06-2122,1,we would also like to point out that alignment task is simpler than decoding where a stronger component of reordering is required to produce a fluent english sentence.
P06-2122,1,investigating the impact of bilexical dependencies on decoding is our future work.
P06-2123,1,"by setting the confidence threshold, r-oov and r-iv can be changed accordingly."
P06-3002,3,"to really judge the benefit of an unsupervised tagging system, it should be evaluated in an application-based way."
P06-3003,1,"in the future, we will try to improve the algorithm as well as perform more extensive evaluations on different language pairs."
P06-3005,1,"it is as yet unclear what the final form of a cognitively accurate model along these lines would be, but it is clear from our study that it is worthwhile, for the sake of clarity and explicit testability, to consider models that are simpler and more precisely specified than those assumed by dominant theories of human sentence processing."
P06-3005,1,"One attractive future direction is to carry out simulations that compare the evolution of probabilities in the tagger with that in a theoretically more powerful model trained on the same data, such as an incremental statistical parser (Wang , 2004; Roark, 2001)."
P06-3007,6,experiment showed that this idea achieved promising results.
P06-3007,1,"compared with close related work, we achieved encouraging improvement."
P06-3008,1,"we plan to get a larger team to work on the project, so as to make it more comparable to the english and german rst treebanks."
P06-3008,1,"since the distinctive nucleus status of eudas ended with these pms may be useful in deciding growth point for rs-tree construction or for tree pruning in summarization, we are also interested in testing how well a baseline relation classifier performs if it always predicts the most frequent relations for these pms."
P06-3009,1,"in the future we intend to develop more sophisticated models implementing closer interaction between morphology and syntax, by means of which we hope to boost parsing accuracy and improve morphological disambiguation."
P06-3010,2,"in future work we intend to carry out experiments with different settings: (a) combinations of certain kss; (b) other sample corpora, of different sizes, genres / domains; and (c) different parameters in aleph regarding search strategies, evaluation functions, etc."
P06-3015,1,"we are also currently working on scoring modules that incorporate language modelling (with discriminative training), and prosody-based co-analysis."
P06-3015,1,"second, the architecture lends itself to further parallelism-specifically by permitting concurrent processing units to dynamically decide whether to employ the generaliser or specifier, based on the sizes of shared active subspaces."
P06-3015,1,"first, applying the parser to directed two-party dialogue will explore contextsensitivity and a more complex grammar."
P07-1001,2,future work includes implementing the idea in alternative alignment models and also exploiting prior knowledge derived from such as manually-aligned data and pre-existing linguistic resources.
P07-1002,1,"in the future, we would like to explore tighter integration of our order model with the smt system and to develop more accurate algorithms for constructing projective target dependency trees in translation."
P07-1003,5,"while it remains to be seen whether these improvements impact final translation accuracy, it is reasonable to hope that, all else equal, alignments which better respect syntactic correspondences will be superior for syntactic mt."
P07-1005,1,"finally, besides our proposed approach of integrating wsd into statistical mt via the introduction of two new features, we could explore other alternative ways of integration."
P07-1005,4,"for future work, an immediate step would be for the wsd classifier to provide translations for longer chinese phrases."
P07-1005,1,"also, different alternatives could be tried to match the translations provided by the wsd classifier against the chunks of rules."
P07-1006,3,"we plan to further evaluate our approach for other sets of words, including other parts-of-speech to allow further comparisons with other approaches."
P07-1009,1,"qualitatively, our model can recover many well-known implications as well as many more potential implications that can be the object of future linguistic study."
P07-1014,1,"in the current study we have used the binary features provided in upsid, which could be very well replaced by other representations, including multi-valued feature systems; we look forward to do the same as a part of our future work."
P07-1017,1,"the most obvious next step of our research, therefore, is to further pursue the integration of the proposed model to the end to-end mt scenario."
P07-1017,1,"another area of investigation is capturing longer-distance agreement phenomena, which can be done by implementing a global statistical model, or by using features from dependency trees more effectively."
P07-1025,1,"an important area for future research will be to explore the correlation between our distance metric for syntactic similarity and various quantitative measures of semantic similarity (pedersen, et al., 2004)."
P07-1025,5,particularly interesting would be to explore whether different senses of a given verb exhibited markedly different profiles of syntactic context.
P07-1026,1,"the extension of our work is to improve the performance of the entire semantic role labeling system using the grammar-driven tree kernel, including all four stages: pruning, semantic role identification, classification and post inference."
P07-1026,5,"in addition, a more interesting research topic is to study how to integrate linguistic knowledge and tree kernel methods to do feature selection for tree kernel based nlp applications (suzuki et al., 2004)."
P07-1028,3,"next steps will be to test the similarity-based model “in vivo”, in an srl task; to test the model in a wsd task; to evaluate the model on a primary corpus that is not semantically analyzed, for greater comparability to previous approaches; to explore other vector spaces to address the coverage issue; and to experiment on domain transfer, using an appropriate generalization corpus to induce selectional preferences for a domain different from that of the primary corpus."
P07-1031,1,"firstly, nps with genuine flat structure are currently treated as implicitly right branching."
P07-1031,5,"secondly, there is still ambiguity in determining the head of a noun phrase."
P07-1031,5,there are several distinctions that our annotation currently ignores that we would like to identify correctly in the future.
P07-1031,1,"we would like to be able to identify these multi-head constructs properly, rather than simply treating them as a single entity (or even worse, as two different entities)."
P07-1033,1,the most important avenue of future work is to develop a formal framework under which we can analyze this (and other supervised domain adaptation models) theoretically.
P07-1033,1,an additional future direction is to explore the kernelization interpretation further: why should we use 2 as the similarity between domains—we could introduce a hyperparamter α that indicates the similarity between domains and could be tuned via cross-validation.
P07-1034,1,"the framework opens up many interesting future research directions, especially those related to how to more accurately set/estimate those weighting parameters."
P07-1035,1,"much remains to be done in applying infinite models to language structure, and an interesting extension would be to develop inference algorithms that permit completely unsupervised learning of dependency structure."
P07-1037,1,"we expect more work on system integration to improve results still further, and anticipate that similar increases are to be seen for other language pairs."
P07-1039,3,"as for future work, we first plan to consider different confidence measures for the filtering of the alignment candidates."
P07-1039,2,"finally, we would like to apply this method to other corpora and language pairs."
P07-1039,1,"we also want to bootstrap on different word aligners; in particular, one possibility is to use the flexible hmm word-to-phrase model of deng and byrne (2005) in place of ibm model 4."
P07-1040,1,better alignment methods which take synonymy into account should be investigated.
P07-1048,3,"in future, we hope to conduct a longer term study with repeat users to see how previous experience influences use of newer kinds of inputs such as multimodal and handwriting."
P07-1054,1,"also, we see this work as a proof of concept for the applicability of general random-walk algorithms (and not just pagerank) to the determination of the semantic properties of synsets."
P07-1056,1,thus we can use the adistance to select source domains to label which will give low target domain error.
P07-1056,1,"in the future, we wish to include some of the more recent advances in sentiment classification, as well as addressing the more realistic problem of ranking."
P07-1056,2,we are also actively searching for a larger and more varied set of domains on which to test our techniques.
P07-1063,3,"in future work, we hope to directly compare the direct generation method of section 6.1 with the over generate and rank method of section 6.2, and to use these results to refine personage鈥檚 parameter settings."
P07-1063,1,"research by andre (2000); piwek (2003) uses personality variables to affect the linguistic behaviour of conversational agents, but they did not systematically manipulate parameters, and their generators were not evaluated."
P07-1063,1,cassell and bickmore (2003) show that extraverts prefer systems utilizing discourse plans that include small talk.
P07-1063,3,"reeves and nass (1996) demonstrate that manipulations of personality affect many aspects of user perceptions, but their experiments use handcrafted utterances, rather than generated utterances."
P07-1064,1,"we can also explore an unsupervised fusion of these two sources of information; for instance, we can induce informative prosodic cues by using distributional evidence."
P07-1064,1,"in a supervised framework, we can further enhance audio-based segmentation by combining features derived from pattern analysis with prosodic information."
P07-1065,1,"in a companion paper (talbot and osborne, 2007) we have proposed a framework for deriving conventional smoothed n-gram models from the logfrequency bf scheme allowing us to do away entirely with the standard n-gram model in an smt system."
P07-1067,1,"also, we would like to explore the approach in technical (e.g., biomedical) domains, where jargons are frequently seen and the need for external knowledge is more compelling."
P07-1071,1,the complete set of predictions on the test set can be found at http: //ml.nec-labs.com/software/senna.would improve with more hand-built features.
P07-1074,1,figure (6) depicts the pattern learning and new seed extracting behavior during the iterations for the first experiment.
P07-1074,1,"therefore, we can reach most events in a few iterations."
P07-1078,2,we achieved a 50% (20-33%) reduction in annotation cost for the in-domain (out-of-domain) seed data scenarios.
P07-1078,2,a direction for future research is combining self-training data from various domains to enhance parser adaptation.acknowledgement.
P07-1081,2,"figure 3 shows that constructing a corpus with four or more transliterators, the range of possible word accuracies achieved is less than that of using fewer transliterators."
P07-1081,1,"in addition to computing agreement, we also in vestigated the transliterator perception of difficulty of the transliteration task with the ensuing word accuracy of the systems."
P07-1081,2,"interestingly, when using corpora built from transliterators that perceive the task to be easy, there is a large difference in the word accuracy between the two systems, but on corpora built from transliterators who perceive the task to be more difficult, the gap between the systems narrows."
P07-1083,1,"in particular, we plan to investigate approaches that do not require the bilingual dictionaries or bitexts to generate training data."
P07-1083,1,"furthermore, we have provided a natural framework for future cognate identification research."
P07-1088,5,"scalability is a particularly important concern for open information extraction, the task of extracting large numbers of relations that are not specified in advance."
P07-1089,2,we will also conduct experiments on large scale training data to further examine our design philosophy.
P07-1098,1,"forms of generalization for predicates and arguments within pasns like lsa clusters, wordnet synsets and framenet (roles and frames) information also appear as a promising research area."
P07-1098,1,"in the future, we aim to study ways to capture relations between predicates so that more general se 1 n 1 where n is the number of questions and rank n l-锟絠=1 ranki , i is the rank of the first correct answer to question i. mantics can be encoded by pasn."
P07-1099,3,"in our previous work, we evaluated the performance of the framework for english qa using questions from past trec evaluations (ko , 2007)."
P07-1100,2,we should note here that extending the system鈥檚 dialogue component library will automatically increase the state space and thus policy generation and optimization will become more difficult and require more training data.
P07-1100,1,"in our system, state representations are composed of multiple context feature values (e.g.communication problem earlier in the dialogue, the confidence of the utterance classifier)."
P07-1100,2,"as part of that, we will look at automatically mining libraries of dialogue components from existing dialogue transcript data (e.g.available scripts or transcripts of films, tv series and interviews containing real-life examples of different types of dialogue)."
P07-1101,5,"finally, we are also interested in examining speaker entrainment in cue phrase usage, or how subjects adapt their choice and production of cue phrases to their conversation partner鈥檚."
P07-1104,1,"meanwhile, me estimation with l1 regularization achieves the same level of performance while at the same time producing sparse models, and the averaged perceptron provides an excellent compromise of high performance and fast training."
P07-1104,1,"the choice of which to implement should come down to other considerations: if model sparsity is desired, choose me estimation with l1 regularization (or feature selection methods such as blasso); if quick implementation and training is necessary, use the averaged perceptron; and me estimation with l2 regularization may be used if it is important to achieve the highest obtainable level of performance."
P07-1105,5,"investigating where to add probabilities (ontology, grammar rules, or both) is part of our planned future work."
P07-1110,2,"whether the genre of text used as training data affects the absolute rate of retrieval precision for text of a different genre (e.g. news articles, shopping websites) is a separate question, and one we intend to address more fully in future work."
P07-1112,1,"such an approach is especially interesting either for lexicographers aiming at constructing lexicons, but even more for natural language processing systems relying on deep lexical knowledge as represented by qualia structures."
P07-1117,1,"for instance, describing vowel harmony using a partitioning based on morphemes takes necessarily several rules corresponding to the cases where the harmony is within a morpheme or across several morphemes."
P07-1119,1,"we plan to investigate whether using methods like discriminative reranking (och and ney, 2002) on such an n-best list could improve performance."
P07-1119,1,"we tested both dynamic programming and finite-state transducer implementations, the latter of which enabled us to use a word unigram language model to improve the accuracy of generated transliterations."
P07-1123,1,"in this paper, we described two approaches to generating resources for subjectivity annotations for a new language, by leveraging on resources and tools available for english."
P07-1126,1,the combined salience and visualness provide a score that signals the probability that the entity is present in the accompanying image.
P07-1127,4,we would also like to see how frequent argumentative queries are in other domains (such as tv talk shows or political debates) in order to generalize our results.
P07-1130,1,we presented a system for electronic career guidance utilizing nlp and ir techniques.
P07-1130,4,"furthermore, we applied these measures to an ir task, whereby they were used either in combination with a set of heuristics or the wikipedia based measure was used to directly compute semantic relatedness of topics and documents."
P07-1130,3,we intrinsically evaluated and analyzed the properties of two semantic relatedness measures utilizing the lexical semantic information in a german wordnet and wikipedia on the tasks of estimating semantic relatedness scores and answering multiple-choice questions.
P07-1130,1,"given a natural language professional profile, relevant professions are computed based on the information about semantic relatedness."
P07-1131,2,the counts from the corpus certainly help to filter out false information which would otherwise be difficult to filter.
P07-2002,5,how to fully integrate these functions into the system is our next challenge.
P07-2002,5,"although the effectiveness of this interface is yet to be fully examined in real-world situations, the basic concept should be useful as the idea of awareness level comes from feedback by monitors who used the first version of the system."
P07-2004,1,"as (nesson and shieber, 2006) indicates, this extraction in fact makes the resulting system a special case of synchronous tag where the semantic trees are isomorphic to the syntactic trees and unification variables across the syntactic and semantic components are interpreted as synchronous links."
P07-2010,1,"it does not do either complex reasoning or analogical structurematching as in our own att-meta metaphor system (barnden, 2006) or the cited approaches of fass, hobbs, martin, narayanan and veale."
P07-2010,1,"however, we plan to eventually add simplified versions of att-meta-style reasoning, and in particular to add the att-meta view-neutral mapping adjunct feature to implement the default carry-over of affect (see section 2) and certain other information, as well as handling more signals."
P07-2010,1,"the system currently only deals with a small minority of our own list of metaphoricity signals (see section 3.1), and these signals are only present in a minority of cases of metaphor overall."
P07-2014,5,"of course, many questions are yet to be explored, among them the following: can a singular value decomposition (to be in effect only temporarily for the purpose of clustering) reduce the problem of data sparseness?"
P07-2014,5,"can biclustering (also referred to as co-clustering or two-mode clustering, i.e. the simultaneous clustering of the rows and columns of a matrix) improve results?"
P07-2015,1,"in the future we plan to include features from adjacent sentences (fisher and roark, 2006) and use rouge scores to initially select negative examples."
P07-2015,1,we explore different possibilities for applying them in training svms to rank sentences in order of relevance to the query.
P07-2015,1,the experiments have provided some insights on which can be the best way to exploit the annotations.
P07-2017,1,"on the other hand, we also propose a new method for speeding up classification which is independent to the polynomial kernel degree."
P07-2027,2,"classifier accuracy on two-party data is reasonable, and we see promising results on multiparty data with a basic set of features."
P07-2027,2,we expect the accuracy to go up once we train and test on same genre data and also add features that are more specific to multi-party data.
P07-2028,1,"following the att-meta claim metaphors often convey crucial information via vnmas, we can reanalyze example (1) so that the effects of the necessities as food mapping are obtained by vnmas."
P07-2028,5,"moreover, map-transcending entities pose a problem for analogy-based approaches to metaphor interpretation (falkenhainer , 1989), which require the discovery of an elaborate structural similarity between the source and target domains and/or the imposition of unmapped source domain structures on the target domain, whereas part of our approach is that the unmapped source domain structure introduced by the utterance is by default not carried over."
P07-2029,3,we tested the application for real-time text correction produced in a real-world application.
P07-2030,1,bow denotes a bow feature.
P07-2030,1,experimental results show that our ranking model significantly outperforms baselines that use single ir-related and positional measures for ranking.
P07-2033,1,"in the above wer experiments a 4-gram baseline model was used, which was trained on nearly 1 billion words."
P07-2038,1,"one possible explanation for the unsuitability of the measures of (patwardhan and pedersen, 2006) for the coordinate similarity task could be based on how context is defined when building context vectors."
P07-2038,1,"that is, the distributional similarity measure presented in section 2 defines two words as similar if they occur in coordination patterns with a similar set of words and with similar distributions."
P07-2038,1,"for example, nouns in lists are often semantically similar, and we did not exclude nouns extracted from lists from the non-coordinate test set."
P07-2038,5,"although not all coordinate noun pairs are semantically similar, it seems clear, on inspection of the two sets of data, that they are more likely to be semantically similar than modifier-head word pairs, and the tests carried out for most of the measures of semantic similarity detect a significant difference between the similarity scores assigned to coordinate pairs and those assigned to non-coordinate pairs."
P07-2038,5,"it is not possible to judge, based on the significance tests alone, which might be the most useful measure for the purpose of disambiguation."
P07-2038,1,"a wordnet-based measure of semantic similarity, however, might give a high score to both of the noun pairs."
P07-2039,2,"in addition, we would like to explore the possibility of extending this proposal to other language pairs."
P07-2044,1,we have described a two-stage machine learning approach to event-event temporal relation classification.
P07-2049,5,these facts could be an unplanned side-effect from the way the test topics were produced: annotators might have been influenced by information in the input to be summarizied when defining their topic.
P07-2051,5,"however, we have made idealized assumptions (small inventory of dependency relations and treebank derived chunks) that clearly must be replaced by a realistic setting in our future work."
P07-2054,1,"the extensions consist of accepting and generating word graphs, and introducing two n-gram lm鈥檚 over source and target tagged words."
P07-2057,1,svm-based parsers that have the same characteristics can be constructed if we introduce multi-class classifiers.
P07-2057,1,the other characteristics of our method are using crfs and that long dependencies are parsed in one labeling process.
P07-3002,1,"for a system such as  this to take advantage of the patterns that arise out of the text itself, a much more fine-grained perspective is necessary, since the performance of individual category-assignments to words being the focus of the task."
P07-3005,1,in this paper we proposed an algorithm for the automatic generation of cognates from two different languages sharing the same alphabet.
P07-3005,5,"even if accuracy is rather poor, if we consider that no knowledge repository other than an initial list of cognates was available, we feel that the results are still quite encouraging."
P07-3009,1,"although this seems to work well, we would like to investigate this further and possibly devise a cost based prior that is both theoretically well-grounded and performs well in practice."
P07-3009,1,"in the spv update, the cost was incorporated into the map model in a rather ad-hoc fashion."
P07-3009,1,their performance is on par with other state-of-the-art online learning algorithms for cost-sensitive problems.
P07-3014,1,another interesting avenue for future work in this area is investigation into exactly how joining terms relate to semantic relations.
P08-1001,1,"in the future, we would like to find a way to automatically generate the list of key words and phrases for useful english language categories."
P08-1001,1,"this could implement the work of kazama and torisawa, in particular."
P08-1004,5,"we also plan to explore the capacity of open ie to automatically provide labeled training data, when traditional relation extraction is a more appropriate choice."
P08-1006,3,"in order to obtain general ideas on parser performance, experiments on other tasks are indispensable."
P08-1007,5,"since the accuracy of dependency parsers is not perfect, a possible future work is to identify when best to incorporate such syntactic information."
P08-1007,1,"possible future directions include adding semantic role information, using the distance between item pairs based on the token position within each sentence as additional weighting consideration, etc."
P08-1007,2,"also, we have seen that dependency relations help to improve correlation on the nist dataset, but not on the acl-07 mt workshop datasets."
P08-1011,1,"in the future, we plan to investigate more features and enlarge coverage to improve the quality of measure word generation, especially reduce the errors found in our experiments."
P08-1013,1,it would therefore be interesting to investigate ways of introducing word information into our grammar-based model.
P08-1014,2,in the future we plan to integrate additional knowledge sources into our statistical method in order to more specifically address each of the various phenomena encountered in spontaneous dictation.
P08-1014,2,"while the present experiments have used a separate auto punctuation step, future work will aim to eliminate it by integrating the punctuation features into the transformation step."
P08-1015,1,"in future work, we will examine the strengths and limitations of grounded language modeling in these domains."
P08-1015,4,"also, we are examining extending this approach to other sports domains such as american football."
P08-1015,1,"in future work, we will examine the ability of grounded language models to improve performance for other natural language tasks that exploit text based language models, such as machine translation."
P08-1017,4,"first, we will study the same problem in some specialized domain, such as biology literature, to see whether the proposed approach could be generalized to the new domain."
P08-1017,1,"second, the fact that using axiomatic approaches to incorporate linguistic information can improve retrieval performance is encouraging."
P08-1017,1,"we plan to extend the axiomatic approach to incorporate more linguistic information, such as phrases and word senses, into retrieval models to further improve the performance."
P08-1017,6,there are many interesting future research directions based on this work.
P08-1018,2,"finally, it would be interesting to test the approaches using real web data."
P08-1018,1,"for example, we could integrate other features in the regression model, and use other nonlinear regression models, such as bayesian regression models  (rasmussen and williams, 2006)."
P08-1018,1,our methods can be further improved in several aspects.
P08-1019,4,"thus, as future work, we will try to investigate the use of the proposed approach for other kinds of web services."
P08-1022,1,"finally, further efforts to engineer a grammar suitable for realization from the ccgbank should provide richer feature sets, which, as our feature ablation study suggests, are useful for boosting hypertagging performance, hence for finding better and more complete realizations."
P08-1022,1,"this suggests that further improvements to the hypertagger will lead to more complete realizations, hence more high-quality realizations."
P08-1023,1,"for future work, we would like to use packed forests not only in decoding, but also for translation rule extraction during training."
P08-1025,1,"as future work, new metrics for the final pass may be able to better approximate bleu."
P08-1025,1,"as the bigram decoding pass currently takes the bulk of the decoding time, better heuristics for this phase may speed up the system further."
P08-1027,6,we intend to pursue this promising direction in future work.
P08-1028,2,future directions include constraining the number of free parameters in linguistically plausible ways and scaling to larger datasets.
P08-1029,1,our future work is focused on designing an algorithm to optimally choose a smoothing regime for the learned feature trees so as to better exploit the similarities between domains while neutralizing their differences.
P08-1029,1,"along these lines, we are working on methods to reduce the amount of labeled target domain data needed to tune the prior-based models, looking forward to semi-supervised and unsupervised transfer methods."
P08-1030,1,"ultimately we would like to extend the system to perform essential, although probably lightweight, event prediction."
P08-1032,4,an interesting future direction concerns the application of the proposed model in a semi-supervised setting where the annotation output is iteratively refined with some manual intervention.
P08-1034,1,among the most promising directions for future research in the direction laid out in this paper is the deployment of more advanced classifiers and feature selection techniques that can further enhance the performance of the ensemble of classifiers.
P08-1035,5,"if it is, the whole scheme of ours would fall under what is known as linear programming crfs (tasker, 2004; roth and yih, 2005)."
P08-1035,5,an interesting future exercise would be to explore whether it is feasible to rewrite eq.5 as a linear integer program.
P08-1036,3,the primary area of future work is to incorporate the model into an end to-end sentiment summarization system in order to evaluate it at that level.
P08-1040,1,"another interesting extension involves incorporating parser uncertainty into the model; in particular, our simplification system is capable of seamlessly accepting a parse forest as input."
P08-1044,1,"in addition, our results suggest a rich area for future research in further analyzing the variability of both lexical and prosodic effects on asr behavior for different speakers."
P08-1047,2,future work will be to refine the matching method and to construct even larger gazetteers. their gazetteers.
P08-1048,1,we also plan to study methods of automatically updating the 1911 roget鈥檚 thesaurus with modern words.
P08-1048,2,another longer-term direction of future work could be merging roget’s thesaurus with wordnet.
P08-1050,5,"for our future work, we aim to test whether an automatically created verb classification can be beneficial to other nlp tasks."
P08-1051,2,"we believe that this information, especially the answer agreement of workers, can be successfully used in post-processing and analysing the data, as well as automatically accepting and rejecting workers submissions in similar future data collection exercises."
P08-1052,1,"using a parser with a richer set of syntactic dependency features, e.g., as proposed by pad麓o and lapata (2007), is another promising direction for future work."
P08-1055,1,"in future work we hope to test these algorithms in other domains, and show that intentional summaries can not only be automatically derived but also lead to reduced task times and increased task success."
P08-1059,1,"in the future, we would like to include more sophistication in the design of a lexicon for a particular language pair based on error analysis, and extend our pre-processing to include other operations such as word segmentation. however, the improvement in arabic is not statistically significant on this 100 sentence set."
P08-1060,5,further research is needed to determine whether these observations generalize to other knowledge mining patterns.
P08-1080,5,the obvious next step is to determine how to automatically classify queries according to their predicted result length and type.
P08-1080,6,We plan to pursue this further in future work.
P08-1084,4,"in the future, we hope to apply similar multilingual models to other core unsupervised analysis tasks, including part-of-speech tagging and grammar induction, and to further investigate the role that language relatedness plays in such models.7"
P08-1086,3,another interesting direction of further research is to evaluate the use of the presented clustering technique for language model size reduction.
P08-1086,1,we furthermore expect to be able to increase the gains resulting from using class-based models by using more sophisticated techniques for combining them with word-based models such as linear interpolations of word- and class-based models with coefficients depending on the frequency of the history.
P08-1089,1,in the future we wish to exploit more feature functions in the log-linear model.
P08-1089,1,"in addition, we will try to make better use of the context information when replacing paraphrase patterns in context sentences."
P08-1091,1,"finally, we would like to experiment with automatic parses and different syntactic formalisms such as dependencies and shallow parses."
P08-1091,1,"for future work, we would like to explore further explicit morphological features such as aspect tense and voice as well as richer pos tag sets such as those proposed in (diab, 2007)."
P08-1092,4,in future we plan to explore such a generalization of our procedures to such domains.
P08-1092,1,"since our task is to produce concise summaries, one focus of our future research will be to simplify the sentences we extract before classifying them as biographical or non-biographical."
P08-1093,2,"an important future work is to construct larger test sets (e.g., of biomedical literature) to facilitate evaluation of impact summarization."
P08-1094,1,a more complete characterization of summarization input will be necessary in the future.
P08-1096,1,"in our future work, we would like to investigate more sophisticated clustering methods that would lead to global optimization, e.g., by keeping a large search space (luo et al., 2004) or using integer programming (denis and baldridge, 2007)."
P08-1101,1,"therefore, given the flexibility of the feature-based linear model, an obvious next step is the study of open features in the joint segmentor and pos tagger."
P08-1104,3,"in future, we are to explore more domain independent evidences and evaluate the proposed model on the basis of the data from other domains."
P08-1105,3,"other future work concerns indicator selection: instead of taking all indicators on board, consider selected indicators only, in a topic dependent fashion."
P08-1106,1,"future work will focus on developing methodologies for computer-assisted back-of-the-book indexing, as well as on the use of the automatically extracted indexes in improving the browsing of digital libraries."
P08-1107,1,we are also interested in exploring iterative approaches to jointly resolving mentions.
P08-1107,2,"in future work, we plan to extend our test collection with mention queries that must be resolved in the 鈥渓ong tail鈥 of the identity distribution where less evidence is available."
P08-1108,1,"directions for future research include a more detailed analysis of the effect of feature-based integration, as well as the exploration of other strategies for integrating different parsing models."
P08-1110,1,"an interesting line for future work is to use relationships between schemata to find nonprojective parsers that can be derived from existing projective counterparts. an alternative framework that formally describes some dependency parsers is that of transition systems (mcdonald and nivre, 2007)."
P08-1116,1,"in the future work, we will try to use syntactic and context constraints in paraphrase generation to enhance the acceptability of the paraphrases."
P08-1116,1,"in addition, we will extract paraphrase patterns that contain more structural variation and try to combine the smt-based and pattern-based systems for sentence level paraphrase generation."
P08-2005,1,automating the creation of inventories of pedagogically important concepts may represent an important step towards scalable intelligent tutoring systems.
P08-2009,1,we conclude with an error analysis to provide future direction.
P08-2009,1,this indicates that further work on data driven methods may still improve the state of the art.
P08-2010,5,"we intend to further explore the idea of boosting on n-best lists, drawing inspirations from the large body of work on boosting for classification whenever possible."
P09-1001,5,"finally, we will explore the theoretical foundations and limitations of heterogeneous transfer learning as well."
P09-1001,2,"we may also consider clustering, topic modeling, question answering, etc., to be done using data in different feature spaces."
P09-1001,2,"in natural language processing, there are many future opportunities to apply heterogeneous transfer learning."
P09-1001,2,"we can consider data in different modalities, such as video, image and audio, as the training data."
P09-1001,2,"in (ling et al., 2008) we have shown how to classify the chinese text using english text as the training data."
P09-1002,2,"also, we will extend the current dataset using more annotators and exploring additional lexicon resources."
P09-1002,5,"as a next step, we will automatically cluster the judgments we obtained in the wssim and usim experiments to further explore the degree to which the annotation gives rise to sense grouping."
P09-1002,1,we will also use the ratings in both experiments to evaluate automatically induced models of word meaning.
P09-1004,1,"adapting our algorithm to use the output of these models, either to reduce the little supervision our algorithm requires (pos tagging) or to provide complementary syntactic information, is an interesting challenge for future work."
P09-1005,1,"potential future work may focus on developing an improved ccg parser using the revised (syntactic) adjunct-argument distinctions (guided by the propbank annotation) described in (boxwell and white, 2008)."
P09-1006,1,"future work includes further investigation of our conversion method for other pairs of grammar formalisms, e.g., from the grammar formalism of the penn treebank to more deep linguistic formalism like ccg, hpsg, or lfg."
P09-1012,1,"we believe this will be particularly beneficial to build a language model on such texts as speech transcripts, colloquial texts or unknown languages, where word boundaries are hard or even impossible to identify a priori."
P09-1014,1,kernel functions could also be used to automatically create a richer feature space; preliminary experiments have shown gains in performance using polynomial and rbf kernels with our stress ranker.
P09-1014,1,"in the future, we will investigate additional features to leverage syllabic and morphological information, when available."
P09-1016,2,"a high quality alignment on a small verified corpus such as xinhua can be effectively used to validate a large noisy corpus, such as ldc05."
P09-1016,1,we propose using alignment distance to validate transliterations.
P09-1016,4,"we believe that this property would be useful in transliteration extraction, cross-lingual information retrieval applications."
P09-1017,1,to really know if the carried out theoretical work is valuable we would like to try it out in a real search setting in a search engine and see if the users appreciate the new algorithm鈥檚 results.
P09-1017,1,work with the new affix lemmatizer has until now focused on the algorithm.
P09-1023,1,it is challenging but possibly fruitful to recast the normal documents with wiki styles so as to adopt edcl for free text and enrich the research efforts on other nlp tasks.
P09-1023,1,"more generally, the knowledge hidden in nontextual features of wikipedia allow the model to harvest better definition summaries."
P09-1024,1,"moreover, a promising direction is to consider hierarchical discourse formalisms such as rst (mann and thompson, 1988) to supplement our template-based approach."
P09-1024,1,"diseases and american film actors exhibit fairly consistent article structures, which are successfully captured by a simple template creation process."
P09-1024,1,"however, with categories that exhibit structural variability, more sophisticated statistical approaches may be required to produce accurate templates."
P09-1024,6,This work opens several directions for future research.
P09-1025,3,an important future direction concerns a more detailed assessment of our search procedure.
P09-1027,1,"we will employ the structural correspondence learning (scl) domain adaption algorithm used in (blitzer et al., 2007) for linking the translated text and the natural text."
P09-1027,1,"in future work, we will improve the sentiment classification accuracy in the following two ways: 1) the smoothed co-training approach used in (mihalcea, 2004) will be adopted for sentiment classification."
P09-1028,1,"as a topic of vigorous current activity, there are several very recently proposed competing methodologies for sentiment analysis that we would like to benchmark against."
P09-1028,6,These are topics for future work.
P09-1033,1,future work will investigate another basic property of semantic role labelling schemes: cross-linguistic validity.
P09-1034,3,future work will have to assess the effectiveness of individual features and investigate ways to customize rte systems for the mt evaluation task.
P09-1034,3,"nevertheless, it must be an important focus of future research to develop robust meaning-based metrics for other languages that can cash in the promise that we have shown for evaluating translation into english."
P09-1034,4,fig.2) and may find use in uncovering systematic shortcomings of mt systems.
P09-1036,3,"in the future work, we will use other language pairs to test our models so that we could know whether our method is language-independent."
P09-1040,1,"in addition, we want to explore alternative oracle functions, which try to minimize the number of swaps by allowing the stack to be temporarily 鈥渦unsorted鈥."
P09-1040,5,future research will include an in-depth error analysis to find out why the system works better for some languages than others and why the exact match score improves even when the attachment score goes down.
P09-1046,1,we believe that such efforts can further improve cross document coreference performance.
P09-1046,2,future research directions include developing rich feature sets and using corpus level or external information.
P09-1047,1,this indicates that the framework can reduce the cost of preparing training data in new languages with the help of our english and japanese strong classifiers.
P09-1047,6,Our future work focuses on this issue.
P09-1049,5,"we also plan to explore whether lexicons can be constructed using only the back-off method for hyponym extraction, to make asia completely language independent."
P09-1049,2,"in the future, we plan to investigate the possibility of constructing hypernym hierarchy automatically using semi-structured documents."
P09-1051,1,we also intend to consider the rule base as a directed graph and exploit the graph structure for further rule extraction and validation.
P09-1056,3,further experiments are of course necessary to investigate distributional representations as smoothing techniques.
P09-1056,1,one particularly promising area for further study is the combination of smoothing and instance weighting techniques for domain adaptation.
P09-1056,5,"whether the current techniques are applicable to structured prediction tasks, like parsing and relation extraction, also deserves future attention."
P09-1058,2,"in future work, we plan to apply our framework to other asian languages, including thai and japanese."
P09-1059,2,"in the future, we will continue to research on annotation adaptation for other nlp tasks which have different annotation-style corpora."
P09-1059,4,"especially, we will pay efforts to the annotation standard adaptation between different treebanks, for example, from hpsg lingo redwoods treebank to ptb, or even from a dependency treebank to ptb, in order to obtain more powerful ptb annotation-style parsers."
P09-1060,3,"to demonstrate practicality of our method for automatic speech transcription, an experiment using a continuous speech recognition system will be performed in the future."
P09-1061,2,"we also plan to use this algorithm to annotate different types of data, such as spontaneous speech, and incorporate prosodic events in spoken language applications."
P09-1061,1,"for the future work, we will perform analysis of loss function of each classifier in order to estimate parameters without labeled development data."
P09-1061,3,"in addition, we plan to compare this to other semi-supervised learning techniques such as active learning."
P09-1062,1,"additional analysis suggests that the acoustics-based approach is useful in overcoming situations where out-ofvocabulary error may be more prevalent, and we suggest that a hybrid approach of traditional asr with acoustics-based pattern matching may be the most desirable future direction of research."
P09-1064,1,"therefore, future work may show additional benefits from fast consensus decoding."
P09-1065,1,"in the future, we plan to optimize feature weights for max-translation decoding directly on the entire packed translation hypergraph rather than on n-best derivations, following the latticebased mert (macherey , 2008)."
P09-1066,1,"in the future, we will extend our method to use lattice or hypergraph to compute consensus statistics instead of n-best lists."
P09-1068,1,finding the best argument representation is an important future direction.
P09-1068,1,we hope in the future to show that a range of nlu applications can benefit from the rich inferential structures that narrative schemas provide.
P09-1068,5,"the exact balance between lexical units, clusters, or more general (traditional) semantic roles remains to be solved, and may be application specific."
P09-1072,1,we look forward to future research to extend the intermediate representation and to experiment with different modeling methodologies.
P09-1073,1,"because their work basically builds on inductive logic programing, we can naturally extend this to incorporate our caching mechanism into the global optimization by expressing cache constraints as predicate logic, which is one of our next challenges in this research area."
P09-1075,1,"borrowing techniques from generic global optimization meta algorithms such as simulated annealing (kirkpatrick et al., 1983) should allow us to better deal with issues of local optimality while retaining acceptable time-complexity."
P09-1075,1,"future directions for this work notably include a better tree-building algorithm, with improved exploration of the solution space."
P09-1081,1,we will investigate experiment 3 by using real questions from different sources and construct different test datasets.(2) we will use other distance measures to better explain entailment between q/a pairs and compare with other semisupervised and transductive approaches.
P09-1082,3,"in the future, we would like to further evaluate the models presented in this paper for different tasks, such as question paraphrase retrieval, and larger datasets."
P09-1082,1,we also plan to improve question analysis by automatically identifying question topic and question focus.
P09-1085,1,"that is, beginning with only characters in the lexicon and using the training data to alter the current lexicon in each iteration."
P09-1085,1,"this can be amended by involving the discriminative language model adaptation in the iteration, which results in a unified language model and lexicon adaptation framework."
P09-1085,6,"however, there still remain lots to be improved."
P09-1085,1,"moreover, the same procedure can be used in the construction."
P09-1087,4,we plan to pursue other research directions using dependency models discussed in this paper.
P09-1087,1,"while we use a dependency language model to exemplify the use of hierarchical structure within phrase based decoders, we could extend this work to incorporate dependency features of both sourceand target side."
P09-1088,1,"in future we envision it will be possible to use the techniques developed here to directly induce grammars which match state-of-the-art decoders, such as hiero grammars or tree substitution grammars of the form used by galley et al.(2004)."
P09-1089,2,"for future work we suggest generating entailed texts with a more extensive set of rules, in particular lexical-syntactic ones."
P09-1089,1,"this is an approach that we intend to explore in future work, as a way to efficiently handle the different source language alternatives generated by entailment rules."
P09-1089,1,developing better context-models to be applied on the source is expected to further improve our method鈥檚 performance.
P09-1089,1,"specifically, we suggest taking into account the prior likelihood that a rule is correct as part of the model score."
P09-1089,1,combining rules from monolingual and bilingual resources seems appealing as well.
P09-1090,2,this course of future action is suggested by the fact that smaller sentences are much more fluent in translation compared to medium length and long sentences.
P09-1090,5,future work consists of investigations into (i) how the internal structure of constituents can be strictly preserved and (ii) how to glue together correctly the syntactically well-formed bits and pieces of the sentences.
P09-1094,1,"first, we will improve the components of the method, especially the paraphrase planning algorithm."
P09-1094,1,"the algorithm currently used is simple but greedy, which may miss some useful paraphrase units."
P09-1094,4,"second, we will extend the method to other applications, we hope it can serve as a universal framework for most if not all applications."
P09-1094,6,our future work will be carried out along two directions.
P09-1097,1,another interesting and practically useful problem that we have left for future work is to design an unsupervised learning algorithm for cfsg similar to its phrase structure counterpart: inside-outside algorithm (baker 1979).
P09-1097,1,"note that although strict ordering constraints such as those imposed by psg is not appropriate for modeling query structure, it might be helpful to take ordering information into account when resolving ambiguity."
P09-1097,1,"having such a capability, we are able to automatically learn the underlying structure of queries by processing the huge amount of available unlabeled queries."
P09-1098,1,"in the future, we would like to integrate the text segmentation module with the seed mining and pattern learning module to improve the accuracy of text segmentation."
P09-1098,3,we also want to evaluate the usefulness of our mined data for machine translation or other applications.
P09-1100,2,our long term goal is to provide guidance of how to effectively build user simulations for different dialog system development tasks given limited resources.
P09-1100,3,"in the future, we will conduct follow up studies to confirm our current findings since there are several factors that can impact our results."
P09-1101,1,"we also plan to use the information from the segmentation to examine the structure of segments, especially the sequences of dialogue acts within them, with a view to improving a dialogue act tagger."
P09-1102,1,"since there are numerous unlabeled full forms on the web, it is possible to use a semisupervised approach in order to make use of such raw data."
P09-1102,6,This is an area for future research.
P09-1103,2,"therefore, further work of our studies includes the optimization of the large rule set of the snctssg based model."
P09-1105,1,for future work we would like to explore richer models to estimate alignment posterior probability.
P09-1105,5,"in most cases, exact calculation by summing over all possible alignments is impossible, and approximation using n-best alignments is needed."
P09-1109,1,we will also pursue integration of our method with parsers.
P09-1109,3,"we will investigate similarity measures different from sequence alignment, to better capture the symmetry of these conjuncts."
P09-1109,1,"because they have advantages in different coordination phrase types, their integration looks promising."
P09-1113,5,"it remains for future work to see whether simpler, chunk-based syntactic features might be able to capture enough of this gain without the overhead of full parsing, and whether coreference resolution could improve performance."
P09-1117,1,future research is needed to empirically investigate into this area and quantify the savings in terms of the time achievable with sesal in the ner scenario.
P09-1118,2,"moreover, for a thorough analysis on the effect of our framework, additional experiments on larger and more realistic collections (e.g. the web environment) would be required."
P09-1118,6,These will be our future work.
P09-1119,1,"as to (i), we first want to determine whether a query should be expanded, and next select the appropriate expansion model."
P09-1119,1,future work focuses on two themes: (i) topic dependent model selection and (ii) improved estimates of components.
P09-1119,1,"finally, we can also include the estimation of formula, the importance of a document in the collection."
P09-1119,1,Another possibility is to look at solutions used in distributed IR.
P09-1120,1,"finally, we will consider other alternatives to the decision tree framework when combining the results of the models with their confidence scores."
P09-1120,1,"furthermore, we would like to improve the accuracy of classifier2 with additional non-linguistic features."
P09-1120,2,we are also planning to extend the number of languages in our data set.
P09-1120,2,"in future, we would like to improve the accuracy of our data generation system by considering additional features proposed in the studies of automated query taxonomy, and doing a more careful examination in the assignment of the parameter values."
P09-1121,1,"furthermore, we may involve more sophisticated monolingual features that do not transfer cross-lingually but are asymmetric for either side, such as clustering, document classification features built from domain taxonomies like dmoz."
P09-2003,1,"this way, one could process predicates whose range boundaries are better known first."
P09-2003,1,we plan to include this strategy in future work.
P09-2009,2,"third, if this system can be expanded to trigrams or even n-grams using a larger training corpus, we believe that the tagging accuracy will increase."
P09-2009,2,we are currently working on developing a small and more compact tag set.
P09-2009,2,"first, the size of the manually tagged part of the corpus will have to be increased."
P09-2009,1,"second, a suitable procedure for handling unknown proper nouns will have to be developed."
P09-2009,1,we propose the following additional work for improved performance.
P09-2010,1,"in terms of future work, a more extensive error analysis will be performed to locate the pre cise benefits of the parser combination."
P09-2010,4,"we will also investigate the application of the method directly to raw text and application to a task which may benefit specifically from the combined analyses, such as semantic role labeling or semantic verb classification."
P09-2011,1,"to process indirect left-recursive structures, we need to extend our method."
P09-2011,1,"in future work, we will investigate the incremental parser for head-final language such as japanese."
P09-2011,1,"in this paper, we dealt with direct left-recursive structures only."
P09-2013,3,"furthermore, we believe it would be interesting to discuss linguistically and psycholinguistically the differences between japanese and other european languages such as english."
P09-2013,1,we also expect that structure analysis of compound nouns can be incorporated by extending the dependency relation types.
P09-2013,1,in future work we plan to combine morphological analysis or word segmentation into our proposed algorithm.
P09-2013,5,we would like to know what differences lead to easiness of analyzing a japanese sentence.
P09-2017,1,it would be interesting to explore ways to substitute querying yahoo! so as to make the system quicker.
P09-2017,3,experimentation with more sophisticated graph connectivity measures could possibly improve accuracy.
P09-2018,1,in future work we plan to apply disambiguation techniques to address this problem.
P09-2018,3,"we also plan to evaluate the performance of directional measures in additional tasks, and compare it with additional symmetric measures."
P09-2019,2,"we are particularly interested in domain adaptation, and whether distributional similarities can profit from domain corpora for better performance."
P09-2019,1,current efforts are devoted to study the integration of the selectional preference models presented in this paper in a in-house srl system.
P09-2020,2,"future work involves studying the robustness of our discourse segments on other corpora, such as formal texts from the medical domain and other informal texts."
P09-2021,1,our future work includes adding more expert knowledge through error analysis to incrementally improve the performance.
P09-2021,3,"furthermore, actual development and evaluation of a db-call system will be arranged so that we may investigate how much the cost of collecting data and evaluation would be reduced by using language learner simulation."
P09-2022,3,"however, because our analyzer has scalability that can freely add new features, for our future work, we hope to adopt the case frames as new features and compare their effect."
P09-2023,1,"thus, our future work will include integrating the proposed features with bottom-up information such as acoustic-score based confidence measures."
P09-2023,5,"additionally, we simply assumed in this study that all affirmative and negative responses following the explicit confirmation are correct."
P09-2024,1,"in the future, we will further investigate the relationships between the concept clusters in the question and the answers."
P09-2026,1,"in our future work, we hope to explore alternative approaches that allow reordering or paraphrasing along with deleting words to make compressed sentences more grammatical and coherent."
P09-2027,1,"since the notion of query-focus is apparently missing in any or all of the algorithms, the future summarization algorithms must try to incorporate this while designing new algorithms."
P09-2030,4,"meanwhile, we would like to investigate more appropriate techniques to use feedback, and we are interested in applying co-frank to the other applications, such as opinion summarization where the integration of opinion-biased and document-biased ranking is necessary."
P09-2030,1,"although we show the promising achievements of co-frank from the perspective of experimental studies, we expect a more theoretical analysis on co-frank."
P09-2030,6,There is still a lot of work to be done on this topic.
P09-2035,4,"in the future, we will extend our work from decoding to training time, where we divide the bilingual sentences accordingly."
P09-2036,3,"in future work, we look forward to evaluating the wide array of forest binarization strategies that are enabled by our asynchronous approach."
P09-2038,5,the handling of negative words and metaphors and their impact in detecting sentence level emotion along with document level analysis are the future areas to be explored.
P09-2039,1,"in our future work, we plan to classify comparative types and to extract comparative relations from identified comparative sentences."
P09-2042,1,"in future work, we plan to extend the proposed hierarchical learning method to the case where the hierarchy is a dag instead of tree and scale up the method further."
P09-2043,1,we plan to find out product attributes that contribute most to modeling the interaction among the proposed clues in effective sentiment summarization.
P09-2044,2,"the transient nature of the weasel tag suggests to use the wikipedia edit history for future work, since the edits faithfully record all occurrences of weasel tags."
P09-2047,1,"in future, we will further explore a new method of parameter k selection to achieve higher performance."
P09-2047,1,our approach will be capable of extracting semantic concepts from queries.
P09-2047,4,"Besides, it can extended to Chinese word segmentation."
P09-3002,5,we propose to look into these for further insights.
P09-3002,5,"as the hydt grows, we are bound to come across more instances as well as more types of non-projective constructions that could bring forth interesting phenomenon."
P09-3007,4,"although there have been speculations and trails on things that function labels might help with, it remains to be important to discover how function labels contribute to other nlp applications, such as the japanesechinese machine translation system we have been working on."
P09-3011,1,one of the main directions of our future work will be how to improve the performance of personal name disambiguation.
P09-3011,5,"moreover, word-based text features haven't solved two difficult problems of natural language problems: synonym and polysemy, which seriously affect the precision and efficiency of clustering algorithms."
P09-3011,1,text representation based on concept and topic may solve the problem.
P09-3011,1,computing weight based on a window around names may be helpful.
P09-3012,3,the next step will be to compared the output of a standard clustering algorithm to the gold standard.
P09-3012,3,this gold standard will be used for further experiments on clustering for multi-document summarization.
P10-1001,1,"one idea would be to consider extensions and modifications of our parsers, some of which have been suggested in sections 5 and 7.4."
P10-1001,4,a second area for future work lies in applications of dependency parsing.
P10-1002,1,"however, considering its lower performance on human-annotated treebanks, the dependency parsing method itself still need a lot of investigations, especially on the training method of the classifier."
P10-1003,4,"first, we may attempt to apply the bilingual subtree constraints to transition based parsing models (nivre, 2003; yamada and matsumoto, 2003)."
P10-1003,2,"third, larger unannotated data can be used to improve the performance further."
P10-1003,2,"second, we may apply the proposed method for other language pairs such as japanese-english and chinese-japanese."
P10-1003,1,"here, we may design new features for the models."
P10-1004,4,"in the future, it will be interesting to explore how these semantic representations can be used in applications."
P10-1005,1,"as discussed, this requires a contextually appropriate selection of the quantifier restriction8, as well as determining inheritance of properties from classes to individuals and the formalization of defaults."
P10-1005,4,"as a next step, we will apply our approach to the classification of generic sentences."
P10-1005,1,"although our results are satisfying, in future work we will extend the range of features for further improvements."
P10-1005,1,"in particular, we will address lexical semantic features, as they tend to be effected by sparsity."
P10-1006,1,"for future work, we want to develop a framework which can uniformly model the semantic knowledge and the contextual clues for named entity disambiguation."
P10-1008,3,"in future work, we will evaluate the learned policies with real users to examine how well they adapt, and examine how real users evaluate them (subjectively) in comparison to baselines."
P10-1008,5,whether the learned policies perform better or as well as a hand-coded policy painstakingly crafted by a domain expert (or learned using supervised methods from an expert-layperson corpus) is an interesting question that needs further exploration.
P10-1008,1,"also, it would also be interesting to make the learned policy account for the user's learning behaviour and adapt accordingly."
P10-1009,1,"we list below some possible future extensions: 1) integrating different selection strategies, e.g., the listwise strategy that defines the loss function on all the sentences associated with a document to be summarized, into this framework, 2) exploring different modeling approaches for this framework, 3) investigating discriminative training criteria for training the component models in this framework, and 4) extending and applying the proposed framework to multidocument summarization tasks."
P10-1010,1,we need leaner methods for building machine translation systems; new algorithms for cross-linguistic bootstrapping via multiple paths; more effective techniques for leveraging human effort in labeling data; scalable ways to get bilingual text for unwritten languages; and large scale social engineering to make it all happen quickly.
P10-1011,2,"it would be interesting to further investigate this observation with other sources of lexicons (e.g., obtained from parallel or comparable corpora) and for other tasks, such as cross-lingual word sense disambiguation and information retrieval."
P10-1011,1,"in fact, nas can be viewed as a general measure for word similarity between languages."
P10-1012,2,future studies will also include experiments using data of various languages.
P10-1012,1,"future studies will improve our method, enabling it to achieve high correlation in sentence-level fluency."
P10-1013,2,"in the future, we plan to run woe over the billion document cmu clueweb09 corpus to compile a giant knowledge base for distribution to the nlp community."
P10-1013,2,"other data sources, such as freebase, could be used to create an additional training dataset via self-supervision."
P10-1013,1,"we are also interested in merging lexicalized and open extraction methods; the use of some domain-specific lexical features might help to improve woe’s practical performance, but the best way to do this is unclear."
P10-1013,1,"finally, we wish to combine woeparse with woepos (e.g., with voting) to produce a system which maximizes precision at low recall."
P10-1019,4,"in the future we hope to develop a domain with more realistic speech acts and a more difficult dialogue task that will, among other things, highlight this situation."
P10-1019,1,"we also plan on implementing a fully functional idtb system, using an incremental processing architecture that not only detects, but generates, a wide array of turn-cues."
P10-1021,1,a key objective for future work will be to investigate models that integrate semantic constraint with syntactic predictions more tightly.
P10-1021,1,"at the same time, the semantic model should have access to syntactic information, i.e., the composition of word representations should take their syntactic relationships into account, rather than just linear order."
P10-1021,1,"for example, we could envisage a parser that uses semantic representations to guide its search, e.g., by pruning syntactic analyses that have a low semantic probability."
P10-1023,2,"as future work, we plan to apply our method to other languages, including eastern european, arabic, and asian languages."
P10-1023,2,"we plan in the near future to apply babelnet to the challenging task of cross-lingual wsd (lefever and hoste, 2009)."
P10-1023,4,"finally, we aim to apply babelnet to a variety of applications which are known to benefit from a wide-coverage knowledge resource."
P10-1023,1,"we also intend to link missing concepts in wordnet, by establishing their most likely hypernyms."
P10-1024,1,this calls for a future study into the semantics of prepositions and their relation to the core-adjunct distinction.
P10-1025,1,"moreover, dimensionality reduction methods alternative to lsa, as currently studied on semisupervised spectral learning (johnson and zhang, 2008), will be experimented."
P10-1025,4,"future work will study the application of the flexible srl method proposed to other languages, for which less resources are available and worst training conditions are the norm."
P10-1027,3,"for example, we can also evaluate its effectiveness and costs during the operation of a discussion forum, where the discussion thread is continually updated by new comments and votes."
P10-1027,1,"indeed, its power is yet to be further improved and investigated."
P10-1027,1,This study can be extended in a few interesting ways.
P10-1028,2,"for example, in future work we plan to investigate the combination of the clickthrough data collected from a web browser with the noisy but large query sessions collected from a commercial search engine."
P10-1029,4,"In future work, we plan to investigate whether these semantic taggers can be used to improve other tasks."
P10-1031,1,developing and testing alternate methods for hierarchical modeling in ontousp;
P10-1031,4,investigating the theoretical properties of ontousp learning approach and generalizing it to other tasks;
P10-1031,1,directions for future work include: exploiting the ontological structure for principled handling of antonyms and (more generally) expressions with opposite meanings;
P10-1031,2,scaling up learning and inference to larger corpora;
P10-1031,5,answering questions that require inference over multiple extractions;
P10-1033,1,"as the success of hp-ditg illustrates the merit of hierarchical phrase pair, in future we should investigate more features on the relationship between span pair and hierarchical phrase pair."
P10-1037,4,we have a plan to incorporate our proposed methods to the annotation tool.
P10-1037,2,we will use it to accelerate building of the large annotated corpus to improved our japanese parser.
P10-1037,2,"it would be interesting to explore the use of partially labeled constituents in a sentence in another language, e.g., english, for active learning."
P10-1040,1,"future work should explore methods for inducing phrase representations, as well as techniques for increasing in accuracy by using word representations in compound features."
P10-1042,5,how the classification performance will be affected by variances of the generated sots is worthy of study.
P10-1042,6,we plan to investigate on these issues in our future work.
P10-1043,1,"in the future work, we will integrate the subjectivity summarization strategy (pang and lee, 2004) to help discard noisy objective sentences."
P10-1043,1,"another interesting and practical idea is to integrate active learning (settles, 2009), another popular but principally different kind of semi-supervised learning approach, with our two-view learning approach to build high-performance systems with the least labeled data."
P10-1043,5,"moreover, we need to consider the cases when both x and y appear in a sentence."
P10-1044,4,"in the future, we wish to apply our model to automatically discover new inference rules and paraphrases."
P10-1047,6,this is an avenue of research that we intend to look at in the very near future.
P10-1050,1,"we hope this work opens a new perspective on decoding algorithms for a wide range of nlp problems, not just sequence labeling."
P10-1052,1,"in the future, we intend to study how sparsity can be used to speed-up training in the face of more complex dependency patterns (such as higher-order crfs or hierarchical dependency structures (rozenknop, 2002; finkel et al., 2008)."
P10-1052,1,"from a performance point of view, it might also be interesting to combine the use of large-scale feature sets with other recent improvements such as the use of semi-supervised learning techniques (suzuki and isozaki, 2008) or variable-length dependencies (qian , 2009)."
P10-1057,1,our future goal is to combine summarization and bibliometric techniques towards building automatic surveys that employ context information as an important part of the generated surveys.
P10-1058,4,an obvious next step is to examine how the model generalizes to other domains and text genres.
P10-1058,4,"we would also like to generalize the model to arbitrary rewrite operations, as our results indicate that compression rates are likely to improve with more sophisticated paraphrasing."
P10-1058,1,"although coherence is not so much of an issue for highlights, it certainly plays a role when generating standard summaries."
P10-1058,6,Future extensions are many and varied.
P10-1060,1,"we would also like to extend potential opinion targets to include multi-word phrases (nps and vps), in addition to individual words."
P10-1060,1,"finally, we do not identify polarity yet: this can be partially inherited from the initial lexicon and refined automatically via bootstrapping."
P10-1060,1,we also want to look at more complex syntactic patterns: choi et al.(2009) report that many errors are due to exclusive use of unigrams.
P10-1060,1,existing sentenceor phrase-level (trained) sentiment classifiers can also be used easily: when collecting/counting targets we can weigh them by prior score provided by such classifiers.
P10-1060,1,"as to future work, we intend to combine our method with known methods for topic-specific lexicon expansion (our method is rather concerned with lexicon “restriction”)."
P10-1061,2,"for future work, we aim extend this work to constructing a multilingual sentiment analysis system and evaluate it with multilingual datasets such as product reviews collected from different countries."
P10-1061,1,"we also plan to resolve the lexiconbased classifiers' classification bias towards subjective meanings with a list of objective words (esuli and sebastiani, 2006) and their multilingual expansion (kim , 2009), and evaluate the multilanguage-comparability of systems constructed with resources from different sources."
P10-1062,6,"future work in this direction involve detecting particular error types such as incorrect positions, inappropriate/unnecessary words (elliott, 2006) and automatically correcting errors."
P10-1062,4,"therefore our approach can be used for other machine translation systems, such as rule-based or example-based system, which generally do not produce n-best lists."
P10-1064,1,"furthermore, we want to experiment and improve on the adaptability of this method, as the current experiment is on a specific domain and language pair."
P10-1066,1,it is also possible to introduce certain rules or constraints to selectively form template slots rather than treating all words labeled with d as template slots.
P10-1066,1,"first, we can possibly apply linguistic knowledge to improve the quality of sentence patterns."
P10-1066,1,we plan to study how to use linguistic knowledge to guide the construction of sentence patterns and make them more meaningful.
P10-1066,3,"second, we have not quantitatively evaluated the quality of the template slots, because our judgment is only at the whole sentence pattern level."
P10-1066,3,we plan to get more human judges and more rigorously judge the relevance and usefulness of both the sentence patterns and the template slots.
P10-1067,1,"in the future, we would like to improve extraction pattern application and mine rare extraction patterns."
P10-1067,1,we also plan to develop methods to summarize answers pooled by a given comparator pair.
P10-1067,5,how to identify comparator aliases such as lv and louis vuitton and how to separate ambiguous entities such “paris vs. london” as location and “paris vs. nicole” as celebrity are all interesting research topics.
P10-1071,1,the latest developments in the lexical acquisition technology will in the near future enable fully automated corpus based processing of metaphor.
P10-1073,1,"in future, we plan to model entity relations which constitute 24% of implicit entity no relation cases, thus to improve the accuracy of relation detection."
P10-1074,1,"future directions for this work include automatically learning the variances, qm and q* in the hierarchical model, so that the degree of information sharing between the models is optimized based on the training data available."
P10-1074,1,"we are also interested in ways to modify the objective function to place more emphasis on learning a good joint model, instead of equally weighting the learning of the joint and single-task models."
P10-1075,3,"given that the rule representations and comparison methods use both pos and dependency information, a next step in evaluating and improving the methods is to examine automatically pos tagged data."
P10-1075,5,"furthermore, although we have indicated that differences in accuracy can be linked to differences in the granularity and particular distinctions of the annotation scheme, it is still an open question as to which methods work best for which schemes and for which constructions (e.g., coordination)."
P10-1077,1,"in the future, an important task would be the refinement or unsupervised generation of new hierarchies, using information theoretic or data-driven approaches."
P10-1077,1,"with regard to algorithms, we are also interested in other formulations for structural svms and their large-scale implementation as well as the combination of different distance measures, for example in ensemble learning."
P10-1079,3,"it would also be interesting to test the impact of another lexical language model, learned on nonsms sentences."
P10-1080,5,"one open question that we would like to investigate in the future is whether l2p conversion accuracy could be improved by treating letter-phoneme alignment links as latent variables, instead of committing to a single best alignment."
P10-1081,1,"in addition, extending the approach to cross-document information, following (ji and grishman 2008), may be able to further improve performance."
P10-1082,1,the participants will also be interviewed in order to find out which version of the system is more pleasant to use.
P10-1082,3,the strategies will then be evaluated through user tests where the participants will compare an application with these strategies with an application without them.
P10-1082,1,next we intend to implement strategies for interruption and resumption in the dico dialogue system.
P10-1086,4,"in our future work, we may try this algorithm on syntax-based mt systems and phrase-based mt systems with different context features."
P10-1087,1,"this can be extended to include mappings from multiple languages to wordnet synsets, with the hope that the weights and link structure will then allow the algorithm to make the final disambiguation decision."
P10-1087,1,additional scenarios include dealing with co-reference on the linked data web or mappings between thesauri.
P10-1087,1,"in future work, we would like to investigate how our algorithm behaves in extended settings, e.g. we can use heuristics to connect isolated, unconnected articles to likely candidates in other wikipedias using weighted edges."
P10-1088,2,"also, we would like to test with more languages, increase the amount of data we can gather, and investigate stopping criteria further."
P10-1088,1,"also, we would like to investigate increasing the efficiency of the selection algorithm by addressing issues such as the one raised by the 12 may example presented earlier."
P10-1090,1,one possible solution is to do the probability rescaling off-line before kernel calculation.
P10-1090,1,"in addition, as suggested by one reviewer, we may consider rescaling the probabilities (exponentiating them by a constant value) that are used to compute the fractional counts."
P10-1090,3,"in the future, we would like to verify the forest kernel in more nlp applications."
P10-1090,6,this would be a very interesting research topic of our future work.
P10-1093,1,"thus, another avenue of further research is to generalize from the linear approach."
P10-1093,4,"instead, we might need some in-between application of simple nonlinear functions in the spirit of quantum-collapsing of a ""superposed"" mental state (such as the winner takes it all, survival of the top-k vector entries, and so forth)."
P10-1094,3,"in future work, we will manually translate english reference summaries into chinese reference summaries, and then adopt the rouge metrics to perform automatic evaluation of the extracted chinese summaries by comparing them with the chinese reference summaries."
P10-1096,1,"the future for this work would involve natural extensions such as mixing over the space of word alignments; this would allow application to mt-like tasks where flexible word reordering is allowed, such as abstractive sentence compression and paraphrasing."
P10-1097,1,"another direction for further study will be the generalization of our model to larger syntactic contexts, including more than only the direct neighbors in the dependency graph, ultimately incorporating context information from the whole sentence in a recursive fashion.acknowledgments."
P10-1099,1,"there are several potential avenues for further progress towards this goal, including the development of more portable srl pipeline systems, and especially parsers."
P10-1099,1,developing techniques that can incrementally adapt to new domains without the computational expense of retraining the crf model every time would help make open-domain srl more practical.
P10-1100,6,we will restrict the user input to lexicon words to avoid manual orthography correction.
P10-1100,2,"as far as the data collection is concerned, we plan to replace the web form with a browser game, following the example of von ahn and dabbish (2008)."
P10-1100,6,"to realize this goal, we are going to automatize several processing steps that were done manually for the current study."
P10-1100,1,"further, we will implement some heuristics to filter unusable instances by matching them with the remaining data."
P10-1101,1,"perhaps with the help of semantic feedback the system can automatically improve predicate identification, which in turn allows it to correct the observed intransitive sentence error."
P10-1104,1,"future work should consider the dynamics in more detail, develop more complex models (for example, by relaxing the infinite population assumption, allowing for stochastic dynamics), and quantitatively compare model predictions and observed dynamics."
P10-1107,3,our model currently operates purely on the vocabulary level and thus fails to take this contextual information into account.
P10-1107,1,our model fails to take into account the known frequency of hebrew words and morphemes.
P10-1107,6,we hope to address several issues in future work.
P10-1110,1,For future work we plan to extend it to constituency parsing.
P10-1114,4,future work includes the extension of cl-scl towards a general approach for cross-lingual adaptation of natural language processing technology.
P10-1115,5,it should be very interesting to explore how to assign weights to the edges and study whether weighted graphs can further improve performance.
P10-1115,1,"second, it would also be interesting to further extend pclsa to accommodate discovering topics in each language that aren't well-aligned with other languages."
P10-1115,6,our work opens up some interesting future research directions to further explore.
P10-1115,3,"first, in this paper, we have only experimented with uniform weighting of edge in the bilingual graph."
P10-1118,1,"still, future work will be required to optimize a cost model for eventual application where even more accurate cost models may be required."
P10-1120,1,"computationally, the inclusion of markov logic allowed the discourse module to compute well-formed coreference chains, and opens two avenues of future research."
P10-1120,4,"second, we would like to make greater use of the logical elements by applying it to problems where inference is necessary, such as resolving bridging anaphora (haviland and clark, 1974)."
P10-1120,1,"first, it ought to be possible to make the probabilistic logic more naturally incremental, rather than re-running from scratch at each word."
P10-1123,5,the next challenge is to translate the theoretical gain into practical benefit.
P10-1123,1,"our analysis demonstrates that improvements are necessary both on the side of discourse reference resolution systems, which need to cover more types of references, as well as a better integration of discourse information in entailment systems, even for those relations which are within the scope of available resolvers."
P10-1124,2,"this paper used distributional similarity, but other sources of information are likely to improve performance further."
P10-1124,1,"this will introduce a challenge to our current optimization algorithm due to complexity issues, and will require careful handling of predicate ambiguity."
P10-1124,1,"in future work, we would like to learn general entailment graphs over a large number of nodes."
P10-1124,1,"additionally, we will investigate novel features for the entailment classifier."
P10-1125,1,"firstly, we will further improve the performance of our method by adopting the nontextual features."
P10-1125,1,"secondly, more research will be taken to put forward other architectures of the deep networks for qa detection."
P10-1126,1,"we could, however, improve grammaticality more globally by generating a well-formed tree (or dependency graph)."
P10-1126,1,"rather than adopting a two-stage approach, where the image processing and caption generation are carried out sequentially, a more general model should integrate the two steps in a unified framework."
P10-1126,1,we also believe that our approach would benefit from more detailed linguistic and non-linguistic information.
P10-1126,1,"indeed, an avenue for future work would be to define a phrase-based model for both image annotation and caption generation."
P10-1127,4,one is to explore how dependency patterns could be used to produce generative summaries and/or perform sentence trimming.
P10-1127,1,"finally, we also plan to analyze automated ways for learning information structures (e.g. what is the flow of facts to describe a location) from existing image descriptions to produce better summaries."
P10-1127,3,evaluation should be extended to investigate the utility of the automatically generated image descriptions for image retrieval.
P10-1127,1,"another is to investigate how dependency patterns might be automatically clustered into groups expressing similar or related facts, rather than relying on manual categorization of dependency patterns into categories such as type, year, etc. as was done here."
P10-1127,6,there are a number of avenues to pursue in future work.
P10-1128,1,"second, representing distractors in a reference resolution model is also a key."
P10-1128,2,"finally, more investigation is needed for considering other extra-linguistic information, such as eye-gaze, for exploring what kinds of information is critical to recognizing reference in dialogue."
P10-1128,1,"in order to enhance this kind of reference resolution, there are several possible future directions."
P10-1128,2,"first, in the current problem setting, we exclude zero-anaphora (i.e.omitted expressions refer to either an expression in the previous utterances or an object on a display deictically)."
P10-1128,1,"however, zero-anaphora is essential for precise modeling and recognition of reference because it is also directly related with the recency of referents, either textually or situationally."
P10-1129,1,an interesting avenue of future work is to explore an alternative approach which learns these phenomena by combining linguistic information with knowledge gleaned from an automatically induced environment model.
P10-1130,2,a logical next step would be to explore the connection between syntax and mark-up for genres other than a news-style blog and for languages other than english.
P10-1132,1,in future work we intend to analyze the output of such algorithms in order to improve pos tag sets.
P10-1133,2,"hence a promising direction would be to use our approach in combination with wikipedia data and with additional manually created attribute rich sources such as web tables, to achieve the best possible performance and coverage."
P10-1133,1,we would also like to explore the incorporation of approximate discovered numerical attribute data into existing nlp tasks such as noun compound classification and textual entailment.
P10-1134,4,"in the near future, we aim to apply the output of our classifiers to the task of automated taxonomy building, and to test the wcl approach on other information extraction tasks, like hypernym extraction from generic sentence fragments, as in snow (2004)."
P10-1136,1,"in the future, it would be useful to explore other approaches to automatic lexicon discovery to improve the quality or to increase the coverage of both ih and im lexicons, and to systematically evaluate their impact on query understanding performance."
P10-1137,1,"as part of future work, we plan to vary the model interpolation parameters dynamically to improve the performance in case of multiple assisting languages."
P10-1139,1,"it would be interesting to study opinion holders, e.g. its seniority, for opinion retrieval."
P10-1141,4,in the future we plan to test the proposed weighting functions in other domains such as topic classification and additionally extend the approach to accommodate multi-class classification.
P10-1145,3,"in the future, we will do more experiments on rule coverage to compare the constituency-to constituency model with our model."
P10-1145,1,"furthermore, we will replace 1-best dependency trees on the target side with dependency forests to further increase the rule coverage."
P10-1147,1,"in future work, we look forward to developing extraction set models for richer formalisms, including hierarchical grammars."
P10-1148,1,"in addition to refinements of our work, our next step is to develop a method for representing and extracting actual experiences from experience-revealing sentences."
P10-1148,4,"furthermore, considering that only 13% of the blog data we processed contain experiences, an interesting extension is to apply the methodology to extract other types of knowledge such as facts, which are not necessarily experiences."
P10-1148,1,"in order to increase the coverage even further and reduce the errors in lexicon construction, i.e., verb classification, caused by data sparseness, we need to devise a different method, perhaps using domain specific resources."
P10-1149,1,"topics for future work include the incorporation of other kinds of semantic constraint for improved class-instance acquisition, further investigation into per-node sparsity constraints in graph-based ssl, and moving beyond bipartite graph constructions."
P10-1150,1,"we plan to use the algorithm described in this paper to learn the selectional restrictions of numerous other relations, in order to build a rich knowledge repository"
P10-1153,1,"the modified failure detection concept suggests several directions for future work, including evaluation of the new encodings in the context of a large-scale hpsg parser; incorporation of further developments in constraint solvers; and the possibility of approximate encodings that would permit one-sided errors as in traditional bloom filtering."
P10-1154,2,"however, since wordnet++ is part of a multilingual semantic network (navigli and ponzetto, 2010), we plan to explore the impact of this knowledge in a multilingual setting."
P10-1154,4,"moreover, while the mapping has been used to enrich wordnet with a large amount of semantic edges, the method can be reversed and applied to the encyclopedic resource itself, that is wikipedia, to perform disambiguation with the corresponding sense inventory (cf.the task of wikification proposed by mihalcea and csomai (2007) and milne and witten (2008b))."
P10-1155,3,"as future work, we would like to test our work on the environment domain data which was released as part of the semeval 2010 shared task on “allwords word sense disambiguation on a specific domain”."
P10-1156,2,adding semcor examples to transcont should have a positive impact on performance.
P10-1156,2,also adding more languages as illustrated by the dr02 work should also yield much better performance.
P10-1157,5,"nevertheless, future work should investigate whether syntactic information can improve performance in more complex domains."
P10-1159,1,"an interesting topic for future work, for instance, is to expand our notion of context by taking visual and discourse salience into account when generating res."
P10-1159,1,"in addition, we plan to experiment with assigning costs to planning operators in a metric planning problem (hoffmann, 2002) in order to model the cognitive cost of an re (krahmer et al., 2003) and compute minimal-cost instruction sequences."
P10-2001,4,"in future work, we plan to apply this method with paraphrases derived from a massive corpus such as the web corpus and apply this method to a hierarchical phrase based smt."
P10-2002,2,a challenge might exist when running the me training toolkit over a big size of training instances from the large scale training data.
P10-2002,2,"in the future work, we will explore more useful features and test our method over the large scale training corpus."
P10-2003,1,"in addition, how to further improve the reordering model by distinguishing the derivations with different probabilities will become another study emphasis in further research."
P10-2004,1,the idea of selecting syntactic constraints is compatible with the idea of using constraints softly; we plan to combine the two ideas and obtain further improvements in future work.
P10-2008,3,"finally, we would like to compare the performance of our method to other state-of-the-art approaches to authorship prediction."
P10-2008,1,an interesting extension of our current approach is to consider discriminative training of pcfgs for each author.
P10-2009,1,"therefore, designing better prompts would be the key factor in improving learning and user satisfaction."
P10-2009,1,one way to improve the help messages may be to have the system indicate more clearly when user terminology is a problem.
P10-2012,3,"in machine translation, evaluating the fluency of system output is crucial, and a model that predicts processing difficulty could be used for this, or to guide the choice between alternative translations, and maybe even to inform human post-editing."
P10-2013,1,"because the masc is an open resource that the community can continually enhance with additional annotations and modifications, the project serves as a model for community-wide resource development in the future."
P10-2013,2,"past experience with corpora such as the wall street journal shows that the community is eager to annotate available language data, and we anticipate even greater interest in masc, which includes language data covering a range of genres that no existing resource provides."
P10-2014,1,"in future work, we will explore a method of increasing the recall of error correction by constructing a wide-coverage stsg."
P10-2015,2,"in future work with mncd as an mt evaluation measure, we are planning to evaluate synonym dictionaries for other languages than english."
P10-2015,1,the synonym module for english does not distinguish between different senses of words.
P10-2017,1,we will experiment with those methods to determine the tradeoff of runtime and accuracy for this task.
P10-2017,1,"another area of future work is to move beyond bag-of-words context: it is known from wsd that syntactic and bag-of-words contexts provide complementary information (florian , 2002; szpektor , 2008), and we hope that they can be integrated in a more sophisticated exemplar model."
P10-2017,3,"finally, we will to explore task-based evaluations."
P10-2018,2,a further research direction we are investigating is exploitation of unlabeled texts.
P10-2019,5,it is still an open question what kinds of syntactic information is most important for chinese srl.
P10-2019,1,we suggest that our attempt at semantics-driven shallow parsing is a possible way to better exploit this problem.
P10-2020,1,future research into association measures that are not based on the independence assumption will also include considering different em variants and other automatically learnable models besides the amms used in this paper.
P10-2020,5,a possible obstacle in the adoption of amms in collocation extraction is that we have not provided any heuristic for setting the number of classes for the amms.
P10-2020,6,we hope to be able to look into this question in future research.
P10-2021,1,"yet another direction of this research is to investigate if our methodology is applicable to other types of collocations, such as an and pn in addition to vn dealt with in this paper."
P10-2021,3,we will conduct a user study to investigate whether our system would improve a learner's writing in a real setting.
P10-2025,2,future work will involve examining the proposed method for different language pairs such as english-chinese and english japanese and evaluating the impact of our proposed method on smt performance.
P10-2025,2,we will also apply our proposed method to a larger data sets of multiple domains since we can expect a further improvement in word alignment accuracy if we use more bilingual sentences and more monolingual knowledge.
P10-2026,1,"besides, the quality of the parser is another effect for this method."
P10-2026,1,"in the future, we  are plan to exploit some discriminative approach to train parameters of this feature, such as em algorithm (hasan et al., 2008) or maximum entropy (he et al., 2008)."
P10-2026,1,"in the future, we we are plan to exploit some discriminative approach to train parameters of this feature, such as em algorithm (hasan , 2008) or maximum entropy (he , 2008)."
P10-2032,1,future research aims at the investigation of improved cms to be integrated in our imt system.
P10-2033,1,"in the future, we plan to improve robustness to parsing errors by using not just one, but multiple subject boundary hypotheses."
P10-2033,1,we will also investigate the integration of vs reordering in smt decoding.
P10-2036,4,"in the future, we would also like to try applying similar constraints to the more complex task of joint induction of pos tags and dependency parses."
P10-2040,1,we believe that the svd2 algorithm presented here could provide a launching pad for an approach that would successfully address the disambiguation challenge.
P10-2040,1,it would do so by allowing a gradual and carefully controlled amount of ambiguity into an initially non disambiguating model.
P10-2042,1,"while it is difficult to extend the local gibbs sampler to the case where the tree is not observed, the dynamic program for our blocked sampler can be easily used for unsupervised inference by omitting the tree matching constraints."
P10-2042,1,a particularly interesting avenue for further research is to employ our blocked sampler for unsupervised grammar induction.
P10-2043,1,"possibilities for future work include the pairing of this method with algorithms for dynamic clustering, as well as exploring algorithms for different distances (e.g., l2) and estimators (e.g., asymmetric estimators (dong et al., 2009))."
P10-2044,1,"in future work we will extend the types of questions that we consider, and also allow for multiword answers."
P10-2045,1,"in future work we plan to investigate combining framenet and wordnet rule-sets in a transitive manner, instead of their simple union."
P10-2047,1,"another direction is exploring words as members of semantic fields 鈥 while word use might be insufficiently consistent within a perspective, selection of a semantic domain might show better consistency."
P10-2047,1,"In future work, we plan to experiment with additional features."
P10-2048,1,incorporating the confidence from the translator may further improve the performance.
P10-2048,1,"also, in the current work, we select the pivot features by simple ranking with mutual information, which only considers the distribution information."
P10-2048,1,as future research we believe a promising avenue of exploration is to construct a probabilistic version of the scl approach which could offer a more explicit model of the relations between the two domains and the relations between the search engine results and the model parameters.
P10-2051,5,"another issue of our two-step crf method is that the training complexity increases quadratically according to the size of the label set, and how to reduce the training time needs more research."
P10-2051,2,in future work we are planning to combine our system with multilingual systems.
P10-2051,1,also we want to make use of acoustic information in machine transliteration.
P10-2051,1,we are currently investigating discriminative training as a method to further improve the jscm.
P10-2053,1,one line of future research is to investigate other types of domain-independent frames that exhibit useful regularities.
P10-2056,1,we plan to investigate this problem in the future since the choice of hyperparameters has a strong impact on the performance of the model.
P10-2057,1,"in future work, we plan to (a) investigate cascaded learning methods (sutton , 2007) to improve the detection of ddas further by using detected decision regions and (b) extend hgms beyond three levels in order to integrate useful semantic information such as topic structure."
P10-2060,1,we also plan to model the order of sentences globally.
P10-2060,1,one future work is to include important information other than sentiments in the summaries.
P10-2061,1,"based on the rating results we will draw polarity profiles in order to see where, within customer reviews, polarity is best manifested and whether there are other 鈥渃andidates鈥 sentences that would serve as useful polarity indicators."
P10-2061,1,the profiles will be used as a feature in our computational analysis.
P10-2063,4,"we also will be using this system as a preprocessing step for a parser, as part of a complete arabic nlp pipeline."
P10-2063,1,obvious future work starts with the need to include determiner information in the pos tags and the important noun/adj distinction.
P10-2063,3,we will be implementing and comparing these alternatives.
P10-2065,3,"we would like to further investigate the role of parsing in error detection by looking at other error types and other text types, e.g. machine translation output."
P10-2065,3,"for our immediate future work, we plan to carry out the esl evaluation on a larger test set to better gauge the usefulness of a parser in this context, to carry out a detailed error analysis to understand why certain parse features are effective and to explore a larger set of features."
P10-2066,1,"in addition, we also plan to combine some syntactic patterns (etzioni et al.2005; sarmento et al.2007) to further improve the results."
P10-2066,3,"in our future work, we plan to experiment with various other pu learning methods (liu 2003; lee and liu, 2003; li 2007; elkan and noto, 2008) on this entity set expansion task, as well as other tasks that were tackled using distributional similarity."
P10-2067,2,in future we wish to work with word alignments for other language pairs like arabic and english.
P10-2067,2,we have tested out the feasibility of obtaining human word alignment data using amazon mechanical turk and plan to obtain more data reduce the cost of annotation.
P10-2070,1,in future work we plan on working towards improving the quality of our sentence reconstruction step in order to produce better and more readable sentences.
P10-3002,1,"besides the implementation, future work will focus on refining the theoretical foundations of relational pomdps for dialogue (including how to specify the transition, observation and reward functions in such a relational framework), as well as investigating the use of reinforcement learning for policy optimisation based on simulated data."
P10-3005,3,we intend to compare our system to other available work in the future.
P10-3005,4,our future work will include a further examination of the merits of its application for knowledge-sparse languages.
P10-3006,2,other future work includes optimizing the segmentation of both sides of the corpus and experimenting with other language pairs.
P10-3006,1,one avenue for future work is to relax some of the several independence assumptions made in the generative model.
P10-3006,1,"for example, independence of consecutive morphs could be relaxed by an hmm model for transitions between morphs (creutz and lagus, 2007)."
P10-3007,4,"furthermore, we plan to integrate the proposed interface within an computer-based interactive platform for speech therapy."
P10-3007,6,a formal usability study is needed in order to establish the degree of utility and satisfaction with the interface.
P10-3008,6,"in addition, a new affective lexicon could be automatically detected based on learning correlation of the blog text and the moods tagged."
P10-3008,1,another direction is to integrate negation information to learn more cohesive association in affect scores between moods and affective words.
P10-3008,1,future work will take into account the temporal dimension to trace changes in mood patterns over time in blogosphere.
P10-3009,6,we can discover how words are organized and connected within this network.
P10-3009,1,"in addition, we want to build a network of chinese word association."
P10-3009,2,we plan to remove noisy words in the future.
P10-3009,5,"fourthly, how to deal with ambiguous query word is also left as our future work."
P10-3009,1,"furthermore, we want to take the advantage of learning to rank literature (liu, 2009) to further improve the performance of related word retrieval task."
P10-3009,5,another important issue is how to build a complete and accurate ground truth for related word retrieval task.
P10-3009,4,and this word association network will be quite useful for foreigners to learn chinese.
P10-3010,2,another possibility is to extend the feature set in a more critical way than what is done now.
P10-3010,3,another comparison we would like to do is with linear svms.
P10-3010,1,for instance the combination of a pos-tag and cpostag for a given word is now included.
P10-3010,1,a possible solution is to use kernels with confidence-weighted classification in the same way they are used with the svms.
P10-3010,1,we will also try to do feature selection on a more general level as this can boost accuracy a lot.
P10-3011,4,we hope that future research is able to use this feature to provide more specific individual frames.
P10-3011,1,"because cyc is consistently changing and growing, an approach that uses cyc relationships will be able to improve as the knowledge base improves its coverage."
P10-3013,1,"in order to measure the impact of the frame clusters for the sp acquisition, we plan to run the system for sp acquisition without performing the clustering step, thus defining all constructions as singleton sets containing one frame each."
P10-3013,2,"for this, a larger automatically parsed corpus will be necessary."
P10-3014,1,"also, the system will be expanded to identify arguments using a tree distance algorithm."
P10-3014,1,"future work will focus on improving the performance of the system by: a) trying to extend the sub-trees which will contain more contextual information, b) using different approaches to label semantic relations discussed in section 5."
P10-3015,1,we are also exploring the possibility of including some semantic information about the words while defining weights.
P10-3015,2,the sandhi with white spaces also needs to be handled.
P10-3016,4,"we are currently planning to port this setting to co-training, another bootstrapping algorithm."
P10-3016,1,another direction can be adapting bootstrapping parameters to fit the semantic role labeling complexity.
P10-3016,1,one direction for future work can be adapting the architecture of the srl system to better match with the bootstrapping process.
P10-3017,1,"also, we are going to analyze the influence of single features on the classification and determining optimal feature sets, as well as the question of including patterns in the feature set."
P10-3017,1,in future work we will integrate word sense disambiguation as well as information about predicate-argument structure.
P10-4002,1,"further training pipelines are under development, including minimum risk training using a linearly decomposable approximation of bleu (li and eisner, 2009), and mira training (chiang et al., 2009)."
P10-4002,1,we are also improving support for parallel training using hadoop (an open-source implementation of mapreduce).
P10-4003,2,"we also plan to annotate the data we collected for evidence of misunderstandings, i.e., situations where the system arrived at an incorrect interpretation of a student utterance and took action on it."
P10-4003,5,"in dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment."
P10-4003,1,the annotated data will be used to evaluate the accuracy of existing paraphrasing and textual entailment approaches and to investigate how to combine such algorithms with the current deep linguistic analysis to improve system robustness.
P10-4003,3,we are also planning experiments that will allow us to evaluate the effectiveness of individual strategies implemented in the system by comparing system versions using different tutoring policies.
P10-4004,1,"in future work, we plan to adapt the tool so that it can be used with wordnets for other languages as well."
P10-4005,1,"for that purpose, an integration of the escidoc research environment3 into weblicht is planned."
P10-4005,4,"in the future, an online workspace has to be implemented so that annotated text corpora created with weblicht can also be stored in and retrieved from the net."
P10-4006,1,"future work will be focused on providing support for syntactic features, including dependency parsing as described by (pado and lapata, 2007), reference implementations of algorithms that use this information, non-linear dimensionality reduction techniques, and more advanced clustering algorithms."
P10-4007,1,on the one hand we will focus on single components like hybrid parsing of input utterances and dialog interpretation in terms of precision and recall.
P10-4007,3,on the other hand an evaluation of the two different scenarios regarding the usability are planned in experiments with end users.
P10-4007,1,moreover we will integrate some opinion mining and sentiment analysis functionality which can be helpful to better detect and understand the users' preferences in the furniture sales agents scenario.
P10-4010,6,"our preliminary work on risk maps can be put on a more theoretical footing (hunter, 2000)."
P10-4010,4,"extracted negative and also positive risks can be used in many applications, ranging from e-mail alerts to determinating credit ratings."
P11-1002,2,"for future work, it will be interesting to see if we can exploit both parallel and non-parallel data to improve on both."
P11-1003,4,"also, it is interesting to automatically learn a word set for realigning."
P11-1003,3,it will be valuable to investigate the feasibility to re-align all the target words to source tree fragments.
P11-1003,4,"finally, we intend to extend the proposed approach to tree-to-tree translation frameworks by this idea comes from one reviewer, we express our thankfulness here.re-aligning subtree pairs (liu , 2009; chiang, 2010) and consistency-to-dependency frameworks by re-aligning consistency-tree-to-dependency-tree pairs (mi and liu, 2010) in order to tackle the rulesparseness problem."
P11-1004,5,in future work we hope to explore the utility of phrases with productive morpheme boundaries and explore why they are not used more pervasively in the decoder.
P11-1004,3,evaluation measures for morphologically complex languages and tuning to those measures are also important future work directions.
P11-1004,1,"also, we would like to explore a non-pipelined approach to morphological preand post-processing so that a globally trained model could be used to remove the target side morphemes that would improve the translation model and then predict those morphemes in the target language."
P11-1005,1,we also plan to improve the filtering heuristics and to explore further ways of detecting human coder errors.
P11-1005,1,future work should focus on finding optimal parameter settings to make the filtering method more robust even for noisier data sets.
P11-1005,3,"finally, we plan to test our method in a real-world annotation scenario."
P11-1006,4,"as future work, we plan to add contextual features into the model and apply our method to other data sets in other tasks."
P11-1007,4,"the primary area of the future work is to apply our method to structured prediction problems in nlp, such as syntactic parsing or semantic role labeling, where construction of auxiliary tasks proved problematic."
P11-1007,1,"another direction is to favor domain invariability not only of the expectations of individual variables but rather those of constraint functions involving latent variables, features and labels."
P11-1011,4,"we are encouraged by the success of our joint query annotation technique, and intend to pursue the investigation of its utility for ir applications."
P11-1011,4,"in the future, we intend to research the use of joint query annotations for additional ir tasks, e.g., for constructing better query formulations for ranking algorithms."
P11-1013,1,"second, it might be interesting to study the effect of introducing a tradeoff parameter to balance the effect of original and new features."
P11-1013,1,"first, polarity-bearing topics generated by the jst model were simply added into the original feature space of documents, it is worth investigating attaching different weight to each topic maybe in proportional to the posterior probability of sentiment label and topic given a word estimated by the jst model."
P11-1016,1,we are also interested in exploring relations between twitter accounts for classifying the sentiments of the tweets published by them.
P11-1016,3,"as mentioned in section 4.1, in future we would like to explore the relations between a target and any of its extended targets."
P11-1019,1,"we plan to experiment with better error detection techniques, since the overall error-rate of a script is one of the most discriminant features."
P11-1022,1,"first, we plan to apply confidence estimation to perform a second-pass constraint decoding."
P11-1022,3,"moreover, we also intend to perform a user study on our visualization prototype to see if it increases the productivity of post-editors."
P11-1025,4,"for future work, there are other interesting decipherment tasks where our method can be applied."
P11-1025,5,"one challenge is to crack the unsolved zodiac-340 cipher, which presents a much harder problem than the solved version."
P11-1030,4,as future work we would like to explore the use of lowbow representations for profile-based aa and related tasks.
P11-1030,1,"also, we would like to develop model selection strategies for learning what combination of hyperparameters works better for modeling each author."
P11-1031,5,"many interesting research questions still remain pertaining to the best way to select and partition the training corpora, align adult and intermediate lsa models, correlate the results with real school grade levels, as well as other free parameters in the model."
P11-1032,1,"many additional approaches to detecting deceptive opinion spam are also possible, and a focus on approaches with high deceptive precision might be useful for production environments."
P11-1032,3,"possible directions for future work include an extended evaluation of the methods proposed in this work to both negative opinions, as well as opinions coming from other domains."
P11-1033,4,"in future work, we would like to apply the joint learning idea to other learning frameworks (e.g."
P11-1033,4,"svms), and to extend the proposed model to handle word-level parallel information, e.g. bilingual dictionaries or word alignment information."
P11-1033,2,another issue is to investigate how to improve multilingual sentiment analysis by exploiting comparable corpora.
P11-1034,3,"second, we will evaluate using other extraction units, such as applying preprocessing to remove disfluencies and concatenate incomplete sentence segments together."
P11-1034,3,"in addition, it would be interesting to test our system on speech recognition output and automatically generated da boundaries to see how robust it is."
P11-1034,5,"in future work, we will address some issues identified from our error analysis."
P11-1034,1,"first, we will investigate ways to represent a sentence’s topic relevance."
P11-1035,1,discovering and developing methods for issues which involve more than two disputants groups is a future work.
P11-1037,1,"firstly, we hope to develop tweet normalization technology to make tweets friendlier to the ner task."
P11-1037,2,"secondly, we are interested in integrating new entities from tweets or other channels into the gazetteers."
P11-1037,1,"in future, we plan to further improve the performance of our method through two directions."
P11-1038,1,"first, we plan to improve our ill-formed word detection classifier by introducing an oov word whitelist."
P11-1038,1,"furthermore, we intend to alleviate noisy contexts with a bootstrapping approach, in which ill-formed words with high confidence and no ambiguity will be replaced by their standard forms, and fed into the normalization model as new training data."
P11-1038,6,"in future work, we propose to pursue a number of directions."
P11-1041,4,"in the future, we plan to generalize our approach so that it can be applied to the task of generating transliterations, and to combine data from distinct g2p dictionaries."
P11-1041,4,"we would also like to apply our approach to web data; we have shown that it is possible to use noisy transliteration data, so it may be possible to leverage the noisy ad hoc pronunciation data as well."
P11-1041,1,"finally, we plan to investigate earlier integration of such external information into the g2p process for single systems; while we noted that re-ranking provides a general approach applicable to any system that can generate n-best lists, there is a limit as to what re-ranking can do, as it relies on the correct output existing in the n-best list."
P11-1045,1,"first, we will try combining our approach with constituent-level coarse-to fine pruning."
P11-1045,5,we plan to explore a number of remaining questions in future work.
P11-1046,5,whether np-hardness can be shown for unrestricted parsing strategies is an important question for future work.
P11-1048,1,we also plan to revisit the idea of combined training.
P11-1048,1,"in future work we plan to integrate the pos tagger, which is crucial to parsing accuracy (clark and curran, 2004b)."
P11-1053,1,"moreover, extending word clustering to phrase clustering (lin and wu, 2009) and pattern clustering (sun and grishman, 2010) is worth future investigation for relation extraction."
P11-1053,1,"based on the experimental results, we plan to investigate additional ways to improve the performance of relation detection."
P11-1056,5,there are probably many near misses when we apply our structure patterns on predicted mentions.
P11-1056,5,relaxing this might potentially recover additional valid mention pairs and improve performance.
P11-1056,5,"for instance, for both premodifier and possessive structures, we require that one mention completely includes the other."
P11-1056,1,it will also be interesting to feedback the predictions of the structure patterns to the mention entity typing classifier and possibly retrain to obtain a better classifier.
P11-1056,1,we could also try to learn classifiers to automatically identify and disambiguate between the different syntactico-semantic structures.
P11-1062,1,this would require explicit modeling of predicate ambiguity and using approximation techniques when an optimal solution cannot be attained.
P11-1062,1,"in future work, we aim to scale the algorithm further and learn entailment rules between untyped predicates."
P11-1063,1,"by increasing the beam size and distortion limit of the baseline system, future work may examine whether a baseline system with comparable runtimes can achieve comparable translation quality."
P11-1063,1,our future work seeks to incorporate largescale n-gram language models in conjunction with incremental syntactic language models.
P11-1064,1,"for future work, we plan to refine hlen to use a more appropriate model of phrase length than the uniform distribution, particularly by attempting to bias against phrase pairs where one of the two phrases is much longer than the other."
P11-1064,3,"in addition, we will test probabilities learned using the proposed model with an itg-based decoder."
P11-1064,1,"we will also examine the applicability of the proposed model in the context of hierarchical phrases (chiang, 2007), or in alignment using syntactic structure (galley et al., 2006)."
P11-1066,1,"first, question structure should be considered, so it is necessary to combine the proposed approach with other question retrieval methods (e.g., (duan et al., 2008; wang et al., 2009; bunescu and huang, 2010)) to further improve the performance."
P11-1066,4,"second, we will try to investigate the use of the proposed approach for other kinds of data set, such as categorized questions from forum sites and faq sites."
P11-1067,1,"in the future, we suggest reporting ned along with the current measures."
P11-1072,4,"we plan to further explore our new lexicons performance for other languages and tasks, such as oov spoken term detection."
P11-1073,1,we will further experiment with syntactic complexity measures to balance construct richness and model simplicity.
P11-1073,1,"furthermore, we can also experiment with additional types of machine learning models and tune parameters to derive scoring models with better performance."
P11-1074,1,"in our future work, we will develop models with only within-word context, and thus allowing us to explore lattice rescoring, which we expect will yield more performance gain."
P11-1075,1,"it would be interesting to model triggers as latent variables in the document coding process, in a manner similar to how latent subjective sentences have been used in document level sentiment analysis (yessenalina et al., 2010)."
P11-1075,1,this would allow us to employ a learned matching component that is trained to compliment our classification component.
P11-1075,1,"in the future, we would like to augment our dictionary-based matching component with entity recognition technology."
P11-1076,1,future work will concentrate on improving the quality of the answer alignments by training a model to directly output graph-to-graph alignments.
P11-1077,1,"in the future we plan on using age and other metadata to improve results in larger tasks such as identifying opinion, persuasion and power by targeting our approach in those tasks to the identified age of the person."
P11-1077,1,"another approach that we will experiment with is the use of ranking, regression, and/or clustering to create meaningful age groups."
P11-1078,1,"in addition, we had promising early results for classification of author-recipient links with 200 to 500 words, so we plan to explore performance improvements for links of few words."
P11-1078,1,"finally, we hope to investigate how spm and sna can enhance one another, and explore other lect classification problems for which the ground truth can be found."
P11-1078,1,"in early, unpublished work, we had promising results with generative model-based approach to spm, and we plan to revisit it; language models are a natural fit for lect modeling."
P11-1080,2,"although we demonstrate scalability to more than a million mentions, we plan to explore performance on datasets in the billions."
P11-1080,4,"our work enables cross document coreference on very large corpora, and we would like to explore the downstream applications that can benefit from it."
P11-1080,1,"since our approach supports parameter estimation, we expect significant accuracy gains with additional features and supervised data."
P11-1080,1,we also plan to examine inference on complex coreference models (such as with entity-wide factors).
P11-1080,1,Another possible avenue for future work is that of learning the factors.
P11-1081,3,"finally, we would like to test our model with english constructions which closely resemble zero anaphora."
P11-1081,1,we plan to examine ways of appropriately estimating an absolute score from a set of relative scores for further refinement.
P11-1081,1,we also intend to experiment with introducing more sophisticated antecedent identification models in the ilp framework.
P11-1084,1,"in the future, we plan to improve the learning of translation rules with binarized forests."
P11-1091,6,we also plan to explore aspects of corpus maintenance in dynamic (constantly changing) domains.
P11-1091,1,"in future work, we plan to explore how to utilize additional domain knowledge to better estimate the correlation between words."
P11-1094,1,"for future work, we plan to use a more accurate language model, and add more types of complex error models, such as word deletion and word ordering error models to improve performance and address other types of errors."
P11-1097,4,"in future work, we plan to develop more efficient methods of using search results for cross-domain generalization to avoid the cost of issuing a large number of queries to search engines."
P11-1097,4,"in the future, we hope to be able to show that other nlp tasks can also benefit from such an enriched context representation. future work."
P11-1098,1,we will investigate new text sources and algorithms to try and capture more knowledge.
P11-1098,1,"while all learning algorithms require parameters, we think it is important for future work to focus on removing some of these to help the algorithm be even more robust to new domains and genres."
P11-1101,4,"moreover, it would be interesting to carry over our methodology to a purely statistical linearization system where the relation between an input representation and a set of candidate realizations is not so clearly defined as in a grammar-based system."
P11-1101,1,"in future work, we will extend our experiments to a wider range of alternations and try to capture inter-sentential context more explicitly."
P11-1102,2,further use of contextual features will more thoroughly represent the information we want our model to take into account.
P11-1102,4,"we are now interested in how we can apply this to the larger questions of positioning we began this paper by asking, especially in describing speaker positioning at various instants throughout a single discourse."
P11-1102,3,"our segmentation accuracy is also fairly low, and further examination of segmentation accuracy using a more sophisticated evaluation metric, such as windowdiff (pevzner and hearst, 2002), would be helpful."
P11-1102,6,this will be the main thrust of our future work.
P11-1109,1,"first, we will release extracted paraphrases from all of the 29,661,812 definition sentence pairs that we acquired, after human annotators check their validity."
P11-1109,6,Our future work is threefold.
P11-1110,1,"we also plan to build a generation system that employs the yule model (yule, 1925) to determine the importance of each aspect (e.g. who, when, where, etc.) in order to produce summaries that include diverse aspects of a story."
P11-1110,6,"in the future, we plan to move to content from other collective systems on web."
P11-1110,1,"in order to generalize our findings, we plan to examine blog comments, online reviews, and tweets (that discuss the same url)."
P11-1112,1,an important direction for future work lies in investigating how the approach generalizes across languages as well as reducing our systems reliance on a treebank-trained parser.
P11-1114,1,"in future work, we hope to identify additional types of document genre styles and incorporate genre directly into the extraction model."
P11-1114,1,coreference resolution and discourse analysis will also be important to further improve event extraction performance.
P11-1119,1,another important direction for future work involves more fully exploring the ways in which affect expression differs between textual and spoken dialogue.
P11-1119,4,"finally, as automatic facial tagging technologies mature, they may prove powerful enough to enable broadly deployed dialogue systems to feasibly leverage facial expression data in the near future."
P11-1123,1,"second, predicting projectable constituents is for improving machine translation and we are integrating the component into a syntax-based machine translation system."
P11-1123,5,"first, the results for predicting function tags and chinese empty elements were obtained on human-annotated trees and it would be interesting to do it on parse trees generated by system."
P11-1124,1,"moreover, we also plan to experiment with phrase-by-phrase classification instead of sentence by-sentence classification presented in this paper, in order to obtain more stable classification results."
P11-1124,1,"furthermore, we will explore methods to promote translation consistency at document level."
P11-1124,3,"in the future, we plan to investigate the impact of tm quality on translation consistency when using our approach."
P11-1124,1,"we also plan to label the training examples using other sentence-level evaluation metrics such as meteor (banerjee and lavie, 2005), and to incorporate features that can measure syntactic similarities in training the classifier, in the spirit of (owczarzak , 2007)."
P11-1126,1,we also plan to investigate more complicated reordering models in hm decoding.
P11-1126,1,"in the future, we will include more smt models and explore more features, such as syntax-based features, helping to improve the performance of hm decoding."
P11-1128,1,"we plan to introduce left to right target generation (huang and mi, 2010) into the stag tree to string system."
P11-1129,1,we are also interested in exploring more morphologically- or syntactically informed triggers.
P11-1129,1,"in future work, we would like to integrate the backward language model into a syntax-based system in a way that is similar to the proposed algorithm shown in figure 1."
P11-1130,1,"in future work, we want to improve the probability estimations for our paraphrasing models."
P11-1130,1,we also want to experiment with other morphologically complex languages and other smt models.
P11-1134,1,on the other side we will investigate more sophisticated methods to exploit the acquired lexical knowledge.
P11-1134,2,"our future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for te and clte."
P11-1134,1,"one possible direction is to consider linguistically motivated approaches, such as the extraction of syntactic phrase tables as proposed by (yamada and knight, 2001)."
P11-1137,1,in the future we hope to consider richer linguistic models capable of identifying multi-word expressions and syntactic variation.
P11-1139,4,a natural avenue for future work is the extension of our method to other nlp tasks.
P11-1140,1,"in the future, the weights of the cost function should be learned automatically by optimizing an appropriate error function."
P11-1145,2,"the second general direction is the use of the unsupervised methods we propose to expand the coverage of existing semantic resources, which typically require substantial human effort to produce."
P11-1145,1,"first, we would like to relax some of unrealistic assumptions made in our model: for example, proper modeling of alterations requires joint generation of syntactic realizations for predicateargument relations (grenager and manning, 2006; lang and lapata, 2010), similarly, proper modeling of nominalization implies support of arguments not immediately local in the syntactic structure."
P11-1145,6,We plan to explore at least two directions in our future work.
P11-1148,3,"first of all, we would like to evaluate the approach presented here using a more balanced and unbiased corpus, and compare its performance on such a corpus to local approaches."
P11-1148,1,"secondly, we would also like to include grammatical dependency information in the disambiguation step of the algorithm."
P11-1148,1,"for now, the disambiguation step only uses a word’s context words; enriching the feature set with dependency information is likely to improve the performance of the disambiguation."
P11-1148,6,We conclude with some issues for future work.
P11-1149,1,"particularly, we intend to explore new approaches for confidence estimation and their usage in the unsupervised and semi-supervised versions of the task."
P11-1149,1,in future work we hope to further improve unsupervised semantic parsing performance.
P11-1151,1,future approaches may be able to do away with this arbitrary separation of features by training a local classifier to consider all words in terms of their impact on content-only classification and their relations to neighbors.
P11-1151,1,an opportunity for future work is to consider normalization approaches for other classifiers.
P11-1152,1,"we also would like to look at other back off mechanisms (in addition to history length and classes) and incorporate them into the model, e.g., similarity and topic."
P11-1152,1,we would like to explore training regimes that lie between unique-event clustering and all-event clustering and upweight rare events less.
P11-1152,1,"finally, training classes on unique events is an extreme way of highly weighting rare events."
P11-1152,5,"in future work, we would like to find a theoretical justification for the surprising fact that polynomial discounting does at least as well as kneser-ney discounting."
P11-1153,1,"besides, advanced nlp techniques for document analysis, e.g., shallow parsing, may also be used to further improve structure finding."
P11-1153,1,"our work can be extended by incorporating richer features, such as named entity and co-reference, to enhance the models capability of structure finding."
P11-1155,1,"though our attempt to use giza++ for evaluating the similarity between chinese sentences and english sentences failed, we will exploit more advanced measures based on statistical alignment model for cross-language similarity computation."
P11-1155,1,"in future work, we will investigate to use the machine translation quality factor to further improve the fluency of the chinese summary, as in wan et al.(2010)."
P11-1158,1,"in future work we plan to examine better a* heuristics for ccg, and to look at principled approaches to combine the strengths of a*, adaptive super tagging, and other techniques to the best advantage."
P11-1159,1,"in future work, we intend to improve the prediction of functional morphological features in order to improve parsing accuracy."
P11-1159,4,we plan to make our parser available to other researchers.
P11-1159,1,we also intend to investigate how these features can be integrated into other parsing frameworks; we expect them to help independently of the framework.
P11-1159,6,please contact the authors if interested.
P11-1164,6,our first plan is to complete the mining process on all the types of sentences.
P11-1164,1,"since we perform task 1 and task 2 separately, we need to build an end to end system."
P11-1164,3,the second one is to conduct more experiments for obtaining better performance.
P11-1164,6,"in our future work, we have the following plans."
P11-1164,1,the final one is about an integrated system.
P11-2002,1,"we also plan to experiment with other unsupervised techniques, such as clustering and outlier detection, that can lead to better retrieval of rare classes."
P11-2002,4,"finally, we plan to investigate the applicability of our approach to a multi-class scenario."
P11-2002,1,"this should result in better performance on the rare classes, which is currently still low."
P11-2002,1,our plans for future work include improving our lm by incorporating syntactic information such as pos tags.
P11-2004,4,"going beyond our results provided for synthetic data, future work will explore applications of this technique, such as in experiments with streaming social media like twitter."
P11-2008,2,we also believe that the annotated data can be useful for research into domain adaptation and semi-supervised learning.
P11-2010,2,considering sources other than western languages as well as targets other than japanese is the future work.
P11-2013,3,"in the future, we would like to compare our method with a statistical machine translation approach performed at the letter level, evaluate the system using sentences by incorporating context word information, and consider many-to-one letter transformation in the model."
P11-2014,2,we would also like to test our method on a range of languages and texts.
P11-2014,1,"some possible extensions of our work include automatically generating the set of possible rhyme schemes r, and incorporating partial supervision into our algorithm as well as better ways of using and adapting pronunciation information when available."
P11-2015,1,we also are investigating how we can better utilize the output of our pcfg parsers for classification.
P11-2015,1,we look to automate the expansion of the training set of vandalized revisions to include examples from outside of wikipedia that reflect similar language styles.
P11-2018,5,interesting future work should be devoted to address the use of structural kernels for the proposed reranker.
P11-2020,4,"in future, we will extend this analysis to the complementary turn-taking category of turn yielding cues and explore how a spoken dialogue system may take advantage of information about entrainment to improve dialogue coordination and the user experience."
P11-2025,4,we hope that availability of this corpus motivates more research on statistical scope disambiguation.
P11-2025,1,"since world knowledge plays a major role in scope disambiguation, we believe that leveraging unlabeled domain specific data in order to extract lexical information is a promising approach for scope disambiguation."
P11-2025,2,our goal is to expand the corpus up to twice in size.20% of the corpus will be annotated and the rest will be left for the purpose of semisupervised learning.
P11-2027,1,"as future research, we plan to study the impact of different dataset sizes and vector space model parameters for improving the performance of the am component of the metric."
P11-2027,1,"finally, we also plan to study alternative uses of am-fm within the context of statistical machine translation as, for example, a metric for mert optimization, or using the am component alone as an additional feature for decoding, rescoring and/or confidence estimation."
P11-2027,3,"this will include the study of learning curves based on the amount of training data used, and the evaluation of different vector model construction strategies, such as removing stop-words and considering bigrams and word categories in addition to individual words."
P11-2029,1,a future direction for smt is to develop translation models that can effectively employ such information.
P11-2034,1,"the integration of knowledge from parse disambiguation and fluency ranking could be beneficial for tasks which combine aspects of parsing and generation, such as word-graph parsing or paraphrasing."
P11-2035,4,"in the future, we plan to apply our joint training technique to other rich filtering regimes (zhang et al., 2010), and to other nlp problems that combine the predictions of overlapping classifiers."
P11-2037,2,we also plan to extend our work here to recover coindexation information (links between a moved element and the trace which marks the position it was moved from).
P11-2037,4,"as a step towards shallow semantic analysis, this may further benefit other natural language processing tasks such as machine translation and summary generation."
P11-2037,1,"chung and gildea (2010) have shown that such information indeed helps translation, and we plan to extend this work by handling more empty categories (rather than just *pro* and *pro*), and to incorporate them into a syntax-based translation model instead of a phrase-based model."
P11-2039,4,"also, we plan to extend our approach to abstractive summarization."
P11-2039,2,"moreover, as the principles of qsbp are basically language independent, we will investigate the effectiveness of qsbp in other languages."
P11-2043,1,"the next step in this work would be to build a style transformation system that uses the features discussed in this paper as the bases for determining when, where, and how to do the style transformation."
P11-2044,1,"finally, we analyze the alignments produced and suggest that further improvements are possible through careful feature/constraint design, as well as the use of named-entity recognition and additional resources."
P11-2047,1,"future work will focus on using synonym-based expansion in the contexts (not just the time expressions headwords), and on incorporating contextual information and syntactic transformations."
P11-2053,4,"second, the user is supposed to gain a better understanding of semantic change by interactively exploring a corpus."
P11-2053,4,"for future research, we want to test our methodology on a broader range of terms, texts and languages and develop novel interactive visualizations to aid investigations in two ways."
P11-2053,4,"as a first aim, the user should be allowed to check the validity and quality of the visualizations by experimenting with parameter settings and inspecting their outcome."
P11-2055,1,"in future work, we want to run additional experiments with different classifiers (svm) and apply a genetic algorithm to perform joint feature selection, parameter optimization and instance selection."
P11-2055,1,we also plan to expand our feature set by including global context features (content words from the english sentence) and to examine the relationship between the performance and the number (and nature) of languages that is added to the feature vector.
P11-2056,6,we hope to shorten the gap to supervised systems with more unlabeled data.
P11-2056,1,"we also plan on training our models with em with features (berg-kirkpatrick et al., 2010)."
P11-2059,4,"future work will explore incorporating lu predictions to predict the social roles played by the participants in a thread, for example using persuasion and credibility to establish which participants in a discussion are serving as informal leaders."
P11-2062,1,"in future work, we plan to use both corpus annotations and agreement rules to automatically learn functional features for unseen words and detect and correct annotation errors."
P11-2062,1,we also plan to extend agreement rules to include complex structures beyond bigrams.
P11-2066,1,"we will continue working on this line of research and improve our discriminative learning model in the future, for example, by adding more phrase level features."
P11-2069,5,integrating the use of patterns within an edit rate computation technique will however raise new difficulties.
P11-2069,1,"our future work also includes the acquisition of paraphrase patterns (e.g.(zhao , 2008)) to generalize the acquired equivalence units to more contexts, which could be both used in applications and to attempt improving further paraphrase acquisition techniques."
P11-2074,1,further improvement may be achieved with other feature space partition approaches in the future.
P11-2076,4,"in the future, we want to apply our proposed method to other language pairs and domains."
P11-2078,3,we plan to test its effectiveness in hierarchical and syntax-based smt systems.
P11-2078,1,we also plan to investigate the relative usefulness of lm biasing as we move from low resource languages to those for which significantly larger parallel corpora and lm training data are available.
P11-2081,1,in future work we would like to study the impact of non-determinism on higher order models in the standard alignment model sequence and to gain more insight into the impact of finer-grained features in alignment.
P11-2082,1,"on the technical level, we would like to apply a sequence model to account for the dependencies among sentences, and obtain more meaningful features for formal and informal address."
P11-2082,1,"in order to remove idiosyncratic features like names, we will only consider features that occur in several novels; furthermore, we will group words using distributional clustering methods (clark, 2003) and predict t/v based on cluster probabilities."
P11-2082,6,Our analyses suggest a number of directions for future research.
P11-2084,1,"our next steps involve experiments with other topic models and other corpora, and combining this unsupervised approach with other tools for lexicon extraction and synonymy detection from unrelated and comparable corpora."
P11-2085,1,below we sketch three possible directions for the future work: (1) we should consider position features in analyzing pinyin errors.
P11-2085,1,"for example, it is less likely that users make errors in the first letter of an input pinyin.(2) we aim at designing a selfadaptive input method that provide error-tolerant features (chen and lee, 2000; zheng , 2011a).(3) we want to build a chinese spelling correction system based on extracted error-correction pairs."
P11-2086,2,"we are in the process of obtaining more documents in the domain, which will allow the use of more complex models and more sophisticated representations."
P11-2086,1,"in particular, we are considering clusters of terms and probabilistic topic models such as lda (blei et al., 2003)."
P11-2087,2,"future work because the method does not place any restrictions on the complex and simple corpora, we plan to validate it on different domains and expect it to be easily portable."
P11-2087,1,"we also plan to extend androutsopoulos, ion and prodromos malakasiotis.2010."
P11-2088,1,"finally, we would like to integrate our helpfulness model into a web-based peer-review system to improve the quality of both peer reviews and paper revisions."
P11-2088,3,"therefore, we are planning to investigate the impact of these different helpfulness ratings on the utilities of features used in modeling peer-review helpfulness."
P11-2088,1,"in the future, we would like to replace the manually coded peer-review specialized features (cogs) with their automatic predictions, since we have already shown in our prior work that some important cognitive-science constructs can be successfully identified automatically."
P11-2088,3,"also, it is interesting to observe that the average helpfulness ratings assigned by experts (used as the gold standard in this study) differ from those given by students."
P11-2091,1,"in the future, we plan to add more sophisticated sub-systems in this framework, and also explore combining ranking outputs from different sub-systems."
P11-2091,6,"furthermore, we will incorporate negative seeds into the process of interactive suggestion."
P11-2092,2,"we then want to extend our model to other languages, which could be more challenging, as certain languages have a more complex morphology than english, but also worthwhile, if the unknown word rate is higher."
P11-2092,4,we plan to use our model for domain adaptation in applications like machine translation.
P11-2092,1,the model could be further improved by using contextual information for the word clustering and training a classifier based on morphological features to assign oov words to these clusters.
P11-2093,4,future work in this area will include examination of performance on other tasks and languages.
P11-2094,2,we plan to carry out more transliteration experiments on other language pairs in the future.
P11-2095,2,"one area for future work is a full investigation of the performance of these algorithms in polysynthetic languages such as inuktitut, where each word contains many morphemes."
P11-2095,2,"it is likely that in such languages, the algorithms will find morphs rather than words."
P11-2096,3,"in the future, we would like to see more empirical evaluations and detailed studies comparing the practical merits of various paraphrase generation techniques."
P11-2096,3,"as madnani and dorr (madnani and dorr, 2010) suggested, it would be beneficial to the research community to develop a standard, shared evaluation that would act to catalyze further advances and encourage more meaningful comparative evaluations of such approaches moving forward."
P11-2097,4,"future work will include: (i) applying the method to other part-of-speech words, (ii) comparing the method with existing other automated method, and (iii) extending the method to find domain-specific senses with unknown words."
P11-2098,2,"accordingly we plan to explore relaxing this strict conjunctive behavior through models such as noisy-and (pearl, 1988)."
P11-2098,1,"we also intend to explore the contribution of our model, and particularly its estimated parameter values, within a complex system that integrates multiple levels of inference."
P11-2101,1,our proposed method determines feature polarity not only by opinion words that modify the features but also by its surrounding context.
P11-2101,1,Our future work will focus on improving the precision.
P11-2105,2,"in the future, we will investigate our method in the larger and more noisy data."
P11-2108,1,we also plan to explore the fusion of multi-modal features to enhance recognition and increase our understanding of multi-modal rapport behavior.
P11-2108,1,"in future research, we would like to extend our work to exploit sequential learning frameworks to predict verbal feedback."
P11-2108,5,"we will also work to analyze how quickly people can establish rapport, as the short duration of our spanish dyads poses substantial challenges."
P11-2110,1,"moreover, optimizing memory usage, for example via feature pruning or randomized algorithms, would allow incorporation of richer feature sets and would likely lead to further improvements, as indicated by the experiments in this paper."
P11-2110,3,"future work includes further evaluation of the vmm, e.g.as a language model within a speech recognition or machine translation system."
P11-2110,3,"we also intend to evaluate the performance of the vmm on other lexical prediction tasks and more generally, on other classification tasks with similar characteristics."
P11-2113,4,"some future work includes applying this model to areas such as topic tracking and text segmentation, and coherently adjusting it to fit an n-gram modeling approach."
P11-2114,1,"in future work, we will utilize more semantic information such as localized latent topics to help capture comparative aspects, and use machine learning technologies to tune weights of concepts."
P11-2117,1,"in the future, we hope to explore alignment techniques more tailored to simplification as well as applications of this data to text simplification."
P11-2121,2,"in the future, we will test the robustness of these approaches in more languages."
P11-2122,5,we are focusing on accounting for these issues in current work to allow such automatic correction.
P11-2122,2,"however, because the derivation trees and etrees are somewhat abstracted from the actual trees in the treebank, it can be challenging to automatically correct the structure in every location to reflect the correct derivation tree fragment."
P11-2122,1,this is because of details concerning the surrounding structure and the interaction with annotation style guidelines such as having only one level of recursive modification or differences in constituent bracketing depending on whether a constituent is a “single-word” or not.
P11-2124,1,agreement information could be very useful for disambiguating various constructions in hebrew and other morphologically rich languages.
P11-2128,3,"also, comparisons with the previous works are remaining work."
P11-2128,1,"from another perspective, we are considering the use of graph-based approaches (komachi et al., 2008) incorporated with the topic information using phits (cohn and chang, 2000), to further enhance entity extraction accuracy."
P11-2128,1,"to resolve this problem, we will incorporate the active learning or the distributional approaches."
P11-3001,2,"in the future, we will investigate combining word alignments on language pairs where both languages have no explicit word boundaries such as chinese-japanese."
P11-3002,6,adapting the proposed scheme to multi-document summary generation is the ongoing work we are engaged in.
P11-3002,1,"in the next step, we will experiment with alternative sentence representations and ordering algorithms to achieve better performance."
P11-3004,2,since non-english versions of wikipedia often are less extensive than the english version we find it promising to combine wikipedia versions of different languages and to use them as a source for multilingual ned.
P11-3004,2,In future work we plan to explore multilingual data for NED.
P11-3005,2,"we also consider harvesting data sources from the web such as lists of cities, common names and companies in pakistan and india."
P11-3005,4,another area for future work is to extend the extraction and classification to trigrams to improve the results especially for locations and person names.
P11-3006,3,"as future works, we will conduct experiments with various types of data and query, and further investigate the characteristic of our proposed method."
P11-3009,1,we will continue to train smt systems on automatically labeled discourse connectives in large corpora.
P11-3009,1,"we will try to better model the context of a connective, for instance by integrating word similarity distances from wordnet as features."
P11-3012,2,future work needs to explore features that can address the difference in language usage that the different authors use.
P11-3012,1,"further work is also needed to classify extra-sentential relations, as the current methods look only at relations occurring within a single sentence thus ignoring a large percentage of relations between entities."
P11-3013,1,future work should examine similar experiments with maltparser and other machine translation systems.
P11-3015,1,future directions include trying to improve the performance by modelling negations using a more sophisticated approach.
P11-3015,1,exploring longer citation scopes by including citation contexts might also improve citation sentiment detection.
P11-3017,1,future work in this direction would include growing the set of features by adding more prosodic ones and introducing lexical ones such as bi-grams and uni-grams.
P11-3017,1,"when a large feature bank has been developed, significant cues will be used in conjunction with machine learning techniques to build a model for turn taking which can be implemented in a spoken dialogue tutoring system."
P11-3017,1,thus the first line of future inquiry is to redo this method using a smaller silence boundary (50 ms) and different set of prosodic features so that it is truly comparable to gravano and hirschberg’s (2009) work with the game corpus.
P11-3019,3,as future work we are going to explore other ways of representing morphemes in the model.
P11-3019,1,"here we represented morphemes as separate states, but including them as features together with the root state may produce better models."
P11-3019,1,"another approach we will also focus is dividing words into characters and applying character-level models (klein et al., 2003)."
P11-3022,1,"in the future, we will analyze the technique on other learning methods such as kmeans++ and experiment on various real-data nlp tasks."
P11-4002,1,"future work, for which we are urgently seeking funding, could include integration of further nlp-based features such as coreference resolution or question answering, as well as citation classification and graphical navigation along the ideas in schafer and kasterka (2010)."
P11-4006,1,"therefore, our future works are threefold: for sentiment analysis, we will consider more sophisticated ways to improve the baseline accuracy and to aggregate individual posts into a collective consensus."
P11-4006,2,"for music generation, we plan to add more instruments and exploit learning approaches to improve the selection of chords."
P11-4006,6,"for visualization, we plan to add more interactions between music, sentiments, and users."
P11-4008,2,"additionally, we are actively mining other language pairs to build a multi-language learning system."
P11-4008,1,"in future work, we are examining extracting language knowledge from the real-time web for translation in news scenarios."
P11-4009,5,but these problems lead to another research issue that may be termed as cross lingual sentiment synset linking.
P11-4009,1,presently we are giving a closer look to the qualitative analysis of developed multilingual psycho-sentiment lexicons.
P11-4012,2,future work includes researching further on the benefits provided by our online learning techniques with experiments involving real users.
P11-4014,1,"future work will aim at improving the relevance of semantic search, at modeling context to  improve timing of results, and at inferring relevance feedback from users."
P11-4020,1,another important direction is to refine the interaction design through task-based user studies.
P11-4020,1,"to this end, a future improvement that we plan to use is a variant on mmr (maximum marginal relevance) (carbonell , 1998), which can be used to optimize the diversity of selected text tiles as well as the relevance based ordering of clusters, i.e., so that more diverse sets of extracts from the co-cited articles will be placed at the ready fingertips of users."
P11-4024,1,we plan to provide additional functionality such as pattern frequency counts.
P11-4024,1,the annotation librarian has been enhanced over the course of a number of biomedical nlp use cases and we plan to continue to enhance the interface as new use cases arise.
P12-1002,1,"in future work, we would like to investigate more sophisticated features, better learners, and in general improve the components of our system that have been neglected in the current investigation of relative improvements by scaling the size of data and feature sets."
P12-1003,1,"we believe that it should be possible to use insights from this paper in an active learning setting, to select, from an available monolingual source, a subset of a given size for manual translation, in such a way at to yield the highest performance, and we plan to extend our work in this direction."
P12-1005,1,"in the future, we plan to explore phonological context and use more flexible topological structures to model acoustic units within our framework."
P12-1007,1,"our future work will be to fully implement an end-to-end discourse parser using our rich linguistic features, and focus on improving performance on cross-sentence instances."
P12-1010,4,"in future work, we plan to investigate how to best apply the dependency structure approach to such domains."
P12-1011,1,"although we focused on year mentions here, there are several avenues for future study, including explorations of how other types of time expressions might inform the task."
P12-1013,3,"we also plan to examine the benefit of tnf in learning similar structures, e.g., taxonomies or ontologies."
P12-1013,3,"in future work, we aim to evaluate tnf on large graphs that are automatically generated from huge corpora."
P12-1016,1,one potential avenue of future work would be to adapt our component models to new genres by self-training them on the target side of a large bitext.
P12-1022,1,future work could consist in combining both strategies: pre-grouping could suggest a set of potential mwe segmentations in order to make it more flexible for a parser; final decisions would then be made by the reranker.
P12-1027,4,"as future work, we plan to apply this fast learning method on other large-scale natural language processing tasks."
P12-1030,4,"in addition, while our novel components were motivated by the search space of textual inference, we foresee their potential utility in other application areas for search, such as automated planning and scheduling."
P12-1032,1,"in the future, we will explore other consensus features and other similarity measures, which may take document level information, or syntactic and semantic information into consideration."
P12-1037,1,a primary goal for future research is developing an on-line structure learner for blps that can directly learn probabilistic first-order rules from uncertain training data.
P12-1046,2,future work will involve examining the srtsg model for different languages and for unsupervised grammar induction.
P12-1048,1,"furthermore, since the in-domain phrase-topic distribution is currently estimated with simple smoothing interpolations, we expect that the translation system could benefit from other sophisticated smoothing methods."
P12-1048,1,"finally, the reasonable estimation of topic number for better translation model adaptation will also become our study emphasis."
P12-1049,2,"in future work, we plan to adapt our approach to language pairs where one language is alphabetic and the other language is non-alphabetic such as english/japanese."
P12-1050,1,"finally, we may try to exploit not only the ranking, but also the scores produced by the reordered lms, as an additional decoding feature."
P12-1054,1,"in the future, we plan to extend the coranking framework so as to incorporate information credibility and temporal recency."
P12-1055,2,"second, we are interested in incorporating knowledge mined from wikipedia into our factor graph."
P12-1055,1,"first, we are going to develop advanced tweet normalization technologies to resolve slang expressions and informal abbreviations."
P12-1055,1,"in the future, we plan to explore two directions to improve our method."
P12-1056,1,a more interesting and useful task is to detect real time bursts in an online fashion.
P12-1056,6,this is one of the directions we plan to study in the future.
P12-1056,1,another limitation of the current method is that the number of topics is predetermined.
P12-1058,1,"in the future, we plan to explore using constrained crf for more accurate dependency tagging."
P12-1058,4,we will also use the result from this work in other tasks such as answer quality ranking and answer summarization.
P12-1060,4,"in the future, we will work on leveraging parallel sentences and word alignments for other tasks in sentiment analysis, such as building multilingual sentiment lexicons."
P12-1065,4,"as future work, we would like to apply our algorithm to a structured task such as part of speech tagging."
P12-1069,1,we are in the process of extending our head-driven parser for non-projective structures as our future work.
P12-1074,2,"as future work, we plan to improve the quality of the output by considering word sense disambiguation techniques to reduce the effect of inappropriate ingredients."
P12-1074,1,we also want to extend the model to include multiword ingredients and to generate not only words but also short phrases.
P12-1078,4,a key aspect of future work will be to extend the sparse mixed-effects paradigm to other problems within the social sciences where metadata is available but qualitative analysis at a large scale is difficult or impossible.
P12-1079,2,"in the future, we are interesting to find ways to exploit topic model on bilingual data without document boundaries, thus to enlarge the size of training data."
P12-1085,5,"however, we did not address the issue of property value cleaning and normalization."
P12-1085,6,This is an important direction for future work.
P12-1087,1,"thus, techniques that improve the accuracy of crowd-sourced answers are an interesting direction for future work."
P12-1089,1,"finally, it is worth exploring scaling the approach to unrestricted event extraction, and jointly model extracting more than one relation per document."
P12-1091,3,"for future work, we would like to compare the text modeling performance of wtmf with lsa and lda on regular length documents."
P12-1095,1,"in the future work, we will extend our predicate translation model to translate both verbal and nominal predicates."
P12-1096,1,we also expect to explore better way to integrate ranking reorder model into smt system instead of a simple penalty scheme.
P12-1096,1,"in future work, we plan to extend the ranking model to handle reordering between multiple levels of source trees."
P12-1098,2,"in future work, we plan to tune the free parameter a for each language pair."
P12-1099,1,future work includes extending this approach to use multiple translation models with multiple language models in ensemble decoding.
P12-1107,1,"in the future we would like to investigate how we can boost the number of edits the system performs, while still producing grammatical and meaning preserving output."
P12-1110,1,"in future work, probabilistic pruning techniques such as the one based on a maximum entropy model are expected to improve the efficiency of the joint model further because the accuracies are apparently still improved if a larger beam can be used."
P12-1111,1,"there are more interesting ways in applying these constraints, which we are going to study in future work."
P12-1111,4,these two simple applications suggest that it is of interest to explore data-driven deterministic constraints learnt from training examples.
P12-2001,6,"however, higher-order constituent parsing inevitably leads to a high computational complexity."
P12-2001,1,we intend to deal with the efficiency problem of our model with some advanced parallel computing technologies in our future works.
P12-2008,1,"in the future, we plan to go a step further to see whether we can enhance dissimilarity with penalizing phrase tables used in both of the translation processes."
P12-2010,1,"for future work, we plan to relieve the complexity problem for dealing with more expanded graph structure to improve the performance of our proposed approach."
P12-2025,1,probably the most important goal for future work is improving the recall achieved in the complete disambiguation pipeline.
P12-2026,1,"additionally, an interesting direction to explore is to identify phrase types and train type-specific crf model."
P12-2026,2,"for example, existing query expansion methods could be implemented to retrieve more webpages containing translations."
P12-2026,1,"in addition, natural language processing techniques such as word stemming and word lemmatization could be attempted."
P12-2026,6,Many avenues exist for future research and improvement.
P12-2027,1,"future research here could profitably focus on this relationship, especially for terms whose success in the english and german hip hop communities is highly disparate."
P12-2032,1,"using sophisticated nlp techniques, we may be able to enrich the network and use standard sna metrics to predict the dominance relations in the gold standard."
P12-2033,1,"in future work, we will incorporate more syntactic information in the model to better evaluate sentence quality."
P12-2033,3,"we also plan to perform a human evaluation for the compressed sentences, and use sentence compression in summarization."
P12-2036,4,"although we have presented our approach in the context of 3d virtual worlds, we believe our technique is also applicable to other domains such as the web, video games, or human robot interaction."
P12-2037,1,how to automatically classify the extracted patterns is also an interesting future issue.
P12-2037,4,"these patterns are potentially useful for many other applications, which will be studied in the future work."
P12-2040,2,"as future work, we intend to expand the current size of the collection from 0.7k to 2k movies, as well as to improve some of our parsing and postprocessing algorithms for reducing the amount of noise still present in the collection and enhance the quality of the current version of the dataset."
P12-2042,1,"other directions of subsequent research may include address more elaborate models of events, and the investigation of the relationship between relation words and taxonomies of discourse relations."
P12-2042,4,"another prospective field of application can be seen in nlp applications, where selection preferences for relation words may serve as a cheap replacement for full-fledged discourse parsing."
P12-2043,2,"for future work, we would like to try the proposed technique on other languages, because it would likely be effective in automatically learning character-level morphological transformations as well as overcoming some of the problems associated with stemming."
P12-2045,1,"as well as using event linking to add referentially precise hyperlinks to a news archive, further characteristics of news will emerge by analyzing the graph of event references."
P12-2047,1,"an area for future work is to transfer other syntactic information, such as parse structures or super tags using a similar transfer approach."
P12-2048,5,the jumbled nature of the dig site means that the process of assembling new texts from this site will be one of the major tasks in for hittite scholars in the near future.
P12-2051,1,"in future work, we plan to do an in-depth analysis of the features that best characterize the changes in word usage over time, and develop representations that allow us to track sense changes."
P12-2055,1,"in our future work, it is worth studying how to combine the best of our approach and discriminative word alignment models to improve rule extraction for smt models."
P12-2056,1,"we believe that in this framework, using other finer-grained segmentation, with fewer ambiguities than character, would better parameterize the alignment models, while using other coarser-grained segmentation as wsr can help capture more linguistic knowledge than word to get better translation."
P12-2064,3,a better automatic evaluation metric would be needed as further research.
P12-2065,1,"for future work, we propose to incorporate prior knowledge of latent variables to the model."
P12-2066,4,"we expect our method to be complementary with sophisticated methods used in state-of-the-art sentiment classification systems, which is to be explored in future work."
P12-2068,3,"as future work, we will compare our model with the other state-of-the-art systems."
P12-2068,3,we will also investigate the correlation between readability and srlbased score by manual evaluations.
P12-2068,1,"furthermore, we would like to combine discourse constraints with sr constraints."
P12-2069,1,we are now expanding the variety and complexity of the abstraction schemes and generation patterns to deal with more aspects and other categories.
P12-2069,5,"although fully abstractive summarization is a daunting challenge, our work shows the feasibility and usefulness of this new direction for summarization research."
P12-2071,1,"for future work, we will experiment with more diverse training and testing data and also more sophisticated algorithms."
P12-2072,2,"for this reason, it can be expected to be easily portable across languages enabling good quality processing of languages with complex morphology and scarce resources."
P12-2072,1,"the adaptive general classification model used in our approach makes use of different sources of information that can be found in a small annotated corpus, with no need for comprehensive, manually constructed morphological dictionaries."
P12-2073,1,our next step is to utilize the collected data and analysis results to build online and offline spelling correction models.
P12-2075,1,"if it is, automatic clustering of the input data may be an important pre-processing step for this kind of systems."
P12-2075,6,this suggest that consistency of the input data is as important as the amount of data.
P12-2075,3,this hypothesis has to be confirmed in futur studies.
P12-3004,6,"in addition, a hadoop-based mapreduce-parallelized version is underway and will be released in near future."
P12-3006,1,we would also like to further enhance the system to convert the translated english chat messages back to the social media language as defined by the user.
P12-3007,1,"as future work, we intend to improve iris performance by addressing some of the already identified common failures."
P12-3010,2,"in the future, it would be interesting to extend the parallel corpus to the internet to retrieve more rich data for the computer assisted translation."
P12-3017,4,"making the illume components real products like the home lighting system, the intelligent table lamp, or the music album promoter is also a future plan."
P12-3017,2,"we will continue collecting annotated materials and user feedbacks for learning, and make the materials a corpus for the research community."
P13-1005,3,"finally, we would like to perform extensive controlled experimentation to examine the relative contribution of the various aspects of our approach."
P13-1005,1,we will look to identify the issues with such models and provide general guidelines for prepping models prior to processing.
P13-1006,1,"such can be handled with discriminative training, a topic we plan to address in the future."
P13-1006,2,"as such, it might require more training data for convergence than a method that also makes use of negative training sentences that are not true of a given video."
P13-1007,1,"this work can be improved in many directions, among which are scoping more elements such as other scopal operators and implicit entities, deploying more complex learning models, and developing models which require less supervision."
P13-1008,1,"to improve the accuracy of end to-end ie system, we plan to develop a complete joint framework to recognize entities together with event mentions for future work."
P13-1008,4,also we are interested in applying this framework to other ie tasks such as relation extraction.
P13-1010,1,future work should first investigate the integration of information about entities.
P13-1013,1,future work includes investigations of our parser and annotations on chinese nlp tasks.
P13-1015,1,"in the future, we will explore irtg binarization with fanout increase."
P13-1017,1,"for future work, we will investigate more settings of different hyper-parameters in our model."
P13-1017,1,"secondly, we want to explore the possibility of unsupervised training of our neural word alignment model, without reliance of alignment result of other models."
P13-1025,4,"we hope the publicly available collection of annotated requests enables further study of politeness and its relation to social factors, as this paper has only begun to explore this area."
P13-1026,6,we make our thesis clarity annotations publicly available in order to stimulate further research on this task.
P13-1026,1,"in addition to developing these models, we proposed novel features for use in our thesis clarity error model and employed these features, each of which was explicitly designed for one or more of the error types, to train our scoring model."
P13-1030,1,"the most important direction of future work for ostag is the development of a principled grammar induction model, perhaps using the same techniques that have been successfully applied to tsg and tig."
P13-1033,1,in this paper the model was only used to infer word alignments; in future work we intend to develop a decoding algorithm for directly translating with the model.
P13-1034,1,"in future work, we plan to explore utilizing richer statistics from the unlabeled data, beyond word marginals."
P13-1034,1,"lastly, we also wish to explore ensemble approaches that combine the best supervised classifiers with the improved class-conditional estimates provided by mnb-fm."
P13-1034,1,"further, we plan to experiment with techniques for unlabeled data sets that also include continuous-valued features."
P13-1037,1,"another avenue for future research is to study variation in possessive use across genres, including scientific and technical genres."
P13-1037,6,"instead of trying to find the one-best interpretation for a given possessive example, we would like to produce a list of all appropriate interpretations."
P13-1038,1,"because we confirmed in this paper that these two approaches have different characteristics, it would be interesting to incorporate textual clues into the distribution-based approach by using, for example, machine learning techniques."
P13-1038,1,one is to explore a more sophisticated approach for precisely modeling the contexts of numbers.
P13-1038,6,There are three important future directions for this research.
P13-1040,2,"furthermore, we aim to extend the existing dictionaries and possibly our training data with terms extracted from comparable corpora."
P13-1040,4,"finally, we plan to investigate the usefulness of the terms in different application scenarios, including computer assisted translation and machine translation."
P13-1040,1,"exploring ways to add contextual or distributional features to our term representations is also an avenue for future work, though it clearly significantly complicates the approach, one of whose advantages is its simplicity."
P13-1046,1,"in future, we also hope to explore unsupervised online adaptation, where the trained model can be updated as test data is processed."
P13-1049,1,in the future we will apply the hierarchies on finer feature spaces to make more accurate optimizations.
P13-1049,1,"instead of cutting product-hierarchies, we will employ usual techniques to build decision trees10 and apply our cutting method on their structure."
P13-1049,1,we also plan to extend our method by breaking the symmetry of our hierarchies.
P13-1049,1,"the objective is twofold: first, we will get rid of the sequence of indicators as parameter."
P13-1051,6,"that will be a complementary point of view, and thus a natural direction of future work for us."
P13-1051,5,"while the focus of this paper is to explore and describe the expressive power of various annotation styles, we did not address the learnability of the styles by parsers."
P13-1052,1,"we also plan to exploit the acquired html patterns for implementing an open-source glossary crawler, along the lines of google define."
P13-1053,3,"in future work, the framework we have presented here should be tested more extensively, not only against a gold standard but also in terms of the usefulness of the derived collective annotations for training supervised learning systems."
P13-1053,1,"on the theoretial side, it would be interesting to study the axiomatic properties of the methods of aggregation we have proposed here in more depth and to define axiomatic properties of aggregators that are specifically tailored to the task of collective annotation of linguistic resources."
P13-1054,1,"in future pargrambank releases, we will provide more theory-neutral dependencies along with the lfg representations."
P13-1057,1,"despite the consistent superiority of type annotations in our experiments, it of course may be the case that techniques such as active learning may better select sentences for token annotation, so this should be explored in future work."
P13-1059,2,"furthermore, we are interested in extending this framework to translate other out-of-vocabulary terms."
P13-1059,1,"in the future we intend to improve the framework by training a discriminative model to automatically assign weights to combine name translation and baseline translation with additional features including name confidence values, name types and global validation evidence, as well as conducting lm adaptation through bilingual topic modeling and clustering based on name annotations."
P13-1063,2,"secondly, knowledge in a minor language may also help improve extraction performance for a major language due to the cultural and religion differences."
P13-1063,1,"last but not least, we will try to extract multiple attr-value pairs at the same time for each article."
P13-1063,1,"firstly, more attributes in more info box templates should be explored to make our results much stronger."
P13-1064,2,"monolingual and cross-lingual textual entailment in particular would be interesting applications, because they require finding shared meaning on two text fragments."
P13-1064,1,"we also plan to explore further uses for this language bridge, at a finer semantic level."
P13-1066,1,"in our future work, we intend to extend the model to account for stances, and issue specific interactions which would pave the way for user profiling and behavioral modeling."
P13-1067,2,"not on a last place, we would like to improve the built valence prediction models and to collect more data for spanish, russian and farsi."
P13-1067,4,in the future we are interested in studying the affect of metaphors for domains different than governance.
P13-1067,5,"we want to conduct studies with the help of social sciences who would research whether the tagging of affect in metaphors depends on the political affiliation, age, gender or culture of the annotators."
P13-1068,1,"from the tagging perspective, our future plans include testing the system on other highly inflectional languages such as czech and slovene and investigating different methods for automatically determining a more suitable custom network topology, such as genetic algorithms."
P13-1071,1,"in future work, we aim to extend our quality flaw detection system to not only find articles that contain a particular flaw, but also to identify the flaws within the articles, which can be achieved by leveraging the positional information of in-line cleanup templates."
P13-1072,2,we believe incorporating informal word normalization into the inference process may help address this important source of error.
P13-1073,1,"finally, it would be interesting to see how our algorithm performs on other news domains."
P13-1073,1,"in future work, we want to generate more diverse and intriguing questions by selecting relevant named entities for template instantiation that do not appear in the article."
P13-1073,1,"another direction would be take a supervised approach, training classifiers over a labeled dataset for filtering irrelevant templates and incorrect instantiations."
P13-1074,2,"in future work, we will try our method on other languages such as chinese and japanese, where treebank data is available."
P13-1074,3,we would also like to test the mt performance over transcribed speech texts with punctuation symbols inserted based on our method proposed in this paper.
P13-1075,1,"in the future, we will compare this method with self-training to better illustrate the importance of boundary information, and give error analysis on what types of errors are reduced by the method to make this investigation more complete."
P13-1075,1,"we will also investigate more efficient algorithms to leverage more massive web text with natural annotations, and further extend the strategy to other nlp problems such as named entity recognition and parsing."
P13-1078,1,we plan to explore more work on the additive neural networks in the future.
P13-1078,2,"for example, we will train word embedding matrices for source and target languages from a larger corpus, and take into consideration the bilingual information, for instance, word alignment; the multi-layer neural network within the additive neural networks will be also investigated in addition to the single-layer neural network; and we will test our method on other translation tasks with larger training data as well."
P13-1079,3,"furthermore, we will investigate any tradeoffs between the accuracy of the probability estimation and the coverage of phrase pairs."
P13-1079,1,"in future work, we will also introduce incremental learning for phase pair extraction inside a domain, which means using the current translation probabilities already obtained as the base measure of sampling parameters for the upcoming domain."
P13-1081,4,"we also applied the predicted ecs to a large-scale chinese-to english machine translation task and achieved significant improvement over two strong mt base lines, i.e. a hierarchical phase-based system and a tree-to-string syntax-based system."
P13-1082,4,"future work could involve merging our translation model framework with the online adaptation of other models, or the log-linear weights."
P13-1083,1,"our experiments showed that a more favorable pos tagset can be induced by integrating aligned information, and furthermore, the pos tagset generated by the proposed method is more effective for smt than an existing pos tagset (the ipa pos tagset)."
P13-1084,1,"as future work, we plan to incorporate the question structure (e.g., question topic and question focus (duan et al., 2008)) into the question representation for question retrieval."
P13-1085,1,at this stage no further information flows from the scf and sp models to the clustering model.
P13-1085,1,a natural extension of our unified framework is to construct a joint model in which the predictions for all three tasks inform each other at all stages of the prediction process.
P13-1086,4,"the signals generated by this algorithm could improve the prediction of a financial time series model, such as ads (rydberg and shephard, 2003)."
P13-1086,2,"our future work will consider the contextual information for sentence selection, and an aggregation of weighted news content based on the decay effect over time for individual companies."
P13-1086,1,we have presented a model for predicting stock price movement from news.
P13-1086,1,it also facilitates human interpretable analysis to understand the relation between a company's market value and its business activities.
P13-1089,1,"in this paper, we have described two approaches for amortizing inference costs over datasets."
P13-1090,2,"as a future work, we will investigate using session data, namely the entire dialog between the human and the computer."
P13-1090,2,"rather than using single turn utterances, we hope to utilize the context information, e.g., information from previous turns for improving the performance of the semantic tagging of the current turns."
P13-1093,6,a further incremental increase is achieved by considering only transcriptions above a minimum length.
P13-1093,6,"for instance, confidence can be approximated in mlps by the entropy across continuous-valued output nodes, and in rfs by the number of component decision trees that agree on a classification."
P13-1093,1,"this phenomenon appears to be manifested in the current study by the extent to which classification increases generally across the 5-, 6-, and 7-yearold groups, as shown in table 5."
P13-1094,1,"our experiments demonstrated that sentiment relevance and subjectivity are related, but different."
P13-1096,1,"in future work, we plan to explore alternative multimodal fusion methods, such as decision-level and meta-level fusion, to improve the integration of the visual, acoustic, and linguistic modalities."
P13-1098,1,"finally, in the application level, we aim at an indepth analysis of patterns and characteristics in the extracted sets of features by collaborating with domain experts (e.g., political analysts)."
P13-1098,1,future work may investigate further modelling improvements achieved by applying different regularization functions as well as the adaptation of the presented models to classification problems.
P13-1098,4,"the application domain in this paper was politics, though the presented methods are generic and could be easily applied on various other domains, such as health or finance."
P13-1099,1,"from a series of experiments, we found that there is little difference between the two ilp modules, and that the improved system performance is attributed to the fact that our proposed supervised bigram estimation module can successfully gather the important bigram and assign them appropriate weights."
P13-1100,4,"this is an interesting extension of their previous submodular framework and while the new formulation permits more complex functions, the resulting function is still submodular and hence can be combined with the dispersion measures proposed in this paper."
P13-1100,1,"a different body of work uses determinantal point processes (dpp) to model subset selection problems and adapt it for document summarization (kulesza and taskar, 2011)."
P13-1100,1,"in a very recent work, lin and bilmes (2012) demonstrate a further improvement in performance for document summarization by using mixtures of submodular shells."
P13-1101,1,our future work will improve the local search algorithm to remove this restriction.
P13-1103,1,the obtained treebank is then transformed into ccg derivations.
P13-1104,1,"in the future, we will experiment with more advanced dependency representations (de marneffe and manning, 2008; choi and palmer, 2012b) to show robustness of our approach."
P13-1105,1,"in the future work we will investigate such kind of strategies, such as bilingually unsupervised induction."
P13-1107,1,"our ongoing work includes identifying candidate morphs from scratch, as well as discovering morphs for a given target based on anomaly analysis and textual coherence modeling."
P13-1107,1,both of the meta-path based and social correlation based semantic similarity measurements are proven powerful and complementary.
P13-1109,2,"in this scenario, we also can look for paraphrases and translations for phrases containing oovs and add them to the phrase-table as new translations along with the translations for unigram oovs."
P13-1110,4,we plan to investigate its utility elsewhere in nlp (e.g. for parsing) as well as in other domains involving high-dimensional structured prediction.
P13-1111,5,"in this paper, we focus on the problem of ambiguities for pass."
P13-1112,1,"based on the resulting trees, we have then hypothesized that the following relation holds in mother tongue interference: interfamily distance &gt; non-nativeness &gt; intrafamily distance."
P13-1114,5,"although the work presented here established that more than word-to-word normalization was necessary to produce parser-ready normalizations, it remains unclear which specific normalization tasks are most critical to parser performance."
P13-1114,1,the proposed framework builds a statistical model over a series of replacement generators.
P13-1114,1,this work presents a framework for normalization with an eye towards domain adaptation.
P13-1119,1,our setup is entirely reproducible: we have built a static web search environment consisting of a search engine along with a means to browse a large corpus of web pages as if it were the 鈥渞eal鈥 web.
P13-1123,3,"this study will contain additional evaluation categories, such as the understandability or informativeness of system utterances."
P13-1123,1,"finally, we would like to explore methods for unsupervised data labelling so as to facilitate portability across domains further."
P13-1123,1,we have presented a novel technique for surface realisation that treats generation as a sequence labelling task by combining a crf with tree-based semantic representations.
P13-1123,3,"in addition, we may compare different sequence labelling algorithms for surface realisation (nguyen and guo, 2007) or segmented crfs (sarawagi and cohen, 2005) and apply our method to more complex surface realisation domains such as text generation or summarisation."
P13-1123,3,"in a human rating study, we confirmed that judges rated our output as better phrased, more natural and less repetitive than systems that just take local features into account."
P13-1125,1,"we hope to be able to learn lexical information such as how many arguments a verb takes, what nouns are potential subjects for a given verb by gathering statistics from an english parser and projecting to the source language via our word/phrase translation table."
P13-1127,2,"programmers read the specifications, then develop source code that parses inputs in the format."
P13-1129,1,"a joint approach to resolving speaker attribution, relationship extraction, co-reference resolution, and alias-to-character mapping would not only improve the accuracy on all these tasks, but also represent a step towards deeper understanding of complex plots and stories."
P13-1130,4,hbms have been successfully used for a number of language acquisition tasks capturing both patterns of under- and overgeneralization found in child language acquisition.
P13-1131,1,a natural extension of our work would be to extend our two level model to accommodate context sensitive lexical similarity.
P13-1131,2,they learn an lda model on the source language side of the training corpus with the purpose of identifying implicit sub-domains.
P13-1131,1,"finally, they train a classifier to translate a given target word based on these tables and the inferred topic distribution of the given document in which the target word appears."
P13-1138,4,the development time to adapt our system to new domains is small compared to other nlg systems; around a week to adapt the system to weather and biography domains.
P13-1139,5,this is the first time a concrete answer to this question has been provided.
P13-1142,4,"to demonstrate this point, we carried out an evaluation on a creative sentence generation benchmark showing that brainsup can effectively produce catchy, memorable and successful sentences that have the potential to inspire the work of copywriters."
P13-1142,1,"we have presented brainsup, a novel system for creative sentence generation that allows users to control many aspects of the creativity process, from the presence of specific target words in the output, to the selection of a target domain, and to the injection of phonetic and semantic properties in the generated sentences."
P13-1142,4,"the system has been designed as a supporting tool for a variety of real-world applications, from advertisement to entertainment and education, where at the very least it can be a valuable support for time-consuming and knowledgeintensive sentence generation needs."
P13-1142,1,"to our best knowledge, this is the first systematic attempt to build an extensible framework that allows for multi-dimensional creativity while at the same time relying on syntactic constraints to enforce grammaticality."
P13-1143,1,experiments on the hoo 2011 shared task show that ilp inference achieves state-of-the-art performance on grammatical error correction.
P13-1147,1,we proposed syntactic tree kernels enriched by lexical semantic similarity to tackle the portability of a relation extractor to different domains.
P13-1148,1,"to conclude, we presented a model that provides a clean framework to test the usefulness of different factors for word segmentation and handling phonological variation in a controlled manner."
P13-1149,4,"we would also like to apply composition to inflectional morphology (that currently lies outside the scope of distributional semantics), to capture the nuances of meaning that, for example, distinguish singular and plural nouns (consider, e.g., the difference between the mass singular tea and the plural teas, which coerces the noun into a count interpretation (katz and zamparelli, 2012))."
P13-1149,1,"finally, in our current setup we focus on a single composition step, e.g., we derive the meaning of inoperable by composing the morphemes in- and operable."
P13-1151,1,"for other simplification tasks, the optimal parameters will need to be investigated."
P13-1151,1,these improvements are achieved over a simple-only model that uses all simple english data currently available in this domain.
P13-1151,2,"for both tasks, the best improvements were seen when using language model adaptation techniques, however, the adaptation results also indicated that the role of normal data is partially task dependent."
P13-1151,1,"on the perplexity task, the best results were achieved with an equal weighting between the simple-only and normal-only model."
P13-1151,1,"for example, on the lexical simplification task, when using a linearly interpolated model, the model combining 100k simple sentences with all the normal data achieved comparable results to the model combining all the simple sentences with all the normal data."
P13-1153,2,transliteration mining requires limited amounts of training examples.
P13-1153,4,"we believe that the proposed cross-lingual features can be used to help ner for other languages, particularly languages that lack good features that generalize well."
P13-1155,1,this shows a characteristic of the proposed approach; it is very conservative in proposing normalization which is desirable as a preprocessing step for nlp applications.
P13-1156,1,we have presented a technique to combine phrasebased features and tree-based features into one model.
P13-1157,1,"we plan to extend our method to detect machine-translated sentences produced by different mt systems, e.g., a rule-based system, and develop a unified framework for cleaning various types of noise in web-mined data."
P13-1157,1,"in addition, we will investigate the effect of source and target languages on translation in terms of mt detection."
P13-1157,2,"therefore, we expect that our method is basically effective on different language pairs."
P13-1157,5,"as lopez (2008) describes, a phrase-salad is a common phenomenon that characterizes current smt results."
P13-1158,4,"second, we would like to generalize the question understanding framework to produce more complex queries, constructed within a compositional semantic framework, but without sacrificing scalability."
P13-1158,1,"using only a seed lexicon, the approach automatically learns a lexicon and linear ranking function that demonstrated high accuracy on a held-out evaluation set."
P13-1159,4,we are planning to deploy our system and release model files of the classifiers to assist relief efforts in future crisis scenarios.
P13-1159,1,"through a series of experiments, we demonstrated that the performance of the problem-aid matching can be improved with the usage of semantic orientation of excitation polarities, proposed in (hashimoto , 2012), and trouble expressions."
P13-1162,4,our study of bias in wikipedia has implications for linguistic theory and computational linguistics.
P13-1166,4,systematic testing can provide an indication of this variation.
P13-1167,3,"for all evaluations, however, a justification for the biased/unbiased metrics used should be given, and more than one metric should be reported so as to allow a reader to ascertain for themselves whether a particular automatic segmenter鈥檚 bias in some manner is cause for concern or not."
P13-1168,1,our future work in this area would specifically target understanding and formalization of the theoretical model underpinning a query.
P13-1169,1,"with the manually labeled data set, we first predict the deceptive answers with traditional classification method."
P13-1171,1,"second, because the task of answer sentence selection is very similar to paraphrase detection (dolan , 2004) and recognizing textual entailment (dagan , 2006), we would like to investigate whether systems for these tasks can be improved by incorporating enhanced lexical semantic knowledge as well."
P13-1172,1,"other semantic relations, such as the topical associations between opinion targets (or opinion words) should also be employed."
P13-1172,1,we believe that considering multiple semantic associations will help to improve the performance.
P13-1174,1,"we presented a broad-coverage connotation lexicon that determines the subtle nuanced sentiment of even those words that are objective on the surface, including the general connotation of realworld named entities."
P13-1174,1,"via a comprehensive evaluation, we provided empirical insights into three different types of induction algorithms, and proposed one with good precision, coverage, and efficiency."
P13-2004,1,experiments showed that dc-admm drastically reduced model complexity in terms of the degrees of freedom in trained models while maintaining the performance.
P13-2004,1,"this paper also introduced a feasible algorithm, dcadmm, which can vanish the infeasible combinatorial optimization part from the entire learning algorithm with the help of the admm technique."
P13-2010,1,"however, to scale up compositionally beyond the simplest constructions, cdsms must deal with grammatical terms such as determiners."
P13-2010,1,"thus, a top priority in future work is to explore different contextual features, such as adverbs and grammatical terms, that might carry information that is more directly relevant to the semantics of determiners."
P13-2010,1,theoretical considerations would lead one to expect a 鈥渇unctional鈥 approach to determiner representations along the lines of baroni and zamparelli (2010) and coecke (2010) to outperform those approaches that combine vectors separately representing determiners and nouns.
P13-2011,1,we perform uncertainty identification experiments on the generated dataset to explore the effectiveness of different types of features.
P13-2014,1,"future work will explore sampling based approaches to belief update and decision making (doshi and gmytrasiewicz, 2009) to overcome these problems."
P13-2014,1,we showed that implicatures arise in cooperative contexts from nested belief models.
P13-2017,2,"finally, this data is available on an open source repository in the hope that the community will commit new data and make corrections to existing annotations."
P13-2017,1,"it will also allow the inclusion of language-specific functional or morphological markers (case markers, topic markers, classifiers, etc.)at the leaves of the tree, where they can easily be ignored in applications that require a uniform cross-lingual representation."
P13-2025,1,"we showed, on two corpora, that knowing multiple scores for each example instead of a single score results in a more reliable estimation of the quality of a nlp system."
P13-2032,1,in this paper we have presented an effective yet simple approach to chinese word segmentation on micro-blog texts.
P13-2040,1,"by sharing parameters across different groundings, we should be able to identify semantic neighborhoods with fewer training instances."
P13-2041,1,"we have presented a fully unsupervised humor generation system for generating jokes of the type i like my relationships like i like my source, open i like my coffee like i like my war, cold i like my boys like i like my sectors, bad i like my x like i like my y, z, where x, y, and z are slots to be filled in."
P13-2042,1,"evaluated on the wsj corpus, the proposed td and to models reduced the bigram鈥檚 and trigram鈥檚 perplexities up to 23.5% and 14.0%, respectively."
P13-2044,5,"furthermore, the possible presence of crowd-working scammers (only partially filtered by the gold standard questions) could have reduced the statistical power of our analysis."
P13-2044,5,"finally, the adopted humor generation task (based on a single word substitution) is extremely simple and the constraints might have not been sufficiently capable to produce a detectable increase of humor appreciation."
P13-2044,5,"the statistical significance is particularly high, even though there were several limitations in the experimental setting."
P13-2044,3,the statistically strong results that we obtained can make this evaluation approach attractive for related tasks.
P13-2044,1,"in our methodology, we focused attention to the correlation between the parameters of the system (in our case, the constraints used in lexical selection) and the performance of humor generation."
P13-2044,1,this would offer a novel way to intentionally control the humorous effect.
P13-2045,2,"tables 4 and 5 show the results of the various methods on the cartoon captions and crossword clues datasets, respectively."
P13-2045,1,"on the crossword clues datasets, the random-walk-based methods are clearly superior to the other methods tested, whereas simple clustering is more effective on the in some sense, the two datasets in this paper both represent difficult domains, ones in which authors are intentionally obscure."
P13-2045,5,the good results acheived on the crossword clues dataset indicate that this obscurity can be overcome when discourse units are short.
P13-2046,1,"in this paper, we introduced our machine learning based approach for identifying lvcs in hungarian and english free texts."
P13-2047,2,to this end we will re-use human evaluation data gathered within the 2013 campaign.
P13-2047,5,we will also address the problem of tailoring automatic evaluation measures to russian accounting for complex morphology and free word order.
P13-2052,1,"in future, we will revise the equivalence measure and also experiment with clustering algorithms such as (beeferman et al., 2000)."
P13-2052,6,we will also study the contribution of individual components of the measure in such task.
P13-2054,1,"in future, we plan to first develop stemming algorithms for both sorani and kurmanji and then leverage those algorithms to examine the lexical differences between the two dialects."
P13-2055,2,"as igt data is available for hundreds of languages through the odin database and other sources, one could produce a small parallel treebank for a language pair after spending a few hours manually correcting the output of a projection algorithm."
P13-2057,5,maybe there are wrong translations with high probability which the language model cannot remove them from the best translations.
P13-2058,1,the key of our approach is a translation-based high-quality retrieval model which gradually adapts to the target domain by iteratively re-training the underlying smt model on a few thousand parallel sentences retrieved in the step before.
P13-2059,2,"propositions can also be changed, or even induced, by existing written sign representation languages such as zebedee (filhol, 2008) or hamnosys (hanke, 2004), mainly for the sake of extend ability."
P13-2059,1,"for verification and model extraction, further optimizations are expected, including the handling of data inconsistencies and repairing broken queries when verifying the graph."
P13-2059,1,"from the application side, we still need to create an extensive sign database codified in pdlsl and try recognition on other corpora, with different tracking information."
P13-2059,1,"the traits we have chosen to represent were imposed by the limits of the tracking tools we had to our disposition, most notably working with 2d coordinates."
P13-2062,2,extending the study to different language pairs and studying the applicability of this technique for machine translation quality estimation are also on the agenda.
P13-2063,1,"we presented a novel sequence labelling based, context-sensitive pruning method for a string-totree mt model."
P13-2063,4,our method achieves more than 60% speed-up over a state-of-the-art baseline on a full-scale translation task.
P13-2066,1,"in this paper, we present an rst-based translation framework for modeling semantic structures in translation model, so as to maintain the semantically functional integrity and hierarchical relations of edus during translating."
P13-2067,3,"we presented the first ever results to demonstrate that tuning an smt system against meant produces much adequate translation than tuning against bleu or ter, as measured across all other commonly used metrics and human subjective evaluation."
P13-2071,1,our modification to the osm model produces the best results giving significant improvements in most cases.
P13-2071,1,we have addressed the problem of the independence assumption in pbsmt by integrating ngram-based models inside a phrase-based system using a log-linear framework.
P13-2071,1,"although our modifications to the osm model enables discontinuous mtus, we did not fully utilize these during decoding, as moses only uses continous phrases."
P13-2072,1,other flavors of partial entailment should be investigated as well.
P13-2073,4,"in the future, we plan to explore other features, e.g., the number of the pivot phases used in connecting the source and target phrase pair and the similarity between these pivot phrases."
P13-2078,3,we compared the kullback-leibler divergence to a simple self-information measure.
P13-2079,1,"besides, learning-based approaches to extract phenomena and multi-class te recognition will be explored in the future."
P13-2080,1,"future research may add additional features and more complex feature combination methods, such as weighted sums tuned by machine learning."
P13-2082,1,"the learning process can still utilize the transferred knowledge, as it provides scaffolding for the latent learning process, resulting in a significant improvement in performance."
P13-2087,4,"while we have shown promising results in a single task, we believe that the method is general enough to be applied to a range of supervised tasks and source embeddings."
P13-2088,2,"furthermore, we also plan to explore using arabic-specific and more powerful features."
P13-2092,1,"future work will include building a system able to perform the task we have defined, as well as extending this work to include indirect quotes."
P13-2095,5,the aim of this work was to revisit these tasks as classical supervised learning problems that usually carry to high accuracy levels with high performance when faced with standard machine learning techniques.
P13-2095,1,"we presented an approach to reveal definitions and extract underlying hypernym relations from plain text, making use of local syntactic information fed into a support vector machine classifier."
P13-2098,3,we presented a set of experiments on incorporating features into an existing ocr system via nbest list reranking.
P13-2100,1,"we presented an ilp model for nlg that jointly considers the choices in content selection, lexicalization, and aggregation to avoid greedy local decisions and produce more compact texts."
P13-2102,1,"in this paper, we described a pipeline for the generation of scientific surveys starting from a topic query."
P13-2102,2,one of the main contributions of this work is a manually annotated data set for evaluating both the tasks.
P13-2103,2,we further intend to use this scheme and computational frameworks to serve a wide cross-parser investigation on inferring functional structures across languages.
P13-2109,1,"we decoded with ad3, an accelerated dual decomposition algorithm which we adapted to handle large components, including specialized head automata for the third-order features, and a sequence model for head bigrams."
P13-2109,1,we presented new third-order non-projective parsers which are both fast and accurate.
P13-2112,1,"we have proposed a method for unsupervised pos tagging that performs on par with the current stateof-the-art (das and petrov, 2011), but is substantially less-sophisticated (specifically not requiring convex optimization or a feature-based hmm)."
P13-2114,2,identifying further explicit sources of temporal information applicable to new sets of relations may reveal promising paths for investigation.
P13-2116,2,our ongoing work is to apply these ideas to a much larger corpus from each of the three domains.
P13-2120,1,"in particular, we have observed that there is a huge room for improvements in the aggregation step."
P13-2127,1,we will consider a sense-assignment method (voting or mace) as more appropriate if it provides the sense tags that are easiest to learn by our wsd system.
P13-2128,1,the only information strictly necessary for the methods we propose is a grouping of lemmas into derivationally related classes.
P13-2128,1,the estimation of generic semantic similarity can profit more from derivational smoothing than the induction of specific lexical relations.
P13-2129,2,we have demonstrated the merits of using das for verb clustering compared to the scf data from which they are derived on standard verb classification datasets and when integrated in a stateof-the-art verb clustering system.
P13-2133,1,this method disambiguates polysemous words in context vectors by selecting only the most relevant translations.
P13-2133,1,five semantic similarity and relatedness measures were used for this purpose.
P13-2134,6,gains from fixing resources may sometimes even exceed what the best possible algorithmic improvements can provide.
P13-2137,1,"we have described the construction of dm. hr, a syntax-based distributional memory for croatian built from a dependency-parsed web corpus."
P13-2142,3,we examined the under-studied task of stance classification of ideological debates.
P13-2153,1,"our findings show that the low- and highexposure groups produced similar numbers of words, but the high-exposure group tended to produce longer sequences of phonetically similar words."
P13-3003,5,"with regard to interactions with others, the prominence of different individuals and associated affect differed depending on condition."
P13-3003,5,previous research has found that fibromyalgia patients report a lack of understanding from medical practitioners and others around them (e.g.
P13-3004,1,"in this paper, we presented a mildly supervised method for identifying metaphorical verb usage by taking the local context into account."
P13-3005,1,as future work we would like to run more experiments with predicted supertags.
P13-3005,1,"ptb tags and supertags are complementary, and for all three parsers we observe slight benefits from being supplied with both types of tags."
P13-3005,1,"in the absence of a specialized supertagger, we can follow the pipeline of (ytrestl, 2011) who reached the stateof-the-art supertagging accuracy of 95%."
P13-3006,1,"the best performing classifier is crf-based and combines lexical, syntactical and semantical features in order to obtain an f-score of 79.35%."
P13-3009,6,information implicitly stated in the sentence should be stated explicitly.
P13-3011,1,"moreover, methods for facilitating annotated corpus construction will be explored, potentially adding new knowledge to the science of annotation."
P13-3015,5,low precision is a constant factor in all techniques and future research should aim to address this.
P13-3016,6,we hope to address these topics in the future.
P13-3017,1,"that is because, when ambiguity increases, the baseline method inaccurately assigns one pos tag to word types."
P13-3017,1,"on the other hand, the gap statistic method is not fully efficient in guessing the number of clusters."
P13-3018,1,"in the next phase of our work we will extend the existing experiments and also apply some more techniques like, crowd sourcing and language games to collect more relevant rt and compositionality data."
P13-3019,3,it will be determined how much are the segment level evaluation results influenced by these ranking orders.
P13-3019,2,"the accuracy of cesm can be further increased by the use of paraphrases, which can be obtained by using a german thesaurus or a lexical resource like germanet (hamp and feldweg, 1997)."
P13-3023,1,"the results from the evaluation on six different languages from the conll 2009 data sets indicate that the system is able to learn most morphological rules correctly and is able to cope with previously unseen input, performing significantly better than a dictionary learned from the same amount of data."
P13-3024,3,future work is needed to assess the extent to which the sdt-p measure and its word-level variant provide a general framework for dsms evaluation without external resources.
P13-3024,3,"such a word-specific measure could assess the semantic stability of different parts of the lexicon such as concrete vs. abstract word categories, or the distribution properties of different linguistic categories (verb, adjectives, ..)."
P13-4001,1,"besides, webanno supports project definition, import/export of tag and tagsets."
P13-4004,3,a beta version is currently open to researchers for experimentation.
P13-4005,1,"in this demo paper, we presented a system that links mainstream media stories to tweets that comment on the events covered."
P13-4005,1,"the system retrieves relevant tweets, extracts the links they contain and subsequently performs sentiment analysis."
P13-4006,1,"in particular, several distributional models of word meaning in context share important similarities with composition models, and we plan to add them to dissect."
P13-4007,1,"the framework is under active development, with work on several new features planned or in progress."
P13-4009,1,"besides, we will also optimize the algorithms and codes to improve the system performances."
P13-4010,2,"in the future, specialized plugins are planned to work with different linguistic annotations, e.g. cross-sentence annotations as used to annotate coreference chains."
P13-4011,3,edgar has been tested with real users for the last year and we are currently performing a detailed evaluation of it.
P13-4012,1,"in this paper, we introduce a simple chatterbot for answering non-obstructive psychological questions."
P13-4016,1,"first, we plan to support advanced rule extraction techniques, such as fuller support for count regularization and forest-based rule extraction (mi and huang, 2008), and using the em algorithm to choose attachments for null-aligned words (galley , 2006) or the direction of rule binarization (wang , 2007)."
P13-4023,3,"hence, we also provide hyena-live as a json compliant entity classification web-service."
P13-4023,1,these nodes can be further expanded in order to check which sub-classes have been accepted or rejected by hyena-live.
P13-4025,4,"in the future, we plan to use this tool for analyzing the nature of pivoted paraphrases."
P13-4029,3,our initial evaluations have shown encouraging results and further evaluations are now planned.
P13-4030,1,our evaluation on small scale data shows that the multistage approach performs better than complete sentence translation.
P13-4032,1,"in the future, we would like to add automatic localization strategies, new aggregation functions and a completely new package for fusing image- and text-based representations."
P13-4033,1,we expect that the availability of a document-level decoder will make it substantially easier to leverage discourse information in smt and make smt models explore new ground beyond the next sentence boundary.
P14-1001,5,"while we aimed for an exhaustive study, including multiple on-learning algorithms, different conversions to batch and derandomizations, we are aware that the problem we studied is very rich and admits many more facets and scenarios that we plan to investigate in the future."
P14-1006,1,further experiments and analysis support our hypothesis that bilingual signals are a useful tool for learning distributed representations by enabling models to abstract away from mono-lingual surface realizations into a deeper semantic space.
P14-1007,4,"it would also be interesting to try a task-specific adaptation of the erg parse ranking model, for example retraining on the pre-existing treebanks but giving preference to analyses that lead to correct crawler results downstream."
P14-1007,1,"in future work, we will seek to better understand the division of labor between the systems involved through contrastive error analysis and possibly another oracle experiment, constructing gold-standard mrss for part of the data."
P14-1008,1,other directions of our future work include further exploitation of the new semantic representation.
P14-1008,3,"for example, since abstract denotations are readily suited for data querying, they can be used to verify newly generated assumptions by fact search in a database."
P14-1008,4,this may open a way towards a hybrid approach to rte wherein logical inference is intermingled with large scale database querying.
P14-1008,1,"as such, our current rte system is at a proofof-concept stage, in that many of the above techniques are yet to be implemented."
P14-1010,4,"in the future, we plan to explore introducing multiple segmentation options into the lattice, and the application of our method to a full morphological analysis (as opposed to segmentation) of the target language."
P14-1010,4,"eventually, we would like to replace the functionality of factored translation models (koehn and hoang, 2007) with lattice transformation and augmentation."
P14-1011,1,"in the future work, we will explore four directions.1) we will try to model the decoding process with dnn based on our semantic embeddings of the basic translation units."
P14-1013,1,"since the translation of the current sentence is usually influenced by the topic of previous sentences, we plan to leverage recurrent neural networks to model this phenomenon, where the history translation information is naturally combined in the model."
P14-1013,4,"in the future research, we will extend our neural network methods to address document-level translation, where topic transition between sentences is a crucial problem to be solved."
P14-1014,1,"for future work, we would like to investigate the two-phase approach to more challenging tasks, such as web domain syntactic parsing."
P14-1016,2,"another direction involves incorporating richer feature space for better inference performance, such as multi-media sources (i.e. pictures and video)."
P14-1016,2,"one direction of our future work involves exploring more general categories of user profile at tributes, such as interested books, movies, hometown, religion and so on."
P14-1017,1,"moreover, understanding the underlying psychological and cultural mechanisms that establish the effectiveness of these features is a fundamental problem of interest."
P14-1017,1,"in future work, it will be interesting to examine how these features generalize to longer and more extensive arguments."
P14-1018,4,"in addition, we aim to experiment with neighborhood-specific classifiers applied towards the tweets from neighborhood-specific streams e.g., friend classifier used for friend tweets, retweet classifier applied to retweet tweets etc."
P14-1018,1,"in future work, we plan to incorporate iterative model updates from newly classified communications similar to online perceptron-style updates."
P14-1022,4,"moreover, we show that our parser is adaptable to other tree-structured tasks such as sentiment analysis; we outperform the recent system of socher (2013) and obtain state of the art performance on their dataset."
P14-1023,1,"to give just one last example, distributional semanticists have looked at whether certain properties of vectors reflect semantic relations in the expected way: e.g., whether the vectors of hypernyms 鈥渄istributionally include鈥 the vectors of hyponyms in some mathematical precise sense."
P14-1025,2,using a newly gathered corpus we measured the effects of various forms of weak supervision on performance.
P14-1029,1,"we further make the models to be dependent on the text being modified by negators, through adaptation of a state-of the-art recursive neural network to incorporate the syntax and semantics of the arguments; we discover this further reduces fitting errors."
P14-1029,1,"the detailed analysis reveals the differences in the behavior among negators, and we argue that they should always be modeled separately."
P14-1032,1,"in future work, we plan to extend the proposed method to jointly mine product features along with customers' opinions on them."
P14-1035,4,our method establishes the possibility of representing the relationship between character and narrative form in a hierarchical bayesian model.
P14-1035,1,"it is also worth noting that the models tested above diverge from many structuralist theories of narrative (propp, 1998) by allowing multiple instances of the same persona in a single work."
P14-1035,1,"learning structural limitations on the number of “protagonists” likely to coexist in a single story, for example, may be another fruitful area to explore."
P14-1035,1,"in all cases, the machinery of hierarchical models gives us the flexibility to incorporate such effects at will, while also being explicit about the theoretical assumptions that attend them."
P14-1037,5,"in our case, we only have the natural language query, which presents the more difficult problem of associating the entity class in the query (e.g., hiking trails) to concrete entities (e.g., avalon super loop)."
P14-1038,1,"for the first time, we addressed this challenging task by an incremental beam-search algorithm in conjunction with structured perceptron."
P14-1039,4,we leave these application-centered issues for investigation in future work.
P14-1040,1,"in the future, we plan to remedy this using a ranking approach such as proposed in (velldal and oepen, 2006; white and rajkumar, 2009)."
P14-1041,5,"in the future, we would like to investigate how our framework deals with such discourse level simplifications i.e., simplifications which involves manipulation of the coreference and of the discourse structure."
P14-1041,1,"as argued by siddharthan (2006), correctly capturing the interactions between these phenomena is essential to ensuring text cohesion."
P14-1042,6,detailed analysis reveals some important directions for future investigation.
P14-1043,1,"for future work, among other possible extensions, we would like to see how our approach performs when employing more diverse parsers to compose the parse forest of higher quality for the unlabeled data, such as the easyfirst non-directional dependency parser (goldberg and elhadad, 2010) and other constituent parsers (collins and koo, 2005; charniak and johnson, 2005; finkel , 2008)."
P14-1047,1,we conclude that multiagent rl of dialogue policies is a promising alternative to using single-agent rl and sus or learning directly from corpora.
P14-1047,4,"furthermore, we intend to apply multi-agent rl to more complex negotiation domains, e.g., experiment with more than two types of resources (not just apples and oranges) and more types of actions (not just offers and acceptances)."
P14-1047,1,the advantage of this approach is that it does not require sus to train against or corpora to learn from.
P14-1047,1,two agents interacted with each other and both learned at the same time.
P14-1048,1,"last but not least, as reflected by the low mafs in our experiments, some particularly difficult relation types might need specifically designed features for better recognition."
P14-1048,1,"in this paper, we presented an efficient text-level discourse parser with time complexity linear in the total number of sentences in the document."
P14-1049,1,"in future work, we will focus on exploring more contextual discourse information via the graph model and better ways of integrating intra and inter-sentence information on negation focus identification."
P14-1049,1,"in this graph model, the relatedness between words is calculated by word co-occurrence, wordnetbased similarity, and topic-driven similarity."
P14-1049,3,evaluation on the *sem 2012 shared task corpus indicates the usefulness of contextual discourse information on negation focus identification and our graph model in capturing such global information.
P14-1050,1,"in order to extract new sentiment words from large-scale user-generated content, this paper proposes a fully unsupervised, purely data-driven, and weibo post with/without new sentiment words."
P14-1050,1,"from linguistic perspectives, our framework is capable to extract adjective new words because the lexical patterns usually modify adjective words."
P14-1052,1,"struct formalizes the generation problem as an mdp and applies a version of the uct algorithm, a fast online mdp planner, to solve it."
P14-1053,4,"the next step would be to integrate bitext alignment across texts in two natural languages, inevitably introducing another stochastic component into the pipeline."
P14-1053,1,"for generating effecting l2 content, it is important that the user be kept in a one of proximal development and a tight region where the level of the taught content is at just the right difficulty."
P14-1058,3,we evaluated the proposed method in two domain adaptation tasks: cross-domain pos tagging and crossdomain sentiment classification.
P14-1068,4,"in the future, we would like to apply our model to other tasks, such as image and text retrieval (hodosh et al., 2013; socher et al., 2013b), zero-shot learning (socher et al., 2013a), and word learning (yu and ballard, 2007)."
P14-1070,3,"we experimented strategies to predict both mwe analysis and dependency structure, and tested them on the dependency version of french treebank (abeill麓e and barrier, 2004), as instantiated in the spmrl shared task (seddah , 2013)."
P14-1071,1,this paper presented a novel framework called error case frames for correcting preposition errors with feedback messages.
P14-1072,1,we have demonstrated the effectiveness of our method in two such areas and showed significant improvements in both.
P14-1073,1,"our primary contribution consists of new modeling ideas, and associated inference techniques, for the problem of cross-document coreference resolution."
P14-1076,5,"because of the practical importance of domain adaptation for relation extraction due to lack of labeled data in new domains, we hope our study and findings will lead to further investigations into this problem."
P14-1076,1,experimental results on ace 2004 and yago have shown that the our domain adaptation method achieves the best performance on f1 measure compared with the other baselines when only few labeled target instances are used.
P14-1080,1,our contributions can be summarized as: 1) the new translation rules are more discriminative and sensitive to cohesive information by converting the source string into a css-based taggedflattened string; 2) the new additional features embedded in the log-linear model can encourage the decoder to produce transitional expressions.
P14-1084,1,"in this paper, we describe a memory-based approach in which we use a corpus of past news to learn valid syntactic sentence structures."
P14-1091,1,"any method that can generate answers to questions, such as the web-based qa approach, can be integrated into this framework, by using them in the question translation component."
P14-1091,5,"as we discussed in the experiment part, how to mine high-quality question patterns is worth further study for the qa task; (ii) we plan to integrate an ad-hoc ner into our kb-qa system to alleviate the entity detection issue; (iii) in fact, our proposed qa framework can be generalized to other intelligence besides knowledge bases as well."
P14-1100,1,"empirically, our algorithm performs favorably to the ccm of klein and manning (2002) without the need for careful initialization."
P14-1102,1,"in future, it would be interesting to incorporate lexicalization into the model presented in this paper, as this feature seems likely to bridge the gap between this model and babysrl in transitive settings."
P14-1102,5,this fact suggests that the findings of gagliardi and lidz (2010) may be partially explained by a frequency effect: perhaps the input to children is simply biased such that wh-relatives are much more common than that-relatives (as shown in table 5).
P14-1102,1,"this model also initially reflects the 1-1 role bias observed in children (gertner and fisher, 2012) as well as previous models (connor , 2008; connor , 2009; connor , 2010) without sacrificing accuracy in canonical intransitive settings."
P14-1102,1,a lexical model could potentially pick up on clues which could indicate when that is a relativizer or simply improve on its comprehension of wh-relatives even more.
P14-1102,1,it is interesting to note that the cuurent model does not make use of that as a cue at all and yet is still slower at acquiring that-relatives than wh-relatives.
P14-1105,2,"we use this approach to create a new dataset from the ibc, which is labeled at both the sentence and phrase level."
P14-1112,1,"poor entity mention detection is a major source of error in both cases, suggesting that future work should consider integrating entity linking with joint syntactic and semantic parsing."
P14-1112,1,"by including the hypernym relation constraints while training word embeddings, we expect to improve the embeddings such that they become more suitable for this task."
P14-1114,1,"third, we try to improve our generated summary by resolving coreferences and incorporating speaker information (e.g., names) in the clustering and sentence generation phases."
P14-1114,1,"first, we are trying to improve our model by incorporating conversational features (e.g., speech acts)."
P14-1114,1,"second, we aim at implementing a strategy to order the clusters for generating more coherent abstracts."
P14-1122,3,"the first game, infection, validates concept-concept relations, and the second, the knowledge towers, validates image-concept relations."
P14-1122,4,"all annotated resources, demos of the games, and a live version of the top-ranking items for each concept are currently available online.5 in the future we will apply our video games to the validation of more data, such as the new wikipedia bitaxonomy (flati , 2014)."
P14-1122,3,"in experiments involving online players, we demonstrate three contributions."
P14-1122,6,"first, games were released in two conditions whereby players either saw financial incentives for playing or a personal satisfaction incentive where they were thanked by us."
P14-1130,2,future work involves extending the tensor component to capture higher-order structures.
P14-1130,1,our parser outperforms the turbo and mst parsers across 14 languages.
P14-1130,1,we implement the approach on first-order to third-order dependency parsing.
P14-1131,1,"we also presented extensions of cosimrank for a number of applications, thus demonstrating the flexibility of cosimrank as a similarity measure."
P14-1136,1,"finally, we presented results on propbank-style semantic role labeling with a system that included the task of automatic verb frame identification, in tune with the framenet literature; we believe that such a system produces more interpretable output, both from the perspective of human understanding as well as downstream applications, than pipelines that are oblivious to the verb frame, only focusing on argument analysis."
P14-1138,1,"we also plan to enrich each hidden layer in our model with multiple layers following the success of yang et al.(2013), in which multiple hidden layers improved the performance of the ffnn-based model."
P14-1138,1,"in future, we plan to employ contexts composed of surrounding words (e.g., c(fj) or c(eaj) in the ffnn-based model) in our model, even though our model implicitly encodes such contexts in the alignment history."
P14-1139,1,the algorithm increases the number of exact solution found on the model of denero and macherey (2011) from 6% to 86%.
P14-1146,1,these methods typically only model the context information of words so that they cannot distinguish words with similar context but opposite sentiment polarity (e.g.good and bad).
P14-2002,2,"with these results in hand, future work may now consider the automatic construction of a properly balanced text collection, such as originally desired by the creators of the brown corpus."
P14-2005,1,"the proposed blanc is free from this assumption, and we have shown that it subsumes the original blancgold."
P14-2005,4,"since blanc works on imperfect system mentions, we have used it to score the conll 2011 and 2012 coreference systems."
P14-2006,5,"we have cleared several misunderstandings about coreference evaluation metrics, especially when a response contains imperfect predicted mentions, and have argued against mention manipulations during coreference evaluation."
P14-2006,2,"furthermore, we have a reference implementation of these metrics that has been rigorously tested and has been made available to the public as open source software."
P14-2007,1,"as a future work, we would like to investigate how sac of a test sentence can be used to choose a classifier from an ensemble, and to determine the pre-processing steps (entity relationship extraction, for example)."
P14-2007,2,"first, the process of data preparation through eye tracking, labeled with the sac score was elaborated."
P14-2010,2,"in future work, we also envision the possibility of sprinkling knowledge from background knowledge sources like wikipedia (gabrilovich and markovitch, 2007) to realize an alignment of topics to wikipedia concepts."
P14-2012,1,our results show that page-rank in conjunction with re-ranking by initial confidence score can be used as an effective approach to collectively disambiguate named entity textual mentions in a document.
P14-2012,1,"our proposed features are very simple and easy to extract, and work well when employed in pr."
P14-2015,1,"we found that the entity status within the document is one of the important clues for solving the relevance detection problem, and showed that this information can be effectively automatically extracted using supervised classification."
P14-2016,1,"in future work, we hope to explore the ability of the method to detect domain-specific dictionaries (e.g.training over domain-specific dictionaries from other language pairs), and low-density languages where there are few dictionaries and wikipedia articles to train the method on."
P14-2017,5,"we are interested to find out if the orthographic rules depend on the source language, or if they are rather specific to the target language."
P14-2017,3,"finally, we plan to make a performance comparison on cognate pairs versus word-etymon pairs and to investigate false friends (nakov , 2007)."
P14-2019,2,"as a next step, we will focus on morphological analysis and disambiguation of turkish words."
P14-2019,1,"after determining the correct morphological analysis of turkish words, we will use the parts of these analyses to replace the leaf nodes that we intentionally left as 鈥*none*鈥."
P14-2031,1,"as far as we are aware, this is the first study to automatically analyze this relationship involving the textual content of edits and turns."
P14-2031,1,"based on the types of turn and edit in an edit-turn-pair, we have operationalized the notion of corresponding and noncorresponding edit-turn-pairs."
P14-2036,1,"and to make the most of external knowledge, better ways to build topic space should be considered."
P14-2037,1,we presented a new probabilistic model for learning bilingual word representations.
P14-2041,1,"we also found that the most effective importance models are those that equate importance of an n-gram with its preferential use in higherscoring essays than in lower-scoring ones, above and beyond merely looking at the n-grams used in good essays."
P14-2041,1,"this demonstrates the utility of using not only gold, high-quality human summaries, but also sub-standard ones when developing content importance models."
P14-2042,1,"in our error analysis, we believe that by exploring the characterlevel pos and the internal word structure (zhang , 2013) at the same time, it is possible to further improve the performance of morphological analysis and parsing."
P14-2043,1,"in future work, we aim to perform a more fine-grained error analysis to gain a better understanding where the improvement in accuracy takes place."
P14-2043,1,one could also attempt to optimize the compound label splits to maximize prediction accuracy instead of applying a priori partitions.
P14-2052,3,"hence, we plan to conduct evaluations with people7."
P14-2052,1,we only used the rhetorical structures between sentences in this study.
P14-2054,1,"in the future, we will study the non-projective cases based on the recent parsing techniques for 1-endpoint-crossing trees (pitler et al., 2013)."
P14-2057,2,our future work will extend the work by including more views such as the stylistic and vocabulary richness views.
P14-2062,1,"in general, we find that the use of a dictionary tends to make aggregations more useful, irrespective of aggregation method."
P14-2063,4,"similar approaches can be extended to other nlp tasks using different semantic links, specific dictionary and special seed words."
P14-2064,5,"in this setting, we showed that the presence of difficult instances in training data misleads a machine learner into misclassifying clear-cut, easy cases."
P14-2072,1,"the main findings from these experiments are: 1) we can build deception classifiers for different cultures with accuracies ranging between 60-70%, with better performance obtained when using psycholinguistic word classes as compared to simple unigrams; 2) the deception classifiers are not sensitive to different topics, with cross-topic classification experiments leading to results comparable to the within-topic experiments; 3) we can use data originating from one culture to train deception detection classifiers for another culture; the use of psycholinguistic classes as a bridge across languages can be as effective or even more effective than the use of translated unigrams, with the added benefit of making the classification process less costly and less time consuming."
P14-2079,2,we conclude that it is possible to learn interpretable type systems directly from data.
P14-2081,1,"in our future work, we plan to design better data representation which can well fit into the two-stage hashing theme; we also intend to apply the proposed hashing approach to more informal genres (e.g., tweets) and other down-stream nlp applications (e.g., first story detection)."
P14-2081,1,"in this paper, we proposed a novel two-stage unsupervised hashing framework for efficient and effective nearest neighbor search in massive document collections."
P14-2083,1,"we propose to embrace such disagreements rather than using annotation guidelines to optimize inter-annotator agreement, which would bias our models in favor of some linguistic theory."
P14-2084,1,"the obvious next step is to develop new machine learning models that exploit the contextual information available in the corpus we have curated (e.g., previous comments by the same user, the thread topic)."
P14-2084,2,the data comprises comments scraped from the social news website reddit.
P14-2084,1,"we have shown that annotators rely on contextual cues (in addition to word and grammatical features) to discern irony and argued that this implies computers should, too."
P14-2085,1,"we have described a new, context-aware approach to automatically predicting aspectual class, including a new set of distributional features."
P14-2085,2,we have also introduced two new data sets of clauses labeled for aspectual class.
P14-2087,1,"we would also like to explore how to incorporate syntactic features and employ alternative statistical methods (e.g., parametric models) to improve probability estimation and inference."
P14-2087,1,the base model on average more than doubled the accuracy of lesk in senseval-2 on both fine- and coarse-grained tracks.
P14-2090,1,"in addition, the greedy+dp algorithm uses only one feature per a position in this paper."
P14-2093,4,"in the future, we are interested in applying  our methods into domain adaptation task of statistical machine translation in model level."
P14-2094,1,"also, the findings from the time/productivity tradeoffs indicate that more time efficient algorithms and implementations should be explored."
P14-2097,1,"compression methods for removing visually irrelevant information (kuznetsova , 2013) may also help increase the relevance of extracted captions."
P14-2103,1,evaluation on a standard data set shows that our method consistently outperforms the supervised state-of-the-art method for the task.
P14-2107,1,"it is worth pointing out that other dependency parsing frameworks (e.g., transitionbased parsing (zhang and clark, 2008; zhang and nivre, 2011)) could also benefit from modeling structural diversity in search."
P14-2107,3,"it was shown that by keeping a diverse beam significant improvements could be achieved on standard benchmarks, in particular with respect to difficult attachment decisions."
P14-2108,1,"adjusting for two major differences that are a matter of annotation convention, we showed that the ppcmbe can be parsed at approximately the same level of accuracy as the ptb."
P14-2112,1,the approach is fully general and not just limited to language modelling.
P14-2114,1,experimental results show our proposed framework outperforms the state-of-the-art baseline by over 7% in f-measure.
P14-2114,3,our proposed model has been evaluated on the fsd corpus.
P14-2138,1,"we showed that a small set of high-frequency function words have disproportionate influence on the accuracy of a bigram-based nli classifier, and that the majority of the indicative bigrams appear to be independent of l1."
P14-3004,1,this knowledge can be used to estimate a network of support or opposition.
P14-3004,1,"another idea would be to make use of the speaker's given party affiliations and bootstrap an approach to analyze their positions: if we assume that a majority of the speakers actually does follow their parties' lines, we can train a classifier for each party for each topic, and apply it to the same data to detect outliers."
P14-3007,1,"firstly, the most general hypernym of subordinating conjunctions exerts an initial restrict to the following splitting step."
P14-3008,4,"the approach developed in this paper can also be applied to parse tree querying and manipulation problem (levy and galen, 2006)."
P14-3008,2,another obvious direction is applying tree kernels to classify short texts based on standard corpus data.
P14-3010,1,we see that this implicit handling is preferable to having no sense handling and also to having a full wsd module as part of a pipeline.
P14-3011,1,"our future plans include implementation of shallow parsing and syntactic n-grams (sidorov , 2012; sidorov , 2013; sidorov , 2014; sidorov, 2013a; sidorov, 2013b), as well as learning techniques, and analysis of their influence on the system鈥檚 performance."
P14-3011,1,we have introduced an approach to open ie based on syntactic constraints over pos tag sequences targeted at spanish language.
P14-5001,1,evaluation has shown that the method can identify more keywords and rank them higher in the candidate list than monolingual keas.
P14-5001,5,"as for future work, we would like to explore the possibility of incorporating the articles鈥 reader feedback into keyword extraction."
P14-5005,2,"as for the large corpus, on which the recommended words and sentences are based, and the corpus mining based on nlp techniques (e.g., word vector representation and topic model lda), experimental results show that our system is both helpful and meaningful."
P14-5005,1,"overall, our system provides syntactically and semantically related words, as well as recommends contextually related sentences to users."
P14-5006,1,we expect that a supervised system using a large variety of features would improve the state of the art in keyphrase extraction.
P14-5009,1,we also plan to build tools to import and export the work done in welt in order to facilitate collaboration among linguists working on similar languages or cultures.
P14-5014,1,"future plans include enriching the feature set, adding a tree-based language model and considering forest input for multiple parses to provide robustness against parsing errors."
P15-1003,2,"for future work, we will consider encoding more complex linguistic structures to further enhance the joint model."
P15-1004,1,"building on the success of this paper, we plan to develop other neural network-based features, and to also relax the limitation of current rule extraction heuristics by generating translations word-by-word."
P15-1005,1,another direction for future work would be to use a n-gram based language model constrained by the structured predicted in vdr.
P15-1006,5,another important problem not addressed here is the role of context and discourse in interpreting scene descriptions.
P15-1006,2,"however, additional data collection and experiments are necessary to confirm this and identify challenges specific to other languages."
P15-1006,1,an obvious improvement would be to expand our learned lexical grounding approach to include spatial relations.
P15-1006,4,"thus, we expect that our method for lexical grounding can facilitate development of text-to-scene systems in other languages."
P15-1009,2,"as future work, we would like to: 1) construct the manifold regularization terms using other data sources."
P15-1009,1,"we would try entity similarities derived in different ways, e.g., specified by users or calculated from entities textual descriptions.2) enhance the efficiency and scalability of sse."
P15-1011,3,"from our own viewpoints, creating more evaluation data for measuring further progress in contrasting-meaning modeling, e.g., handling real oov issues, is interesting to us."
P15-1011,5,"also, the degree of contrast may be better formulated as a regression problem rather than a classification problem, in which finer or even real-valued annotation would be desirable."
P15-1013,5,future work should explore how to combine efficiently different transformations.
P15-1015,1,"future work will remove the fixed ordering for feature templates, and dynamically add additional features based on the current scores of different labels."
P15-1020,1,"in addition, we hope to expand the methods proposed here to a more incremental setting, where both parsing and decoding are performed incrementally, and the information from these processes can be reflected in the decision of segmentation boundaries."
P15-1020,1,"as future work, we are planning to devise more sophisticated methods for language modeling using constituent tags, and ways to incorporate previously translated segments into the estimation process for left-hand constituents."
P15-1021,1,"future work includes developing a bottom-up btg parser with latent variables, and comparing the results to the top-down parser."
P15-1022,3,our future objective is to extend our evaluation to streams of data coming from a larger number of domains.
P15-1022,2,"however, we are confident that the gradual shift of the translation industry towards human mt post-editing will not only push for further research on these problems, but also provide data for larger scale evaluations in a short time."
P15-1024,1,"in the future, we plan to use logic-like semantic representations of texts, questions and answers and explore approaches to perform structured inference over richer semantic representations."
P15-1025,2,"for the future work, we will explore how to incorporate more types of metadata information, such as the user ratings, like signals and poll and survey signals, into the learning process to obtain more powerful word representations."
P15-1026,1,"furthermore, as our model is capable of detecting the most important words in a question, it would be interesting to use the results to mine effective question patterns."
P15-1026,2,"for instance, we are integrating more external knowledge source, such as clueweb (lin et al., 2012), to train mccnns in a multi-task learning manner."
P15-1027,1,"in particular, we want to devise more semantically-motivated methods to select chimera components and negative samples."
P15-1027,1,"both chimera and the intruder methods are flexible, and we plan to explore them further in future research."
P15-1028,2,"and finally, we would like to further explore the performance of the lexical function model and generalized lexical function model on different datasets, which involve more complex compositional phenomena."
P15-1028,6,"we could, e.g., try to separate the intersective adjectives from non-intersective adjectives."
P15-1028,1,"in future work, we would like to test different sizes of dimensionality reduction, in order to optimize our generalized lexical function model."
P15-1028,1,"moreover, it is possible that better results may be obtained by proposing multiple generalised lexical functions, rather than a single one."
P15-1035,1,"In the future, we are interested in extending these techniques to also exploit unlabeled data."
P15-1036,4,"as future work, we would like to use our method to extract more than just binary relations."
P15-1037,4,"skip-node kernel and its approximations are particularly effective for comparison identification, and potentially applicable to other relation extraction or natural language tasks (the direction of our future work)."
P15-1038,1,"in the future, we plan to add regular expression search over parses, and sorting within results tables."
P15-1038,3,we also leave for future work the comparison of these parsers across languages.
P15-1038,1,our hope is that the results from the evaluation as well as the tool will give non-experts in parsing better insight into which parsing tool works well under differing conditions.
P15-1038,4,we also hope that the tool can be used to facilitate evaluation and be used as a teaching aid in nlp courses.
P15-1038,2,"in addition, it may be possible to achieve good performance in particular genres by doing “mini-ensembles” trained on general purpose data (e.g.wb) and genre-specific data."
P15-1041,1,"in the future, we will explore more suitable knowledge representations and knowledge validation in the credboost framework."
P15-1042,1,"in the following work, we will further investigate the relationship between semantic and sentiment information for clsc, and balance their functions to optimize their combination for clsc."
P15-1043,1,"since the different content models use different kinds of lexical information, further gains might be obtained by combining some of these models into a joint model."
P15-1043,6,We plan to explore this in future work.
P15-1044,1,"in order to improve the performance of our generator and remove the dependency on domain specific features, we plan to replace the perceptron ranker with a neural network."
P15-1044,3,"the generator source code, along with configuration files for experiments on the bagel data set, is available for download on github.11 in future work, we plan to evaluate our generator on further domains, such as geographic information (kate , 2005), weather reports (liang , 2009), or flight information (dahl , 1994)."
P15-1045,1,"in addition, we plan to investigate other methods about multisentence compression and sentence fusion, such as supervised methods."
P15-1045,6,"firstly, we plan to introduce event relations to learning event salience."
P15-1045,6,"For future work, we plan to explore two directions."
P15-1048,2,"in future work, we will try to add new classes of features to further improve performance by capturing the property of disfluencies."
P15-1048,1,we would also like to make an end to-end mt test over transcribed speech texts with disfluencies removed based on the method proposed in this paper.
P15-1049,3,"in the future, we would like to investigate the advantages and disadvantages between tree based models and other non-linear models such as deep neural networks or recurrent neural networks."
P15-1051,1,"in our future work, we wish to explore a better way to encode distributional semantics by proposing a modified lda for better triples representation."
P15-1054,5,it would be an interesting future problem to estimate the value of k automatically in our setting.
P15-1054,1,"as future work, we also plan to extend our techniques to produce a hierarchical summary of topics and scale it across heterogeneous collection of objects (from different domains) to bring all of them under the same topic dag and investigate interesting cases thereon."
P15-1055,3,"in future work we aim to evaluate how our method performs on entities and relationships of  any type and popularity, including tail entities and miscellaneous relationships."
P15-1055,5,"we also want to investigate moving beyond wikipedia and extract candidate sentences from documents that are not related to the knowledge graph, such as web pages or news articles."
P15-1056,1,"moreover, we plan to study the generation of entity-driven event chronicles, leveraging more fine-grained entity and event extraction approaches."
P15-1056,1,"in the future, we plan to investigate automatically adapting an event’s granularity and learn the principle of summarizing the event according to the reference event chronicle."
P15-1057,6,our future work includes exploiting the profiles of target entities as feedback to refine the results of morph mention extraction.
P15-1057,1,we will also extend the framework for event morph decoding.
P15-1058,6,"in future work, new objectives should be integrated into the framework and existing objectives could be enhanced."
P15-1063,1,"in the future, we plan to test our approaches over longer time spans and to design the way to automatically “explain” temporal counterparts by outputting “evidence” terms for clarifying the similarity between the counterparts."
P15-1064,1,"in the future we will explore more effective features to improve the negation and speculation identification in chinese language, and focus on joint learning of the two subtasks."
P15-1065,1,an interesting future direction is to use products of these random walk features to express their conjunctions.
P15-1067,1,we will seek methods to complete knowledge graphs with new triplets whose entities and relations come from plain texts.
P15-1067,1,one possible way to obtain these new triplets is to extract facts from plain texts.
P15-1069,2,"in future, we plan to integrate a larger diversity of surface, semantic and linguistic information for relevant sentence selection."
P15-1069,1,"to move further in this direction, we plan to focus on exploiting morphological term variations taking advantage of the alternative terms provided by dbpedia."
P15-1070,1,"first, we would like to try adapting and evaluating some additional wsd algorithms for use with puns."
P15-1070,1,"second, we would like to investigate alternative tie-breaking strategies, such as the domain similarity measures used by mihalcea et al.(2010)."
P15-1070,5,"finally, whereas in this paper we have treated only the task of sense disambiguation for the case where a word is known a priori to be a pun, we are interested in exploring the requisite problem of pun detection, where the object is to determine whether or not a given context contains a pun, and more precisely whether any given word in a context is a pun."
P15-1071,4,"in future, we plan to apply the proposed method to other types of domain adaptation tasks such as cross-domain part of-speech tagging, named entity recognition, and relation extraction."
P15-1074,1,"one possible next step would be to expand this work to more complex semantic spaces which include stronger notions of compositionality, semantic roles, and so on, such as the distributional approaches of baroni and lenci (2010), sayeed and demberg (2014), and greenberg (2015) that contain grammatical information but rely on vector operations."
P15-1075,1,"other directions include developing methods to learn the sbt structure from data, as well as applying the sbt prior to other tasks, such as sequential language modeling."
P15-1075,5,one limitation of the current work is that the sbt is defined only implicitly.
P15-1075,1,we plan to investigate explicit representations of the sbt prior or related variants.
P15-1075,6,There are several directions of future work.
P15-1077,1,"an interesting extension to our work would be the ability to handle polysemous words based on multi-prototype vector space models (neelakantan et al., 2014; reisinger and mooney, 2010) and we keep this as an avenue for future research."
P15-1078,1,we further plan to incorporate features from the source sentence.
P15-1078,6,"in future work, we would like to experiment with an extension that allows for multiple references."
P15-1080,1,"as future work, it is necessary to integrate more features into our learning framework."
P15-1080,4,it is also interesting to see how the non-linear modeling fits in to more complex learning tasks which involves domain specific learning techniques.
P15-1083,6,"for the future work, we aim to use automatic prediction of deceivers to help truth-tellers win games more easily."
P15-1083,6,"in addition, it is worth exploring the impact of cross-cultural analysis in detecting deception."
P15-1083,1,"it will be interesting to study non-verbal features such as blink rate, gaze aversion and pauses (granhag and stromwall, 2002) when people play this game face-to-face and combine the non-verbal and verbal features for deception detection."
P15-1087,1,one of our future goals is to reduce the initial lexicon to be even smaller by further automating the nl2kr gui interaction component.
P15-1090,6,"first, we plan to consider how to conduct nsw detection and normalization at the same time."
P15-1090,4,"third, we want to investigate the impact of nsw and normalization on other nlp tasks such as parsing in social media data."
P15-1090,1,"second, we like to try a joint method to  simultaneously train the nsw detection and ner models, rather than just combining models in decoding."
P15-1092,2,"in the future, we would like to apply sp interpolation to multilingual sp learning, i.e. integrating data from multiple languages for more accurate sp induction and projecting universal semantic relations to low-resource languages."
P15-1092,1,"it is also interesting to investigate sp learning at the level of semantic predicates (e.g. automatically inducing framenet-style frames), where combining the visual and linguistic knowledge is likely to outperform text-based models on their own."
P15-1092,1,"in the future, it would be interesting to derive the information about predicate-argument relations from low-level visual features directly."
P15-1093,4,"since this framework is applicable to the argument classification in srl, applying our methods to that task is an interesting line of the future research."
P15-1093,2,"for future work, we plan to incorporate external resources for our joint methods."
P15-1094,1,"we also intend to explore more systematic ways to incorporate supervised signals into learning, to fine-tune c-phrase vectors to specific tasks."
P15-1094,3,"having established a strong empirical baseline with this parsimonious approach, in future research we want to investigate the impact of possible extensions on both lexical and sentential tasks."
P15-1094,1,"when combining the vectors, either for induction or composition, we will try replacing plain addition with other operations, starting with something as simple as learning scalar weights for different words in a phrase (mitchell and lapata, 2010)."
P15-1094,3,"we plan to perform a systematic analysis of both existing benchmarks and natural corpus data, both to assess the actual impact that such factors have on the aspects of meaning we are interested in (take two sentences in an entailment relation: how often does shuffling the words in them make it impossible to detect entailment?), and to construct new benchmarks that are more challenging for additive methods."
P15-1095,1,we hope our decomposition provides a useful framework to guide future work in ner++ and amr in general.
P15-1095,6,a clear direction of future work is improving the coverage of the defined actions.
P15-1095,1,"for example, a richer lemmatizer could shift the burden of lemmatizing unknown words into the amr lemma semantics and away from the dictionary lookup component."
P15-1096,1,"in future work, we also plan to integrate our work with robobrain (saxena et al., 2014) to leverage these existing systems for building a robotic system capable of working with physical world data."
P15-1097,2,"additionally, we would like to test our models on other rte challenges and on several qa datasets, which for space constraints we could not do in this work."
P15-1097,1,"in the future, it would be interesting defining graph kernels that can combine more than two substructures."
P15-1097,6,another possible extension regards the use of node similarity in graph kernels.
P15-1100,2,future work will include expanding the corpus and experimenting with datasets outside of the political domain.
P15-1100,2,"we also plan to evaluate this strategy on data from different online sources, e.g., twitter or youtube."
P15-1101,1,"in the future work, we would like to explore better ways of modeling the label and context dependence and apply our dfg approach in more applications, e.g. micro-blogging emotion classification."
P15-1107,4,"in any case, we look forward to more sophi4d applications of neural models to the important task of natural language generation."
P15-1111,1,"we are extending our work to other popular dependency parsers and non-projective parsing algorithms, and hope to develop features to improve and mitigate the cascading impact of punctuation attachment errors in parsing."
P15-1112,4,"moreover, we also wish to investigate the ability of our model for other nlp tasks."
P15-1112,1,"for the future research, we will develop an integrated parser to combine rcnn with a decoding algorithm."
P15-1112,1,we believe that the integrated parser can achieve better performance without the limitation of base parser.
P15-1113,4,"in the future, we plan to apply our neural network structure to dependency parsing."
P15-1113,1,"we are also interested in using long short-term memory neural networks (hochreiter and schmidhuber, 1997) to better model the locality of propagated information from the stack and queue."
P15-1113,6,the parameter estimation under semi-supervised setting will be investigated further.
P15-1114,4,"in the future, we would like to extend our technique to other real valued kernels such as the string kernels and tagging kernels."
P15-1115,1,"in future work, we plan to combine our incremental parsing/role labeling approach with a compositional model of semantics, which would have to be modified to take semantic role triples as input (rather than words or word pairs)."
P15-1116,2,"furthermore, we will experiment with larger feature sets that add lexical information."
P15-1116,1,"in future work, we will concentrate on methods that could remedy the data sparseness concerning discontinuous constituents, such as self-training."
P15-1116,1,an formal investigation of the expressivity of our parsing model is currently under way.
P15-1118,1,we plan to investigate methods which use paraphrases to augment parsing models created at train time.
P15-1118,2,"as part of future work, we plan to integrate existing larger paraphrase resources, such as wikianswers (fader et al., 2013) and ppdb (ganitkevitch et al., 2013)."
P15-1118,5,poor alignments are one of the larger sources of errors and improving alignments could help dramatically.
P15-1118,1,ppdb includes phrasal and syntactic alignments which could supplement our existing alignments or be used as proxies for paraphrases.score).+x indicates extending the above system with x. bllip-st is bllip using the self-trained model.
P15-1118,1,one simple extension is to use multiple paraphrases and their alignments instead of just one.
P15-1122,2,"in the near future, we plan to create and release a larger null instantiation corpus."
P15-1122,3,"for our future work, we plan to manually annotate coreference information so that we can compare with more methods."
P15-1122,2,"finally, we hope to exploit some additional knowledge resources, such as hownet, which could  potentially further improve the performance of our proposed method."
P15-1123,2,"no large corpora exist to date, but studying the interaction of these phenomena is on our research agenda."
P15-1123,4,"another related distinction is the one between habitual, stative and episodic sentences (mathew and katz, 2009), which applies to both what we call generic and non-generic sentences."
P15-1123,5,"we have not attempted to tackle the task of classifying the genericity status of other dependents, as they are even harder to classify than subjects, and a concise annotation scheme has to be worked out in order achieve an acceptable inter-annotator agreement on this task."
P15-1124,1,it would also be interesting to formally investigate the theoretical merits and algorithmic possibility of solving the variance weighted objective in eq.(6).
P15-1124,1,using cca to induce representations other than word embeddings is another important future work.
P15-1124,1,"even though the objective is hard to optimize in the worst case, it may be tractable under natural conditions."
P15-1125,2,another interesting aspect of future work is to incorporate other sources of knowledge to further enrich the semantics.
P15-1126,1,future work ought to pursue models in which all morphemes contribute both semantic and syntactic content to the word representations.
P15-1127,2,"it would also be interesting to gather data with compositional phenomena, such as negation and disjunction, and study its impact on the performance of the semantic parser."
P15-1128,4,applying other structured-output prediction methods to graph generation will also be investigated.
P15-1128,1,"in the future, we would like to extend our query graph to represent more complicated questions, and explore more features and models for matching constraints and aggregation functions."
P15-1129,4,"we believe that our methodology is a promising way to build semantic parsers, and in future work, we would like to extend it to handle anaphora and nested quantification."
P15-1131,1,"to overcome it, tslda should be extended as a non-parametric topic model estimating the number of topics inherent in the data."
P15-1131,6,This will be done in our future work.
P15-1135,5,"in summary, we hope to begin to pull back the veil on the types of information that a truly unsupervised system, if one should ever exist, would need to learn, and we pose a challenge to the community to find ways that a learner might discover this knowledge without hand-engineering it."
P15-1137,1,"because our approach automatically learns intermediate representations given raw features, directions for further research might alternately explore including additional (perhaps semantic) raw features, as well as developing loss functions that further discourage learning representations that allow for common errors (such as those involving pleonastic pronouns)."
P15-1139,1,"in future work, we plan to pursue the new framework suggested by our analyses, investigating the interaction of issue polarization and framing-based polarization."
P15-1142,4,a natural direction is to combine the logical compositionality of this work with the even broader knowledge source of general web pages.
P15-1142,1,"pasupat and liang (2014) used a framework similar to ours to extract entities from web pages, where the “logical forms” were xpath expressions."
P15-1143,1,"furthermore, we will investigate methods for speeding up graph parsing further, e.g. with different heuristics."
P15-1143,5,a challenge for grammar-based semantic parsing is grammar induction from data.
P15-1143,6,we will explore this problem in future work.
P15-1145,4,"at the end, we plan to apply the swe word embedding models for more natural language processing tasks."
P15-1145,2,"as for the future work, we would incorporate more types of knowledge, such as knowledge graphs and framenet, into the learning process for more powerful word representations."
P15-1145,1,we also expect that some common sense related semantic knowledge may be generated as ordinal inequality constraints by human annotators for learning semantic word embeddings.
P15-1150,2,our results suggest further lines of work in characterizing the role of structure in producing distributed representations of sentences.
P15-1152,1,"in future work, we would consider adding the intention (or sentiment) of users as an external signal of decoder to generate responses with specific goals."
P15-1153,1,"another aspect is to improve time efficiency of our framework, and its major bottleneck is the time consuming ilp optimization."
P15-1153,1,"for future work, one aspect is to enhance the grammar quality of the generated new sentences and compressed sentences."
P15-1158,3,future work will test to what extent this latent discourse information could affect the model predictions.
P15-1158,4,"in future work we hope to look at a broader range of referring expressions, such as null pronouns and definite descriptions, and to explore the extent to which our model can be applied to other linguistic phenomena that rely on discourse information."
P15-1159,5,a challenge for future work is to find reliable linguistic cues that generalize well between such settings.
P15-1160,2,"in the future, we plan to consider a greater variety of diseases and symptoms in order to develop applications for public health, e.g., monitoring the mental condition of individuals."
P15-1160,1,"thus, we can not only improve the accuracy of subject identification but also enhance the generality of this task."
P15-1167,1,"we would like to try using convolutional neural network to automatically encode ngram-like features, in order to further shrink parameter space."
P15-1167,4,it is also interesting to study whether extending our model with deep architectures can benefit cws.
P15-1167,1,"in the future, we plan to investigate methods for our model to better utilize external resources."
P15-1167,1,"lastly, it might be useful to adapt our model to tasks such as pos tagging and name entity recognition."
P15-1171,1,"since semi-supervised learning requires generative models in advance, our proposed bayesian generative model will also lay foundations to such an extension."
P15-1171,1,"in order to adapt to human standards given in supervised data, it is important to conduct a semisupervised learning with discriminative classifiers."
P15-1172,2,"for future, our immediate plan is to annotate more data with both ctb and pd tags (a few thousand sentences), and to investigate our coupled model with small amount of such annotation as extra training data."
P15-2003,2,"considering the coverage of word senses in our training data, in future work we plan to filter out those sense vectors which are under-represented in the training corpus."
P15-2003,3,we will also further investigate the feasibility of applying the multi-prototype word embeddings in a wide range of nlp tasks.
P15-2009,1,"for example, we can explore more effective ways to incorporate tweets for sentence compression; we can study joint models to combine both sentence extraction and compression with the help of relevant tweets; it will also be interesting to use the parallel dataset of the news articles and the tweets for timeline generation for a specific event."
P15-2009,6,There are some interesting future directions.
P15-2013,5,"as future work, it is necessary to further this investigation by taking into account various degrees of discrimination."
P15-2013,1,"as suggested in (gatt , 2013), the effect of discrimination may be perceived as a continuum, and in that case a practical reg algorithm should be able to make more complex decisions that those presently implemented."
P15-2019,2,in future we plan to upgrade the current word prediction pathway to a sentence reconstruction and/or sentence paraphrasing task in order to encourage the formation of representations of full sentences.
P15-2019,1,"we also want to explore the acquired structure further, especially for generalizing the grounded meanings to those words for which visual data is not available."
P15-2020,1,"in future work, we plan to explore more sophisticated visual generality measures, other semantic relations and different ways of fusing visual representations with linguistic knowledge."
P15-2022,1,in the future we will investigate stronger network structure such as lstm to further improve the prediction power of our model.
P15-2024,3,our future work includes experiments with other types of slpt problems that focus on different aspects of translation quality and language understanding.
P15-2030,1,"in the future, we will investigate ways for automatically optimizing the hyperparameters of the network (snoek et al., 2012) and various extensions to recursive or hierarchical convolutions."
P15-2030,1,"in the future, we will investigate ways for automatically optimising the hyperparameters of the network (snoek , 2012) and various extensions to recursive or hierarchical convolutions."
P15-2033,1,future research will be devoted to find models to effectively combine tks and dnn.
P15-2033,1,"this as well as further research will be integrated in our cp system described in (barlacchi et al., 2015)."
P15-2033,1,"in particular, our previous model exploiting linked open data in qa (tymoshenko , 2014) seems very promising to find correct answer to clues."
P15-2035,2,a focus of our future work will be to manually annotate the data to determine the frequency and nature of the topic excursions.
P15-2035,4,we also plan to apply our methods to asr output rather than manual transcripts.
P15-2037,1,"in the future, it would be interesting to improve our method to cover more kb predicates, and extend our nn model with more advanced structures to further improve the performances and also simultaneously characterize the target and comparison set involved."
P15-2039,2,we are planning to port our corpus and compare our scheme with ud to contribute to the improvement of ud for japanese.
P15-2043,1,"as future work, we plan to exploit the full tree structure of synthetic words to improve not only cws but also additional downstream tasks such as sentence parsing."
P15-2045,2,furthermore we would like to examine the pra-reduced data in more detail.
P15-2045,1,in future we would like to explore alternative  methods for selecting pra relation paths to identify false negatives.
P15-2045,2,we would like to find which kind of entity pairs are detected by our proposed method and whether the reduced data can also be used to extend the positive training data.
P15-2045,4,we would also like to apply the approach to other domains and alternative knowledge bases.
P15-2045,3,finally it would be interesting to compare our approach to other state of the art relation extraction systems for distant supervision or biased-svm approaches such as liu et al.(2003).
P15-2046,1,we will also investigate other ways of collapsing different types of tags in the lexicalized tree representation.
P15-2046,1,"in the future, we plan to mitigate the performance drop on the clueweb set by adding information about context words around relation words."
P15-2051,1,"in future work, we plan to experiment with applying more expressive machine learning techniques to this task."
P15-2055,1,"as part of on-going and future work, we will be incorporating additional retrieval models, such as the okapi bm25, in our evaluation framework."
P15-2057,5,"in future work, we will investigate how to update the category topic hierarchy incrementally with the creation of new related articles."
P15-2059,1,"as future work, we plan to explore in more detail this research line by applying more sophisticated approaches in the temporal analysis at document level."
P15-2060,1,"in the future, our plans include: (i) to explore the joint approaches for event extraction with cnns; (ii) and to investigate other neural network architectures for information extraction."
P15-2061,4,"future research may explore additional features and knowledge resources, investigate alternative approaches for creating effective seed lists, and extend our approach to argument labeling."
P15-2063,6,"we believe it will be useful for web scale extraction problems, where language identification and coarse language modeling are used to filter large amounts of data."
P15-2063,6,we plan to investigate a new hardware version that intel is preparing.
P15-2064,4,"for the future work, we would like to investigate the generality of our approach in broader languages and domains.and application of a wide-coverage multilingual semantic network."
P15-2066,6,additional research is needed to further explore this idea.
P15-2066,1,"the top features induced for each classification task can also be interpreted as our systems ability to discover new feature spaces, which can be utilized independently or along with a simpler feature space (e.g., bag of words) to learn a better classification model."
P15-2071,2,we intend to further account for finer-grained characteristics of the words and to extend our experiments to more languages.
P15-2071,1,"the method we propose is language-independent, but we believe that incorporating language-specific knowledge might improve the system’s performance."
P15-2074,2,"in the future, the evaluation of proposed method needs to be extended to large-scale test corpus and detailed context sensitive rules are used to identify tibetan unknown words."
P15-2077,1,we plan to extend this work by studying how the combination of the unsupervised selection of examples and their use for training supervised classifiers can be exploited for improving distributional thesauri through feature selection.
P15-2077,1,"we will also investigated the interest of taking into account word senses in this framework, as in (huang et al., 2012) or (reisinger and mooney, 2010)."
P15-2080,3,"in the future, we plan to study the influence of other factors such as temporal information to btm and its variants."
P15-2082,5,"in our future work, we work to automatically determine the number of authors of a multi-author document."
P15-2082,1,"furthermore, we will explore an adaptive learning method to select the optimal value of the threshold q for the probability indication procedure."
P15-2084,1,further work will include extending the dependency tree language modeling to long short-term memory rnns to handle longer syntactic dependencies.
P15-2086,4,"in the future, we plan to apply this approach to other cross-domain prediction tasks such as named entity recognition or semantic role labeling."
P15-2086,4,we also plan to extend our method to learn cross-lingual representations with auxiliary resources such as bilingual dictionaries or parallel sentences.
P15-2088,2,"in the future, we will try to exploit contextual information at the target side (e.g., partial translations)."
P15-2090,4,we hope that this release sparks more interesting research on decipherment and its applications to machine translation.
P15-2091,3,"going further, we could study the effect of using other hypotheses instead of the rerank one-best to perform the comparison with the moses one-best hypothesis."
P15-2091,1,"for our future work, we also plan to study approaches that can enhance the diversity in the k-best lists (chatterjee and cancedda, 2010; gimpel , 2013) between each iteration of the multi-pass decoding to train a better rerank after each decoding pass."
P15-2091,1,"another area for improvement lies in the addition of yet more complex features, for instance to allow a better dis6interestingly, a control experiment showed that using iteration-specific tables yields slightly better performance than fusioning all bi-phrases of a given type in a non iterationspecific table, possibly allowing later tunings to prefer the contents of the most recent, and possibly more reliable tables.course coherence modelling over iterations (ture , 2012; hardmeier , 2012)."
P15-2092,1,this suggests that future work on improving smt across genres needs to investigate approaches that increase model coverage.
P15-2093,4,its application in statistical machine translation and cross-lingual model transfer remains to be explored.
P15-2093,1,learning multiple embeddings per word and compositional embeddings with matrix factorization are also interesting future directions.
P15-2094,1,"in the future, we plan to explore more refined methods to devising effective intermediate expressions, and improve estimation of probabilities for triangulated rules."
P15-2101,2,"in the future, we want to incorporate wordnet knowledge to further reduce perplexity on infrequent words."
P15-2102,2,"to further improve the work, we will incorporate more information to enrich the hierarchical representation in the future."
P15-2102,1,the experimental results demonstrate the advantage of using 5-level pam and semantic enhancement against n-gram models and lda-like models.
P15-2104,1,"we also plan to investigate more nuanced methods for differentiating between global and local celebrity nodes, to be able to filter out global celebrity nodes but preserve local nodes that can have high geolocation utility."
P15-2104,1,"as future work, we plan to use temporal data and also look at improving the text-based geolocation model using sparse coding (cha et al., 2015)."
P15-2105,4,"in future work, we plan to use the keyword extraction to perform numerous nlp tasks on the twitter domain, such as document summarization."
P15-2106,1,we plan to study other ways to retrieve such a context like the conversation thread.
P15-2107,2,we expect the dataset to be helpful for studies on email overload problems.
P15-2107,2,we plan to increase the size of our dataset through amt.
P15-2107,2,"we believe features regarding such information (e.g.the recipients email history with the contact, the recipients personal preference in categorizing emails, etc.)should also be incorporated for importance prediction."
P15-2107,2,"meanwhile, we are aware that the current corpus lacks social and personal information."
P15-2108,4,"in future work, we plan to introduce this method to normalize the nonstandard language used in twitter, applying the methods to problems in search and other areas."
P15-2109,2,"in future, we will: 1) collect more ntas texts from various users; 2) do further work on how to fully leverage ntas to improve word segmentation; 3) call for dominant text editors to record ntas."
P15-2110,5,"in the future, we plan to further study this problem by focusing on omission detection, verb tense preference from the view of pragmatics, and jointly learning the local and global predictors."
P15-2110,5,"in addition, we will study predicting the tense of multiple predicates in a sentence and identifying imperative sentences in a conversation, which is also a challenge of tense prediction."
P15-2113,1,"in future work, we plan to (i) experiment with more sophisticated thread-level features, as well as with other features that model context in general; (ii) try data from other cqa websites, e.g., where dialogue between users is marked explicitly; and finally, (iii) integrate sequence, precedence, dependency information with global — thread-level— features in a unified framework."
P15-2117,5,"based on this work, more research can be conducted on topic recognition and semantic roles labeling for human-human conversations in real-world."
P15-2117,1,"in the future, we plan to explore the methods on training the unbalance data to improve the overall performances of our approach."
P15-2121,1,"since our new model involves simpler features, including unigram features defined over individual semantic unit – word pairs, we believe our new model would aid the joint modeling of both distributional and logical semantics (lewis and steedman, 2013) within a single framework."
P15-2121,6,We plan to explore this avenue in the future.
P15-2122,1,"for future work, we would like to employ more complicated features like the sentiment of the context, and dictionary features based on an npi lexicon."
P15-2122,1,"also, if available, prosodic information like focus, pauses, and intonation may be useful."
P15-2124,1,we are currently exploring a more robust incorporation of inter-sentential incongruity for sarcasm detection.
P15-2124,5,"our error analysis points to potential future work such as: (a) role of numbers for sarcasm, and (b) situations with subjective sentiment."
P15-2125,3,"in future work, we would like to explore the relation among emotions and caused languages for detecting the emotion and caused languages collectively."
P15-2126,4,"in the future, we plan to further explore this linear transformation based adaptation from different perspectives, e.g., sharing adaptation operations across users or review categories."
P15-2127,4,"in the future, we plan to further refine and employ it to other nlp applications."
P15-2127,1,"also, additional work can be done on combining statistical models into different components of pba."
P15-2128,1,"as future work, we plan to investigate the exact effect of the reordering constraints in terms of possible translation model phrase pairs and target language model n-grams which may not be used depending on the constraint parameters, in order to find the best configuration."
P15-2129,2,future work will focus on extending the va prediction from the word-level to the sentence- and document-levels.
P15-2130,1,"in our future work, we intend to focus on initializing good belief tracking models when no annotated dialogs are available for the new dialog domain."
P15-2139,2,"in future work, we would like to investigate joint training on the source and target languages."
P15-2141,2,the extensions include designing a new action to infer abstract concepts and training the parser with additional semantic role labeling and coreference based features.
P15-2142,4,"the same model can be applied as an efficient syntactic language model, and for future work it should be integrated into language generation tasks such as machine translation."
P15-2144,4,one interesting direction for further research would be to show the effect of this feature in other natural language processing tasks.
P15-3002,1,"more generally, we will explore the encoding of coreference constraints into probabilistic models that can be combined with smt systems, so that coreference constraints are considered in the decoding process."
P15-3002,2,"in the future, we will generalize this constraint to complex noun phrases which are not compounds."
P15-3004,3,"for future work, we will first conduct experiments on how well the dynamic-eager algorithm performs on different treebanks, including multi-head dependencies (such as the danish treebank (kromann, 2003))."
P15-3004,1,"secondly, we will conduct experiments on previously described static oracle parsing algorithms by using different classifiers such as support vector machines."
P15-3005,1,"apart from that, an information extraction approach that looks for more specific patterns should be verified."
P15-3005,4,"finally, we would like to adopt these findings to improve the prediction of epidemics."
P15-3005,1,"as future work, we would like to disambiguate functional expressions using sequence labeling techniques (utsuro , 2007); we would also like to identify the predicate–argument structure of disease events (kanouchi , 2015)."
P15-3007,3,"to verify this, we hope to submit our improved alignment results to a state-of-the-art amr parser, and evaluate the parsing results."
P15-3007,2,"therefore, increasing the training data size from the release is one solution to improve the performance of our aligner from the unsatisfactory results."
P15-3007,1,"in the future, we will be following these steps to develop the proposed rewrite-based parser: implementation of our rewrite-based amr parser: we would like to implement the proposed rewrite-based amr parser."
P15-3007,2,"using external lexical resources, like wordnet, is another promising solution to extend to snyonyms."
P15-3007,2,we also plan to experiment with the data generated by an automatic dependency parser.
P15-4001,6,"in the future, we attempt to integrate more feedback content such as video tutorial or articulation animation for teaching pronunciation."
P15-4003,1,"for future work, we plan to speed up the learning process (e.g.by saving feature vectors instead of re-calculating them), and also add the ability for users to configure the features used to train the classifier, e.g.incorporating lemmata or named entities instead of only using the parse tree."
P15-4007,2,"in future work, we plan to extend vex with functionality for visualizing additional error types, and for exploring entities not only in a single document, but across documents."
P15-4007,1,"given the structural similarities entities in coreference resolution and entities in entity linking share, we also will add methods for visualizing entities found by coreference resolution systems."
P15-4009,5,"in the future, we plan to investigate how to further represent and utilize these extracted concepts efficiently in more nlp tasks which call for deep language understanding."
P15-4014,6,"in the future, we would like to release the solver to allow researchers to contribute to the project and make the system even more competitive."
P15-4015,1,"in the future, we intend to incorporate in lexenstein approaches for complex word identification, as well as more approaches for the remaining tasks of the usual ls pipeline."
P15-4016,1,"we will focus in particular on modular extensions to the specification for supporting search, tagging, and bulk modifications."
P15-4016,1,"in future work, we will continue to develop the api specification further in collaboration with the relevant standardization efforts and interested parties using a fully open process."
P15-4016,1,"we will also continue to develop and extend the tools, with emphasis on reversible conversions between oa json-ld and major related formats."
P15-4017,2,"hadoop clusters), thus enabling the processing of large volumes of data."
P15-4017,5,we are now investigating how data annotation can run on multiple machines in a distributed environment (e.g.
P15-4018,1,"furthermore, we are working on incorporating more complex pre-processing for the holing operation in the visualization, e.g.aggregating context features over co-reference chains, as well as relation extraction and frame-semantic parsing for term–context representations."
P15-4023,2,future work could include enriching properties of a story using wikipedia info box and better summarizing events and stories.
P15-4024,2,"for example, corpora for different language levels, genres (e.g., emails, news) could be used to make the suggestions more relevant to users with diverse proficiency levels and interests."
P15-4024,1,"nlp, ir, and machine learning techniques could be used to provide more relevant ranking, to pin-point grammatical errors, or to generate finer-grained semantic patterns (e.g., assist someone in something or attend activity/institution) additionally, an interesting direction is identifying grammar patterns using a crf sequence labeller."
P15-4024,6,many avenues exist for future research and improvement of writeahead.
P15-4025,1,"in addition, we will develop a faster constituent parsers by using recurrent neural network."
P15-4025,1,"first of all, we will integrate a new subsystem which conducts dependency-based semantic role labeling."
P15-4025,1,"in our future work, we will add more function alities to niuparser."
