id,chapter,year,text
2020.acl-demos.1.txt,7 Conclusion,2020,another direction for improvement is to further enhance the ability to interact with users via a conversation interface.
2020.acl-demos.1.txt,7 Conclusion,2020,"one such important direction for future improvement is the expansion of areas that it can work in, which can be achieved through a promising approach of adopting model based technologies together with rule/template based ones."
2020.acl-demos.1.txt,7 Conclusion,2020,"so far, xiaomingbot has been deployed online and is serving users."
2020.acl-demos.10.txt,6 Conclusion,2020,"in particular, we plan to incorporate human performance as a reference metric, integrating psycholinguistic experimental results and supporting easy experimental design starting from the test suite format."
2020.acl-demos.10.txt,6 Conclusion,2020,"syntaxgym is continually evolving: we plan to add new features to the site, and to develop further in response to user feedback."
2020.acl-demos.10.txt,6 Conclusion,2020,"we also plan to further incorporate language models into the lm-zoo tool, allowing broader access to state-of-the-art language models in general."
2020.acl-demos.10.txt,6 Conclusion,2020,"we welcome open-source contributions to the website and to the general framework, and especially encourage the nlp community to contribute their models to the lm-zoo repository."
2020.acl-demos.14.txt,5 Conclusion and Future Work,2020,"for future work, we consider the following areas of improvement in the near term: • models downloadable in sta n z a are largely trained on a single dataset."
2020.acl-demos.14.txt,5 Conclusion and Future Work,2020,"simultaneously, sta n z a ’s corenlp client extends its functionality with additional nlp tools."
2020.acl-demos.14.txt,5 Conclusion and Future Work,2020,"to make models robust to many different genres of text, we would like to investigate the possibility of pooling various sources of compatible data to train “default” models for each language; • the amount of computation and resources available to us is limited."
2020.acl-demos.14.txt,5 Conclusion and Future Work,2020,"we would like to further investigate reducing model sizes and speeding up computation in the toolkit, while still maintaining the same level of accuracy.• we would also like to expand sta n z a ’s functionality by adding other processors such as neural coreference resolution or relation extraction for richer text analytics."
2020.acl-demos.16.txt,4 Conclusion,2020,"we will extend mt-dnn to support natural language generation tasks, e.g. question generation, and incorporate more pre-trained encoders, e.g. t5 (raffel et al., 2019) in future."
2020.acl-demos.17.txt,6 Conclusion and Future Work,2020,"an interesting direction to explore is re-ranking corrective suggestions, so that the suggestion more relevant to the original sentence goes to the top."
2020.acl-demos.17.txt,6 Conclusion and Future Work,2020,"for example, the method for introducing additional training data or generating artificial training data could be implemented to improve the performance."
2020.acl-demos.17.txt,6 Conclusion and Future Work,2020,many avenues exist for future research and improvement of our system.
2020.acl-demos.17.txt,6 Conclusion and Future Work,2020,yet another direction of research would be to detect fine-grained error types.
2020.acl-demos.18.txt,5 Conclusions,2020,"our case study on the wmt2019 metrics shared task further highlights the potential of clir as a proxy task for mt evaluation, and we hope that clireval can facilitate future research in this area."
2020.acl-demos.18.txt,5 Conclusions,2020,"the aim of this project is not to replace current automatic evaluation metrics or fix the limitations in those metrics, but to bridge the gap between machine translation and cross-lingual information retrieval and to show that clir is a feasible proxy task for mt evaluation."
2020.acl-demos.19.txt,5 Conclusion,2020,"based on convlab (lee et al., 2019b), convlab-2 integrates more powerful models, supports more datasets, and develops an analysis tool and an interactive tool for comprehensive end-to-end evaluation."
2020.acl-demos.19.txt,5 Conclusion,2020,we hope that convlab-2 is instrumental in promoting the research on task-oriented dialogue.
2020.acl-demos.2.txt,6 Conclusion and Future Work,2020,"for example, its usability in generation tasks such as machine translation has not been tested."
2020.acl-demos.2.txt,6 Conclusion and Future Work,2020,"in the future, we aim to integrate neural architecture search into the toolkit to automate the searching for model structures."
2020.acl-demos.2.txt,6 Conclusion and Future Work,2020,we will keep adding more examples and tests to expand textbrewer’s scope of application.
2020.acl-demos.20.txt,4 Conclusions and Future Work,2020,"additionally, we would like to explore opusfilter’s use in different scenarios and for other language pairs."
2020.acl-demos.20.txt,4 Conclusions and Future Work,2020,especially interesting would be the application in low-resource settings and various levels of noise in the original data.
2020.acl-demos.20.txt,4 Conclusions and Future Work,2020,"furthermore, the use for domain adaptation and data selection should be further explored."
2020.acl-demos.20.txt,4 Conclusions and Future Work,2020,"in future work, we would like to extend the toolbox with additional filters and classification options."
2020.acl-demos.22.txt,6 Discussion,2020,"however, exbert can effectively narrow the scope and refine hypotheses through quick testing iterations."
2020.acl-demos.24.txt,6 Conclusion and Future Work,2020,"we also plan to improve the performance of core models in photon, such as semantic parsing (text-to-sql), response generation (table-to-text) and context-aware user interaction (text-to-text)."
2020.acl-demos.24.txt,6 Conclusion and Future Work,2020,"we will continue to add more features to photon, such as voice input, spelling checking, and visualizing the output when appropriate to inspect the translation process."
2020.acl-demos.25.txt,5 Discussion and Future Work,2020,5.1 using guis for language grounding sugilite illustrates the great promise of using guis as a resource for grounding and understanding natural language instructions in itl.
2020.acl-demos.25.txt,5 Discussion and Future Work,2020,"a promising direction is to use gui references to help with repairing conversational breakdowns (beneteau et al., 2019; ashktorab et al., 2019; myers et al., 2018) caused by incorrect semantic parsing, intent classification, or entity recognition."
2020.acl-demos.25.txt,5 Discussion and Future Work,2020,"another promising approach to enable more robust natural language understanding is to leverage the pre-trained generalpurpose language models (e.g., bert (devlin et al., 2018)) to encode the user instructions and the information extracted from app guis.5.3 extracting task semantics from guis an interesting future direction is to better extract semantics from app guis so that the user can focus on high-level task specifications and personal preferences without dealing with low-level mundane details (e.g., “buy 2 burgers” means setting the value of the textbox below the text “quantity” and next to the text “burger” to be “2”)."
2020.acl-demos.25.txt,5 Discussion and Future Work,2020,"collecting and aggregating personal task instructions across many users also introduce important concerns on user privacy, as discussed in (li et al., 2020).5.4 multi-modal interactions in conversational learning sugilite combines speech and direct manipulation to enable a “speak and point” interaction style, which has been studied since early interactive systems like put-that-there (bolt, 1980)."
2020.acl-demos.25.txt,5 Discussion and Future Work,2020,"for itl, an interesting future challenge is to combine these user-independent domain-agnostic machine-learned models with the user’s personalized instructions for a specific task."
2020.acl-demos.25.txt,5 Discussion and Future Work,2020,"this could be supported by improved background knowledge and task models, and more flexible dialog frameworks that can handle the continuous refinement and uncertainty inherent in natural language interaction, and the variations in user goals."
2020.acl-demos.25.txt,5 Discussion and Future Work,2020,"this will likely require a new kind of mixedinitiative instruction (horvitz, 1999) where the agent is more proactive in guiding the user and takes more initiative in the dialog."
2020.acl-demos.25.txt,5 Discussion and Future Work,2020,we are currently exploring other ways of using multi-modal interactions to supplement natural language instructions in itl.
2020.acl-demos.25.txt,5 Discussion and Future Work,2020,we are looking at alternative approaches for parsing natural language instructions into our domainspecific language (dsl) for representing data description queries and task execution procedures.
2020.acl-demos.26.txt,7 Conclusion,2020,"for future work, we plan to keep adding the state-of-the-art algorithms, reduce latency and fine-tune the implemented models on larger and/or more comprehensive corpus to improve performance."
2020.acl-demos.26.txt,7 Conclusion,2020,"we unified these nlp algorithms in a single codebase, implemented demos as top-level managers to access different models, and provide strategies to allow more organic integration across the models."
2020.acl-demos.28.txt,6 Conclusion and Future Work,2020,"in future, we would like to extend the funlines data collection setup to a more general crowdsourcing framework, for example, to collect style transfer data."
2020.acl-demos.29.txt,6 Discussion,2020,interactive fiction is an art form with an uncertain future that is connected in no small way to its proximity to games and the social norms separating games and fine art.
2020.acl-demos.3.txt,9 Conclusions,2020,"we intend to release the code as open source, as well as providing hosted open access to a pubmed-based corpus."
2020.acl-demos.30.txt,7 Conclusion,2020,detection and control of toxic output will be a major focus of future investigation.
2020.acl-demos.30.txt,7 Conclusion,2020,"dialogpt is fully opensourced and easy to deploy, allowing users to extend the pre-trained conversational system to bootstrap training using various datasets."
2020.acl-demos.30.txt,7 Conclusion,2020,we will investigate leveraging reinforcement learning to further improve the relevance of the generated responses and prevent the model from generating egregious responses.
2020.acl-demos.31.txt,7 Conclusions,2020,"we provide a large variety of functionalities, ranging from speech processing to core dialog system capabilities and social signal processing."
2020.acl-demos.32.txt,5 Conclusion and Future Work,2020,"in future work, we plan to add more media sources, especially from non-english media and regions."
2020.acl-demos.32.txt,5 Conclusion and Future Work,2020,"we further want to extend the tool to support other propaganda techniques such as cherrypicking and omission, among others, which would require analysis beyond the text of a single article."
2020.acl-demos.32.txt,5 Conclusion and Future Work,2020,"we have made publicly available our data and models, as well as an api to the live system."
2020.acl-demos.33.txt,5 Conclusion,2020,"the dilated cnn, which is first applied to the icd coding task, aims to capture semantic information for non-continuous words, and the n-gram matching mechanism aims to capture the continuous semantic."
2020.acl-demos.33.txt,5 Conclusion,2020,we will try to utilize external resources to solve the few-shot and zero-shot problem in the future.
2020.acl-demos.34.txt,5 Conclusion,2020,"in the future, we will support more corpora and implement novel techniques to bridge the gap between end-to-end and cascaded approaches."
2020.acl-demos.35.txt,6 Conclusion,2020,"existing work on amr has targeted the penman string, the parsed tree, or the interpreted graph, and penman accommodates all of these use cases by allowing users to work with the tree or graph data structures or to encode them back to strings."
2020.acl-demos.35.txt,6 Conclusion,2020,"transformations defined at both the graph and tree level make it applicable for pre- and postprocessing steps for corpus creation, evaluation, machine learning projects, and more."
2020.acl-demos.36.txt,5 Conclusion,2020,"finally, we are aware that keyword-based boolean filtering might be prone to the same biases and challenges inherent in the traditional search queries, as discussed above."
2020.acl-demos.36.txt,5 Conclusion,2020,"in future work, we will focus on expanding the database to include additional domains and article sources."
2020.acl-demos.36.txt,5 Conclusion,2020,"we will also seek to improve discovery performance by testing more recent text embedding methods (e.g., bert (devlin et al., 2018)) and by optimizing the search for different input text lengths, such as a whole document, a paragraph, or even a single sentence."
2020.acl-demos.36.txt,5 Conclusion,2020,"we will investigate whether query expansion techniques (azad and deepak, 2019) could mitigate this issue by suggesting or automatically appending semantically related keywords to the boolean filters."
2020.acl-demos.36.txt,5 Conclusion,2020,"we will work on augmenting the workflow with automated tasks, such as suggesting references as the author writes a manuscript, or notifying users about the latest publications relevant to their work."
2020.acl-demos.37.txt,4 Conclusion,2020,"as a next step, we will improve the prototype based on the participants’ valuable feedback."
2020.acl-demos.37.txt,4 Conclusion,2020,"furthermore, an eye tracker will be integrated into the prototype that can be used in combination with speech for cursor placement, thereby simplifying multi-modal pe."
2020.acl-demos.37.txt,4 Conclusion,2020,"last, we will investigate whether using the different modalities has an impact on cognitive load during pe (herbig et al., 2019b)."
2020.acl-demos.37.txt,4 Conclusion,2020,our study with professional translators shows a high level of interest and enthusiasm about using these new modalities.
2020.acl-demos.37.txt,4 Conclusion,2020,"users can directly cross out or hand-write new text, drag and drop words for reordering, or use spoken commands to update the text in place."
2020.acl-demos.38.txt,7 Conclusion and Future Work,2020,"finally, we hope to explore further optimizations to make core algorithms competitive with highly-optimized neural network components."
2020.acl-demos.38.txt,7 Conclusion and Future Work,2020,"in addition to the problems discussed so far, torch-struct also includes several other example implementations including supervised dependency parsing with bert, unsupervised tagging, structured attention, and connectionist temporal classification (ctc) for speech."
2020.acl-demos.38.txt,7 Conclusion and Future Work,2020,"in the future, we hope to support research and production applications employing structured models."
2020.acl-demos.39.txt,5 Conclusion,2020,"using the cl machine teaching ui, the dialog author can provide corrections to the logged user-system dialogs and further improve the cl’s dm performance."
2020.acl-demos.39.txt,5 Conclusion,2020,"we are planning to extend this work by looking into following problems: 1) investigating effectiveness of different ranking algorithms for log correction recommendation, 2) optimizing number of training samples and action masks generated from the rule-based dm, and 3) improving predictions of hcn-based dm by looking into alternative network architectures."
2020.acl-demos.4.txt,4 Conclusion,2020,"as suggested by an anonymous reviewer, another possible addition to the game could then be to predict the age appropriateness of a given topic, allowing for cards to be filtered on the basis of an age setting."
2020.acl-demos.4.txt,4 Conclusion,2020,"in addition to improving the banned words selection process, future work on tabouid includes generating specific lists of cards based on school programs to use the game as an educational tool, using the category system of wikipedia to let users select more or less specific categories to play with, and adapting the algorithms to leverage the wide variety of languages wikipedia is available in beyond english and french."
2020.acl-demos.40.txt,6 Conclusion,2020,"there are many open questions which we intend to research, such as whether autoregressivity in neural sentence compression can be exploited and how to compose themes over longer time periods."
2020.acl-demos.41.txt,Conclusion,2020,we hope to encourage additional research to improve the safety and benefits of dietary supplements for their consumers.
2020.acl-demos.42.txt,7 Conclusion,2020,we hope that our work on lean-life will allow for researches and practitioners alike to more easily obtain useful labeled datasets and models for the various nlp tasks they face.
2020.acl-demos.5.txt,6 Conclusion,2020,future work include (1) expanding the database to more papers (2) improving the qa model using the collected data to better handle question answering in the context of research domain.
2020.acl-demos.8.txt,6 Conclusion,2020,the retrieved evidence sentences can be easily located in the background corpora for better visualization.
2020.acl-demos.8.txt,6 Conclusion,2020,we are further developing evidenceminer to be a more intelligent system that can assist in more efficient and in-depth scientific discoveries.
2020.acl-demos.9.txt,5 Conclusions,2020,"moving forward, we hope to refine the linking of extracted snippets to structured vocabularies to run a more comprehensive user-study to evaluate the use of the system in practice by different types of users."
2020.acl-demos.9.txt,5 Conclusions,2020,"we also hope to develop a joint extraction and inference model, rather than relying on the current pipelined approach."
2020.acl-main.1.txt,7 Discussion,2020,"in future work, we intend to curate a test set with data from separate sources, which can serve as a benchmark for the models we study."
2020.acl-main.1.txt,7 Discussion,2020,"the combined test set scores are more directly comparable, but ideally, we would like to compare the generalization of both models on an independent test set."
2020.acl-main.1.txt,7 Discussion,2020,we also intend to use tools for interpreting the knowledge encoded in neural networks (such as diagnostic classifiers and representational similarity analysis) to investigate the emergent representation of linguistic units such as phonemes and words.
2020.acl-main.1.txt,7 Discussion,2020,"we find indications that learning to extract meaning from speech is initially faster when learning from child-directed speech, but learning from adultdirected speech eventually leads to similar task performance on the training register, and better generalization to the other register."
2020.acl-main.1.txt,7 Discussion,2020,we intend to explore how a curriculum of cds followed by ads affects learning trajectories and outcomes.
2020.acl-main.100.txt,5 Conclusion,2020,"in the future, we are interested in injecting knowledge into text representation learning (cao et al., 2017, 2018b) for deeply understanding expert language, and will help to generate knowledgeenhanced questions (pan et al., 2019) for laymen."
2020.acl-main.102.txt,5 Conclusion,2020,"since dynamic memory can be a learning mechanism more general than what we have used here for fewshot learning, we will investigate this type of models in other learning problems."
2020.acl-main.103.txt,7 Conclusion and Future Work,2020,one interesting future direction is to explore whether the beam search is helpful to our model.
2020.acl-main.104.txt,6 Conclusion,2020,"furthermore, our framework is extended into a parallel variant based on multi-label attention and a serial variant of text feature propagation."
2020.acl-main.105.txt,4 Conclusion,2020,other retrieval tasks may also benefit from using keyphrase information and we expect our results to serve as a basis for further improvements.
2020.acl-main.105.txt,4 Conclusion,2020,we presented the first study of the usefulness of keyphrase generation for scientific document retrieval.
2020.acl-main.107.txt,10 Conclusion,2020,"annotated with 4 standard morphosyntactic layers, two of them following the universal dependency annotation scheme, and provided with translation to french as well as glosses and word language identification, we believe that this corpus will be useful for the community at large, both for linguistic purposes and as training data for resource-scarce nlp in a high-variability scenario."
2020.acl-main.107.txt,10 Conclusion,2020,"in addition to the annotated data, we provide around 1 million tokens (over 46k sentences) of unlabeled narabizi content, resulting in the largest dataset available for this dialect."
2020.acl-main.107.txt,10 Conclusion,2020,"more over, being made of user-generated content, this treebank covers a large variety of language variation among native speakers and displays a high level of codeswitching."
2020.acl-main.108.txt,6 Summary,2020,"we gave an overview of the topics discussed in the corpus, demonstrating that it is a valuable source for several nlp tasks, such as argument mining."
2020.acl-main.109.txt,9 Discussion,2020,"indeed, since function words in ud tend to be dependents of content words, we may analyze the former by considering the distribution of function word types that each type of content word has."
2020.acl-main.109.txt,9 Discussion,2020,"moreover, sub-typing dependency paths based on their linear direction can allow investigating word-order differences.11 other than informing the development of cross-lingual transfer learning, our analysis directly supports the validation of ud annotation."
2020.acl-main.109.txt,9 Discussion,2020,our method can be used to detect and bridge such inconsistencies.
2020.acl-main.11.txt,5 Conclusion and Future Work,2020,"in future work, we plan to experiment with multi-domain span extraction architectures."
2020.acl-main.11.txt,5 Conclusion and Future Work,2020,"we have also introduced restaurants8k, a new challenging data set that will hopefully encourage further work on span extraction for dialogue."
2020.acl-main.11.txt,5 Conclusion and Future Work,2020,"we have shown that, due to pretrained representations, span-convert is especially useful in few-shot learning setups on small data sets."
2020.acl-main.110.txt,9 Conclusions,2020,"as a final remark, we believe that the proposed framework can be useful for other nlg tasks such as paraphrase generation or text simplification."
2020.acl-main.110.txt,9 Conclusions,2020,"in this scenario, automation strategies, such as natural language generation, are necessary to help ngo operators in their countering effort."
2020.acl-main.111.txt,6 Conclusion,2020,"its goal is to drive the development of better nlu models, so careful selection of tasks was crucial."
2020.acl-main.111.txt,6 Conclusion,2020,we leave it as future work.
2020.acl-main.111.txt,6 Conclusion,2020,we plan to continue the work on herbert and use the klej benchmark to guide its development.
2020.acl-main.112.txt,7 Conclusion,2020,"a promising direction would be to combine a multilingual sense inventory such as babelnet (navigli and ponzetto, 2012) with sense embeddings (camacho-collados and pilehvar, 2018)."
2020.acl-main.112.txt,7 Conclusion,2020,"emotion lexicons are at the core of sentiment analysis, a rapidly flourishing field of nlp."
2020.acl-main.112.txt,7 Conclusion,2020,future work will have to deepen the way we deal with word sense ambiguity by way of exchanging the simplifying type-level approach our current work is based on with a semantically more informed sense-level approach.
2020.acl-main.112.txt,7 Conclusion,2020,"while there are techniques to tackle these three forms of sparsity in isolation, we introduced a methodology which allows us to cope with them simultaneously by jointly combining emotion representation mapping, machine translation, and embedding-based lexicon expansion."
2020.acl-main.113.txt,7 Conclusions,2020,"finally, we would like to test this approach for comparing different mt systems."
2020.acl-main.113.txt,7 Conclusions,2020,"first, we plan to test whether similar observations will hold for more language pairs and text domains."
2020.acl-main.113.txt,7 Conclusions,2020,"second, the score combination strategies could be improved by learning weights for each component."
2020.acl-main.113.txt,7 Conclusions,2020,this work can be extended in numerous ways.
2020.acl-main.115.txt,8 Conclusion and Future Work,2020,the field of nlp has developed deep neural models that can exploit large amounts of data to achieve high scores on downstream tasks.
2020.acl-main.116.txt,8 Conclusion,2020,a natural next step is to combine the datasets in a multi-task setting to investigate to what extent models can profit from combining the information annotated in the respective datasets.
2020.acl-main.116.txt,8 Conclusion,2020,"further research will investigate the joint modeling of entity extraction, typing and experiment frame recognition."
2020.acl-main.116.txt,8 Conclusion,2020,"in addition, there are also further natural language processing tasks that can be researched using our dataset."
2020.acl-main.116.txt,8 Conclusion,2020,"in section 7.1, we have shown that our findings generalize well by applying model architectures developed on our corpus to another dataset."
2020.acl-main.116.txt,8 Conclusion,2020,"they include the detection of events and sub-events when regarding the experiment-descriptions as events, and a more linguistically motivated evaluation of the framesemantic approach to experiment descriptions in text, e.g., moving away from the one-experimentper-sentence and one-sentence-per-experiment assumptions and modeling the graph-based structures as annotated."
2020.acl-main.117.txt,7 Discussion and Future Work,2020,"the overall size of the released data (600 training questions) is in line with real-world scenarios, where the high cost of domain expert time limits the amount of quality data that can reasonably be collected."
2020.acl-main.117.txt,7 Discussion and Future Work,2020,we consider techqa to be a stepping stone on which to build future data collections and leaderboards.
2020.acl-main.117.txt,7 Discussion and Future Work,2020,"we envision a roadmap where future releases of techqa will require synergy between multiple ai disciplines, from deep-learning based mrc to reasoning, knowledge base acquisition, and causality detection."
2020.acl-main.117.txt,7 Discussion and Future Work,2020,we plan on releasing questions with answers in a broader and more diverse collection that will include documents with a less formulaic structure than the technotes.
2020.acl-main.117.txt,7 Discussion and Future Work,2020,"we will also relax the length limitations to include questions rich in details, and answers that include complex procedures; in the same spirit, we will allow answers consisting of multiple spans from a single document."
2020.acl-main.118.txt,7 Conclusion and Future Work,2020,"we aim to promote research in sarcasm detection, and to encourage future investigations into sarcasm in general and how it is perceived across cultures."
2020.acl-main.118.txt,7 Conclusion and Future Work,2020,we believe this dataset will allow future work in sarcasm detection to progress in a setting free of the noise found in existing datasets.
2020.acl-main.119.txt,7 Conclusion,2020,an interesting future work is to make the number of inference steps adaptive to input sentences.
2020.acl-main.120.txt,6 Conclusion,2020,important challenges for future work include how to scale deep learning methods to such large amounts of source documents and how to close the gap to the oracle methods.
2020.acl-main.120.txt,6 Conclusion,2020,"we conducted extensive experiments to establish baseline results, and we hope that future work on mds will use this dataset as a benchmark."
2020.acl-main.120.txt,6 Conclusion,2020,we hope this dataset will facilitate the creation of real-world mds systems for use cases such as summarizing news clusters or search results.
2020.acl-main.121.txt,6 Conclusion and Future Work,2020,"besides, we will also explore the influence of probabilistic bilingual lexicon obtained by learning only from monolingual data on our method."
2020.acl-main.121.txt,6 Conclusion and Future Work,2020,"in our future work, we consider incorporating our method into the multi-task method."
2020.acl-main.121.txt,6 Conclusion and Future Work,2020,"this method first attends to the source words, then obtains the translation candidates, and incorporates them into the generation of the final summary."
2020.acl-main.122.txt,7 Conclusion,2020,"potential future directions include a more principled use of our proposed heuristic for detecting content relevant to specific dates, the use of abstractive techniques, a more effective treatment of the redundancy challenge, and extending the new dataset with multiple sources."
2020.acl-main.123.txt,7 Conclusion and future work,2020,"in the future, we explore a more sophisticated method to improve the relevance and truthfulness of generated headlines, for example, removing only deviated spans in untruthful headlines rather than removing untruthful headlines entirely from the supervision data."
2020.acl-main.123.txt,7 Conclusion and future work,2020,"moreover, it will be also interesting to see whether the same issue occurs in other related tasks such as data-to-text generation."
2020.acl-main.123.txt,7 Conclusion and future work,2020,other directions include an extensive evaluation of relevance and truthfulness of abstractive summarization and an establishment of an automatic evaluation metric for truthfulness.
2020.acl-main.124.txt,7 Conclusion,2020,"we explored unsupervised multi-document summary evaluation methods, which require neither reference summaries nor human annotations."
2020.acl-main.125.txt,6 Conclusion,2020,"for future work, we intend to apply our method to other transformer-based summarization models."
2020.acl-main.126.txt,7 Conclusion,2020,"the ultimate chatbot evaluation metric should be user-centric, as chatbots are there to provide human with an enjoyable experiences."
2020.acl-main.127.txt,5 Conclusions,2020,"in the future, we will adapt the method to more neural models especially the generation-based methods for the dialog system."
2020.acl-main.127.txt,5 Conclusions,2020,we use the word alignment model from machine translation to calculate the cross-sentence co-occurrence and train the embedding on word and sentence level.
2020.acl-main.129.txt,7 Conclusions and Future Work,2020,"in future, we want to continue to investigate the possibility of using even weaker demonstrations."
2020.acl-main.13.txt,4 Conclusion,2020,we will explore cross-lingual transfer learning for supporting more languages.
2020.acl-main.130.txt,5 Conclusion,2020,there is a large gap between the model performance and human performance.
2020.acl-main.130.txt,5 Conclusion,2020,we hope that this dataset facilitates future research on multi-turn conversation reasoning problem.
2020.acl-main.131.txt,7 Conclusion & Future Work,2020,"for future work, we would like to extend receiver to conversational recommender systems."
2020.acl-main.132.txt,7 Conclusions,2020,"finally, we will release our experimental qa datasets (in the squad json format) for bridging anaphora resolution on isnotes and bashi."
2020.acl-main.133.txt,5 Conclusions,2020,"for future work, we would like to deeply study the impacts of our perturbations on the coherence of the examined dialogues."
2020.acl-main.133.txt,5 Conclusions,2020,we will also investigate to what extent the rankings of dialogues obtained by our model correlate with human-provided rankings.
2020.acl-main.135.txt,5 Conclusion and Future Works,2020,"although dp-based and srl-based semantic parsing are widely used, more advanced semantic representations could also be explored, such as discourse structure representation (van noord et al., 2018; liu et al., 2019b) and knowledge graph-enhanced text representations (cao et al., 2017; yang et al., 2019)."
2020.acl-main.135.txt,5 Conclusion and Future Works,2020,there are at least two potential future directions.
2020.acl-main.137.txt,6 Conclusions,2020,the large number of errors that we find in oie extractions from scientific texts render them near-useless to downstream computing tasks.
2020.acl-main.137.txt,6 Conclusions,2020,"the presented approach is currently limited to the domain of biology and the use of trade-off relations, but we expect that central relations can be identified for other scientific domains that enable sore."
2020.acl-main.138.txt,6 Conclusions,2020,future work will involve improvements in the proposed noise model to study the importance of fidelity to real-world error patterns.
2020.acl-main.138.txt,6 Conclusions,2020,it may support an automatic correction method that uses recognized entity types to narrow the list of feasible correction candidates.
2020.acl-main.138.txt,6 Conclusions,2020,"moreover, we plan to evaluate nat on other real noise distributions (e.g., from asr) and other sequence labeling tasks to support our claims further."
2020.acl-main.139.txt,5 Conclusion,2020,"future work will investigate how to take into account potential correlations between labelling functions in the aggregation model, as done in e.g.(bach et al., 2017)."
2020.acl-main.139.txt,5 Conclusion,2020,this paper presented a weak supervision model for sequence labelling tasks such as named entity recognition.
2020.acl-main.139.txt,5 Conclusion,2020,we also wish to evaluate the approach on other types of sequence labelling tasks beyond named entity recognition.
2020.acl-main.14.txt,5 Conclusion,2020,"we interpret the discourse relations as translation in low-dimensional embedding space, which reflects the geometric structure of argument-relation, and also can obtain the richer argument representations based on the multi-level encoder."
2020.acl-main.140.txt,6 Conclusion,2020,"in future work, we want to extend the probing tasks to also cover specific linguistic patterns such as appositions, and also investigate a model’s ability of generalizing to specific entity types, e.g.company and person names."
2020.acl-main.140.txt,6 Conclusion,2020,"we conducted a comprehensive evaluation of common re encoder architectures, and studied the effect of explicitly and implicitly provided semantic and syntactic knowledge, uncovering interesting properties about the architecture and input features."
2020.acl-main.141.txt,5 Conclusion,2020,one possible direction is to extend the scope of structure induction for constructions of nodes without relying on an external parser.
2020.acl-main.141.txt,5 Conclusion,2020,there are multiple avenues for future work.
2020.acl-main.142.txt,7 Conclusion and Future Work,2020,"to improve the evaluation accuracy and reliability of future re methods, we provide a revised, extensively relabeled tacred."
2020.acl-main.144.txt,6 Conclusions and Further Work,2020,"as regards distributed representations, we plan to study alternative networks to more accurately model the identification and incorporation of additional context."
2020.acl-main.144.txt,6 Conclusions and Further Work,2020,"in our future work, we plan to optimise the thresholds used with the retrieval algorithms in order to more intelligently select those translations providing richest information to the nmt model and generalize the use of edit distance on the target side."
2020.acl-main.144.txt,6 Conclusions and Further Work,2020,"we would also like to explore better techniques to inject information of small-size n-grams with possible convergence with terminology injection techniques, unifying framework where target clues are mixed with source sentence during translation."
2020.acl-main.145.txt,6 Conclusion,2020,"in future work, we will extend our analysis to include additional source and target languages from different language families, such as more asian languages."
2020.acl-main.145.txt,6 Conclusion,2020,"we will also work towards improving the training efficiency of character-level models, which is one of their main bottlenecks, as well as towards improving their effectiveness in multilingual training."
2020.acl-main.146.txt,7 Conclusion,2020,"as the resulting model repurposes a pre-trained translation model without changing its parameters, it can directly benefit from improvements in translation quality, e.g.by adaptation via fine-tuning."
2020.acl-main.148.txt,7 Conclusion and Future Work,2020,"in the future, we will develop lightweight alternatives to lalt to reduce the number of model parameters."
2020.acl-main.148.txt,7 Conclusion and Future Work,2020,"we release opus-100, a multilingual dataset from opus including 100 languages with around 55m sentence pairs for future study."
2020.acl-main.148.txt,7 Conclusion and Future Work,2020,"we will also exploit novel strategies to break the upper bound of robt and obtain larger zero-shot improvements, such as generative modeling (zhang et al., 2016; su et al., 2018; garcía et al., 2020; zheng et al., 2020)."
2020.acl-main.149.txt,6 Conclusion,2020,"in future work, we plan to extend this analysis across more translation pairs, more diverse languages and multiple domains, as well as investigating the effect of translationese or source-side grammatical errors (anastasopoulos, 2019)."
2020.acl-main.151.txt,6 Conclusion,2020,"in this work, we investigate a range of reference-free metrics based on cutting-edge models for inducing cross-lingual semantic representations: cross-lingual (contextualized) word embeddings and cross-lingual sentence embeddings."
2020.acl-main.153.txt,6 Conclusions and Future Work,2020,"in the future, we plan to extend the cross-lingual position encoding to non-autoregressive mt (gu et al., 2018) and unsupervised nmt (lample et al., 2018)."
2020.acl-main.155.txt,6 Conclusion,2020,"an exploration of multi-modal measuring approaches (herbig et al., 2019b) shows the feasibility of this, so we will try to combine explicit multi-modal input, as done in this work, with implicit multi-modal sensor input to better model and support the user during pe."
2020.acl-main.155.txt,6 Conclusion,2020,"as a next step, we will integrate the participants’ valuable feedback to improve the prototype."
2020.acl-main.155.txt,6 Conclusion,2020,"while more and more professional translators are switching to the use of pe to increase productivity and reduce errors, current cat interfaces still heavily focus on traditional mouse and keyboard input, even though the literature suggests that other modalities could support pe operations well."
2020.acl-main.155.txt,6 Conclusion,2020,"while the presented study provided interesting first insights regarding participants’ use of and preferences for the implemented modalities, it did not allow us to see how they would use the modalities over a longer time period in day-to-day work, which we also want to investigate in the future."
2020.acl-main.157.txt,4 Conclusions,2020,"also, the multi-domain nature of the dataset enables future research in cross-target and cross-domain adaptation, a clear weak point of current models according to our evaluations."
2020.acl-main.157.txt,4 Conclusions,2020,"future research directions might explore the usage of transformer-based models, as well as of models which exploit not only linguistic but also network features, which have been proven to work well for existing stance detection datasets (aldayel and magdy, 2019)."
2020.acl-main.158.txt,5 Discussion,2020,it remains to be seen to just what extent contemporary artificial systems do the same.
2020.acl-main.158.txt,5 Discussion,2020,"this understanding will be further advanced by new targeted-evaluation test suites covering a still wider variety of syntactic phenomena, additional trained models with more varied hyperparameters and randomization seeds, and new architectural innovations."
2020.acl-main.158.txt,5 Discussion,2020,this work addresses multiple open questions about syntactic evaluations and their relationship to other language model assessments.
2020.acl-main.159.txt,5 Conclusions,2020,"nonetheless, the german plural system continues to challenge ed architectures."
2020.acl-main.160.txt,5 Discussion,2020,one possibility would be to include infelicitous “colorless green ideas” sentences with grammatical syntax (cf.
2020.acl-main.160.txt,5 Discussion,2020,"the possibility of side-channel information is already known in relation to these higher-level tasks (e.g., poliak et al., 2018; geva et al., 2019), and various challenge data sets have been constructed to mitigate it in different ways (levesque et al., 2011; chao et al., 2017; dua et al., 2019; lin et al., 2019; dasigi et al., 2019)."
2020.acl-main.160.txt,5 Discussion,2020,"the two have similar reasoning: from an engineering perspective, a family of models that is capable of inducing syntax is useful because it may be expected to improve performance on downstream tasks."
2020.acl-main.160.txt,5 Discussion,2020,"uniting these with a collection of hard sentence types (e.g., marvin and linzen, 2018; warstadt et al., 2019) in something like a syntax-focused qa challenge set would provide new insights into which families of models capture the practical benefits of true hierarchical syntactic representation."
2020.acl-main.161.txt,7 Conclusions,2020,a more sophisticated model would incorporate similar ideas.
2020.acl-main.161.txt,7 Conclusions,2020,an analogue of this would be adversarial examples used in computer vision.
2020.acl-main.161.txt,7 Conclusions,2020,it provides a springboard to further interesting applications and research on suspense in storytelling.
2020.acl-main.161.txt,7 Conclusions,2020,"our overall findings suggest that by implementing concepts from psycholinguistic and economic theory, we can predict human judgements of suspense in storytelling."
2020.acl-main.161.txt,7 Conclusions,2020,"strong psycholinguistic claims about suspense are difficult to make due to several weaknesses in our approach, which highlight directions for future research: the proposed model does not have a higher-level understanding of event structure; most likely it picks up the textual cues that accompany dramatic changes in the text."
2020.acl-main.162.txt,6 Conclusion,2020,we hope this resource can benefit future research into developing techniques to model and understand human responses to document sized text.
2020.acl-main.163.txt,6 Conclusion,2020,"in future, we aim to refine our generative model to better emphasise this difference of the two tasks."
2020.acl-main.163.txt,6 Conclusion,2020,the proposed model is also suitable for other translation tasks between two modalities.
2020.acl-main.164.txt,8 Conclusion,2020,"finally, we would like to note that all of our experiments were performed with english language models, and it remains an open question how the trade-off between ease of human detection and ease of automatic detection might differ for languages that are very different from english."
2020.acl-main.164.txt,8 Conclusion,2020,identifying ways to improve the language models and decoding strategies we use in order to generate text that is both exciting (ie.unlikely) and semantically plausible.2.
2020.acl-main.164.txt,8 Conclusion,2020,this variance in likelihood is what makes human-written text interesting and exciting to read.
2020.acl-main.164.txt,8 Conclusion,2020,we therefore suggest three prongs for future research: 1.
2020.acl-main.165.txt,6 Conclusions,2020,"moreover, we remark that our approach can be extended to other multi-domain or multi-task nlp problems."
2020.acl-main.165.txt,6 Conclusions,2020,we also show mixing method can be combined with embedding based methods to make further improvement.
2020.acl-main.166.txt,5 Conclusion,2020,"in the future, we will investigate how to extend the cg to support hierarchical topic management in conversational systems."
2020.acl-main.167.txt,5 Conclusions,2020,future work will focus on incorporating better encoding of the amr graph into the current system and exploring data augmentation techniques leveraging the proposed approach.
2020.acl-main.170.txt,8 Conclusions,2020,future research directions include adaptive dropout rates for different merges and an in-depth analysis of other pathologies in learned token embeddings for different segmentations.
2020.acl-main.171.txt,6 Discussion,2020,can the nar model perfectly recover the ar model’s performance with much larger monolingual datasets?
2020.acl-main.171.txt,6 Discussion,2020,"other work in nmt has examined this issue in the context of backtranslation (e.g., edunov et al.(2018)), and we expect the conclusions to be similar in the nar-mt case."
2020.acl-main.171.txt,6 Discussion,2020,there are several open questions to investigate: are the benefits of monolingual data orthogonal to other techniques like iterative refinement?
2020.acl-main.171.txt,6 Discussion,2020,we will consider these research directions in future work.
2020.acl-main.174.txt,6 Conclusions,2020,"although currently our approach relies solely on textual information, it would be interesting to incorporate additional modalities such as video or audio."
2020.acl-main.174.txt,6 Conclusions,2020,"besides narrative structure, we would also like to examine the role of emotional arcs (vonnegut, 1981; reagan et al., 2016) in a screenplay."
2020.acl-main.175.txt,6 Conclusions,2020,"in the future, we would like to model aspects and sentiment more explicitly as well as apply some of the techniques presented here to unsupervised single-document summarization."
2020.acl-main.177.txt,9 Discussion and Conclusion,2020,"first, two of our probes (known word perturbation and consistency) are based on the idea of starting from a test item that is classified correctly, and applying a transformation that should result in a classifiable item (for a model that represents word meaning systematically)."
2020.acl-main.177.txt,9 Discussion and Conclusion,2020,our analyses contain two ideas that may be useful for future studies of systematicity.
2020.acl-main.177.txt,9 Discussion and Conclusion,2020,"second, our analyses made critical use of differential sensitivity (i.e., variance) of the models across test blocks with different novel words but otherwise identical information content."
2020.acl-main.177.txt,9 Discussion and Conclusion,2020,we believe these are a novel ideas that can be employed in future studies.
2020.acl-main.178.txt,6 Conclusion,2020,"to investigate the use of nlp tools for studying the cognitive traces of recollection versus imagination in stories, we collect and release hip-pocorpus, a dataset of imagined and recalled stories."
2020.acl-main.178.txt,6 Conclusion,2020,we hope these findings bring attention to the feasibility of employing statistical natural language processing machinery as tools for exploring human cognition.
2020.acl-main.179.txt,8 Discussion,2020,experimental findings from psycholinguistics suggest that this issue could follow from a more general mismatch between language production and language comprehension.
2020.acl-main.179.txt,8 Discussion,2020,future work needs to be done to understand more fully what biases are present in the data and learned by language models.
2020.acl-main.179.txt,8 Discussion,2020,"the mismatch between human interpretation biases and production biases suggested by this work invalidates the tacit assumption in much of the natural language processing literature that standard, production-based training data (e.g., web text) are representative of the linguistic biases needed for natural language understanding and generation."
2020.acl-main.179.txt,8 Discussion,2020,"this discrepancy is likely the reason that simply adding more data doesn’t improve model quality (e.g., van schijndel et al., 2019; bisk et al., 2020)."
2020.acl-main.18.txt,5 Conclusion,2020,"our basic idea of acquiring language modeling prior can be potentially extended to a broader scope of generation tasks, based on various input structured data, such as knowledge graphs, sql queries, etc."
2020.acl-main.180.txt,5 Discussion,2020,because speakers enhance or reduce their speech in ways other than changing duration (see e.g.
2020.acl-main.180.txt,5 Discussion,2020,"first, while there are advantages of using naturalistic speech data (gahl et al.2012), it would be desirable to have experimental validation of the confusability measure and its relationship to speaker reduction."
2020.acl-main.180.txt,5 Discussion,2020,the study suggests several directions for future work.
2020.acl-main.180.txt,5 Discussion,2020,"third, a more sophisticated channel model would allow for insertions and deletions, and better capture transitional coarticulatory cues (wright 2004)."
2020.acl-main.180.txt,5 Discussion,2020,"this supports the hypothesis that variation and structure in natural languages are shaped not only by pressures for eﬃcient signals, but also pressures for eﬀective communication of the speaker’s intended message in the face of noise and uncertainty (lindblom 1990, lindblom et al.1995, hall et al.2018)."
2020.acl-main.180.txt,5 Discussion,2020,"under one hypothesis, neighborhood density eﬀects reflect spillover of activation between words with overlapping subsequences of speech sounds (e.g."
2020.acl-main.181.txt,6 Conclusion,2020,it is an open question how to implement ig for these postor mixed-placement adjectives; one possibility is to measure the information gained when the set of adjectives associated to a noun an is partitioned by an adjective a.
2020.acl-main.181.txt,6 Conclusion,2020,such studies will be crucial to achieve a complete computational understanding of natural language syntax.
2020.acl-main.181.txt,6 Conclusion,2020,"the available behavioral evidence suggests that mirror-image preferences (e.g., “box blue big”) may be the norm in post-nominal adjective languages (martin, 1969; scontras et al., 2020)."
2020.acl-main.182.txt,7 Conclusions,2020,we hope the dataset we release will be used to benchmark future dialog system uncertainty research.
2020.acl-main.183.txt,5 Discussion and Conclusion,2020,"one natural extension would be to generalize these findings to other skills than the three addressed here, such as humor/wit, eloquence, image commenting, etc."
2020.acl-main.183.txt,5 Discussion and Conclusion,2020,"we have shown several ways to leverage previous work focusing on individual conversational skills, either by combining trained singleskill models in a two-stage way, by re-using the datasets for simultaneous multi-task training, and by fine-tuning on the overall blended task."
2020.acl-main.184.txt,6 Conclusion and Future Work,2020,"gpt-2, and how to effectively and efficiently introduce more concepts in generation models."
2020.acl-main.184.txt,6 Conclusion and Future Work,2020,"in future, we plan to explore how to combine knowledge with pre-trained language models, e.g."
2020.acl-main.186.txt,5 Conclusion and Future Work:,2020,"as future work, we are extending the proposed approach and test its efficacy on real human conversations."
2020.acl-main.186.txt,5 Conclusion and Future Work:,2020,"more broadly, we continue to explore strategies that combine semantic parsing and neural networks for frame generation."
2020.acl-main.187.txt,7 Conclusions and Future Work,2020,"future work can explore improving the correction models, leveraging logs of natural language feedback to improve text-to-sql parsers, and expanding the dataset to include multiple turns of correction."
2020.acl-main.189.txt,5 Discussion and Limitations,2020,"despite these limitations, we hope that leaqi provides a useful (and relatively simple) bridge that can enable using rule-based systems, heuristics, and unsupervised models as building blocks for more complex supervised learning systems."
2020.acl-main.19.txt,5 Conclusion,2020,"this method is used to modify the squad 2.0 dataset such that it includes conversational answers, which is used to train seq2seq based generation models."
2020.acl-main.190.txt,6 Discussion,2020,extending expbert to other natural language tasks where this relationship might not hold is an open problem that would entail finding different ways of interpreting an explanation with respect to the input.
2020.acl-main.190.txt,6 Discussion,2020,"first, combining our expbert approach with more complex state-of-the-art models can be conceptually straightforward (e.g., we could swap out bert-base for a larger model) but can sometimes also require overcoming technical hurdles."
2020.acl-main.190.txt,6 Discussion,2020,"however, more work will need to be done to make this approach more broadly applicable."
2020.acl-main.190.txt,6 Discussion,2020,recent progress in general-purpose language representation models like bert open up new opportunities to incorporate language into learning.
2020.acl-main.190.txt,6 Discussion,2020,we outline two such avenues of future work.
2020.acl-main.191.txt,5 Conclusion,2020,"from a linguistic perspective, it is worth investigating what the generator encodes in the produced representations."
2020.acl-main.191.txt,5 Conclusion,2020,"in fact, the generator network is only used in training, while at inference time only the discriminator is necessary."
2020.acl-main.191.txt,5 Conclusion,2020,"moreover, we will investigate the potential impact of the adversarial training directly in the bert pre-training."
2020.acl-main.191.txt,5 Conclusion,2020,"this first investigation paves the way to several extensions including adopting other architectures, such as gpt-2 (radford et al., 2019) or distilbert (sanh et al., 2019) or other tasks, e.g., sequence labeling or question answering."
2020.acl-main.192.txt,6 Conclusion,2020,future directions include (1) devising hierarchical span representations that can handle spans of different length and diverse content more effectively and efficiently; (2) robust multitask learning or meta-learning algorithms that can reconcile very different tasks.
2020.acl-main.193.txt,6 Conclusion,2020,the proposed learning framework also shows promising results on other nlp tasks like text classification.
2020.acl-main.194.txt,6 Conclusion,2020,"for future direction, we plan to explore the effectiveness of mixtext in other nlp tasks such as sequential labeling tasks and other real-world scenarios with limited labeled data."
2020.acl-main.195.txt,5 Conclusion,2020,we believe our findings are generic and can be applied to other model compression problems.
2020.acl-main.196.txt,5 Conclusion,2020,"thus, we encourage future research into obtaining tighter bounds on latent lm perplexity, possibly by using more powerful proposal distributions that consider entire documents as context, or by considering methods such as annealed importance sampling."
2020.acl-main.196.txt,5 Conclusion,2020,we investigate the application of importance sampling to evaluating latent language models.
2020.acl-main.197.txt,6 Conclusion,2020,we will explore this direction as future work.
2020.acl-main.198.txt,6 Conclusion,2020,alternative architectures that can overcome the stolen probability effect are an item of future work.
2020.acl-main.199.txt,5 Conclusion,2020,"in the future, we would like to figure out different strategies to merge individual gains, obtained by separate application of the dag constraint, into a setup that can take the best of both precision and recall improvements, and put forth a better performing system."
2020.acl-main.199.txt,5 Conclusion,2020,we also plan on looking into strategies to improve recall of the constructed taxonomy.
2020.acl-main.2.txt,6 Conclusion,2020,future work can further investigate temporal patterns in how language used by depressed people evolves over the course of an interaction.
2020.acl-main.2.txt,6 Conclusion,2020,we hope that this combination will encourage the research community to make more progress in this direction.
2020.acl-main.20.txt,5 Conclusion,2020,"as future work, we plan to extend our qag model to a meta-learning framework, for generalization over diverse datasets."
2020.acl-main.200.txt,6 Conclusion,2020,"for industrial applications where there is a trade-off typically between accuracy and latency, our findings suggest it might be feasible to gain accuracy for faster models by collecting more training examples."
2020.acl-main.201.txt,6 Conclusion and Discussion,2020,an obvious fix is adding training words to the bli test set.
2020.acl-main.201.txt,6 Conclusion and Discussion,2020,"however, it is unclear how to balance between training and test words."
2020.acl-main.201.txt,6 Conclusion and Discussion,2020,"there are other ways to fit the dictionary better; e.g., using a non-linear projection such as a neural network."
2020.acl-main.201.txt,6 Conclusion and Discussion,2020,"therefore, future work should focus on downstream tasks instead of bli."
2020.acl-main.201.txt,6 Conclusion and Discussion,2020,we leave the exploration of non-linear projections to future work.
2020.acl-main.201.txt,6 Conclusion and Discussion,2020,"we then add a synthetic dictionary to balance bli test and training accuracy, which further helps downstream models on average."
2020.acl-main.203.txt,6 Conclusion,2020,our findings point to future research opportunities to build stealthy authorship obfuscation methods.
2020.acl-main.203.txt,6 Conclusion,2020,we suggest that obfuscation methods should strive to preserve text smoothness in addition to semantics.
2020.acl-main.204.txt,5 Conclusions and Future Work,2020,"there are a few interesting questions left unanswered in this paper, which would provide interesting future research directions: (1) deebert’s training method, while maintaining good quality in the last off-ramp, reduces model capacity available for intermediate off-ramps; it would be important to look for a method that achieves a better balance between all off-ramps.(2) the reasons why some transformer layers appear redundant2 and why dee-bert considers some samples easier than others remain unknown; it would be interesting to further explore relationships between pre-training and layer redundancy, sample complexity and exit layer, and related characteristics."
2020.acl-main.205.txt,6 Conclusion,2020,"finally, we also open a path to study integration of knowledge into the decoding phase, which can benefit other tasks such as neural machine translation."
2020.acl-main.206.txt,6 Conclusion,2020,"therefore future work could include different sampling methods, generation of synthetic data, or training objectives which reward models which are less conservatively drawn to the middle of the scoring scale."
2020.acl-main.207.txt,8 Conclusions and Future Work,2020,another item of future work is to develop better multitask approaches to leverage multiple signals of relatedness information during training.
2020.acl-main.207.txt,8 Conclusions and Future Work,2020,including other information such as outgoing citations as additional input to the model would be yet another area to explore in future.
2020.acl-main.207.txt,8 Conclusions and Future Work,2020,it would be interesting to initialize our model weights from more recent transformer models to investigate if additional gains are possible.
2020.acl-main.207.txt,8 Conclusions and Future Work,2020,the landscape of transformer language models is rapidly changing and newer and larger models are frequently introduced.
2020.acl-main.207.txt,8 Conclusions and Future Work,2020,"we used citations to build triplets for our loss function, however there are other metrics that have good support from the bibliometrics literature (klavans and boyack, 2006) that warrant exploring as a way to create relatedness graphs."
2020.acl-main.208.txt,7.3 Code Piece Error Analysis,2020,"to help the readers understand the bottleneck for code piece generation and point out important future directions, we randomly sampled 200 “hard” lines and manually analyzed why the generation fails by looking at the top 1 candidate of the model."
2020.acl-main.21.txt,7 Conclusion,2020,"besides, more powerful question clustering and coarse-to-fine generation scenarios are also worth exploration."
2020.acl-main.21.txt,7 Conclusion,2020,"finally, performing sqg on other types of inputs, e.g., images and knowledge graphs, is an interesting topic."
2020.acl-main.21.txt,7 Conclusion,2020,"for future works, the major challenge is generating more meaningful, informative but concise questions."
2020.acl-main.211.txt,6 Discussion and Future Work,2020,"however, a host of other options could be considered in future work."
2020.acl-main.212.txt,6 Discussion,2020,"an alternative would be to create training sets that adequately represent a diverse range of linguistic phenomena; crowdworkers’ (rational) preferences for using the simplest generation strategies possible could be counteracted by approaches such as adversarial filtering (nie et al., 2019)."
2020.acl-main.212.txt,6 Discussion,2020,"in the interim, however, we conclude that data augmentation is a simple and effective strategy to mitigate known inference heuristics in models such as bert."
2020.acl-main.212.txt,6 Discussion,2020,"ultimately, it would be desirable to have a model with a strong inductive bias for using syntax across language understanding tasks, even when overlap heuristics lead to high accuracy on the training set; indeed, it is hard to imagine that a human would ignore syntax entirely when understanding a sentence."
2020.acl-main.213.txt,5 Conclusions,2020,"we improve the generalization of autoregressive predictive coding by multi-target training of future prediction lf and past memory reconstruction lr, where the latter serves as a regularization."
2020.acl-main.214.txt,7 Conclusion,2020,"using a proposed multimodal adaptation gate (mag), bert and xlnet were successfully fine-tuned in presence of vision and acoustic modalities."
2020.acl-main.216.txt,5 Conclusions,2020,"finally, we will evaluate the multiresolution loss on larger datasets to analyze it’s regularizing effects."
2020.acl-main.216.txt,5 Conclusions,2020,"furthermore, we will experiment with more ellaborate, attention-based fusion mechanisms."
2020.acl-main.216.txt,5 Conclusions,2020,"in the future, we plan to alleviate this by incorporating ideas from sparse transformer variants (kitaev et al., 2020; child et al., 2019)."
2020.acl-main.216.txt,5 Conclusions,2020,our proposed framework uses a crossmodal dot-product attention to map visual features to audio feature space.
2020.acl-main.217.txt,10 Conclusion,2020,"we hope that these model comparisons and results inform development of more robust end-to-end models, and provide a stronger benchmark for performance on low-resource settings."
2020.acl-main.218.txt,7 Conclusion,2020,from human evaluations of dialogue models trained with various data configurations we find that spolin is useful—when including it we are able to build models that can generate yes-ands more consistently than when we leave it out.
2020.acl-main.218.txt,7 Conclusion,2020,we plan to continue our data-driven approach for grounded conversations by expanding our dataset through our iterative data collection process with other larger text-based open-domain dialogue corpora and extend our work to model and collect longer conversations exhibiting more complex improv-backed turns.
2020.acl-main.219.txt,6 Conclusion,2020,"the next challenge will also be to combine this engagingness with other skills, such as world knowledge (antol et al., 2015) relation to personal interests (zhang et al., 2018), and task proficiency."
2020.acl-main.219.txt,6 Conclusion,2020,this result is made possible by the creation of a new dataset image-chat3.
2020.acl-main.219.txt,6 Conclusion,2020,"while our human evaluations were on short conversations, initial investigations indicate the model as is can extend to longer chats, see appendix g, which should be studied in future work."
2020.acl-main.220.txt,5 Conclusion,2020,"since it provides immediate continuous rewards and at the singlestep level, maude can be also be used to optimize and train better dialogue generation models, which we want to pursue as future work."
2020.acl-main.222.txt,6 Discussion,2020,"future work should consider adding these tasks to the ones used here, while continuing the quest for improved models."
2020.acl-main.222.txt,6 Discussion,2020,"still, despite leveraging 12 tasks, there are many skills not included in our set."
2020.acl-main.222.txt,6 Discussion,2020,"this work tries to bridge the gap to avoid agents with niche skills, to move towards evaluating an open-domain set of skills."
2020.acl-main.223.txt,5 Conclusion,2020,"and finally, we would like to adapt the model for automatic poetry translation—as we feel that the constraint-based approach lends itself perfectly to a poetry translation model that is able to adhere to an original poem in both form and meaning."
2020.acl-main.223.txt,5 Conclusion,2020,"first of all, we would like to experiment with different neural network architectures."
2020.acl-main.223.txt,5 Conclusion,2020,"in order to facilitate reproduction of the results and encourage further research, the poetry generation system is made available as open source software."
2020.acl-main.223.txt,5 Conclusion,2020,"secondly, we would like to incorporate further poetic devices, especially those based on meaning."
2020.acl-main.223.txt,5 Conclusion,2020,"specifically, we believe hierarchical approaches (serban et al., 2017) as well as the transformer network (vaswani et al., 2017) would be particularly suitable to poetry generation."
2020.acl-main.223.txt,5 Conclusion,2020,we conclude with a number of future research avenues.
2020.acl-main.224.txt,6 Conclusion,2020,future work will validate the effectiveness of this method on more varied data-to-text generation tasks.
2020.acl-main.225.txt,8 Conclusion,2020,"in future work, we plan to incorporate these features into co-creation systems which assist humans in the writing process."
2020.acl-main.225.txt,8 Conclusion,2020,"we hope that our work encourages more investigation of infilling, which may be a key missing element of current writing assistance tools."
2020.acl-main.226.txt,5 Conclusions and Outlook,2020,"a large-scale pre-trained variational autoencoder (li et al., 2020) could possibly improve the smoothness of sentence embeddings.(ii) our model predicts a feature vector for the missing sentence."
2020.acl-main.226.txt,5 Conclusions and Outlook,2020,"a post-editor to fix the issue (voita et al., 2019) should be able to understand inter-sentential relationship and to generate fluent sentences in the surrounding context, both of which can be learned from sentence infilling.note."
2020.acl-main.226.txt,5 Conclusions and Outlook,2020,"after this paper was posted on arxiv, some related works appeared.(shen et al., 2020) proposes blank language model for text infilling and other tasks.(donahue et al., 2020) trains (finetunes) a language model (gpt-2) for text and sentence infilling.(li et al., 2020) pre-trains a largescale variational autoencoder with a pair of bert and gpt-2.(ippolito et al., 2020) uses a sentencelevel language model, which operates on sentence embeddings obtained from bert, to predict story endings."
2020.acl-main.226.txt,5 Conclusions and Outlook,2020,"one can try to use a variational autoencoder (kingma and welling, 2014) instead."
2020.acl-main.226.txt,5 Conclusions and Outlook,2020,our approach can be modified or extended in some ways.(i) we use a denoising autoencoder to obtain sentence embeddings.
2020.acl-main.226.txt,5 Conclusions and Outlook,2020,"this vector can be fed into and serve as a guide to token-level models including the baseline (zhu et al., 2019)."
2020.acl-main.227.txt,7 Conclusions,2020,"furthermore, this framework can alleviate the sparse-reward issue, as the intermediate rewards are used to optimize the generator."
2020.acl-main.227.txt,7 Conclusions,2020,the guider network provides a plan-ahead mechanism for next-word selection.
2020.acl-main.228.txt,6 Conclusion and Future Work,2020,"by performing analysis on gigaword, we find that there exists room to improve summarization performance with better post-ranking algorithms, a promising direction for future research."
2020.acl-main.228.txt,6 Conclusion and Future Work,2020,"moving forward, we would like to apply this framework to other retrieve-and-edit based generation scenarios such as dialogue, conversation, and code generation."
2020.acl-main.228.txt,6 Conclusion and Future Work,2020,"this is in line with our overall goal, which is not to find the best possible way to do the postranking, but only to show that gains are possible by editing multiple candidates and then comparing the results."
2020.acl-main.229.txt,6 Discussion,2020,"for instance, developing agents that are robust to variations in both visual appearance and instruction descriptions is an important next step."
2020.acl-main.229.txt,6 Discussion,2020,"secondly, richer and more complicated variations between the learning setting and the real physical world need to be tackled."
2020.acl-main.229.txt,6 Discussion,2020,there are a few future directions to pursue.
2020.acl-main.230.txt,6 Conclusions and Future Work,2020,"in the future, we aim to extend our framework to extract events from videos, and make it scalable to new event types."
2020.acl-main.230.txt,6 Conclusions and Future Work,2020,"we plan to expand our annotations by including event types from other text event ontologies, as well as new event types not in existing text ontologies."
2020.acl-main.230.txt,6 Conclusions and Future Work,2020,"we will also apply our extraction results to downstream applications including cross-media event inference, timeline generation, etc."
2020.acl-main.231.txt,8 Discussion,2020,"future work might explore methods for incorporating richer learned representations both of the diverse visual observations in videos, and the narration that describes them, into such models."
2020.acl-main.231.txt,8 Discussion,2020,we hope that future work will continue to evaluate broadly.
2020.acl-main.231.txt,8 Discussion,2020,"while action segmentation in videos from diverse domains remains challenging – videos contain both a large variety of types of depicted actions, and high visual variety in how the actions are portrayed – we find that structured generative models provide a strong benchmark for the task due to their abilities to capture the full diversity of action types (by directly modeling distributions over action occurrences), and to benefit from weak supervision."
2020.acl-main.232.txt,7 Conclusion and Future Work,2020,"and, finally, an approach like the speaker-follower models of fried et al.(2018) could be used to train our builder model and the architect model of narayan-chen et al.(2019) jointly."
2020.acl-main.232.txt,7 Conclusion and Future Work,2020,"in the future, richer representations of the dialogue history (e.g.by using bert (devlin et al., 2019) or of past builder actions) combined with de-noising of the human data and perhaps more exhaustive data augmentation should produce better output sequences."
2020.acl-main.234.txt,6 Conclusion and Related Work,2020,"studying this type of difference between expressive models and their less expressive, restricted variants remains an important direction for future work."
2020.acl-main.234.txt,6 Conclusion and Related Work,2020,"the question of what is captured by vision and language models has been studied before, including for visual question answering (agrawal et al., 2016, 2017; goyal et al., 2017), referring expression resolution (cirik et al., 2018), and visual navigation (jain et al., 2019)."
2020.acl-main.235.txt,6 Conclusions and Future Work,2020,further study in this direction may be interesting.
2020.acl-main.235.txt,6 Conclusions and Future Work,2020,"however, it is hard to get the same formula for some more strong or sophisticated priors, e.g., the dirichlet prior."
2020.acl-main.236.txt,5 Conclusion,2020,"we hope this work inspires future research on better understanding the differences between embedding methods, and on designing simpler and more efficient models."
2020.acl-main.237.txt,8 Conclusion,2020,a potential future research direction is to bridge the gap between this simple bootstrapping paradigm and the incorporation of user free-form responses to allow the system to handle free-text responses.
2020.acl-main.237.txt,8 Conclusion,2020,"we hope our work will encourage more research on different possibilities of building interactive systems that do not necessarily require handling full-fledged dialogue, but still benefit from user interaction."
2020.acl-main.238.txt,6 Conclusion,2020,"in the future, we would like to jointly learn discrete representations of entities as well as relations."
2020.acl-main.238.txt,6 Conclusion,2020,our approaches learn to represent entities in a kg as a vector of discrete codes in an end-to-end fashion.
2020.acl-main.238.txt,6 Conclusion,2020,this is a barrier in successful deployment of models using knowledge graphs at scale on user-facing computing devices.
2020.acl-main.239.txt,5 Conclusion,2020,"yet, our approach can be applied also with other more recent language models that have stronger context-based embeddings."
2020.acl-main.240.txt,6 Conclusion,2020,"future work could find additional modular uses of mlms, simplify maskless pll computations, and use plls to devise better sentence- or document-level scoring metrics."
2020.acl-main.241.txt,5 Conclusions,2020,"first, ote extends the modeling of rotate from 2d complex domain to high dimensional space with orthogonal relation transforms."
2020.acl-main.241.txt,5 Conclusions,2020,"second, graph context is proposed to integrate graph structure information into the distance scoring function to measure the plausibility of the triples during training and inference."
2020.acl-main.242.txt,5 Conclusion and Future Directions,2020,"the theoretical underpinnings of our poscal idea are not explored in detail here, but developing formal statistical support for these ideas constitutes interesting future work."
2020.acl-main.243.txt,10 Conclusion,2020,another direction is to improve the sources of weak supervision and such as interactive new constraints provided by users.
2020.acl-main.243.txt,10 Conclusion,2020,"finally, it would be interesting to explore alternative training methods for these models, such as reducing reliance on hard sampling through better relaxations of structured models."
2020.acl-main.243.txt,10 Conclusion,2020,induction of grounded control states opens up many possible future directions for this work.
2020.acl-main.243.txt,10 Conclusion,2020,one could also explore alternative posterior constraints based on pre-trained models for summarization or paraphrase tasks to induce semantically grounded latent variables.
2020.acl-main.243.txt,10 Conclusion,2020,these states can be used to provide integration with external rule-based systems such as hard constraints at inference time.
2020.acl-main.244.txt,6 Conclusion,2020,"to accomplish this, we carefully restructured and matched previous datasets to induce numerous realistic distribution shifts."
2020.acl-main.245.txt,6 Discussion,2020,"besides typos, other perturbations can also be applied to text."
2020.acl-main.245.txt,6 Discussion,2020,"gong et al.(2019) apply a spellcorrector to correct typos chosen to create ambiguity as to the original word, but these typos are not adversarially chosen to fool a model."
2020.acl-main.245.txt,6 Discussion,2020,"in computer vision, chen et al.(2019) discretizes pixels to compute exact robust accuracy on mnist, but their approach generalizes poorly to other tasks like cifar-10."
2020.acl-main.245.txt,6 Discussion,2020,"many recent advances in nlp have been fueled by the rise of task-agnostic representations, such as bert, that facilitate the creation of accurate models for many tasks."
2020.acl-main.245.txt,6 Discussion,2020,"other attack surfaces involving insertion of sentences (jia and liang, 2017) or syntactic rearrangements (iyyer et al., 2018) are harder to pair with roben, and are interesting directions for future work."
2020.acl-main.245.txt,6 Discussion,2020,"using context is not fundamentally at odds with the idea of robust encodings, and making contextual encodings stable is an interesting technical challenge and a promising direction for future work."
2020.acl-main.245.txt,6 Discussion,2020,we hope our work inspires new task-agnostic robust encodings that lead to more robust and more accurate models.
2020.acl-main.246.txt,5 Conclusions,2020,we empirically study its practical effects on tasks in document classification and sentiment analysis.
2020.acl-main.247.txt,7 Conclusion and Future Work,2020,"in future work, we will address end-to-end question answering with pre-training for both the answer selection and retrieval components."
2020.acl-main.247.txt,7 Conclusion and Future Work,2020,"we hope to progress to a model of general purpose language modeling that uses an indexed long term memory to retrieve world knowledge, rather than holding it in the densely activated transformer encoder layers."
2020.acl-main.248.txt,4 Conclusion and Future Work,2020,our future work is to include the paragraph representation in the constraint prediction model.
2020.acl-main.248.txt,4 Conclusion and Future Work,2020,this will help our methodology to have the benefit of making informed decision while also solving constraints.
2020.acl-main.249.txt,7 Conclusion,2020,"we hope that this work makes clear the necessity for asserting the genuineness of pre-trained weights, just like there exist similar mechanisms for establishing the veracity of other pieces of software."
2020.acl-main.25.txt,6 Conclusion,2020,we believe our work paves the way for better understanding of neural text generation models and understanding that modeling choices reveals the model configurations is a first crucial step.
2020.acl-main.251.txt,6 Conclusion,2020,"in the future, we seek to expand upon energy-based translation using our method."
2020.acl-main.252.txt,7 Conclusion and Future Directions,2020,"future work should explore techniques like iterative back-translation (hoang et al., 2018) for further improvement and scaling to larger model capacities and more languages (arivazhagan et al., 2019b; huang et al., 2019) to maximize transfer across languages and across data sources."
2020.acl-main.253.txt,6 Conclusions,2020,"in the future, we plan to investigate more thoroughly the use of language models for evaluating fluency, the effect of domain mismatch in the choice of monolingual data, and ways to generalize this study to other applications beyond mt."
2020.acl-main.253.txt,6 Conclusions,2020,"we recommend distinguishing between direct and reverse translations for automatic evaluation, and to make final judgements based on human evaluation."
2020.acl-main.255.txt,6 Conclusion,2020,we leave it as future work to explore ways to raise accuracy on unseen synsets without harming performances on frequent synsets.
2020.acl-main.257.txt,7 Conclusion and Future Work,2020,in future work we will investigate more sophisticated neural (sub-)networks within the proposed framework.
2020.acl-main.257.txt,7 Conclusion and Future Work,2020,the pre-trained word vectors used in this work are available online at: https://github.com/cambridgeltl/fs-wrep.
2020.acl-main.257.txt,7 Conclusion and Future Work,2020,"we induced a joint vector space 6with separate parameters we merge vectors from “duplicate” vector spaces by non-weighted averaging.in which several groups of words (e.g., s, v, and o words forming the svo structures) are represented while taking into account the mutual associations between the groups."
2020.acl-main.257.txt,7 Conclusion and Future Work,2020,"we will also apply the idea of functionspecific training to other interrelated linguistic phenomena and other languages, probe the usefulness of function-specific vectors in other language tasks, and explore how to integrate the methodology with sequential models."
2020.acl-main.259.txt,7 Conclusions and Future Work,2020,"directionality of edges did not result in improvement in our models in this work, however for future, we plan to develop gcns that incorporate edge typing, which would enable us to differentiate between different mwe types and dependency relations while comparing them against the current models."
2020.acl-main.259.txt,7 Conclusions and Future Work,2020,"for future work, we plan to add vmwe annotations to the vu amsterdam corpus (steen, 2010) which is the largest metaphor dataset and extend our experiments using that resource."
2020.acl-main.259.txt,7 Conclusions and Future Work,2020,the code used in the experiments will be made publicly available 6.
2020.acl-main.260.txt,5 Conclusion,2020,"however, due to the limitation of available resources, this study is limited to the european languages."
2020.acl-main.260.txt,5 Conclusion,2020,"however, most of the work only focuses on english corpora and little is known about the bias in multilingual embeddings."
2020.acl-main.260.txt,5 Conclusion,2020,"in this work, we build different metrics and datasets to analyze gender bias in the multilingual embeddings from both the intrinsic and extrinsic perspectives."
2020.acl-main.260.txt,5 Conclusion,2020,we hope this study can work as a foundation to motivate future research about the analysis and mitigation of bias in multilingual embeddings.
2020.acl-main.261.txt,5 Moving Forward,2020,"finally, while we have used one particular paper as a case study throughout this paper, our intent was in no way to name and shame the authors, but rather to use it as a case study to explore different ethical dimensions of research publications, and attempt to foster much broader debate on this critical issue for nlp research."
2020.acl-main.261.txt,5 Moving Forward,2020,"given all of the above, what should have been the course of action for the paper in question?"
2020.acl-main.261.txt,5 Moving Forward,2020,looking to other scientific disciplines that have faced similar issues in the past may provide some guidance for our future.
2020.acl-main.264.txt,7 Conclusion,2020,"however, a comprehensive study is required to prove the conjecture and we leave this as future work."
2020.acl-main.264.txt,7 Conclusion,2020,"however, our analysis and the mitigation framework is general and can be adopted to other applications and other types of bias."
2020.acl-main.264.txt,7 Conclusion,2020,we posit that the regularization and the over-fitting nature of deep learning models might contribute to the bias amplification.
2020.acl-main.265.txt,6 Conclusion,2020,"additionally, future work should further probe the source of gender bias in the model’s predictions, perhaps by visualizing attention or looking more closely at the model’s outputs."
2020.acl-main.265.txt,6 Conclusion,2020,it is an open and difficult research question to build unbiased neural relation extractors.
2020.acl-main.265.txt,6 Conclusion,2020,we encourage future work to dive deeper into this problem.
2020.acl-main.265.txt,6 Conclusion,2020,"we only consider binary gender, but future work should consider non-binary genders."
2020.acl-main.265.txt,6 Conclusion,2020,"while these findings will help future work avoid gender biases, this study is preliminary."
2020.acl-main.267.txt,6 Conclusion and Future Work,2020,"first, we plan to explore why the model prefers “soft” attentions rather than “hard” ones, which is different from the findings in several prior works based on hard attention."
2020.acl-main.267.txt,6 Conclusion and Future Work,2020,"in our future work, we will explore several potential directions."
2020.acl-main.267.txt,6 Conclusion and Future Work,2020,"instead of using a fixed pooling norm for universal text representation learning, we propose to learn the norm in an end-to-end framework to automatically find the optimal ones for learning text representations in different tasks."
2020.acl-main.267.txt,6 Conclusion and Future Work,2020,"second, we plan to study how to model the differences on the characteristics of different samples and use different pooling norms, which may have the potential to further improve our approach."
2020.acl-main.267.txt,6 Conclusion and Future Work,2020,"third, we will explore how to generalize our approach to other modalities, such as images, audios and videos, to see whether it can facilitate more attention-based methods."
2020.acl-main.268.txt,6 Conclusion,2020,"in future work, apart from improving the similarity measures, it could be examined to predict mtl scores or estimate the right amount of auxiliary data or shared parameters in the neural network."
2020.acl-main.269.txt,6 Conclusion,2020,"future directions include validating our findings on other san architectures (e.g., bert (devlin et al., 2019)) and more general attention models (bahdanau et al., 2015; luong et al., 2015)."
2020.acl-main.27.txt,8 Conclusion,2020,"as future work, we will extend our framework to more complex contexts by devising efficient learning algorithms."
2020.acl-main.27.txt,8 Conclusion,2020,our proposed framework also verifies the necessity of the type information in the code translation related tasks with a practical framework and good results.
2020.acl-main.270.txt,8 Conclusion,2020,"by showing that sublayer ordering can improve models at no extra cost, we hope that future research continues this line of work by looking into optimal sublayer ordering for other tasks, such as translation, question answering, and classification."
2020.acl-main.270.txt,8 Conclusion,2020,"sublayer reordering can improve the performance of transformer models, but an ordering that improves models on one group of tasks (word/character-level language modeling) might not improve the performance on another task."
2020.acl-main.271.txt,7 Conclusion,2020,"the proposed method is not limited to the two aforementioned tasks, but can be applied to any nlp as well as other tasks such as machine translation and image recognition."
2020.acl-main.272.txt,5 Conclusion,2020,"beyond that, we use reinforcement learning to learn data selection policy automatically, thus obviating the need to manual adjustment."
2020.acl-main.272.txt,5 Conclusion,2020,"in this way, our approach could leverage unlabeled data and alleviate the domain shift between seen classes and unseen classes."
2020.acl-main.272.txt,5 Conclusion,2020,"to realize the transferring between classes with low similarity, our method essentially turns a zero-shot learning problem into a semi-supervised learning problem."
2020.acl-main.273.txt,5 Conclusion,2020,"besides, how to introduce scene graphs into multi-modal nmt is a worthy problem to explore."
2020.acl-main.273.txt,5 Conclusion,2020,"finally, we will apply our model into other multi-modal tasks such as multi-modal sentiment analysis."
2020.acl-main.273.txt,5 Conclusion,2020,"in the future, we plan to incorporate attributes of visual objects and dependency trees to enrich the multi-modal graphs."
2020.acl-main.276.txt,5 Conclusion,2020,the riemannian framework allows to exploit the geometry of the doubly stochastic manifold.
2020.acl-main.278.txt,7 Conclusion,2020,"as well-calibrated confidence estimation is more likely to establish trustworthiness with users, we plan to apply our work to interactive machine translation scenarios in the future."
2020.acl-main.278.txt,7 Conclusion,2020,"through a series of in-depth analyses, we report several interesting findings which may help to analyze, understand and improve nmt models."
2020.acl-main.279.txt,4 Conclusion,2020,"in the future, we plan to enable the glyph and phonetic variation detection by integrating the variation graph representation learning, which may improve signal’s performance."
2020.acl-main.28.txt,5 Conclusion and Future Work,2020,"in the future, we plan to apply the sa framework on syntactic parse trees in hopes of generating more syntactically different sentences (motivated by our case study)."
2020.acl-main.280.txt,6 Conclusion,2020,"in the future, we plan to study complicated situations such as a law case with multiple defendants and charges."
2020.acl-main.282.txt,5 Conclusion,2020,"moreover, we use the graph convolutional network to capture the co-occurrence correlation."
2020.acl-main.282.txt,5 Conclusion,2020,"we believe our method can also be applied to other tasks that need to exploit hierarchical label structure and label co-occurrence, such as fine-grained entity typing and hierarchical multi-label classification."
2020.acl-main.283.txt,7 Conclusion,2020,adaptive routing is additionally applied to improve the scalability of hypercaps by controlling the number of capsules during the routing procedure.
2020.acl-main.283.txt,7 Conclusion,2020,"as recent works explore the superiority of hyperbolic space to euclidean space for serval natural language processing tasks, we intend to couple with the hyperbolic neural networks (ganea et al., 2018b) and the hyperbolic word embedding method such as poincare´glove (tifrea et al., 2019) in the future."
2020.acl-main.284.txt,7 Conclusion,2020,"it is still valuable to study models on open datasets, however, as these are readily available to the community."
2020.acl-main.284.txt,7 Conclusion,2020,"our future research direction includes a thorough study of differences in this dataset with actual tickets, and potential for transfer."
2020.acl-main.285.txt,6 Conclusion and Future Work,2020,promising future directions include: 1) utilize more types of data from mooccube to facilitate existing topics; 2) employ advanced models in existing tasks; 3) more innovative nlp application tasks in online education domain.
2020.acl-main.287.txt,7 Conclusion,2020,our findings help to understand how effective argumentation works in the political sphere of editorial argumentation — and how to generate such argumentation.
2020.acl-main.289.txt,6 Conclusion and Future Work,2020,"finally, it would be interesting to study the semantic roles of emotion (bostan et al., 2020), which considers the full structure of an emotion expression and is more challenging."
2020.acl-main.289.txt,6 Conclusion and Future Work,2020,"in future work, we shall explore the following directions."
2020.acl-main.289.txt,6 Conclusion and Future Work,2020,"our approach effectively models inter-clause relationships to learn clause representations, and integrates relative position enhanced clause pair ranking into a unified neural network to extract emotioncause pairs in an end-to-end fashion."
2020.acl-main.289.txt,6 Conclusion and Future Work,2020,"second, designing effective methods to inject appropriate linguistic knowledge into neural models is valuable to emotion analysis tasks (ke et al., 2019; zhong et al., 2019)."
2020.acl-main.29.txt,6 Conclusion and Future Work,2020,"having shown that segment bounds contain useful supervisory signal, it would be interesting to examine if segment hierarchies might also contain useful signal."
2020.acl-main.29.txt,6 Conclusion and Future Work,2020,"s-lstm is agnostic as to the sentence encoder used, so we would like to investigate the potential usefulness of transformer-based language models as sentence encoders."
2020.acl-main.29.txt,6 Conclusion and Future Work,2020,"there are additional engineering challenges associated with using models such as bert as sentence encoders, since encoding entire documents can be too expensive to fit on a gpu without model parallelism."
2020.acl-main.29.txt,6 Conclusion and Future Work,2020,we would also like to investigate the usefulness of an unconsidered source of document structure: the hierarchical nature of sections and subsections.
2020.acl-main.293.txt,7 Conclusion and Future work,2020,"one can try to adapt our proposed csae architecture for an integrated approach by applying the unified tagging scheme; thereby, aspect extraction and sentiment classification can be achieved simultaneously."
2020.acl-main.293.txt,7 Conclusion and Future work,2020,the substantial improvements highlight the under-performance of recent contextualized embedding models in “understanding” syntactical features and suggests future directions in developing more syntax-learning contextualized embeddings.
2020.acl-main.296.txt,5 Conclusion,2020,"for future works, we will explore pair-wise at and ot extraction together with aspect category and sentiment polarity classification."
2020.acl-main.297.txt,7 Conclusions,2020,we compare different representations of syntactic dependency information and propose dependency gcn to encode richer structural information from different processing levels of the parser.
2020.acl-main.298.txt,8 Conclusion,2020,"for future work, we aim to develop a universal model to handle both tree and non-tree arguments."
2020.acl-main.299.txt,6 Conclusion,2020,"in addition, we build a new normalization method, which can add constraints on all the spans with the same right boundary."
2020.acl-main.30.txt,8 Conclusions and Future Work,2020,"also, extending our method for other types of textual data, such as short texts, multi-lingual data, and code-switched data is a potential direction."
2020.acl-main.30.txt,8 Conclusions and Future Work,2020,"in the future, we are interested in generalizing contextualized weak supervision to hierarchical text classification problems."
2020.acl-main.300.txt,4 Discussion,2020,we make the following recommendations for future experiments on unsupervised constituency parsing.
2020.acl-main.302.txt,6 Conclusions,2020,"we also empirically verify that the complex outside algorithm can be implicitly performed via efficient back-propagation, which naturally produces gradients and marginal probabilities."
2020.acl-main.303.txt,7 Discussion,2020,"finally, future work should investigate whether data augmentation can fully bridge the gap between low-bias learners and structured tree lstms, and whether our conclusions apply to other syntactic phenomena besides agreement."
2020.acl-main.303.txt,7 Discussion,2020,future work should further explore both of these approaches.
2020.acl-main.303.txt,7 Discussion,2020,"in future work, these syntactic limitations may be overcome by giving the model typed dependencies (which would distinguish between a subject-verb dependency and a verb-object dependency)."
2020.acl-main.303.txt,7 Discussion,2020,it seems particularly promising to explore alternative formulations of the dependency lstm (as mentioned above) and the effect of learning embeddings of non-terminal symbols for the constituency lstm.
2020.acl-main.303.txt,7 Discussion,2020,our conclusions about the importance of explicit mechanisms for representing syntactic structure can be strengthened by developing different formulations of the tree lstms.
2020.acl-main.303.txt,7 Discussion,2020,"though the models we used require parse trees to be provided, it is possible that models can learn to induce tree structure in an unsupervised or weakly-supervised manner (bowman et al., 2016; choi et al., 2018; shen et al., 2019)."
2020.acl-main.305.txt,6 Conclusion,2020,we also analyze our model’s outputs to get more insights into user interest dynamics.
2020.acl-main.306.txt,5 Conclusion,2020,"on the other hand, since the size of existing mner datasets is relatively small, we plan to leverage the large amount of unlabeled social media posts in different platforms, and propose an effective framework to combine them with the small amount of annotated data to obtain a more robust mner model."
2020.acl-main.306.txt,5 Conclusion,2020,there are several future directions for this work.
2020.acl-main.306.txt,5 Conclusion,2020,"therefore, our next step is to enhance umt so as to dynamically filter out the potential noise from images."
2020.acl-main.307.txt,8 Conclusion,2020,"this significant gain suggests further potential of our stock embedding for modeling the correlation among stocks in a financial market, and for further applications, such as risk control and asset pricing."
2020.acl-main.308.txt,5 Conclusion and Future Work,2020,"finally, we plan to experiment with other languages."
2020.acl-main.308.txt,5 Conclusion and Future Work,2020,"in future work, we plan to perform user profiling with respect to polarizing topics such as gun control (darwish et al., 2020), which can then be propagated from users to media (atanasov et al., 2019; stefanov et al., 2020)."
2020.acl-main.308.txt,5 Conclusion and Future Work,2020,"we further want to model the network structure, e.g., using graph embeddings (darwish et al., 2020)."
2020.acl-main.309.txt,7 Discussion and Conclusion,2020,alleviating this restriction is an important future direction.
2020.acl-main.309.txt,7 Discussion and Conclusion,2020,"in terms of language acquisition, the supervision provided in our approach can be seen as direct negative evidence (marcus, 1993)."
2020.acl-main.310.txt,8 Conclusion,2020,this study shed light on understanding the behaviors of language encoders against grammatical errors and encouraged future work to enhance the robustness of these models.
2020.acl-main.311.txt,7 Conclusion,2020,"furthermore, little is known for whether and how we can utilize them for better capturing linguistic properties and eventually improving the performance of downstream tasks for which the embeddings are constructed."
2020.acl-main.311.txt,7 Conclusion,2020,"furthermore, we explored how the hundreds of attention heads underwent performance variation during the fine-tuning process on the downstream tasks, revealing the internal behaviors with the proposed analysis method."
2020.acl-main.311.txt,7 Conclusion,2020,immediate attention should be paid to the investigation of how heat maps would vary during the extensive pre-training so that we have a better understanding of the dynamics of the learning processes.
2020.acl-main.312.txt,6 Conclusions,2020,"moreover, we can also examine the influence of using deep contextualized input encoders such as elmo (peters et al., 2018) or bert (devlin et al., 2018)."
2020.acl-main.312.txt,6 Conclusions,2020,"specifically, we can further examine the influence of using pre-trained word embeddings –whether similar words can help each other boost their polarity and attention scores."
2020.acl-main.312.txt,6 Conclusions,2020,there are some future directions that are worth exploring.
2020.acl-main.312.txt,6 Conclusions,2020,"we also extended to and empirically examined additive attention, multi-label classification and models with an affine input layer, and observed similar behaviors."
2020.acl-main.313.txt,5 Conclusion,2020,"in future work, we plan to extend r-men for multi-hop knowledge graph reasoning."
2020.acl-main.315.txt,4 Conclusion,2020,"furthermore, the word-aligned attention can also be applied to english plms to bridge the semantic gap between the whole word and the segmented word-piece tokens, which we leave for future work."
2020.acl-main.316.txt,7 Conclusions,2020,we couple the vae model with a deterministic network and improve the parameterizations via encoder weight sharing and decoder signal matching.
2020.acl-main.317.txt,5 Conclusion,2020,"an naı¨ve way is to add the “oov” token into the synonyms set of every word, but potentially better procedures can be further explored."
2020.acl-main.317.txt,5 Conclusion,2020,"compared with previous work such as jia et al.(2019); huang et al.(2019), our method is structure-free and thus can be easily applied to any pre-trained models (such as bert) and character-level models (such as char-cnn)."
2020.acl-main.317.txt,5 Conclusion,2020,"in further work, we will explore more efficient ways for constructing the perturbation set."
2020.acl-main.317.txt,5 Conclusion,2020,"we also plan to generalize our approach to achieve certified robustness against other types of adversarial attacks in nlp, such as the out-of-list attack."
2020.acl-main.318.txt,7 Conclusion,2020,"in the future, we will consider combining our method with graph neural networks to update the word graphs we build."
2020.acl-main.318.txt,7 Conclusion,2020,our method uses clique-level information and reduces the bad effect of noise in the pre-trained embeddings.
2020.acl-main.319.txt,7 Conclusion,2020,"as a result, the mass production of adversarial examples for the victim model’s analysis and further improvement of robustness become convenient."
2020.acl-main.319.txt,7 Conclusion,2020,"furthermore, we notice some exceptional cases which we call as “reinforced samples”, which we leave as the future work."
2020.acl-main.32.txt,5 Conclusion,2020,"besides, developing correlated topic modelsis another promising direction."
2020.acl-main.32.txt,5 Conclusion,2020,"in the future, we would like to devise a nonparametric neural topic model based on adversarial training."
2020.acl-main.323.txt,5 Conclusion,2020,"to improve the efficiency of our approach with large models, we propose a sampling approach to select gradients of parameters sensitive to the batch size."
2020.acl-main.324.txt,8 Conclusion and Future Work,2020,"in the future, we intend to extend the work to include language types such as asian languages."
2020.acl-main.324.txt,8 Conclusion and Future Work,2020,we will also introduce other effective methods to improve zero-shot translation quality.
2020.acl-main.325.txt,6 Conclusion,2020,our constraint insertion step is simple and we have empirically validated its effectiveness.
2020.acl-main.327.txt,6 Conclusion,2020,"in future work, we will work around the problem of evaluation errors in the low da range."
2020.acl-main.327.txt,6 Conclusion,2020,we also investigated why our proposed method worked poorly in the other conditions and found the importance of tlm training.
2020.acl-main.328.txt,9 Conclusions,2020,a practical line of future work is embedding our plotting agent in interactive environments such as jupyter lab.
2020.acl-main.329.txt,6 Conclusion,2020,"all the datasets used in the gluecos benchmark are publicly available, and we plan to make the nli dataset available for research use."
2020.acl-main.329.txt,6 Conclusion,2020,"in future work, we would like to experiment with a multi-task setup wherein tasks with less training data can significantly benefit from those having abundant labelled data, since most code-switched datasets are often small and difficult to annotate."
2020.acl-main.329.txt,6 Conclusion,2020,"in this work, we use standard architectures to solve each nlp task individually and vary the embeddings used."
2020.acl-main.329.txt,6 Conclusion,2020,"the benchmark contains datasets in english-hindi and english-spanish for six nlp tasks - lid, pos tagging, ner, sentiment analysis, question answering and a new code-switched natural language inference task."
2020.acl-main.329.txt,6 Conclusion,2020,"we hope that this will encourage researchers to test multilingual, cross-lingual and code-switched embedding techniques and models on this benchmark."
2020.acl-main.329.txt,6 Conclusion,2020,we would like to add more diverse tasks and language pairs to the gluecos benchmark in a future version.
2020.acl-main.33.txt,5 Conclusion,2020,"in future work, we will examine semantic relations between class labels in the auxiliary task."
2020.acl-main.33.txt,5 Conclusion,2020,"moreover, we will adapt our model to text generation tasks."
2020.acl-main.33.txt,5 Conclusion,2020,"we expect that our model will encourage a generation model to generate texts with different labels, such as styles, have distinct representations, which will result in class specific expressions."
2020.acl-main.331.txt,6 Conclusion and Discussion,2020,"besides news recommendation, the mind dataset can also be used in other natural language processing tasks such as topic classification, text summarization and news headline generation."
2020.acl-main.331.txt,6 Conclusion and Discussion,2020,"in addition, besides the click behaviors, we plan to incorporate other user behaviors such as read and engagement to support more accurate user modeling and performance evaluation."
2020.acl-main.331.txt,6 Conclusion and Discussion,2020,"in the future, we plan to extend the mind dataset by incorporating image and video information in news as well as news in different languages, which can support the research of multi-modal and multi-lingual news recommendation."
2020.acl-main.331.txt,6 Conclusion and Discussion,2020,"it contains 1 million users and more than 160k english news articles with rich textual content such as title, abstract and body."
2020.acl-main.332.txt,8 Conclusions and Future Work,2020,"in future work, we plan to extend this work to more datasets and to more languages."
2020.acl-main.332.txt,8 Conclusions and Future Work,2020,"we further want to go beyond textual claims, and to take claimimage and claim-video pairs as an input."
2020.acl-main.333.txt,6 Conclusion,2020,it is our hope the proposed holistic metrics may pave the way towards the comparability of open-domain dialogue models.
2020.acl-main.333.txt,6 Conclusion,2020,we recruit gpt-2 as a strong language model to evaluate the context coherency and response fluency.
2020.acl-main.334.txt,5 Conclusion and Future Work,2020,"future work includes i) improving projection learning to model complicated linguistic properties of hypernymy; ii) extending our model to address other tasks, such as graded lexical entailment (vulic et al., 2017) and cross-lingual graded lexical entailment (vulic et al., 2019); and iii) exploring how deep neural language models (such as bert (devlin et al., 2019), transformer-xl (dai et al., 2019), xlnet (yang et al., 2019)) can improve the performance of hypernymy detection."
2020.acl-main.335.txt,7 Conclusion,2020,"although the datasets used in our experiments are in english, we expect that our methodology would work in any language as long as there is a synonym dictionary for the language."
2020.acl-main.335.txt,7 Conclusion,2020,"for future work, an extrinsic evaluation of our methods is needed to prove the effectiveness of learned biomedical entity representations and to prove the quality of the entity normalization in downstream tasks."
2020.acl-main.337.txt,6 Conclusion and Future Work,2020,the investigation in this study leveraged only general-purpose word vectors to represent the meaning of a word.
2020.acl-main.337.txt,6 Conclusion and Future Work,2020,"thus, in the future, we will take the uncertainty, polysemy, and context sensitivity of the word meanings and the frequency of words into account and explore better ways of modeling the word-class distributions in semantic vector spaces."
2020.acl-main.338.txt,6 Conclusion,2020,"in our future work, we would like to improve the performance of the asc task by using unlabeled data since our graph-based neural network approach is easy to add unlabeled data."
2020.acl-main.338.txt,6 Conclusion,2020,"moreover, we would like to apply our approach to other sentiment analysis tasks, e.g., aspect-oriented opinion summarization and multi-label emotion detection."
2020.acl-main.339.txt,6 Conclusion,2020,"in the future, we will develop a syntax-based multi-scale graph convolutional network to deal with both short and long aspects."
2020.acl-main.339.txt,6 Conclusion,2020,our findings reveal that illustrative aspects in scientific literature are generally long-winded.
2020.acl-main.34.txt,6 Conclusion and Future Works,2020,"in future work, we will investigate the impact of fine-grained word categories (such as nouns, verbs, and adjectives) on the translation performance and design specific methods according to these categories."
2020.acl-main.341.txt,6 Conclusion,2020,"for future work, we will extend sentibert to other applications involving phrase-level annotations."
2020.acl-main.342.txt,7 Conclusion,2020,"besides, graph neural network-based (kipf and welling, 2016) methods are also worth investigating to model the relations among nodes for this task."
2020.acl-main.342.txt,7 Conclusion,2020,"in the future, one possible direction is creating complete graphs with their nodes being input clauses to achieve full coverage."
2020.acl-main.343.txt,7 Conclusion,2020,"in the future, we will further explore the connection between multimodal analysis and multi-task learning and incorporate more fusion strategy, including early- and middle-fusion."
2020.acl-main.343.txt,7 Conclusion,2020,"the unified multimodal annotations may mislead the model to learn inherent characteristics of unimodal representations.• with the help of unimodal annotations, models can learn more differentiated information and improve the complementarity between modalities.• when performing multi-task learning, the asynchrony of learning in different subtasks may cause an adverse effect on multimodal sentiment analysis."
2020.acl-main.343.txt,7 Conclusion,2020,we hope that the introduction of ch-sims will provide a new perspective for researches on multi-modal analysis.
2020.acl-main.344.txt,5 Conclusion and Future Work,2020,"besides, we expect the idea of curriculum pre-training can be adopted on other nlp tasks."
2020.acl-main.344.txt,5 Conclusion and Future Work,2020,"in the future, we will explore how to leverage unlabeled speech data and large bilingual text data to further improve the performance."
2020.acl-main.345.txt,8 Summary,2020,"the insights we gleaned from this investigation provide hints on how we could potentially adapt such end-to-end asr models, using auxiliary losses, to be robust to variations across accents."
2020.acl-main.345.txt,8 Summary,2020,we will investigate this direction in future work.
2020.acl-main.346.txt,6 Conclusion,2020,"in future work, we intend to explore the idea of self-training for parsing written texts."
2020.acl-main.346.txt,6 Conclusion,2020,"the first step is to develop parsing models that parse asr output, rather than speech transcripts."
2020.acl-main.346.txt,6 Conclusion,2020,we also aim at integrating syntactic parsing and self-training more closely with automatic speech recognition.
2020.acl-main.348.txt,6 Conclusion,2020,"finally, we will explore further the generability of our meta-transfer learning approach to more downstream multilingual tasks in our future work."
2020.acl-main.348.txt,6 Conclusion,2020,our model recognizes individual languages and transfers them so as to better recognize mixed-language speech by conditioning the optimization objective to the code-switching domain.
2020.acl-main.349.txt,5 Conclusion,2020,"to model the cross-modality contrast in the associated context of multimodal sarcastic tweets, we propose the d&r net to represent the commonality and discrepancy between image and text and multi-view semantic associations in cross-modality context."
2020.acl-main.350.txt,7 Conclusion,2020,"for future work, we will design more flexible policies to achieve better translation quality and lower delay in simultaneous spoken language translation."
2020.acl-main.350.txt,7 Conclusion,2020,we will also investigate simultaneous translation from the speech in a source language to the speech in a target.
2020.acl-main.351.txt,7 Future work,2020,"we plan to do a detailed analysis along two lines: 1) comparing if the proposed modeling technique can help bridge gap between predicted and human annotations, and 2) effect of environment variables e.g., background noise, speaker features, different languages etc."
2020.acl-main.352.txt,5 Conclusion,2020,"furthermore, visualisation of the topics and attention signals shows that ntom captures the dynamics in the focused topics and contextual attention."
2020.acl-main.353.txt,7 Conclusion,2020,an exciting synthesis would incorporate deception and language generation into an agent’s policy; our data would help train such agents.
2020.acl-main.353.txt,7 Conclusion,2020,"likewise, our lie-detection models can help a user in the moment better decide whether they are being deceived (lai et al., 2020)."
2020.acl-main.353.txt,7 Conclusion,2020,"the seeds of players’ negotiations and deceit could, we hope, yield fruit to help others: understanding multi-party negotiation and protecting internet users."
2020.acl-main.355.txt,6 Conclusions and Future Work,2020,"as a potential direction for future work, it would be interesting to investigate the use of the ema technique on transformer models as well and conduct similar studies to examine needless architectural complexity in other nlp tasks."
2020.acl-main.356.txt,5 Conclusion and Future Work,2020,"in the future, we plan to extend the applicability of the presented model to other linguistics tasks as well as recommendations and medical inference tasks."
2020.acl-main.357.txt,6 Conclusions,2020,future works include applying such scoring method on broader classification tasks like natural language inference and sentiment analysis.
2020.acl-main.357.txt,6 Conclusions,2020,"we also think that our token-level scoring method could be used during the self-supervised pretraining phase to extend traditional next sentence prediction and sequence ordering tasks, bringing more commonsense knowledge in the model."
2020.acl-main.358.txt,5 Conclusion and Future Work,2020,"in the future, we plan to extend the key insights of segmenting features and facilitating interactions to other representation learning problems."
2020.acl-main.359.txt,7 Conclusions and Future Work,2020,"finally, we intend to analyse the effect of these measures in a wider range of language pairs and settings, in order to propose a more general solution."
2020.acl-main.359.txt,7 Conclusions and Future Work,2020,"further investigation is required to study under which conditions our proposed rescoring method is beneficial, but our experiments with both low- and high-resource language pairs suggest that if the systems used for backtranslation are poor, then this technique will be of little value; clearly this is closely related to the amount of resources available for the language pair under study."
2020.acl-main.359.txt,7 Conclusions and Future Work,2020,"in the future, we plan to investigate ways to directly incorporate the rescoring metrics into the data selection process itself, so that penalising similar sentences can also be taken into account."
2020.acl-main.359.txt,7 Conclusions and Future Work,2020,we also aim to conduct a human evaluation of the translated sentences in order to obtain a better understanding of the effects of data selection and backtranslation on the overall quality.
2020.acl-main.36.txt,6 Conclusion,2020,"in the future, we will extend the investigation on the functionalities of the encoder and decoder to other sequence-to-sequence tasks such as text summarization and text style transfer to explore more applications of our model."
2020.acl-main.360.txt,6 Conclusion,2020,"in future work, we suggest performing a hyperparameter search over possible values for t in slt pruning (i.e., the number of training steps that are not discarded during model reset), and over si for the switch from slt to mp in slt-mp."
2020.acl-main.360.txt,6 Conclusion,2020,"in slt-mp, slt pruning first discards 60% of all parameters, so mp can focus on fine-tuning the model for maximum accuracy."
2020.acl-main.361.txt,5 Conclusion and Future Work,2020,"as future work, we plan to extend our method to other nlp tasks which rely on evidence finding, such as natural language inference."
2020.acl-main.362.txt,6 Conclusion,2020,"for future work, we aim to consider more complex relationships among the quantities and other attributes to enrich quantity representations further."
2020.acl-main.362.txt,6 Conclusion,2020,we will also explore adding heuristic in the tree-based decoder to guide and improve the generation of solution expression.
2020.acl-main.363.txt,6 Conclusions,2020,future work includes the application of cem at scales other than the ordinal.
2020.acl-main.364.txt,6 Conclusion,2020,"to showcase the general applicability of adacomp, we conduct four different nlp tasks, which are sentence classification, chunking, natural language inference, and language modeling."
2020.acl-main.364.txt,6 Conclusion,2020,we leave this as an avenue for future work.
2020.acl-main.365.txt,7 Conclusion,2020,"in recent work, we have experimented with alternative ways of obtaining usage representations (using a different language model, fine-tuning, and various layer selection strategies) and we have obtained very promising results in detecting semantic change across four languages (kutuzov and giulianelli, 2020)."
2020.acl-main.365.txt,7 Conclusion,2020,"in the future, we plan to investigate whether usage representations can provide an even finer grained account of lexical meaning and its dynamics, e.g., to automatically discriminate between different types of meaning change."
2020.acl-main.365.txt,7 Conclusion,2020,we expect our work to inspire further analyses of variation and change which exploit the expressiveness of contextualised word representations.
2020.acl-main.367.txt,5 Conclusion,2020,this points to the usefulness of building semantic structure into the model.
2020.acl-main.368.txt,6 Conclusion,2020,"furthermore, it would be interesting to explore more complex ways of incorporating surface-form information – e.g., by using a character-level cnn similar to the one of kim et al.(2016) – to balance out the potency of bertram’s form and context parts."
2020.acl-main.368.txt,6 Conclusion,2020,"in future work, we want to investigate bertram’s potential benefits for such frequent words."
2020.acl-main.369.txt,7 Conclusions,2020,"as future work, we plan to refine our approach by exploiting other strategies for weighting the words in the clusters and to leverage them for automatically building multilingual sense-tagged corpora."
2020.acl-main.369.txt,7 Conclusions,2020,"in this paper we presented clubert, an automatic multilingual approach which induces the distribution of word senses in an arbitrary input corpus by exploiting the contextual information coming from bert and the lexical-semantic knowledge available in babelnet."
2020.acl-main.37.txt,5 Conclusion,2020,"in addition, we incorporate the generated phrase representations into the transformer translation model to help it capture long-distance relationships."
2020.acl-main.370.txt,5 Conclusion and Future Work,2020,"in the future, we would like to investigate the application of our theory in these domain adaptation tasks."
2020.acl-main.370.txt,5 Conclusion and Future Work,2020,our purpose is to inject the target domain knowledge to bert and encourage bert to be domain-aware.
2020.acl-main.370.txt,5 Conclusion and Future Work,2020,"the proposed post-training procedure could also be applied to other domain adaptation scenarios such as named entity recognition, question answering, and reading comprehension."
2020.acl-main.371.txt,5 Conclusion,2020,"finally, detecting the more implicit relations between the argument and the key point, as seen in our error analysis, is another intriguing direction for future work."
2020.acl-main.371.txt,5 Conclusion,2020,"in addition, we plan to apply the methods presented in this work also to automatically-mined arguments."
2020.acl-main.371.txt,5 Conclusion,2020,the natural next step for this work is the challenging task of automatic key point generation.
2020.acl-main.372.txt,7 Conclusion,2020,data disclaimer: we are aware that the dataset contains biases and is not representative of global diversity.
2020.acl-main.372.txt,7 Conclusion,2020,"future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains."
2020.acl-main.372.txt,7 Conclusion,2020,we are aware that the dataset contains potentially problematic content.
2020.acl-main.372.txt,7 Conclusion,2020,"we build a strong baseline by fine-tuning a bert model, however, the results suggest much room for future improvement."
2020.acl-main.373.txt,7 Conclusion,2020,"in the future, we plan to develop more complex models to be added in the next stages of the cascade classifier as well as automatically identify irony, gender stereotypes and sexist vocabulary."
2020.acl-main.374.txt,7 Conclusion,2020,"in the future, we hope to apply skep on more sentiment analysis tasks, to further see the generalization of skep, and we are also interested in exploiting more types of sentiment knowledge and more fine-grained sentiment mining methods."
2020.acl-main.374.txt,7 Conclusion,2020,"our work verifies the necessity of utilizing sentiment knowledge for pre-training models, and provides a unified sentiment representation for a wide range of sentiment analysis tasks."
2020.acl-main.375.txt,7 Conclusion and Future Work,2020,"for future work, besides seeking a deeper understanding of the interplay of linguistic factors and tree shape, we want to explore probes that combine the distance and depth assumptions into a single transformation, rather than learning separate probes and combining them post-hoc, as well as methods for alleviating treebank supervision altogether."
2020.acl-main.375.txt,7 Conclusion and Future Work,2020,"lastly, given recent criticisms of probing approaches in nlp, it will be vital to revisit the insights produced here within a non-probing framework, for example, using representational similarity analysis (rsa) (chrupała and alishahi, 2019) over symbolic representations from treebanks and their encoded representations."
2020.acl-main.376.txt,5 Conclusion,2020,"in addition, any advances in seq2seq neural architectures or pretrained transformer-based language models (devlin et al., 2019) can be directly used to enhance our approach."
2020.acl-main.378.txt,7 Conclusion and Future work,2020,"in principle, they could be applied to any task involving sequential structure prediction."
2020.acl-main.378.txt,7 Conclusion and Future work,2020,"such parsers can potentially make much more informed decisions about the next word, compared to the models based on mere sequence of words prefix, by including semantic and referential meaning (altmann and steedman, 1988), as well as syntax."
2020.acl-main.38.txt,5 Conclusion,2020,we also study the effects of deep decoders in addition to deep encoders extending previous works.
2020.acl-main.38.txt,5 Conclusion,2020,"we conjecture that the convergence issue of deep transformers is because layer normalization sometimes shrinks residual connections, we support our conjecture with a theoretical analysis (table 2), and propose a lipschitz constrained parameter initialization approach for solving this problem."
2020.acl-main.38.txt,5 Conclusion,2020,"we first investigate convergence differences between the published transformer (vaswani et al., 2017) and its official implementation (vaswani et al., 2018), and compare the differences of computation orders between them."
2020.acl-main.380.txt,6 Conclusion,2020,"it’s worth mentioning that our method is general enough to be applied to other tasks, as the key idea is to obtain the loss on the non-discrimination distribution, and we leave this to future works."
2020.acl-main.381.txt,5 Conclusion,2020,in future we will sample target models with a larger number of plausible combinations of factors.
2020.acl-main.381.txt,5 Conclusion,2020,in future work we hope to further disentangle these differences.
2020.acl-main.382.txt,5 Summary and Outlook,2020,"future work will focus on developing more advanced procedures for detecting inconsistencies, and on building robust models that do not generate inconsistencies."
2020.acl-main.383.txt,8 Discussion & Conclusion,2020,"in summary, we propose a parameter-free probing technique to complement current line of work on interpreting bert through probes."
2020.acl-main.383.txt,8 Discussion & Conclusion,2020,"this matrix mirrors the function of attention mechanism that captures inter-word correlations, except that it emerges through the output of bert model, instead of from intermediate representations."
2020.acl-main.383.txt,8 Discussion & Conclusion,2020,"we also extend our method to probe document structure, which sheds lights on bert’s effectiveness in modeling long sequences."
2020.acl-main.383.txt,8 Discussion & Conclusion,2020,"we leave it for future work to use our technique to test other linguistic properties (e.g., coreference) and to extend our study to more downstream tasks and systems."
2020.acl-main.384.txt,5 Conclusion,2020,"future work should investigate where these primitive referential abilities stem from and how they can be fostered in future architectures and training setups for language modeling, and neural models more generally."
2020.acl-main.384.txt,5 Conclusion,2020,"we find that the two models behave similarly, but the transformer performs consistently better (around 10% higher accuracy in the probe tasks).8 future work should test other architectures, like cnn-based lms and lstms with attention, to provide additional insights into the linguistic capabilities of language models."
2020.acl-main.384.txt,5 Conclusion,2020,"we have tested two models representative of the prevailing architectures (transformer, lstm), and our methodology can be extended to any other architecture."
2020.acl-main.385.txt,5 Conclusion,2020,"following this work, we can build the attention graph with effective attention weights (brunner et al., 2020) instead of raw attentions."
2020.acl-main.386.txt,9 Conclusion,2020,"second, faithfulness is often evaluated in a binary “faithful or not faithful” manner, and we believe strictly faithful interpretation is a “unicorn” which will likely never be found."
2020.acl-main.387.txt,7 Conclusion & Future work,2020,"as future work, we would like to extend our analysis and proposed techniques to more complex models and downstream tasks."
2020.acl-main.388.txt,6 Conclusion,2020,"in the future, it would be fruitful to develop a novel weighting strategy for the tchebycheff procedure."
2020.acl-main.389.txt,7 Conclusion,2020,"the findings of our experiments provide important insights for translating morphologically rich languages, and are particularly important for low-resource settings."
2020.acl-main.39.txt,8 Discussion,2020,it would also be interesting to test such attention mechanisms in self-attentive seq2seq models without recurrence.
2020.acl-main.39.txt,8 Discussion,2020,"this is a bottleneck for extrapolation, suggesting that removing this heuristic is key to reaching perfect extrapolation and should be investigated in future work."
2020.acl-main.390.txt,7 Conclusion,2020,additional experiments with an adaptive λ for our trade-off sampling strategy show that properly balancing system and user objective can lead to considerable improvements in performance for both objectives.
2020.acl-main.390.txt,7 Conclusion,2020,additional use cases like the training of personalized recommendation models as well as the use of reinforcement learning to find a good trade-off between system and user objective remain to be investigated in future work.
2020.acl-main.390.txt,7 Conclusion,2020,"in this work, we investigated how we can incorporate user feedback into existing active learning approaches without hurting the user’s actual needs."
2020.acl-main.391.txt,7 Conclusion,2020,"in future work, we will investigate whether bert-init can be used effectively by using methods to deal with catastrophic forgetting."
2020.acl-main.393.txt,7 Conclusion,2020,we hope to address this problem with a completely semantic-based approach in the future.
2020.acl-main.394.txt,7 Conclusion,2020,"for instance, we expect that abuse detection may also benefit from joint learning with complex semantic tasks, such as figurative language processing and inference."
2020.acl-main.394.txt,7 Conclusion,2020,"the mutually beneficial relationship that exists between these two tasks opens new research avenues for improvement of abuse detection systems in other domains as well, where emotion would equally play a role."
2020.acl-main.395.txt,5 Conclusion,2020,future adoptions to fuse will include the integration of a dialog component.
2020.acl-main.395.txt,5 Conclusion,2020,giving feedback to newly learned method definitions that may be lengthy and therefore unhandy to repeat as a whole is an interesting challenge.
2020.acl-main.395.txt,5 Conclusion,2020,"it will be interesting to see, if we can reuse (or transfer) the machine learning models as well as the rest of the approach."
2020.acl-main.395.txt,5 Conclusion,2020,"more precisely, we aim to enable laypersons to teach an intelligent system new functionality with nothing but spoken instructions.our approach is three-tiered."
2020.acl-main.395.txt,5 Conclusion,2020,we plan to evaluate fuse in other domains.
2020.acl-main.395.txt,5 Conclusion,2020,we will also implement a sanity check that considers feasibility and meaningfulness of the sequence of actions in the method body.
2020.acl-main.396.txt,4 Conclusions and Future Work,2020,"a limitation of our work is that we considered a narrow contextual context, comprising only the previous comment and the discussion title.11 it would be interesting to investigate in future work ways to improve the annotation quality when more comments in the discussion thread are provided, and also if our findings hold when broader context is considered (e.g., all previous comments in the thread, or the topic of the thread as represented by a topic model)."
2020.acl-main.396.txt,4 Conclusions and Future Work,2020,our experiments and datasets provide an initial foundation to investigate these important directions.
2020.acl-main.397.txt,6 Conclusion,2020,"we also propose to incorporate the latent graph into other multi-task learning problems (chen et al., 2019; kurita and søgaard, 2019)."
2020.acl-main.398.txt,7 Conclusion,2020,"in future work we aim to extend the model to represent a database with multiple tables as context, and to effectively handle large tables."
2020.acl-main.399.txt,7 Conclusion,2020,a more elaborate phrasing approach may take over context information from the premises.
2020.acl-main.399.txt,7 Conclusion,2020,"combining target inference with stance classification in future work, we can already generate basic conclusions, say, “raising the school leaving age is good”."
2020.acl-main.40.txt,6 Conclusion and Future Work,2020,"in the future, we would like to extend our model to extremely large datasets, such as wmt’14 english-to-french with about 36m sentence-pairs."
2020.acl-main.400.txt,4 Conclusion,2020,"in future work, we would like to investigate the effectiveness of our model in these tasks."
2020.acl-main.401.txt,6 Conclusion,2020,"along with investigating new techniques, we hope that assembling a bigger curated dataset with quality annotations will help in better performance."
2020.acl-main.402.txt,6 Conclusion and Future Work,2020,"in future, conversation history, speaker information, fine-grained modality encodings can be incorporated to predict da with more accuracy and precision."
2020.acl-main.403.txt,7 Conclusion,2020,"in the future, we plan to study more in depth the stylistic and figurative devices used for parody, extend the data set beyond the political case study and explore human behavior regarding parody, including how this is detected and diffused through social media."
2020.acl-main.404.txt,6 Conclusion,2020,"since frequency is known to be strongly correlated with performance in machine learning-based nlp, such biases should be investigated more systematically in areas building on nlp such as computational social sciences."
2020.acl-main.404.txt,6 Conclusion,2020,"to remove these biases, however, presumably more sophisticated methods will be necessarily in the general case."
2020.acl-main.404.txt,6 Conclusion,2020,"while we only evaluated the strategy on one model, we believe its benefits carry over to other model architectures and similar tasks."
2020.acl-main.406.txt,7 Conclusion and Future Work,2020,"for example, the edge labels, indicating the evolution operators of a claim should also be useful."
2020.acl-main.406.txt,7 Conclusion and Future Work,2020,"in particular, this will support a more informed study of influence of specific sources and of trustworthiness, and possibly other aspects of information spread."
2020.acl-main.406.txt,7 Conclusion and Future Work,2020,"this could be difficult when the authors are not mentioned in the text, which might require a deeper understanding of sources’ writing style and positions."
2020.acl-main.407.txt,7 Discussion,2020,a thorough investigation of neural network codes that can generalize while being partially entangled might shed light on similar phenomena in human languages.
2020.acl-main.407.txt,7 Discussion,2020,"as we argued that compositionality has, after all, desirable properties, future work could adapt methods for learning disentangled representations (e.g., higgins et al., 2017; kim and mnih, 2018) to let (more) compositional languages emerge."
2020.acl-main.407.txt,7 Discussion,2020,"compositionality might act like a “dominant” genetic feature: it might arise by a random mutation but, once present, it will survive and thrive, as it guarantees that languages possessing it will generalize and will be easier to learn."
2020.acl-main.407.txt,7 Discussion,2020,"it is thus worth exploring the link between the area of language emergence and that of representation learning (bengio et al., 2013)."
2020.acl-main.407.txt,7 Discussion,2020,"our result might also speak to those linguists who are exploring the non-fully-compositional corners of natural language (e.g., goldberg, 2019)."
2020.acl-main.408.txt,7 Conclusions and Future Directions,2020,eraser is intended to facilitate progress on explainable models for nlp.
2020.acl-main.408.txt,7 Conclusions and Future Directions,2020,"it also serves as an ideal starting point for several future directions such as better evaluation metrics for interpretability, causal analysis of nlp models and datasets of rationales in other languages."
2020.acl-main.408.txt,7 Conclusions and Future Directions,2020,"our hope is that eraser enables future work on designing more interpretable nlp models, and comparing their relative strengths across a variety of tasks, datasets, and desired criteria."
2020.acl-main.408.txt,7 Conclusions and Future Directions,2020,"we believe these metrics provide reasonable means of comparison of specific aspects of interpretability, but we view the problem of measuring faithfulness, in particular, a topic ripe for additional research (which eraser can facilitate)."
2020.acl-main.409.txt,9 Conclusions,2020,"our method can be used with any feature importance metric, is very simple to implement and train, and empirically often outperforms more complex rationalized models."
2020.acl-main.409.txt,9 Conclusions,2020,we view these as interesting directions for future work.
2020.acl-main.410.txt,7 Conclusion,2020,"moreover, questions that involve complex relations and are across different domains should be included, and then more advanced external knowledge incorporation methods as well as domain adaptation methods can be carefully designed and systematically evaluated."
2020.acl-main.410.txt,7 Conclusion,2020,our qualitative and quantitative analysis as well as exploration of the two desired aspects of clinirc systems show that future clinical qa datasets should not only be large-scale but also less noisy and more diverse.
2020.acl-main.411.txt,6 Conclusion,2020,the distillation techniques further reduce the performance gap with respect to the original model.
2020.acl-main.413.txt,4 Conclusion,2020,"we aim to experiment with other datasets and other domains, incorporate our synthetic data in a semi-supervised setting and test the feasibility of our framework in a multi-lingual setting."
2020.acl-main.414.txt,6 Conclusion,2020,"in addition of improving qa, we hypothesize that these simple unsupervised components of air will benefit future work on supervised neural iterative retrieval approaches by improving their query reformulation algorithms and termination criteria."
2020.acl-main.415.txt,6 Conclusion,2020,"nonetheless, we hope that directly releasing our alignments and token-level features enables greater research accessibility in this area."
2020.acl-main.415.txt,6 Conclusion,2020,we hope this corpus will motivate and enable further developments in both phonetic typology and methodology for working with cross-linguistic speech corpora.
2020.acl-main.417.txt,11 Conclusions,2020,"each of the processing steps we describe here still have great potential for improvement, and we hope that our work contributes to the development of novel methods both in terms of better processing of raw parallel data sources, but also increasing the robustness of neural machine translation training when faced with noisy data."
2020.acl-main.417.txt,11 Conclusions,2020,we are especially interested in further extending this work into low resource languages where resources tend to be noisier and underlying models to support data mining less reliable.
2020.acl-main.418.txt,6 Discussion and Moving Forward,2020,"finally, because the social construct of gender is fundamentally contested, some of our results may apply only under some frameworks."
2020.acl-main.418.txt,6 Discussion and Moving Forward,2020,"in particular, the gender taxonomy we presented, while not novel, is (to our knowledge) previously unattested in discussions around gender bias in nlp systems; we hope future work in this area can draw on these ideas."
2020.acl-main.418.txt,6 Discussion and Moving Forward,2020,"it may be the case that to truly build gender inclusive datasets and systems, we need to hire or consult experiential experts (patton et al., 2019; young et al., 2019)."
2020.acl-main.418.txt,6 Discussion and Moving Forward,2020,"more broadly, we found that trans-exclusionary assumptions around gender in nlp papers is made commonly (and implicitly), a practice that we hope to see change in the future because it fundamentally limits the applicability of nlp systems."
2020.acl-main.418.txt,6 Discussion and Moving Forward,2020,"this is particularly limiting because english lacks a grammatical gender system, and some extensions of our work to languages with grammatical gender are non-trivial."
2020.acl-main.418.txt,6 Discussion and Moving Forward,2020,"this led us to counterfactual text manipulation, which, while useful, is essentially impossible to do flawlessly."
2020.acl-main.418.txt,6 Discussion and Moving Forward,2020,we also hope that developers of datasets or systems can use some of our analysis as inspiration for how one can attempt to measure—and then root out—different forms of bias in coreference resolution systems and nlp systems more broadly.
2020.acl-main.418.txt,6 Discussion and Moving Forward,2020,we hope this paper can serve as a roadmap for future studies.
2020.acl-main.419.txt,8 Conclusion,2020,"our research not only results in insights into significant similarities between bidirectional rnns and human attention, but also opens the avenue for promising future research directions."
2020.acl-main.420.txt,7 Conclusion,2020,"on a final note, we apply our formalization to evaluate multilingual bert’s syntactic knowledge on a set of eleven typologically diverse languages."
2020.acl-main.421.txt,7 Conclusions,2020,"to provide a more comprehensive benchmark to evaluate cross-lingual models, we also released the cross-lingual question answering dataset (xquad)."
2020.acl-main.422.txt,7 Conclusion,2020,"an exciting direction for future work is to combine the two approaches in order to identify which linguistic properties are captured in model components that are similar to one another, or explicate how localization of information contributes to the learnability of particular properties."
2020.acl-main.422.txt,7 Conclusion,2020,one could also study whether a high similarity entail that two models converged to a similar solution.
2020.acl-main.422.txt,7 Conclusion,2020,"our localization score can also be compared to other aspects of neural representations, such as gradient distributions and their relation to memorization/generalization (arpit et al., 2017)."
2020.acl-main.423.txt,6 Conclusion,2020,we introduce lexical semantic information into a neural language model’s pre-training objective.
2020.acl-main.424.txt,7 Conclusion,2020,"finally, we hope that asset’s multi-transformation features will motivate the development of ss models that benefit a variety of target audiences according to their specific needs such as people with low literacy or cognitive disabilities."
2020.acl-main.426.txt,8 Conclusions,2020,the multi-label nature of emotion prediction lends itself naturally to use the correlations between the labels themselves.
2020.acl-main.426.txt,8 Conclusions,2020,we believe these models can improve many other nlp tasks where the class labels carry inherent semantic meaning in their names.
2020.acl-main.427.txt,7 Conclusion,2020,"we believe this data will be useful to researchers studying semantic parsing, especially interactive semantic parsing, human-robot interaction, and even imitation and reinforcement learning."
2020.acl-main.428.txt,5 Conclusion,2020,"future work could apply this same technique with other supervised data, e.g.correcting causal or commonsense reasoning errors (zellers et al., 2019; qin et al., 2019)."
2020.acl-main.43.txt,6 Conclusion,2020,we hope they will guide the development of more expressive rational rnns.
2020.acl-main.430.txt,5 Conclusions,2020,"investigating longer contexts, the diminishing dominance of the primary path, and the requisite algorithmic scalability requirements are elements of our ongoing work."
2020.acl-main.430.txt,5 Conclusions,2020,"we also note that our method can be expanded to explore number agreement in more complicated sentences with clausal structures, or other syntactic/semantic signals such as coreference or gender agreement."
2020.acl-main.431.txt,9 Conclusion,2020,"we further study the extent to which various social biases (gender, race, religion) are encoded, employing several different quantification schemas."
2020.acl-main.433.txt,6 Conclusion,2020,"future work will explore other measures and alternative game settings for the emergence of compositionality, as well as more subtle psychological effects (categeorical perception) of continuous biological systems exhibiting discrete structure, like the auditory system."
2020.acl-main.434.txt,10 Conclusion,2020,"overall, these results help to clarify the patterns of distribution of context information within contextual embeddings— future work can further clarify the impact of more diverse syntactic relations between words, and of additional types of word features."
2020.acl-main.434.txt,10 Conclusion,2020,"we apply these tests to examine the distribution of contextual information across sentence tokens for popular contextual encoders bert, elmo, and gpt."
2020.acl-main.438.txt,8 Conclusions,2020,"the learned constraints can be used for structured prediction problems in two ways: (1) combining them with an existing model to improve prediction performance, or (2) incorporating them into the training process to train a better model."
2020.acl-main.438.txt,8 Conclusions,2020,we presented a systematic way for discovering constraints as linear inequalities for structured prediction problems.
2020.acl-main.439.txt,5 Discussion,2020,future work should explore the extent to which our model could further benefit from initializing with stronger models and what computational challenges may arise.
2020.acl-main.439.txt,5 Discussion,2020,"in future work, we hope to better understand how a discourse model can also learn fine-grained relationship types between sentences from unlabeled data."
2020.acl-main.44.txt,4 Conclusion and Future Work,2020,"a more general form, f ∝ ∏k(r + γk)−βk , can be considered for further investigation."
2020.acl-main.440.txt,8 Conclusion,2020,"diy (do-it-yourself) videos and websites, for instance, are an obvious next target."
2020.acl-main.440.txt,8 Conclusion,2020,"ultimately, we believe this work will further the goal of building agents that can work with human collaborators to carry out complex tasks in the real world."
2020.acl-main.440.txt,8 Conclusion,2020,we also envision extending this work by including audio and video features to enhance the quality of our alignment algorithm.
2020.acl-main.441.txt,7 Discussion & Conclusion,2020,"annotators were employed to act as adversaries, and encouraged to find vulnerabilities that fool the model into misclassifying, but that another person would correctly classify."
2020.acl-main.441.txt,7 Discussion & Conclusion,2020,future work could explore a detailed cost and time trade-off between adversarial and static collection.
2020.acl-main.441.txt,7 Discussion & Conclusion,2020,"hamlet was applied against an ensemble of models in rounds 2 and 3, and it would be straightforward to put more diverse ensembles in the loop to examine what happens when annotators are confronted with a wider variety of architectures."
2020.acl-main.441.txt,7 Discussion & Conclusion,2020,our finding that adversarial data is more data-efficient corroborates this theory.
2020.acl-main.441.txt,7 Discussion & Conclusion,2020,the dataset also presents many opportunities for further study.
2020.acl-main.441.txt,7 Discussion & Conclusion,2020,"we provided inference labels for the development set, opening up possibilities for interesting more fine-grained studies of nli model performance."
2020.acl-main.442.txt,6 Conclusion,2020,"more importantly, the abstractions and tools in checklist can be used to collectively create more exhaustive test suites for a variety of tasks."
2020.acl-main.443.txt,7 Conclusion,2020,"in this work, we investigated the task of named entity recognition in the social computer programming domain."
2020.acl-main.443.txt,7 Conclusion,2020,"we believe our corpus, stackoverflow-specific bert embeddings and named entity tagger will be useful for various language-and-code tasks, such as code retrieval, software knowledge base extraction and automated question-answering."
2020.acl-main.444.txt,7 Conclusions,2020,"in the future, we are interested in investigating the generality of our defined schema for other comedies and different conversational registers, identifying the temporal intervals when relations are valid (surdeanu, 2013) in a dialogue, and joint dialogue-based information extraction as well as its potential combinations with multimodal signals from images, speech, and videos."
2020.acl-main.445.txt,6 Conclusion and Future Work,2020,"in the future, we will investigate multi-document summarization datasets such as duc (paul and james, 2004) and tac (dang and owczarzak, 2008) to see whether our findings coincide when multiple references are provided."
2020.acl-main.445.txt,6 Conclusion and Future Work,2020,we also evaluate sentence regression approaches and explore the feasibility of fully-automatic evaluation without any human annotation.
2020.acl-main.445.txt,6 Conclusion and Future Work,2020,we will also explore better sentence regression approaches for the use of both extractive summarization methods and automatic fam creation.
2020.acl-main.446.txt,6 Conclusion,2020,"working under the same assumption that a subset of participants produce diverse data compared to the corpus, our method can be extended to other diversity measures and can be modified to work with other corpus-level metrics."
2020.acl-main.447.txt,8 Conclusion,2020,we aggregate metadata and abstracts from hundreds of trusted sources.
2020.acl-main.448.txt,6 Conclusion,2020,"any single number is not adequate to describe the data, and visualising metric scores against human scores is the best way to gain insights into metric reliability."
2020.acl-main.448.txt,6 Conclusion,2020,"thus, future evaluations should also measure correlations after removing outlier systems."
2020.acl-main.449.txt,5 Conclusion,2020,"in our future work, we want to study the effective incorporation of code structure into the transformer and apply the techniques in other software engineering sequence generation tasks (e.g., commit message generation for source code changes)."
2020.acl-main.450.txt,9 Conclusion,2020,"additionally, incorporating a content selection mechanism to focus the generated questions on salient facts is a promising direction."
2020.acl-main.450.txt,9 Conclusion,2020,"the framework we present is general, and extending it to other conditional text generation tasks such as image captioning or machine translation is a promising directions."
2020.acl-main.450.txt,9 Conclusion,2020,"we introduce a framework for automatically detecting factual inconsistencies in conditionally generated texts and use this framework to develop qags, a metric for measuring inconsistencies in abstractive summarization."
2020.acl-main.451.txt,6 Conclusion,2020,"for future work, we will explore better graph encoding methods, and apply discourse graphs to other tasks that require long document encoding."
2020.acl-main.453.txt,6 Discussion and Conclusion,2020,"in future work, we plan to experiment more with this, examining how we can combine constituents to make fluent sentences without including potentially irrelevant context."
2020.acl-main.453.txt,6 Discussion and Conclusion,2020,"we hypothesize that because we use rouge to score summaries of extracted constituents without context, the selected content is packed into the word budget; there is no potentially irrelevant context to count against the system."
2020.acl-main.453.txt,6 Discussion and Conclusion,2020,"we would also like to further experiment with abstractive summarization to re-examine whether large, pre-trained language models (liu and lapata, 2019) can be improved for our domain."
2020.acl-main.455.txt,7 Conclusions and Future Work,2020,"in the future, we intend to investigate different meaning representation formalisms, such as amr (banarescu et al., 2013) and dynamic syntax (kempson et al., 2001) and extend to other datasets (e.g.multiplereference summarization) and tasks (e.g.response generation in dialogue)."
2020.acl-main.458.txt,8 Conclusion,2020,we hope that our work draws the community’s attention to the factual correctness issue of abstractive summarization models and inspires future work in this direction.
2020.acl-main.459.txt,6 Conclusion and Future Work,2020,"we also hope that the dataset can be added to in the future with multi-modal extractions, more granular annotations, and deeper mining of the wiki."
2020.acl-main.459.txt,6 Conclusion and Future Work,2020,"we hope crd3 offers useful, unique data for the community to further explore dialogue modeling and summarization."
2020.acl-main.46.txt,5 Conclusion and Future Work,2020,"at present, we are working on an incremental construction of the space of categories."
2020.acl-main.46.txt,5 Conclusion and Future Work,2020,"in that case, two types of surface realization need to be considered: word order and morphological markers."
2020.acl-main.46.txt,5 Conclusion and Future Work,2020,the adaptation to other types of morphological markers will necessitate more elaborate linguistic reflection.
2020.acl-main.46.txt,5 Conclusion and Future Work,2020,the current formulation of our syntax learning scheme needs adjustments in order to be applicable to real natural language corpora.
2020.acl-main.46.txt,5 Conclusion and Future Work,2020,the second direction is towards extending the approach to morphologically rich languages.
2020.acl-main.46.txt,5 Conclusion and Future Work,2020,two orientations can be identified for future work.
2020.acl-main.462.txt,7 Conclusion,2020,we hope that a deeper appreciation of the role of construal in language use will spur progress toward systems that more closely approximate human linguistic intelligence.
2020.acl-main.463.txt,10 Conclusion,2020,with this we hope to encourage a top-down perspective on our field which we think will help us select the right hill to climb towards human-analogous nlu.
2020.acl-main.464.txt,6 Conclusions,2020,a crucial direction of future work is to develop richer ways of capturing scholarly impact.
2020.acl-main.464.txt,6 Conclusions,2020,"the analyses presented here, and the associated dataset of papers mapped to citations, have a number of uses including, understanding how the field is growing and quantifying the impact of different types of papers."
2020.acl-main.464.txt,6 Conclusions,2020,the dataset can potentially also be used to compare patterns of citations in nlp with those in other fields.
2020.acl-main.464.txt,6 Conclusions,2020,"we used the citation counts of a subset (∼27k papers) to examine patterns of citation across paper types, venues, over time, and across areas of research within nlp."
2020.acl-main.466.txt,5 Conclusion,2020,"for example, professionals can spend more time on complex cases and leave the simple cases for the model."
2020.acl-main.466.txt,5 Conclusion,2020,"for tasks that do not yet have a dataset or the datasets are not large enough, we can try to build a large-scale and high-quality dataset or use few-shot or zero-shot methods to solve these problems."
2020.acl-main.466.txt,5 Conclusion,2020,"furthermore, we need to take the ethical issues of legalai seriously."
2020.acl-main.466.txt,5 Conclusion,2020,"in addition to these applications and tasks we have mentioned, there are many other tasks in legalai like legal text summarization and information extraction from legal contracts."
2020.acl-main.466.txt,5 Conclusion,2020,"in the future, for these existing tasks, researchers can focus on solving the three most pressing challenges of legalai combining embedding-based and symbol-based methods."
2020.acl-main.466.txt,5 Conclusion,2020,"nevertheless, no matter what kind application is, we can apply embedding-based methods for better performance, together with symbol-based methods for more interpretability."
2020.acl-main.467.txt,6 Conclusion and Future Work,2020,future work in this area would benefit greatly from improvements to both the breadth and depth of available probing tasks.
2020.acl-main.467.txt,6 Conclusion and Future Work,2020,our results therefore suggest a need for further work on efficient transfer learning mechanisms.
2020.acl-main.468.txt,5 Conclusion,2020,having a formal framework of the causes can help us achieve this.
2020.acl-main.468.txt,5 Conclusion,2020,"if outcome disparity or error disparity, check for potential origins: (a) if label bias: use post-stratification or retrain annotators.(b) if selection bias: use stratified sampling to match source to target populations, or use post-stratification, re-weighting techniques.(c) if overamplification: synthetically match distributions or add outcome disparity to cost function.(d) if semantic bias: retrain or retrofit embeddings considering approaches above, but with attributed (e.g., gendered) words (rather than people) as the population."
2020.acl-main.468.txt,5 Conclusion,2020,"rather than giving the impression that bias is a growing problem, we would like to point out that bias is not necessarily something gone awry, but rather something nearly inevitable in statistical models."
2020.acl-main.468.txt,5 Conclusion,2020,specify target population and an ideal distribution of the attribute (a) to be investigated for bias; consult datasheets and data statements5 if available for the model source; 2.
2020.acl-main.468.txt,5 Conclusion,2020,"we hope it inspires further work in both identifying and countering bias, as well as conceptually and mathematically defining bias in nlp."
2020.acl-main.468.txt,5 Conclusion,2020,"we would like to leave the reader with these main points: (1) every predictive model with errors is bound to have disparities over human attributes (even those not directly integrating human attributes); (2) disparities can result from a variety of origins — the embedding model, the feature sample, the fitting process, and the outcome sample — within the standard predictive pipeline; (3) selection of “protected attributes” (or human attributes along which to avoid biases) is necessary for measuring bias, and often helpful for mitigating bias and increasing the generalization ability of the models."
2020.acl-main.469.txt,5 Conclusion and Future Work,2020,"for future work, it would be interesting to see if more linguistically-inspired phenomena can be systematically found in cross-modal models."
2020.acl-main.47.txt,7 Conclusion and Future work,2020,"furthermore, we would like to extend a comparison between machine and human language processing beyond the perspective of word order."
2020.acl-main.47.txt,7 Conclusion and Future work,2020,"since lms are language-agnostic, analyzing word order in another language with the lm-based method would also be an interesting direction to investigate."
2020.acl-main.47.txt,7 Conclusion and Future work,2020,"we plan to further explore the capability of lms on other linguistic phenomena related to word order, such as “given new ordering” (nakagawa, 2016; asahara et al., 2018)."
2020.acl-main.470.txt,6 Discussion and Future Work,2020,"for instance, our method could assist human supervisors in monitoring the progress of ongoing conversations to detect instances of rushing or stalling, or enable largerscale analyses of conversational behaviors to inform how counselors are trained."
2020.acl-main.470.txt,6 Discussion and Future Work,2020,future work could bolster the measure’s usefulness in several ways.
2020.acl-main.470.txt,6 Discussion and Future Work,2020,"however, the method’s efficacy in the present setting is likely boosted by the relative uniformity of crisis counseling conversations; and future work could aim to better accomodate settings with less structure and more linguistic variability."
2020.acl-main.470.txt,6 Discussion and Future Work,2020,technical improvements like richer utterance representations could improve the measure’s fidelity; more sophisticated analyses could better capture the dynamic ways in which the balance of objectives is negotiated across many turns.
2020.acl-main.470.txt,6 Discussion and Future Work,2020,"the preliminary explorations in section 5.4 could also be extended to gauge the causal effects of counselors’ behaviors (kazdin, 2007)."
2020.acl-main.470.txt,6 Discussion and Future Work,2020,"we expect balancing problems to recur in conversational settings beyond crisis counseling, such as court proceedings, interviews, debates and other mental health contexts like long-term therapy."
2020.acl-main.470.txt,6 Discussion and Future Work,2020,"with such improvements, it would be interesting to study other domains where interlocutors are faced with conversational challenges."
2020.acl-main.474.txt,8 Discussion and Conclusion,2020,"computational social science is an exciting, rapidly expanding discipline."
2020.acl-main.474.txt,8 Discussion and Conclusion,2020,"while text data ought to be as useful for measurement and inference as “traditional” low-dimensional social-scientific variables, combining nlp with causal inference methods requires tackling major open research questions."
2020.acl-main.474.txt,8 Discussion and Conclusion,2020,"with greater availability of text data, alongside improved natural language processing models, there is enormous opportunity to conduct new and more accurate causal observational studies by controlling for latent confounders in text."
2020.acl-main.476.txt,6 Summary,2020,"we model the rich relationship between these entities and the content of the bills using a joint text and graph prediction model on top of bert and rgcn, outperforming each one of the models in isolation."
2020.acl-main.478.txt,8 Conclusion,2020,"in the future, we will further improve the performance of news discourse profiling by investigating subgenres of news articles, and extensively explore its usage for various other nlp tasks and applications."
2020.acl-main.479.txt,7 Conclusion and future work,2020,"considering the importance of context in drawing both scalar and other inferences in communication (grice, 1975; clark, 1992; bonnefon et al., 2009; zondervan, 2010; bergen and grodner, 2012; goodman and stuhlmu¨ller, 2013; degen et al., 2015), the development of appropriate representations of larger context is an exciting avenue for future research."
2020.acl-main.479.txt,7 Conclusion and future work,2020,"it would be interesting to investigate how much supervision is necessary and, for example, to what extent a model trained to perform another task such as predicting natural language inferences is able to predict scalar inferences (see jiang and de marneffe (2019b) for such an evaluation of predicting speaker commitment, and jereticˇ et al.(2020) for an evaluation of different nli models for predicting lexically triggered scalar inferences)."
2020.acl-main.479.txt,7 Conclusion and future work,2020,it would be straightforward to train similar models for other types of inferences.
2020.acl-main.479.txt,7 Conclusion and future work,2020,"lastly, the fact that the attention weights provided insights into the model’s decisions suggests possibilities for using neural network models for developing more precise theories of pragmatic language use."
2020.acl-main.479.txt,7 Conclusion and future work,2020,one further interesting line of research would be to extend this work to other pragmatic inferences.
2020.acl-main.479.txt,7 Conclusion and future work,2020,"our goal here was to investigate whether neural networks can learn associations for already established linguistic features but it would be equally interesting to investigate whether such models could be used to discover new features, which could then be verified in experimental and corpus work, potentially providing a model-driven approach to experimental and formal pragmatics."
2020.acl-main.48.txt,6 Conclusion,2020,"besides, while fake news usually targets at some events, we will also extend gcan to study how to remove eventspecific features to further boost the performance and explainability."
2020.acl-main.48.txt,6 Conclusion,2020,we will explore model generalization in the future work.
2020.acl-main.480.txt,5 Conclusion,2020,"we discussed several future directions, including data augmentation for downstream transferability, applicability of pretrained encoders to discourse, and utilizing larger discourse contexts."
2020.acl-main.481.txt,6 Conclusion and Future Work,2020,"in future work, we plan to extend this work to longer documents such as the recently released dataset of bamman et al.(2019)."
2020.acl-main.483.txt,7 Conclusion & Future Work,2020,"for example, this post from the ghc requires background information and reasoning across sentences in order to classify as offensive or prejudiced: “donald trump received much criticism for referring to haiti, el salvador and africa as ‘shitholes’."
2020.acl-main.483.txt,7 Conclusion & Future Work,2020,"future work includes direct extension and validation of this technique with other language models such as gpt-2 (radford et al., 2019); experimenting with other hate speech or offensive language datasets; and experimenting with these and other sets of identity terms."
2020.acl-main.483.txt,7 Conclusion & Future Work,2020,"he was simply speaking the truth.” the examples we presented (see appendix 4 and 5) show that regularization leads to models that are context-sensitive to a degree, but not to the extent of reasoning over sentences like those above."
2020.acl-main.483.txt,7 Conclusion & Future Work,2020,"in this work, we effectively applied this technique to hate speech classifiers biased towards group identifiers; future work can determine the effectiveness and further potential for this technique in other tasks and contexts."
2020.acl-main.486.txt,8 Conclusion,2020,this indicates that more sophisticated models are required for social bias frames inferences.
2020.acl-main.487.txt,7 Discussion and Conclusion,2020,future work is required to study if our ﬁndings carry over to other languages and cultural contexts.
2020.acl-main.487.txt,7 Discussion and Conclusion,2020,"social biases in nlp models are deserving of concern, due to their ability to moderate how people engage with technology and to perpetuate negative stereotypes."
2020.acl-main.487.txt,7 Discussion and Conclusion,2020,"we have presented evidence that these concerns extend to biases around disability, by demonstrating bias in three readily available nlp models that are increasingly being deployed in a wide variety of applications."
2020.acl-main.488.txt,5 Conclusion,2020,leveraging these developments will allow researchers to further characterize and remove social biases from sentence representations for fairer nlp.
2020.acl-main.488.txt,5 Conclusion,2020,our experiments show that we can remove biases that occur in bert and elmo while preserving performance on downstream tasks.
2020.acl-main.489.txt,6 Conclusion,2020,"combined with inappropriate evaluation protocol, such methods reported inflated performance."
2020.acl-main.489.txt,6 Conclusion,2020,we also strongly encourage the research community to follow the random evaluation protocol for all kgc evaluation purposes.
2020.acl-main.490.txt,7 Conclusions,2020,"in future work, we intend to expand the coverage of clams by incorporating language-specific and non-binary phenomena (e.g., french subjunctive vs. indicative and different person/number combinations, respectively), and by expanding the typological diversity of our languages."
2020.acl-main.490.txt,7 Conclusions,2020,it is possible that its performance drop in hebrew and russian could be mitigated with fine-tuning on more data in these languages.
2020.acl-main.490.txt,7 Conclusions,2020,"this issue could be mitigated in the future with architectural changes to neural lms (such as better handling of morphology), more principled combinations of languages (as in dhar and bisazza 2020), or through explicit separation between languages during training (e.g., using explicit language ids)."
2020.acl-main.492.txt,7 Conclusion,2020,"finally, we are interested in exploring how these types of explanations are actually interpreted by users, and whether providing them actually establishes trust in predictive systems."
2020.acl-main.492.txt,7 Conclusion,2020,"future work might explore how rankings induced over training instances by influence functions can be systematically analyzed in a stand-alone manner (rather than in comparison with interpretations from other methods), and how these might be used to improve model performance."
2020.acl-main.492.txt,7 Conclusion,2020,we posit that influence functions may be a more suitable approach to interpreting models for such relatively complex natural language ‘understanding‘ tasks (while simpler attribution methods like gradients may be sufficient for tasks like sentiment analysis).
2020.acl-main.493.txt,7 Discussion,2020,"by evaluating spearman correlation between all pairs of words, one directly evaluates the extent to which the ordering of words j by distance to each word i is correctly predicted, a key notion of the geometric interpretation of the structural probe."
2020.acl-main.493.txt,7 Discussion,2020,future work could extend this analysis to include quantitative results on the extent of agreement with ud.
2020.acl-main.493.txt,7 Discussion,2020,"future work should explore other multilingual models like xlm and xlm-roberta (lample and conneau, 2019) and attempt to come to an understanding of the extent to which the properties we’ve discovered have causal implications for the decisions made by the model, a claim our methods cannot support."
2020.acl-main.493.txt,7 Discussion,2020,"moreover, while we quantitatively evaluate cross-lingual transfer in recovering dependency distances, we only conduct a qualitative study in the unsupervised emergence of dependency labels via t-sne."
2020.acl-main.495.txt,7 Conclusion,2020,"we encourage future work to judge model interpretability using the proposed evaluation and publicly published annotations, and explore techniques for improving faithfulness and interpretability in compositional models."
2020.acl-main.496.txt,8 Conclusion,2020,balancing performance and interpretability in deep learning models has become an increasingly important aspect of model design.
2020.acl-main.496.txt,8 Conclusion,2020,"furthermore, our method is agnostic to the underlying nature of the two objects being aligned and can therefore align disparate objects such as images and captions, enabling a wide range of future applications within nlp and beyond."
2020.acl-main.497.txt,5 Conclusion,2020,we believe that in future they can be used more directly to yield better performance gains.
2020.acl-main.5.txt,6 Conclusion,2020,annotations complement for multiwoz dataset in the future might enable dst-sc to handle the related-slot problem more effectively and further improve the joint accuracy.
2020.acl-main.50.txt,7 Conclusion and Future Work,2020,doing so would enable us to easily retarget this work to new countries and languages.
2020.acl-main.50.txt,7 Conclusion and Future Work,2020,"ideally, we would like to automatically identify such polarizing topics."
2020.acl-main.50.txt,7 Conclusion and Future Work,2020,"in future work, we plan to increase the number of topics that we use to characterize media."
2020.acl-main.50.txt,7 Conclusion and Future Work,2020,"next, we expand the discovered sets using supervised learning that is trained on the automatically discovered user clusters."
2020.acl-main.500.txt,5 Conclusion,2020,"possible future directions include a systematic study of different aspects of qg diversity (e.g., lexical and factual) and controlled diversification of individual aspects in generation."
2020.acl-main.500.txt,5 Conclusion,2020,we hope that our work will encourage further exploration of diversity-promoting qg and its evaluation.
2020.acl-main.500.txt,5 Conclusion,2020,"while diversity of generation has received significant attention in other text generation problems (e.g., dialog), we show in this paper that it is also an important and measurable dimension of quality in question generation for qa."
2020.acl-main.501.txt,7 Conclusions,2020,"a future direction is to extend this work to question answering tasks that require reasoning over multiple documents, e.g., open-domain qa."
2020.acl-main.501.txt,7 Conclusions,2020,"in addition, the findings may generalize to other tasks, e.g., corpus-level distantly-supervised relation extraction."
2020.acl-main.502.txt,6 Conclusion,2020,"through scde, we aim to encourage the development of more advanced language understanding models."
2020.acl-main.503.txt,6 Discussion,2020,"across many tasks, nlp models struggle on out-of-domain inputs."
2020.acl-main.503.txt,6 Discussion,2020,our work provides a framework to study how models can more judiciously abstain in these challenging environments.
2020.acl-main.504.txt,6 Conclusions and Future Work,2020,"in future work, we plan to explore techniques to automatically learn where to place intermediate classifiers, and what drop ratio to use for each one of them."
2020.acl-main.505.txt,4 Conclusion,2020,we will evaluate our approach on other machine comprehension tasks using dialogues as evidence documents to further verify the generalizability of this work.
2020.acl-main.506.txt,6 Conclusion,2020,we hope that this work provides a complementary picture of hypothesis assessment techniques for the field and encourages more rigorous reporting trends.
2020.acl-main.507.txt,6 Discussion,2020,"our results demonstrate the promise of our annotation framework and dataset in supporting a wide range of reading behavior analyses, as well as the feasibility of developing automated question validation tools for reading comprehension examinations for humans as exciting directions for future work."
2020.acl-main.507.txt,6 Discussion,2020,we leverage the novel structure of our annotations to develop a methodology for automatic validation of annotations and to perform detailed comparisons between human and machine reading comprehension.
2020.acl-main.508.txt,7 Conclusion,2020,"by doing so, we better understand the strengths and limitations of current commonsense reasoning models."
2020.acl-main.508.txt,7 Conclusion,2020,"more importantly, we better know about what kinds of commonsense knowledge are required to be acquired for better commonsense reasoning."
2020.acl-main.511.txt,6 Conclusion,2020,"a second field of application is debate systems, where a dataset can be of use for training a system to formulate new arguments."
2020.acl-main.511.txt,6 Conclusion,2020,the developed annotation approach is also not only limited to rate argument quality: it can easily be transferred to other questions or criteria that can be rated by comparison.
2020.acl-main.512.txt,5 Conclusion,2020,"apart from that, we also plan to design novel models to perform the related tasks of entity extraction and aspect extraction from comparative sentences."
2020.acl-main.512.txt,5 Conclusion,2020,our future work aims to improve the cpc performance further.
2020.acl-main.514.txt,6 Conclusions,2020,exploring the space of subsets of our preprocessing factors might yield more interesting combinations; we leave this for future work.
2020.acl-main.514.txt,6 Conclusions,2020,we systematically examined the role of preprocessing training corpora used to induce word representations for affect analysis.
2020.acl-main.515.txt,5 Conclusion and Future Work,2020,"although conkadi has achieved a notable performance, there is still much room to improve.1) while ats2smmi is behind our conkadi, we find mmi can effectively enhance the ats2s; hence, in the future, we plan to verify the feasibility of the re-ranking technique for knowledge-aware models.2) we will continue to promote the integration of high-quality knowledge, including more types of knowledge and a more natural integration method."
2020.acl-main.515.txt,5 Conclusion and Future Work,2020,"besides, the proposed context-knowledge fusion and flexible mode fusion can facilitate the integration of the knowledge in the conkadi."
2020.acl-main.516.txt,5 Conclusion and Future Work,2020,"in the future, we plan to extend our approach to improve the consistency of multi-turn dialogues."
2020.acl-main.518.txt,5 Conclusions,2020,"despite using gpt-2 models, our framework can be extended with other language models and similarly adopted to improve other multi-modal dialogues."
2020.acl-main.518.txt,5 Conclusions,2020,"in this work, we leverage pre-trained language models for a video-grounded dialogue task."
2020.acl-main.519.txt,6 Conclusion,2020,"in the future, we would like to explore variants of the model architecture."
2020.acl-main.52.txt,6 Conclusion,2020,"since existing methods in this field only focus on the emotion expression of target label but fail to consider the emotion of queries, the safe response problem deteriorates and hurts the content consistency."
2020.acl-main.521.txt,8 Conclusions and Discussion,2020,"additionally, we also contribute a novel technique to combine multiple openie datasets to create a high-quality dataset in a completely unsupervised manner."
2020.acl-main.521.txt,8 Conclusions and Discussion,2020,"this general observation may be of independent interest beyond openie, such as in text summarization."
2020.acl-main.523.txt,4 Conclusion,2020,"our model supports multi-class classification where the sentence and token labels can be weakly related, which indicates the potential of our model for many other real-world applications."
2020.acl-main.523.txt,4 Conclusion,2020,"using a larger amount of general domain texts to build pre-trained representations (peters et al., 2018; radford et al., 2018; devlin et al., 2019; clark et al., 2020) can complement with our model and is one of the directions that we plan to take in future work."
2020.acl-main.529.txt,6 Conclusion,2020,"to further improve the translation quality, we also incorporate an existing vicinity distribution, similar to mixup for observed examples in the training set."
2020.acl-main.53.txt,7 Conclusion,2020,"from this result, we propose that tackling dst with our proposed problem definition is a promising future research direction."
2020.acl-main.530.txt,5 Conclusions,2020,"in future work, we plan to investigate translation of other discourse phenomena that may benefit from the use of future context."
2020.acl-main.531.txt,5 Conclusion,2020,"more specifically, our approach can extract the templates from the sub-tree, which derives from the specific depth of the constituency-based parse tree."
2020.acl-main.531.txt,5 Conclusion,2020,"then, we use a transformer model to predict the soft target templates conditioned on the source text."
2020.acl-main.532.txt,4 Discussions,2020,"for future work, following the work on automatic identification of translationese (rabinovich and wintner, 2015; rubino et al., 2016), we plan to investigate the impact of tagging translationese texts inside parallel training data, such as parallel sentences collected from the web."
2020.acl-main.534.txt,5 Conclusion,2020,"in this paper, we aim to evaluate document influence from a fine-grained level by additionally considering word semantic shifts."
2020.acl-main.535.txt, 5 Conclusion ,2020,"in future work, we intend to adapt our editor module for other learning tasks with both the structured input and structured output."
2020.acl-main.535.txt, 5 Conclusion ,2020,this editor learns how to extract edits from a paraphrase pair and also when and how to apply these edits to a new input sentence.
2020.acl-main.537.txt, 6 Future work ,2020,"these promising results point to future works in (1) linearizing the speed-speedup curve; (2) extending this approach to other pre-training architectures such as xlnet (yang et al., 2019) and elmo (peters et al., 2018); (3) applying fastbert on a wider range of nlp tasks, such as named entity recognition and machine translation."
2020.acl-main.538.txt, 4 Conclusion and Future Work ,2020,"in the future, evaluation by automatically executing generated code with test cases could be a better way to assess code generation results."
2020.acl-main.538.txt, 4 Conclusion and Future Work ,2020,"it will also likely be useful to generalize our re-sampling procedures to zero-shot scenarios, where a programmer writes a library and documents it, but nobody has used it yet."
2020.acl-main.540.txt, 6 Conclusion and Future Work ,2020,"in the future, we will try to increase the robustness gains of adversarial training and consider utilizing sememes in adversarial defense model."
2020.acl-main.541.txt, 8 Conclusion ,2020,better methods are needed to solve this dataset; we show that such methods might generalize well to real-world settings.
2020.acl-main.542.txt, 6 Conclusion ,2020,"in the future, we look forward to extend cl strategy to the pretraining stage, and guide deep models like transformer from a language beginner to a language expert."
2020.acl-main.543.txt, 6 Conclusion ,2020,this suggests that models fail to capture inferential systematicity of monotonicity and its productivity.
2020.acl-main.543.txt, 6 Conclusion ,2020,we hope that our work will be useful in future research for realizing more advanced models that are capable of appropriately performing arbitrary inferences.
2020.acl-main.545.txt, 6 Conclusion and Future Work ,2020,"in the future, we will explore more diverse and advanced paraphrase expanding methods for both sentence and paragraph level qg."
2020.acl-main.545.txt, 6 Conclusion and Future Work ,2020,"moreover, we will apply our methods to other similar tasks, such as sentence simplification."
2020.acl-main.546.txt, 6 Conclusions ,2020,"for future works, to further improve the method, we will explore the introduction of additional information, such as rules and external texts."
2020.acl-main.546.txt, 6 Conclusions ,2020,"in this paper, we use only n-ary facts in the datasets to conduct knowledge inference."
2020.acl-main.548.txt, 6 Conclusion ,2020,is there a problem with memory keeping up with the speed of the cpu on these machines?over-dispersed and hierarchically dependent characteristics.
2020.acl-main.549.txt,7 Conclusion,2020,"a potential solution is to jointly learn evidence selection and claim verification model, which we leave as a future work."
2020.acl-main.549.txt,7 Conclusion,2020,"when assessing the veracity of a claim giving multiple evidence sentences, our approach is built upon an automatically constructed graph, which is derived based on semantic role labeling."
2020.acl-main.55.txt,5 Conclusion,2020,"in the future, we will provide labels that indicate “why this candidate is false” for false candidates in our test set, so that one can easily detect weak points of systems through error analysis."
2020.acl-main.550.txt, 8 Conclusion and Future Work ,2020,"in future work, we will consider introducing more information like the citation texts to the cited paper in other papers to help the generation."
2020.acl-main.551.txt, 5 Conclusions ,2020,we also apply reinforcement learning to leverage edu selection and edu fusion for improving summarization performance.
2020.acl-main.553.txt, 6 Conclusion ,2020,"furthermore, our models have achieved the best results on cnn/dailymail compared with non-bert-based models, and we will take the pretrained language models into account for better encoding representations of nodes in the future."
2020.acl-main.553.txt, 6 Conclusion ,2020,it is also convenient to adapt our singledocument graph to multi-document with document nodes.
2020.acl-main.553.txt, 6 Conclusion ,2020,the introduction of more fine-grained semantic units in the summarization graph helps our model to build more complex relationships between sentences .
2020.acl-main.554.txt, 9 Conclusions ,2020,we also propose methods to enhance the isomorphism and cross-lingual transfer between languages.
2020.acl-main.554.txt, 9 Conclusions ,2020,"we design training objectives for supervised and unsupervised cross-lingual summarizations, respectively."
2020.acl-main.555.txt, 5 Conclusion ,2020,"in the future we would like to explore other more informative graph representations such as knowledge graphs, and apply them to further improve the summary quality."
2020.acl-main.555.txt, 5 Conclusion ,2020,"we also propose an effective method to combine our model with pre-trained lms, which further improves the performance of mds significantly."
2020.acl-main.556.txt, 5 Conclusion and Future Work ,2020,"in the future, we will introduce more tasks like document ranking to supervise the learning of the multi-granularity representations for further improvement."
2020.acl-main.557.txt, 5 Conclusion,2020,"remarkably, probabilities for these tags can be estimated fully in parallel by a simple classification layer on top of a neural network architecture such as bert."
2020.acl-main.557.txt, 5 Conclusion,2020,"we hope that this formulation can be useful as a simple and low-overhead way of integrating syntax into any neural nlp model, including for multi-task training and to predict syntactic annotations during inference."
2020.acl-main.558.txt, 5 Conclusions and future work ,2020,"this dataset, named mlqe, has been released to the research community3 and will be used for the wmt20 shared task on quality estimation.4 in future work, we will test the partial input hypothesis on this data."
2020.acl-main.558.txt, 5 Conclusions and future work ,2020,we hope it will be useful for general research in qe towards more reliable models.
2020.acl-main.558.txt, 5 Conclusions and future work ,2020,"we selected language pairs with varying degrees of resource availability, which led to more diverse translation quality distributions (particularly for the mediumresource languages), mitigating the issue of imbalanced datasets, as shown in figure 2.improving lexical diversity."
2020.acl-main.559.txt, 8 Conclusions,2020,"one of our main goals was to stimulate a discussion; moving forward, we welcome comments, feedback, and suggestions."
2020.acl-main.559.txt, 8 Conclusions,2020,we hope that the paradigm presented here will help provide coherence to such efforts.
2020.acl-main.56.txt,5 Conclusion,2020,"moreover, the visualization analysis of the off-topic model was given to study the essence of the model."
2020.acl-main.560.txt, 6 Conclusion,2020,pertinent questions should be posed to authors of future publications about whether their proposed language technologies extend to other languages.
2020.acl-main.560.txt, 6 Conclusion,2020,there are ways to improve the inclusivity of acl conferences.
2020.acl-main.560.txt, 6 Conclusion,2020,we believe these findings will play a strong role in making the community aware of the gap that needs to be filled before we can truly claim state-of-the-art technologies to be language agnostic.
2020.acl-main.561.txt, 6 Conclusions,2020,"but of one thing we can be certain: the immense success of adapting deep learning architectures to fit with our computational-linguistic understanding of the nature of language will doubtless continue, with greater insights for both natural language processing and machine learning."
2020.acl-main.561.txt, 6 Conclusions,2020,"if we can induce the entities at a given level, a more challenging task will be the induction of the levels themselves."
2020.acl-main.561.txt, 6 Conclusions,2020,"often deep learning models only address one level at a time, whereas a full model would involve levels ranging from the perceptual input to logical reasoning."
2020.acl-main.561.txt, 6 Conclusions,2020,"the levels of representation are hand-coded, based on linguistic theory or available resources."
2020.acl-main.561.txt, 6 Conclusions,2020,the presumably-innate nature of linguistic levels suggests that this might not even be possible.
2020.acl-main.561.txt, 6 Conclusions,2020,this analysis suggests that an important next step in deep learning architectures for natural language understanding will be the induction of entities.
2020.acl-main.562.txt, 7 The Final Frontier – An Outlook ,2020,"however, many questions regarding the future of corpus querying still remain, two of which we consider of particular importance and will discuss in the following sections.7.1 one language to query them all?"
2020.acl-main.562.txt, 7 The Final Frontier – An Outlook ,2020,navigating this ocean in order to find the right tool for the job and then learn to use it can already be as much effort as manually investigating the data at hand.
2020.acl-main.562.txt, 7 The Final Frontier – An Outlook ,2020,"they usually make it very difficult, if not impossible, to implement changes or extensions retrospectively or from the outside."
2020.acl-main.562.txt, 7 The Final Frontier – An Outlook ,2020,"we would like to see them become true enablers instead, allowing queries to go far beyond of what a corpus has to offer with its bare annotations alone and for example include the following extensions to create more informed search solutions: • use knowledge bases and similar external resources to allow more generalized queries, e.g.“find verbal constructions containing a preposition in combination with some sort of furniture”.• add (semantic) similarity measures (e.g.word embeddings) and other approaches for increased fuzziness to improve example-based search.• offer true scripting support for users to extent or customize the ability provided by a system."
2020.acl-main.562.txt, 7 The Final Frontier – An Outlook ,2020,"while this effort is still in an early stage, we are looking forward to having catalogs available in the not too distant future, allowing us to browse for query languages based on our individual information needs."
2020.acl-main.563.txt, 6 Conclusion,2020,we will explore it in the future.
2020.acl-main.564.txt, 5 Conclusion ,2020,"future work will investigate other data manipulation techniques (e.g., data synthesis), which can be further integrated to improve the performance."
2020.acl-main.564.txt, 5 Conclusion ,2020,the resulting data manipulation model is fully end-to-end and can be trained jointly with the dialogue generation model.
2020.acl-main.565.txt, 5 Conclusion,2020,"in addition, a dynamic fusion layer is proposed to dynamically capture the correlation between a target domain and all source domains."
2020.acl-main.566.txt, 5 Conclusion,2020,"by using policy shaping and reward shaping, s2agent can leverage knowledge distilled from the demonstrations to calibrate actions from underlying rl agents for better trajectories, and obtains extra rewards for these state-actions similar to demonstrations alleviating reward sparsity for better exploration."
2020.acl-main.567.txt, 8 Conclusions and Future Work,2020,designing a new model to address these problems may be our future work.
2020.acl-main.567.txt, 8 Conclusions and Future Work,2020,"the slot attention of sas enables it to isolate the key information for each slot, while the slot information sharing enhances the expressiveness of the information passed to each slot by integrating the information from similar slots."
2020.acl-main.567.txt, 8 Conclusions and Future Work,2020,"we believe that sas provides promising potential extensions, such as adapting our model on other tasks where are troubled by excessive information."
2020.acl-main.568.txt, 8 Conclusion and Future Work,2020,there are several future directions to improve ssrem.
2020.acl-main.568.txt, 8 Conclusion and Future Work,2020,"third, we can extend using ssrem to various conversation corpora such as task-oriented dialogues."
2020.acl-main.568.txt, 8 Conclusion and Future Work,2020,we will apply ssrem to various conversation tasks for evaluating the generated text automatically.
2020.acl-main.568.txt, 8 Conclusion and Future Work,2020,we will explore these directions in our future work.
2020.acl-main.568.txt, 8 Conclusion and Future Work,2020,we will make ssrem more robust on the attacks.
2020.acl-main.568.txt, 8 Conclusion and Future Work,2020,"we will use ranking loss (wang et al., 2014; schroff et al., 2015) to learn the difference among samples."
2020.acl-main.568.txt, 8 Conclusion and Future Work,2020,we will use the contextual embedding to represent utterances.
2020.acl-main.569.txt, 5 Conclusion,2020,"in the future work, we will focus on more effective discourse parsing with additional carefully designed features and joint learning with edu segmentation."
2020.acl-main.57.txt,5 Conclusion and Future Work,2020,"in the future, we plan to use more powerful encoders and evaluate our methods on real dialog data."
2020.acl-main.570.txt, 7 Conclusion and Future Work ,2020,further there are plenty of options for improving the fusion technique to enhance the overall performance of the model.
2020.acl-main.570.txt, 7 Conclusion and Future Work ,2020,future work aims at enhancing sequence feature extraction methods to improve the classification performance as those suffer from low accuracy.
2020.acl-main.570.txt, 7 Conclusion and Future Work ,2020,"in this work, we have generated some multi-modal protein-protein interaction databases by amalgamating protein structures and sequences with existing text information available in the biomedical literature."
2020.acl-main.572.txt, 5 Conclusion and Future Work ,2020,"next, we are considering to use this framework to conduct kg entity type noise detection."
2020.acl-main.573.txt, 5 Conclusion and Future Work ,2020,"for future work, how to combine open relation learning and continual relation learning together to complete the pipeline for emerging relations still remains a problem, and we will continue to work on it."
2020.acl-main.574.txt, 8 Conclusion,2020,"importantly, without using external knowledge nor fine tuning of large pretrained models, our methods enable a sequence labeling model to outperform models fine-tuned on bert."
2020.acl-main.574.txt, 8 Conclusion,2020,our analysis also indicates large potential of further performance improvements by exploiting oov and lf entities.
2020.acl-main.576.txt, 6 Conclusion and Future Work ,2020,"in the future, we should further leverage the internal relations in the candidate end, and try to introduce rich medical background knowledge into our work."
2020.acl-main.578.txt, 6 Conclusion,2020,we achieve this by using a new sampling-based approach to choose the most informative neighbors for each entity.
2020.acl-main.58.txt,5 Conclusion,2020,we hope to provide new guidance for the future slot tagging work.
2020.acl-main.58.txt,5 Conclusion,2020,we present a novel knowledge integration mechanism of incorporating background kb and deep contextual representations to facilitate the few-shot slot tagging task.
2020.acl-main.580.txt, 8 Conclusion and Future Work ,2020,"in future work, we hope to tackle repeated fields and learn domainspecific candidate generators."
2020.acl-main.580.txt, 8 Conclusion and Future Work ,2020,"in this initial foray into this challenging problem, we limited our scope to fields with domain-agnostic types like dates and numbers, and which have only one true value in a document."
2020.acl-main.580.txt, 8 Conclusion and Future Work ,2020,"we are also actively investigating how our learned candidate representations can be used for transfer learning to a new domain and, ultimately, in a few-shot setting."
2020.acl-main.582.txt, 5 Conclusion,2020,"meanwhile, the synchronization unit is devised to integrate high-level interaction information and enable the mutual benefit on opinion entity extraction and relation detection."
2020.acl-main.583.txt, 7 Conclusions and Future Work ,2020,future work should study this additional relation in the context of caption annotation and generation.
2020.acl-main.583.txt, 7 Conclusions and Future Work ,2020,the presented work has limitations that can be addressed in future research.
2020.acl-main.583.txt, 7 Conclusions and Future Work ,2020,this is a step forward towards designing systems that learn commonsense inferences in images and text and use that to communicate naturally and effectively with the users.
2020.acl-main.584.txt, 5 Discussion and Conclusion ,2020,"future work should look at more complex fusion strategies, possibly coupled with bottom-up recalibration mechanisms (zarrieß and schlangen, 2016; mojsilovic, 2005) to further enhance colour classification under difficult illumination conditions."
2020.acl-main.585.txt, 5 Conclusion,2020,"however, there are two major differences between video and text."
2020.acl-main.585.txt, 5 Conclusion,2020,the effectiveness of vslnet (and even vslbase) suggest that it is promising to explore span-based qa framework to address nlvl problems.
2020.acl-main.586.txt, 6 Conclusion,2020,we hope that ref-hard and ref-adv will foster more research in this area.
2020.acl-main.588.txt, 6 Conclusion,2020,"first, the edge information of the dependency trees needs to be exploited in later work."
2020.acl-main.588.txt, 6 Conclusion,2020,"in future work, we can further improve our method in the following aspects."
2020.acl-main.588.txt, 6 Conclusion,2020,recently neural structures with syntactical information such as semantic dependency tree and constituent tree are widely employed to enhance the word-level representation of traditional neural networks.
2020.acl-main.588.txt, 6 Conclusion,2020,"second and last, domain-specific knowledge can be incorporated into our method as an external learning source."
2020.acl-main.588.txt, 6 Conclusion,2020,these structures are often modeled and described by treelstms or gcns.
2020.acl-main.588.txt, 6 Conclusion,2020,"to introduce transformer into our task and diminish the error induced by incorrect dependency trees, we propose a dual-transformer structure which considers the connections in dependency tree as a supplementary gcn module and a transformer-like structure for self alignment in traditional transformer."
2020.acl-main.588.txt, 6 Conclusion,2020,we plan to employ an edgeaware graph neural network considering the edge labels.
2020.acl-main.589.txt, 6 Conclusion,2020,"in the future, we would like to extend our work to make a syntactically-aware window that can automatically learn tree (or phrase) structures."
2020.acl-main.59.txt,7 Conclusion,2020,"as future work, we will apply madpl in the more complex dialogs and verify the role-aware reward decomposition in other dialog scenarios."
2020.acl-main.59.txt,7 Conclusion,2020,we also introduce role-aware reward decomposition to integrate the task knowledge into the algorithm.
2020.acl-main.591.txt, 7 Conclusion,2020,"apart from the explicit observations in achieving strong perplexity scores, our model reveals several interesting aspects of the quality of the trees learned by the model."
2020.acl-main.592.txt, 6 Conclusions,2020,it learns intra-cell and inter-cell architectures simultaneously.
2020.acl-main.592.txt, 6 Conclusions,2020,"more interestingly, it is observed that transferring the pre-learned architectures to other tasks also obtains a promising performance improvement."
2020.acl-main.593.txt, 8 Conclusion,2020,"it also allows for controlling the speed/accuracy tradeoff using a single model, without retraining it for any point along the curve."
2020.acl-main.594.txt, 7 Conclusion,2020,"for researchers developing robust morphological analyzers for low resource, morphologically complex languages, this work represents a template of model development which is well-suited for the context."
2020.acl-main.594.txt, 7 Conclusion,2020,"producing a viable morphological analyzer is the first step towards building improved dictionary search interfaces, spell-checking tools, and computer-assisted language learning applications for communities who speak low-resource languages."
2020.acl-main.594.txt, 7 Conclusion,2020,"similarly, once we have applied the complete neural model to a corpus of natural text, we will no longer need to approximate distributional information."
2020.acl-main.594.txt, 7 Conclusion,2020,"the pattern of training robust systems on data that has been augmented by the knowledge captured in symbolic systems could be applied to areas outside of morphological analysis, and is a promising avenue of future exploration."
2020.acl-main.596.txt, 7 Conclusion and Future Work ,2020,future work will aim to extend the current model to capture particularly challenging morphological patterns such as templatic non-concatenative morphology and polysynthetic composition.
2020.acl-main.596.txt, 7 Conclusion and Future Work ,2020,it can also tell us the productivity of each morphological process and thus can obtain much deeper knowledge in terms of morphological structures of languages.
2020.acl-main.596.txt, 7 Conclusion and Future Work ,2020,"our next step will be to attempt to automate the determination of language typology, yielding somewhat better performance with a system requiring no human intervention per language at all."
2020.acl-main.596.txt, 7 Conclusion and Future Work ,2020,this unsupervised model can be quickly and easily extended to novel languages without data annotation or expert input.
2020.acl-main.598.txt, 6 Conclusion,2020,"by substituting the current transducers in our pipeline, we expect that we will be able to improve the overall performance of our system."
2020.acl-main.598.txt, 6 Conclusion,2020,"further analysis showed the importance of our individual components and detected possible sources of errors, like wrongly identified edit trees early in the pipeline or syncretism."
2020.acl-main.598.txt, 6 Conclusion,2020,"in the future, we will explore the following directions: (i) a difficult challenge for our proposed system is to correctly determine the paradigm size."
2020.acl-main.598.txt, 6 Conclusion,2020,"since transfer across related languages has shown to be beneficial for morphological tasks (jin and kann, 2017; mccarthy et al., 2019; anastasopoulos and neubig, 2019, inter alia), future work could use typologically aware priors to guide the number of paradigm slots based on the relationships between languages.(ii) we plan to explore other methods, like word embeddings, to incorporate context information into our feature function.(iii) we aim at developing better performing string transduction models for the morphological inflection step."
2020.acl-main.598.txt, 6 Conclusion,2020,"we further developed a system for the task, which performs the following steps: (i) edit tree retrieval, (ii) additional lemma retrieval, (iii) paradigm size discovery, and (iv) inflection generation."
2020.acl-main.599.txt, 6 Conclusion,2020,improving our graph structure of representing the document as well as the document-level pretraining tasks is our future research goals.
2020.acl-main.599.txt, 6 Conclusion,2020,"on the natural questions dataset, which contains two sub-tasks predicting a paragraph-level long answer and a token-level short answer, our method jointly trains the two sub-tasks to consider the dependencies of the twograined answers."
2020.acl-main.6.txt,5 Conclusion,2020,"in future work, we plan to analyze each turn of dialogue with reinforcement learning architecture, and to enhance the diversity of the whole dialogue by avoiding knowledge reuse."
2020.acl-main.600.txt, 6 Conclusion,2020,we first use the wikipedia paragraphs and its references to construct a synthetic qa data refqa and then use the qa model to iteratively refine data over re-fqa.
2020.acl-main.601.txt, 6 Conclusions and Future Works,2020,"in order to tackle the labeled data shortage problem, we learned the structural patterns from the unlabeled data by the hidden semi-markov model."
2020.acl-main.601.txt, 6 Conclusions and Future Works,2020,we will investigate the robustness and scalability of the model.
2020.acl-main.601.txt, 6 Conclusions and Future Works,2020,"with the patterns as a prior, we transferred this fundamental knowledge into the generation model to produce the optimal results."
2020.acl-main.602.txt, 6 Conclusions,2020,"for modeling, we plan to explore recent advances in conditional language models for jointly modeling qa with generating their derivations."
2020.acl-main.602.txt, 6 Conclusions,2020,one immediate future work is to evaluate state-of-the-art rc systems’ internal reasoning on our dataset.
2020.acl-main.603.txt, 6 Conclusion,2020,we also add a recurrent mechanism to allow the information to flow across segments so that the model could have knowledge beyond the current segment when selecting answers.
2020.acl-main.606.txt, 7 Conclusion and Future Work ,2020,"future research may involve tailoring existing parsers to learner data, combining literal and intended meanings in a unified framework, evaluating gec models in terms of speakers’ intention and parsing for other languages."
2020.acl-main.607.txt, 6 Conclusion,2020,"by adding unlabeled data, our model exhibits further performance improvements."
2020.acl-main.607.txt, 6 Conclusion,2020,"in particular, our semi-supervised model performs well in the low resource setting and on the out-of-domain test set."
2020.acl-main.607.txt, 6 Conclusion,2020,this points to future directions of applying our model to low-resource languages and cross-domain settings.
2020.acl-main.608.txt, 6 Conclusion,2020,the first stage utilizes the dual structure of an unsupervised paraphrase model to rewrite the input natural language utterance into canonical utterance.
2020.acl-main.608.txt, 6 Conclusion,2020,"three self-supervised tasks, namely denoising auto-encoder, back-translation and dual reinforcement learning, are introduced to iteratively improve our model through pre-training and cycle learning phases."
2020.acl-main.611.txt, 6 Conclusion and Future Work ,2020,we leave adjusting our model to different kinds of lattice or graph as our future work.
2020.acl-main.612.txt, 6 Conclusion,2020,"fgs2ee first uses the word embeddings of semantic type words to generate semantic embeddings, and then combines them with existing entity embeddings through linear aggregation."
2020.acl-main.612.txt, 6 Conclusion,2020,"for the future work, we are planning to extract fine-grained semantic types from unlabelled documents and use the relatedness between the finegrained types and contexts as distant supervision for entity linking."
2020.acl-main.612.txt, 6 Conclusion,2020,thus can facilitate the learning of semantic commonalities about entity-context and entity-entity relations.
2020.acl-main.613.txt, 7 Conclusions,2020,"for example, the word ”development” probably in most cases means in medicine the growth or spread of a disease (or a tumor), while in the general domain we can not say without a context, and in that case, the need for linguistics information in the queries will be more important to solve the translation ambiguity."
2020.acl-main.613.txt, 7 Conclusions,2020,the smt systems for qt were specifically trained and tuned to translate medical search queries.
2020.acl-main.613.txt, 7 Conclusions,2020,"to conduct this study, we investigated various mt systems and their configurations and performed a thorough large-scale evaluation based on the test collection produced within the clef ehealth tasks on patient-centered information retrieval during 2013–2015, and extended with additional relevance assessments."
2020.acl-main.613.txt, 7 Conclusions,2020,"we presented a comparative study between querytranslation (qt), and document translation (dt) approaches in the cross-lingual information retrieval (clir) task."
2020.acl-main.615.txt, 7 Conclusion,2020,we therefore advocate the use of alternate forms of entropy regularization for language generation tasks.
2020.acl-main.616.txt, 6 Conclusion and Future Work ,2020,"in the future, it is necessary to interpret the semantics that transformer layers in different depths can convey, which is beneficial for the computing-efficiency."
2020.acl-main.617.txt, 6 Conclusion,2020,"atth learns embeddings with trainable hyperbolic curvatures, allowing it to learn the right geometry for each relationship and generalize across multiple embedding dimensions."
2020.acl-main.617.txt, 6 Conclusion,2020,"future directions for this work include exploring other tasks that might benefit from hyperbolic geometry, such as hypernym detection."
2020.acl-main.617.txt, 6 Conclusion,2020,the proposed attention-based transformations can also be extended to other geometric operations.
2020.acl-main.618.txt, 4 Conclusion and Future Work ,2020,"this proof-of-concept work opens up a wide spectrum of interesting avenues for future research, including the use of more powerful classifiers, more sophisticated features (e.g., character-level transformers), and fine-grained linguistic analyses on the importance of disparate features over different language pairs."
2020.acl-main.619.txt, 6 Conclusion,2020,"we investigated for the first time the importance of this information in st, analysing the behaviour of cascade (the state of the art in the field) and end-to-end st technology (the emerging approach)."
2020.acl-main.62.txt,6 Conclusions,2020,"for future work, we will explore the scenarios that annotations are absent for all expert dialogues."
2020.acl-main.620.txt, 6 Conclusion and Future Work ,2020,another promising direction is to design more powerful training strategies to replace the baby step.
2020.acl-main.620.txt, 6 Conclusion and Future Work ,2020,"as our model is not limited to machine translation, it is interesting to validate the proposed framework into other nlp tasks that need to exploit cl."
2020.acl-main.622.txt, 6 Conclusion,2020,"furthermore, a new speaker modeling strategy can also boost the performance in dialogue settings."
2020.acl-main.622.txt, 6 Conclusion,2020,"in future work, we will explore novel approaches to generate the questions based on each mention, and evaluate the influence of different question generation methods on the coreference resolution task."
2020.acl-main.623.txt, 8 Conclusions and Future Work ,2020,"future work would include a comparison with other, more complex, methods for uncertainty estimation, incorporating uncertainty to affect model decisions over time, and further investigating links between uncertainty values and linguistic features of the input."
2020.acl-main.623.txt, 8 Conclusions and Future Work ,2020,"the methods presented here can be selected based on knowledge of the properties of the data at hand, for example prioritising the use of aleatoric uncertainty estimates on imbalanced and heterogeneous datasets such as pheme."
2020.acl-main.623.txt, 8 Conclusions and Future Work ,2020,"using uncertainty estimation methods can help identify which instances are hard for the model to classify, thus highlighting the areas where one should focus during model development."
2020.acl-main.624.txt, 6 Conclusion,2020,"in the future, we want to investigate more powerful recommenders, combine interactive entity linking with knowledge base completion and use online learning to leverage deep models, despite their long training time."
2020.acl-main.625.txt, 6 Conclusion,2020,"in addition, we presented in-depth analysis and extensive ablation studies on various aspects of the model functioning mechanism and architecture design, showing the necessity of our design contribution in achieving good results."
2020.acl-main.626.txt, 6 Conclusion,2020,"applying our proposed controlled crowdsourcing protocol to qa-srl successfully attains truly scalable high-quality annotation by laymen, facilitating future research of this paradigm."
2020.acl-main.626.txt, 6 Conclusion,2020,"we release our data, software and protocol, enabling easy future dataset production and evaluation for qa-srl, as well as possible extensions of the qa-based semantic annotation paradigm."
2020.acl-main.627.txt, 6 Conclusion,2020,experiment analysis is offered to understand the proposed method in depth.
2020.acl-main.628.txt, 5 Conclusion,2020,"because sentence metaembeddings are agnostic to the size and specifics of their ensemble, it should be possible to add new encoders to the ensemble, potentially improving performance further."
2020.acl-main.629.txt, 6 Conclusions and Future work ,2020,"despite the promising results, the accuracy of our approach could probably be boosted further by experimenting with new feature information and specifically tuning hyper-parameters for the sdp task, as well as using different enhancements such as implementing the hierarchical decoding recently presented by liu et al.(2019), including contextual string embeddings (akbik et al., 2018) like he and choi (2019), or applying multi-task learning across the three formalisms like peng et al.(2017)."
2020.acl-main.63.txt,6 Conclusion,2020,"this paper proposes a general learning framework leveraging the duality between language understanding and generation, providing the flexibility of incorporating supervised and unsupervised learning algorithms to jointly train two models."
2020.acl-main.630.txt, 6 Conclusion,2020,"another research direction is to investigate if introducing more sophisticated topic models, such as named entity promoting topic models (krasnashchok and jouili, 2018) into the proposed framework can further improve results."
2020.acl-main.630.txt, 6 Conclusion,2020,future work may focus on how to directly induce topic information into bert without corrupting pretrained information and whether combining topics with other pretrained contextual models can lead to similar gains.
2020.acl-main.631.txt, 6 Conclusion,2020,"moreover, the proposed augmentation method tends not to be unique to the current task and could be applied to other low-resource sequence labeling tasks such as chunking and named entity recognition."
2020.acl-main.632.txt, 6 Conclusion,2020,"in the future, we plan to consider the ethos mode of persuasion by exploring how debaters strengthen their credibility in debates."
2020.acl-main.633.txt, 7 Conclusions,2020,"future research may focus on the motivation we described, but may also utilize the large speeches corpus we release as part of this work to a variety of additional different endeavors."
2020.acl-main.633.txt, 7 Conclusions,2020,"we collected, and release as part of this work, more than 3,600 debate speeches annotated for the proposed task."
2020.acl-main.633.txt, 7 Conclusions,2020,"we presented baselines for the task, considering a variety of contemporary nlp models."
2020.acl-main.634.txt, 7 Conclusion and Future Work ,2020,"the model can be potentially improved by filtering the corpus according to different domains, or augmenting with a retrieve-and-rewrite mechanism, which we leave for future work."
2020.acl-main.635.txt, 5 Conclusion and Future Work ,2020,"in addition, kd-conv covers three domains, including film, music, and travel, that can be used to explore domain adaptation or transfer learning for further research."
2020.acl-main.636.txt, 6 Conclusion,2020,"in future work, we intend to explore more with the combination of rl and dst on the basis of reward designing, trying to explore more in the internal mechanism."
2020.acl-main.636.txt, 6 Conclusion,2020,"in the long run, we are interested in combing many tasks into one learning process with meta-learning."
2020.acl-main.639.txt,5 Conclusion,2020,"in the future, we plan to adapt variational neural network to refine our style transfer model, which has shown effectiveness in other conditional text generation tasks, such as machine translation (zhang et al., 2016; su et al., 2018)."
2020.acl-main.639.txt,5 Conclusion,2020,"then, equipped with the style component, our model can exploit the word-level predicted style relevance for better style transfer."
2020.acl-main.640.txt, 6 Conclusion,2020,"on the other hand, we would also like to investigate how to make use of our proposed model to solve sequence-to-sequence tasks."
2020.acl-main.640.txt, 6 Conclusion,2020,one is to investigate how the other graph models can benefit from our proposed heterogeneous mechanism.
2020.acl-main.640.txt, 6 Conclusion,2020,our proposed heterogeneous mechanism can adaptively model the different representation subgraphs.
2020.acl-main.640.txt, 6 Conclusion,2020,there are two directions for future works.
2020.acl-main.641.txt, 7 Conclusion,2020,"as our model is interpretable in the correspondence between segments and input records, it can be easily combined with hand-engineered heuristics or user-specific requirements to further improve the performance."
2020.acl-main.642.txt, 5 Conclusion,2020,"furthermore, we explicitly construct the relations between words by dependency tree and align the image and question representations by an attention alignment module to reduce the gaps between vision and language."
2020.acl-main.642.txt, 5 Conclusion,2020,we will explore more complicated object relation modeling in future work.
2020.acl-main.643.txt, 6 Conclusions,2020,a future research direction is to combine rgcs with distant supervision by an external knowledge base to answer the visual questions that need external knowledge; for example which animal in this photo can climb a tree?
2020.acl-main.645.txt, 8 Conclusion,2020,the question of knowing whether pretraining on small domain specific content will be a better option than transfer learning techniques such as fine-tuning remains open and we leave it for future work.
2020.acl-main.645.txt, 8 Conclusion,2020,this paves the way for the rise of monolingual contextual pre-trained language-models for under-resourced languages.
2020.acl-main.646.txt, 8 Discussion,2020,"we hope this work, i.e.the organised review it contributes and the techniques it introduces, will pave the way to deeper—in statistical hierarchy—generative models of language."
2020.acl-main.647.txt, 8 Conclusion,2020,we aim to explore those directions in a future work.
2020.acl-main.648.txt, 5 Takeaways and Open Questions ,2020,"we anticipate that this study would be useful to tc nlp practitioners, as we address several research gaps, namely script conversion and a lack of benchmark datasets."
2020.acl-main.648.txt, 5 Takeaways and Open Questions ,2020,we leave some open questions to explore: • how can we exploit subword variations to reduce skewness in the nlu tasks?• would subword-segmentation-transfer be helpful for other nmt-nlu task pairs like we did for 2kenize (script conversion) to 1kenize (classification)?
2020.acl-main.649.txt, 8 Conclusion,2020,"in future work, we intend to further fine-tune our methodological apparatus for tackling mfep."
2020.acl-main.649.txt, 8 Conclusion,2020,"overall, we see our study as an exciting step in the direction of bringing together computational social science and derivational morphology."
2020.acl-main.649.txt, 8 Conclusion,2020,"this reflection of external events makes morphological families a promising tools for various fields drawing upon nlp techniques for tracing temporal dynamics in text (e.g., virality detection)."
2020.acl-main.65.txt,7 Conclusion,2020,"in future work, we plan to seek better ways to guide the learning of latent variables, such as using dynamic routing (sabour et al., 2017) method to align the latent variables and sememes, and learn more explainable latent codes."
2020.acl-main.650.txt, 8 Conclusion ,2020,"the techniques developed here readily apply to other types of normalization data (e.g.informal, dialectal)."
2020.acl-main.651.txt, 5 Conclusion and Future Work ,2020,"we hope that this dataset will encourage research into clarification question generation and, in the long run, enhance dialog and question-answering systems."
2020.acl-main.652.txt,8 Conclusion and Future Work,2020,"for the future, we would like to exploit the abstractive answers in our dataset, explore more sophisticated systems in both scenarios and perform user studies to study how real users interact with a conversational qa system when accessing faqs."
2020.acl-main.652.txt,8 Conclusion and Future Work,2020,"the expert can rephrase the selected span, in order to make it look more natural."
2020.acl-main.652.txt,8 Conclusion and Future Work,2020,the goal of this work is to access the large body of domain-specific information in the form of frequently asked question sites via conversational qa systems.
2020.acl-main.652.txt,8 Conclusion and Future Work,2020,"these dialogues are created by crowdworkers that play the following two roles: the user asks questions about a certain topic posted in stack exchange, and the domain expert who replies to the questions by selecting a short span of text from the long textual reply in the original post."
2020.acl-main.653.txt, 7 Conclusion,2020,we hope that mlqa will help to catalyse work in cross-lingual qa to close the gap between training and testing language performance.
2020.acl-main.654.txt, 6 Conclusion,2020,"in future work, we explore to extend this approach for other low resource tasks in nlp."
2020.acl-main.655.txt, 7 Conclusion,2020,"in the future, we will further study this properties of kernel-based attentions in neural networks, both in the effectiveness front and also the explainability front."
2020.acl-main.655.txt, 7 Conclusion,2020,"this paper presents kgat, which uses kernels in graph neural networks to conduct more accurate evidence selection and fine-grained joint reasoning."
2020.acl-main.655.txt, 7 Conclusion,2020,"while the dot-product attentions are rather scattered and hard to explain, the kernel-based attentions show intuitive and effective attention patterns: the node kernels focus more on the correct evidence pieces; the edge kernels accurately gather the necessary information from one node to the other to complete the reasoning chain."
2020.acl-main.656.txt, 7 Conclusions,2020,"for future work, an obvious next step is to investigate the possibility of generating veracity explanations from evidence pages crawled from the web."
2020.acl-main.656.txt, 7 Conclusions,2020,"furthermore, other approaches of generating veracity explanations should be investigated, especially as they could improve fluency or decrease the redundancy of the generated text."
2020.acl-main.657.txt, 7 Conclusion & Future Work ,2020,"as future work, we will explore different heuristics for navigating in the premises graph, as researched before for textual entailment (silva et al., 2019, 2018) and selective reasoning (freitas et al., 2014)."
2020.acl-main.657.txt, 7 Conclusion & Future Work ,2020,the qualitative analysis indicates that there is the demand to design principled embeddings for better capturing the semantics of proofs which are denser in mathematical formulae.
2020.acl-main.660.txt, 7 Discussion and Conclusion ,2020,"this type of research needs to be extended to the investigation of multiple tasks, multiple languages, and multiple possible pre-training regimes (words, chars, morphemes, lattices) in order to investigate whether this trend extends to other languages and tasks."
2020.acl-main.660.txt, 7 Discussion and Conclusion ,2020,"we proceeded to define the three deep counterparts to the challenges proposed in tsarfaty et al.(2010), namely, the deep architectural challenge, deep modeling challenge and deep lexical challenge, and sketched plausible research avenues that the nmrl community might wish to explore towards their resolution."
2020.acl-main.661.txt, 7 Conclusion,2020,we exposed a significant space of both modeling ideas and application-specific requirements left to be addressed in future research.
2020.acl-main.662.txt, 5 A Call to Action ,2020,"again, we emphasize that human and computer skills are not identical, but this is a benefit: humans’ natural aversion to unfairness will help you create a better task, while computers will blindly optimize an objective function (bostrom, 2003)."
2020.acl-main.662.txt, 5 A Call to Action ,2020,"as you go through the process of playing on your question–answer dataset, you can see where you might have fallen short on the goals we outline in section 3."
2020.acl-main.662.txt, 5 A Call to Action ,2020,but you can use some quizbowl intuitions to improve discrimination.
2020.acl-main.662.txt, 5 A Call to Action ,2020,"eat your own dog food as you develop new question answering tasks, you should feel comfortable playing the task as a human."
2020.acl-main.662.txt, 5 A Call to Action ,2020,"embrace multiple answers or specify specificity as qa moves to more complicated formats and answer candidates, what constitutes a correct answer becomes more complicated."
2020.acl-main.662.txt, 5 A Call to Action ,2020,"finally, if you want to make human–computer comparisons, pick the right humans."
2020.acl-main.662.txt, 5 A Call to Action ,2020,"for more traditional qa tasks, you can maximize the usefulness of your dataset by ensuring as many questions as possible are challenging (but not impossible) for today’s qa systems."
2020.acl-main.662.txt, 5 A Call to Action ,2020,"for this to feel real, you will need to keep score; have all of your coauthors participate and compare scores."
2020.acl-main.662.txt, 5 A Call to Action ,2020,here are our recommendations if you want to have an effective leaderboard.
2020.acl-main.662.txt, 5 A Call to Action ,2020,"if your task is realistic, fun, and challenging, you will find experts to play against your computer."
2020.acl-main.662.txt, 5 A Call to Action ,2020,"in between full automation and expensive humans in the loop are automatic metrics that mimic the flexibility of human raters, inspired by machine translation evaluations (papineni et al., 2002; specia and farzindar, 2010) or summarization (lin, 2004)."
2020.acl-main.662.txt, 5 A Call to Action ,2020,"in contrast to low-paid crowdworkers, public platforms for question answering and citizen science (bowser et al., 2013) are brimming with free expertise if you can engage the relevant communities."
2020.acl-main.662.txt, 5 A Call to Action ,2020,"in short, consider multiple versions/views of your data that progress from difficult to easy."
2020.acl-main.662.txt, 5 A Call to Action ,2020,"make questions discriminative we argue that questions should be discriminative (section 2.3), and while quizbowl is one solution (section 4), not everyone is crazy enough to adopt this (beautiful) format."
2020.acl-main.662.txt, 5 A Call to Action ,2020,"moreover, another lesson the qa community could learn from trivia games is to turn it into a spectacle: exciting games with a telegenic host."
2020.acl-main.662.txt, 5 A Call to Action ,2020,"not only will this give you human baselines worth reporting—they can also tell you how to fix your qa dataset...after all, they’ve been at it longer than you have."
2020.acl-main.662.txt, 5 A Call to Action ,2020,"revel in spectacle however, with more complicated systems and evaluations, a return to the yearly evaluations of trecqa may be the best option."
2020.acl-main.662.txt, 5 A Call to Action ,2020,talk to trivia nerds you should talk to trivia nerds because they have useful information (not just about the election of 1876).
2020.acl-main.662.txt, 5 A Call to Action ,2020,these skills are exactly those we want computers to develop.
2020.acl-main.662.txt, 5 A Call to Action ,2020,this can be problematic for several reasons.
2020.acl-main.662.txt, 5 A Call to Action ,2020,"to ensure that our datasets properly “isolate the property that motivated [the dataset] in the first place” (zaenen, 2006), we need to explicitly appreciate the unavoidable ambiguity instead of silently glossing over it.14 this is already an active area of research, with conversational qa being a new setting actively explored by several datasets (reddy et al., 2018; choi et al., 2018); and other work explicitly focusing on identifying useful clarification questions (rao and daumé iii), thematically linked questions (elgohary et al., 2018) or resolving ambiguities that arise from coreference or pragmatic constraints by rewriting underspecified question strings (elgohary et al., 2019; min et al., 2020)."
2020.acl-main.663.txt, 6 Conclusion,2020,i hope that this survey paper will help other researchers to develop the field in a way that keeps long-term goals in mind.
2020.acl-main.663.txt, 6 Conclusion,2020,"in particular, it seems desirable to represent the meaning of a word as a region of space or as a classifier, and to work with probability logic."
2020.acl-main.663.txt, 6 Conclusion,2020,"my own recent work in this direction has been to develop the pixie autoencoder (emerson, 2020a), and i look forward to seeing alternative approaches from other authors, as the field of distributional semantics continues to grow."
2020.acl-main.664.txt, 5 Conclusions,2020,"during generation, the network is regularized to take into account explicit object/predicate constraints with multi-task learning."
2020.acl-main.664.txt, 5 Conclusions,2020,in the near future we plan to extend the proposed approach to several other language-vision modeling tasks.
2020.acl-main.665.txt,4 Conclusion,2020,another avenue of research would be to investigate the role of synthetic data in surface realization in other languages.
2020.acl-main.665.txt,4 Conclusion,2020,"assuming the use of synthetic data, more needs to be investigated in order to fully maximize its benefit on performance."
2020.acl-main.665.txt,4 Conclusion,2020,"future work will look more closely at the choice of corpus, construction details of the synthetic dataset, as well as the tradeoff between training time and accuracy that comes with larger vocabularies."
2020.acl-main.666.txt, 5 Conclusions,2020,"as future work, we plan to further evaluate the impact of different sequential architectures, longer contexts, alternative sentence embeddings, and cleverer selection of distractors."
2020.acl-main.666.txt, 5 Conclusions,2020,"inspired by deliberation networks and automatic post editing methods (xia et al., 2017; freitag et al., 2019), we ultimately want to apply our model to two-step generation, first selecting a sentence from a large set before refining it to fit the context."
2020.acl-main.666.txt, 5 Conclusions,2020,this work introduces a sentence-level language model which takes a sequence of sentences as context and predicts a distribution over a finite set of candidate next sentences.
2020.acl-main.667.txt, 4 Conclusion,2020,we hope that this work can shed some light and inspire future work at this line of research.
2020.acl-main.668.txt, 9 Conclusion,2020,future work will explore the use of domain adaptation techniques to enhance performance where the domains of the ci and event text differ substantially.
2020.acl-main.669.txt, 5 Conclusion,2020,"ure remains challenging, which requires improved methods to deal with silver data."
2020.acl-main.669.txt, 5 Conclusion,2020,"we also plan to use different types of labelled data, e.g., domain specific data sets, to ascertain whether entity type information is more discriminative in sub-languages."
2020.acl-main.67.txt,7 Conclusion and Future Work,2020,"also, we plan to use large-scale unlabeled data to improve the performance further."
2020.acl-main.67.txt,7 Conclusion and Future Work,2020,"furthermore, our framework can be efficiently applied to other graph-to-sequence tasks such as webnlg (gardent et al., 2017) and syntax-based neural machine translation (bastings et al., 2017)."
2020.acl-main.67.txt,7 Conclusion and Future Work,2020,in future work we would like to do several experiments on other related tasks to test the versatility of our framework.
2020.acl-main.670.txt, 6 Conclusion,2020,"as our research community moves towards document level ie and discourse modeling, we position this dataset as a testing ground to focus on this important and challenging task."
2020.acl-main.670.txt, 6 Conclusion,2020,"this task poses multiple technical and modeling challenges, including 1. the use of transformer-based models on long documents and related device memory issues, 2. aggregating coreference information from across documents in an end-to-end manner, 3. identifying salient entities in a document and 4. performing n-ary relation extraction of these entities."
2020.acl-main.670.txt, 6 Conclusion,2020,"we introduce scirex, a comprehensive and challenging dataset for information extraction on full documents."
2020.acl-main.671.txt, 6 Conclusion,2020,"furthermore, we seek to investigate the transferability of the obtained inductive bias to other commonsense-demanding downstream tasks, which are distinct from the winograd-structure."
2020.acl-main.671.txt, 6 Conclusion,2020,"therefore, future work will aim at relaxing the prior of winograd-structured twin-question pairs."
2020.acl-main.672.txt, 6 Discussion,2020,"we explore a set of interventions to the transformer-xl’s architecture that are very simple to implement, i.e.a few lines of code, but shed light on the fundamental workings of the model when modelling long sequences of text."
2020.acl-main.673.txt, 6 Conclusions,2020,"for future work, our model can be extended to disentangled representation learning with non-categorical style labels, and applied to zero-shot style transfer with newly-coming unseen styles."
2020.acl-main.674.txt, 6 Conclusion and Future work ,2020,"better emotion, scene, scene-text, object detection and captions might lead to further improvement of performance."
2020.acl-main.675.txt, 4 Conclusion,2020,"first, we will explore mechanisms for instance-specific translation that are more sophisticated than the aggregation of translation vectors of nearest dictionary neighbours."
2020.acl-main.675.txt, 4 Conclusion,2020,"second, we plan to couple instance-based mapping with other informative features (e.g., character-level features) in classification-based bli frameworks (heyman et al., 2017; karan et al., 2020)."
2020.acl-main.675.txt, 4 Conclusion,2020,we plan to extend this work in two directions.
2020.acl-main.676.txt, 7 Discussion,2020,"more generally, the present results underline the extent to which current models fail to learn simple, context-independent notions of reuse, but also how easy it is to make progress towards addressing this problem without fundamental changes in model architecture."
2020.acl-main.676.txt, 7 Discussion,2020,"the procedure detailed in this paper relies on exact string matching to identify common context; future work might take advantage of learned representations of spans and their environments (mikolov et al., 2013; peters et al., 2018)."
2020.acl-main.676.txt, 7 Discussion,2020,"the two lines of work could be combined, e.g.by using geca-identified fragments to indicate productive locations for sub-sentential paraphrasing."
2020.acl-main.677.txt, 6 Conclusion,2020,"this representation learning will be beneficial in tasks beyond text-to-sql, as long as the input has some predefined structure."
2020.acl-main.678.txt, 5 Conclusion,2020,the proposed method may be an important module for future applications related to time.
2020.acl-main.679.txt, 6 Conclusion,2020,it remains an open question whether this task-specific approach to generalisation constitutes a true advancement in “reasoning”.
2020.acl-main.680.txt, 7 Conclusions,2020,"we expect our data, results, and error analysis to inform the design of similar experimental setups for other nlp tasks beyond ner, such as part-of-speech tagging or relation extraction."
2020.acl-main.681.txt, 5 Conclusion,2020,"future directions to explore include incorporating noise-robust training procedures (goldberger and ben-reuven, 2017) and example weighting (dehghani et al., 2018) during self-training, and exploring lexical alignment methods from literature on learning cross-lingual embeddings."
2020.acl-main.681.txt, 5 Conclusion,2020,"while these results are encouraging, we are yet to match supervised in-domain model performance."
2020.acl-main.682.txt, 7 Conclusions & Future Work ,2020,"bleu); the attribute-prediction task would use the decoder representation to predict the attributes of the objects in the image (elliott and ka´da´r, 2017, e.g.); and the zero-shot setting could leverage the nocaps dataset (agrawal et al., 2018)."
2020.acl-main.682.txt, 7 Conclusions & Future Work ,2020,compguesswhat?!requires models to learn to combine the co-grounded information provided for every turn.
2020.acl-main.682.txt, 7 Conclusions & Future Work ,2020,"for example, in image captioning, the goal-oriented evaluation would be the textual similarity metrics (e.g."
2020.acl-main.682.txt, 7 Conclusions & Future Work ,2020,"for instance, in the context of compguesswhat?!, it would be used to learn a representation for the current turn that is influenced by both the language and visual modality."
2020.acl-main.682.txt, 7 Conclusions & Future Work ,2020,"however, the same framework could be applied to other multi-modal tasks."
2020.acl-main.682.txt, 7 Conclusions & Future Work ,2020,"in addition, we will use the compguesswhat?!image annotations to design a visual grounding evaluation to assess the ability of the model to attend to the correct objects during the turns of the dialogue."
2020.acl-main.682.txt, 7 Conclusions & Future Work ,2020,"in the following, we discuss insights gained from the evaluation and new research directions for this task."
2020.acl-main.682.txt, 7 Conclusions & Future Work ,2020,we hope that grolla and the compguesswhat?!data will encourage the implementation of learning mechanisms that fuse taskspecific representations with more abstract representations to encode attributes in a more compositional manner.
2020.acl-main.684.txt, 6 Conclusion,2020,"conversely, this would be complicated in domains with rich composition and nesting in logical forms, which go beyond simple features and relations.e.g., “click the third email from jeanette”, and where modeling inverse semantics is infeasible."
2020.acl-main.684.txt, 6 Conclusion,2020,"future work can also explore curriculum learning in this domain, by first learning simpler tasks, which can be compositionally invoked in explanations for complex tasks."
2020.acl-main.684.txt, 6 Conclusion,2020,"here, we posed the learning of web-based tasks as similar to instruction-following problem, with no aspect of interactivity or exploration of the environment."
2020.acl-main.684.txt, 6 Conclusion,2020,"in future work, the possibility of learning from a mix of explanations, exploration and a limited budget of interaction with the environment can be explored."
2020.acl-main.684.txt, 6 Conclusion,2020,our work here is a step in the direction of teachable ai agents that can learn new behavior from conversational interactions with ordinary users.
2020.acl-main.685.txt, 8 Discussion and Limitations ,2020,"however, since the functional signal is not currently influencing the sampling from the language model, this will lead to poor performance when using more general language models with weaker conditioning (e.g."
2020.acl-main.685.txt, 8 Discussion and Limitations ,2020,"moving towards integrating our findings into more realistic applications of self-play, e.g., user simulation in dialogue (schatzmann et al., 2006; shah et al., 2008), these shortcomings need to be addressed."
2020.acl-main.685.txt, 8 Discussion and Limitations ,2020,"since speakers and listeners are learning concurrently, they can co-adapt to pair-specific policies that deviate from the policies that humans learn."
2020.acl-main.685.txt, 8 Discussion and Limitations ,2020,"this pathological behaviour of self-play is not specific to language and extends to other policies (carroll et al., 2019)."
2020.acl-main.686.txt, 6 Conclusion,2020,we hope hat can open up an avenue towards efficient transformer deployments for real-world applications.
2020.acl-main.686.txt, 6 Conclusion,2020,we propose hardware-aware transformers (hat) framework to solve the challenge of efficient deployments of transformer models on various hardware platforms.
2020.acl-main.687.txt, 8 Conclusion,2020,our work provides a foundation for future work into simpler and more computationally efficient neural machine translation.
2020.acl-main.688.txt, 7 Conclusion,2020,"in transfer learning, we can also transfer the alignment."
2020.acl-main.688.txt, 7 Conclusion,2020,"therefore, a promising research direction to investigate would involve the development and assessment of improved initialisation methods that would more efficiently yield the benefits of the model transfer."
2020.acl-main.688.txt, 7 Conclusion,2020,"therefore, models can train and converge faster, which is useful in high-resource settings."
2020.acl-main.69.txt,5 Discussion,2020,verb-focused rules help approach long-distance dependencies and reduce the need for explicit sentence simplification by breaking down a sentence into clauses while custom rules like implications serve a purpose similar to a reranker to discard irrelevant questions but with increased determinism.
2020.acl-main.69.txt,5 Discussion,2020,"while our work focuses on sentence-level qg, it would be interesting to see how questions generated from verbnet predicates would have an impact on multi-sentence or passage level qg, where the verb-agnostic states of the participants would change as a function of multiple verbs."
2020.acl-main.692.txt, 6 Conclusions and Future Work ,2020,"another interesting avenue is applying this to unsupervised nmt, which is highly sensitive to domain mismatch (marchisio et al., 2020; kim et al., 2020)."
2020.acl-main.692.txt, 6 Conclusions and Future Work ,2020,"this work just scratches the surface with what can be done on the subject; possible avenues for future work include extending this with multilingual data selection and multilingual lms (conneau and lample, 2019; conneau et al., 2019; wu et al., 2019; hu et al., 2020), using such selection methods with domain-curriculum training (zhang et al., 2019; wang et al., 2019b), applying them on noisy, web-crawled data (junczys-dowmunt, 2018) or for additional tasks (gururangan et al., 2020)."
2020.acl-main.692.txt, 6 Conclusions and Future Work ,2020,"we hope this work will encourage more research on finding the right data for the task, towards more efficient and robust nlp."
2020.acl-main.693.txt, 4 Conclusions and future work ,2020,we finish by noting that the original mert procedure developed for smt optimised document-level bleu and with our procedure we reintroduce this to nmt.
2020.acl-main.693.txt, 4 Conclusions and future work ,2020,"while the scope of this work does not extend to sampling sentences given document context, this would be an interesting direction for future work."
2020.acl-main.694.txt, 4 Discussions and Conclusions ,2020,"also, we plan to consider more structured latent variables beyond modeling the sentence-level variation as well as to apply our vnmt model to more language pairs."
2020.acl-main.694.txt, 4 Discussions and Conclusions ,2020,we plan to conduct a more in-depth investigation into actual multimodality condition with high-coverage sets of plausible translations.
2020.acl-main.697.txt, 5 Summary & Conclusion ,2020,"subsequent research and practice have delivered on page’s minimum desiderata for an awe system; current research is working to address the outstanding challenges dealing with a variety of languages, content domains, and writing tasks."
2020.acl-main.697.txt, 5 Summary & Conclusion ,2020,"we believe that we, as researchers, can help users find value in our technology by considering the goals, engaging partners from other relevant disciplines, and designing the tools as well as their evaluations to focus on specific types of use."
2020.acl-main.698.txt, 6 Conclusion,2020,"implications for future work on pretrained language models.(i) both factual knowledge and logic are discrete phenomena in the sense that sentences with similar representations in current pretrained language models differ sharply in factuality and truth value (e.g., “newton was born in 1641” vs. “newton was born in 1642”)."
2020.acl-main.699.txt, 4 Conclusions,2020,future work includes a deeper qualitative analysis of which (type of) papers are being cited; a more fine-grained analysis of different research topics in nlp to determine whether changes are more prevalent within certain areas than others; or extending the analysis to a larger set of the papers in the acl anthology.
2020.acl-main.699.txt, 4 Conclusions,2020,"in addition, our analysis is limited to studying the age of the papers cited in the acl anthology – it does not make any claims about the complex network effects involved in researchers from particular institutions, countries, or sub-fields, and it does not study other venues that also publish nlp papers."
2020.acl-main.699.txt, 4 Conclusions,2020,"we presented an analysis of citations in publications from major acl venues between 2010 and 2019, focusing on the distribution of the age of cited papers."
2020.acl-main.70.txt,6 Conclusion,2020,it dynamically assigns each arriving document into an existing cluster or generating a new cluster based on the poly urn scheme.
2020.acl-main.70.txt,6 Conclusion,2020,"more importantly, osdm tried to incorporate semantic information in the proposed graphical representation model to remove the term ambiguity problem in short-text clustering."
2020.acl-main.700.txt, 8 Conclusion,2020,"modeling demographic and personal variables as dynamic and social will allow to reflect the variety of ways individuals construct their identity by language, and to conduct novel sociolinguistic experiments to better understand the development in online communities."
2020.acl-main.700.txt, 8 Conclusion,2020,"reflecting the current possibilities with available web and mobile data, i propose to expand the existing user modeling approaches in deep learning models with contextual personalization, mirroring different facets of one user in dynamic, socially conditioned vector representations."
2020.acl-main.701.txt, 6 Taking the ToU idea forward ,2020,this includes refining and perhaps expanding the questions; better defining the answers and evaluation procedures; building mrc corpora based on the tou; and developing better-performing systems.
2020.acl-main.701.txt, 6 Taking the ToU idea forward ,2020,"we ourselves are working on all four, and we welcome collaboration."
2020.acl-main.703.txt, 8 Conclusions,2020,"future work should explore new methods for corrupting documents for pretraining, perhaps tailoring them to specific end tasks."
2020.acl-main.704.txt, 7 Conclusion,2020,"future research directions include multilingual nlg evaluation, and hybrid methods involving both humans and classifiers."
2020.acl-main.705.txt, 5 Conclusion,2020,"for future work, we will explore the extension of conditional mlm to multimodal input such as image captioning."
2020.acl-main.706.txt, 7 Conclusions and Future Directions ,2020,an rl agent that leverages natural language descriptions of physical events to reason about the solution for a given goal (similar to zhong et al.(2020)) or for reward shaping (similar to goyal et al.(2019)) could be a compelling line of future research.
2020.acl-main.706.txt, 7 Conclusions and Future Directions ,2020,we hope that the dataset we collected will facilitate research in using natural language for physical reasoning.
2020.acl-main.707.txt, 5 Conclusion,2020,"in future work, we plan to add paraphrase generation to generate diverse simple sentences."
2020.acl-main.708.txt, 8 Conclusion,2020,"to promote the research in this direction, we host a logicnlg challenge2 to help better benchmark the current progress."
2020.acl-main.710.txt, 7 Conclusion and Future Work ,2020,"in future work, we plan to investigate how target phrase order affects the generation behavior, and further explore set generation in an order invariant fashion."
2020.acl-main.711.txt, 8 Conclusion,2020,"language models conflate the two, so developing methods that are nuanced enough to recognize this difference is key to future progress."
2020.acl-main.713.txt, 6 Conclusions and Future Work ,2020,"in the future, we plan to incorporate more comprehensive event schemas that are automatically induced from multilingual multimedia data and external knowledge to further improve the quality of ie."
2020.acl-main.713.txt, 6 Conclusions and Future Work ,2020,"our framework is also proved to be language-independent and can be applied to other languages, and it can benefit from multi-lingual training."
2020.acl-main.713.txt, 6 Conclusions and Future Work ,2020,we also plan to extend our framework to more ie subtasks such as document-level entity coreference resolution and event coreference resolution.
2020.acl-main.714.txt, 6 Conclusion and Future Work ,2020,"in the future work, it would be interesting to further explore how the model can be adapted to jointly extract role fillers, tackles coreferential mentions and constructing event templates."
2020.acl-main.715.txt, 5 Conclusion,2020,"in the future, we plan to apply ceon-lstm to other related nlp tasks (e.g., event extraction, semantic role labeling) (nguyen et al., 2016a; nguyen and grishman, 2018a)."
2020.acl-main.716.txt, 7 Conclusion and Future Work ,2020,"in our ongoing research, we are investigating the expansion of this technique to language pairs where english may not be involved."
2020.acl-main.718.txt, 7 Conclusion,2020,we hope that rams will stimulate further work on multi-sentence argument linking.
2020.acl-main.72.txt,6 Conclusion,2020,our systematic study will pave the way to future research about the effective construction of dictionaries for text analytics.
2020.acl-main.720.txt, 6 Conclusions,2020,"in future work, we will explore whether the observed trends hold in much larger polyglot settings, e.g.the wikiann ner corpus (pan et al., 2017b)."
2020.acl-main.720.txt, 6 Conclusions,2020,"on the other hand, when the objective is to maximize performance on a single target language it may be possible to improve the proposed fine-tuning approach further using methods such as elastic weight consolidation (kirkpatrick et al., 2016)."
2020.acl-main.720.txt, 6 Conclusions,2020,"with this in mind, exploring different training strategies, such as multi-objective optimization, may prove beneficial (sener and koltun, 2018)."
2020.acl-main.721.txt, 8 Conclusion,2020,future extensions of this work involve a more general pre-training objective allowing for the learned representations to be useful in many tasks as well as distantly or semi-supervised approaches to benefit from more data.
2020.acl-main.722.txt, 6 Conclusion,2020,possible future directions include using more sophisticated feature design and combinations of candidate retrieval methods.
2020.acl-main.723.txt, 7 Conclusions and Future Work ,2020,"in our experiment, a “relevant item” is a person classified by experts as being at risk of attempting suicide in the near future."
2020.acl-main.723.txt, 7 Conclusions and Future Work ,2020,"there are certainly limitations in our study and miles to go before validating our approach in the real world, but our framework should make it easy to integrate and explore other individual rankers, document rankers and explanation mechanisms, and to actually build user interfaces like the schematic in figure 1."
2020.acl-main.723.txt, 7 Conclusions and Future Work ,2020,"we introduced htbg, a new evaluation measure, as a step toward moving beyond risk classification to a paradigm in which prioritization is the focus, and where time matters."
2020.acl-main.724.txt, 6 Conclusion,2020,"as future work, we intend to apply cluhtm in other representative applications on the web, such as hierarchical classification by devising a supervised version of cluhtm."
2020.acl-main.724.txt, 6 Conclusion,2020,our new method exploits a more elaborate (global) semantic data representation – cluwords – as well as an original application of a stability measure to define the “shape” of the hierarchy.
2020.acl-main.724.txt, 6 Conclusion,2020,we also intend to incorporate some type of attention mechanism into our methods to better understand which cluwords are more important to define certain topics.
2020.acl-main.725.txt, 6 Conclusions,2020,another interesting direction is to generate a class name hierarchy via language model probing.
2020.acl-main.725.txt, 6 Conclusions,2020,"for example, we may expand the set {“machine translation”, “information extraction”, “syntactic parsing”} to acquire more nlp task concepts."
2020.acl-main.725.txt, 6 Conclusions,2020,"in the future, we plan to expand the method scope from expanding concrete entity sets to more abstract concept sets."
2020.acl-main.726.txt, 5 Conclusion,2020,"in our future work, we will consider extending it to graph-based methods such as gcn for graph data, and to generation-based methods such as gan for adversarial learning."
2020.acl-main.726.txt, 5 Conclusion,2020,"our current method is designed only for traditional text classification methods such as lstm, cnn, and transformer."
2020.acl-main.727.txt, 7 Conclusion,2020,future visual grounding methods should be tested with a more comprehensive experimental setup and datasets for proper evaluation.
2020.acl-main.729.txt, 7 Conclusion,2020,"for example, action span extraction is related to both semantic role labeling (he et al., 2018) and extraction of multiple facts from text (jiang et al., 2019) and could benefit from innovations in span identification and multitask learning."
2020.acl-main.729.txt, 7 Conclusion,2020,"lastly, our work provides a technical foundation for investigating user experiences in language-based human computer interaction."
2020.acl-main.732.txt, 7 Discussion & Conclusion ,2020,"although we apply our joint model on arabic, this model provides a framework for other languages that include diacritics whenever resources become available."
2020.acl-main.732.txt, 7 Discussion & Conclusion ,2020,"although we observed improvements in terms of generalizing beyond observed data when using the proposed linguistic features, the oov performance is still an issue for diacritic restoration."
2020.acl-main.732.txt, 7 Discussion & Conclusion ,2020,this shows the importance of considering additional linguistic information at morphological and/or sentence levels.
2020.acl-main.733.txt, 7 Conclusion,2020,"thus, ssa might be replaced with a less costly architecture while our model might be improved by conditioning on semantics and jointly decoding from a variable number of sources."
2020.acl-main.734.txt, 6 Conclusion,2020,further experiments and analyses also demonstrate the robustness of wm-seg in the cross-domain scenario as well as when using different lexicons and wordhood measures.
2020.acl-main.734.txt, 6 Conclusion,2020,"the framework follows the sequence labeling paradigm, and the encoders and decoders in it can be implemented by various prevailing models."
2020.acl-main.735.txt, 6 Conclusion,2020,"for future work, we plan to apply the same methodology to other nlp tasks."
2020.acl-main.735.txt, 6 Conclusion,2020,"overall, this work presents an elegant way to use autoanalyzed knowledge and enhance neural models with existing nlp tools."
2020.acl-main.737.txt, 7 Conclusion,2020,"another research avenue that could be explored is modeling specific user preferences: since each user likely favors a certain set of character substitutions, allowing user-specific parameters could improve decoding and be useful for authorship attribution."
2020.acl-main.737.txt, 7 Conclusion,2020,"while these mappings provide a convenient way to avoid formalizing the complex notions of the phonetic and visual similarity, they are restrictive and do not capture all the diverse aspects of similarity that idiosyncratic romanization uses, so designing more suitable priors via operationalizing the concept of character similarity could be a promising direction for future work."
2020.acl-main.739.txt, 6 Conclusions,2020,"beyond word embeddings, the lstm benefits from additional embeddings indicating the tokens that are the possessor and possessee."
2020.acl-main.739.txt, 6 Conclusions,2020,standard relation extraction does not provide information about for how long relations hold true or whether relations are one-to-one or one-to-many.
2020.acl-main.739.txt, 6 Conclusions,2020,"while the work presented here targets possession relations, we believe that a similar approach could be used to to determine for how long any semantic relation holds true."
2020.acl-main.740.txt, 7 Conclusion ,2020,our findings suggest it may be valuable to complement work on ever-larger lms with parallel efforts to identify and use domain- and taskrelevant corpora to specialize models.
2020.acl-main.740.txt, 7 Conclusion ,2020,"our work points to numerous future directions, such as better data selection for tapt, efficient adaptation large pretrained language models to distant domains, and building reusable language models after adaptation."
2020.acl-main.741.txt, 5 Conclusion,2020,in this work we explored how to apply mutual information (mi) as a semantic similarity measure for continuous dense word embeddings.
2020.acl-main.742.txt, 7 Discussion,2020,"future work could also make the stronger assumption that a small number of in-domain training examples are available, and train and evaluate in a few-shot setting."
2020.acl-main.742.txt, 7 Discussion,2020,"several significant generalization challenges remaining, including improving commonsense and in-domain reasoning and table schema understanding capabilities."
2020.acl-main.742.txt, 7 Discussion,2020,"using a model that performs well on evaluation data designed for xsp, we are able to move towards addressing some of the generalization challenges on these additional evaluation sets without any indomain training data."
2020.acl-main.743.txt, 6 Conclusions,2020,"second, the lower results after including context, at least with the current architecture, are largely due to additional semantic errors via distractors in the previous and next sentences."
2020.acl-main.744.txt, 5 Discussion & Conclusion ,2020,"the work presented here can be generalized to several different flavors of the task, and indeed, constraints could be used to model the interplay between them."
2020.acl-main.744.txt, 5 Discussion & Conclusion ,2020,there are some recent works on the design of models and loss functions by relaxing boolean formulas.
2020.acl-main.745.txt, 7 Conclusion and Future Work ,2020,"finally, to extend tabert to cross-lingual settings with utterances in foreign languages and structured schemas defined in english, we plan to apply more advanced semantic similarity metrics for creating content snapshots."
2020.acl-main.745.txt, 7 Conclusion and Future Work ,2020,"first, we plan to evaluate tabert on other related tasks involving joint reasoning over textual and tabular data (e.g., table retrieval and table-to-text generation)."
2020.acl-main.745.txt, 7 Conclusion and Future Work ,2020,"second, following the discussions in § 5, we will explore other table linearization strategies with transformers, improving the quality of pretraining corpora, as well as novel unsupervised objectives."
2020.acl-main.745.txt, 7 Conclusion and Future Work ,2020,this work also opens up several avenues for future work.
2020.acl-main.746.txt, 8 Conclusion,2020,"the scalar valued, multi-attribute nature of uds provides for a distinct structured prediction problem as compared to other existing representations."
2020.acl-main.746.txt, 8 Conclusion,2020,we envision future efforts exploring the interactions between improving the underlying graphstructure prediction and ever-better correlations to human judgements on individual properties.
2020.acl-main.75.txt,5 Conclusions,2020,"moreover, we would love to apply the proposed model to other problems, such as general humor recognition, irony discovery, and sarcasm detection, as the future work."
2020.acl-main.750.txt, 8 Conclusions,2020,future work will focus on domain adaptation at the embedding layer.
2020.acl-main.750.txt, 8 Conclusions,2020,robustness of nlp models is essential to their wider adoption and usability.
2020.acl-main.751.txt, 6 Conclusions and Future Work ,2020,"interesting future work includes applying our techniques to different taxonomies (e.g., biomedical) and training a model for different attributes."
2020.acl-main.751.txt, 6 Conclusions and Future Work ,2020,"our proposed model, txtract, is both efficient and effective: it leverages the taxonomy into a deep neural network to improve extraction quality and can extract attribute values on all categories in parallel."
2020.acl-main.753.txt, 8 Conclusion,2020,"instead, by providing a new analysis of the conditional vae objective to improve it in a principled way and incorporating an auxiliary decoding objective, we measurably prevented posterior collapse."
2020.acl-main.754.txt, 8 Conclusion,2020,"notably, multidds is not limited to nmt, and future work may consider applications to other multilingual tasks."
2020.acl-main.754.txt, 8 Conclusion,2020,"we extend and improve over previous work on dds (wang et al., 2019b), with a more efficient algorithmic instantiation tailored for the multilingual training problem and a stable reward to optimize multiple objectives."
2020.acl-main.756.txt, 5 Conclusions ,2020,"because it is artificial to use synthetic data for training a filter classifier, future work can focus on a better objective that models parallelism more smoothly."
2020.acl-main.756.txt, 5 Conclusions ,2020,future work also includes extending the method to low-resource languages not covered by multilingual bert.
2020.acl-main.757.txt, 4 Conclusions,2020,"in the future, we believe more work on alleviating context control problem has the potential to improve translation performance as quantified in table 3."
2020.acl-main.759.txt, 6 Conclusion and Future Work ,2020,"currently, the tclda are trained on student essays, while the tcpr only works on the source article."
2020.acl-main.759.txt, 6 Conclusion and Future Work ,2020,"one of our next steps is to investigate the impact of tc extraction methods on a corresponding awe system (zhang et al., 2019), which uses the feature values produced by aesrubric to generate formative feedback to guide essay revision."
2020.acl-main.759.txt, 6 Conclusion and Future Work ,2020,"this leads to an interesting future investigation direction, which is training the aesneural using the gold standard that can be extracted automatically."
2020.acl-main.760.txt, 6 Discussion,2020,"depending on the application, a less accurate but faster linker might be a better choice (e.g.for all clinical notes at a medical institution)."
2020.acl-main.760.txt, 6 Discussion,2020,deploying our system in a large-volume clinical setting would likely require several alterations.
2020.acl-main.760.txt, 6 Discussion,2020,future work will consider additional methods for integrating ontology structure into representation learning.
2020.acl-main.760.txt, 6 Discussion,2020,"lbp for lower back pain), and 14% are mentions containing some other abbreviation (a shorted word, e.g.post nasal drip for posterior rhinorrhoea, or a partial acronym, seizure d / o for epilepsy)."
2020.acl-main.760.txt, 6 Discussion,2020,"the main computational barrier to labeling a large amount of data, the speed of prediction, can be addressed by using an accurate candidate selection system to prune the number of concepts considered."
2020.acl-main.761.txt, 7 Conclusion,2020,an end-to-end approach or a query reformulation step (re-writing claims to be similar to fever) might make the model more resilient as new attacks are introduced.
2020.acl-main.761.txt, 7 Conclusion,2020,"future work in multi-hop reasoning could represent the relation between consecutive pieces of evidence and future work in temporal reasoning could incorporate numerical operations with bert (andor et al., 2019)."
2020.acl-main.762.txt, 6 Conclusion,2020,"for example, we can use font similarity techniques and enable users to pick a group of fonts, or to provide increased flexibility for the fonts available to users."
2020.acl-main.764.txt, 8 Conclusion and Future Work ,2020,"in this work, we investigate whether the experiment setting itself is informative for predicting the evaluation scores of nlp tasks."
2020.acl-main.764.txt, 8 Conclusion and Future Work ,2020,we believe that modeling domain shift is a promising future direction to improve performance prediction.
2020.acl-main.764.txt, 8 Conclusion and Future Work ,2020,"while investigating the systematic implications of model structures or hyperparameters is practically infeasible in this study, we may use additional information such as textual model descriptions for modeling nlp models and training procedures more elaborately in the future."
2020.acl-main.764.txt, 8 Conclusion and Future Work ,2020,"while this discovery is a promising start, there are still several avenues on improvement in future work."
2020.acl-main.765.txt, 6 Conclusion and Future Work ,2020,further investigations are thus required to fully understand how narratives can be effectively used in dialogue generation.
2020.acl-main.766.txt, 6 Conclusion,2020,"in the future, we will investigate a hub language ranking/selection model a la lin et al.(2019)."
2020.acl-main.766.txt, 6 Conclusion,2020,"more importantly, we hope that by providing new dictionaries and baseline results on several language pairs, we will stir the community towards evaluating all methods in challenging scenarios that include under-represented language pairs."
2020.acl-main.767.txt, 6 Conclusions,2020,there are several directions for future work including better architecture design for utilizing structured meta-data and replacing the two-stage framework with a multi-task generation model that can jointly identify helpful context for the task and perform corresponding text generation.
2020.acl-main.768.txt, 8 General Discussion & Conclusion ,2020,"pragmatic or logical reasoning was not diagnosable for the other scales, whose meaning was not fully understood by our models (as most scalar pairs were treated as synonymous)."
2020.acl-main.768.txt, 8 General Discussion & Conclusion ,2020,this work is an initial step towards rigorously investigating the extent to which nli models learn semantic versus pragmatic inference types.
2020.acl-main.769.txt, 7 Conclusion,2020,"additionally, we extend our methods to combat multiple bias patterns simultaneously."
2020.acl-main.769.txt, 7 Conclusion,2020,future work may include developing debiasing strategies that do not require prior knowledge of bias patterns and can automatically identify them.
2020.acl-main.77.txt,6 Conclusion and Future Work,2020,"in the future, we will do more tests and surveys on the improvement of business objectives such as user experience, user engagement and service revenue."
2020.acl-main.770.txt, 7 Conclusion,2020,several challenges in this direction of research may include extending the debiasing methods to overcome multiple biases at once or to automatically identify the format of those biases which simulate a setting where the prior knowledge is unavailable.
2020.acl-main.773.txt, 7 Conclusion,2020,"finally, we would like to point out that our methods and results here do not mean to belittle the importance of collecting clean/unbiased data."
2020.acl-main.773.txt, 7 Conclusion,2020,joint efforts are needed for promoting unbiased models that learn true semantics; and we hope our paper can encourage more work towards this important direction.
2020.acl-main.773.txt, 7 Conclusion,2020,"since none of our methods is biastype specific, we believe these results can also be generalized to other similar lexical biases."
2020.acl-main.773.txt, 7 Conclusion,2020,"therefore, our work stresses that it is also very important to encourage the development of models that are unlikely to exploit these inevitable biases/shortcuts in the dataset."
2020.acl-main.774.txt, 6 Conclusion,2020,"humans are able to make finer distinctions between meanings than is being captured by current annotation approaches; we advocate the community strives for systems that can do the same, and therefore shift away from categorical nli labels and move to something more fine-grained such as our unli protocol."
2020.acl-main.775.txt, 6 Conclusion and Further Directions ,2020,"an interesting additional use-case for our joint decoder is when a downstream task, e.g., relation extraction, requires output structures from both a parser and a tagger."
2020.acl-main.775.txt, 6 Conclusion and Further Directions ,2020,another limitation of our current work is that our joint decoder only produces projective dependency parse trees.
2020.acl-main.775.txt, 6 Conclusion and Further Directions ,2020,future community efforts on a unified representation of flat structures for all languages would facilitate further research on linguistically-motivated treatments of headless structures in “headful” dependency treebanks.
2020.acl-main.775.txt, 6 Conclusion and Further Directions ,2020,our study has been limited to a few treebanks in ud partially due to large variations and inconsistencies across different treebanks.
2020.acl-main.775.txt, 6 Conclusion and Further Directions ,2020,"to handle non-projectivity, one possible solution is pseudo-projective parsing (nivre and nilsson, 2005)."
2020.acl-main.775.txt, 6 Conclusion and Further Directions ,2020,we leave it to future work to design a non-projective decoder for joint parsing and headless structure extraction.
2020.acl-main.776.txt, 4 Conclusion,2020,"another interesting line of research would be to evaluate the contribution of higher-order features in a cross-lingual setting, leveraging structure learned from larger treebanks to underresourced languages."
2020.acl-main.776.txt, 4 Conclusion,2020,"considering the exact match of complete parse trees or all modifiers of a word, second-order models exhibit an advantage over first-order ones."
2020.acl-main.776.txt, 4 Conclusion,2020,our results indicate that even a powerful encoder as bert can still benefit from explicit output structure modelling; this would be interesting to explore in other nlp tasks as well.
2020.acl-main.777.txt, 6 Conclusion,2020,"overall, our approach is highly effective for chunking, ner and slot filling, and can be easily extended to solve other sequence labeling problems in both supervised and semi-supervised settings."
2020.acl-main.778.txt, 7 Conclusion,2020,another area for future work is to explore what information treebank vectors encode.
2020.acl-main.778.txt, 7 Conclusion,2020,"future work should also test even simpler strategies which do not use the las of previous parses to gauge the best treebank vector, e. g. always picking the largest treebank."
2020.acl-main.778.txt, 7 Conclusion,2020,"in experiments with czech, english and french, we investigated treebank embedding vectors, exploring the ideas of interpolated vectors and vector weight prediction."
2020.acl-main.778.txt, 7 Conclusion,2020,"interpolating treebank vectors adds a layer of opacity, and, in future work, it would be interesting to carry out experiments with synthetic data, e. g. varying the number of unknown words, to get a better understanding of what they may be capturing."
2020.acl-main.778.txt, 7 Conclusion,2020,"we plan to explore other methods to predict tree-bank vectors, e. g. neural sequence modelling, and to apply our ideas to the related task of language embedding prediction for zero-shot learning."
2020.acl-main.79.txt,5 Conclusion,2020,"some interesting observations include the effects of regions and the sensitivity of gnn-based models, which open potentials for further improvements that we plan to address in our future work."
2020.acl-main.8.txt,7 Conclusions,2020,"in the future, we aim to leverage these pre-trained models to advance sota on downstream conversational tasks, such as knowledge-grounded conversations or question answering."
2020.acl-main.81.txt,5 Conclusions,2020,"beyond csc, spellgcn can be generalized to other situations where specific prior knowledge is available, and to other languages by leveraging specific similarity graphs analogously."
2020.acl-main.81.txt,5 Conclusions,2020,"our method can also be adapted to grammar error correction, which needs insertion and deletion, by utilizing more flexible extractors such as levenshtein transformer (gu et al., 2019)."
2020.acl-main.81.txt,5 Conclusions,2020,we leave this direction to future work.
2020.acl-main.82.txt,5 Conclusion,2020,"as future work, we plan to extend soft-masked bert to other problems like grammatical error correction and explore other possibilities of implementing the detection network."
2020.acl-main.84.txt,8 Conclusion,2020,"based on these results, we will explore ways to leverage the token assignment to domain adaption and few-shot learning."
2020.acl-main.84.txt,8 Conclusion,2020,"furthermore, our procedure allows a fine-grained alignment of tokens to operations."
2020.acl-main.84.txt,8 Conclusion,2020,"our procedure more than triples the velocity of annotation in comparison to previous methods, while ensuring a larger variety of different types of queries and covering a larger part of the underlying databases."
2020.acl-main.84.txt,8 Conclusion,2020,"this baseline achieves scores of up to 48% precision, which are already reasonable while also leaving large potential for improvement in future research."
2020.acl-main.84.txt,8 Conclusion,2020,we also plan to enhance the annotation process by automatically generating proposals for the nl questions and token assignments and letting the annotators only perform corrections.
2020.acl-main.84.txt,8 Conclusion,2020,we hope that this increases annotation efficiency even more.
2020.acl-main.85.txt,5 Conclusion,2020,we efficiently train our sparse representations by kernelizing the sparse inner product space.
2020.acl-main.86.txt,4 Conclusions,2020,our goal was to investigate which instance sampling method and epoch scheduling strategy gives optimal performance in a multi-task reading comprehension setting.
2020.acl-main.86.txt,4 Conclusions,2020,"this suggests that for specific cases, we observe an effect similar to data augmentation (like exposure to squad benefitting quoref performance as mentioned above) but this needs to be explored further."
2020.acl-main.86.txt,4 Conclusions,2020,"we hope that future work experiments further with dynamic sampling such as by modifying the metric (e.g., using bleu or rouge score if applicable) and/or modifying other values like number of instances per epoch based on performance metrics (not only does this effectively change learning rate, but it would also allow the model to update the sampling distribution more or less frequently)."
2020.acl-main.9.txt,5 Conclusion,2020,"in the future, we will also explore to boost the latent selection policy with reinforcement learning and extend our pre-training to support dialogue generation in other languages."
2020.acl-main.9.txt,5 Conclusion,2020,our work can be potentially improved with more fine-grained latent variables.
2020.acl-main.9.txt,5 Conclusion,2020,"to pre-train our model, two reciprocal tasks of response generation and latent recognition are carried out simultaneously on large-scale conversation datasets."
2020.acl-main.90.txt,8 Conclusion,2020,incorporating our three-way attentive pooling network into open domain conversational qa systems will be interesting future work.
2020.acl-main.94.txt,6 Conclusion and Future Work,2020,we hope that our study will provide insights into ways to improve cross-lingual embeddings by not only mapping methods but also the properties of monolingual embedding spaces.
2020.acl-main.95.txt,8 Conclusion,2020,"another direction for future work would improve few-shot approaches to wsd, which is both important for moving wsd into new domains and for modeling rare senses that naturally have less support in wsd data."
2020.acl-main.95.txt,8 Conclusion,2020,"however, we still see a large gap in performance between mfs and lfs examples, with our model still performing over 40 points better on the mfs subset."
2020.acl-main.95.txt,8 Conclusion,2020,"in this work, we address the issue of wsd systems underperforming on uncommon senses of words."
2020.acl-main.95.txt,8 Conclusion,2020,"potential directions include finding ways to obtain more informative training signal from uncommon senses, such as with different approaches to loss reweighting, and exploring the effectiveness of other model architectures on lfs examples."
2020.acl-main.95.txt,8 Conclusion,2020,this leaves better disambiguation of less common senses as the main avenue for future work on wsd.
2020.acl-main.96.txt,6 Conclusion,2020,further we plan to investigate other nlp applications that can benefit from the simple linguistic features introduced here.
2020.acl-main.96.txt,6 Conclusion,2020,"in addition, we exploit the modern deep learning machinery to improve the performance further."
2020.acl-main.96.txt,6 Conclusion,2020,"in future, we would like to extend this work for other language pairs."
2020.acl-main.97.txt,5 Conclusion,2020,"in the future, we will extend the proposed framework by considering more context (meta data) information, such as time, storylines, and comment sentiment, to further enrich our explainability."
2020.acl-main.98.txt,6 Conclusion,2020,"the complexity in durecdial makes it a great testbed for more tasks such as knowledge grounded conversation (ghazvininejad et al., 2018), domain transfer for dialog modeling, target-guided conversation (tang et al., 2019a) and multi-type dialog modeling (yu et al., 2017)."
2020.acl-main.98.txt,6 Conclusion,2020,the study of these tasks will be left as the future work.
2020.acl-main.98.txt,6 Conclusion,2020,we demonstrate usability of this dataset and provide results of state of the art models for future studies.
2020.acl-main.99.txt,6 Conclusion,2020,"in future work, we plan to conduct more empirical studies on seg and further improve its performance on new intent identification."
2020.acl-main.99.txt,6 Conclusion,2020,we also plan to conduct more case studies in applying seg to boost the performance of current zero-shot intent classification methods.
2020.acl-srw.1.txt, 6 Conclusion,2020,"while the empirical results are encouraging, important future work includes explorations of higher efficient adaptive and sparse mechanisms that can significantly cause flops and parameter reduction with minimal loss in performance."
2020.acl-srw.10.txt, 6 Conclusion ,2020,"further extensions may include studying the behavior of more powerful subword combination strategies (e.g.convolutions, self-attention) and the application of subword merging to the target side."
2020.acl-srw.10.txt, 6 Conclusion ,2020,"future extensions to this work may include applying it to character-level instead of subword representations, and using it for morphologically richer languages, especially low-resourced agglutinative ones, where our approach, together with the incorporation of linguistic information, may provide larger improvements in translation quality."
2020.acl-srw.11.txt, 7 Conclusions and Future Work ,2020,"in the future, we intend to use the english translation data of north korean news articles to create an evaluation dataset that considers differences in words, and attempt to develop a translation method using a language model with context, such as bert (devlin et al., 2019)."
2020.acl-srw.11.txt, 7 Conclusions and Future Work ,2020,"therefore, the differences in word meanings are a major challenge."
2020.acl-srw.13.txt, 5 Conclusion and Future Work ,2020,"since scar learns to drop inferable components of the input and therefore reduces noise, it can be used as a preprocessing step for machine translation and other information retrieval tasks."
2020.acl-srw.14.txt, 5 Conclusion,2020,"in the future, we will strengthen tags that contain semantic information to extract keywords for more accurate information, such as disease information, location, and size."
2020.acl-srw.15.txt, 6 Conclusions,2020,"in future work, we plan to utilize other word segmentation methods for model training."
2020.acl-srw.15.txt, 6 Conclusions,2020,the model jointly learns to perform bi-directional translation and agglutinative language stemming by utilizing the shared encoder and decoder under standard nmt framework.
2020.acl-srw.15.txt, 6 Conclusions,2020,we also plan to combine the proposed multi-task neural model with back-translation method to enhance the ability of the nmt model on target-side language modeling.
2020.acl-srw.16.txt, 5 Conclusion,2020,"in the future, we want to extend this method to language features other than words."
2020.acl-srw.17.txt, 8 Conclusion and Future Work ,2020,"next, we aim to develop an actionable bayesian game-theoretic model for social talk, focusing on decomposing its utility function."
2020.acl-srw.17.txt, 8 Conclusion and Future Work ,2020,"particularly, we seek to learn from social interaction work such as stevanovic and koski (2018) for designing the goal-directedness aspect of the model."
2020.acl-srw.18.txt, 6 Conclusion,2020,"while this would be a negative result, it is still a relevant insight and can be used as a basis for new predictions."
2020.acl-srw.19.txt, 7 Conclusion and Future Work ,2020,"future work could include finding a way to incorporate other linguistic features like case-markers, gender, number, person, tense, aspect and verb agreement information into the parser."
2020.acl-srw.2.txt, 4 Summary,2020,we plan to focus mostly on studying the possible application of gcn in this task.
2020.acl-srw.2.txt, 4 Summary,2020,we will perform extensive experiments and report results in future work.
2020.acl-srw.20.txt, 6 Conclusion,2020,"this suggests future work to reconsider how to match the training and evaluation to the actual objective of downstream applications, and thus create more reliable evaluation metrics and benchmarks."
2020.acl-srw.22.txt, 5 Conclusion & Future Work ,2020,"in future, we would like to work on effective techniques to exploit monolingual data and parallel data from other languages together to improve the translation of low-resource languages."
2020.acl-srw.23.txt, 5 Conclusions,2020,"further, we plan to use this decoder in an iterative, semi-supervised learning scenario akin to co-training (blum and mitchell, 1998)."
2020.acl-srw.23.txt, 5 Conclusions,2020,"in the longer term, we envision a decoder with constraints, which enforces that the generated rules follow correct odin syntax."
2020.acl-srw.23.txt, 5 Conclusions,2020,we plan to include constraints as part of decoding to aid in rule synthesis.
2020.acl-srw.23.txt, 5 Conclusions,2020,we suspect that including such validity constraints will further improve the quality of the decoded rules.
2020.acl-srw.24.txt, 5 Conclusion,2020,future qualitative work could also suggest further variables whose inclusion would enhance our knowledge of humor perception.
2020.acl-srw.24.txt, 5 Conclusion,2020,"this could set a new standard for shared tasks which aim to model humor in future, and could outline a methodology that can be replicated with other cultures and languages."
2020.acl-srw.26.txt, 6 Conclusion,2020,"our findings shed light on the importance of modeling points of correspondence, suggesting important future directions for sentence fusion."
2020.acl-srw.27.txt, 7 Conclusions ,2020,we will release all code and datasets (tweet ids) to promote the reproducibility of our experiments.4 the readers are referred to our code to evaluate their dialogue systems for their native languages.
2020.acl-srw.28.txt, 5 Conclusion,2020,"additionally, the process of building an fst proves to be a great way to examine the validity of the linguistic analyses."
2020.acl-srw.28.txt, 5 Conclusion,2020,"future work would entail analysing and implementing more detailed underlying morphonological rules, and investigating the cross-over from fsts to neural models."
2020.acl-srw.28.txt, 5 Conclusion,2020,"if the user, prefers structural granularity or a one-to-one mapping between the computational implementation and the linguistic grammar then the decomposition approach can be taken."
2020.acl-srw.28.txt, 5 Conclusion,2020,nen shows distributed exponence; multiple morphs can contribute to the specification of a particular feature value.
2020.acl-srw.3.txt, 5 Conclusion,2020,our future work will target demonstrating the method on other languages.
2020.acl-srw.3.txt, 5 Conclusion,2020,"we also hope to address semantic paraphasia in future work and create, deploy aac systems building on the method proposed in this paper."
2020.acl-srw.30.txt, 5 Conclusion,2020,"also, we plan to extend the simple label embedding calculation methods to more sophisticated ones."
2020.acl-srw.30.txt, 5 Conclusion,2020,"for future work, we envision to apply our method to other tasks and datasets and investigate the effectiveness."
2020.acl-srw.31.txt, 7 Conclusion ,2020,"it is also interesting to apply our methods to other languages requiring word segmentation, most notably, chinese."
2020.acl-srw.31.txt, 7 Conclusion ,2020,"while the focus of this paper was on data construction, developing a higher-quality typo correction system is the future direction to pursue."
2020.acl-srw.32.txt, 6 Conclusion and Future Work ,2020,"however, some issues remain, for example, how to determine the effective threshold τ that can strictly guarantee zero cse is still unknown."
2020.acl-srw.32.txt, 6 Conclusion and Future Work ,2020,"moreover, we must develop a method for more accurately estimating the confidence scores, which is our primary focus in the next step."
2020.acl-srw.33.txt, 7 Conclusion,2020,"from the cognitive neuroscience perspective, it would be interesting to investigate if the proposed decay rnn can capture some aspects of actual neuronal behaviour and language cognition."
2020.acl-srw.33.txt, 7 Conclusion,2020,"our results here do at least indicate that the complex gating mechanisms of lstms (whose cognitive plausibility has not been established) may not be essential to their performance on many linguistic tasks, and that simpler and perhaps more cognitively plausible rnn architectures are worth exploring further as psycholinguistic models."
2020.acl-srw.33.txt, 7 Conclusion,2020,"thus, similar dynamical-system-based analysis can be extended to our settings to further understand the working of the decay rnn."
2020.acl-srw.34.txt, 5 Conclusion,2020,for the future we would like to apply our model on other cross-lingual nlp tasks such as xnli or cross-lingual semantic textual similarity.
2020.acl-srw.34.txt, 5 Conclusion,2020,we proposed a completely unsupervised method to train multilingual sentence embeddings which can be used for building a parallel corpus with no previous translation knowledge.
2020.acl-srw.35.txt, 4 Conclusion,2020,"in future work, we will extend our analysis to cover the more complex constructions mentioned in section 3."
2020.acl-srw.35.txt, 4 Conclusion,2020,"we are also considering combining our system with an abduction mechanism that uses large knowledge bases (yoshikawa et al., 2019) for handling commonsense reasoning with external knowledge."
2020.acl-srw.36.txt, 6 Conclusion and Future Work ,2020,"in the future, we plan to introduce constraints for asymmetric relations as well as extend our proposed method to leverage them."
2020.acl-srw.36.txt, 6 Conclusion and Future Work ,2020,"moreover, we plan to experiment with adapting our model to a multilingual scenario, to be able to use it in a neural machine translation task."
2020.acl-srw.37.txt,6 Conclusion,2020,"in the future, we plan to experiment with even more challenging language pairs such as japanese–russian and attempt to leverage monolingual corpora belonging to diverse language families.we might be able to identify subtle relationships among languages and approaches to better leverage assisting languages for several nlp tasks."
2020.acl-srw.38.txt,6 Conclusions and Future Work,2020,it is another promising area to be looked upon for reranking.
2020.acl-srw.38.txt,6 Conclusions and Future Work,2020,one can investigate our approach with varying beam sizes and analyzing the effect of length penalty wu et al.(2016) and comparing it with methods such as yang et al.(2018).
2020.acl-srw.38.txt,6 Conclusions and Future Work,2020,we also plan to explore the work by c¸aglar gu¨lc¸ehre et al.(2017) and c¸aglar gu¨lc¸ehre et al.(2015) that introduces language models into the existing neural architecture with methods such as shallow fusion and deep fusion.
2020.acl-srw.39.txt,6 Conclusion,2020,"in the future, it would be interesting to explore weak suervision and other data augmentation techniques to improve models’ robustness further."
2020.acl-srw.39.txt,6 Conclusion,2020,we also motivate the use of manifold mixup for further improvement.
2020.acl-srw.39.txt,6 Conclusion,2020,we present a way to aggregate prior disaster-related resources to compile a large scale tweet dataset for multi-label classification utilizing both multi-class classes and binary classes.
2020.acl-srw.4.txt, 5 Conclusion,2020,"in the field of biomedicine, the co-occurrence relationship of tags is very common and useful."
2020.acl-srw.4.txt, 5 Conclusion,2020,"we use the co-occurrence relationship between tags to design the adjacency matrix by the gcn using the data-driven method, which can also be extended to other extreme multi-label classification fields."
2020.acl-srw.40.txt,4 Discussion and Conclusion,2020,"for future work, we consider conducting the same experiments on cola, a dataset for judging the grammatical acceptability of a sentence (warstadt et al., 2019)."
2020.acl-srw.40.txt,4 Discussion and Conclusion,2020,"nonetheless, we conclude with encouragement for the community to look deeper into interpretation-based methods and their connections with semantic and syntactic theory in linguistics."
2020.acl-srw.42.txt,5 Conclusion,2020,"we believe this factorization prevents the model from memorizing spurious correlations in the data, and note that similar ideas may be useful in other natural language tasks."
2020.acl-srw.43.txt,6 Conclusion and Future Work,2020,our future agenda includes further bifurcating and exploring the specific types of victim blaming and the efficacy of the proposed approach on such a multi label classification task.
2020.acl-srw.43.txt,6 Conclusion and Future Work,2020,the prevalence of rape culture and the subsequent victim blaming on unsolicited social media forums like twitter has not been studied from a computational linguistic perspective before.
2020.acl-srw.43.txt,6 Conclusion and Future Work,2020,we anticipate that this study encourages further research on how victims of sexual assault are portrayed on social media.
2020.acl-srw.43.txt,6 Conclusion and Future Work,2020,we plan to explore the different weighting factors for the language modelling loss and classification loss described in section 4 to determine if weighting factors can help customize the auxiliary loss for different tasks.
2020.acl-srw.6.txt, 5 Summary,2020,it helps achieve better generalization and utilization of the training datasets.
2020.acl-srw.6.txt, 5 Summary,2020,our research aims to apply the current work on transfer learning to new tasks and also find novel methods to obtain better multi-task learning models.
2020.acl-srw.6.txt, 5 Summary,2020,transfer learning is a promising area of research for deep neural network based machine learning models.
2020.acl-srw.8.txt, 7 Conclusion,2020,"this research aims to transfer word binary attributes (e.g., gender) for applications such as data augmentation of a sentence."
2020.acl-srw.8.txt, 7 Conclusion,2020,"we can transfer the word attribute with analogy of word vectors, but it requires explicit knowledge whether the input word has the attribute or not (e.g., man ∈ gender, woman ∈ gender, person /∈ gender)."
2020.acl-srw.9.txt, 5 Discussion and conclusion ,2020,learning an unbalanced topic model from unbalanced text collection is a non-trivial task for all of the existing modelling methods.
2020.acl-srw.9.txt, 5 Discussion and conclusion ,2020,we described our approach in terms of plsa regularization and brought theoretical justification for the rtopicprior regularizer.
2020.emnlp-main.1.txt,"
7 Conclusion
",2020,"our work contributes a new application to the growing literature on causal inference from text (egami et al., 2018), in the setting of text as a treatment."
2020.emnlp-main.1.txt,"
7 Conclusion
",2020,"our work could be improved also by including discourse properties (coherence, cohesiveness)."
2020.emnlp-main.1.txt,"
7 Conclusion
",2020,"specifically, our findings in section 5 pave the way towards answering the causal question: would attacking a certain type of sentence (e.g., questions or expressions of confusion) in an argument increase the probability of persuading the opinion holder?"
2020.emnlp-main.1.txt,"
7 Conclusion
",2020,we leave this analysis to future work.
2020.emnlp-main.1.txt,"
7 Conclusion
",2020,we studied how to detect attackable sentences in arguments for successful persuasion.
2020.emnlp-main.10.txt,"
6.5 Future Directions
",2020,a future extension could explore more robust techniques for identifying abstract chains which do not make such assumptions.
2020.emnlp-main.10.txt,"
6.5 Future Directions
",2020,extending the proposed approaches for longer chains is an important future direction.
2020.emnlp-main.10.txt,"
6.5 Future Directions
",2020,"nonetheless, a useful future direction is exploring answer prediction and explanation prediction as joint goals, and perhaps they can benefit each other."
2020.emnlp-main.101.txt,"
6 Concluding Remarks
",2020,successes in these investigations will certainly extend the applicability of vae to much broader application domains and model families.
2020.emnlp-main.101.txt,"
6 Concluding Remarks
",2020,thus it will be interesting to investigate the performance of rrt in other applications of vae beyond topic modelling.
2020.emnlp-main.103.txt,"
7 Conclusion
",2020,"future work includes using these approaches to induce model structure, develop accurate models with better interpretability, and to apply these approaches in lower data regimes."
2020.emnlp-main.103.txt,"
7 Conclusion
",2020,"hmms are a useful class of probabilistic models with different inductive biases, performance characteristics, and conditional independence structure than rnns."
2020.emnlp-main.105.txt,"
6 Conclusion
",2020,"stateof-the-art performance on zest is 12%, leaving much room for future improvement."
2020.emnlp-main.105.txt,"
6 Conclusion
",2020,"this is an interesting avenue for future work, for which zest should also be useful."
2020.emnlp-main.105.txt,"
6 Conclusion
",2020,"to facilitate future work, we make our models, code, and data available at https://allenai.org/data/ zest."
2020.emnlp-main.105.txt,"
6 Conclusion
",2020,"while we have been focused on zero shot learning from task descriptions, our framework also permits few-shot scenarios where a task description is given along with a handful of examples, making meta-learning approaches applicable."
2020.emnlp-main.109.txt,"
7 Discussion
",2020,(2) focusing on one of the most important crises of the future: water;
2020.emnlp-main.109.txt,"
7 Discussion
",2020,• unseen attribution factor: our model can generalize to unseen attributions factors.
2020.emnlp-main.109.txt,"
7 Discussion
",2020,this merits a deeper exploration with a holdout attribution set we aim to investigate in future.• flint water crisis: we were curious to know how our model performs in the wild on a data set of a different water crisis.
2020.emnlp-main.110.txt,"
6 Discussion
",2020,future semeval challenges should consider this when constructing test datasets and mention the hashtags and keywords they use for data collection.
2020.emnlp-main.110.txt,"
6 Discussion
",2020,"in the future, we hope to explore abstract topics like ‘immigration’ where differentiating between direct and indirect stance is non-trivial and ensemble models that combine the strengths of multiple methods."
2020.emnlp-main.110.txt,"
6 Discussion
",2020,"keeping in mind this vision, we investigate how an important construct in css, political approval, can be operationalized using existing nlp techniques, either through off-the-shelf sentiment and stance detection methods or through custom domain-specific methods."
2020.emnlp-main.110.txt,"
6 Discussion
",2020,"second, we only consider approval towards named entities, which we find is already a difficult task, especially for indirect stances."
2020.emnlp-main.110.txt,"
6 Discussion
",2020,"this suggests that future research should explore approaches like coreference resolution (for pronouns), word sense disambiguation (for epithets), and background knowledge (relationships to other entities)."
2020.emnlp-main.110.txt,"
6 Discussion
",2020,"while researchers interested in measuring approval should use targeted constructs like stance or targeted sentiment instead of overall sentiment to avoid conceptual confusion, current targeted methods need to be improved before they can be used in an off-the-shelf manner."
2020.emnlp-main.111.txt,"
6 Conclusion
",2020,"in the future, we will explore how to apply our model to more domains, and enhance the interpretability of the reasoning path when the model answers questions."
2020.emnlp-main.111.txt,"
6 Conclusion
",2020,"moreover, kmqa implicitly takes advantage of factual information via learning from an intermediate task and also transfers structural knowledge to enhance entity representation."
2020.emnlp-main.115.txt,"
6 Conclusion
",2020,"we also address model explainability by experimenting with a simple (yet effective) linear attention mechanism, and emphasize the interaction between models and users in the design of a novel protocol to evaluate explanations."
2020.emnlp-main.116.txt,"
6 Conclusions and Future Work
",2020,"in the future, we plan to focus on how to improve the performance of medical entity normalization when resources are limited."
2020.emnlp-main.116.txt,"
6 Conclusions and Future Work
",2020,the “generating and re-ranking” strategy is employed to integrate the proposed generative model with a discriminative similarity re-ranking method to further improve normalization performance.
2020.emnlp-main.118.txt,"
7 Conclusion
",2020,"by open-sourcing our source code and benchmark, we believe that our work can facilitate the community to inform the design and development of next-generation mrs.implications."
2020.emnlp-main.118.txt,"
7 Conclusion
",2020,our findings have clear implications for future work.
2020.emnlp-main.118.txt,"
7 Conclusion
",2020,"to express the argmax semantics, one can either use subquery or the orderby clause.8 having identified these sources, developers need to determine using which expression in what context, e.g., argmax is always expressed with subquery, and the unordered expressions in conjunctions are always sorted by characters."
2020.emnlp-main.12.txt,"
6 Conclusion
",2020,"while this is not a dataset paper (since our focus is on more on the value of natural perturbations for robust model design), we provide the natural perturbations resource for boolq constructed during the course of this study.4 this work suggests a number of interesting lines of future investigation."
2020.emnlp-main.12.txt,"
6 Conclusion
",2020,"while we leave a detailed study to future work, we expect general trends regarding the value of perturbations to hold broadly."
2020.emnlp-main.120.txt,"
7 Conclusions
",2020,it reconstructs the original order by pointing to the next sentence among encoded sentence representations using a pointer network and a transformer decoder.
2020.emnlp-main.120.txt,"
7 Conclusions
",2020,we are excited about future work that could extend our motivation and further aim at incorporating stronger hierarchy into the language model architectures and the pre-training tasks.
2020.emnlp-main.121.txt,"
9 Conclusion
",2020,"inspired by the rationale-based annotation process, we show that predicting token-level and sentence-level divergences jointly is a promising direction for further distinguishing between coarser and finer-grained divergences."
2020.emnlp-main.122.txt,"
6 Conclusion
",2020,"in future work, we will explore generalizing this approach to the multilingual setting, or applying it to the pre-train and fine-tune paradigm used widely in other models such as bert."
2020.emnlp-main.122.txt,"
6 Conclusion
",2020,"we also find our model to be especially effective on unsupervised cross-lingual semantic similarity, due to its stripping away of language-specific information allowing for the underlying semantics to be more directly compared."
2020.emnlp-main.123.txt,"
5 Conclusion
",2020,"future work includes adopting multilingual word embeddings (lample et al., 2018) to produce alignments for other languages."
2020.emnlp-main.123.txt,"
5 Conclusion
",2020,"this simple approach may be adopted for other languages with few resources, aiming to get tools for natural language understanding tasks."
2020.emnlp-main.124.txt,"
5 Conclusions
",2020,"in the future, we want to explore semi-supervised methods for sentence embedding and its transferability across domains."
2020.emnlp-main.125.txt,"
7 Discussion and Future Work
",2020,"we intend to expand our method to conduct forest alignments for making it robust against parsing errors, which are inevitable in handling large corpora."
2020.emnlp-main.125.txt,"
7 Discussion and Future Work
",2020,"we plan to apply it to a comparable corpus of partial paraphrases and investigate the performance, with the aim of creating a large-scale syntactic and phrasal paraphrase dataset."
2020.emnlp-main.126.txt,"
5 Conclusion
",2020,"the proposed method can further contribute to other semi-structured data (table, graph, etc.)related tasks, e.g."
2020.emnlp-main.126.txt,"
5 Conclusion
",2020,there still exists plenty of potentials that require future studies in this direction.
2020.emnlp-main.127.txt,"
6 Conclusion
",2020,it also uses an entity-level graph with a proposed path reasoning mechanism to infer relations more explicitly.
2020.emnlp-main.128.txt,"
6 Conclusion and Future Work
",2020,"in the future, we would adapt our method to other ie tasks to study its application scope."
2020.emnlp-main.129.txt,"
7 Conclusion and Future work
",2020,"in the future, we will extend maven to more event-related tasks like event argument extraction, event sequencing, etc."
2020.emnlp-main.129.txt,"
7 Conclusion and Future work
",2020,the results indicate that general domain ed is still challenging and maven may facilitate further research.
2020.emnlp-main.129.txt,"
7 Conclusion and Future work
",2020,"we also explore some promising directions with analytic experiments, including modeling multiple event correlations (section 5.3), utilizing the hierarchical event schema to distinguish close types (section 5.6), and improving other ed tasks with transfer learning (section 5.5)."
2020.emnlp-main.13.txt,"
6 Conclusion
",2020,"an investigation of how, whether, and why formalisms and their implementations affect probing results for tasks beyond role labeling and for frameworks beyond edge probing constitutes an exciting avenue for future research."
2020.emnlp-main.13.txt,"
6 Conclusion
",2020,"finally, assembling corpora with parallel cross-formalism annotations would facilitate further research on the effect of formalism in probing."
2020.emnlp-main.13.txt,"
6 Conclusion
",2020,"second, if possible, results on multiple formalisations of the same task should be reported and validated for several languages."
2020.emnlp-main.132.txt,"
6 Conclusion
",2020,"by designning three pre-training objectives, we can learn better pre-trained encoders customized for entity relation extraction task."
2020.emnlp-main.133.txt,"
7 Conclusion
",2020,another direction is to generalize the way in which the table and sequence interact to other types of representations.
2020.emnlp-main.133.txt,"
7 Conclusion
",2020,"in the future, we would like to investigate how the table representation may be applied to other tasks."
2020.emnlp-main.136.txt,"
6 Conclusion
",2020,"in the future, we will investigate the feasibility of incorporating classical mds guidance to abstractive models with large-scale pre-training (gu et al., 2020) and more challenging settings where each document set may contain hundreds or even thousands of documents."
2020.emnlp-main.137.txt,"
7 Conclusions and Future Work
",2020,another intriguing direction is exploring the connection between our methods and neural network interpretability.
2020.emnlp-main.137.txt,"
7 Conclusions and Future Work
",2020,"finally, although we are motivated primarily by the widespread use of topic models for identifying interpretable topics (boyd-graber et al., 2017, ch.3), we plan to explore the ideas presented here further in the context of downstream applications like document classification."
2020.emnlp-main.137.txt,"
7 Conclusions and Future Work
",2020,"in future work, we also hope to explore the effects of the pretraining corpus (gururangan et al., 2020) and teachers (besides bert) on the generated topics."
2020.emnlp-main.137.txt,"
7 Conclusions and Future Work
",2020,"in our work, as the weight on the bert autoencoder logits λ goes to one, the topic model begins to describe less the corpus and more the teacher."
2020.emnlp-main.137.txt,"
7 Conclusions and Future Work
",2020,"we believe mining this connection can open up further research avenues; for instance, by investigating the differences in such teacher-topics conditioned on the pre-training corpus."
2020.emnlp-main.137.txt,"
7 Conclusions and Future Work
",2020,we do this in order to combine the expressivity of probabilistic topic models with the precision of pretrained transformers.
2020.emnlp-main.138.txt,"
6 Conclusion
",2020,future works could focus on employing the proposed model in more downstream tasks.
2020.emnlp-main.140.txt,"
6 Conclusions and Future Work
",2020,"as a first attempt to introduce macro-level meta-features for strategy selection, we believe there is much potential to refine and improve our approach."
2020.emnlp-main.140.txt,"
6 Conclusions and Future Work
",2020,"different from traditional keyphrase extraction models mainly focusing on text, smartkpe illustrates the advantage of incorporating other modalities to help keyphrases location and salience prediction."
2020.emnlp-main.140.txt,"
6 Conclusions and Future Work
",2020,"furthermore, the smart-kpe framework can be easily adapted to other nlp tasks, and we believe there is much potential in combining smart-kpe with different models to further boost performance on opendomain kpe and other web-related tasks."
2020.emnlp-main.140.txt,"
6 Conclusions and Future Work
",2020,"one high-level idea is to add further supervision to the current selector model based on empirical web page clustering, to better train the model to develop a set of more distinct keyphrase prediction strategies, and more effectively adjust the respective selector weights."
2020.emnlp-main.140.txt,"
6 Conclusions and Future Work
",2020,we also plan to add more types of meta-features to generate richer multimodal representations.
2020.emnlp-main.141.txt,"
5 Conclusion
",2020,a state-of-the-art model was trained to establish strong baselines for future studies.
2020.emnlp-main.141.txt,"
5 Conclusion
",2020,"cmumoseas contains 20 annotated labels including sentiment (and subjectivity), emotions, and personality traits."
2020.emnlp-main.141.txt,"
5 Conclusion
",2020,"we believe that data of this scale presents a step towards learning human communication at a more fine-grained level, with the long-term goal of building more equitable and inclusive nlp systems across multiple languages."
2020.emnlp-main.142.txt,"
7 Conclusion
",2020,"in this work, we explore unsupervised disfluency detection by combining self-training and selfsupervised learning."
2020.emnlp-main.143.txt,"
6 Conclusion
",2020,"then, we interpret our approach by analyzing the routing coefficients, showing great variation of feature importance in different samples."
2020.emnlp-main.143.txt,"
6 Conclusion
",2020,"we believe that this work sheds light on the advantages of understanding human behaviors from a multimodal perspective, and makes a step towards introducing more interpretable multimodal language models."
2020.emnlp-main.147.txt,"
5 Conclusion
",2020,we will explore this direction in future work.
2020.emnlp-main.148.txt,"
6 Conclusion
",2020,"in the future, we will further explore how to explicitly incorporate linguistics information, such as named entities into the latent states."
2020.emnlp-main.149.txt,"
6 Conclusion and Outlook
",2020,"in future work, we will further investigate othercontent generation problems by leveraging multi-granularity copying mechanism."
2020.emnlp-main.15.txt,"
8 Conclusion
",2020,future work will be separated into two strands.
2020.emnlp-main.15.txt,"
8 Conclusion
",2020,"the first will focus on how to better model the distribution of embeddings given a morphosyntactic attribute; as mentioned above, this should yield a better probe overall."
2020.emnlp-main.15.txt,"
8 Conclusion
",2020,the idea is to use probe performance on different subsets of dimensions as a gauge for how much information about a linguistic property different subsets of dimensions jointly encode.
2020.emnlp-main.150.txt,"
6 Conclusion
",2020,"in the future, we will move on to develop a more general dialogue dependency parser and better incorporate dependency information into dialogue context modeling tasks."
2020.emnlp-main.152.txt,"
4 Conclusion
",2020,"in the future, we plan to extend our nonautoregressive refiner to other natural language understanding (nlu) tasks, e.g., named entity recognition (tjong kim sang and de meulder, 2003), semantic role labeling (he et al., 2018), and natural language generation (nlg) tasks, e.g., machine translation (vaswani et al., 2017), summarization (liu and lapata, 2019)."
2020.emnlp-main.153.txt,"
7 Conclusion
",2020,"in future work, we would like to apply our approach on document-level and multi-document nlu tasks."
2020.emnlp-main.154.txt,"
8 Conclusion
",2020,"another possible avenue for future work is to use crows-pairs to help directly debias lms, by in some way minimizing a metric like ours."
2020.emnlp-main.154.txt,"
8 Conclusion
",2020,"this highlights the danger of deploying systems built around mlms like these, and we expect crows-pairs to serve as a metric for stereotyping in future work on model debiasing."
2020.emnlp-main.154.txt,"
8 Conclusion
",2020,"while our evaluation is limited to mlms, we were limited by our metric, a clear next step of this work is to develop metrics that would allow one to test autoregressive language models on crowspairs."
2020.emnlp-main.156.txt,"
9 Discussion and conclusion
",2020,we hope that answers to these questions will not just demystify the empirical success of rnns but ultimately drive new methodological improvements as well.
2020.emnlp-main.157.txt,"
6 Discussion and Recommendations
",2020,"people who use “hers”, “theirs” and “themself” to align their current social gender(s) with their pronouns’ grammatical gender are marginalized when applications fail to identify those pronouns."
2020.emnlp-main.157.txt,"
6 Discussion and Recommendations
",2020,"that might include words other than pronouns, especially for multilingual models."
2020.emnlp-main.157.txt,"
6 Discussion and Recommendations
",2020,we recommend that creators of language models use the methods introduced in this paper for partially-synthetic data generation to diagnose potential bias in their models and that this text generation strategy is explored for other applications.
2020.emnlp-main.158.txt,"
7 Conclusion
",2020,future works include looking into models and continual learning algorithms to better address the challenge.
2020.emnlp-main.159.txt,"
5 Conclusions
",2020,detailed analysis is also provided to help future works investigate other critical feature enrichment and alignment methods for this task.
2020.emnlp-main.16.txt,"
7 Future Work & Conclusion
",2020,"finally, while our results naturally lead to the conclusion that we should continue to pursue models with ever more pretraining, such as gpt-3 (brown et al., 2020), we do not wish to suggest that this will be the only or best way to build models with stronger inductive biases."
2020.emnlp-main.16.txt,"
7 Future Work & Conclusion
",2020,future work might use msgs as a diagnostic tool to measure how effectively new model architectures and selfsupervised pretraining tasks can more efficiently equip neural networks with better inductive biases.
2020.emnlp-main.16.txt,"
7 Future Work & Conclusion
",2020,these models could prove to be a helpful resource for future studies looking to study learning curves of various kinds with respect to the quantity of pretraining data.
2020.emnlp-main.160.txt,"
4 Discussion
",2020,"given the lexical and visual identifiability issues explored in §1, incorporating prior human concreteness judgments (e.g., nelson et al.(2004)) for vocabulary items might enable entsharp to learn for these sorts of ambiguous lexical items."
2020.emnlp-main.160.txt,"
4 Discussion
",2020,one area for future work would be to better identify and model words that either don’t have a visual grounding or whose identified visual grounding doesn’t align with human expectation.
2020.emnlp-main.161.txt,"
5 Conclusion
",2020,"we consider extension of our model to other videoand-language tasks as future work, as well as developing more well-designed pre-training tasks."
2020.emnlp-main.163.txt,"
7 Summary Of Exploitable Weaknesses and Defense Directions
",2020,adversaries can easily exploit this fact to create misleading disinformation by generating fake articles and combining them with manually sourced images and captions.
2020.emnlp-main.163.txt,"
7 Summary Of Exploitable Weaknesses and Defense Directions
",2020,other interesting avenues for future research is to understand the importance of metadata in this multimodal setting and investigating counter-attacks to improved generators that incorporate image-text consistency.
2020.emnlp-main.163.txt,"
7 Summary Of Exploitable Weaknesses and Defense Directions
",2020,"we hope future work will address any potential limitations of this work, such as expanding the dataset to evaluate generalization across different news sources, and a larger variety of neural generators."
2020.emnlp-main.163.txt,"
7 Summary Of Exploitable Weaknesses and Defense Directions
",2020,"while this is not entirely representative of all the future challenges presented by neural fake news, we believe that this comprehensive study will provide an effective initial defense mechanism against articles with images and captions."
2020.emnlp-main.165.txt,"
5 Conclusion and Future Work
",2020,"as to future work, we plan to explore how to jointly extract entities and relations in federated settings."
2020.emnlp-main.167.txt,"
7 Conclusions and Future Work
",2020,"there are many potential directions for future work on oia, including 1) more labeled data; 2) better learning algorithm; 3) becoming crosslingual by adding support for more natural languages; 4) porting existing oie strategies on oia and evaluating the performance compared with the original ones."
2020.emnlp-main.169.txt,"
7 Conclusion
",2020,"compared with existing gcns and sans, ldgcns maintain a better balance between parameter efficiency and model capacity."
2020.emnlp-main.169.txt,"
7 Conclusion
",2020,"in future work, we would like to investigate methods to stabilize the training of weight tied models and apply our model on other tasks in natural language generation."
2020.emnlp-main.170.txt,"
8 Conclusion
",2020,"we hypothesize that beam search has an inductive bias which can be linked to the promotion of uniform information density (uid), a theory from cognitive science regarding even distribution of information in linguistic signals."
2020.emnlp-main.171.txt,"
8 Conclusions
",2020,"however, the lack of gold-truth latent structures makes it impossible to assess recovery performance."
2020.emnlp-main.171.txt,"
8 Conclusions
",2020,"we derive promising new algorithms, and novel insight into existing ones."
2020.emnlp-main.171.txt,"
8 Conclusions
",2020,"we hope that our insights, including some of our negative results, may encourage future research on learning with latent structures."
2020.emnlp-main.173.txt,"
6 Conclusion
",2020,"in the future, we are interested in social science topics, such as modeling the causal effect between mental health and the suicide decisions reflected through social media, which may help predict and stop the final decisions."
2020.emnlp-main.174.txt,"
7 Conclusion
",2020,developing methods improving both memory and inference efficiency without sacrificing task performance can open the possibility of widely deploying the powerful pretrained language models to more nlp applications.
2020.emnlp-main.174.txt,"
7 Conclusion
",2020,"future work may explore the possibility of applying masking to the pretrained multilingual encoders like mbert (devlin et al., 2019) and xlm (conneau and lample, 2019)."
2020.emnlp-main.175.txt,"
7 Conclusion and Future Work
",2020,"in the future, we will select context sentences in larger candidate space, and explore more effective ways to extend our approach to select target-side context sentences."
2020.emnlp-main.175.txt,"
7 Conclusion and Future Work
",2020,the candidate context sentences are scored and selected by two proposed strategies.
2020.emnlp-main.175.txt,"
7 Conclusion and Future Work
",2020,we propose a dynamic selection method to choose variable sizes of context sentences for documentlevel translation.
2020.emnlp-main.175.txt,"
7 Conclusion and Future Work
",2020,"we train the whole model via reinforcement learning, and design a novel reward to encourage the selection of useful context sentences."
2020.emnlp-main.176.txt,"
6 Conclusion
",2020,"future directions include exploring advanced identification and rejuvenation models that can better reflect the learning abilities of nmt models, as well as validating on other nlp tasks such as dialogue and summarization."
2020.emnlp-main.177.txt,"
7 Conclusions and Future Work
",2020,"although we focus on pronoun translations, our fine-tuning method is generic and can be used to correct other kinds of errors in machine translations, like named entities or other rare words."
2020.emnlp-main.177.txt,"
7 Conclusions and Future Work
",2020,"in future work, we will explore other such applications of our proposed methods."
2020.emnlp-main.179.txt,"
7 Conclusion and Future Works
",2020,"in the future, the proposed mgl method can potentially applied to more cross-lingual natural language understanding (xlu) tasks (conneau et al., 2018b; wang et al., 2019; lewis et al., 2019; karthikeyan et al., 2020), and be generalized to learn to learn for domain adaptation (blitzer et al., 2007), representation learning (shen et al., 2018), multi-task learning (shen et al., 2019) problems, etc. universal syntactic interpretations are valuable language interpretations, which have been developed in years of study."
2020.emnlp-main.18.txt,"
5 Conclusions
",2020,"as kermit has a clear description of the used syntactic subtrees and gives the possibility of visualizing how syntactic information is exploited during inference, it opens the possibility of devising models to include explicit syntactic inference rules in the training process."
2020.emnlp-main.18.txt,"
5 Conclusions
",2020,"finally, kermit is in the line of research of human-in-the-loop artificial intelligence (zanzotto, 2019), since it gives the opportunity to track how human knowledge is used by learning algorithms."
2020.emnlp-main.182.txt,"
8 Conclusion
",2020,our framework consists of a structured-output evaluation criterion based on reference models and a seq2seq sentence generator.
2020.emnlp-main.182.txt,"
8 Conclusion
",2020,"we believe that our framework is general and can be applied to many other structured prediction tasks in nlp, such as neural machine translation, semantic parsing and so on."
2020.emnlp-main.183.txt,"
6 Conclusion
",2020,future work includes finding applications of our novel tagging scheme in other tasks involving extracting triplets as well as extending our approach to support other tasks within sentiment analysis.
2020.emnlp-main.184.txt,"
6 Conclusion
",2020,"we believe that our work can also benefit research in multimodal speech translation (niehues et al., 2019) where the audio stream is accompanied by a video stream."
2020.emnlp-main.184.txt,"
6 Conclusion
",2020,"we hope that future research continues this line of work, especially by finding novel ways to devise adaptive policies – such as reinforcement learning models with the visual modality."
2020.emnlp-main.185.txt,"
6 Conclusion and Future Work
",2020,we hope that this new challenging evaluation set will foster further research in multilingual commonsense reasoning and cross-lingual transfer.
2020.emnlp-main.186.txt,"
6 Further Discussion and Conclusion
",2020,"both measures leverage information from singular values in different ways: econd-hm uses the ratio between two singular values, and is grounded in linear algebra and numerical analysis (blum, 2014; roy and vetterli, 2007), while svg directly utilizes the full range of singular values."
2020.emnlp-main.186.txt,"
6 Further Discussion and Conclusion
",2020,in future research we also plan an in-depth study of these factors and their relation to our spectral analysis.
2020.emnlp-main.186.txt,"
6 Further Discussion and Conclusion
",2020,"our findings, suggesting that it is possible to effectively combine these two types of language distance measures, call for further research that will advance our understanding of: 1) what knowledge is captured in monolingual and cross-lingual embedding spaces (gerz et al., 2018; pires et al., 2019; artetxe et al., 2020); 2) how that knowledge complements or overlaps with linguistic knowledge compiled into lexical-semantic and typological databases (dryer and haspelmath, 2013; wichmann et al., 2018; ponti et al., 2019); and 3) how to use the combined knowledge for more effective transfer in cross-lingual nlp applications (ponti et al., 2018; eisenschlos et al., 2019)."
2020.emnlp-main.186.txt,"
6 Further Discussion and Conclusion
",2020,"the differences in embedding spaces of different languages do not only depend on linguistic properties of the languages in consideration, but also on other factors such as the chosen training algorithm, underlying training domain, or training data size and quality (søgaard et al., 2018; arora et al., 2019; vulic et al.´ , 2020)."
2020.emnlp-main.186.txt,"
6 Further Discussion and Conclusion
",2020,"this work introduces two spectral-based measures, svg and econd-hm, that excel in predicting performance on a variety of cross-lingual tasks."
2020.emnlp-main.186.txt,"
6 Further Discussion and Conclusion
",2020,we believe that the main insights from this study will inform and guide different cross-lingual transfer learning methods and scenarios in future work.
2020.emnlp-main.187.txt,"
10 Conclusion
",2020,"the benefits are noticeable in multilingual nmt tasks, like language clustering and ranking related languages for multilingual transfer."
2020.emnlp-main.187.txt,"
10 Conclusion
",2020,"we plan to study how to deeply incorporate our typologically-enriched embeddings in multilingual nmt, where there are promising avenues in parameter selection (sachan and neubig, 2018) and generation (platanios et al., 2018)."
2020.emnlp-main.19.txt,"
5 Conclusions
",2020,"as future work, we would like to investigate complementary attention mechanisms like those of reformer (kitaev et al., 2020) or routing transformer (roy et al., 2020), push scalability with ideas like those from revnet (gomez et al., 2017), and study the performance of etc in datasets with even richer structure."
2020.emnlp-main.19.txt,"
5 Conclusions
",2020,the ability to represent dataset structure in etc further improves the model quality.
2020.emnlp-main.191.txt,"
5 Conclusion
",2020,"in future, we plan to explore how to incorporate discourse parsing into the current decision making model for end-to-end learning."
2020.emnlp-main.191.txt,"
5 Conclusion
",2020,one possibility would be to frame them as multi-task learning with a common (shared) encoder.
2020.emnlp-main.192.txt,"
7 Conclusion
",2020,"in the future, we will make a model learn commonsense with the obtained dataset and consider applying it to semantic tasks, such as anaphora resolution and discourse parsing."
2020.emnlp-main.192.txt,"
7 Conclusion
",2020,"to acquire a wider range of commonsense, it is possible to combine our method with other methods based on physical world resources, such as video captions used in swag."
2020.emnlp-main.193.txt,"
7 Conclusion
",2020,"we represent the factual structure of a document as a graph, which is utilized to learn graph-enhanced sentence representations."
2020.emnlp-main.194.txt,"
7 Conclusion
",2020,"for instance, combining our approach with additional pre-training objectives such as the inverse cloze task (chang et al., 2020) could substantially increase the amount of training data for the large quantity of smaller forums."
2020.emnlp-main.194.txt,"
7 Conclusion
",2020,researchers could also use our 140 domain-specific adapters and investigate further combination techniques to make them even more broadly applicable.
2020.emnlp-main.194.txt,"
7 Conclusion
",2020,we clearly demonstrated the effectiveness and the relevance of zero-shot transfer in many realistic scenarios and believe that our work lays foundations for a wide range of research questions.
2020.emnlp-main.195.txt,"
6 Conclusion
",2020,"it would therefore be promising to extend this line of our research to exploit larger multilingual semantic resources, in order to further improve the parsing quality."
2020.emnlp-main.195.txt,"
6 Conclusion
",2020,these amr representations could then be integrated into downstream crosslingual tasks to investigate their added value.
2020.emnlp-main.195.txt,"
6 Conclusion
",2020,we explored transfer learning techniques to enable high performance cross-lingual amr parsing.
2020.emnlp-main.197.txt,"
7 Conclusion and Future Work
",2020,"as a future research, semantically challenging cases at fine-grained level with respect to complexities of abusive/offensive (targeted) and profane (untargeted) language demand further investigation."
2020.emnlp-main.198.txt,"
4 Discussion and Conclusions
",2020,our research can be the first step toward proper intervention programs and institutional support for soldiers with mental health issues.
2020.emnlp-main.198.txt,"
4 Discussion and Conclusions
",2020,"we also plan to add domain-specific features to our model, collect more data, integrate existing suicidal risk datasets with various languages to improve performance."
2020.emnlp-main.2.txt,"
7 Conclusion
",2020,"for reported speech, bert-based models demonstrated high effectiveness in identifying speech content and source by utilizing the rich semantic information in the pretrained model."
2020.emnlp-main.20.txt,"
5 Conclusion
",2020,"while slightly underperforming electra on downstream tasks, electric is useful for its ability to quickly produce pseudo-log-likelihood scores for text."
2020.emnlp-main.200.txt,"
6 Conclusion
",2020,"in addition, it is worthwhile to further model information propagation and temporal correlation of comments in the future."
2020.emnlp-main.200.txt,"
6 Conclusion
",2020,such results can encourage future studies to develop advanced graph neural networks in better representing the interactions between heterogeneous information.
2020.emnlp-main.201.txt,"
5 Conclusion
",2020,"by modifying the cue tweet selection criteria, our method can be adapted to related domains such as sentiment analysis and emotion detection, thereby advancing the quality and quantity of data collection and offering new research directions in affective computing."
2020.emnlp-main.201.txt,"
5 Conclusion
",2020,"these new features, including labels for sarcasm perspective and unique context (e.g., oblivious texts), offer opportunities for advances in sarcasm detection.reactive supervision is generalizable."
2020.emnlp-main.202.txt,"
5 Summary and Conclusions
",2020,"this raises the question of how ssnmt will perform on really distant languages (less homographs) or when using smaller bpe sizes (more homographs), which is something that we will examine in our future work."
2020.emnlp-main.203.txt,"
6 Conclusions
",2020,the models are robust to input noise and better capture some morphological phenomena.
2020.emnlp-main.204.txt,"
8 Conclusions
",2020,"in this work, we evaluated transfer learning and distant supervision on multilingual transformer models, studying realistic low-resource settings for african languages."
2020.emnlp-main.204.txt,"
8 Conclusions
",2020,we hope that our new datasets and our reflections on assumptions in low-resource settings help to foster future research in this area.
2020.emnlp-main.206.txt,"
6 Conclusions
",2020,"in terms of future work, there are many ways of improving the segmenter system that has been presented here."
2020.emnlp-main.206.txt,"
6 Conclusions
",2020,the segmenter model itself could also benefit from the incorporation of additional text data as well as pre-training procedures.
2020.emnlp-main.206.txt,"
6 Conclusions
",2020,"we also devise two supplementary research lines, the integration of the segmentation into the translation process, so the system learns how to segment and translate at the same time, and moving from an offline mt system to a streaming mt system to improve response time, but without performance degradation."
2020.emnlp-main.206.txt,"
6 Conclusions
",2020,we plan to look into additional acoustic features as well as possible ways to incorporate asr information to the segmentation process.
2020.emnlp-main.207.txt,"
8 Conclusion and Future Works
",2020,"furthermore, we wish to explore semi-supervised and unsupervised approaches to leverage monolingual data and explore multilingual machine translation for low-resource indic languages."
2020.emnlp-main.207.txt,"
8 Conclusion and Future Works
",2020,"in future, we plan to design segmentation-agnostic aligners or aligners that can jointly segment and align sentences."
2020.emnlp-main.207.txt,"
8 Conclusion and Future Works
",2020,"laser fails to identify one-tomany/many-to-one sentence alignments, we want to address this."
2020.emnlp-main.207.txt,"
8 Conclusion and Future Works
",2020,"we want to experiment more with the laser toolkit: we used laser out-of-the-box, we want to train it with our data, and modify the model architecture to improve it further."
2020.emnlp-main.207.txt,"
8 Conclusion and Future Works
",2020,"we would also like to experiment with bert (devlin et al., 2019) embeddings for similarity search."
2020.emnlp-main.208.txt,"
6 Conclusions and Future work
",2020,"firstly, we are interested in applying csp to other related nlp areas for code-switching problems."
2020.emnlp-main.208.txt,"
6 Conclusions and Future work
",2020,"secondly, we plan to investigate the pre-training objectives which are more effective in utilizing the cross-lingual alignment information for nmt."
2020.emnlp-main.208.txt,"
6 Conclusions and Future work
",2020,there are two promising directions for the future work.
2020.emnlp-main.210.txt,"
6 Conclusion
",2020,"in future work, we will pre-train on larger corpus to further boost the performance."
2020.emnlp-main.210.txt,"
6 Conclusion
",2020,we leave different alignment approaches to be explored in the future.
2020.emnlp-main.211.txt,"
11 Conclusions
",2020,we hope our paper will inspire further work on attention-sparse architectures.
2020.emnlp-main.212.txt,"
5 Conclusion
",2020,"in the future, we will continue investigate the learning method for effectively utilizing self-generated samples and expand to other text generation tasks."
2020.emnlp-main.212.txt,"
5 Conclusion
",2020,"our work can employ on different text generation tasks, e.g., text summarization and dialogue, to enhance the key phrases (or terms) generation."
2020.emnlp-main.213.txt,"
8 Conclusions and Future Work
",2020,"a primary avenue for future work on comet will look at the impact of more compact solutions such as distilbert (sanh et al., 2019)."
2020.emnlp-main.213.txt,"
8 Conclusions and Future Work
",2020,future work will investigate the optimality of this formulation and further examine the interdependence of the different inputs.
2020.emnlp-main.214.txt,"
6 Conclusions
",2020,"in future work, we will apply our method to languages with corpora from diverse domains and also to other languages."
2020.emnlp-main.216.txt,"
5 Conclusion and Future Work
",2020,"in addition, learning universal representations among semantically-equivalent source and target sentences (wei et al., 2020) can complete the proposed method."
2020.emnlp-main.216.txt,"
5 Conclusion and Future Work
",2020,"while we showed that uncertainty-aware semantic augmentation with gaussian priors is effective, more work is required to investigate if such an approach will also be successful for more sophisticated priors."
2020.emnlp-main.217.txt,"
8 Conclusion
",2020,the newly collected subedits corpus is the largest corpus of nmt human post-edits collected so far.
2020.emnlp-main.218.txt,"
5 Conclusion
",2020,"in the future work, we will extend our method by replacing the simple role matching score with grammatical or semantic similaritybased measures to improve the alignment accuracy."
2020.emnlp-main.219.txt,"
5 Conclusion
",2020,future research could explore neural architectures and training losses tailored to our approach.
2020.emnlp-main.22.txt,"
6 Discussions and Future Work
",2020,adapting the current methods to be robust to an active eavesdropper who may alter the cover text is another interesting direction.
2020.emnlp-main.22.txt,"
6 Discussions and Future Work
",2020,"second, we will study whether this current method is still effective when a small-scale neural lm (e.g., distilgpt-2) is applied."
2020.emnlp-main.22.txt,"
6 Discussions and Future Work
",2020,there are several directions we will further explore in the future.
2020.emnlp-main.220.txt,"
6 Discussion and Conclusion
",2020,we hope this is a factor that designers of future syntactic treebanks will take into account.
2020.emnlp-main.221.txt,"
7 Conclusion
",2020,"the key contribution consisted in predicting a continuous tree with a rearrangement of the leaf nodes to shape discontinuities, and defining various ways to encode such a rearrangement as a sequence of labels associated to each word, taking advantage of the fact that in practice they are nearly ordered permutations."
2020.emnlp-main.222.txt,"
4 Conclusion
",2020,it uses global context information and syntax category labels to help improve the modeling of sub-trees and thus better sentence representations.
2020.emnlp-main.222.txt,"
4 Conclusion
",2020,our future work will include conducting experiments on dependency trees and more nlp tasks.
2020.emnlp-main.223.txt,"
7 Conclusion
",2020,"moreover, we display the ability of ted-cdb to help address the issue of insufficient or unbalanced data on other corpora and improve the performance of models for other languages."
2020.emnlp-main.224.txt,"
8 Conclusion
",2020,"in future work, we plan to extend the annotation process to also cover inter-sentential relations."
2020.emnlp-main.224.txt,"
8 Conclusion
",2020,"this intuitive representation enables scalable, high-quality annotation via crowdsourcing, which paves the way for learning robust parsers of informational discourse qa pairs."
2020.emnlp-main.225.txt,"
7 Conclusion
",2020,"in future, we plan to evaluate disa on other discourse analysis tasks."
2020.emnlp-main.226.txt,"
5 Future Work
",2020,"for example, how long the control signal would propagate for longer generations?"
2020.emnlp-main.226.txt,"
5 Future Work
",2020,one way to mitigate this issue is to leverage longer context information to identify better keywords which is subject of the future work.
2020.emnlp-main.226.txt,"
5 Future Work
",2020,other interesting direction may include incorporating the structure-level controllability by adding it as either an extra input for the conditional generator or a multitask learning supervision for each sequence.
2020.emnlp-main.226.txt,"
5 Future Work
",2020,"writingprompts (fan et al., 2018)) is a subject for future work."
2020.emnlp-main.227.txt,"
6 Conclusion & Future Work
",2020,"in the future, we will investigate on extending our approach to more areas."
2020.emnlp-main.228.txt,"
5 Conclusion
",2020,"the samples generated by our adversarial attack algorithm are more meaningful and valuable than those produced by recently proposed methods, such as the direct noise and the back translation."
2020.emnlp-main.229.txt,"
4 Conclusion
",2020,"however,as discussion in error analysis, there are several challenges to solve in the future."
2020.emnlp-main.230.txt,"
6 Discussion
",2020,"by combining neural generation with constraints based on content words included in the input sequence, we aim to achieve both reliability and naturalness."
2020.emnlp-main.230.txt,"
6 Discussion
",2020,"however, the question remains: why pursue this approach when templates perform satisfactorily?"
2020.emnlp-main.230.txt,"
6 Discussion
",2020,"if augmenting an input sequence with surface forms allows us to restrict decoding and generate utterances that are as reliable as templates, then this is an approach worth investigating further."
2020.emnlp-main.230.txt,"
6 Discussion
",2020,"in our proposed method, surface forms still need to be joined together with function words."
2020.emnlp-main.230.txt,"
6 Discussion
",2020,"in the surface realization shared task (mille et al., 2018), the deep task dataset was created by pruning function words from a dependency tree, leaving only content words remaining."
2020.emnlp-main.235.txt,"
5 Conclusion
",2020,"as future work, we will further study our method’s ability of extreme multi-label learning (bhatia et al., 2016) and different document encoders."
2020.emnlp-main.236.txt,"
9 Conclusion
",2020,"to solve the performance problem remaining for alignment-based sts methods, we proposed word rotator’s distance (wrd), a new unsupervised, emd-based sts metric."
2020.emnlp-main.237.txt,"
5 Conclusion and Future Work
",2020,"in the future, we will explore to extend the idea of disentanglement in the continual learning of other nlp tasks."
2020.emnlp-main.238.txt,"
7 Conclusions
",2020,blu employs a bidirectional retrieval to enlarge the annotated data and stabilize the training of supervised bli approaches.
2020.emnlp-main.239.txt,"
5 Conclusion and Future Work
",2020,"in the future, we plan to study different regularizers in the asymmetrical text matching task, for further exploring their effectiveness in bridging the gap between asymmetrical domains."
2020.emnlp-main.24.txt,"
8 Discussion and Conclusions
",2020,find is suitable for any text classification tasks where a model might learn irrelevant or harmful features during training.
2020.emnlp-main.24.txt,"
8 Discussion and Conclusions
",2020,"for future work, it would be interesting to extend find to other nlp tasks, e.g., question answering and natural language inference."
2020.emnlp-main.24.txt,"
8 Discussion and Conclusions
",2020,"in general, the principle of find is understanding the features and then disabling the irrelevant ones."
2020.emnlp-main.24.txt,"
8 Discussion and Conclusions
",2020,"in order to generalize the framework beyond cnns, there are two questions to consider."
2020.emnlp-main.24.txt,"
8 Discussion and Conclusions
",2020,"using the proposed framework on cnn text classifiers, we found that (i) word clouds generated by running lrp on the training data accurately revealed the behaviors of cnn features, (ii) some of the learned features might be more useful to the task than the others and (iii) disabling the irrelevant or harmful features could improve the model predictive performance and reduce unintended biases in the model."
2020.emnlp-main.24.txt,"
8 Discussion and Conclusions
",2020,we believe that our work will inspire other researchers to foster advances in both topics towards the more tangible goal of model debugging.
2020.emnlp-main.24.txt,"
8 Discussion and Conclusions
",2020,"we exemplified this with two word clouds representing each bilstm feature in appendix c, and we plan to experiment with advanced visualizations such as lstmvis (strobelt et al., 2018) in the future."
2020.emnlp-main.240.txt,"
4 Conclusion
",2020,"overall, our results encourage the development of simple multi-stage models for learning multilingual word embeddings."
2020.emnlp-main.240.txt,"
4 Conclusion
",2020,the proposed framework has the flexibility to be easily employed in hybrid setups where supervision is available for a few language pairs but is unavailable for others.
2020.emnlp-main.241.txt,"
5 Conclusion
",2020,"in the future, we will focus our attention on the topic of generalization in the presence of domain differences such as novel objects; and given goal statements in test games that were not seen by the agent during training."
2020.emnlp-main.242.txt,"
6 Conclusion
",2020,"in addition, a cost attention mechanism is designed to further improve the model’s performance and accelerate convergence time by learning the layer weights used in emd automatically."
2020.emnlp-main.243.txt,"
7 Conclusion
",2020,"in future work, the supporting span annotation can be added to the datasets of a task-oriented dialog system, for the reason that supporting span serves as a bridge between diverse descriptions of users and the normative values in the system."
2020.emnlp-main.244.txt,"
6 Discussions and Future Works
",2020,it will be interesting to integrate these two approaches to achieve further computation reduction for odqa models.
2020.emnlp-main.244.txt,"
6 Discussions and Future Works
",2020,"on gpus we cannot expect a reduction in the number of operations to translate 1:1 to lower execution times, since they are highly optimised for parallelism.3 we leave the parallelism enhancements of skylinebuilder for future work."
2020.emnlp-main.244.txt,"
6 Discussions and Future Works
",2020,we also notice that the distillation technique is complementary to the adaptive computation methods.
2020.emnlp-main.245.txt,"
6 Conclusion
",2020,we have additionally presented some analysis of ropes that should inform future work on this dataset.
2020.emnlp-main.245.txt,"
6 Conclusion
",2020,"while our model is not a neural module network, as our model uses a single fixed layout instead of different layouts per question, we believe there are enough similarities that future work could explore combining our modules with those used in other neural module networks over text, leading to a single model that could perform the necessary reasoning for multiple different datasets."
2020.emnlp-main.246.txt,"
7 Discussion and Future Work
",2020,"furthermore, it would be worth investigating how inter-annotator agreement or potential human biases manifest in traditional datasets as compared to those generated with our framework."
2020.emnlp-main.246.txt,"
7 Discussion and Future Work
",2020,outcomes of such an experiment would be sensitive to the design of the user interface as well as the study design itself.
2020.emnlp-main.246.txt,"
7 Discussion and Future Work
",2020,we assume for the purposes of this study that questions have an answer span contained in a single document and leave an extension to multi-hop questions and unanswerable questions to future research.
2020.emnlp-main.246.txt,"
7 Discussion and Future Work
",2020,we deliberately choose to leave experiments including real human annotators to future research for the following reason.
2020.emnlp-main.248.txt,"
6 Conclusion
",2020,"in addition, integrating our multi-span architecture into existing models further improves performance on drop, as is evident from the leading models on drop’s leaderboard."
2020.emnlp-main.249.txt,"
5 Conclusion & Future Work
",2020,"additional future investigations may include a deeper analysis of the mathematical and statistical properties of the weighted coefficients ρw, τw, as well as a rigorous derivation of the optimal values for the parameters of the data collection approach."
2020.emnlp-main.249.txt,"
5 Conclusion & Future Work
",2020,"as future work, we plan to collect human annotations (i) to test the proposed data collection approach on real data and (ii) to assess the validity and estimate the parameters of the proposed stochastic transitivity model."
2020.emnlp-main.25.txt,"
5 Conclusion and Future Work
",2020,we plan to address these challenges in future work.
2020.emnlp-main.250.txt,"
5 Conclusion and Future Work
",2020,"in the future, we plan to apply mft to other language models (e.g., transformerxl (dai et al., 2019) and albert (lan et al., 2019)) and for other nlp tasks."
2020.emnlp-main.251.txt,"
6 Conclusions
",2020,"in future work, we will focus on producing novel continuations of the user’s search intent, extending the approach to other domains, and automating the design of behavioral hypotheses."
2020.emnlp-main.251.txt,"
6 Conclusions
",2020,qualitative evaluation for open-ended generation is also an interesting topic on the roadmap.
2020.emnlp-main.252.txt,"
7 Conclusion and Future Work
",2020,"besides, how to enable the existing emotion-cause pair extraction models to consider the effect of context is also a meaningful task."
2020.emnlp-main.252.txt,"
7 Conclusion and Future Work
",2020,"for example, how to quantify the effect of context on the targeted causal relationship is another important task to study this type of causal relationship."
2020.emnlp-main.252.txt,"
7 Conclusion and Future Work
",2020,"furthermore, we propose a prediction aggregation module (pam) with low computational complexity, to enable the models to dynamically adjust the final prediction according to the type of emotion-cause pair contained in a document."
2020.emnlp-main.253.txt,"
6 Conclusion
",2020,"nonetheless, the missing performance of 28-46% (depending on the evaluation scenario) encourages future research on this area to take this corpus as a challenging yet reliable evaluation benchmark for further development of models specific to this domain."
2020.emnlp-main.255.txt,"
5 Conclusion
",2020,"it will be meaningful to interpret the state-of-the-art models such as xlnet (yang et al., 2019) and electra (clark et al., 2019)."
2020.emnlp-main.255.txt,"
5 Conclusion
",2020,"regarding the model-agnostic and task-agnostic properties of our method, they are applicable to any types of nlp model for various tasks, such as neural machine translation and visual question answering."
2020.emnlp-main.256.txt,"
5 Conclusion
",2020,"as fc is only one test bed for adversarial attacks, it would be interesting to test this method on other nlp tasks requiring semantic understanding such as question answering to better understand shortcomings of models."
2020.emnlp-main.256.txt,"
5 Conclusion
",2020,we improve upon previous work on universal adversarial triggers by determining how to construct valid claims containing a trigger word.
2020.emnlp-main.257.txt,"
5 Conclusion
",2020,"the study suggests that besides improving our alignment algorithms for distant languages (vulic et al.´ , 2019), we should also focus on improving monolingual word vector spaces, and monolingual training conditions to unlock a true potential of cross-lingual learning."
2020.emnlp-main.258.txt,"
8 Conclusion and Future Work
",2020,"in the future, we plan to apply fa-rnn to other tasks and explore other variants of fa-rnn."
2020.emnlp-main.258.txt,"
8 Conclusion and Future Work
",2020,"it can be initialized from res and can also learn from data, hence applicable to various scenarios including zero-shot, cold-start, low-resource and rich-resource scenarios."
2020.emnlp-main.26.txt,"
7 Discussion and conclusion
",2020,a follow-up idea is training a classifier that predicts more precisely how likely it is that the final labels will be accurate based on the development of eo.
2020.emnlp-main.26.txt,"
7 Discussion and conclusion
",2020,"finally, we believe that using attention mechanisms to study the grounding of the edits, similarly to the ideas in kohn ¨ (2018), can be an important step towards understanding how the preliminary representations are built and decoded; we want to test this as well in future work."
2020.emnlp-main.261.txt,"
6 Conclusion
",2020,we hope that this work will help the research community interpret bert for other complex tasks and explore the above open-ended questions.
2020.emnlp-main.262.txt,"
5 Conclusion
",2020,faithfulness is validated in a controlled experiment pointing more clearly to some flaws of other attribution methods.
2020.emnlp-main.262.txt,"
5 Conclusion
",2020,we used our method to study bert-based models on sentiment classification and question answering.
2020.emnlp-main.264.txt,"
6 Conclusion and Future Works
",2020,we also defined and experimented with a set of pre-training tasks and showed the improvement due to pre-training on the problem of cqa.
2020.emnlp-main.264.txt,"
6 Conclusion and Future Works
",2020,we discussed the current line of cqa work and proposed future directions by outlining the limitations of the current datasets and models.
2020.emnlp-main.266.txt,"
5 Conclusion
",2020,our method also has the potential to be a generic approach to benefit performance on the knowledge graph completion problem.
2020.emnlp-main.266.txt,"
5 Conclusion
",2020,"we first use bert to augment training data of op and oa, and then employ constrained tucker factorization to complete the knowledge graph."
2020.emnlp-main.267.txt,"
5 Conclusion
",2020,"we manually annotated 308k human utterances and 308k verbal and 81k non-verbal responses of agents, which are grounded in the agents’ first-person images with human eye-gaze locations."
2020.emnlp-main.27.txt,"
5 Discussion and Future Work
",2020,our experiments consistently show that the generation framework is suitable for sequence labeling and sets a new record for few-shot learning.
2020.emnlp-main.27.txt,"
5 Discussion and Future Work
",2020,"our simple yet effective approach is also easily extensible to other applications such as multi-label classification, or structured prediction via nested tagging patterns."
2020.emnlp-main.270.txt,"
6 Conclusion
",2020,"as a consequence, we still have very little understanding of what kind of information these languages encode."
2020.emnlp-main.270.txt,"
6 Conclusion
",2020,"examples of more sophisticated game scenarios are bidirectional conversations where multi-symbol messages are challenging to analyse (kottur et al., 2017; bouchacourt and baroni, 2019) or games with image sequences as input (santamaría-pang et al., 2019)."
2020.emnlp-main.270.txt,"
6 Conclusion
",2020,"in light of these results, it would be interesting to explore the use of unsupervised tokenisers that work well for languages without spaces (e.g."
2020.emnlp-main.270.txt,"
6 Conclusion
",2020,"sentencepiece kudo and richardson, 2018) prior to our approach and to try other word embedding models for diora, such as the character-based elmo embeddings8 (peters et al., 2018) or the more recent bert (devlin et al., 2019)."
2020.emnlp-main.270.txt,"
6 Conclusion
",2020,"to facilitate such analysis, we bundled our tests in a comprehensive and easily usable evaluation framework.9 we hope to have inspired other researchers to apply syntactic analysis techniques and encourage them to use our code to evaluate new emergent languages trained in other scenarios."
2020.emnlp-main.271.txt,"
7 Conclusion
",2020,the sub-instruction module enables the agent to attend to one particular sub-instruction at each time-step and decides whether the agent needs to proceed to the next subinstruction.
2020.emnlp-main.271.txt,"
7 Conclusion
",2020,we believe that the idea of subinstruction module and a sub-instruction annotated dataset can benefit future studies in the vln task as well as other vision-and-language problems.
2020.emnlp-main.273.txt,"
5 Conclusion
",2020,"in future work, we plan to explore taskoriented dialogues domain-adaptive pre-training methods (wu et al., 2020; peng et al., 2020) to enhance our language model backbones, and extend the framework for mixed chit-chat and taskoriented dialogue agents (madotto et al., 2020a)."
2020.emnlp-main.274.txt,"
5 Conclusion
",2020,"we would also like to explore different implementations in line with recent advances in dialog models, especially using large-scale pre-trained language models."
2020.emnlp-main.274.txt,"
5 Conclusion
",2020,"with recent trends in goal-oriented dialog systems gravitating towards end-to-end approaches (lei et al., 2018), we wish to explore a self-supervised model, which discriminatively generates samples that directly benefit the downstream models for the target task."
2020.emnlp-main.275.txt,"
7 Conclusion
",2020,"in the future, we would explore three aspects: (1) more efficient posterior information representation and corresponding prediction module, (2) the interpretability of knowledge selection and (3) knowledge selection without knowledge label."
2020.emnlp-main.278.txt,"
5 Conclusions
",2020,this might be an interesting topic for future work.
2020.emnlp-main.28.txt,"
6 Conclusion
",2020,"for the future work, we suggest to integrate the ranking models and generation model, e.g., in beam search stage or reinforcement learning using ranking score as reward signal."
2020.emnlp-main.28.txt,"
6 Conclusion
",2020,we ensemble the feedback prediction models and a humanlike scoring model to rank the machine generated dialog responses.
2020.emnlp-main.283.txt,"
6 Conclusion
",2020,for future work it would be interesting to test these sense embeddings in a wider range of applications outside wsd.
2020.emnlp-main.283.txt,"
6 Conclusion
",2020,"since the embedding space is clearly more diversified, as shown in figure 2, this may lead to improvements in other downstream tasks."
2020.emnlp-main.285.txt,"
10 Conclusion
",2020,"as future work, we plan to exploit the information brought by our embeddings to other downstream tasks, such as multilingual semantic role labeling (di fabio et al., 2019; conia et al., 2020) and cross-lingual semantic parsing (blloshmi et al., 2020)."
2020.emnlp-main.287.txt,"
5 Conclusion
",2020,"in some sentences, phrases or clauses rather than words indicate the given aspect category, future work could consider multi-grained instances, including words, phrases and clauses."
2020.emnlp-main.287.txt,"
5 Conclusion
",2020,"since directly finding the key instances for some aspect categories is ineffective, we will try to first recognize all opinion snippets in a sentence, then assign these snippets to the aspect categories mentioned in the sentence."
2020.emnlp-main.288.txt,"
5 Conclusion
",2020,"one future direction is to investigate how to integrate the two different attention mechanisms, namely the standard attention and structured attention for nlp applications."
2020.emnlp-main.289.txt,"
4 Conclusion
",2020,"in the future, we will explore the extension of this approach to achieve full coverage."
2020.emnlp-main.29.txt,"
8 Discussion and Conclusion
",2020,"automatic constraint induction from database content and schema descriptions might also be possible, which is on its own an open research problem."
2020.emnlp-main.29.txt,"
8 Discussion and Conclusion
",2020,"beyond semantic evaluation although test suite evaluation provably never creates false negatives in a strict programming language sense, it might still consider “acceptable answers” to be wrong and result in false negatives in a broader sense."
2020.emnlp-main.29.txt,"
8 Discussion and Conclusion
",2020,"fortunately, this issue is mitigated by current models."
2020.emnlp-main.29.txt,"
8 Discussion and Conclusion
",2020,"our framework for creating test suites is general and only has two requirements: (1) the input is strongly typed so that the fuzzing distribution ig can be defined and the sample input can be meaningfully executed, and (2) there exist neighbor queries ng that are semantically close but different from the gold g. since these two conditions hold in many execution environments, our framework might potentially be applied to other logical forms, such as λ-dcs (liang, 2013), knowledge graphs (lin et al., 2018), and python code snippets (yin et al., 2018; oda et al., 2015) if variable types can future work that evaluates approximate semantic accuracy on the existing benchmarks and formulates new tasks amenable to test suite evaluation."
2020.emnlp-main.29.txt,"
8 Discussion and Conclusion
",2020,our test suites will be released for eleven datasets so that future works can conveniently evaluate test suite accuracy.
2020.emnlp-main.290.txt,"
5 Conclusion
",2020,"then, in each window, we use the emotion clause as a pivot to extract the corresponding one or more cause clauses based on multi-label learning (cmll)."
2020.emnlp-main.291.txt,"
6 Conclusion
",2020,"furthermore, we would like to investigate other approaches (e.g., graph-based neural network) to better model the modality and label dependence in multi-modal multi-label emotion detection."
2020.emnlp-main.291.txt,"
6 Conclusion
",2020,"in our future work, we will extend our approach to more multi-modal multi-label scenarios, such as intention detection in video conversations and aspect analysis in multi-modal reviews."
2020.emnlp-main.293.txt,"
6 Conclusion
",2020,"we propose to use large pre-trained language models to estimate the information amount of given text units, by filtering out the background knowledge as encoded in the large models."
2020.emnlp-main.296.txt,"
6 Conclusions
",2020,"in the future, we would like to generate abstractive summaries following an unsupervised approach (baziotis et al., 2019; chu and liu, 2019) and investigate how recent advances in open domain qa (wang et al., 2019; qi et al., 2019) can be adapted for query focused summarization."
2020.emnlp-main.296.txt,"
6 Conclusions
",2020,we explored the potential of leveraging distant supervision signals from question answering to better capture the semantic relations between queries and document segments.
2020.emnlp-main.297.txt,"
6 Conclusion
",2020,"in the future, we would like to investigate other objectives to pre-train seq2seq models for abstractive summarization."
2020.emnlp-main.298.txt,"
6 Conclusion
",2020,"experiments and case studies prove that (i) both context and entity mentions (mainly as type information) provide critical information for relation extraction, and (ii) existing re datasets may leak superficial cues through entity mentions and models may not have the strong abilities to understand context as we expect."
2020.emnlp-main.298.txt,"
6 Conclusion
",2020,"in the future, we will continue to explore better re pre-training techniques, especially with a focus on open relation extraction and relation discovery."
2020.emnlp-main.3.txt,"
6 Conclusion
",2020,"in future work, we would like to improve comment matching, e.g., by making it stance-aware."
2020.emnlp-main.3.txt,"
6 Conclusion
",2020,we also plan to experiment with sequence-tosequence neural models for generating key point candidates from comments.
2020.emnlp-main.300.txt,"
5 Conclusion
",2020,"in the future, we will explore how to improve the efficiency of our pre-training."
2020.emnlp-main.301.txt,"
7 Conclusion
",2020,and we call for a unified end-to-end re evaluation setting to prevent future mistakes and enable more meaningful cross-domain comparisons.
2020.emnlp-main.302.txt,"
7 Discussion and Conclusion
",2020,"continuing gardner et al.(2020), we conclude that challenge sets are an effective tool of benchmarking against shallow heuristics, not only of models and systems, but also of data collection methodologies."
2020.emnlp-main.302.txt,"
7 Discussion and Conclusion
",2020,"we suggest the following recommendation for future re data collection: evaluation sets should be exhaustive, and contain all relevant entity pairs."
2020.emnlp-main.303.txt,"
5 Conclusion
",2020,"entity global representations model the semantic information of an entire document with r-gcn, and entity local representations aggregate the contextual information of mentions selectively using multi-head attention."
2020.emnlp-main.303.txt,"
5 Conclusion
",2020,"in future work, we plan to integrate knowledge graphs and explore other document graph modeling ways (e.g., hierarchical graphs) to improve the performance."
2020.emnlp-main.304.txt,"
6 Conclusion
",2020,"as future work, we intend to explore its application in those fields."
2020.emnlp-main.304.txt,"
6 Conclusion
",2020,"specifically, these methods assume that a shared network is sufficient to capture the correlations between the entity recognition task and the relation classification task, and that the shared features derived from this network can be passed into models for the task-specific tasks to make predictions independently."
2020.emnlp-main.304.txt,"
6 Conclusion
",2020,"we note that our model can be adapted for other nlp tasks, including aspect level sentiment classification and slot filling."
2020.emnlp-main.305.txt,"
7 Conclusion
",2020,"it also learns soft temporal consistency constraints, which allow knowledge of one temporal fact to influence belief in another fact."
2020.emnlp-main.306.txt,"
9 Conclusion
",2020,we plan to explore the utility of this architecture in other nlp problems.
2020.emnlp-main.308.txt,"
6 Conclusion
",2020,"as future work, we plan to generalize the ept to other datasets, including non-english word problems or non-algebraic domains in math, to extend our model."
2020.emnlp-main.31.txt,"
7 Discussion
",2020,"future work is also needed to handle attributes containing long free-form text, as autoqa currently only supports database operations without reading comprehension."
2020.emnlp-main.310.txt,"
4 Conclusion
",2020,"in the future, we would like to extend gtm to corpora with explicit doc-doc interactions, e.g., scientific documents with citations or social media posts with user relationships."
2020.emnlp-main.311.txt,"
5 Conclusion and Future Work
",2020,"for example, the clustering ability of routing algorithm can be used to control the style of generated texts."
2020.emnlp-main.311.txt,"
5 Conclusion and Future Work
",2020,there are several directions to explore in the future.
2020.emnlp-main.312.txt,"
6 Conclusion
",2020,"in the future, we would like to explore if the learner-like agent can be extended to materials and data beyond the example sentences for near-synonyms."
2020.emnlp-main.312.txt,"
6 Conclusion
",2020,"using the emla learner-like agent, we find more helpful learning material for learners, as demonstrated by the learner study."
2020.emnlp-main.312.txt,"
6 Conclusion
",2020,"we select good example sentences in practice, which confirms the usefulness of modeling learner behavior."
2020.emnlp-main.313.txt,"
7 Conclusion and Future Work
",2020,"moreover, we will conduct further exploration of the multiproduct ad post form, including more vivid multimedia information, such as pictures and videos."
2020.emnlp-main.314.txt,"
5 Conclusion
",2020,we are also releasing a part of our forms dataset to aid further research in this direction.
2020.emnlp-main.317.txt,"
5 Conclusion
",2020,our model uses the proposed gdtransformer encoder to take sequence input and biaffine attention scorer to directly predict the word boundaries.
2020.emnlp-main.318.txt,"
5 Conclusion
",2020,"in particular, we will enhance the practicability of chinese word segmentation to improve the effectiveness of other downstream chinese nlp tasks."
2020.emnlp-main.318.txt,"
5 Conclusion
",2020,"in the future, we will continue studying the efficiency of the neural architecture, and pay attention to improving the speed of both training and testing steps on an ever-increasing dataset."
2020.emnlp-main.319.txt,"
5 Conclusions
",2020,"we also plan to extend our framework to semi-supervised learning, where a small number of annotations might also be available in the target language."
2020.emnlp-main.319.txt,"
5 Conclusions
",2020,we have also contributed two quality controlled datasets (compatible with propbank-style guidelines) which we hope will be useful for the development of crosslingual models.
2020.emnlp-main.32.txt,"
5 Conclusion
",2020,extending our method to an abstractive setting is meaningful future work.
2020.emnlp-main.321.txt,"
5 Conclusions
",2020,future work should address the application of our method to more and typologically more divergent languages.
2020.emnlp-main.322.txt,"
7 Conclusions
",2020,"given that gcns over dependency and constituency structure have access to very different information, it would be interesting to see in future work if combining two types of representations can lead to further improvements."
2020.emnlp-main.322.txt,"
7 Conclusions
",2020,"we applied spangcn to srl, on propbank and framenet."
2020.emnlp-main.322.txt,"
7 Conclusions
",2020,"while we experimented only with constituency syntax, spangcn may be able to encode any kind of span structure, for example, co-reference graphs, and can be used to produce linguistically-informed encoders for other nlp tasks rather than only srl."
2020.emnlp-main.323.txt,"
7 Conclusion
",2020,"in future work, one could make the a* parser more accurate by extending it to non-projective dependency trees, especially on dm, eds and amr."
2020.emnlp-main.323.txt,"
7 Conclusion
",2020,it would also be interesting to see if our method for avoiding dead ends can be applied to other formalisms with complex symbolic restrictions.
2020.emnlp-main.324.txt,"
5 Final Remarks
",2020,"in image classification problems, for instance, word graphs related to visual words could be computed."
2020.emnlp-main.324.txt,"
5 Final Remarks
",2020,"we believe that the approach is general enough to be applied to others areas and presents ideas to develop more accurate classifiers in general, across multiple areas, particularly in contexts where outof-scope samples are common."
2020.emnlp-main.325.txt,"
Conclusion
",2020,we introduce ssil to combine these two methods effectively.
2020.emnlp-main.327.txt,"
7 Discussion
",2020,future work should investigate more rewards for training an open-domain dialog model such as long term conversation rewards that may need to be computed over many conversation turns.
2020.emnlp-main.327.txt,"
7 Discussion
",2020,"this allows the dialog systems practitioner to train models that learn language structure from vast, readily-available corpora, then fine-tune for specific desirable behaviors post-hoc through rl rewards."
2020.emnlp-main.330.txt,"
6 Conclusion
",2020,"since surprisal has been associated with cognitive effort during language production (e.g., kello and plaut, 2000) and comprehension (hale, 2001), we can relate cs to situations in which the speaker faces difficulties; and/or similar to mysl´ın and levy (2015), situations where the speaker uses cs as a strategy to emphasize highly informative content to the comprehender in order to facilitate communication."
2020.emnlp-main.330.txt,"
6 Conclusion
",2020,"the second contribution is a new chinese-english cs dataset, which includes translations to the dominant language, which we hope will be used in further models of cs."
2020.emnlp-main.330.txt,"
6 Conclusion
",2020,"the translations of the code-switched sentences were compared to sentences with similar syntactic structure but without code-switches, in order to see what factors affected the propensity to cs.surprisal predicts cs."
2020.emnlp-main.331.txt,"
6 Discussion
",2020,"if number agreement can be correctly learned from a few samples (fsl samples), then one would expect model performance to either a) improve with more data, as more fsl samples are observed, or b) improve with more data up to some threshold, and then asymptote after learning has saturated."
2020.emnlp-main.331.txt,"
6 Discussion
",2020,"the models learn the agreement properties of a novel noun from just a few samples, and the data supporting few-shot learning appears to be densely distributed; nearly all types of syntactic and semantic data examined lead to improvements on the reflexive pronoun or subject-verb agreement tasks.2."
2020.emnlp-main.332.txt,"
6 Conclusion
",2020,we tested our methods with several base wsd systems.
2020.emnlp-main.333.txt,"
6 Conclusion
",2020,"in the future, we plan to apply the transformed representations on more lexical semantics tasks such as word sense disambiguation within an application (navigli and vannella, 2013)."
2020.emnlp-main.334.txt,"
8 Conclusions
",2020,we hope this will support work on solutions for nlp applications and resources that can better serve minorities and underrepresented groups.
2020.emnlp-main.336.txt,"
6 Conclusion
",2020,"in the future, we plan to annotate some of the data, explore supervised segmentation models (li et al., 2018) and introduce more conversation structures like dialogue acts (oya and carenini, 2014; joty and hoque, 2016) into abstractive dialogue summarization."
2020.emnlp-main.336.txt,"
6 Conclusion
",2020,"via thorough error analyses, we concluded a set of challenges that current models struggled with, which can further facilitate future research on conversation summarization."
2020.emnlp-main.338.txt,"
4 Conclusion
",2020,future work may explore the use of points of correspondence and sentence fusion in the standard setting of document summarization.
2020.emnlp-main.339.txt,"
8 Conclusion
",2020,"future work will focus on extending our models to generate extractive plans for better abstractive summarization of long or multiple documents (liu et al., 2018)."
2020.emnlp-main.34.txt,"
6 Conclusion
",2020,our work paves the way for further research on bridging q-learning and unsupervised text summarization.
2020.emnlp-main.340.txt,"
5 Conclusion and future work
",2020,"for future work, we think it will be interesting to look at: 1. zero-shot clir models for low-resource languages, 2. comparison of end-to-end neural rankers with traditional translation+ir pipelines in terms of both scalability, cost, and retrieval accuracy, 3. advanced neural architectures and training algorithms that can exploit our large training data, 4. building universal models for multilingual ir."
2020.emnlp-main.340.txt,"
5 Conclusion and future work
",2020,"the large number of supported language directions allows the research community to explore and build new models for many more languages, especially the low-resource ones."
2020.emnlp-main.340.txt,"
5 Conclusion and future work
",2020,"we present clirmatrix, the largest and the most comprehensive collection of bilingual and multilingual clir datasets to date."
2020.emnlp-main.341.txt,"
6 Conclusion
",2020,we hope that techniques like sledge-z can help overcome the global covid-19 crisis.
2020.emnlp-main.342.txt,"
7 Conclusion
",2020,decoupling representation and interaction provides new understanding of transformer rankers.
2020.emnlp-main.342.txt,"
7 Conclusion
",2020,these findings provide opportunities for future work towards more efficient and interpretable neural ir.
2020.emnlp-main.342.txt,"
7 Conclusion
",2020,"this paper proposes mores, a modular transformer ranking framework that decouples ranking into document representation, query representation, and interaction."
2020.emnlp-main.342.txt,"
7 Conclusion
",2020,"with our proposed document representation pre-compute and re-use methods, mores can achieve 120x speedup in online ranking while retaining accuracy."
2020.emnlp-main.343.txt,"
5 Conclusions and Future work
",2020,"as a future work, we plan to utilize automatic summarization for missing abstracts, instead of taking the first 512 content tokens."
2020.emnlp-main.343.txt,"
5 Conclusions and Future work
",2020,"we have cast a solution for faq retrieval to a solution for ad-hoc document retrieval, where titles and abstracts took the role of questions and answers in faqs."
2020.emnlp-main.345.txt,"
6 Conclusion and Future Work
",2020,"another is to learn rules and dictionaries jointly, which may also aid sentiment analysis (wilson et al., 2005)."
2020.emnlp-main.345.txt,"
6 Conclusion and Future Work
",2020,ideas presented here are general enough to enable other applications.
2020.emnlp-main.345.txt,"
6 Conclusion and Future Work
",2020,one avenue of future work is to learn explainable rules that domain experts can interact with on top of such embeddings.
2020.emnlp-main.346.txt,"
8 Conclusion
",2020,"although we focus only on masked language models in this paper, our method can be trivially extended to standard language models, and thus maybe useful for constructing inputs for models like gpt-3 (brown et al., 2020)."
2020.emnlp-main.349.txt,"
6 Conclusion
",2020,"in order to keep track of plot elements, we create plotmachines which generates paragraphs using a high-level discourse structure and a dynamic plot memory keeping track of both the outline and story."
2020.emnlp-main.349.txt,"
6 Conclusion
",2020,we facilitate training by altering three datasets to include plot outlines as input for long story generation.
2020.emnlp-main.35.txt,"
6 Conclusion and future work
",2020,"in the future, we will study the effectiveness of ta on other nlp tasks, such as the document-level translation, and investigate whether ta is useful for transformer pre-training."
2020.emnlp-main.350.txt,"
7 Conclusion
",2020,a promising research direction is to investigate the root cause behind memorization.
2020.emnlp-main.350.txt,"
7 Conclusion
",2020,"methodologically, we introduced a simple way to examine the content of latent variables by looking at the reconstruction loss per position."
2020.emnlp-main.350.txt,"
7 Conclusion
",2020,"more powerful decoders with alternative factorizations could avoid this issue, for example, non-autoregressive transformers (gu et al., 2017) or transformers with flexible word orders (gu et al., 2019)."
2020.emnlp-main.350.txt,"
7 Conclusion
",2020,"moreover, the agreement metric is another complementary evaluation that is automatic and focused on generation."
2020.emnlp-main.350.txt,"
7 Conclusion
",2020,"therefore, another research avenue would be to blend the two frameworks (im et al., 2017)."
2020.emnlp-main.350.txt,"
7 Conclusion
",2020,we hope that these methods will see widespread adoption for measuring progress more reliably.
2020.emnlp-main.351.txt,"
8 Conclusion and Future Work
",2020,our findings also suggest future work on additional ways to incorporate story principles into plot generation.
2020.emnlp-main.351.txt,"
8 Conclusion and Future Work
",2020,"there is also further work to be done in investigating what models are best able to incorporate plots, which would enable plot improvements to be even more effective."
2020.emnlp-main.352.txt,"
5 Conclusion
",2020,"our method learns the pair relationship between prompts and responses as a regression task on a latent space, which is more suitable for the openended nature of this task."
2020.emnlp-main.353.txt,"
7 Conclusion
",2020,generating subsequent references with such properties has the potential to enhance user adaptation and successful communication in dialogue systems.
2020.emnlp-main.353.txt,"
7 Conclusion
",2020,"we believe that the resulting dataset of referring utterance chains can be a useful resource to analyse and model other dialogue phenomena, such as saliency or partner specificity, both on language alone or on the interaction of language and vision."
2020.emnlp-main.354.txt,"
6 Conclusion
",2020,"we resort to using compound pcfgs which enables us to complement the alignment loss with a language modeling objective, resulting in a fully-differentiable end-to-end visually grounded learning."
2020.emnlp-main.355.txt,"
7 Conclusion
",2020,"in the future, we plan to examine the hierarchical classification architecture’s potential for reducing computational runtime."
2020.emnlp-main.355.txt,"
7 Conclusion
",2020,we extend the concept of active learning to class-based active learning for choosing the most informative query pair.
2020.emnlp-main.356.txt,"
6 Conclusion
",2020,we have only begun to explore the possibilities opened up by pose traces.
2020.emnlp-main.357.txt,"
5 Conclusion and Future Work
",2020,"despite without groundtruth resulting images, we propose cross-task consistency (ctc) to provide a more explicit training signal and train these counterfactual instructions in a self-supervised scenario."
2020.emnlp-main.357.txt,"
5 Conclusion and Future Work
",2020,"for the real world, both visual and linguistic will be more complicated."
2020.emnlp-main.357.txt,"
5 Conclusion and Future Work
",2020,we present a self-supervised counterfactual reasoning (sscr) framework that introduces counterfactual thinking to cope with the data scarcity limitation for iterative language-based image editing.
2020.emnlp-main.358.txt,"
7 Conclusion
",2020,in future work we plan to incorporate crosslingual signals as vulic et al.´ (2019) argue that a fully unsupervised setting is hard to motivate.
2020.emnlp-main.359.txt,"
6 Conclusion
",2020,"while prior efforts focus on improving sharing and cross-lingual alignment, we provide new insights and a different perspective on unsharing and resolving language conflicts."
2020.emnlp-main.36.txt,"
5 Conclusion
",2020,"for future work, we plan to investigate the use of a more powerful language model, such as megatron-lm (shoeybi et al., 2019), as the teacher; and different strategies for choosing hard negatives to further boost the performance."
2020.emnlp-main.361.txt,"
7 Conclusion
",2020,"future work will investigate the compositional capability of these adapters, and combine domain and monolingual adapters for nmt."
2020.emnlp-main.361.txt,"
7 Conclusion
",2020,"more generally, this work adds to the growing evidence of the flexibility of adapter layers (pfeiffer et al., 2020a), and their potential for lightweight fine-tuning, including in zero-shot scenarios (pfeiffer et al., 2020b) and in a variety of tasks (üstün et al., 2020)."
2020.emnlp-main.361.txt,"
7 Conclusion
",2020,"we introduced monolingual adapters and compared them to bilingual adapters, which we also applied to zero-shot translation."
2020.emnlp-main.362.txt,"
5 Discussion
",2020,"more broadly, the community must establish a robust evaluation scheme for zero-shot crosslingual transfer as a single run with one random seed does not reflect the variance of the method (especially in a zero-shot or few-shot setting).5 while keung et al.(2020) advocate using oracle for model selection, we instead argue reporting the variance of test performance, following the few-shot learning literature."
2020.emnlp-main.362.txt,"
5 Discussion
",2020,"therefore, we make the following recommendations for future work on cross-lingual alignment or multilingual representations: 1) evaluations should consider average quality data, not exclusively high-quality bitext.2) evaluation must consider multiple nlp tasks or datasets.3) evaluation should report mean and variance over multiple seeds, not a single run."
2020.emnlp-main.363.txt,"
5 Conclusion
",2020,"our results have revealed that structural language similarity determines the transfer success for lower-level tasks like pos-tagging and dependency parsing; on the other hand, the pretraining corpora size of the target language is crucial for explaining transfer results for higher-level language understanding tasks, such as question answering and natural language inference."
2020.emnlp-main.363.txt,"
5 Conclusion
",2020,"this finding provides a strong incentive for intensifying future research efforts that focus on cheap or naturally occurring supervision (vulic et al.´ , 2019; artetxe et al., 2020c; marchisio et al., 2020), quick and simple annotation procedure, and the more effective few-shot transfer learning setups."
2020.emnlp-main.364.txt,"
8 Conclusions
",2020,"for future work, we plan to expand the automatic domain induction methods and test the mdkd framework on generic mt with data exhibiting varying degrees of heterogeneity: as mdkd distills domain-specific models to create multiple simpler data distributions, we want to investigate if inducing train-time specializations and using them for distillation through mdkd can lead to better quality."
2020.emnlp-main.364.txt,"
8 Conclusions
",2020,"since the approach is architecture-independent, it is easy to combine with other multi-domain nmt models."
2020.emnlp-main.365.txt,"
9 Conclusion
",2020,models can be extended to multiple languages in the same training process.
2020.emnlp-main.365.txt,"
9 Conclusion
",2020,"then, in an independent step, it can be extended to support further languages."
2020.emnlp-main.365.txt,"
9 Conclusion
",2020,we extensively tested the approach for various languages from different language families.
2020.emnlp-main.366.txt,"
5 Discussion
",2020,"we also apply our method to both semantic and syntactic parsing, demonstrating our method’s broader applicability to tasks that process variable-output-length data in a sequential manner."
2020.emnlp-main.368.txt,"
7 Conclusion
",2020,future studies will extend this work to other crosslingual nlp tasks and more languages.
2020.emnlp-main.369.txt,"
5 Conclusion
",2020,we present a curated subset of amazon reviews specifically designed to aid research in multilingual text classification.
2020.emnlp-main.369.txt,"
5 Conclusion
",2020,"with these contributions, we hope that this corpus will be an important resource to the research community."
2020.emnlp-main.370.txt,"
8 Conclusions
",2020,"second, in order to evaluate how well ai models can predict glucose knowledge on novel inputs, the ultimate value of such a dataset, we defined a standalone evaluation task for predicting specific and general inference rules given a story/sentence pair and a dimension."
2020.emnlp-main.370.txt,"
8 Conclusions
",2020,"together with this paper, we release our dataset16 and models17, which we hope will enable the ai research community to explore effective approaches to incorporate commonsense reasoning capabilities into various downstream tasks."
2020.emnlp-main.371.txt,"
6 Conclusion
",2020,these methods should be applicable to other semantic parsing and perhaps other natural language analysis tasks.
2020.emnlp-main.373.txt,"
7 Discussion and Conclusion
",2020,"an interesting future direction is to generate each clarification in response to the previous ones, in a dialogue setup (saeidi et al., 2018)."
2020.emnlp-main.373.txt,"
7 Discussion and Conclusion
",2020,"another challenge is the “needle in a haystack” problem of the clarifications, and one way to address it is to develop a model that is capable of “introspection”, specifically knowing what it doesn’t know."
2020.emnlp-main.373.txt,"
7 Discussion and Conclusion
",2020,filling in knowledge gaps and making implicit intermediate reasoning steps explicit is imperative going forward.
2020.emnlp-main.373.txt,"
7 Discussion and Conclusion
",2020,we hope that our framework will facilitate future research in this area.
2020.emnlp-main.374.txt,"
7 Summary
",2020,this implies a strong potential for pre-training models to better generalize in low-data scenarios.
2020.emnlp-main.375.txt,"
5 Discussion
",2020,"a summary of our results can be seen in table 1, with few-shot learning outcomes in colored cells on the left, and the effect of structural supervision on the right."
2020.emnlp-main.375.txt,"
5 Discussion
",2020,"in this paper, we have tested the few-shot learning capabilities of neural language models, as well as whether these models can learn grammatical representations that are invariant to syntactic transformation."
2020.emnlp-main.375.txt,"
5 Discussion
",2020,scaling these carefully-controlled methods to the larger data setting will be an important next step.
2020.emnlp-main.375.txt,"
5 Discussion
",2020,"thus, robust fewshot generalization is still an important problem in these environments."
2020.emnlp-main.376.txt,"
6 Conclusions
",2020,"another line of future research is to compare the incremental predictions of neural models to finergrained eye-tracking evidence during sentence processing of double-object sentences (e.g.filik et al., 2004)."
2020.emnlp-main.376.txt,"
6 Conclusions
",2020,"as neural language models become more complex, subtler phenomena like verb bias may yield new insights into how lexical and grammatical representations are jointly learned and successfully integrated for language understanding."
2020.emnlp-main.376.txt,"
6 Conclusions
",2020,one possibility is that the transformer’s self-attention mechanism and layer-wise organization improves its ability to represent lexicallyspecific structures.
2020.emnlp-main.377.txt,"
9 Conclusions
",2020,"comparing different ways of aligning the gaze modality with language production, as we have done in the present work, can shed light on how these processes unfold in human cognition."
2020.emnlp-main.377.txt,"
9 Conclusions
",2020,"in the future, our approach and new evaluation measure could be applied to larger eyetracking datasets, such as the english dataset by he et al.(2019)."
2020.emnlp-main.377.txt,"
9 Conclusions
",2020,"since different eye-tracking datasets tend to make use of different gaze encodings and formats, the amount of pre-processing and analysis steps required to apply our method to other resources was beyond the scope of this paper."
2020.emnlp-main.377.txt,"
9 Conclusions
",2020,"taken together, our results open the door to further work in this direction and support the case for computational approaches leveraging cognitive data."
2020.emnlp-main.377.txt,"
9 Conclusions
",2020,"we believe, however, that there is value in conducting research with languages other than english."
2020.emnlp-main.377.txt,"
9 Conclusions
",2020,we leave testing whether the reported pattern of results holds across different languages to future work.
2020.emnlp-main.378.txt,"
6 Discussion
",2020,one future direction is to consider more sophisticated mechanisms to gain stronger controlability over longer sentences while maintaining the compactness of latent representations.
2020.emnlp-main.378.txt,"
6 Discussion
",2020,we hope that this paper will help renew interest in dgms for this purpose.
2020.emnlp-main.378.txt,"
6 Discussion
",2020,"while deep generative models (dgms) such as vaes are theoretically attractive due to its principle nature, it is now rarely used by practitioners in the modern pre-trained language modeling era where bert/gpt dominate with strong empirical performance."
2020.emnlp-main.379.txt,"
6 Conclusion
",2020,"for example, model size is a secondary factor to vocabulary set for token classification task."
2020.emnlp-main.379.txt,"
6 Conclusion
",2020,the model size is a secondary factor; larger model size can probably further improve the performance of a a domain- and applicationspecific language model.
2020.emnlp-main.38.txt,"
7 Conclusion
",2020,"this opens up the possibility of exploring large-scale meta-learning in nlp for various meta problems, including neural architecture search, continual learning, hyperparameter learning, and more."
2020.emnlp-main.38.txt,"
7 Conclusion
",2020,we introduced an approach to leverage unlabeled data to crate meta-learning tasks for nlp.
2020.emnlp-main.380.txt,"
7 Conclusion
",2020,"in future work, we plan to further investigate how different techniques apply to the problem of text segmentation, including data augmentation (wei and zou, 2019; lukasik et al., 2020b) and methods for regularization and mitigating labeling noise (jiang et al., 2020; lukasik et al., 2020a)."
2020.emnlp-main.381.txt,"
7 Conclusion
",2020,"we invite developers, researchers, and ai experts to join our project."
2020.emnlp-main.383.txt,"
6 Discussions and future work
",2020,"the data size for yahoo finance and news, while being one of the largest in the context of hatespeech classification, is nevertheless small in the context of language modeling."
2020.emnlp-main.383.txt,"
6 Discussions and future work
",2020,"the main goal here is not to develop a more powerful misspelling corrector, but rather to propose a new and stronger language modeling approach."
2020.emnlp-main.383.txt,"
6 Discussions and future work
",2020,we plan to perform large-scale pre-training and evaluation on glue datasets for the comprehensive analysis.
2020.emnlp-main.383.txt,"
6 Discussions and future work
",2020,we will continue to explore this line in the future.
2020.emnlp-main.384.txt,"
6 Conclusion
",2020,"in our future work, we would like to extend this idea to unseen numbers in vocabulary as a function of seen ones."
2020.emnlp-main.385.txt,"
6 Conclusion
",2020,future work might explore combining more expressive flows with discrete latent variables.
2020.emnlp-main.385.txt,"
6 Conclusion
",2020,in this work we carried out a large scale empirical investigation of masked number prediction and numerical anomaly detection in text.
2020.emnlp-main.386.txt,"
6 Conclusion
",2020,"finally, the models to generate cross-lingual annotations should be thoroughly evaluated in downstream music retrieval and recommendation tasks."
2020.emnlp-main.386.txt,"
6 Conclusion
",2020,"hence, the effectiveness of language-specific concept representations to model the culturally diverse perception could be further probed."
2020.emnlp-main.386.txt,"
6 Conclusion
",2020,"however, inspired by paraphrastic sentence embedding learning, one can also consider the music genre relations as paraphrasing forms with different strengths (wieting et al., 2016)."
2020.emnlp-main.386.txt,"
6 Conclusion
",2020,our work provides a methodological framework to study the annotation behavior across languagebound cultures in other domains too.
2020.emnlp-main.387.txt,"
8.1 Future Work
",2020,"the next step towards this goal would be to recognize when characters refer to one another, and how this contributes to the movie-level risk behavior rating."
2020.emnlp-main.389.txt,"
9 Conclusion
",2020,"furthermore, we used the interpretability of constituency tests to highlight and explain the parser’s strengths and shortcomings, like the “[ subject verb ]” and “adverb [ adjective noun ]” misbracketings, revealing potential next steps for improvement."
2020.emnlp-main.391.txt,"
5 Conclusion and Future Work
",2020,"in addition, stem and morpheme information can be utilized as additional signals in training."
2020.emnlp-main.391.txt,"
5 Conclusion and Future Work
",2020,"in the future, we plan to enhance the system for handling morphologically complex languages trough unsupervised morphological segmentation."
2020.emnlp-main.393.txt,"
5 Conclusion
",2020,"while it has helped create more accurate models, we argued that this has been at the expense of fairness, efficiency, and robustness, among other desiderata."
2020.emnlp-main.394.txt,"
9 Conclusion
",2020,"in the future work wish to explore more data independent methods such as lrc, for both speed and lack of data dependency, as well as manipulation of the decay w.r.t.what we have discovered from our layer-wise analysis."
2020.emnlp-main.394.txt,"
9 Conclusion
",2020,we further explored constraint and replay based mitigation techniques to close the performance gap between general and domain specific natural language tasks.
2020.emnlp-main.395.txt,"
7 Conclusion
",2020,"our results reinforce previous findings and also illuminate further insights: i) while the information in neural language models is massively distributed, it is possible to extract a small number of features to carry out a downstream nlp task, ii) the number of extracted features varies based on the complexity of the task, iii) the neurons that learn word morphology and lexical semantics are predominantly found in the lower layers of the network, whereas the ones that learn syntax are at the higher layers, with the exception of bert, where neurons were spread across the entire network, iv) closed-class words (for example interjections) are handled using fewer neurons compared to polysemous words (such as nouns and adjectives), v) features in xlnet are more localized towards individual properties as opposed to other architectures where neurons are distributed across many properties."
2020.emnlp-main.396.txt,"
8 Conclusion
",2020,"first, the approach could apply the same meta-learning approach to other classes of tasks beyond span id."
2020.emnlp-main.396.txt,"
8 Conclusion
",2020,our current study could be extended in various directions.
2020.emnlp-main.396.txt,"
8 Conclusion
",2020,"such patterns can be used for manual fine-grained model selection, but our meta-learning model could also be incorporated directly into automl systems."
2020.emnlp-main.396.txt,"
8 Conclusion
",2020,"we explain such patterns by interpreting the parameters of the regression model, which yields insights into how the properties of span id tasks interact with properties of neural model architectures."
2020.emnlp-main.397.txt,"
9 Conclusions and future directions
",2020,"future work can apply these tests to a broader range of models, and continue to develop controlled tests that target encoding of complex compositional meanings, both for two-word phrases and for larger meaning units."
2020.emnlp-main.397.txt,"
9 Conclusions and future directions
",2020,we hope that our findings will stimulate further work on leveraging the power of these generalized transformers while improving their capacity to capture compositional meaning.
2020.emnlp-main.398.txt,"
8 Conclusion and Future Directions
",2020,"while our analysis is helpful in understanding pretrained models, it suggests interesting research directions towards building compact models and models with better architectural choices."
2020.emnlp-main.4.txt,"
6 Conclusion and Future Work
",2020,"besides the future extensions of this approach that we mentioned in our results discussion and error analysis, this work opens several interesting research paths."
2020.emnlp-main.4.txt,"
6 Conclusion and Future Work
",2020,"for our evaluation, we annotated arguments from debatepedia regarding their stance and whether they involve consequences or not."
2020.emnlp-main.4.txt,"
6 Conclusion and Future Work
",2020,the method exploits grammatical dependencies and lexicons to identify effect words and their impact.
2020.emnlp-main.4.txt,"
6 Conclusion and Future Work
",2020,"therefore, we plan to complement this work with approaches for other frequently applied schemes such as arguments by expert opinion and arguments by example."
2020.emnlp-main.400.txt,"
8 Conclusion
",2020,"furthermore, integrating information from knowledge-bases can further improve the quality of entity representation."
2020.emnlp-main.400.txt,"
8 Conclusion
",2020,"future work can explore representations for rare or unseen entities, as well as developing less memory-intensive ways to learn and integrate entity representations."
2020.emnlp-main.400.txt,"
8 Conclusion
",2020,"our entity representations influence the answer predictions for open-domain question answering system and are of high quality, compared to prior work such as knowbert."
2020.emnlp-main.402.txt,"
5 Summary and Outlook
",2020,future work may investigate whether these results translate to other language models besides roberta as well as other training datasets besides winogrande.
2020.emnlp-main.403.txt,"
6 Conclusion
",2020,we hope the insights provided here will help guide the development of better language models in the future.
2020.emnlp-main.403.txt,"
6 Conclusion
",2020,"we investigate and support several reasons why next-sentence prediction is ill-suited for bert pretraining, we provide better inference-based alternatives, and we develop other novel auxiliary tasks based on word importance and soft clustering that provide substantial benefits to bert pre-training."
2020.emnlp-main.404.txt,"
6 Conclusion and Future Work
",2020,"in future work, we plan to explore topic-level bias prediction as well as going beyond left-centerright bias."
2020.emnlp-main.404.txt,"
6 Conclusion and Future Work
",2020,"last but not least, we plan to experiment with other languages, and to explore to what extent a model for one language is transferable to another one given that the left-center-right division is not universal and does not align perfectly across countries and cultures, even when staying within the western political world."
2020.emnlp-main.404.txt,"
6 Conclusion and Future Work
",2020,"we further proposed an adversarial media adaptation approach, as well as a special triplet loss in order to prevent modeling the source instead of the political bias in the news article, which is a common pitfall for approaches dealing with data that exhibit high correlation between the source of a news article and its class, as is the case with our task here."
2020.emnlp-main.404.txt,"
6 Conclusion and Future Work
",2020,"we further want to develop models that would be able to detect specific fragments in an article where the bias occurs, thus enabling explainability."
2020.emnlp-main.405.txt,"
5 Conclusion
",2020,"in future work, we plan to apply our semantic label smoothing technique to various sequence to sequence problems, including text summarization (zhang et al., 2019) and text segmentation (lukasik et al., 2020b)."
2020.emnlp-main.405.txt,"
5 Conclusion
",2020,"we achieve this by using a pre-trained model to find semantically similar sequences from the training corpus, and then we use bleu score to rerank the closest targets."
2020.emnlp-main.405.txt,"
5 Conclusion
",2020,we also plan to study the relation between pretraining and data augmentation techniques.
2020.emnlp-main.406.txt,"
6 Conclusion
",2020,"graphical models with task specific inductive bias have been successful for tasks like ner, coreference resolution, relation extraction, and parsing."
2020.emnlp-main.406.txt,"
6 Conclusion
",2020,our proposed method paves the way for new neural graphical models to be designed for these tasks.
2020.emnlp-main.407.txt,"
7 Conclusion
",2020,"we introduce benchmarks, based on existing datasets, to evaluate model performance on such tasks."
2020.emnlp-main.409.txt,"
6 Conclusion
",2020,"from the ranking results of two different probings, we show a list of interesting observations to provide model selection guidelines and shed light on future research towards a more advanced language modeling learning for dialogue applications."
2020.emnlp-main.409.txt,"
6 Conclusion
",2020,"we investigate representations from pre-trained language models for task-oriented dialogue tasks, including domain identification, intent detection, slot tagging, and dialogue act prediction."
2020.emnlp-main.409.txt,"
6 Conclusion
",2020,we use a supervised classifier probe and a proposed unsupervised mutual information probe.
2020.emnlp-main.41.txt,"
6 Conclusion
",2020,"future works include using other multilingual pretraining models such as xlm-roberta (conneau et al., 2019) for a more accurate model and distilmbert (sanh et al., 2019) for a more compact model."
2020.emnlp-main.41.txt,"
6 Conclusion
",2020,one significant merit of framing word alignment as a squad-style span prediction task is that we can easily import the progress of the latest question answering and multilingual language modeling technologies.
2020.emnlp-main.41.txt,"
6 Conclusion
",2020,our cross-language span prediction method can be used for any alignments between two sequences.
2020.emnlp-main.41.txt,"
6 Conclusion
",2020,"we have already applied it to bilingual sentence alignment (chousa et al., 2020) and we plan to extend it to other related problems."
2020.emnlp-main.410.txt,"
7 Conclusion
",2020,we release our multiatis++ corpus to facilitate future research on cross-lingual nlu to bridge the gap between cross-lingual transfer and supervised methods.
2020.emnlp-main.410.txt,"
7 Conclusion
",2020,"we use our corpus to evaluate various cross-lingual transfer methods including the use of multilingual bert encoder, machine translation, and label projection."
2020.emnlp-main.411.txt,"
7 Conclusion
",2020,future work includes its cross-lingual transfer and cross-dataset (or cross-task) generalization.
2020.emnlp-main.412.txt,"
4 Conclusion
",2020,"in the future, we plan to explore maskaugment for other tasks in nlp domain."
2020.emnlp-main.412.txt,"
4 Conclusion
",2020,"to combat this shortcoming, we investigate domain adaptation through the proposed unsupervised teacher-student training that leverages the maskaugment method for data augmentation."
2020.emnlp-main.413.txt,"
7 Conclusion
",2020,"last but not least, we collect the topv2 dataset, a large-scale multi-domain task-oriented semantic parsing dataset with 8 domains and more than 180k annotated samples to evaluate our models, which we release to the research community."
2020.emnlp-main.415.txt,"
6 Conclusions
",2020,"in the future, we plan to explore this approach with other language pairs and other generation tasks."
2020.emnlp-main.416.txt,"
6 Discussion
",2020,"furthermore, to move towards more practical applications, we would also need to conduct communication-based evaluation (newman et al., 2020) in addition to annotating individual utterances."
2020.emnlp-main.416.txt,"
6 Discussion
",2020,future work can aim to propose more general formulations that encapsulate more properties of the circumstance.forms of assistance.
2020.emnlp-main.416.txt,"
6 Discussion
",2020,"future work can consider adapting experiment designs from prior work (gao et al., 2015; hohenstein and jung, 2018) to establish the impact of offering such intention-preserving paraphrases in real conversations, potentially by considering downstream outcomes."
2020.emnlp-main.416.txt,"
6 Discussion
",2020,"future work may consider more complex stylistic aspects and strategies that are more tied to the content, such as switching from active to passive voice."
2020.emnlp-main.416.txt,"
6 Discussion
",2020,"future work may consider more comprehensive modeling of how people form politeness perceptions or obtain more reliable causal estimates for strategy strength (wang and culotta, 2019).task formulation."
2020.emnlp-main.416.txt,"
6 Discussion
",2020,"given the fine-grained and individualized nature of the task, using humans to ascertain the politeness of the outputs would require an extensive and relatively complex annotation setup (e.g., collecting finegrained labels from annotators with known backgrounds for training and evaluating individualized perception models)."
2020.emnlp-main.416.txt,"
6 Discussion
",2020,"hence, while we work towards providing fully automated suggestions, we might also want to utilize the language ability humans possess and consider assistance approaches in the form of interpretable (partial) suggestions.evaluation."
2020.emnlp-main.416.txt,"
6 Discussion
",2020,the results and limitations of our method open up several natural directions for future work.modeling politeness perceptions.
2020.emnlp-main.417.txt,"
5 Conclusion and Discussion
",2020,"as for future directions, one natural extension is how we can automatically identify those attributes."
2020.emnlp-main.417.txt,"
5 Conclusion and Discussion
",2020,"in this paper, we propose a controlled adversarial text generation model that can generate more diverse and fluent adversarial texts."
2020.emnlp-main.417.txt,"
5 Conclusion and Discussion
",2020,our current generation is controlled by a few pre-specified attributes that are label-invariant by definition.
2020.emnlp-main.418.txt,"
5 Discussion
",2020,"alternatively, it may be useful to explore generation in a non left-to-right order to improve the efficiency of inference."
2020.emnlp-main.418.txt,"
5 Discussion
",2020,"another line of future work is to extend our model to sequence rewriting tasks, such as machine translation post-editing, that do not have existing error-tag dictionaries."
2020.emnlp-main.418.txt,"
5 Discussion
",2020,"even though our approach is open-vocabulary, future work will explore task specific restrictions."
2020.emnlp-main.418.txt,"
5 Discussion
",2020,this research would require induction of error tag inventories using either linguistic insights or unsupervised methods.
2020.emnlp-main.419.txt,"
10 Conclusion
",2020,"additionally, we identify limitations to this ability, specifically in the small data, random permutation setting, and will focus on this going forward."
2020.emnlp-main.42.txt,"
6 Conclusion
",2020,we leave it for future work to extend our study to more downstream tasks and systems.
2020.emnlp-main.420.txt,"
5 Conclusion
",2020,"blm has plenty of future applications, including template filling, information fusion, assisting human writing, etc."
2020.emnlp-main.420.txt,"
5 Conclusion
",2020,"depending on the application, we could also train the model to generate in specific orders by placing higher weights on the corresponding trajectories."
2020.emnlp-main.420.txt,"
5 Conclusion
",2020,"given partially specified text with one or more blanks, blm will fill in the blanks with a variable number of tokens consistent with the context."
2020.emnlp-main.420.txt,"
5 Conclusion
",2020,"moreover, we can extend our formulation to a conditional generative model."
2020.emnlp-main.420.txt,"
5 Conclusion
",2020,"such models can be used in machine translation to support editing and refining translation, as well as in dialogue systems to compose a complete sentence with given elements."
2020.emnlp-main.420.txt,"
5 Conclusion
",2020,"while we proposed blm for language generation, it would also be interesting to compare the representations learned by blm with those produced by other pre-training methods."
2020.emnlp-main.421.txt,"
5 Conclusion
",2020,"cod3s leads to more diverse outputs in a multi-target generation task in a controllable and interpretable manner, suggesting the potential of semantically guided diverse decoding for a variety of text generation tasks in the future."
2020.emnlp-main.422.txt,"
7 Future Work
",2020,"we also plan to expand our methodology for extracting grammar rules from raw text to other aspects of morphosyntax, such as argument structure and word order phenomena."
2020.emnlp-main.422.txt,"
7 Future Work
",2020,we leave a more expressive model and evaluation on more languages as future work.
2020.emnlp-main.424.txt,"
6 Conclusion
",2020,another approach would be to compare improvements between manual-only cleaning and cleaning done by a linguist working with someone who can write scripts to automatically correct repeated patterns of noise.
2020.emnlp-main.424.txt,"
6 Conclusion
",2020,it could be integrated into linguists’ workflow in order to improve the study of inflection and increase igt data.
2020.emnlp-main.424.txt,"
6 Conclusion
",2020,there is room for future improvement.
2020.emnlp-main.426.txt,"
6 Conclusion
",2020,"in general, such models can have educational applications by enabling children to explore creative writing at an early age and addressing the literary learning needs of learners with disabilities."
2020.emnlp-main.426.txt,"
6 Conclusion
",2020,"in this work, we focused only on the protagonist, but future works can explore modeling motivations, goals, achievements, and emotional trajectory of all characters."
2020.emnlp-main.426.txt,"
6 Conclusion
",2020,"our approach is general and provides a blueprint for similar works going forward and can be used outside emotion-aware storytelling, e.g., for generating other emotional content or text with other attributes or properties."
2020.emnlp-main.426.txt,"
6 Conclusion
",2020,the various assumptions and choices made in this paper and the specific characteristics of the dataset we chose can introduce biases and errors.
2020.emnlp-main.426.txt,"
6 Conclusion
",2020,this paper is a step towards future research directions on planning emotional trajectory while generating stories.
2020.emnlp-main.427.txt,"
8 Conclusion
",2020,future work needs to look into how question and reply context can improve automatic identification of advice.
2020.emnlp-main.429.txt,"
9 Conclusion
",2020,"for example, blog sites, which target researchers, use more jargon and focus on the main findings of a paper, while magazine articles, which target a much broader audience of readers, tell more stories and use more active voice."
2020.emnlp-main.429.txt,"
9 Conclusion
",2020,"we expect that our strategy formulations, classifiers, annotations and dataset will enable nlppowered tools to support effective science communication for different audiences."
2020.emnlp-main.43.txt,"
6 Conclusion and Future Work
",2020,"besides, we propose our chr-en and en-chr baselines, including both smt and nmt models, using both supervised and semi-supervised methods, and exploring both transfer learning and multilingual joint training methods with 4 other languages."
2020.emnlp-main.43.txt,"
6 Conclusion and Future Work
",2020,our future work involves converting the monolingual data to parallel and collecting more data from the news domain.
2020.emnlp-main.43.txt,"
6 Conclusion and Future Work
",2020,we hope these diverse baselines will serve as useful strong starting points for future work by the community.
2020.emnlp-main.430.txt,"
9 Conclusions
",2020,"in the future, we would like to explore uses of the subevent knowledge base for other eventoriented applications such as event tracking."
2020.emnlp-main.430.txt,"
9 Conclusions
",2020,"we performed extensive evaluations showing that the harvested subevent knowledge not only improves subevent relation extraction, but also improve a wide range of nlp tasks such as causal and temporal relation extraction and discourse parsing."
2020.emnlp-main.431.txt,"
8 Conclusion
",2020,beesl is broadly applicable to event extraction and other tasks that can be recast as sequence labeling.
2020.emnlp-main.431.txt,"
8 Conclusion
",2020,"we release the code freely, to foster research on using beesl for other nlp tasks as well, e.g., enhanced dependency parsing, fine-grained named entity recognition, and semantic parsing."
2020.emnlp-main.433.txt,"
7 Conclusion
",2020,"in the future, we plan to extend our annotation to include event arguments and other properties of events."
2020.emnlp-main.433.txt,"
7 Conclusion
",2020,our experiments also suggest that document-level information is necessary to perform ed for cybersecurity domain.
2020.emnlp-main.435.txt,"
5 Conclusion
",2020,"in the future, we plan to apply the proposed model for the related tasks and other settings of ed, including new type extension (nguyen et al., 2016b; lai and nguyen, 2019), and few-shot learning (lai et al., 2020a,b)."
2020.emnlp-main.435.txt,"
5 Conclusion
",2020,"we demonstrate how gating mechanisms, gate diversity, and graph structure can be used to integrating syntactic information and improve the hidden vectors for ed models."
2020.emnlp-main.436.txt,"
5 Conclusions and Future Work
",2020,"for the future, instead of using the roberta baseline model for the self-training experiments, we could run several iterations by retraining on the data produced by our best self-trained model(s); this could be a good avenue for further improvements."
2020.emnlp-main.436.txt,"
5 Conclusions and Future Work
",2020,"in addition we plan to extend our work by moving to other languages beyond english (we currently have not tried this due to lack of data) using cross-lingual models, (subburathinam et al., 2019), applying other architectures like cnns (nguyen and grishman, 2015), incorporating tree structure in our models (miwa and bansal, 2016) and/or by handling jointly performing event recognition and temporal ordering (li and ji, 2014; katiyar and cardie, 2017)."
2020.emnlp-main.437.txt,"
4 Conclusion
",2020,"third, the maximum-likelihood objective used to train our model provides no guarantees as to whether a model will learn a fact or not."
2020.emnlp-main.437.txt,"
4 Conclusion
",2020,"this model size can be prohibitively expensive in resource-constrained settings, prompting future work on more efficient language models."
2020.emnlp-main.437.txt,"
4 Conclusion
",2020,"we are therefore interested in measuring performance on question answering tasks that require reasoning capabilities such as drop (dua et al., 2019)."
2020.emnlp-main.438.txt,"
7 Conclusion and Future Work
",2020,"in future work, we plan to extend the dataset with more questions, more subjects, and more languages."
2020.emnlp-main.438.txt,"
7 Conclusion and Future Work
",2020,we further plan to develop new models to address the specific challenges we identified.
2020.emnlp-main.438.txt,"
7 Conclusion and Future Work
",2020,we further proposed new fine-grained evaluation that allows precise comparison across different languages and school subjects.
2020.emnlp-main.439.txt,"
5 Conclusions
",2020,improving lm-score-based filtering is a future direction of our work.
2020.emnlp-main.439.txt,"
5 Conclusions
",2020,it would be interesting to explore how one can adapt the generative models to the type of target domain questions.
2020.emnlp-main.44.txt,"
8 Limitations and Future Work
",2020,"finally, our assumption that human judgements are not reliable for this task makes evaluation difficult, and this task would benefit from the development of additional evaluation metrics."
2020.emnlp-main.44.txt,"
8 Limitations and Future Work
",2020,"similarly, while our model uses o txt for propensity matching in the training data, thus encouraging the model to encode indicators of bias, a model to classify comments as biased or unbiased should also incorporate o txt when assessing test data."
2020.emnlp-main.44.txt,"
8 Limitations and Future Work
",2020,there are additional avenues for future work beyond our proposed framework.
2020.emnlp-main.44.txt,"
8 Limitations and Future Work
",2020,we first focus on limitations within our proposed framework.
2020.emnlp-main.44.txt,"
8 Limitations and Future Work
",2020,"while our work serves as an initial approach toward unsupervised detection of comment-level gender bias, we identify several limitations and areas for future work."
2020.emnlp-main.440.txt,"
6 Conclusion
",2020,"in future work, we aim to extend our approach to more domains and explore more generalizable approaches for unsupervised domain adaptation."
2020.emnlp-main.441.txt,"
7 Conclusion
",2020,"additional effort will also be needed in activities like the development of large diagram datasets, including the semantic annotation of diagram constituents and connectors, and annotating diagram questions with the reasoning and knowledge types required to answer them."
2020.emnlp-main.441.txt,"
7 Conclusion
",2020,"key to this success are transformers, butd attention, pre-training on related datasets, and the selection of complementary background information to train and ensemble different solvers."
2020.emnlp-main.441.txt,"
7 Conclusion
",2020,"still, further research is necessary to keep pushing the boundaries of textbook understanding, e.g.by charting and expanding the reasoning skills of transformers, making model outcomes more interpretable by humans, and further exploiting diagrams."
2020.emnlp-main.442.txt,"
7 Conclusion
",2020,"we further implement a subjectivity-aware model and evaluate it, along with 4 strong baseline models."
2020.emnlp-main.442.txt,"
7 Conclusion
",2020,"we hope this dataset opens new avenues for research on querying subjective content, and into subjectivity in general."
2020.emnlp-main.443.txt,"
6 Conclusion
",2020,"the winner configuration—a transformer structural encoder coupled with a resnet cnn—can generate semantically meaningful captions for sparsely labeled elements on the screen, which shows the feasibility of this task and opportunities for future research."
2020.emnlp-main.444.txt,"
5 Conclusion
",2020,this prevents us from integrating visual knowledge into the text encoder.
2020.emnlp-main.446.txt,"
6 Conclusion
",2020,addressing potential ethical concerns the goal of our work is to help to make nlp models more robust.
2020.emnlp-main.446.txt,"
6 Conclusion
",2020,"furthermore, we contacted the three companies (google, bing, and systran) to report the vulnerabilities."
2020.emnlp-main.446.txt,"
6 Conclusion
",2020,"moving forward, we hope to improve and deploy defenses against adversarial attacks in nlp, and more broadly, we hope to make security and privacy a more prominent focus of nlp research."
2020.emnlp-main.447.txt,"
6 Conclusion
",2020,"seqmix is efficient and easy to implement, and as a secondary contribution, we provide a framework that unifies several data augmentation strategies for compositionality, which naturally suggests avenue for future research (e.g., a relaxed variant of geca)."
2020.emnlp-main.448.txt,"
7 Conclusion
",2020,"we extended the notion of consistency of a recurrent language model put forward by chen et al.(2017) to incorporate a decoding algorithm, and used it to analyze the discrepancy between a model and the distribution induced by a decoding algorithm."
2020.emnlp-main.448.txt,"
7 Conclusion
",2020,"we suspect the absence of decoding in maximum likelihood estimation as a cause behind this inconsistency, and suggest investigating sequence-level learning as an alternative."
2020.emnlp-main.450.txt,"
7 Conclusion
",2020,"finally, snoek et al.(2019) find that deep ensembles can significantly improve outof-domain performance over single models, and we are interested in exploring whether our distillation techniques retain these benefits."
2020.emnlp-main.450.txt,"
7 Conclusion
",2020,"furthermore, we show that calibration of the single student models can be further improved by other, orthogonal, re-calibration methods."
2020.emnlp-main.450.txt,"
7 Conclusion
",2020,"in future work, we are interested in exploring nat using distilled ensembles with truncated distributions, and assessing how improved calibration impacts non-sequential decoding performance."
2020.emnlp-main.450.txt,"
7 Conclusion
",2020,"non-autoregressive translation (nat) is an active area of research for nmt (gu et al., 2017; stern et al., 2019; ghazvininejad et al., 2019)."
2020.emnlp-main.450.txt,"
7 Conclusion
",2020,summary of contributions.
2020.emnlp-main.452.txt,"
8 Conclusion
",2020,we also believe that the general idea behind our discourse-enhanced self-training approach could be useful for many other types of problems where additional information can be extracted from larger contexts to serve as a secondary signal to help confirm or disconfirm a classifier’s predictions.
2020.emnlp-main.453.txt,"
6 Conclusion
",2020,"we propose a novel joint model that inherits the advantage of high-level feature learning, logic reasoning and structured learning which can be trained smoothly in an end-to-end manner."
2020.emnlp-main.454.txt,"
7 Conclusion
",2020,"finally, we developed an unsupervised technique to extract tags that identify complementary attributes of movies from user reviews."
2020.emnlp-main.454.txt,"
7 Conclusion
",2020,"we believe that this coarse story understanding approach can be extended to longer stories, i.e., entire books, and are currently exploring this path in our ongoing work."
2020.emnlp-main.455.txt,"
7 Conclusion
",2020,"hence, we guide the data-driven tokenizer by incorporating linguistic information to learn a more efficient vocabulary and generate symbol sequences that increase the network’s robustness to inflectional variation."
2020.emnlp-main.455.txt,"
7 Conclusion
",2020,"the tokenization stage of the modern deep learning nlp pipeline has not received as much attention as the modeling stage, with researchers often defaulting to common subword tokenizers like bpe.we can do better."
2020.emnlp-main.456.txt,"
8 Conclusion
",2020,"one could apply them more broadly other aspects of the lexicon, e.g.to indo-european verb classes, bantu noun classes, or diachronic time slices of a single language’s gender system, data permitting."
2020.emnlp-main.456.txt,"
8 Conclusion
",2020,"we note that we could further extend our measures to fuzzy partitions, which remain less explored in community detection, but are a promising avenue for future work."
2020.emnlp-main.458.txt,"
9 Conclusion
",2020,"in the presence of one-to-many mappings between pinyin and characters, the mapping accuracy is severely downgraded, leaving open an opportunity to design more robust unsupervised vector mapping systems."
2020.emnlp-main.459.txt,"
6 Conclusion
",2020,"in future work, we plan to improve the quality of the additional actions."
2020.emnlp-main.46.txt,"
7 Conclusion
",2020,"for one, as is the case for rheault and cochrane (2020), our model is not able to produce uncertainty bounds, as deriving uncertainty measures from neural networks remains an open area of research without a clear solution within the field of machine learning.7 an avenue for improving the model is to allow it to capture legislators’ dynamic attitudes toward trump over time."
2020.emnlp-main.46.txt,"
7 Conclusion
",2020,"legislator embeddings can be used to explore how legislators appeal to different audiences, such as party leaders and constituents."
2020.emnlp-main.46.txt,"
7 Conclusion
",2020,"possible extensions of this work could investigate enriching trump vectors by incorporating other sources of text, such as white house press releases and speeches."
2020.emnlp-main.46.txt,"
7 Conclusion
",2020,the method presented here could similarly be used to evaluate how members of congress are punished and rewarded in elections for their criticism of praise of the president.
2020.emnlp-main.46.txt,"
7 Conclusion
",2020,"while legislator attitudes are currently modeled as static embeddings, allowing each legislator’s embedding to change over time would enable the exploration of temporal dynamics and hypothesis testing about when legislators are more likely to tweet negatively about trump, what factors contribute to a legislator’s decision to tweet about trump, and how the trump’s tweets interact with legislator’s tweets over time."
2020.emnlp-main.460.txt,"
6 Conclusion and Future Work
",2020,"for future work, we plan to incorporate hyperbolic rnns (ganea et al., 2018) to encode auxiliary information for zero-shot entity and concept representations."
2020.emnlp-main.460.txt,"
6 Conclusion and Future Work
",2020,"we also seek to investigate the use of hyperka for cross-domain representations of biological and medical knowledge (hao et al., 2020)."
2020.emnlp-main.461.txt,"
9 Conclusion
",2020,"we plan to apply the proposed framework on various event reasoning tasks and construct novel distributional constraints that could leverage domain knowledge beyond corpus statistics, such as the larger unlabeled data and rich information contained in knowledge bases."
2020.emnlp-main.462.txt,"
5 Conclusion
",2020,future work involves exploring the generalization of temp to continuous tkgc and better imputation techniques to induce representations for infrequent and inactive entities.
2020.emnlp-main.462.txt,"
5 Conclusion
",2020,"our work is potentially beneficial to other tasks such as temporal information extraction and temporal question answering, by providing beliefs about the likelihood of facts at particular points in time."
2020.emnlp-main.463.txt,"
7 Conclusion
",2020,"it leads to many interesting future works, including generalizing theorem 2 to other models, designing new algorithms to automatically adapt deep networks to different training configurations, upgrading the transformer architecture, and applying our proposed admin to conduct training in a larger scale."
2020.emnlp-main.464.txt,"
6 Conclusion
",2020,"this opens a wide range of possibilities for generation tasks where monotonic orderings are not the most natural choice, and we would be excited to explore some of these areas in future work."
2020.emnlp-main.465.txt,"
5 Conclusion
",2020,for future work we would like to explore if their success transfers to other generation tasks with mlms where inference efficiency is a concern.
2020.emnlp-main.466.txt,"
7 Conclusion & Future Work
",2020,"furthermore, future work may build on the ambigqa task with more open-ended approaches such as (1) applying the approach to qa over structured data (such as ambiguous questions that require returning tables), (2) handling questions with no answer or ill-formed questions that require inferring and satisfying more complex ambiguous information needs, and (3) more carefully evaluating usefulness to end users."
2020.emnlp-main.467.txt,"
5 Conclusion
",2020,"under the guidance of a pre-trained mrc model, the original question is revised in a continuous embedding space with gradient-based optimization and then decoded back to the discrete space as a new question data sample."
2020.emnlp-main.468.txt,"
8 Conclusions and Future Work
",2020,extension of this work to unanswerable and boolean questions is also a future work direction.
2020.emnlp-main.468.txt,"
8 Conclusions and Future Work
",2020,"finally, we generate synthetic text from a wikipedia-finetuned gpt-2 model, generate answer candidates and synthetic questions based on those answers, and then train a bert-large model and achieve similar question answering accuracy."
2020.emnlp-main.468.txt,"
8 Conclusions and Future Work
",2020,"for such a regime, one needs to analyze the effect of domain transfer and bootstrapping from a very small human labelled dataset."
2020.emnlp-main.468.txt,"
8 Conclusions and Future Work
",2020,"more generally application of this work to multi dataset question generation with datasets such as multiqa (talmor and berant, 2019) is a promising avenue for future work."
2020.emnlp-main.468.txt,"
8 Conclusions and Future Work
",2020,of particular interest for future work is handling low-resource question answering domains.
2020.emnlp-main.468.txt,"
8 Conclusions and Future Work
",2020,we build upon existing work in large scale language modeling and question generation to push the quality of synthetic question generation.
2020.emnlp-main.468.txt,"
8 Conclusions and Future Work
",2020,we hope that better synthetic questions will enable new breakthroughs in question answering systems and related natural language tasks.
2020.emnlp-main.469.txt,"
5 Conclusion
",2020,"in the future, we plan to improve mrl-cqa by designing a retriever that could be optimized jointly with the programmer under the meta-learning paradigm, instead of manually pre-defining a static relevance function."
2020.emnlp-main.469.txt,"
5 Conclusion
",2020,"other potential directions of research could be toward learning to cluster questions into fine-grained groups and assign each group a set of specific initial parameters, making the model finetune the parameters more precisely."
2020.emnlp-main.469.txt,"
5 Conclusion
",2020,we take a meta-reinforcement learning approach to effectively adapt the meta-learned programmer to new questions based on the most similar questions retrieved.
2020.emnlp-main.47.txt,"
6 Conclusion
",2020,"in developing this pipeline to examine how authors depict the transmission of information within narrative texts, we hope to drive a variety of future research in this space, including not only such narratological questions as how “gossip impels plots” (spacks, 1985), but also questions pertaining to issues of bias in representation, the flow of information, and factuality."
2020.emnlp-main.47.txt,"
6 Conclusion
",2020,"this work offers a new perspective on the analysis of social networks in literary texts by considering the dynamics of how information flows through them—both as a result of the structural topology of the network (characters who successfully propagate are information bridges between communities), and as a result of the specific characteristics of each node (women are depicted more frequently as successful propagators than men)."
2020.emnlp-main.470.txt,"
6 Conclusion
",2020,"finally, we would also like to apply our models to languages with even less resources available to help coping with the problem of offensive language in social media."
2020.emnlp-main.470.txt,"
6 Conclusion
",2020,"in future work, we would like to further evaluate our models using solid, a novel large english dataset with over 9 million tweets (rosenthal et al., 2020), along with datasets in four other languages (arabic, danish, greek, and turkish) that were made available for the second edition of offenseval (zampieri et al., 2020)."
2020.emnlp-main.470.txt,"
6 Conclusion
",2020,"this opens exciting new avenues for future research considering the multitude of phenomena (e.g.hate speech, aggression, cyberbulling), annotation schemes and guidelines used in offensive language datasets."
2020.emnlp-main.472.txt,"
11 Conclusion
",2020,"ultimately, we hope our work can open up new horizons for studying mds in various languages."
2020.emnlp-main.472.txt,"
11 Conclusion
",2020,"we have also exploited our own data to train marbert, a very large and powerful masked language model covering all arabic varieties."
2020.emnlp-main.473.txt,"
5 Conclusion
",2020,"in addition, we present a new dataset consisting of intent-parallel aave/sae tweet pairs, which can be used in future works studying sae and aave in nlp models."
2020.emnlp-main.473.txt,"
5 Conclusion
",2020,we hope our findings can pave the way for further inclusion of diverse language in future nlg models.
2020.emnlp-main.474.txt,"
5 Conclusion
",2020,"in the future, we would like to extend our method to enhance the back-translation method in multidomain settings."
2020.emnlp-main.477.txt,"
6 Conclusion
",2020,"it is an interesting question for future work whether strong alignment always comes at a cost, or if better training techniques will lead to models that can improve on all these measures simultaneously."
2020.emnlp-main.478.txt,"
8 Conclusion
",2020,"additionally, it would be valuable to examine whether our method can improve the ocr on highresource languages, which typically have much better recognition rates in the first pass transcription than the endangered languages in our dataset."
2020.emnlp-main.478.txt,"
8 Conclusion
",2020,"as future work, we plan to investigate the effect of using other available data for the three languages (for example, word lists collected by documentary linguists or the additional griko folk tales collected by anastasopoulos et al.(2018))."
2020.emnlp-main.478.txt,"
8 Conclusion
",2020,future work that focuses on overcoming the challenges of applying language-specific models to endangered language texts would be needed to confirm our method’s applicability to post-correcting the first pass transcriptions from different ocr systems.
2020.emnlp-main.478.txt,"
8 Conclusion
",2020,"future work will focus on large-scale digitization of scanned documents, aiming to expand our ocr benchmark on as many endangered languages as possible, in the hope of both easing linguistic documentation and preservation efforts and collecting enough data for nlp system development in under-represented languages."
2020.emnlp-main.479.txt,"
8 Conclusion
",2020,future directions include other pre-training or fine-tuning methods to improve retrieval performance and methods that encourage the lm to predict entities of the right types.
2020.emnlp-main.48.txt,"
8 Conclusion
",2020,comprehensive modeling of social norms presents a promising challenge for nlp work in the future.
2020.emnlp-main.480.txt,"
7 Conclusion & Future Works
",2020,additional work could also leverage aligned documents as supervision to learn better cross-lingual document representations.
2020.emnlp-main.480.txt,"
7 Conclusion & Future Works
",2020,one natural followup to this work is to develop techniques to better mine parallel sentences from these aligned documents – especially for low-resource language pairs.
2020.emnlp-main.480.txt,"
7 Conclusion & Future Works
",2020,"to spur further work, we release the list of aligned urls as well as code to generate aligned documents given commoncrawl snapshots."
2020.emnlp-main.481.txt,"
6 Conclusion
",2020,our methodology enables further investigation and creation of new benchmarks to trigger more research on this topic.
2020.emnlp-main.482.txt,"
6 Conclusion and Future Work
",2020,another line of future work is to investigate alternative user interfaces.
2020.emnlp-main.482.txt,"
6 Conclusion and Future Work
",2020,combining active learning with clime further improves the system.
2020.emnlp-main.482.txt,"
6 Conclusion and Future Work
",2020,"in the future, we plan to train a policy that dynamically combines the two interactions with reinforcement learning (fang et al., 2017)."
2020.emnlp-main.482.txt,"
6 Conclusion and Future Work
",2020,"therefore, future advances in these areas may also improve clime."
2020.emnlp-main.482.txt,"
6 Conclusion and Future Work
",2020,we also explore a simple combination of active learning and clime.
2020.emnlp-main.482.txt,"
6 Conclusion and Future Work
",2020,we test clime on cross-lingual information triage in international health emergencies for four low-resource languages.
2020.emnlp-main.483.txt,"
5 Conclusion
",2020,"it also increases downstream mt performance in a low-resource setting over prior work, including a margin-based comparable corpora method (artetxe and schwenk, 2019a)."
2020.emnlp-main.483.txt,"
5 Conclusion
",2020,"our method uses multilingual sentence embeddings and explicitly models the order of sentences in documents, in both candidate generation and candidate re-scoring."
2020.emnlp-main.486.txt,"
5 Conclusions
",2020,"for future work, we will apply hit to other languages, and further explore potential cases of overlapping entities in nested ner task."
2020.emnlp-main.487.txt,"
4 Conclusion
",2020,"further analysis is performed to investigate using different types of edges, which reveals their quality and confirms the necessity of introducing attention to gcn for ccg supertagging."
2020.emnlp-main.488.txt,"
6 Conclusion
",2020,"the generated data introduce more diversity to reduce overfitting, since they are generated from scratch instead of modifying the gold training data."
2020.emnlp-main.489.txt,"
7 Discussion
",2020,"this is just a first step towards the goal of fully-automated interpretable evaluation, and applications to new attributes and tasks beyond ner are promising future directions."
2020.emnlp-main.49.txt,"
6 Conclusion
",2020,"for future work, it would be interesting to try incorporating broader context (e.g., paragraph/document-level context (ji and grishman, 2008; huang and riloff, 2011; du and cardie, 2020) in our methods to improve the accuracy of the predictions."
2020.emnlp-main.49.txt,"
6 Conclusion
",2020,"we investigate how the question generation strategies affect the performance of our framework on both trigger detection and argument span extraction, and find that more natural questions lead to better performance."
2020.emnlp-main.490.txt,"
4 Conclusion
",2020,we hope to provide new guidance for the future slot filling work.
2020.emnlp-main.491.txt,"
5 Conclusion
",2020,"since our framework can be used with any pretrained autoencoder, it will benefit from large-scale pretraining in future research."
2020.emnlp-main.494.txt,"
7 Conclusion
",2020,another possibility is to design imitation-based fine-tuning analogs to the seqinter method.
2020.emnlp-main.494.txt,"
7 Conclusion
",2020,"finally, although our experiments in this paper focused on sequenceto-sequence settings, we are interested in exploring the use of imitkd for compressing large language models aimed at transfer learning."
2020.emnlp-main.494.txt,"
7 Conclusion
",2020,"one branch of ideas involves incorporating more advanced il algorithms beyond dagger, such as lols (chang et al., 2015), to further improve the distillation process."
2020.emnlp-main.494.txt,"
7 Conclusion
",2020,we are excited about several possible avenues for future work.
2020.emnlp-main.495.txt,"
6 Conclusions
",2020,"these results shed light on an effective way to examine the robustness of a wide range of nlp models, thus paving the way for the development of a new generation of more reliable and effective nlp methods."
2020.emnlp-main.497.txt,"
7 Conclusion
",2020,the intuition behind the objective is that the adaptation effort should focus on a subset of tokens which are challenging to the mlm.
2020.emnlp-main.499.txt,"
5 Conclusion
",2020,"in the future, we would like to explore data-free distillation on more complex tasks."
2020.emnlp-main.5.txt,"
8 Conclusions
",2020,we consider collecting additional human translations as well as generating more diverse and natural references through paraphrasing.
2020.emnlp-main.5.txt,"
8 Conclusions
",2020,we suggest using a single paraphrased reference for more reliable automatic evaluation going forward.
2020.emnlp-main.50.txt,"
7 Conclusions and Future Work
",2020,"in the future, we aim to extend graph schemas to encode hierarchical and temporal relations, as well as rich ontologies in open domain."
2020.emnlp-main.50.txt,"
7 Conclusions and Future Work
",2020,"we will also assemble our graph schemas to represent more complex scenarios involving multiple events, so they can be applied to more downstream applications including event graph completion and event prediction."
2020.emnlp-main.500.txt,"
6 Conclusion
",2020,"thus, enhancing language models to generate more semantically related perturbations can be one possible solution to perfect bert-attack in the future."
2020.emnlp-main.501.txt,"
5 Conclusion
",2020,"our work prompts a deeper investigation into associated topics such as theoretical similarities to distillation, defenses against such multilingual extractions, and improving performance on out-ofdomain vocabulary."
2020.emnlp-main.502.txt,"
7 Conclusion and Future Work
",2020,"in the future, we will extend the similar approach to multilingual (yu et al., 2020a) or crosslingual (upadhyay et al., 2018) lexical entailment tasks."
2020.emnlp-main.502.txt,"
7 Conclusion and Future Work
",2020,"moreover, one interesting direction is to use hyperbolic embeddings (le et al., 2019; balazevic et al., 2019) for pattern-based models due to their inherent modeling ability of hierarchies."
2020.emnlp-main.504.txt,"
7 Conclusion
",2020,"for future work, we intend to scale srefkb to a multilingual version and explore the possibilities of using the multilingual wordnet so that abundant knowledge regarding english can be transferred to other languages."
2020.emnlp-main.504.txt,"
7 Conclusion
",2020,it is also worth investigating regarding how to better incorporate sense embedding into other downstream tasks.
2020.emnlp-main.508.txt,"
6 Conclusion
",2020,we believe this approach can power future analyses of pre-trained text generation systems.
2020.emnlp-main.509.txt,"
5 Conclusion
",2020,"the method can be extended to other text genres such as public policies to aid reader comprehension, which will be our future work to explore."
2020.emnlp-main.509.txt,"
5 Conclusion
",2020,we make a first attempt to create sub-sentence summary highlights that are understandable and require minimum information from the surrounding context.
2020.emnlp-main.51.txt,"
5 Conclusion
",2020,"for future work, we plan to extend the framework towards an end-to-end system with event extraction."
2020.emnlp-main.51.txt,"
5 Conclusion
",2020,"the proposed framework bridges temprel and subevent relation extraction tasks with a comprehensive set of logical constraints, which are enforced during learning by converting them into differentiable objective functions."
2020.emnlp-main.51.txt,"
5 Conclusion
",2020,we also seek to extend the conjunctive constraints along with event argument relations.
2020.emnlp-main.510.txt,"
5 Conclusions
",2020,"the promising empirical results motivate us to explore further the integration of more external knowledge and other rich forms of supervisions (e.g., constraints, interactions, auxiliary models, adversaries) (hu and xing, 2020; ziegler et al., 2019) in learning."
2020.emnlp-main.510.txt,"
5 Conclusions
",2020,"we are also interested in extending the aspect-based summarization in more application scenarios (e.g., summarizing a document corpus)."
2020.emnlp-main.512.txt,"
5 Conclusion
",2020,"from the decoding side, a promising direction would be to make global inference in a more efficient way."
2020.emnlp-main.512.txt,"
5 Conclusion
",2020,"from the encoding side, it would be ideal to encode an utterance within its context."
2020.emnlp-main.512.txt,"
5 Conclusion
",2020,there are some possible future directions from our work.
2020.emnlp-main.513.txt,"
6 Conclusion
",2020,for future work we are interested in exploring how definition modeling could be adapted to a multilingual or cross-lingual setting.
2020.emnlp-main.515.txt,"
5 Conclusion and Future Work
",2020,"in the future, we are interested in replacing bert with knowledge enhanced and number sensitive text representations models (cao et al., 2017; geva et al., 2020)."
2020.emnlp-main.516.txt,"
7 Conclusion
",2020,"in the future, we want to extend our system to other few-shot sequence tagging problems such as part-of-speech tagging and slot filling."
2020.emnlp-main.517.txt,"
4 Concluding Remarks
",2020,one immediate future work is to generate explanations for model predictions using structured vector.
2020.emnlp-main.519.txt,"
7 Conclusion
",2020,future work includes: • enriching entity representations by adding entity type and entity graph information; • modeling coherence by jointly resolving mentions in a document; • extending our work to other languages and other domains; • joint models for mention detection and entity linking.
2020.emnlp-main.520.txt,"
7 Conclusion
",2020,an exciting direction is to leverage visuals of each step to deal with unmentioned entities and indirect effects.
2020.emnlp-main.520.txt,"
7 Conclusion
",2020,"as future work, we will explore more sophisticated models that can address the highlighted shortcomings of the current model."
2020.emnlp-main.521.txt,"
7 Conclusion
",2020,"given that our experiments show a 25% increase in the candidate generation, one future research direction is to improve candidate ranking in lrl by incorporating coherence statistics and entity types."
2020.emnlp-main.521.txt,"
7 Conclusion
",2020,"moreover, given the effectiveness of query logs, we believe it can be applied to other cross-lingual tasks like relation extraction and knowledge base completion."
2020.emnlp-main.523.txt,"
6 Conclusions
",2020,"future work involves applying luke to domain-specific tasks, such as those in biomedical and legal domains."
2020.emnlp-main.524.txt,"
8 Conclusion
",2020,"future directions include exploration of other knowledge bases to help the inference process and applying our simile generation approach to different creative nlg tasks such as pun (he et al., 2019), sarcasm (chakrabarty et al., 2020), and hyperbole (troiano et al., 2018)."
2020.emnlp-main.525.txt,"
7 Conclusion
",2020,our dataset and evaluation platform will be made publicly available to spur progress into story generation.
2020.emnlp-main.525.txt,"
7 Conclusion
",2020,"we devise a metric on top of their edits that correlates strongly with judgments of the relevance of the generated text, which user interviews suggest is the most important area for improvement moving forward."
2020.emnlp-main.526.txt,"
7 Conclusion
",2020,"although we focus on the recipe domain, our method naturally generalizes to other domains where procedural tasks can be substantively rewritten."
2020.emnlp-main.526.txt,"
7 Conclusion
",2020,"as language generation becomes more grounded in signals outside of language, work in the area of substantive transfer becomes increasingly relevant."
2020.emnlp-main.527.txt,"
8 Conclusion and Future Work
",2020,"another interesting line of future work is to investigate the use of t2g2 for generating user utterances, which could be useful for dialogue data augmentation and user simulation."
2020.emnlp-main.527.txt,"
8 Conclusion and Future Work
",2020,we also hope to apply t2g2 to languages other than english.
2020.emnlp-main.527.txt,"
8 Conclusion and Future Work
",2020,"we also present the first set of results on the multidomain sgd dataset, which we hope will pave the way for further research in few-shot, zero-shot and multi-domain language generation."
2020.emnlp-main.527.txt,"
8 Conclusion and Future Work
",2020,"while in this paper we use standard pre-trained models, designing pre-training tasks tailored to sentence fusion is an interesting line of future work."
2020.emnlp-main.528.txt,"
6 Conclusion
",2020,future work involves collecting data to addresses weaknesses of lerc.
2020.emnlp-main.528.txt,"
6 Conclusion
",2020,"this in turn will allow better learned metrics, which can be used to evaluate ever more complex models."
2020.emnlp-main.528.txt,"
6 Conclusion
",2020,we also anticipate a continual cycle of generative rc model and dataset developments that will enable easier collection of more diverse and useful candidates.
2020.emnlp-main.529.txt,"
6 Conclusion
",2020,combining the heterogeneous planning systems will be a crucial step towards developing a human-like language generation.
2020.emnlp-main.529.txt,"
6 Conclusion
",2020,"for example, one can study the generation quality with respect to the position of the target sentences (beginning, middle, end), the comparison of plan keywords predicted by human and system, the effect of data augmentation by their positions (e.g., masking the only middle), the generation quality with respect to the ratio between masked and unmasked sentences, and more."
2020.emnlp-main.529.txt,"
6 Conclusion
",2020,"our results suggest several promising directions: although our ablation tests show the effect of each self-supervision module, types of plan keywords, and the amount of keywords with respect to generation quality, there are more spaces to explore in self-supervised text planning."
2020.emnlp-main.529.txt,"
6 Conclusion
",2020,"our selfsupervised planning, in addition to other types of planning (e.g., discourse, goals, coreference, tenses) can be an important step toward modeling a long-term coherence in text generation."
2020.emnlp-main.529.txt,"
6 Conclusion
",2020,predicting such structural plans from context and imposing them into the generator would be a potential direction for future work.
2020.emnlp-main.529.txt,"
6 Conclusion
",2020,"second, we can extend the set of plan keywords to be more structured like a discourse tree."
2020.emnlp-main.529.txt,"
6 Conclusion
",2020,"ssplanner consists of different kinds of self-supervision modules: sentence positions, a sequence of words or sentences, and the topical relationship between context and target."
2020.emnlp-main.529.txt,"
6 Conclusion
",2020,"to generate more human-like utterances, different planning stages should be simultaneously combined together (kang, 2020), such as abstractive planning, strategic planning, coherence planning, and diversity planning."
2020.emnlp-main.53.txt,"
5 Conclusion and Future Work
",2020,"in the future, we will extend this approach to argument role induction to discover complete event schemas."
2020.emnlp-main.530.txt,"
7 Conclusion
",2020,we show that people use rich language and adopt a range of pragmatic strategies to generate such questions.
2020.emnlp-main.532.txt,"
6 Conclusions and Future Work
",2020,"in the future, we will explore fullfledged solutions to address the privacy concerns of both humans and dialogue systems."
2020.emnlp-main.532.txt,"
6 Conclusions and Future Work
",2020,we hope this work and the dataset will pave the way for the research on privacy leakage in conversations.
2020.emnlp-main.534.txt,"
5 Conclusion
",2020,"as a result, the student is effectively regularized to reach a robust local minimum that represents better generalization performance."
2020.emnlp-main.534.txt,"
5 Conclusion
",2020,"for future work, we will incorporate pre-trained models into our framework (e.g., bert as a teacher and gpt as a student) to further unlock the performance improvement and explore how to balance diverse prior knowledge from multiple teachers."
2020.emnlp-main.534.txt,"
5 Conclusion
",2020,"to incorporate such scenario knowledge without requiring future conversation in inference, we propose an imitation learning framework."
2020.emnlp-main.539.txt,"
7 Conclusion and Discussion
",2020,we believe kvpi will be a useful resource for the research of open-domain dialogue consistency.
2020.emnlp-main.539.txt,"
7 Conclusion and Discussion
",2020,we further test the proposed method on two downstream tasks.
2020.emnlp-main.539.txt,"
7 Conclusion and Discussion
",2020,we hope that the data will aid training dialogue agents to be more consistent.
2020.emnlp-main.54.txt,"
5 Conclusion
",2020,the proposed method leverages both the structural and semantic information of the external knowledge base by performing dynamic multi-hop reasoning on the relational paths.
2020.emnlp-main.540.txt,"
5 Conclusion
",2020,it also fills the gap between experimental and practical applications on multi-label samples.
2020.emnlp-main.541.txt,"
6 Conclusion
",2020,"interesting future work includes developing a fast and efficient version of re-net, and modeling lasting events and performing inference on the long-lasting graph structures."
2020.emnlp-main.542.txt,"
6 Conclusion and Future Work
",2020,"emoji prediction has become a popular task in the nlp community, but the lack of publicly available large-scale datasets with high-quality annotations remains a bottleneck for this task."
2020.emnlp-main.542.txt,"
6 Conclusion and Future Work
",2020,"first, the aspect-level annotation method can be applied to other nlp tasks."
2020.emnlp-main.542.txt,"
6 Conclusion and Future Work
",2020,"second, our annotations in the emoji prediction dataset can be enhanced by including an enriched label set."
2020.emnlp-main.542.txt,"
6 Conclusion and Future Work
",2020,there are two main paths for extending this work.
2020.emnlp-main.542.txt,"
6 Conclusion and Future Work
",2020,we also benchmarked our dataset using a pre-trained bert-largecased model.
2020.emnlp-main.543.txt,"
7 Conclusion & Future Work
",2020,"we hope future research to explore scenarios where human intuition is not working as well as text classification, such as graph attention (velickovic et al., 2017)."
2020.emnlp-main.544.txt,"
5 Conclusion
",2020,"in future work, we plan to validate its effectiveness for aspect-level sentiment classification."
2020.emnlp-main.545.txt,"
4 Conclusion
",2020,"in future work, we will try other types of single networks (e.g., (lai et al., 2015; yang et al., 2016; shimura et al., 2019))."
2020.emnlp-main.545.txt,"
4 Conclusion
",2020,the proposed method can improve the performance of single networks with diverse loss objectives on the tail categories or entire categories.
2020.emnlp-main.546.txt,"
5 Conclusion
",2020,larger pre-training dataset with supervised labels or self-supervised learning strategies could be explored.
2020.emnlp-main.546.txt,"
5 Conclusion
",2020,"moreover, we are interested in understanding what features or traits of essays are captured by the deep models for scoring."
2020.emnlp-main.546.txt,"
5 Conclusion
",2020,"the proposed method has a limitation that it pays more attention to the score range that most essays are from, and may hurt the performance in other ranges."
2020.emnlp-main.546.txt,"
5 Conclusion
",2020,we plan to investigate these in future.
2020.emnlp-main.546.txt,"
5 Conclusion
",2020,we suggest that the prompts’ properties should be investigated more for applying aes.
2020.emnlp-main.548.txt,"
6 Conclusion and Future Work
",2020,"meanwhile, extending these models to a larger scope of question types or more complex scenarios is still a challenge, and we will further investigate the trade-off between explainability and scalability."
2020.emnlp-main.549.txt,"
5 Conclusion
",2020,"in the future, we plan to extend our model to learn the heterogeneous graph automatically, which assures more flexibility for numerical reasoning."
2020.emnlp-main.549.txt,"
5 Conclusion
",2020,"our method not only builds a more compact graph containing different types of numbers, entities, and relations, which can be a general method for other sophisticated reasoning tasks but also conditions the reasoning directly on the question language embedding, which modulates the attention over graph neighbors and change messages being passed iteratively to achieve reasoning."
2020.emnlp-main.549.txt,"
5 Conclusion
",2020,"we would also explore to learn the types of numbers and entities together the reasoning modules using variational autoencoder techniques (kingma and welling, 2014), which may help the ner system better adapt to the numerical reasoning task."
2020.emnlp-main.55.txt,"
8 Conclusion
",2020,"possible future work includes (1) exploring other applications of diverse paraphrasing, such as data augmentation; (2) performing style transfer at a paragraph level; (3) performing style transfer for styles unseen during training, using few exemplars provided during inference."
2020.emnlp-main.551.txt,"
6 Discussion and Conclusion
",2020,"moreover, despite the improvements we observed, the performance of the nlp models is still substantially below the performance of the gnn teacher used for distillation (see appendices b & c), highlighting that significant work that remains to close the gap between the reasoning performance of text-based and gnn-based models."
2020.emnlp-main.551.txt,"
6 Discussion and Conclusion
",2020,"the gnn-based models are particularly strong in this setting (see appendix c), and this suggests that transferring knowledge about the relevancy of facts from structured to unstructured models may be a promising direction."
2020.emnlp-main.552.txt,"
7 Conclusion
",2020,"in these ontologies, we found clear connections to existing categories, such as personhood of named entities."
2020.emnlp-main.552.txt,"
7 Conclusion
",2020,"the high specificity of our method opens doors to more insights from future work, which may include investigating how lsl results vary with probe architecture, developing intrinsic quality measures on latent ontologies, or applying the technique to discover new patterns in settings where gold annotations are not present."
2020.emnlp-main.553.txt,"
7 Conclusion
",2020,this work aims to unveil the mystery of the pretrained language model by looking into how it evolves.
2020.emnlp-main.553.txt,"
7 Conclusion
",2020,we hope our work can bring more insights into what makes a pretrained language model a pretrained language model.
2020.emnlp-main.554.txt,"
7 Discussion
",2020,"a first step in future work would be to test if the results of this paper hold on transformer architectures, or if instead transformers result in different patterns of structural encoding transfer."
2020.emnlp-main.554.txt,"
7 Discussion
",2020,"future work expanding on our results could focus on ablating specific structural features by creating hypothetical languages that differ in single grammatical features from the l2, in the style of galactic dependencies (wang and eisner, 2016), and testing the effect of structured data that’s completely unrelated to language, such as images."
2020.emnlp-main.554.txt,"
7 Discussion
",2020,"last, that encodings derived from hierarchically structured tokens are equally useful for modelling human language as those derived from texts made up of pairs of tokens that are linked but non-hierarchical."
2020.emnlp-main.554.txt,"
7 Discussion
",2020,"our method could be used to test many other hypotheses regarding neural language models, by choosing a discerning set of pretraining languages."
2020.emnlp-main.554.txt,"
7 Discussion
",2020,"our results also contribute to the long-running nature-nurture debate in language acquisition: whether the success of neural models implies that unbiased learners can learn natural languages with enough data, or whether human abilities to acquire language given sparse stimulus implies a strong innate human learning bias (linzen and baroni, 2020)."
2020.emnlp-main.554.txt,"
7 Discussion
",2020,"while the majority of past work analyzing the structural abilities of neural models looks at a model’s treatment of structural features that are realized in specific input sentences, our method compares the encoding and transfer of general grammatical features of different languages."
2020.emnlp-main.555.txt,"
6 Conclusion
",2020,"as a result, it is believed that this study will benefit future work about choosing suitable positional encoding functions or designing other modeling methods for position information in the target nlp tasks based on their properties."
2020.emnlp-main.556.txt,"
7 Ethical Considerations and Conclusion
",2020,"first, we evaluated only english lms, thus we cannot assume these results will extend to lms in different languages."
2020.emnlp-main.556.txt,"
7 Ethical Considerations and Conclusion
",2020,"we hope future work may better address this limitation, as in the work of cao and daume iii ´ (2019)."
2020.emnlp-main.556.txt,"
7 Ethical Considerations and Conclusion
",2020,"while our experiments shed light on artifacts of certain common u.s. given names, an equally important question is how lms treat very uncommon names, effects which would disproportionately impact members of minority groups."
2020.emnlp-main.557.txt,"
7 Conclusion
",2020,"powerful pre-trained models such as bert and roberta perform surprisingly poorly, even after fine-tuning with high-quality distant supervision."
2020.emnlp-main.557.txt,"
7 Conclusion
",2020,we hope our findings and probing dataset will provide a basis for improving pre-trained masked language models’ numerical and other concrete types of commonsense knowledge.
2020.emnlp-main.558.txt,"
5 Conclusion and Future work
",2020,"in future work, we will consider how to interpret environment specifications to facilitate grounded adaptation in these other areas."
2020.emnlp-main.559.txt,"
7 Conclusion and Future Work
",2020,"finally, we believe our algorithm can be applied to save annotation effort for other nlp tasks, especially the low-resource ones (mayhew et al., 2019)."
2020.emnlp-main.559.txt,"
7 Conclusion and Future Work
",2020,"in the future, we will look into more accurate confidence measure via neural network calibration (guo et al., 2017) or using machine learning components (e.g., answer triggering (zhao et al., 2017) or a reinforced active selector (fang et al., 2017))."
2020.emnlp-main.559.txt,"
7 Conclusion and Future Work
",2020,one important future work is thus to conduct large-scale user studies and train parsers from real user interaction.
2020.emnlp-main.559.txt,"
7 Conclusion and Future Work
",2020,"we also plan to derive a more realistic formulation of user/expert annotation costs by analyzing real user statistics (e.g., average time spent on each question)."
2020.emnlp-main.56.txt,"
7 Conclusion and Future Work
",2020,"based on the ac-nlg method, in the future, we can explore the following directions: (1) improve the accuracy of judgment on a claim-level.(2) add external knowledge (e.g.a logic graph) to the predictor for the interpretability of the model."
2020.emnlp-main.560.txt,"
7 Conclusion and Future work
",2020,"for future work, we will explore methods attempting to solve hard and extra hard questions."
2020.emnlp-main.561.txt,"
8 Conclusion and Future Work
",2020,"in the future, we are interested in distilling and reusing the common knowledge from users’ selections."
2020.emnlp-main.562.txt,"
7 Conclusion
",2020,"for future work, we will continually improve the scale and quality of our dataset, to facilitate future research and to meet the need of database-oriented applications."
2020.emnlp-main.563.txt,"
5 Conclusion and Future Work
",2020,"since the current automatic-generated annotations are still noisy, it is useful to further improve the automatic annotation procedure."
2020.emnlp-main.563.txt,"
5 Conclusion and Future Work
",2020,we also plan to extend our approach to cope with multitable text-to-sql task spider.
2020.emnlp-main.564.txt,"
6 Conclusion
",2020,"our study sheds light on the characteristics of text-tosql parsing for future efforts including advanced modeling, problem identification, dataset construction and model evaluation."
2020.emnlp-main.565.txt,"
6 Conclusion
",2020,further research may be concerned with zero-shot learning on new categories.
2020.emnlp-main.565.txt,"
6 Conclusion
",2020,"furthermore, the shared encoder and decoder layers weaken catastrophic forgetting in the incremental learning task."
2020.emnlp-main.566.txt,"
4 Conclusion
",2020,"besides, there are still two important directions for future work: (1) how to apply task-guided pre-training to general domain data when the indomain data is limited.(2) how to design more effective strategies to capture domain-specific and task-specific patterns for selective masking."
2020.emnlp-main.568.txt,"
6 Conclusion
",2020,"another promising direction is to leverage taxonomy construction algorithms (huang et al., 2020) to capture more fine-grained aspects, such as “smell” and “taste” for “food”."
2020.emnlp-main.568.txt,"
6 Conclusion
",2020,"in the future, we plan to adapt our methods to more general applications that are not restricted to the field of sentiment analysis, such as doing multiple-dimension classification (e.g., topic, location) on general text corpus."
2020.emnlp-main.569.txt,"
7 Conclusions and Future Work
",2020,"in the future, we will explore the latent information between peer reviews and author responses to improve argument pair extraction."
2020.emnlp-main.569.txt,"
7 Conclusions and Future Work
",2020,we will also explore related useful research tasks using extra collected information related to scientific work submissions in rr.
2020.emnlp-main.570.txt,"
5 Conclusion
",2020,"in the future, we plan to further improve d-miln with aspect-level annotations and find appropriate way to combine d-miln with pre-training methods (tian et al., 2020)."
2020.emnlp-main.570.txt,"
5 Conclusion
",2020,"we formulate this problem as multiple instance learning, so as to model the relation between aspect-level sentiment and document-level sentiment."
2020.emnlp-main.571.txt,"
6 Conclusion
",2020,"to stimulate research on this topic, we make hypo-cn publicly available.8 in future work, we plan to use hypo and hypo-cn to conduct a cross-lingual study on whether there are differences in the way exaggeration is expressed in english and chinese."
2020.emnlp-main.572.txt,"
6 Conclusion
",2020,"in instance-based domain adaptation, we employ a domain classifier to learn to assign appropriate weights for each word."
2020.emnlp-main.573.txt,"
8 Conclusion
",2020,"more precisely, we show that bert, roberta and distilbert models mostly fail on questions that require inference over the compositional aspects of language, such as semantic roles and negation."
2020.emnlp-main.574.txt,"
7 Conclusions and future work
",2020,"furthermore, we expect to extend the scope of analysis from the attention to an entire transformer architecture to better understand the inner workings and linguistic capabilities of the current powerful systems in nlp."
2020.emnlp-main.574.txt,"
7 Conclusions and future work
",2020,"in future work, we plan to apply our norm-based analysis to attention in other models, such as finetuned bert, roberta (liu et al., 2019), and albert (lan et al., 2020)."
2020.emnlp-main.574.txt,"
7 Conclusions and future work
",2020,one possible direction is to design an attention mechanism that can collect almost no information from an input sequence as the current systems achieve it by exploiting the [sep] token.
2020.emnlp-main.574.txt,"
7 Conclusions and future work
",2020,we believe that these findings can provide insights not only into the interpretation of the behaviors of blackbox nlp systems but also into developing a more sophisticated transformer-based system.
2020.emnlp-main.574.txt,"
7 Conclusions and future work
",2020,we hope that this paper will inspire researchers to have a broader view of the possible methodological choices for analyzing the behavior of transformer-based models.
2020.emnlp-main.576.txt,"
8 Discussion
",2020,"although our results have some implications on them, we leave a detailed study on context-free languages for future work."
2020.emnlp-main.576.txt,"
8 Discussion
",2020,another interesting direction would be to understand whether certain modifications or recently proposed variants of transformers improve their performance on formal languages.
2020.emnlp-main.576.txt,"
8 Discussion
",2020,"evidently, the performance and capabilities of transformers heavily depend on architectural constituents e.g., the positional encoding schemes and the number of layers."
2020.emnlp-main.576.txt,"
8 Discussion
",2020,regular and counter languages model some aspects of natural language while contextfree languages model other aspects such as hierarchical dependencies.
2020.emnlp-main.576.txt,"
8 Discussion
",2020,we showed that transformers can easily generalize on certain counter languages such as shuffle-dyck and boolean expressions in a manner similar to our proposed construction.
2020.emnlp-main.576.txt,"
8 Discussion
",2020,what does the disparity between the performance of transformers on natural and formal languages indicate about the complexity of natural languages and their relation to linguistic analysis?(see also hahn (2020)).
2020.emnlp-main.579.txt,"
5 Conclusion
",2020,we used an entity graph to incorporate common sense knowledge from external knowledge bases into the proposed model.
2020.emnlp-main.581.txt,"
6 Conclusion and Future Work
",2020,"it is inspiring and promising to be generalized to more rewriting tasks, which will be studied as our future work."
2020.emnlp-main.581.txt,"
6 Conclusion and Future Work
",2020,"through our experiments in gec, we verify the feasibility of span-specific decoding, which has been explored for text infilling (raffel et al., 2019) and text rewriting."
2020.emnlp-main.582.txt,"
6 Conclusion and Future Work
",2020,"hence, it is worth developing a novel strategy such as selfsupervised learning to further consider the pronoun."
2020.emnlp-main.582.txt,"
6 Conclusion and Future Work
",2020,"however, the automatic labeling mechanism inevitably accompanies with the wrong labeling problem and it is still an open problem to mitigate the noise.(2) the ds assumption does not consider pronouns in the text, while pronouns play an important role in coreferential reasoning."
2020.emnlp-main.582.txt,"
6 Conclusion and Future Work
",2020,"in the future, there are several prospective research directions: (1) we introduce a distant supervision (ds) assumption in our mrp training task."
2020.emnlp-main.583.txt,"
4 Conclusions
",2020,"in addition, we point out the adjacency matrix and the graph structure can be regarded as some kind of task-related prior knowledge."
2020.emnlp-main.583.txt,"
4 Conclusions
",2020,our results suggest that future works introducing graph structure into nlp tasks should explain their necessity and superiority.
2020.emnlp-main.583.txt,"
4 Conclusions
",2020,this study set out to investigate whether graph structure is necessary for multi-hop qa and what role it plays.
2020.emnlp-main.584.txt,"
7 Conclusions
",2020,"as for future work, we plan to investigate using languages other than english for training (e.g., our larger french and german training sets) in our cross-lingual transfer experiments, since english may not always be the optimal source language (anastasopoulos and neubig, 2020)."
2020.emnlp-main.584.txt,"
7 Conclusions
",2020,"finally, while in our comparative analysis we have focused on a quantitative evaluation for all languages, an additional error analysis per language would be beneficial in revealing the weaknesses and limitations of cross-lingual models."
2020.emnlp-main.586.txt,"
5 Further Discussion and Conclusion
",2020,"given the current large gaps between monolingual and multilingual lms, we will also focus on lightweight methods to enrich lexical content in multilingual lms (wang et al., 2020; pfeiffer et al., 2020)."
2020.emnlp-main.586.txt,"
5 Further Discussion and Conclusion
",2020,"in future work, we plan to investigate how domains of external corpora affect aoc configurations, and how to sample representative contexts from the corpora."
2020.emnlp-main.586.txt,"
5 Further Discussion and Conclusion
",2020,"in-depth analyses of these factors are out of the scope of this work, but they warrant further investigations.opening future research avenues."
2020.emnlp-main.586.txt,"
5 Further Discussion and Conclusion
",2020,"we will also extend the study to more languages, more lexical semantic probes, and other larger underlying lms."
2020.emnlp-main.588.txt,"
8 Conclusion
",2020,"in future work, we hope that slurp will be a valuable resource for developing e2e-slu systems, as well as more traditional pipeline approaches to slu."
2020.emnlp-main.588.txt,"
8 Conclusion
",2020,"the next step is to extend slurp with spontaneous speech, which would again increase its complexity, but also move it one step closer to real-life applications."
2020.emnlp-main.59.txt,"
5 Conclusion and Future Work
",2020,"since way encapsulates multiple embodied localization tasks, there remains much to be explored."
2020.emnlp-main.592.txt,"
6 Conclusion and Future work
",2020,"the above findings shed light on the promising directions for open ner, including 1) exploiting name regularity more efficiently with easilyobtainable resources such as gazetteers; 2) preventing the overfit on popular in-dictionary mentions with constraints or regularizers; and 3) reducing the need of training data by decoupling the acquisition of context knowledge and name knowledge."
2020.emnlp-main.594.txt,"
7 Conclusions
",2020,"also, given the recent success of models such as elmo and bert, it would be interesting to explore extensions of graphglove to the class of contextualized embeddings."
2020.emnlp-main.594.txt,"
7 Conclusions
",2020,"possible directions for future work include using graphglove for unsupervised hypernymy detection, analyzing undesirable word associations, comparing learned graph topologies for different languages, and downstream applications such as sequence classification."
2020.emnlp-main.596.txt,"
7 Conclusion
",2020,"in the future, we aim at applying stare for node and graph classification tasks as well as extend our approach to large-scale kgs."
2020.emnlp-main.596.txt,"
7 Conclusion
",2020,"in the future, we plan to enrich wd50k entities with class labels and probe it against node classification tasks."
2020.emnlp-main.597.txt,"
6 Conclusion
",2020,"in future studies, we plan to increase the number of dimensions of the relational position encodings, since a scalar value may not be able to express positional information adequately."
2020.emnlp-main.598.txt,"
8 Conclusion
",2020,"in future work, we plan to extend our methodology to new languages, and experiment with multilingual and language specific bert models."
2020.emnlp-main.598.txt,"
8 Conclusion
",2020,our intention is also to address adjective ranking in full scales (instead of half-scales) and evaluate the capability of contextualised representations to detect polarity.
2020.emnlp-main.599.txt,"
6 Conclusion
",2020,"we specifically study unsupervised domain adaptation of prlms, where we transfer the models trained in labeled source domain to the unlabeled target domain based on prlm features."
2020.emnlp-main.6.txt,"
5 Conclusion
",2020,this results in our first recommendation in future mt evaluations to avoid the use of source side test data that was created via human translation from another language.
2020.emnlp-main.6.txt,"
5 Conclusion
",2020,"we provided guidance in relation to sample size and statistical power to help planning future human evaluations of mt, particularly relevant to document-level human-parity investigations."
2020.emnlp-main.60.txt,"
6 Conclusion
",2020,image and text aligned data is rich in semantic correspondence.
2020.emnlp-main.60.txt,"
6 Conclusion
",2020,we plan to investigate other automatic tools in curating more accurate denotation graphs with a complex composition of fine-grained concepts for future directions.
2020.emnlp-main.600.txt,"
5 Conclusion
",2020,"in future work, we plan to extend our approach to further improve the reward and policy functions, and to reduce the human-labeling factor."
2020.emnlp-main.601.txt,"
7 Conclusion
",2020,there are exciting avenues for multilingual work to account for language and cultural differences.
2020.emnlp-main.603.txt,"
5 Conclusions and Future Work
",2020,"this work can be extended in several ways: (i) we plan to investigate into further functions for τ to enhance the exploration-exploitation tradeoff.(ii) additional strategies to assign nuclearity should be explored, considering the excessive n-nclassification shown in our evaluation.(iii) we plan to apply our approach to more sentiment datasets (e.g., diao et al.(2014)), creating even larger treebanks.(iv) our new and scalable solution can be extended to also predict discourse relations besides structure and nuclearity.(v) we also plan to use a neural discourse parser (e.g."
2020.emnlp-main.603.txt,"
5 Conclusions and Future Work
",2020,"yu et al.(2018)) in combination with our large-scale treebank to fully leverage the potential of data-driven discourse parsing approaches.(vi) taking advantage of the new mega-dt corpus, we want to revisit the potential of discourse-guided sentiment analysis, to enhance current systems, especially for long documents.(vii) finally, more long term, we intend to explore other auxiliary tasks for distant supervision of discourse, like summarization, question answering and machine translation, for which plenty of annotated data exists (e.g., nallapati et al.(2016); cohan et al.(2018); rajpurkar et al.(2016, 2018))."
2020.emnlp-main.605.txt,"
7 Conclusion and Future Work
",2020,"moreover, we intend to instantiate our proposed framework to other domains such as teacher/student conversations and other types of discourse such as social media narratives."
2020.emnlp-main.605.txt,"
7 Conclusion and Future Work
",2020,we also wish to expand the current face framework to a more comprehensive politeness framework that incorporates notions of power and social distance between the interlocutors.
2020.emnlp-main.605.txt,"
7 Conclusion and Future Work
",2020,we believe that our work may be extended to language generation in chatbots for producing more polite language to mediate face threats.
2020.emnlp-main.605.txt,"
7 Conclusion and Future Work
",2020,we develop computational models for predicting face acts as well as observe the impact of these predicted face acts on the donation outcome.
2020.emnlp-main.607.txt,"
6 Conclusions
",2020,"finally, we would like to combine plts with bert, similarly to attention-xml, but the computational cost of fine-tuning multiple bert encoders, one for each plt node, would be massive, surpassing the training cost of very large transformerbased models, like t5-3b (raffel et al., 2019) and megatron-lm (shoeybi et al., 2019) with billions of parameters (30-100x the size of bert-base)."
2020.emnlp-main.607.txt,"
6 Conclusions
",2020,"in future work, we would like to further investigate few and zero-shot learning in lmtc, especially in bert models that are currently unable to cope with zero-shot labels."
2020.emnlp-main.607.txt,"
6 Conclusions
",2020,pretraining bert from scratch on discharge summaries with a new bpe vocabulary is a possible solution.
2020.emnlp-main.608.txt,"
9 Conclusions
",2020,we hope this work provides some assistance to both those entering the nlp community and those already using contextualized encoders in looking beyond sota (and twitter) to make more educated choices.
2020.emnlp-main.609.txt,"
8 Conclusion and future work
",2020,"scientific claim verification presents a number of promising avenues for research on models capable of incorporating background information, reasoning about scientific processes, and assessing the strength and provenance of various evidence sources."
2020.emnlp-main.609.txt,"
8 Conclusion and future work
",2020,"this last challenge will be especially crucial for future work that seeks to verify scientific claims against sources other than the research literature – for instance, social media and the news."
2020.emnlp-main.609.txt,"
8 Conclusion and future work
",2020,"we hope that the resources presented in this paper encourage future research on these important challenges, and help facilitate progress toward the broader goal of scientific document understanding."
2020.emnlp-main.61.txt,"
8 Conclusion
",2020,"our evaluation verifies the effectiveness of our method, while also indicating a scope for further study, enhancement, and extensions in the future."
2020.emnlp-main.61.txt,"
8 Conclusion
",2020,"our experiments on using the v2c-transformer as a component for the v2cqa task show that the model has transfer learning capabilities that can be applied to other vision-andlanguage tasks such as question-answering, that require commonsense reasoning."
2020.emnlp-main.610.txt,"
6 Conclusion
",2020,this work shows promise of a syntactic treatment of srl and opens up possibilities of applying existing dependency parsing techniques to srl.
2020.emnlp-main.610.txt,"
6 Conclusion
",2020,"we invite future research into further integration of syntactic methods into shallow semantic analysis in other languages and other formulations, such as frame-semantic parsing, and other semantically oriented tasks."
2020.emnlp-main.611.txt,"
6 Conclusion and Future Work
",2020,"in addition, since parade provides entities like “machine code” for definitions, this new dataset could also be useful for other tasks like entity linking (shen et al., 2014), entity retrieval (petkova and croft, 2007) and entity or word sense disambiguation (navigli, 2009)."
2020.emnlp-main.611.txt,"
6 Conclusion and Future Work
",2020,"in the future, we will continue to investigate effective ways to obtain domain knowledge and incorporate it into enhanced models for paraphrase identification."
2020.emnlp-main.612.txt,"
6 Conclusions and Future Work
",2020,"a causal definition is in no way limited to this pairwise case, and future work may generalize it to the sequential case or to event representations that are compositional."
2020.emnlp-main.612.txt,"
6 Conclusions and Future Work
",2020,"having a causal model shines a light on the assumptions made here, and indeed, future work may further refine or overhaul them, a process which may further shine a light on the nature of the knowledge we are after."
2020.emnlp-main.613.txt,"
7 Conclusion
",2020,"based on our analysis, future work in the direction of automatic bias mitigation may include identifying potentially biased examples in an online fashion and discouraging models from exploiting them throughout the training."
2020.emnlp-main.614.txt,"
5 Discussion
",2020,"we suggest that one possibility for future work is to focus on fully unsupervised criteria, such as language model perplexity (shen et al., 2018a, 2019; kim et al., 2019b; peng et al., 2019; li et al., 2020) and model stability across different random seeds (shi et al., 2019), for model selection, as discussed in unsupervised learning work (smith and eisner, 2005, 2006; spitkovsky et al., 2010a,b, inter alia)."
2020.emnlp-main.615.txt,"
7 Conclusions
",2020,"also, we would like to explore how to overcome the obstacles that prevent us from fully exploiting large pretrained lms (e.g., gpt-2) in low-resource settings."
2020.emnlp-main.615.txt,"
7 Conclusions
",2020,"in future work, we intend to experiment with the lm-prior under more challenging conditions, such as when there is domain discrepancy between the parallel and monolingual data."
2020.emnlp-main.616.txt,"
6 Conclusion
",2020,"as a continuation to this work, we intend to evaluate whether multilingual translation models are more resilient to lexical disambiguation biases and, as a consequence, are less susceptible to adversarial attacks that exploit source-side homography."
2020.emnlp-main.616.txt,"
6 Conclusion
",2020,"as such, the presented approach is expected to be transferable to other language pairs and translation directions, assuming that the employed translation models share this underlying weakness."
2020.emnlp-main.616.txt,"
6 Conclusion
",2020,extending model-agnostic attack strategies to incorporate other types of dataset biases and to target natural language processing tasks other than machine translation is likewise a promising avenue for future research.
2020.emnlp-main.616.txt,"
6 Conclusion
",2020,"lastly, the targeted development of models that are resistant to dataset artifacts is a promising direction that is likely to aid generalization across linguistically diverse domains."
2020.emnlp-main.617.txt,"
8 Conclusion
",2020,"in future work, we will apply mad-x to other pre-trained models, and employ adapters that are particularly suited for languages with certain properties (e.g.with different scripts)."
2020.emnlp-main.617.txt,"
8 Conclusion
",2020,it leverages a small number of additional parameters to mitigate the capacity issue which fundamentally hinders current multilingual models.
2020.emnlp-main.617.txt,"
8 Conclusion
",2020,"we will also evaluate on additional tasks, and investigate leveraging pre-trained language adapters from related languages for improved transfer to truly low-resource languages with limited monolingual data."
2020.emnlp-main.618.txt,"
7 Conclusions
",2020,"so as to facilitate similar studies in the future, we release our nli dataset,13 which, unlike previous benchmarks, was annotated in a non-english language and human translated into english."
2020.emnlp-main.619.txt,"
7 Conclusion
",2020,"additionally, in the future, we would also want to quantify the impact of varying degrees of granularity of learning emotional features from tweets on statenet’s performance."
2020.emnlp-main.619.txt,"
7 Conclusion
",2020,"building on psychological studies on analyzing a user’s temporal emotional spectrum, statenet models the time aware emotional context of users through historical tweets for more accurate suicide risk estimation on social media."
2020.emnlp-main.619.txt,"
7 Conclusion
",2020,"priority-based suicide risk assessment for ranking tweets for suicidal risk, rather than classifying them forms our future direction."
2020.emnlp-main.619.txt,"
7 Conclusion
",2020,"through this work, we aim to form a component in a larger human-in-the-loop infrastructure for analyzing potentially concerning suicide-related social media posts."
2020.emnlp-main.619.txt,"
7 Conclusion
",2020,we plan to explore the impact of varying amounts of historical context for a user in our future work.
2020.emnlp-main.62.txt,"
7 Conclusion and Future Work
",2020,future work would be well-suited to explore 1) methods for better understanding which datasets (and individual instances) can be rebalanced and which cannot; and 2) the non-trivial task of estimating additive human baselines to compare against.• hypothesis 2: modeling feature interactions can be data-hungry.
2020.emnlp-main.62.txt,"
7 Conclusion and Future Work
",2020,"our hope is that future work on multimodal classification tasks report not only the predictive performance of their best model + baselines, but also the emap of that model."
2020.emnlp-main.62.txt,"
7 Conclusion and Future Work
",2020,"so, we may need models with different inductive biases and/or much more training data."
2020.emnlp-main.62.txt,"
7 Conclusion and Future Work
",2020,"we postulate the following potential explanations, pointing towards future work: • hypothesis 1: these unbalanced tasks don’t require complex cross-modal reasoning."
2020.emnlp-main.621.txt,"
8 Conclusions
",2020,our framework can be used for other multimodal retrieval tasks (e.g.searching for verified sites as we suggested in the previous section).
2020.emnlp-main.622.txt,"
4 Conclusion
",2020,"future work can explore how to enhance a sub-optimal model using the teacher-student setup for tasks that change across domains or over time, or in scenarios where the original model and data are restricted for privacy reasons."
2020.emnlp-main.623.txt,"
6 Conclusion and Future work
",2020,"furthermore, we hope to explore the quality of fact-checking explanations with respect to properties other than coherence, e.g., actionability and impartiality.lastly, we plan to explore congruity between veracity prediction and explanation generation tasks, i.e., generating explanations which are compatible with the predicted label and vice versa."
2020.emnlp-main.623.txt,"
6 Conclusion and Future work
",2020,"in order to do this, we hope to explore other subjects, in addition to public health, for which factchecking requires a level of expertise in the subject area."
2020.emnlp-main.623.txt,"
6 Conclusion and Future work
",2020,we hope to explore the topics of explainable fact-checking and specialist fact-checking further.
2020.emnlp-main.624.txt,"
5 Conclusion
",2020,"our formulation also bridges broader nlu/rc techniques to address other critical challenges in if games for future work, e.g., common-sense reasoning, noveltydriven exploration, and multi-hop inference."
2020.emnlp-main.626.txt,"
9 Conclusion
",2020,we believe that the contributions made in this work would also generalize to other kinds of expert-lay dialogue like customer-service chats.
2020.emnlp-main.627.txt,"
6 Conclusion
",2020,hesm operates at evidence set level initially and combines information from all the evidence sets using hierarchical aggregation to verify the claim.
2020.emnlp-main.628.txt,"
6 Conclusions
",2020,"in the future, we will investigate the properties of our proposed method on verifying statements with more complicated operations and explore the explainability of the model."
2020.emnlp-main.63.txt,"
6 Discussion and Conclusion
",2020,"our type exposure model allows our network to consider all valid answers per question type as equally probable answer candidates, thus moving away from the negative question-answer linguistic priors."
2020.emnlp-main.63.txt,"
6 Discussion and Conclusion
",2020,we envision that the concept of input mutations can be extended to other vision and language tasks for robustness.
2020.emnlp-main.630.txt,"
6 Conclusion
",2020,future work could investigate the use of non-expert human raters to improve the dataset quality further.
2020.emnlp-main.630.txt,"
6 Conclusion
",2020,"in pursuit of improved entity representations, future work could explore the joint use of complementary multi-language descriptions per entity, methods to update representations in a light-weight fashion when descriptions change, and incorporate relational information stored in the kb."
2020.emnlp-main.631.txt,"
5 Conclusions
",2020,"while our experiments are limited to cjk languages on bert, we believe the methods proposed are generic and simple to implement and expect the performance gains to also apply to different languages and models."
2020.emnlp-main.632.txt,"
8 Conclusion
",2020,"these preliminary results pave the way for further experiments with other language models, various architectures and new downstream tasks."
2020.emnlp-main.633.txt,"
6 Discussion
",2020,"although our model has achieved good performance compressing bert, it would be interesting to explore its possible applications in other neural models."
2020.emnlp-main.633.txt,"
6 Discussion
",2020,"for future work, we would like to explore the possibility of applying theseus compression on heterogeneous network modules."
2020.emnlp-main.633.txt,"
6 Discussion
",2020,"in addition, we would like to conduct theseus compression on more types of neural networks including convolutional neural networks and graph neural networks."
2020.emnlp-main.633.txt,"
6 Discussion
",2020,"therefore, it is potential to apply theseus compression to other large models (e.g., resnet (he et al., 2016) in computer vision)."
2020.emnlp-main.633.txt,"
6 Discussion
",2020,"we will also investigate the combination of our compression-based approach with recently proposed dynamic acceleration method (zhou et al., 2020b) to further improve the efficiency of pretrained language models."
2020.emnlp-main.634.txt,"
6 Conclusion
",2020,then we introduce the objective shifting mechanism to better balance the learning of the pretraining and downstream tasks.
2020.emnlp-main.634.txt,"
6 Conclusion
",2020,"to cope with the absence of pretraining data during the joint learning of the pretraining task, we introduce a pretraining simulation mechanism to learn the pretraining task without data."
2020.emnlp-main.635.txt,"
5 Conclusion
",2020,"our analysis suggests that data size, the similarity between the source and target tasks and domains, and task complexity are crucial for effective transfer, particularly in data-constrained regimes."
2020.emnlp-main.635.txt,"
5 Conclusion
",2020,these task embeddings allow us to predict source tasks that will likely improve target task performance.
2020.emnlp-main.636.txt,"
5 Conclusion & Future Work
",2020,"as part of future work, we will explore cvt on other sequence-labeling tasks such as chunking, elementary discourse unit segmentation and argumentative discourse unit segmentation, thus moving beyond entity-level spans."
2020.emnlp-main.636.txt,"
5 Conclusion & Future Work
",2020,"furthermore, we intend to implement cvt as a training strategy over transformers (bert) and compare it with adaptivelypretrained bert."
2020.emnlp-main.636.txt,"
5 Conclusion & Future Work
",2020,"moreover, other supervised tasks such as classification could also be studied in this context."
2020.emnlp-main.637.txt,"
8 Conclusion
",2020,future work may focus on finding representations that encode the most important information for al.
2020.emnlp-main.637.txt,"
8 Conclusion
",2020,"nevertheless, like other deep models, their accuracy and stability require fine-tuning on large amounts of data."
2020.emnlp-main.637.txt,"
8 Conclusion
",2020,our method is unique because it only relies on self-supervision to conduct sampling.
2020.emnlp-main.637.txt,"
8 Conclusion
",2020,using the pre-trained loss guides the al process to sample diverse and uncertain examples in the cold-start setting.
2020.emnlp-main.638.txt,"
6 Conclusions
",2020,a natural future direction is to conduct a similar empirical investigation of al over bert in the context of multi-class classification and regression tasks.
2020.emnlp-main.638.txt,"
6 Conclusions
",2020,"it would also be interesting to investigate the realm of larger annotation budgets, and more recent bert variants (liu et al., 2019; lan et al., 2019)."
2020.emnlp-main.638.txt,"
6 Conclusions
",2020,"the development of novel al methods, that are tailored for pre-trained models such as bert, seems like an important direction for future work."
2020.emnlp-main.638.txt,"
6 Conclusions
",2020,"these representations, in turn, are proven to be effective for a multitude of downstream nlp tasks."
2020.emnlp-main.638.txt,"
6 Conclusions
",2020,"using the al pipeline, bert improves its recall by a large margin, generalizing beyond the narrow data it was initially exposed to."
2020.emnlp-main.638.txt,"
6 Conclusions
",2020,"we hope that the experimental results and analyses reported here, as well as the release of the research framework we developed, would be instrumental for these and other future studies."
2020.emnlp-main.64.txt,"
5 Conclusion
",2020,"in the future, we will investigate debiasing retrieval-based dialogue models and more complicated pipeline-based dialogue systems."
2020.emnlp-main.640.txt,"
6 Conclusion and Future Work
",2020,"distilling models to their vvma counterparts would be an interesting experiment, and potentially an orthogonal enhancement to pre-existing frameworks (sanh et al., 2019)."
2020.emnlp-main.640.txt,"
6 Conclusion and Future Work
",2020,"in future work, we plan to optimize the lowlevel code and to develop new hardware to deploy vvmas in real-world applications."
2020.emnlp-main.640.txt,"
6 Conclusion and Future Work
",2020,"vvmas could also be an orthogonal contribution to other factorizations of nlp models, such as in (lan et al., 2020)."
2020.emnlp-main.641.txt,"
4 Conclusion
",2020,"we plan to extend these results by studying the mixing of such textual filler-oriented representations with acoustic representations, and further investigate the representation of fillers learnt during pre-training."
2020.emnlp-main.641.txt,"
4 Conclusion
",2020,"when working with deep contextualised representations of transcribed spoken language, we showed that retaining fillers can improve results, both when modelling language and on a downstream task (foak and stance prediction)."
2020.emnlp-main.643.txt,"
8 Conclusion and Future Work
",2020,"analyzing the demographic, cultural, and gender bias in research pertaining to financial disclosures, particularly earnings calls, forms a future direction of research."
2020.emnlp-main.643.txt,"
8 Conclusion and Future Work
",2020,"experimenting with other sets of commonly used acoustic features such as mfcc coefficients, opensmile features and audeep features for representing audio utterances also form a future direction for audio feature extraction."
2020.emnlp-main.643.txt,"
8 Conclusion and Future Work
",2020,"first, we want to improve upon the audio feature extraction."
2020.emnlp-main.643.txt,"
8 Conclusion and Future Work
",2020,"second, we want to expand the analysis presented in this paper beyond the s&p 500 index and us-based companies."
2020.emnlp-main.643.txt,"
8 Conclusion and Future Work
",2020,there are several promising directions of future work that we wish to explore.
2020.emnlp-main.643.txt,"
8 Conclusion and Future Work
",2020,"volatility, measured as a deviation in returns, is a reliable indicator of market risk linked with a stock."
2020.emnlp-main.643.txt,"
8 Conclusion and Future Work
",2020,"we would also want to work on studying a wider set of earnings calls and companies spanning multiple languages, demographics, speakers and gender."
2020.emnlp-main.645.txt,"
4 Conclusions and Future work
",2020,"future direction also include alternate architectures, reward schemes, and evaluation using human judges."
2020.emnlp-main.645.txt,"
4 Conclusions and Future work
",2020,"recent works (lu et al., 2018; d’autume et al., 2019) have proposed some solutions to address these challenges and we plan to explore them."
2020.emnlp-main.646.txt,"
7 Conclusion and future work
",2020,another important direction is to investigate how to integrate the ability to aggregate entities derived from training on tesa into an abstractive summarizer.
2020.emnlp-main.646.txt,"
7 Conclusion and future work
",2020,"in future work, we would like to expand the domains covered by our dataset, which is biased towards topics found in the source corpus, such as politics."
2020.emnlp-main.646.txt,"
7 Conclusion and future work
",2020,this would require models to tackle another challenging issue which we have not addressed: which set of entities should a model aggregate in the first place?
2020.emnlp-main.647.txt,"
7 Conclusion
",2020,"in future works we plan to add other languages including arabic and hindi, and to investigate the adaptation of neural metrics to multilingual summarization."
2020.emnlp-main.649.txt,"
9 Conclusion
",2020,our findings suggest that more intentional and deliberate decisions should be made in selecting summarization datasets for downstream modelling research and that further scrutiny should be placed upon summarization datasets released in the future.
2020.emnlp-main.65.txt,"
6 Conclusion
",2020,an important future direction will be generating the distractors and learning the rationality coefficients.
2020.emnlp-main.65.txt,"
6 Conclusion
",2020,this work investigated how modeling public selfconsciousness can help dialogue agents improve persona-consistency.
2020.emnlp-main.652.txt,"
5 Conclusion
",2020,we hope this work will inspire and assist both dialogue and document modeling for tackling more real-life dialogue tasks.
2020.emnlp-main.653.txt,"
7 Conclusion
",2020,"our results mirror findings from linguistic studies of news interviews (weizman, 2008; heritage, 1985)."
2020.emnlp-main.654.txt,"
7 Conclusion and Future Work
",2020,"finally, another interesting exploration is to extend the model with a jointly trainable movie recommendation and movie information modules."
2020.emnlp-main.654.txt,"
7 Conclusion and Future Work
",2020,"then, we plan to investigate the strategy patterns for people with different personalities and movie preferences to make dialog system more personalized."
2020.emnlp-main.654.txt,"
7 Conclusion and Future Work
",2020,"this work opens up several directions for future studies in building sociable and personalized recommendation dialog systems as follows: first, we will explore more ways of utilizing the strategies, including dynamic strategy selection after decoding."
2020.emnlp-main.655.txt,"
7 Future Work and Conclusion
",2020,the first is to augment our charm model with a text generation module to make a digital version of our human assistants.
2020.emnlp-main.655.txt,"
7 Future Work and Conclusion
",2020,we hope that our dataset will encourage further interest in curiosity-driven dialog.
2020.emnlp-main.655.txt,"
7 Future Work and Conclusion
",2020,we see two immediate directions for future work.
2020.emnlp-main.656.txt,"
6 Conclusion
",2020,newly collected or constructed datasets should consider how to carefully craft the collection to mitigate bias issues from the very start.
2020.emnlp-main.656.txt,"
6 Conclusion
",2020,"the methods described in this paper combine data augmentation, positive-bias data collection, and bias controlled training."
2020.emnlp-main.657.txt,"
8 Conclusions and Future Work
",2020,"future work may explore generating of diverse sets of hypotheses for a given premise and label, with the goal of performing data augmentation."
2020.emnlp-main.657.txt,"
8 Conclusions and Future Work
",2020,other future work will be to measure the performance of gennli on adversarial and similarly challenging nli datasets.
2020.emnlp-main.658.txt,"
5 Conclusion
",2020,beyond this: work on incentive structures and task design could facilitate the creation of crowdsourced datasets that are both creative and consistently labeled.
2020.emnlp-main.658.txt,"
5 Conclusion
",2020,bias mitigation in models and datasets remains a crucial direction for future work if systems based on datasets like the ones we study are to be widely deployed.
2020.emnlp-main.658.txt,"
5 Conclusion
",2020,"while these interventions may be helpful for future evaluation data, it appears that the type of creativity induced by our relatively open-ended base prompt works well for pretraining, and the resulting artifacts are a tolerable side-effect of that creativity."
2020.emnlp-main.659.txt,"
6 Conclusions
",2020,auxiliary analysis datasets are meant to be important resources for debugging and understanding models.
2020.emnlp-main.659.txt,"
6 Conclusions
",2020,"finally, we give suggestions on future research directions and on better analysis variance reporting."
2020.emnlp-main.659.txt,"
6 Conclusions
",2020,"however, large instability of current models on some of these analysis sets undermine such benefits and bring non-ignorable obstacles for future research."
2020.emnlp-main.659.txt,"
6 Conclusions
",2020,we hope this paper will guide researchers on how to handle instability and inspire future work in this direction.
2020.emnlp-main.66.txt,"
8 Conclusion
",2020,it also has a clear advantage in the few-shot experiments when only limited labeled data is available.
2020.emnlp-main.66.txt,"
8 Conclusion
",2020,"tod-bert is easy-to-deploy and will be open-sourced, allowing the nlp research community to apply or fine-tune any task-oriented conversational problem."
2020.emnlp-main.660.txt,"
5 Summary
",2020,"in this work, we studied how to build a textual entailment system that can work in open domains given only a couple of examples, and studied the common patterns in a variety of nlp tasks in which textual entailment can be used as a unified solver."
2020.emnlp-main.660.txt,"
5 Summary
",2020,our goal is to push forward the research and practical use of textual entailment in a broader vision of natural language processing.
2020.emnlp-main.660.txt,"
5 Summary
",2020,the final entailment system ufo-entail generalizes well to open domain entailment benchmarks and downstream nlp tasks including question answering and coreference resolution.
2020.emnlp-main.661.txt,"
7 Conclusion
",2020,"however, we also show limitations of our proposed methods, thereby encouraging future work on conjnli for better understanding of conjunctive semantics."
2020.emnlp-main.662.txt,"
5 Conclusion
",2020,"first, we used nli-tr to analyze the effects of in-language pretraining."
2020.emnlp-main.662.txt,"
5 Conclusion
",2020,"in addition, mt will presumably get cheaper, faster, and better over time, thereby further strengthening our core claims."
2020.emnlp-main.662.txt,"
5 Conclusion
",2020,we also used nli-tr to investigate central issues in turkish nli.
2020.emnlp-main.663.txt,"
7 Conclusion
",2020,we enhance the target semantic model by incorporating syntax in a multitask learning framework.
2020.emnlp-main.664.txt,"
5 Conclusion
",2020,"for mc formalizations, we follow liu et al.for wsc and use spacy to mine candidate nps."
2020.emnlp-main.664.txt,"
5 Conclusion
",2020,this echoes the strong results seen with t5 and offers further motivation to explore these kinds of design decisions in other tasks.
2020.emnlp-main.664.txt,"
5 Conclusion
",2020,we also encourage future reports of system performances to use the same task formalization whenever possible.
2020.emnlp-main.666.txt,"
7 Conclusions
",2020,"in the future, we plan to study how we can apply synsetexpan at the entity mention level for conducting contextualized synonym discovery and set expansion."
2020.emnlp-main.668.txt,"
5 Conclusion
",2020,it also enables parallelization and pre-training in gnn models for further research.
2020.emnlp-main.669.txt,"
7 Conclusion and outlook
",2020,"furthermore, text can be used for few-shot link prediction, an emerging research direction (xiong et al., 2017; shi and weninger, 2017)."
2020.emnlp-main.669.txt,"
7 Conclusion and outlook
",2020,"overall, we hope that codex will provide a boost to research in kgc, which will in turn impact many other fields of artificial intelligence."
2020.emnlp-main.669.txt,"
7 Conclusion and outlook
",2020,"some promising future directions on codex include: • better model understanding codex can be used to analyze the impact of hyperparameters, training strategies, and model architectures in kgc tasks.• revival of triple classification we encourage the use of triple classification on codex in addition to link prediction because it directly tests discriminative power.• fusing text and structure including text in both the link prediction and triple classification tasks should substantially improve performance (toutanova et al., 2015)."
2020.emnlp-main.67.txt,"
6 Conclusion
",2020,"risawoz is featured with large scale, wide domain coverage, rich semantic annotation and functional diversity, which can facilitate the research of task-oriented dialogue modeling from different aspects."
2020.emnlp-main.670.txt,"
6 Conclusion and Future Work
",2020,"in the future, we are interested in effectively integrating different forms of supervision including annotated documents."
2020.emnlp-main.670.txt,"
6 Conclusion and Future Work
",2020,this is another potential direction for the extension of our method.
2020.emnlp-main.671.txt,"
5 Conclusion
",2020,we aims at generating more accurate uncertainty score to improve the performance of text classification with human involvement.
2020.emnlp-main.672.txt,"
6 Conclusion and Future Work
",2020,"finally, it would be interesting to do a deeper dive into variations of author strategies in chapterization, focusing more intently on books with large numbers of short chapters as being more reflective of episode boundaries."
2020.emnlp-main.672.txt,"
6 Conclusion and Future Work
",2020,"our work opens up avenues for further research in text segmentation, with potential applications in summarization and discourse analysis."
2020.emnlp-main.672.txt,"
6 Conclusion and Future Work
",2020,potential future work includes combining the neural and cut-based approaches into a stronger method.
2020.emnlp-main.674.txt,"
7 Conclusion
",2020,we believe our work opens up the necessity of further investigation pertaining to careful information fusion techniques for downstream tasks.
2020.emnlp-main.675.txt,"
6 Conclusions
",2020,"as next steps, we plan to address further types of revisions and extend our experiments to document-level settings."
2020.emnlp-main.676.txt,"
8 Conclusion and Future Work
",2020,"another interesting direction of future research is to explore the cold start problem, where man-sf could be leveraged to predict stock movements for new stocks."
2020.emnlp-main.676.txt,"
8 Conclusion and Future Work
",2020,"lastly, we would also like to extend man-sf’s architecture to not be limited to model all stocks together (because of its gat component) to increase scalability to cross-market scenarios."
2020.emnlp-main.676.txt,"
8 Conclusion and Future Work
",2020,"we plan to further use news articles, earnings calls, and other data sources to capture market dynamics better."
2020.emnlp-main.677.txt,"
7 Conclusions and Future Work
",2020,"future work will investigate ways to improve performance (and especially precision scores) on our data, in particular on low-support labels."
2020.emnlp-main.677.txt,"
7 Conclusions and Future Work
",2020,"in this study, we provided a first stepping stone towards future research at the intersection of nlp and sustainable development (sd)."
2020.emnlp-main.677.txt,"
7 Conclusions and Future Work
",2020,"possible research direction could include more sophisticated thresholding selection techniques (fan and lin, 2007; read et al., 2011) to replace the simple threshold finetuning which is currently used for simplicity."
2020.emnlp-main.677.txt,"
7 Conclusions and Future Work
",2020,"we hope our work will spark interest and open a constructive dialogue between the fields of nlp and sd, and result in new interesting applications."
2020.emnlp-main.679.txt,"
6 Conclusion
",2020,we will continuously explore to improve these models by integrating expert’s knowledge.
2020.emnlp-main.68.txt,"
10 Conclusion
",2020,"the proposed scoring method estimates the quality of utterance pairs by focusing on the two crucial aspects of dialogue, namely, the connectivity and content relatedness of utterance pairs."
2020.emnlp-main.68.txt,"
10 Conclusion
",2020,we hope that this study will facilitate discussions in the dialogue response generation community regarding the issue of filtering noisy corpora.
2020.emnlp-main.680.txt,"
6 Conclusion
",2020,we hope that the dataset shall broaden the target domain of gec beyond learner and/or exam writing and facilitate the development of robust gec models in the open-domain setting.
2020.emnlp-main.682.txt,"
6 Conclusion and Future Work
",2020,future work can use anomaly detection approaches operating on our model’s predicted word vectors to detect anomalies in a word’s representation across time.
2020.emnlp-main.682.txt,"
6 Conclusion and Future Work
",2020,"we also plan to investigate different architectures, such as variational autoencoders (kingma and welling, 2014), and incorporate contextual representations (devlin et al., 2019; hu et al., 2019) to detect new senses of words."
2020.emnlp-main.684.txt,"
5 Conclusion
",2020,the extension of the scope will be the future work.
2020.emnlp-main.685.txt,"
7 Conclusion and Future Work
",2020,"in future work we plan to apply our model to longer, book length documents, and plan to add more structure to the memory."
2020.emnlp-main.687.txt,"
8 Conclusion
",2020,we believe there is much potential for additional selfsupervision tasks and leave those for future work.
2020.emnlp-main.688.txt,"
5 Conclusions
",2020,"in future work, we would like to study how to introduce acyclic rules to the walk-based systems."
2020.emnlp-main.689.txt,"
6 Conclusion and Future Work
",2020,"as discussed in section 5, we also plan to develop a more flexible scoring function which can handle equivalent trees."
2020.emnlp-main.689.txt,"
6 Conclusion and Future Work
",2020,"finally, we plan to evaluate bertft on other temporal relation datasets as part of a larger pipeline, which will include a mapping between tdts and other temporal relation annotation schemas such as the tempeval-3 dataset (uzzaman et al., 2013)."
2020.emnlp-main.689.txt,"
6 Conclusion and Future Work
",2020,"for future research, we plan to explore other types of deep neural lms such as transformerxl (dai et al., 2019) and xlnet (yang et al., 2019)."
2020.emnlp-main.69.txt,"
6 Conclusions
",2020,"future directions include the incorporation of phonological and morphosyntactic features, application to other languages, and most importantly, a model extension to infer temporal ordering."
2020.emnlp-main.691.txt,"
6 Conclusion
",2020,"by performing sequence mixup in the latent space, seqmix improves data diversity during active learning, while being able to generate plausible augmented sequences."
2020.emnlp-main.691.txt,"
6 Conclusion
",2020,"for future research, it is interesting to enhance seqmix with language models during the mixup process, and harness external knowledge for further improving diversity and plausibility."
2020.emnlp-main.691.txt,"
6 Conclusion
",2020,this method is generic to different active learning policies and various sequence labeling tasks.
2020.emnlp-main.692.txt,"
8 Conclusions
",2020,"future work may want to build on our approach for more comprehensive extraction tasks, focussing on more types of result, as well as other information contained in papers such as architectural details and hyperparameters."
2020.emnlp-main.692.txt,"
8 Conclusions
",2020,"our method performs well across various tasks and leaderboards within machine learning, with a taxonomy that can be easily extended without retraining."
2020.emnlp-main.693.txt,"
6 Conclusion and discussion
",2020,a potential attempt might be to use kg to design the reward in the rl framework to provide weak supervision.
2020.emnlp-main.693.txt,"
6 Conclusion and discussion
",2020,our framework also contributes to areas of knowledge graph completion and automatic question-answering for attribute values.
2020.emnlp-main.693.txt,"
6 Conclusion and discussion
",2020,our solution for attribute value extraction can be extended to other nlp tasks.
2020.emnlp-main.693.txt,"
6 Conclusion and discussion
",2020,the remaining computation cost from rl framework is comparably small during both the training and the prediction process.
2020.emnlp-main.693.txt,"
6 Conclusion and discussion
",2020,we leave this as our future work.
2020.emnlp-main.694.txt,"
6 Conclusion
",2020,"future work can focus on expanding the capabilities to generating whole paragraphs of text from graphs in kb, as well as converting large parts of text into coherent graph structures."
2020.emnlp-main.696.txt,"
5 Conclusion
",2020,our work also suggests that standard model components like embedding tying should be retested as we continue to explore the space of language modeling.
2020.emnlp-main.697.txt,"
5.8 Conclusion
",2020,such a framework provides a plausible solution to greatly reduce human annotation costs in future nlg applications.
2020.emnlp-main.698.txt,"
5 Conclusion
",2020,"in future work, we hope to leverage sentence structure, such as the use of constituency parsing, to further enhance the design of the progressive hierarchy."
2020.emnlp-main.698.txt,"
5 Conclusion
",2020,our model can be also extended to allow inflected/variant forms and arbitrary ordering of given lexical constraints.
2020.emnlp-main.7.txt,"
7 Conclusion
",2020,"as paraphrasing continues to improve and cover more languages, we are optimistic smrt will provide larger improvements across the board—including for higher-resource mt and for additional target languages beyond english."
2020.emnlp-main.70.txt,"
5 Conclusions
",2020,future work will be required to assess the extent to which these effects do in fact reflect the acquisition of a latent form of discourse modeling ability.
2020.emnlp-main.70.txt,"
5 Conclusions
",2020,"second, we have focused here on broad contrasts between context types that have been studied in the psycholinguistic literature."
2020.emnlp-main.70.txt,"
5 Conclusions
",2020,"since an experiment that collects data on this scale would require a substantial annotation effort, a more careful comparison of this sort must be left for future work."
2020.emnlp-main.70.txt,"
5 Conclusions
",2020,"that having been said, we consider the identification of alternative language model architectures that are capable of capturing the requisite discourse modeling capability for this task to be an interesting challenge problem for future work."
2020.emnlp-main.70.txt,"
5 Conclusions
",2020,"this improves the robustness of our results in terms of items—whereas participants in psycholinguistic studies typically see only one example sentence for each verb, the lms here saw 24—it also means that no lab data exists for the exact stimuli used here."
2020.emnlp-main.70.txt,"
5 Conclusions
",2020,we hope that this short paper will inspire further research that takes next steps in this and a variety of other directions.
2020.emnlp-main.70.txt,"
5 Conclusions
",2020,"whereas this limitation is shared with previous work that probes lms for inherently acquired syntactic knowledge, the robustness of the findings would be enhanced by examining a broader range of systems and/or system configurations so as to better capture the kinds of variation found among groups of human participants."
2020.emnlp-main.700.txt,"
5 Conclusions
",2020,"in this work, we propose palm, a novel approach to pre-training an autoencoding and autoregressive language model on a large unlabeled corpus, designed to be fine-tuned on downstream generation conditioned on context."
2020.emnlp-main.700.txt,"
5 Conclusions
",2020,our future work will explore the potential of training palm for longer on much more unlabeled text data.
2020.emnlp-main.701.txt,"
5 Conclusion & Future Works
",2020,future research works can be conducted to make the generation process more robust and interpretable.
2020.emnlp-main.701.txt,"
5 Conclusion & Future Works
",2020,"we applied our method in two tasks, keyword-to-sentence generation, and unsupervised paraphrasing."
2020.emnlp-main.702.txt,"
5 Conclusion
",2020,"overall, teaforn is a promising approach for improving quality and/or reducing inference costs in sequence generation models."
2020.emnlp-main.703.txt,"
7 Conclusions
",2020,computer vision and speech recognition are mature enough for investigation of broader linguistic contexts (ws3).
2020.emnlp-main.703.txt,"
7 Conclusions
",2020,"our call to action is to encourage the community to lean in to trends prioritizing grounding and agency, and explicitly aim to broaden the corresponding world scopes available to our models."
2020.emnlp-main.703.txt,"
7 Conclusions
",2020,simulators and videogames provide potential environments for social language learners (ws5).
2020.emnlp-main.703.txt,"
7 Conclusions
",2020,the robotics industry is rapidly developing commodity hardware and sophisticated software that both facilitate new research and expect to incorporate language technologies (ws4).
2020.emnlp-main.704.txt,"
6 Conclusion
",2020,by releasing our dataset and code we hope to provide a solid foundation to accelerate work in this direction.
2020.emnlp-main.704.txt,"
6 Conclusion
",2020,"in the future, we believe that strong linguistic priors will continue to be a key ingredient for building nextlevel learning agents in these games."
2020.emnlp-main.705.txt,"
7 Conclusion
",2020,"we defined and studied the capwap task, where question-answer pairs provided by users are used as a source of supervision for learning their visual information needs."
2020.emnlp-main.705.txt,"
7 Conclusion
",2020,we hope this work will motivate the image captioning field to learn to anticipate and provide for the information needs of specific user communities.
2020.emnlp-main.708.txt,"
6 Conclusion
",2020,"we also recommend that when collecting a new dataset, the test set should include more parallel references for fair evaluation, while for the training set, when the text generations are expected to be distinctive and complicated, more parallel references should be collected otherwise a larger variety of visual appearances is more favorable."
2020.emnlp-main.710.txt,"
5 Conclusion
",2020,future work includes investigating the interaction and joint training between hgn and paragraph retriever for performance improvement.
2020.emnlp-main.711.txt,"
5 Conclusion
",2020,"powerful pre-trained models allow us to score sentences one at a time, without looking at other paragraphs."
2020.emnlp-main.711.txt,"
5 Conclusion
",2020,"this result shows that supporting sentence identification in hotpotqa is itself not a multi-hop problem, and suggests focusing on other multi-hop datasets to demonstrate the value of more complex retrieval techniques."
2020.emnlp-main.712.txt,"
6 Conclusions
",2020,"additionally, for factual reading comprehension datasets where the correct answer can be arrived at without consulting all annotated facts in the input context, our probe will unfairly penalize a model that uses implicitly known facts, even if it correctly connects information across these facts."
2020.emnlp-main.712.txt,"
6 Conclusions
",2020,"extending it to a sequence of facts (e.g., as in multirc (khashabi et al., 2018)) requires accounting for the potential of new artifacts by, for instance, carefully replacing rather than dropping facts."
2020.emnlp-main.712.txt,"
6 Conclusions
",2020,"using a notion of contrastive sufficiency, it showed how to automatically transform existing support-annotated multi-hop datasets to create a more difficult and less cheatable dataset that results in reduced disconnected reasoning."
2020.emnlp-main.712.txt,"
6 Conclusions
",2020,we leave further exploration to future work.
2020.emnlp-main.713.txt,"
7 Conclusion
",2020,"overall, this work opens up exciting avenues for leveraging methods in unsupervised learning and natural language generation to improve the interpretability and generalization of machine learning systems."
2020.emnlp-main.714.txt,"
7 Conclusion
",2020,"in creating the edges and nodes of the graph, we exploit a semantic role labeling sub-graph for each sentence and connect the candidate supporting facts."
2020.emnlp-main.714.txt,"
7 Conclusion
",2020,"moreover, we evaluate the model (excluding the paragraph selection module) on other reading comprehension benchmarks."
2020.emnlp-main.715.txt,"
9 Conclusion and Future Work
",2020,"finally, we will carry out a thorough investigation into emotion-cause pairs (xia and ding, 2019)."
2020.emnlp-main.715.txt,"
9 Conclusion and Future Work
",2020,"in the future, we plan to study how contextual information (i.e., different aspects of people’s interactions captured through contiguous posts in a discussion thread) affects the perceived emotions."
2020.emnlp-main.715.txt,"
9 Conclusion and Future Work
",2020,we also plan to perform a cross-corpus analysis to investigate if emotions are expressed differently in the health domain compared to other domains.
2020.emnlp-main.715.txt,"
9 Conclusion and Future Work
",2020,"we believe that these characteristics add interestingness and challenges to our dataset and we hope that our work will spur future research in emotion detection from health data, especially in the context of life-threatening diseases such as cancer."
2020.emnlp-main.717.txt,"
6 Conclusion
",2020,"in addition, we will study more explicitly how to decouple stance models from sentiment, and how to improve performance further on difficult phenomena."
2020.emnlp-main.717.txt,"
6 Conclusion
",2020,"in future work we plan to investigate additional methods to represent and use generalized topic information, such as topic modeling."
2020.emnlp-main.720.txt,"
5 Conclusion
",2020,an important avenue of future work will be to assess to what extent there may be cultural differences in these associations (see discussion in section 3.1).
2020.emnlp-main.720.txt,"
5 Conclusion
",2020,"as emoji use is now ubiquitous on mobile devices and social media, we believe that ultimately any nlp task involving social media text may benefit from such emoji resources."
2020.emnlp-main.720.txt,"
5 Conclusion
",2020,"finally, we rely on our annotated data to study how well we can automatically estimate emotional association ratings for a given emoji, considering a series of different baseline methods and resources."
2020.emnlp-main.720.txt,"
5 Conclusion
",2020,"however, this connection has not been studied in sufficient detail, at the level of individual emojis."
2020.emnlp-main.720.txt,"
5 Conclusion
",2020,"similarly, variation with respect to age and other variables merits further study as well."
2020.emnlp-main.720.txt,"
5 Conclusion
",2020,this opens up further research avenues on possible downstream applications exploiting this knowledge.
2020.emnlp-main.722.txt,"
6 Conclusion
",2020,"in the future, instead of pre-training on sentences, we will leverage raw text at passage or document level to alleviate the performance degeneration brought by short context."
2020.emnlp-main.722.txt,"
6 Conclusion
",2020,"in this work, we aim at equipping pre-trained lms with structured knowledge via self-supervised tasks."
2020.emnlp-main.722.txt,"
6 Conclusion
",2020,"moreover, we will use a combination of commonsense and ontological kgs, and large-scale corpora (e.g., common crawl) to pre-train an mlm from scratch, which we expect to benefit a wide range of tasks."
2020.emnlp-main.724.txt,"
6 Conclusions
",2020,we also point out several directions for future work by generalizing our methods to other tasks or combining with other techniques.
2020.emnlp-main.725.txt,"
5 Conclusion
",2020,"in the future, we plan to extend our model to cope with external word or document semantics."
2020.emnlp-main.725.txt,"
5 Conclusion
",2020,it would also be interesting to explore alternative architectures other than cyclegan under our formulation of topic modeling.
2020.emnlp-main.725.txt,"
5 Conclusion
",2020,the effectiveness of tomcat and stomcat is verified by experiments on topic modeling and text classification.
2020.emnlp-main.726.txt,"
8 Conclusion
",2020,"by leveraging an off-the-shelf language model (gpt-2), we successfully guide the generation towards a specified direction (i.e, target class), with the help of reinforcement learning."
2020.emnlp-main.726.txt,"
8 Conclusion
",2020,"in the future, we plan to implement a more sophisticated guidance for the augmentation by adding syntactic and position features to the reward function, to enable augmentation of more diverse types of text data."
2020.emnlp-main.727.txt,"
5 Conclusion and Future Work
",2020,it would be even better if the state detection and segmentation step can be integrated with subsequent state-independent feature extraction and rumor detection in an end-to-end framework.
2020.emnlp-main.727.txt,"
5 Conclusion and Future Work
",2020,"secondly, it is a retrospective algorithm which depends on the condition all posts along the timeline should be provided in advance."
2020.emnlp-main.727.txt,"
5 Conclusion and Future Work
",2020,"therefore, one direction for future work is to explore an online state detection algorithm and perform it for each event, but at the same time ensure that the state of each event is globally defined."
2020.emnlp-main.728.txt,"
5 Conclusion
",2020,"looking forward, we plan to leverage pymt5 for various downstream automated software engineering tasks—including code documentation and method generation from natural language statements—and develop more model evaluation criteria to leverage the unique properties of source codes."
2020.emnlp-main.729.txt,"
7 Conclusion and Future Work
",2020,"first, we would like to explore more explainable reasoning method for question generation, such as symbolic-based models."
2020.emnlp-main.729.txt,"
7 Conclusion and Future Work
",2020,"in the future, there can be two research directions."
2020.emnlp-main.73.txt,"
8 Conclusion
",2020,"this will be particularly interesting, as recent study showed latent variable models with a flexible prior give high test loglikelihoods, but suffer from poor generation quality as inference is challenging (lee et al., 2020)."
2020.emnlp-main.73.txt,"
8 Conclusion
",2020,"while we showed that iterative inference with a learned score function is effective for spherical gaussian priors, more work is required to investigate if such an approach will also be successful for more sophisticated priors, such as gaussian mixtures or normalizing flows."
2020.emnlp-main.730.txt,"
7 Conclusions and Future Work
",2020,future work includes applying time inference models to question answering and other nlp systems.
2020.emnlp-main.730.txt,"
7 Conclusions and Future Work
",2020,we also seek to annotate information about dates and seasons.
2020.emnlp-main.731.txt,"
8 Conclusion
",2020,what architecture would be needed to solve cogs?
2020.emnlp-main.732.txt,"
7 Conclusions
",2020,"despite these facts, negation is underrepresented in some natural language inference benchmarks (rte and snli)."
2020.emnlp-main.733.txt,"
5 Conclusion and Future Work
",2020,"in the future, we are looking forward to diving in representation learning with flow-based generative models from a broader perspective."
2020.emnlp-main.734.txt,"
6 Discussion & Conclusion
",2020,"in conclusion, we hope our data and analysis inspire future directions such as explicit modeling of collective human opinions; providing theoretical supports for the connection between human disagreement and the difficulty of acquiring language understanding in general; exploring potential usage of these human agreements; and studying the source of the human disagreements and its relations to different linguistic phenomena."
2020.emnlp-main.734.txt,"
6 Discussion & Conclusion
",2020,"to address this concern, we suggest nlp models be evaluated against the collective human opinion distribution rather than one opinion aggregated from a set of opinions, especially on tasks which take a descriptivist approach10 to language and meaning, including nli and common sense reasoning."
2020.emnlp-main.736.txt,"
5 Conclusion
",2020,"as future work, we will explore the similar idea of designing unreferenced metrics for dialog generation."
2020.emnlp-main.737.txt,"
5 Conclusion
",2020,"since it can be quickly adopted to replace the traditional likelihood objective, we believe in broader applicability of f2-softmax."
2020.emnlp-main.737.txt,"
5 Conclusion
",2020,"thus, future work involves extending the method to other related tasks, such as machine translation and text summarization, and investigating the potential gains from transfer learning."
2020.emnlp-main.739.txt,"
7 Conclusion
",2020,"as future work, we would like extend the prior network to sample more than one persona sentences by expanding the sample space of the discrete random variable to generate more interesting responses."
2020.emnlp-main.739.txt,"
7 Conclusion
",2020,"while our expansions are limited by the performance of comet or paraphrase systems, we envision future work to train the dialog model endto-end along with the expansion generation."
2020.emnlp-main.74.txt,"
5 Conclusion and Future Work
",2020,"in our future work, i) we are interested in distilling from deep nmt models into extremely small students with ckd, in the hope of achieving the same results of large models with much smaller counterparts.ii) we also try to improve the combination module and find a better alternative than concatenation.iii) finally, we plan to evaluate ckd in other tasks such as language modeling."
2020.emnlp-main.74.txt,"
5 Conclusion and Future Work
",2020,we applied our technique in nmt and showed its potential in training high-quality and compact models.
2020.emnlp-main.740.txt,"
7 Conclusion and Future Work
",2020,"furthermore, we develop labes-s2s, which is a copy-augmented seq2seq model instantiation of labes."
2020.emnlp-main.740.txt,"
7 Conclusion and Future Work
",2020,there are some interesting directions for future work.
2020.emnlp-main.741.txt,"
7 Conclusions
",2020,it automatically constructs different types of grayscale data and uses a multi-level ranking objective.
2020.emnlp-main.742.txt,"
5 Conclusion and Discussion
",2020,"besides, we also release a new large-scale human evaluation bench-mark to facilitate future research on automatic metrics."
2020.emnlp-main.743.txt,"
6 Conclusions and Future Works
",2020,"for future work, we will annotate medical entities in our datasets."
2020.emnlp-main.743.txt,"
6 Conclusions and Future Works
",2020,such annotations can facilitate the development of goal-oriented medical dialog systems.
2020.emnlp-main.743.txt,"
6 Conclusions and Future Works
",2020,"to facilitate the research and development of medical dialogue systems that can potentially assist in telemedicine, we build large-scale medical dialogue datasets – meddialog – which contain 1) a chinese dataset with 3.4 million conversations between patients and doctors, 11.3 million utterances, 660.2 million tokens, covering 172 specialties of diseases, and 2) an english dataset with 0.26 million conversations, 0.51 million utterances, 44.53 million tokens, covering 96 specialties of diseases."
2020.emnlp-main.743.txt,"
6 Conclusions and Future Works
",2020,we use transfer learning to apply these pretrained models for low-resource dialogue generation.
2020.emnlp-main.746.txt,"
8 Conclusion
",2020,our work shows the effectiveness of simple training dynamics measures based on mean and standard deviation; exploration of more sophisticated measures to build data maps is an exciting future direction.
2020.emnlp-main.747.txt,"
7 Concluding Discussion
",2020,our goal is to highlight the variance in these properties and call for more widespread empirical evaluations thereof.actionable implications.
2020.emnlp-main.747.txt,"
7 Concluding Discussion
",2020,"the explainable ai community hopes to use them as a guide for evaluating model explanations and, possibly, for teaching models to make robust and wellreasoned decisions."
2020.emnlp-main.748.txt,"
5 Conclusion
",2020,"as with other problem domains, we have observed that abstractive summaries generated by transformers can generate imaginary content."
2020.emnlp-main.748.txt,"
5 Conclusion
",2020,it would be very interesting to see what kind of performance larger models could achieve.
2020.emnlp-main.748.txt,"
5 Conclusion
",2020,"while we believe that this work is a step forward towards generating more abstractive summaries, it remains an open challenge to develop abstactive models that respect the underlying facts of the content being summarized while matching the creative ability of humans to coherently and concisely synthesize summaries."
2020.emnlp-main.749.txt,"
5 Conclusion
",2020,"for future work, we plan to apply our method for other type of spans, such as noun phrases, verbs, and clauses."
2020.emnlp-main.75.txt,"
6 Conclusion
",2020,"for future work, we are interested in investigating the proposed approach in a scaled setting with more languages and a larger amount of monolingual data."
2020.emnlp-main.75.txt,"
6 Conclusion
",2020,"furthermore, we would also like to explore the most sample efficient strategy to add a new language to a trained mnmt system."
2020.emnlp-main.75.txt,"
6 Conclusion
",2020,scheduling the different tasks and different types of data would be an interesting problem.
2020.emnlp-main.750.txt,"
6 Conclusions
",2020,shortcomings of our approach explained in section 5.2 can serve as guidelines for future work.
2020.emnlp-main.750.txt,"
6 Conclusions
",2020,we hope that this work will encourage continued research into factual consistency checking of abstractive summarization models.
2020.emnlp-main.751.txt,"
6 Implications and Future Directions
",2020,future works on meta-evaluation should investigate the effect of these settings on the performance of metrics.(2) metrics easily overfit on limited datasets.
2020.emnlp-main.752.txt,"
7 Conclusion
",2020,"in near future, we aim to incorporate the video script information in the multimodal summarization process."
2020.emnlp-main.76.txt,"
7 Conclusion
",2020,further analyses show that our method can also improve the lexical diversity.
2020.emnlp-main.78.txt,"
5 Conclusion
",2020,"future directions include continuing the exploration of this research topic for large sequenceto-sequence pre-training models (liu et al., 2020) and multi-domain translation models (wang et al., 2019b)."
2020.emnlp-main.78.txt,"
5 Conclusion
",2020,"we also analyze the gains from perspectives of learning dynamics and linguistic probing, which give insightful research directions for future work."
2020.emnlp-main.78.txt,"
5 Conclusion
",2020,"we will employ recent analysis methods to better understand the behaviors of rejuvenated models (he et al., 2019; yang et al., 2020)."
2020.emnlp-main.8.txt,"
7 Conclusion and Future Work
",2020,"in future work, we would like to extend prism to paragraph- or document-level evaluation by training a paragraph- or document-level multilingual nmt system, as there is growing evidence that mt evaluation would be better conducted at the document level, rather than the sentence level (laubli et al., 2018)."
2020.emnlp-main.8.txt,"
7 Conclusion and Future Work
",2020,we are optimistic our method will improve further as stronger multilingual nmt models become publicly available.
2020.emnlp-main.80.txt,"
5 Conclusion
",2020,"besides, as this idea is not limited to machine translation, it is also interesting to validate our model in other nlp tasks, such as low-resource nmt model training (lample et al., 2018; wan et al., 2020) and neural architecture search (guo et al., 2020)."
2020.emnlp-main.80.txt,"
5 Conclusion
",2020,"it is interesting to combine with other techniques (li et al., 2018; hao et al., 2019) to further improve nmt."
2020.emnlp-main.81.txt,"
4 Discussions and Conclusions
",2020,"we’re encouraged by our findings that in tandem with the great machinery that could bring powerful results, simplistic approaches could be just as efficacious."
2020.emnlp-main.82.txt,"
8 Conclusion
",2020,"also, we’ll try to extend our methods in a wider range of nlp tasks."
2020.emnlp-main.82.txt,"
8 Conclusion
",2020,"in future work, firstly, since our model is randomly sampled from model distribution to generate diverse translation, it is meaningful to explore better algorithms and training strategies to represent model distribution and search for the most distinguishable results in model distribution."
2020.emnlp-main.82.txt,"
8 Conclusion
",2020,"we represent the transformer model distribution with dropout, and train the model distributions to minimize its distance to the posterior distribution under specific training dataset."
2020.emnlp-main.83.txt,"
7 Conclusion
",2020,applying these latent alignment models for parallel translation of long documents can be an interesting research direction.
2020.emnlp-main.84.txt,"
6 Conclusion
",2020,"we introduce several de-biasing methods to make models to ignore the spurious positional cues, and find out that the sentence-level answer prior is very useful."
2020.emnlp-main.85.txt,"
7 Conclusion
",2020,"in addition to the elements of this task which are appealing from a common sense modeling perspective, the inherent appeal of this task to humans opens a number of possibilities for future data collection and evaluation."
2020.emnlp-main.85.txt,"
7 Conclusion
",2020,the inclusion of counts over clusters of answers provides a very rich dataset for training and evaluation.
2020.emnlp-main.86.txt,"
7 Conclusion
",2020,"these questions require identifying what information is missing from a paragraph in order to answer a question, predicting where to find it, then synthesizing the retrieved information in complex ways."
2020.emnlp-main.87.txt,"
13 Discussion and Future Work
",2020,how to most efficiently and effectively adapt transformer-based qa systems remains an important topic for future research.
2020.emnlp-main.87.txt,"
13 Discussion and Future Work
",2020,"we then investigated iterative “pre-train then fine-tune” approaches to target domain adaptation, proposed question answer posterior (qap) as an alternative form of consistency filtering, and provided theoretical justification for roundtrip consistency filtering."
2020.emnlp-main.88.txt,"
8 Conclusion
",2020,"torque has 3.2k news snippets, 9.5k hard-coded questions asking which events had happened, were ongoing, or were still in the future, and 21.2k human-generated questions querying more complex phenomena."
2020.emnlp-main.9.txt,"
5 Discussion and Future Work
",2020,another modeling choice could explicitly condition the qa module on the node and edge modules so that the answer is predicted from the proof.
2020.emnlp-main.9.txt,"
5 Discussion and Future Work
",2020,"however, in scenarios where questions have open-ended answers, generating answer from a ‘proof’ in a consistent manner needs more exploration."
2020.emnlp-main.9.txt,"
5 Discussion and Future Work
",2020,"however, other tasks may require imposing additional constraints to ensure valid explanations.prover’s inference mechanism can be extended to incorporate these."
2020.emnlp-main.9.txt,"
5 Discussion and Future Work
",2020,"in such scenarios, prover’s unweighted proof graphs can be extended to weighted ones to represent this probabilistic nature."
2020.emnlp-main.9.txt,"
5 Discussion and Future Work
",2020,"qa and proof consistency: currently, prover predicts the answer and generates the proof by jointly optimizing the qa, node and edge modules using a shared roberta model."
2020.emnlp-main.9.txt,"
5 Discussion and Future Work
",2020,we hope that prover will encourage further work towards developing interpretable nlp models with structured explanations.
2020.emnlp-main.90.txt,"
7 Conclusions and Future Work
",2020,"in the future, we will explore more informative generation and consider applying mgcn to other nlp tasks for better information extraction and aggregation."
2020.emnlp-main.91.txt,"
7 Conclusion and Future Work
",2020,"moreover, future work should inspect the effect of split and rephrase on downstream tasks such as machine translation or information retrieval, and examine if models’ performance on these tasks correlate with that on our benchmarks."
2020.emnlp-main.92.txt,"
6 Conclusion
",2020,we investigated back-parsing for amr-to-text generation by integrating the prediction of projected amrs into sentence decoding.
2020.emnlp-main.93.txt,"
6 Conclusions and Future Work
",2020,"in future, we plan to explore the following two directions: (1) interpolating the contexts between consecutive steps by introducing a new infilled image, and (2) addressing the underspecification problem by controlling the content in infilled image with explicit guidance."
2020.emnlp-main.94.txt,"
6 Conclusion
",2020,our neural poet is available at https://nala-cub.github.io/resources as a baseline for future research on the task.
2020.emnlp-main.96.txt,"
6 Conclusion
",2020,"in future work, it would be interesting to investigate to what extent pretrained language models benefit from groc on such zero-resource or lowresource adaptation settings."
2020.emnlp-main.96.txt,"
6 Conclusion
",2020,"this work indicates several other future directions for language modeling in low-resource domains: extension to other languages, scaling training to even larger vocabularies, and applying groc in a large pretraining setting to expand its zero-shot generalization."
2020.emnlp-main.97.txt,"
8 Conclusion
",2020,"future work will explore applying ssmba to the target side manifold in structured prediction tasks, as well as other natural language tasks and settings where data augmentation is difficult."
2020.emnlp-main.98.txt,"
5 Conclusion
",2020,"moreover, the proposed method can be easily migrated and applied to data of other types (e.g., images) with few modifications."
2020.emnlp-main.98.txt,"
5 Conclusion
",2020,we leave it for future work.
2020.emnlp-main.99.txt,"
8 Conclusion
",2020,the proposed mhgrn generalizes and combines the advantages of gnns and path-based reasoning models.
2020.emnlp-main.99.txt,"
8 Conclusion
",2020,"we present a principled, scalable method, mhgrn, that can leverage general knowledge via multi-hop reasoning over interpretable structures (e.g.conceptnet)."
P16-1001,"
7 Conclusions
",2016,"we anticipate the approaches that we have found useful in the case of amr to reduce the impact of noise, efficiently support large action spaces with targeted exploration, and cope with unbounded trajectories in the transition system will be of relevance to other structured prediction tasks."
P16-1002,"
6 Discussion
",2016,"additionally, these paraphrasing-based transformations can be described in terms of grammar induction, so they can be incorporated into our framework."
P16-1002,"
6 Discussion
",2016,"another piece of related work is luong et al.(2015b), who train a neural machine translation system to copy rare words, relying on an external system to generate alignments."
P16-1002,"
6 Discussion
",2016,prior work has explored using paraphrasing for data augmentation on nlp tasks.
P16-1002,"
6 Discussion
",2016,"some of our induced grammars generate examples that are not in the test distribution, but nonetheless aid in generalization."
P16-1002,"
6 Discussion
",2016,there has been growing interest in applying neural networks to semantic parsing and related tasks.
P16-1002,"
6 Discussion
",2016,zhang et al.(2015) augment their data by swapping out words for synonyms from wordnet.
P16-1004,"
5 Conclusions
",2016,"beyond semantic parsing, we would also like to apply our seq2tree model to related structured prediction tasks such as constituency parsing."
P16-1004,"
5 Conclusions
",2016,"for example, it would be interesting to learn a model from question-answer pairs without access to target logical forms."
P16-1005,"
8 Conclusions and Future Work
",2016,"besides considering the cross-sentence conflicts, we also want to investigate the within-sentence conflicts caused by the competition of triggers."
P16-1005,"
8 Conclusions and Future Work
",2016,in the future we aim to label slot types based on contextual information as well as sentence structures instead of trigger gazetteers only.
P16-1005,"
8 Conclusions and Future Work
",2016,"we attempt to combine multi-prototype approaches (e.g., (reisinger and mooney, 2010)) to better disambiguate senses of trigger words."
P16-1006,"
9 Conclusions and Future Work
",2016,"furthermore, we will exploit edge labels while walking through a knowledge base to retrieve more relevant entities."
P16-1006,"
9 Conclusions and Future Work
",2016,"in the future we will apply visual pattern recognition and concept detection techniques to perform deep content analysis of the retrieved images, so we can do matching and inference on concept/entity level instead of shallow visual similarity."
P16-1006,"
9 Conclusions and Future Work
",2016,our long-term goal is to extend this framework to other knowledge extraction and population tasks such as event extraction and slot filling to construct multimedia knowledge bases effectively from multiple languages with low cost.
P16-1006,"
9 Conclusions and Future Work
",2016,we will also extend anchor image retrieval from documentlevel into phrase-level or sentence-level to obtain richer background information.
P16-1007,"
9 Conclusion
",2016,"instead, the learning objective, model, and inference procedure should all be tailored to the task."
P16-1007,"
9 Conclusion
",2016,the complementary strengths of both systems suggest future work in combining these techniques.
P16-1008,"
7 Conclusion
",2016,we propose two variants of coverage models: linguistic coverage that leverages more linguistic information and nn-based coverage that resorts to the flexibility of neural network approximation .
P16-1009,"
6 Conclusion
",2016,future work will explore the effectiveness of our approach in more settings.
P16-1009,"
6 Conclusion
",2016,"it is conceivable that larger synthetic data sets, or data sets obtained via data selection, will provide bigger performance benefits."
P16-1010,"
7 Conclusion
",2016,"in experiments, this model further improves our system."
P16-1010,"
7 Conclusion
",2016,"in the future, we will extend this model to allow discontinuity on target sides and explore the possibility of directly encoding reordering information in translation rules."
P16-1010,"
7 Conclusion
",2016,we are also interested in using graphs for neural machine translation to see how it can translate and benefit from graphs.
P16-1011,"
9 Conclusion
",2016,"given a language command, the induced hypothesis space, together with a learned hypothesis selector, can be applied by the agent to plan for lower-level actions."
P16-1011,"
9 Conclusion
",2016,"more importantly, as our approach is based on incremental learning, it can be potentially integrated in a dialogue system to support life-long learning from humans."
P16-1011,"
9 Conclusion
",2016,our future work will extend the current approach with dialogue modeling to learn more reliable hypothesis spaces of resulting states for verb semantics.
P16-1012,"
6 Conclusion
",2016,as future work we aim to make our approach completely knowledgefree by eliminating this dependency.
P16-1012,"
6 Conclusion
",2016,"the remaining key challenge is to better characterize possible substitutes from bad substitutes in ranked lists of distributionally similar words, which frequently contain antonyms and cohyponyms."
P16-1012,"
6 Conclusion
",2016,"we will explore unsupervised acquisition of relational similarity (mikolov et al., 2013b) for this task."
P16-1013,"
7 Conclusion
",2016,"the proposed novel technique for finding an optimal curriculum is general, and can be used with other datasets and models."
P16-1015,"
8 Conclusion
",2016,"finally, we would like to highlight two insights that the experiments provide."
P16-1015,"
8 Conclusion
",2016,we adjust the transition system to work on a stack in a uniform way starting at a node on the stack and ending with the top node of the stack.
P16-1015,"
8 Conclusion
",2016,we think that the transition systems with more active tokens or the combination with edges that span over more words provide very attractive transition systems for possible future parsers.
P16-1016,"
6 Conclusion
",2016,"this can be a useful starting point for several lines of research: implementing more advanced transitionbased techniques (beam search, dynamic oracles, deep learning); extending other classical transition systems like arc-eager and hybrid as well as handling non-projectivity."
P16-1018,"
7 Conclusion
",2016,"finally, it would be interesting to investigate modeling metaphorical mappings as nonlinear mappings within the deep learning framework."
P16-1018,"
7 Conclusion
",2016,"for instance, the lit and met adjective matrices and the cm mapping matrix learned with our methods could be applied to improve automated paraphrasing of an phrases."
P16-1018,"
7 Conclusion
",2016,"it may also be possible to extend the coverage of our system by using automated word-sense disambiguation to bootstrap annotations and therefore construct lit and met matrices in a minimally supervised fashion (kartsaklis et al., 2013b)."
P16-1018,"
7 Conclusion
",2016,our work is also directly extendable to other syntactic constructions.
P16-1019,"
6 Conclusions and Future Work
",2016,"in addition, we also plan to compare our work to the method of sporleder et al.(2010) as well apply our work on the idx corpus (sporleder et al., 2010) and to other languages."
P16-1019,"
6 Conclusions and Future Work
",2016,in future work we plan to investigate the use of sent2vec to encode larger samples of text - not only the sentence containing idioms.
P16-1019,"
6 Conclusions and Future Work
",2016,the focus of these future experiments will be to test how our approach which is relatively less dependent on nlp resources compares with these other methods for idiom token classification.
P16-1019,"
6 Conclusions and Future Work
",2016,we also plan to further analyse the errors made by our “general” model and investigate the “general” approach on the skewed part of the vnc-tokens dataset.
P16-1019,"
6 Conclusions and Future Work
",2016,we also plan to investigate an end-to-end approach based on deep learning-based representations to classify literal and idiomatic language use.
P16-1019,"
6 Conclusions and Future Work
",2016,we followed the intuition that the distributed representations generated by sent2vec also include information regarding the context where the potential idiomatic expression is inserted and therefore is sufficient for distinguishing between idiomatic and literal language use.
P16-1019,"
6 Conclusions and Future Work
",2016,we tested this approach with different machine learning (ml) algorithms (k-nearest neighbours and support vector machines) and compared our work against a topic model representation that include the full paragraph or the surrounding paragraphs where the potential idiom is inserted.
P16-1020,"
8 Conclusion and Future Work
",2016,"in future work, we will apply our method to other kinds of phrases and tasks."
P16-1021,"
6 Conclusion
",2016,"additionally, language expressing emotion and cognition relates to metaphor, but in ways specific to particular candidate words."
P16-1021,"
6 Conclusion
",2016,our proposed features can be expanded to other domains.
P16-1021,"
6 Conclusion
",2016,"though in other domains, the specific topic transition and emotion/cognition patterns would likely be different, these features would still be relevant to metaphor detection."
P16-1023,"
6 Conclusion and future work
",2016,"based on this paper, there are serveral lines of investigation we plan to conduct in the future.(i) we will attempt to support our results on artificially generated corpora by conducting experiments on real natural language data.(ii) we will study the coverage of our four criteria in evaluating word representations.(iii) we modeled the four criteria using separate pcfgs, but they could also be modeled by one single unified pcfg."
P16-1023,"
6 Conclusion and future work
",2016,but the validity of the assumption that embedding spaces can be decomposed into “linear” subspaces should be investigated in the future.
P16-1023,"
6 Conclusion and future work
",2016,one question that arises is then to what extent the four criteria are orthogonal and to what extent interdependent.
P16-1024,"
5 Conclusions and Future Work
",2016,"encouraged by the excellent results, we also plan to test the portability of the approach to more language pairs, and other tasks and applications."
P16-1024,"
5 Conclusions and Future Work
",2016,"in future work, we plan to investigate other methods for seed pairs selection, settings with scarce resources (agic et al., 2015; zhang et al., 2016), ′ other context types inspired by recent work in the monolingual settings (levy and goldberg, 2014a; melamud et al., 2016), as well as model adaptations that can work with multi-word expressions."
P16-1025,"
5 Conclusions and Future Work
",2016,"in the future, we will extend this framework to other information extraction tasks."
P16-1026,"
5 Conclusions
",2016,"in future work, we will investigate scalable and parallel model learning to explore the performance of our model for large-scale real-time event extraction and visualization."
P16-1028,"
7 Conclusion
",2016,"in future work, we plan to apply semlms to other semantic related nlp tasks e.g.machine translation and question answering."
P16-1030,"
6 Conclusion
",2016,"our work also empirically explores different methods of inducing and modelling these connotation frames, incorporating the interplay between relations within frames."
P16-1030,"
6 Conclusion
",2016,"our work suggests new research avenues on learning connotation frames, and their applications to deeper understanding of social and political discourse."
P16-1031,"
5 Conclusions and Future Work
",2016,"first, since deep learning may obtain better generalization on large-scale data sets (bengio, 2009), a straightforward path of the future research is to apply the proposed btdnns for domain adaptation on a much larger industrial-strength data set of 22 domains (glorot et al., 2011)."
P16-1031,"
5 Conclusions and Future Work
",2016,"second, we will try to investigate the use of the proposed approach for other kinds of data set, such as 20 newsgroups and reuters21578 (li et al., 2012; zhuang et al., 2013)."
P16-1031,"
5 Conclusions and Future Work
",2016,"the proposed btdnns attempts to transfer the source domain examples to the target domain, and also transfer the target domain examples to the source domain."
P16-1031,"
5 Conclusions and Future Work
",2016,there are some ways in which this research could be continued.
P16-1032,"
8 Conclusion
",2016,"experiments demonstrated that the approach can infer implied sentiment and point toward potential directions for future work, including joint entity detection and incorporation of more varied types of factual relationships."
P16-1033,"
7 Conclusions
",2016,"finally, one anonymous reviewer comments that we may use automatically projected trees (rasooli and collins, 2015; guo et al., 2015; ma and xia, 2014) as the initial seed labeled data, which is cheap and interesting."
P16-1033,"
7 Conclusions
",2016,"for future work, we would like to advance this study in the following directions."
P16-1033,"
7 Conclusions
",2016,"intuitively, it would be more profitable to annotate instances that are both difficult for the current model and representative in capturing common language phenomena."
P16-1033,"
7 Conclusions
",2016,"our plan is to study which syntactic structures are more suitable for human annotation, and balance informativeness of a candidate task and its suitability for human annotation."
P16-1033,"
7 Conclusions
",2016,"second, we so far assume that the selected tasks are equally difficult and take the same amount of effort for human annotators."
P16-1035,"
9 Conclusion
",2016,"instead of using a “sriracha sauce of deep learning,” as embedding techniques like word2vec have been called, we contend that the situation sometimes requires, say, that we make a b′echamel or a mole verde or a sambal—or otherwise learn to cook."
P16-1035,"
9 Conclusion
",2016,the strength of these results suggests that other research adopting global embedding vectors should consider local embeddings as a potentially superior representation.
P16-1036,"
9 Conclusions

",2016,"also, we would like to experiment with other deep neural architectures such as recurrent neural networks, long short term memory networks, etc.to form the sub-networks."
P16-1036,"
9 Conclusions

",2016,"as part of future work, we would like to enhance scqa with the meta-data information like categories, user votes, ratings, user reputation of the questions and answer pairs."
P16-1038,"
10 Conclusion
",2016,"4 future directions for this work include further improving the number and quality of g2p models, as well as performing external evaluations of the models in speech- and text-processing tasks."
P16-1038,"
10 Conclusion
",2016,we plan to use the presented data and methods for other areas of multilingual natural language processing.
P16-1038,"
10 Conclusion
",2016,we then leverage lang2lang distance metrics and phon2phon phoneme distances to adapt g2p resources for highresource languages for 229 related low-resource languages.
P16-1038,"
10 Conclusion
",2016,"with this publication, we release a number of resources to the nlp community: a large multilingual wiktionary pronunciation dictionary, scraped wikipedia ipa help tables, compiled named entity resources (including a multilingual gazetteer), and our phon2phon and lang2lang distance tables."
P16-1041,"
7 Conclusion
",2016,"working with our model has emphasized to us the following (not necessarily novel) concepts, which we record here to promote further empirical validation."
P16-1043,"
7 Conclusion
",2016,the approach is quite general and we hope that this paper will encourage more nlp researchers to explore curriculum learning in their own works.
P16-1044,"
6 Conclusion
",2016,"potential future work include: 1) evaluating the proposed approaches for different tasks, such as community qa and textual entailment; 2) including the sentential attention mechanism; 3) integrating the hybrid and the attentive mechanisms into a single framework."
P16-1045,"
7 Conclusions
",2016,"we believe it offers interesting challenges that go beyond the scope of this paper – such as question parsing, or textual entailment – and are exciting avenues for future research."
P16-1046,"
7 Conclusions
",2016,"a third direction would be to adopt an information theoretic perspective and devise a purely unsupervised approach that selects summary sentences and words so as to minimize information loss, a task possibly achievable with the dataset created in this work."
P16-1046,"
7 Conclusions
",2016,it would also be interesting to apply the neural models presented here in a phrase-based setting similar to lebret et al.(2015).
P16-1046,"
7 Conclusions
",2016,"one way to improve the word-based model would be to take structural information into account during generation, e.g., by combining it with a tree-based algorithm (cohn and lapata, 2009)."
P16-1047,"
6 Conclusion and Future Work
",2016,can we transfer our model to other languages?
P16-1047,"
6 Conclusion and Future Work
",2016,future work can be directed towards answering two main questions: can we improve the performance of our classifier?
P16-1047,"
6 Conclusion and Future Work
",2016,"most importantly, we are going to test the model using word-embedding features extracted from a bilingual embedding space."
P16-1047,"
6 Conclusion and Future Work
",2016,"to do this, we are going to explore whether adding language-independent structural information (e.g.universal dependency information) can help the performance on exact scope matching."
P16-1048,"
5 Conclusion
",2016,"by inducing concept information, the proposed conceptual sentence embedding maintains and enhances the semantic information of sentence embedding."
P16-1048,"
5 Conclusion
",2016,"furthermore, we extend the proposed models by introducing attention model, which allows it to consider contextual words within the window in a non-uniform way while maintaining the efficiency."
P16-1048,"
5 Conclusion
",2016,"we compare them with different algorithms, including bag-of-word models, topic model-based model and other state-of-the-art sentence embedding models."
P16-1049,"
8 Conclusion
",2016,"we evaluate our method on both question answering and chatbot scenarios, and obtain promising results."
P16-1049,"
8 Conclusion
",2016,we leave better triggering component and multiple rounds of conversation handling to be addressed in our future work.
P16-1051,"
6 Conclusion

",2016,"there is, of course, no reason to think that multi-party dialogue should work differently; we leave the empirical examination as an open task."
P16-1052,"
5 Conclusion
",2016,the model was exemplified within a qa sales assistant with domain-specific world knowledge for conducting sales dialogues.
P16-1053,"
5 Conclusion and Future Work
",2016,"moreover, applying the models to other tasks, such as semantic relatedness measurement and paraphrase identification, would also be interesting attempts."
P16-1053,"
5 Conclusion and Future Work
",2016,"so, we are going to extend sin to tree-based sin for sentence modeling as future work."
P16-1053,"
5 Conclusion and Future Work
",2016,we also introduce a convolution layer into sin (sinconv) to improve its phrase modeling ability so that phrase interactions can be handled.
P16-1056,"
6 Conclusion
",2016,"finally, we use our best performing neural network model to generate a corpus of 30m question and answer pairs, which we hope will enable future researchers to improve their question answering systems."
P16-1058,"
5 Discussion and Conclusion
",2016,"an obvious direction for future work is to automatically induce such a strategy, based on confidence measures that automatically predict the trust-worthiness of a word for an object."
P16-1058,"
5 Discussion and Conclusion
",2016,"another extension that we have planned for future work is to implement relational expressions, similar to (kennington and schlangen, 2015)."
P16-1058,"
5 Discussion and Conclusion
",2016,"based on relational expressions, we will be able to generate reformulations and installments tailored to the interaction with the user."
P16-1058,"
5 Discussion and Conclusion
",2016,"for instance, a very natural option for installments is to relate the wrong target object clicked on by the user to the intended target, e.g.something like to the left of that one, the bigger object."
P16-1058,"
5 Discussion and Conclusion
",2016,"in order to achieves this, we have augmented our approach with some manually designed installment strategies."
P16-1059,"
7 Conclusion
",2016,"however, the simplicity of the star-shaped model, its empirical effectiveness, and ease of learning parameters make it an attractive approach for easily incorporating attention into existing resolution models."
P16-1059,"
7 Conclusion
",2016,"it is of course possible to extend the global single-link model to the multi-focus case, by modifying the model factors and resulting messages."
P16-1059,"
7 Conclusion
",2016,"the model can also readily be applied to other structured prediction problems in language processing, such as selecting antecedents in coreference resolution."
P16-1062,"
7 Conclusions and Future Work
",2016,"a pilot effort to use word embeddings to alter the variety of vocabulary in a dataset has so far not succeeded, but future experiments altering vocabulary width or modularity of a dataset and finding that the modified dataset behaved like natural datasets with the same properties could increase confidence in causality."
P16-1062,"
7 Conclusions and Future Work
",2016,"future work can also explore finer clusters within these datasets, such as clustering clue by word sense of the answers and toon by joke sense."
P16-1062,"
7 Conclusions and Future Work
",2016,future work can manipulate datasets’ text properties to confirm that a specific property is the cause of observed differences in clustering.
P16-1062,"
7 Conclusions and Future Work
",2016,"future work will explore further how the goals of short text authors translate into measurable properties of the texts they write, and how measuring those properties can help predict which similarity metrics and clustering methods will combine to provide the best performance."
P16-1062,"
7 Conclusions and Future Work
",2016,these results are a first step towards determining the best way to cluster a new dataset based on properties of the text.
P16-1062,"
7 Conclusions and Future Work
",2016,this work has shown that creativity can influence the best way to cluster text.
P16-1062,"
7 Conclusions and Future Work
",2016,"unlike most work on clustering short texts, we examined how the similarity metric interacts with the clustering method."
P16-1065,"
7 Conclusions and Future Work
",2016,"a separate dirichlet prior for each block captures its topic preferences, serving as an informed prior when inferring documents’ topic distributions."
P16-1065,"
7 Conclusions and Future Work
",2016,"as next steps, we plan to explore model variations to support a wider range of use cases."
P16-1065,"
7 Conclusions and Future Work
",2016,"in the spirit of treating links probabilistically, we plan to explore application of the model in suggesting links that do not exist but should, for example in discovering missed citations, marking social dynamics (nguyen et al., 2014), and identifying topically related content in multilingual networks of documents (hu et al., 2014)."
P16-1065,"
7 Conclusions and Future Work
",2016,"our model better captures the connections and content of paper abstracts, as measured by predictive link rank and topic quality."
P16-1065,"
7 Conclusions and Future Work
",2016,"we are also interested in modeling changing topics and vocabularies (blei and lafferty, 2006; zhai and boyd-graber, 2013)."
P16-1066,"
7 Conclusion

",2016,"moreover, their high coverage suggests the possibility of using them in different genres such as product reviews."
P16-1066,"
7 Conclusion

",2016,this is a possible future research direction.
P16-1066,"
7 Conclusion

",2016,"we also plan to make the classification multi-way: positive, negative, neutral and mixed."
P16-1069,"
7 Conclusion
",2016,"we also discussed various ways to improve generalization and reduce overfitting, including adding synthetic training data by paraphrasing sentences, using multiple grammars, applying feature selection and ensembling multiple systems."
P16-1071,"
8 Conclusion
",2016,"while our approach may not be readily applicable for developing nlp models today, we believe that the presented results may inspire nlp researchers to consider learning models from combinations of linguistic resources and auxiliary, behavioral data that reflects human cognition."
P16-1073,"
6 Conclusions
",2016,"furthermore, we also want to employ sentence rewriting techniques for other challenges in semantic parsing, such as the spontaneous, unedited natural language input, etc."
P16-1073,"
6 Conclusions
",2016,"in future work, we will explore more advanced sentence rewriting methods."
P16-1075,"
9 Discussion and Conclusion
",2016,"another avenue of potential research is to use multi-task learning to predict scores for different aspects of text quality (e.g.coherence, grammaticality, topicality)."
P16-1075,"
9 Discussion and Conclusion
",2016,"future work will aim to study different dimensions of the prompt (e.g.genre, topic) using multitask learning at a finer level."
P16-1075,"
9 Discussion and Conclusion
",2016,"therefore, although some extra information (i.e.the expected distribution of gold scores) would need to be used to produce accurate scores with a high quality ranker, the ranking is still useful for assessment in a number of scenarios (e.g.grading on a curve where the distribution of student scores is predefined)."
P16-1075,"
9 Discussion and Conclusion
",2016,we also aim to further study the characteristics of the multi-task model in order to determine which features transfer well across tasks.
P16-1076,"
7 Conclusion
",2016,future work could be extending the proposed method to handle more complex questions.
P16-1077,"
6 Conclusion
",2016,"our thorough evaluation and analysis yields detailed insights into the semantic characteristics of the inferred classes, and we hope that this allows an informed use of the resulting resource in various semantic nlp tasks."
P16-1078,"
7 Conclusion
",2016,"moreover, the attention mechanism allows the tree-based encoder to align not only the input words but also input phrases with the output words."
P16-1080,"
8 Conclusions
",2016,"another avenue of future research can look at the annotators’ own traits and how these relate to perception (flekova et al., 2015)."
P16-1080,"
8 Conclusions
",2016,"follow-up studies can analyze the perception of other user traits such as education level, race or political orientation."
P16-1081,"
5 Conclusions
",2016,"in addition, this method can also be applied to domain adaptation, where a domain taxonomy enables a hierarchically shared model adaptation."
P16-1081,"
5 Conclusions
",2016,the idea of shared model adaptation is general and can be further extended.
P16-1082,"
7 Conclusions
",2016,"the most important link in the graph is the concept dependency relation, which indicates that one concept helps a learner to understand another, e.g., markov logic networks depends on probability."
P16-1082,"
7 Conclusions
",2016,we are releasing human annotations of concept nodes and possible dependency edges learned from the acl anthology as well as implementations of the methods described in this paper to enable future research on modeling scientific corpora.
P16-1083,"
7 Conclusion
",2016,"in future work, we plan to improve performace of feature weight tuning and investigate more general features."
P16-1085,"
7 Conclusions
",2016,"as future work, we plan to investigate the possibility of designing word representations that best suit the wsd framework."
P16-1085,"
7 Conclusions
",2016,in this paper we studied different ways of integrating the semantic knowledge of word embeddings in the framework of wsd.
P16-1085,"
7 Conclusions
",2016,our hope is that this work will serve as the first step for further studies on re-designing standard wsd features.
P16-1085,"
7 Conclusions
",2016,we release at https:// github.com/iiacobac/ims_wsd_emb all the codes and resources used in our experiments in order to provide a framework for research on the evaluation of new vsm models in the wsd framework.
P16-1087,"
8 Conclusion
",2016,"in future work, we plan to explore the effects of pre-training (bengio et al., 2009) and scheduled sampling (bengio et al., 2015) for training our lstm network."
P16-1087,"
8 Conclusion
",2016,we would also like to explore re-ranking methods for our problem.
P16-1087,"
8 Conclusion
",2016,"with respect to the fine-grained opinion mining task, a potential future direction to be able to model overlapping and embedded entities and relations and also to extend this model to handle cross-sentential relations."
P16-1088,"
7 Conclusion

",2016,"in future work, we will develop lexical features which are captured by nonlocal dependencies."
P16-1089,"
6 Conclusion
",2016,"however, it would be interesting to see how siamese cbow embeddings would affect results in supervised tasks."
P16-1089,"
6 Conclusion
",2016,it is beyond the scope of this paper to provide a comprehensive analysis of all supervised methods using word or sentence embeddings and the effect siamese cbow would have on them.
P16-1089,"
6 Conclusion
",2016,"it would be interesting to see how embeddings for larger pieces of texts, such as documents, would perform in document clustering or filtering tasks."
P16-1089,"
6 Conclusion
",2016,word and sentence embeddings are ubiquitous and many different ways of using them in supervised tasks have been proposed.
P16-1090,"
6 Discussion and related work
",2016,"a natural next step is to explore our framework with additional modeling improvements—especially in dealing with context, structure, and noise."
P16-1090,"
6 Discussion and related work
",2016,"another avenue for providing user confidence is probabilistic calibration (platt, 1999), which has been explored more recently for structured prediction (kuleshov and liang, 2015)."
P16-1090,"
6 Discussion and related work
",2016,"the idea of computing consistent hypotheses appears in the classic theory of version spaces for binary classification (mitchell, 1977) and has been extended to more structured settings (vanlehn and ball, 1987; lau et al., 2000)."
P16-1090,"
6 Discussion and related work
",2016,"they use this notion for active learning, whereas we use it to support unanimous prediction."
P16-1090,"
6 Discussion and related work
",2016,"this work, however, focuses on the classification setting, whereas we considered more structured output settings (e.g., for semantic parsing)."
P16-1091,"
5 Conclusions

",2016,"convolutional neural networks were proposed to capture the semantic aspects of utterances given at each moment, while recurrent neural networks were intended to incorporate temporal aspects in dialogue histories into tracking models."
P16-1091,"
5 Conclusions

",2016,"firstly, the architectures based on a single convolutional layer and a single bi-directional recurrent layer in the proposed models can be extended by adding more layers as well as utilizing more advanced components including hierarchical cnns (kalchbrenner et al., 2014b) to deal with utterance compositionalities or attention mechanisms (denil et al., 2012) to focus on more important segments in dialogue sequences."
P16-1091,"
5 Conclusions

",2016,"furthering this work, there would be still much room for improvement in future."
P16-1091,"
5 Conclusions

",2016,"if we develop a good way of leveraging other useful resources into the neural network architectures, better performance can be expected especially for guide-driven and inter-categorical topic transitions that are considered to be more dependent on background knowledge of the speakers."
P16-1091,"
5 Conclusions

",2016,"secondly, the use of external knowledge could be a key to success in dialogue topic tracking, as proved in the previous studies."
P16-1091,"
5 Conclusions

",2016,the other direction of our future work is to investigate joint models for tracking dialogue topics and states simultaneously.
P16-1092,"
8 Conclusion
",2016,"we also investigated whether the semantic models learned from particular l1s are portable to other languages, and in particular to languages that are typologically close to the investigated l1s."
P16-1093,"
6 Conclusions
",2016,"further, we evaluated the extent to which mismatched native language (l1) affects distractor plausibility."
P16-1093,"
6 Conclusions
",2016,"in future work, we plan to conduct larger-scale evaluations to further validate these results, and to apply these methods on other common learner errors."
P16-1094,"
7 Conclusions
",2016,"there are many other dimensions of speaker behavior, such as mood and emotion, that are beyond the scope of the current paper and must be left to future work."
P16-1095,"
7 Conclusion
",2016,experiments on different real-world datasets represent adequate stability of ddrw.
P16-1095,"
7 Conclusion
",2016,the future work has two main directions.one is semi-supervised learning.
P16-1095,"
7 Conclusion
",2016,the other direction is to promote the random walk step.
P16-1097,"
5 Conclusion
",2016,"in the future, we plan to apply our approach to real-world non-parallel corpora to further verify its effectiveness."
P16-1097,"
5 Conclusion
",2016,"it is also interesting to extend the phrase translation model to more sophisticated models such as ibm models 2-5 (brown et al., 1993) and hmm (vogel and ney, 1996)."
P16-1098,"
8 Conclusion and Future Work
",2016,"in future work, we would like to investigate our model on more text matching tasks."
P16-1099,"
6 Conclusion
",2016,"additionally we recognize that the hashtag inventory used to discover business accounts from job-related topics might need to change over time, to achieve robust performance in the future."
P16-1099,"
6 Conclusion
",2016,"besides providing insights for discourse and its links to social science, our study could lead to practical applications, such as: aiding policy-makers with macro-level insights on job markets, connecting job-support resources to those in need, and facilitating the development of job recommendation systems.this work has limitations."
P16-1099,"
6 Conclusion
",2016,this is left for future work.
P16-1099,"
6 Conclusion
",2016,we did not study whether providing contextual information in our humans-in-the-loop framework would influence the model performance.
P16-1100,"
7 Conclusion
",2016,"for future work, we hope to be able to improve the memory usage and speed of purely character-based models."
P16-1101,"
6 Conclusion
",2016,another interesting direction is to apply our model to data from other domains such as social media (twitter and weibo).
P16-1101,"
6 Conclusion
",2016,"first, our model can be further improved by exploring multi-task learning approaches to combine more useful and correlated information."
P16-1101,"
6 Conclusion
",2016,"for example, we can jointly train a neural network model with both the pos and ner tags to improve the intermediate representations learned in our network."
P16-1101,"
6 Conclusion
",2016,"it is a truly end-to-end model relying on no task-specific resources, feature engineering or data pre-processing."
P16-1101,6 Conclusion,2016,there are several potential directions for future work.
P16-1102,"
6 Conclusion and Future Work
",2016,further exploration of different topic vector representations and their combinations is necessary in future work.
P16-1103,"
7 Conclusion
",2016,"therefore, one important extension of our work is to further study the interaction between our model and the underlying language model."
P16-1104,"
7 Conclusion
",2016,"our general approach may be useful in other nlp sub-areas like sentiment and emotion analysis, text summarization and question answering, where considering textual clues alone does not prove to be sufficient."
P16-1104,"
7 Conclusion
",2016,using cognitive features in an nlp processing system like ours is the first proposal of its kind.
P16-1104,"
7 Conclusion
",2016,we augmented traditional linguistic features with cognitive features obtained from readers’ eye-movement data in the form of simple gaze-based features and complex features derived from a graph structure.
P16-1104,"
7 Conclusion
",2016,we propose to augment this work in future by exploring deeper graph and gaze features.
P16-1106,"
7 Conclusions and outlook

",2016,"in our case, the key concepts are excursions and truncation thereof, and the identification of top and bottom regions."
P16-1106,"
7 Conclusions and outlook

",2016,"in the case of salvati (2015), the key concept is that of the complex exponential function, which seems to restrict the proof technique to two-dimensional geometry."
P16-1107,"
7 Conclusions and Future Work
",2016,"also, we will apply our contextaware argumentative relation mining to different argument mining corpora to further evaluate its generality."
P16-1107,"
7 Conclusions and Future Work
",2016,"in addition, predicting an argumentative relation will benefit most from combining these two set of features as they capture complementary aspects of context to better characterize the argumentation in justification."
P16-1107,"
7 Conclusions and Future Work
",2016,our next step will investigate uses of topic segmentation to identify context sentences and compare this linguistically-motivated approach to our current window-size heuristic.
P16-1107,"
7 Conclusions and Future Work
",2016,the results obtained in this preliminary study are promising and encourage us to explore more directions to enable contextual features.
P16-1107,"
7 Conclusions and Future Work
",2016,"we have explored different ways to incorporate our proposed features with baseline features used in a prior study, and obtained insightful results about feature effectiveness."
P16-1107,"
7 Conclusions and Future Work
",2016,we plan to follow prior research on graph optimization to refine the argumentation structure and improve argumentative relation prediction.
P16-1108,"
7 Conclusion

",2016,"in the future, we plan to expand our method to predict morphological analyses, as well as to incorporate other information such as parts-of-speech."
P16-1110,"
8 Conclusion and Future Work
",2016,"finally, with slight changes to what the system considers a document, we believe alto can be extended to nlp applications other than classification, such as named entity recognition or semantic role labeling, to reduce the annotation effort."
P16-1110,"
8 Conclusion and Future Work
",2016,we can further improve alto to help users gain better and faster understanding of text corpora.
P16-1111,"
8 Conclusion
",2016,access to longer time-spans—along with varying data sources such as grants and patents—would also allow us to more completely model the trajectory of a topic as it moves from being active area of research to potentially impacting commercial industries and economic development.
P16-1111,"
8 Conclusion
",2016,"nonetheless, we hope this work offers another step towards using computational tools to better understand the ‘rhetorical structure of science’ (latour, 1987)."
P16-1112,"
9 Conclusion
",2016,"as part of future work, it would be beneficial to investigate the effect of automatically generated training data for error detection (e.g., rozovskaya and roth (2010))."
P16-1112,"
9 Conclusion
",2016,"finally, we performed an extrinsic evaluation by incorporating probabilities from the error detection system as features in an essay scoring model."
P16-1113,"
7 Conclusions
",2016,"beyond srl, we expect dependency path embeddings to be useful in related tasks and downstream applications."
P16-1114,"
7 Conclusion
",2016,"also, we examined how the length of the observation period affects prediction performance, and investigated the trade-off between prediction accuracy and instancy."
P16-1114,"
7 Conclusion
",2016,the future work includes using those prediction models in a real service to take targeted actions to users who are likely to stop using intelligent assistants.
P16-1115,"
7 Conclusions
",2016,all this is future work.
P16-1115,"
7 Conclusions
",2016,"in its current state— besides, we believe, strongly motivating this future work—, we hope that the model can also serve as a strong baseline to other future approaches to reference resolution, as it is conceptually simple and easy to implement."
P16-1115,"
7 Conclusions
",2016,"kennington and schlangen (2015) do this for spatial relations in their simpler domain; for our domain, new and more richly annotated data such as visualgenome looks promising for learning a wide variety of relations.9 the use of denotations / extensions might make possible transfer of methods from extensional semantics, e.g.for the addition of operators such as negation or generalised quantifiers."
P16-1115,"
7 Conclusions
",2016,"the design of the model, as mentioned in the introduction, makes it amenable for use in interactive systems that learn; we are currently exploring this avenue."
P16-1116,"
7 Conclusion
",2016,"future work may be done to integrate our method into a joint approach, use some global feature, which may improve our performance."
P16-1116,"
7 Conclusion
",2016,"however, we only use sentence-level features and our method is a pipelined approach."
P16-1116,"
7 Conclusion
",2016,then we propose a regularization method to make full use of the argument relationship.
P16-1117,"
6 Conclusion
",2016,"in the future, we plan to extend our model to incorporate coreference resolution and intersentential zero anaphora resolution."
P16-1117,"
6 Conclusion
",2016,"we learned selectional preferences from a large raw corpus, and incorporated them into a pas analysis model, which considers the consistency of all pass in a given sentence."
P16-1119,"
7 Conclusions and Future Work
",2016,"future work can use our annotated corpus to develop classifiers that deal better with prepositional and adjectival modifiers, which require deeper semantic analysis."
P16-1120,"
8 Conclusions
",2016,"as future work, we would like to verify the effectiveness of the proposed models for other datasets or other cross-lingual tasks, such as cross-lingual document classification (ni et al., 2009; platt et al., 2010; ni et al., 2011; smet et al., 2011) and cross-lingual information retrieval (vulic et al., ′ 2013)."
P16-1122,"
7 Conclusion and Future Work
",2016,in the future we plan to apply our inner-attention intuition to other neural networks such as cnn or multi-layer perceptron.
P16-1122,"
7 Conclusion and Future Work
",2016,our models can be further extended to other nlp tasks such as recognizing textual entailments where attention mechanism is important for sentence representation.
P16-1123,"
5 Conclusion
",2016,"we expect this sort of architecture to be of interest also beyond the specific task of relation classification, which we intend to explore in future work."
P16-1124,"
6 Conclusion
",2016,it will be interesting to study whether one can merge the clustering step and the coupling step so as to have a richer inter-task dependent structure.
P16-1124,"
6 Conclusion
",2016,"the key idea of cpra is to (i) automatically discover relations highly correlated to each other through agglomerative clustering, and (ii) effectively couple the prediction of such relations through multi-task learning."
P16-1124,"
6 Conclusion
",2016,we will investigate such topics in our future work.
P16-1124,"
6 Conclusion
",2016,"we would like to design new mechanisms to discover loosely correlated relations, and investigate whether coupling such relations still provides benefits."
P16-1125,"
7 Conclusion
",2016,"lastly, it is important to evaluate the impact of the proposed largercontext models in downstream tasks such as machine translation and speech recognition."
P16-1125,"
7 Conclusion
",2016,"our part-of-speech analysis revealed that content words, including nouns, adjectives and verbs, benefit most from an increasing number of context sentences."
P16-1125,"
7 Conclusion
",2016,"second, more analysis, beyond the one based on part-of-speech tags, should be conducted in order to better understand the advantage of such larger-context models."
P16-1125,"
7 Conclusion
",2016,"to explore the potential of such a model, there are several aspects in which more research needs to be done."
P16-1127,"
6 Conclusion and Future Work
",2016,"but there are other possible choices that might make the encoding even more easily learnable by the lstm, and we would like to explore those in future work."
P16-1127,"
6 Conclusion and Future Work
",2016,"in order to improve performance, other promising directions would involve adding re-reranking techniques and extending our neural networks with attention models in the spirit of (bahdanau et al., 2015)."
P16-1129,"
8 Conclusion and Future Work

",2016,"although more data can be easily collected in this case, the noisiness of audio transcripts may bring some additional challenges, therefore worthwhile for further study."
P16-1129,"
8 Conclusion and Future Work

",2016,another important direction is to focus on the construction of datasets in larger scale.
P16-1129,"
8 Conclusion and Future Work

",2016,one feasible approach is to use a speech recognition system on live videos or broadcasts of sports games to collect huge amount of transcripts as our raw data source.
P16-1129,"
8 Conclusion and Future Work

",2016,we would like to extend our system to produce sports news beyond pure sentence extraction.
P16-1130,"
6 Conclusion
",2016,"for future work, we will explore more sophisticated features for the csrs model, such as syntactic dependency relationships and head words, since only simple lexical features are used in the current incarnation."
P16-1133,"
5 Conclusion and Future Work
",2016,"in addition, we will also explore the possibility of using more complex neural network models such as convolutional neural network and recurrent neural network to build bilingual document representation system."
P16-1133,"
5 Conclusion and Future Work
",2016,our future work will focus on extending the bilingual document representation model into the multilingual scenario.
P16-1133,"
5 Conclusion and Future Work
",2016,we will try to learn a single embedding space for a source language and multiple target languages simultaneously.
P16-1134,"
6 Conclusions
",2016,"in future work, we are interested in exploring better ways of utilizing vast unlabelled data to improve grsemi-crfs, e.g., to learn phrase embeddings from unlabelled data or designing a semisupervised version of grsemi-crfs."
P16-1135,"
7 Conclusion
",2016,"although we have focused exclusively on wikipedia, these methods could be adapted to other domains and languages."
P16-1135,"
7 Conclusion
",2016,"causality is not easily expressed in english using a fixed set of phrases, so we would expect these methods to apply to formal and informal text ranging from news and journals to social media."
P16-1135,"
7 Conclusion
",2016,"in the future, we may improve this step using a machine learning approach."
P16-1135,"
7 Conclusion
",2016,"linguistic expressions of causality in other languages is another avenue for future research, and it would be interesting to note if other languages have the same variety of expression."
P16-1135,"
7 Conclusion
",2016,"thus, we did not evaluate some intermediate steps, such as the quality of the automatically annotated corpus."
P16-1135,"
7 Conclusion
",2016,to evaluate on the intermediate step would have required an additional annotation process.
P16-1136,"
5 Conclusions
",2016,"in the future, we would like to study the impact of relation paths for additional basic kb embedding models and knowledge domains."
P16-1137,"
8 Conclusion
",2016,"in future work, we will explore how to use our model to improve downstream nlp tasks, and consider applying our methods to other knowledge bases."
P16-1139,"
5 Conclusions and future work

",2016,"for a more ambitious goal, we expect that it should be possible to implement a variant of spinn on top of a modified stack data structure with differentiable push and pop operations (as in grefenstette et al., 2015; joulin and mikolov, 2015)."
P16-1139,"
5 Conclusions and future work

",2016,"it is plausible that giving the tracking lstm access to more information from the buffer and stack at each step would allow it to better represent the context at each tree node, yielding both better parsing and better sentence encoding."
P16-1139,"
5 Conclusions and future work

",2016,one promising way to pursue this goal would be to encode the full contents of the stack and buffer at each time step following the method used by dyer et al.(2015).
P16-1139,"
5 Conclusions and future work

",2016,"this would make it possible for the model to learn to parse using guidance from the semantic representation objective, which currently is blocked from influencing the key parsing parameters by our use of hard shift/reduce decisions."
P16-1140,"
7 Conclusion
",2016,it would also be a promising direction to incorporate the factor of language typological diversity when designing advanced word representation model for languages other than english.
P16-1141,"
5 Discussion
",2016,"future studies of semantic change must account for frequency’s conforming effect: when examining the interaction between some linguistic process and semantic change, the law of conformity should serve as a null model in which the interaction is driven primarily by underlying frequency effects."
P16-1141,"
5 Discussion
",2016,"our work builds upon a wealth of previous research on quantitative approaches to semantic change, including prior work with distributional methods (sagi et al., 2011; wijaya and yeniterzi, 2011; gulordava and baroni, 2011; jatowt and duh, 2014; kulkarni et al., 2014; xu and kemp, 2015), as well as recent work on detecting the emergence of novel word senses (lau et al., 2012; mitra et al., 2014; cook et al., 2014; mitra et al., 2015; frermann and lapata, 2016)."
P16-1141,"
5 Discussion
",2016,"overall, these two factors—frequency and polysemy—explain between 48% and 88% of the variance10 in rates of semantic change (across conditions)."
P16-1141,"
5 Discussion
",2016,the two statistical laws we propose have strong implications for future work in historical semantics.
P16-1141,"
5 Discussion
",2016,we extend these lines of work by rigorously comparing different approaches to quantifying semantic change and by using these methods to propose new statistical laws of semantic change.
P16-1141,"
5 Discussion
",2016,we show how distributional methods can reveal statistical laws of semantic change and offer a robust methodology for future work in this area.
P16-1143,"
8 Discussion and Future Work
",2016,another obvious extension would be to further explore the alignment component of hca-wsi.
P16-1143,"
8 Discussion and Future Work
",2016,"in conclusion, we have created extensive resources for future work in nlp and related disciplines."
P16-1143,"
8 Discussion and Future Work
",2016,"in particular, we intend to expand lexsemtm by applying hca-wsi across the vocabularies of languages other than english, and also to multiword lemmas."
P16-1143,"
8 Discussion and Future Work
",2016,the most immediate extension of our work would be to apply our sense learning method to a broader range of data.
P16-1143,"
8 Discussion and Future Work
",2016,"this could be used to expand existing sense inventories with new senses, for example using the methodology of cook et al.(2013)."
P16-1144,"
5 Conclusion
",2016,"the test set will be made available at the time of the competition.stored in memory, in order to retrieve the right information from it."
P16-1144,"
5 Conclusion
",2016,"to promote research in this direction, we plan to announce a public competition based on the lambada data.5 our own hunch is that, despite the initially disappointing results of the “vanilla” memory network we tested, the ability to store information in a longer-term memory will be a crucial component of successful models, coupled with the ability to perform some kind of reasoning about what’s 5the development set of lambada, along with the training corpus, can be downloaded at http://clic.cimec.unitn.it/lambada/."
P16-1144,"
5 Conclusion
",2016,we hope the computational community will be stimulated to develop novel language models that are genuinely capturing the non-local phenomena that lambada reflects.
P16-1145,"
6 Conclusion
",2016,"in light of this finding, we suggest some focus areas for future research."
P16-1145,"
6 Conclusion
",2016,"our character-level model improved substantially after language model pretraining, suggesting that further training optimizations may yield continued gains."
P16-1145,"
6 Conclusion
",2016,we have demonstrated the complexity of the wikireading task and its suitability as a benchmark to guide future development of dnn models for natural language understanding.
P16-1147,"
6 Conclusions
",2016,"in future work, we will extend this idea beyond sequence modeling to improve models in nlp that utilize parse trees as features."
P16-1147,"
6 Conclusions
",2016,this suggests a new methodology to building deep neural models for nlp: we can design them from the ground up to incorporate multiple sources of annotation and learn far more effective intermediate representations.
P16-1147,"
6 Conclusions
",2016,we observe that the ideas presented in this work can also be as a principled way to optimize upstream nlp components for down-stream applications.
P16-1148,"
7 Summary
",2016,"moreover, our models for predicting user demographics can be effectively used for a variety of downstream nlp tasks e.g., text classification (hovy, 2015), sentiment analysis (volkova et al., 2013), paraphrasing (preotiucpietro et al., 2016), part-of-speech tagging (hovy and s?gaard, 2015; johannsen et al., 2015) and visual analytics (dou et al., 2015)."
P16-1148,"
7 Summary
",2016,our observations can effectively improve personalized intelligent user interfaces in a way that reflects and adapts to user-specific characteristics and emotions.
P16-1150,"
5 Conclusion and future work
",2016,"we believe that the presence of 44k reasons (550k tokens) is another important asset of the newly created corpus, which deserves future investigation."
P16-1151,"
4 Conclusion
",2016,our methodology has a wide variety of applications.
P16-1151,"
4 Conclusion
",2016,"the methodology is also useful for observational data—for studying the effects of complicated treatments, such as how a legislator’s roll call voting record affects their electoral support."
P16-1151,"
4 Conclusion
",2016,"this includes numerous alternative experimental designs, providing a methodology that computational social scientists could use widely to discover and then confirm the effects of messages in numerous domains—including images and other high dimensional data."
P16-1152,"
5 Conclusion

",2016,"however, the result can be explained by considering important empirical factors such as the variance of stochastic updates."
P16-1153,"
5 Conclusion
",2016,"future work includes: (i) adding an attention model to robustly analyze which part of state/actions text correspond to strategic planning, and (ii) applying the proposed methods to more complex text games or other tasks with actions defined through natural language."
P16-1154,"
7 Conclusion and Future Work
",2016,"for future work, we will extend this idea to the task where the source and target are in heterogeneous types, for example, machine translation."
P16-1155,"
6 Conclusions
",2016,the efficacy of the proposed algorithm for cross-domain classification across disparate label sets will expand the horizon for ml-based algorithms to be more widely applicable in more general and practically observed scenarios.
P16-1156,"
9 Conclusion and Future Work
",2016,future work will consider the role of derivational morphology in embeddings as well as noncompositional cases of inflectional morphology.
P16-1157,"
7 Conclusion

",2016,"we presented the first systematic comparative evaluation of cross-lingual embedding methods on several downstream nlp tasks, both intrinsic and extrinsic."
P16-1159,"
6 Conclusion
",2016,"as our approach is transparent to loss functions and architectures, we believe that it will also benefit more end-to-end neural architectures for other nlp tasks."
P16-1159,"
6 Conclusion
",2016,"in the future, we plan to test our approach on more language pairs and more end-to-end neural mt systems."
P16-1159,"
6 Conclusion
",2016,it is also interesting to extend minimum risk training to minimum risk annealing following smith and eisner (2006).
P16-1160,"
8 Conclusion
",2016,"however, this has allowed us a more fine-grained analysis, but in the future, a setting where the source side is also represented as a character sequence must be investigated."
P16-1162,"
6 Conclusion
",2016,"one avenue of future research is to learn the optimal vocabulary size for a translation task, which we expect to depend on the language pair and amount of training data, automatically."
P16-1162,"
6 Conclusion
",2016,"we also believe there is further potential in bilingually informed segmentation algorithms to create more alignable subword units, although the segmentation algorithm cannot rely on the target text at runtime."
P16-1164,"
8 Conclusion
",2016,"on a more general level, we believe that quotation detection is interesting as a representative of tasks involving long sequences, where markov assumptions become inappropriate."
P16-1165,"
6 Conclusions and Future Work
",2016,"in the future, we would like to combine crfs with lstms for doing the two steps jointly, so that the lstms can learn the embeddings using the global thread-level feedback."
P16-1165,"
6 Conclusions and Future Work
",2016,"we would also like to apply our models to conversations, where the graph structure is extractable using the meta data or other clues, e.g., the fragment quotation graphs for email threads (carenini et al., 2008)."
P16-1166,"
8 Conclusion
",2016,"a next major step in our research agenda is to integrate se type information into various applications, including argument mining, temporal reasoning, and summarization."
P16-1166,"
8 Conclusion
",2016,"among others, we plan to create subtypes of the state label, which currently subsumes clauses stativized by negation, modals, lexical information or other aspectual operators."
P16-1166,"
8 Conclusion
",2016,"here we focus on the automatic identification of se types, leaving the identification of discourse modes to future work."
P16-1166,"
8 Conclusion
",2016,"our annotation scheme and guidelines for se types (friedrich and palmer, 2014b) follow established traditions in linguistics and semantic theory."
P16-1166,"
8 Conclusion
",2016,"our publicly available system can readily be applied to any written english text, making it easy to explore the utility of se types for other nlp tasks.discussion."
P16-1166,"
8 Conclusion
",2016,"the present work, using the se type inventory introduced by smith (2003), is also the basis for research on more fine-grained aspectual type inventories."
P16-1167,"
8 Conclusion
",2016,"finally, we provide a dataset depicting 12 scenarios with ～1.5 m images for future research."
P16-1167,"
8 Conclusion
",2016,future directions could include exploring nuances in the type of temporal knowledge that can be learned across different scenarios.
P16-1167,"
8 Conclusion
",2016,"our event induction method incorporates learned knowledge about events, partitions photo albums into segments, and assigns events to those segments."
P16-1168,"
7 Conclusion
",2016,"we have created an image caption dataset for the japanese language by collecting 131,740 captions for 26,500 images using the yahoo!crowdsourcing service in japan."
P16-1168,"
7 Conclusion
",2016,we hope that our dataset and proposed method kick start studies on cross-lingual image caption generation and that many others follow our lead.
P16-1171,"
7 Conclusion
",2016,"in the future, we plan to build a resource for modeling physical causality for action verbs."
P16-1172,"
Future Work
",2016,"for example, we plan to incorporate lexicalsemantic information in the feature representation and leverage large-scale unsupervised pretraining."
P16-1172,"
Future Work
",2016,the promising results we obtained for summarization with a basic learner (see section 4.3) encourage future work on plugging in more sophisticated supervised learners in our framework.
P16-1174,"
6 Conclusion
",2016,"finally, while we conducted a cursory analysis of model weights in §4.2, an interesting next step would be to study such weights for even deeper insight.(note that using lexeme tag component features, as suggested above, should make this anaysis more robust since features would be less sparse.)"
P16-1174,"
6 Conclusion
",2016,"for example, one could see whether the ranking of vocabulary and/or grammar components by feature weight is correlated with external standards such as the cefr (council of europe, 2001)."
P16-1174,"
6 Conclusion
",2016,"hlr combines a psycholinguistic model of human memory with modern machine learning techniques, and generalizes two popular algorithms used in language learning technology: leitner and pimsleur."
P16-1174,"
6 Conclusion
",2016,"instead of the sparse indicator variables used here, it may be better to decompose lexeme tags into denser and more generic features of tag components9 (e.g., part of speech, tense, gender, case), and also use corpus frequency, word length, etc."
P16-1175,"
7 Conclusion
",2016,this opens the door to a number of future directions with applications to language acquisition using personalized content and learners’ knowledge.
P16-1175,"
7 Conclusion
",2016,"we also leave as future work the integration of this model into an adaptive system that tracks learner understanding and creates scaffolded content that falls in their zone of proximal development, keeping them engaged while stretching their understanding."
P16-1175,"
7 Conclusion
",2016,we plan a deeper investigation into how learners detect and combine cues for incidental comprehension.
P16-1176,"
7 Conclusion
",2016,we also plan to investigate how the results vary when limited to specific l1s.
P16-1176,"
7 Conclusion
",2016,"we are interested in expanding the preliminary results of this work: we intend to replicate the experiments with more languages and more domains, investigate additional varieties of constrained language and employ more complex lexical, syntactic and discourse features."
P16-1178,"
7 Conclusions
",2016,"as future work, we plan to investigate other linguistic representations that can improve the automated extraction of the proposed aspects to better predict the article’s perceived quality."
P16-1178,"
7 Conclusions
",2016,"to this end, we performed an editorial study with expert judges either in computational linguistics, journalism, or media monitoring experts."
P16-1179,"
9 Conclusions and Future Work
",2016,13 the analysis of the acquired information and the error analysis show several avenues for future work.
P16-1180,"
9 Conclusion and Future Work
",2016,another task for future work is semantic alignment.
P16-1180,"
9 Conclusion and Future Work
",2016,"in this paper we focused on a traditional distributional approach that has the advantage of being explainable, but it would be interesting and useful to explore other options such as word embeddings, matrix factorization and semantic similarity metrics."
P16-1180,"
9 Conclusion and Future Work
",2016,"our approach discovers paraphrasal templates without aligning them to a semantic meaning representation; while these are perfectly usable by summarization, question answering, and other text-to-text generation applications, it would be useful for concept-to-text generation and other applications to have each cluster of templates aligned to a semantic representation of the meaning expressed."
P16-1180,"
9 Conclusion and Future Work
",2016,we leave these to future work.
P16-1183,"
6 Conclusion
",2016,an alternative method of integration would be to move the decoder itself to gpu.
P16-1183,"
6 Conclusion
",2016,"because it uses batch processing, our on-chip language model could be integrated into a machine translation decoder using strategies similar to those used to integrate an on-network language model nearly a decade ago (brants et al., 2007)."
P16-1183,"
6 Conclusion
",2016,"our language model is implemented on a gpu, but its general design (and much of the actual code) is likely to be useful to other hardware that supports simd parallelism, such as the xeon phi."
P16-1183,"
6 Conclusion
",2016,we intend to explore these possibilities in future work.
P16-1184,"
7 Conclusion
",2016,our results provide a strong baseline for future work in weakly supervised morphological tagging.
P16-1184,"
7 Conclusion
",2016,we showed that downstream tasks such as dependency parsing can be improved by using the predictions from the tagger as features.
P16-1185,"
5 Conclusion

",2016,"another interesting direction is to enhance the connection between source-to-target and target-tosource models (e.g., letting the two models share the same word embeddings) to help them benefit more from interacting with each other."
P16-1185,"
5 Conclusion

",2016,"as our method is sensitive to the oovs present in monolingual corpora, we plan to integrate jean et al.(2015)’s technique on using very large vocabulary into our approach."
P16-1185,"
5 Conclusion

",2016,it is also necessary to further validate the effectiveness of our approach on more language pairs and nmt architectures.
P16-1186,"
6 Conclusions
",2016,"an interesting future direction is to combine complementary approaches, either through combined parameterization (e.g.hierarchical softmax with differentiated capacity per word) or through a curriculum (e.g.transitioning from target sampling to regular softmax as training progresses)."
P16-1186,"
6 Conclusions
",2016,further promising areas are parallel training as well as better rare word modeling.
P16-1186,"
6 Conclusions
",2016,we conclude that there is a lot to explore in training from a combination of normalized and unnormalized objectives.
P16-1187,"
5 Conclusions
",2016,"as future work, we plan to examine the use of a voting scheme for combining the output of complementary dsms."
P16-1187,"
5 Conclusions
",2016,"moreover, we also plan to combine additional sources of information for building the models, such as multilingual resources or translation data, to improve even further the compositionality prediction."
P16-1187,"
5 Conclusions
",2016,we would also like to propose and evaluate more sophisticated compositionality functions that take into account the unbalanced contribution of individual words to the global meaning of a compound.
P16-1189,"
6 Conclusions
",2016,"it thus allows for the fast deployment of a crucial component in comparable corpora alignment, which opens the path for an increase in the amount of such corpora that can be exploited in the future."
P16-1189,"
6 Conclusions
",2016,stacc is a highly portable method which requires no adaptation for its application to new domains and language pairs.
P16-1191,"
8 Conclusions and Future Work

",2016,"in follow-up work, we aim to apply our embedding method on smaller, yet gold-standard corpora such as semcor (miller et al., 1994) and streusle (schneider and smith, 2015) to examine the impact of the corpus choice in detail and extend the training data beyond wordnet vocabulary."
P16-1192,"
8 Conclusion
",2016,it would also be interesting to combine the strengths of the condensed and sibling-finder algorithms for further efficiency gains.
P16-1193,"
6 Conclusion
",2016,"further work is needed to explore the full power of these abilities to extract information about entailment from both unlabelled text and labelled entailment data, encode it all in a single vector space, and efficiently perform complex inferences about vectors and entailments."
P16-1193,"
6 Conclusion
",2016,this future work on compositional distributional semantics should further demonstrate the full power of the proposed framework for modelling entailment in a vector space.
P16-1194,"
5 Conclusions
",2016,"a dialogue may actually have a hierarchy structure of latent states, therefore the proposed model can be extended to a deep model to capture more complex structures."
P16-1194,"
5 Conclusions
",2016,another possible way to extend the model is to consider modeling long distance dependency between latent states.
P16-1194,"
5 Conclusions
",2016,this may further improve the model’s performance.
P16-1195,"
8 Conclusion
",2016,"in future work, we plan to develop better models for capturing the structure of the input, as well as extend the use of our system to other applications such as automatic documentation of source code."
P16-1196,"
4 Conclusion and Future Work
",2016,"in addition we plan to further investigate how to fine-tune some of the hyper parameters of the cpm such as spline scaling, single global scaling factor, convergence tolerance, and initialization of the latent trace with a centroid."
P16-1196,"
4 Conclusion and Future Work
",2016,"in addition, to aid cpm convergence to a good local optimum, in future work we will investigate dimensionality reduction approaches that are reversible such as principal component analysis (pearson, 1901) and other pre-processing approaches similar to (listgarten, 2007), where the training data set is coarsely pre-aligned and pre-scaled based on the center of mass of the time series."
P16-1196,"
4 Conclusion and Future Work
",2016,"in subsequent work, we would like to explore alternatives for enhancing cpms by incorporating contextual features in the training data set such as timing of hand movements, and preceding, succeeding, and co-occurring facial expressions."
P16-1196,"
4 Conclusion and Future Work
",2016,"while this work used the latent trace as the basis for animation, in future work, we also plan to explore methods for sampling from the model to produce variations in face and head movement."
P16-1197,"
5 Conclusion
",2016,"deriving sentiment labels for supervised training is an important topic for future study, as is inferring the sentiment of published news from stock price fluctuations instead of the reverse."
P16-1197,"
5 Conclusion
",2016,"in four task-based experiments, we evaluated the usefulness of sentiment analysis to simple trading strategies."
P16-1197,"
5 Conclusion
",2016,we should also study how “sentiment” is defined in the financial world.
P16-1198,"
6 Discussion
",2016,"in addition, we are currently exploring potential extensions of the techniques presented in this paper to higher order, projective and non-projective, dependency parsing."
P16-1198,"
6 Discussion
",2016,the potential embodied in this work extends to a number of promising research directions: ?
P16-1198,"
6 Discussion
",2016,we believe this contribution has the potential to affect future research on additional nlp problems.
P16-1198,"
6 Discussion
",2016,we intend to investigate all of these directions in future work.
P16-1199,"
6 Conclusion and Future Works

",2016,"in the next step, we plan to exploit fine-grained discourse structures, e.g., dialogue acts (ritter et al., 2010), and propose a unified model that jointly inferring discourse roles and topics of posts in context of conversation tree structures."
P16-1200,"
5 Conclusion and Future Works
",2016,"in the future, we will explore the following directions: ?"
P16-1200,"
5 Conclusion and Future Works
",2016,"in the future, we will incorporate our instance-level selective attention technique with those models for relation extraction."
P16-1200,"
5 Conclusion and Future Works
",2016,it can be used in not only distant supervised relation extraction but also other multi-instance learning tasks.
P16-1200,"
5 Conclusion and Future Works
",2016,we will explore our model in other area such as text categorization.?
P16-1201,"
6 Conclusions and Future Work
",2016,"finally, we alleviate the data sparseness problem of ed by using events detected from fn as extra training data."
P16-1201,"
6 Conclusions and Future Work
",2016,"in the future, we will extend this work to the complete event extraction task."
P16-1201,"
6 Conclusions and Future Work
",2016,the key of this research is to detect events in fn.
P16-1201,"
6 Conclusions and Future Work
",2016,we plan to refine the event schemas by the finer-grained frames defined in fn (i.e.
P16-1202,"
6 Conclusion
",2016,our future plan is to apply this model to general arithmetic problems which require multiple applications of formulas.
P16-1203,"
6 Conclusions and Future Work
",2016,"character polarity annotations in written literature could be created by, for example, applying sentiment analysis to the full text of the work."
P16-1203,"
6 Conclusions and Future Work
",2016,"it would, for example, be interesting to seek differences between spoken names (as in films) and names that are only meant to be read (as in literature)."
P16-1203,"
6 Conclusions and Future Work
",2016,our future research will test the correlation between the polarity and the name of a fictional character beyond the movie domain.
P16-1206,"
8 Conclusion
",2016,"additionally, the proposed evaluation metric can be easily extended to word segmentation task for other languages (e.g."
P16-1206,"
8 Conclusion
",2016,"japanese) and other sequence labelling-based nlp tasks, with just tiny changes."
P16-1206,"
8 Conclusion
",2016,our metric also points out a promising direction for the researchers to take into the account of the biased distribution of test case difficulty and focus on tackling the hard bones of natural language processing.
P16-1206,"
8 Conclusion
",2016,we will release the weighted datasets to the academic community.
P16-1207,"
7 Conclusion
",2016,"another option is to increase the granularity, but this requires that the information in the documents also allow this more precise anchoring."
P16-1207,"
7 Conclusion
",2016,"in case the temporal ordering is important for the application scenario, the annotation scheme could be extended and tlinks could be annotated for events that fall on the same date."
P16-1208,"
6 Discussion and Conclusions
",2016,"for future work, we intend to study the problem in the context of other languages."
P16-1208,"
6 Discussion and Conclusions
",2016,the purpose of this work is to gain a better understanding of the advantages offered by each learning method in order to make further progress on the gec task.
P16-1208,"
6 Discussion and Conclusions
",2016,we built several systems that draw on the strengths of each approach individually and in a pipeline.
P16-1210,"
7 Discussion
",2016,"finally, we would like to explore further our finding that the performance of the overall model seems to be predicted by the difference in performance between the non-pivot features and the new correspondence features, especially to see if this can be predicted at training time rather than as a post-hoc analysis."
P16-1210,"
7 Discussion
",2016,"first, though our median-based approach was successful in converting pivot feature values to binary classification problems, learning a regression model might be an even better approach for count-based features."
P16-1210,"
7 Discussion
",2016,"in the future, we would like to extend this work in several ways."
P16-1210,"
7 Discussion
",2016,"second, since the svd basis-shift seems to be the source of much of the gains, we would like to explore replacing the svd with other algorithms, such as independent component analysis."
P16-1210,"
7 Discussion
",2016,we proposed several extensions to the popular structural correspondence learning (scl) algorithm for domain adaptation to make it more amenable to tasks like authorship attribution.
P16-1212,"
5 Conclusion and Future Work

",2016,"in the future, we will conduct experiments on large corpus in different domains."
P16-1213,"
5 Conclusion
",2016,"our proposed system, liel is trained on the english wikipedia corpus, after building its own knowledge-base by exploiting the rich information present in wikipedia."
P16-1214,"
4 Conclusion
",2016,our future research includes improving the search performance for embeddings with heavy-tailed distributions and creating embeddings that can keep both task quality and search performance high.
P16-1214,"
4 Conclusion
",2016,"since we need to implement additional glue codes for running flann and sash, our code would be useful for researchers who want to compare their results with ours."
P16-1215,"
6 Conclusion
",2016,"analyzing the internal mechanism of lstm, gru, and gac, we plan to explore an alternative architecture of neural networks that is optimal for relational patterns."
P16-1215,"
6 Conclusion
",2016,"we expect that several further studies will use the new dataset not only for distributed representations of relational patterns but also for other nlp tasks (e.g., paraphrasing)."
P16-1216,"
8 Conclusion
",2016,"also, negation of group membership is a complex linguistic phenomenon that was handled in a crude manner in our system."
P16-1216,"
8 Conclusion
",2016,"in future work, we look to incorporate methods that incur less cost, possibly tolerating some error in the formation of sentences without significantly degrading performance."
P16-1216,"
8 Conclusion
",2016,we look to devote future work to handling such cases.
P16-1217,"
6 Conclusions and Future Work
",2016,"in future work, we will explore to make use of all the three kinds of labels together to improve the users’ experience when they want to browse, understand and leverage the topics."
P16-1217,"
6 Conclusions and Future Work
",2016,"in future work, we will try to make the summary label more coherent by considering the discourse structure of the summary and leveraging sentence ordering techniques."
P16-1220,"
7 Conclusion and Future Work
",2016,a potential application of our method is to improve kb-question answering using the documents retrieved by a search engine.
P16-1220,"
7 Conclusion and Future Work
",2016,our future work involves exploring other alternatives such as treating structured and unstructured data as two independent resources in order to overcome the knowledge gaps in either of the two resources.
P16-1223,"
7 Conclusion
",2016,"as future work, we need to consider how we can utilize these datasets (and the models trained upon them) to help solve more complex rc reasoning tasks (with less annotated data)."
P16-1224,"
6 Related Work and Discussion
",2016,"if these systems could quickly adapt to user feedback in real-time as in this work, then we might be able to more readily create systems for resource-poor languages and new domains, that are customizable and improve through use."
P16-1224,"
6 Related Work and Discussion
",2016,"looking forward, we believe that the illg setting is worth studying and has important implications for natural language interfaces."
P16-1225,"
6 Conclusion
",2016,"finally, we would like to test our model using other representations of word-form and wordmeaning."
P16-1225,"
6 Conclusion
",2016,future work could also test whether a more interpretable meaningspace representation such as that provided by binary wordnet feature vectors reveals patterns of systematicity not found using a distributional semantic space.
P16-1225,"
6 Conclusion
",2016,future work may investigate to what extent the smlkr model can predict human intuitions about form-meaning systematicity in language.
P16-1225,"
6 Conclusion
",2016,"however, it would be interesting to verify our results in a phonological setting, perhaps using a monodialectal corpus."
P16-1225,"
6 Conclusion
",2016,"thus, it may be worthwhile to augment the representation of edit distance in our model by making it context-sensitive."
P16-1225,"
6 Conclusion
",2016,we would also like to investigate the degree to which our statistical model predicts the behavioral effects of phonosemantic systematicity during human semantic processing that have been reported in the psycholinguistics literature.
P16-1226,"
8 Conclusion
",2016,"finally, our architecture seems straightforwardly applicable for multi-class classification, which, in future work, could be used to classify term-pairs to multiple semantic relations."
P16-1227,"
5 Conclusions and Further Work
",2016,a further avenue of future research is improving models such as that presented in elliott et al.(2015) by crucial components of neural mt such as “attention mechanisms”.
P16-1227,"
5 Conclusions and Further Work
",2016,combining these two types of attention mechanisms in a neural caption translation model is a natural next step in caption translation.
P16-1227,"
5 Conclusions and Further Work
",2016,"despite the fact that our simple distance metric performed comparably to human object annotations, using such high-level semantic distance metrics for caption translation by multimodal pivots is a promising avenue for further research."
P16-1227,"
5 Conclusions and Further Work
",2016,"in future work, we plan to evaluate our approach in more naturalistic settings, such machine translation for captions in online multimedia repositories such as wikimedia commons16 and digitized art catalogues, as well as e-commerce localization."
P16-1227,"
5 Conclusions and Further Work
",2016,"learning semantically informative distance metrics using deep learning techniques is an area under active investigation (wu et al., 2013; wang et al., 2014; wang et al., 2015)."
P16-1227,"
5 Conclusions and Further Work
",2016,"using our approach, smt can, in certain cases, profit from multimodal context information."
P16-1228,"
6 Discussion and Future Work
",2016,"finally, we also would like to generalize our framework to automatically learn the confidence of different rules, and derive new rules from data."
P16-1228,"
6 Discussion and Future Work
",2016,our framework is general and applicable to various types of neural architectures.
P16-1228,"
6 Discussion and Future Work
",2016,"the encouraging empirical results indicate a strong potential of our approach for improving other application domains such as vision tasks, which we plan to explore in the future."
P16-1228,"
6 Discussion and Future Work
",2016,we plan to explore these diverse knowledge representations to guide the dnn learning.
P16-1230,"
5 Conclusion
",2016,"consistent with all of our previous work, the reward function studied here is focused primarily on task success."
P16-1230,"
5 Conclusion
",2016,the system enables stable policy optimisation by robustly modelling the inherent noise in real user feedback and uses active learning to minimise the number of feedback requests to the user.
P16-1230,"
5 Conclusion
",2016,this may be too simplistic for many commercial applications and further work will be needed in conjunction with human interaction experts to identify and incorporate the extra dimensions of dialogue quality that will be needed to achieve the highest levels of user satisfaction.
p17-1002,"
6 Conclusion
",2017,"its suitability for the end-to-end learning task is scope for future work, but its adequacy for component classification and relation identification has been investigated in potash et al.(2016)."
p17-1004,"
5 Conclusion
",2017,"hence, the word-level multi-lingual attention, which may discover implicit alignments between words in multiple languages, will further improve multi-lingual relation extraction."
p17-1004,"
5 Conclusion
",2017,"in future, we will extend mnre to more languages and explore its significance."
p17-1004,"
5 Conclusion
",2017,"we will explore the effectiveness of word-level multilingual attention for relation extraction as our future work.(2) mnre can be flexibly implemented in the scenario of multiple languages, and this paper focuses on two languages of english and chinese."
p17-1004,"
5 Conclusion
",2017,"we will explore the following directions as future work: (1) in this paper, we only consider sentence-level multi-lingual attention for relation extraction."
p17-1005,"
5 Discussion
",2017,"aside from relaxing strict isomorphism, we would also like to perform crossdomain semantic parsing where the first stage of the semantic parser is shared across domains."
p17-1005,"
5 Discussion
",2017,our model essentially jointly learns how to parse natural language semantics and the lexicons that help grounding.
p17-1005,"
5 Discussion
",2017,"while it would be challenging to handle drastically non-isomorphic structures in the current model, it is possible to perform local structure matching, i.e., when the mapping between natural language and domainspecific predicates is many-to-one or one-to-many."
p17-1005,"
5 Discussion
",2017,"within our current framework, these two types of structural mismatches can be handled with semi-markov assumptions (sarawagi and cohen, 2005; kong et al., 2016) in the parsing (i.e., predicate selection) and the grounding steps, respectively."
p17-1006,"
7 Conclusion and Future Work
",2017,"future work will focus on other potential sources of morphological knowledge, porting the framework to other morphologically rich languages and downstream tasks, and on further refinements of the post-processing specialisation algorithm and the constraint selection."
p17-1008,"
7 Conclusion
",2017,"semantic schemes diverge in whether they are anchored in the words and phrases of the text (e.g., all types of semantic dependencies and ucca) or not (e.g., amr and logic-based representations)."
p17-1010,"
6 Conclusion
",2017,"for training purpose, two-step training approach is employed, i.e.a pre-training and adaptation step, and this can be also easily applied to other tasks as well."
p17-1010,"
6 Conclusion
",2017,"second, we will develop other neural network architecture to make it more appropriate for zero pronoun resolution task."
p17-1010,"
6 Conclusion
",2017,"the future work will be carried out on two main aspects: first, as experimental results show that the unknown words processing is a critical part in comprehending context, we will explore more effective way to handle the unk issue."
p17-1011,"
6 Conclusion
",2017,"a corpus of narrative student essays was manually annotated with discourse modes at sentence level, with acceptable inter-annotator agreement."
p17-1011,"
6 Conclusion
",2017,"in future, we plan to exploit discourse mode identification for providing novel features for more downstream nlp applications."
p17-1011,"
6 Conclusion
",2017,"the corpus analysis revealed several aspects of characteristics of discourse modes including the distribution, co-occurrence and transition patterns."
p17-1012,"
6 Conclusion
",2017,"also, we plan to investigate the effectiveness of our architecture on other sequence-tosequence tasks, e.g.summarization, constituency parsing, dialog modeling."
p17-1012,"
6 Conclusion
",2017,future work includes better training to enable faster convergence with the convolutional encoder to better leverage the higher processing speed.
p17-1012,"
6 Conclusion
",2017,our architecture also leads to large generation speed improvements: translation models with our convolutional encoder can translate twice as fast as strong baselines with bi-directional recurrent encoders.
p17-1014,"
9 Conclusions
",2017,"crucially, we avoid relying on resources such as knowledge bases and externally trained parsers."
p17-1014,"
9 Conclusions
",2017,"for future work, we would like to extend our work to different meaning representations such as the minimal recursion semantics (mrs; copestake et al.(2005))."
p17-1014,"
9 Conclusions
",2017,"taking a step further, we would like to apply our models on semantics-based machine translation using mrs as an intermediate representation between pairs of languages, and investigate the added benefit compared to directly translating the surface strings, especially in the case of distant language pairs such as english and japanese (siegel, 2000)."
p17-1016,"
6 Conclusions
",2017,"in future work, it would be useful to investigate models based on morphemes, rather than characters, which offers potentially superior performance for complex and rare words (luong et al., 2013), which are common in poetry."
p17-1017,"
5 Conclusion
",2017,"conversely, the ratio of text per input is twice higher thereby providing better support for learning paraphrases."
p17-1017,"
5 Conclusion
",2017,the ratio between input and input patterns is five times lower in our dataset thereby making learning harder but also diminishing the risk of overfitting and providing for wider linguistic coverage.
p17-1017,"
5 Conclusion
",2017,we hope that the webnlg dataset which we have made available for the webnlg shared task will drive the deep learning community to take up this new challenge and work on the development of neural generators that can handle the generation of kb verbalisers and of linguistically rich texts.
p17-1018,"
7 Conclusion
",2017,"as for future work, we are applying the gated self-matching networks to other reading comprehension and question answering datasets, such as the ms marco dataset (nguyen et al., 2016)."
p17-1019,"
6 Conclusion and Future Work
",2017,"and the future work includes: a) lots of questions cannot be answered directly by facts in a kb (e.g.鈥淲ho is jet li鈥檚 father-in-law?鈥), we plan to learn qa system with latent knowledge (e.g."
p17-1019,"
6 Conclusion and Future Work
",2017,"kb embedding (bordes et al., 2013)); b) we plan to adopt memory networks (sukhbaatar et al., 2015) to encode the temporary kb for each question."
p17-1020,"
8 Conclusion
",2017,"in future work we would like to deepen the use of structural clues and answer questions over multiple documents, using paragraph structure, titles, sections and more."
p17-1020,"
8 Conclusion
",2017,incorporating coreference resolution would be another important direction for future work.
p17-1021,"
6 Conclusion
",2017,"firstly, we consider the impacts of the different answer aspects when representing the question, and propose a novel cross-attention model for kb-qa."
p17-1021,"
6 Conclusion
",2017,"secondly, we leverage the global kb information, which could take full advantage of the complete kb, and also alleviate the oov problem for the attention model."
p17-1022,"
9 Conclusion
",2017,"the current work has focused on learning a purely categorical model of the translation process, supported by an unstructured inventory of translation candidates, and future work could explore the compositional structure of messages, and attempt to synthesize novel natural language or neuralese messages from scratch."
p17-1023,"
7 Discussion and Conclusion
",2017,"also from an reg perspective, various extensions of this approach are possible, such as the inclusion of contextual information during object naming and its combination with attribute selection."
p17-1023,"
7 Discussion and Conclusion
",2017,"as we have tested these approaches on a rather small vocabulary, which may limit generality of conclusions, future work will be devoted to scaling up these findings to larger test sets, as e.g.recently collected through conversational agents (das et al., 2016) that circumvent the need for human-human interaction data."
p17-1024,"
6 Conclusion
",2017,"to complete the analysis of these results, we plan to carry out a further task, namely ask the system to detect in the image the area that produces the mismatch with the foil word (the red box around the bird in figure 1.)"
p17-1024,"
6 Conclusion
",2017,"we will then have a pipeline able to not only test whether a mistake can be detected, but also whether the system can explain its decision: 鈥榯he wrong word is dog because the cyclists are in fact approaching a bird, there, in the image鈥."
p17-1024,"
6 Conclusion
",2017,"with our work, we would like to push the community to think of ways that models can better merge language and vision modalites, instead of merely using one to supplement the other"
p17-1027,"
7 Conclusion
",2017,"we believe that gains from both techniques should be complementary, as they apply to orthogonal components of the parsing system (the scoring model vs. the transition system), although we might see a 鈥漝iminishing returns鈥漞ffect."
p17-1027,"
7 Conclusion
",2017,"while we used a perceptron classifier for our experiments, our oracle could also be used in neuralnetwork implementations of greedy transitionbased parsing (chen and manning, 2014; dyer et al., 2015), providing an interesting avenue for future work."
p17-1028,"
6 Conclusions and Future Work
",2017,"we also plan to investigate extension of the crowd component in our hmm method with hierarchical models, as well as a fully-bayesian approach."
p17-1028,"
6 Conclusions and Future Work
",2017,"we expect our methods to be applicable to and similarly benefit other sequence labeling tasks, such as pos tagging and chunking (hovy et al., 2014)."
p17-1029,"
7 Conclusion and Future Work
",2017,"future work will adapt this framework to other sequence transduction scenarios such as machine translation, dialogue generation, question answering, where continuous and discrete latent variables can be abstracted to guide sequence generation."
p17-1031,"
7 Conclusion and Future Work
",2017,reranking the predictions with a language model could be one possible way to improve on this.
p17-1032,"
5 Discussion and Conclusions
",2017,"finally, the optimization of the kda methodology through the suitable parallelization on multicore architectures, as well as the exploration of mechanisms for the dynamic reconstruction of kernel spaces (e.g., operating over hn y) also constitute interesting future research directions on this topic."
p17-1032,"
5 Discussion and Conclusions
",2017,"future work will address experimentations with larger scale datasets; moreover, it is interesting to experiment with more landmarks in order to better understand the trade-off between the representation capacity of the nystrom approximation 篓 of the kernel functions and the over-fitting that can be introduced in a neural network architecture."
p17-1032,"
5 Discussion and Conclusions
",2017,"in this work, we promoted a methodology to embed structured linguistic information within nns, according to mathematically rich semantic similarity models, based on kernel functions."
p17-1032,"
5 Discussion and Conclusions
",2017,"notice that other low-dimensional approximations of kernel functions have been studied, as for example the randomized feature mappings proposed in rahimi and recht (2008)."
p17-1032,"
5 Discussion and Conclusions
",2017,"the problem of combining such methodologies has been studied in specific works, such as (baldi et al., 2011; cho and saul, 2009; yu et al., 2009)."
p17-1034,"
6 Conclusion and Future Work
",2017,it can learn to represent the new review of the new reviewer with the similar textual information and the correlated behavioral information in an unsupervised way.
p17-1034,"
6 Conclusion and Future Work
",2017,we are going to explore more effective models in future.
p17-1035,"
9 Conclusion and Future Directions
",2017,"our future agenda includes: (a) optimizing the cnn framework hyper-parameters (e.g., filter width, dropout, embedding dimensions, etc.)to obtain better results, (b) exploring the applicability of our technique for documentlevel sentiment analysis and (c) applying our framework to related problems, such as emotion analysis, text summarization, and questionanswering, where considering textual clues alone may not prove to be sufficient."
p17-1037,"
6 Conclusion
",2017,"for our immediate future work, we plan to embed the topic and user vectors to create a crosstopic stance detector."
p17-1037,"
6 Conclusion
",2017,"it is possible to generalize our work to model heterogeneous signals, such as interests and behaviors of people, for example, 鈥渢hose who are interested in a also support b,鈥 and 鈥渢hose who favor a also vote for b鈥."
p17-1037,"
6 Conclusion
",2017,"therefore, we believe that our work will bring about new applications in the field of nlp and other disciplines."
p17-1038,"
7 Conclusion and Future Work
",2017,"also, we provide a dmcnn-mil model for this data as a baseline for further research."
p17-1038,"
7 Conclusion and Future Work
",2017,"in the future, we will use the proposed automatically data labeling method to more event types and explore more models to extract events by using automatically labeled data."
p17-1039,"
6 Conclusion and future work
",2017,in the future we will try our analytical method on other parts of language.
p17-1039,"
6 Conclusion and future work
",2017,"since language usage relates to human habits (zipf, 1949; chomsky, 1986; pinker, 1995), we might expect that humans would share some common habits, and therefore expect that other parts of language would more or less follow the same principle."
p17-1039,"
6 Conclusion and future work
",2017,the test for other languages needs only to construct a collection of token regular expressions in the target language under our defined token types.
p17-1040,"
8 Conclusions
",2017,"one of our key innovations is to exploit a curriculum learning based training method to gradually learn to model the underlying noise pattern without direct guidance, and to provide the flexibility to exploit any prior knowledge of the data quality to further improve the effectiveness of the transition matrix."
p17-1042,"
7 Conclusions and future work
",2017,"finally, we would like to apply our model in the decipherment scenario (dou et al., 2015)."
p17-1042,"
7 Conclusions and future work
",2017,"in addition to that, we would like to explore non-linear transformations (lu et al., 2015) and alternative dictionary induction methods (dinu et al., 2015; smith et al., 2017)."
p17-1042,"
7 Conclusions and future work
",2017,"in the future, we would like to delve deeper into this direction and fine-tune our method so it can reliably learn high quality bilingual word embeddings without any bilingual evidence at all."
p17-1043,"
7 Conclusion and Future Work
",2017,"a better approach to disambiguation is to consider predicates separately, solving for a set of coefficients for each verb found in the training set."
p17-1043,"
7 Conclusion and Future Work
",2017,but it would also be fairly easy to add gazetteer information to the network features in order to remove the need for ner preprocessing.
p17-1043,"
7 Conclusion and Future Work
",2017,"since our preferred wikifier application generates ner information, we used the generated ner tags as input to the sg network."
p17-1044,"
6 Conclusion and Future Work
",2017,"finally, we posed the question of whether deep srl still needs syntactic supervision."
p17-1045,"
7 Conclusions and Discussion
",2017,"as the user interacts with this agent, the collected data can be used to train the e2e agent, which has a strong learning capability."
p17-1045,"
7 Conclusions and Discussion
",2017,"effective implementation of this, however, requires the e2e agent to learn quickly and this is the research direction we plan to focus on in the future."
p17-1045,"
7 Conclusions and Discussion
",2017,"given these results, we propose the following deployment strategy that allows a dialogue system to be tailored to specific users via learning from agent-user interactions."
p17-1045,"
7 Conclusions and Discussion
",2017,this work is aimed at facilitating the move towards end-to-end trainable dialogue agents for information access.
p17-1046,"
6 Conclusion and Future Work
",2017,"besides, we publish the first human-labeled multi-turn response selection data set to research communities."
p17-1046,"
6 Conclusion and Future Work
",2017,"in the future, we shall study how to model logical consistency of responses and improve candidate retrieval."
p17-1047,"
6 Conclusions and Future Work
",2017,"additionally, by collecting a second dataset of captions for our images in a different language, such as spanish, our model could be extended to learn the acoustic correspondences for a given object category in both languages."
p17-1047,"
6 Conclusions and Future Work
",2017,"modeling improvements are also possible, aimed at the goal of incorporating both visual and acoustic localization into the neural network itself."
p17-1047,"
6 Conclusions and Future Work
",2017,the future directions in which this research could be taken are incredibly fertile.
p17-1047,"
6 Conclusions and Future Work
",2017,"the same framework we use here could be extended to video, enabling the learning of actions, verbs, environmental sounds, and the like."
p17-1047,"
6 Conclusions and Future Work
",2017,"this paves the way for creating a speech-to-speech translation model not only with absolutely zero need for any sort of text transcriptions, but also with zero need for directly parallel linguistic data or manual human translations."
p17-1048,"
5 Summary and discussion
",2017,"actually, neural machine translation handles this issue by using a sub word unit (concatenating several letters to form a new sub word unit) (wu et al., 2016), which would be a promising direction for end-toend asr."
p17-1048,"
5 Summary and discussion
",2017,"future work will apply this technique to the other languages including english, where we have to solve an issue of long sequence lengths, which requires heavy computation cost and makes it difficult to train a decoder network."
p17-1049,"
7 Conclusion
",2017,"it may be possible to define classifiers implementing other universal facets of translation, e.g., simplification, which will yield good separation between o and t. however, explicitation fails in the reproduction of language typology, whereas interference-based features produce trees of considerable quality."
p17-1049,"
7 Conclusion
",2017,"this holds for the product of highly professional translators, who conform to a common standard, and whose products are edited by native speakers, like themselves."
p17-1049,"
7 Conclusion
",2017,we are presently trying to extend these results to translations in a different domain (literary texts) into a very different language (hebrew).
p17-1049,"
7 Conclusion
",2017,we leave this as a direction for future research.
p17-1050,"
6 Conclusion and Outlook
",2017,"in future work, we also plan to explore the role of native and second language writing system characteristics in second language reading."
p17-1050,"
6 Conclusion and Outlook
",2017,"more broadly, our methodology introduces parallels with production studies in nlp, creating new opportunities for integration of data, methodologies and tasks between production and comprehension."
p17-1050,"
6 Conclusion and Outlook
",2017,"while this work is focused on nlir from fixations, our general framework can be used to address additional aspects of reading, such as analysis of saccades and gaze trajectories."
p17-1051,"
7 Conclusions and Future Work
",2017,"for future work, we plan to address the limitations of morse: minimal supervision, greedy inference, and concatenative orthographic model."
p17-1051,"
7 Conclusions and Future Work
",2017,"moreover, we plan to computationally optimize the training stage for the sake of wider adoption by the community."
p17-1053,"
7 Conclusion
",2017,"for example, our model could be integrated into the decoder in (liang et al., 2016), to provide better sequence prediction."
p17-1053,"
7 Conclusion
",2017,"for future work, we will investigate the integration of our hr-bilstm into end-to-end systems."
p17-1053,"
7 Conclusion
",2017,"we will also investigate new emerging datasets like graphquestions (su et al., 2016) and complexquestions (bao et al., 2016) to handle more characteristics of general qa."
p17-1054,"
7 Conclusions and Future Work
",2017,"in the future, we are interested in comparing the model to human annotators and using human judges to evaluate the quality of predicted phrases.鈥 our current model does not fully consider correlation among target keyphrases."
p17-1054,"
7 Conclusions and Future Work
",2017,it would also be interesting to explore the multiple-output optimization aspects of our model.
p17-1054,"
7 Conclusions and Future Work
",2017,"our future work may include the following two directions.鈥 in this work, we only evaluated the performance of the proposed model by conducting off-line experiments."
p17-1055,"
8 Conclusion
",2017,"also, we are interested to see that if the machine really 鈥渃omprehend鈥 our language by utilizing neural networks approaches, but not only serve as a 鈥渄ocument-level鈥 language model."
p17-1055,"
8 Conclusion
",2017,"in this context, we are planning to investigate the problems that need comprehensive reasoning over several sentences."
p17-1055,"
8 Conclusion
",2017,the future work will be carried out in the following aspects.
p17-1055,"
8 Conclusion
",2017,"the proposed aoa reader aims to compute the attentions not only for the document but also the query side, which will benefit from the mutual information."
p17-1055,"
8 Conclusion
",2017,"we believe that our model is general and may apply to other tasks as well, so firstly we are going to fully investigate the usage of this architecture in other tasks."
p17-1056,"
7 Conclusions
",2017,"more broadly, these results suggest ways that quantitative methods can be used to make precise application of concepts like 鈥渃ultural fit鈥 at scale."
p17-1056,"
7 Conclusions
",2017,"quantitatively, rates of usage and alignment in the first six months of employment carried information about whether employees left involuntarily, pointing towards fit within the company culture early on as an indicator of eventual employment outcomes."
p17-1057,"
5 Conclusion
",2017,going forward we would like to compare the representations learned by our model to the brain activity of people listening to speech in order to determine to what extent the patterns we found correspond to localized processing in the human cortex.
p17-1057,"
5 Conclusion
",2017,this will hopefully lead to a better understanding of language learning and processing by both artificial and neural networks.
p17-1058,"
8 Conclusions
",2017,"we hope that by comparing and combining our methodology with other approaches of studying dialogue, we can reach a more comprehensive and holistic understanding of this common yet mysterious human practice."
p17-1059,"
6 Conclusions and Future Work
",2017,"for future work, we wish to extend this model by investigating language generation conditioned on other modalities such as facial images and speech, and to applications such as dialogue generation for virtual agents."
p17-1060,"
6 Conclusion
",2017,future work can include an extension of domain experts to take into account dialog history aiming for a holistic framework that can handle contextual interpretation as well.
p17-1060,"
6 Conclusion
",2017,this approach enables creation of new virtual domains through a weighted combination of domain experts鈥 feedback reducing the need to collect and annotate the similar intent and slot types multiple times for different domains.
p17-1061,"
6 Conclusion and Future Work
",2017,all of the above suggest a promising research direction.
p17-1061,"
6 Conclusion and Future Work
",2017,"in addition to dialog acts, we plan to apply our kgcvae model to capture other different linguistic phenomena including sentiment, named entities,etc."
p17-1062,"
7 Conclusion
",2017,"in future work, we plan to extend hcns by incorporating lines of existing work, such as integrating the entity extraction step into the neural network (dhingra et al., 2017), adding richer utterance embeddings (socher et al., 2013), and supporting text generation (sordoni et al., 2015)."
p17-1062,"
7 Conclusion
",2017,"more broadly, hcns are a general model for stateful control, and we would be interested to explore applications beyond dialog systems 鈥 for example, in nlp medical settings or humanrobot nl interaction tasks, providing domain constraints are important for safety; and in resourcepoor settings, providing domain knowledge can amplify limited data."
p17-1062,"
7 Conclusion
",2017,"of course, we also plan to deploy the model in a live dialog system."
p17-1062,"
7 Conclusion
",2017,"we will also explore using hcns with automatic speech recognition (asr) input, for example by forming features from n-grams of the asr n-best results (henderson et al., 2014b)."
p17-1063,"
8 Conclusion
",2017,"finally, it would be interesting to combine our algorithm with a speech synthesis system."
p17-1063,"
8 Conclusion
",2017,"furthermore, we could use this data to refine the costs c(d), c(ia) etc.for the edit operations, possibly assigning different costs to different edit operations."
p17-1063,"
8 Conclusion
",2017,"in future work, we will complement the overhearer experiment presented here with an end-toend evaluation in an interactive nlg setting."
p17-1063,"
8 Conclusion
",2017,this will allow us to further investigate the quality of the correction strategies and refine the shortening strategy.
p17-1064,"
7 Conclusion
",2017,"in our future work, we expect several developments that will shed more light on utilizing source syntax, e.g., designing novel syntactic features (e.g., features showing the syntactic role that a word is playing) for nmt, and employing the source syntax to constrain and guild the attention models."
p17-1065,"
6 Conclusion and Future Work
",2017,"in addition, we will apply our method to other sequenceto-sequence tasks, such as text summarization, to verify the effectiveness."
p17-1065,"
6 Conclusion and Future Work
",2017,"in future work, along this research direction, we will try to integrate other prior knowledge, such as semantic information, into nmt systems."
p17-1066,"
6 Conclusion and Future Work
",2017,"enlightened by tree kernel techniques, our kernel method learns discriminant clues for identifying rumors of finer-grained levels by directly measuring the similarity among propagation trees via kernel functions."
p17-1066,"
6 Conclusion and Future Work
",2017,"in the future, we will focus on improving the rumor detection task by exploring network representation learning framework."
p17-1066,"
6 Conclusion and Future Work
",2017,"moreover, we plan to investigate unsupervised models considering massive unlabeled rumorous data from social media."
p17-1067,"
8 Conclusion
",2017,we also extended the classification to 8 primary emotion dimensions situated in psychological theory of emotion.
p17-1068,"
7 Conclusions
",2017,"another direction of future study will look at political ideology prediction in other countries and cultures, where ideology has different or multiple dimensions."
p17-1068,"
7 Conclusions
",2017,"in addition, our work on user-level modeling can be integrated with work on message-level political bias to study how this is revealed across users with various levels of engagement."
p17-1068,"
7 Conclusions
",2017,"while our study focused solely on text posted by the user, follow-up work can use other modalities such as images or social network analysis to improve prediction performance."
p17-1069,"
7 Conclusion
",2017,"finally, our global psl models can be applied to other domains, such as politics in other countries, simply by changing the initial unigram keywords to reflect the politics of those countries."
p17-1071,"
5 Conclusion
",2017,"among the potential extensions of this work are the inclusion of different kinds of weights such as tf-idf, embedding relatedness and semantic relatedness."
p17-1071,"
5 Conclusion
",2017,"we also intend to test other variants around the same concept, including considering the matched words and sequences to have a negative weight to balance further the weight of missing words."
p17-1072,"
6 Concluding Discussion
",2017,it remains as a further stage of analysis to understand the underlying reasons that lead to these relations between ideas.
p17-1072,"
6 Concluding Discussion
",2017,there are many potential directions to improve our method to account for complex relations between ideas.
p17-1072,"
6 Concluding Discussion
",2017,we not only confirm existing knowledge but also suggest hypotheses around the usage of arab and islam in terrorism and latino and asian in immigration.
p17-1073,"
5 Conclusions
",2017,"however, it is very likely that the annotation framework will work for other slavic languages (e.g."
p17-1075,"
7 Conclusion
",2017,"in future work, we plan to use the analysis from the present study in constructing a system that can be applied to multiple datasets."
p17-1078,"
6 Conclusion
",2017,"taking each type of external resource as an auxiliary classification task, we use neural multi-task learning to pre-train a set of shared parameters for character contexts."
p17-1079,"
5 Conclusion
",2017,one interesting avenue of future work is to automatically learn encodings and error correcting codes that are well-suited for the type of binary code prediction we are performing here.
p17-1080,"
7 Conclusion
",2017,"another area for future work is to extend the analysis to other word representations (e.g.byte-pair encoding), deeper networks, and more semantically-oriented tasks such as semantic rolelabeling or semantic parsing."
p17-1080,"
7 Conclusion
",2017,"for instance, jointly learning translation and morphology can possibly lead to better representations and improved translation."
p17-1081,"
5 Conclusion
",2017,"as future work, we plan to develop a lstmbased attention model to determine the importance of each utterance and its specific contribution to each modality for sentiment classification."
p17-1082,"
8 Conclusion
",2017,our hope is that these stance dimensions will be valuable as a convenient building block for future research on interactional meaning.
p17-1082,"
8 Conclusion
",2017,stancetaking provides a general perspective on the various linguistic phenomena that structure social interactions.
p17-1083,"
5 Conclusion
",2017,"for interactive topic modeling, using anchor facets in place of single word anchors produces higher quality topic models and are more intuitive to use."
p17-1083,"
5 Conclusion
",2017,tandem anchors extend the anchor words algorithm to allow multiple words to be combined into anchor facets.
p17-1084,"
8 Conclusion
",2017,"as a future work, we would like to explore the feasibility of marrying our semantic and neural models to exploit the benefits that each of them has to offer."
p17-1084,"
8 Conclusion
",2017,"for continuing our data collection, we would like to have a targeted entity pair selection where we particularly collect the missing relations in our partial ordering lattice."
p17-1085,"
8 Conclusion
",2017,"in future, we plan to explore pretraining methods for our model which were shown to improve recall on entity and relation performance by miwa and bansal (2016)."
p17-1085,"
8 Conclusion
",2017,"it would also be interesting to see the effect of reranking (collins and koo, 2005) on our joint model."
p17-1085,"
8 Conclusion
",2017,we also plan to extend the identification of entities to full entity mention span instead of only the head phrase as in lu and roth (2015).
p17-1085,"
8 Conclusion
",2017,"we also plan to use sparsemax (martins and astudillo, 2016) instead of softmax for multiple relations, as the former is more suitable for multi-label classification for sparse labels."
p17-1085,"
8 Conclusion
",2017,we think that using probabilistic methods to combine model predictions from both directions may further improve the performance.
p17-1086,"
7 Related work and discussion
",2017,"in the future, we wish to test these ideas in more domains, naturalize a real pl, and handle paraphrasing and implicit arguments."
p17-1087,"
9 Conclusion
",2017,another possibility is to use thesauri and word vector representations together with word sense disambiguation to generate semantically similar clusters for multiple senses of words.
p17-1087,"
9 Conclusion
",2017,"finally, we plan to extend the hard signed clustering presented here to probabilistic soft clustering."
p17-1087,"
9 Conclusion
",2017,"it could also be used to explore multi-view relationships, such as aligning synonym clusters across multiple languages."
p17-1087,"
9 Conclusion
",2017,"our signed spectral clustering method could be applied to a broad range of nlp tasks, such as prediction of social group clustering, identification of personal versus non-personal verbs, and analyses of clusters which capture positive, negative, and objective emotional content."
p17-1088,"
7 Conclusion and Future Work
",2017,"in addition, our framework can also be applied to multi-task learning, promoting a finer sharing among different tasks."
p17-1088,"
7 Conclusion and Future Work
",2017,"in the future, we plan to enable itransf to perform multi-step inference, and extend the sharing mechanism to entity and relation embeddings, further enhancing the statistical binding across parameters."
p17-1093,"
5 Discussions
",2017,"besides implicit connective examples, our model can naturally exploit enormous explicit connective data to further improve discourse parsing."
p17-1094,"
7 Conclusions
",2017,"our study opens several future directions, ranging from defining algorithms based on automatically learned loss functions to learning more effective measures from expert examples."
p17-1095,"
6 Conclusion
",2017,experiments in semisupervised and low-resource settings have demonstrated its applicability to part-of-speech induction and low-resource named-entity recognition.
p17-1095,"
6 Conclusion
",2017,"it would also be interesting to explore more expressive parameterizations, such recurrent neural networks for hy."
p17-1095,"
6 Conclusion
",2017,there are many potential avenues for future work.
p17-1096,"
6 Conclusions
",2017,"in the future, we plan to apply our approach to more question answering datasets in different domains."
p17-1096,"
6 Conclusions
",2017,it will also be intriguing to generalize gdans to other applications.
p17-1098,"
8 Conclusion
",2017,the diversification model proposed is general enough to apply to other nlg tasks with suitable modifications and we are currently working on extending this to dialog systems and general summarization.
p17-1098,"
8 Conclusion
",2017,this gives us the flexibility to: (i) provide diverse context vectors at successive time steps and (ii) pay attention to words repeatedly if need be later in the summary (as opposed to existing models which aggressively delete the history).
p17-1102,"
5 Conclusion and Future Work
",2017,"in the future, it would be interesting to explore the performance of positionrank on other types of documents, e.g., web pages and emails."
p17-1103,"
7 Discussion
",2017,"alternatively, one can combine this with an adversarial evaluation model (kannan and vinyals, 2017; li et al., 2017) that assigns a score based on how easy it is to distinguish the dialogue model responses from human responses."
p17-1103,"
7 Discussion
",2017,an important direction for future work is modifying adem such that it is not subject to this bias.
p17-1103,"
7 Discussion
",2017,an important direction of future research is building models that can evaluate the capability of a dialogue system to have an engaging and meaningful interaction with a human.
p17-1103,"
7 Discussion
",2017,"such models are necessary even for creating a test set in a new domain, which will help us determine if adem generalizes to related dialogue domains."
p17-1103,"
7 Discussion
",2017,"this could be done, for example, by censoring adem鈥檚 representations (edwards and storkey, 2016) such that they do not contain any information about length."
p17-1103,"
7 Discussion
",2017,we leave investigating the domain transfer ability of adem for future work.
p17-1103,"
7 Discussion
",2017,"we view the evaluation procedure presented in this paper as an important step towards this goal; current dialogue systems are incapable of generating responses that are rated as highly appropriate by humans, and we believe our evaluation model will be useful for measuring and facilitating progress in this direction."
p17-1104,"
7 Conclusion
",2017,"a parser for ucca will enable using the framework for new tasks, in addition to existing applications such as machine translation evaluation (birch et al., 2016)."
p17-1104,"
7 Conclusion
",2017,"future work will evaluate tupa in a multilingual setting, assessing ucca鈥檚 cross-linguistic applicability."
p17-1104,"
7 Conclusion
",2017,"in addition, we will explore different conversion procedures (kong et al., 2015) to compare different representations, suggesting ways for a data-driven design of semantic annotation."
p17-1104,"
7 Conclusion
",2017,"we believe ucca鈥檚 merits in providing a cross-linguistically applicable, broadcoverage annotation will support ongoing efforts to incorporate deeper semantic structures into various applications, such as sentence simplification (narayan and gardent, 2014) and summarization (liu et al., 2015)."
p17-1104,"
7 Conclusion
",2017,"we will also apply the tupa transition scheme to different target representations, including amr and sdp, exploring the limits of its generality."
p17-1105,"
5 Conclusion
",2017,"our results demonstrate their promise for tree prediction tasks, and we believe their application to more general output structures is an interesting avenue for future work."
p17-1106,"
6 Conclusion
",2017,"in the future, we plan to apply our approach to more nmt approaches (sutskever et al., 2014; shen et al., 2016; tu et al., 2016; wu et al., 2016) on more language pairs to further verify its effectiveness."
p17-1106,"
6 Conclusion
",2017,it is also interesting to develop relevancebased neural translation models to explicitly control relevance to produce better translations.
p17-1108,"
5 Conclusion and Future Work
",2017,"an appealing direction is to investigate the neural abstractive method on the multi-document summarization task, which is more challenging and lacks training data."
p17-1108,"
5 Conclusion and Future Work
",2017,there is lots of future work we can do.
p17-1109,"
9 Conclusions
",2017,"additionally, we have introduced several novel evaluation metrics for research in vowelsystem typology, which we hope will spark further interest in the area."
p17-1111,"
5 Conclusion
",2017,we also explore the neural network with few features using n-gram bi-lstms.
p17-1112,"
7 Conclusion
",2017,"we believe that there are many future avenues to explore to further increase the accuracy of such parsers, including different training objectives, more structured architectures and semisupervised learning."
p17-1113,"
6 Conclusion
",2017,"in the future work, we will replace the softmax function in the output layer with multiple classifier, so that a word can has multiple tags."
p17-1113,"
6 Conclusion
",2017,"in this paper, we propose a novel tagging scheme and investigate the end-to-end models to jointly extract entities and relations."
p17-1115,"
7 Conclusions and Future Work
",2017,future work may involve testing prewin on an ner task to see if and how it can generalise to a different classification task and how the results compare to the sota and similar methods such as that of collobert et al.(2011) using the conll 2003 ner datasets.
p17-1115,"
7 Conclusions and Future Work
",2017,"if it does perform better, this will be of considerable interest to classification research (and beyond) in nlp."
p17-1115,"
7 Conclusions and Future Work
",2017,the pressing new question is: 鈥淗ow much better the performance could have been if our method availed itself of the extra training data and resources used by previous works?鈥 indeed this may be the next research chapter for prewin.
p17-1116,"
7 Conclusion
",2017,"additionally, we plan to apply the proposed model to other social media analyses such as gender analysis and age analysis."
p17-1116,"
7 Conclusion
",2017,"as future works of this study, we are planning to expand the proposed model to handle multiple locations and a temporal state to capture location changes and states like traveling."
p17-1117,"
6 Conclusion
",2017,"in future work, we are applying our entailment-based multi-task paradigm to other directed language generation tasks such as image captioning and document summarization."
p17-1118,"
6 Conclusions and Future Work
",2017,"as this work is ongoing, we will keep collecting new transcriptions of the abcd retelling subtest to increase the corpus size and obtain more reliable results in our studies."
p17-1118,"
6 Conclusions and Future Work
",2017,"in future work, we intend to explore other methods to enrich cn, such as the recurrent language model, and use other metrics to characterize an adjacency network."
p17-1118,"
6 Conclusions and Future Work
",2017,"there is a clear need for publicly available datasets to compare different methods, which would optimize the detection of mci in elderly people."
p17-1118,"
6 Conclusions and Future Work
",2017,"this adaptation will enable large-scale applications in hospitals and facilitate the maintenance of application history in longitudinal studies, by storing the results in databases immediately after the test application."
p17-1118,"
6 Conclusions and Future Work
",2017,"to the best of our knowledge, these metrics have never been used to detect mci in speech transcripts; cn were enriched with word embeddings to better represent short texts produced in neuropsychological assessments."
p17-1120,"
6 Future Work
",2017,"although we used only text data to perform chat detection, we can also utilize contextual information such as the previous utterances (xu and sarikaya, 2014), the acoustic information (jiang et al., 2015), and the user profile (sano et al., 2016)."
p17-1120,"
6 Future Work
",2017,"an important future work is to develop a sophisticated dialogue manager to handle such utterances, for example, by making clarification questions (schloder and fernandez, 2015).篓 we manually investigated the dialogue acts in the chat detection dataset (c.f., section 3.2)."
p17-1120,"
6 Future Work
",2017,further efforts on improving nontask-oriented dialogue systems is an important future work.
p17-1120,"
7 Conclusion
",2017,"in addition, we investigated using the external resources, tweets and web search queries, to handle open-domain user utterances, which characterize the task of chat detection."
p17-1120,"
6 Future Work
",2017,incorporating these techniques into our methods is also an important future work.
p17-1120,"
6 Future Work
",2017,it is an interesting research topic to use such contextual information beyond text.
p17-1120,"
6 Future Work
",2017,it is considered promising to make use of a neural network for integrating such heterogeneous information.
p17-1120,"
6 Future Work
",2017,it is interesting to automatically determine the dialogue acts to help producing appropriate system responses.
p17-1120,"
6 Future Work
",2017,"some related studies exist in such a research direction (meguro et al., 2010)."
p17-1120,"
7 Conclusion
",2017,"to facilitate future research, we are going to release the dataset together with the feature values derived from the tweets and web search queries."
p17-1120,"
7 Conclusion
",2017,we hope that this study contributes to remove the long-standing boundary between task-oriented and non-task-oriented sds.
p17-1121,"
7 Conclusion and Future Work
",2017,"in future, we would like to include other sources of information in our model."
p17-1121,"
7 Conclusion and Future Work
",2017,"our initial plan is to include rhetorical relations, which has been shown to benefit existing grid models (feng et al., 2014)."
p17-1121,"
7 Conclusion and Future Work
",2017,"we would also like to extend our model to other forms of discourse, especially, asynchronous conversations, where participants communicate with each other at different times (e.g., forum, email)."
p17-1122,"
8 Conclusion
",2017,"finally, we believe that future work should be evaluated in situ, to examine if, and to what extent, the generated responses participate in and affect the discourse (feed) in social media."
p17-1122,"
8 Conclusion
",2017,"some future avenues for investigation include improving the relevance and human-likeness results by improving the automatic parses quality, acquiring more complex templates via abstract grammars, and experimenting with more sophisticated scoring functions for reranking."
p17-1122,"
8 Conclusion
",2017,"with the emergence of deep learning, we further embrace the opportunity to combine the sequence-to-sequence modeling view explored so far with conditioning generation on speakers agendas and user profiles, pushing the envelope of opinionated generation further."
p17-1123,"
7 Conclusion and Future Work
",2017,"besides this, it would also be interesting to consider to incorporate mechanisms for other language generation tasks (e.g., copy mechanism for dialogue generation) in our model to further improve the quality of generated questions."
p17-1123,"
7 Conclusion and Future Work
",2017,here we point out several interesting future research directions.
p17-1123,"
7 Conclusion and Future Work
",2017,we use an attentionbased neural networks approach for the task and investigate the effect of encoding sentence- vs. paragraph-level information.
p17-1123,"
7 Conclusion and Future Work
",2017,we would like to explore how to better use the paragraph-level information to improve the performance of qg system regarding questions of all categories.
p17-1124,"
6 Conclusion and Future Work
",2017,"as future work, we plan to investigate more sophisticated sampling strategies based on active learning and concept graphs to incorporate lexicalsemantic information for concept selection."
p17-1124,"
6 Conclusion and Future Work
",2017,"we also plan to look into ways to propagate feedback to similar and related concepts with partial feedback, to reduce the total amount of feedback."
p17-1125,"
6 Conclusions
",2017,future work involves investigating a better memory selection scheme.
p17-1125,"
6 Conclusions
",2017,"other regularization methods (e.g., norm or drop out) are also interesting and may alleviate the over-fitting problem"
p17-1126,"
5 Conclusion and Future Work
",2017,"in future work, we plan to apply our model to descriptions of time-series data in various domains such as weather forecasts and sports, which share the above writing-style characteristics."
p17-1126,"
5 Conclusion and Future Work
",2017,we also plan to use multiple time-series as input such as multiple brands of stock.
p17-1128,"
5 Conclusion
",2017,"in the future, we will study how to construct a taxonomy from texts in chinese."
p17-1128,"
5 Conclusion
",2017,we also discuss the potential applications of our method besides chinese hypernym prediction.
p17-1129,"
5 Conclusions and Future Work
",2017,another direction to explore is joint learning of syntactic parser and chain-of-trees lstm.
p17-1129,"
5 Conclusions and Future Work
",2017,"for future work, we will investigate the wider applicability of chain-of-trees lstm as a general text encoder that can simultaneously capture local syntactic structure and long-range semantic dependency."
p17-1129,"
5 Conclusions and Future Work
",2017,"it can be applied to named entity recognition, sentiment analysis, dialogue generation, to name a few."
p17-1129,"
5 Conclusions and Future Work
",2017,"we will also apply the tree-guided attention mechanism to nlp tasks that need syntaxaware attention, such as machine translation, sentence summarization, textual entailment, etc."
p17-1131,"
8 Conclusions
",2017,"conversation content: empathic counselors use reflective language and talk about behavior change, while less empathic counselors persuade more and focus on client resistance toward change."
p17-1131,"
8 Conclusions
",2017,"in the future, we plan to build upon the acquired knowledge and the developed classifiers to create automatic tools that provide accurate evaluative feedback of counseling practice."
p17-1132,"
5 Conclusion
",2017,our model can also be extended to integrate knowledge from a richer set of kbs in order to capture the diverse variety and depth of background knowledge required for accurate and deep language understanding.
p17-1132,"
5 Conclusion
",2017,"though our model is evaluated on entity extraction and event extraction, it can be useful for other machine reading tasks."
p17-1132,"
5 Conclusion
",2017,we see many additional opportunities to integrate background knowledge with training of neural network models for language processing.
p17-1133,"
6 Conclusions and Future Work
",2017,"promising future directions would be to investigate how to utilize user interaction in moocs for better prerequisite learning, as well as how deep learning models can be used to automatically learn useful features to help infer prerequisite relations."
p17-1134,"
6 Conclusion and Future Work
",2017,a more sophisticated approach would be to incorporate features into the unsupervised model.
p17-1134,"
6 Conclusion and Future Work
",2017,"applying the approach to our two illustrative applications of 搂1, patchwriting and literary analysis, would require development of relevant corpora."
p17-1134,"
6 Conclusion and Future Work
",2017,"for the literary analysis, as well, to bridge the gap between work like morzinski (1994) and a computational application, it remains to be seen how precise an annotation is possible for this task."
p17-1134,"
6 Conclusion and Future Work
",2017,the incorporation of these features for this segmentation task could be a potentially fruitful avenue for future work.
p17-1134,"
6 Conclusion and Future Work
",2017,"there isn鈥檛 yet a topic-balanced corpus like toefl11 which includes native speaker writing for evaluation, although we expect (given recent results on distinguishing native from nonnative text in malmasi and dras (2015)) that the techniques should carry over."
p17-1134,"
6 Conclusion and Future Work
",2017,we note also that the models are potentially applicable to other stylistic segmentation tasks beyond l1 influence.
p17-1136,"
4 Conclusion
",2017,"apart from it, we intend to propose a neural architecture that accomplishes the joint learning of lemmas with other morphological attributes."
p17-1136,"
4 Conclusion
",2017,"we measure the accuracy on the partial data (keeping the data size comparable to the bengali dataset) for hindi, latin and spanish to check the effect of the data amount on the performance."
p17-1137,"
8 Conclusion
",2017,"to further validate the performance of our model on different languages, we collected multilingual wikipedia corpus for 7 typologically diverse languages."
p17-1137,"
8 Conclusion
",2017,we will investigate a model which can marginalise word segmentation as latent variables in the future work.
p17-1138,"
6 Conclusion
",2017,"in future work, we would like to apply the presented non-linear bandit learners to other structured prediction tasks."
p17-1140,"
6 Conclusions
",2017,"in the future, we plan to validate the effectiveness of our model on more language pairs."
p17-1140,"
6 Conclusions
",2017,our model is convenient to be applied in the attention-based nmt and can be trained in the end-to-end style.
p17-1140,"
6 Conclusions
",2017,we also investigated the effect of hyper-parameters and pre-training strategy and further proved the stable effectiveness of our model.
p17-1141,"
6 Conclusion
",2017,"in future work, we hope to evaluate gbs with models outside of mt, such as automatic summarization, image captioning or dialog generation."
p17-1141,"
6 Conclusion
",2017,"in translation interfaces where translators can provide corrections to an existing hypothesis, these user inputs can be used as constraints, generating a new output each time a user fixes an error."
p17-1141,"
6 Conclusion
",2017,"we also hope to introduce new constraintaware models, for example via secondary attention mechanisms over lexical constraints."
p17-1142,"
7 Conclusion and Future Work
",2017,"another direction involves working to understand text in images, which can provide more information about the subjects of the images."
p17-1142,"
7 Conclusion and Future Work
",2017,exploring language through character modeling.
p17-1142,"
7 Conclusion and Future Work
",2017,future direction involves using graphical modeling to understand interactions in the scene.
p17-1142,"
7 Conclusion and Future Work
",2017,"in order to eliminate the need for retraining the word vectors as the language of the domain evolves, we plan to use character models to learn a better language model for trafficking."
p17-1142,"
7 Conclusion and Future Work
",2017,"to this end, we encourage the research community to reach out to cara jones, an author of this paper, to obtain a copy of trafficking-10k and other training data."
p17-1143,"
6 Conclusion
",2017,we hope that this paper and the accompanying database serve as a first step towards nlp being applied in cybersecurity and that other researchers will be inspired to contribute to the database and to construct their own datasets and implementations.
p17-1144,"
5 Conclusion and Future Work
",2017,"some potential augmentations include more fine-grained revision categories, revision properties such as statement strength (tan and lee, 2014) and quality evaluations, and sub-sentential revision scopes."
p17-1144,"
5 Conclusion and Future Work
",2017,we also plan to augment the corpus to support additional types of research on revision analysis.
p17-1144,"
5 Conclusion and Future Work
",2017,"while in this paper we explored language as one factor influencing rewriting behavior, our corpus also contains information about other potential factors such as gender and education level which we plan to investigate in the future."
p17-1146,"
7 Conclusion
",2017,"because our neural models are applicable to srl, applying our models for multilingual srl tasks presents an interesting future research direction."
p17-1146,"
7 Conclusion
",2017,"in future work, we plan to explore effective methods for exploiting large-scale unlabeled data to learn the neural models."
p17-1147,"
8 Conclusion and Future Work
",2017,"while not the focus of this paper, triviaqa also provides a provides a benchmark for a variety of other tasks such as ir-style question answering, qa over structured kbs and joint modeling of kbs and text, with much more data than previously available."
p17-1148,"
7 Future Work
",2017,"other document-level features, such as example input-output pairs, unit tests, might be useful in this endeavor."
p17-1148,"
7 Future Work
",2017,we also see these resources as useful for investigations into natural language programming.
p17-1148,"
7 Future Work
",2017,"we believe that good performance on our datasets should lead to better performance on more conventional semantic parsing tasks, and raise new challenges involving sparsity and multilingual learning."
p17-1148,"
7 Future Work
",2017,"while our experiments look at learning rudimentary translational correspondences between text and code, a next step might be learning to synthesize executable programs via these translations, along the lines of (desai et al., 2016; raza et al., 2015)."
p17-1149,"
7 Conclusions and Future Work
",2017,"in the future, we will improve the scalability of our model and learn multi-prototype embeddings for the mentions without reference entities in a knowledge base, and introduce compositional approaches to model the internal structures of multiword mentions."
p17-1150,"
6 Conclusion
",2017,future work can explore deep neural network to alleviate feature engineering.
p17-1150,"
6 Conclusion
",2017,human partners who work side-by-side with these cognitive robots are great resources that the robots can directly learn from.
p17-1150,"
6 Conclusion
",2017,one of the important questions to address in the future is how to learn new predicates through interaction with humans.
p17-1150,"
6 Conclusion
",2017,our future work will incorporate interactive learning of verb semantics with task learning to enable autonomy that can learn by communicating with humans.
p17-1150,"
6 Conclusion
",2017,"recent years have seen an increasing amount of work on task learning from human partners (saunders et al., 2006; chernova and veloso, 2008; cantrell et al., 2012; mohan et al., 2013; asada et al., 2009; mohseni-kabir et al., 2015; nejati et al., 2006; liu et al., 2016)."
p17-1151,"
5 Discussion
",2017,"alternatively, we could imagine a dependent mixture model where the distributions over words are evolving with time and other covariates."
p17-1151,"
5 Discussion
",2017,"in the future, multimodal word distributions could open the doors to a new suite of applications in language modelling, where whole word distributions are used as inputs to new probabilistic lstms, or in decision functions where uncertainty matters."
p17-1151,"
5 Discussion
",2017,it would also be informative to explore inference over the number of components in mixture models for word distributions.
p17-1151,"
5 Discussion
",2017,"one could also build new types of supervised language models, constructed to more fully leverage the rich information provided by word distributions."
p17-1151,"
5 Discussion
",2017,"such an approach could potentially discover an unbounded number of distinct meanings for words, but also distribute the support of each word distribution to express highly nuanced meanings."
p17-1152,"
6 Conclusions and Future Work
",2017,"future work interesting to us includes exploring the usefulness of external resources such as wordnet and contrasting-meaning embedding (chen et al., 2015) to help increase the coverage of wordlevel inference relations."
p17-1153,"
6 Conclusion
",2017,future work includes expanding the analyses to non-english movies and combining the linguistic metrics with character networks.
p17-1153,"
6 Conclusion
",2017,"specifically, character network edges can be weighted using the psycholinguistic metrics to analyze the emotional patterns in inter-character interactions."
p17-1154,"
6 Conclusion and Future Work
",2017,"as future work, we plan to apply the linguistic regularizers to tree-lstm to address the scope issue since the parsing tree is easier to indicate the modification scope explicitly."
p17-1155,"
8 Discussion and Future Work
",2017,consider the tweet 鈥滳an you imagine if lebron had help?#sarcasm鈥.
p17-1155,"
8 Discussion and Future Work
",2017,designing automatic measures is hence left for future research.
p17-1155,"
8 Discussion and Future Work
",2017,"finally, tweets that present sentiment in phrases or slang words are particularly challenging for our approach which relies on the identification and clustering of sentiment words."
p17-1155,"
8 Discussion and Future Work
",2017,future research directions rise from cases in which the sign models left the tweet unchanged.
p17-1155,"
8 Discussion and Future Work
",2017,"likewise, the human fluency measure that accounts for the readability of the interpretation is not seriously affected by the translation process."
p17-1155,"
8 Discussion and Future Work
",2017,"sarcasm interpretation as sentiment based monolingual mt: strengths and weaknesses the sign models鈥 strength is revealed when interpreting sarcastic tweets with strong sentiment words, transforming expressions such as 鈥滱udits are a blast to do #sarcasm鈥 and 鈥滲eing stuck in an airport is fun #sarcasm鈥 into 鈥滱udits are a bummer to do鈥 and 鈥滲eing stuck in an airport is boring鈥, respectively."
p17-1155,"
8 Discussion and Future Work
",2017,several challenges are still to be addressed in future research so that sarcasm interpretation can be performed in a fully automatic manner.
p17-1155,"
8 Discussion and Future Work
",2017,the model requires knowledge of who lebron is and what kind of help he needs in order to fully understand and interpret the sarcasm.
p17-1155,"
8 Discussion and Future Work
",2017,these include the design of appropriate automatic evaluation measures as well as improving the algorithmic approach so that it can take world knowledge into account and deal with cases where the sentiment of the input tweet is not expressed with a clear sentiment words.
p17-1155,"
8 Discussion and Future Work
",2017,we hope this new resource will help researchers make further progress on this new task.
p17-1155,"
8 Discussion and Future Work
",2017,"while we believe that identifying the role of can鈥檛 wait and of totally in the sentiment of the above tweets can be a key to properly interpreting them, our approach that relies on a sentiment word lexicon is challenged by such cases."
p17-1156,"
5 Conclusion
",2017,"in addition, we extract domain-specific sentiment similarities among words from unlabeled samples of target domain based on both syntactic rules and cooccurrence patterns, and incorporate them into the domain adaptation process to propagate the general sentiment information to many domain-specific sentiment words in target domain."
p17-1157,"
7 Conclusion
",2017,"we traced this to the size of the available data in each sector, and show that there are still benefits in considering sectors, which could be further explored in the future as more data becomes available."
p17-1158,"
6 Conclusion and Future Work
",2017,"furthermore, there usually exist explicit relations in social networks (e.g., families, friends and colleagues relations between social network users), which are expected to be critical to ne."
p17-1158,"
6 Conclusion and Future Work
",2017,"in future, we will strive to implement cane on a wider variety of information networks with multi-modal data, such as labels, images and so on.(2) cane encodes latent relations between vertices into their context-aware embeddings."
p17-1158,"
6 Conclusion and Future Work
",2017,"thus, we want to explore how to incorporate and predict these explicit relations between vertices in ne."
p17-1158,"
6 Conclusion and Future Work
",2017,we will explore the following directions in future: (1) we have investigated the effectiveness of cane on text-based information networks.
p17-1159,"
7 Conclusion
",2017,possible future work include expanding the investigation to other regional languages such as malay and indonesian.
p17-1160,"
Future work
",2017,an interesting avenue for future work is to explore higher order factorizations for noncrossing digraphs and the related inference.
p17-1160,"
Future work
",2017,"we are planning to extend the coverage of the approach by exploring 1-endpointcrossing and mhk trees (pitler et al., 2013; gomez-rodr 麓 麓谋guez, 2016), and related digraphs 鈥 see (yli-jyra篓, 2004; gomez-rodr 麓 麓谋guez et al., 2011)."
p17-1160,"
Future work
",2017,we would also like to have more insight on the transformation of mso definable properties to the current framework and to logspace algorithms.
p17-1163,"
7 Conclusion
",2017,"in future work, we intend to explore applications of the nbt for multi-domain dialogue systems, as well as in languages other than english that require handling of complex morphological variation."
p17-1165,"
5 Discussion
",2017,"as regards complexity, it is true that more complex models, as the one we are considering, are more prone to underfitting (when data is scarce) and overfitting than simpler models."
p17-1165,"
5 Discussion
",2017,"in the future, we plan on relying on other inference approaches, based for example on variational bayes known to yield better estimates for perplexity (asuncion et al., 2009); it is however not certain that the gain in perplexity one can expect from the use of variational bayes approaches will necessarily result in a gain in, say, topic coherence."
p17-1165,"
5 Discussion
",2017,"the approach requires external annotated data to train the segmentation models, where certain domain and language specific information need to be captured."
p17-1165,"
5 Discussion
",2017,"the coherence between topics is ensured through frank鈥檚 copula, that binds the topics associated to the words of a segment."
p17-1165,"
5 Discussion
",2017,the comparison with other segmentation methods is also an important point.
p17-1166,"
5 Conclusion and Future Works
",2017,"in the future, we will focus on two aspects: (1) our method in this paper considers pairwise intersections between labels, so to better exploit class ties, we will extend our method to exploit all other labels鈥 influences on each relation for relation extraction, transferring second-order to high-order (zhang and zhou, 2014); (2) we will focus on other problems by leveraging class ties between labels, specially on multi-label learning problems (zhou et al., 2012) such as multi-category text categorization (rousu et al., 2005) and multi-label image categorization (zha et al., 2008)."
p17-1167,"
6 Conclusion & Future Work
",2017,"because of the dynamic nature of our framework, it is not trivial to leverage the computational capabilities of gpus using minibatched training; we plan to investigate ways to take full advantage of modern computing machinery in the near future."
p17-1167,"
6 Conclusion & Future Work
",2017,"finally, better resolution of semantic matching errors is a top priority, and unsupervised learning from large external corpora is one way to make progress in this direction."
p17-1167,"
6 Conclusion & Future Work
",2017,"for instance, although our current formal language design covers most question types in sqa, it is nevertheless important to extend it further to make the semantic parser more robust (e.g., by including union or allowing comparison of multiple previous answers)."
p17-1167,"
6 Conclusion & Future Work
",2017,"in the future, we plan to investigate several interesting research questions triggered by this work."
p17-1169,"
6 Conclusions and Future Work
",2017,"a more appealing direction is to develop problem-driven methods to represent a document as a distributional entity, taking into consideration of phrases, sentence structures, and syntactical characteristics."
p17-1169,"
6 Conclusions and Future Work
",2017,it would also be interesting to investigate several possible extensions to the current clustering work.
p17-1169,"
6 Conclusions and Future Work
",2017,its ease of use enables data scientists to apply it for the pre-screening purpose of examining word embeddings in a specific task.
p17-1169,"
6 Conclusions and Future Work
",2017,one direction is to learn a proper ground distance for word embeddings such that the final document clustering performance can be improved with labeled data.
p17-1169,"
6 Conclusions and Future Work
",2017,we believe the framework of wasserstein distance and d2-clustering creates room for further investigation on complex structures and knowledge carried by documents.
p17-1170,"
7 Conclusion and Future Work
",2017,"also, given the promising results observed for supersenses, we plan to investigate taskspecific coarsening of sense inventories, particularly wikipedia, or the use of sentiwordnet (baccianella et al., 2010), which could be more suitable for polarity detection."
p17-1170,"
7 Conclusion and Future Work
",2017,"as future work, we plan to investigate the extension of the approach to other languages and applications."
p17-1170,"
7 Conclusion and Future Work
",2017,we hope that our work will foster future research on the integration of senselevel knowledge into downstream applications.
p17-1171,"
6 Conclusion
",2017,future work should aim to improve over our drqa system.
p17-1174,"
6 Conclusion
",2017,"in addition, we plan to combine our decoder with other encoders that capture language structure, such as a hierarchical rnn (luong and manning, 2016), a tree-lstm (eriguchi et al., 2016b), or an order-free encoder, such as a cnn (kalchbrenner and blunsom, 2013)."
p17-1174,"
6 Conclusion
",2017,"in future work, we will explore the optimal structures of chunk-based decoder for other free word-order languages such as czech, german, and turkish."
p17-1174,"
6 Conclusion
",2017,we utilize the chunk structure to efficiently capture long-distance dependencies and cope with the problem of free word-order languages such as japanese.
p17-1175,"
7 Conclusions and Future Work
",2017,"in the future, we will incorporate coverage into our model and study how to apply it to other natural language processing tasks."
p17-1176,"
6 Conclusion
",2017,"in the future, we plan to test our approach on more diverse language pairs, e.g., zero-resource uyghur-english translation using chinese as a pivot."
p17-1176,"
6 Conclusion
",2017,it is also interesting to extend the teacherstudent framework to other cross-lingual nlp applications as our method is transparent to architectures.
p17-1176,"
6 Conclusion
",2017,"we also analyze zero-resource translation with small source-pivot data, and combine our wordlevel sampling method with initialization and parameter freezing suggested by (zoph et al., 2016)."
p17-1177,"
6 Conclusion
",2017,"for future work, it will be interesting to make use of node labels from the tree, or to use syntactic information on the target side, as well."
p17-1178,"
6 Conclusions and Future Work
",2017,"in the future, we will explore the topological structure of related languages and exploit cross-lingual knowledge transfer to enhance the quality of extraction and linking."
p17-1178,"
6 Conclusions and Future Work
",2017,the general idea of deriving noisy annotations from kb properties can also be extended to other ie tasks such as relation extraction.
p17-1179,"
6 Conclusion
",2017,"future work also includes investigating other divergences that adversarial training can minimize (nowozin et al., 2016), and broader mathematical tools that match distributions (mohamed and lakshminarayanan, 2016)."
p17-1179,"
6 Conclusion
",2017,our work also opens up opportunities for the processing of extremely low-resource languages and domains that lack parallel data completely.
p17-1180,"
8 Conclusion and Future Work
",2017,"as future directions, we plan to extend gwld to several other languages and conduct similar sociolinguistic studies on cs patterns including not only more languages and geographies, but also other aspects like topic and sentiment."
p17-1181,"
5 Conclusions
",2017,cognates detection is an interesting and challenging task.
p17-1182,"
8 Future Work
",2017,"in the future, we want to develop methods to make better use of languages with different alphabets or morphosyntactic features, in order to increase the applicability of our knowledge transfer method."
p17-1183,"
7 Conclusion
",2017,"future work may include applying our model to other nearly-monotonic alignand-transduce tasks like abstractive summarization, transliteration or machine translation."
p17-1184,"
6 Conclusion
",2017,"however, these results do not generalize to all languages, since factors such as morphology and orthography affect the utility of these representations."
p17-1184,"
6 Conclusion
",2017,we plan to explore these effects in future work.
p17-1185,"
7 Conclusions
",2017,possible direction of future work is to apply more advanced optimization techniques to the step 1 of the scheme proposed in section 1 and to explore the step 2 鈥 obtaining embeddings with a given low-rank matrix.
p17-1186,"
6 Conclusion
",2017,"because our techniques apply to labeled directed graphs in general, they can easily be extended to incorporate more formalisms, semantic or otherwise."
p17-1186,"
6 Conclusion
",2017,in future work we hope to explore cross-task scoring and inference for tasks where parallel annotations are not available.
p17-1187,"
5 Conclusion and Future Work
",2017,we will explore the effectiveness of sememe information for wrl in other languages.
p17-1187,"
5 Conclusion and Future Work
",2017,"we will explore the following research directions in future: (1) the sememe information in hownet is annotated with hierarchical structure and relations, which have not been considered in our framework."
p17-1187,"
5 Conclusion and Future Work
",2017,we will explore to utilize these annotations for better wrl.(2) we believe the idea of sememes is universal and could be wellfunctioned beyond languages.
p17-1188,"
7 Conclusion and Future Work
",2017,"in the future, we hope to further generalize this method to other tasks such as pronunciation estimation, which can take advantage of the fact that pronunciation information is encoded in parts of the characters as demonstrated in fig.1, or machine translation, which could benefit from a wholistic view that considers both semantics and pronunciation."
p17-1188,"
7 Conclusion and Future Work
",2017,"we also hope to apply the model to other languages with complicated compositional writing systems, potentially including historical texts such as hieroglyphics or cuneiform."
p17-1189,"
7 Conclusion and Future Work
",2017,"so in the future, we might try to combine more heterogeneous semantic data for other tasks like event extraction and relation classification, etc."
p17-1189,"
7 Conclusion and Future Work
",2017,we believe that progressive learning with heterogeneous data is a promising avenue to pursue.
p17-1189,"
7 Conclusion and Future Work
",2017,we hope that it will be helpful in providing common benchmarks for future work on chinese srl tasks.
p17-1190,"
6 Conclusion
",2017,"future work will explore additional data sources, including from aligning different translations of novels (barzilay and mckeown, 2001), aligning new articles of the same topic (dolan et al., 2004), or even possibly using machine translation systems to translate bilingual text into paraphrastic sentence pairs."
p17-1190,"
6 Conclusion
",2017,"our new techniques, combined with the promise of new data sources, offer a great deal of potential for improved universal paraphrastic sentence embeddings."
p17-1190,"
6 Conclusion
",2017,we also investigated the gran in order to better understand the compositional phenomena it was learning by analyzing the l1 norm of its gate over various inputs.
p17-1191,"
6 Conclusion
",2017,"moreover, the methods described in this paper can be extended to other kinds of structured knowledge sources like freebase which may be more suitable for tasks like question answering."
p17-1191,"
6 Conclusion
",2017,this approach may be extended to other nlp tasks that can benefit from using encoders that can access wordnet information.
p17-1193,"
6 Conclusion and Future Work
",2017,we leave this for future investigation.
p17-1194,"
9 Conclusion
",2017,"at the same time, both of these are also combined, in order to predict the most probable label for each word."
p17-1194,"
9 Conclusion
",2017,future work could investigate the extension of this architecture to additional unannotated resources.
p17-1194,"
9 Conclusion
",2017,"the language modeling objective also provided consistent improvements on other sequence labeling tasks, such as named entity recognition, chunking and pos-tagging."
p17-1194,"
9 Conclusion
",2017,the model is incentivised to discover useful features in order to learn the language distribution and composition patterns in the training data.
p17-1194,"
9 Conclusion
",2017,"while it is common to pretrain word embeddings on large-scale unannotated corpora, only limited research has been done towards useful methods for pre-training or cotraining more advanced compositional modules."
p17-1195,"
10 Conclusion
",2017,our future work includes the expansion of the lexicon with the aid of the semantic parser and the development of a disambiguation model for the binding and scoping structures.
p18-1001,"
6 Conclusion and Future Work
",2018,"future work includes an investigation into the trade-off between learning full covariance matrices for each word distribution, computational complexity, and performance."
p18-1001,"
6 Conclusion and Future Work
",2018,other future work involves co-training pft on many languages.
p18-1001,"
6 Conclusion and Future Work
",2018,"this direction can potentially have a great impact on tasks where the variance information is crucial, such as for hierarchical modeling with probability distributions (athiwaratkun and wilson, 2018)."
p18-1002,"
6 Conclusion
",2018,"a natural and principled integration of recent ideas for composing word vectors, the approach achieves strong performance on several tasks and promises to be useful in many linguistic settings and to yield many further research directions."
p18-1002,"
6 Conclusion
",2018,"both in this area and for n-grams there is great scope for combining our approach with compositional approaches (bojanowski et al., 2016; poliak et al., 2017) that can handle settings such as zero-shot learning."
p18-1002,"
6 Conclusion
",2018,"extensions of the mathematical formulation, such as the use of word weighting when building context vectors as in arora et al.(2018b) or of spectral information along the lines of mu and viswanath (2018), are also worthy of further study."
p18-1002,"
6 Conclusion
",2018,"finally, there remain many language features, such as named entities and morphological forms, whose representation by our method remains unexplored."
p18-1002,"
6 Conclusion
",2018,"more practically, the contextual rare words (crw) dataset we provide will support research on few-shot learning of word embeddings."
p18-1002,"
6 Conclusion
",2018,"more work is needed to understand the usefulness of our method for representing (potentially cross-lingual) entities such as synsets, whose embeddings have found use in enhancing wordnet and related knowledge bases (camachocollados et al., 2016; khodak et al., 2017)."
p18-1002,"
6 Conclusion
",2018,"of particular interest is the replacement of simple window contexts by other structures, such as dependency parses, that could yield results in domains such as question answering or semantic role labeling."
p18-1004,"
6 Conclusion
",2018,"in future work, we will investigate explicit retrofitting methods for asymmetric relations like hypernymy and meronymy."
p18-1004,"
6 Conclusion
",2018,we also intend to apply the method to other downstream tasks and to investigate the zero-shot language transfer of the specialization function for more language pairs.
p18-1005,"
5 Conclusion and Future work
",2018,"besides, we decide to make more efforts to explore how to reinforce the temporal order information for the proposed model."
p18-1005,"
5 Conclusion and Future work
",2018,"in the future, we would like to investigate how to utilize the monolingual data more effectively, such as incorporating the language model and syntactic information into unsupervised nmt."
p18-1005,"
5 Conclusion and Future work
",2018,unsupervised nmt opens exciting opportunities for the future research.
p18-1006,"
5 Conclusion
",2018,"by introducing another rich language, our method can better exploit the additional language pairs to enrich the original low-resource pair."
p18-1006,"
5 Conclusion
",2018,"in the future, we may extend our architecture to other scenarios, such as totally unsupervised training with no bilingual data for the rare language."
p18-1007,"
6 Conclusion
",2018,"additionally, we would like to explore the application of subword regularization for machine learning, including denoising auto encoder (vincent et al., 2008) and adversarial training (goodfellow et al., 2015)"
p18-1007,"
6 Conclusion
",2018,"promising avenues for future work are to apply subword regularization to other nlp tasks based on encoder-decoder architectures, e.g., dialog generation (vinyals and le, 2015) and automatic summarization (rush et al., 2015)."
p18-1008,"
7 Conclusion
",2018,"our focus on a standard single-language-pair translation task leaves important open questions to be answered: how do our new architectures compare in multilingual settings, i.e., modeling an interlingua?"
p18-1008,"
7 Conclusion
",2018,"we hope that our work will motivate nmt researchers to further investigate generally applicable training and optimization techniques, and that our exploration of hybrid architectures will open paths for new architecture search efforts for nmt."
p18-1009,"
8 Conclusion
",2018,"these results set the first performance levels for our evaluation dataset, and suggest that the data will support significant future work."
p18-1010,"
8 Conclusion
",2018,"while this work already demonstrates considerable improvement over non-hierarchical modeling, future work will explore techniques such as box embeddings (vilnis et al., 2018) and poincare麓 embeddings (nickel and kiela, 2017) to represent the hierarchical embedding space, as well as methods to improve recall in the candidate generation process for entity linking."
p18-1011,"
5 Conclusion
",2018,"two types of constraints have been studied: (i) the non-negativity constraints to learn compact, interpretable entity representations, and (ii) the approximate entailment constraints to further encode logical regularities into relation representations."
p18-1012,"
7 Conclusion
",2018,we have also explored the relationship between kg embedding geometry and its task performance.
p18-1014,"
7 Conclusion
",2018,swap-net models this interaction using a new two-level pointer network based architecture with a switching mechanism.
p18-1015,"
5 Conclusion and Future Work
",2018,"on the one hand, since the candidate templates are far inferior to the optimal ones, we intend to improve the retrieve module, e.g., by indexing both the sentence and summary fields."
p18-1015,"
5 Conclusion and Future Work
",2018,"on the other hand, we plan to test our system on the other tasks such as document-level summarization and short text conversation."
p18-1015,"
5 Conclusion and Future Work
",2018,then we extend the seq2seq framework to jointly conduct template reranking and template-aware summary generation.
p18-1015,"
5 Conclusion and Future Work
",2018,we believe our work can be extended in various aspects.
p18-1016,"
8 Conclusion
",2018,future work will leverage ucca鈥檚 cross-linguistic applicability to support multi-lingual ts and ts pre-processing for mt.
p18-1017,"
8 Conclusions
",2018,we used best鈥搘orst scaling to obtain finegrained scores (and word rankings) and addressed issues of annotation consistency that plague traditional rating scale methods of annotation.
p18-1018,"
7 Conclusion
",2018,"we expect that future work will develop methods to scale the annotation process beyond requiring highly trained experts; bring this scheme to bear on other languages; and investigate the relationship of our scheme to more structured semantic representations, which could lead to more robust models."
p18-1020,"
6 Conclusions
",2018,the significant gains demonstrated suggests easl as a promising approach for future dataset curation and system evaluation in the community.
p18-1022,"
6 Conclusion
",2018,"these are only the most salient things that need be done to tackle the problem, and as our cross-section of related work shows, a large body of work must be covered."
p18-1023,"
7 Conclusion
",2018,"to obtain further insights into the nature of counterarguments, deeper linguistic analysis along with supervised learning may be needed, though."
p18-1023,"
7 Conclusion
",2018,"we did not aim to engineer the best approach to this retrieval task, but to study whether we can model the simultaneous similarity and dissimilarity of a counterargument to an argument computationally."
p18-1023,"
7 Conclusion
",2018,"we provide a corpus to train respective approaches, but leave the according research to future work."
p18-1023,"
7 Conclusion
",2018,"while the model can be considered open-topic, a next step will be to study counterargument retrieval open-source."
p18-1024,"
7 Concluding Remarks and Future Work
",2018,"a straightforward approach is to create a unified dataset out of more than two graphs by combining set of triplets as described in section 2, and apply learning and inference on the unified graph without any major change in the methodology."
p18-1024,"
7 Concluding Remarks and Future Work
",2018,"for future work, we would like to extend the current evaluation of our work from a two-graph setting to multiple graphs."
p18-1024,"
7 Concluding Remarks and Future Work
",2018,"however, we point out that our method is not restricted to such use cases鈥攐ne can readily apply our method to directly make inference over multiple graphs to support applications like question answering and conversations."
p18-1024,"
7 Concluding Remarks and Future Work
",2018,"in real-world setting, we envision our method to be integrated in a large scale system that would include various other components for tasks like conflict resolution, active learning and human-in-loop learning to ensure quality of constructed super-graph."
p18-1024,"
7 Concluding Remarks and Future Work
",2018,many data driven organizations such as google and microsoft take the approach of constructing a unified super-graph by integrating data from multiple sources.
p18-1024,"
7 Concluding Remarks and Future Work
",2018,we believe that this work opens a new research direction in joint representation learning over multiple knowledge graphs.
p18-1025,"
6 Conclusion and Future Work
",2018,an exciting direction is the incorporation of multi-relational data for general knowledge representation and inference.
p18-1025,"
6 Conclusion and Future Work
",2018,"secondly, more complex representations, such as 2n-dimensional products of 2-dimensional convex polyhedra, would offer greater flexibility in tiling event space."
p18-1026,"
7 Discussion and Conclusion
",2018,"a better approach would be to allow the encoder to have a dynamic number of layers, possibly based on the diameter (longest path) in the input graph."
p18-1026,"
7 Discussion and Conclusion
",2018,"an interesting alternative is weave module networks (kearnes et al., 2016), which explicitly decouples node and edge representations without incurring in parameter explosion."
p18-1026,"
7 Discussion and Conclusion
",2018,incorporating both ideas to our architecture is an research direction we plan for future work.
p18-1026,"
7 Discussion and Conclusion
",2018,"the first one is that ggnns have a fixed number of layers, even though graphs can vary in size in terms of number of nodes and edges."
p18-1026,"
7 Discussion and Conclusion
",2018,we believe this is an interesting research direction in terms of applications.
p18-1027,"
8 Conclusion
",2018,"while observations in this paper are reported at the token level, deeper understanding of sentence-level interactions warrants further investigation, which we leave to future work."
p18-1028,"
9 Conclusion
",2018,"as a simple version of an rnn, which is more expressive than one-layer cnns, we hope that sopa will encourage future research on the bridge between these two mechanisms."
p18-1028,"
9 Conclusion
",2018,"it naturally models flexible-length spans with insertion and deletion, and it can be easily customized by swapping in different semirings."
p18-1029,"
6 Discussion and Future Work
",2018,"future work can model how these parameters can be adapted in a task specific way (e.g., cases such as cancer prediction where base rates are small), and provide better models of quantifier semantics.e.g., as distributions, rather than point values."
p18-1029,"
6 Discussion and Future Work
",2018,"we believe that language may have as much to help learning, as statistical learning has helped nlp."
p18-1030,"
5 Conclusion
",2018,"next directions also include the investigation of s-lstm to more nlp tasks, such as machine translation."
p18-1030,"
5 Conclusion
",2018,we leave such investigation to future work.
p18-1031,"
6 Discussion and future directions
",2018,another direction is to apply the method to novel tasks and models.
p18-1031,"
6 Discussion and future directions
",2018,"given that transfer learning and particularly fine-tuning for nlp is under-explored, many future directions are possible."
p18-1031,"
6 Discussion and future directions
",2018,"language modeling can also be augmented with additional tasks in a multi-task learning fashion (caruana, 1993) or enriched with additional supervision, e.g.syntax-sensitive dependencies (linzen et al., 2016) to create a model that is more general or better suited for certain downstream tasks, ideally in a weakly-supervised manner to retain its universal properties."
p18-1031,"
6 Discussion and future directions
",2018,"one possible direction is to improve language model pretraining and fine-tuning and make them more scalable: for imagenet, predicting far fewer classes only incurs a small performance drop (huh et al., 2016), while recent work shows that an alignment between source and target task label sets is important (mahajan et al., 2018)鈥攆ocusing on predicting a subset of words such as the most frequent ones might retain most of the performance while speeding up training."
p18-1031,"
6 Discussion and future directions
",2018,"while an extension to sequence labeling is straightforward, other tasks with more complex interactions such as entailment or question answering may require novel ways to pretrain and fine-tune."
p18-1032,"
7 Summary
",2018,"to conduct this study, we introduced evaluation paradigms for explanation methods for two classes of nlp tasks, small context tasks (e.g., topic classification) and large context tasks (e.g., morphological prediction)."
p18-1033,"
6 Conclusion
",2018,developers of future large-scale datasets should incorporate joins and nesting to create more human-like data.
p18-1033,"
6 Conclusion
",2018,evaluating on multiple datasets is necessary to ensure coverage of the types of questions humans generate.
p18-1033,"
6 Conclusion
",2018,our analysis has clear implications for future work.
p18-1034,"
6 Conclusion and Future Work
",2018,"in the future, we plan to improve the accuracy of the column prediction component."
p18-1034,"
6 Conclusion and Future Work
",2018,we also plan to build a large-scale dataset that considers more sophisticated sql queries.
p18-1034,"
6 Conclusion and Future Work
",2018,"we also plan to extend the approach to low-resource scenarios (feng et al., 2018)."
p18-1035,"
10 Conclusion
",2018,"future work will investigate whether a single algorithm and architecture can be competitive on all of these parsing tasks, an important step towards a joint many-task model for semantic parsing."
p18-1036,"
7 Conclusion
",2018,"for agglutinative languages, their performance is limited on data with rich derivational morphology and high contextual ambiguity (morphological disambiguation); and for fusional languages, they struggle on tokens with high number of morphological tags, 鈥 similarity between character and morphology-level models is higher than the similarity within character-level (char and char-trigram) models on languages with high oov%; and vice versa, 鈥 their ability to handle long-range dependencies is very similar to morphology-level models, 鈥 they rely relatively more on training data size."
p18-1036,"
7 Conclusion
",2018,we evaluated their quality on semantic role labeling in a number of agglutinative and fusional languages.
p18-1037,"
6 Conclusions
",2018,another promising direction is integrating character seq2seq to substitute the copy function.
p18-1037,"
6 Conclusions
",2018,"we believe that the proposed approach may be extended to other parsing tasks where alignments are latent (e.g., parsing to logical form (liang, 2016))."
p18-1039,"
8 Conclusion
",2018,"in addition, we plan to expand the coverage of our meaning representation to support more mathematic concepts."
p18-1039,"
8 Conclusion
",2018,"in the future, we will focus on tackling this challenge."
p18-1040,"
8 Conclusions
",2018,"in the future, we plan to model document-level representations which are more in line with drt and the gmb annotations."
p18-1042,"
8 Conclusion
",2018,our procedure is immediately applicable to the wide range of languages for which we have parallel text.
p18-1042,"
8 Conclusion
",2018,"we are actively investigating ways to apply these two new resources to downstream applications, including machine translation, question answering, and additional paraphrase generation tasks."
p18-1042,"
8 Conclusion
",2018,"we hope that paranmt-50m, along with our embeddings, can impart a notion of meaning equivalence to improve nlp systems for a variety of tasks."
p18-1042,"
8 Conclusion
",2018,"we release paranmt-50m, our code, and pretrained sentence embeddings, which also exhibit strong performance as general-purpose representations for a multitude of tasks."
p18-1044,"
6 Conclusion
",2018,"in the future, we will apply this semi-supervised training method to other nlp tasks."
p18-1044,"
6 Conclusion
",2018,"the generator neural network learns japanese pas and selectional preferences, while the validator is trained against the generator errors."
p18-1044,"
6 Conclusion
",2018,this validator enables the generator to be trained from raw corpora and enhance it with external knowledge.
p18-1045,"
6 Conclusions and the Future Work
",2018,"in the future, we will explore the use of additional discourse structures that correlate highly with event coreference chains."
p18-1045,"
6 Conclusions and the Future Work
",2018,"moreover, we will extend this work to other domains such as biomedical domains."
p18-1045,"
6 Conclusions and the Future Work
",2018,"we have presented an ilp based joint inference system for event coreference resolution that utilizes scores predicted by a pairwise event coreference resolution classifier, and models several aspects of correlations between event coreference chains and document level topic structures, including the correlation between the main event chains and topic transition sentences, interdependencies among event coreference chains, genre-specific coreferent mention distributions and subevents."
p18-1046,"
5 Conclusion
",2018,"with adversarial training, our goal is to gradually decrease the performance of the discriminator, while the generator improves the performance for predicting true positives when reaching equilibrium."
p18-1047,"
Conclusions and Future Work
",2018,another future work is test our model in other nlp tasks like event extraction.
p18-1047,"
Conclusions and Future Work
",2018,our future work will concentrate on how to improve the performance further.
p18-1047,"
Conclusions and Future Work
",2018,"our model can jointly extract relation and entity from sentences, especially when triplets in the sentences are overlapped."
p18-1048,"
7 Conclusion
",2018,"besides, the global features extracted in li et al (2013)鈥檚 work are potentially useful for detecting the event instances referred by pronouns, although involve noises."
p18-1048,"
7 Conclusion
",2018,"considering this possibility, we suggest to drive the two discriminators in our self-regulation framework to cooperate with each other."
p18-1048,"
7 Conclusion
",2018,"in this study, the performance of the discriminator in the adversarial network is left to be evaluated."
p18-1048,"
7 Conclusion
",2018,"therefore, in the future, we will encode the global information by neural networks and use the self-regulation strategy to reduce the negative influence of noises."
p18-1050,"
7 Conclusions
",2018,"for the future work, we plan to expand event temporal knowledge acquisition by dealing with event sense disambiguation and event synonym identification (e.g., drag, pull and haul)."
p18-1051,"
5 Conclusion
",2018,"in future work, we intend to thoroughly study the impact of tds given morphosyntactic information."
p18-1052,"
7 Conclusion
",2018,our future goal is to use the coherence model to generate new conversations.
p18-1053,"
5 Conclusion
",2018,"besides, to deal with the problematic scenario when ground truth parse tree and anaphoric zero pronoun are unavailable, we are interested in exploring the neural network model that integrates the anaphoric zero pronoun determination and anaphoric zero pronoun resolution jointly in a hierarchical architecture without using parser or anaphoric zero pronoun detector."
p18-1053,"
5 Conclusion
",2018,"in the future, we plan to explore neural network models for efficaciously resolving anaphoric zero pronoun documents and research on some specific components which might influence the performance of the model, such as the embedding."
p18-1053,"
5 Conclusion
",2018,"meanwhile, we plan to research on the possibility of applying adversarial learning (goodfellow et al., 2014) to generate better rewards than the human-defined reward functions."
p18-1055,"
5 Conclusion
",2018,we are now working on an a* mg parser which can consider the full distribution of supertags for each word and exploit the potential of these rich lexical categories.
p18-1056,"
5 Conclusion
",2018,"therefore, we suggest that future work on social power and language use should consider other (maybe higher-level) linguistic elements."
p18-1056,"
5 Conclusion
",2018,"we call for the inclusion of a wider range of factors in future studies of social influences on language use, especially low-level but interpretable cognitive factors."
p18-1057,"
7 Conclusion and Future Work
",2018,for future work we plan to continue the collection and annotation of resources and to separately explore each of the above research tasks.
p18-1058,"
5 Conclusion
",2018,we believe that this corpus will push the frontiers of research in content-based essay grading by triggering the development of novel computational models concerning argument persuasiveness that could provide useful feedback to students on why their arguments are (un)persuasive in addition to how persuasive they are.
p18-1059,"
5 Conclusion
",2018,"indeed, if outputs all similarly under-correct, correlation studies will not be affected by whether an rbm is sensitive to undercorrection."
p18-1059,"
5 Conclusion
",2018,"napoles et al.(2016) have made progress towards this goal in proposing a reference-less grammaticality measure, using grammatical error detection tools, as did asano et al.(2017), who added a fluency measure to the grammaticality."
p18-1059,"
5 Conclusion
",2018,we believe our findings and methodologies can be useful for similar tasks such as style conversion and automatic post-editing of raw mt outputs.
p18-1060,"
7 Discussion
",2018,"certainly, we can try to improve the automatic metric (which is potentially as difficult as solving the task) and brainstorming alternative ways of soliciting evaluation (which has been less explored)."
p18-1060,"
7 Discussion
",2018,we hope our work provides some clarity on to how to make it more cost effective.
p18-1062,"
7 Conclusion and Next Steps
",2018,"future efforts should be dedicated to improving the community detection phase and generating more abstractive sentences, probably by harnessing deep learning."
p18-1064,"
8 Conclusion
",2018,"we presented a multi-task learning approach to improve abstractive summarization by incorporating the ability to detect salient information and to be logically entailed by the document, via question generation and entailment generation auxiliary tasks."
p18-1065,"
5 Conclusions and Recommendations
",2018,"baseline systems to design a strong baseline system, first, researchers should consider all proposed features so far, including content features, context features, and features used to approach moderating factors."
p18-1065,"
5 Conclusions and Recommendations
",2018,"consequently, we encourage researchers to evaluate the usefulness of these features and study these moderating factors in different domains, platforms, and languages, possibly identifying new features and moderating factors."
p18-1065,"
5 Conclusions and Recommendations
",2018,"features and knowledge sources while we encourage the development of user-specific helpfulness prediction, we by no means imply that a model should be trained for each user."
p18-1065,"
5 Conclusions and Recommendations
",2018,"however, there have been exciting developments in helpfulness prediction: systems that have attempted to exploit user and reviewer information, along with those based on sophisticated models (e.g., probabilistic matrix factorization, hmm-lda) and neural network architectures, are promising prospects for future work."
p18-1065,"
5 Conclusions and Recommendations
",2018,"this is why we consider that user-specific helpfulness prediction, first presented in moghaddam et al.(2012) and tang et al.(2013), should be the goal of future work, as it allows systems to tailor their predictions to users鈥 preferences and needs (much like a recommender system)."
p18-1065,"
5 Conclusions and Recommendations
",2018,we conclude our survey with several recommendations for future work on computational modeling and prediction of review helpfulness.
p18-1066,"
7 Conclusion
",2018,"future directions include: 1) mining cross-cultural differences in general concepts other than names and slang, 2) merging the mined knowledge into existing knowledge bases, and 3) applying the socvec in downstream tasks like machine translation."
p18-1067,"
7 Conclusion
",2018,"in future works, we will exploit the interesting connections between moral foundations and frames for the analysis of more detailed ideological leanings and stance prediction."
p18-1067,"
7 Conclusion
",2018,we also provide an initial approach to the joint modeling of frames and moral foundations.
p18-1068,"
6 Conclusions
",2018,"in the future, we would like to apply the framework in a weakly supervised setting, i.e., to learn semantic parsers from question-answer pairs and to explore alternative ways of defining meaning sketches."
p18-1069,7 Conclusions,2018,directions for future work are many and varied.
p18-1069,"
7 Conclusions
",2018,"the proposed framework could be applied to a variety of tasks (bahdanau et al., 2015; schmaltz et al., 2017) employing sequence-to-sequence architectures."
p18-1070,"
6 Conclusion
",2018,"we propose structvae, a deep generative model with tree-structured latent variables for semi-supervised semantic parsing."
p18-1071,"
6 Conclusions
",2018,"for future work, to solve the problem of the lack of training data, we want to design weakly supervised learning algorithm using denotations (qa pairs) as supervision."
p18-1071,"
6 Conclusions
",2018,"furthermore, structure and semantic constraints can be easily incorporated in decoding to enhance semantic parsing."
p18-1071,"
6 Conclusions
",2018,"furthermore, we want to collect labeled data by designing an interactive ui for annotation assist like (yih et al., 2016), which uses semantic graphs to annotate the meaning of sentences, since semantic graph is more natural and can be easily annotated without the need of expert knowledge."
p18-1072,"
6 Conclusion
",2018,we hope that this work will guide further developments in this new and exciting field.
p18-1073,"
6 Conclusions
",2018,"in order to make selflearning robust, we also added stochasticity to dictionary induction, used csls instead of nearest neighbor, and produced bidirectional dictionaries."
p18-1073,"
6 Conclusions
",2018,"in the future, we would like to extend the method from the bilingual to the multilingual scenario, and go beyond the word level by incorporating embeddings of longer phrases."
p18-1074,"
5 Conclusions and Future Work
",2018,"the next step of this research is to apply this architecture to other types of tasks, such as event extract and semantic role labeling that involve structure prediction."
p18-1074,"
5 Conclusions and Future Work
",2018,we also plan to explore the possibility of integrating incremental learning into this architecture to adapt a trained model for new tasks rapidly.
p18-1074,"
5 Conclusions and Future Work
",2018,we design a multi-lingual multi-task architecture for low-resource settings.
p18-1074,"
5 Conclusions and Future Work
",2018,we evaluate the model on sequence labeling tasks with three language pairs.
p18-1075,"
7 Conclusion
",2018,"furthermore, by adapting the broadly applicable semi-supervised approach of hausser et al.篓 (2017) (which until now has only been applied in computer vision) we were able to effectively exploit unlabeled data to further improve performance."
p18-1076,"
7 Conclusion and Future Work
",2018,"in future work, we will investigate even tighter integration of the attended knowledge and stronger reasoning methods."
p18-1076,"
7 Conclusion and Future Work
",2018,"this opens up for deeper investigation and future improvement of rc models in a targeted way, allowing us to investigate what knowledge sources are required for different data sets and domains."
p18-1076,"
7 Conclusion and Future Work
",2018,"we propose a neural cloze-style reading comprehension model that incorporates external commonsense knowledge, building on a single-turn neural model."
p18-1077,"
6 Conclusion
",2018,another contribution of our work is a thorough set of experiments and analysis of different types of endto-end architectures for qa at their ability to answer multi-relational questions of varying degrees of compositionality.
p18-1079,"
7 Limitations and Future Work
",2018,"developing more effective ways of leveraging the expert鈥檚 time to close the loop, and facilitating more interactive collaboration between humans and sears are exciting areas for future work."
p18-1079,"
7 Limitations and Future Work
",2018,"having demonstrated the usefulness of seas and sears in a variety of domains, we now describe their limitations and opportunities for future work."
p18-1079,"
7 Limitations and Future Work
",2018,"semantic scoring errors: paraphrasing is still an active area of research, and thus our semantic scorer is sometimes incorrect in evaluating rules for semantic equivalence."
p18-1079,"
8 Conclusion
",2018,"we demonstrated that seas and sears can be an invaluable tool for debugging nlp models, while indicating their current limitations and avenues for future work."
p18-1080,"
7 Conclusion
",2018,in future work we intend to apply this technique to debiasing sentences and anonymization of author traits such as gender and age.
p18-1080,"
7 Conclusion
",2018,"in particular, it would be interesting to back-translate through multiple target languages with a single source language (johnson et al., 2016)."
p18-1080,"
7 Conclusion
",2018,"in the future work, we will also explore whether an enhanced back-translation by pivoting through several languages will learn better grounded latent meaning representations."
p18-1081,"
5 Conclusion
",2018,"generating them from facts in a knowledge graph requires not only mapping the structured fact information to natural language, but also identifying the type of entity and then discerning the most crucial pieces of information for that particular type from the long list of input facts and compressing them down to a highly succinct form."
p18-1081,"
5 Conclusion
",2018,"in future work, we hope to explore the potential of this architecture on further kinds of data, including multimodal data (long et al., 2018), from which one can extract structured signals."
p18-1081,"
5 Conclusion
",2018,short textual descriptions of entities facilitate instantaneous grasping of key information about entities and their types.
p18-1083,"
5 Conclusion
",2018,"we believe there are still lots of improvement space in the narrative paragraph generation tasks, like how to better simulate human imagination to create more vivid and diversified stories."
p18-1084,"
8 Conclusion and Future Work
",2018,in future work we plan to improve our methods by exploiting the internal structure of images and sentences as well as by effectively integrating signals from more than two languages.
p18-1084,"
8 Conclusion and Future Work
",2018,our experiments reveal the effectiveness of these methods for both sentence-level and wordlevel tasks.
p18-1085,"
5 Conclusion
",2018,"in future work, we would like to explore other aspects of search engines for language grounding as well as the effect these embeddings may have on learning generic sentence representations (kiros et al., 2015b; hill et al., 2016; conneau et al., 2017a; logeswaran and lee, 2018)."
p18-1085,"
5 Conclusion
",2018,"through the use of multimodal gating, our models lead to interpretable weightings of abstract vs concrete words."
p18-1085,"
5 Conclusion
",2018,we expect that integrating picturebook with these embeddings to lead to further performance improvements as well.
p18-1086,"
6 Discussion and Conclusion
",2018,"nevertheless, we hope this work can motivate more research in this area, enabling physical action-effect reasoning, towards agents which can perceive, act, and communicate with humans in the physical world."
p18-1086,"
6 Discussion and Conclusion
",2018,our current model is very simple and performance is yet to be improved.
p18-1086,"
6 Discussion and Conclusion
",2018,"we also plan to incorporate action-effect prediction to humanrobot collaboration, for example, to bridge the gap of commonsense knowledge about the physical world between humans and robots."
p18-1086,"
6 Discussion and Conclusion
",2018,"we plan to apply more advanced approaches in the future, for example, attention models that jointly capture actions, image states, and effect descriptions."
p18-1086,"
6 Discussion and Conclusion
",2018,"when robots operate in the physical world, they not only need to perceive the world, but also need to act to the world."
p18-1088,"
7 Conclusion and Future Work
",2018,"since asc is a fine-grained and complex task, there are many other directions that can be further explored, like handling sentiment negation, better embedding for multi-word phrase, analyzing sentiment composition, and learning better attention."
p18-1088,"
7 Conclusion and Future Work
",2018,we believe that there will be more effective solutions coming in the near future.
p18-1090,"
5 Conclusions and Future Work
",2018,"for future work, we would like to explore a fine-grained version of sentiment-to-sentiment translation that not only reverses sentiment, but also changes the strength of sentiment."
p18-1091,"
7 Conclusion
",2018,future works involve the choice of discourse markers and some other transfer learning sources.
p18-1091,"
7 Conclusion
",2018,we transfer the knowledge learned from the discourse marker prediction task to the nli task to augment the semantic representation of the model.
p18-1092,"
5 Conclusion
",2018,"although we have used rn as the reasoning module in this work, other options can be tested."
p18-1092,"
5 Conclusion
",2018,it might be interesting to analyze how other reasoning modules can improve different weaknesses of the model.
p18-1092,"
5 Conclusion
",2018,"while other models have focused on different parts of these components, we think that is important to find ways to combine these different mechanisms if we want to build models capable of complex reasoning."
p18-1093,"
5 Conclusion
",2018,"analysis of the intra-attention scores shows that our model learns highly interpretable attention weights, paving the way for more explainable neural sarcasm detection methods."
p18-1095,"
6 Conclusions
",2018,"in the future we want to apply our marginal utility based framework to other metrics, such as mean average precision (map)."
p18-1095,"
6 Conclusions
",2018,"this paper proposes adaptive scaling algorithm for detection tasks, which can deal with its positive sparsity problem and directly optimize f-measure by adaptively scaling the influence of negative instances in loss function."
p18-1096,"
5 Conclusions
",2018,we re-evaluate a range of traditional generalpurpose bootstrapping algorithms in the context of neural network approaches to semi-supervised learning under domain shift.
p18-1098,"
6 Conclusions and Discussions
",2018,the proposed methods can be applied to other tasks in nlp.
p18-1098,"
6 Conclusions and Discussions
",2018,the representations can simultaneously capture the semantics of codes and their relationships.
p18-1098,"
6 Conclusions and Discussions
",2018,"we will address these two issues in future by investigating diversity-promoting regularization (xie et al., 2017) and leveraging an external knowledge base that maps medical abbreviations into their full names."
p18-1101,"
5 Conclusion and Future Work
",2018,"our findings also suggest promising future research directions, including learning better context-based latent actions and using reinforcement learning to adapt policy networks."
p18-1101,"
5 Conclusion and Future Work
",2018,we believe that this work is an important step forward towards creating generative dialog models that can not only generalize to large unlabelled datasets in complex domains but also be explainable to human users.
p18-1103,"
6 Conclusion
",2018,our solution extends the attention mechanism of transformer in two ways: (1) using stacked selfattention to harvest multi-grained semantic representations.(2) utilizing cross-attention to match with dependency information.
p18-1103,"
6 Conclusion
",2018,"we believe that both self-attention and cross-attention could benefit other research area, including spoken language understanding, dialogue state tracking or seq2seq dialogue generation."
p18-1103,"
6 Conclusion
",2018,we would like to explore in depth how attention can help improve neural dialogue modeling for both chatbots and taskoriented dialogue systems in our future work.
p18-1104,"
6 Conclusion and Future Work
",2018,we are also looking forward to transferring the idea of naturally-labeled emojis to task-oriented dialog and multi-turn dialog generation problems.
p18-1104,"
6 Conclusion and Future Work
",2018,we trained a large scale emoji classifier and ran the classifier on the generated responses to evaluate the emotion accuracy of the generated response.
p18-1104,"
6 Conclusion and Future Work
",2018,"we will keep accumulating data and increase the ratio of underrepresented emojis, and advance toward more sophisticated abstractive generation methods."
p18-1105,"
6 Conclusion
",2018,another direction will be to apply fluctuation analysis in formulating a statistical test to evaluate the structural complexity underlying a sequence.
p18-1105,"
6 Conclusion
",2018,"our future work will include an analysis using other kinds of data, such as twitter data and adult utterances, and a study of how taylor鈥檚 law relates to grammatical complexity for different sequences."
p18-1105,"
6 Conclusion
",2018,"this value differed greatly from the exponents for other data sources: enwiki8 (tagged wikipedia, 0.63), child-directed speech (childes, around 0.68), and programming language and music data (around 0.79)."
p18-1105,"
6 Conclusion
",2018,we conducted more detailed analysis varying the data size and the segment size.
p18-1106,"
5 Discussion
",2018,"in our investigation of kauhanen鈥檚 basic assumptions, we discover how seemingly innocuous decisions about population size and learning conspire to drive simulation results."
p18-1106,"
5 Discussion
",2018,"one problem that this line of simulation work has always faced has been the lack of viable comparison between models because every study implements its own learning, network, and interaction models."
p18-1106,"
5 Discussion
",2018,the modular nature of our framework advances against this trend since it is now possible to hold the population model constant while slotting in various learning models to test them against one another and vice-versa.
p18-1106,"
5 Discussion
",2018,"without simulation, it would be difficult or impossible to undercover the interplay between acquisition and social structure on the propagation of language change."
p18-1107,"
8 Conclusion and Future Work
",2018,we further plan to empirically evaluate our lexicalization on an alignment task and to offer a comparison against the lexicalization due to zhang and gildea (2005).
p18-1107,"
8 Conclusion and Future Work
",2018,"we plan to verify whether rank-k pl-rstag is more powerful than rank-k scfg in future work, and to reduce the rank of the transformed grammar if possible."
p18-1108,"
6 Conclusion
",2018,"since the architecture of model is no more than a stack of standard recurrent and convolution layers, which are essential components in most academic and industrial deep learning frameworks, the deployment of this method would be straightforward."
p18-1111,"
7 Conclusion
",2018,"in the future, we plan to take generalization one step further, and explore the possibility to use the bilstm for generating completely new paraphrase templates unseen during training."
p18-1111,"
7 Conclusion
",2018,"this results in better generalization abilities, leading to improved performance in two noun-compound interpretation tasks."
p18-1113,"
7 Conclusion
",2018,future work will introduce weighted cbow and skip-gram to learn positional information within sentences.
p18-1114,"
6 Conclusion
",2018,"in the future, we intend to evaluate our models for some morpheme-rich languages like russian, german and so on."
p18-1115,"
7 Conclusion and Future Work
",2018,"a latent factor model such as darn (gregor et al., 2014) would consider several sources simultaneously."
p18-1115,"
7 Conclusion and Future Work
",2018,"as this is the first work that systematically considers word-level variation in nmt, there are lots of research ideas to explore in the future."
p18-1115,"
7 Conclusion and Future Work
",2018,"extension to other architectures: introducing latent variables into non-autoregressive translation models such as the transformer (vaswani et al., 2017) should increase their translation ability further."
p18-1115,"
7 Conclusion and Future Work
",2018,our experiments confirm our intuition that modelling variation is crucial to the success of machine translation.
p18-1115,"
7 Conclusion and Future Work
",2018,"richer distributions computed by normalising flows (rezende and mohamed, 2015; kingma et al., 2016) will likely improve our model."
p18-1116,"
6 Conclusion and future work
",2018,"as future work, we plan to design some more elaborate structures to incorporate the score layer into the encoder."
p18-1116,"
6 Conclusion and future work
",2018,we will also apply the proposed linearization method to other tasks.
p18-1117,"
7 Conclusions
",2018,"future work could also investigate whether context-aware nmt systems learn other discourse phenomena, for example whether they improve the translation of elliptical constructions, and markers of discourse relations and information structure."
p18-1118,"
7 Conclusion
",2018,"for future work, we intend to investigate models which incorporate specific discourse-level phenomena."
p18-1118,"
7 Conclusion
",2018,our model augments the vanilla sentence-based nmt model with external memories to incorporate documental interdependencies on both source and target sides.
p18-1119,"
6 Conclusions and Future Work
",2018,future work may see our methods applied to document geolocation to assess the effectiveness of scaling geodesic vectors from paragraphs to entire documents.
p18-1119,"
6 Conclusions and Future Work
",2018,we need a means of extracting and encoding this additional knowledge.
p18-1120,"
5 Conclusions and Future Work
",2018,challenges also remain in how to evaluate the accuracy of goal knowledge extracted from text corpora.
p18-1120,"
5 Conclusions and Future Work
",2018,"in future work, we hope to see if we can take advantage of more contextual information as well as other external knowledge to improve the recognition of goalacts."
p18-1120,"
5 Conclusions and Future Work
",2018,"nevertheless, our work represents a first step toward learning goal knowledge about locations, and we believe that learning knowledge about plans and goals is an important direction for natural language understanding research."
p18-1120,"
5 Conclusions and Future Work
",2018,we then created an activity profile framework and applied a semi-supervised label propagation algorithm to iteratively update the activity strengths for locations.
p18-1122,"
7 Conclusion
",2018,"this further enables the usage of crowdsourcing to collect a new dataset, matres, at a lower time cost."
p18-1123,"
6 Conclusions
",2018,in our future work we plan to extend the proposed method to these other applications.
p18-1123,"
6 Conclusions
",2018,"while in this work, we apply the exemplar encoder decoder network on conversational task, the method is generic and could be used with other tasks such as question answering and machine translation."
p18-1124,"
6 Conclusion
",2018,dialsql successfully extracts error spans from queries and offers several alternatives to users.
p18-1124,"
6 Conclusion
",2018,we further investigate the usability of dialsql in a real life setting by conducting human evaluations.
p18-1125,"
7 Conclusions and Future Work
",2018,a promising line of future work could consider the complementary problem of identifying pragmatic strategies that can help bring uncivil conversations back on track.
p18-1125,"
7 Conclusions and Future Work
",2018,"additionally, since our procedure for collecting and vetting data focused on precision rather than recall, it might miss more subtle attacks that are overlooked by the toxicity classifier."
p18-1125,"
7 Conclusions and Future Work
",2018,"beyond the present binary classification task, one could explore a sequential formulation predicting whether the next turn is likely to be an attack as a discussion unfolds, capturing conversational dynamics such as sustained escalation."
p18-1125,"
7 Conclusions and Future Work
",2018,"in this work, we started to examine the intriguing phenomenon of conversational derailment, studying how the use of pragmatic and rhetorical devices relates to future conversational failure."
p18-1125,"
7 Conclusions and Future Work
",2018,"indeed, a manual investigation of conversations whose eventual trajectories were misclassified by our models鈥攁s well as by the human annotators鈥攕uggests that interactions which initially seem prone to attacks can nonetheless maintain civility, by way of level-headed interlocutors, as well as explicit acts of reparation."
p18-1125,"
7 Conclusions and Future Work
",2018,"noting that our framework is not specifically tied to wikipedia, it would also be valuable to explore the varied ways in which this phenomenon arises in other (possibly noncollaborative) public discussion venues, such as reddit and facebook pages."
p18-1125,"
7 Conclusions and Future Work
",2018,our approach has several limitations which open avenues for future work.
p18-1125,"
7 Conclusions and Future Work
",2018,"our correlational analyses do not provide any insights into causal mechanisms of derailment, which randomized experiments could address."
p18-1125,"
7 Conclusions and Future Work
",2018,"supplementing our investigation with other indicators of antisocial behavior, such as editors blocking one another, could enrich the range of attacks we study."
p18-1125,"
7 Conclusions and Future Work
",2018,the human accuracy on predicting future attacks in this setting (72%) suggests it is feasible at least at the level of human intuition.
p18-1125,"
7 Conclusions and Future Work
",2018,"to this end, we develop a computational framework for analyzing how general politeness strategies and domain-specific rhetorical prompts deployed in the initial stages of a conversation are tied to its future trajectory."
p18-1125,"
7 Conclusions and Future Work
",2018,"we show that our computational framework can recover some of that intuition, hinting at the potential of automated methods to identify signals of the future trajectories of online conversations."
p18-1125,"
7 Conclusions and Future Work
",2018,"while our analysis focused on the very first exchange in a conversation for the sake of generality, more complex modeling could extend its scope to account for conversational features that more comprehensively span the interaction."
p18-1127,"
9 Conclusion
",2018,"the difficulties in basing validation on system outputs may be applicable to other text-to-text generation tasks, a question we will explore in future work."
p18-1130,"
5 Conclusion
",2018,"another interesting direction is to further improve our model by exploring reinforcement learning approaches to learn an optimal order for the children of head words, instead of using a predefined fixed order."
p18-1130,"
5 Conclusion
",2018,"combining pointer networks with an internal stack to track the status of the top-down, depth-first search in the decoding procedure, the stackptr parser is able to capture information from the whole sentence and all the previously derived subtrees, removing the leftto-right restriction in classical transition-based parsers, while maintaining linear parsing steps, w.r.t the length of the sentences."
p18-1130,"
5 Conclusion
",2018,"first, we intend to consider how to conduct experiments to improve the analysis of parsing errors qualitatively and quantitatively."
p18-1131,"
6 Conclusion
",2018,aae speakers) are not under-counted or under-represented in results returned to a user or analyst.
p18-1131,"
6 Conclusion
",2018,it remains an open question whether it is better to use a model with a smaller accuracy disparity (e.g.
p18-1131,"
6 Conclusion
",2018,"while the cross-domain strategies we presented can greatly increase accurate parsing of these features, narrowing the performance gap between aae- and mae-like tweets, much work remains to be done for accurate parsing of even linguistically well-documented features."
p18-1133,"
6 Conclusion
",2018,"for our future work, we will consider advanced instantiations for sequicity, and extend sequicity to handle unsupervised cases where information and requested slots values are not annotated."
p18-1133,"
6 Conclusion
",2018,"one simplistic instantiation of sequicity, called two stage copynet (tscp), demonstrates better effectiveness and scalability of sequicity."
p18-1133,"
6 Conclusion
",2018,such belief spans enable a task-oriented dialogue system to be holistically optimized in a single seq2seq model.
p18-1133,"
6 Conclusion
",2018,such properties are important for real-world customer service dialog systems where users鈥 inputs vary frequently and models need to be updated frequently.
p18-1136,"
7 Conclusion
",2018,mem2seq combines the multi-hop attention mechanism in end-to-end memory networks with the idea of pointer networks to incorporate external information.
p18-1137,"
6 Conclusion
",2018,"in future work, we plan to further investigate the impact of risksensitive objective functions, including the relations between model robustness and diverse generations."
p18-1137,"
6 Conclusion
",2018,"while if we want to generate diverse responses, a risk-sensitive objective functions is helpful."
p18-1138,"
5 Conclusion
",2018,"in future work, we plan to introduce reinforcement learning and knowledge base reasoning mechanisms to improve the performance."
p18-1139,"
5 Conclusion
",2018,"as for future work, we will investigate how to apply the technique to multi-turn conversational systems, provided that the most proper sentence function can be predicted under a given conversation context."
p18-1139,"
5 Conclusion
",2018,"to address the compatibility issue, we devise a type controller to handle function-related and topic words explicitly."
p18-1140,"
9 Conclusion
",2018,we believe this approach can be easily generalized to other domains given its end-to-end training procedure and task independence.
p18-1142,"
7 Conclusions and Future Work
",2018,"another possible research direction is learning the mapping between structures from parallel texts jointly with a main task, in the spirit of quasi-synchronous grammars (smith and eisner, 2009)."
p18-1142,"
7 Conclusions and Future Work
",2018,future work will look into automating the tree processing procedure.
p18-1143,"
7 Conclusion
",2018,"further, we would like to study sampling techniques motivated by natural distributions of linguistic structures."
p18-1143,"
7 Conclusion
",2018,"hence, as a future work, we would like to compare the usefulness of different linguistic theories and different constraints within each theory in our proposed lm framework."
p18-1143,"
7 Conclusion
",2018,this can also provide an indirect validation of the theories.
p18-1145,"
6 Conclusions and Future Work
",2018,"because the mismatch between words and extraction units is a common problem in information extraction, we believe our method can also be applied to many other languages and tasks for exploiting inner composition structure during extraction, such as named entity recognition."
p18-1146,"
5 Conclusion
",2018,we are hopeful that the backoff-based factorization idea exploited in tfba will be useful in other sparse factorization settings.
p18-1147,"
8 Conclusions
",2018,"first of all, we plan to explore the use of more advanced forms of entity detection and linking, including propagating features from the edl system forward for both unary and binary deep models."
p18-1147,"
8 Conclusions
",2018,"in addition we plan to exploit unary and binary relations as source of evidence to bootstrap a probabilistic reasoning approach, with the goal of leveraging constraints from the kb schema such as domain, range and taxonomies."
p18-1147,"
8 Conclusions
",2018,our method is extremely effective and complement very nicely existing binary relation extraction methods for kbp.
p18-1147,"
8 Conclusions
",2018,"this is just the first step in our wider research program on kbp, whose goal is to improve recall by identifying implicit information from texts."
p18-1147,"
8 Conclusions
",2018,we will also integrate the new triples gathered from textual evidence with new triples predicted from existing kb relationships by knowledge base completion.
p18-1148,"
5 Conclusion and Future work
",2018,"besides, we would like to examine whether the induced latent relations could be helpful for relation extract."
p18-1148,"
5 Conclusion and Future work
",2018,"in future work, we would like to use syntactic and discourse structures (e.g., syntactic dependency paths between mentions) to encourage the models to discover a richer set of relations."
p18-1148,"
5 Conclusion and Future work
",2018,"in this way we also hope it will lead to interesting follow-up work, as individual relations can be informed by injecting prior knowledge (e.g., by training jointly with relation extraction models)."
p18-1148,"
5 Conclusion and Future work
",2018,we also would like to combine ment-norm and relnorm.
p18-1149,"
8 Conclusion
",2018,we are hopeful that the representation learning techniques explored in this paper will inspire further development and adoption of such techniques in the temporal information processing research community
p18-1153,"
5 Conclusion and Future Work
",2018,"for future work, we hope to improve the results by using the pun data and design a more proper way to select candidates from associative words."
p18-1154,"
6 Conclusions
",2018,a human evaluation study judges outputs from the proposed methods to be as good as human written commentary texts for 鈥楳ove description鈥 subset of the data.
p18-1154,"
6 Conclusions
",2018,an interesting point to explore is whether such pragmatically trained game state representations can be leveraged for the task of game commentary generation.
p18-1154,"
6 Conclusions
",2018,generating commentary for such multi-moves is a potential direction for future work.
p18-1154,"
6 Conclusions
",2018,our dataset also contains multi-move-single commentary pairs in addition to single movesingle commentary pairs.
p18-1154,"
6 Conclusions
",2018,we anticipate this task to require even deeper understanding of the game pragmatics than the single move-single commentary case.
p18-1155,"
7 Discussion
",2018,"given the numerous potential applications of such an oracle, we believe improving its accuracy will be a promising future direction."
p18-1157,"
7 Conclusion
",2018,"due to the strong connection between the proposed model with memory networks and reasonet, we would like to delve into the theoretical link between these models and its training algorithms."
p18-1157,"
7 Conclusion
",2018,"further, we also would like to explore san on other tasks, such as text classification and natural language inference for its generalization in the future."
p18-1161,"
5 Conclusion and Future Work
",2018,"in the future, we will explore the following directions: (1) an additional answer re-ranking step can further improve our model."
p18-1161,"
5 Conclusion and Future Work
",2018,"we will explore how to effectively re-rank our extracted answers to further enhance the performance.(2) background knowledge such as factual knowledge, common sense knowledge can effectively help us in paragraph selection and answer extraction."
p18-1161,"
5 Conclusion and Future Work
",2018,we will incorporate external knowledge bases into our ds-qa model to improve its performance.
p18-1162,"
8 Conclusion and Future Work
",2018,"furthermore, since thread-level features have been explored in previous work (barr贸n-cede帽o et al., 2015; joty et al., 2015, 2016), we will verify their effectiveness in our architecture."
p18-1162,"
8 Conclusion and Future Work
",2018,"in future work, we will try to incorporate more hand-crafted features in our model."
p18-1162,"
8 Conclusion and Future Work
",2018,"to better capture the interaction between the subject-body pair and the question-answer pair, the multi-dimensional attention mechanism is adopted."
p18-1163,"
6 Conclusion
",2018,"as our training framework is not limited to specific perturbation types, it is interesting to evaluate our approach in natural noise existing in practical applications, such as homonym in the simultaneous translation system."
p18-1163,"
6 Conclusion
",2018,"it is also necessary to further validate our approach on more advanced nmt architectures, such as cnn-based nmt (gehring et al., 2017) and transformer (vaswani et al., 2017)."
p18-1163,"
6 Conclusion
",2018,we have proposed adversarial stability training to improve the robustness of nmt models.
p18-1164,"
6 Conclusion
",2018,"additionally, we also intend to apply these methods to other sequence-to-sequence tasks, including natural language conversation."
p18-1164,"
6 Conclusion
",2018,"in future work, we will explore further strategies to bridge the source and target side for sequence-to-sequence and tree-based nmt."
p18-1165,"
7 Conclusion
",2018,"our rating study, comparing 5-point and preference ratings, showed that their reliability is comparable, whilst cardinal ratings are easier to learn and to generalize from, and also more suitable for rl in our experiments."
p18-1166,"
6 Conclusion and Future Work
",2018,"in the future, we plan to apply our model on other sequence to sequence learning tasks."
p18-1166,"
6 Conclusion and Future Work
",2018,"our model employs a cumulative average operation to capture important contextual clues from previous target words, and a feed forward gating layer to enrich the expressiveness of learned hidden representations."
p18-1166,"
6 Conclusion and Future Work
",2018,we will also attempt to improve our model to enhance its modeling ability so as to consistently outperform the original neural transformer.
p18-1168,"
8 Discussion
",2018,in future work we plan to extend this work and automatically learn such a lexicon.
p18-1169,"
7 Conclusion
",2018,"finally, our approach to collecting feedback can also be transferred to other domains."
p18-1169,"
7 Conclusion
",2018,"for example, (yih et al., 2016) designed a user interface to help freebase experts to efficiently create queries."
p18-1170,"
8 Conclusion
",2018,"in future work, we will speed it up through the use of pruning techniques."
p18-1170,"
8 Conclusion
",2018,"in particular, advanced methods for alignments, as in lyu and titov (2018), seem promising."
p18-1170,"
8 Conclusion
",2018,overcoming the need for heuristics also seems to be a crucial ingredient for applying our method to other semantic representations.
p18-1170,"
8 Conclusion
",2018,we will also look into more principled methods for splitting the amrs into elementary as-graphs to replace our hand-crafted heuristics.
p18-1171,"
6 Conclusion
",2018,"while we are focused on amr parsing in this paper, in future work our cache transition system and the presented sequence-to-sequence models can be potentially applied to other semantic graph parsing tasks (oepen et al., 2015; du et al., 2015; zhang et al., 2016; cao et al., 2017)."
p18-1172,"
6 Conclusion
",2018,"in future, we will extend our proposed all-sample learning scheme to deep learning methods, which are more expressive than the shallow embedding model."
p18-1172,"
6 Conclusion
",2018,"lastly, we are interested in exploring the recent adversarial learning techniques to enhance the robustness of word embeddings."
p18-1172,"
6 Conclusion
",2018,"moreover, we will integrate prior knowledge, such as the words that are synonyms and antonyms, into the word embedding process."
p18-1174,"
6 Conclusion
",2018,our efficient algorithmic expert provides state-action pairs from which effective active learning policies can be learned.
p18-1176,"
7 Conclusion
",2018,"additionally, our perturbation attacks (sections 4.4 and 5.5) serve as empirical validation of attributions."
p18-1176,"
7 Conclusion
",2018,we also believe that other qa models may share these weaknesses.
p18-1177,"
7 Conclusion
",2018,"finally, we apply our question generation framework to produce a corpus of 1.26 million questionanswer pairs, which we hope will benefit the qa research community."
p18-1177,"
7 Conclusion
",2018,it would also be interesting to apply our approach to incorporating coreference knowledge to other text generation tasks.
p18-1178,"
6 Conclusion
",2018,"we creatively design three different modules in our model, which can find the answer boundary, model the answer content and conduct cross-passage answer verification respectively."
p18-1179,"
7 Conclusion
",2018,we think may nlp tasks that involve graph manipulation may benefit from this design.
p18-1180,"
10 Conclusion
",2018,"concurrently, we show that derivational transformations can be usefully modeled as nonlinear functions on distributional word embeddings."
p18-1184,"
6 Conclusions and Future Work
",2018,"in our future work, we plan to integrate other types of information such as user properties into the structured neural models to further enhance representation learning and detect rumor spreaders at the same time."
p18-1184,"
6 Conclusions and Future Work
",2018,we also plan to use unsupervised models for the task by exploiting structural information.
p18-1185,"
6 Conclusions and Future Work
",2018,"name tagging for more fine-grained types (e.g.soccer team, basketball team, politician, artist) can benefit more from visual features."
p18-1185,"
6 Conclusions and Future Work
",2018,we hope this work will encourage more research on multimodal social media in the future and we plan on making our benchmark available upon request.
p18-1185,"
6 Conclusions and Future Work
",2018,we plan to expand our model to tasks such as fine-grained name tagging or entity liking in the future.
p18-1185,"
6 Conclusions and Future Work
",2018,we propose a gated visual attention for name tagging in multimodal social media.
p18-1187,"
6 Conclusion
",2018,"in future work, we are interested in modelling the extent to which a social interaction is caused by geographical proximity (e.g.using user鈥搖ser gates)."
p18-1187,"
6 Conclusion
",2018,"we proposed gcn, dcca and mlp-txt+net, three multiview, transductive, semi-supervised geolocation models, which use text and network information to infer user location in a joint setting."
p18-1188,"
5 Conclusion
",2018,"our experiments with extractive document summarization and answer selection tasks validates our model in two ways: first, we demonstrate that external information is important to guide document modeling for natural language understanding tasks."
p18-1188,"
5 Conclusion
",2018,"second, our external attention mechanism successfully guides the learning of the document representation for the relevant end goal."
p18-1188,"
5 Conclusion
",2018,we describe an approach to model documents while incorporating external information that informs the representations learned for the sentences in the document.
p18-1189,"
6 Conclusion
",2018,"furthermore, the flexibility of our model enables intriguing exploration of a text corpus on us immigration."
p18-1189,"
6 Conclusion
",2018,we believe that our model and code will facilitate rapid exploration of document collections with metadata.
p18-1190,"
6 Conclusions
",2018,a neural variational framework is introduced to train our model.
p18-1192,"
6 Conclusion and Future Work
",2018,"however, maybe we will never reach a satisfying conclusion, as whenever one proposes a syntax-agnostic srl system which can outperform all syntax-aware ones at then, always there comes argument that you have never fully explored creative new method to effectively exploit the syntax input."
p18-1192,"
6 Conclusion and Future Work
",2018,"in addition, we consider the sem-f1/las ratio as a mean of evaluating syntactic contribution to srl, and true performance of srl independent of the quality of syntactic parser."
p18-1193,"
9 Discussion
",2018,"future modeling work may include using intermediate world states from previous turns in the interaction, which is required for some of the most complex references in the data."
p18-1193,"
9 Discussion
",2018,"however, it is particularly suitable to recovering from biases acquired early during learning, for example due to biased action spaces, which is likely to lead to incorrect blame assignment in neural network policies."
p18-1193,"
9 Discussion
",2018,"one possible direction for future work is to use an estimator to predict rewards for all actions, rather than observing them."
p18-1193,"
9 Discussion
",2018,"we propose to train our model using sestra, a learning algorithm that takes advantage of single-step reward observations to overcome learned biases in on-policy learning."
p18-1196,"
7 Conclusion
",2018,"our approaches in modelling and evaluation can be used in future work in tasks such as approximate information extraction, knowledge base completion, numerical fact checking, numerical question answering, and fraud detection."
p18-1198,"
6 Conclusion
",2018,"in future work, we would like to extend the probing tasks to other languages (which should be relatively easy, given that they are automatically generated), investigate how multi-task training affects probing task performance and leverage our probing tasks to find more linguistically-aware universal encoders."
p18-1198,"
6 Conclusion
",2018,"their purpose is not to encourage the development of ad-hoc models that attain top performance on them, but to help exploring what information is captured by different pre-trained encoders."
p18-1198,"
6 Conclusion
",2018,"we further uncovered interesting patterns of correlation between the probing tasks and more complex 鈥渄ownstream鈥 tasks, and presented a set of intriguing findings about the linguistic properties of various embedding methods."
p18-1200,"
7 Conclusion
",2018,we believe these findings provide insightful understandings of kb embedding models and might be applied to other neural networks beyond the kbc task.
p18-1201,"
8 Conclusions and Future Work
",2018,"in the future, we will extend this framework to other information extraction problems."
p18-1202,"
6 Conclusion
",2018,we further develop a joint model to combine rnscn/rnscn+ with a sequential labeling model for terms extraction.
p18-1203,"
4 Conclusion
",2018,one interesting topic for future research is exploration in planning.
p18-1203,"
4 Conclusion
",2018,"to this end, we want the agent to explore in the environment, but not so much that the performance would be greatly degraded."
p18-1203,"
4 Conclusion
",2018,"we need to deal with the challenge of adapting the world model in a changing environment, as exemplified by the domain extension problem (lipton et al., 2016)."
p18-1204,"
5 Conclusion and Future Work
",2018,the detector can be implemented by a classifier or some heuristics.
p18-1204,"
5 Conclusion and Future Work
",2018,the work can be extended to multi-turn conversation generation by including an additional detector predicting when to ask a question.
p18-1205,"
6 Conclusion & Discussion
",2018,we believe persona-chat will be a useful resource for training components of future dialogue systems.
p18-1205,"
6 Conclusion & Discussion
",2018,"we hope that the data will aid training agents that can ask questions about users鈥 profiles, remember the answers, and use them naturally in conversation."
p18-1206,"
6 Conclusions
",2018,"in future work, we plan to incorporate various types of context (e.g.anaphora, device-specific capabilities) and dialogue history into a large-scale nlu system."
p18-1207,"
7 Conclusion
",2018,we introduced three fusion strategies with a cnn structure to combine word-level features to classify emotions.
p18-1209,"
6 Conclusion
",2018,"future work on similar topics could explore the applications of using low-rank tensors for attention models over tensor representations, as they can be even more memory and computationally intensive."
p18-1210,"
6 Conclusion
",2018,"we hope the reader is now convinced that, in both psycholinguistic research on discourse coherence and computational work on discourse parsing, one needs to identify and examine evidence for coherence involving more than one discourse relation."
p18-1211,"
5 Conclusion
",2018,"more generally, similar approaches can explore a wider range of scenarios involving sequences of text."
p18-1211,"
5 Conclusion
",2018,"the ability to visualize learning is a key component of our method, which can find significant applications in data mining and data-discovery in large text collections."
p18-1211,"
5 Conclusion
",2018,"while here our focus was on learning discourse structures at the document level, similar methods can also be used at other scales, such as for syntactic or morphological analysis."
p18-1212,"
6 Conclusion
",2018,"we hope that this notable improvement can foster more interest in jointly studying multiple aspects of events (e.g., event sequencing, coreference, parent-child relations) towards the goal of understanding events in natural language."
p18-1213,"
9 Conclusion
",2018,"while our work only use information present in our dataset, we view our dataset as a future testbed for evaluating models trained on any number of resources for learning common sense about emotional reactions and motivations."
p18-1214,"
5 Conclusion
",2018,"in the future, we plan to enrich the architecture of dazer to allow few-shot document filtering by incorporating several labeled examples."
p18-1214,"
5 Conclusion
",2018,"to enable dazer to capture conceptual relevance and generalize well to unseen categories, two kinds of feature interactions, a gated convolutional network and an categoryindependent adversarial learning are devised."
p18-1217,"
5 Conclusion
",2018,"for future work, we are interested in various extensions, including combining stc with autoencoding variational bayes (avb)."
p18-1218,"
7 Conclusion
",2018,we incorporate domain knowledge into the matching system to gain further performance improvement.
p18-1219,"
7 Conclusion and Future Work
",2018,"in future, we plan to use use approaches, like multi-task learning (mishra et al., 2018), in estimating gaze features and using those estimated features for text quality prediction."
p18-1223,"
7 Conclusions
",2018,"though mainly fouced on search, we hope our findings shed some lights on a potential path towards more intelligent neural systems and will motivate more explorations in this direction."
p18-1224,"
6 Conclusions
",2018,the proposed model of infusing neural networks with external knowledge may also help shed some light on tasks other than nli.
p18-1225,"
6 Conclusion
",2018,"our rule-based generators can be expanded to cover more patterns and phenomena, and the seq2seq generator extended to incorporate per-example loss for adversarial training."
p18-1225,"
6 Conclusion
",2018,"our seq2seq and knowledge-guided example generators, trained in an end-to-end fashion, can be used to make any base entailment model more robust."
p18-1226,"
6 Conclusion and Discussions
",2018,"generally, informal korean text contains intentional typos (鈥橂鞛囯嫟鈥榙elicious鈥 with typo鈥), stand-alone jamo as a character, (鈥樸厠銋媗ol鈥) and segmentation errors.(鈥橁皺 鞚搓皜雼も榞o together鈥 without space鈥)."
p18-1226,"
6 Conclusion and Discussions
",2018,"lastly, since our method can capture korean syntactic features through jamo and character n-grams, we can apply the same idea to other tasks such as pos tagging and parsing."
p18-1226,"
6 Conclusion and Discussions
",2018,"meanwhile, we will further train our model over noisy data and investigate how it is dealing with noisy words."
p18-1226,"
6 Conclusion and Discussions
",2018,"since korean words are divisible once more into grapheme level, resulting in longer sequence of jamos for a given word, we plan to explore potential applicability of deeper level of subword information in korean."
p18-1226,"
6 Conclusion and Discussions
",2018,"since these errors occur frequently, it is important to apply the vectors in training nlp models over real-word data."
p18-1226,"
6 Conclusion and Discussions
",2018,"we plan to apply these vectors for various neural network based nlp models, such as conversation modeling."
p18-1227,"
6 Conclusion and Future Work
",2018,"in the future, we will explore methods of exploiting internal information in other languages.(4) we believe that sememes are universal for all human languages."
p18-1227,"
6 Conclusion and Future Work
",2018,"in the future, we will take structured annotations into account.(2) it would be meaningful to take more information into account for blending external and internal information and design more sophisticated methods.(3) besides chinese, many other languages have rich subword-level information."
p18-1227,"
6 Conclusion and Future Work
",2018,we will explore a general framework to recommend and utilize sememes for other nlp tasks.
p18-1227,"
6 Conclusion and Future Work
",2018,"we will explore the following research directions in the future: (1) concepts in hownet are annotated with hierarchical structures of senses and sememes, but those are not considered in this paper."
p18-1228,"
6 Discussion and Conclusion
",2018,"despite these limitations, we identify the following key implications."
p18-1228,"
6 Discussion and Conclusion
",2018,our study may facilitate further investigations on context-dependent text analysis techniques and applications.
p18-1228,"
6 Discussion and Conclusion
",2018,"thus, we believe the effect of deleted comment would be marginal in our analyses."
p18-1228,"
6 Discussion and Conclusion
",2018,we hope that semaxis can facilitate research on other semantic axes so that we will have labeled datasets for other axes as well.
p18-1229,"
7 Conclusion and Future Work
",2018,"in addition, study on how to effectively encode induction history will be interesting."
p18-1229,"
7 Conclusion and Future Work
",2018,"in the future, we will explore more strategies towards term pair selection (e.g., allow the rl agent to remove terms from the taxonomy) and reward function design."
p18-1229,"
7 Conclusion and Future Work
",2018,the error propagation between two phases is thus effectively reduced and the global taxonomy structure is better captured.
p18-1230,"
5 Conclusions and Future Work
",2018,"in the next step, we will consider integrating the rich structural information into the neural network for word sense disambiguation."
p18-1230,"
5 Conclusions and Future Work
",2018,"in this way, we not only make use of labeled context data but also exploit the background knowledge to disambiguate the word sense."
p18-1230,"
5 Conclusions and Future Work
",2018,there is still one challenge left for the future.
p18-1230,"
5 Conclusions and Future Work
",2018,we further extend the gloss information through its semantic relations in wordnet to better infer the context.
p18-1231,"
8 Conclusion
",2018,"in the future, we will extend our model so that it can project multi-word phrases, as well as single words, which could help with negations and modifiers."
p18-1232,"
6 Conclusions
",2018,"compared with existing domain-sensitive methods, our model detects domain-common words according to not only similar context words but also sentiment information."
p18-1232,"
6 Conclusions
",2018,"moreover, our learned embeddings considering sentiment information can distinguish words with similar syntactic context but opposite sentiment polarity."
p18-1234,"
8 Conclusions and Future Work
",2018,how to leverage large-scale sentiment lexicons in neural networks would be our future work.
p18-1235,"
5 Conclusions
",2018,we have found that this is best achieved using a dual-module approach that encourages the learning of models with favourable generalization abilities.
p18-1237,"
5 Discussion and Conclusion
",2018,"also, our model helps analyzing the influence of user interaction and behavior on the effectiveness of discussion decisions."
p18-1237,"
5 Discussion and Conclusion
",2018,"in contrast, other metadata might require the use of computational methods, such as clustering, keyphrase extraction, and textual entailment."
p18-1237,"
5 Discussion and Conclusion
",2018,"in future work, we plan to study how to distinguish effective from ineffective discussions based on our model as well as how to learn from the strategies used in successful discussions, in order to predict the best next deliberative move in an ongoing discussion."
p18-1237,"
5 Discussion and Conclusion
",2018,"many categories in our model will apply to deliberative discussions in general, particularly the discourse acts and argumentative relations."
p18-1237,"
5 Discussion and Conclusion
",2018,"we believe that these corpora will help foster research on tasks such as argument mining, among others."
p18-1238,"
6 Conclusions
",2018,we hope that the availability of the conceptual captions dataset will foster considerable progress on the automatic image-captioning task.
p18-1240,"
5 Conclusion
",2018,"in this paper, we study how to automatically generate textual reports for medical images, with the goal to help medical professionals produce reports more accurately and efficiently."
p18-1240,"
5 Conclusion
",2018,"our proposed methods address three major challenges: (1) how to generate multiple heterogeneous forms of information within a unified framework, (2) how to localize abnormal regions and produce accurate descriptions for them, (3) how to generate long texts that contain multiple sentences or even paragraphs."
p18-1240,"
5 Conclusion
",2018,"to cope with these challenges, we propose a multi-task learning framework which jointly predicts tags and generates descriptions."
p18-1241,"
5 Conclusion
",2018,"we believe this paper provides potential means to evaluate and possibly improve the robustness (for example, by adversarial training or data augmentation) of a wide range of visual language grounding and other nlp models."
p18-1243,"
6 Discussion
",2018,"although it might be non-trivial to extend the proposed approach to real natural language directly, we regard this work as an initial step towards this ultimate ambitious goal and our game might shed some light on designing more advanced games or performing real-world data collection."
p18-1243,"
6 Discussion
",2018,we plan to investigate the generalization and application of the proposed approach to more realistic environments with more diverse tasks in future work.
p18-1243,"
6 Discussion
",2018,"while offering flexibility in training, one downside of using a synthetic task is its limited amount of variation compared with real-world scenarios with natural languages."
p18-1244,"
6 Conclusions
",2018,"future work includes data collection and evaluation in a real world scenario since the data used in our experiments are simulated mixed speech, which is already extremely challenging but still leaves some acoustic aspects, such as lombard effects and real room impulse responses, that need to be alleviated for further performance improvement."
p18-1245,"
8 Conclusion
",2018,"as the model鈥檚 rich parameterization prevents tractable inference, we craft a variational inference procedure, based on the wake-sleep algorithm, to marginalize out the latent variables."
p18-1247,"
7 Conclusion and Future Work
",2018,we believe this framework can be extended to other sequence labeling tasks in nlp such as semantic role labeling.
p18-1249,"
7 Conclusion
",2018,our results suggest that further research into different ways of encoding utterances can lead to additional improvements in both parsing and other natural language processing tasks.
p18-1249,"
7 Conclusion
",2018,"the gains we see come not only from incorporating more information (such as subword features or externally-trained word representations), but also from structuring the architecture to separate different kinds of information from each other."
p18-1250,"
6 Conclusion
",2018,it should be a well-justified solution to identify empty categories as well as to integrate empty categories into syntactic analysis.
p18-1251,"
5 Future Work
",2018,"for future work, other potential bottlenecks could be addressed."
p18-1251,"
5 Future Work
",2018,the largest bottleneck is the queue used on the host to keep track of the edges to expand on the gpu.
p18-1251,"
5 Future Work
",2018,the queue will also require a mechanism to avoid inserting duplicate tuples into the queue.
p18-1251,"
5 Future Work
",2018,using a similar data structure on the gpu to keep track of the states to expand would yield higher speedups.
p18-1252,"
6 Conclusions and Future Work
",2018,"in future, we would like to advance this work in two directions: 1) proposing more effective conversion approaches, especially by exploring the potential of treelstms; 2) constructing bi-tree aligned data for other treebanks and exploiting all available single-tree and bi-tree labeled data for better conversion."
p18-1255,"
7 Conclusion
",2018,"first, we need it to be able to generalize: for instance by constructing templates of the form 鈥淲hat version of are you running?鈥 into which the system would need to fill a variable."
p18-1255,"
7 Conclusion
",2018,"in order to move to a full system that can help users like terry write better posts, there are three interesting lines of future work."
p18-1255,"
7 Conclusion
",2018,one can naturally extend our evpi approach to a full reinforcement learning approach to handle multi-turn conversations.
p18-1255,"
7 Conclusion
",2018,"second, in order to move from question ranking to question generation, one could consider sequence-to-sequence based neural network models that have recently proven to be effective for several language generation tasks (sutskever et al., 2014; serban et al., 2016; yin et al., 2016)."
p18-1256,"
8 Conclusion
",2018,"in future work, we would like to focus more on designing models that can deal with and be optimized for scenarios with severe data imbalance."
p18-1256,"
8 Conclusion
",2018,"we would like to also explore various applications of presupposition trigger prediction in language generation applications, as well as additional attention-based neural network architectures."
P19-1001,"
7 Conclusions and Future Work
",2019,"in the future, we plan to integrate our ioi model with models like elmo (peters et al., 2018) and bert (devlin et al., 2018) to study if the performance of ioi can be further improved."
P19-1002,"
5 Conclusion and Future Work

",2019,"in the future, we plan to apply reinforcement learning to further improve the performance."
P19-1003,"
6 Conclusion
",2019,the rewriter is trained to recover the coreferred and omitted information of user utterances.
P19-1003,"
6 Conclusion
",2019,we collect a high-quality manually annotated dataset and designed a transformer-pointer based architecture to train the utterance rewriter.
P19-1003,"
6 Conclusion
",2019,we hope the collected dataset and proposed model can benefit future related research.
P19-1004,"
5 Conclusion
",2019,we also foresee this paradigm being useful when building new dialog datasets to understand the kinds of information models use to solve them.
P19-1006,"
4 Conclusion and Future Work
",2019,"in the future, we would like to explore the effectiveness of various attention methods to solve indefinite choices task with interpretive features."
P19-1007,"
6 Conclusions and Future Work
",2019,"in the future, we want to incorporate this framework with much refined primal and dual models, and design more informative reward signals to make the training more efficient."
P19-1007,"
6 Conclusions and Future Work
",2019,"it would be appealing to apply graph neural networks (chen et al., 2018b, 2019) to model structured logical forms."
P19-1009,"
8 Conclusion
",2019,"for future work, we would like to extend our model to other semantic parsing tasks (oepen et al., 2014; abend and rappoport, 2013)."
P19-1009,"
8 Conclusion
",2019,"we are also interested in semantic parsing in cross-lingual settings (zhang et al., 2018; damonte and cohen, 2018)."
P19-1010,"
6 Conclusions
",2019,"for future direction, we are interested in exploring constrained decoding, better incorporating pre-trained language representations within our architecture, conditioning on additional relations between entities, and different gnn formulations."
P19-1011,"
6 Conclusion
",2019,"to this end, we explore four distinct strategies to convert pre-trained continuous sentence embeddings into a binarized form."
P19-1012,"
7 Discussion and Conclusion
",2019,we examined how the application of bilstms influences the modern transition- and graph-based parsing architectures.
P19-1012,"
7 Discussion and Conclusion
",2019,we leave this question for future work.
P19-1014,"
8 Conclusions
",2019,"in future work, we’d like to improve this integration in order to gain from training on examples from different domains for tags like ‘name’ and ‘location’."
P19-1016,"
5 Conclusions and Future Work
",2019,"our future work includes integrating advanced word representation methods (e.g., elmo and bert) and extending the proposed model to other tasks, such as event extraction and co-reference resolution."
P19-1016,"
5 Conclusions and Future Work
",2019,we also plan to incorporate external knowledge and common sense as additional signals into our architecture as they are important for human readers to recognize names but still absent from the current model.
P19-1017,"
6 Conclusions and Future Work
",2019,"for further works, we will leverage more supervised translation hops to improve the performance of unsupervised translation for distant languages."
P19-1017,"
6 Conclusions and Future Work
",2019,we will extend our method to more distant languages.
P19-1018,"
6 Conclusions
",2019,"we also find that translating in a common embedding space, as opposed to the target embedding space, obtains orthogonal gains for bli, and plan on investigating this in the semi-supervised setting in future work."
P19-1019,"
6 Conclusions and future work
",2019,"finally, we would like to adapt our approach to more relaxed scenarios with multiple languages and/or small parallel corpora."
P19-1019,"
6 Conclusions and future work
",2019,"in addition to that, we use our improved smt approach to initialize a dual nmt model that is further improved through on-the-fly backtranslation."
P19-1019,"
6 Conclusions and future work
",2019,"in addition to that, we would like to incorporate a language modeling loss during nmt training similar to he et al.(2016)."
P19-1019,"
6 Conclusions and future work
",2019,"in the future, we would like to explore learnable similarity functions like the one proposed by (mccallum et al., 2005) to compute the characterlevel scores in our initial phrase-table."
P19-1020,"
7 Conclusion
",2019,"we believe that adversarial regularization can be one of the common and fundamental technologies to further improve the translation quality, such as model ensemble, byte-pair encoding, and back-translation."
P19-1023,"
5 Conclusions
",2019,"in the future, we plan to explore contextbased similarity to complement the lexical similarity to improve the overall performance."
P19-1023,"
5 Conclusions
",2019,our model thus reduces the error propagation between relation extraction and ned that existing approaches are prone to.
P19-1023,"
5 Conclusions
",2019,"to obtain high-quality training data, we adapt distant supervision and augment it with co-reference resolution and paraphrase detection."
P19-1024,"
5 Conclusion
",2019,"one natural question we would like to ask is how to make use of the proposed framework to perform improved graph representation learning for graph related tasks (bastings et al., 2017)."
P19-1024,"
5 Conclusion
",2019,there are multiple venues for future work.
P19-1025,"
6 Conclusion
",2019,to evaluate the quality of the discovered topics we proposed a metric based on space-time scan statistics.
P19-1025,"
6 Conclusion
",2019,"while our study focused on spatial aggregation, aggregation on other types of metadata such as authors, hashtags, or communities is expected to work equally well and discover other types of distinct topics from large collections of documents."
P19-1027,"
4 Limitations and Conclusions
",2019,choose smaller subword vocabulary sizes in low-resource settings.
P19-1027,"
4 Limitations and Conclusions
",2019,"with limited computational resources, use small monolingual, non-contextual representations, such as bpemb combined with character embeddings."
P19-1028,"
6 Conclusions
",2019,our experiments were designed to explore the flexibility of our framework with different constraints in diverse tasks.
P19-1029,"
5 Conclusion
",2019,"future research directions will involve the development of reinforcement learning model with multi-dimensional rewards, and modeling explicit credit assignment for improving the capabilities of the regulator to make context-sensitive decisions in mini-batch learning."
P19-1029,"
5 Conclusion
",2019,"the proposed framework can naturally be expanded to integrate more feedback modes suitable for the interaction with humans, e.g., pairwise comparisons or output rankings."
P19-1032,"
4 Conclusion
",2019,"this mechanism allows for models with longer context, and thus with the capability to catch longer dependencies."
P19-1032,"
4 Conclusion
",2019,we have shown the importantce of this feature in the context of character level modeling where information is spread over great distances.
P19-1034,"
6 Conclusion
",2019,another interesting topic for future research is the comparison of domain adaptation based on our domain-specific word embeddings vs. based on word embeddings trained on much larger corpora.
P19-1034,"
6 Conclusion
",2019,"in the future, we plan to investigate whether there are changes over time that significantly impact the linguistic characteristics of the data, in the simplest case changes in the meaning of a word."
P19-1035,"
8 Conclusion
",2019,another strand of research will be combining both strategies and deploying the manipulation strategies in a large scale testing platform that allows the system to adapt to an individual learner over time.
P19-1035,"
8 Conclusion
",2019,future manipulation strategies that take the competencies into account have the potential to train particular skills and to better control the competencies required for a placement test.
P19-1035,"
8 Conclusion
",2019,our error analysis points out important directions for future work on detecting ambiguous gaps and modeling gap interdependencies for c-tests deviating from the default generation scheme.
P19-1036,"
6 Conclusion
",2019,this is certainly an avenue that we seek to explore.
P19-1036,"
6 Conclusion
",2019,we also investigated whether a consolidation step that removes non discriminant words from the label dictionaries could have an effect on performance.
P19-1036,"
6 Conclusion
",2019,"we have not explored whether recent advances in word embeddings from instance elmo (peters et al., 2018) and bert (devlin et al., 2018) could add further benefits."
P19-1037,"
9 Conclusion + Future Work
",2019,"additionally, we could improve the measure of lexical complexity for single and multi word expressions."
P19-1037,"
9 Conclusion + Future Work
",2019,"currently, we are only using frequency as an indicator of lexical complexity, however other factors such as word length, etymology, etc.may be used."
P19-1037,"
9 Conclusion + Future Work
",2019,doctors and patients speak a different language and we hope that our work will help them communicate.
P19-1037,"
9 Conclusion + Future Work
",2019,"finally, we will explore adaptations of our methodology for general (non-medical) domains, e.g., simplified search interfaces (ananiadou et al., 2013) for semantically annotated news (thompson et al., 2017)."
P19-1037,"
9 Conclusion + Future Work
",2019,one clear avenue of future work is to apply this system in a clinical setting and to test the results with actual patients.
P19-1037,"
9 Conclusion + Future Work
",2019,we could also look to use parallel simplified medical text to augment the general language parallel text used in the nts system.
P19-1038,"
8 Conclusion
",2019,"even though our work is an application of financial domain, we hope our multimodal learning model can also be useful in other areas (such as social media and customer service) where multimodality data is available."
P19-1039,"
6 Conclusions, Limitations, and Future
Directions
",2019,"first, more samples and machine learning experiments could have been done, had we had more time, funding, and resources."
P19-1039,"
6 Conclusions, Limitations, and Future
Directions
",2019,"fourth, additional experiments with identified significant acoustic and linguistic features would add more weight to the current paper."
P19-1039,"
6 Conclusions, Limitations, and Future
Directions
",2019,"in addition, this line of work might inspire new methods for detecting insider trading in financial markets."
P19-1039,"
6 Conclusions, Limitations, and Future
Directions
",2019,"lastly, we look forward to further exploring this line of research by investigating: 1. the individual differences in both detecting concealed information and concealing information, by analyzing the features across groups defined by individual personality traits (fornaciari et al., 2013, an et al., 2018), ethnics, native languages, and different dimensions of professional skills; 2. the result and model robustness by collecting and testing other field data such as board games; 3. the predictive power of phonotactic variation features; 4. the relationship between perceived information concealment and concealing information; 5. how soon can we detect concealed information; 6. how to conduct domain adaptation with regards to detecting concealed information; 7. efficient ways to make the multi-task learning framework scalable."
P19-1039,"
6 Conclusions, Limitations, and Future
Directions
",2019,the current study is by no means perfect.
P19-1039,"
6 Conclusions, Limitations, and Future
Directions
",2019,"third, more analyses of acoustics such as pitch and tonal contour, phonotactic variations could be incorporated to further explore the space of information concealment in speech."
P19-1040,"
5 Conclusions and Future Work
",2019,"our framework can be extended to deal with sources that generate a spectrum of perspectives, each with a stance relative to claim and with evidence supporting it."
P19-1040,5 Conclusions and Future Work,2019,we leave this for future work.
P19-1040,"
5 Conclusions and Future Work
",2019,"while we presented the framework here as applying to claims with two truth values, we believe that this framework can apply more broadly."
P19-1043,"
7 Conclusions and Future Work
",2019,"in the future, we would like to generalize it to multiple domains and datasets."
P19-1043,"
7 Conclusions and Future Work
",2019,"we are also interested in generating more effective and appropriate subjects by incorporating prior email conversations, social context, the goal and style of emails, personality, among others."
P19-1043,"
7 Conclusions and Future Work
",2019,we propose our model of subject generation by multi-sentence selection and rewriting with email subject quality estimation reward.
P19-1044,"
7 Conclusions and future work
",2019,"in the future, we plan to evaluate temporal referencing against the related dynamic embedding models on an annotated empirical lexical change dataset with multiple languages."
P19-1044,"
7 Conclusions and future work
",2019,next we evaluated whether the noise reduction carries over performance on a synthetic lexical semantic change detection task.
P19-1044,"
7 Conclusions and future work
",2019,"we also plan on testing how well temporal referencing deals with corpora that are too small for alignment-based methods, hopefully opening new avenues of quantitative research."
P19-1045,"
5 Conclusion
",2019,"in our future work, we would like to improve the model structure and the adversarial learning algorithm."
P19-1045,"
5 Conclusion
",2019,"last but not least, we would like to apply our approach to other heterogeneous texts-concerned nlp tasks."
P19-1045,"
5 Conclusion
",2019,"moreover, we would like to seek a stable and controllable way to conduct adversarial learning among more than two objects."
P19-1046,"
5 Conclusion
",2019,"in future work, we intend to explore multiple local fusion methods within our framework."
P19-1046,"
5 Conclusion
",2019,our fusion strategy is generic for other concrete fusion methods.
P19-1047,"
6 Conclusions and future work
",2019,"in this work we (a) correlate pragmatic features of analysts’ questions with the pre-call judgment of the questioner, (b) explore the influence of market, semantic and pragmatic features of earnings calls on analysts’ subsequent decisions."
P19-1047,"
6 Conclusions and future work
",2019,"promising directions for future research include examining additional features and feature representations: pragmatic features such as formality (pavlick and tetreault, 2016) or politeness (danescu-niculescu-mizil et al., 2013); acousticprosodic features from earnings call audio; more sophisticated semantic representations such as claims (lim et al., 2016), automatically induced entity-relation graphs (bansal et al., 2017) or question-answer motifs (zhang et al., 2017) (these representations are non-trivial to construct because a single turn may contain many questions or answers); or even discourse structures."
P19-1047,"
6 Conclusions and future work
",2019,"the models used in this work aim to be just complex enough to determine whether useful signals exist for this task; future modeling work could include training a complete end-to-end system such as a hierarchical attention network (yang et al., 2016), or building industry-specific models."
P19-1048,"
5 Conclusion
",2019,"the proposed architecture can potentially be applied to similar tasks such as relation extraction, semantic role labeling, etc."
P19-1049,"
5 Conclusion
",2019,it would also be nice to explore about more fine-grained functional components and grammatical entities in the future works.
P19-1050,"
7 Conclusion
",2019,"building upon this dataset, future research can explore the design of efficient multimodal fusion algorithms, novel erc frameworks, as well as the extraction of new features from the audio, visual, and textual modalities."
P19-1050,"
7 Conclusion
",2019,we believe this dataset will also be useful as a training corpus for both conversational emotion recognition and multimodal empathetic response generation.
P19-1052,"
6 Conclusion
",2019,"in order to solve the problem of lacking aspect-level labeled data, we wish to utilize the abundant document-level labeled data."
P19-1053,"
6 Conclusion and Future Work
",2019,"thus, we plan to extend our approach to other neural nlp tasks with attention mechanisms, such as neural document classification (yang et al., 2016) and neural machine translation (zhang et al., 2018)."
P19-1054,"
6 Conclusion
",2019,future work should thus study the overlapping nature of argument clustering.
P19-1054,"
6 Conclusion
",2019,the first challenge is to identify suitable arguments.
P19-1055,"
7 Conclusions
",2019,"the modular approach also provides interesting directions for future research, focusing on alleviating the supervision bottleneck by using large amount of partially labeled data that are cheaper and easy to obtain, together with only a handful amount of annotated data, a scenario especially suitable for low-resource languages."
P19-1055,"
7 Conclusions
",2019,we experiment with several sentiment analysis tasks.
P19-1056,"
5 Conclusion
",2019,the framework uses a joint sequence labeling approach and focuses on the interaction between two separate routes for aspect term extraction and aspect sentiment classification.
P19-1058,"
6 Conclusion
",2019,"in the future work, we will focus on how to mine different representations for different discourse relation types and apply the topic information to other languages."
P19-1059,"
5 Conclusion
",2019,future work should provide further exploration of the data regime in which pragmatic learning is most beneficial and its correspondence to realworld language use.
P19-1060,"
6 Conclusion
",2019,"as part of future work, we would like to investigate the use of contextualized embeddings (e.g., bert, devlin et al.(2018)) for coherence assessment – as such representations have been shown to carry syntactic information of words (tenney et al., 2019) – and whether they allow multi-task learning frameworks to learn complementary aspects of language."
P19-1061,"
6 Conclusions and Future Work
",2019,"in future work, we will enrich our weak supervision system by giving the lfs access to more sophisticated contexts that take into account global structuring constraints in order to see how they compare to exogenous decoding constraints applied in (muller et al., 2012; perret et al., 2016)."
P19-1062,"
6 Conclusion
",2019,"nevertheless, calculating statistics on these trees and comparing them to parsed rst trees shows they still contain no meaningful discourse structure."
P19-1064,"
5 Conclusion
",2019,another interesting direction would be introducing intermediate step rewards for each action to better guide the behaviour of the rl agent.
P19-1064,"
5 Conclusion
",2019,our model transforms the supervised higher order coreference model to a policy gradient model that can directly optimizes coreference evaluation metrics.
P19-1064,"
5 Conclusion
",2019,"there are several potential improvements to our model as future work, such as incorporating mention detection result as a part of the reward."
P19-1065,"
6 Conclusion and Future Work
",2019,"since implicit discourse relation identification is a key task for dialogue systems, there are still many approaches worth investigating in future work."
P19-1066,"
8 Conclusion
",2019,"furthermore, we can consider more structured representations of entities that reflect entity attributes and inter-entity relations."
P19-1066,"
8 Conclusion
",2019,in the future we plan to further enrich these representations by considering information from across the document.
P19-1068,"
7 Conclusion
",2019,another option for improving performance is to combine string kernels and neural networks into an ensemble model.
P19-1068,"
7 Conclusion
",2019,"in future work, we aim to determine if the same level of accuracy can be obtained when single sentences will be used as samples for training and testing."
P19-1068,"
7 Conclusion
",2019,we leave these ideas for future exploration.
P19-1068,"
7 Conclusion
",2019,we would like to stress out that the methods presented in this paper are only provided as baselines in order to enable comparisons in future work.
P19-1069,"
6 Conclusions
",2019,"as future work we plan to exploit a subset of the wikipedia categories as coarse-grained sense inventory and enrich our dataset with coarser labels, hence enabling wsd at different granularities."
P19-1070,"
5 Conclusion
",2019,we hope that this study will encourage future work on cle evaluation and analysis and help guide the development of new cle models.
P19-1071,"
8 Conclusion
",2019,"compared with other evaluation methods, sp-10k has much larger coverage and can better represent ground truth sp."
P19-1072,"
7 Conclusion
",2019,we further improved the performance of the best approach with the application of mean-centering as an important pre-processing step for rotational vector space alignment.
P19-1076,"
7 Conclusion
",2019,"we propose that this new metric, which we colloquially refer to as consistency, be adopted alongside evaluations of global topic quality for future work with topic model comparison."
P19-1077,"
9 Discussion and Conclusions
",2019,"as a rule of thumb, of the two best-known forms of crowdsourcing, microtask crowdsourcing using platforms such as amazon mechanical turk is best to label small to medium size amounts of data in a short time, and for labelling data of no intrinsic interest."
P19-1078,"
7 Conclusion
",2019,"in future work, transferring knowledge from other resources can be applied to further improve zero-shot performance, and collecting a dataset with a large number of domains is able to facilitate the application and study of metalearning techniques within multi-domain dst."
P19-1079,"
7 Conclusions
",2019,we further explored learning these features in parallel and serial mtl architectures.
P19-1080,"
7 Conclusions
",2019,we also propose a constrained decoding technique that leverages tree-structured mrs to exert precise control over the discourse structure and semantic correctness of the generated text.
P19-1083,"
8 Conclusion
",2019,"the proposed model is an attempt of the general solution of incorporating knowledge (in the form of kg) into the deep learning based pronoun coreference model, rather than using knowledge as features or rules in a dedicated manner."
P19-1084,"
8 Conclusion
",2019,"our methodology can be extended to handle biases in other tasks where one is concerned with finding relationships between two objects, such as visual question answering, story cloze completion, and reading comprehension."
P19-1084,"
8 Conclusion
",2019,we hope to encourage such investigation in the broader community.
P19-1085,"
6 Conclusion
",2019,"in the future, we would like to design a multi-step evidence extractor and incorporate external knowledge into our framework."
P19-1086,"
8 Conclusion
",2019,"although sherliic’s creation is entirely data-driven, it shows a large variety of linguistic challenges for nli, ranging from lexical relations like troponymy, synonymy or morph.derivation to typical actions and common sense knowledge (cf.table 1)."
P19-1086,"
8 Conclusion
",2019,"the large unlabeled resources, sherliic-infcands and sherliic-teg, are potentially useful for further linguistic analysis (as we showed in § 6), as well as for data-driven models of lexical semantics, e.g., techniques such as representation learning and domain adaptation."
P19-1086,"
8 Conclusion
",2019,we hope that sherliic will foster better modeling of lexical inference in context as well as progress in nli in general.
P19-1087,"
7 Conclusions
",2019,training the model on asr transcripts or on both asr and manual transcripts does not help bridge the performance gap.
P19-1087,"
7 Conclusions
",2019,we plan to investigate the impact of combining the two models.
P19-1088,"
8 Conclusions
",2019,"in the future, we plan to build upon the acquired knowledge and the developed classifiers to generate systems able to provide actionable feedback on how to achieve high-quality counseling."
P19-1088,"
8 Conclusions
",2019,"this is an important finding as an open problem in the counseling field is the need for computational tools that allow scaling-up the evaluation of the quality of mi interventions (atkins et al., 2014)."
P19-1089,"
7 Discussion and Future Work
",2019,future work could adapt our framework to examine more complex forms of conversational development.
P19-1089,"
7 Discussion and Future Work
",2019,"future work could more directly model how counselors respond to texter behaviors, hence gauging the extent to which counselors evolve in their interactional practices."
P19-1089,"
7 Discussion and Future Work
",2019,"in order to derive such prescriptive recommendations, further work is needed to causally connect our signals of linguistic experience with actual expertise and skill, as reflected in conversational outcomes."
P19-1089,"
7 Discussion and Future Work
",2019,"other approaches, such as qualitative labeling by domain experts, could examine whether such changes in language use also result in better conversations."
P19-1089,"
7 Discussion and Future Work
",2019,"our methodology could also be extended to examine other conversational contexts such as academic advising or business interactions, where individuals are expected to learn from experience."
P19-1089,"
7 Discussion and Future Work
",2019,"such domains may contain crucial differences that motivate extensions to our framework; for example, feedback might be more readily available and conversational partners may recur, both of which can interact with experience to further shape linguistic development."
P19-1089,"
7 Discussion and Future Work
",2019,understanding how different components in a conversation change with experience could also inform the particular aspects to focus their training on.
P19-1090,"
6 Conclusion and Future Work
",2019,"a next step would be comprehensive error analysis for a better understanding of where the models succeed or fail in capturing semantic information, particularly for the low-resource languages."
P19-1090,"
6 Conclusion and Future Work
",2019,"in the case where the human does not agree with any of the suggested answers, the option can remain for the human operator to manually select the correct standardized response, as is currently done."
P19-1090,"
6 Conclusion and Future Work
",2019,we are also working to include into our models the long tail in the distribution of template answers.
P19-1090,"
6 Conclusion and Future Work
",2019,"we considered various approaches to the answer selection problem in a noisy, multilingual, low-resource setting."
P19-1090,"
6 Conclusion and Future Work
",2019,"we further intend to explore transfer learning techniques (zhang et al., 2017) as well as deep architectures designed specifically for answer selection (lai et al., 2018)."
P19-1091,"
5 Conclusion
",2019,"for future work, we plan to investigate the model on other related tasks such as relation extraction, normalization as well as the use of advanced conditional models."
P19-1092,"
5 Conclusion
",2019,"such questions correspond to examinations to access specialized positions in the spanish healthcare system, and require specialized knowledge and reasoning to be answered."
P19-1092,"
5 Conclusion
",2019,we hope this work will encourage research on designing more powerful qa systems that can carry out effective information extraction and reasoning.
P19-1093,"
7 Discussion and future work

",2019,"a possibility that we did not expand on in this paper is to pre-train one leg of the network on an argument detection data set, like the one of shnarch et al.(2018)."
P19-1093,"
7 Discussion and future work

",2019,"in addition, more careful design of the architecture details, which was not the focus of this work, will probably yield better results yet, e.g., contextualized word embeddings (peters et al., 2018), batch normalization (ioffe and szegedy, 2015; cooijmans et al., 2017), deeper networks and other architecture practical heuristics."
P19-1093,"
7 Discussion and future work

",2019,"in the future we aim to test and adapt other improvements in the learning to rank field to our task, hoping for further improvement by those models (burges, 2010; severyn and moschitti, 2015)."
P19-1093,"
7 Discussion and future work

",2019,we believe that it is useful to evaluate methods for identifying the more convincing argument on this more challenging data set.
P19-1094,"
8 Conclusion
",2019,"this work introduced a new task, debate topic expansion, along with a corresponding benchmark dataset, which we plan to make publicly available."
P19-1094,"
8 Conclusion
",2019,we plan to pursue these research directions in future work.
P19-1096,"
6 Conclusions and Future Work
",2019,"in the future work, we will try to build a one-step model that directly extract the emotion-cause pairs in an end-to-end fashion."
P19-1096,"
6 Conclusions and Future Work
",2019,"on the other hand, the mistakes made in the first step will affect the results of the second step."
P19-1098,"
7 Conclusion
",2019,we strengthen a dpp-based multi-document summarization system with improved similarity measure inspired by capsule networks for determining sentence redundancy.
P19-1100,"
5 Conclusion
",2019,our detailed observations can provide more hints for the follow-up researchers to design more powerful learning frameworks.
P19-1101,"
4 Conclusion and Future Work
",2019,a summarizer intends to extract or generate a summary maximizing θi .
P19-1101,"
4 Conclusion and Future Work
",2019,"by aggregating over different people but in one domain, one can uncover a domain-specific k. similarly, by aggregating over many topics for one person, one would find a personalized k. these consistute promising research directions for future works."
P19-1101,"
4 Conclusion and Future Work
",2019,"characters, character n-grams, morphemes, words, n-grams, phrases, and sentences do not actually qualify as semantic units."
P19-1101,"
4 Conclusion and Future Work
",2019,"for the background knowledge k, a promising direction would be to use the framework to actually learn it from data."
P19-1101,"
4 Conclusion and Future Work
",2019,"in particular, one can apply supervised techniques to automatically search for k, α and β: finding the values of these parameters such that θi has the best correlation with human judgments."
P19-1101,"
4 Conclusion and Future Work
",2019,they are design choices which can be explored empirically by subsequent works.
P19-1101,"
4 Conclusion and Future Work
",2019,"this fits within the general optimization framework for summarization (mcdonald, 2007; peyrard and eckle-kohler, 2017b; peyrard and gurevych, 2018) the background knowledge and the choice of semantic units are free parameters of the theory."
P19-1102,"
8 Conclusion
",2019,in the future we plan to explore interactions among documents beyond concatenation and experiment with summarizing longer input documents.
P19-1103,"
6 Conclusion
",2019,"in the future, we would like to evaluate the attacking effectiveness and efficiency of our methods on more datasets and models, and do elaborate human evaluation on the similarity between clean texts and the corresponding adversarial examples."
P19-1104,"
6 Conclusion
",2019,"our study opens up interesting avenues for future research: obfuscation by addition instead of by reduction, development of more powerful, targeted paraphrasing operators, and, theoretical analysis of the search space properties."
P19-1104,"
6 Conclusion
",2019,"we consider heuristic search-based obfuscation a key enabling technology that, combined with tailored deep generative models for paraphrasing, will yield better and stronger obfuscations."
P19-1105,"
5 Conclusion
",2019,"future work will include: (i) incorporating lexical semantics such as named entities for further improvement, (ii) comparing our model to other deep contextualized word representation such as elmo and bert, and (iii) applying the method to other domains for quantitative evaluation."
P19-1106,"
7 Conclusion
",2019,"however, considering the sensitivity of the topic, we would like to further dive deep into exploring the subtle nuances that leads into the grading of peer review aspects."
P19-1106,"
7 Conclusion
",2019,we aim to include review reliability prediction in the pipeline of our future work.
P19-1106,"
7 Conclusion
",2019,we intend to work upon those and also explore more sophisticated techniques for sentiment polarity encoding.
P19-1106,"
7 Conclusion
",2019,"with further exploration, we aim to mould the ongoing research to an efficient ai-enabled system that would assist the journal editors or conference chairs in making informed decisions."
P19-1107,"
7 Conclusion
",2019,"by incorporating external embeddings with gating mechanism, our model can achieve further improvement with better conversational-context representation."
P19-1107,"
7 Conclusion
",2019,"this work is easy to scale and can potentially be applied to any speech related task that can benefit from longer context information, such as spoken dialog system, sentimental analysis."
P19-1108,"
8 Conclusions
",2019,"also, a similar application to improve disaster mention detection could be useful (for figurative sentences such as ‘my heart is on fire’)."
P19-1108,"
8 Conclusions
",2019,"other ways of combining the two modules, more sophisticated classifiers for both phm detection and figurative usage detection, are possible directions of future work."
P19-1109,"
5 Conclusions
",2019,"finally, we plan to investigate alternative methods to modelling phrase and multi-word expression complexity."
P19-1109,"
5 Conclusions
",2019,our future research will focus on the relative nature of complexity judgements and will use the seq model to predict complexity on a scale.
P19-1109,"
5 Conclusions
",2019,we will also investigate whether the seq model may benefit from sources of information other than word embeddings and character-level morphology.
P19-1111,"
4 Conclusion
",2019,"from a pedagogical perspective, it will be beneficial for learners of the language to look into the prose of the verses for an easier comprehension of the concepts discussed in the verse."
P19-1112,"
7 Conclusion
",2019,"as future work, we plan to investigate emphasis selection on a larger and more diverse dataset."
P19-1112,"
7 Conclusion
",2019,its goal is to develop models that suggest which part of the text to emphasize.
P19-1112,"
7 Conclusion
",2019,"we also plan to investigate the role of word sentiment and emotion intensity as well as more advanced language models such as bert (devlin et al., 2018) in modeling emphasis."
P19-1113,"
5 Conclusion
",2019,"this model incorporates the user credibility information into the rumor detection layer, and uses attention mechanism in the rumor detection process."
P19-1114,"
6 Conclusions and Future Work

",2019,"after collecting more labeled data, and tuning our model using anomaly detection techniques like isolation forests (liu et al., 2008), we hope to expand this study to the stage where we are able to use unbalanced data sets."
P19-1114,"
6 Conclusions and Future Work

",2019,"given that, in our future work, we will be investigating those false positive cases with our collaborators to assess what the correct label for these ads should be."
P19-1114,"
6 Conclusions and Future Work

",2019,"moreover, since the proposed full feature set involves hundreds of features we plan to increase our sample size to have a better estimation of the performance of our final predictor."
P19-1115,"
7 Conclusion
",2019,promising future work includes extension to tree-structured inputs and application to other tasks.
P19-1117,"
7 Conclusion
",2019,we first introduce a representor for replacing both encoder and decoder so as to fully explore the commonality among languages.
P19-1118,"
6 Conclusions
",2019,we publicly release our system to support mt communities especially for low-resource setups.
P19-1119,"
8 Conclusion
",2019,"in previous methods, the pre-trained ubwe is only used to initialize the word embedding of unmt."
P19-1120,"
7 Conclusion
",2019,"5 as for future work, we will test our methods in the nmt transfer where the target language is switched."
P19-1120,"
7 Conclusion
",2019,"lastly, we reuse parallel data of the parent language pair in the child training phase to avoid an abrupt change of the training data distribution."
P19-1120,"
7 Conclusion
",2019,"we also plan to compare different algorithms for learning the cross-lingual mapping (artetxe et al., 2018a; xu et al., 2018; joulin et al., 2018) to optimize the transfer performance."
P19-1122,"
7 Conclusions & Future Work
",2019,"finally, we hope that future work in this area will follow our lead in using carefully-controlled experiments to enable meaningful comparisons."
P19-1122,"
7 Conclusions & Future Work
",2019,"while our method is currently restricted to languages that have reliable constituency parsers, an exciting future direction is to explore unsupervised tree induction methods for low-resource target languages (drozdov et al., 2019)."
P19-1123,"
6 Conclusion
",2019,"in future, we would like to extend the method to handle more than two curricula objectives."
P19-1123,"
6 Conclusion
",2019,"we further refine the co-curriculum with an em-style optimization procedure and show its effectiveness, in particular on small-capacity models."
P19-1123,"
6 Conclusion
",2019,"we present a co-curricular learning method to make domain-data selection work better on noisy data, by dynamically composing it with clean-data selection."
P19-1124,"
6 Conclusions and Future Work
",2019,"in the future, we believe more work on improving cfs alignment is potential to improve translation quality, and we will investigate on using source context and target history context in a more robust manner for better predicting cfs and cft words."
P19-1125,"
7 Conclusion
",2019,another direction is to apply the proposed imitation learning framework to similar scenarios such as simultaneous interpretation.
P19-1125,"
7 Conclusion
",2019,"as a future work, we can try to improve the performance of the nmt by introducing more powerful demonstrator with different structure (e.g.right to left)."
P19-1128,"
6 Conclusion and Future Work
",2019,"our model can also be considered as a more generic framework for graph generation problem with unstructured input other than text, e.g.image, video, audio."
P19-1132,"
5 Conclusion
",2019,"in the future work, we will explore the usage of this method with other applications."
P19-1132,"
5 Conclusion
",2019,"our idea of encoding a passage regarding multiple entities has potentially broader applications beyond relation extraction, e.g., entity-centric passage encoding in question answering (song et al., 2018a)."
P19-1133,"
5 Conclusion
",2019,"future work will investigate more complex and recent neural network models such as devlin et al.(2018), as well as alternative losses."
P19-1134,"
7 Conclusion
",2019,"because of its generic architecture, distre allows for integration of additional contextual information, e.g.background knowledge about entities and relations, which could also prove useful to further improve performance."
P19-1134,"
7 Conclusion
",2019,"in future work, we want to further investigate the extent of syntactic structure captured in deep language language representations."
P19-1135,"
6 Conclusion
",2019,"a bootstrap learning procedure is built to iteratively improve the model, training data and trustable pattern set."
P19-1135,"
6 Conclusion
",2019,"in the future, we hope to improve our work by the utilization of better model-based pattern extractor, and resorting to latent variable model (kim et al., 2018) for jointly modeling instance selector."
P19-1135,"
6 Conclusion
",2019,"what is more, we also hope to verify the effectiveness of our method on more tasks, including open information extraction and event extraction, and also overlapping relation extraction models (dai et al., 2019)."
P19-1135,"
6 Conclusion
",2019,"with a more interpretable model, we then conduct noise reduction by evaluating how well the model explains the relation of an instance."
P19-1136,"
6 Conclusion
",2019,we evaluate the proposed method on the nyt and webnlg datasets.
P19-1137,"
5 Conclusion and Future Work
",2019,"for the future work, we plan to extend diagnre to other ds-based applications, such as question answering (lin et al., 2018), event extraction (chen et al., 2017), etc."
P19-1139,"
5 Conclusion
",2019,"there are three important directions remain for future research: (1) inject knowledge into feature-based pre-training models such as elmo (peters et al., 2018); (2) introduce diverse structured knowledge into language representation models such as conceptnet (speer and havasi, 2012) which is different from the world knowledge database wikidata; (3) annotate more real-world corpora heuristically for building larger pre-training data."
P19-1139,"
5 Conclusion
",2019,these directions may lead to more general and effective language understanding.
P19-1140,"
7 Conclusions
",2019,"in future, we are interested in introducing text information of entities for alignment by considering word ambiguity (cao et al., 2017b); and meanwhile, through cross-kg entity proximity (cao et al., 2015)."
P19-1140,"
7 Conclusions
",2019,"through two channels, mugnn not only explicitly completes the kgs, but also pruning exclusive entities by using different relation weighting schemes: kg selfattention and cross-kg attention, showing robust graph encoding capability."
P19-1141,"
4 Conclusion and Future Work
",2019,"although we specifically investigated the ner task for chinese in this work, we believe the proposed model can be extended and applied to other languages, for which we leave as future work."
P19-1142,"
8 Conclusion
",2019,"also, it will be worthwhile to ascertain the efficacy of pdr for language models using transformers and in combination with frage embeddings."
P19-1142,"
8 Conclusion
",2019,future work includes exploring the application of pdr to other seq2seq models that have a similar input-output symmetry.
P19-1144,"
7 Conclusion
",2019,"in future work, we aim to capture better structural information and possible connections to unsupervised grammar induction."
P19-1145,"
6 Conclusion
",2019,we believe that this direction paves the way for more efficient and effective representation learning in nlp.
P19-1145,"
6 Conclusion
",2019,we evaluate these models on eight different nlp tasks and a total of thirteen data sets.
P19-1146,"
6 Conclusion and Future Work
",2019,"a natural next step is to apply entmax to self-attention (vaswani et al.,2017)."
P19-1146,"
6 Conclusion and Future Work
",2019,"in a different vein, the strong morphological inflection results point to usefulness in other tasks where probability is concentrated in a small number of hypotheses, such as speech recognition."
P19-1147,"
9 Conclusions
",2019,future work includes developing a adversarial training scheme as well as devising a more robust architecture based on our findings.
P19-1149,"
7 Conclusion and Future Work
",2019,"in the future, we are interested in testing lowlevel optimizations of lrn, which are orthogonal to this work, such as dedicated cudnn kernels."
P19-1150,"
5 Conclusion
",2019,"in the future, we plan to apply capsule networks to even more challenging nlp problems such as language modeling and text generation."
P19-1150,"
5 Conclusion
",2019,"through our modifications and enhancements, we hope to have made capsule networks more suitable to large-scale problems and, hence, more mature for real-world applications."
P19-1153,"
4 Conclusion & Future Work
",2019,"continuing in the direction of training our model on different nlp tasks, we would like our representations to generalize well on downstream tasks while maintaining their reconstruction property."
P19-1153,"
4 Conclusion & Future Work
",2019,"finally, we would like to learn our sentence embeddings’ latent space, similarly to subramanian et al.(2018)’s method, so as to leverage our autoencoder’s strong reconstruction ability and generate very long sequences of text."
P19-1153,"
4 Conclusion & Future Work
",2019,we would also like to further explore the usage of sub-sentence representations in natural language processing.
P19-1154,"
8 Conclusion
",2019,"in addition, we propose an online vocabulary updating algorithm for further performance enhancement by tracking users behavior effectively."
P19-1159,"
5 Conclusion and Future Directions
",2019,"below, we identify a few future directions."
P19-1159,"
5 Conclusion and Future Directions
",2019,discussions between computer scientists and sociologists may improve understanding of latent gender bias found in machine learning data sets and model predictions.
P19-1159,"
5 Conclusion and Future Directions
",2019,future work can look to apply existing methods or devise new techniques towards mitigating gender bias in other languages as well.
P19-1159,"
5 Conclusion and Future Directions
",2019,it remains to be discovered how these individual parts harmonize together to form an ideally unbiased system.
P19-1159,"
5 Conclusion and Future Directions
",2019,"non-binary genders (richards et al., 2016) as well as racial biases have largely been ignored in nlp and should be considered in future work."
P19-1159,"
5 Conclusion and Future Directions
",2019,"second, most gender debiasing methods have only been empirically verified in limited applications (zhang et al., 2018; zhao et al., 2017), and it is not clear that these methods can generalize to other tasks or models."
P19-1159,"
5 Conclusion and Future Directions
",2019,"similar issues of algorithmic bias have also been discussed extensively in artificial intelligence, machine learning, data mining, and several other application fields (e.g., (calders and verwer, 2010; feldman et al., 2015; hardt et al., 2016; misra et al., 2016; kleinberg et al., 2016; pleiss et al., 2017; beutel et al., 2017; misra et al., 2016))."
P19-1159,"
5 Conclusion and Future Directions
",2019,"to perform gender-swapping in such languages, besides swapping those gendered nouns, we also need to change the modifiers.non-binary gender bias."
P19-1160,"
5 Conclusion

",2019,"in future, we plan to extend the proposed method to debias other types of demographic biases such as ethnic, age or religious biases."
P19-1161,"
7 Conclusion

",2019,"finally, we also identified avenues for future work, such as the inclusion of co-reference information."
P19-1165,"
6 Conclusions
",2019,"finally, the introduction of an output layer which predicts pretrained embeddings enables us to use larger vocabularies instead of using the slower softmax."
P19-1165,"
6 Conclusions
",2019,trying more complex networks is also within our scope and is left as future work.
P19-1165,"
6 Conclusions
",2019,"we did, in fact, explore alternative configurations, for instance, using several layers or replacing the lstms with gated recurrent units (cho et al., 2014) or the transformer architecture (vaswani et al., 2017)."
P19-1165,"
6 Conclusions
",2019,we release the word and sense embeddings at the following url: http: //lcl.uniroma1.it/lstmembed.
P19-1167,"
5 Conclusion and Limitations
",2019,"in future work, we intend to conduct a diachronic analysis in english using the same corpus, in addition to a cross-linguistic study of gendered language."
P19-1167,"
5 Conclusion and Limitations
",2019,our study has a few limitations that we wish to highlight.
P19-1168,"
5 Conclusion
",2019,it may be of interest to extend these techniques to embed knowledge graph elements.
P19-1169,"
5 Conclusion and Future Work

",2019,"additionally, the mapping technique used for relation-aware semantic projection can be further improved to model different linguistic properties of lexical relations (e.g., the “one-to-many” mappings for meronymy)."
P19-1169,"
5 Conclusion and Future Work

",2019,"in the future, we will improve our model to deal with datasets containing a relatively large number of lexical relation types and random term pairs."
P19-1170,"
5 Conclusion
",2019,"finally, the preliminary results we have shown on aligning more than two languages at the same time provide an exciting path for future research."
P19-1171,"
6 Conclusion
",2019,"an avenue for future work is connecting our discovered phonesthemes to putative meanings, as done by abramova et al.(2013) and abramova and fernandez ′ (2016)."
P19-1173,"
8 Conclusions and Future Work
",2019,"future work includes joint and cross-dialectal lemmatization models, in addition to further extension to other dialects."
P19-1173,"
8 Conclusions and Future Work
",2019,we also presented several extensions for cross-dialectal modeling.
P19-1174,"
7 Conclusion and Future Work
",2019,"in future work, we will further explore the effectiveness of the reordering mechanism and apply it to other natural language processing tasks, such dependency parsing (zhang et al., 2016; li et al., 2018), and semantic role labeling (he et al., 2018; li et al., 2019)."
P19-1175,"
7 Conclusion
",2019,"in a next step, we plan to compare the performance of nfr to other approaches to tm-nmt integration, for example by carrying out evaluations on the jrc-acquis corpus (gu et al., 2018; koehn and senellart, 2010a; zhang et al., 2018)."
P19-1175,"
7 Conclusion
",2019,"in addition, it would be informative to carry out a qualitative analysis of the nfr output in terms of how and to what extent the information contained in the fuzzy matches is used in the final translation, in comparison with the nmt baseline."
P19-1175,"
7 Conclusion
",2019,"the approach also needs to be tested on data sets with a lower frequency of repeated sentences, other language pairs as well as different domains, ultimately also involving human evaluation (both in term of perceived quality and post-editing time)."
P19-1175,"
7 Conclusion
",2019,"we also intend to carry out further tests to potentially improve the quality of the output, for example by testing different match metrics and retrieval methods, nmt architectures (e.g.transformer), ways to include alignment information and by applying additional morphological preprocessing."
P19-1175,"
7 Conclusion
",2019,we believe that the ease of implementation of nfr could lead to the wider adoption of tm-nmt integration.
P19-1177,"
5 Analysis and Conclusion
",2019,"by examining the results, we found the syntaxbased model tends to produce one translation in active voice and another in passive voice."
P19-1178,"
5 Conclusions and Future Work
",2019,"as future work, we will apply our methodology to domain adaptation."
P19-1178,"
5 Conclusions and Future Work
",2019,"in the same vain as unsupervised mt, we want to continue our research by using back translation for rejected pairs and dealing with phrases instead of full sentences."
P19-1178,"
5 Conclusions and Future Work
",2019,our architecture is also useful for data selection in data rich language pairs and we will perform experiments on cleaning noisy parallel corpora.
P19-1178,"
5 Conclusions and Future Work
",2019,that will allow us to extract more parallel text from a corpus and facilitate using these approaches for low-resourced languages.
P19-1178,"
5 Conclusions and Future Work
",2019,"we focus on data representation, an adequate function for the selection process, and studying how to avoid additional hyperparameters that depend on the input corpus."
P19-1179,"
6 Conclusion
",2019,"our method does not introduce additional parameters: we hope to motivate future work on learning speech representations, with continued performance on lowerresource settings if additional parameters are introduced."
P19-1180,"
5 Discussion
",2019,"along the way, the model acquires estimates of word concreteness."
P19-1180,"
5 Discussion
",2019,"finally, it may be possible to extend our approach to other linguistic tasks such as dependency parsing (christie et al., 2016b), coreference resolution (kottur et al., 2018), and learning pragmatics beyond semantics (andreas and klein, 2016)."
P19-1180,"
5 Discussion
",2019,"its performance may be boosted by considering structured representations of both images (e.g., lu et al., 2016; wu et al., 2019) and texts (steedman, 2000)."
P19-1180,"
5 Discussion
",2019,the results suggest multiple future research directions.
P19-1180,"
5 Discussion
",2019,there are also limitations to the idea of grounded language acquisition.
P19-1181,"
6 Conclusion
",2019,"for example, future extensions of vln will likely involve games (baldridge et al., 2018) where the instructions being given take the agent around a trap or help it avoid opponents."
P19-1181,"
6 Conclusion
",2019,"furthermore, our findings suggests ways that future datasets and metrics for judging agents should be constructed and set up for evaluation."
P19-1181,"
6 Conclusion
",2019,future agents will need to make effective use of language and its connection to the environment to both drive cls up and bring ne down in r4r.
P19-1181,"
6 Conclusion
",2019,"in such scenarios, going straight to the goal could be literally deadly to the robot or agent."
P19-1181,"
6 Conclusion
",2019,"similar constraints could hold in search-and-rescue human-robot teams (kruijff et al., 2014; kruijff-korbayov et al., 2016), where the direct path could take a rolling robot into an area with greater danger of collapse."
P19-1182,"
6 Conclusion
",2019,"for future work, we are going to further explore the possibility to merge the three datasets by either learning a joint image representation or by transferring domain-specific knowledge."
P19-1182,"
6 Conclusion
",2019,we are also aiming to enlarge our image editing request dataset with newly-released posts on reddit and zhopped.
P19-1183,"
6 Conclusion
",2019,we handled this task based on the multiple instance learning framework.
P19-1184,"
8 Conclusion
",2019,"in future work, the data can be used to further investigate common ground and conceptual pacts; be extended through manual annotations for a more thorough linguistic analysis of co-reference chains; exploit the combination of vision and language to develop computational models for referring expression generation; or use the photobook task in the parlai framework for turing-test-like evaluation of dialogue agents."
P19-1184,"
8 Conclusion
",2019,our results suggest that more sophisticated models are needed to fully exploit shared linguistic history.
P19-1185,"
8 Conclusion
",2019,we also explore multi-task cell learning for generalizability.
P19-1187,"
6 Conclusions
",2019,in the future work we would like to consider setups where human-annotated data is combined with naturally occurring one (i.e.distantly-supervised one).
P19-1187,"
6 Conclusions
",2019,it would also be interesting to see if mistakes made by fully-supervised systems differ from the ones made by our system and other wikipediabased linkers.
P19-1188,"
9 Conclusion
",2019,"as a result, much of ai and nlp community has focused on making larger and larger datasets, but we believe it is equally important to go the other direction and explore methods that help performance with little data."
P19-1189,"
6 Conclusion
",2019,"through the generator, different instances from the source domain are selected to train a task-specific predictor."
P19-1190,"
6 Conclusion
",2019,"as future work, we will consider integrating more kinds of syntactic features from linguistic analysis such as dependency parsing."
P19-1191,"
5 Conclusions and Future Work
",2019,"in the future, we plan to develop techniques for extracting entities of more fine-grained entity types, and extend paperrobot to write related work, predict authors, their affiliations and publication venues."
P19-1191,"
5 Conclusions and Future Work
",2019,paperrobot is merely an assistant to help scientists speed up scientific discovery and production.
P19-1192,"
5 Conclusion and Future work
",2019,"in the future, we will investigate the possibility of incorporating additional forms of rhetoric, such as parallelism and exaggeration, to further enhance the model and generate more diverse poems."
P19-1193,"
6 Conclusion
",2019,"in addition, the adversarial training based on a multi-label discriminator is employed to further enhance topic-consistency."
P19-1193,"
6 Conclusion
",2019,the proposed model integrates commonsense from the external knowledge base into the generator through a dynamic memory mechanism to enrich the source information.
P19-1196,"
5 Conclusion and Future Work
",2019,"aside from such syntax templates, in the future, we aim to explore how semantic templates contribute to type description generation."
P19-1198,"
7 Conclusion
",2019,"in future, we would like to improve the system further by incorporating better architectural designs and training schemes to tackle complex simplification operations."
P19-1200,"
6 Conclusion
",2019,"a hierarchy of stochastic layers is employed, where the priors of the latent variables are learned from the data."
P19-1200,"
6 Conclusion
",2019,it consists of a multi-level lstm generative network to model the semantic coherence at both the wordand sentence-levels.
P19-1200,"
6 Conclusion
",2019,we introduce a hierarchically-structured variational autoencoder for long text generation.
P19-1201,"
6 Conclusion
",2019,"in the future, we will further apply dim to learn semantic parser and nl generator from the noisy datasets."
P19-1201,"
6 Conclusion
",2019,"in this work, we propose to jointly train the semantic parser and nl generator by exploiting the structural connections between them."
P19-1201,"
6 Conclusion
",2019,we further extend supervised dim to semi-supervised scenario (semidim).
P19-1202,"
7 Conclusion
",2019,we also explored the effects of incorporating writer information to data-to-text models.
P19-1203,"
6 Conclusion
",2019,"besides, it would also be interesting to consider using linguistic knowledge such as named entities or part-of-speech tags to improve the coherence of the conversation."
P19-1203,"
6 Conclusion
",2019,"in the future, we would like to explore how to better select the rationale for each question."
P19-1204,"
5 Conclusion
",2019,"in the future, we plan to study the effect of other video modalities on the alignment algorithm."
P19-1204,"
5 Conclusion
",2019,we hope our method and dataset will unlock new opportunities for scientific paper summarization.
P19-1205,"
5 Conclusion
",2019,"finally, an inference algorithm produces the abstractive summaries."
P19-1206,"
6 Conclusion
",2019,"in future work, we will make comparisons with those of a humanannotated dataset."
P19-1206,"
6 Conclusion
",2019,"our model can also be applied to other applications, such as argument mining, because arguments typically have the same discourse structure as reviews."
P19-1208,"
7 Conclusion and Future Work
",2019,"another interesting direction is to apply our rl approach on the microblog hashtag annotation problem (wang et al., 2019; gong and zhang, 2016; zhang et al., 2018b)."
P19-1208,"
7 Conclusion and Future Work
",2019,"in our rl approach, we introduce an adaptive reward function rf1, which encourages the model to generate both sufficient and accurate keyphrases."
P19-1208,"
7 Conclusion and Future Work
",2019,"one potential future direction is to investigate the performance of other encoder-decoder architectures on keyphrase generation such as transformer (vaswani et al., 2017) with multi-head attention module (li et al., 2018; zhang et al., 2018a)."
P19-1209,"
6 Conclusion
",2019,our method provides a promising avenue for domain-specific summarization where content selection and summary generation are only loosely connected to reduce the costs of obtaining massive annotated data.
P19-1210,"
5 Conclusions and Future Work
",2019,"in the future, we plan to further integrate higher level participant interactions, such as gestures, face expressions, etc."
P19-1210,"
5 Conclusions and Future Work
",2019,"we also plan to construct a larger multimedia meeting summarization corpus to cover more diverse scenarios, building on our previous work (bhattacharya et al., 2019)."
P19-1211,"
6 Summary
",2019,"in the future, we would like to understand the usefulness of artificial titles for training the decoder relative to other factors that may impact performance, e.g., how similar the true titles or summaries are in the different domains."
P19-1212,"
6 Conclusion
",2019,bigpatent can enable future research to build robust systems that generate abstractive and coherent summaries.
P19-1215,"
6 Conclusion
",2019,"in future work, we intend to examine multitask approaches combining question summarization and question understanding."
P19-1216,"
6 Conclusion
",2019,"in the future, we will consider extending the current approach to the single document or multiple document summarization."
P19-1216,"
6 Conclusion
",2019,"in this work, we propose a coarse-to-fine rewriter for multi-sentence compression with a specific focus on improving the quality of compression."
P19-1217,"
5 Conclusions and Future Works
",2019,"in the future, we will expand it to support the questions on text span selection by using the relation type rather than the option as the terminated condition."
P19-1219,"
7 Conclusion
",2019,"in the future, we plan to use some larger knowledge bases, such as conceptnet and freebase, to improve the quality and scope of the general knowledge."
P19-1220,"
7 Conclusion
",2019,our future work will involve exploring the potential of our multi-style learning towards natural language understanding.
P19-1220,"
7 Conclusion
",2019,the key to its success is transferring the style-independent nlg capability to the target style by use of the question-passages reader and the conditional pointer-generator decoder.
P19-1221,"
6 Conclusion
",2019,"future work will concentrate on designing a fast neural pruner to replace the ir-based pruning component, developing better end-to-end training strategies, and adapting our approach to other datasets such as natural questions (kwiatkowski et al., 2019)."
P19-1222,"
6 Concluding Remarks
",2019,an interesting improvement to our approach would be to allow the retriever to automatically determine whether or not more retrieval iterations are needed.
P19-1222,"
6 Concluding Remarks
",2019,we hope to tackle this problem in future work to allow learning more than two retrieval iterations.
P19-1224,"
8 Conclusion
",2019,"additionally, we hope that our specificity-labeled reading comprehension dataset is useful in other applications such as 1) finer control over question generation systems used in education applications, curiositydriven chatbots and healthcare (du et al., 2017)."
P19-1226,"
6 Conclusion
",2019,"this work demonstrates the feasibility of further enhancing advanced lms with knowledge from kbs, which indicates a potential direction for future research."
P19-1227,"
7 Conclusion
",2019,"the performance of translation-based methods can be increased by applying machine translation system that better translates name entities, while the multilingual bert model may be improved by incorporating parallel data with monolingual data."
P19-1227,"
7 Conclusion
",2019,we further examine the performance of two translation-based methods and one zero-shot cross-lingual method on the xqa dataset.
P19-1227,"
7 Conclusion
",2019,we hope our work could contribute to the development of cross-lingual openqa systems and further promote the research of overall cross-lingual language understanding.
P19-1228,"
7 Conclusion
",2019,learning deep generative models which exhibit such conditional markov properties is an interesting direction for future work.
P19-1228,"
7 Conclusion
",2019,the collapsed amortized variational inference approach is general and can be used for generative models which admit tractable inference through partial conditioning.
P19-1229,"
6 Conclusions
",2019,"this work addresses the task of semi-supervised domain adaptation for chinese dependency parsing, based on our two newly-annotated large-scale domain-aware data, i.e., pb and zx."
P19-1230,"
6 Conclusions
",2019,"our experiments show that joint learning of constituent and dependency is indeed superior to separate learning mode, and combining constituent and dependency score in joint training to parse a simplified hpsg can obtain further performance improvement."
P19-1230,"
6 Conclusions
",2019,"thus, this work is more than proposing a high-performance parsing model by exploring the relation between constituent and dependency structures."
P19-1233,"
7 Conclusion
",2019,"in the future, we would like to extend gcdt to other analogous sequence labeling tasks and explore its effectiveness on other languages."
P19-1234,"
7 Conclusion
",2019,"this work proposes a neural pcfg inducer which employs context embeddings (peters et al., 2018) in a normalizing flow model (dinh et al., 2015) to extend pcfg induction to use semantic and morphological information."
P19-1235,"
9 Conclusion
",2019,this work explores the non-optimality of data likelihood for model selection in unsupervised grammar induction.
P19-1238,"
6 Conclusion
",2019,another important and optimistic result of this investigation is that minimalist grammar parsing is not as slow as may have been expected given its worst case time complexity.
P19-1238,"
6 Conclusion
",2019,"however, the gap will likely narrow as the size and quality of mgbank improves and as better probabilistic models are developed enabling these systems to parse a higher number of sentences."
P19-1238,"
6 Conclusion
",2019,the results of this initial attempt are optimistic.
P19-1239,"
7 Conclusion and Future Work
",2019,"in future work, we will incorporate other modality such as audio into the sarcasm detection task and we will also investigate to make use of common sense knowledge in our model."
P19-1240,"
6 Conclusion and Future Work
",2019,"also, our topic-aware neural keyphrase generation model can be investigated in a broader range of text generation tasks."
P19-1240,"
6 Conclusion and Future Work
",2019,"in the future, we will explore how to explicitly leverage the topic-word distribution to further improve the performance."
P19-1241,"
8 Conclusion and Future Work

",2019,our future agenda includes exploring the applicability of our analysis and system for identifying patterns and potential prevention.
P19-1241,"
8 Conclusion and Future Work

",2019,we also hope this study enables further research in terms of how people seek support online on sexual harassment and mental health-related problems.
P19-1241,"
8 Conclusion and Future Work

",2019,we also plan to use this model to solve other downstream medium-specific tasks pertaining to mental health and welfare.
P19-1242,"
8 Conclusion
",2019,"although we focused on english hashtags, our pairwise ranking approach is language-independent and we intend to extend our toolkit to languages other than english as future work."
P19-1242,"
8 Conclusion
",2019,we demonstrated that hashtag segmentation helps with downstream tasks such as sentiment analysis.
P19-1243,"
7 Conclusions and Future Work
",2019,"we leave alternative solutions for future work, including training embeddings from scratch or fine-tuning on the target corpus (however, these ideas are only feasible with a large target corpus, and the need for fine-tuning reduces the usefulness of pre-trained embeddings)."
P19-1243,"
7 Conclusions and Future Work
",2019,"while we explore masking target words as a possible solution to this problem, we find that masking significantly decreases performance."
P19-1244,"
6 Conclusions and Future Work
",2019,"for the future work, beyond what we have mentioned, we plan to examine our model on different information sources."
P19-1244,"
6 Conclusions and Future Work
",2019,"we will also try to incorporate relevant metadata into it, e.g., author profile, website credibility, etc."
P19-1246,"
9 Conclusion
",2019,"first, we wish to investigate the role of additional variables (such as age and gender)."
P19-1246,"
9 Conclusion
",2019,"inspired by labov and encouraged by recent interest in computational sociolinguistics, we developed accurate neural models to predict socioeconomic status from text."
P19-1246,"
9 Conclusion
",2019,"nevertheless, there are limitations of distant labelling and social media data — with issues related specifically to the language of food (askalidis and malthouse, 2016) — that we will take into account in future work."
P19-1246,"
9 Conclusion
",2019,"second, we will take steps to mitigate the risk of fake reviews and validate the distant labelling with human annotation."
P19-1246,"
9 Conclusion
",2019,this is encouraging for further studies in computational social science in ecologically valid and relatively labour-free settings.
P19-1247,"
7 Conclusion

",2019,"we intend to study fine-grained political perspectives, capturing how different events are framed."
P19-1248,"
6 Conclusions and Future Work
",2019,"for example, revising spoiler contents in a ‘non-spoiler’ way would be an interesting language generation task."
P19-1248,"
6 Conclusions and Future Work
",2019,"in addition to review semantics, syntax information could be incorporated in a spoiler language model."
P19-1248,"
6 Conclusions and Future Work
",2019,"our new dataset, analysis of spoiler language, and positive results facilitate several directions for future work."
P19-1249,"
5 Conclusion
",2019,"in future work, we plan on improving the corpus by incorporating verified accounts from other social networks, and, by inferring new labels for as of yet unlabeled celebrities through link prediction."
P19-1250,"

5 Conclusion
",2019,our future work will include efficiently labeling promising comments via active learning.
P19-1250,"

5 Conclusion
",2019,we created a new labeled dataset for ranking constructive comments.
P19-1251,"
4 Conclusions and Discussions
",2019,"after filtering irrelevant tweets, a cnn derives a feature vector for each tweet with max-over-time pooling."
P19-1252,"
5 Conclusion and Future Work

",2019,"directions of future research include adaptation of our methods to a large scale, sparsely connected social network."
P19-1252,"
5 Conclusion and Future Work

",2019,"one might also want to investigate the inductive settings of gcn (hamilton et al., 2017) to predict demographic information of a user from outside the black network."
P19-1253,"
7 Conclusion and Future Work
",2019,"the daml also provides promising potential extension, such as applying daml on reinforcement learning-based dialog system."
P19-1253,"
7 Conclusion and Future Work
",2019,we also plan to adapt daml to multi-domain dialog tasks.
P19-1255,"
8 Conclusion
",2019,"an argument generation component then employs a text planning decoder to conduct content selection and specify a suitable language style at sentence-level, followed by a content realization decoder to produce the final argument."
P19-1256,"
4 Discussion
",2019,"we are still on the way to find out solutions for cases with huge noise (perezbeltrachini and lapata, 2018; wiseman et al., 2017), where heavy manual intervention or external knowledge should be desperately needed."
P19-1257,"
6 Conclusion
",2019,"further analysis demonstrates that with multiple modal information and co-attention, the generated comments are more diverse and informative."
P19-1258,"
4 Conclusion
",2019,our future work will focus on how to transfer the long-term memory across different tasks.
P19-1259,"
6 Discussion and Conclusion
",2019,"finally, we believe that our framework can generalize to other cognitive tasks, such as conversational ai and sequential recommendation."
P19-1259,"
6 Discussion and Conclusion
",2019,"moreover, we expect that prospective architectures combining attention and recurrent mechanisms will largely improve the capacity of system 1 by optimizing the interaction between systems."
P19-1259,"
6 Discussion and Conclusion
",2019,multiple future research directions may be envisioned.
P19-1260,"
5 Conclusion
",2019,"in the future, we would like to investigate explainable gnn for this task, such as explicit reasoning path in (kundu et al., 2018), and work on other data sets such as hotpotqa."
P19-1265,"
6 Conclusion
",2019,"we expect our work to have a large impact on future work requiring expert annotations, in particular regarding new tasks with no or little available data, for example for legal (nazarenko et al., 2018), chemical (guo et al., 2014), or psychiatric (mieskes and stiegelmayr, 2018) text processing."
P19-1266,"
7 Conclusions
",2019,"having released our code, we hope this will become a new evaluation standard in the nlp community."
P19-1267,"
5 Conclusions
",2019,"while linguistic theory and statistical learning theory both have much to contribute to part-ofspeech tagging, we still lack a theory of the tagging task rich enough to guide hypothesis formation."
P19-1268,"
6 Conclusion
",2019,"in order to encourage the development of models that perform well on difficult items, we propose to use non-obvious f1 scores (f1n) as a complementary ranking metric for model evaluation."
P19-1271,"
5 Conclusion
",2019,"as future work, we intend to continue collecting more data for islam and to include other hate targets such as migrants or lgbt+, in order to put the dataset at the service of other organizations and further research."
P19-1271,"
5 Conclusion
",2019,"finally, we expanded the dataset through translation and paraphrasing."
P19-1271,"
5 Conclusion
",2019,"moreover, as a future direction, we want to utilize conan dataset to develop a counter-narrative generation tool that can support ngos in fighting hate speech online, considering counter-narrative type as an input feature."
P19-1272,"
8 Conclusions
",2019,"future work will look deeper into using the similarity between the content of the text and image (leong and mihalcea, 2011), as the text task results showed room for improvements."
P19-1272,"
8 Conclusions
",2019,"the frequency of use is influenced by the age of the poster, with younger users employing images with a more prominent role in the tweet, rather than just being redundant to the text or as a means of illustrating it."
P19-1272,"
8 Conclusions
",2019,"we developed models that use both text and image features to classify the text-image relationship, with especially high performance (f1 = 0.81) in identifying if the image is redundant, which is immediately useful for downstream applications that maximize screen estate for users."
P19-1272,"
8 Conclusions
",2019,"we envision that our data, task and classifiers will be useful as a preprocessing step in collecting data for training large scale models for image captioning (feng and lapata, 2010) or tagging (mahajan et al., 2018) or for improving recommendations (chen et al., 2016) by filtering out tweets where the text and image have no semantic overlap or can enable new tasks such as identifying tweets that contain creative descriptions for images."
P19-1273,"
7 Conclusion
",2019,"finally, an interleaved cross-disciplinary collaboration may support the future research process further: the claim ontology for a new field of debate could be constructed in a bootstrapping process, combining the political scientists’ analytical insights with (preliminary) predictions of computational seed models from partially overlapping fields."
P19-1274,"
7 Conclusions
",2019,"future work could study other types of accounts with similar posting behaviors such as organizational accounts, explore other sources for ground truth tweet identity information (robinson, 2016) or study the effects of user traits such as gender or political affiliation in tweeting signed content."
P19-1275,"
6 Conclusion
",2019,we suggest a future research direction in sarcasm detection where the two types of sarcasm are treated as separate phenomena and socio-cultural differences are taken into account.
P19-1276,"
6 Conclusion
",2019,"in addition, gnbusiness dataset, a largescale dataset annotated with diverse event types and explainable event schemas, is released along with this paper."
P19-1277,"
6 Conclusions
",2019,studying few-shot relation classification with data generated by distant supervision and extending our mlman model to zero-shot learning will be the tasks of our future work.
P19-1278,"
11 Conclusion and Future Work
",2019,"we note that there are a wide range of future directions: (1) human prior knowledge could be incorporated into the similarity quantification; (2) similarity between relations could also be considered in multi-modal settings, e.g., extracting relations from images, videos, or even from audios; (3) by analyzing the distributions corresponding to different relations, one can also find some “meta-relations” between relations, such as hypernymy and hyponymy."
P19-1279,"
6 Conclusion and Future Work
",2019,"in future work, we plan to work on relation discovery by clustering relation statements that have similar representations according to bertem+mtb."
P19-1279,"
6 Conclusion and Future Work
",2019,this would take us some of the way toward our goal of truly general purpose relation identification and extraction.
P19-1279,"
6 Conclusion and Future Work
",2019,we will also study representations of relations and entities that can be used to store relation triples in a distributed knowledge base.
P19-1281,"
6 Conclusions
",2019,"future research directions include making selection routines more robust to evaluation outliers, relaxing our gaussian assumptions and developing more effective batch strategies."
P19-1282,"
7 Related and Future Work
",2019,"finally, another direction for future work would be to extend the importance-ranking comparisons that we deploy here for evaluation purposes into a method for deriving better, more informative rankings, which in turn could be useful for the development of new, more interpretable models."
P19-1282,"
7 Related and Future Work
",2019,"others have explored alternative ways of comparing the behavior of proposed explanation methods (adebayo et al., 2018)."
P19-1282,"
7 Related and Future Work
",2019,"yet another line of work focuses on aligning models with human feedback for what is interpretable (fyshe et al., 2015; subramanian et al., 2017), which could refine our idea of what defines a highquality explanation derived from attention."
P19-1283,"
6 Conclusion
",2019,"we apply the same techniques to english sentence embeddings, and show where and to what extent each representation encodes syntactic information."
P19-1283,"
6 Conclusion
",2019,we plan to explore these options in future work.
P19-1284,"
8 Conclusions
",2019,we leave further explorations for future work.
P19-1285,"
5 Conclusions
",2019,"we envision interesting applications of transformer-xl in the fields of text generation, unsupervised feature learning, image and speech modeling."
P19-1287,"
6 Conclusion and Future Work
",2019,"in the future, we would like to investigate the feasibility of our methods on non-recurrent nmt models such as transformer (vaswani et al., 2017)."
P19-1287,"
6 Conclusion and Future Work
",2019,"moreover, we are also interested in incorporating discourse-level relations into our models."
P19-1288,"
6 Conclusion
",2019,"first, exploiting other sequencelevel training objectives like bag-of-words (ma et al., 2018)."
P19-1288,"
6 Conclusion
",2019,"in the future, we plan to investigate better methods to leverage the sequential information."
P19-1288,"
6 Conclusion
",2019,we believe that the following two directions are worth study.
P19-1289,"
8 Conclusions
",2019,"we leave many open questions to future work, e.g., adaptive policy using a single model (zheng et al., 2019)."
P19-1290,"
6 Conclusion
",2019,we train our model by designing an rl algorithm with the reward shaping strategy.
P19-1292,"
5 Conclusion and Future Work
",2019,"in future work, we would like to do an extensive analysis on the capabilities of bert and transfer learning in general for different domains and language pairs in ape."
P19-1292,"
5 Conclusion and Future Work
",2019,we explored various ways for coupling bert in the decoder for language generation.
P19-1292,"
5 Conclusion and Future Work
",2019,we found it beneficial to initialize the context attention of the decoder with bert’s self-attention and to tie together the parameters of the self-attention layers between the encoder and decoder.
P19-1296,"
5 Conclusion
",2019,"in future work, we intend to apply these methods to other natural language tasks."
P19-1297,"
7 Conclusion
",2019,"in future, we would like to explore other languages with diverse linguistic characteristics."
P19-1299,"
5 Conclusion
",2019,"for future work, we plan to apply man-moe to more challenging languages for tasks such as syntactic parsing, where multilingual data exists (nivre et al., 2017)."
P19-1299,"
5 Conclusion
",2019,"furthermore, we would like to experiment with multilingual contextualized embeddings such as the multilingual bert (devlin et al., 2018)."
P19-1299,"
5 Conclusion
",2019,"when transferring to distant languages such as chinese or japanese (from european languages), where the quality of cross-lingual word embeddings are unsatisfactory, man-moe remains highly effective and substantially mitigates the performance gap introduced by cross-lingual supervision."
P19-1300,"
6 Conclusion
",2019,it would be also interesting to investigate how our approach compares to the baselines given a large amount of data such as wikipedia.
P19-1300,"
6 Conclusion
",2019,our future work is to exploit character and subword information in our model and see how those information affect the performance in each language pair.
P19-1301,"
8 Conclusion
",2019,"through analyzing the learned ranking models, we also gain some insights on the types of features that are most influential in selecting transfer languages for each of the nlp tasks, which may inform future ad hoc selection even without useing our method."
P19-1302,"
7 Conclusions
",2019,"the resource has been made available online, together with a graphical webbased tool for the exploration of cognate data, our hope being to attract both linguists and computer scientists as potential users."
P19-1303,"
7 Conclusions
",2019,"we design the model and training procedure following fundamental principles of decipherment from historical linguistics, which effectively guide the decipherment process without supervision signal."
P19-1304,"
5 Conclusions
",2019,"in the future, we will explore more applications of the proposed idea of attentive graph matching."
P19-1306,"
5 Conclusion
",2019,we believe the idea of combining bilingual document representations using cross-lingual word embeddings can be generalized to other models as well.
P19-1307,"
6 Conclusion
",2019,"in the future, we will investigate whether our method helps other downstream tasks."
P19-1312,"
4 Conclusion and future work
",2019,"as a future work, we would like to study, for training bwe, the impact of the use of synthetic parallel data generated by unsupervised nmt, or of a different nature, such as translation pairs extracted from monolingual corpora without supervision."
P19-1312,"
4 Conclusion and future work
",2019,"since our approach works on top of unsupervised mapping for bwe and uses synthetic data generated by unsupervised mt, it will directly benefit from any future advances in these two types of techniques."
P19-1314,"
5 Conclusion
",2019,we hope this paper will foster more discussions on the necessity of the long-existing task of cws in the community.
P19-1316,"
6 Conclusion
",2019,4 two directions for future work are (i) to extend our approach to other languages by using multilingual resources or translation data; and (ii) to explore various compositionality functions to combine the words’ representation on the basis of their grammatical function within a phrase.
P19-1317,"
5 Conclusion
",2019,"alternatively, sequence encoding models such as gru, cnn, transformer, or even encoders with contextualized word embeddings like bert (devlin et al., 2018), or elmo (peters et al., 2018) can be used to replace this bilstm, however, with additional computation cost."
P19-1317,"
5 Conclusion
",2019,we also discuss different ways of representing the contextual and conceptual information in our framework.
P19-1318,"
7 Conclusions
",2019,"for future work, we would be interested in further exploring the behavior of neural architectures for nlp tasks which intuitively would benefit from having access to relational information, e.g., text classification (espinosa anke and schockaert, 2018; camacho-collados et al., 2019) and other language understanding tasks such as natural language inference or reading comprehension, in the line of joshi et al.(2019)."
P19-1319,"
6 Conclusion
",2019,"in addition, we proposed to pre-train this network, relying on the claim that two antonyms of the same word tend to be synonyms, through a siamese network; and a relaxed version of the contrastive loss function."
P19-1319,"
6 Conclusion
",2019,we presented a supervised approach to distinguish antonyms and synonyms using pre-trained word embeddings.
P19-1322,"
5 Conclusion
",2019,it is an interesting future work and will make learning word embeddings more like human learning a language.
P19-1322,"
5 Conclusion
",2019,"we proposed delta embedding learning, a supervised embedding learning method that not only improves performance in nlp tasks, but also learns better universal word embeddings by letting the embedding “grow” under supervision."
P19-1323,"
6 Conclusion and future work
",2019,"future work will offer a more principled account of aspectual classification for specific verb classes, among them speech act and communication verbs (e.g., promise or call) that occur frequently in corpora but have hitherto been neglected in aspectual analyses."
P19-1323,"
6 Conclusion and future work
",2019,"on a more general scale, we envisage examining the interplay of verb class (e.g., the classes of levin 1993), verb sense, and aspectual class, with the purpose of estimating the influence of the sentential context on the aspectual value of the predicate."
P19-1323,"
6 Conclusion and future work
",2019,"we also intend to develop a more principled treatment for the aspectual classification of metaphors, which are frequent in other corpora."
P19-1323,"
6 Conclusion and future work
",2019,"we report substantial interannotator agreement, and validate our resource by training automatic aspectual classifiers, permitting favourable comparisons to prior work."
P19-1324,"
4 Future work
",2019,"though we focused on lstm lms for english, this method can be applied to other architectures, objective tasks, and languages; possibilities to explore in future work."
P19-1324,"
4 Future work
",2019,"we also plan to carry out further analyses aimed at individuating factors that challenge the resolution of lexical ambiguity (e.g., morphosyntactic vs. semantic ambiguity, frequency of a word or sense, figurative uses), as well as clarifying the interaction between prediction and processing of words within neural lms."
P19-1325,"
7 Conclusion
",2019,structured knowledge contained in language networks is useful for nlp applications but is difficult to use directly in neural architectures.
P19-1326,"
6 Conclusion and Future Work
",2019,future research directions include a theoretical explanation of kg2vec and applications to downstream nlp tasks.
P19-1329,"
6 Conclusion
",2019,this work also raises important questions about other categories of word-like tokens that need to be treated like special cases.
P19-1329,"
6 Conclusion
",2019,"we hope this work will promote a more careful treatment of language, and serve a cautionary purpose against using word embeddings in downstream tasks without recognizing their limitations."
P19-1330,"
7 Conclusion and Future Work
",2019,"also, we will explore ways of automating parts of the process, e.g.the highlight annotation."
P19-1330,"
7 Conclusion and Future Work
",2019,"in future work, we would like to extend our framework to other variants of summarization e.g.multi-document."
P19-1332,"
5 Conclusion
",2019,"in the future, we plan to investigate more efficient methods of unsupervised domain adaptation with decomposition mechanism on other nlp tasks."
P19-1333,"
7 Conclusion
",2019,"in the future, we plan to investigate the constituency type classification and rhetorical relation identification steps and port this approach to languages other than english."
P19-1335,"
8 Conclusion
",2019,future variations of the task could incorporate nil recognition and mention detection (instead of mention boundaries being provided).
P19-1335,"
8 Conclusion
",2019,we also expect models that jointly resolve mentions in a document would perform better than resolving them in isolation.
P19-1337,"
6 Conclusion
",2019,"our approach is a general one that can be applied to other student model architectures, such as transformers (vaswani et al., 2017)."
P19-1338,"
5 Conclusion
",2019,"in future work, we would like to combine more potential parsers—including chartstyle parsing and shift-reduce parsing—and transfer knowledge from one to another in a co-training setting."
P19-1340,"
6 Conclusion
",2019,the remarkable effectiveness of unsupervised pretraining of vector representations of language suggests that future advances in this area can continue improving the ability of machine learning methods to model syntax (as well as other aspects of language).
P19-1341,"
6 Conclusion
",2019,"to address the fact that the small-size zs lexicons are specific to pbc+’s domain, we conduct domain adaptation and induce large-size generic da (domain-adapted) lexicons for 200 languages."
P19-1344,"
5 Conclusion and Future Work
",2019,"in our future work, we plan to apply our approach to other sequence labeling tasks, such as named entity recognition, word segmentation and so on."
P19-1344,"
5 Conclusion and Future Work
",2019,our proposed method can take full advantage of the meaning of the whole sentence and the previous label during the decoding process.
P19-1345,"
7 Conclusion
",2019,"furthermore, we would like to explore the effectiveness of our approach to asc-qa in other languages."
P19-1345,"
7 Conclusion
",2019,"in our future work, we would like to solve other challenges in asc-qa such as data imbalance and negation detection to improve the performance."
P19-1346,"
7 Conclusion
",2019,"we hope eli5 will inspire future work in all aspects of long-form qa, from the information extraction problem of obtaining information from long, multi-document input to generating more coherent and accurate paragraph-length answers."
P19-1347,"
6 Conclusion
",2019,we believe that our work can be a meaningful step in realistic multimodal qa and solving the out-of-domain issue.
P19-1348,"
5 Conclusion
",2019,"in particular, we present a model which jointly generates question-related captions and uses them to provide additional information to aid vqa."
P19-1350,"
6 Conclusion
",2019,we assessed to what extent a multimodal model suffers from catastrophic forgetting in a vqa task.
P19-1350,"
6 Conclusion
",2019,we reserve a deeper investigation of this aspect to future research.
P19-1351,"
6 Conclusion
",2019,"in future work, we are investigating whether the vtqa model can be jointly trained with the paragraph captioning model."
P19-1353,"
7 Conclusion
",2019,we hope this initial application inspires further research by literary scholars and computational humanists in the future.
P19-1353,"
7 Conclusion
",2019,"yet simply by analyzing the ratio of realis events, one can capture a meaningful distinction between novels written by authors whose works are reviewed by elite literary journals and those written by authors whose work is not."
P19-1354,"
6 Conclusion
",2019,"as our approach is not limited to the nmt encoders, it is also interesting to explore how do the models trained on other nlp tasks learn word order information."
P19-1354,"
6 Conclusion
",2019,our further analyses for the encoders pretrained on the nmt data suggest that 1) the learning objective sometimes plays a crucial role on learning a specific feature (e.g.word order) in a downstream nlp task; 2) modeling recurrence is universally-effective to learn word order information for san; and 3) rnn is more sensitive on erroneous word order noises in machine translation system.
P19-1355,"
5 Conclusions
",2019,"an additional avenue through which nlp and machine learning software developers could aid in reducing the energy associated with model tuning is by providing easyto-use apis implementing more efficient alternatives to brute-force grid search for hyperparameter tuning, e.g.random or bayesian hyperparameter search techniques (bergstra et al., 2011; bergstra and bengio, 2012; snoek et al., 2012)."
P19-1355,"
5 Conclusions
",2019,our experiments suggest that it would be beneficial to directly compare different models to perform a cost-benefit (accuracy) analysis.
P19-1355,"
5 Conclusions
",2019,researchers should prioritize computationally efficient hardware and algorithms.
P19-1355,"
5 Conclusions
",2019,there is already a precedent for nlp software packages prioritizing efficient models.
P19-1355,"
5 Conclusions
",2019,"this even more deeply promotes the already problematic “rich get richer” cycle of research funding, where groups that are already successful and thus well-funded tend to receive more funding due to their existing accomplishments."
P19-1355,"
5 Conclusions
",2019,"this will enable direct comparison across models, allowing subsequent consumers of these models to accurately assess whether the required computational resources are compatible with their setting."
P19-1355,"
5 Conclusions
",2019,"while software packages implementing these techniques do exist,10 they are rarely employed in practice for tuning nlp models."
P19-1356,"
8 Conclusion
",2019,it would be interesting to see if our results transfer to other domains with higher variability in syntactic structures (such as noisy user generated content) and with higher word order flexibility as experienced in some morphologically-rich languages.
P19-1357,"
5 Conclusion
",2019,"first, expanding our purview for abuse detection to include both extreme behaviors and the more subtle— but still offensive—behaviors like microaggressions and condescension."
P19-1357,"
5 Conclusion
",2019,"second, nlp must develop methods that go beyond reactive identifyand-delete strategies to one of proactivity that intervenes or nudges individuals to discourage harm before it occurs."
P19-1357,"
5 Conclusion
",2019,"the nlp community has proposed computational methods to help mitigate this problem, yet has also struggled to move beyond the most obvious tasks in abuse detection."
P19-1358,"
6 Future Work
",2019,"in this way, a dialogue agent could both improve its dialogue ability and its potential to improve further."
P19-1358,"
6 Future Work
",2019,"one could even use the flexible nature of dialogue to intermix data collection of more than one type— sometimes requesting new feedback examples, and other times requesting new satisfaction examples (e.g., asking “did my last response make sense?”)."
P19-1358,"
6 Future Work
",2019,we leave exploration of this metalearning theme to future work.
P19-1359,"
5 Conclusion
",2019,the sequence-to-sequence framework has been extended with a lexicon-based attention mechanism that works by seamlessly “plugging” emotional words into the texts by increasing their probability at the right time steps.
P19-1360,"
7 Conclusion and Future Work
",2019,"in the future, we intend to infer the dialog acts from the annotated responses and use such noisy data to guide the response generation."
P19-1361,"
8 Conclusion
",2019,"as the usage grows, ids will cumulate more and more knowledge over time."
P19-1362,"
5 Conclusion
",2019,"in addition, the detailed content information can be considered in the relevant contexts to further improve the quality of generated response."
P19-1362,"
5 Conclusion
",2019,"in future work, we plan to further investigate the proposed recosa model."
P19-1363,"
6 Conclusion
",2019,"future work may also incorporate contradiction information into the dialogue model itself, and extend to generic contradictions."
P19-1364,"
4 Conclusion
",2019,another interesting direction is to design a trainable budget scheduler.
P19-1364,"
4 Conclusion
",2019,"in future, we plan to investigate the effectiveness of our method on more complex task-oriented dialogue datasets."
P19-1364,"
4 Conclusion
",2019,"one possible solution is transfer learning, in which we train the budget scheduler on some welldefined dialogue tasks, then leverage this scheduler to guide the policy learning on other complex dialogue tasks."
P19-1365,"
8 Conclusion
",2019,"while we have focused on evaluating each decoding strategy under the specifics reported to be the best in the original, further work is necessary to conclude whether observed differences in quality and diversity may simply be due to each work’s chosen hyperparameters."
P19-1366,"
6 Conclusion and Future Work
",2019,"in addition, we will also explore how to integrate external knowledge in other formats, like the knowledge graph, into adversarial training so that the quality could be further improved."
P19-1366,"
6 Conclusion and Future Work
",2019,"in future research, we will further investigate how to better leverage larger training data to improve the reat method."
P19-1366,"
6 Conclusion and Future Work
",2019,we propose a retrieval-enhanced adversarial training method for neural response generation in dialogue systems.
P19-1367,"
6 Conclusion and Future Work
",2019,"in the future, there are some promising explorations in vocabulary pyramid networks.1) we will further study how to obtain multi-level vocabularies, such as employing other clustering methods and incorporating semantic lexicons like wordnet; 2) we also plan to design deep-pass encoding and decoding for vpn; 3) we will investigate how to apply vpn to other natural language generation tasks such as machine translation and generative text summarization."
P19-1367,"
6 Conclusion and Future Work
",2019,"in this study, we tackle the issues of one fixed vocabulary and one-pass decoding in response generation tasks."
P19-1368,"
6 Conclusion
",2019,"we conducted experiments on wide range of nlp applications such as dialog acts, intent prediction and customer feedback."
P19-1368,"
6 Conclusion
",2019,we trained quantized versions of sgnn++ showing that we can further reduce the model size while preserving quality.
P19-1372,"
6 Conclusion and future work
",2019,directions of future work may be pursuing betterdefined features and easier training strategies.
P19-1373,"
7 Conclusion and Future Work
",2019,"finally, the addition of word-level pretraining methods to improve the dialog context representations should be explored."
P19-1373,"
7 Conclusion and Future Work
",2019,"in this paper, unsupervised pretraining has been shown to learn effective representations of dialog context, making this an important research direction for future dialog systems."
P19-1373,"
7 Conclusion and Future Work
",2019,"second, it would be interesting to test the representations learned using unsupervised pretraining on less-related downstream tasks such as sentiment analysis."
P19-1373,"
7 Conclusion and Future Work
",2019,these results open three future research directions.
P19-1374,"
6 Conclusion
",2019,"the models we develop have already enabled new directions in dialogue research, providing disentangled conversations for dstc 7 track 1 (gunasekara et al., 2019; yoshino et al., 2018) and will be used in dstc 8."
P19-1375,"
5 Conclusion
",2019,"theoretically, we believe this self-supervision can be generalized to other types of temporal order in different nlp tasks."
P19-1376,"
6 General discussion and conclusions
",2019,"although it learns some of the relevant features anyway, it would be interesting to see whether its behaviour becomes more human-like if the correct features are provided in the input."
P19-1376,"
6 General discussion and conclusions
",2019,"as noted by seidenberg and plaut (2014), models’ failures as well as successes can be informative, and we hope that our detailed exploration of the ed model’s behaviour will inspire future developments in these models, both for cognitive modelling and nlp."
P19-1376,"
6 General discussion and conclusions
",2019,"in future, a stricter test might use nonce words that are intentionally less similar to real words (e.g., the example from prasada and pinker (1993): to out-gorbachev)."
P19-1376,"
6 General discussion and conclusions
",2019,"there are many other potential architectures and modelling decisions that could be explored, as well as other behavioural data such as developmental patterns (blything et al., 2018; ambridge, 2010) and inflection in other languages (e.g., clahsen et al., 1992; ernestus and baayen, 2004)."
P19-1378,"
6 Conclusion
",2019,"another interesting direction is to explore combining different semantic similarity measures (lin et al., 2015) for our task."
P19-1378,"
6 Conclusion
",2019,"in future work, we will explore ensemble learning."
P19-1379,"
5 Conclusion and Future Work
",2019,"in addition to tracking the language evolvement in the history, we believe it is promising future work to use deep contextual embeddings in predicting the future change or trend, as well as detecting novel senses that are not included in existing dictionaries."
P19-1379,"
5 Conclusion and Future Work
",2019,"overall, our study sheds some light on diachronic language study with deep contextualized embeddings."
P19-1381,"
5 Conclusion
",2019,fully understanding generalization of deep seq2seq models might require a less clear-cut view of the divide between statistical pattern matching and symbolic composition.
P19-1381,"
5 Conclusion
",2019,"in future work, we would like to further our insights on the cnn aspects that are crucial for the task, our preliminary analyses of kernel width and attention."
P19-1381,"
5 Conclusion
",2019,"in informal experiments, we found shallow cnns incapable to handle even the simplest random split."
P19-1381,"
5 Conclusion
",2019,we leave a proper formulation of a tighter comparison to future work.
P19-1382,"
6 Conclusions
",2019,"we defined the notion of probabilistic implications, and presented a computational model which successfully identifies known universals, including greenberg universals, but also uncovers new ones, worthy of further investigation by typologists."
P19-1384,"
4 Conclusions
",2019,"in future work, we will extend our set of languages, aiming at more typological variety (indoeuropean languages are greatly over-represented in our current data)."
P19-1385,"
6 Conclusions & Future work
",2019,"in the future, we aim to incorporate more elaborate linguistic resources (e.g.knowledge bases) and to investigate the performance of our methods on more complex nlp tasks, such as named entity recognition and sequence labelling, where prior knowledge integration is an active area of research."
P19-1385,"
6 Conclusions & Future work
",2019,our approach can be applied to any rnn-based architecture as a extra module to further improve performance with minimal computational overhead.
P19-1386,"
5 Conclusion
",2019,"in the future, we wish to study the use of knowref to improve performance on general coreference resolution tasks (e.g., the conll 2012 shared tasks)."
P19-1386,"
5 Conclusion
",2019,we also plan to develop new models on knowref and transfer them to difficult common sense reasoning tasks.
P19-1387,"
7 Conclusion
",2019,we believe this work will usher considerable interest in understanding linguistic patterns in wikipedia edit history and application of deep models in this domain.
P19-1388,"
6 Conclusion and Discussion
",2019,"below, we discuss a few interesting issues brought up by the data collection process that should be addressed in future work."
P19-1388,"
6 Conclusion and Discussion
",2019,"it is interesting to note that in the case of temperatures, both in the u.s states case (figure 3) and the world case (figure 2 in the appendix), the exaggeration is towards hot temperatures, and not cold ones."
P19-1389,"
8 Conclusions
",2019,"we first show that classifiers trained on the stcsefun dataset can be used to automatically annotate a large conversation corpus with highly reliable sentence functions, as well as to estimate the proper response sentence function for a test query."
P19-1390,"
5 Conclusion
",2019,we believe our annotations will be a valuable resource to the nlp community.
P19-1390,"
5 Conclusion
",2019,we chose to annotate the essays in icle that have previously been scored along multiple dimensions in order to facilitate future developments of joint models that can capture the interactions among different dimensions.
P19-1392,"
5 Conclusions and Further Work
",2019,in further work we plan to carry out a multilingual alignment of the collocations in each language.
P19-1392,"
5 Conclusions and Further Work
",2019,"this dataset can serve as a basis to evaluate systems designed to automatically extract collocations and identify their lexical functions, which in turn may be useful for different nlp and corpus linguistics tasks."
P19-1394,"
6 Conclusion and Future Work
",2019,"in the future, we aim to analyze how changes in the training procedure and hyperparameters of ulmfit affect resulting model performance."
P19-1394,"
6 Conclusion and Future Work
",2019,"on top of that, we hope to improve model generalization by augmenting negative examples with a split of jokes into setups and punchlines, as they should not be funny by themselves."
P19-1394,"
6 Conclusion and Future Work
",2019,we also plan to reproduce the experiment on english data.
P19-1396,"
5 Conclusion
",2019,"for future study, we would like to investigate how to automatically learn the number of latent clusters with nonparametric bayesian methods."
P19-1397,"
6 Conclusion
",2019,future work could potentially expand our work into an end-to-end invertible model that is able to produce high-quality representations by omnidirectional computations.
P19-1397,"
6 Conclusion
",2019,"two types of decoders, including an orthonormal regularised linear projection and a bijective transformation, whose inverses can be derived effortlessly, are presented in order to utilise the decoder as another encoder in the testing phase."
P19-1398,"
5 Conclusion
",2019,"our method, context vector data description (cvdd), enables contextual anomaly detection and has strong interpretability features."
P19-1399,"
6 Conclusion
",2019,"future works include applying the method to more language pairs and more domain-specific lexicon induction, e.g., terminologies."
P19-1400,"
7 Conclusions
",2019,"besides, we will consider additional knowledge bases (e.g.yago and wikidata)."
P19-1400,"
7 Conclusions
",2019,"in future work we will aim to improve candidate selection (including different strategies to select candidate lists e+, e?)."
P19-1400,"
7 Conclusions
",2019,we will also use extra document information and jointly predict entities for different mentions in the document.
P19-1401,"
7 Conclusion
",2019,"our approach is the first study to interleave (i) the wake phase, where the al policy is exploited to improve the student learner and (ii) the dream phase, where the student learner in turn acts as an imperfect annotator to enhance the al policy."
P19-1404,"
6 Conclusion & Future Work
",2019,the other part captures the common representation between the source and target domain pair which contributes to both source domain learning as well as generalizes to the unlabelled target domain task.
P19-1405,"
6 Conclusions 
",2019,"in the future, we will refine the bayes test of p, r, and f1 in an m × 2 bcv and provide accurate interval estimation of other evaluation metrics on the basis of the confusion matrix."
P19-1407,"
7 Conclusion
",2019,"by letting the decoder ’keep notes’ on the encoder, or said another way, re-encode the input at every decoding step, the scratchpad mechanism effectively guides future generation."
P19-1408,"
8 Conclusions
",2019,a future direction is to investigate the effect of using mina spans not only in evaluation but also for training existing coreference resolvers.
P19-1408,"
8 Conclusions
",2019,"investigating the use of mina in other nlp areas, e.g., evaluating spans in named entity recognition or reading comprehension, is another future line of work."
P19-1409,"
8 Conclusion
",2019,"future directions include investigating ways to minimize the pipeline errors from the extraction of predicate-argument structures, and incorporating a mention prediction component, rather than relying on gold mentions."
P19-1409,"
8 Conclusion
",2019,"we represent a mention using its text, context, and— inspired by the joint model of lee et al.(2012)— we make an event mention representation aware of coreference clusters of entity mentions to which it is related via predicate-argument structures, and vice versa."
P19-1410,"
6 Conclusions
",2019,"also, in the future, we would like to investigate how our approach can be generalized to other discourse frameworks such as the penn discourse treebank (pdtb)."
P19-1410,"
6 Conclusions
",2019,"based on what we have done so far, it is natural for us to move our focus from sentence-level to document-level rst parsing."
P19-1411,"
5 Conclusion
",2019,"in the future work, we plan to extend the idea of multi-task learning/transfer learning with label embeddings to the problems in information extraction (e.g., event detection, relation extraction, entity mention detection) (nguyen and grishman, 2015a,b, 2016d; nguyen et al., 2016a,b,c; nguyen and nguyen, 2018b, 2019)."
P19-1411,"
5 Conclusion
",2019,"in these problems, the labels are often organized in the hierarchies (e.g., types, subtypes) and the label embeddings can exploit such hierarchies to transfer the knowledge between different label-specific prediction tasks."
P19-1411,"
5 Conclusion
",2019,"our proposed model features the embeddings of the implicit connectives and discourse relations, and the three penalization terms to encourage the knowledge sharing between the prediction tasks."
P19-1412,"
6 Conclusion
",2019,"in the long run, to perform robust language understanding, models will need to incorporate more linguistic foreknowledge and be able to generalize to a wider range of linguistic constructions."
P19-1413,"
5 Summary
",2019,"in the future we would like to expand this direction, and find ways to connect event and relation representation, learning and inference in a unified framework."
P19-1414,"
6 Conclusion and Future Work
",2019,"as a future work, we plan to introduce a wider range of background knowledge including another type of event causality (hashimoto et al., 2012, 2014, 2015; kruengkrai et al., 2017)."
P19-1415,"
5 Conclusions
",2019,"as for future work, we would like to enhance the ability to utilize antonyms for unanswerable question generation by leveraging external resources."
P19-1416,"
6 Conclusions
",2019,"in this context, we suggest that future work can explore better retrieval methods for multi-hop questions."
P19-1416,"
6 Conclusions
",2019,"instead, future datasets must carefully consider what evidence they provide in order to ensure multi-hop reasoning is required."
P19-1417,"
5 Conclusion
",2019,"in future work, we will extend the proposed idea to other qa tasks with evidence of multimodality, e.g.combining with symbolic approaches for visual qa (gan et al., 2017; mao et al., 2019; hu et al., 2019)."
P19-1420,"
6 Conclusion

",2019,we proposed a weakly supervised framework to utilize the full information in data.
P19-1421,"
6 Conclusions and Future Work
",2019,"promising future directions would be to investigate how to utilize user interaction in moocs more adequately, as well as how attributes of course concepts can help expanding."
P19-1421,"
6 Conclusions and Future Work
",2019,we precisely define the problem and propose an active model to search external knowledge base for candidate concepts and detect high-quality ones with a classifier.
P19-1423,"
6 Conclusion
",2019,"as future work, we plan to incorporate joint named entity recognition training as well as sub-word embeddings in order to further improve the performance of the proposed model."
P19-1424,"
7 Limitations and Future Work
",2019,"finally, we plan to adapt bespoke models proposed for the chinese criminal court (luo et al., 2017; zhong et al., 2018; hu et al., 2018) to data from other courts and explore multitask learning."
P19-1424,"
7 Limitations and Future Work
",2019,"providing valid justifications is an important priority for future work and an emerging topic in the nlp community.8 in this direction, we plan to expand the scope of this study by exploring the automated analysis of additional resources (e.g., relevant case law, dockets, prior judgments) that could be then utilized in a multi-input fashion to further improve performance and justify system decisions."
P19-1424,"
7 Limitations and Future Work
",2019,"we also plan to apply neural methods to data from other courts, e.g., the european court of justice, the us supreme court, and multiple languages, to gain a broader perspective of their potential in legal justice prediction."
P19-1425,"
6 Conclusion
",2019,"in future work, we plan to explore the direction to generate more natural adversarial examples dispensing with word replacements and more advanced defense approaches such as curriculum learning (jiang et al., 2018, 2015)."
P19-1428,"
7 Conclusions and future work
",2019,"another issue to research will be the use of regression models to estimate the expected fitness of a pipeline given its features, as illustrated in section 6."
P19-1428,"
7 Conclusions and future work
",2019,"as future work, we plan to study the introduction of high-level knowledge to deal with the issue of invalid pipelines and improve the performance of the optimization process."
P19-1428,"
7 Conclusions and future work
",2019,"finally, by modifying the grammar, this strategy can be extensible to other machine learning challenges."
P19-1428,"
7 Conclusions and future work
",2019,"in addition, the data gathered during the optimization provided insights about the optimal settings to deal with the challenge faced."
P19-1428,"
7 Conclusions and future work
",2019,"therefore, we plan to explore this line of research in the future, to compare our proposal with other automl frameworks in standard benchmarks."
P19-1428,"
7 Conclusions and future work
",2019,"this addition would support meta-learning algorithms, allowing to reduce the optimization time and increase its performance by learning from past executions."
P19-1428,"
7 Conclusions and future work
",2019,this knowledge can be in the form of explicit rules that guarantee the validity of the pipelines sampled from the grammar; and in the form of statistical information extracted from similar challenges that helps pre-defining a probabilistic model.
P19-1429,"
6 Conclusions
",2019,"for future work, we plan to investigate new auxiliary ?-learning algorithms using our ?-learning framework."
P19-1429,"
6 Conclusions
",2019,"representation learning is a fundamental technique for nlp tasks, especially for resolving the ambiguity and the diversity problem of natural language expressions."
P19-1430,"
5 Conclusion and Future Work
",2019,"although we have used word, sense and character information in our work, more level of information can be incorporated into the mg lattice."
P19-1430,"
5 Conclusion and Future Work
",2019,"here, sememe is the minimum semantic unit of word sense, whose information may potentially assist the model to explore deeper semantic features."
P19-1430,"
5 Conclusion and Future Work
",2019,"in the future, we will attempt to improve the ability of the mg lattice to utilize multi-grained information."
P19-1430,"
5 Conclusion and Future Work
",2019,"the model incorporates word-level information into character sequences to explore deep semantic features and avoids the issue of polysemy ambiguity by introducing external linguistic knowledge, which is regarded as sense-level information."
P19-1431,"
5 Conclusion
",2019,"future research will look into applying such methods to reason jointly about text and kg, by attending to textual mentions of entities in addition to graph (verga et al., 2016)."
P19-1432,"
5 Conclusion & Future Work
",2019,"consequently, in the future work, we plan to develop methods that can automatically induce the sentence structures for efp."
P19-1434,5 Conclusion,2019,further qualitative analysis of memory contents after learning confirms that such good performance comes from its ability to retain important instances for future qa tasks.
P19-1435,7 Conclusion,2019,"in this paper, we take a close look at the selection bias of nlsm datasets and focus on the selection bias embodied in the comparing relationships of sentences."
P19-1435,7 Conclusion,2019,"we suggest for future nlsm datasets, the providers should pay more attention to this problem."
P19-1436,7 Conclusion,2019,future work includes better phrase representation learning to close its accuracy gap with qa models with query-dependent document encoding.
P19-1436,7 Conclusion,2019,"our phrase representations leverage sparse and dense vectors to capture lexical, semantic, and syntactic information."
P19-1436,7 Conclusion,2019,utilizing the phrase index as an external memory for an interaction with text-based knowledge is also an interesting direction.
P19-1437,7 Conclusion,2019,"in this work, we aim to improve language modeling with shared grammar."
P19-1438,7 Conclusion,2019,promising future directions include experimenting with our zero-shot adaptation methods in the context of neural semantic parsing (after increasing the number of examples per domain) and extending the dataset to include more complicated applications and multi-utterance instructions.
P19-1438,7 Conclusion,2019,we hope this work will inspire readers to use our framework for collecting a larger dataset and experimenting with more approaches.
P19-1439,8 Conclusions,2019,"target task performance continues to improve with more lm data, even at large scales, suggesting that further work scaling up lm pretraining is warranted."
P19-1439,8 Conclusions,2019,these trends suggest that improving on lm pretraining with current techniques will be challenging.
P19-1439,8 Conclusions,2019,we present a systematic comparison of tasks and task combinations for the pretraining and intermediate fine-tuning of sentence-level encoders like those seen in elmo and bert.
P19-1439,8 Conclusions,2019,"while further work on language modeling seems straightforward and worthwhile, we believe that the future of this line of work will require a better understanding of the settings in which target task models can effectively utilize outside knowledge and data, and new methods for pretraining and transfer learning to do so."
P19-1441,5 Conclusion,2019,"at last, we also would like to verify whether mt-dnn is resilience against adversarial attacks (glockner et al., 2018; talman and chatzikyriakidis, 2018; liu et al., 2019)."
P19-1441,5 Conclusion,2019,"there are many future areas to explore to improve mt-dnn, including a deeper understanding of model structure sharing in mtl, a more effective training method that leverages relatedness among multiple tasks, for both fine-tuning and pre-training (dong et al., 2019), and ways of incorporating the linguistic structure of text in a more explicit and controllable manner."
P19-1446,4 Conclusion,2019,"sembleu can be potentially used to compare other types of graphs, including cyclic graphs."
P19-1447,4 Conclusion,2019,in the future we plan to apply the reranker to other parsers and more benchmark datasets.
P19-1447,4 Conclusion,2019,we will also attempt to jointly train the base semantic parser and the reranker by using the reranker’s output as supervision to fine tune the base parser.
P19-1449,5 Conclusion,2019,"given these results, and the continued difficulty neural methods have with the winograd schema challenge, we argue that future work on glue-style sentence understanding tasks might benefit from a focus on learning from smaller training sets."
P19-1450,5 Conclusion,2019,"in the future, we would like to extend our approach to sembanks which are annotated with different types of semantic representation, e.g."
P19-1450,5 Conclusion,2019,we will explore latent-variable models to learn the dependency trees automatically.
P19-1452,5 Conclusion,2019,we employ the edge probing task suite to explore how the different layers of the bert network can resolve syntactic and semantic structure within a sentence.
P19-1455,7 Conclusion and Future Work,2019,"advanced techniques to learn multimodal relationships could incorporate better relationship modeling (majumder et al., 2018), and exploit models that provide gesture, facial and pose information about the people in the scene (cao et al., 2018)."
P19-1455,7 Conclusion and Future Work,2019,another direction could be to create fusion strategies that can better model incongruity among modalities to identify sarcasm.
P19-1455,7 Conclusion and Future Work,2019,"future work could investigate advanced spatiotemporal fusion strategies (e.g., tensor-fusion (zadeh et al., 2017), cca (hotelling, 1936)) to better encode the correspondence between modalities."
P19-1455,7 Conclusion and Future Work,2019,future work should try to leverage these factors to improve the baseline scores reported in this paper.
P19-1455,7 Conclusion and Future Work,2019,"future work should try to overcome this issue with solutions involving pre-training, transfer learning, domain adaption, or low-parameter models."
P19-1455,7 Conclusion and Future Work,2019,"moreover, while conducting this research, we identified several challenges that we believe are important to address in future research work on multimodal sarcasm detection."
P19-1455,7 Conclusion and Future Work,2019,"this, however, arises the problem of over-fitting in complex neural models."
P19-1455,7 Conclusion and Future Work,2019,we also experimented with the integration of context and speaker information as additional input for our models.
P19-1456,6 Conclusion,2019,"besides, developing techniques to incorporate the claim stance and specificity detection models in argument generation to generate more coherent and consistent arguments is another interesting research direction to be explored."
P19-1456,6 Conclusion,2019,"for future work, it may be interesting to understand which other models would be effective in claim specificity and stance detection tasks."
P19-1458,"
9 Discussion and Future Considerations
",2019,"in the future, we will investigate other nlp tasks such as named entity recognition (ner), question answering (qa) and aspect-based sentiment analysis (absa) (pontiki et al., 2016) to see whether results we saw in sentiment analysis is consistent across these tasks."
P19-1458,"
9 Discussion and Future Considerations
",2019,this research is a work in progress and will be regularly updated with new benchmarks and baselines.
P19-1458,"
9 Discussion and Future Considerations
",2019,we hope that our experimental results inspire future research dedicated to japanese.
P19-1459,8 Conclusion,2019,arct provides a fortuitous opportunity to see how stark the problem of exploiting spurious statistics can be.
P19-1459,8 Conclusion,2019,"taken with a growing body of previous work, our results indicate the need for further research into the extent of this problem in nlp more generally."
P19-1459,8 Conclusion,2019,the adversarial dataset should be adopted as the standard in future work on arct.
P19-1459,8 Conclusion,2019,we hope that providing a more robust evaluation will help to spur more productive research on this problem.
P19-1460,5 Conclusion and Future Work,2019,"finally, it would be insightful to further visualise the reasons in the embedded space with more advanced visualisation tools."
P19-1460,5 Conclusion and Future Work,2019,"first, it is necessary to evaluate our model on more stance data with different linguistic properties (e.g., the much longer and richer stance utterances in posts or articles)."
P19-1460,5 Conclusion and Future Work,2019,"in the future, this work can be progressed in several ways."
P19-1462,4 Conclusion,2019,"in future works, we will explore the extension of this approach for other tasks."
P19-1463,5 Conclusion,2019,"for future work, we plan to i) automatically predict relations between argument components in the uselecdeb60to16 dataset, and ii) propose a new task, i.e., fallacy detection so that common fallacies in political argumentation (zurloni and anolli, 2010) can be automatically identified, in line with the work of (habernal et al., 2018)."
P19-1464,5 Conclusion and Future work,2019,"another direction is to explore span representations in several related tasks such as rst-style discourse parsing or new span-related argumentation mining tasks (trautmann et al., 2019)."
P19-1464,5 Conclusion and Future work,2019,one interesting line of our future work is to investigate the performance of our model in an endto-end setting (including ac segmentation).
P19-1464,5 Conclusion and Future work,2019,"specifically, we have investigated (i) an lstm-minus-based span representation originally developed for other nlp tasks and (ii) a task-specific extended representation capturing the am/ac distinction for asp."
P19-1465,5 Conclusion,2019,"due to its fast speed and strong performance, the model is quite suitable for a wide range of related applications."
P19-1466,5 Conclusion and Future Work,2019,"additionally, we generalize and extend graph attention mechanisms to capture both entity and relation features in a multihop neighborhood of a given entity."
P19-1466,5 Conclusion and Future Work,2019,"in the future, we intend to extend our method to better perform on hierarchical graphs and capture higher-order relations between entities (like motifs) in our graph attention model."
P19-1466,5 Conclusion and Future Work,2019,our detailed and exhaustive empirical analysis gives more insight into our method’s superiority for relation prediction on kgs.
P19-1466,5 Conclusion and Future Work,2019,"the proposed model can be extended to learn embeddings for various tasks using kgs such as dialogue generation (he et al., 2017; keizer et al., 2017), and question answering (zhang et al., 2016; diefenbach et al., 2018)."
P19-1467,7 Conclusion,2019,"in future work, we intend to refine these alignment boundaries and to optimize the alignment procedure for speed."
P19-1467,"
7 Conclusion
",2019,we hope that this work will raise more interest in developing alignment systems for longer paraphras.
P19-1469,"
6 Conclusion
",2019,"for future work, we plan to focus on generating more realistic text and use the generated text in other tasks e.g.data augmentation, addressing adversarial attack."
P19-1469,"
6 Conclusion
",2019,"given a pair of text sequences and the corresponding label, it uses one of the sequences and the label as input and generates the other sequence."
P19-1469,"
6 Conclusion
",2019,"we also defined multiple semantic constraints to further regularize the model, and observed that fine-tuning with them gives additional increase in performance."
P19-1469,"
6 Conclusion
",2019,we expect the model could perform more natural generation via applying recent advancements on deep generative models.
P19-1470,"
7 Conclusion
",2019,"these positive results point to future work in extending the approach to a variety of other types of knowledge bases, as well as investigating whether comet can learn to produce openie-style knowledge tuples for arbitrary knowledge seeds."
P19-1473,"
7 Conclusions and Future Work
",2019,an interesting direction would be to enable domain experts to identify and actively request for program annotations given the knowledge shared by other domains.
P19-1473,"
7 Conclusions and Future Work
",2019,this would further make it feasible to perform transfer learning on a new domain.
P19-1473,"
7 Conclusions and Future Work
",2019,we also plan to investigate the possibility of augmenting the parallel corpus by bootstrapping from shared templates across domains.
P19-1473,"
7 Conclusions and Future Work
",2019,we plan to explore if further fine-tuning using denotations based training on the distilled model can lead to improvements in the unified parser.
P19-1473,"
7 Conclusions and Future Work
",2019,we would also like to explore if guiding the decoder through syntactical and domain-specific constraints helps in reducing the search space for the weakly supervised unified parser.
P19-1474,"
6 Conclusion
",2019,a prominent direction for future work is using the hyperbolic embeddings as the sole signal for taxonomy extraction.
P19-1474,"
6 Conclusion
",2019,"since distributional and hyperbolic embeddings cover different relations between terms, it may be interesting to combine them."
P19-1474,"
6 Conclusion
",2019,"this observation confirms the theoretical capability of poincare embeddings to learn ′ hierarchical relations, which enables their future use in a wide range of semantic tasks."
P19-1476,"
4 Conclusion
",2019,"we next plan to evaluate glen on multilingual and cross-lingual graded le datasets (vulic et al.′ , 2019) and release a large multilingual repository of le-specialized embeddings."
P19-1476,"
4 Conclusion
",2019,"we presented glen, a general framework for specializing word embeddings for lexical entailment."
P19-1477,"
5 Conclusion
",2019,"future work will entail adaption of the attentions, to further improve the performance."
P19-1477,"
5 Conclusion
",2019,"however, although bert seems to implicitly establish complex relationships between entities facilitating tasks such as coreference resolution, the results also suggest that solving commonsense reasoning tasks might require more than leveraging a language model trained on huge text corpora."
P19-1478,"
6 Summary and Outlook
",2019,"furthermore, to further improve the results on wsc273, data-filtering procedures may be introduced to find harder wsc-like examples."
P19-1478,"
6 Summary and Outlook
",2019,"in future work, other uses and the statistical significance of maskedwiki’s impact and its applications to different tasks will be investigated."
P19-1478,"
6 Summary and Outlook
",2019,"this is particularly surprising, because previous work (opitz and frank, 2018) implies that generalizing to wsc273 is hard."
P19-1479,"
5 Conclusion
",2019,"in the future, we would like to explore how to introduce external knowledge into the graph to make the generated comments more logical."
P19-1480,"
7 Conclusion and Future Work
",2019,integrating answer span identification into the presented system is a promising direction.
P19-1480,"
7 Conclusion and Future Work
",2019,there are several future directions for this setting.
P19-1481,"
7 Conclusion
",2019,"in future work, we will explore the use of cross-lingual embeddings to further improve performance on this task."
P19-1484,"
6 Conclusion
",2019,"however, we note that whilst our results are encouraging on this relatively simple qa task, further work is required to handle more challenging qa elements and to reduce our reliance on linguistic resources and heuristics."
P19-1484,"
6 Conclusion
",2019,"in this work, we explore whether it is possible to to learn extractive qa behaviour without the use of labelled qa data."
P19-1487,"
7 Conclusion and Future Work
",2019,"with a sufficient dataset of explanations (analogous to cos-e) for many tasks, it might be possible to fine-tune a more general explanatory language model that generates more useful explanations for unseen tasks."
P19-1487,"
7 Conclusion and Future Work
",2019,"with deferral of explanation to neural models, it will be crucial in the future to study the ethical implications of biases that are accumulated during pretraining or fine-tuning."
P19-1489,"
7 Discussion: What Modularity Can and
Cannot Do
",2019,"additionally, we believe that modularity could serve as a useful prior for the algorithms that learn cross-lingual word embeddings: during learning prefer updates that avoid increasing modularity if all else is equal."
P19-1489,"
7 Discussion: What Modularity Can and
Cannot Do
",2019,"future work should investigate how to combine techniques that use both word meaning and nearest neighbors for a more robust, semisupervised cross-lingual evaluation."
P19-1489,"
7 Discussion: What Modularity Can and
Cannot Do
",2019,this work focuses on modularity as a diagnostic tool: it is cheap and effective at discovering which embeddings are likely to falter on downstream tasks.
P19-1489,"
7 Discussion: What Modularity Can and
Cannot Do
",2019,"thus, practitioners should consider including it as a metric for evaluating the quality of their embeddings."
P19-1490,"
5 Conclusion and Future Work
",2019,"in the future, we will work on cross-lingual extensions of monolingual hyperbolic embedding models (nickel and kiela, 2017; ganea et al., 2018)."
P19-1490,"
5 Conclusion and Future Work
",2019,"we have proposed a novel graded cross-lingual lexical entailment (le) task, introducing new monolingual and cross-lingual graded le datasets that hold promise to support future research on this topic."
P19-1490,"
5 Conclusion and Future Work
",2019,"we will also experiment with other sources of bilingual information (e.g., cross-lingual word embeddings) and port the transfer approach to more language pairs, with a particular focus on resource-poor languages."
P19-1491,"
7 Conclusion
",2019,"our mixed-effects approach could be used to assess other nlp systems via parallel texts, separating out the influences on performance of language, sentence, model architecture, and training procedure."
P19-1492,"
6 Conclusions and future work
",2019,"in particular, we would like to explore new methods to jointly learn cross-lingual embeddings on monolingual corpora."
P19-1493,"
6 Conclusion
",2019,it is our hope that these kinds of probing experiments will help steer researchers toward the most promising lines of inquiry by encouraging them to focus on the places where current contextualized word representation approaches fall short.
P19-1493,"
6 Conclusion
",2019,"the model handles transfer across scripts and to code-switching fairly well, but effective transfer to typologically divergent and transliterated targets will likely require the model to incorporate an explicit multilingual training objective, such as that used by lample and conneau (2019) or artetxe and schwenk (2018)."
P19-1494,"
6 Conclusions and future work
",2019,"in addition to that, we would like to integrate our induced dictionaries in other downstream tasks like unsupervised cross-lingual information retrieval (litschko et al., 2018)."
P19-1494,"
6 Conclusions and future work
",2019,"in the future, we would like to further improve our method by incorporating additional ideas from unsupervised machine translation such as joint refinement and neural hybridization (artetxe et al., 2019)."
P19-1495,"
7 Conclusions & Future Work

",2019,a predictive model for identification of complaints is useful to companies that wish to automatically gather and analyze complaints about a particular event or product.
P19-1495,"
7 Conclusions & Future Work

",2019,"another research direction is to study the role of complaints in personal conversation or in the political domain, e.g., predicting political stance in elections (tsakalidis et al., 2018)."
P19-1495,"
7 Conclusions & Future Work

",2019,"in the future, we plan to identify the target of the complaint in a similar way to aspect-based sentiment analysis (pontiki et al., 2016)."
P19-1495,"
7 Conclusions & Future Work

",2019,this would allow them to improve efficiency in customer service or to more cheaply gauge popular opinion in a timely manner in order to identify common issues around a product launch or policy proposal.
P19-1495,"
7 Conclusions & Future Work

",2019,we plan to use additional context and conversational structure to improve performance and identify the sociodemographic covariates of expressing and phrasing complaints.
P19-1497,"
5 Conclusion and Future Work
",2019,"besides, how to better leverage prior knowledge during openqg (like human often do) is also interesting."
P19-1497,"
5 Conclusion and Future Work
",2019,"first, we will explore more powerful qg structure to deal with the huge difference between the length of input and output texts."
P19-1497,"
5 Conclusion and Future Work
",2019,there are many future works to be done.
P19-1498,"
5 Conclusion
",2019,"using a human labeled dataset with rumor-veracity labels for source posts and stance labels for replies, we evaluated the proposed models and compared their strengths and weaknesses."
P19-1499,"
5 Conclusions

",2019,adding the large open-domain dataset to pre-training leads to even better performance.
P19-1499,"
5 Conclusions

",2019,"in the future, we plan to apply models to other tasks that also require hierarchical document encodings (e.g., document question answering)."
P19-1499,"
5 Conclusions

",2019,we are also interested in improving the architectures of hierarchical document encoders and designing other objectives to train hierarchical transformers.
P19-1499,"
5 Conclusions

",2019,we proposed a method to pre-train document level hierarchical bidirectional transformer encoders on unlabeled data.
P19-1500,"
6 Conclusions
",2019,in the future we would like to apply our hierarchical transformer to question answering and related textual inference tasks.
P19-1501,"
7 Conclusion and Future Work
",2019,"moreover, as the ambiguity is a challenging problem in natural language processing, it would be interesting to capture the particular meaning of each word in the text so that our methodology manages to uncover the specific semantic meaning of words."
P19-1501,"
7 Conclusion and Future Work
",2019,"since currently only nouns are considered for generalization, an expansion to verbs could result in additional improvement."
P19-1501,"
7 Conclusion and Future Work
",2019,the positive results may be attributed to the optimization of the parameters of the deep leaning model and the ability of the method to handle oov and very low frequency words.
P19-1502,"
5 Conclusion
",2019,such annotations would also be greatly beneficial to improve summarization systems and evaluation metrics alike.
P19-1503,"
5 Conclusion
",2019,future work could be comparing language models of different types and scales in this direction.
P19-1505,"
6 Conclusion
",2019,"a canonical example is the past participle of stride (e.g., ?strode/ ?stridden/ ?strided)."
P19-1505,"
6 Conclusion
",2019,"more generally, we observe that our wug-test techniques provides a general way of studying regularity and predictability within languages and may prove useful for attacking other difficult problems in the literature, such as detecting inflectional classes."
P19-1505,"
6 Conclusion
",2019,perhaps of greater interest than this positive result is the difference in the strength of the correlation between the level of individual forms and the level of lexemes.
P19-1505,"
6 Conclusion
",2019,"such models seem necessary to account for paradigmatic structure cross linguistically and to deal with phenomena such as the existence of defective paradigms—the phenomenon whereby certain inflected forms of a word seem to be impossible for speakers (baerman et al., 2010)."
P19-1506,"
5 Discussion and Conclusions

",2019,"the surprisal of a word or phrase refers to the degree of mismatch between what a human listener expected to be said next and what is actually said, for example, when a garden path sentence forces the listener to abandon a partial, incremental parse (ferreira and henderson, 1991; hale, 2001)."
P19-1507,"
6 Conclusion

",2019,"encouraged by these findings, we use deep networks to generate synthetic brain data to show that it helps in improving accuracy in a subsequent stimulus decoding task."
P19-1508,"
8 Conclusion

",2019,our work paves the way towards understanding the extent to which human meaning representation is impacted by negation.
P19-1509,"
5 Discussion
",2019,"a better understanding of what are the “innate” biases of standard models in highly controlled setups, such as the one studied here, should complement large-scale simulations, as part of the effort to develop new methods to encourage the emergence of more human-like language."
P19-1509,"
5 Discussion
",2019,how to incorporate “effort”-based pressures in neural networks is an exciting direction for future work.
P19-1509,"
5 Discussion
",2019,the research direction we introduced might lead to a better understanding of the biases that affect the linguistic behaviour of lstms and similar models.
P19-1509,"
5 Discussion
",2019,"this could help current efforts towards the development of artificial agents that communicate to solve a task, with the ultimate goal of developing ais that can talk with humans."
P19-1510,"
5 Summary
",2019,"we are optimistic that nne will encourage the development of new ner models that recognize structural information within entities, and therefore understand finegrained semantic information captured."
P19-1511,"
6 Conclusions and Future Work
",2019,"as the head-driven structures are widely spread in natural language, the solution proposed in this paper can also be used for modeling and exploiting this structure in many other nlp tasks, such as semantic role labeling and event extraction."
P19-1512,"
6 Conclusion
",2019,"looking forward, our attention mechanism can also be applied to tasks such as relational networks (santoro et al., 2017), natural language inference (maccartney and manning, 2009), and qa systems (zhou et al., 2015)."
P19-1513,"
8 Conclusions
",2019,"also the work reported in this paper is based on a small tdm taxonomy, we plan to construct a tdm knowledge base and provide an applicable system for a wide range of nlp papers."
P19-1513,"
8 Conclusions
",2019,"in the future, more effort is needed to extract the best score."
P19-1513,"
8 Conclusions
",2019,we created two datasets in the nlp domain to test our system.
P19-1516,"
7 Conclusion
",2019,"in future, we would like to assist the model with multiple linguistic aspects of social media text like figurative languages."
P19-1517,"
4 Conclusion and Future Work
",2019,"in the future, we would also like to investigate better models that are capable to address general arithmetic word problems, including addition, subtraction, multiplication and division."
P19-1520,"
5 Conclusion and Future Work
",2019,"for future work, we plan to apply the main idea of our approach to other tasks."
P19-1522,"
6 Conclusion and Discussion
",2019,"such features are ignored in our model, but they deserve more investigation for improving the extraction model."
P19-1522,"
6 Conclusion and Discussion
",2019,"therefore, for the future work, we will incorporate relation between events and relation between arguments into pre-trained language models, and take effective measures to overcome the deviation problem of roles in the generation."
P19-1522,"
6 Conclusion and Discussion
",2019,"to solve the roles overlap problem, our extraction approach tries to separate the argument predictions in terms of roles."
P19-1523,"
5 Conclusion
",2019,an error analysis is performed to shed light on possible future directions.
P19-1524,"
4 Discussion
",2019,"also, we would like to further explore the possibility to use domain-specific gazetteers or dictionaries to boost the performance of ner in various domains (shang et al., 2018), beyond the standard corpora."
P19-1524,"
4 Discussion
",2019,"future directions will include trying similarly enhanced modules on other different types of segmental models (kong et al., 2016; liu et al., 2016; zhuo et al., 2016; zhai et al., 2017; sato et al., 2017), along with richer representations for further gain."
P19-1528,"
7 Conclusion
",2019,the problem of adapting this approach to higher order statistical language models (such as pcfgs) remains open.
P19-1529,"
6 Conclusion and Future Work
",2019,"for example, is it better for all the models in an ensemble to use the same external information, or is it more effective if they make use of different kinds of information?"
P19-1529,"
6 Conclusion and Future Work
",2019,in future work we will explore how external information is best used in ensembles of models for srl and other tasks.
P19-1529,"
6 Conclusion and Future Work
",2019,we will also investigate whether the choice of method for injecting external information has the same impact on other nlp tasks as it does on srl.
P19-1530,"
5 Conclusion
",2019,our proposed conversion method can be easily combined with other tree-based parsers.
P19-1531,"
5 Conclusion
",2019,"it combines multi-task learning, auxiliary tasks, and sequence labeling parsing, so that constituency and dependency parsing can benefit each other through learning across their representations."
P19-1532,"
5 Conclusion
",2019,"considering reinforce method is used in training, a novel method is introduced to reduce the variance of rewards."
P19-1532,"
5 Conclusion
",2019,"since our prism module can be easily integrated into existing models, it can be applied in a wide range of neural architectures."
P19-1533,"
7 Conclusion

",2019,future work will consider problems where more challenging forms of neighbor manipulation are necessary for prediction.
P19-1567,"
7 Conclusion
",2019,we hope that this work will raise more interest in developing alignment systems for longer paraphrase.
P19-1568,"
7 Conclusion and Future Work

",2019,another potential future work would be to explore other ways of providing rich supervision from textual descriptions as targets.
P19-1568,"
7 Conclusion and Future Work

",2019,ewise uses sense embeddings as targets instead of discrete sense labels.
P19-1568,"
7 Conclusion and Future Work

",2019,"our modular architecture opens up various avenues for improvements in few-shot learning for wsd, viz., context encoder, definition encoder, and leveraging structural knowledge."
P19-1568,"
7 Conclusion and Future Work

",2019,"this helps the model gain zero-shot learning capabilities, demonstrated through ablation and detailed analysis."
P19-1569,"
7 Future Work
",2019,in future work we plan to use multilingual resources (i.e.embeddings and glosses) for improving our sense embeddings and evaluating on multilingual wsd.
P19-1569,"
7 Future Work
",2019,"we expect our sense embeddings to be particularly useful in downstream tasks that may benefit from relational knowledge made accessible through linking words (or spans) to commonsense-level concepts in wordnet, such as natural language inference."
P19-1569,"
7 Future Work
",2019,"we’re also considering exploring a semi-supervised approach where our best embeddings would be employed to automatically annotate corpora, and repeat the process described on this paper until convergence, iteratively fine-tuning sense embeddings."
P19-1570,"
8 Conclusion and future work
",2019,a natural extension to this work would be to capture the sense distribution of sentences using the same framework.
P19-1570,"
8 Conclusion and future work
",2019,this will make our model more comprehensive by enabling the embedding of words and short texts in the same space.
P19-1570,"
8 Conclusion and future work
",2019,"we motivated an efficient unsupervised method to embed words, in and out of context, in a way that captures their multiple senses in a corpus in an interpretable manner."
P19-1571,"
6 Conclusion and Future Work
",2019,"in the future, we will explore the following directions: (1) context information is also essential to mwe representation learning, and we will try to combine both internal information and external context information to learn better mwe representations; (2) many mwes lack sememe annotation and we will seek to calculate an mwe’s scd when we only know the sememes of the mwe’s constituents; (3) our proposed models are also applicable to the mwes with more than two constituents and we will extend our models to longer mwes; (4) sememe is universal linguistic knowledge and we will explore to generalize our methods to other languages."
P19-1571,"
6 Conclusion and Future Work
",2019,we first design an sc degree (scd) measurement experiment to preliminarily prove the usefulness of sememes in modeling sc.
P19-1572,"
6 Conclusion
",2019,"finally, we plan to further extend and use the humour dataset to investigate open questions on the linguistics of humour, such as what relationships hold between a pun’s phonology and its “successfulness” or humorousness (lagerquist, 1980; hempelmann and miller, 2017)."
P19-1572,"
6 Conclusion
",2019,"given that our model achieves good results with rudimentary, task-agnostic linguistic features, in future work we plan to investigate the use of humourand metaphor-specific features, including some of those used in past work (see §2) as well as those inspired by the prevailing linguistic theories of humour (attardo, 1994) and metaphor (black, 1955; lakoff and johnson, 1980)."
P19-1572,"
6 Conclusion
",2019,"the benefits of including word and bigram frequency also point to possible further improvements using n-grams, tf–idf, or other task-agnostic linguistic features."
P19-1573,"
7 Conclusion
",2019,"further research is required to find out in what lies the success of laser embeddings: in the embedding size, in the magnitude of training data, or maybe in the multitude of used languages."
P19-1573,"
7 Conclusion
",2019,we presented a methodology of empirical research on retention of linguistic information in sentence embeddings using probing and downstream tasks.
P19-1575,"
6 Conclusions
",2019,future work can improve this method further by examining the effects of different dimensionality reduction methods with varying properties on extracting the most informative pathways from the activations.
P19-1575,"
6 Conclusions
",2019,"this general interpretation method draws similar conclusions to highly domain-specific analyses, and while it will not replace the need for deep analysis, it provides a much simpler starting point for a broad class of models."
P19-1576,"
5 Conclusions and Future Work
",2019,"another exciting avenue would involve exploring cross-lingual transfer of lfs, taking advantage of recent development in unsupervised cross-lingual embedding learning (artetxe et al., 2017; conneau et al., 2017)."
P19-1576,"
5 Conclusions and Future Work
",2019,"in addition, we have used the diffvec (vylomova et al., 2016) dataset to provide a frame of reference, as this dataset has been extensively studied in the distributional semantics literature, mostly for evaluating the role of vector difference."
P19-1576,"
5 Conclusions and Future Work
",2019,"in the future, we would like to experiment with more data, so that enough training data can be obtained for less frequent lfs."
P19-1576,"
5 Conclusions and Future Work
",2019,"to this end, we could benefit from the supervised approach proposed in (rodr′?guez-fernandez et al.′ , 2016), and then filter by pairwise correlation strength metrics such as pmi."
P19-1577,"
5 Conclusion 
",2019,"in future, we plan to develop an automatic procedure of finding thesaurus regularities in dbag of problematic words, which can make more evident what kind of relations or senses are missed in the thesaurus."
P19-1578,"
4 Discussion and Future Work
",2019,"for the future work, we hope to extend this idea proposed in this paper to train a model capable of handling different types of errors through the generative model since it can generate different lengths of results."
P19-1578,"
4 Discussion and Future Work
",2019,one concern is that we need to reconsider how to incorporate confusionsets into the encoder-decoder architecture.
P19-1579,"
7 Conclusion
",2019,"in future work, we will explore methods for controlling the induced dictionary quality to improve word substitution as well as m-umt."
P19-1579,"
7 Conclusion
",2019,"we propose a generalized data augmentation framework for low-resource translation, making best use of all available resources."
P19-1579,"
7 Conclusion
",2019,we will also attempt to create an end-toend framework by jointly training m-umt pivoting system and low-resource translation system in an iterative fashion in order to leverage more versions of augmented data.
P19-1580,"
8 Conclusions
",2019,"in future work, we would like to investigate how our pruning method compares to alternative methods of model compression in nmt."
P19-1583,"
4 Conclusion
",2019,"we propose target conditioned sampling (tcs), an efficient data selection framework for multilingual data by constructing a data sampling distribution that facilitates the nmt training of lrls."
P19-1584,"
10 Conclusions & Future Work
",2019,future work: the automatic pseudonymization approach could serve as a data augmentation scheme to be used as a regularizer for deidentification models.
P19-1584,"
10 Conclusions & Future Work
",2019,pooling of training data for de-identification from multiple institutions would lead to much more robust classifiers.
P19-1584,"
10 Conclusions & Future Work
",2019,private character embeddings that are learned from a perturbed source could be an interesting extension to our models.
P19-1584,"
10 Conclusions & Future Work
",2019,"when more training data from multiple sources become available in the future, it will be possible to evaluate our adversarially learned representation against unseen data."
P19-1586,"
7 Conclusion
",2019,our frameworks of transfer and active learning for deep learning models are potentially applicable to low-resource settings beyond entity resolution.
P19-1586,"
7 Conclusion
",2019,these results serve as further support for the claim that deep learning can provide a unified data integration method for downstream nlp tasks.
P19-1587,"
5 Conclusion
",2019,this work also opens up a range of questions from modeling to evaluation methodology
P19-1588,"
6 Conclusion
",2019,"in the future, first, we try to utilize more eyetracking corpus and estimate more features of reading behavior."
P19-1588,"
6 Conclusion
",2019,"then, we will attempt to analyze real human reading behavior on social media and thereby explore more specific human attention features on social media."
P19-1589,"
5 Conclusion and Future Work
",2019,"in future work, we want to extend this approach to other natural language processing tasks."
P19-1590,"
9 Conclusions
",2019,"the emergence of models like elmo and bert has revived semi-supervised nlp, demonstrating that pretraining large models on massive amounts of data can provide representations that are beneficial for a wide range of nlp tasks."
P19-1591,"
6 Conclusions
",2019,"in future work we would like to develop more sophisticated trl algorithms, for both in-domain and domain adaptation nlp setups."
P19-1591,"
6 Conclusions
",2019,"moreover, we would like to establish the theoretical groundings to the improved stability achieved by trl, and to explore this effect beyond domain adaptation."
P19-1591,"
6 Conclusions
",2019,the resulting pblm-cnn model improves both the accuracy and the stability of the original pblm-cnn model where pblm is trained without trl.
P19-1591,"
6 Conclusions
",2019,we proposed task refinement learning algorithms for domain adaptation with representation learning.
P19-1592,"
6 Conclusion
",2019,"additionally, we hope to apply stance to a wider-range of entity resolution tasks, for which string similarity is a component of model that considers additional features such as the natural language context of the entity mention."
P19-1592,"
6 Conclusion
",2019,"in future work, we hope to further study the connections between our optimal transport-based alignment method and methods based on attention."
P19-1592,"
6 Conclusion
",2019,"we also hope to consider connections to work on probabilistic latent representation of permutations and matchings (mena et al., 2018; linderman et al., 2018)."
P19-1593,"
5 Conclusion
",2019,"a key question for future work is the performance on longer texts, such as the full-length news articles encountered in ontonotes."
P19-1593,"
5 Conclusion
",2019,"another direction is to further explore semi-supervised learning, by reducing the amount of training data and incorporating linguistically-motivated constraints based on morphosyntactic features."
P19-1594,"
5 Conclusions
",2019,"for example, we are interested in word-level language models that make use of character-level pnfa to compute expectations, which is useful to make predictions on words and substrings which do not appear in training."
P19-1594,"
5 Conclusions
",2019,it is also interesting to consider a pnfa as a special case of an rnn which uses linear transitions.
P19-1594,"
5 Conclusions
",2019,the ability of the spectral method for pnfa to estimate substring expectations can be exploited in other contexts.
P19-1595,"
6 Discussion and Conclusion
",2019,"achieving robust multi-task gains across many tasks has remained elusive in previous research, so we hope our work will make multi-task learning more broadly useful within nlp."
P19-1595,"
6 Discussion and Conclusion
",2019,it remains to be fully understood to what extent “self-supervised pre-training is all you need” and where transfer/multi-task learning from supervised tasks can provide the most value.
P19-1596,"
6 Conclusions
",2019,"finally, we are interested in reproducing our corpus generation method on various other domains to allow for the creation of numerous useful datasets for the nlg community."
P19-1596,"
6 Conclusions
",2019,"for future work, we plan on exploring other models for nlg, and on providing models with a more detailed input representation in order to help preserve more dependency information, as well as to encode more information on syntactic structures we want to realize in the output."
P19-1596,"
6 Conclusions
",2019,"we are also interested in including richer, more semanticallygrounded information in our mrs, for example using abstract meaning representations (amrs) (dorr et al., 1998; banarescu et al., 2013; flanigan et al., 2014)."
P19-1596,"
6 Conclusions
",2019,"we train different models with varying levels of information related to attributes, adjective dependencies, sentiment, and style information, and present a rigorous set of evaluations to quantify the effect of the style markup on the ability of the models to achieve multiple style goals."
P19-1597,"
5 Conclusion and Future Work
",2019,and unsupervised approaches to leverage massive chess comments in social media is also worth exploring.
P19-1597,"
5 Conclusion and Future Work
",2019,another interesting direction is to extend our models to multimove commentary generation tasks.
P19-1598,"
7 Conclusions and Future Work
",2019,"our distantly supervised approach to dataset creation can be used with other knowledge graphs and other kinds of text as well, providing opportunities for accurate language modeling in new domains."
P19-1598,"
7 Conclusions and Future Work
",2019,"the limitations of the kglm model, such as the need for marginalization during inference and reliance on annotated tokens, raise new research problems for advancing neural nlp models."
P19-1598,"
7 Conclusions and Future Work
",2019,this work lays the groundwork for future research into knowledge-aware language modeling.
P19-1600,"
6 Conclusion and Future Work
",2019,"in the future, we will explore the representation for the implicit information like whether a man is retired or not or how long a sportsman’s career is given starting and ending years, in the table by including some inference strategies."
P19-1600,"
6 Conclusion and Future Work
",2019,"we set up 3 goals for comprehensive description generation for attribute-value factual tables: accurate, informative and loyal."
P19-1601,"
5 Conclusions and Future W
",2019,how to combine the back-translation with our training algorithm is also a good research direction that is worth to explore.
P19-1601,"
5 Conclusions and Future W
",2019,"in the future, we are planning to adapt our style transformer to the multiple-attribute setting like lample et al.(2019)."
P19-1601,"
5 Conclusions and Future W
",2019,"on the other hand, the backtranslation technique developed in lample et al.(2019) can also be adapted to the training process of style transformer."
P19-1603,"
6 Conclusion and Future Work
",2019,"future work can combine the analyzer and generator via joint training, hopefully to achieve better results."
P19-1604,"
5 Conclusions and Future Work
",2019,"we are extending the proposed approach to other qa datasets, and adapting it to use pretrained language models such as bert (devlin et al., 2018), to evaluate the consistency of the mechanisms introduced."
P19-1605,"
6 Conclusion
",2019,access to parallel data is therefore still advantageous for paraphrase generation and our monolingual method can be a helpful resource for languages where such data is not available.
P19-1606,"
6 Conclusions

",2019,we plan on exploring backpropable variants as a scaffold for structure and also extend the techniques to other how-to domains in future.
P19-1606,"
6 Conclusions

",2019,we plan to explore explicit evaluation of the latent structure learnt.
P19-1608,"
6 Conclusion
",2019,further work on these inductive biases could help understand how a pretrained transfer learning model can be adapted in the most optimal fashion to a given target task.
P19-1610,"
6 Conclusion
",2019,"as post-processing is required to remove semantically dissimilar paraphrased questions, there is scope for developing better techniques for semantic similarity scoring."
P19-1610,"
6 Conclusion
",2019,there are several possible future directions stemming from this work.
P19-1610,"
6 Conclusion
",2019,"there is also scope for better techniques to generate more coherent question paraphrasing when significant question re-writing is required, such as for the situation when we want to paraphrase the question using context words."
P19-1611,"
6 Conclusion
",2019,"altogether, rankqa provides a new, strong baseline for future research on neural qa."
P19-1612,"
11 Conclusion
",2019,"we presented orqa, the first open domain question answering system where the retriever and reader are jointly learned end-to-end using only question-answer pairs and without any ir system."
P19-1613,"
5 Conclusion
",2019,"we recasted sub-question generation as a span prediction problem, allowing the model to be trained on 400 labeled examples to generate high quality sub-questions."
P19-1614,"
5 Conclusion
",2019,at other times a more involved knowledge about actions and properties is needed.
P19-1614,"
5 Conclusion
",2019,it is a general approach may be applied to other commonsense reasoning tasks which require the both the knowledge embedded in the pre-trained language models and more involved knowledge about actions and properties.
P19-1614,"
5 Conclusion
",2019,"so, in this work we utilized the knowledge embedded in the pretrained language models and developed a technique to automatically extract the more involved commonsense knowledge from text repositories."
P19-1615,"
6 Conclusion
",2019,"our analysis also shows the limitations of bert based mcq models, the challenge of learning natural language abductive inference and the multiple types of reasoning required for an openbookqa task."
P19-1616,"
7 Conclusion
",2019,"similar problems may exist in other nlp tasks, which will be interesting to investigate in the future."
P19-1617,"
5 Conclusion
",2019,"in the future, we may incorporate new advances in building entity graphs from texts, and solve more difficult reasoning problems, e.g.the cases of comparison query type in hotpotqa."
P19-1618,"
6 Discussion and Future Work
",2019,"additionally, it would be interesting to study the behavior of nlprolog in the presence of multiple wikihop query predicates."
P19-1618,"
6 Discussion and Future Work
",2019,"for instance, a prover for temporal logic (orgun and ma, 1994) would allow to model temporal dynamics in natural language."
P19-1618,"
6 Discussion and Future Work
",2019,"we are also interested in incorporating future improvements of symbolic provers, triple extraction systems and pretrained sentence representations to further enhance the performance of nlprolog."
P19-1618,"
6 Discussion and Future Work
",2019,"while we focused on a subset of first order logic in this work, the expressiveness of nlprolog could be extended by incorporating a different symbolic prover."
P19-1620,"
5 Conclusion
",2019,"we additionally proposed a possible direction for formal grounding of this method, which we hope to develop more thoroughly in future work."
P19-1621,"
5 Discussion
",2019,"ideally, we want models to be able to reason that “what color is the rose? red” implies “is the rose red?"
P19-1621,"
5 Discussion
",2019,"yes” without needing to add every possible implication or rephrasing of every (q, a) to the training data."
P19-1622,"
4 Conclusion
",2019,we will study further the capability of our approaches on other datasets and tasks in the future work.
P19-1624,"
5 Conclusion
",2019,"specifically, the shallow and the deep strategies exploit the top encoder layer and all the encoder layers, respectively."
P19-1626,"
6 Conclusion
",2019,3 future work includes the fusion of additional operations in neural models.
P19-1627,"
5 Conclusion
",2019,"here, we believe that the new framework can provide considerable help to future research, specifically also, because it does not not require the technical support of high-end clusters."
P19-1627,"
5 Conclusion
",2019,our cognate detection method’s weak performance on south-east asian languages could be addressed by enabling it to detect partial cognates instead of complete cognates.
P19-1627,"
5 Conclusion
",2019,our methods are best used for the purpose of exploratory analysis on larger datasets which have so far not yet been thoroughly studied.
P19-1628,"
6 Conclusions
",2019,"in the future, we would like to investigate whether some of the ideas introduced in this paper can improve the performance of supervised systems as well as sentence selection in multi-document summarization."
P19-1628,"
6 Conclusions
",2019,we revisited a popular graph-based ranking algorithm and refined how node (aka sentence) centrality is computed.
P19-1629,"
8 Conclusions
",2019,"in the future, we would like to more faithfully capture the semantics of documents by explicitly modeling entities and their linking."
P19-1630,"
6 Conclusions
",2019,"an interesting challenge, and open research question, concerns the extent to which synthetic training impacts the overall model generalizability."
P19-1630,"
6 Conclusions
",2019,the latent document structure is induced jointly with the summarization objective.
P19-1630,"
6 Conclusions
",2019,"we believe that training models on heuristic, but inexpensive data sets is a valuable approach which opens up exciting opportunities for future research."
P19-1631,"
6 Conclusion and Future Work
",2019,"another avenue is to incorporate different model attribution strategies such as deeplrp (bach et al., 2015) into the objective function."
P19-1631,"
6 Conclusion and Future Work
",2019,our technique can be applied to implement a more robust model by penalizing the attributions falling outside of tokens annotated to be relevant to the predicted class.
P19-1631,"
6 Conclusion and Future Work
",2019,there are several avenues we can explore as future research.
P19-1633,"
5 Conclusions and Future Work
",2019,"as future work, we will investigate approaches to hyperparameter tuning to find better model architectures for hierarchical multi-label text classification tasks."
P19-1634,"
6 Conclusion
",2019,"fortunately, this will come naturally to researchers in the field as they are already trained to avoid features that encode topic rather than style."
P19-1634,"
6 Conclusion
",2019,"in particular, we strongly suggest that future evaluations should adopt a stateless one-case-at-a-time test policy."
P19-1635,"
6 Conclusion
",2019,"in future work, we plan to extend our work to further applications such as detecting exaggerated statements by investors in social media data."
P19-1636,"
6 Limitations and Future Work
",2019,"finally, we plan to investigate generalized zero-shot learning (liu et al., 2018)."
P19-1636,"
6 Limitations and Future Work
",2019,"furthermore, experimenting with more datasets e.g., rcv1, amazon-13k, wiki-30k, mimic-iii will allow us to confirm our conclusions in different domains."
P19-1636,"
6 Limitations and Future Work
",2019,we also plan to experiment with hierarchical flavors of bert to surpass its length limitations.
P19-1636,"
6 Limitations and Future Work
",2019,we leave the investigation of methods for extremely large label sets for future work.
P19-1636,"
6 Limitations and Future Work
",2019,"we plan to investigate more computationally efficient methods, e.g., dilated cnns (kalchbrenner et al., 2017) and transformers (vaswani et al., 2017; dai et al., 2019)."
P19-1637,"
6 Conclusion
",2019,"an important question for future models, especially interactive ones, is how to signal to the user when their desires do not comport with reality."
P19-1637,"
6 Conclusion
",2019,"while we simulate user behavior for good and random users, future work should compare these systems with end users, as well as compare end user ratings of control with our proposed automated metrics."
P19-1638,"
5 Conclusions and Future work
",2019,"future work can investigate other embedding methods with a richer set of probe tasks, or explore a wider range of downstream tasks."
P19-1639,"
4 Conclusion
",2019,"in the future, we hope to learn models that are able to sample search queries or information needs from sentences and use the output of that model to get relevant documents."
P19-1639,"
4 Conclusion
",2019,we used sentences from the target side of the parallel corpus as queries to retrieve relevant document and use terms from those documents to train a word embedding model along with nmt.
P19-1640,"
7 Conclusion and Future Work
",2019,a future direction is to improve the gan-based training of w-lda.
P19-1640,"
7 Conclusion and Future Work
",2019,another future direction is to experiment with more complex priors than the dirichlet prior.
P19-1640,"
7 Conclusion and Future Work
",2019,the w-lda framework that we have proposed offers the flexibility of matching more sophisticated prior distributions via mmd or gan.
P19-1640,"
7 Conclusion and Future Work
",2019,"we believe these discoveries are of independent interest to the broader research on mmd, gan and wae."
P19-1642,"
5 Conclusions and Future work
",2019,"in future work we will explore other generative models for multi-modal mt, as well as different ways to directly incorporate images into these models."
P19-1642,"
5 Conclusions and Future work
",2019,"we are also interested in modelling different views of the image, such as global vs. local image features, and also in using larger image collections and modelling images directly, i.e.pixel intensities."
P19-1643,"
7 Conclusion
",2019,"in future work, we plan to explore additional representations and architectures to improve the accuracy of our model, and to identify finer-grained alignments between visual actions and their verbal descriptions."
P19-1645,"
9 Conclusion
",2019,"the development of these categories likely interact with the development of the lexicon and acquisition of semantics (feldman et al., 2013; fourtassi and dupoux, 2014), and thus subsequent work should seek to unify more aspects of the acquisition problem."
P19-1645,"
9 Conclusion
",2019,"while this work unifies several components that had previously been studied in isolation, our model assumes access to phonetic categories."
P19-1646,"
8 Conclusion
",2019,our results indicate that rig is a more promising approach to build betterperforming agents capable of displaying strategy and coherence in an end-to-end architecture for visual dialogue.
P19-1647,"
6 Conclusion
",2019,going forward we would like to go beyond matching tasks and evaluate the impact of an explicit speech-to-text decoder as an auxiliary task.
P19-1647,"
6 Conclusion
",2019,"limitations and future work our current model does not include an explicit speech-to-text decoder, which limits the types of analyses we can perform."
P19-1647,"
6 Conclusion
",2019,this would be especially interesting given that one motivation for a visually-supervised end-to-end approach is the un-availability of large amounts of transcribed speech in certain circumstances.
P19-1647,"
6 Conclusion
",2019,we are also planning to investigate how sensitive our approach is to amount of data for the auxiliary task.
P19-1651,"
7 Conclusion
",2019,"we hope that the grounded, goal-driven communication setting that codraw is a testbed for can lead to future progress in building agents that can speak more naturally and better maintain coherency over a long dialog, while being grounded in perception and actions."
P19-1652,"
5 Conclusion and Future Work
",2019,"furthermore, it is also interesting to design a mutual reinforcement mechanisms between the vocabulary constructor and the text generator to improve both components simultaneously."
P19-1652,"
5 Conclusion and Future Work
",2019,"in future, we plan to study more effective ways to construct the image-grounded vocabulary."
P19-1652,"
5 Conclusion and Future Work
",2019,two strategies have then been explored to utilize the constructed vocabulary via hard constraints and soft constraints.
P19-1653,"
6 Conclusions
",2019,our findings suggest that the combination of a deliberation approach and information from additional modalities is a promising direction for machine translation that is robust to noisy input.
P19-1656,"
5 Discussion
",2019,"at the heart of mult is the crossmodal attention mechanism, which provides a latent crossmodal adaptation that fuses multimodal information by directly attending to low-level features in other modalities."
P19-1656,"
5 Discussion
",2019,"we believe the results of mult on unaligned human multimodal language sequences suggest many exciting possibilities for its future applications (e.g., visual question answering tasks, where the input signals is a mixture of static and time-evolving signals)."
P19-1656,"
5 Discussion
",2019,"we hope the emergence of mult could encourage further explorations on tasks where alignment used to be considered necessary, but where crossmodal attention might be an equally (if not more) competitive alternative."
P19-1659,"
6 Conclusions

",2019,"in the future, we would like to extend this work to generate multidocument (multi-video) summaries and also build end-to-end models directly from audio in the video instead of text-based output from pretrained asr."
P19-1660,"
6 Conclusion
",2019,"an orthogonal line of future work might involve using a visual question answering (vqa) task (such as in krishna et al.(2017)), either on its own replacing the captioning task, or in conjunction with the captioning task with a multi-task learning objective."
P19-1660,"
6 Conclusion
",2019,"one possible line of work involves removing the requirement of ground truth bounding boxes altogether by leveraging a recent line of work that does weakly-supervised object detection (such as (oquab et al., 2015; bilen and vedaldi, 2016; zhang et al., 2018; bai and liu, 2017; arun et al., 2018))."
P19-1660,"
6 Conclusion
",2019,there are several interesting avenues for future work.
