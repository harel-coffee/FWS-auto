{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9GrfNNyerEF"
   },
   "source": [
    "### 1.data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3709,
     "status": "ok",
     "timestamp": 1657876815015,
     "user": {
      "displayName": "向熠向",
      "userId": "12244358555449300675"
     },
     "user_tz": -480
    },
    "id": "LfstoyxcoQVd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df_fws = pd.read_excel('data/raw/future work sentence.xlsx')\n",
    "df_abs = pd.read_excel('data/raw/title and abstract.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 444,
     "status": "ok",
     "timestamp": 1657876378269,
     "user": {
      "displayName": "向熠向",
      "userId": "12244358555449300675"
     },
     "user_tz": -480
    },
    "id": "Gahm8iYC5LDl",
    "outputId": "199cf522-e332-42dc-ce3b-6e12a14c162a"
   },
   "outputs": [],
   "source": [
    "df_fws.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 1834,
     "status": "ok",
     "timestamp": 1657872736360,
     "user": {
      "displayName": "向熠向",
      "userId": "12244358555449300675"
     },
     "user_tz": -480
    },
    "id": "YT-nJfwJ42k1",
    "outputId": "0eda7214-4eaa-4f36-a702-6b0bc67f6552"
   },
   "outputs": [],
   "source": [
    "df_abs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 459,
     "status": "ok",
     "timestamp": 1657872741867,
     "user": {
      "displayName": "向熠向",
      "userId": "12244358555449300675"
     },
     "user_tz": -480
    },
    "id": "8n9kcKyM5S5G",
    "outputId": "613c5a7a-5c4b-449c-ea51-d3632e0be8be"
   },
   "outputs": [],
   "source": [
    "types = df_fws['FWS_TYPE'].tolist()\n",
    "set(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 398,
     "status": "ok",
     "timestamp": 1657876386883,
     "user": {
      "displayName": "向熠向",
      "userId": "12244358555449300675"
     },
     "user_tz": -480
    },
    "id": "NlPiWoNXq1R5"
   },
   "outputs": [],
   "source": [
    "# there nan value in abstract of df_abs\n",
    "df_abs = df_abs[df_abs['abs']!=' 0']\n",
    "df_fws = df_fws[df_fws['FWS_TYPE']!='other']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dxtpqwz264jF"
   },
   "source": [
    "##### 1.1 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 389,
     "status": "ok",
     "timestamp": 1657876628280,
     "user": {
      "displayName": "向熠向",
      "userId": "12244358555449300675"
     },
     "user_tz": -480
    },
    "id": "IIbj-Lu0JWZV",
    "outputId": "33a9a616-1fb1-4db1-88ae-0ccb937a1107"
   },
   "outputs": [],
   "source": [
    "len(df_fws['FWS'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 463,
     "status": "ok",
     "timestamp": 1657873179209,
     "user": {
      "displayName": "向熠向",
      "userId": "12244358555449300675"
     },
     "user_tz": -480
    },
    "id": "62gtdQz1jjpF",
    "outputId": "2c70c55c-2aa5-4ef4-c80f-bb2101159284"
   },
   "outputs": [],
   "source": [
    "print(\"The number of the Articles in fws is {}\".format(len(set(df_fws['ID']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1657875279649,
     "user": {
      "displayName": "向熠向",
      "userId": "12244358555449300675"
     },
     "user_tz": -480
    },
    "id": "zLSQyBcnWcXG",
    "outputId": "7dae1a6f-86f3-4cfa-e0fc-8a31c3322c3d"
   },
   "outputs": [],
   "source": [
    "print(\"The number of the Articles in abs is {}\".format(len(df_abs['ID'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sC3N48Uw7glU"
   },
   "outputs": [],
   "source": [
    "fws = df_fws['FWS'].tolist()\n",
    "abs = df_abs['abs'].tolist()\n",
    "titles = df_abs['title'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 747,
     "status": "ok",
     "timestamp": 1657873192264,
     "user": {
      "displayName": "向熠向",
      "userId": "12244358555449300675"
     },
     "user_tz": -480
    },
    "id": "v96XVGHtxQop",
    "outputId": "2ac2dcf0-dcbd-4da9-9809-059f78f7398e"
   },
   "outputs": [],
   "source": [
    "# compare the sentence's average length of those sets\n",
    "import numpy as np\n",
    "\n",
    "len_fw = [len(element.split(\" \")) for element in fws]\n",
    "len_ab = [len(element.split(\" \")) for element in list(set(abs))]\n",
    "len_title = [len(element.split(\" \")) for element in list(set(titles))]\n",
    "\n",
    "\n",
    "print(np.sum(np.array(len_fw))/len(len_fw))\n",
    "\n",
    "print(np.sum(np.array(len_ab))/len(len_ab))\n",
    "\n",
    "print(np.sum(np.array(len_title))/len(len_title))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiWxxIgjyoEJ"
   },
   "source": [
    "* By compare,the length of fws is shorter and the length of abstract is longer\n",
    "* so when we extract the keywords from fws and abstract,we should control the number of keywords.For fws,the number is smaller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3skO06ocbQ2"
   },
   "source": [
    "### 2.Keyword extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9T-2AyaCfcpY"
   },
   "source": [
    "#### KeyBert\n",
    "\n",
    "link: https://towardsdatascience.com/enhancing-keybert-keyword-extraction-results-with-keyphrasevectorizers-3796fa93f4db\n",
    "\n",
    "github: https://github.com/MaartenGr/KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyICZMK5fWfh"
   },
   "outputs": [],
   "source": [
    "!pip install keybert\n",
    "!pip install keyphrase-vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XV1YoQ_5fkSr"
   },
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "\n",
    "kw_model = KeyBERT()\n",
    "vectorizer = KeyphraseCountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_fws = kw_model.extract_keywords(fws,vectorizer=KeyphraseCountVectorizer(),nr_candidates=2*3,top_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_abs = kw_model.extract_keywords(abs,vectorizer=KeyphraseCountVectorizer(),nr_candidates=2*12,top_n=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_titles = kw_model.extract_keywords(titles,vectorizer=KeyphraseCountVectorizer(),nr_candidates=2*3,top_n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbvEBjLCyir3"
   },
   "source": [
    "### 3.Simple preprocess to the extracted keyphrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 rejust the keywords that extracted by keybert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFNO2pohzc0P"
   },
   "source": [
    "* target：Remove meaningless words as much as possible and keep the most important words\n",
    "* implement：\n",
    "  * （1）Set threshold, and words less than threshold are removed\n",
    "  * （2）In a collection, the word with the smaller value is first arranged, and if the word with the smaller value is included in the word with the larger value, the word with the smaller value is deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_fws[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# In a sentence,filter some less important words\n",
    "def filterByValue(datas):\n",
    "  keywords = [data for data in datas]\n",
    "\n",
    "  nKeywords = []\n",
    "    \n",
    "  for elements in keywords:\n",
    "        wValues = [element[1] for element in elements]\n",
    "        threshold = np.mean(np.array(wValues))\n",
    "        \n",
    "        words = []\n",
    "        for element in elements:\n",
    "            if element[1] >= threshold:\n",
    "                words.append(element[0])\n",
    "                \n",
    "        nKeywords.append(words)\n",
    "\n",
    "  return nKeywords\n",
    "\n",
    "\n",
    "# Lemma\n",
    "def filterByLemma(datas):\n",
    "  keywords = [data for data in datas]\n",
    "\n",
    "  nKeywords = []\n",
    "    \n",
    "  for elements in keywords:\n",
    "        nElements = []\n",
    "        \n",
    "        for element in elements:\n",
    "            phrase = \"\"\n",
    "            for word in element[0].split(\" \"):\n",
    "                phrase += lemmatizer.lemmatize(word)\n",
    "                phrase += \" \"\n",
    "                \n",
    "            nElements.append((phrase[:-1],element[1]))\n",
    "       \n",
    "        nKeywords.append(nElements)\n",
    "                \n",
    "  return nKeywords\n",
    "    \n",
    "def filterBySingle(datas):\n",
    "  # Remove the words that have inclusion relationships, with the following examples:\n",
    "  # the original collection： human language、speech、synthetic speech、human language acquisition research、speech signal\n",
    "  # should be removed: human language、speech\n",
    "\n",
    "  # however,there is a problem with this approach, as follows:\n",
    "  # the original collection：human language、language acquisition、human language acquisition reserach\n",
    "  # According to the above rules, it should be removed：human language 、language acquisition，\n",
    "  # But perhaps in other content sections,Language acquisition is the main word\n",
    "  keywords = [data for data in datas]\n",
    "\n",
    "  nKeywords = []\n",
    "\n",
    "  for elements in keywords:\n",
    "    words = []\n",
    "    for i in range(len(elements)):\n",
    "        word = elements[i][0]\n",
    "        \n",
    "        flag = False\n",
    "        \n",
    "        for j in range(i+1,len(elements)):\n",
    "            # when i+1 > len(elements) , no errors will be reported and no build will continue to be made, so there is no impact \n",
    "            # word[:-1] The main purpose is to avoid the situation that the word \"keyphrases\" cannot be converted into \"keyphrase\" by lemma or stemm\n",
    "            if word in elements[j][0] or word[:-1] in elements[j][0]:\n",
    "                flag = True\n",
    "                break\n",
    "        \n",
    "        if flag:\n",
    "            continue\n",
    "        else:\n",
    "            words.append((word,elements[i][1]))\n",
    "            \n",
    "    nKeywords.append(words)\n",
    "    \n",
    "  return nKeywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_keywords = filterByLemma(keywords_fws)\n",
    "f_keywords = filterBySingle(f_keywords)\n",
    "f_keywords = filterByValue(f_keywords)\n",
    "f_keywords[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Is7mfLRs6w4K"
   },
   "source": [
    "#### 3.2 Combine these phrases based on year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(datas,years):\n",
    "    resDict = {}\n",
    "    for year in list(set(years)):\n",
    "        resDict[year] = {}\n",
    "    \n",
    "    for i in range(len(years)):\n",
    "        elements = datas[i]\n",
    "        year = years[i]\n",
    "        \n",
    "        words = [element[0] for element in elements]\n",
    "        \n",
    "        for word in words:\n",
    "            if word in resDict[year]:\n",
    "                resDict[year][word] += 1\n",
    "            else:\n",
    "                resDict[year][word] = 1\n",
    "        \n",
    "    for year in list(set(years)):\n",
    "        try:\n",
    "            resDict[year] = sorted(resDict[year].items(),key=lambda x:x[1],reverse=True)\n",
    "        except Exception as e:\n",
    "            print(year)\n",
    "            \n",
    "    return resDict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_dict = combine(f_keywords,df_fws['Year'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orMiYdiKkUWo"
   },
   "source": [
    "#### 3.3 data save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveData(datas,years,filename):\n",
    "    resDict = combine(datas,years)\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    min_len = 10000\n",
    "    for year in list(set(years)):\n",
    "        length = len(resDict[year])\n",
    "        if length < min_len:\n",
    "            min_len = length\n",
    "            \n",
    "    for year in list(set(years)):\n",
    "        df[str(year)] = [element[0] for element in resDict[year]][:min_len]\n",
    "        df['count'+str(year)] = [element[1] for element in resDict[year]][:min_len]\n",
    "    \n",
    "    df.to_excel(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveData(f_keywords,df_fws['Year'].tolist(),'fws_keywords.xlsx')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "B9GrfNNyerEF",
    "_3skO06ocbQ2",
    "wft9BW5NcnOj",
    "YN82BFmLd8TX",
    "WgXd1I9qvfaU",
    "jtrHo4DEABKT",
    "ORU5G8D9_Pya"
   ],
   "name": "ExtractKeyphrases.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "vscode": {
   "interpreter": {
    "hash": "f3809b6174eebfea234ac6faba85ebc971f15b87031a02e15e94fb2aa656411a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
