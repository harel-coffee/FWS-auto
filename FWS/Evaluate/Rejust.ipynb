{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33c16dfe",
   "metadata": {},
   "source": [
    "### 1. data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86cb98b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2000</th>\n",
       "      <th>count2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>count2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>count2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>count2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>count2004</th>\n",
       "      <th>...</th>\n",
       "      <th>2017</th>\n",
       "      <th>count2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>count2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>count2019</th>\n",
       "      <th>2020</th>\n",
       "      <th>count2020</th>\n",
       "      <th>2021</th>\n",
       "      <th>count2021</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>corpus</td>\n",
       "      <td>7</td>\n",
       "      <td>machine translation</td>\n",
       "      <td>5</td>\n",
       "      <td>statistical machine translation</td>\n",
       "      <td>4</td>\n",
       "      <td>statistical machine translation</td>\n",
       "      <td>7</td>\n",
       "      <td>statistical machine translation</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>neural machine translation</td>\n",
       "      <td>23</td>\n",
       "      <td>neural machine translation</td>\n",
       "      <td>50</td>\n",
       "      <td>neural machine translation</td>\n",
       "      <td>48</td>\n",
       "      <td>neural machine translation</td>\n",
       "      <td>31</td>\n",
       "      <td>language model</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hidden markov model</td>\n",
       "      <td>3</td>\n",
       "      <td>corpus</td>\n",
       "      <td>4</td>\n",
       "      <td>word sense disambiguation</td>\n",
       "      <td>2</td>\n",
       "      <td>question</td>\n",
       "      <td>5</td>\n",
       "      <td>feature</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>attention</td>\n",
       "      <td>11</td>\n",
       "      <td>word embeddings</td>\n",
       "      <td>18</td>\n",
       "      <td>attention</td>\n",
       "      <td>27</td>\n",
       "      <td>language model</td>\n",
       "      <td>23</td>\n",
       "      <td>neural machine translation</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>word sense disambiguation</td>\n",
       "      <td>3</td>\n",
       "      <td>constraint</td>\n",
       "      <td>4</td>\n",
       "      <td>grammar</td>\n",
       "      <td>2</td>\n",
       "      <td>web</td>\n",
       "      <td>3</td>\n",
       "      <td>conditional random field</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>neural network</td>\n",
       "      <td>8</td>\n",
       "      <td>attention</td>\n",
       "      <td>13</td>\n",
       "      <td>text generation</td>\n",
       "      <td>20</td>\n",
       "      <td>bert</td>\n",
       "      <td>23</td>\n",
       "      <td>bert</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>empirical study</td>\n",
       "      <td>3</td>\n",
       "      <td>question</td>\n",
       "      <td>3</td>\n",
       "      <td>hmm</td>\n",
       "      <td>2</td>\n",
       "      <td>support vector machine</td>\n",
       "      <td>3</td>\n",
       "      <td>feature selection</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>language</td>\n",
       "      <td>7</td>\n",
       "      <td>text</td>\n",
       "      <td>12</td>\n",
       "      <td>question</td>\n",
       "      <td>18</td>\n",
       "      <td>self</td>\n",
       "      <td>18</td>\n",
       "      <td>transformer</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>information extraction</td>\n",
       "      <td>3</td>\n",
       "      <td>grammar</td>\n",
       "      <td>3</td>\n",
       "      <td>word</td>\n",
       "      <td>2</td>\n",
       "      <td>information extraction</td>\n",
       "      <td>3</td>\n",
       "      <td>active learning</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>word embeddings</td>\n",
       "      <td>7</td>\n",
       "      <td>question</td>\n",
       "      <td>12</td>\n",
       "      <td>text</td>\n",
       "      <td>17</td>\n",
       "      <td>transformer</td>\n",
       "      <td>18</td>\n",
       "      <td>text</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        2000  count2000                 2001  count2001  \\\n",
       "0                     corpus          7  machine translation          5   \n",
       "1        hidden markov model          3               corpus          4   \n",
       "2  word sense disambiguation          3           constraint          4   \n",
       "3            empirical study          3             question          3   \n",
       "4     information extraction          3              grammar          3   \n",
       "\n",
       "                              2002  count2002  \\\n",
       "0  statistical machine translation          4   \n",
       "1        word sense disambiguation          2   \n",
       "2                          grammar          2   \n",
       "3                              hmm          2   \n",
       "4                             word          2   \n",
       "\n",
       "                              2003  count2003  \\\n",
       "0  statistical machine translation          7   \n",
       "1                         question          5   \n",
       "2                              web          3   \n",
       "3           support vector machine          3   \n",
       "4           information extraction          3   \n",
       "\n",
       "                              2004  count2004  ...  \\\n",
       "0  statistical machine translation          9  ...   \n",
       "1                          feature          4  ...   \n",
       "2         conditional random field          4  ...   \n",
       "3                feature selection          4  ...   \n",
       "4                  active learning          3  ...   \n",
       "\n",
       "                         2017  count2017                        2018  \\\n",
       "0  neural machine translation         23  neural machine translation   \n",
       "1                   attention         11             word embeddings   \n",
       "2              neural network          8                   attention   \n",
       "3                    language          7                        text   \n",
       "4             word embeddings          7                    question   \n",
       "\n",
       "   count2018                        2019  count2019  \\\n",
       "0         50  neural machine translation         48   \n",
       "1         18                   attention         27   \n",
       "2         13             text generation         20   \n",
       "3         12                    question         18   \n",
       "4         12                        text         17   \n",
       "\n",
       "                         2020  count2020                        2021  \\\n",
       "0  neural machine translation         31              language model   \n",
       "1              language model         23  neural machine translation   \n",
       "2                        bert         23                        bert   \n",
       "3                        self         18                 transformer   \n",
       "4                 transformer         18                        text   \n",
       "\n",
       "   count2021  \n",
       "0         52  \n",
       "1         43  \n",
       "2         31  \n",
       "3         26  \n",
       "4         20  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_fws = pd.read_excel('data/Keyphrases/fws_keywords_less.xlsx')\n",
    "df_abs = pd.read_excel('data/Keyphrases/abs_keywords.xlsx')\n",
    "df_ts =  pd.read_excel('data/Keyphrases/titles_keywords.xlsx')\n",
    "df_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b94630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(2000,2022)\n",
    "\n",
    "abs_titles_dict = {}\n",
    "\n",
    "for year in years:\n",
    "\n",
    "    resDict = {}\n",
    "    \n",
    "    try:\n",
    "        abs_words = df_abs[str(year)]\n",
    "    except Exception as e:\n",
    "        abs_words = df_abs[year]\n",
    "        \n",
    "    abs_weight = df_abs['count'+str(year)]\n",
    "    \n",
    "    for i in range(len(abs_words)):\n",
    "        resDict[abs_words[i]] = abs_weight[i]\n",
    "        \n",
    "    try:\n",
    "        title_words = df_ts[str(year)]\n",
    "    except Exception as e:\n",
    "        title_words = df_ts[year]\n",
    "        \n",
    "    title_weight = df_ts['count'+str(year)]\n",
    "    \n",
    "    for i in range(len(title_words)):\n",
    "        if title_words[i] in resDict:\n",
    "            resDict[title_words[i]] += title_weight[i]\n",
    "        else:\n",
    "            resDict[title_words[i]] = title_weight[i]\n",
    "    \n",
    "    elements = sorted(resDict.items(),key=lambda x:x[1],reverse=True)\n",
    "\n",
    "    abs_titles_dict[str(year)] = elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "628ad886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(abs_titles_dict['2000'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f2e195",
   "metadata": {},
   "source": [
    "### 2. keywords rejust\n",
    "\n",
    "* Remove hyphens\n",
    "* Abbreviation merge\n",
    "* Remove some meaningless modifier adjectives,such as: more classfiy 、classify;more tagger、tagger;different\n",
    "* Remove some meaningless suffixes,such as: task、technique、technologies\n",
    "* For words like bert and bert model, merge\n",
    "* Combine the words that have been stemmed and extracted\n",
    "* Remove stopwords\n",
    "\n",
    "other words like dialogue system、dialog system and so on,we combine these words by human observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b527d7",
   "metadata": {},
   "source": [
    "#### 2.1 Write functional functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f74b57ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove hyphen\n",
    "# word - word 、 word- word、 word -word 三种形式\n",
    "import regex as re\n",
    "\n",
    "pattern = r'(\\s*)-(\\s*)'\n",
    "\n",
    "def filterHyphen(words,values):\n",
    "    resDict = {}\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        resDict[words[i]] = values[i]\n",
    "        \n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "        newWord = re.sub(pattern,\"\",word)\n",
    "        \n",
    "        if newWord == word:\n",
    "            continue\n",
    "            \n",
    "        if newWord in resDict.keys():\n",
    "            resDict[newWord] += resDict[word]\n",
    "            del resDict[word]\n",
    "        else:\n",
    "            resDict[newWord] = resDict[word]\n",
    "            del resDict[word]\n",
    "        \n",
    "    return list(resDict.keys()),list(resDict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b43bc314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the word's acronyms into the original form\n",
    "def combineByAcronym(words,values):\n",
    "    resDict = {}\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        resDict[words[i]] = values[i]\n",
    "        \n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "        chars = word.split(\" \")\n",
    "        new_str = \"\"\n",
    "        \n",
    "        if len(chars) > 1:\n",
    "            for single_char in chars:\n",
    "                new_str += single_char[0]\n",
    "        \n",
    "        if new_str in resDict.keys():\n",
    "            resDict[word] += resDict[new_str]\n",
    "            del resDict[new_str]\n",
    "        \n",
    "        \n",
    "    return list(resDict.keys()),list(resDict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "261b564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove meaningless part from words\n",
    "adjs = ['more','different','most','much','better','other','such','new','noun','verb']    \n",
    "suffix = ['model','task','technique','technologies','technology','approch','system']\n",
    "\n",
    "def combineBySpecialWordAdj(words,values):\n",
    "    resDict = {}\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        resDict[words[i]] = values[i]\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "        new_str = \"\"\n",
    "        for adj in adjs:\n",
    "            if adj in word:\n",
    "                new_str = word.replace(adj,\"\")\n",
    "                break\n",
    "        \n",
    "        new_str = new_str.strip()\n",
    "        \n",
    "        if new_str in resDict.keys():\n",
    "            resDict[new_str] += resDict[word]\n",
    "            del resDict[word]\n",
    "        \n",
    "    return list(resDict.keys()),list(resDict.values())\n",
    "\n",
    "def combineBySpecialWordSuffix(words,values):\n",
    "    resDict = {}\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        resDict[words[i]] = values[i]\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "        new_str = \"\"\n",
    "        for suf in suffix:\n",
    "            if suf in word:\n",
    "                new_str = word.replace(suf,\"\")\n",
    "                break\n",
    "        \n",
    "        new_str = new_str.strip()\n",
    "                  \n",
    "        if new_str in resDict.keys():\n",
    "            resDict[word] += resDict[new_str]\n",
    "            del resDict[new_str]\n",
    "        \n",
    "    return list(resDict.keys()),list(resDict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c260f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer  \n",
    "snowball_stemmer = SnowballStemmer(\"english\")  \n",
    "\n",
    "def combineByStem(words,values):\n",
    "    resDict = {}\n",
    "    newDict = {}\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        resDict[words[i]] = values[i]\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "        phrase = \"\"\n",
    "        for singleword in word.split(\" \"):\n",
    "            phrase += snowball_stemmer.stem(singleword)\n",
    "            phrase += \" \"\n",
    "        phrase = phrase[:-1]\n",
    "        \n",
    "        if phrase in newDict:\n",
    "            newDict[phrase] += resDict[word]\n",
    "        else:\n",
    "            newDict[phrase] = resDict[word]\n",
    "    \n",
    "    return list(newDict.keys()),list(newDict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0183e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "df_stopwords = pd.read_csv('data/raw/Stopwords.csv')\n",
    "stopwords = df_stopwords['word'].tolist()\n",
    "\n",
    "def filterStopwords(words,values):\n",
    "    resDict = {}\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        \n",
    "        flag = False\n",
    "        elements = word.split(\" \")\n",
    "        for element in elements:\n",
    "            if element in ['other','task','future','direction','improve','improvement','problem']:\n",
    "                flag = True\n",
    "                break\n",
    "        if flag == True:\n",
    "            continue\n",
    "        \n",
    "        resDict[word] = values[i]\n",
    "    \n",
    "    return list(resDict.keys()),list(resDict.values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23471d8b",
   "metadata": {},
   "source": [
    "#### 2.2 Unify processing flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "528bbdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rejust(datas,values):\n",
    "    word_1,value_1 =  filterHyphen(datas,values)\n",
    "    word_2,value_2 = combineByAcronym(word_1,value_1)\n",
    "    word_3,value_3 = combineBySpecialWordAdj(word_2,value_2)\n",
    "    word_4,value_4 = combineBySpecialWordSuffix(word_3,value_3)\n",
    "    word_5,value_5 = filterStopwords(word_4,value_4)\n",
    "    word_6,value_6 = combineByStem(word_5,value_5)\n",
    "    word_7,value_7 = combineBySpecialWordAdj(word_6,value_6)\n",
    "    \n",
    "    resDict = {}\n",
    "    for i in range(len(word_7)):\n",
    "        resDict[word_7[i]] = value_7[i]\n",
    "        \n",
    "    return sorted(resDict.items(),key = lambda x:x[1],reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1773b631",
   "metadata": {},
   "source": [
    "#### 2.3 data save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9380f699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def SaveData(df,years,filename):\n",
    "    allDict = {}\n",
    "    for year in years:\n",
    "        try:\n",
    "            data = df[year]\n",
    "        except Exception as e:\n",
    "            data = df[int(year)]\n",
    "            \n",
    "        value = df['count'+str(year)]\n",
    "        \n",
    "        resDict = rejust(data.tolist(),value.tolist())\n",
    "        allDict[str(year)] = resDict\n",
    "\n",
    "    with open(filename,'w') as f:\n",
    "        json.dump(allDict,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c8a76e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [str(year) for year in range(2000,2022)]\n",
    "SaveData(df_abs,years,'abs_keywords_rejusted.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb526394",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [str(year) for year in range(2000,2021)]\n",
    "SaveData(df_fws,years,'fws_keywords_rejusted.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
